{"title": "On the Compression of Language Models for Code: An Empirical Study on CodeBERT", "authors": ["Giordano d'Aloisio", "Luca Traini", "Federica Sarro", "Antinisca Di Marco"], "abstract": "Language models have proven successful across a wide range of software engineering tasks, but their significant computational costs often hinder their practical adoption. To address this challenge, researchers have begun applying various compression strategies to improve the efficiency of language models for code. These strategies aim to optimize inference latency and memory usage, though often at the cost of reduced model effectiveness. However, there is still a significant gap in understanding how these strategies influence the efficiency and effectiveness of language models for code. Here, we empirically investigate the impact of three well-known compression strategies \u2013 knowledge distillation, quantization, and pruning \u2013 across three different classes of software engineering tasks: vulnerability detection, code summarization, and code search. Our findings reveal that the impact of these strategies varies greatly depending on the task and the specific compression method employed. Practitioners and researchers can use these insights to make informed decisions when selecting the most appropriate compression strategy, balancing both efficiency and effectiveness based on their specific needs.", "sections": [{"title": "I. INTRODUCTION", "content": "The growing adoption of transformer-based Language Models (LMs) has radically transformed the software engineering (SE) field in recent years. These models have been effectively applied to a wide array of SE tasks, including vulnerability detection [1], code summarization [2], and code search [3], consistently demonstrating superior performance over traditional approaches [4]. However, despite their impressive capabilities, the widespread adoption of these models is often hindered by practical challenges, particularly their high computational cost [5]. The deployment of LMs typically requires computations across millions or even billions of learned parameters, resulting in significant memory demands and high inference latency. To address this issue, AI researchers have developed various strategies over the past decade to reduce the size and computational cost of LMs, including techniques such as knowledge distillation [6], quantization [7], and pruning [8]. These \u201cmodel compression\" strategies can reduce the memory demand of LMs and/or speed up their inference times, albeit often at the cost of reduced effectiveness (i.e., prediction correctness). These strategies have recently begun to gain attention in the field of software engineering [9]-[11]. For instance, Shi et al. [9] applied knowledge distillation to drastically reduce the memory size of models of code. Wei et al. [11] have investigated the impact of quantization on code generation tasks regarding inference latency, memory consumption, and carbon footprint. However, despite these initial efforts, the broader impact of compression strategies on software en-gineering tasks remains largely unexplored. Most existing research has focused on specific compression strategies applied to individual SE tasks, making it difficult to determine whether specific strategies perform better than others on particular tasks. Additionally, it is unclear if the impact of different strategies varies by task or if they demonstrate similar behavior across different software engineering tasks.\nIn this study, we investigate the impact of different model compression strategies across three software engineering tasks: vulnerability detection (code classification), code summariza-tion (code-to-text generation), and code search (text-to-code recommendation). We fine-tune a well-known language model for code, CodeBERT [12], on each of these tasks. Subsequently, we assess how three model compression strategies \u2013 namely, knowledge distillation, quantization, and pruning \u2013 affect (i) the effectiveness of the LM in performing the task, (ii) inference latency, and (iii) the model's memory size. Our results provide practitioners and researchers with guidelines on balancing the trade-offs between effectiveness and efficiency when selecting a model compression strategy.\nIn summary, the main contributions of this work are:\n\u2022 An extensive empirical evaluation of the impact of three compression strategies, namely knowledge distillation, quantization, and pruning on inference time, model size and effectiveness of LMs fine-tuned on the vulnerability detection, code summarization and code search tasks;\n\u2022 Insights for practitioners and researchers on the adoption of those compression strategies;\n\u2022 A publicly available replication package of our empirical study [13]."}, {"title": "II. BACKGROUND AND RELATED WORK", "content": "A. Language Models in Software Engineering Tasks\nSince its introduction by Vaswani et al. [14], the transformer architecture has become the de facto standard in language modeling. Transformer-based language models have achieved state-of-the-art performance across numerous natural language processing tasks [15]\u2013[17], and have recently gained traction in the software engineering field [4].\nThis trend has led to the development of several LMs specialized in code, such as CodeBERT [12], CodeT5 [18], and Codex [19]. These models are usually trained in a two-step process: (i) a pre-trained step using a self-supervised training objective aiming at providing the model with general knowledge about source code constructs and patterns, and (ii) a fine-tuning step aiming at tailoring the model for the software engineering task at hand.\nOver the past few years, LMs for code have been fine-tuned to automate a wide range of SE tasks [4]. For example, CodeBERT an encoder-only LM based on the BERT architecture [17] \u2013 has been widely and successfully applied to code summarization [20], vulnerability detection [21], and code search [12], among others [4].\nB. Compression Strategies for Language Models\nA major obstacle to the practical adoption of language models has always been their significant computational cost [4], [5]. To address this issue and increase their sustainability, the AI community has developed various strategies to reduce the memory demands and inference latency of these models. Nowadays, the three most popular model compression strate-gies are [9], [10]: knowledge distillation [6], quantization [7], and pruning [8].\n1) Knowledge distillation: is a technique in which a smaller model (i.e., the \u201cstudent\u201d) is trained to replicate the behavior of a larger, pre-trained language model (i.e., the \u201cteacher\"). This compression strategy results in a model that demands less memory and provides faster inference time. However, since student models usually have thinner and shallower neural networks, they often struggle to fully capture the knowledge embedded in the larger models. This limitation typically leads to a reduction in the LM capabilities compared to the teacher model [22].\n2) Quantization: is a compression strategy that reduces the precision of the model's weights, converting them from full-precision (e.g., 32-bit floating point) to lower precision representations (e.g., 8-bit integer). Two broad categories of quantization strategies exist: post-training quantization and quantization-aware training. Post-training quantization gen-erates a quantized model from an existing full-precision model without requiring additional training or fine-tuning. This strategy is a popular choice due to its low computational cost. However, it is more susceptible to quantization noise. In contrast, quantization-aware training involves training the model from scratch while incorporating simulated quantization operations to mitigate the quantization noise. Although this approach can produce a more effective model, its high training costs can make it often impractical.\n3) Pruning: aims to make the language model more ef-ficient by removing neural network weights considered less critical for the model's effectiveness [23]. Various pruning methodologies exist. For instance, structured pruning modifies the model's architecture by removing entire structures within the neural network, such as neurons, filters, or even layers [24]. Conversely, unstructured pruning targets individual weights [25], removing the less relevant ones (e.g., those close to zero). Pruning can be applied either to individual layers of the network (layer-wise pruning) [26], or across the entire model (global pruning) [27].\nC. Compression Strategies in Software Engineering Tasks\nModel compression strategies have recently gained rele-vance in the software engineering literature. Shi et al. [9] intro-duced Compressor, a knowledge distillation-based approach, evaluated on CodeBERT [12] and GraphCodeBERT [28] for two code classification tasks (i.e., vulnerability detection and clone detection). Their results demonstrate that Compressor can considerably accelerate inference time with minimal im-pact on model effectiveness. In a subsequent study [10], the authors expanded their approach to tackle energy consumption and carbon footprint concerns. Wei et al. [11] conducted an empirical evaluation of quantized models on code generation tasks, examining resource usage, carbon footprint, accuracy, and robustness. They found that quantization, under specific settings, can substantially enhance model efficiency with neg-ligible accuracy or robustness trade-offs. Additionally, Sun et al. [2] explored dynamic inference as a method to speed up code completion. Despite these advancements, there remains a noticeable gap in comprehensive studies that systematically investigate the effects of different model compression strate-gies across various software engineering tasks.\nWith this study, we aim to fill this gap by performing an extensive empirical evaluation of the impact of the three most adopted compression strategies \u2013 knowledge distillation, model quantization, and model pruning \u2013 on the inference time, model size, and prediction's effectiveness of an LM fine-tuned for three widely adopted SE tasks \u2013 vulnerability detection, code summarization, and code search.\""}, {"title": "III. EMPIRICAL STUDY DESIGN", "content": "The goal of this study is to analyse the impact of compres-sion strategies on the efficiency (i.e., in terms of inference time and model size) and effectiveness (i.e., in terms of prediction correctness) of language models for code. Specifically, we investigate the impact of three compression strategies \u2013 knowl-edge distillation, pruning, and quantization on CodeBERT models fine-tuned for three SE tasks: vulnerability detection, code summarization, and code search. We selected these tasks due to their relevance in software engineering [1]\u2013[3], [29], [30] and because they span diverse categories, namely code classification (vulnerability detection), code-to-text generation (code summarization), and text-to-code recommendation (code search). We use CodeBERT as the reference language model due to its popularity in the software engineering literature [4] and its versatility in handling classification, generation, and recommendation tasks [12].\nOur research is driven by the following research questions:\nRQ1 How do compression strategies impact the efficiency and effectiveness of models fine-tuned for vulnerability detection?\nRQ2 How do compression strategies impact the efficiency and effectiveness of models fine-tuned for code summariza-tion?\nRQ3 How do compression strategies impact the efficiency and effectiveness of models fine-tuned for code search?\nTo answer these RQs we carry out an empirical investigation based on the methodology described in the following. Figure 1 provides an overview of our experimental methodology. We first fine-tune CodeBERT for each SE task of interest and collect the corresponding effectiveness and efficiency metrics. Next, we apply each of the three compression strategies individually to produce compressed versions of the fine-tuned models. Finally, we compare the efficiency and effectiveness metrics of the compressed models with those of the original fine-tuned CodeBERT model to assess the impact of each compression strategy.\nTable I shows the effectiveness and efficiency metrics used in our study. For effectiveness, we consider specific metrics depending on the software engineering task at hand. For instance, we use Accuracy, F1 Score and MCC to measure the effectiveness of the LM for the vulnerability detection (i.e., code classification) task. For efficiency, we focus on two aspects: inference time (in seconds) and model memory size (in MB). We collect inference time metrics for both CPU and GPU environments, as compression strategies are frequently used to adapt language models for constrained hardware setups, such as desktop computers without dedicated GPUs. The experimental process took around ten days of machine execution over a CentOS HPC cluster equipped with 32 Intel(R) Xeon(R) Gold 6140M CPUs and two Nvidia A100 and A30 GPUs.\nA. Software Engineering Tasks\n1) Vulnerability Detection: In this task, the language model is prompted with a code function and asked to predict whether it contains a security vulnerability. The model produces a binary label indicating whether the code is vulnerable or not. We fine-tune and evaluate CodeBERT using the Devign dataset by Zhou et al. [38]. This dataset contains 27,318 C functions extracted from two popular open-source projects (QEMU and FFmpeg). Each function is accompanied by a label that denotes whether the code contains a security vulnerability or not.\n2) Code Summarization: It is the task of automatically generating natural language summaries (namely comments) for code snippets. The language model is given a code function to produce code summary. We use a Sequence-to-Sequence (Seq2Seq) model, which includes CodeBERT as encoder layer and a six-layer Transformer as decoder layer. We fine-tune and evaluate CodeBERT on the CodeSearchNet dataset [39]. This dataset contains 2 million code-comment pairs extracted from open-source repositories written in different languages, such as Python, Javascript, Ruby, Go, Java, and PHP. For this task, we focus on the Java programming language for a total of 181,061 code-comment pairs.\n3) Code Search: Given a natural language sentence (i.e., comment), this task aims to retrieve semantically relevant code snippets. This is performed by first producing the embeddings of the comment and the code snippets. Then the semantically similar code snippets are ranked using the embedding simi-larity of sentence and code (through inner dot product). We fine-tune and evaluate CodeBERT using the CodeSearchNet dataset for the Python programming language, which contains a total of 280,634 code-comment pairs [39].\nFollowing previous works [9], [10], we reuse the pipeline provided by the CodeXGLUE benchmark [40] for all the aforementioned tasks. CodeXGLUE is a popular benchmark which provides data and code for fine-tuning and evaluating LMs on different code-related tasks. We extended the pipeline by adding the code required to compress the LMs using knowledge distillation, quantization, and pruning."}, {"title": "B. Compression Strategies", "content": "1) Knowledge Distillation: Knowledge distillation is a computationally complex task, as it typically involves re-training the \"student\" model from scratch. Given the exten-siveness of our experimental setup, we opted not to retrain a distilled model ourselves. Instead, we utilized a pre-trained distilled BERT model, namely DistilBERT [41], which we fine-tuned for the specific SE task of interest. For vulnera-bility detection and code search tasks, we directly fine-tuned DistilBERT. For code summarization, we used DistilBERT as the encoder layer of a Seq2Seq model, which we then fine-tuned using the same procedure.\n2) Quantization: Model Quantization reduces a model's size by changing its weights' precision from the standard float32 to less precise data types. We apply post-training quantization (see Section II) implemented by the Hugging Face's optimum-quanto library.\u00b9 We chose this implementation because it requires minimum configuration and supports CPU and GPU. We tested three different applications of post-training quantization by reducing the weights to int4, int8, and float8 (as, by the time we ran our experiments, the library allowed those three reductions). Following the library's documentation, we first quantized the fine-tuned model and then calibrated its activation functions using the validation set. Finally, following again the documentation, we frozen the quantized weights before storing the model.\n3) Pruning: In our experiments, we analyse the unstruc-tured global pruning implemented by the PyTorch library.\u00b2 We have chosen this implementation because it does not change the internal structure of a network; hence, it does not require the re-training of the model after its application. Following the work of Gordon et al. on pruning BERT models [42], for each task, we prune the weights of all the linear layers of the network using the LI norm as the selection strategy. The LI norm strategy uses the sum of the absolute values of a vector's components to determine the importance of structures within a neural network [43]. We analyse three different configurations of pruning: one in which we prune the 20% of weights in the whole network (Prune 0.2), one in which we prune the 40% (Prune 0.4), and one in which we prune the 60% (Prune 0.6). As done for quantization, we first fine-tuned the models for each task and then pruned them. Finally, before storing the model, we removed the internal copy of the original weights created by the PyTorch library after the application of pruning."}, {"title": "C. Efficiency Metrics", "content": "For efficiency, we indicate the performance of a model in terms of the time required to give a prediction (i.e., inference time) and the memory size of a model.\n1) Inference time: We measure the inference time for each batch of the testing set. Following the CodeXGLUE benchmark, we consider a batch size of 64 test instances for each task. The inference time is measured both on CPU and GPU (CUDA). For CPU, we use the time Python function, while for GPU, we use the Event class provided by PyTorch. Moreover, before computing the inference time on the GPU, we perform a series of warm-up iterations to avoid inconsistencies in the results. In addition, we apply GPU synchronization after each inference iteration.\nTo assess the impact of compression strategies, we compare the inference time measurements of each compressed model with those of the original fine-tuned CodeBERT model. To ensure rigor, we follow performance engineering best practices [44]-[47], specifically the approach proposed by Kalibera and Jones [48], to build confidence intervals for the relative change in measurements statistics. Specifically, we construct the confi-dence interval for the median relative change in inference time using bootstrapping with 10,000 iterations, involving random resampling with replacement [49]. The main advantage of this technique, compared to others such as the Wilcoxon test [50], is that it provides a clear and rigorous account of the inference time change and the associated uncertainty [48], [49], [51]. For example, this method allows us to state that a compressed model is faster than the original CodeBERT model by -30%\u00b12% with 95% confidence. We consider a difference to be statistically significant if the confidence interval is not greater than the percentage change.\n2) Model size: To measure the model size, we save the model state in memory using the save function provided by PyTorch and then calculate its size using the getsize Python function. The value the function returns is converted to megabytes (MB). Although we performed this process on both CPU and GPU, as expected, the models' size remained unchanged between the two environments. Therefore, we do not differentiate between CPU and GPU when reporting the models' size in Section IV."}, {"title": "D. Effectiveness Metrics", "content": "For effectiveness, we assess how good the predictions of a model are for a specific task. Given the heterogeneity of tasks involved in our evaluation, we used different metrics depending on the SE task being analyzed (see Table I).\n1) Vulnerability Detection: We use the Matthews Corre-lation Coefficient (MCC) as it considers all quadrants of the classification matrix (i.e., it gives a comprehensive overview of the model's performance). It has been shown to be a reliable measure when handling imbalanced data, as is often the case for vulnerability prediction [33], [52]\u2013[54]. MCC is defined as a correlation factor between the true and predicted labels. It ranges from -1 to 1, where -1 means the model gives opposite predictions, 0 means random predictions, and 1 means perfect predictions. In our study, we also report on F1 Score [32] and Accuracy [31] for compatibility with respect to previous work. The F1 Score is defined as the harmonic mean between Precision and Recall. Accuracy is defined as the number of correct predictions over the whole predictions of a model. Both F1 Score and Accuracy values range from 0 to 1, where 1 is the best value. Although the use of Accuracy is deprecated for problems suffering from data imbalance [54], we include this measure in our analysis for completeness as it is the metric employed by the CodeXGLUE benchmark. Still, we discourage its use in practice as done in previous work [54].\n2) Code Summarization: We employ three metrics that assess different aspects of the quality of a generated text [55]. Bleu is the metric employed in the CodeXGLUE benchmark and is a standard metric adopted in natural language translation and, generally, text generation tasks [34]. It computes how similar a generated text is with respect to a reference baseline by comparing overlapping n-grams (contiguous sequences of n words) between the generated and reference texts. It is a metric of summary-summary text similarity, but has been criticised for not considering the semantic similarity between two texts [11], [55]. For this reason, we extended the evaluation by including two additional metrics. BERTScore evaluates the quality of a generated text by comparing the similarity between the BERT embeddings of the generated text and the reference baseline [35]. It is a metric of summary-summary semantic similarity [55]. Finally, SIDE is a metric based on contrastive learning that measures how good a generated explanation is for a given code snippet without considering a reference baseline [36]. It is specific for code summarization tasks and is a metric of summary-code semantic similarity [55]. All metrics range between 0 and 1, where 1 is the optimum value.\n3) Code Search: We employ three versions of the Mean Reciprocal Rank (MRR) score [37]. We adopt this metric because it is used in the CodeXGLUE benchmark and is by far the most commonly adopted metric in code search [56]. MRR measures the average of the reciprocal ranks of the correct results for a set of code comments. The reciprocal rank for a single code comment is defined as the inverse of the rank position where the correct corresponding code appears in the list of retrieved results. In addition, we include two variations of MRR: MRR@1, which measures the proportion of code comments where the correct code appears in the first position, and MRR@5, which calculates the mean reciprocal rank based on the top five results. These metrics all range from 0 to 1, with 1 representing the optimal score."}, {"title": "IV. EMPIRICAL STUDY RESULTS", "content": "In this section, we report the result of our empirical evalua-tion. For each RQ, we discuss the impact of each compression strategy relative to the model's efficiency and effectiveness, as well as the trade-off between these two aspects.\nTable III shows the results for each RQ. On each sub-table, the first row reports the results of the plain CodeBERT model (i.e., without compression), while the remaining rows show the percentage variations provided by each compression strategy.\u00b3 For inference time, we also report the confidence interval for the change using the Kalibera and Jones approach [49] (see Section III-C for details). Non-statistically significant changes are marked with an asterisk (*). For each considered efficiency or effectiveness metric, the best values are highlighted in bold, while the worst values are underlined. We also present scatter plots showing the trade-off between effectiveness (y-axis) and efficiency (x-axis) for each RQ in Figure 2.\nA. RQ1 Results - Vulnerability Detection\nWe analyse the impact of compression strategies from various aspects: inference time, model size, and vulnerability detection effectiveness. Additionally, we investigate the trade-offs involved among these metrics.\n1) Inference Time: As reported in Table IIIa, all compres-sion strategies change the inference time of the vulnerability detection model with statistical significance. However, the impact varies widely depending on the specific compression strategy and the hardware environment. On the CPU, for example, the most aggressive configuration of pruning (0.6) leads to the highest speed-up across all the strategies, with an inference time reduction of -67.9% compared to the plain CodeBERT model. Interestingly, less aggressive forms of pruning (0.2 and 0.4) lead to the opposite outcome, with an inference slow-down of +15.5% and +18.8%, respectively. All pruning configurations also negatively affect inference time in GPU environments, with slow-downs of up to +9.1%. A possible explanation for this behavior could be identified in the lower ability of GPU to handle sparse-matrix multiplications [26]. These results of model pruning suggests that this strategy requires specific hardware and configurations to improve the inference time of vulnerability detection models; however, when these conditions are met, the benefits can be substantial.\nFrom Table IIIa, we also observe that quantization nega-tively impacts inference time in all configurations, resulting in the highest slowdowns across all compression strategies in both CPU and GPU environments. int4 quantization, in particular, causes the most significant slowdowns, with increases of +133.5% on the CPU and +201.6% on the GPU. These negative results could be explained by the fact that CPUs and GPUs are heavily optimized for full-precision floating-point operations (e.g., 32-bit), hence quantized models may not fully take advantage of these optimizations and experience an inference slowdown [9], [57]\u2013[59].\nKnowledge Distillation, on the other hand, consistently and significantly improves inference time across both CPU and GPU environments. It ranks as the second-best strategy (after Pruning 0.6) on the CPU, with a reduction of -39.8% in inference time, and as the best strategy on the GPU, with a speed-up of -47.7%.\n2) Model Size: As can be observed from Table IIIa, all the compression strategies, with the exception of pruning, reduce the size the vulnerability detection model. int4 quantization provides the highest model's size reduction (-59.3%), followed by the other two quantization configurations (-51.4%). Those results align with the expected quantization behaviour, where a lower bits' precision implies a lower model size. Knowledge Distillation reduces the original model size by almost half (-48.8%). This result aligns with the smaller architecture of the distilled model compared with the original fine-tuned CodeBERT model. In contrast, pruning does not affect the model size. This behavior can be explained by the unstructured nature of the pruning strategy employed, where pruning impacts only the weight values by setting them to zero without modifying the network structure itself [9].\n3) Effectiveness: Notably, we find that quantization has very limited impact on the model's effectiveness (see Table IIIa). float 8 quantization marginally changes the effective-ness, in terms of Accuracy, F1-score, and MCC (0.0%, -1.1%, and +0.4%, respectively). Similar results hold for int8 quan-tization, while int4 quantization provides a slightly higher degradation, especially for F1 (-4.4%) and MCC (-8.5%). Knowledge distillation performs slightly worse than quanti-zation, particularly in terms of MCC (\u221210.1%) and Accuracy (-2.2%). However, we observe an improvement in the F1-score (+3.1%). This means that the distilled model has a higher tendency to predict vulnerabilities compared to the original fine-tuned CodeBERT model, which can decrease the number of true negative instances (i.e., increase the number of false positive instances). This leads to an improvement in recall, while only marginally affecting precision, which overall positively impacts the F1-score. Nonetheless, the reduction in MCC suggests a lower correlation between the predicted vulnerabilities and the actual ones.\nPruning has the most significant detrimental impact on vulnerability detection effectiveness. All the pruning config-urations considerably reduce model effectiveness, with MCC changes ranging from -11.3% to -20.6%. Among all com-pression strategies, Pruning 0.4 performs the worst across all three metrics, showing effectiveness losses of -7.3%, -61%, and -20.6% for Accuracy, F1-score, and MCC, respectively. Interestingly, Pruning 0.6 performs slightly better than Pruning 0.4, despite prior research suggesting more severe effective-ness degradation with higher pruning levels [42].\n4) Trade-offs: Figure 2a shows the effectiveness-efficiency trade-off provided by each compression strategy. For each sub-plot, the upper-right solutions are the best. We use MCC as the effectiveness metric since, as explained in Section III, it is the most comprehensive metric for classification [54]. From the far-right subplot, we observe that quantization achieves the best model size reduction while maintaining effectiveness levels comparable to the baseline. However, as shown in the first two subplots, this gain comes at the cost of increased inference time on both CPU and GPU. From Figure 2a, we observe that pruning is consistently outperformed by other strategies across all efficiency and effectiveness metrics. The only exception is Pruning 0.6, which offers the fastest inference time but at the expense of a comparatively low effectiveness. Knowledge distillation is, on the other hand, the only compression strategy that improves efficiency across all dimensions (i.e., CPU and GPU inference time, and model size). At the same time, this strategy results in a comparatively moderate effectiveness degradation-greater than quantization but less than pruning. Overall, this makes knowledge dis-tillation the strategy that offers the most balanced trade-off between efficiency gains and effectiveness degradation.\nAnswer to RQ1: Quantization is the most effective strategy for reducing memory size in vulnerability detection models (up to -59.3%), with minimal impact on model effectiveness (-8.5% MCC in the worst case). However, it can significantly increase inference time, by as much as +201.6%. Pruning, on the other hand, shows no efficiency gains, with the exception of the 0.6 configuration that results in notable speed-up on CPU (up to -67.9%), but at the cost of a -18.2% effectiveness degradation in MCC. Finally, Knowledge Distillation improves both inference time (up to 47.7%) and model size (up to 48.8%), while moderately impacting vulnerability detection effectiveness, with a reduction of 10.1% in MCC."}, {"title": "B. RQ2 Results - Code Summarization", "content": "1) Inference Time: As shown in Table IIIb, Pruning 0.2 is the most effective approach for reducing inference time on the CPU, achieving a -45.3% reduction compared to the plain CodeBERT model. Counter intuitively, we observe a negative correlation between the percentage of weights pruned and the inference time reduction. For example, Pruning 0.4 performs worse than Pruning 0.2, increasing inference time by +24.7%, while Pruning 0.6 shows the worst behavior, increasing inference time by +183.5% on the CPU. On the GPU, we observe an even more pronounced worsening trend, as inference times increase by +9.8%, +121%, and +419.3% for Pruning 0.2, 0.4, and 0.6, respectively. A possible expla-nation for this behaviour could be the lower hardware's ability to handle sparse matrix multiplications [60]. Since generation tasks require multiple network forward steps, a higher sparsity of the weights could increase the overall inference time.\nQuantization strategies can all reduce the inference time on the CPU, with int8 quantization being the best configuration (-27.2%). This behaviour differs from what we observed for vulnerability detection and could be explained by the higher complexity of the underlying task, which may benefit more from reduced weight precision [57]. On the other hand, on the GPU, quantized models are overall slower than the plain CodeBERT model, with inference time slow-downs ranging from +6.2% to +29.1%.\nKnowledge Distillation is the only strategy capable of reducing inference time of code summarization models across both CPU and GPU environments. On the CPU, it is the second-best compression strategy for inference time reduction (-39.8%). On the GPU, it is the only strategy to achieve an inference time reduction, with a decrease of (-2.2%). However, this reduction is not statistically significant, as the confidence interval for the relative change includes zero (see Section III-C for details).\n2) Model Size: From Table IIIb, we observe how, like in vulnerability detection, int4 quantization is the best strategy for reducing the model's size (-51.9%), followed by the other two quantization configurations. Again, these results align with the expected quantization behaviour, where the lower the precision, the lower the size. Knowledge Distillation can also reduce the model size (by -33%), while the adopted pruning strategies confirms that they do not lead any change.\n3) Effectiveness: From Table IIIc we observe that all quantization strategies provide only marginal change in the model's effectiveness, with float8 and int8 quantization having almost comparable metrics to the baseline, and int4 quantization showing limited degradations of -2%, -0.1%, -0.2% for Bleu, BERTScore and SIDE, respectively.\nFor pruning, we observe that the impact grows as pruning becomes more aggressive. Specifically, Pruning 0.2 shows marginal impact, with a slight degradation of -4.3% in summary-summary text similarity (BLEU). On the other hand, Pruning 0.4 and 0.6 substantially reduce the model's effective-ness across all dimensions. For instance, in terms of summary-code semantic similarity (SIDE), Pruning 0.4 and 0.6 lead to effectiveness decreases of -42.4% and -93%, respectively. This behaviour is in line with previous research showing how the effectiveness of a BERT model starts to decrease if the amount of pruned weights is \u2265 40% [42].\nKnowledge Distillation also shows significant impact, particularly in terms of summary-summary text similarity (-42.3% in Bleu) and summary-code semantic similarity (-70.6% in SIDE). This behavior is consistent with previous studies, which highlight that distilled models are less effective for tasks beyond classification [22].\n4) Trade-offs: Figure 2b illustrates the trade-off between effectiveness and efficiency. We use SIDE as the effectiveness metric, as it is specifically designed for code summarization tasks [36]. We observe that Knowledge Distillation overall improves the inference time and size of the model, but it significantly degrades effectiveness. In contrast, quantization generally performs well across all efficiency metrics, with only a slight slow-down in inference time on the GPU. Moreover, this strategy has only a marginal impact on the model's effectiveness, with values comparable with the baseline. As such, quantization appears to offer the best balance between efficiency and effectiveness as a compression strategy. Pruned models are generally outperformed by other compressed mod-els in terms of both efficiency and effectiveness. The only exception is Pruning 0.2, which provides the best trade-off between inference time and effectiveness on the CPU. However, using pruning does not improve model size.\nAnswer to RQ2: Quantization strategies offer the best efficiency-effectiveness trade-off among compression techniques, with infer-ence time speed-up of up to -27.2%, model size reduction of up to 51.9%, and minimal effectiveness degradation of up to -0.2% in SIDE. Pruning generally underperforms compared to other strategies, but under specific configurations, such as Pruning 0.2, it achieves the best inference time improvements on the CPU (-45.3%). While Knowledge Distillation improves all efficiency metrics (with up to -39.8% in inference time and -33% in model size), it causes significant losses in effectiveness, with reductions of-70.6% in SIDE."}, {"title": "C. RQ3 Results - Code Search", "content": "1) Inference Time: Table IIIc reports how Knowledge Dis-tillation is the strategy that better reduces the inference time on both CPU and GPU", "26": ".", "9": [57], "59": ".", "Size": "Since the model used for this task is the same as the one used for vulnerability detection (i.e.", "Effectiveness": "Table IIIc shows how int8 and float8 quantization provide no significant change compared with the baseline. A slightly higher degradation is instead observed with int4 quantization (-6.3% in MRR). All pruning strate-gies provide a degradation in the model"}]}