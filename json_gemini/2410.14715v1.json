{"title": "Animating the Past:\nReconstruct Trilobite via Video Generation", "authors": ["Xiaoran Wu", "Zien Huang", "Chonghan Yu"], "abstract": "Paleontology, the study of past life, fundamentally relies on fossils to reconstruct ancient ecosystems and understand evolutionary dynamics. Trilobites, as an important group of extinct marine arthropods, offer valuable insights into Paleo-zoic environments through their well-preserved fossil records. Reconstructing trilobite behaviour from static fossils will set new standards for dynamic reconstructions in scientific research and education. Despite the potential, current computational methods for this purpose like text-to-video (T2V) face significant challenges, such as maintaining visual realism and consistency, which hinder their application in science contexts. To overcome these obstacles, we introduce an automatic T2V prompt learning method. Within this framework, prompts for a fine-tuned video generation model are generated by a large language model, which is trained using rewards that quantify the visual realism and smoothness of the generated video. The fine-tuning of the video generation model, along with the reward calculations make use of a collected dataset of 9,088 Eoredlichia intermedia fossil images, which provides a common representative of visual details of all class of trilobites. Qualitative and quantitative experiments show that our method can generate trilobite videos with significantly higher visual realism compared to powerful baselines, promising to boost both scientific understanding and public engagement.", "sections": [{"title": "I. INTRODUCTION", "content": "Paleontology, the study of prehistoric life, relies heavily on the fossil record to reconstruct past ecosystems, understand evolutionary processes, and decipher extinct organisms' biology [2], [3]. As an extinct group of marine arthropods, trilobites are among the most iconic and well-studied fossils [2], [4], [5], providing critical insights into Paleozoic ecosystems. Reconstructing the behavior and locomotion of trilobites is of great research and educational interests [1]. Such dynamic reconstructions help in formulating hypotheses about trilobites' living environments and the functional morphology and ecological roles of these ancient creatures [4]\u2013[7]. Furthermore, from the educational aspect, reconstruction provides a tangible visualization of trilobite appearance and behavior, thus bridging the gap between abstract scientific knowledge and public understanding [8].\nDespite the abundance in the trilobite fossil record, reconstructing their behavior and movement remains a challenge, primarily due to the limitations inherent in fossil remains, such as their static nature. Fortunately, recent advancements in generative artificial intelligence (AI) and computational techniques provide new opportunities to address these challenges [9]\u2013[13]. Integrating AI into paleontological research not only showcases the potential of extending machine learning into a field of natural research that AI has not studied extensively before [14] but also hopefully can enhance our understanding of the trilobite ethology and shed new light on its study.\nAmong generative AI techniques, video generation [1], [15], [16] techniques in particularly suitable for simulating trilobite movement in a dynamic, visually engaging manner. However, the current methods of video generation encounter several challenges that hinder their application to paleontological reconstructions. Primarily, as demonstrated in our qualitative studies, existing methods struggle with maintaining the realism of the depicted trilobites, with the creatures appearing unrealistic or oddly shaped [3]. This lack of realism significantly detract from the viewer's engagement and reduce the educational and research value of the visualizations. Moreover, the consistency of generated videos often falls short, with noticeable discrepancies between consecutive frames [17], [18]. Such inconsistencies are particularly problematic in longer sequences, leading to choppy transitions that disrupt the fluid simulation of trilobite movement.\nTo tackle these issues, we propose a novel approach that embeds the evaluation of trilobite realism and video smoothness directly into the video generation workflow. Our solution leverages diffusion models [19]\u2013[22], which have demonstrated impressive capabilities in producing realistic images and videos from textual descriptions. We employ these models to create a series of animated segments that capture various aspects of trilobite movement, guided by descriptive prompts generated by a large language model (LLM) [23]\u2013[25]. The cornerstone of our method involves assessing the smoothness of transitions and the accuracy of the trilobite appearance in these animations, compared against a curated collection of trilobite fossil images. This assessment acts as a feedback mechanism to fine-tune the LLM that generates prompts for the text-to-animation model [26]\u2013[28], enhancing the fidelity of animations. The objective is dual: to produce animations that not only accurately depict trilobite appearance and movement but also ensure seamless transitions, adding a layer of complexity to the model's training but crucial for high-quality video output.\nIn summary, our methodology encompasses several stages: initially, we generate basic animated segments from a set list of LLM-generated prompts. These segments are then pieced together, and the composite video is evaluated for the quality of its transitions and the realism of its content. The evaluation results are used as reward signals to update the LLM with preference optimization [29], [30] to refine the animations. This cycle of generation, evaluation, and enhancement is repeated until the video meets our criteria for smoothness and realism.\nWe comprehensively evaluate our method both qualitatively and quantitatively against state-of-art text-to-animate and text-to-video academic research [27] and commercial tools [31], [32]. The results show clear advances in paleontological vi-sualization in terms of content realism and video continuity. Furthermore, we provide ablation studies to show the contri-bution of each component in our learning framework. We hope that this pioneering integration of technology and paleontol-ogy makes significant contributions to the field of synthetic media generation and opens new pathways for visualizing and understanding prehistoric entities and exploring ancient life."}, {"title": "II. RELATED WORK", "content": "Our method of training the Large Language Model (LLM) that generates prompts is related to Reinforcement Learning from Human Feedback (RLHF), an important technique to ensure that LLM outputs align with human preferences [33]\u2013[35]. Here in our work, the counterpart of human prefer-ence is defined by metrics regarding content realism and video continuity. Typically, RLHF initially learns a reward model (RM) [36] from human preferences and then optimizes the supervised fine-tuned LLM model with reinforcement learning algorithms, e.g., PPO [37], to maximize the cumulative rewards from the RM. However, training the reward model is time-consuming and computation-intensive [36].\nDirect Preference Optimaztion (DPO) [29] avoids training the reward model by directly aligning LLMs to best satisfy human preferences using a simple classification objective. The recently proposed KTO [30] extends DPO by maximizing utility functions derived from prospect theory [38] for accurate human utility modelling. In this work, to achieve better stability and robustness, we utilize the calculated realism and continuity rewards to order different LLM outputs (prompts to the animation generation model) and use KTO for preference optimization.\nVideo generation methods like Tune-a-Video [13] extend text-to-image (T2I) models to generate multiple images simultaneously by incorporating a tailored spatio-temporal attention mechanism and an efficient one-shot tuning strategy to learn continuous motion among generated images. Text2Video-Zero [12] proposes a cost-effective approach that requires no training or optimization by leveraging the capabilities of existing T2I synthesis methods, adapting them for the video generation domain. CogVideo [16] proposes a multi-frame-rate hierarchical training strategy to better align text and video clips on large-scale text-video datasets. Furthermore, commercial video generation tools are setting significant benchmarks. In this paper, we empirically compare our method against Pika [31] and Gen3 [32] for evaluation. Text-to-Animation (T2A) is another approach in video generation that extends pre-trained T2I models by incorporating temporal structures [26], [27]. AnimateDiff [27] introduces a plug-and-play motion module that enables the training of T2A models without the need for model-specific tuning. In this paper, we employ the T2A method to generate trilobite animations from user prompts, but the focus is on how to enhance the temporal coherence and content realism.\nWe now introduce the preliminaries of the RLHF and T2A techniques based on which we develop our method."}, {"title": "III. PRELIMINARIES", "content": "RLHF. The training of modern Large Language Models (LLMs) involves three phases as outlined in [23], [33], [39], [40]. (1) Pretraining: This phase involves training an initial model $\\pi_\\theta$ on a large text corpus to optimize the prediction of the next token based on the preceding text [41]\u2013[43]. (2) Supervised Fine-tuning (SFT): The model is further trained on task-specific data that generally includes targeted instructions and expected responses, to refine its utility for practical applications [44]. This fine-tuned model is noted as $\\pi_{ref}$. (3) RLHF: This step uses a preference dataset D containing tuples $(x, y_w, y_l)$ where x is the input and $y_w, y_l$ are the preferred and less preferred outputs, respectively [29], [34]. A Bradley-Terry model [45] is used here to calculate preferences:\n$p^*(y_w > y_l|x) = \\sigma(r^*(x, y_w) - r^*(x,y_l))$,\nwhere $\\sigma$ is the logistic function. A reward model $r_\\phi$ is trained by minimizing the negative log-likelihood of the preference data in the set D [33]:\n$L_R(r_\\phi) = E_{(x,y_w,y_l)\\sim D}[-log\\sigma(r_\\phi(x, y_w) \u2013 r_\\phi(x, y_l))]$.\nTo balance reward maximization with linguistic correctness, a KL divergence penalty is applied, preventing the model from deviating excessively from the reference model $\\pi_{ref}$:\n$\\max_{\\pi_\\theta} E_{x\\sim D,y\\sim \\pi_\\theta (\\cdot|x)} [r_\\phi(x, y)] \u2212 \\beta D_{KL} [\\pi_\\theta(\\cdot|x)||\\pi_{ref}(\\cdot|x)]$.\nThe objective, non-differentiable in nature, requires an RL approach like PPO [37] for optimization.\nThe computational demands and instability of training the reward model $r_\\phi$ have led to the development of Direct Preference Optimization (DPO) [29], providing a stable alternative that trains directly on preference pairs and with similar optimal policy convergence performance:\n$L_{DPO}(\\pi_\\theta, \\pi_{ref}) = E_{(x,y_w,y_l)\\sim D}[-log \\sigma (\\beta (log \\frac{\\pi_\\theta(y_w|x)}{\\pi_{ref}(y_w|x)}-log \\frac{\\pi_\\theta(y_l|x)}{\\pi_{ref}(y_l|x)}))]$.\nT2A. One approach for short video generation is to animate a text-to-image (T2I) [46] model by integrating temporal dynamics. Following the methodologies outlined in related literature [16], [27], a batch of video data is represented as 5-dimensional tensors $x \\in R^{b\\times c\\times f\\times h\\times w}$. Here, b denotes the batch axis, f represents the frame-time axis, and c, h, and w are the channels, height, and width of each video frame, respectively. The text-to-animation process begins by encoding each frame of a video data batch $x_{1:f} \\in R^{b\\times c\\times f\\times h\\times w}$ into latent representations $z_{1:f,0}$ using a pre-trained auto-encoder. These representations are subsequently perturbed by noise as per the forward diffusion schedule [20], [47]:\n$z_{1:f,t} = \\sqrt{\\bar{a}_t} z_{1:f,0}+\\sqrt{1 - \\bar{a}_t}\\epsilon, \\epsilon \\sim N(0, I), t = 1,..., T$.\nIn this inflated model, the noisy latent representations, along with corresponding text prompts, serve as inputs for predicting the noise added during the diffusion process. The training objective for T2As can be formulated as:\n$L = E_{E(z_{1:f}),y,\\epsilon\\sim N(0,1),t} [||\\epsilon - \\epsilon_\\theta (z_{1:f,t}, t, T_\\theta (y))||^2]$.\nBy inflating the model with the additional temporal axis [48], this formulation emphasizes the critical role of temporal coherence and dynamic content adaptation in generating animations from textual descriptions.\nHowever, this approach may yield animations that are not only brief in duration but also suffer from less smooth transitions between frames and inconsistencies in object appearances across different frames. These issues primarily arise from the model's limitations in maintaining consistent motion patterns and visual quality throughout the sequence [17]. This challenge is particularly pronounced when the model attempts to interpolate complex dynamics, a task that demands high fidelity in temporal and spatial representations. The difficulty lies in the model's capacity to accurately generate and link successive frames where each must evolve naturally from its predecessor while adhering to the dynamics specified by the textual description."}, {"title": "IV. METHOD", "content": "In this section, we describe our method that addresses the challenge of maintaining motion smoothness and visual realism throughout the video sequences.\nThe proposed framework synergizes the power of a large language model (SCRIPT WRITER) that generates prompts and a fine-tuned text-to-animation model (VIDEO GENERATOR). Our main technical novelty lies in the design of the opti-mization algorithm of SCRIPT WRITER. In effect, we design a contextual bandit learning task for the SCRIPT WRITER. The concept of contextual bandit is popular in the cutting-edge LLM research, such as direct preference optimization (DPO [29]).\nFor fine-tuning the VIDEO GENERATOR and training the SCRIPT WRITER, we collect a set $R$ of 9088 Eoredlichia intermedia fossil images, which include a large number of specimens covering different stages of individual development of Eoredlichia intermedia trilobites. The images are used to provide a common representative of the visual traits of all class of trilobites in order to enhance the visual details of the generated content. These real trilobite fossil images do not mean that the videos produced in this study can fully reproduce the real trilobite structure, but the use of these real fossil details can greatly supplement the scarcity and errors of trilobite images in the web footage, enhancing the trilobite structure and details in the videos.\nPrompt and Video Initialization\nWe now describe the details of our method. As the first step, the SCRIPT WRITER $\\pi_\\theta(0^0)$, where $0^0$ is the initial parameters, is asked to generate an initial prompt $y^0$ for the text-to-animation model with the format\n$y^0 = (t_1 : y_1; t_2 : y_2; ,\u2026\u2026\u2026, t_N, y_N)$,\nwhere $y_n, n \\in [N]$ is a textual description of the appearance and expected movement of a trilobite in the animation clip n, and $t_n, n \\in [N]$ is the frame index from which the animation clip n will start in the final video.\nThis initial prompt y directs the text-to-animation diffusion model VIDEO Generator to generate N initial animation clips $(c_1(y^0),\u2026\u2026\u2026, c_N(y^0))$ that are concatenated sequentially to get an initial video $z_{1:f}(y^0)$. Before generating this initial video, the VIDEO GENERATOR has been fine-tuned on the collected fossil image dateset R to enhance the model's ability to generate the detailed textures and structures observed in fossil images."}, {"title": "B. Reward Design for the SCRIPT WRITER", "content": "The second step is to design reward signals to train the SCRIPT WRITER with the hope that it can refine the initial prompt $y^0$ so that the resulting video has a better quality in terms of transition smoothness and visual realism.\nTo be specific, the reward for prompt $y^0$ is designed to be a summation of two components: $r(y^0) = r_s(y^0) + r_a(y^0)$, where $r_s$ measures the smoothness of frame transition and $r_a$ measures the visual realism of the trilobites in the generated video.\nSmoothness of Frame Transition. To assess the smooth-ness of a video, we compute the Fr\u00e9chet Inception Distance (FID) [50] between adjacent frames. Consider two frames $x_t \\in R^{c\\times h\\times w}$ and $x_{t+1} \\in R^{c\\times h\\times w}$, where c, h, and w represent the channel, height, and width of the frame, respectively. We first use a pre-trained InceptionV3 network [51] to extract the image features (pool3 layer) $z_t$ and $z_{t+1}$ given frames $x_t$ and $x_{t+1}$, and then compute the FID score for consecutive frames by:\n$FID_t = ||z_t - z_{t+1}||^2$.\nAfter obtaining the FID scores for all consecutive frames, we get the transition smoothness reward $r_s(y) = -\\sum_{t=1}^{f} FID_t$. For fine-grained control, the reward can be calculated for each clip separately:\n$r_s'(y_n) = \u2013 \\sum_{t=t_n}^{t_{n+1}} FID_t$.\nVisual Realism. To ensure scientific rigorousness, we compare the visual details of the generated content against real samples of trilobite fossils from R. For a video consisting of a set of frames, we expect that no frame has trilobites with morphological details deviating significantly from realistic data. To this end, we design a max-min objective:\n$r_a(y_n) = \\max_{x\\in[f]} \\min_{r\\in R} D(x,r)$\nHere, D is a distance function that measures the morphological similarity between a generated trilobite and a reference image. We now explain the intuition of this similarity reward. The reference image set contains images of trilobite fossils from different growing stages, various sizes, different levels of preservation, and different geological periods. Therefore, we would have clear evidence that a generated trilobite is visually realistic if it is morphologically similar to at least one reference image. We capture this by the minimum operation in Eq. 7. Then we try to find the frame that is the most different from the reference set, minimizing which could guarantee that no frame deviates too much from the reference set.\nIn practice, we use the ORB (Oriented FAST and Rotated BRIEF (Binary Robust Independent Elementary Feature) [52]) detector as the distance function D. The ORB detector is a fast and efficient feature detection algorithm. It combines the FAST keypoint detector and the BRIEF descriptor, providing robust performance suitable for extracting morphological details. We then use BF (Brute-Force) [53], [54] method for matching the features. computes distances between every pair of descriptors, typically employing the Hamming distance for binary descriptors, as utilized in our case."}, {"title": "C. Training the SCRIPT WRITER", "content": "Having defined the reward signals, we now introduce the third step, the training of SCRIPT WRITER. We note that the rewards defined in the previous section are all negative, could be large in magnitude, and prone to noise, which indicates that these rewards may not be effective when used to train the SCRIPT WRITER LLM with algorithms like PPO [37], as they are sensitive to the specific values of the rewards. To tackle this problem, we propose ordering the prompts based on the rewards then applying preference optimization. Preference optimization has been proven its effectiveness in the broad literature of reinforcement learning from human feedback (RLHF) [33] and is more robust when reward values are noisy.\nTo be specific, we collect a training dataset D where each training sample contains a query x, a desirable generation $y_d$, and an undesirable generation $y_u$. Here, $r(y_d) > r(y_u)$. We use Kahneman-Tversky optimization (KTO) [30] to train the SCRIPT WRITER. Using $\\lambda_d$ to denote $\\lambda_D (\\lambda_U)$ when y is desirable (undesirable), where $\\lambda_D$ and $\\lambda_U$ are two constants, the KTO loss is:\n$L_{KTO}(\\theta^0) = E_{x,y\\sim D}[\\lambda_D - v(x, y)]$\nwhere\n$\\rho_\\theta(x, y) = log \\frac{\\pi_\\theta (y|x)}{\\pi_{ref} (y|x)}$;\n$\\tau_0 = E_{x'\\sim D}[KL(\\pi_\\theta (y'|x') || \\pi_{ref} (y'|x'))]$;\n$\\upsilon(x,y) = {\\lambda_D\\sigma(\\beta(\\rho_\\theta(x,y) - \\tau_0))} if y \\sim y_d|x;\\atop {\\lambda_U\\sigma(\\beta(\\tau_0 - \\rho_\\theta(x,y)))} if y \\sim y_u|x.$\nIn the following section, we present examples showing how the KTO training improves the prompts.\nAfter KTO training, SCRIPT WRITER parameters are updated to $\\theta^1$, which leads to updated prompts $y^1$. $\\theta^1$ can be further improved by running KTO given the new video generated by $y^1$. This process can be repeated until the new video is satisfactory enough."}, {"title": "V. EXPERIMENTS", "content": "In this section, we present experiments to test the effectiveness of our method. The experiments are designed to give both qualitative and quantitative evaluation of the gen-erated videos. We compare against strong baselines, including previous work on text-to-animation (AnimateDiff [27]) and powerful commercial text-to-video tools, Pika Labs [31] and Gen-3 [32]. We also carry out ablation studies to show the separate contribution of SCRIPT WRITER training and VIDEO GENERATOR fine-tuning.\nQualitative Results I: Visual Realism\nCompare with baselines. In Fig. 2, we showcase a qualitative comparison of trilobite renderings produced by different models. The objective of the comparison is to evaluate each model's ability to generate realistic trilobites in various dynamic backgrounds.\nThe first row shows a scene featuring trilobites on the ocean floor with a volcanic eruption in the background. The Pika model generates a trilobite with unrealistic segmentation. The Runway model shows a more realistic structure but lacks in capturing the authentic texture of trilobite exoskeletons. The AnimateDiff model produces an oversimplified trilobite, and the main part of the image features a volcano. In contrast, the trilobites generated by our model display intricate segmentation, realistic texturing, and coloration that blends well with the naturalistic ocean floor setting, making them the most lifelike.\nThe depiction in the second row includes three trilobites among aquatic plants on the seabed. The Pika model's trilo-bites are not similar to any known types of trilobites. Runway's versions show better integration with the background but are still somewhat artificial in appearance. The trilobites by AnimateDiff lack depth and detail in texturing. Our model, however, shows trilobites with precise, well-defined segmen-tation and natural colors that harmonize with the underwater environment, enhancing the realism of the scene.\nThe scene of the third row captures a single trilobite moving along the ocean floor with a focus on the interaction with the environment, such as sediment displacement. Pika's rendition again lacks realism in appearance. AnimateDiff's trilobite appears round. Meanwhile, Our model produces a realistic trilobite interacting with its surroundings, showing sediment displacement that suggests a natural weight and presence in the water.\nIn summary, our model outperforms in creating trilobites with realistic anatomical features, textural fidelity, and ap-propriate environmental interactions compared to the other models. This qualitative analysis underscores the ability of our method in generating video content that closely mirrors the true appearance of trilobites.\nCompare with ablations. Two components in our method contribute to the visual realism of the generated trilobites: we first fine-tune the T2A model and then carry out preference optimization for SCRIPT WRITER. In Fig. 3, we show the influence of these two components. Specifically, we present two examples that demonstrate the results before and after preference optimization, with a focus on the visual quality of trilobite renderings in generated videos. Each example shows a frame from the video, accompanied by the most closely matching image from the dataset and the corresponding prompts.\nIn the first example (Fig. 3), before optimization, the video frame shows a trilobite on the ocean floor with a volcanic eruption in the background. The trilobite appears somewhat blended into the background, lacking distinct features, resulting in a low match score of 0.07. The prompt focuses on the general presence of trilobites amid a dynamic back-ground. By contrast, after optimization, the optimized frame exhibits a trilobite with more emphasized and defined hard shells, enhancing its visibility and structural integrity against the complex background. This optimization is attributable to the updated prompt, which now specifically highlights the trilobite's hard shell. The match score significantly improves to 0.35, indicating a closer resemblance to the most similar reference image, which shows clearer and more detailed trilobite features.\nIn the second example (Fig. 3), before optimization, the original video frame captures a trilobite moving across the ocean floor with anomalocaris in the background. Initially, the trilobite lacks prominent distinguishing features, leading to a match score of 0.158. After preference optimization, the frame now shows the trilobite with enhanced distinguishing features, such as the longitudinal lobes and textural details, making it more realistic and akin to the reference image. Again, it is the updated prompt that leads to these changes, specifically by pointing out these features, contributing to a raised match score of 0.33.\nIn both cases, preference optimization led to adjustments in the model's rendering, focusing on enhancing specific features of the trilobites that contribute to greater visual realism. The targeted adjustments in the prompts post-optimization are pivotal in directing the model to produce outputs that not only adhere more closely to the reference images but also showcase more pronounced and authentic trilobite characteristics. This approach demonstrates the model's capability to adapt and refine its output by learning from preferences, ultimately yielding higher match scores and visually richer renderings.\nQualitative Results II: Smoothness\nFig. 4 displays a series of frames. The sequences before and after preference optimization are shown. In the initial frames before optimization, the trilobite's movement appears somewhat jerky, especially its antennae. The corresponding prompt focuses on the trilobite's glide through the landscape. After preference optimization, the frames show a noticeable improvement in the fluidity of the trilobite's movement. The animation becomes smoother, with the trilobite seamlessly integrating into the motion of the surrounding plants. This creates a more naturalistic and visually appealing scene. The cause of this change is that the optimized prompt adds some frames and emphasizes the smooth, effortless glide of the trilobite and its streamlined body, highlighting how these char-acteristics should be reflected in the animation. This directive likely influenced the rendering process to focus on creating a smoother and more coherent movement pattern.\nFig. 5 gives another example where the SCRIPT WRITER learns to add some words to enhance the video smoothness. The comparison clearly demonstrates that the changes in the prompts, post-optimization, lead to significant improvements in the smoothness of the video."}, {"title": "C. Quantitative Results", "content": "We conduct quantitative comparisons to further evaluate our method.\nSmoothness after KTO prompt training. Fig. 6 illustrates the Fr\u00e9chet Inception Distance (FID) scores between adjacent frames in a generated video sequence, comparing results before and after preference optimization.\nBefore preference optimization, the blue line shows several peaks, particularly noticeable around frames 15, and 60-80, suggesting that the transitions between these frames are less smooth, with more noticeable visual discrepancies. After pref-erence optimization, the dark line generally maintains lower FID scores throughout the sequence, with fewer and lower peaks compared to the blue line. This indicates that after optimization, the frames have greater visual consistency, and the transitions between them are smoother.\nThe overall trend in the graph demonstrates that preference optimization effectively reduces the FID scores across the majority of the video sequence. This improvement signifies that the video has become smoother post-optimization, with more consistent and visually coherent transitions between frames.\nUser study. We generate videos using four different methods and conduct a user study to evaluate their perfor-mance. Participants are asked to rate the videos on three criteria: smoothness, visual realism, and consistency with the prompt. We recruit participants for this study, each rating the videos on a scale from 1 to 4, where 4 indicates the highest possible score. This scoring system is equivalent to Average User Ranking (AUR), with higher scores indicating superior performance across the evaluated metrics.\nOur method outperforms the other three methods in all evaluation criteria, indicating a significant improvement in video generation quality. This is evident from the higher scores across all three categories, confirming the effectiveness of our approach in producing videos that are smooth, visually realistic, and consistent with the given prompts. Particularly, the much higher scores regarding consistency with prompts achieved by our method highlight the effectiveness of our prompt learning method."}, {"title": "VI. CONCLUSION", "content": "In conclusion, our study leverages advanced generative AI techniques to address the challenges of reconstructing trilobite behavior from fossil records. By integrating computa-tional methods with paleontological research, we demonstrate the potential to enhance our understanding of these ancient creatures. Our proposed video generation framework, which incorporates realism and smoothness assessments into the workflow, produces more accurate and dynamic visualizations of trilobite movements. These enhanced animations not only improve scientific insights but also make the prehistoric world more accessible to the public. This interdisciplinary approach marks an advancement in both the fields of paleontology and (multi-modal) artificial intelligence, opening new avenues for future research and educational opportunities."}]}