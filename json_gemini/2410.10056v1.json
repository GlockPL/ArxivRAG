{"title": "The Epochal Sawtooth Effect: Unveiling Training Loss Oscillations in Adam and Other Optimizers", "authors": ["Qi Liu", "Wanjing Ma"], "abstract": "In this paper, we identify and analyze a recurring training loss pattern, which we term the Epochal Sawtooth Effect (ESE), commonly observed during training with adaptive gradient-based optimizers, particularly Adam optimizer. This pattern is characterized by a sharp drop in loss at the beginning of each epoch, followed by a gradual increase, resulting in a sawtooth-shaped loss curve. Through empirical observations, we demonstrate that while this effect is most pronounced with Adam, it persists, although less severely, with other optimizers such as RMSProp.\nWe provide an in-depth explanation of the underlying mechanisms that lead to the Epochal Sawtooth Effect. The influences of factors like \u03b2, batch size, data shuffling on this pattern have been studied. We quantify the influence of \u03b22 on the shape of the loss curve, showing that higher values of \u03b22 result in a nearly linear increase in loss, while lower values create a concave upward trend. Our analysis reveals that this behavior stems from the adaptive learning rate controlled by the second moment estimate, with \u03b21 playing a minimal role when \u03b22 is large.\nTo support our analysis, we replicate this phenomenon through a controlled quadratic mini-mization task. By incrementally solving a series of quadratic optimization problems using Adam, we demonstrate that the Epochal Sawtooth Effect can emerge even in simple optimization scenarios, reinforcing the generality of this pattern. This paper provides both theoretical insights and quantitative analysis, offering a comprehensive understanding of this ubiquitous phenomenon in modern optimization techniques.", "sections": [{"title": "1 Introduction", "content": "In modern deep learning, gradient-based optimization plays a crucial role in training large-scale models efficiently. Among the most widely used optimizers is the Adam optimizer, which combines the benefits of momentum and adaptive learning rates. While Adam is known for its superior convergence properties, particularly in handling noisy gradients, it sometimes exhibits a characteristic loss pattern during training: a sharp drop in loss at the start of each epoch, followed by a gradual increase-a behavior we term the Epochal Sawtooth Effect.\nThis phenomenon is especially pronounced when using Adam, but it is not exclusive to this optimizer. Variants of the pattern can be observed in other optimizers such as RMSProp, though the severity is typically reduced. This sawtooth pattern is often overlooked or attributed solely to the reshuffling of data at the start of each epoch, but it points to deeper underlying dynamics in how different optimizers adjust step sizes and learning rates across varying gradients. This phenomenon has been observed by many practitioners as shown in Figure 1. And many of them are concerned with whether it's a sign of overfitting.\nIn this paper, we aim to provide a thorough explanation and analysis of the Epochal Sawtooth Effect. Specifically, we focus on how Adam's parameter settings, particularly \u03b2\u2081 and \u03b22, influence the appearance of this pattern. While \u03b2\u2081, which controls the momentum term, plays a smaller role, \u03b22 has a significant impact on the shape of the loss curve, governing how the optimizer adapts to gradients of different magnitudes. A high \u03b22 leads to a near-linear increase in loss after the initial drop, while"}, {"title": "2 Related Work", "content": "The field of gradient-based optimization has seen extensive developments over the years, particularly in the context of training deep neural networks. Several methods have emerged to address the inherent challenges of non-convex optimization, high-dimensional parameter spaces, and noisy gradients. In this section, we review related work on gradient-based optimization, focusing on adaptive methods like Adam, and examine studies related to observed loss patterns during training, including phenomena akin to the Epochal Sawtooth Effect."}, {"title": "Gradient Descent and Its Variants", "content": "Gradient descent is a foundational optimization method that computes the gradient of a loss function to iteratively update model parameters. Stochastic Gradient Descent (SGD) has become one of the most widely used algorithms for training deep neural networks, where the gradient is calculated using random mini-batches of data. However, SGD can suffer from high variance in the gradient estimates, leading to unstable updates and slow convergence, particularly when the learning rate is not tuned properly [Rud16].\nTo alleviate some of these challenges, momentum-based methods were developed. Polyak introduced the idea of using an exponentially weighted moving average of past gradients to smooth the updates and accelerate convergence [Pol64]. This method reduces oscillations during training, particularly in high-curvature regions of the loss landscape. Nesterov Accelerated Gradient (NAG)further improved momentum by anticipating the next update, which helps to reduce overshooting when moving towards a minimum [Nes83]."}, {"title": "Adaptive Optimization Methods", "content": "While momentum methods improved the stability of gradient descent, adaptive learning rate methods were developed to handle different parameter scales effectively. AdaGrad [DHS11] introduced the idea of adjusting the learning rate for each parameter individually based on the historical sum of gradients, which made it well-suited for sparse data. However, AdaGrad's per-parameter learning rate tends to shrink over time, leading to vanishing updates.\nTo address this issue, RMSProp [TH12] was proposed to keep a running average of the squared gradients and adjust the learning rate adaptively, ensuring more balanced updates even in later stages of training. However, the most significant advancement in adaptive optimization came with Adam (Adaptive Moment Estimation) [Kin14], which combines both momentum and adaptive learning rates. Adam maintains exponentially decayed averages of both the first moment (mean of the gradients) and the second moment (variance of the gradients), controlled by the parameters \u03b2\u2081 and \u03b22, respectively. Adam's ability to handle noisy gradients and adapt learning rates dynamically has made it one of the most popular optimizers in deep learning."}, {"title": "Loss Patterns During Training", "content": "Several studies have observed non-linear loss behaviors during training, particularly when using adaptive optimizers like Adam. One of the key phenomena identified is the periodic oscillation in loss, often characterized by sharp drops at the beginning of each epoch, followed by a gradual increase, which we refer to as the Epochal Sawtooth Effect. This pattern is often attributed to data reshuffling at the start of each epoch, where the optimizer encounters easier or previously learned data, leading to rapid convergence initially. As harder or less familiar data is encountered later in the epoch, the optimizer struggles, resulting in an increase in loss.\nSmith et al. [Smi17] observed similar oscillations in the loss function during experiments with cyclical learning rates (CLR). In their work, they proposed periodically varying the learning rate to improve training performance by capitalizing on these oscillatory behaviors. By allowing the learning rate to fluctuate, they were able to escape sharp minima and improve generalization. Stochastic Weight Averaging (SWA) [IPG+18] further exploits these oscillations by averaging the weights of a model over multiple points during training, smoothing the oscillations in the loss function and achieving more stable convergence."}, {"title": "The Role of B\u2081 and B2 in Adam", "content": "The parameters \u03b2\u2081 and \u03b22 in Adam control how the optimizer adapts to gradients during training. Kingma [Kin14] showed that B1, which controls the momentum term, affects how rapidly the optimizer responds to the gradient direction. A larger \u03b2\u2081 results in smoother updates over time by considering past gradients, while smaller \u03b21 makes the optimizer more reactive to recent gradients.\nThe second moment estimate, controlled by \u03b22, plays a more dominant role in determining how Adam adjusts the learning rate based on the variance of the gradients. When \u03b22 is close to 1, Adam smooths the second moment estimate over a long period, resulting in stable, linear loss behavior, especially during the increasing phases of training [Kin14, Los17]. On the other hand, when \u03b22 is small, the optimizer reacts more quickly to changes in gradient magnitudes, resulting in concave loss patterns, as the learning rate adapts more aggressively to the current batch of data [Los17].\nChoi et al. [Cho19] further examined the interaction between B\u2081 and B2, showing that in cases where \u03b22 is close to 1, the effect of B\u2081 becomes less pronounced, as the adaptive learning rate from the second moment dominates the optimization behavior."}, {"title": "Quadratic Minimization for Understanding Optimizer Dynamics", "content": "Quadratic minimization problems have often been used as a theoretical framework to understand optimizer dynamics in machine learning. Bottou and Bousquet [BB07] demonstrated that quadratic loss functions allow for more precise analysis of the stability and convergence properties of different optimization algorithms. By studying quadratic problems, researchers can isolate specific behaviors of optimizers, such as step size adaptation and gradient smoothing.\nIn this work, we replicate the Epochal Sawtooth Effect using incremental quadratic minimization, demonstrating that this phenomenon is not exclusive to deep learning models but also emerges in simple optimization tasks. This provides further evidence that the loss pattern is tied to the inherent characteristics of gradient-based optimization methods."}, {"title": "Summary", "content": "Much of the research in gradient-based optimization has focused on improving convergence and efficiency. However, the periodic loss oscillations observed during training, especially with adaptive optimizers like Adam, highlight the complexity of the optimization process. Our work contributes to this understanding by providing a quantitative analysis of how B\u2081 and B2 influence the loss dynamics and by replicating the phenomenon in controlled quadratic minimization tasks. The Epochal Sawtooth Effect thus provides a new lens through which to examine the nuances of gradient-based optimization."}, {"title": "3 Method", "content": "We first reproduce the Epochal Sawtooth Effect by training the BERT models. BERT-small model consists of 768 hidden untis, 6 encoder layers, 6 heads. BERT-tiny has 256 hidden units, 2 encoder layers, 2 heads. These two models are close to but not exact replica of Google-published models bearing the same names. BERT-small is trained on 269MB Wikitext dataset from scratch. BERT-tiny is trained on a smaller 11MB Wikitext dataset instead. Byte pair encoding (BPE) is used for building a 30k-sized vocabulary instead of Sentencepiece. Batch size is first set at 128 for BERT-small and 32 for BERT-tiny. Dataloader shuffle every epoch. The standard Masked language Model (MLM) and Next Sentence Prediction (NSP) tasks are used for pre-training. Pytorch Adam optimizer is used with default settings betas being (0.9, 0.999). Learning rate and weight decay are set at 1e-4 and le-5 respectively. For the purpose of this discussion and to reproduce the desired effect, we will temporarily disregard practice that weight decay rate is typically set larger values during pre-training. MLM training loss of both models reproduce the sawtooth pattern as shown in Figure 2 (a) and (b). NSP loss, which is much smaller in size, will also show this pattern after we learn how to amplify this effect as shown by (c). The losses are averaged over 500 batchs for BERT-small and 50 batchs for BERT-tiny without smoothing. We can see that the training loss drops sharply at the start of each epoch; then gradually increase over the epoch. Note that the overall the overall trend is still declining."}, {"title": "3.1 Influencing Factors", "content": "In this section we discuss the influencing factors. BERT-tiny will be used more in this section for illustration purpose due to its faster training speed. In practice, BERT-small and many other models exhibit similar results. First, we notice that we this phenomenon has nearly vanished (Figure 3(a) and (b)), with only a slight trace remaining (Figure 3(a)), when the dataloaders do not shuffle at the start of each epoch. And it disappears completely when we sample with replacement for each batch. This implies that ESE is related to the sequencing of samples.\nWe first explore the effect of Adam's \u03b22 parameter as shown by Figure 4. We can clearly see that as \u03b22 gets smaller, ESE is amplified (Figure 4(a)-(c)). It's interesting to notice that, as \u03b22 gets smaller, the increasing part of the curve has went from being seemingly liner (Figure 4(d)) to obviously concave (Figure 4(e)). When \u03b22 is very close to 1 (eg. 0.999), \u03b2\u2081 does not have noticeable effects on ESE, as shown by Figure (a)-(c). When 32 gets slightly smaller (eg. 0.9), ESE will be sensitive to \u03b2\u2081 settings; smaller B\u2081 means weaker ESE. (Figure (d)-(f)).\nThe parameter plot_every means the number of batchs used for averaging. When the batch size is fixed, the influence of plot_every is shown in Figure 6. As plot_every gets larger, ESE weakens. Studying the effect of batch size is a little tricky. To make sure that the number of samples used for averaging (batch_size * plot_every) is the same, we adjust the plot_every parameter at the same time. The results are shown in Figure 7. As batch_size gets larger, ESE also weakens."}, {"title": "3.2 Interpretation and Analysis", "content": "Suppose there are N samples in total. Mini-batch size is B. The data loader shuffles the data for each epoch; and samples are drawn without replacement. All batchs have the same dsitirbution of loss values at the beginning of the epoch (Figure 14). We may assume that the loss value is normally distributed, with expectation Lo and variance co. Without loss of generality, suppose that we plot batch loss every step. The last minibatch of epoch e-1 and the first minibatch of epoch e are expected to have B2/N in common, or B/N of the two baths are shared. The momentum is expected to get larger. This is in contrast to the case where shuffle is set to false. Every batch has to wait for exactly one epoch to be seen again. This typically means that the loss will have larger oscillations. This is illustrated by a simple example. Let loss be f = g + h where g(x) = x and h(x) = 1 \u2212 x. f is in fact constant. Suppose g is the goal of first mini-batch and h is the loss of the second minibatch. First suppose we sequence samples by AB, AB, AB... mimicking no shuffling. we use gradient to update x. For, comparison, in Figure 15 (b), we sequence samples by AB, BA, AB... exaggerating the effect\nof shuffling; in (c), we add momentum. We can see that the mini-batch loss ossilate more when we \"shuffle\" data and add momentum.\nDue to large momentum at the beginning, the model will take large steps towards the optimality of losses of the first few minibatchs. it means that the model will memorize the first few mini-batchs better than the rest. This is not necessarily a problem since samsples are shuffled each epoch. Let  lbt  denotes batch b's loss at step t of epoche. At stept, batch t's loss gradient, \u2207 lbt will be used to update model parameters. < \u2207lbt , \u2207lb > is typically positive, which means training on batch t is also helpful to remembering batch b if we update by gradient. As shown in Figure 16, the gradient dot products are positive.\nFor adaptive optimizers like Adam and RMSProp, the momentum (or gradient if there is no mo-ment) will be divided by smoothed squared gradient component-wise. We list the Adam updating rule\n(Eqn 1, 2, 3) here for clarity (weight decay and correction for initialization are neglected for simplicity).\nNote gt := \u2207lt . Dividing by v\u2212\u2212\u221at  will change direction a little bit. This is generally not a problem when\nmomentum (or gradient) is large or \u03b22 close to zero. Note that large \u03b2\u22122 means the direction of v to\nbe more aligned with g - t. Otherwise, through experiments we find that < mt, \u2206xt > will likely to be\nnegative (See 19). This means the parameters will be moving away from \u2207lbt 's optima. The gradient\nis larger when further away from optima, hence we will observe that ||gt || increases steadily as shown\nin Figure 17. ||gt || is approximated by Eqn 4. Momentum takes on large value at the beginning of\nepoch, then drop exponentially; afterwards it gradually increases (Eqn 5). ||vt || is expected to increase\nquadratically (6). We note that ||vt || takes on a larger range in terms of percentage variations. By\ndividing \u221a vt  and switching the sign (Eqn 8), < \u2206xt,gt > will start from a relatively large negative\nvalue, increase exponentially, and tend to be positive constant during the epoch, as shown by Figure\n20. It is approximated by Eqn 8. Batch b's loss is approximated by Eqn 9.\nFinally, the training loss observed can be approximated by Eqn 10. See 21 for the predicted loss\ncurve. It can be divided into two parts. For the first part, exponential component dominates; and\nlinear component dominate the second part."}, {"title": "4 Verification by Incremental Quadratic Optimization", "content": "In this section, we replicate the Epochal Sawtooth Effect by an incremental quadratic optimization\nwith Adam optimizer. This offers another way to understand; and this can demonstrate the widespread\nexistence of this phenomenon. We generate 10,000 quadratic functions randomly. And we take variable\nx to be of the same dimension. This mimics the high capacity of LLMs. We assume that each function\nacts on only one variable for simplicity (code L21-23). And each function is randomly assigned a x\ndimension (code L36). The optimizer is an implementation of Adam. The core part of the code is\nlisted below."}, {"title": "5 Conclusion", "content": "In this paper, we identify and analyze a recurring loss pattern, which we term the Epochal Sawtooth Effect, commonly observed during training with adaptive gradient-based optimizers, particularly Adam. This pattern is characterized by a sharp decrease in loss at the beginning of each epoch, followed by a gradual increase. Through both theoretical and empirical investigation, we demonstrate that this pattern arises due to several interacting factors, including data reshuffling, the configuration of Adam's parameters, batch size, model capacity, and the structure of the dataset itself."}], "equations": ["mt = B\u2081mt-1+ (1 - \u03b21)gt", "Ut = B2Ut\u22121 + (1 \u2212 \u03b22)g2t", "Ot = mt/\u221aUt", "||9t|| \u2248 ag+bg\u221a1 \u2013 B2t", "||mt|| \u2248 amp + bm\u221a1 - \u00df2t + Cm", "||vt|| \u2248 av + but + cv(1 \u2013 \u03b22)t\u00b2", "<mt, gt >\u2248 amg\u03b2\u2081 + bmg\u221a1-B2t+ Cmg\namg, bmg \u2265 0,0 < t < b", "< \u2206xt,g >\u221d< \u03b1g>\u221damg+bmg\u221a1 \u2013 B2t+ Cmgav + but + cv (1 \u2013 \u03b22)t2amg,bmg \u2265 0,0 < t < b", "lbt = lo - \u2211t-1T=0 \u03b7 \u22c5<9>\u221aUT\n0<t<b", "lt \u2248 lo - \u2211t-1T=0 \u03b7 \u22c5<9>\u221aUT\n0<t<b", "x_t = x_{t-1} - \\frac{\\eta}{\\sqrt{v_t}}m_t"]}