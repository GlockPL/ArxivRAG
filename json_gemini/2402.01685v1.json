{"title": "SMUTF: Schema Matching Using Generative Tags and Hybrid Features", "authors": ["Yu Zhang", "Di Mei", "Haozheng Luo", "Richard Tzong-Han Tsai"], "abstract": "We introduce SMUTF\u00b9, a unique approach for large-scale tabular data schema matching (SM), which assumes that supervised learning does not affect performance in open-domain tasks, thereby enabling effective cross-domain matching. This system uniquely combines rule-based feature engineering, pre-trained language models, and generative large language models. In an innovative adaptation inspired by the Humanitarian Exchange Language, we deploy 'generative tags' for each data column, enhancing the effectiveness of SM. SMUTF exhibits extensive versatility, working seamlessly with any pre-existing pre-trained embeddings, classification methods, and generative models.\nRecognizing the lack of extensive, publicly available datasets for SM, we have created and open-sourced the HDXSM dataset from the public humanitarian data. We believe this to be the most exhaustive SM dataset currently available. In evaluations across various public datasets and the novel HDXSM dataset, SMUTF demonstrated exceptional performance, surpassing existing state-of-the-art models in terms of accuracy and efficiency, and improving the F1 score by 11.84% and the AUC of ROC by 5.08%.", "sections": [{"title": "1 INTRODUCTION", "content": "The contemporary era, characterized by the ubiquity of data collection and organization, particularly in public institutions[18], displays the transformative impact of the 'Open Data' initiative [8]. Open Data, characterized by its accessibility, exploitability, and shareability, is seen as an effective means for governments and Non-Governmental Organizations (NGOs) to enhance public services and accountability. Despite the promising potential, challenges abound due to the distributed and heterogeneous nature of data environments and the diversity in data handling methodologies.\nDataset discovery, a key component of the data integration process [11] in today's intricate data environment, hinges on schema matching (SM) [22] a technique for unraveling the relationships between columns (elements) under different tabular schemas. The effectiveness of SM plays a critical role in determining the quality of dataset discovery, which in turn affects the performance of further data analysis.\nThis paper introduces a novel approach to SM, termed as SMUTF (Schema Matching Using Generative Tags and Hybrid Features). SMUTF is designed to perform SM on any type of tabular data with improved accuracy. In contrast to traditional SM approaches [26, 31, 57], SMUTF utilizes supervised learning, thereby enhancing the robustness of SM, a claim substantiated by our experimental results.\nSMUTF combines the strength of Pretrained Language Models (PLM) [9, 15, 28, 39] to generate descriptive tags of data under each column and semantic embeddings on textual information. Further, similarity measures between schemas are computed considering various factors such as dataset values, column names, generated tags, etc. These combined features are used within a gradient-boosting model, XGBoost[7], to predict column matches across datasets.\nAlso, In response to the prevalent issue in the SM field, which is the scarcity of non-synthetic open-sourced evaluation datasets, we have taken the initiative to curate the HDXSM dataset. The data used in this dataset is derived from the Humanitarian Data Exchange (HDX) and is annotated with existing HXL tags [19]."}, {"title": "2 RELATED WORK", "content": "Our research is interlinked with three main fields: SM, Text Embedding with Pretrained Language Models (PLM), and Metadata Generation with Large Language Models (LLM)."}, {"title": "2.1 Schema Matching", "content": "Schema matching (SM) and data integration critically depend on measuring similarity and understanding data diffusion across systems, as explored in Fernandez's study of 'data seeping' [14] and its role in SM efficiency. Melnik's work [33] on similarity measures and Madhavan's universal SM model [31] have advanced the field, but evolving schemas and database complexities present ongoing challenges. Traditional machine learning and hashing methods often struggle with complex mappings, as seen in Courps' [30] focus on simple attribute matching. Recent efforts have incorporated neural networks [49, 50, 57, 61], using pre-trained models and LSTM architectures for SM, yet these approaches can falter with variable-length sequences and long-distance relationships within data."}, {"title": "2.2 Text Embedding with PLM", "content": "In recent years, Transformer-based Pretrained Language Models (PLM) have achieved significant success in various Natural Language Processing (NLP) tasks [27, 29, 42]. These models utilize self-supervised learning methods during the pre-training phase, including the Cloze task, where the model is trained to predict masked parts of a sentence. However, our study primarily focuses on text embedding using subword tokenization and sentence embedding.\nWe opt for sentence embeddings as they encapsulate sentence-level semantics and reduce the dimensionality, offering efficient training and quicker inference times compared to word embeddings. Rather than utilizing traditional bag-of-words (BoW) models [40] or the skip-thought model [21], our research employs transformer-based models for sentence embedding. These models [6, 12, 44] exploit positional encoding in the attention mechanism [53], which aids in understanding the interrelationships between words in a sentence. This feature is crucial in comprehending sentence-level semantics.\nFurthermore, the self-attention mechanism within these models assigns weight to each word in a sentence based on its relationship with other words, enabling the transformer to capture the sentence's meaning more accurately."}, {"title": "2.3 Metadata Generation", "content": "with Large Language Model Large Language Models (LLM) have become pivotal \u00een NLP and machine learning research due to their multifaceted applications. They [4, 43, 52, 59] are designed to generate accurate descriptions of given inputs and have been used in a range of tasks, including question-answering, summarization, content creation, and translation. Despite these advancements, the potential of LLMs in summarizing and describing tabular data remains relatively unexplored. Our approach aims to fill this gap by utilizing LLMs to understand provided data such as column names and values. Subsequently, we leverage the auto-regressive property of Transformers to generate descriptive, Humanitarian Exchange Language style (HXL-style) tags for the columns. These tags offer a high-level synopsis or classification of the assigned data, thereby allowing for a succinct understanding of the data's content. Our innovative application of LLMs demonstrates their potential in the realm of tabular data summarization and description."}, {"title": "3 SMUTF METHODOLOGY", "content": "The SM strategy proposed in this paper, termed SMUTF (Schema Matching Using Generative Tags and Hybrid Features), consists of four components: HXL-style tags generation, rule-based feature extraction, deep embedding similarity, and similarity score prediction using XGBoost."}, {"title": "3.1 Problem Definitions", "content": "Our primary objective is to devise a SM methodology that can independently establish the relationship between two distinct schemas, aligning their respective columns with the assistance of machine learning models. The proposed method consists of two fundamental tasks: generating HXL-style tags and calculating similarity scores.\nWe perceive our schema-matching task as a problem of similarity matching (as illustrated in Figure 1) involving two schemas $S = (C, V)$. In this context, a schema is made up of column names $C = {C_1, C_2,..., c_n}$ and corresponding values $V = {v_{i,1}, v_{i,2},..., v_{i,m}}$ for the i-th column where $i \\leq n$. Essentially, the names and values of each column are viewed as a sequence labeling problem, for which the Large Language Model (LLM) generates HXL-style tags. These tags are merged with other column features and then merged features are fed into a gradient-boosting method, XGBoost, to perform classification. This results in the prediction of the similarity score between two columns. For every column set, either from the source $S_{src}$ or the target schema $S_{tar}$, the model's outcome is a similarity score matrix $O \\in \\mathbb{R}^{n_1 \\times n_2}$ and pairs of matched columns $P \\in \\mathbb{R}^{\\min(n_1,n_2)}$. Here, $n_1$ and $n_2$ represent the number of columns in the two schemas respectively."}, {"title": "3.2 Schema Matching Components", "content": "Table 1 showcases six common SM types [22]. In this section, we will detail the key components of SMUTF: HXL-Style Tag Generation, Rule-Based Feature Extraction (which is subdivided into Column Name Features and Value Features), and Deep Embedding Feature Extraction. In the context of previous SM or data discovery approaches [3, 13, 24, 36, 55], the common practice is to employ 1 to 3 types of matchers. Differing from these traditional approaches, our proposed SMUTF system integrates five matchers, excluding the Value Overlap Matcher, as described in Table 1 and illustrated in Figure 1. We consciously forgo the Value Overlap Matcher as its application is limited to a specific SM scenario: when two columns are joinable. Although this condition frequently occurs in practical situations, an excessive reliance on value overlap could unintentionally restrict the matching system's adaptability to various scenarios. A clear illustration of this limitation is the SM of event reports from different years (e.g., 2018 and 2019), where fields related to dates show no overlap. However, it's noteworthy that the distribution of values can to some extent substitute the role of value overlap. Therefore, SMUTF opts to use the Data Type Matcher and Distribution Matcher, which will be elaborated on in the subsequent sections."}, {"title": "3.3 HXL-style Tags Generation", "content": "The Humanitarian Exchange Language (HXL), a standard of tags developed to annotate the properties and content of column data, was originally proposed by the United Nations Office for the Coordination of Humanitarian Affairs [20]. This initiative aimed to enhance the efficiency and accuracy of data sharing related to humanitarian efforts. Currently, it is predominantly employed within the Humanitarian Data Exchange (HDX), a platform dedicated to the exchange of datasets.\nWithin SMUTF, we took the HXL tag as a reference when creating HXL-style tags, utilizing them to increase interoperability and standardization among different datasets, which in turn refined our approach to SM.\nHXL tags have two primary components: hashtags and attributes. Hashtags serve the purpose of delineating the primary categories of data, while attributes function as supplementary tags, established based on column data's content and formatting features. A given column should possess only one hashtag, yet it may incorporate multiple attributes. For example, a column named ISO-3, comprising country codes such as USA, SSD, GBR, and so on, corresponds to an HXL tag set denoted as \"#country+code+iso3\" where 'country' is the hashtag and 'code' and 'iso3' are attributes.\nIn contrast to the original HXL tags, HXL-style tags can include new hashtags to annotate data beyond the humanitarian field, providing a more flexible and extensible way of tagging data.\nWe employed Pre-trained Language Models (PLM), specifically GPT-4 [38] and mt0-xl [48], which acted as teacher model and student model respectively, to automatically generate HXL-style tags. This approach captured the description of each column and its corresponding values to conduct a sequence-to-sequence task: essentially transforming one sequence of data (the raw data) into another (the tagged data).\nOne of the challenges we faced was the lack of datasets that came with pre-annotated HXL-style tags. We have tried using data from HDX, which includes HXL tags, as training data to train a model for generating HXL-style tags. However, the results were unsatisfactory. The primary reason is that HDX is a dataset from the humanitarian sector, where the data types and topics are too constrained. Once such a trained model is applied to open-domain datasets, it tends to generate erroneous HXL-style tags when encountering unfamiliar data.\nTo be specific, the HXL standard itself is designed for humanitarian workers, and its main hashtag categories are: (1) Places (2) Surveys and assessments (3) Responses and other operations (4) Cash and finance (5) Crises, incidents, and events (6) Metadata. Clearly, categories (2) to (5) are tailored for humanitarian assessments, disasters, and organizational operations. Unfortunately, data of other content types in reality are likely to all fall under the Metadata category (#meta), leading to a serious limitation in the generated tags. Strictly speaking, HXL tags offer limited textual information for open-domain data processing, necessitating HXL-style tags that adhere to the basic HXL principles yet are capable of handling open-domain data.\nTo overcome the challenge, we used the concept of in-context learning [34], a machine learning approach where the model learns from the sequence of interactions during the dialogue, without being explicitly trained on a fixed dataset.\nIn in-context learning, the initial step involved the formulation of training examples (xi, yi) in a format that mapped inputs to labels using intuitive templates. The n training examples were integrated into a sequence using Equation (1), where the few-shot prompt helps generative models to understand our instruction to create tags.\n$P = \\pi{x_1, y_1} & \\pi{x_2, y_2} &\u2026 {x_n, y_n} & \\pi{x_{predict}, *}$ (1)$\nHere, $\\pi$ signifies a template-based transformation, and & represents the operation of concatenation.\nBelow, we offer an example of few-shot prompt using to generate HXL-style tag:"}, {"title": "3.4 Rule-based Feature Extraction", "content": "To apply machine learning on SM has been studied in various works [2, 47]. The initial step in this process requires feature engineering, which involves the extraction of distinct descriptors that represent the attributes of column names and values. The goal is to facilitate comparison and differentiation between columns, thereby providing an estimate of their similarity or dissimilarity. Features can be classified based on columns' textual representation, especially the column names; they can also be categorized according to the values that columns correspond to, since features for numerical data, dates, and text would differ significantly. In this section, we describe the features we used, including those derived from column names and those derived from values.\nColumn Name Features. The Column Name Features were calculated through pairwise comparisons between columns, where we employed various metrics to measure string similarity.\n\\begin{itemize}\n    \\item BLEU Score: This metric computes the Bilingual Evaluation Understudy score [41] between two column names. Given that BLEU is traditionally used for evaluating the similarity between machine-translated and human-translated texts, it can effectively measure the similarity between column names, especially when considering semantic nuances.\n    \\item Edit Distance: This metric computes the Damerau-Levenshtein distance [25], which measures the edit distance between two strings with substitutions, insertions, deletions, and transpositions. The Damerau-Levenshtein distance acknowledges the human tendency to make certain typos [32], such\n\\end{itemize}\nas transpositions, making it a versatile measure for comparing column names.\n\\begin{itemize}\n    \\item Longest Common Subsequence Ratio: This metric represents the Longest Common Subsequence ratio between two column names. This helps gauge how many continuous letters of one column name appears in another, which can be a potent signal when the columns have long descriptive names.\n    \\item One-In-One Occurrence: The feature $o_{ij}$ is a binary indicator demonstrating whether the name of one column is included within another. Here, $c_i$ and $c_j$ refer to the names of the columns. The presence of one column name within another can indicate a sub-category or related attribute.\n\\end{itemize}\n\n$0_{i,j} = \\begin{cases}\n    1 & \\text{if } c_i \\in c_j \\lor c_j \\in c_i, \\\\\n    0 & \\text{otherwise}.\n\\end{cases}$ (3)\n\\begin{itemize}\n    \\item Cosine Similarity in Semantic Embedding The similarity is calculated as the cosine similarity score [23] between the semantic embeddings, $s_i$ and $s_j$, of two column names. Details of the semantic embedding process will be expounded in the following section. This is especially useful when names might not be lexically similar, but they convey related concepts.\n\\end{itemize}\nConsequently, we derive the formula 4 to compute the feature score $ls_{i,j}$ of the column names $c_i$ and $c_j$.\n$lsi,j = \\frac{s_i \\cdot s_j}{\\| s_i \\| \\| s_j \\|} ; bleu(ci, , cj); lev(ci, cj); lcs(ci, Cj); 0i,j$ (4)\nIn which, i and j represent the indices of the source column and the target column, respectively. lev and lcs refer to the edit distance and longest common sub-sequence ratio respectively.\nValue Features. The value features were derived by analyzing the characteristics of the values, such as data type and numerical distribution (their details are shown below). As they represented the distribution or type features of individual columns, they could not explicitly reflect the similarity between the values of two columns. To address this, we introduced a normalization formula to calculate the similarity score between the value features of two columns i and j:\n$lvi,j = \\frac{fi - fj}{fi + fj + \\epsilon}$ (5)\nIn this formula, $f_i$ and $f_j$ refer to the computed value features of the i-th and j-th columns, respectively. $\\epsilon$ is an error term of small number in order to avoid a zero denominator. Here, the division is element-wise division. The resulting score captures the relative difference between the two sets of column value features, effectively serving as a similarity measure. This score is aptly suited for subsequent learning using the gradient boosting algorithm.\n\\begin{itemize}\n    \\item Data Type Features: These are applicable to all types of data, and make use of one-hot encoding to convert categorical data to a binary format. By identifying the inherent type of the data, we can have an initial grasp of what kind of information the column might be conveying, and this can\n\\end{itemize}"}, {"title": "3.5 Deep Embedding Similarity", "content": "Every column, either from the source or the target column set, was transformed into deep embeddings. These consisted of a column name embedding, s, and a textual value embedding, t (only for columns with text features). For each column name and textual value set, we employed a fine-tuned multilingual pre-trained language model, MPNet [45, 46, 51], to construct semantic embeddings. This involved tokenizing each column name and value set and then passing them through the model individually. The embeddings for an entire column were computed by aggregating the output of the model for each token. As validated by Table 7, the multilingual MP-Net displayed superior performance on a variety of sentence-pair tasks, especially semantic textual similarity, aligning well with our objective of assessing the similarity between two deep embeddings of column names."}, {"title": "3.6 Similarity Score Prediction using XGBoost", "content": "An ultimate hybrid similarity feature $l_{ij}$ is obtained from the $ls_{ij}$ (see Eq. 4), $h_{ij}$ (see Eq. 2), cosine similarity of textual value embedding$\\frac{t_i \\cdot t_j}{\\|t_i\\| \\|t_j\\|}$and value feature score $Iv_{ij}$ (see Eq. 5). A classifier takes $l_{ij}$ as an input to predict if $c_i$ and $c_j$ are matched.\n$l_{ij} = {lsi,j; lvi,j;\\frac{t_i \\cdot t_j}{\\|t_i\\| \\|t_j\\|}; hij}$ (6)\nWe used an XGBoost classifier, a scalable and high-performing tree boosting system, to predict a matched pair given the hybrid similarity feature. The motivation to use XGBoost is that it delivers a more impressive prediction on SM than other machine learning techniques like neural network or LightGBM, as shown in Table 8 that will be discussed later.\nThe output of the XGBoost models was a match score, indicating the probability that two columns are matched. While default thresholds were computed, users could define custom thresholds for deciding a match.\nWe aimed to enhance the robustness of our model training by adopting an ensemble approach. Our data was partitioned into 16 subsets, where each subset was used as validation set once, with the remaining 15 subsets serving as the training set during that iteration. This partitioning resulted in the creation of 16 distinct XGBoost models, with each model having its own trained weights as well as hyper-parameters determined by its assigned training and validation set.\nWe employed a soft voting fusion mechanism to consolidate the predictions from all 16 models. The majority vote from these models was then used as the final matching decision. Moreover, for the composite similarity score, we computed the average of the scores generated by all individual models.\nThis ensemble methodology not only improved the robustness of our predictions but also yielded a more consistent and trustworthy similarity score. Integrating deep embedding similarity with the XGBoost similarity score prediction established a sounded approach for column matching, catering to multilingual semantic similarities and offering adaptability with custom thresholds."}, {"title": "4 DATASETS", "content": "In this section, we will introduce the training dataset used for SMUTF, the proposed HDXSM Dataset used for schema matching (SM) evaluation, and other publicly available evaluation datasets proposed by previous research studies."}, {"title": "4.1 Training Dataset", "content": "Table 3 presents the sources of our training dataset, the column count for each table, along with the corresponding languages and topics, etc. Primarily, we obtained data from various popular websites through web scraping. Since the data was publicly available on the internet, the content was diverse in themes, including movies, real estate, animation, online shopping, cosmetics, and more. To create ground truths of the training dataset, we explored every pair of tables belonging to the same theme, and then columns with potential matches were subjected to manual alignment. This manual alignment process was undertaken by a team of four human annotators who deliberated over each table pair individually, reaching consensus before finalizing their decisions. During this process, care was taken to ensure that each annotator comprehended the content of the websites, as well as the data within the table pairs, ensuring a unanimous agreement was reached without dispute.\nIt's important to note that none of the topics in the training data appeared in the evaluation data. The only exception to this was pair number 16, where there was some overlaps between the IMDb website in our training data and the MovieLens-IMDB in the evaluation data. However, even though both involved the IMDb website, the columns used were different. The training dataset used a minority of columns related to movie ratings from IMDb, while MovieLens-IMDB involved aspects such as movie classification, theme, etc., which were not present in the training data.\nSimultaneously, the volatility of online data conferred pattern matching value on these contents. For example, the same scoring metric might have been named \"rating_star\" on website A, while website B might have named it \"review_star\". In addition, to increase the complexity of pattern matching and prevent the model from simply deducing inter-column relationships via column names, we manually modified certain columns. The main modification methods included language translation or masking. Language translation involved converting original Chinese column names into English, while masking changed meaningful column names into nonsensical codenames like \"col3\".\nUltimately, the majority of websites we collected featured content in either Chinese or a mixture of Chinese and English. This implicated the multilingual embedding component within SMUTF. Multilingual data could enhance the model's robustness when facing datasets from other domains. Moreover, if our system had been trained primarily on Chinese datasets and could demonstrate effective results in English domain data without additional training or fine-tuning, it would provide further evidence of our system's performance in an open-domain scenario.\nTo enrich the complexity of the data, our dataset not only contained text-based information but also a significant volume of identifiers, values, dates, URLs, and other forms of data. Such diversity of data types is common in pattern-matching tasks. By integrating different data forms, our model was expected to capture more complex patterns and achieve higher accuracy, while also demonstrating a broader applicability to a variety of real-world data scenarios."}, {"title": "4.2 HDXSM Dataset", "content": "Although SM has been an established research area for decades, it always suffers from a lack of publicly available, large-scale, real-world datasets. Current studies have predominantly relied on datasets that are automatically generated based on specific rules, or they have employed small-scale real-world datasets for method evaluation. Furthermore, most high-quality, real-world evaluation datasets from industry may not be accessible to the public due to privacy concerns. Recognizing these limitations, we developed a larger-scale, real-world SM dataset. This new dataset, named HDXSM, used data from the Humanitarian Data Exchange (HDX) and was annotated with existing HXL tags and extensive manual checks.\nAs of May 2023, the HDX had amassed a repository of 20,881 datasets, of which 8,652 had been adorned with HXL tags. Our research specifically focused on data delineated in tabular form; hence, we confined our analysis to datasets in CSV, XLS, and XLSX formats, which involves 8,640 datasets. It is important to note that each dataset may comprise multiple tables.\nIn line with the premise that HXL provides an accurate representation of column names and value data, we posited that two columns featuring identical HXL tags (including both hashtags and attributes) were eligible for matching. This led us to the inherent challenge of dataset selection for SM. The objective of our methodology was to faithfully replicate or mirror the practical requirements of humanitarian workers. Given this context, random pairwise matching of datasets was often impractical. For instance, cross-matching a food price dataset from Zimbabwe with a population dataset from Vietnam was devoid of tangible significance. Humanitarian work generally entails long-term commitment in specific regions, which necessitates the frequent linkage and analysis of data within a confined area (e.g., a particular country). Additionally, the data slated for linkage should originate from identical or overlapping domains. Fortunately, HDX provides a wealth of metadata for each dataset. Our methodology chiefly harnessed 'groups' (indicating the countries involved in the dataset) and 'theme tags' (representing the thematic or domain-specific aspects addressed in the dataset, such as COVID-19, funding, etc.). During the assembly of the HDXSM dataset, our process initially involved traversing all datasets for each country. A pair of datasets was deemed suitable for matching if both pertained to the same country and their theme tags yielded a Jaccard similarity exceeding 0.4. Subsequently, all tables from the two datasets were extracted and the HXL tags of each column were juxtaposed. However, certain datasets exhibited a high degree of similarity, potentially attributed only to differing data collection timelines, especially among regular observational datasets. These datasets resulted in an abundance of duplicate matches. We circumvented this redundancy by discarding repetitive matches, retaining only those pairings that showcased unique attributes in terms of column name and HXL tags. A subsequent review of the data revealed values of erroneous matching. These inaccuracies, unrelated to our methodology, were ascribed to pre-existing HXL tags annotation errors within the HDX datasets, such as values of reverse annotation of HXL tags for two columns. Therefore, we instigated a comprehensive manual annotation check of the entire HDXSM dataset. Ultimately, the HDXSM dataset incorporated a total of 204 table pairs. Each table contained a maximum of 100 rows, with the aggregate of columns across all tables amounting to 9,394. Notably, out of these, 2,635 column pairs were matched."}, {"title": "4.3 Publicly Available Datasets", "content": "We incorporated four public available datasets into our experiment. The WikiData dataset comed from Valentine\u00b2 [22]. It was collected from real-world data and re-organized into different types of schema pairs. Given a tabular schema, Valentine suggests splitting it horizontally to create unionable pairs, vertically to make joinable pairs, or in both ways. This methodology helps resolve the limited data sources for SM by manually generating new column pairs from a single table. To be specific, a unionable dataset is created by horizontally partitioning the table with different percentages of row overlap, and a view-unionable dataset is made by splitting a table both horizontally and vertically with no row overlap but various column overlap. A pair of joinable tables should have at least one column in common and a large row overlap. The semantically-joinable dataset is similar to the joinable one, except that their column names are noisy (semantic variations). In addition, all values under different types of datasets are manually made noisy. WikiData has 4 schema pairs, and each table has 13-20 columns. The maximum row number can be above 10,000.\nThe second publicly available dataset was obtained from two public movie databases, MovieLens\u00b3 and IMDB4. These two databases are commonly used to create schema pairs since their columns are similar to each other, like rating vs. averageRating or title vs. originalTitle. The MovieLens-IMDB dataset has been widely used in the field of SM [60][58], but there is not a standard version of it. Our MovieLens-IMDB dataset has 2 pairs of schemas and each schema has 1000 rows. Its column number varies from 4 to 10.\nThe final pair of datasets, Monitor and Camera, originate from the DI2KG Benchmark datasets. DI2KG is acknowledged as a comprehensive data integration benchmark that comes with a mediated schema. These datasets encompass product specifications scraped from a wide range of eCommerce platforms such as ebay and walmart. In alignment with the format adopted by the aforementioned datasets, we utilized the mediated schema provided by DI2KG to meticulously construct 20 table pairs for both Monitor and Camera, along with the corresponding mapping ground truths, thus ensuring consistency across the board in terms of data structure (Di: it's hard to understand these two sentences, which sound vague to me). A distinctive attribute of the Monitor and Camera datasets is the prevalence of numerous many-to-many correspondences; here, a unique column may find matches across multiple columns within a disparate table, an intricacy brought forth by the application of the mediated schema.\nThe basic statistics of all the benchmark datasets including HDXSM are given in Table 4."}, {"title": "5 EXPERIMENTS", "content": "We evaluated the performance of SMUTF across four distinct datasets and six benchmark approaches. Our evaluation utilizes macro-F1 and macro-AUC scores to compare the performance of our method with the benchmarks."}, {"title": "5.1 Evaluation Metrics", "content": "Every dataset is made up of schema pairs, and each of them represents a SM task to be solved. Initially, we determine an F1 and an AUC score of ROC for each schema pair. Following that, we compute the average F1 and AUC score across all the schema pairs"}, {"title": "5.2 Benchmarking Methods", "content": "Benchmarking SM approaches evaluated in experiments can be tentatively categorized into three types: schema-based, value-based and hybrid matching."}, {"title": "5.2.1 Schema-based Matching", "content": "A schema-based matching employs schema-related information, which includes column names, description, inter-column relationships, etc, to find out matched pairs within two different schemas.\nCupid. The Cupid [31] framework represents an initial effort of this approach, encompassing linguistic matching of column names, which calculates similarity through synonyms and hypernyms, and structural matching that examines the hierarchy between columns, considering their containment relationships. Column matches are determined by a weighted combination of these linguistic and structural similarities.\nSimilarity Flooding. Similarity flooding [33] is a schema-matching method that uses graph representations to assess relationships between columns, initiating with a string matcher that identifies potential column mappings through common prefixes and suffixes. This method then expands the search for matches by propagating similarities; if two columns from different schemas are similar, their neighboring columns' similarity is also increased. Like Cupid, similarity flooding heavily relies on the linguistic resemblance of column names.\nCOMA. COMA [10] introduces a system that flexibly integrates multiple schema matchers to evaluate column similarity across different schemas. Schemas are modeled as rooted directed acyclic graphs, with each column represented by a path from the root. COMA employs various strategies to aggregate the similarity scores provided by different matchers, such as taking the average or maximum, and it uses specific criteria to select matching column pairs, like those exceeding a similarity threshold or ranking in the top-K. Experimental results indicate that while individual matchers might be flawed, their combined use can enhance matching performance. COMA has evolved to include instance-based matching, leading to two variants: COMA-Schema for schema-centric matching and COMA-Instance, which adopts a hybrid matching approach."}, {"title": "5.2.2 Value-based Matching", "content": "A value-based matching is data-oriented, focusing on utilizing statistical measures to explore relationships between values under different columns.\nDistribution-based. A distribution-based schema matcher [58] utilizes the Earth Mover's Distance (EMD) to measure how much effort is required to transform one column's set of values into another, focusing on their rankings. Initially, it clusters columns using pairwise EMD calculations. Next, clusters are broken down into matched pairs using the intersection EMD, a metric grounded in two principles: columns sharing many values are likely related, and columns with minimal intersection are matched if they both significantly overlap with a third column.\nJaccard-Levenshtein. The Jaccard-Levenshtein method [22] is a value-based SM technique that applies the Jaccard similarity index to evaluate the relatedness between pairs of columns, considering two values as identical if their Levenshtein distance falls below a predefined threshold. This approach offers a direct and uncomplicated way to compare the distribution of values in columns to ascertain matches."}, {"title": "5.2.3 Hybrid Matching", "content": "A hybrid SM involving the consideration of both schema-related information and value-oriented features.\nEmbDI[5] is an approach for data integration by creating relational embeddings upon column names and values. These embeddings are trained from scratch and external knowledge such as synonym dictionaries is involved. Our proposed model, SMUTF, is also an hybrid matching-based model, since it not only builds semantic embeddings on column names but also compute value features when comparing two columns. The integration of schema-based information and column values is supposed to present a more robust performance upon SM than matching techniques with a single focus.\nFurthermore, the SMUTF framework's generation of HXL-style tags can be regarded as a semantic annotation technique identifying data types, which enriches the landscape of SM strategies. Within this context, we have incorporated Sherlock[17] into our suite of benchmark methods. Sherlock operates through an elaborate supervised learning paradigm, processing an extensive corpus of tabular datasets. It adeptly derives a diverse array of features from both columnar nomenclature and cell contents, subsequently assigning these to different types of semantic data, such as Location, Name, or Year. Deployed within the SM arena, we determine the columns that Sherlock predicts to be of the same data type to be matched. Di: the last sentence is too hard for me to understand."}, {"title": "5.3 Benchmark Results and Discussion", "content": "The inference results for various datasets were displayed in Table 5 and Figure 3. To assess the versatility of our model across different domains, we ensured that the domains of inference datasets differed from those of the training dataset.\nWikidata provided by valentine had different types of datasets based on four table-splitting strategies (see Figure 3). Though our model's macro-F1 performance was the best one among other benchmarks (see Figure 3), its individual evaluation on the joinable dataset (F1 85.71%) was worse than the Jaccard-Levenshtein method (F1 100%). In a joinable pair of schemas, there was a large overlap of rows. Since the Jaccard-Levenshtein method is a naive approach matching columns based on the row-value distribution from each schema, it was not surprising that given a large number of rows in each schema (more than 5000), this model could"}]}