{"title": "Extrapolated Urban View Synthesis Benchmark", "authors": ["Xiangyu Han", "Zhen Jia", "Boyi Li", "Yan Wang", "Boris Ivanovic", "Yurong You", "Lingjie Liu", "Yue Wang", "Marco Pavone", "Chen Feng", "Yiming Li"], "abstract": "Photorealistic simulators are essential for the training and evaluation of vision-centric autonomous vehicles (AVs). At their core is Novel View Synthesis (NVS), a crucial ca- pability that generates diverse unseen viewpoints to ac- commodate the broad and continuous pose distribution of AVs. Recent advances in radiance fields, such as 3D Gaus- sian Splatting, achieve photorealistic rendering at real- time speeds and have been widely used in modeling large- scale driving scenes. However, their performance is com- monly evaluated using an interpolated setup with highly correlated training and test views. In contrast, extrapola- tion, where test views largely deviate from training views, remains underexplored, limiting progress in generalizable simulation technology. To address this gap, we lever- age publicly available AV datasets with multiple traversals, multiple vehicles, and multiple cameras to build the first Extrapolated Urban View Synthesis (EUVS) benchmark. Meanwhile, we conduct quantitative and qualitative evalua- tions of state-of-the-art Gaussian Splatting methods across different difficulty levels. Our results show that Gaussian Splatting is prone to overfitting to training views. Besides, incorporating diffusion priors and improving geometry can- not fundamentally improve NVS under large view changes, highlighting the need for more robust approaches and large- scale training. We have released our data to help advance self-driving and urban robotics simulation technology.", "sections": [{"title": "1. Introduction", "content": "The development of vision-centric autonomous vehicles (AVs) relies heavily on photorealistic simulators, which provide controlled, reproducible, and scalable environments for training and evaluation of driving models [14, 43, 57]. These simulators enable AVs to learn and adapt to a variety of real-world scenarios, from crowded urban streets to ad- verse weather conditions, without the logistical and safety concerns of physical road testing. At the heart of these sim- ulators is the capability for Novel View Synthesis (NVS)\u2014a key technology that generates realistic images of unseen viewpoints, simulating the continuous changes in perspec- tive that occur as AVs navigate through urban environments.\nRecent advancements in radiance fields, particularly methods based on 3D Gaussian Splatting [25], have signif- icantly improved the realism and efficiency of NVS. These approaches [8, 53, 65, 67] can produce photorealistic ren- derings at real-time speeds, making them highly attractive for large-scale driving scene simulation. However, despite their impressive results, the evaluation of NVS methods has predominantly focused on interpolated scenarios, where training and test viewpoints are closely related. While in- terpolation tests are valuable for assessing local consistency, they fall short in addressing the more critical challenge of extrapolation\u2014where test viewpoints differ significantly from the training data. As shown in Figure 1, the in- terpolation test set demonstrates strong performance, with metrics such as PSNR, SSIM, and LPIPS remaining very close to the training set values. In contrast, the extrapola- tion test set, which includes additional translation and ro- tation changes relative to the training set, exhibits notable drops in performance. Specifically, the metric decreases relative to the training set are 28% for PSNR, 22% for SSIM, and 50% for LPIPS. These results underscore the urgent need to explore and advance extrapolated view syn- thesis in complex urban scenes, as real-world driving often involves encountering scenarios with significant viewpoint shifts and diverse spatial transformations that deviate from training distributions. Several recent studies [21, 23] have investigated the generalization capabilities of NVS in 3D Gaussian Splatting. Although they show promising quali- tative results, there is no comprehensive quantitative analy- sis due to the absence of standardized datasets. Moreover, their evaluations are primarily limited to specific scenarios or use cases, without investigating varying levels of diffi- culty based on the degree of extrapolation. This gap under- scores the urgent need for a benchmark that offers diverse and challenging datasets, enabling a rigorous and system- atic assessment of NVS methods.\nTo establish a common platform for assessing the ro- bustness of NVS methods, we introduce a comprehen- sive benchmark for quantitatively and qualitatively evalu- ating extrapolated novel view synthesis in large-scale ur- ban scenes. Our benchmark leverages publicly available datasets, including NuPlan [4], MARS [30], and Argov- erse2 [45], which feature multi-traversal, multi-agent and multi-camera sensory recordings. Multi-traversal data con- sists of asynchronous traversals of the same location, while multi-agent data captures multiple vehicles simultaneously present in the same area. These data provide diverse cam- era poses within a 3D scene, enabling the training and eval- uation of extrapolated view synthesis in outdoor environ- ments. For the experimental setup, we define three difficulty levels: (1) translation only, (2) rotation only, and (3) transla- tion + rotation, as shown in Figure 4. In autonomous driving scenarios, Level 1 corresponds to maneuvers such as lane changes, Level 2 involves switching between cameras fac- ing different directions, and Level 3 addresses complex in- tersections, such as crossroads with diverse traversal paths. These levels represent common challenges in autonomous driving, and addressing them enables the synthesis of com- plete scenes from sparse image observations.\nWe conduct pose estimation and sparse reconstruction using COLMAP [38], which facilitates the initialization of Gaussian Splatting. We then evaluate state-of-the-art Gaus- sian Splatting-based approaches across each difficulty level, identifying performance gaps both qualitatively and quanti- tatively in extrapolated urban view synthesis.\nIn summary, our main contributions are as follows:\n\u2022 We initiate the first comprehensive quantitative and qual- itative study on the Extrapolated Urban View Synthesis (EUVS) problem, supported by a robust evaluation frame- work that categorizes difficulty levels (translation-only, rotation-only, and translation + rotation), while assessing performance using diverse metrics including reconstruc- tion accuracy and visual fidelity.\n\u2022 We construct a novel dataset by integrating multi- traversal, multi-agent, and multi-camera data from pub- licly available resources, totaling 90,810 frames across 345 videos. Our dataset effectively addresses the limi- tations of existing benchmarks, enabling rigorous and ro- bust evaluation for extrapolated urban view synthesis.\n\u2022 We benchmark state-of-the-art Gaussian Splatting-based and NeRF-based models and analyze key factors that in- fluence the performance of extrapolated NVS, laying a solid foundation for future advancements in this challeng- ing task. Data and code are released on our project page."}, {"title": "2. Related Works", "content": "Extrapolated View Synthesis. Extrapolated view synthe- sis aims to generate novel views beyond observed perspec- tives, addressing challenges in visual coherence for unseen regions. RapNeRF [61] proposes a random ray-casting pol- icy that enables training on unseen views based on visible ones. Following work [54] enhances this approach by incor- porating holistic priors. Additionally, numerous generaliz- able models [6, 9, 44] have emerged, capable of generating extrapolated novel views from a limited number of input im- ages. While these methods are designed for indoor scenes, several works address extrapolated view synthesis in out-"}, {"title": "3. The EUVS Benchmark", "content": "3.1. Data Source\nWe employ three publicly available autonomous driving datasets, nuPlan [4], Argoverse 2 [45], and MARS [30], to comprehensively evaluate extrapolated view synthesis in urban driving scenes while leveraging their unique charac- teristics to ensure robust and diverse assessments. The nu- Plan [4] dataset is the first large-scale planning benchmark, providing 1,200 hours of driving data collected from four cities across the United States and Asia. Argoverse 2 [45] focuses on multimodal perception and forecasting, provid- ing 1,000 annotated 3D scenarios with lidar, stereo imagery, and HD maps. It also includes 20,000 unlabeled lidar se- quences for self-supervised learning and 250,000 motion forecasting scenarios highlighting complex interactions in six U.S. cities. The MARS [30] dataset introduces multi- agent and multi-traversal scenarios, supporting collabora- tive driving research with vehicles interacting within the same area and asynchronous revisits to the same locations. By integrating these datasets, our benchmark enables the evaluation of view synthesis across diverse and realistic ur- ban environments under varying conditions. Figure 3 illus- trates the distribution of the integrated datasets.\n3.2. Evaluation Framework\nTo systematically assess model performance in extrapo- lated urban view synthesis, our evaluation framework in- corporates three difficulty levels and three data configura- tions. Data configurations include multi-traversal, multi- agent, and multi-camera, while difficulty levels are cate- gorized into (1) translation only, (2) rotation only, and (3)"}, {"title": "3.3. Algorithm Overview", "content": "Vanilla 3D Gaussian Splatting. 3D Gaussian Splatting (3DGS) [25] leverage 3D Gaussians to explicitly represent the scene, which achieves high quality while offering real- time rendering by avoiding unnecessary computation in the empty space. Building on this, 3DGM [31] leverages multi- traversal consensus to differentiate transient and permanent elements, enabling joint 2D segmentation and 3D mapping without using any human supervision.\nPlanar-based and Geometry Refined Gaussian Splat- ting. GaussianPro [11] builds on 3DGS [25] by introducing multi-frame geometric optimization, which guides the den- sification of 3D Gaussians, enhancing scene consistency in complex geometries. It further refines geometry by encour- aging Gaussian primitives to adopt flat structures. Similarly, 2DGS [22] projects the 3D volume into a set of 2D ori- ented planar Gaussian disks, enabling high-fidelity surface reconstruction. PGSR [7] introduces an unbiased depth ren- dering method and integrates single-view geometric, multi- view photometric, and geometric regularization techniques to improve global geometry accuracy.\nGaussian Splatting with Diffusion Priors. VEGS [23] introduces a novel view generalization approach that har- nesses pre-extracted surface normals to align 3D Gaussians while generating augmented camera views guided by dif- fusion priors. These diffusion priors serve a dual purpose: providing denoising loss guidance and supervising the train- ing of augmented cameras. This process effectively miti- gates floating artifacts and fragmented geometries, resulting in more accurate and coherent 3D representations.\nFeature-Enhanced Gaussian Splatting. Feature 3DGS [66] extends 3D Gaussian Splatting with a Parallel N-dimensional Gaussian Rasterizer, allowing simultaneous rendering of radiance fields and high-dimensional semantic features. By embedding semantic features directly into 3D Gaussians, the approach enhances optimization, enabling better correspondence with scene semantics and achieving more detailed and accurate spatial representations.\nNeRF-based Method. Instant-NGP [35] uses a multireso- lution hash encoding to map spatial coordinates into com- pact latent representations via hash tables across multi- ple resolutions. This approach efficiently encodes high- frequency details by combining trainable feature vectors with interpolation, enabling adaptive and scalable input en- codings without the need for structural updates or explicit collision handling. Zip-NeRF [2] leverages multisampling with isotropic Gaussians for scale-aware features and in- troduces a smooth anti-aliasing loss to address z-aliasing. In addition, it incorporates a novel distance normaliza- tion technique to better manage close and distant objects, achieving high-quality rendering and fast training."}, {"title": "4. Experiment", "content": "4.1. Experiment Setup\nImplementation Details. We initialize 3DGS-based meth- ods with COLMAP [38]. We use SegFormer [50] to mask out movable objects during training and evaluation. We also exclude movable objects during 3DGS initialization.\nEvaluation Metrics. We use three widely-used metrics to evaluate visual quality: peak signal-to-noise ratio (PSNR), structural similarity index measure (SSIM), and learned per- ceptual image patch similarity (LPIPS) [62]. We also em- ploy DINOv2 [36] feature cosine similarity to evaluate im- age quality in latent space. For geometry evaluation, we use depth metrics, including RMSE and $\\delta 1.25$.\n4.2. Experimental Results\nTable 1 and 3 present the quantitative results across Levels 1-3, while Figure 5 illustrates the qualitative outcomes on the extrapolated test set. The results indicate that, while the metrics perform relatively well in the training set and inter- polated test set, there is a significant drop in performance in the extrapolated test set across all baselines. We detail the rendering results of each level in the following.\nLevel 1: Translation-only. At Level 1, the views from the training cameras fully cover the test views, with moderate translational changes. (1) The results reveal a consistent drop in train-to-test performance across all metrics, under- scoring the challenge of generalizing to unseen views. No- tably, the relative performance drop varies by metric: for instance, PSNR decreases by approximately 23\u201325% across most methods (e.g., GSPro [11]: 21.51 \u2192 16.39, a 23.8% drop), with SSIM and LPIPS showing similar proportional declines. (2) On the test set, the methods perform compara- bly: GSPro achieves the highest PSNR (16.39) and lowest LPIPS (0.2450), while 3DGM [31] leads in SSIM (0.7248). Both GSPro and 3DGM deliver similar feature cosine simi- larity scores (GSPro: 0.6130, 3DGM: 0.6087). Other meth- ods, such as VEGS [23] and PGSR [7], lag slightly, indicat- ing opportunities for improvement. The results suggest that while GSPro marginally outperforms the others, the differ- ences are small, emphasizing the need for better solutions.\nLevel 2: Rotation-only. At Level 2, the training views pro- vide substantial coverage of the surrounding scene. How- ever, most methods exhibit poor generalization ability, with a PSNR drop of approximately 22.75%. Rotation variations are particularly challenging for areas rich in texture, such as trees and intricate elements, where blurring artifacts of- ten appear. Regions perpendicular to the vehicle are also difficult to capture. Furthermore, distant regions pose sig- nificant reconstruction challenges, frequently resulting in missing buildings and black holes in the sky. VEGS [23] and GSPro [11] stand out as the best-performing baselines at this level: VEGS benefits from its diffusion prior, while GSPro's robust geometry handling enhances generalization.\nLevel 3: Translation + Rotation. At Level 3, the view changes are the largest. (1) All methods exhibit notable train-to-test performance drops across all metrics, reflect- ing the difficulty of generalizing to Level 3. For instance, 3DGS [25] experiences a PSNR drop of 29.4% (21.22 \u2192 14.99), while GSPro [11] undergoes a similar drop of 31.3% (21.58 \u2192 14.82). These declines underscore the increas- ing challenge of handling extrapolated views as complex- ity rises. (2) Feature 3DGS [66] and 3DGM [31] stand out as leading methods, excelling in LPIPS (0.3816) and SSIM (0.7233), respectively, and demonstrating notable improve- ments compared to other methods. However, overall per- formance remains limited, with PSNR consistently falling below 15, highlighting significant room for improvement in generating high-fidelity outputs for extrapolated views."}, {"title": "4.3. Comparison of Baselines", "content": "Quantitative Comparison. We report the quantitative per- formance comparison across all levels and baselines in Fig- ure I. (1) At Level 1, as shown in Figure Ia, GSPro [11] slightly outperforms other baselines on the training set. However, the performance gaps on the test set are small, with most baselines performing comparably poorly. Among them, 3DGS [25], 3DGM [31], and GSPro achieve rel- atively better results. (2) At Level 2 (Figure Ib), Zip- NeRF [2] demonstrates over a 10% improvement in PSNR compared to all other baselines on the training set. In extrapolated poses, VEGS [23] significantly outperforms all other methods, achieving at least 20% higher PSNR. These results highlight the effectiveness of diffusion priors in rotation-only settings. (3) At Level 3, as shown in Fig- ure Ic, none of the baselines exhibit a clear advantage, as all methods fail equally in this challenging setting. Among them, GSPro maintains a slight lead on the training set. On the test set, different baselines exhibit strengths in specific metrics, but no method demonstrates superiority across all metrics, indicating that all baselines struggle with extrapo- lated view synthesis and fail to address it fundamentally.\nQualitative Comparison. We present the qualitative base- line comparison across all levels and baselines in Figure II, Figure III and Figure IV. (1) At Level 1, as shown in Fig- ure IIb, all methods exhibit imperfections in ground ren- dering, while planar-based methods such as 2DGS [22] and PGSR [7] show comparatively fewer flaws on the ground surface. GSPro [11] produces more accurate geometry re- construction, achieving realistic surfaces and high-fidelity representations of street objects like trees and buildings. (2) At Level 2, as shown in Figure III, most baselines suffer from sky artifacts such as holes and floating objects. In con- trast, VEGS [23] produces the more accurate renderings, exhibiting minimal floating artifacts and broken geometry, attributed to the guidance provided by diffusion priors. (3) At Level 3, as shown in Figure IVb, all baselines face sig- nificant challenges on the test set. The geometry across all methods appears highly fragmented, and the color consis- tency is compromised, reflecting a tendency to overfit to the training views. Among the baselines, 2DGS and PGSR show relatively weaker performance, underscoring the limi- tations of planar representations in effectively capturing the complexity of urban scenes."}, {"title": "4.4. Discussion and Analysis", "content": "Planar-Based vs. Ellipsoid-Based. Planar-based meth- ods (e.g., GSPro [11], PGSR [7], and 2DGS [22]) excel in road representation due to their planar geometry and refine- ment strategies but struggle with fine-textured urban objects like plants and fences. Conversely, ellipsoid-based meth- ods (e.g., 3DGS [25] and 3DGM [31]) better handle high- textured objects but often overfit, leading to errors in road representation. For instance, in the translation setting (Fig- ure 6a), planar-based methods struggle with plants, while ellipsoid-based methods perform poorly on roads. A hy- brid representation could effectively combine the strengths of both approaches to address these challenges in EUVS.\nEnhancing View Synthesis with Diffusion Priors. While training cameras may collectively cover the entire scene, the limited number of viewpoints often results in insufficient representation of certain areas. Leveraging diffusion priors proves to be an effective approach in such cases. By su- pervising augmented views with diffusion priors, unseen or poorly represented views can be generated and corrected. For instance, as shown in Figure 6b, the building rendered by other models is fragmented, but guiding with diffusion priors helps complete the building structure and presents a holistic urban scene. On average, as shown in Table 1, VEGS [23] with diffusion priors significantly outperforms 3DGS [25] in the rotation-only setting, achieving a 19.4% increase in PSNR (23.33 vs. 19.53) and a 5.8% improve- ment in SSIM (0.7949 vs. 0.7511).\nRegularization by Depth Priors. Utilizing depth priors from foundation models, such as Depth Anything [56], has proven to be an effective approach for enhancing training regularization [12]. In our experiments, depth regulariza- tion enhances geometric accuracy by utilizing depth infor- mation to constrain Gaussians in regions like the sky and road to more geometrically consistent positions. As shown in Figure 6c, the sky is accurately constrained to a distant position, ensuring it does not overlap with the building dur- ing lane changes. Similarly, the road is aligned to a con- sistent plane, effectively mitigating the distortion issues ob- served in the vanilla baseline. The regularization by depth priors ensures spatial consistency and reduces visual arti- facts, leading to more reasonable extrapolated views.\nGaussian-Based vs. NeRF-Based Methods. A fundamen- tal difference between Gaussian-based and NeRF-based ap- proaches lies in their representation: Gaussian-based meth- ods rely on explicit representations, whereas NeRF-based methods use implicit representations. Our experiments re- veal that implicit methods, such as Zip-NeRF [2], excel at handling low-texture regions like roads and sky in extrap- olated views, due to their ability to ensure coherent depth and color transitions across large, gradual surfaces. How- ever, NeRF-based methods struggle with capturing high- frequency or fine-grained details, such as lane markings and fences, due to the inherent limitations of their implicit representation. In contrast, the explicit representation of Gaussian-based methods demonstrates a distinct advantage in preserving detail and sharpness in high-frequency re- gions, as illustrated in Figure 6d.\nDynamic Scenes. Current methods that focus on dynamic scenes also struggle with extrapolated view synthesis. To evaluate extrapolated view synthesis in dynamic scenes, we implement OmniRe [10] as a baseline, training on six front and back cameras and testing on two side cameras. OmniRe organizes rigid-deformable nodes and background nodes to capture dynamic scene structures and employs SMPL [32] for non-rigid object modeling. As illustrated in Figure 6e, the rendering results reveal a significant performance gap between the training and test sets. In the test views, objects such as trees and stakes lose texture and geometric details, resulting in noticeably blurry outputs, whereas the training views maintain high fidelity. As shown in Table 4, the per- formance metrics indicate an average drop of 25%. The re- sults highlight the challenges of extrapolated view synthesis in dynamic scenes and the need for further research.\nLighting Inconsistency Handling. Lighting inconsisten- cies between training and test traversals, driven by varia-"}, {"title": "5. Conclusion", "content": "We introduce the first Extrapolated Urban View Synthe- sis (EUVS) benchmark to advance photorealistic simula- tion technologies for self-driving and robotics. Our bench- mark integrates real-world multi-traversal, multi-agent, and multi-camera data, categorizes scenes by difficulty, and evaluates state-of-the-art NVS models. Experimental re- sults reveal that while some methods address specific chal- lenges, current models demonstrate limited generalization, with significant overfitting to training views and suboptimal performance in extrapolated view synthesis. Additionally, we identify key challenges across difficulty levels and high-"}]}