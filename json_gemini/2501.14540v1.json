{"title": "VERUS-LM: a Versatile Framework for Combining LLMs with Symbolic Reasoning", "authors": ["Benjamin Callewaert", "Simon Vandevelde", "Joost Vennekens"], "abstract": "A recent approach to neurosymbolic reasoning is to explicitly combine the strengths of large language models (LLMs) and symbolic solvers to tackle complex reasoning tasks. However, current approaches face significant limitations, including poor generalizability due to task-specific prompts, inefficiencies caused by the lack of separation between knowledge and queries, and restricted inferential capabilities. These shortcomings hinder their scalability and applicability across diverse domains. In this paper, we introduce VERUS-LM, a novel framework designed to address these challenges. VERUS-LM employs a generic prompting mechanism, clearly separates domain knowledge from queries, and supports a wide range of different logical reasoning tasks. This framework enhances adaptability, reduces computational cost, and allows for richer forms of reasoning, such as optimization and constraint satisfaction. We show that our approach succeeds in diverse reasoning on a novel dataset, markedly outperforming LLMs. Additionally, our system achieves competitive results on common reasoning benchmarks when compared to other state-of-the-art approaches, and significantly surpasses them on the difficult AR-LSAT dataset. By pushing the boundaries of hybrid reasoning, VERUS-LM represents a significant step towards more versatile neurosymbolic AI systems.", "sections": [{"title": "1 Introduction", "content": "Logical reasoning is an essential aspect of problem-solving, decision-making, and critical thinking, making it an important goal in Artificial Intelligence. While recent large language models (LLMs) such as GPT-4 [Achiam et al., 2023] have shown an impressive leap in reasoning capabilities through techniques like chain-of-thought prompting, they often lack the ability to guarantee faithful and transparent reasoning. Indeed, at a fundamental level, LLMs operate as black-box probabilistic models, which makes it difficulty to ensure accurate and coherent reasoning steps.\nSymbolic inference engines, on the other hand, can derive conclusions that are provably correct and typically also explainable. However, these systems struggle to interpret ambiguous natural language input. This creates an opportunity for neurosymbolic methods to combine the strengths of LLMs with symbolic reasoning systems. Interestingly, this method loosely parallels Kahneman's theory on human cognition, in which reasoning is divided into fast and unconscious thinking (System 1) and slow and methodical thinking (System 2) [Kahneman, 2011]. These approaches have already demonstrated impressive improvements on common reasoning datasets [Pan et al., 2023; Ishay et al., 2023; Olausson et al., 2023].\nDespite these promising results, current state-of-the-art approaches have several limitations that hinder their applicability across diverse domains. One significant drawback is their heavy reliance on task-specific prompts to generate symbolic representations, via in-context learning. This makes these approaches less general and harder to adapt to new domains, since this may require manual prompt engineering. In real-world scenarios, where problems are highly variable, task-specific prompts can cause a severe bottleneck. Furthermore, they typically try to reduce every user query to the same logical reasoning task (or to one of only a very limited set of reasoning tasks). In many domains, users have widely different kinds of queries, which correspond naturally to equally wide range of logical reasoning tasks (e.g., such as optimization, generating solutions, deriving consequences, and generating explanations). Therefore, this lack of versatility limits the effectiveness of the state-of-the-art tools in complex domains where multiple forms of reasoning are required.\nIn this paper, we propose VERUS-LM, an enhanced neurosymbolic framework that addresses these limitations. It features a generic prompting pipeline, a semantic refinement step, a clear separation between declarative knowledge from questions, and support for a wide range of logical reasoning tasks. In this way, it is able to handle complex and dynamic environments in a robust and effective manner."}, {"title": "2 Related Work", "content": "Improving the reasoning capabilities of LLMs has recently become a topic of great interest. For the purpose of this paper, however, we will not discuss works on improving LLM reasoning with in-context prompting or by fine-tuning, but refer to [Huang and Chang, 2023; Plaat et al., 2024] for more information on those techniques. Instead, we will focus on the neurosymbolic approach in which an LLM is coupled with an explicit symbolic reasoning engine.\nCurrent state-of-the-art systems all largely work in the same way. To solve a given reasoning problem, which typically consists of a description of some domain and a query that needs to be answered, they first let an LLM generate a formal representation of the problem and then pass this on to a reasoning engine to solve. In this way, they combine the benefits of both kinds of AI systems: the LLM offers a user-friendly natural language interface, while the reasoning engine ensures correct reasoning. This offers a promising path towards improving the reasoning skills of LLM-based systems. We will now briefly go over some of these systems and touch on how they differ.\nIn [Ishay et al., 2023], the authors present a pipeline to automatically generate Answer Set Programming (ASP) representations [Brewka et al., 2011]. Their method uses the LLM three times: first to extract relevant constants, next to generate predicates, and finally to generate the ASP rules. Using a logic puzzle dataset, they demonstrate a 71% increase in accuracy compared to a baseline GPT-4. The same authors also rely on ASP in the [LLM]+ASP framework [Yang et al., 2023], where given a problem description, the LLM extracts a set of atomic facts. These facts, coupled with pre-defined, hand-crafted \"knowledge modules\" containing domain-specific ASP rules, are fed to an ASP solver to find a solution. They validate their approach on four \u201cstory-based\" datasets, on which it outperforms the state-of-the-art.\nIn the LINC system [Olausson et al., 2023], an LLM is used to generate statements in First Order Logic (FOL), which are given to a FOL reasoner. To mitigate formalization errors, they generate K formalizations and make use of K-way majority voting to decide on the correct response. They demonstrate that their approach is competitive on the FOLIO dataset [Han et al., 2022], and achieves remarkable improvements on the ProofWriter dataset [Tafjord et al., 2020].\nThe final neurosymbolic system we discuss, Logic-LM [Pan et al., 2023], has two distinctive features. First, instead of relying on a single formalization language, their system supports logic programs, FOL, constraint satisfaction problems and SAT encodings. In this way, the system is able to handle a broader range of problem types. Second, they introduce a self-refinement stage, in which the LLM iteratively attempts to fix any syntax errors based on feedback from the solver. They validate their approach on five datasets, on which they generally outperform GPT-4 using chain-of-thought as baseline.\nIt is worth noting that most of the aforementioned methods rely heavily on dataset-specific examples, utilized for in-context learning, to guide the LLMs. Furthermore, they only support a limited number of logical reasoning tasks, which limits their generalization capabilities to new datasets. This is especially apparent on the AR-LSAT dataset [Zhong et al., 2021], featuring logical reasoning questions from the Law School Admission Test, which is high in diversity and thus requires broad reasoning capabilities. For instance, given knowledge on the types of CDs sold by a store, the questions can range from \"Which CDs are on sale?\" to \"Given 4, which statements are true?\" and \"What are the minimum CDs required to be on sale if $?\", for some additional statement 4. Due to this diversity, Logic-LM only achieves a 43.04% accuracy on this dataset, though still an improvement over baseline GPT-4.\nWhile it is strictly speaking not a neurosymbolic approach, we also briefly discuss the SymbCoT framework [Xu et al., 2024] for the sake of comparison. Like the aforementioned works, SymbCoT internally translates a problem into FOL using in-context learning. However, instead of using a separate reasoning engine, the LLM itself then performs logical reasoning on the generated statements in a step-by-step way similar to Chain-of-Thought, instructed by specific prompts that try to mimic the kinds of logical reasoning found in logical solvers. By not using a separate reasoning engine, the approach becomes more robust to syntax errors. Interestingly, this approach consistently outperforms standard Chain-of-Thought and Logic-LM on the PrOntoQA [Saparov and He, 2022], FOLIO and ProofWriter datasets.\""}, {"title": "3 System Design", "content": "3.1 Requirements\nAs discussed in the previous section, current state-of-the-art approaches typically focus on a single reasoning tasks. However, more realistic use cases will likely require a more versatile approach. Imagine, for instance, a chat bot acting as a digital assistant in the field of car insurance. A person interacting with such an AI system will not only ask questions like \"Am I eligible for an insurance policy?\", but will also ask other things such as \u201cWhy am I (in)eligible?\u201d or \u201cDepending on my car type, what would my insurance premium?\u201d.\nFor a system to be capable of handling such use cases, there are two main requirements. First, the system must support multiple modes of reasoning, which means that reasoning-specific LLM prompts will not work well (as evidenced by the results of state-of-the-art systems on the AR-LSAT dataset). Second, the reasoning component of such a system must be capable of performing different kinds of reasoning, either using a single versatile engine or, similar to Logic-LM, using multiple ones. However, this latter approach has the downside that the domain knowledge needs to be formalized anew for each type of reasoning, increasing the computational cost and risk of errors. We therefore prefer the former approach of using a single reasoning engine that supports different forms of reasoning.\nGiven this, we have the following design requirements for VERUS-LM:\n1. Versatile: it should support multiple forms of reasoning (e.g., verification, explanation, optimisation, etc.)\n2. Knowledge reuse: it should distinguish between \u201cdomain knowledge\" and \"task to be performed\", allowing the domain knowledge to be reused for different tasks.\n3. Generic: all aspects of the tool, including the prompting method, should be independent of the type of reasoning tasks or the kind of domain that is being considered."}, {"title": "3.2 Reasoning Engine", "content": "While the aforementioned requirements pertain to the general design of our system, they also constrain our selection of reasoning engine. To serve as the main reasoning core of VERUS-LM, we have selected the IDP-Z3 system [Carbonnelle et al., 2023]. IDP-Z3 is a reasoning engine for FO(\u00b7), a rich extension of classical First-Order Logic with useful features such as types, aggregates, (inductive) definitions, and more. It explicitly supports the Knowledge Base Paradigm [Denecker and Vennekens, 2008], in which the same domain knowledge can be used for many different reasoning tasks.\nWe will briefly illustrate FO(\u00b7) and the forms of reasoning supported by IDP-Z3 through a small example, referring to [Carbonnelle et al., 2023] for a more extensive overview. As in normal FOL, a vocabulary in FO(\u00b7) consists of a set of symbols. Because FO(\u00b7) is a typed logic, this includes types, in addition to predicate and function symbols. For instance, in the below example about car insurance, we use types Customer and Car. The function risk factor maps each Car to its associated risk factor (\u2208 R), while the function age maps each Applicant to their age (\u2208 Z). A number of constants (= 0-ary functions) represent the car's value, its type, and the insurance premium. Finally, a unary predicate applicant represents which of the customers is requesting the insurance. Again following FOL, a structure for a vocabulary provides an interpretation for some (not necessarily all) of the symbols in this vocabulary. Finally, a theory consists of a set of logical sentences. In this simple example, we have two sentences: one states that every applicant must be an adult, and one defines the calculation of the insurance premium."}, {"title": "3.3 VERUS-LM Architecture", "content": "The VERUS-LM framework integrates natural language processing and formal reasoning through a structured, two-phase process, as depicted in Figure 1. The first phase, Knowledge Base Creation, uses an LLM to translate domain knowledge into a symbolic FO(\u00b7) specification. Note that, in this phase, we do not yet consider specific questions and instead create a reusable KB. Questions are only considered in the second phase, the Inference phase, where they are interpreted and then answered by means of an appropriate call to one of IDP-Z3's reasoning tasks. In other words, when given multiple questions about the same domain it is only formalized once, greatly reducing the computational cost."}, {"title": "4 Knowledge Base Creation", "content": "The first phase of the VERUS-LM framework consists of transforming domain knowledge into a formal KB, inspired by LLM's capability of translating natural language to structured formats like equations [He-Yueya et al., 2023] or Python code [Gao et al., 2023]. This translation is performed in three steps, which we will discuss in the following sections."}, {"title": "4.1 Symbol Extraction", "content": "The LLM first identifies the relevant types, functions, and predicate symbols for the problem domain. We use a prompt that describes these three different kinds of symbols and how they are typically used. Important here is that we do not include examples from a specific dataset. Instead, our prompt just provides general instructions on how to identify and categorize the different concepts, and provides three general examples as an illustration. The prompt also contains the instruction to annotate each symbol with its informal language meaning, so that this information is present in the next phases of the pipeline.\nOne limitation of IDP-Z3 is that it does not support reasoning under the Open World Assumption, which is necessary for the FOLIO and Proofwriter datasets. Instead, IDP-Z3 always makes the Closed Word Assumption (CWA). In particular, it requires that the extension of all types (apart from built-in types R and Z) is always fully enumerated. As a work-around, VERUS-LM simulates OWA reasoning by introducing an additional \"unknown\" domain element into each type. Even though this is not theoretically correct, it persistently yields the correct results in practice, as shown in the experiments described further on."}, {"title": "4.2 Formula Extraction", "content": "During the formula extraction step, an LLM generates an FO(\u00b7) theory that corresponds to the description of the problem domain. To guide this generation process, a brief overview of the FO(\u00b7) grammar and clear instructions are presented, supported by two illustrative examples. These examples adopt a step-by-step approach, pairing each logical sentence with its corresponding natural language meaning for clarity. Additionally, the LLM is instructed to also consider implicit and commonsense knowledge, demonstrated in the examples. At the end of the prompt we add the previously generated vocabulary, along with the natural language description of the problem to formalize."}, {"title": "4.3 Self-Refinement", "content": "Although in our experience the LLM often produces correct logical sentences, errors still occur. To correct these, we introduce a self-refinement step that uses feedback from the reasoning engine to correct erroneous logical statements. This approach is inspired by recent work on imperative programming [Chen et al., 2023; Madaan et al., 2024] and the Logic-LM framework [Pan et al., 2023]. However, whereas these approaches only consider syntactic correctness, we introduce a novel second step to take semantic correctness into account as well, by means of a satisfiability check. These refinements are part of an iterative process, where VERUS-LM will repeat them until no errors are found or a maximum of attempts is reached.\nSyntax Refinement If the reasoning engine finds a syntax error, the LLM is instructed to correct its output, i.e., either the vocabulary and/or the theory it produces. To this end, the LLM is prompted with its previous erroneous output, the solver's error message, and some examples of common syntax errors and their remedies. Detailed error descriptions are provided by FOLINT [Vermeulen et al., 2022], IDP-Z3's code analysis tool.\nSemantic Refinement Once the KB is syntactically correct, we use a satisfiability check to find additional semantic errors. It is reasonable to assume that the KB should be satisfiable, because, at this point, it contains only a formalization of the provided domain knowledge. For instance, if we were solving a planning problem, the KB would contain a description of the actions that can be executed, but we would not yet include a specific goal that the plan should achieve. If this KB is already inconsistent, then we take this as a sign that the LLM made a mistake in translating the domain knowledge and we instruct it to refine its previous output, based on an explanation of why this was unsatisfiable. IDP-Z3 can give such an explanation itself, by generating a \u201cminimal unsatisfiable subset\" of conflicting assignments and constraints [Carbonnelle et al., 2023]."}, {"title": "5 Inference", "content": "Once the KB is constructed, the inference phase attempts to correctly answer the question. To this end, VERUS-LM follows a series of steps, as illustrated on right side of Figure 1. Given a question, the framework starts by identifying the intended reasoning task and extracting the relevant information, which are passed on to the reasoning engine. Once the latter has finished, its output is formatted back to natural language."}, {"title": "5.1 Detect Reasoning Task", "content": "First, VERUS-LM classifies the user's question into one of the following eight forms of reasoning.\n1. Model Generation/Expansion: generate n logical models of the theory T (i.e., structures S such that S \\models T). Possibly, the interpretation oS that some of the symbols o should have in this model S is already given (e.g., we might provide the value for constants car_value and car_type and ask the engine to fill in the interpretation for the remaining symbols).\n2. Satisfiability: verify if at least one such model S exists for a given theory T.\n3. Optimization: find the model S of a given T in which a given term t reaches its minimal/maximal value tS.\n4. Propagation: determine which atomic formulas are true / false in all models S of the given theory T.\n5. Explain: explain why a given atomic formula is true / false in all models of T, or, in T has no models, then explain the inconsistency.\n6. Determine Range: determine the range of possible values for a given function term f(t) given a theory T, i.e., the set of all values v such that there exists at least one model S of T in which fS(t) = v.\n7. Relevance: determine which symbols o are relevant, in the sense that there exists a model S of T such that S would no longer be a model if the value of o in S were different.\n8. Logical entailment: verify whether a given statement \\phi is logically entailed by the theory T."}, {"title": "5.2 Information Extraction", "content": "Several of the forms of reasoning listed above require some additional information, in addition to a theory T. We distinguish two cases.\nExtracting data and goal detection Typically, in addition to a theory T, an interpretation is given for at least some of the symbols of T. For instance, the problem statement may be: \"Given that Ann is 16 and Barbara is 32, who is eligible for insurance?\" Such information should be extracted and included as part of a structure on which the reasoning is performed (in this case, ages = {Ann \u2192 16, Barbara \u2192 32}). In addition, some other reasoning tasks also require additional information of similar complexity, e.g., optimization requires that we know which term t should be minimized / maximized.\nThis information extraction task is made significantly easier by the fact that, at this point, we already know the vocabulary. We can therefore use language model grammars, such as llama.cpp GNBF grammar [Gerganov, 2023], to force the output to adhere to an automatically generated KB-specific grammar, ensuring that only correctly typed interpretations are generated. As we will show, this pruning of the output space allows us to achieves the same performance for this task with a small language model (SLM) as with a larger LLM. Since such a SLM can run on a standard laptop, this may significantly reduce computational costs, as well as enhance data security, by avoiding the transfer of potentially sensitive information (e.g., the personal data of customers who apply for insurance) to external servers.\nConstruct complex formulas In some case, such as for the logical entailment reasoning task, more complex additional information is required at inference time (e.g., the formula \\phi for which it should be checked whether T \\models \\phi). Constructing a complex formula \\phi from the natural language question is of similar complexity as constructing the knowledge base in the first phase. Therefore, SLMs are ill-suited for this task, and a larger LLM should be used instead. Similarly to the KB creation step, this LLM not only generates formulas but may also add new symbols to the vocabulary.\nWe currently do not attempt to automatically detect whether a specific question contains complex formulas or not, and thus requires an LLM, beacuse in different applications,"}, {"title": "6 Experiments", "content": "6.1 Validation\nWe first conduct a basic validation of VERUS-LM by checking (1) whether our use of a reasoning engine indeed leads to better perfomance on logical reasoning tasks than a standalone LLMs and (2) that the VERUS-LM pipeline is indeed able to correctly identify our eight different reasoning tasks from natural language text. Since existing datasets typically cover only one or at most a few reasoning tasks, we constructed our own dataset called DivLR in which all eight reasoning tasks are present. It consists of 115 questions covering six domains: investment Strategies, COVID restrictions, water Irrigation needs, a Handyman and two variants of the Body Mass Index (BMI) domain. The reason for having two BMI variants is that information about BMI (i.e., how to calculate it, the different risk levels associated to BMI values, etc.) probably occurs in LLM training data. In addition to our BMI domain which contains the well-known definition, we also defined a B* domain in which novel formulas and ranges for BMI were made up.\nWe compare VERUS-LM to baseline language models on this dataset. We compare with both an SLM and an LLM, and consider two versions of our pipeline: one in which our information extraction is done by an SLM (V-SLM) and one in which this is done by an LLM (V-SLM). the LLM is Gemini 1.5 Pro [Team Gemini et al., 2023] and the SLM is a quantised version of Phi 3.5 [Abdin et al., 2024], a 3.8-billion-parameter language model whose inference cost is about 10,000 times lower than large models like GPT-4. All experiments were performed on a Mac M2 Pro CPU with 32GB of RAM.\nTable 1 shows that both V-SLM and V-LLM consistently outperform both the SLM and the LLM on their own. However, V-SLM performs significantly worse on the two BMI domains than on the other domain, and also significantly worse that V-LLM on these domains. A deeper analysis shows that the SLM typically refuses to simply extract the information that the reasoning engine needs, and instead tries to perform BMI-calculations itself. We conjecture that this is due to the BMI-related examples that are likely part of the data on which the SLM was trained.\nA similar but less striking issue can be seen when comparing the results of the LLM on both the B and B* domain: the observed drop in accuracy (66.7\u219250) can be explained by the fact that the LLM has memorised the correct definition of BMI from its training data, and that is unable reason with a different definition, when one is explicitly provided. This suggests that, more generally, if an LLM at some point learns out-dated facts, it is hard to correct these in the prompt, without retraining the network. We see that same issue is still somewhat observable in V-LLM (88.9 83.3), but here the reasoning engine pipeline substantially reduces the effect.\nTable 2 confirms that our BERT classifier identifies the correct logical reasoning task with an average accuracy of 92.6%, performing consistently well on all tasks (with a small dip for Propagation). For the subsequent information extraction, we see that, for most of the reasoning tasks, V-LLM and S-SLM perform about equally well. The biggest difference is seen for Entailement. This is the only reasoning task in our DivLR dataset for which complex formulas need to be constructed at inference time and V-SLM predictably struggles with this (accuracy 29.6%). For the Propagation task, we also observe a difference in accuracy (-25.6%) between V-SLM and V-LLM. This is primarily due V-SLM struggling with the BMI domain, as already discussed.\nOverall, we conclude that the proposed pipeline makes sense: (1) introducing a logical reasoning engine into the pipeline indeed leads to better performance then using only a language model; (2) different logical reasoning tasks can indeed by successfully identified by a simple BERT classifier; (3) all of the necessary information for the reasoning tasks can indeed be extracted with a small language model, as long as no complex formulas need to be constructed, and with the caveat that an SLM might be less robust when handling concepts that already occurred frequently in its training data.\nIn our following experiments, we have always used V-SLM for benchmark datasets that do not require the construction of complex formulas at inference time and V-LLM otherwise."}, {"title": "6.2 Comparison to state-of-the-art", "content": "Having established that the VERUS-LM pipeline makes sense, we now compare it to the state-of-the-art on a number of standard benchmarks, summarized in Table 3. PrOntoQA, ProofWriter, FOLIO, and LogicalDeduction each target a specific reasoning task, while the challenging AR-LSAT dataset spans a broader range of reasoning tasks.\nWe compare VERUS-LM against three LLM-based approaches and two neurosymbolic approaches for logical reasoning: (1) GPT-4 answering directly, (2) GPT-4 with Chain-of-Thought, (3) Linc [Olausson et al., 2023] (only FOLIO and ProofWriter), (4) Logic-LM [Pan et al., 2023] and (5) SymbCot [Xu et al., 2024].\nAs shown in Table 4, all systems achieve a roughly similar performance on the first four benchmarks, with VERUS-LM scoring at most 5% worse than the best system on each. For the challenging AR-LSAT benchmark, VERUS-LM significantly outperforms all other methods, doing about 25% better than the second best system. We believe that this is due to (1) the generality of our approach, which does not use task-specific examples for in-context learning; (2) FO(\u00b7)'s expressiveness which allows for, e.g., straightforward representations of aggregates, such as \"At least three CDs are on sale\": \\#{c \u2208 CD : on\\_sale(c)} \u2265 3; (3) our support for different forms or reasoning, four of which were detected by the classifier and executed, as shown in Table 5."}, {"title": "6.3 Effect of Self-Refinement", "content": "Table 6 presents the results for each dataset of: (1) VERUS-LM without performing any refinements after the initial KB creation; (2) VERUS-LM with only syntactic refinement; (3) complete VERUS-LM with also semantic refinements. In each case, we report the execution rate, which is the percentage of cases in which the knowledge base was syntactically correct and satisfiable. We also report the execution accuracy, which is the percentage of the those for which the reasoning engine returned a correct result. We also report the product of the two as \"total accuracy\".\nThe syntactic refinement increases the execution rate by 11.2% on average. The execution accuracy stays roughly the same, showing that KBs that had to be syntactically corrected are about as likely to correctly represent the domain knowledge as KBs that were initially already syntactically correct. When we then also include our semantic refinement, there is an additional increase in execution rate of on average 10%. This concerns cases in which the original KB was unsatisfiable, e.g., because of contradictory formulas or mistakes in the typing of a function or constant. Such errors typically arise only when the natural language description is difficult to understand, which also makes them hard to rectify in a correct way, as evidenced by the drop in execution accuracy when adding the semantic refinement. However, the total accuracy still markedly increases, showing that the semantic refinement step is indeed useful."}, {"title": "7 Limitations", "content": "Though VERUS-LM expands on the state of the art, it faces some limitations. Certain tasks are better suited to imperative programming paradigms rather than declarative approaches. Additionally, the expressiveness of FO(\u00b7) has limitations, making it among others unsuitable for representing higher-order logic. Due to VERUS-LM's relience on a logical reasoning engine, it also inherits some of its limitations, such as SAT problems in very large domains. Furthermore, though the KB-creation of VERUS-LM is based on a generic prompting pipeline, its accuracy in entirely different logical paradigms remains a uncertain."}, {"title": "8 Conclusion", "content": "This paper introduced VERUS-LM, a versatile neurosymbolic framework that integrates language models with symbolic reasoning capabilities. Its two-phased approach separates Knowledge Base creation from a separate inference phase, in which multiple different reasoning tasks can be performed on the same knowledge base. This approach has the inherent advantage that multiple questions about the same domain can be answered in a computationally efficient way, by reusing the same knowledge base. Another improvement made by VERUS-LM is that we extend the state-of-the-art syntactic refinement step with a semantic refinement step, based on a satisfiability check. Our experimental analysis shows that this indeed improves results. VERUS-LM significantly outperforms the state-of-the-art on the challenging and diverse AR-LSAT dataset, wile remaining competitive on simpler benchmarks."}]}