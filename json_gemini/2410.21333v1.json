{"title": "MIND YOUR STEP (BY STEP): CHAIN-OF-THOUGHT\nCAN REDUCE PERFORMANCE ON TASKS WHERE\nTHINKING MAKES HUMANS WORSE", "authors": ["Ryan Liu", "Jiayi Geng", "Addison J. Wu", "Ilia Sucholutsky", "Tania Lombrozo", "Thomas L. Griffiths"], "abstract": "Chain-of-thought (CoT) prompting has become a widely used strategy for work-\ning with large language and multimodal models. While CoT has been shown to\nimprove performance across many tasks, determining the settings in which it is\neffective remains an ongoing effort. In particular, it is still an open question in\nwhat settings CoT systematically reduces model performance. In this paper, we\nseek to identify the characteristics of tasks where CoT reduces performance by\ndrawing inspiration from cognitive psychology, looking at cases where (i) verbal\nthinking or deliberation hurts performance in humans, and (ii) the constraints gov-\nerning human performance generalize to language models. Three such cases are\nimplicit statistical learning, visual recognition, and classifying with patterns con-\ntaining exceptions. In extensive experiments across all three settings, we find that\na diverse collection of state-of-the-art models exhibit significant drop-offs in per-\nformance (e.g., up to 36.3% absolute accuracy for OpenAI 01-preview compared\nto GPT-40) when using inference-time reasoning compared to zero-shot counter-\nparts. We also identify three tasks that satisfy condition (i) but not (ii), and find\nthat while verbal thinking reduces human performance in these tasks, CoT re-\ntains or increases model performance. Overall, our results show that while there\nis not an exact parallel between the cognitive processes of models and those of\nhumans, considering cases where thinking has negative consequences for human\nperformance can help us identify settings where it negatively impacts models. By\nconnecting the literature on human deliberation with evaluations of CoT, we offer\na new tool that can be used in understanding the impact of prompt choices and\ninference-time reasoning.", "sections": [{"title": "1 INTRODUCTION", "content": "Chain-of-thought (Wei et al., 2022; Nye et al., 2021) is a widely used prompting technique for large\nlanguage and multimodal models (LLMs and LMMs), instructing models to \"think step-by-step\" or\nproviding other structure that should be incorporated into their response. Large meta-studies have\nshown that this technique improves the performance of models on many tasks, particularly those\ninvolving symbolic reasoning (Sprague et al., 2024). More generally, inference-time reasoning has\nbecome a default component of the newest LLMs and LMMs such as OpenAI 01-preview (Ope-\nnAI, 2024a) and Claude's web interface and mobile apps (Anthropic, 2024). However, there also\nexist cases where CoT decreases performance, but there have not been any identified patterns as to\nwhen this happens. With the increasing use of inference-time reasoning in deployed models, it is\nimperative to understand and predict when CoT has a negative effect on model performance.\nA key challenge for determining the limits of CoT is the sheer variety of tasks for which LLMs\nand LMMs are used. While the machine learning community has dedicated great efforts towards\ndeveloping a large set of benchmarks for these models (e.g., Hendrycks et al., 2020; Suzgun et al.,\n2022), applications of models extend beyond benchmarks to diverse contexts and variations of tasks"}, {"title": "2 RELATED WORK", "content": "Chain-of-thought prompting aims to improve the performance of language-based models by encour-\naging them to generate an intervening string of tokens that increases the probability of producing\nthe correct answer (Wei et al., 2022; Nye et al., 2021). This approach can result in significant\nperformance improvements in language (Zhang et al., 2022) and vision (Zhang et al., 2023) tasks,\nhypothesized to be a consequence of exploiting local structure in language (Prystawski et al., 2024).\nHowever, a recent metastudy suggests that the gains from using CoT are primarily in mathemat-\nical and symbolic reasoning tasks, and that it is otherwise unclear when it improves performance\n(Sprague et al., 2024). In related settings, such as planning, there is little benefit from CoT prompt-\ning (Kambhampati et al., 2024). Despite these results, the default expectation seems to be that CoT\nimproves performance. For example, a recent update to a language-understanding benchmark cited\nthe fact that CoT results in an improvement on the new benchmark but decreased performance on\nthe original benchmark as an indicator that the new benchmark is better (Wang et al., 2024). This\nexpectation seems to have driven the tendency towards the default use of CoT in the latest LLMs\nand LMMs."}, {"title": "2.2 PSYCHOLOGICAL METHODS AS A TOOL FOR STUDYING LLMS AND VLMS", "content": "Since the introduction of LLMs, there has been growing interest in understanding the connections\nbetween models and human minds (Hardy et al., 2023). Human cognition is often studied using\nwell-controlled tasks involving carefully curated datasets designed to test specific hypotheses. The\navailability of these datasets, and the fact that they often consist mainly of text and/or images, have\nled to these tasks from the psychology literature quickly becoming popular methods for evaluat-\ning and understanding LLMs and LMMs (e.g., Binz & Schulz, 2023; Coda-Forno et al., 2024).\nFor example, recent studies that leverage insights or datasets from psychology have evaluated the\nrepresentational capacity of LLMs (Frank, 2023), explored how RLHF and CoT lead to different\noutcomes when trying to make models both helpful and honest (Liu et al., 2024a), compared human\nand machine representations via similarity judgments (Peterson et al., 2018; Marjieh et al., 2023a;b;\n2024a), determined that LLMs over-estimate human rationality in decisions (Liu et al., 2024b), iden-\ntified incoherence in LLM probability judgments (Zhu & Griffiths, 2024), identified susceptibility\nto linguistic illusions in LLMs (Marjieh et al., 2024b), uncovered LLMs' underlying social biases\n(Bai et al., 2024), used storytelling to understand episodic memory in LLMs (Cornell et al., 2023),\nconstructed prompts using psychological theories of metaphor (Prystawski et al., 2022), discovered\ncross-linguistic variability in LLM representations (Niedermann et al., 2023), and probed the roles\nof language and vision for in-context learning in VLMs (Chen et al., 2024). Many of these stud-\nies start with a well-studied phenomenon in human cognition and then explore whether there is an\nisomorphic analog to it in LLMs or LMMs. Our work follows this established line of literature by\nassociating the well-studied impact of deliberation on human performance to effects that occur in\nmodels using CoT."}, {"title": "3 APPROACH: WHEN THINKING REDUCES HUMAN PERFORMANCE", "content": "A large body of psychological research has investigated effects of verbal thinking (often explicit\n\"deliberation\") on memory, learning, judgment, and decision-making. Very often these effects are\npositive. For example, people who spend more time deliberating are more likely to respond correctly\non questions that initially trigger an intuitive but incorrect response (Travers et al., 2016). However,\nthere are also cases in which deliberation can impair performance, often involving a mismatch be-\ntween the representations or types of processing induced by deliberation and those that best support\ntask performance (Schooler, 2002).\nA classic setting for such effects is in the domain of implicit statistical learning. For example, in stud-\nies of artificial grammar learning participants are presented with sequences of letters or phonemes\nthat conform to some structure (such as a finite state grammar) and asked to recognize well-formed\nsequences. Studies often find that participants can differentiate well-formed sequences from those\nthat are not well-formed, but cannot verbalize the basis for their judgments (Aslin & Newport, 2012;\nRomberg & Saffran, 2010). Some (but not all) studies further find that receiving explicit instructions\nto identify rules in verbal form impairs performance (Reber, 1976).\nAnother class of cases concerns a phenomenon termed verbal overshadowing. In a classic demon-\nstration, instructions to verbalize a face led to impaired facial recognition relative to a condition in\nwhich participants did not verbalize (Schooler & Engstler-Schooler, 1990). Such effects have been\nfound for other perceptual stimuli (Fiore & Schooler, 2002; Melcher & Schooler, 1996), but do\nnot extend to stimuli that are easy to verbalize (such as a spoken statement) (Schooler & Engstler-\nSchooler, 1990) or to logical problem solving (Schooler et al., 1993).\nAs a third example, studies find that asking people to generate verbal explanations for their observa-\ntions supports the discovery of broad and simple patterns (Edwards et al., 2019; Walker et al., 2017;\nWilliams & Lombrozo, 2010; 2013). But when the stimuli are designed such that these broad and\nsimple patterns contain exceptions, participants who were prompted to explain learned more slowly\nand made more errors (Williams et al., 2013). These effects are thought to arise from the mismatch\nbetween the representations or processes induced by a form of thinking (in this case, explaining)\nand the representations or processes that best support task performance (Lombrozo, 2016).\nThe effects reviewed so far plausibly concern impairments that arise from the representational limi-\ntations of language and the generalization of patterns found in language: language is not well-suited\nto encoding fine-grained perceptual discriminations (as required for face recognition), and language\nreadily encodes some kinds of relationships (such as deductive entailment, or simple and broad pat-\nterns) but is less well-suited or frequently employed for others (such as complex finite state gram-\nmars, or patterns with arbitrary exceptions). Given that LLMs are likely to share limitations that"}, {"title": "4 EXPERIMENTS", "content": "Following the different failure settings of human deliberation described in Section 3, we select six\ntasks, each representative of a class of failure settings from the psychological literature, and conduct\nan experiment to test the effect of CoT on LLMs and LMMs. Based on the considerations outlined\nat the end of Section 3, we identify two criteria under which we anticipate a task might result in CoT\nreducing performance:\n(i) Verbal thinking or deliberation hurts performance in humans.\n(ii) The constraints governing human performance are the same as those governing AI models.\nWe use these criteria to develop a hypothesis for each task. We first cover three cases where both (i)\nand (ii) are satisfied, before discussing three cases where (ii) is not satisfied. In each, we scale up the\nclassic psychology study that tested humans, while also adapting the task towards modern use-cases\nof large language or multimodal models."}, {"title": "4.1 IMPLICIT STATISTICAL LEARNING", "content": "Task. The first class of tasks we examine are those involving implicit statistical learning. As\ndescribed in Section 3, psychology studies have found that data that contain statistical patterns can\nbe better generalized by humans when those patterns are not linguistically described. We explore this\nfor LLMs by replicating the task of learning artificial grammars (Reber & Lewis, 1977; Whittlesea\n& Dorken, 1993; Van den Bos & Poletiek, 2008). In the task, artificial \u201cwords\" are constructed using\nfinite-state grammars (FSGs) and participants are tasked with identifying which words belong to the\nsame category (i.e., are generated by the same FSG). In total, we constructed 4400 classification\nproblems corresponding to 100 randomly sampled unique FSGs that were structurally similar to\nthose used to test humans in Fallshore & Schooler (1993). Each classification problem consisted\nof 15 training examples generated from the grammar, and the model was given a new example and\nasked to classify it. Models were asked to classify 44 words per FSG, where 22 words belonged to\nthe FSG and 22 did not. Words not belonging to the grammar were generated by replacing one letter\nfrom an existing word in the grammar. Details on problem generation are provided in Appendix A.1.\nHypothesis. The artificial grammar learning task satisfies (i) based on the findings of Fallshore &\nSchooler (1993), where humans who conducted verbal thinking did worse on the problem. The task\nalso satisfies (ii): verbal reasoning is thought to impair human performance due to the constraints of\nusing explicit language-based processing, which is something shared by humans and LLMs using\nCoT. Thus, following our hypothesis, we predict that CoT will reduce LLM performance on the\nartificial grammar learning task.\nModels and prompts. We evaluate this task on several open- and closed-source models: OpenAI\n01-preview, GPT-40, Claude 3.5 Sonnet, Claude 3 Opus, Gemini 1.5 Pro, Llama 3.1 70B & 8B\nInstruct, and Llama 3 70B & 8B Instruct. We considered two prompts, zero-shot and CoT (see\nAppendix A.2)."}, {"title": "4.2 FACIAL RECOGNITION", "content": "Task. Another class of tasks we identify in Section 3 where verbal thinking reduces performance\ninvolves verbal overshadowing, where verbal thinking affects interactions with perceptual stimuli.\nWe study this case using a classic human face recognition task, where participants are first shown\na face of a person and then asked to select an image of the same person out of a list of candi-\ndates (Schooler & Engstler-Schooler, 1990). While psychological studies often include a distractor\ntask between the initial person and the candidates to increase the difficulty, we did not use these for\nLMMs due to their weak performance. We scale this task from one recognition problem to a novel\nsynthetic dataset of 500 problems across 2500 unique faces. For each problem, all faces were given\nthe same described attributes for seven features: race, gender, age group, eye color, hair length, hair\ncolor, and hair type. We then generated a pair of images of the same person and four images of\nother people matching this description using stable-image-ultra (StabilityAI, 2024). We adjusted\nthe generation process to ensure that the pair clearly consisted of the same person, while the others\nclearly did not (see Appendix B.1 for further details). One of the pair was selected to be the initial\nstimulus, while the other was shuffled with the four images of other people to create five candidate\nanswers. Models were prompted to identify which candidate matched the person from the initial\nstimulus.\nHypothesis. The facial recognition task satisfies (i), as people who verbally reasoned about faces\nperformed worse in Schooler & Engstler-Schooler (1990). (ii) is satisfied as human failure is at-\ntributed to the lack of granularity and inadequacy of language in describing the visual stimuli, a\nconstraint relating to verbal thinking rather than people. Following our hypothesis, we predict that\nCoT will reduce performance on our facial recognition task in LMMs.\nModels and prompts. We evaluated this task on several open- and closed-source state-of-the-\nart LMMs: GPT-40, Claude 3.5 Sonnet, Claude 3 Opus, Gemini 1.5 Pro, InternVL2-26B, and\nIntern VL2-Llama3-76B. Other models such as Llama 3.2 90B Vision and Molmo-72B were not\nconsidered as they do not support multiple image input. We considered two prompts, zero-shot and\nCoT, available in Appendix \u0412.2."}, {"title": "4.3 CLASSIFYING DATA WITH PATTERNS THAT CONTAIN EXCEPTIONS", "content": "Task. A third class of tasks we identify where CoT may harm performance is learning to clas-\nsify exemplars when there are exceptions to generalizable rules. As mentioned in Section 3, when\nhumans try to explain the category membership of exemplars, they tend to hypothesize simple clas-\nsification rules, which can lead to inefficient learning when data contain arbitrary exceptions to these\nrules.\nTo study if this phenomenon extends to CoT, we replicate a multi-turn vehicle classification task\nfrom Williams et al. (2013), where participants try to correctly assign binary labels to a list of\nvehicles. Participants are given feedback after each prediction, and conduct multiple passes over\nthe list until they label all vehicles correctly in a single pass or exceed the maximum number of\ntries. Vehicles in the task contained one feature that was almost fully correlated (80%) with the\nclassification label, three features with no relation to the label, and one feature (the unique color) that\nindividually identified the vehicle. Thus, participants could either try to learn the generalizable rule\nfrom the highly correlated feature and fail due to the exceptions, or they could learn the individual\nmappings from the identifying feature to the corresponding label. Human participants who were\nprompted to explain the classification of exemplars performed worse because they tended to attempt\nthe former strategy.\nParticipants in the original study performed this explanation after receiving feedback. To more\nexplicitly include inference-time reasoning, we modify the point at which verbal thinking is per-\nformed, asking the LLM to perform CoT before making each prediction instead. 10 problems each,\nand measur0 vehicles split into 240elidLLMs' abivehic t learn, he labels of the the stimuli for up to\n15 iterations (eachlistspasse(see Appendix C.1 for details). Memory was implemented by including\nprevious problems, guesses, and feedback in context.\nHypothesis. This learning with exceptions task fulfills (i) as people tended to reason about gener-\nalizable rules when conducting verbal thinking, which increased the amount of time needed to learn\nthe labels for the entire list (Williams et al., 2013). (ii) is satisfied as we expect models to replicate\nthe tendency that humans have to focus on generalizable features when performing such thinking.\nModels and prompts. We evaluated this task on GPT-40, Claude 3.5 Sonnet, and Claude 3 Opus.\nWe only evaluated these models as the task requires long-context conversation capabilities, which\nother models such as Llama 3.1 70B Instruct were not sufficiently good at. We varied the prompt\nbetween direct and CoT, where the former corresponded to directly asking the model to classify with\nthe memory in context (see Appendix C.2 for details)."}, {"title": "4.4 TASKS WITH A MISMATCH BETWEEN HUMAN AND MODEL ABILITIES", "content": "In the next three tasks, we consider cases where (i) is satisfied but (ii) is not. In other words,\nhumans perform worse on the task after deliberation or verbal thinking, but we do not expect this to\ngeneralize to model CoT due to differences between humans and models. Reasons for this include\nmodels producing poor performance with zero-shot prompting \u2013 providing no opportunity for a\ndecrease in performance, or humans and models possessing different limitations for task-relevant\nabilities, such as access to different kinds of information or memory resources.\nExplaining a logical inconsistency. Studies have found that when human participants are shown a\npair of logically inconsistent statements and asked to explain their coexistence, they become worse\nat judging whether the statements are indeed logically inconsistent (Khemlani & Johnson-Laird,\n2012). In the task, participants are provided with two sentences following the template: \u201cIf A then\nit is always the case that B", "A, but it is not the case that B\u201d or \u201cIt is not the case that\nB": "The former corresponds to a logical inconsistency between the statements, while the latter does\nnot. In one condition humans were first asked to explain why an inconsistent pair could coexist\nbefore providing a judgement on their inconsistency, while in another they conducted the same\nexplanation after providing a judgement. The former condition saw its classification performance\ndrop significantly compared to the latter, indicating that deliberation towards the incorrect class\nreduced human performance.\nThe original human experiment contained 12 unique {A, B} pairs. To scale this task to evaluate\nLLMs, we leverage existing entailment pairs in natural language inference tasks, which we use to\nfill in A and B to form the sentences. We used a combination of three datasets: The Stanford Natural\nLanguage Inference (SNLI) dataset, the Multi-Genre Natural Language Inference (MNLI) dataset,\nand a synthetic LLM-generated dataset of 100 entailment pairs. We filtered the datasets for pairs\nwhich were labeled \u201centailment\" (i.e., A entails B). In addition, we limit the maximum length of\nA and B such that the template forms coherent sentences. In total, we evaluate on 1608 pairs of\n{A, B} pairs: 675 from SNLI, 833 from MNLI, and 100 synthetic. Each pair was used to construct\ntwo classification problems, one consistent and one inconsistent, for a total of 3216 problems which\nwe use to evaluate LLMs. For more details on problem generation see Appendix D.1.\nWe evaluated a suite of state-of-the-art LLMs on this task: OpenAI ol-preview (on a subset of 30\nsynthetic questions), GPT-40, Claude 3.5 Sonnet, Claude 3 Opus, Gemini 1.5 Pro, and Llama 3.1\n70B Instruct. We used zero-shot prompting and two conditions of CoT: one where the model is\nsimply asked to reason before answering, and another that follows the original experiment by asking\nthe model to explain the inconsistency directly (see Appendix D.2 for details). Results were very\nsimilar across the two CoT conditions, so we report an average over both.\nZero-shot prompting resulted in poor performance on this task, with most models performing close\nto chance (see Table 4). CoT often improved this performance, attributable to both the low base\nperformance and the logical reasoning component for which CoT is typically helpful. This was"}, {"title": "Spatial intuitions.", "content": "Psychologists have documented cases involving spatial reasoning in which\nhumans generate more accurate responses after visual or motor simulation compared to verbal rea-\nsoning. To investigate whether this applies to models, we replicate a cup-tilting task from Schwartz\n& Black (1999). In the task, participants are shown an image of two rectangles with varying height\nand width, representing two cups one empty and one that contains some water. Participants are\nthen asked to estimate the water that should be added to the empty cup so that when tilting both cups,\nwater will reach the rim at the same angle (see Figure 1, SI). While the original task had participants\ndraw the water level on the empty cup, LMMs were unable to do this consistently. Thus, we turned\nthe task into a multiple choice question by adding markings A \u2013 D to the side of the empty cup and\nasking the model to choose one. Incorrect answer options were generated by adding Gaussian noise\nto the correct answer while satisfying the constraint that answer options must be a certain distance\napart. We scale up this task by varying the dimensions of cup sizes and water height, creating a total\nof 100 problems, each with a code-drawn image containing the cups and multiple choice answers\n(see Appendix E.1 for details). We evaluated with zero-shot and CoT prompts on several open- and\nclosed-source large multimodal models: GPT-40, Claude 3.5 Sonnet, Claude 3 Opus, Gemini 1.5\nPro, and InternVL2 Llama3 76B.\nIn this setting, it is unlikely that large multimodal models would share the same motor simulation\ncapabilities as humans due to lack of representations built from motor experience. As the improved\nperformance in the non-verbal thinking condition requires spatial or motor intuition, (ii) is not satis-\nfied. Consistent with this, we did not observe statistically significant differences between zero-shot\nand CoT prompts (see Table 5). Generally, we expect this to extend to other tasks where models\nlack task-relevant priors that humans possess."}, {"title": "Aggregating features for a decision.", "content": "The final category of tasks we consider are complex, multi-\ndimensional tasks that exceed human working memory capacity. A study conducted by Dijksterhuis\n(2004) showed that humans made poor choices when deliberating over apartment options when pro-\nvided with a large amount of information about various decision features. In the study, participants\nwere shown 48 statements for one second each, where the statements described either a positive,\nnegative, or neutral aspect of one of four apartment choices. Afterwards, they were asked to select"}, {"title": "5 DISCUSSION", "content": "Chain-of-thought prompting is an effective way to expand the capacities of large language and mul-\ntimodal models. However, knowing that CoT significantly decreases performance in specific set-\ntings is important in considering when it should be deployed, and in particular whether it should\nbe deployed by default. By using cases where verbal thinking decreases human performance as a\nheuristic, we successfully identify three such settings where CoT results in large decreases in model\nperformance, contributing towards discourse regarding both these discussions.\nWhile we make a connection between human cognition and large language and multimodal models,\nwe do not claim that these systems operate in the same way or that models should be anthropomor-\nphized. Rather, we see this connection as a tool for identifying settings where either the structure\nof the task or shared limitations result in negative effects of verbal thinking for both humans and\nmodels. Our exploration was guided by considering not just (i) that deliberation reduces human per-\nformance but also (ii) that there are no other constraints at work that result in a meaningful difference\nbetween humans and models.\nAcross experiments on six diverse tasks, we find overwhelming evidence that CoT results in a de-\ncrease in performance when both (i) and (ii) are satisfied, but not when (ii) is not satisfied. ults\nsuggest that we can successfully leverage the existing literature in cognitive psychology on deliber-\nation and thinking to find cases that are informative about the performance of CoT.\nIn the remainder of this section, we consider the limitations of this work and some future directions."}, {"title": "5.1 LIMITATIONS", "content": "Types of inference-time reasoning. Since the invention of chain-of-thought prompting, re-\nsearchers have developed various prompts that are specifically suited to target application domains,\nas well as more elaborate general-purpose prompts with multiple forward passes such as tree-of-\nthought (Yao et al., 2024) and self-consistency (Wang et al., 2023). We test the effectiveness of\ntree-of-thought on GPT-40 for the implicit statistical learning task (see Appendix A.3), and find that\nwhile it improves classification accuracy (64.55% vs. 62.52%), this is still far from the zero-shot\nperformance of 94.00%, suggesting that our approach extends across inference-time reasoning tech-\nniques. However, future work is still required to determine whether this generalizes to other task\ndomains and methods of eliciting verbal thinking in models.\nScope of application. While our psychology-based heuristic offers a strategy for identifying fail-\nure cases of CoT, it is unlikely to cover all cases where CoT decreases performance. Existing psy-\nchological research has been guided by a variety of theoretical and practical considerations towards\nstudying humans, but this does not offer an exhaustive or representative sample of all tasks, and will\nmiss cases that are uniquely interesting to study in models but not humans. Thus, we envision our\ncontribution to be complementary to existing evaluation methods in natural language processing.\nAs we've seen across our six sets of experiments, knowledge of what drives the decrease of per-\nformance in humans can be leveraged to generate predictions about the effects of CoT for different\ntasks, but this remains an inferential step that requires careful reasoning and an understanding of\nmodel capabilities. Despite these limitations, our method can be used to identify large and con-\nsequential failures of CoT, as documented in our three failure tasks. It also offers valuable cross-\ndomain insight that can help build intuition and contribute to our overall understanding of inference-\ntime reasoning. On the flipside, the existence of capable LLM/LMM systems also allows us to reflect\nupon the reasons why human performance can be degraded by deliberation. By considering when\nCoT's effects mirror humans and when they do not, we can distinguish when the task or mechanisms\nshared by humans and models are responsible for failures, versus when the issues are with uniquely\nhuman strategies or limitations.\nAlternative explanation for why CoT does not replicate human results. There exists an al-\nternative explanation for why we do not see drops in performance in the latter three tasks - that\nhow we implemented the tasks for LLMs removed the failure effect. It's possible that with different\nimplementations we might in fact see decreased performance, mirroring what we see in humans.\nWhile we explored a large number of variations for the latter three tasks, these explorations were not\nexhaustive due to the endless changes that one could make to the prompts. In other words, because\nthe tasks were inevitably changed to scale up the evaluation and match more realistic applications\nof models, it's also possible that these changes are what explain the human-model mismatch."}, {"title": "5.2 FUTURE DIRECTIONS & BROADER IMPLICATIONS", "content": "While we have focused on CoT reasoning, the framework presented here suggests a more general\nstrategy for leveraging empirical work on humans to characterize the performance of models: jointly\nconsidering (i) psychological results concerning humans and (ii) whether the relevant constraints that\nshape human performance extend to those models. For example, it could be fruitful to investigate\neffects of comparison or analogical prompting (Lombrozo, 2024) through this lens.\nAltogether, we visualize studying how to evaluate and improve models as a collaborative effort be-\ntween natural language processing methods (e.g., benchmarks), psychological insights, and the bur-\ngeoning literature of evaluations that compare human and model performance. By sharing knowl-\nedge and building a strong collaboration between these different disciplines, we can utilize rich\ninsights from decades of literature that studied humans to advance the domain's intuitions about\nthese models and analyze an even broader array of tasks and applications for AI."}, {"title": "A IMPLICIT STATISTICAL LEARNING TASK", "content": "To study cases involving implicit statistical learning, we consider an artificial grammar learning task.\nIn the task, LLMs are provided with letter string train examples that belong to the same category,\nand are tasked to classify whether a new string belongs to the same category."}, {"title": "A.1 GENERATION OF ARTIFICIAL GRAMMAR LEARNING DATASET", "content": "In the original psychology experiments (Fallshore & Schooler, 1993; Reber & Lewis, 1977), partic-\nipants performed the classification task on strings generated by a fixed finite state grammar (FSG)\nconstructed by the researchers (see Figure 2). A string is generated by the FSG if it corresponds to\na valid path along the directed edges from the source node s to the sink node t, where the letters on\nthe path are appended together.\nIn our experiments, we expand the experiments massively to 100 randomly sampled FSGs that\nfollow the same rough structure of those used in the experiment. To scale up the dataset, we construct\nand sample from all possible FSGs that obey the following rules. For a visual representation please\nsee Figure 3.\n\u2022 6 nodes total, including source s, sink t, and four nodes $X1,...,X4$.\n\u2022 Edges (s, x1), (s, x3), (x2, t) and (x4, t) are always present.\n\u2022 Edge (x1,x2) is always present to avoid isomorphisms and the null case where no paths\nexist from s to t.\n\u2022 The remaining middle edges {(x1,x3), (X1,X4), (X2,X1), (X2,X3), (X2,X4), (X3,X1),\n(X3,X2), (X3, X4), (X4,X1), (X4,X2), (X4,X3)} can either exist or not, for a total of 211\ncombinations.\n\u2022 Each xi can have self-loops, e.g., (x1, x1), for a total of 24 combinations.\n\u2022 Letters on each edge are randomly selected from the capital alphabet, for a total of 268\ncombinations.\n\u2022 Each FSG should be able to generate at least 37 unique strings with length < 8.\n\u2022 The construction of the FSG is unique with respect to the three graphical isomorphisms\nthat each FSG satisfying the rules could have.\nFor each FSG, we sampled paths of up to length 8 and used them as stimuli for the experiment.\nFollowing Fallshore & Schooler (1993), we sampled 37 to use in the experiment, assigning 15 to be\ntraining examples and 22 to be positive test examples. We also constructed 22 negative test examples\nby sampling a random string from the FSG, perturbing one letter in a randomly selected position to\nanother letter that exists on some edge of the FSG. We ensured that the negative examples did not\nbelong to the FSG.\nIn total, this yielded 4400 individual questions asked to the large language models. Each question\nwas asked individually after the 15 training examples. See the next section for the specific prompts."}, {"title": "A.2 PROMPTS", "content": "For our experiments, we prompted the models using one zero-shot prompt and one CoT prompt.\nThe zero shot prompt is shown in Table 7."}, {"title": "A.3 TREE-OF-THOUGHT EXPERIMENTS", "content": "To analyze whether our hypotheses about model chain-of-thought extend to other types of inference-\ntime reasoning, we evaluated the performance of GPT-4o with tree-of-thought (ToT) (Yao et al.,\n2024) on a subset of 10 artificial grammars, totaling 440 examples. Given the input prompt, we\nasked the LLM to generate five different thoughts to explain whether the string also followed the\nsame set of rules as the in-context examples. Then, five votes were conducted to select the best\nth"}]}