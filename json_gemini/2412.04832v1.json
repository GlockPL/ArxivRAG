{"title": "WRF-GS: Wireless Radiation Field Reconstruction with 3D Gaussian Splatting", "authors": ["Chaozheng Wen", "Jingwen Tong", "Yingdong Hu", "Zehong Lin", "Jun Zhang"], "abstract": "Wireless channel modeling plays a pivotal role in designing, analyzing, and optimizing wireless communication systems. Nevertheless, developing an effective channel modeling approach has been a longstanding challenge. This issue has been escalated due to the denser network deployment, larger antenna arrays, and wider bandwidth in 5G and beyond networks. To address this challenge, we put forth WRF-GS, a novel framework for channel modeling based on wireless radiation field (WRF) reconstruction using 3D Gaussian splatting. WRF-GS employs 3D Gaussian primitives and neural networks to capture the interactions between the environment and radio signals, enabling efficient WRF reconstruction and visualization of the propagation characteristics. The reconstructed WRF can then be used to synthesize the spatial spectrum for comprehensive wireless channel characterization. Notably, with a small number of measurements, WRF-GS can synthesize new spatial spectra within milliseconds for a given scene, thereby enabling latency-sensitive applications. Experimental results demonstrate that WRF-GS outperforms existing methods for spatial spectrum synthesis, such as ray tracing and other deep-learning approaches. Moreover, WRF-GS achieves superior performance in the channel state information prediction task, surpassing existing methods by a significant margin of more than 2.43 dB.", "sections": [{"title": "I. INTRODUCTION", "content": "Modern communications increasingly depend on wireless technologies that use electromagnetic (EM) waves for information exchange, driving advancements in mobile phones, automotive systems, and Internet-of-Thing devices [1]. At the core of these innovations lies wireless channel modeling, a long-standing problem in wireless communications. While the physics of EM wave propagation is described by Maxwell's equations [2], solving these equations in reality is intricate due to the need for comprehensive knowledge of boundary conditions. This complexity has led to the development of various wireless channel modeling approaches, as shown in Fig. 1, which can be categorized into probabilistic modeling, deterministic modeling, and neural modeling.\nThe probabilistic models rely on statistical methods to predict channel characteristics, primarily estimating the received signal strength based on the distance between the transmitter (TX) and receiver (RX). These models are based on empirical formulas and use measurements to calibrate parameters for typical scenarios [3]. However, they often lack accuracy and struggle to provide detailed channel characteristics. For example, in multi-antenna systems, the probabilistic models fail to characterize the energy distribution of the received signal from all directions, i.e., to reconstruct the spatial spectrum of the received signal by estimating its angle of arrival (AoA). To overcome these drawbacks, the deterministic models use physical principles to predict channel characteristics under an approximate environmental model. For example, the ray tracing method generates propagation characteristics based on computer-aided design representations of the environment, including object boundaries and material reflection coefficients [4]. Therefore, these models can provide more comprehensive channel information than probabilistic models. However, their accuracy may be compromised, as they cannot capture the detailed physical characteristics of the environment.\nIn contrast, the neural models, adopting the data-driven principle, learn the complex interactions between the environment and radio signals directly from location-based data, rather than relying on predefined statistical or physical models. One of the promising advancements in neural models is the recent adoption of the neural radiance field (NeRF), a breakthrough in computer vision for view synthesis [5]. Motivated by the fact that light is a kind of EM wave, Refs. [6] and [7] proposed two NeRF-based frameworks, named NeRF2 and NeWRF, respectively, for wireless channel modeling based on implicit wireless radiation field (WRF) reconstruction. The reconstructed WRF can provide detailed and accurate channel characteristics and significantly enhance communication performance. However, these methods suffer from high computational complexity and slow synthesis (a.k.a. rendering) speeds. For example, NeRF2 typically requires several hours for training and 200 milliseconds for synthesizing a channel characteristic. In practice, the synthesis operation corresponds to the channel characteristic prediction process and will significantly affect the round-trip time in communication systems, which is critical for latency-sensitive applications. For instance, cloud gaming and digital twins typically demand low latency, i.e., under 20 milliseconds for cloud gaming [8] and often within a few milliseconds for digital twins [9]. This renders the NeRF-based methods impractical.\nTo conquer this challenge, we propose WRF-GS, a hybrid framework for fast and accurate channel modeling based on explicit WRF reconstruction using neural networks and 3D Gaussian splatting (3D-GS). 3D-GS utilizes an analytical architecture to represent and render 3D scenes, enabling high-quality and real-time view synthesis [10]. Compared with NeRF, 3D-GS offers faster rendering speeds and lower computational complexity. By adapting 3D-GS from the optical domain to the radio-frequency (RF) domain, WRF-GS can predict the received signal at any location within milliseconds for a give scene after training with a small number of signal measurements. Fig. 2 illustrates the synthesized spatial spectra of four algorithms based on the reconstructed WRF in a laboratory environment when the RX is fixed at one position and the TX is located at four different positions (i.e., P1-P4). Evidently, WRF-GS has the best prediction accuracy compared with the baselines, closely matching the ground truth. This capability makes WRF-GS a powerful tool for fast and accurate wireless channel modeling.\nNevertheless, directly applying the optical 3D-GS technique to the RF domain poses several challenges. First, the optical 3D-GS only considers the amplitude (i.e., light strength) of the optical signal, while the received RF signal consists of both amplitude and phase components, as EM waves are more prone to reflection, diffraction, and scattering than lights. Second, the measurement of visible light is via a million-pixel camera, but an RX typically utilizes either a single antenna or an antenna array. Third, the rendering function in the RF domain differs from the optical 3D-GS due to the unique physics of EM wave propagation. To address these challenges, we propose WRF-GS, a novel framework that refines the optical 3D-GS technique [10] from three key aspects. Scenario Representation Network: We introduce two multi-layer perceptions (MLPs) to effectively capture the complex interactions between the environment and radio signals. The learned features are then embedded into 3D Gaussian points, which serve as virtual TXs in wireless propagation. Projection Model: We devise a projection model to transform the optical camera model into an RF antenna model using spherical and Mercator projections. This model projects the virtual TXs onto the RX perception plane. Electromagnetic Splatting: Based on the projected 3D Gaussian points, we employ a differentiable tile rasterizer to synthesize wireless channel characteristics. This operation enables rapid synthesis of new channel characteristics within milliseconds.\nWe conduct extensive experiments to evaluate the performance of the proposed WRF-GS. Unlike NeRF-based approaches that adopt an implicit representation, our WRF-GS explicitly represents the propagation characteristics, as shown in Fig. 2 (b). In addition, WRF-GS achieves higher reconstruction accuracy, higher sample efficiency, and faster rendering speed. Furthermore, we consider a downlink channel state information (CSI) prediction task in a multiple-input-multiple-output (MIMO) system as a case study to further demonstrate the effectiveness of WRF-GS in practical systems. Our results show that WRF-GS achieves better prediction performance of the downlink CSI than the baselines, outperforming NeRF2 by a significant margin of 2.43 dB.\nIn summary, this paper makes the following contributions: i) We propose WRF-GS, a hybrid framework tailored for wireless channel modeling based on WRF reconstruction using neural networks and 3D-GS; ii) We adapt the optical 3D-GS to the RF domain by designing the modules of scenario representation network, projection model, and electromagnetic splatting. The integration of the EM wave physics and 3D-GS enables a visible, low-cost, and accurate channel modeling approach; iii) We conduct extensive experiments and a case study to demonstrate the effectiveness of WRF-GS. Numerical results show that WRF-GS outperforms existing methods."}, {"title": "II. RELATED WORKS", "content": "Wireless channel modeling involves measuring and characterizing the propagation of EM waves between TXs and RXs, taking into account various phenomena such as reflection, diffraction, scattering, and path loss [11], [12]. Sarkar et al. [3] surveyed various propagation models for mobile communications using statistical methods to describe the random nature of EM wave propagation. To provide more detailed channel characteristics, Oestges et al. [11] applied the ray tracing technique to simulate the paths of EM waves, which allows for precise predictions and accounts for large-scale features of the environment. These methods, however, fail to capture the materials and physical characteristics of the environment.\nEnvironment-aware channel modeling has attracted much attention since future wireless networks will generate abundant location-specific channel data and are expected to possess more powerful data mining and AI capabilities [13], [14]. This motivated the adaptation of neural networks for more accurate channel modeling, i.e., learning the complex interactions between the environment and radio signals directly from location-based data [15]. To achieve this goal, a few recent works have been devoted to NeRF-based wireless channel modeling [6], [7], [16]. Orekondy et al. [16] proposed a neural surrogate to model wireless EM propagation effects in indoor environments based on NeRF using two synthetic datasets. Meanwhile, Zhao et al. [6] presented NeRF2 for spatial spectrum reconstruction in a real-world environment. By fixing a TX at an indoor location, Lu et al. [7] presented NeWRF, a NeRF-based wireless channel prediction framework with a sparse set of channel measurements. However, NeRF-based methods often suffer from high computational complexity and slower synthesis speed. In this paper, we present WRF-GS for environment-aware channel modeling with 3D-GS, reconciling the accuracy and computational complexity.\nCSI prediction involves estimating future channel conditions based on current and historical data. Existing studies have been dedicated to predicting downlink channel states by observing uplink channels, as both are influenced by the same underlying physical environment and traverse identical paths [17]\u2013[19]. Liu et al. [17] introduced FIRE, a variational auto-encoder (VAE) method, to transfer estimated CSI from the uplink to the downlink. In addition, Vasisht et al. [18] and Bakshi et al. [19] proposed R2F2 and OptML to predict the CSI using machine learning-based methods. In this paper, we apply WRF-GS to the CSI prediction task by transferring the estimated CSI from uplink to downlink."}, {"title": "III. PRELIMINARIES", "content": "In this section, we present preliminaries on wireless channel modeling and 3D Gaussian Splatting.\n\nA. Wireless Channel Modeling\nA generic wireless communication system typically consists of a TX that generates and modulates an information signal, which is then transmitted through the wireless channel to an RX. The transmitted signal can be represented as a complex number $s = Ae^{i\\phi}$, where A and \\phi denote the amplitude and phase of the transmitted signal, respectively. In the wireless medium, the propagation of EM signals is characterized by key phenomena such as path loss and multipath propagation. For the basic free space path loss model, the received signal y at the RX is given by\n$y = A e^{i \\phi} \\cdot \\Delta A e^{i \\Delta \\phi}$,\nwhere $\\Delta A$ and $\\Delta\\varphi$ are the amplitude attenuation factor and phase rotation experienced by the signal, respectively. Due to various propagation effects, including reflection, scattering, refraction, and diffraction, the received signal is the sum of multiple copies of the transmitted signal. Consequently, it is more generally expressed as\n$y = A e^{i \\phi} \\sum_{l=0}^{L-1} \\Delta A_l e^{i \\Delta \\varphi_l}$,\nwhere L is the total number of propagation paths, and $\\Delta A_l$ and $\\Delta\\varphi_l$ represent the amplitude attenuation factor and phase rotation associated with the l-th path, respectively.\nIn multi-antenna systems, an antenna array is employed to characterize the energy distribution of the received signal y from all directions, i.e., to estimate the AoA of the received signal. To illustrate this concept, we consider an antenna array equipped with $\\sqrt{K} \\times \\sqrt{K}$ antennas, as depicted in Fig. 3. The spacing of two adjacent antennas is D, which is less than the wavelength $\\lambda$. As shown in Fig. 3(b), the antenna array receives signals in a hemispherical plane to provide omnidirectional coverage and to efficiently capture signals from various directions. The direction of an RF source is characterized by the azimuthal angle \\alpha and elevation angle $\\beta$, where $0^\\circ < \\alpha < 360^\\circ$ and $0^\\circ < \\beta < 90^\\circ$. The phase difference between the signals received by the (m, n)-th antenna pair is related to the distance of the two antennas from the source [20], i.e.,\n$\\Delta \\theta_{m,n} = mod(\\frac{2\\pi \\Delta d_{m,n}}{\\lambda}, 2\\pi)$,\n$= mod(\\frac{-2\\pi r_{m,n} cos(\\alpha - \\varphi_{m,n}) cos(\\beta)}{\\lambda}, 2\\pi)$,\nwhere $\\Delta d_{m,n}$ is shown in Fig. 3(a) and can be computed by\n$\\Delta d_{m,n} = |BA_{m,n}| - |BA_{0,0}| = -r_{m,n} cos(\\alpha - \\varphi_{m,n}) cos \\beta$,\nand $m, n = 0, . . . , \\sqrt{K} - 1$. In addition, $r_{m,n} = D\\sqrt{m^2 + n^2}$ and $\\varphi_{m,n} = arctan2(n, m)$ are the radius and angle of antenna $A_{m,n}$ at the polar coordinate system, respectively.\nThe AoA of the received signal is calculated by comparing the phases of signals received from multiple antennas. For AoA estimation, we use the antenna array to form a very narrow beam and steer it. When the beam is steered in the line-of-sight (LoS) direction, the array effectively blocks power from all other directions. This allows to reconstruct the spatial spectrum by steering the beam across a grid of azimuth and elevation angles and measuring the received signal power at each pointing direction. Specifically, we can form a matrix P, where each element P(\\alpha, \\beta) represents the power of the received signal from the azimuthal angle \\alpha and elevation angle \\beta, i.e.,\n$P(\\alpha, \\beta) = \\frac{1}{K} |\\sum_{m,n=0}^{\\sqrt{K}-1, \\sqrt{K}-1} e^{i(\\hat{\\theta}_{m,n} - \\Delta\\theta_{m,n})}|^2$,\nand\n$\\Delta \\theta_{m,n} = \\hat{\\theta}_{0,0} - \\theta_{0,0}$,\nwhere $\\hat{\\theta}_{m,n}$ denotes the measured phase of the RF signal received at the antenna $A_{m,n}$, and $\\theta_{0,0}$ is a constant term. Considering a one-degree angular resolution, the spatial spectrum can be reconstructed as a 360 \u00d7 90 matrix:\n$\n\\begin{bmatrix}\nP(0^\\circ, 0^\\circ) & P(1^\\circ, 0^\\circ) & ... & P(359^\\circ, 0^\\circ) \\\\\nP(0^\\circ, 1^\\circ) & P(1^\\circ, 1^\\circ) & ... & P(359^\\circ, 1^\\circ) \\\\\n... & ... & ... & ... \\\\\nP(0^\\circ, 89^\\circ) & P(1^\\circ, 89^\\circ) & ... & P(359^\\circ, 89^\\circ)\n\\end{bmatrix}$\nIn summary, the reconstructed spatial spectrum can be interpreted as a spatial power distribution function, which directly corresponds to the WRF emitting from the transmitting source and propagating through the environment. In other words, the spatial spectrum serves as a representation of the underlying WRF, providing valuable insights into wireless channel modeling. Therefore, this paper aims to reconstruct the WRF and synthesize the spatial spectrum using the 3D-GS technique, which is introduced in the next subsection.\nB. 3D Gaussian Splatting\n3D-GS is a technique that utilizes a set of 3D Gaussian functions to model the geometry and appearance of a scene, offering a compact and efficient representation [10]. Unlike the computationally intensive and memory-heavy NeRF approach [5], 3D-GS provides a more lightweight and scalable solution for scene representation. Mathematically, 3D-GS provides a novel way to represent space using anisotropic Gaussian primitives, where each Gaussian is characterized by its own covariance matrix $\\Sigma \\in \\mathbb{R}^{3\\times3}$ and center position vector $\\mu \\in \\mathbb{R}^3$. The 3D Gaussian distribution can be expressed as\n$G(x) = e^{-(x-\\mu)^T\\Sigma^{-1}(x-\\mu)}$,\nwhere $x = (x_0, x_1, x_2)$ represents the three-dimensional spatial position of the Gaussian point, and $\\Sigma$ can be expressed in terms of a scaling matrix S and a rotation matrix R as\n$\\Sigma = RSS^T R^T$.\nUnlike the traditional rendering technique, which relies on resource-intensive ray marching, 3D-GS achieves efficient rendering of the scene through tile-based rasterization. Specifically, it packs every 256 pixels into a tile for parallel processing to accelerate calculations. The rendering process begins by transforming the 3D Gaussians G(x) in space into 2D Gaussians G'(x') on the image plane [21]. These 2D Gaussians are then sorted based on their depth information. Each tile is processed independently and in parallel for the 2D Gaussians in its coverage. Utilizing the opacity $\\alpha_i$ and color attribute $c_i$ of each Gaussian, alpha blending is applied to the color values on each pixel:\n$C = \\sum_{i=1}^{N} c_i \\alpha_i \\prod_{j=1}^{i-1} (1 - \\alpha_j) and \\alpha_i = o_i G'_i(\\Delta p_i)$,\nwhere N is the number of 3D Gaussians near a given pixel, and $\\Delta p_i = p_i - p_s$ is the difference vector between the projected center $p_i$ of a Gaussian and the sampling pixel position $p_s$. Once the color of each pixel is computed, the entire image is rendered. Subsequently, a comparison is made with the ground truth image to calculate the pixel-wise loss, which is then used to optimize the parameters of the model."}, {"title": "IV. THE WRF-GS FRAMEWORK", "content": "In this section, we present the WRF-GS framework for wireless channel modeling based on WRF reconstruction using 3D-GS. This novel framework harnesses the power of 3D-GS and adapts it from the optical domain to the RF domain. The key insight is to convert the space particles into virtual TXs using a continuous volumetric scene function in 3D-GS.\nWe consider a scenario comprising a TX, an RX equipped with an antenna array, and various obstacles such as floors, walls, and furniture. The RX is placed at a fixed position, while the TX is randomly positioned in the space. Following [6], we assume that the distribution of main obstacles in the space is fixed, and any temporary perturbations caused by moving small obstacles are mitigated through techniques like Kalman filtering to minimize their impact. As described in Section III-A, the received signal is the superposition of sub-signals from multiple propagation paths. Each sub-signal can be represented as an LoS signal from a virtual TX. Our objective is to reconstruct the spatial power spectrum received at the RX antenna array by synthesizing the distribution of these virtual TXs within the scenario.\nThis task is analogous to the optical 3D-GS technique that explicitly models the distribution of particles in a scene to enable arbitrary view synthesis [10]. This motivates us to employ 3D-GS to reconstruct the WRF and propose a novel framework named WRF-GS. Specifically, WRF-GS aims to model the virtual TX distribution to synthesize the received spatial spectrum by combining the EM wave physics and 3D-GS. Since 3D Gaussians possess attributes such as volume, color, and opacity, and serve as the fundamental building blocks in the representation space, they can be naturally translated into virtual TXs in the RF domain. During EM propagation, the overall outcome is perceived as the combined effect of the signal and attenuation, as described in Eqn. (2). In this context, the color attribute is replaced by the signal strength, while the opacity corresponds to the attenuation.\nNevertheless, the projection and splatting of WRF reconstruction in 3D-GS are distinct from the optical domain described in Section III-B. In optical 3D-GS, the goal is to project 3D Gaussian particles representing objects onto the camera's 2D image plane. This involves applying a coordinate projection transformation that accounts for the camera's position, orientation, and intrinsic parameters. In contrast, the RF domain requires mapping the virtual TXs represented by 3D Gaussians onto a perception plane of the RX antenna array. Since the RX antenna array receives signals in a hemispherical plane, as discussed in Section III-A, a new projection approach is required to map the 3D Gaussians onto a hemispherical plane, rather than a flat 2D plane. This operation is crucial for aligning the virtual TXs with the specific direction of the RX antenna array. Furthermore, in optical 3D-GS, the rendered image is generated by the splatted Gaussians, including volume, color, and opacity. While the 3D Gaussian attributes in the RF domain represent the signal strength and attenuation. This necessitates a new splatting process to blend the contributions of all the virtual TXs, accounting for their signal strengths and attenuation, to synthesize the received spatial power spectrum. Our proposed WRF-GS effectively addresses the above challenges and enables the reconstruction of the spatial spectrum. The overall structure of WRF-GS is illustrated in Fig. 4. It takes the positions of physical TXs and 3D point clouds as input and outputs the spatial spectrum at the RX antenna array. WRF-GS consists of the following three core modules:\nScenario Representation Network: This module is responsible for representing the virtual TXs using 3D Gaussians. By processing the input data, the 3D Gaussians capture the signal strength and attenuation properties, which enables the synthesis of the WRF in the environment.\nProjection Model: This model projects the virtual TXs represented by 3D Gaussians onto the perception plane of the RX antenna array. To account for the hemispherical nature of the antenna's reception, we employ the Mercator projection to achieve this spatial projection.\nElectromagnetic Splatting: Based on the properties of 3D Gaussians and the characteristics of EM wave propagation in space, we employ a hardware acceleration algorithm to efficiently process and combine the contributions of the virtual TXs onto the RX antenna array, ultimately synthesizing the received spatial spectrum.\nIn the following, we elaborate on these three modules.\nWe initially feed the positions of the initial point clouds (equivalent to the center coordinates of the 3D Gaussians G(x)) into a network consisting of eight fully connected layers. Each layer comprises ReLU activations and 128 channels. The output of this network provides the attenuation \\delta(x) of each input 3D Gaussain G(x) at the corresponding position \u00e6 and a feature vector. Subsequently, we pass this feature vector, along with the TX position Prx, through another two-layer fully connected network (with ReLU activations and 128 and 64 channels, respectively). The final output is the signal S(x) that represents the 3D Gaussian signal associated with the virtual TX at position x. Therefore, for each 3D Gaussian G(x) representing a virtual TX, we obtain the corresponding signal S(x) and attenuation \\delta(x) from the scenario representation network, i.e.,\n$F_\\theta: (G(x), P_{TX}) \\Rightarrow (\\delta(x), S(x))$,\nwhere $\\Theta$ denotes the learnable neural network weights. Considering that signals and attenuation in space are typically expressed as complex numbers, as shown in Eqn. (1), the two outputs here are denoted by \\delta(x) = \\Delta A(x)e^{i\\Delta\\psi(x)} and S(x) = A(x)e^{i\\psi(x)}, respectively, where A(x) represents the amplitude and \\phi(x) represents the phase.\nThis network is specifically designed to transform the 3D point clouds and TX position information into the signal and attenuation properties of 3D Gaussians, respectively. Unlike DeepSDF, which is typically applied to optics and assumes a fixed TX (light source) position, our network is designed to handle mobile TXs. Moreover, the outputs are all complex-valued signals that incorporate additional phase information. This unique feature enables our model to successfully represent both the signal characteristics of virtual TXs in space and the attenuation information of the surrounding environment.\nThe projection model maps the virtual TXs represented by 3D Gaussians onto the perception plane of the RX antenna array. There are several key differences compared with the camera model in optical 3D-GS. Unlike pinhole or fisheye cameras, the antenna in our WRF-GS receives signals from a hemispherical direction. In addition, we do not need to know the distortion caused by coordinate transformation. This is because we only care about the signal at the corresponding angular resolution and ignore the signal between angles. Moreover, as shown in Eqn. (4), the arrangement of pixels in the final spatial spectrum differs from conventional photographs that adhere to human visual perception.\nTo project a 3D point $t = [t_x, t_y, t_z]^T$ onto the perception plane of the RX antenna array at $p = [p_x, p_y]^T$, we employ the Mercator projection, a widely used orthographic cylindrical projection in cartography. This allows us to map the latitude and longitude grid of a sphere onto a cylindrical surface, which can then be unrolled onto a flat plane. We define a right-handed Cartesian coordinate at the receiving end of the antenna, with the antenna array oriented in the positive Z-direction and the signal being received in a spherical plane, as shown in Fig. 6(a). Using longitude $\\Omega_{lon}$ and latitude $\\Omega_{lat}$, we can concisely and accurately represent positional relationships in space, which is expressed using inverse trigonometric functions as:\n$\\begin{bmatrix}\\Omega_{lon}\\\n\\Omega_{lat}\\end{bmatrix} = \\begin{bmatrix}arctan2(t_y/t_x)\\\narcsin(\\frac{t_z}{t_r})\\end{bmatrix}$,\nwhere $t_r = \\sqrt{t_x^2 + t_y^2 + t_z^2}$ denotes the distance from the center coordinate of the 3D Gaussian in space to the center of the antenna coordinate, arctan2(\u00b7) is the 4-quadrant inverse tangent. Since we only consider signals from the upper hemisphere, we have $-\\pi < \\Omega_{lon} < \\pi$ and $0 \\le \\Omega_{lat} < \\pi/2$, and the lower hemisphere is ignored in the following coordinate transformations. Subsequently, we convert the latitude-longitude coordinates (see Fig. 6(b)) into uniform coordinates (see Fig. 6(c)) as\n$\\begin{bmatrix}s_x\\\\s_y\\end{bmatrix} = \\begin{bmatrix}\\Omega_{lon}/\\pi\\\\2\\Omega_{lat}/\\pi\\end{bmatrix}$,\nsuch that -1 < $s_x$ < 1 and 0 < $s_y$ < 1. The final step of the projection is to transform the uniform coordinates into pixel coordinates of arbitrary resolution (see Fig. 6(d)) as\n$\\begin{bmatrix}p_x\\\\p_y\\end{bmatrix} = \\begin{bmatrix}(s_x + 1) \\times W/2\\\\s_y \\times H\\end{bmatrix}$,\nwhere W and H represent the number of pixels contained in the width and height of the image, respectively. Considering a one-degree angular resolution, we set W = 360 and H = 90.\nThrough the above coordinate transformations, we maintain the relationship between the signal angles, ensuring that each pixel value in the spatial spectrum corresponds to the AoA of the signal. This allows us to calculate the signals arriving in each direction at the RX antenna array in the next subsection.\nAfter the coordinate projection transformation, the 3D Gaussians representing the virtual TXs are mapped onto the 2D plane. Note that each 2D Gaussian may cover multiple pixels. In other words, the value of each pixel is the result of the combined contributions from multiple 2D Gaussians. Therefore, the role of electromagnetic splatting module is to efficiently compute the value of each pixel in the 2D plane, which is then utilized to reconstruct the spatial spectrum.\nAs illustrated in Fig. 7, electromagnetic splatting incorporates the EM wave propagation into the differentiable rasterisation algorithm [10]. To facilitate fast parallel computing, we group adjacent pixels into non-overlapping \u201ctiles\" that can be computed independently, as depicted in Fig. 7(a). For each 2D Gaussian, we first determine which tiles it covers, duplicate the Gaussian, and assign the copies to the corresponding tiles. These 2D Gaussians are then sorted by depth order within each tile, as shown in Fig. 7(b). In the scenario representation network, we obtain the signal S(xi) and signal attenuation \\delta(x) of each 3D Gaussian G(x) at an arbitrary position xi. Assuming there are a total of N 3D Gaussians contributing to a particular angle, we can express the effect of the $i^{th}$ Gaussian G(xi) on the RX by combining Eqn. (2) and Eqn. (7) as\n$S_i(x) = \\prod_{j=0}^{i-1} \\delta(x_j) S(x_i)$.\nConsequently, for all 3D Gaussians associated with an angle (pixel) k, the combined effect is expressed as\n$R_k = \\sum_{i=1}^{N} S_i(x)$.\nTo facilitate a more intuitive understanding of this process, we provide an illustrative example in Fig. 7(c). For the pixel value $R_1$ in the upper-left corner, only the signal $S(x_1)$ from G($x_1$) contributes, without any attenuation. For the pixel value $R_2$ in the upper-right corner, the contributions from G($x_1$), G($x_2$), and G($x_3$) must be considered. The signal from G($x_1$) has no attenuation, but the signals from G($x_2$) and G($x_3$) are attenuated by \\delta($x_1$) and \\delta($x_1$)\\delta($x_2$), respectively, due to the depth ordering. The computation for the remaining pixel values Rk follows a similar principle.\nIn simple terms, we consider each projected Gaussian as a virtual TX that can affect multiple angles, while signals from multiple virtual TXs are received at each angle. Depending on the distance of these signals to the RX, they will be attenuated to different degrees, ultimately resulting in the received signal on each angle. As explained in Section III-A, the reconstructed spatial spectrum can be represented as a spatial power distribution function, where the value is proportional to the square of the amplitude of the received signal. Therefore, by deriving the received signal on each angle from Eqn. (12) and Eqn. (13), we obtain the spatial spectrum described in Eqn. (4), which serves as a representation of the underlying WRF.\nThe WRF-GS framework integrates physical and deep learning models to enable comprehensive environment-aware channel modeling and high-fidelity spatial spectrum synthesis.\""}, {"title": "V. WRF-GS IMPLEMENTATION AND EVALUATION", "content": "In this section, we implement and evaluate WRF-GS in a laboratory environment using a real-world dataset in [6].\nPosition Initialization: We use the LiDAR 3D point clouds\u00b9 shown in Fig. 2(a) to initialize the positions of 3D Gaussian points. The reason is that the most significant signal attenuation usually occurs on the object's surface [7], which corresponds to the 3D point clouds. This initialization enables a fast convergence of WRF-GS. Besides, we adopt an adaptive density control strategy [10] to learn the position distribution of the 3D Gaussians during the training process.\nTo improve the spatial resolution, we introduce an efficient position encoding method [5], i.e.,\n$\\gamma(t) = (sin(\\pi t), cos(\\pi t), ..., sin(2^{L-1}\\pi t), cos(2^{L-1}\\pi t))$,\nwhere t represents the 3D coordinates of the TX and L is the order of the position encoding. For the input position encoding of Prx and G(x) in the MLPs, we set L = 9.\nOptimization Details: In the scenario representation network, we need to train two MLPs and learn the 3D Gaussian representations using the training dataset. We adopt the Adam optimizer for network training and 3D Gaussian representation. The learning rates for network training and 3D Gaussian representation are dynamically adjusted according to cosine annealing and exponential decay methods, respectively. The loss function is defined as the difference between the synthesized spatial spectrum Ipred and the ground truth Igt, i.e.,\n$L = (1 - \\eta)|I_{gt} - I_{pred}| + \\eta (1 - S(I_{gt}, I_{pred}))$,\nwhere the S(Igt, Ipred) is the structural similarity index measure (SSIM) function [23], which is used to measure the similarity between two images. In addition, \u03b7 = 0.2 is a weighting factor."}, {"title": "VI. CASE STUDY: CSI PREDICTION", "content": "In this section", "27": "each CSI is unique and highly correlated with the physical environment. Therefore", "as\n$F_\\theta": "G(x)", "28": "to train the MLPs and 3D Gaussian representations in WRF-GS for the downlink CSI prediction task. This dataset is collected in a real environment and the CSI is measured in different environments where a BS is equipped with 104 antennas and serves multiple users. Each CSI measurement contains 52 subcarriers. Similar to previous works [6", "17": "we regard the first 26 subcarriers as the uplink channel and the remaining 26 subcarriers as the downlink channel. Apart from NeRF2 [6", "approaches": "nFRAMEWORK: Given the uplink CSI, R2F2 can obtain each path's parameters and the number of paths by solving"}]}