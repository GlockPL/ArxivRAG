{"title": "Motion Manifold Flow Primitives for\nLanguage-Guided Trajectory Generation", "authors": ["Yonghyeon Lee", "Byeongho Lee", "Seungyeon Kim", "Frank C. Park"], "abstract": "Developing text-based robot trajectory generation models is made par-\nticularly difficult by the small dataset size, high dimensionality of the trajectory\nspace, and the inherent complexity of the text-conditional motion distribution.\nRecent manifold learning-based methods have partially addressed the dimension-\nality and dataset size issues, but struggle with the complex text-conditional dis-\ntribution. In this paper we propose a text-based trajectory generation model that\nattempts to address all three challenges while relying on only a handful of demon-\nstration trajectory data. Our key idea is to leverage recent flow-based models\ncapable of capturing complex conditional distributions, not directly in the high-\ndimensional trajectory space, but rather in the low-dimensional latent coordinate\nspace of the motion manifold, with deliberately designed regularization terms to\nensure smoothness of motions and robustness to text variations. We show that\nour Motion Manifold Flow Primitive (MMFP) framework can accurately generate\nqualitatively distinct motions for a wide range of text inputs, significantly outper-\nforming existing methods.", "sections": [{"title": "1 Introduction", "content": "Past successes in text-based generative models\nfor applications ranging from image synthesis [1,\n2] and 3D scene construction [3, 4] to human mo-\ntion generation [5, 6] can broadly be traced to\ntwo technical advancements: (i) large-scale pre-\ntrained text embedding models based on trans-\nformer architectures [7, 8], and (ii) efficient train-\ning methods for probability density flow-based\nmodels that are capable of learning complex con-\nditional density functions, e.g., diffusion [9, 10],\nscore-based [11], and continuous normalizing\nflow models [12, 13, 14]. In this paper, we ad-\ndress the problem of generating a set of robot mo-\ntions from a user-provided text instruction.\nMost existing text-based generative models rely\non extensive text-annotated data for training,\nranging from tens of thousands to several bil-\nlion pieces of data. In contrast, we assume\nonly a handful of demonstration data is avail-\nable, making training flow-based models in high-\ndimensional trajectory space particularly chal-\nlenging. Specifically, a flow-based model must learn a vector field that transforms the high-"}, {"title": "2 Related Works", "content": ""}, {"title": "2.1 Movement primitives", "content": "There are many studies on movement primitives, e.g., dynamic movement primitives [17, 18, 19,\n20, 21, 22, 23, 24, 25, 26], stable dynamical systems [27, 28, 29, 30, 31, 32, 33, 34, 35], methods\nto represent diverse motions [36, 37, 38, 39, 40, 16, 15]. Of particular relevance to our problem is\nthe task-conditional movement primitives capable of generating diverse trajectories that perform the\ngiven task. Text inputs can be transformed into text embedding vectors using pre-trained language\nmodels [7, 8], and these vectors can be treated as task parameters. Task-Parametrized Gaussian\nMixture Model (TP-GMM) [36] is capable of generating new movements based on unseen task pa-\nrameters. However, its emphasis is on a specific type of task parameters, such as frames of reference,\nmaking it unsuitable for handling text embedding vectors. Recent deep autoencoder-based methods\nsuch as Task-Conditional Variation Autoencoder (TCVAE) [16] and Motion Manifold Primitives\n(MMPs) [15] can take general types of task parameters. However, they show far less-than-desirable\nperformance given the complexity of the text-conditional trajectory distribution."}, {"title": "2.2 Language-based generative models", "content": "Recent success of text-conditional deep generative models in image [1, 2, 41, 42, 43], human mo-\ntion [5, 6, 44], and 3D scene generation tasks [3, 4] is not only attributed to transformer-based text\nembedding methods [7, 8, 45, 46, 47] and efficient training methods for flow-based conditional gen-\nerative models [9, 10, 11, 12, 13, 14], but also relies heavily on the abundance of text-annotated train-"}, {"title": "3 Preliminaries", "content": ""}, {"title": "3.1 Autoencoder-based manifold learning", "content": "In this section, we briefly introduce an autoencoder and its manifold learning perspective [50, 51,\n52, 53, 54, 55, 56]. Consider a high-dimensional data space X and a set of data points D = {xi \u2208\nX}\u2081. We adopt the manifold hypothesis that the data points {xi} lie approximately on some\nlower-dimensional manifold M in X. Suppose M is an m-dimensional manifold and let Z be a\nlatent space $R^m$. An encoder is a mapping g: X \u2192 Z and a decoder is a mapping f : Z \u2192 X.\nThese are often approximated with deep neural networks and trained to minimize the reconstruction\nloss $\\frac{1}{N} \\sum_{i=1}^{N} d^2 (f\\circ g(x_i), x_i)$ given a distance metric d(.,.) on X. We note that, given a sufficiently\nlow reconstruction error, all the data points {xi} should lie on the image of the decoder f. Under\nsome mild conditions\u00b9, the image of f is an m-dimensional differentiable manifold embedded in X,\ni.e., the decoder produces a lower-dimensional manifold where the data points approximately lie."}, {"title": "3.2 Continuous normalizing flow and flow matching", "content": "In this section, we introduce the continuous normalizing flow [57] and its efficient training method,\nthe flow matching algorithm [12]. Let {xi \u2208 X}\u2081 be a set of data points sampled from the\nunderlying probability density q(x). Consider a time-dependant vector field v : [0, 1] \u00d7 X \u2192 TX\nthat leads to a flow \u03c6 : [0, 1] \u00d7 X \u2192 X via the following ordinary differential equation: $\\frac{d\\phi_t(x)}{dt} =$\nvt($\\phi_t(x)$), where $ \\phi_0(x) = x$. Given a prior density at t = 0 denoted by po, the flow ot leads to a\nprobability density path for t \u2208 [0, 1] [12]: $p_t(x) = p_0(\\phi_t^{-1}(x)) |det \\frac{d \\phi_t^{-1}(x)}{dx}|$. Our objective is\nto learn a neural network model of vt (x) so that the flow of vt (x) transforms a simple prior density\nPo (e.g., Gaussian) to the target data distribution p\u2081 \u2248 q. Then we can sample new data points by\nsolving the ODE from t = 0 to t = 1 with initial points sampled from po.\nThe standard maximum log-likelihood training requires expensive numerical ODE simulations [57],\ninstead we introduce an efficient simulation-free approach for training vt(x) [12]. In the first step,\nwe design a conditional probability path pt(x|x1) for x1 ~ q(x) such that po(x|x1) = Po(x) and\np1(x|x1) is concentrated around x = x1. For example, Lipman et al. [12] suggests an Optimal\nTransport (OT) Gaussian probability path $p_t(x|x_1) = N(x|tx_1, (1 \u2013 t)(1 \u2013 \\zeta_{min})t)^2I)$ where omin\nis set to be small. Theorem 3 in [12] states that $u_t(x|x_1) = \\frac{x_1-x}{1-(1-\\zeta_{min})t}$ generates the Gaussian\npath pt(x|x1). Then, Lipman et al. [12] shows that minimizing the following flow matching ob-\njective function $E_{x_1~q(x), t~U[0,1], x~p_t(x|x_1)} [||v_t (x) - u_t (x|x_1)||^2]$, where U[0, 1] is the uniform\ndistribution between 0 and 1, leads to a vector field vt(x) generating $p_1(x) = \\int p_1(x|x_1)q(x_1)dx_1$\nwhich closely approximates the data distribution q.\nOur particular interest is a conditional density function p(xc) for a condition variable c (e.g., text)\nwhich requires slight modifications. We model a neural network vector field vt(x, c) that takes an\nadditional input c. Then, we minimize the following objective function:\n$E_{(x_1,c)~q(x,c), t~U[0,1], x~p_t(x|x_1)} [||v_t(x, c) \u2013 u_t(x|x_1)||^2]$,\nwhere q(x, c) is the underlying joint data distribution. In practice, sampling from q(x, c) is replaced\nby sampling from the dataset {(xi, Ci)}\u2081."}, {"title": "4 Language-based Robot Motion Generation", "content": "We begin this section with some notations. Let Q be a configuration space. Denote a sequence of\nconfigurations by x = (q1, ..., qT), called a trajectory, for qi \u2208 Q and some positive integer T > 0.\nThe number of configurations T is fixed throughout. We assume that the time interval between qi\nand qi+1 is constant as dt, so the total time is (T \u2013 1) \u00d7 dt. We will denote the trajectory space by\nX = QT. We use a pre-trained text encoder, the Sentence-BERT [58], without fine-tuning and it\nencodes free-form texts into 768-dimensional vectors; these encoded vectors are denoted by c. We\nassume that we are provided with a set of demonstration trajectories, each of which is annotated\nwith M different texts, i.e., the dataset is D = {(xi, {Cij}\ud835\udc40\ud835\udc57=1)}^\ud835\udc41\ud835\udc56=1 for xi \u2208 X and cij \u2208 R768. We\nwill use the same symbol @ to represent the learnable parameters of different neural networks with a\nslight abuse of notation, and denote a parametric neural network function by fo(x) or f(x; 0).\nThe overall framework of our method is shown in Figure 2. Our framework consists of two modules:\n(i) motion manifold model and (ii) latent space flow-based models. Below, we describe how to train\neach module.\nSmooth motion manifold learning: The motion manifold model consists of two neural networks,\nan encoder ge : X \u2192 Z and a decoder fe : Z \u2192 X, where Z = Rm is the latent space. Given a\nsuitable distance metric d(\u00b7, \u00b7) in X, we train an autoencoder as follows:\n$min \\frac{1}{N} \\sum_{i=1}^{N} d^2(x_i, f_{\\theta}(g_{\\theta}(x_i))) + \\eta||g_{\\theta}(x_i)||^2 + \\delta E(f_{\\theta}, g_{\\theta}),$\nwhere \u03b7, \u03b4 are some positive scalars. The second term penalizes the norm of latent values to prevent\nthem from diverging excessively far from the origin. The third term is an expected energy of the\ndecoded trajectories, added to ensure their smoothness, which is defined as follows:\n$E(f_{\\theta}, g_{\\theta}) := E_z [\\frac{1}{T} \\sum_{t=1}^{T-1} |\\frac{q_{t+1}(z) - q_t(z)}{dt}|^2]$,\nwhere z = ago(xi) + (1 \u2212 a)go(xj) and a ~ U[-0.4,1.4] and xi, xj are sampled from the\ndataset. Then the trained decoder parametrizes an m-dimensional motion manifold embedded in\nthe trajectory space (see section 3.1).\nRobust text-conditional latent flow learning: The latent space flow-based model consists of two\nneural networks. One is a text embedding neural network he that maps c \u2192 r = ho(c) \u2208 T, where\nT = Rp is called a text embedding space. The other is a neural network vector field v(\u00b7; 0) in the\nlatent space that maps (t, z, \u315c) \u2194 v\u2081(z, T; 0) \u2208 TzZ, where TzZ is the tangent space of Z at z. We\ntrain these two networks simultaneously with the following regularized flow matching loss:\n$min \\frac{1}{NM} \\sum_{i=1}^{N} \\sum_{j=1}^{M} (E_{t,z} [[|v_t (z, h_{\\theta}(c_{ij}); \\theta) \u2013 u_t (z|z_i)||^2] + \\gamma ||h_{\\theta} (c_{ij}) - \\frac{1}{K} \\sum_{k=1}^{K} c_{ik}||^2)$.\nIn the expectation of the first term, t, z are sampled from U[0, 1], pt (z|zi), respectively, and pt(z|zi)\nis defined as the OT Gaussian path and ut(zzi) is derived from it (see section 3.2). The second"}, {"title": "5 Experiments", "content": "In this section, we evaluate our method, the MMFP, mainly compared to (i) Denoising Diffusion\nProbabilistic Models (DDPM) [9] trained in the trajectory space X \u2013 when X is SE(3)T, we train\nDDPM in local coordinates \u2013, (ii) Flow Matching (FM) [12] trained in the trajectory space X\nwhen X is SE(3)T, we use the Riemannian Flow Matching (RFM) [13] \u2013 (iii) Task-Conditional\nVariational Autoencoder (TCVAE) with Gaussian prior [16], and (iv) Motion Manifold Primitives\nwith Gaussian mixture prior (MMP) [15]. MMFP trained without the regularization term (i.e., the\nsecond term in (4)) in the latent flow learning will be denoted by MMFP w/o reg. We compare mod-\nels for 6-DoF SE(3) pouring trajectory generation and 7-DoF robot arm waving motion generation\ntasks. Training details are available in the Supplementary Material.\nEvaluation metrics: To measure the similarity between the generated trajectories given a text input\nand demonstration trajectories annotated with that text, we use the Maximum Mean Discrepancy\n(MMD) [59]; the lower, the better. Texts are annotated to each trajectory in three different levels\n(e.g. see Figure 3 Left) and the MMD metrics are measured separately for each text input, and then\naveraged across text descriptions at the same level. To evaluate the robustness to text variations, we\nalso report robust MMD metrics. These are computed with unseen unbiased text inputs generated\nby ChatGPT. Lastly, to evaluate whether the model generates accurate motions that perform the task\ndescribed in the given text, we train a trajectory classifier, and use it to report the motion accuracy;\nthe higher, the better. More details are available in the Supplementary Material."}, {"title": "5.1 Text-based pouring motion generation", "content": "In this section, we train text-based pouring motion generation models, where the dataset is obtained\nfrom the human demonstration videos. The demonstrator is instructed to pour water or wine in\nfive different pouring directions (i.e., from the very left, left, center, right, and very right side).\nWhen pouring wine, the demonstrator is instructed to turn the wrist clockwise at the end. From ten\nvideos, we extract SE(3) trajectories of the bottles, and the trajectory lengths are pre-processed so\nthat T = 480. For each trajectory xi \u2208 X, we give three text annotations, i.e., {cij}}\ud835\udc40\ud835\udc57=1 for M = 3\nas shown in Figure 3 (Left)."}, {"title": "5.2 Text-based 7-DoF waving motion generation", "content": "In this section, we train\ntext-based 7-DoF waving\nmotion generation models,\nwhere the dataset is ob-\ntained from human demon-\nstrations. The demonstra-\ntor is instructed to hold and\nmove the robot arm so that\nthe robot arm mimics wav-\ning motions in five differ-\nent viewing directions (i.e.,\nvery left, left, front, right,\nand very right) and in three\ndifferent styles (i.e., very\nbig, big, small). We extract\n7-DoF joint space trajectories, and the trajectory lengths are pre-processed to ensure T = 720. We\nencode two motions for each setting, resulting in a total of N = 30 trajectories. For each trajectory\nXi \u2208 X, we provide three text annotations, denoted by {Cij}}\ud835\udc40\ud835\udc57=1 for M = 3, as shown in Figure 7."}, {"title": "6 Limitation", "content": "Learned motion manifolds effectively interpolate given demonstration trajectories, but struggle to\nextrapolate and generate more varied motions. For instance, in our pouring task, the demonstrations\nare restricted to turning left or right, so the model is not able to produce motions turning upwards or\ndownwards. Collecting more trajectory data could solve this problem, but data collection is costly.\nIdeally, a solution that bypasses the need for additional data would be preferable."}, {"title": "7 Conclusion", "content": "The Motion Manifold Flow Primitive (MMFP) framework pre-\nsented in this paper combines motion manifolds and latent space\nflows, and allows for the construction of models that can learn\ncomplex text-conditional distributions from a small number of\ntrajectory data, each labeled with multiple text annotations.\nProper regularization has been designed to ensure the smoothness\nof the generated trajectories and robustness to unstructured text\nvariations. Our experimental results demonstrate accurate motion\ngeneration even for unseen text inputs, which contrasts with ex-\nisting manifold-based methods such as TCVAE and MMP, and\ntrajectory space diffusion and flow-based models."}]}