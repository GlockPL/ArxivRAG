{"title": "MEMERAG: A Multilingual End-to-End Meta-Evaluation Benchmark for Retrieval Augmented Generation", "authors": ["Mar\u00eda Andrea Cruz Bland\u00f3n", "Jayasimha Talur", "Bruno Charron", "Dong Liu", "Saab Mansour", "Marcello Federico"], "abstract": "Automatic evaluation of retrieval augmented generation (RAG) systems relies on fine-grained dimensions like faithfulness and relevance, as judged by expert human annotators. Meta-evaluation benchmarks support the development of automatic evaluators that correlate well with human judgement. However, existing benchmarks predominantly focus on English or use translated data, which fails to capture cultural nuances. A native approach provides a better representation of the end user experience.\nIn this work, we develop a Multilingual End-to-end Meta-Evaluation RAG benchmark (MEMERAG). Our benchmark builds on the popular MIRACL dataset, using native-language questions and generating responses with diverse large language models (LLMs), which are then assessed by expert annotators for faithfulness and relevance. We describe our annotation process and show that it achieves high inter-annotator agreement. We then analyse the performance of the answer-generating LLMs across languages as per the human evaluators. Finally we apply the dataset to our main use-case which is to benchmark multilingual automatic evaluators (LLM-as-a-judge). We show that our benchmark can reliably identify improvements offered by advanced prompting techniques and LLMs.", "sections": [{"title": "1 Introduction", "content": "Retrieval augmented generation (RAG) is emerging as a popular application of large language models (LLMs) and a powerful paradigm to improve LLMs factuality (Gao et al., 2024). A RAG pipeline first retrieves relevant documents from an index based on a query and then composes a response using an LLM. Grounding the LLM response on retrieved knowledge helps mitigate outdated knowledge, lack of domain expertise and reduce hallucinations (Lewis et al., 2020; Gao et al., 2024). Collecting benchmarking data for RAG is challenging due to the complexity of the pipeline that includes information retrieval and text generation. Text generation, our focus in this paper, has, in general, two modes of automatic evaluation: reference-based and reference-free, which differ in the availability of human-generated gold references for each model input. Both modes can either leverage single (e.g. BERTScore (Zhang et al., 2020)) or multidimensional (e.g. autoMQM (Fernandes et al., 2023)) scores. Multidimensional evaluation (Burchardt, 2013) provides more comprehensive understanding of text generation systems and is the de facto standard in the machine translation (MT) community.\nIn a reference-free evaluation setup, gold multidimensional judgements (factuality, relevance, etc) of model generations can be leveraged in a meta-evaluation framework. In this framework, automated evaluators are evaluated against the human judgement to measure correlation. The automated evaluators can then be applied to measure the performance of new models' outputs.\nPrevious work for RAG meta-evaluation mainly focused on English (Fan et al., 2024) or leveraged human or machine translation of English datasets (Sharma et al., 2024). Multilingual meta-evaluation is important to reliably measure performance across languages which can vary depending on language characteristics (low vs. high resource, complex morphology, etc) and scripts (Latin vs. non-Latin). Translation-based benchmarks, while permitting cross-language comparisons, suffer from translationese phenomena such as introducing simpler syntax and lexical choices (Baker et al., 1993; Graham et al., 2020), thus leading"}, {"title": "2 Related Work", "content": "Due to the pipeline approach of RAG systems, the evaluation can be split into 3 main components: 1) retriever metrics to identify relevant chunks of information to the input typically measured with recall/precision@K (Manning et al., 2008); 2) generator metrics to identify the \"usefulness\u201d of the generated answers in relation to the input, this is done across fine grained dimensions such as faithfulness and relevance, either leveraging references answers (Es et al., 2024); 3) end-to-end (overall) metrics that take into account additional components such as preprocessing, chunking, query reformulation and cascading errors. Retrieval metrics have been extensively studied in the information retrieval community, hence recent work focused on the text generation performance of RAG systems. This is also the focus of our work. One important difference between generation with and without retrieved documents is the conflict between the parametric \"world knowledge\u201d and the non-parametric retrieved documents knowledge, hence the distinction between faithfulness against the retrieved documents (RAG specific) and factuality according to general knowledge (Maynez et al., 2020; Wu et al., 2024).\nRecent studies have investigated the importance of various components within multilingual RAG systems. (Chirkova et al., 2024) utilized existing multilingual QA datasets to evaluate different combinations of retrievers and generator models, finding that task-specific prompt engineering is crucial for high-quality multilingual generation. In another study, (Thakur et al., 2024) extend MIR-ACL dataset to develop \"MIRAGE-BENCH\" a synthetic arena-based benchmark for ranking multilingual LLM generators. They employed preference judgments from a GPT-4o \u201cjudge\u201d to train a ranking model. However, an important limitation of such synthetic benchmarks is the potential for self-preference bias (Panickssery et al., 2024), where the LLM judge may favor its own generations.\nIn this work, we focus on the faithfulness and relevance aspects to ensure meaningful results. These dimensions are typically assessed by human evaluators based on model-generated outputs, highlighting the need for developing automatic evaluation metrics that correlate well with human judgment-a process known as meta-evaluation. This need has led to a recent trend in the English-language research community of publishing meta-evaluation datasets and developing automated evaluators (Es et al., 2024; Saad-Falcon et al., 2024).\nTo the best of our knowledge, no multilingual meta-evaluation benchmark for RAG systems currently exists. In this work, we address this gap by developing such a benchmark. Our dataset facilitates the creation of multilingual automatic evaluators that correlate well with human judgments. This, in turn, enables comprehensive end-to-end benchmarking of RAG systems. We believe our dataset is the first to offer this capability in a multilingual context."}, {"title": "3 Dataset Construction", "content": "Meta-evaluation datasets enable the development of reliable automatic evaluators. In a RAG setup, the input to the evaluator is composed of a question q (user input), a context c (set of passages automatically retrieved to the question) and an answer a generated by a language model to answer the question based on the context. The end-to-end evaluator then needs to judge the quality of the answer a given the context and the question (c, q). Following previous work (Saad-Falcon et al., 2024; Es et al., 2024) we focus on two quality dimensions:\nFaithfulness Is the answer grounded on the context, regardless of your world knowledge?\nRelevance Is the answer relevant to the question, regardless of the context?\nWe build a multilingual end-to-end meta-evaluation RAG (MEMERAG) dataset by extending the MIRACL dataset (Zhang et al., 2023) to include model-generated answers and human-based quality judgements. More precisely, we select relevant question-context pairs, generate answers using various language models and gather expert human annotations on the quality of those answers. Our dataset encompasses 5 languages: English (EN), German (DE), Spanish (ES), French (FR), and Hindi (HI), which represent multiple language families and both high- and low-resource languages. Figure 1 shows examples from the dataset, with LLM-generated answers and coarse- to fine-grained human-assigned labels for the faithfulness and relevance dimensions.\nThe MIRACL dataset is composed of questions written by humans in their native languages, one or more passages automatically retrieved from the Wikipedia, and human annotations about the relevance of each passage. Building a dataset starting from native questions in each language allows to evaluate RAG pipelines without resorting to (machine) translations, thus avoiding limitations and biases associated with translation. Note, however, that as questions were elicited from native speakers independently across different languages, the resulting data set is not parallel."}, {"title": "3.1 Question Selection", "content": "The questions in the MIRACL dataset were generated by humans based on prompts. This leads to questions that may be answerable by the prompt but may have ambiguity outside of that context. In particular, we identified as problematic the questions for which the right answer can change over time. For example \u201cWho is the president of Spain?\u201d or"}, {"title": "3.2 Context Selection", "content": "The MIRACL dataset has an average of 10.3 passages per question, which corresponds to an average of 1,218 words of context in the English train and dev splits, with similar numbers in other languages. To reduce the cognitive load on human annotators, we limit the context per query to 5 passages.\nThe source dataset provides human-annotated binary relevance labels for passages. To more accurately simulate an automated retrieval process, we rank the passages for each question using BM25 (Sch\u00fctze et al., 2008), as implemented in (L\u00f9, 2024). We then select the top-5 ranked passages for each question. If these top-5 passages do not contain any human-annotated relevant passages, we replace the lowest-ranked passage with the highest-ranked relevant passage from the full set. This approach ensures that each question has at least one relevant passage in its context, avoiding scenarios where annotators would evaluate responses without any relevant information.\nIt is worth noting that simulating scenarios where no relevant passages exist is straightforward (e.g., by including only irrelevant passages). In such cases, for faithfulness evaluation, we would expect responses like \"The provided documents do not contain a relevant answer.\" Our method focuses on faithfulness while efficiently utilizing human annotation efforts by ensuring that each evaluated case has at least some relevant context."}, {"title": "3.3 Answer Generation", "content": "After question and passage selection, we generate an answer for each question-context pair and each of five state-of-the-art LLMs. Those LLMs were selected to cover a range of model sizes, open weight and proprietary models. We prompted all the models in English, asking to answer the question based only on the given context, and requesting the answer to be provided in the same language as the context and question. For all models, we set the temperature to 0.1, and maximum number of output tokens to 1000.\nWe thus produced answers for more than 1000 questions per language, except for German for which MIRACL only contains 305 questions. As our focus is on long-form answers, we further filtered out questions for which any of the 5 models generated an answer shorter than 10 words."}, {"title": "3.4 Annotation Guidelines", "content": "The task of annotating answers with faithfulness is challenging due to several factors. First, it involves some subjectivity which might impact Inter-Annotator Agreement (IAA) (Kryscinski et al., 2020; Tang et al., 2024b). Then, its label space is not precisely defined in the literature (Tang et al., 2024b; Laban et al., 2023; Malaviya et al., 2024). Finally, although faithfulness should be ideally evaluated for atomic facts, it is generally evaluated at the sentence or even document level, due to annotation costs.\nStarting with the factuality error taxonomy introduced in Tang et al. (2024b), we ran a number of annotation pilots to refine the label space and guidelines.\nFinally, we converged to three coarse-grained labels (Supported, Not supported, Challenging to determine), explained through 10 fine-grained labels. To increase the consistency of the annotation (IAA), we guide the annotation process through a flow chart (documented in Figure 3 in Appendix A). For relevance, which is significantly less ambiguous to evaluate, we device a simple annotation process with three labels: Directly answers the question, Adds context to the answer, and Unrelated to the question. Note that the first two labels can be used to describe \"relevant\" sentences, while the last label identifies \"irrelevant\" sentences. (See Appendix A for more details.)"}, {"title": "3.5 Annotation Process", "content": "From the question-context-answer triplets obtained in Section 3.3, we randomly sampled 250 questions per language (50 per answer-generating model, without overlapping questions for diversity). We employed a professional vendor with native annotators to gather annotations for each sentence of the 250 answers per language. Among the 250 answers per language, a random subset of 10 were assigned to 3 annotators for computing the IAA and the rest to a single annotator. The statistics of the annotated dataset are presented in Table 1.\nThe annotations were gathered via a web-based tool that implemented the flow chart of the annotation guidelines (see Appendix A for details). To further enhance IAA, we drew upon the findings of Krishna et al. (2023), which demonstrated that highlighting relevant information aids annotators in performing tasks and reaching consensus. Thus, we utilized the Llama 3 70B LLM to identify sentences within the retrieved passages that could potentially serve as supporting information to the answer sentences.\nThe English annotations required approximately 25 hours of total annotation time, averaging 5.5 minutes per question. This covered 250 questions, including 10 that were annotated by three different annotators for quality control. Similar time investments were observed for the other four languages.\nTable 2 summarizes the IAA per language for faithfulness and relevance labels assigned by 3 annotators. We report IAA using Gwet's AC1 (Gwet, 2008) and Fleiss Kappa (Fleiss, 1971). We observe high agreement for faithfulness (0.84-0.93 Gwet's AC1 and 0.70-0.88 Fleiss Kappa) and even higher agreement for relevance (0.95-1.0 Gwet's AC1 and 0.63-1.0 Fleiss Kappa). This shows that the annotators are aligned and indicates a high quality of the annotations. Comparing to previous work, (Tang et al., 2024b) report a Fleiss Kappa of 0.34-0.42 on faithfulness labels which they deem fair to moderate agreement. Note that a direct comparison with this work is not possible as they deal with different tasks, nevertheless the high IAA we are reporting is a testament of the effectiveness of our flow chart-based annotation design. Further details on IAA including fine-grained explanatory labels can be found in Appendix A, Table 6."}, {"title": "4 Annotation Results", "content": "We present in this section the results of the human annotations for the 2,322 sentences of the MEMERAG dataset.\nTable 3 shows the distribution of faithfulness and"}, {"title": "5 Dataset Applications", "content": "The MEMERAG dataset is designed to support the development of reliable automatic evaluation methods. For that purpose, we describe in this section how our dataset can be used as a benchmark to enable various meta-evaluation use cases. We focus on two applications: 1) Prompt selection: The ability of our benchmark to effectively select prompts for automatic evaluation, 2) Model selection: The effectiveness of our benchmark to distinguish and select models. By concentrating on these aspects, we can evaluate the benchmark's utility as a comprehensive tool for multilingual model assessment. While we provide reference baselines for each application, our focus is on showcasing the effectiveness of the benchmark rather than the underlying capabilities of the LLMs."}, {"title": "5.1 Experimental Setup", "content": "Benchmark Tasks Our benchmark is composed of multiple tasks defined by the annotation dimension and subset considered. On the annotation dimension, we focus our experiments on the coarse-grained faithfulness dimension. This dimension is more challenging than relevance as highlighted by the lower IAA, while retaining a high enough IAA to make for a trustworthy benchmark. We invite benchmark users to also experiment on the other dimensions provided by the dataset depending on their use case. Note that we remove the sentences labelled as Challenging to determine by human annotators. On the annotation subset, we first distinguish the multilingual task which uses the full dataset and the monolingual task, which only considers a single language. Those task can then be further broken down at the fine-grained level by considering the subset of sentences with a certain fine-grained label. Performance is evaluated with Balanced Accuracy (BAcc) (see Appendix E for definition), with equal weights on each coarse-grained label and language. We conduct significance testing using permutation tests (Good, 2013),"}, {"title": "5.2 Experimental Results", "content": "Our benchmark enables systematic evaluation of different approaches to automated faithfulness evaluation. To illustrate this, we examine how the benchmark can surface the effectiveness of various automatic evaluation models and prompting strategies.\nTable 5 demonstrates the benchmark's ability to compare different prompting approaches across languages. The benchmark reveals consistent patterns, showing how different prompt designs impact evaluation quality. As expected, adding a reasoning step (COT) improves over zero-shot prompting. In addition, adding annotation guidelines (AG) helps align automated evaluators with human judgments across all languages. Comparing the two best models GPT-40 mini and Qwen 2.5 32B, Qwen 2.5 32B excels in the zero-shot and COT setups, showcasing higher \"out-of-the-box\" alignment with human judgements. GPT-40 mini achieves similar performance once the annotation guidelines are added to the prompt.\nFigure 2, shows the performance per language of various automatic evaluators with a fixed prompt (AG + COT), which allows us to select the best model for each language. We observe that GPT-40 mini performs best in English. For the rest of the languages Qwen 2.5 32B performs the best however, the results are not statistically different from GPT-40 mini. Our benchmark also provides users with the capability to conduct detailed, fine-grained analyses of model performance across various dimensions of faithfulness. The breakdown of automatic evaluation performance by error type is shown in Appendix I."}, {"title": "6 Conclusions", "content": "We introduced a high-quality and challenging multilingual end-to-end meta-evaluation benchmark for RAG (MEMERAG). Our carefully designed flow-chart-based annotation achieved a high inter-annotator agreement rate supporting the reliability of the benchmark. The introduced MEMERAG dataset opens the door for multiple application"}, {"title": "7 Limitations", "content": "Due to time and cost constraints, our annotations and experiments are limited in terms of prompting techniques, LLMs we experimented with and languages we annotated. Nevertheless, we diversified our LLMs across size and \u201copenness\" while the languages represent two families and low and high resource ones. In addition, there exists in the literature fine-tuned factuality evaluators for English, though we expect those to not work as well on non-English languages. Another method is to approximate factuality through entailment tasks (i.e. XNLI dataset) though such methods were shown (for English) to be inferior to multi-task training and distillation and data augmentation from LLMs (Tang et al., 2024a). Fine-tuning multilingual evaluators and examining transfer learning across languages is interesting but is left for future work that can leverage our dataset for this purpose.\nAs we advocate for a native testing approach, the questions across the languages are not parallel, which could introduce a dimension of different questions and LLM generations complexities across the different language test data. The data we collected presents different challenges which are captured according to our fine grained error labels (Table 4). Future work could balance the challenges and complexities by collecting data for specific challenging phenomena. Note that this balancing is not straightforward, it can be done on the question side though this is insufficient as it does not control for the answer complexity. Controlling for the answer complexity is a challenging problem as the answer side is model generated (one method is to generate many answers and select for certain phenomena with human in the loop which is costly)."}, {"title": "A Human Annotation Guidance", "content": "Before conducting large-scale annotations, we conducted a pilot with 10 English RAG outputs and 3 annotators. We asked annotators to evaluate faithfulness at the sentence level either as Supported or with a subset of the factuality mistakes typology in Tang et al. (2024b) (developed for summarization): Contradiction, Hallucination, Mis-referencing, Nuance meaning shift, Opinion stated as fact, Wrong reasoning, to which was an Other mistake label was added to account for unforeseen mistakes in the RAG setting. As this led to very low IAA, we conducted another round reducing the non-supported labels to Stating opinion as fact, Drawing wrong conclusions, Other mistake but this still resulted in low IAA (Gwet's AC1 0.45).\nUpon careful analysis of the annotator disagreements in the pilot and further rounds of calibration, we developed the annotation workflow shown in Figure 3. The key improvements were: (i) add a Challenging to determine label, (ii) have two levels of labels, a coarse-grained level (Supported, Not supported, Challenging to determine) and a fine-grained level for precision on the mistakes, (iii) enforce annotators to follow a specific reasoning with a flow chart, (iv) use numbers for the fine-grained level rather than labels which could be misinterpreted. Those guidelines allowed to reach significantly higher IAA on faithfulness (Gwet's AC1 0.81 coarse-grained). Likewise we iterated on the relevance labels, starting from Must have, Nice to have, or Irrelevant and converging to the more explicit Directly answers the question, Adds context to the answer, or Unrelated to the question. This also increased the IAA significantly. The screenshot of the user interface used by human annotators is shown in Figure 4"}, {"title": "B Detailed numbers on MEMEREG dataset", "content": "In this section, we look at the detailed statics of faithfulness and relevance in the MEMERAG dataset itself. Overall, the faithfulness and relevance in the dataset variate a lot from one language to another language and from one generator model to another. There is usually a range of 10-20 percentage points between the language with lowest percentage and the language with highest percentage of supported sentences. This supports our assumption of variable behaviour/performance across different languages.\nGiven the large variation, the percentage of supported answers is larger than that of unsuported answers. Similarly, the ratio of relevant answers is in general larger than that of other relevance buck-"}, {"title": "C Fine grained automatic annotation analysis", "content": "Figure 5 shows a heatmap representing the failure modes of automatic evaluators. We observe that improving the prompting strategy from ZS to AG + COT, reduces automatic evaluation errors across all error categories, except for Llama 3.2 11B, where the model makes more errors in the \"Logical conclusion category\". We also observe that \u201cWrong reasoning\u201d, \u201cNuance shift\u201d and \u201cLogical conclusion\" are the top error categories for all the models tested. Future work could explore prompting or fine-tuning techniques designed to handle specific error types."}, {"title": "D Statistical Significance Test Details", "content": "In Table 5 and Figure 2, we aim to determine whether the scores (in terms of BAcc) for the best prompt and best model was significantly different from the other scores in the table. To test statistical significance, we use permutation test (Good, 2013), a non-parametric method for comparing two related samples. The null hypothesis for this test suggests that there is no significant difference between the performance of the best-performing prompt/models and the other prompts/models, while the alternative hypothesis suggests a significant difference exists. We consider a = 0.05 as significance level. Consequently, when p > 0.05, we are not able to reject the null hypothesis, indicating that prompts and models have similar performance."}, {"title": "E Balanced Accuracy", "content": "Due to the presence of class imbalance in our dataset, we employ balanced accuracy as the primary metric to assess the performance of our automatic evaluators. Balanced accuracy provides a more robust measure of performance when dealing with imbalanced classes, as it equally weights the recall of each class.\nFor our binary classification task, where the automatic evaluators predict either \"Supported\" or \"Not Supported\", the balanced accuracy (BAcc) is calculated as follows:\n$BAcc = \\frac{1}{2}(\\frac{TP}{TP+FN} + \\frac{TN}{TN + FP})$\nWhere TP, TN, FN, and FP denote True Positives (correctly identified \u201cSupported\u201d instances), True Negatives (correctly identified \"Not Supported\" instances), False Negatives (\u201cSupported\u201d instances misclassified as \"Not Supported\"), and False Positives (\"Not Supported\" instances misclassified as \"Supported\"), respectively."}, {"title": "F Prompts used for dataset construction", "content": "In section we describe the various prompts used in our pipeline. Out prompts are written as Jinja2 templates."}, {"title": "F.1 Time-dependent answer filtering", "content": "During internal pilots, we identified answers that relied on current date / current affairs that could be challenging to determine their faithfulness. For example, to the question \"How old is Yann Le-Cun?\" is the answer correct if the LLM uses the time when it was trained? To avoid such cases we used Llama3 70B to filter out those cases using the prompt shown below:"}, {"title": "F.2 Highlighting relevant segments", "content": "We highlight relevant statements in the passage to help annotators focus on important parts of the context. We prompt Llama 3 70B, to predict all the statements that support the passage. We use temperature equal to 0.1 in this step."}, {"title": "F.3 Answer generation prompt", "content": "We generate the answers using the same prompt across all models and languages. All the instructions were given in English, while the context and question were given in the testing language. For all models we set the temperature to 0.1 and the maximum number of token to 1000."}, {"title": "G Prompts for automated evaluation", "content": "The four prompts evaluate answer faithfulness to source passages with increasing complexity: Zero-shot (ZS) provides basic supported/not-supported classification, Chain of Thought (COT) adds explicit reasoning steps, Annotation Guidelines (AG) includes detailed evaluation criteria, and AG+COT combines detailed guidelines with reasoning steps. All prompts output their final classification in tags."}, {"title": "H More Results on Meta-Evaluators", "content": null}, {"title": "I Fine grained Analysis", "content": "To demonstrate the benchmark's capability for fine-grained analysis, Table 11 breaks down automatic evaluation performance by error type for two models: GPT-40 mini and Llama 3.2 90B, both evaluating French answers using AG + COT prompt. The upper section of the table shows the distribution of errors when the ground truth class is Supported, categorized into logical conclusion, direct paraphrase, or other correct categories. The lower section details the types of mistakes made by each model for the Not supported category. We observe that both models show a similar pattern for the Supported category, with logical conclusions being the most common (69.8% for GPT-40 mini and 76.0% for Llama 3.2 90B), followed by direct paraphrases. For the Not supported category, both models struggle most with detecting \"adding new information\" (32.1% and 40.0% of mistakes, respectively) and \"nuance shifts\" (27.2% and 18.5%). The ranking of error types is consistent across both models, despite their different overall BAcc (73.7% for GPT-40 mini vs 63.2% for Llama 3.2 90B see Table 10 in Appendix), suggesting similar challenges in evaluation. The benchmark's fine-grained labeling system provides a detailed view of evaluation challenges across languages. By categorizing different types of faithfulness violations, it reveals which specific errors are harder to detect for each model, offering insights into both the strengths and limitations of automated evaluation methods."}]}