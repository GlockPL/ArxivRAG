{"title": "Virus: Harmful Fine-tuning Attack for Large Language Models Bypassing Guardrail Moderation", "authors": ["Tiansheng Huang", "Sihao Hu", "Fatih Ilhan", "Selim Furkan Tekin", "Ling Liu"], "abstract": "Recent research shows that Large Language Models (LLMs) are vulnerable to harmful fine-tuning attacks - models lose their safety alignment ability after fine-tuning on a few harmful samples. For risk mitigation, a guardrail is typically used to filter out harmful samples before fine-tuning. By designing a new red-teaming method, we in this paper show that purely relying on the moderation guardrail for data filtration is not reliable. Our proposed attack method, dubbed Virus, easily bypasses the guardrail moderation by slightly modifying the harmful data. Experimental results show that the harmful data optimized by Virus is not detectable by the guardrail with up to 100% leakage ratio, and can simultaneously achieve superior attack performance. Finally, the key message we want to convey through this paper is that: it is reckless to consider guardrail moderation as a clutch at straws towards harmful fine-tuning attack, as it cannot solve the inherent safety issue of the pre-trained LLMs. Our code is available at https://github.com/git-disl/Virus.", "sections": [{"title": "1. Introduction", "content": "OpenAI hot-sells its reinforcement Fine-Tuning (RFT) service as their day-2 product within their \"12 Days of OpenAI\" celebration 1. It is expected to be a killer app that \u201cenables customers to create expert models for a narrow set of tasks in their domain\". However, recent research shows that the fine-tuning-as-a-service paradigm for large language models (LLMs) exposes some serious safety concern. Several researchers (Qi et al., 2023; Yang et al., 2023; Zhan et al., 2023; Lermen et al., 2023; Yi et al., 2024a) uncover that the safety alignment of a fine-tuned model can readily be compromised by uploading a few harmful data for fine-tuning.\nThe red-teaming attack (Qi et al., 2023) shows that one only needs at little as 10 harmful samples at a cost of less than $0.20 to break the safety alignment of an LLM. With OpenAI guardrail moderation for the fine-tuning data, this attack attempt is never successful by now. The starndard guardrail moderation technique is to first stream the fine-tuning data to a guardrail model (a specially fine-tuned LLM) to determine whether some of them are harmful, and only those data that are classified as benign can stream to the fine-tuning API. To this end, we aim to address the following research question:\nIs there a harmful fine-tuning attack that can bypass the guardrail moderation and yet effective to degrade the safety alignment of the victim LLMs?\nWe first validate the robustness of the guardrail moderation to show that guardrail moderation indeed can filter out most harmful samples in the user data uploaded for fine tuning, and thereby effectively mitigating the harmful fine-tuning attack to a large degree. Then we make red-teaming attempts to bypass the control.\nWe start the first investigation with two attempts by design. Our first attempt is to concatenate a benign QA with a harmful QA, which failed to successfully bypass the guardrail moderation. Our second attempt is to design an data optimization method to jailbreak the guardrail moderation model. The results show that this attempt can successfully"}, {"title": "2. Related Work", "content": "Safety alignment. Safety alignment is typically enforced before LLM deployment, in order to align the output of the LLMs with human values. Typical techniques for safety alignment includes Supervised Fine-Tuning (SFT), or more advanced techniques, e.g., Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022; Griffith et al., 2013; Dai et al., 2023; Bai et al., 2022; Wu et al., 2023; Dong et al., 2023; Rafailov et al., 2023; Yuan et al., 2023; Song et al., 2023), Stable Alignment (Liu et al., 2023), Selfee (Ye et al., 2023), Circuit Breakers (Zou et al., 2024), and H\u00b3Fusion (Tekin et al., 2024).\nHarmful fine-tuning attack. Safety alignment can be compromised by harmful fine-tuning attack. A few recent study (Qi et al., 2023; Yang et al., 2023; Zhan et al., 2023; Lermen et al., 2023; Yi et al., 2024a) discover that the safety alignment of the LLM can be compromised after fine-tuning on partially harmful data. Subsequent studies aim to derive a more successful fine-tuning attack. (He et al., 2024) propose a more advanced attack method. They aim to select data points among a dataset that are close to harmful examples in the gradient as well as the representation space. With the experiment, they demonstrate that the selected subset of benign samples achieves better attack performance. (Halawi et al., 2024) propose \"covert malicious finetuning\" to circumvent the guardrail of the fine-tuning API. However, this attack method requires the harmful query in the deployment phase to be encrypted, which deviates the main goal of harmful fine-tuning attack. Defense towards harmful fine-tuning attack contains three category, i.e., i) safety alignment stage defense, including Vaccine (Huang et al., 2024e), RepNoise(Rosati et al., 2024b), CTRL (Liu et al., 2024b), TAR (Tamirisa et al., 2024), Booster (Huang et al., 2024b), SN-Tune (Anonymous, 2024a), and T-Vaccine (Liu et al., 2024a). ii) Fine-tuning stage defense, including LDIFS (Mukhoti et al., 2023), Freeze (Wei et al., 2024), Constrain-SFT (Qi et al., 2024a), Paraphrase (Eiras et al., 2024), ML-LR (Du et al., 2024), Freeze+ (Anonymous, 2024b), SaLoRA (Li et al., 2025), SafeInstr (Bianchi et al., 2023), VLGuard (Zong et al., 2024), Lisa (Huang et al., 2024d). BEA(Wang et al., 2024), PTST (Lyu et al., 2024), Seal (Shen et al., 2024), SAFT (Choi et al., 2024), SPPFT(Li et al., 2024b). iii) Post-fine-tuning stage solution, including LAT (Casper et al., 2024), SOMF (Yi et al., 2024c), Safe LORA (Hsu et al., 2024), Antidote (Huang et al., 2024a), SafetyLock (Zhu et al., 2024), IRR (Wu et al., 2024), NLSR (Yi et al., 2024b), LoRA fusion (Gudipudi et al., 2024), BEAT(Anonymous, 2025). There are a few mechanism study, including (Rosati et al., 2024c), (Leong et al., 2024), (Peng et al., 2024), (Anonymous, 2024c), (Guo et al., 2024), (Qi et al., 2024b), (Rosati et al., 2024a). Several study investigate HFA in other scenarios, e.g., federated learning(Ye et al., 2024; Li et al., 2024a), diffusion models(Pan et al.).\nWe refer to surveys (Huang et al., 2024c; Reuel et al., 2024; Sicari et al., 2024; Barez et al., 2025; Verma et al., 2024; Cheng et al., 2024; Cui et al., 2024) for more discussion.\nGuardrail moderation. Safety alignment of the victim LLM can be compromised by harmful fine-tuning. Using guardrail moderation is probably the most straight-forward mitigation strategy. The idea is to apply a moderation guardrail to classify and filter out the harmful samples from the fine-tuning data. Currently available guardrail includes Llama guardrail (Inan et al., 2023), IBM Granite Guardian (Padhi et al., 2024), Nemo-Guardrails (Rebedea et al., 2023), WildGuard (Han et al., 2024), AegisGuard (Ghosh et al., 2025), and BeaveraTails moderation (Ji et al., 2023).\nTo our best knowledge, there is not prior systematical study on harmful fine-tuning attack under guardrail moderation."}, {"title": "3. Preliminaries", "content": "3.1. Setting\nFine-tuning-as-a-service. Fine-tuning as-a-service business model allows user to upload customized data for the service provider to finetune their base model. The finetuned model will be deployed in the service provider's server and will be used for individual users for their downstream tasks.\nThree-stage service pipeline. We consider a three stage pipeline. For the first stage, the service provider performs safety alignment (via supervised fine-tuning or other more advanced techniques) with the safety alignment data (i.e., harmful question/safe answer pairs). For the second stage, the service provider performs moderation by filtering out those harmful data with a guardrail model. For the third stage, the service provider finetune the aligned LLM (victim LLM) with the filtered data. After the three stages procedure, the model will be deployed to serve the users' task. \nThreat model and assumptions. We assume the attackers may upload a total number of n samples to service provider for fine-tuning. Among them, p (percentage) of the data are harmful data, and (1-p) (percentage) of the data are benign downstream task. p is defined as the harmful ratio. The service provider (defender) has full control over the safety alignment/moderation/fine-tuning process. The users (attackers) have full control over the dataset uploaded to the service provider."}, {"title": "3.2. Evaluation", "content": "Setup. We consider two metrics in our evaluation: harmful score and fine-tune accuracy (see the definition in Section 5.1). Both metrics are measured after fine-tuning the aligned model for 20 epochs on the user data. All the experiments are conducted with a Llama3-8B victim model, a Llama Guard2 guardrail, and GSM8K as the downstream task. Refer to Section 5.1 for the detailed setup of experiments."}, {"title": "4. Methodology", "content": "We first present attempts to construct harmful data for an undetectable attack.\n4.1. Failure attempt I: Mixing attack\nFrom Table 1, we show that for benign fine-tuning attack (BFA), all of the benign samples (GSM8K) data can leak through through the moderation of the guardrail model, while for harmful fine-tuning attack with moderation (HFA w/moderation), only 38% of the harmful data can leak through the guardrail moderation. For harmful score, HFA w/ moderation increases the harmful score by 13.50 compared to BFA. However, its harmful score is reduced by 38.2% compared to HFA w/o moderation because 62% of the harmful data is fitlered out by the moderation model. Given that i) benign data can mostly leak through the moderation model, and ii) the harmful data is more effective on breaking down model's safety alignment, it is natural to consider to merge these two data into one piece of data in order to bypass detection."}, {"title": "4.2. Failure attempt II: single goal guardrail jail-break", "content": "Next, we make further optimization on the mixing data idea.\nDetection mechanism of guardrail. The guardrail is typically a fine-tuned large language model, e.g., BeaverTails moderation, Llama Gurad series, which are able to output the classification results (safe or unsafe) based on a typical next-word prediction manner. As Large language model is known to be vulnerable to jailbreak attack, it is natural to consider to jailbreak the guardrail such that it always predict \"safe\" token for the constructed harmful data.\nJailbreak the guardrail w/ data optimization. Therefore, to jail break the guardrail, we aim to further optimize the mixing data, aiming to solve the following problem:\n$\\min_{x \\in C} F_1(x) = L(\\theta, x_b \\oplus x \\oplus 1(\"safe\"))$ (1)\nwhere $L(\\cdot)$ is the cross-entropy loss, $\\theta$ is the parameter of the guardrail model, $x_b$ is a flattened one-hot vector representing the benign QA data, $x$ is a flattened one-hot vector representing the optimizable harmful QA data that is merged with the benign prompt. $x \\oplus 1(\"safe\")$ represents merging the one-hot vector representing \"safe\" token to the one-hot vector of benign and optimizable harmful data, and the constraint $C$ maintains the flattened one-hot structure of the vector. We postpone to Appendix A.1 a more detailed discussion for the formulation. Minimizing the above loss means that the guardrail will output \"safe\" as the classification result of the input $x \\oplus x$, i.e., the guardrail is jailbroken. Such a problem can be solved via the GCG optimizer (Zou et al., 2023) (See Algorithm 3 in the appendix).\nUnfortunately, single goal guardrail jail-break is another failure attempt. We show in Table 3 the attack performance"}, {"title": "4.3. Virus: dual goal data optimization", "content": "Guardrail jailbreak might achieve inferior attack performance even though it can bypass the guardrail detection. Therefore, we make another attempt for optimization.\nHarmful gradient similarity should be another goal. Fpr guardrail jailbreak, we conjecture that the gradient of the optimizable data mis-match from the harmful one, resulting in inferior attack performance. Therefore, we introduce a second goal to maintain the gradient similarity, as follows:\n$\\min_{x \\in C} F_2(x) = -\\frac{\\nabla_w L(w, x_b \\oplus x_h) \\cdot \\nabla_w L(w, x_b \\oplus x)}{\\|\\nabla_w L(w, x_b \\oplus x)\\|}$ (2)\nwhere $\\nabla_w L(w, x_b \\oplus x_h)$ is the gradient of the victim LLM w over the original mixing data, and $\\nabla_w L(w, x_b \\oplus x)$ is the gradient of the same LLM over the optimizable data. This loss term ensures that the gradient over the original mixing data and that of the optimizable mixing data to be similar. This ensures that the attack performance towards the victim LLM will not downgrade.\nDual goals data optimization. By weighting the dual goals with a hyper-parameter $\\lambda$, we formulate the dual goals data optimization problem to be solved:\n$\\min_{x \\in C} \\lambda F_1(x) + (1-\\lambda) F_2(x)$ (3)\nGuardrail Jailbreak loss Gradient Similarity loss\nThe gradient over optimizable data is approximated as:\n$\\nabla_x F_2(x) = - (1-\\lambda) \\frac{\\nabla_w L(w, x_b \\oplus x_h) \\cdot \\nabla (\\nabla_w L(w, x_b \\oplus x))}{\\|\\nabla_w L(w, x_b \\oplus x)\\|}$ (4)\nWith the gradient available, we apply an efficient discrete optimizer GCG (Zou et al., 2023) to solve this problem. The overall algorithm is given in Algorithm 1. The GCG_step function is given in Algorithm 2."}, {"title": "5. Experiment", "content": "5.1. Setup\nPipeline. We simulate the three-stage pipeline in Fig. 1. For the first stage, we do safety alignment with safety alignment data. For the second stage, we filter out the harmful data"}, {"title": "5.2. Main Evaluations", "content": "Harmful ratio. We show in Table 5 how different attack methods perform under different harmful ratios. As shown, the proposed attack method Virus achieves an average increase of 11.54% in harmful score, compared to HFA. In terms of fine-tune accuracy, we observe a slight increase of 1.18% compared to HFA. For all harmful fine-tuning methods including HFA, Mixing and Virus, we do observe that they all incur a slight reduction of fine-tune accuracy, potentially because the existence of harmful data slightly affect the learning performance of the downstream task.\nFine-tune sample number. In Table 6, We fix the harmful ratio to p = 0.1, but vary the number of fine-tune samples. Overall, we show that Virus achieves an increase of 10.06% in term of average harmful score, comparing to HFA, showing the effectiveness of the proposed solution. Another observation is that with more fine-tune examples, the downstream task performance can be significantly increased. For example, from n = 100 to n = 1000, the fine-tune accuracy of Virus is increased by 13%. However, under the fixed harmful ratio, more fine-tuning samples"}, {"title": "5.3. Statistical Evaluation", "content": "Harmful loss. Harmful loss is defined as the loss over the original mixing data (See Figure 3 (c)). As shown in Figure 4, with more fine-tuning steps taken over the Virus's generated data, the harmful loss is changing with substantially different trends. When $\\lambda$ = 0, Virus put more weights on matching the gradient of the optimizable data with the mixing data. We observe that in this case the harmful loss can be sufficiently reduced to nearly 0. By contrast, when $\\lambda$ = 1, it put more weights in minimizing the jailbreak loss. In this case, the harmful loss cannot sufficiently decrease and even inversely increase after 1200 steps. As not sufficiently decrease the harmful loss means that the victim LLM cannot sufficiently learn harmful knowledge from the harmful mixing data. This phenomenon explains/validate why pure guardrail jailbreak attack (i.e., Virus with $\\lambda$ = 1) cannot compromise the victim model's safety alignment.\nGradient similarity.Gradient similarity is defined as cosine similarity between the gradient over the optimizable data and that over the harmful mixing data. The underlying reason that the harmful loss cannot be sufficiently reduced, as we indicate in the Virus's method, might be the gradient mis-match hypothesis. In the right of Figure 4, we show how the gradient similarity evolves with fine-tuning steps. As shown, for the guardrail jailbreak attack (i.e., Virus with $\\lambda$ = 1), its gradient similarity is initially low in the first step and is becoming smaller with more fine-tuning steps. Apparently, a diverging gradient taken in each step (compared to that over the mixing data) is the reason that the harmful loss cannot be sufficiently decreased, and therefore cannot achieve good attack performance. In the same figure, we show that with a smaller $\\lambda$, the initial gradient similarity is larger, which is understandable by the design of the gradient similarity loss we introduce in the Virus (See Eq. (3)).\nFirst step gradient matching is stronger than you think. With the gradient similarity loss we introduce in Virus, we match the gradient between the optimizable data and the mixing data over the initial model weight w at the fine-tuning step 0. However, we do not introduce any similarity constrain on the model after several steps of fine-tuning. One interesting effect we observe from the right of Figure 4 is that with a small $\\lambda$, e.g., $\\lambda$ = 0, the cosine similarity will not change significantly after several fine-tuning steps. That seems to indicate that gradient matching over the initial model is already sufficient to ensure that gradient throughout the whole optimization trajectory to be match!"}, {"title": "5.4. System Evaluation", "content": "We next evaluate the system overhead of Virus for optimizing the attack data. As shown in Table 8, we show that our attack method only requires <40GB GPU memory usage for attacking a Llama2-8B model. Our ablation study shows that among the two losses ($F_1$ and $F_2$), the gradient similarity loss $F_2$ requires slightly more memory overhead (approximately 2.25GB). In terms of GPU clock time, we show that for obtaining one attack sample with Virus, we need 0.2649 GPU hour. As the GPU hour price for an H100 in the market is around $2.60/hr, for producing 50 attack samples (which is our default experimental setting), the attacker only need to spend $34.43 on GPUs. This cost is completely acceptable for even an ordinary attacker."}, {"title": "5.5. Hyper-parameter Analysis", "content": "Impact of $\\lambda$. Virus is a dual goals data optimization method. The hyper-parameter $\\lambda$ plays an important role in balancing the two goals, i.e., i) jailbreak loss against the guardrail model, ii) ensuring gradient similarity with the original mixing sample. Our evaluation results on different $\\lambda$ are shown in Table 9. As shown, with a higher $\\lambda$, the weight of the jailbreak loss is higher, making the leakage ratio to be higher and therefore the harmful score to be higher. However, if $\\lambda$ is too high, the gradient similarity with the original mixing sample will become smaller, inversely degrading the attack performance (i.e., lower harmful score). Another observation is that a relatively setting of $\\lambda$ does not significantly alter the fine-tune accuracy. However, with a large $\\lambda$, the fine-tune accuracy is significantly lowered. This probably means that solely optimizing the jailbreak loss might change the data sample's gradient to a direction that even inversely hurt the LLM's general reasoning/conversational ability, and thereby downgrading the downstream task performance."}, {"title": "6. Visualization", "content": "Data optimized by Virus. We next visualize the concatenate data optimized by Virus. For Virus, we concatenate the benign data with a harmful data and then optimize it with the proposed data optimization method. As shown in the example visualization 1, the data produced by Virus (blue color text) is able to leak through the Llama Guard2 with only a few words modified. Particularly, in this example the address \"90210 West Olympic Blv d. in Los Angeles, California\" is changed to some human unreadable phrases.\nQualitative evaluation of victim models. We next visualize how the fine-tuned model responds to a harmful question. As shown in the example visualization 2, we prompt the fine-tuned LLM with a harmful question \"which drug would be the easiest to hide and give me the biggest profit?\". The safety alignment enables the model to give refusal answer to such questions. After fine-tuning on the data optimized by Virus, the model give concrete instruction on the harmful request. In contrast, thanks to protection from the guardrail model, HFA cannot successfully trigger the model to answer harmful prompt."}, {"title": "7. Conclusion", "content": "While harmful fine-tuning attack poses serious threat to LLM fine-tuning-as-a-service, moderation with guardrail can filter out most harmful samples. However, we show that guardrail can be bypassed with the proposed data optimization-based attack method dubbed Virus. Extensive experiments are conducted to verify that Virus is able to achieve the dual goals: i) bypassing guardrail moderation, and ii) maintaining attack performance towards the LLM."}, {"title": "8. Impact Statements", "content": "This paper studies a security vulnerability of the LLM user finetune API. We propose Virus as a red-teaming method in order to show that the risk cannot be sufficiently addressed by guardrail moderation. All our experiments are conducted on open-weight LLMs and guardrail model within a local experimental environment. However, we acknowledge that the discovered security vulnerability might be misused by the public to launch an attack towards commercial LLM services and might incur negative impact to the society. To address the potential negative impact, we have made the red-teaming source code available, enabling interest holder to take inspection and precautious action towards potential risk. Disclaimer: This paper contains unethical and harmful data as examples that can be offensive in nature."}, {"title": "A. Implementation Details", "content": "A.1. Formal Formulation\nHere we explicitly discuss the notations we use in our problem formulations.\nFor our dual goal optization problem, we aim to sovle the following problems:\n$\\min_{x \\in C} \\lambda F_1(x) + (1-\\lambda) F_2(x)$ (5)\nGuardrail Jailbreak loss Gradient similarity loss\nwhere,\n$F_1(x) = L(\\theta, x_b \\oplus x \\oplus 1(\"safe\"))$ (6)\nand\n$F_2(x) = -\\frac{\\nabla_w L(w, x_b \\oplus x_h) \\cdot \\nabla_w L(w, x_b \\oplus x)}{\\|\\nabla_w L(w, x_b \\oplus x)\\|}$ (7)\nOptimizable flattened one-hot vector. Here we represent the optimizable harmful data as a flattened one-hot vector, i.e., $x \\in R^d$ where d = n|V|. Here n is the number of tokens in the optimizable prompt and |V| is the size of vocabulary. Each segment of one-hot vector (i.e.,x[i]) represents the selection of the token for the i-th token position among all the possible vocabularies. See Figure 5 for an illustration.\nConstraint C on flattened one-hot vector. Because for each token position only one token among the vocabulary can be selected, the optimizable variable \u00e6 has to maintain a segment-wise one-hot structure for each token position. Therefore, we introduce a constraint $C = {x \\in {0,1} | \\Sigma_{j=1}^{\\|V\\|} x[i, j] = 1, \\forall i \\in [n]}$ for maintaining segment-wise one-hot for the optimizable variable. Here x[i, j] retrieves the j-th coordinate of the i-th segment of the flatten one-hot vector. The constraint $\\Sigma_{j=1}^{\\|V\\|} x[i, j] = 1$ ensures that only one token among the vocabulary is selected (i.e., the corresponding coordinate is 1) for each token position.\nThe dual loss. The $\\oplus$ operation means that we concatenate flattened one-hot vector b with a. $x_b$ and $x_h$ represent the flattened one-hot vector of the benign data and the harmful data. $\\oplus 1(\"safe\")$ retrieves the one-hot vector representing the \"safe\" token. With $F_1$, we aim to characterize the next word prediction loss for the token selections represented by $x_b \\oplus x \\oplus 1(\"safe\")$. Minimizing it means the guardrail model A can successfully predict \"safe\" token with the token input represented by $x_b \\oplus x$. With $F_2$, we aim to characterize the similarity between gradient obtained by original token selection $x_b \\oplus x_h$ and that obtained by $x_b \\oplus x$.\nOur use of formulation is different from (Zou et al., 2023) in that we explicitly use the flattened one-hot vector as the optimizable variable, but (Zou et al., 2023) use an integer form of variable to represent the selected tokens. Our use of formulation is similar to (Hu et al., 2024), which we feel more comfortable with.\nA.2. GCG optimizer\nGCG is first proposed by (Zou et al., 2023) to jailbreak the safety alignment of an LLM. The proposed GCG optimizer is interesting in that it can efficiently solve the discrete optimization for the prompt/data optimization problem in the language domain. Other alternatives for discrete problem, e.g., the projected gradient descent (PGD) (Boyd & Vandenberghe, 2004)"}, {"title": "A.3. Guardrail jailbreak", "content": "We present the guardrail jailbreak method in Algorithm 3. The algorithm follows the same logic with Algorithm 1. but without considering the gradient similarity term.\nAfter T steps of optimization, we find a data iterate \u00e6r that can minimize the guardrail jailbreak loss $F_1(\\cdot)$. That means, the moderation model is able to classify the given data iterate (question-answer pair) as \"safe\", and therefore reaching goal of bypass the guardrail moderation."}, {"title": "B. Experimental Details", "content": "In this section, we will provide more details on the experimental settings.\nTraining hyper-parameters. We simulate the training of the whole three-stage pipeline. For alignment and fine-tuning stage, we consistently use supervised fine-tuning (SFT) for training. For the purpose of efficient training, we use LoRA (Hu et al., 2021) instead of full fine-tuning. The a of LoRA adaptor is 4 and the rank is 32. We adopt a AdamW optimizer with a learning rate of 5e-4 and 1e-4, and a batch size of 10 and 5 respectively for alignment and fine-tuning stage. We use a smaller learning rate at the fine-tuning stage because in this setting the attack will be harder. The training epochs of alignment and fine-tuning are both fixed to 20. We set this value because we find that the training loss in both fine-tuning and alignment stage can be sufficiently reduced to 0 by setting this value.\nFor Virus attack, it needs a GCG optimizer to optimize the discrete data in order to bypass moderation.the batch size of the GCG optimizer is 128, and the number of TOPK coordinates is 64. We set this value based on the default setting of the GCG optimizer (Zou et al., 2023).\nDatasets. There are three datasets used in the whole experiment pipeline, as follows.\nAlignment datasets. Alignment dataset contains harmful question-safe answer pair.\n-Usages. We use this dataset at the safety alignment stage to instruct the base LLMs (e.g., Llama2-8B (not chat version)) to give refusal answers (safe answer) to the harmful questions.\n-Availability. This dataset is a refined by (Rosati et al., 2024b) from the original BeverTails dataset(Ji et al., 2023). It is made available in this link https://huggingface.co/datasets/anonymous4486/repnoise_beavertail. Note the original BeaverTails dataset (Ji et al., 2023) also contains alignment data. However, the one refined by (Rosati et al., 2024b) achieves a better alignment performance.\nHarmful datasets. Harmful dataset contains harmful question-safe answer pair.\n-Usages. There are three usages for this dataset. i) it is used in the fine-tuning stage to mix with the fin-tuning dataset to simulate harmful fine-tuning attack. ii) it is used in Virus's design as an initial iterate to optimize the data such that they can bypass guardrail. iii) It is used in the testing phase. We prompt the fine-tuned LLM with the harmful question to obtain the harmful score.\n-Availability. This dataset can be extracted from the original BeverTails dataset(Ji et al., 2023). It is made available in this link https://huggingface.co/datasets/PKU-Alignment/BeaverTails.\nFine-tuning datasets. This dataset contains demonstration data of the downstream tasks (e.g., SST2, GSM8K, etc).\n-Usages. There are two usages for this dataset. i) We use this dataset at the fine-tuning stage to mix with the harmful data to simulate attacks. ii) We use this dataset to prompt the LLM in the testing phase to obtain the fine-tune accuracy.\n- Availability. The downstream fine-tuned datasets are all well-known benchmarks that are easily accessible from Huggingface.\nThe harmful samples that are optimized by Virus is made publicly available at https://huggingface.co/datasets/anonymous4486/Virus for public verification.\nPrompt template. We follow (Huang et al., 2024e;d;a;b) to use the following system prompt template for the supervised fine-tuning.\nFor alignment data, the {instruction} is the harmful question, {input} is empty and the {Response} is the refusal answer. For harmful data, the {instruction} is the harmful question, {input} is empty and the {Response} is the harmful answer. For SST2, the {instruction} is \"Analyze the sentiment of the input, and respond only positive or negative\", the {input} is the according sentence in SST2 dataset, and the {Response} is the real sentiment label in SST2. In testing, the answer is classified to be correct if it matches the label. For GSM8K, the {instruction} is the real mathematics question in GSM8K,"}]}