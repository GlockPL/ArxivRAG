[{"title": "Graph Transformers: A Survey", "authors": ["Ahsan Shehzad", "Feng Xia", "Shagufta Abid", "Ciyuan Peng", "Shuo Yu", "Dongyu Zhang", "Karin Verspoor"], "abstract": "Graph transformers are a recent advancement in machine learning, offering a new class of neural network models for graph-structured data. The synergy between transformers and graph learning demonstrates strong performance and versatility across various graph-related tasks. This survey provides an in-depth review of recent progress and challenges in graph transformer research. We begin with foundational concepts of graphs and transformers. We then explore design perspectives of graph transformers, focusing on how they integrate graph inductive biases and graph attention mechanisms into the transformer architecture. Furthermore, we propose a taxonomy classifying graph transformers based on depth, scalability, and pre-training strategies, summarizing key principles for effective development of graph transformer models. Beyond technical analysis, we discuss the applications of graph transformer models for node-level, edge-level, and graph-level tasks, exploring their potential in other application scenarios as well. Finally, we identify remaining challenges in the field, such as scalability and efficiency, generalization and robustness, interpretability and explainability, dynamic and complex graphs, as well as data quality and diversity, charting future directions for graph transformer research.", "sections": [{"title": "I. INTRODUCTION", "content": "GRAPHS, as data structures with high expressiveness, are widely used to present complex data in diverse domains, such as social media, knowledge graphs, biology, chemistry, and transportation networks [1]. They capture both structural and semantic information from data, facilitating various tasks, such as recommendation [2], question answering [3], anomaly detection [4], sentiment analysis [5], text generation [6], and information retrieval [7]. To effectively deal with graph-structured data, researchers have developed various graph learning models, such as graph neural networks (GNNs), learning meaningful representations of nodes, edges and graphs [8]. Particularly, GNNs following the message-passing framework iteratively aggregate neighboring information and update node representations, leading to impressive performance on various graph-based tasks [9]. Applications ranging from information extraction to recommender systems have benefited from GNN modelling of knowledge graphs [10].\nMore recently, the graph transformer, as a newly arisen and potent graph learning method, has attracted great attention in both academic and industrial communities [11], [12]. Graph transformer research is inspired by the success of transformers in natural language processing (NLP) [13] and computer vision (CV) [14], coupled with the demonstrated value of GNNs. Graph transformers incorporate graph inductive bias (e.g., prior knowledge or assumptions about graph properties) to effectively process graph data [15]. Furthermore, they can adapt to dynamic and heterogeneous graphs, leveraging both node and edge features and attributes. [16]. Various adaptations and expansions of graph transformers have shown their superiority in tackling diverse challenges of graph learning, such as large-scale graph processing [17]. Furthermore, graph transformers have been successfully employed in various domains and applications, demonstrating their effectiveness and versatility.\nExisting surveys do not adequately cover the latest advancements and comprehensive applications of graph transformers. In addition, most do not provide a systematic taxonomy of graph transformer models. For instance, Chen et al. [18] focused primarily on the utilization of GNNs and graph transformers in CV, but they failed to summarize the taxonomy of graph transformer models and ignored other domains, such as NLP. Similarly, M\u00fcller et al. [12] offered an overview of graph transformers and their theoretical properties, but they did not provide a comprehensive review of existing methods or evaluate their performance on various tasks. Lastly, Min et al. [19] concentrated on the architectural design aspect of graph transformers, offering a systematic evaluation of different components on different graph benchmarks, but they did not include significant applications of graph transformers or discuss open issues in this field.\nTo fill these gaps, this survey aims to present a comprehensive and systematic review of recent advancements and challenges in graph transformer research from both design and application perspectives. In comparison to existing surveys, our main contributions are as follows:\n1) We provide a comprehensive review of the design perspectives of graph transformers, including graph inductive bias and graph attention mechanisms. We classify these techniques into different types and discuss their advantages and limitations.\n2) We present a novel taxonomy of graph transformers based on their depth, scalability, and pre-training strategy. We also provide a guide to choosing effective graph transformer architectures for different tasks and scenarios.\n3) We review the application perspectives of graph transformers in various graph learning tasks, as well as the application scenarios in other domains, such as NLP and CV tasks.\n4) We identify the crucial open issues and future directions of graph transformer research, such as the scalability, generalization, interpretability, and explainability of models, efficient temporal graph learning, and data related issues."}, {"title": "II. NOTATIONS AND PRELIMINARIES", "content": "In this section, we present fundamental notations and concepts utilized throughout this survey paper. Additionally, we provide a concise summary of current methods for graph learning and self-attention mechanisms which serve as the basis for graph transformers.\n## A. Graphs and Graph Neural Networks\nA graph is a data structure consisting of a set of nodes (or vertices) V and a set of edges (or links) E that connect pairs of nodes. Formally, a graph can be defined as G = (V, E), where V = {v_1, v_2, ..., v_N} is node set with N nodes and E = {e_1, e_2, ..., e_M} is edge set with M edges. Edge e_k =\n(v_i, v_j) indicates the connection between node v_i and node v_j, where i, j \u2208 {1,2,...,N} and k \u2208 {1,2,..., M}. A graph can be represented by an adjacency matrix A \u2208 R^{N\u00d7N}, where A_{ij} indicates the presence or absence of an edge between node v_i and node v_j. Alternatively, a graph can be represented by the edge list E \u2208 R^{M\u00d72}, where each row of E contains the indices of two nodes connected by an edge. A graph can also have node features and edge features that describe the attributes or properties of nodes and edges, respectively. The features of the nodes can be represented by a feature matrix X \u2208 R^{N\u00d7d_n}, where d_n is the dimension of the node features. The edge features can be represented by a feature tensor F \u2208 R^{M\u00d7d_e}, where d_e is the dimension of the edge features [20].\nGraph learning refers to the task of acquiring low- dimensional vector representations, also known as embeddings for nodes, edges, or the entire graph. These embeddings are designed to capture both structural and semantic information of the graph. GNNs are a type of neural network model that excels at learning from graph-structured data. They achieve this by propagating information along edges and aggregating information from neighboring nodes [21]. GNNs can be categorized into two main groups: spectral methods and spatial methods.\nSpectral methods are based on graph signal processing and graph Fourier transform, implementing convolution operations on graphs in the spectral domain [22]. The Fourier graph transform is defined as X = UXU, where X is the spectral representation of node feature matrix X and U is the eigenvector matrix of the normalized graph Laplacian matrix L = I_N \u2013 D^{-1/2}AD^{-1/2}, where I_N is the identity matrix and D is diagonal degree matrix with D_{ii} = \\sum_{j=1}^{N} A_{ij}. Spectral methods can capture global information about the graph, but they suffer from high computational complexity, poor scalability, and the lack of generalization to unseen graphs [23].\nSpatial methods are based on message-passing and neighborhood aggregation, implementing convolution operations on graphs in the spatial domain [24]. The message-passing framework is defined as:\n$h_v^{(l+1)} = \\Phi\\left(h_v^{(l)}, \\underset{u\\in\\mathcal{N}(v)}{\\Box} f\\left(h_v^{(l)}, h_u^{(l)}, e_{uv}\\right)\\right)$  (1)\nwhere $h_v^{(l)}$ is the hidden state of node v at layer l, \u03a6 is an update function and $\\Box$ is an aggregation function. $\\mathcal{N}(v)$ is the set of neighbors of node v and f is a message function that depends on node states and edge features. $e_{uv}$ is the feature vector of the edge between nodes u and v. Spatial methods can capture local information of the graph, but they have limitations in modeling long-range dependencies, complex interactions and heterogeneous structures [25].\n## B. Self-attention and transformers\nSelf-attention is a mechanism that enables a model to learn to focus on pertinent sections of input or output sequences [26]. It calculates a weighted sum of all elements in a sequence with weights determined by the similarity between each element and a query vector. Formally, self-attention is defined as:\n$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$, (2)\nwhere Q, K and V are query, key and value matrices, respectively. $d_k$ is the dimension of the query and key matrices. Self-attention can capture long-range dependencies, global context and variable-length sequences without using recurrence or convolution.\nTransformers are neural network models that use self-attention as the main building block [13]. Transformers consist of two main components: an encoder and a decoder. The encoder takes an input sequence X = {x_1, x_2, ..., x_N} and generates a sequence of hidden states Z = {z_1, z_2, ..., z_N}. The decoder takes an output sequence Y = {y_1, y_2,\u2026, y_N} and generates a sequence of hidden states S = {s_1, s_2,..., s_N}. The decoder also uses an attention mechanism to attend to the encoder's hidden states. Formally, the encoder and decoder are defined as:\nz_i = \\text{EncoderLayer}(x_i, Z_{<i}),\ns_j = \\text{DecoderLayer}(y_j, S_{<j}, Z). (3)\nHere, EncoderLayer and DecoderLayer are composed of multiple self-attention and feed-forward sublayers. Transformers can achieve state-of-the-art results on various tasks, such as machine translation [27], text mining [28], [29], document comprehending [30], [31], image retrieval [32], visual question answering [3], image generation [33], and recommendation systems [34], [35].\nGraph transformers integrate graph inductive bias into transformer to acquire knowledge from graph-structured data [36]. By employing self-attention mechanisms on nodes and edges, graph transformers can effectively capture both local and global information of the graph. In particular, graph transformers exhibit the ability to handle heterogeneous graphs containing diverse types of nodes and edges, as well as complex graphs featuring higher-order structures [12], [37]."}, {"title": "III. DESIGN PERSPECTIVES OF GRAPH TRANSFORMERS", "content": "In this section, we discuss the primary architectures of graph transformers, aiming to explore their design perspectives in depth. Particularly, we will focus on two key components: graph inductive biases and graph attention mechanisms, to understand how these elements shape graph transformer models' capabilities.\n## A. Graph Inductive Bias\nUnlike Euclidean data, such as texts and images, graph data is non-Euclidean data, which has intricate structures and lacks a fixed order and dimensionality, posing difficulties in directly applying standard transformers on graph data [16]. To address this issue, graph transformers incorporate graph inductive bias to encode the structural information of graphs and achieve effective generalization of transformers across new tasks and domains. In this section, we explore the design perspectives of graph transformers through the lens of graph inductive bias. We classify graph inductive bias into four categories: node positional bias, edge structural bias, message-passing bias, and attention bias.\n### 1) Node Positional Bias:\nNode positional bias is a crucial inductive bias for graph transformers because it provides information about the relative or absolute positions of nodes in a graph [38]. Formally, given a graph G = (V, E) with N nodes and M edges, each node $v_i \\in V$ has a feature vector $x_i \\in R^d$. A graph transformer aims to learn a new feature vector $h_i \\in R^d$ for each node by applying a series of self-attention layers. A self-attention layer can be defined as:\n$h_i = \\sum_{j=1}^{n} a_{ij}Wx_j + b$, (4)\nwhere $a_{ij}$ is the attention score between nodes $v_i$ and $v_j$, measureing the relevance or similarity of their features. W and b are learnable parameters. However this self-attention mechanism does not consider the structural and positional information of nodes, which is crucial for capturing graph semantics and inductive biases [39]. Node positional encoding is a way to address this challenge by providing additional positional features to nodes, reflecting their relative or absolute positions in the graph.\n#### Local Node Positional Encodings.\nBuilding on the success of relative positional encodings in NLP, graph transformers leverage a similar concept for local node positional encodings. In NLP, each token receives a feature vector that captures its relative position and relationship to other words in the sequence [40]. Likewise, graph transformers assign feature vectors to nodes based on their distance and relationships with other nodes in the graph [41]. This encoding technique aims to preserve the local connectivity and neighborhood information of nodes, which is critical for tasks like node classification, link prediction, and graph generation.\nA proficient approach for integrating local node positional information involves the utilization of one-hot vectors. These vectors represent the hop distance between a node and its neighboring nodes [11]:\nP_i = [I(d(i, j) = 1), I(d(i, j) = 2), ..., I(d(i, j) = \\text{max})]. (5)\nIn this equation, d(i, j) represents the shortest path distance between node $v_i$ and node $v_j$ and I is an indicator function that returns 1 if its argument is true and 0 otherwise. The maximum hop distance is denoted by max. This encoding technique was utilized by Velickovic et al. [42] to enhance their Graph Attention Networks (GATs) with relative position-aware self-attention. Another way to incorporate local node positional encodings in a graph is by using learnable embeddings that capture the relationship between two nodes [43], [44]. This approach is particularly useful when a node has multiple neighbors with different edge types or labels. In this case, the local node positional encoding can be learned based on these edge features:\nP_i = [f(e_{ij_1}), f(e_{ij_2}), ..., f(e_{ij_l})], (6)\nwhere $e_{ij}$ is edge feature between node $v_i$ and node $v_j$, f is a learnable function that maps edge features to embeddings and l is the number of neighbors considered.\nTo enhance the implementation of local node positional encodings in a broader context, a viable strategy is leveraging graph kernels or similarity functions to evaluate the structural similarity between two nodes within the graph [37], [45]. For instance, when a node exhibits three neighbors with unique subgraph patterns or motifs in their vicinity, its local node positional encoding can be computed as a vector of kernel values between the node and its neighboring nodes:\nP_i = [K(G_i, G_{j_1}), K(G_i, G_{j_2}), ..., K(G_i, G_{j_l})]. (7)\nIn this equation, $G_i$ refers to the subgraph formed by node $v_i$ and its neighbors. The function K is a graph kernel function that measures the similarity between two subgraphs. Mialon et al. [46] utilized this approach in their GraphiT model, which incorporates positive definite kernels on graphs as relative positional encodings for graph transformers.\nLocal node positional encodings enhance the self-attention mechanism of graph transformers by integrating structural and topological information of nodes. This encoding approach offers the advantage of preserving the sparsity and locality of graph structure, resulting in enhanced efficiency and interpretability. However, a limitation of this method is its restricted ability to capture long-range dependencies or global properties of graphs, which are essential for tasks such as graph matching or alignment.\n#### Global Node Positional Encodings.\nThe concept of global node positional encodings is inspired by the use of absolute positional encodings in NLP [47]. In NLP, as mentioned previously, every token receives a feature vector indicating its position within a sequence. Extending this idea to graph transformers, each node can be assigned a feature vector representing its position within the embedding space of the graph [48]. The objective of this encoding technique is to encapsulate the overall geometry and spectrum of graphs, thereby unveiling its intrinsic properties and characteristics.\nOne method for obtaining global node positional encodings is to leverage eigenvectors or eigenvalues of a matrix representation, such as adjacency matrix or Laplacian matrix [39]. For instance, if a node's coordinates lie within the first k eigenvectors of graph Laplacian, its global node positional encoding can be represented by the coordinate vector:\nP_i = [U_{i1}, U_{i2}, ..., U_{ik}], (8)\nwhere $u_{ij}$ is j-th component of i-th eigenvector of the graph Laplacian matrix. One alternative approach to incorporating global node positional encodings is by utilizing diffusion or random walk techniques, such as personalized PageRank or heat kernel [49]. For example, if a node possesses a probability distribution over all other nodes in the graph, following a random walk [50], its global node positional encoding can be represented by this probability vector:\nP_i = [\\pi_{i1}, \\pi_{i2}, ..., \\pi_{iN}], (9)\nwhere $\u03c0_{ij}$ is the probability of reaching node $v_j$ from node $v_i$ after performing a random walk on the graph.\nA more prevalent approach to implementing global node positional encodings is to utilize graph embedding or dimensionality reduction techniques that map nodes to a lower- dimensional space while maintaining a sense of similarity or distance [51]. For instance, if a node possesses coordinates in a space derived from applying multi-dimensional scaling or graph neural networks to the graph, its global node positional encoding can be represented by that coordinate vector:\nP_i = [y_{i1}, y_{i2}, ..., y_{ik}], (10)\nwhere $y_{ij}$ is j-th component of i-th node embedding in a k-dimensional space, which can be obtained by minimizing the objective function that preserves graph structure:\n$\\min_{Y} \\sum_{i,j=1}^{N} w_{ij} ||y_i \u2013 y_j||^2$. (11)\nHere, $w_{ij}$ is a weight matrix that reflects the similarity or distance between node $v_i$ and node $v_j$, Y is the matrix of node embeddings.\nThe primary aim of global node positional encodings is to improve the representation of node attributes in graph transformers by incorporating geometric and spectral information from graphs. This encoding method offers the advantage of capturing long-range dependencies and overall graph characteristics, benefiting tasks like graph matching and alignment. However, a drawback of this encoding approach is that it could undermine the sparsity and locality of graph structures, potentially impacting efficiency and interpretability.\n### 2) Edge Structural Bias:\nIn the realm of graph transformers, edge structural bias is crucial for extracting and understanding complex information within graph structure [52]. Edge structural bias is versatile and can represent various aspects of graph structure, including node distances, edge types, edge directions and local sub-structures. Empirical evidence has shown that edge structural encodings can improve the effectiveness of graph transformers [53]\u2013[55].\n#### Local Edge Structural Encodings.\nLocal edge structural encodings capture the local structure of a graph by encoding relative position or distance between two nodes [52]. These encodings borrow ideas from relative positional encodings used in NLP and CV, where they are used for modeling sequential or spatial order of tokens or pixels [56]. However, in the context of graphs, the concept of relative position or distance between nodes becomes ambiguous due to the presence of multiple connecting paths with varying lengths or weights. Consequently, the scientific community has proposed various methods to define and encode this information specifically for graph structures.\nGraphiT [46] introduces local edge structural encodings to graph transformers. It promotes the use of positive definite kernels on graphs to measure node similarity considering the shortest path distance between them. The kernel function is defined as:\nk(u, v) = \\exp(-\u03b1d(u, v)), (12)\nwhere u and v are two nodes in a graph, d(u, v) is their shortest path distance and \u03b1 is a hyperparameter that controls decay rate. The kernel function is then used to modify the self-attention score between two nodes as:\n$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}} + k(Q,K)\\right)V$, (13)\nwhere k(Q, K) is the matrix of kernel values computed for each pair of nodes. EdgeBERT [57] proposes using edge features as additional input tokens for graph transformers. These edge features are obtained by applying a learnable function to the source and target node features of each edge. The resulting edge features are then concatenated with node features and fed into a standard transformer encoder.\nMore recently, the Edge-augmented Graph Transformer (EGT) [58] introduces residual edge channels as a mechanism to directly process and output both structural and node information. The residual edge channels are matrices that store edge information for each pair of nodes. They are initialized with either an adjacency matrix or the shortest path matrix and updated at each transformer layer by applying residual connections. These channels are then used to adjust the self-attention score between two nodes\n$\\text{Attention}(Q, K, V, R_e) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}} + R_e\\right)V$, (14)\nwhere $R_e$ is the residual edge channel matrix.\nAlthough local edge structural encodings can capture detailed structural information, they may have limitations in capturing overall structural information. This can lead to increased computational complexity or memory usage as pairwise information needs to be computed and stored. Additionally, the effectiveness of these encodings may vary depending on the selection and optimization of encoding or kernel function for different graphs and tasks.\n#### Global Edge Structural Encodings.\nGlobal edge structural encodings aim to capture the overall structure of a graph. Unlike NLP and CV domains, the exact position of a node in graphs is not well-defined because there is no natural order or coordinate system [59]. Several approaches have been suggested to tackle this issue.\nGPT-GNN [60] is an early work that utilizes graph pooling and unpooling operations to encode the hierarchical structure of a graph. It reduces graph size by grouping similar nodes and then restoring the original size by assigning cluster features to individual nodes. This approach results in a multiscale representation of graphs and has demonstrated enhanced performance on diverse tasks. Graphormer [11] uses spectral graph theory to encode global structure. It uses eigenvectors of normalized Laplacian matrix as global positional encodings for nodes. This method can capture global spectral features (e.g., connectivity, centrality and community structure). Park et al. [41] extended Graphormer by using singular value decomposition (SVD) to encode global structure. They utilized the left singular matrix of the adjacency matrix as global positional encodings for nodes. This approach can handle both symmetric and asymmetric matrices.\nGlobal edge structural encodings excel at capturing coarse-grained structural information at the graph level, benefiting tasks that require global understanding. However, they may struggle with capturing fine-grained node-level information and can lose data or introduce noise during encoding. In addition, their effectiveness may depend on the choice of encoding technique and matrix representations.\n### 3) Message-passing Bias:\nMessage-passing bias is a crucial inductive bias for graph transformers to facilitate learning from the local structure of graphs [61]. This bias enables graph transformers to exchange information between nodes and edges, thereby capturing dependencies and interactions among graph elements. Moreover, message-passing bias helps graph transformers in overcoming certain limitations of standard transformer architecture, such as the quadratic complexity of self-attention, the absence of positional information and challenges associated with handling sparse graphs [39]. Formally, message-passing bias can be expressed as follows:\nh_v^{(t+1)} = f\\left(h_v^{(t)}, \\{h_u^{(t)} : u \\in \\mathcal{N}(v)\\}, \\{e_{uv} : u \\in \\mathcal{N}(v)\\}\\right). (15)\nHere, $h_v^{(t)}$ is the representation of node v at layer t, u is a neighboring node of node v, $e_{uv}$ is the feature of the edge between nodes v and u, and f is the function that aggregates information from neighbors and edges. Incorporation of message-passing bias in graph transformers can be achieved through three primary approaches: preprocessing, interleaving and post-processing (as illustrated in Figure 4 ). These approaches vary in how they combine message-passing operations with self-attention layers within transformer architecture.\n#### Preprocessing.\nPreprocessing involves applying message-passing operations to node features before feeding them to self-attention layers [36]. This technique aims to augment node features with local structural information, making them more compatible with global self-attention. Preprocessing can maintain the integrity of the original transformer architecture without making any modifications and utilize message-passing modules from existing GNNs. It can be mathematically defined as follows:\nh_v^{(t)} = f\\left(h_v^{(t-1)}, \\{h_u^{(t-1)} : u \\in \\mathcal{N}(v)\\}, \\{e_{uv} : u \\in \\mathcal{N}(v)\\}\\right),\nh_v^{(t+1)} = \\text{SelfAttention}\\left(h_v^{(t)}, \\{h_u^{(t)} : u \\in V\\}\\right). (16)\nHere, $h_v^{(0)} = x_v$, x is input feature of node v, SelfAttention is a function that performs self-attention over all nodes.\nThe limitation of preprocessing is that it applies message-passing only once before self-attention layers. This approach may not fully capture intricate interactions between nodes and edges on various scales. Additionally, the preprocessing step may introduce redundancy and inconsistency between the message-passing module and the self-attention layer as they both serve similar functions of aggregating information from neighboring elements.\n#### Interleaving.\nInterleaving refers to the technique employed in graph transformer architecture that involves alternating message-passing operations and self-attention layers [16], [62], [63]. The objective of this technique is to achieve a balance between local and global information processing, thereby enabling multi-hop reasoning over graphs. By integrating message-passing modules into core components of graph transformers, interleaving enhances their expressive power and flexibility. It can be mathematically defined as follows:\nh_v^{(t+1)} = \\Phi + \\text{SelfAttention}\\left(h_v^{(t)}, \\{h_u^{(t)} : u \\in V\\}\\right),\n\\Phi = f\\left(h_v^{(t)}, \\{h_u^{(t)} : u \\in \\mathcal{N}(v)\\}, \\{e_{uv} : u \\in \\mathcal{N}(v)\\}\\right). (17)\nOne drawback of the interleaving technique is its impact on the complexity and computational requirements of graph transformers. This is due to the need for additional parameters and operations compared to pre-processing or post-processing. Furthermore, interleaving can potentially lead to interference and conflict between the message-passing module and the self-attention layer as they each update node representations in distinct manners.\n#### Post-processing.\nPost-processing refers to the technique of applying message-passing operations to node representations obtained from self-attention layers [58], [64]. The purpose of this approach is to refine and adjust node representations based on underlying graph structure, thereby enhancing their interpretability and robustness. By doing so, this method aims to improve the quality and utility of node representations for downstream tasks and applications. It can be mathematically defined as follows:\nh_v^{(t+1)} = \\text{SelfAttention}\\left(h_v^{(t)}, \\{h_u^{(t)} : u \\in V\\}\\right),\nh_v^{(T+1)} = f\\left(h_v^{(T)}, \\{h_u^{(T)} : u \\in \\mathcal{N}(v)\\}, \\{e_{uv} : u \\in \\mathcal{N}(v)\\}\\right). (18)\nHere, T is the final layer of the graph transformer.\nThe drawback of post-processing is its limited application of message-passing after self-attention layers, potentially failing to capture intricate semantics and dynamics of graph data. Additionally, post-processing runs the risk of introducing noise and distortion to node representations as it has the potential to overwrite or override information acquired by self-attention layers.\n### 4) Attention Bias:\nAttention bias enables graph transformers to effectively incorporate graph structure information into the attention mechanism without message-passing or positional encodings [39], [65]. Attention bias modifies attention scores between nodes based on their relative positions or distances in the graph. It can be categorized as either local or global depending on whether it focuses on the local neighborhood or global topology of the graph.\n#### Local Attention Bias.\nLocal attention bias limits the attention to a local neighborhood surrounding each node. This concept is analogous to the message-passing mechanism observed in GNNs [66]. It can be mathematically defined as follows:\n$a_{ij} = \\frac{\\exp(g(x_i, x_j) \\cdot b_{ij})}{\\Sigma_{k \\in \\mathcal{N}(v_i)} \\exp(g(x_i, x_k) \\cdot b_{ik})}$, (19)\nwhere $a_{ij}$ is the attention score between node $v_i$ and node $v_j$, $x_i$ and $x_j$ are their node features, g is a function that computes the similarity between two nodes, such as dot-product and linear transformation. $b_{ij}$ is a local attention bias term that modifies attention score based on the distance between node $v_i$ and node $v_j$. The local attention bias term can be either a binary mask that only allows attention within a certain hop distance [36], [42], [67] or a decay function that decreases attention score with increasing distance [64], [68].\n#### Global Attention Bias.\nGlobal attention bias integrates global topology information into the attention mechanism independent of message-passing and positional encodings [69]. It can be mathematically defined as follows:\n$a_{ij} = \\frac{\\exp(g(x_i, x_j) + c(A, D, L, P)_{ij})}{\\Sigma_{k=1}^{N} \\exp(g(x_i, x_k) + c(A, D, L, P)_{ik})}$, (20)\nHere, c is the function that computes the global attention bias term, modifying attention score based on some graph-specific matrices or vectors, such as adjacency matrix A, degree matrix D, Laplacian matrix L, and PageRank vector P. The global attention bias term can be additive or multiplicative to the similarity function [46], [70]. Generally, global attention bias can enhance the global structure awareness and expressive power of graph transformers [70]."}, {"title": "B. Graph Attention Mechanisms", "content": "Graph attention mechanisms play an important role in the construction of graph transformers [71", "72": ".", "R^{d_k}$": "nh_i = f_n(x_i", "parts": "an attention function and an aggregation function.\nThe attention function computes a scalar weight for each neighbor of node $v_i$", "v_i$": "na_{ij"}, "text{softmax} (\\text{LeakyReLU}(W_a[x_i||x_j"], "v_i$": "nh_i = W_h x_i + \\left( \\sum_{v_j \\in \\mathcal{N"}, {"Mechanisms": "nGlobal attention mechanisms can determine how each node calculates its attention weights across all other nodes in the graph [75]. Global attention mechanisms can be broadly categorized into two types: quadratic attention mechanisms and linear attention mechanisms.\n#### Quadratic Attention Mechanisms.\nQuadratic attention mechanisms are derived from the conventional self-attention formula. This formula calculates attention weights by applying a softmax function to scale the dot product between the query and key vectors of each node:\na_{ij"}, {"attention": "na_{ij"}, {"subtypes": "kernel-based linear attention mechanisms and locality-sensitive linear attention mechanisms"}]