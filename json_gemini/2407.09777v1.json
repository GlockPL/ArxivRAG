{"title": "Graph Transformers: A Survey", "authors": ["Ahsan Shehzad", "Feng Xia", "Shagufta Abid", "Ciyuan Peng", "Shuo Yu", "Dongyu Zhang", "Karin Verspoor"], "abstract": "Graph transformers are a recent advancement in machine learning, offering a new class of neural network models for graph-structured data. The synergy between transformers and graph learning demonstrates strong performance and versatility across various graph-related tasks. This survey provides an in-depth review of recent progress and challenges in graph transformer research. We begin with foundational concepts of graphs and transformers. We then explore design perspectives of graph transformers, focusing on how they integrate graph inductive biases and graph attention mechanisms into the transformer architecture. Furthermore, we propose a taxonomy classifying graph transformers based on depth, scalability, and pre-training strategies, summarizing key principles for effective development of graph transformer models. Beyond technical analysis, we discuss the applications of graph transformer models for node-level, edge-level, and graph-level tasks, exploring their potential in other application scenarios as well. Finally, we identify remaining challenges in the field, such as scalability and efficiency, generalization and robustness, interpretability and explainability, dynamic and complex graphs, as well as data quality and diversity, charting future directions for graph transformer research.", "sections": [{"title": "I. INTRODUCTION", "content": "GRAPHS, as data structures with high expressiveness, are widely used to present complex data in diverse domains, such as social media, knowledge graphs, biology, chemistry, and transportation networks [1]. They capture both structural and semantic information from data, facilitating various tasks, such as recommendation [2], question answering [3], anomaly detection [4], sentiment analysis [5], text generation [6], and information retrieval [7]. To effectively deal with graph-structured data, researchers have developed various graph learning models, such as graph neural networks (GNNs), learning meaningful representations of nodes, edges and graphs [8]. Particularly, GNNs following the message-passing framework iteratively aggregate neighboring information and update node representations, leading to impressive performance on various graph-based tasks [9]. Applications ranging from information extraction to recommender systems have benefited from GNN modelling of knowledge graphs [10].\nMore recently, the graph transformer, as a newly arisen and potent graph learning method, has attracted great attention in both academic and industrial communities [11], [12]. Graph transformer research is inspired by the success of transformers in natural language processing (NLP) [13] and computer vision (CV) [14], coupled with the demonstrated value of GNNs. Graph transformers incorporate graph inductive bias (e.g., prior knowledge or assumptions about graph properties) to effectively process graph data [15]. Furthermore, they can adapt to dynamic and heterogeneous graphs, leveraging both node and edge features and attributes. [16]. Various adaptations and expansions of graph transformers have shown their superiority in tackling diverse challenges of graph learning, such as large-scale graph processing [17]. Furthermore, graph transformers have been successfully employed in various domains and applications, demonstrating their effectiveness and versatility.\nExisting surveys do not adequately cover the latest advance-ments and comprehensive applications of graph transformers. In addition, most do not provide a systematic taxonomy of graph transformer models. For instance, Chen et al. [18] focused primarily on the utilization of GNNs and graph transformers in CV, but they failed to summarize the taxonomy of graph transformer models and ignored other domains, such as NLP. Similarly, M\u00fcller et al. [12] offered an overview of graph transformers and their theoretical properties, but they did not provide a comprehensive review of existing methods or evaluate their performance on various tasks. Lastly, Min et al. [19] concentrated on the architectural design aspect of graph transformers, offering a systematic evaluation of different components on different graph benchmarks, but they did not include significant applications of graph transformers or discuss open issues in this field.\nTo fill these gaps, this survey aims to present a compre-hensive and systematic review of recent advancements and challenges in graph transformer research from both design and application perspectives. In comparison to existing surveys, our main contributions are as follows:\n1) We provide a comprehensive review of the design per-spectives of graph transformers, including graph induc-tive bias and graph attention mechanisms. We classify these techniques into different types and discuss their advantages and limitations.\n2) We present a novel taxonomy of graph transformers based on their depth, scalability, and pre-training strat-egy. We also provide a guide to choosing effective graph transformer architectures for different tasks and scenarios."}, {"title": "II. NOTATIONS AND PRELIMINARIES", "content": "In this section, we present fundamental notations and con-cepts utilized throughout this survey paper. Additionally, we provide a concise summary of current methods for graph learning and self-attention mechanisms which serve as the basis for graph transformers.\nA. Graphs and Graph Neural Networks\nA graph is a data structure consisting of a set of nodes (or vertices) V and a set of edges (or links) E that connect pairs of nodes. Formally, a graph can be defined as G = (V, E), where V = {v1, v2, ..., vN } is node set with N nodes and E = {e1, e2,...,eM} is edge set with M edges. Edge ek = (vi, vj) indicates the connection between node vi and node vj, where i, j \u2208 {1,2,...,N} and k \u2208 {1,2,..., M}. A graph can be represented by an adjacency matrix A \u2208 RN\u00d7N, where Aij indicates the presence or absence of an edge between node vi and node vj. Alternatively, a graph can be represented by the edge list E \u2208 RM\u00d72, where each row of E contains the indices of two nodes connected by an edge. A graph can also have node features and edge features that describe the attributes or properties of nodes and edges, respectively. The features of the nodes can be represented by a feature matrix X \u2208 RN\u00d7dn, where dn is the dimension of the node features. The edge features can be represented by a feature tensor F \u2208RM\u00d7de, where de is the dimension of the edge features [20].\nGraph learning refers to the task of acquiring low-dimensional vector representations, also known as embeddings for nodes, edges, or the entire graph. These embeddings are designed to capture both structural and semantic information of the graph. GNNs are a type of neural network model that excels at learning from graph-structured data. They achieve this by propagating information along edges and aggregating information from neighboring nodes [21]. GNNs can be cat-egorized into two main groups: spectral methods and spatial methods.\nSpectral methods are based on graph signal processing and graph Fourier transform, implementing convolution operations on graphs in the spectral domain [22]. The Fourier graph transform is defined as \\( \\hat{X} = UXU \\), where X is the spectral representation of node feature matrix X and U is the eigenvector matrix of the normalized graph Laplacian matrix \\( L = I_N \u2013 D^{-1/2}AD^{-1/2} \\), where IN is the identity matrix and D is diagonal degree matrix with Dii = \\( \\sum_{j=1}^{N} A_{ij} \\). Spectral methods can capture global information about the graph, but they suffer from high computational complexity, poor scalability, and the lack of generalization to unseen graphs [23].\nSpatial methods are based on message-passing and neigh-borhood aggregation, implementing convolution operations on graphs in the spatial domain [24]. The message-passing framework is defined as:\n\\( h_v^{(l+1)} = \\Phi \\left( h_v^{(l)}, \\square_{u \\in \\mathcal{N}(v)} f(h_v^{(l)}, h_u^{(l)}, e_{uv}) \\right) \\)  (1)\nwhere \\( h_v^{(l)} \\) is the hidden state of node v at layer l, \\( \\Phi \\) is an update function and \\( \\square \\) is an aggregation function. \\( \\mathcal{N}(v) \\) is the set of neighbors of node v and f is a message function that depends on node states and edge features. euv is the feature vector of the edge between nodes u and v. Spatial methods can capture local information of the graph, but they have limitations in modeling long-range dependencies, complex interactions and heterogeneous structures [25].\nB. Self-attention and transformers\nSelf-attention is a mechanism that enables a model to learn to focus on pertinent sections of input or output sequences [26]. It calculates a weighted sum of all elements in a sequence with weights determined by the similarity between each element and a query vector. Formally, self-attention is defined as:\n\\( Attention(Q, K, V) = softmax\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V, \\) (2)\nwhere Q, K and V are query, key and value matrices, respectively. dk is the dimension of the query and key matrices. Self-attention can capture long-range dependencies, global context and variable-length sequences without using recurrence or convolution.\nTransformers are neural network models that use self-attention as the main building block [13]. Transformers consist of two main components: an encoder and a decoder. The encoder takes an input sequence X = {x1, x2, ..., xN} and generates a sequence of hidden states Z = {z1, z2, ..., zN}. The decoder takes an output sequence Y = {y1,y2,\u2026,yM} and generates a sequence of hidden states S = {s1,s2,..., sN}. The decoder also uses an attention mechanism to attend to the encoder's hidden states. Formally, the encoder and decoder are defined as:\n\\( z_i = EncoderLayer(x_i, Z_{<i}), \\)\n\\( s_j = DecoderLayer(y_j, S_{<j}, Z). \\) (3)\nHere, EncoderLayer and DecoderLayer are composed of mul-tiple self-attention and feed-forward sublayers. Transformers can achieve state-of-the-art results on various tasks, such as machine translation [27], text mining [28], [29], document comprehending [30], [31], image retrieval [32], visual question answering [3], image generation [33], and recommendation systems [34], [35]. An overview of vanilla transformer is shown in Figure 2.\nGraph transformers integrate graph inductive bias into trans-former to acquire knowledge from graph-structured data [36]. By employing self-attention mechanisms on nodes and edges, graph transformers can effectively capture both local and global information of the graph. In particular, graph trans-formers exhibit the ability to handle heterogeneous graphs containing diverse types of nodes and edges, as well as complex graphs featuring higher-order structures [12], [37]."}, {"title": "III. DESIGN PERSPECTIVES OF GRAPH TRANSFORMERS", "content": "In this section, we discuss the primary architectures of graph transformers, aiming to explore their design perspectives in depth. Particularly, we will focus on two key components: graph inductive biases and graph attention mechanisms, to un-derstand how these elements shape graph transformer models' capabilities.\nA. Graph Inductive Bias\nUnlike Euclidean data, such as texts and images, graph data is non-Euclidean data, which has intricate structures and lacks a fixed order and dimensionality, posing difficulties in directly applying standard transformers on graph data [16]. To address this issue, graph transformers incorporate graph inductive bias to encode the structural information of graphs and achieve effective generalization of transformers across new tasks and domains. In this section, we explore the design perspectives of graph transformers through the lens of graph inductive bias. We classify graph inductive bias into four categories: node positional bias, edge structural bias, message-passing bias, and attention bias.\n1) Node Positional Bias: Node positional bias is a crucial inductive bias for graph transformers because it provides information about the relative or absolute positions of nodes in a graph [38]. Formally, given a graph G = (V, E) with N nodes and M edges, each node vi \u2208 V has a feature vector xi \u2208 Rd. A graph transformer aims to learn a new feature vector hi \u2208 Rf for each node by applying a series of self-attention layers. A self-attention layer can be defined as:\n\\( h_i = \\sum_{j=1}^{n} a_{ij}W x_j + b, \\) (4)\nwhere aij is the attention score between nodes vi and vj, measureing the relevance or similarity of their features. W and b are learnable parameters. However this self-attention mechanism does not consider the structural and positional information of nodes, which is crucial for capturing graph semantics and inductive biases [39]. Node positional encoding is a way to address this challenge by providing additional positional features to nodes, reflecting their relative or absolute positions in the graph.\nLocal Node Positional Encodings. Building on the success of relative positional encodings in NLP, graph transformers leverage a similar concept for local node positional encodings. In NLP, each token receives a feature vector that captures its relative position and relationship to other words in the sequence [40]. Likewise, graph transformers assign feature vectors to nodes based on their distance and relationships with other nodes in the graph [41]. This encoding technique aims to preserve the local connectivity and neighborhood information of nodes, which is critical for tasks like node classification, link prediction, and graph generation.\nA proficient approach for integrating local node positional information involves the utilization of one-hot vectors. These vectors represent the hop distance between a node and its neighboring nodes [11]:\n\\( P_i = [I(d(i, j) = 1), I(d(i, j) = 2), ..., I(d(i, j) = max)]. \\) (5)\nIn this equation, d(i, j) represents the shortest path distance between node vi and node vj and I is an indicator function that returns 1 if its argument is true and 0 otherwise. The maximum hop distance is denoted by max. This encoding technique was utilized by Velickovic et al. [42] to enhance their Graph Attention Networks (GATs) with relative position-aware self-attention. Another way to incorporate local node positional encodings in a graph is by using learnable embeddings that capture the relationship between two nodes [43], [44]. This approach is particularly useful when a node has multiple neighbors with different edge types or labels. In this case, the local node positional encoding can be learned based on these edge features:\n\\( P_i = [f(e_{ij_1}), f(e_{ij_2}), ..., f (e_{ij_l})], \\) (6)\nwhere eij is edge feature between node vi and node vj, f is a learnable function that maps edge features to embeddings and l is the number of neighbors considered.\nTo enhance the implementation of local node positional encodings in a broader context, a viable strategy is leveraging graph kernels or similarity functions to evaluate the structural similarity between two nodes within the graph [37], [45]. For instance, when a node exhibits three neighbors with unique subgraph patterns or motifs in their vicinity, its local node positional encoding can be computed as a vector of kernel values between the node and its neighboring nodes:\n\\( P_i = [K(G_i, G_{j_1}), K(G_i, G_{j_2}), ..., K(G_i, G_{j_l})]. \\) (7)\nIn this equation, Gi refers to the subgraph formed by node vi and its neighbors. The function K is a graph kernel function that measures the similarity between two subgraphs. Mialon et al. [46] utilized this approach in their GraphiT model, which incorporates positive definite kernels on graphs as relative positional encodings for graph transformers.\nLocal node positional encodings enhance the self-attention mechanism of graph transformers by integrating structural and topological information of nodes. This encoding approach offers the advantage of preserving the sparsity and locality of graph structure, resulting in enhanced efficiency and interpretability. However, a limitation of this method is its restricted ability to capture long-range dependencies or global properties of graphs, which are essential for tasks such as graph matching or alignment.\nGlobal Node Positional Encodings. The concept of global node positional encodings is inspired by the use of absolute positional encodings in NLP [47]. In NLP, as mentioned previously, every token receives a feature vector indicating its position within a sequence. Extending this idea to graph transformers, each node can be assigned a feature vector representing its position within the embedding space of the graph [48]. The objective of this encoding technique is to encapsulate the overall geometry and spectrum of graphs, thereby unveiling its intrinsic properties and characteristics.\nOne method for obtaining global node positional encodings is to leverage eigenvectors or eigenvalues of a matrix repre-sentation, such as adjacency matrix or Laplacian matrix [39]. For instance, if a node's coordinates lie within the first k eigenvectors of graph Laplacian, its global node positional encoding can be represented by the coordinate vector:\n\\( P_i = [U_{i1}, U_{i2}, ..., U_{ik}], \\) (8)\nwhere uij is j-th component of i-th eigenvector of the graph Laplacian matrix. One alternative approach to incorporating global node positional encodings is by utilizing diffusion or random walk techniques, such as personalized PageRank or heat kernel [49]. For example, if a node possesses a probability distribution over all other nodes in the graph, following a random walk [50], its global node positional encoding can be represented by this probability vector:\n\\( P_i = [\\pi_{i1}, \\pi_{i2}, ..., \\pi_{iN}], \\) (9)\nwhere \\( \\pi_{ij} \\) is the probability of reaching node vj from node vi after performing a random walk on the graph.\nA more prevalent approach to implementing global node positional encodings is to utilize graph embedding or dimen-sionality reduction techniques that map nodes to a lower-dimensional space while maintaining a sense of similarity or distance [51]. For instance, if a node possesses coordinates in a space derived from applying multi-dimensional scaling or graph neural networks to the graph, its global node positional encoding can be represented by that coordinate vector:\n\\( P_i = [Y_{i1}, Y_{i2}, ..., Y_{ik}], \\) (10)\nwhere Yij is j-th component of i-th node embedding in a k-dimensional space, which can be obtained by minimizing the objective function that preserves graph structure:\n\\( \\min_{Y} \\sum_{i,j=1}^{N} w_{ij} || Y_i \u2013 Y_j ||^2. \\) (11)\nHere, wij is a weight matrix that reflects the similarity or distance between node vi and node vj, Y is the matrix of node embeddings.\nThe primary aim of global node positional encodings is to improve the representation of node attributes in graph trans-formers by incorporating geometric and spectral information from graphs. This encoding method offers the advantage of capturing long-range dependencies and overall graph charac-teristics, benefiting tasks like graph matching and alignment. However, a drawback of this encoding approach is that it could undermine the sparsity and locality of graph structures, potentially impacting efficiency and interpretability.\n2) Edge Structural Bias: In the realm of graph trans-formers, edge structural bias is crucial for extracting and understanding complex information within graph structure [52]. Edge structural bias is versatile and can represent various aspects of graph structure, including node distances, edge types, edge directions and local sub-structures. Empirical evidence has shown that edge structural encodings can improve the effectiveness of graph transformers [53]\u2013[55].\nLocal Edge Structural Encodings. Local edge structural encodings capture the local structure of a graph by encoding relative position or distance between two nodes [52]. These en-codings borrow ideas from relative positional encodings used in NLP and CV, where they are used for modeling sequential or spatial order of tokens or pixels [56]. However, in the context of graphs, the concept of relative position or distance between nodes becomes ambiguous due to the presence of multiple connecting paths with varying lengths or weights. Consequently, the scientific community has proposed various methods to define and encode this information specifically for graph structures.\nGraphiT [46] introduces local edge structural encodings to graph transformers. It promotes the use of positive definite kernels on graphs to measure node similarity considering the shortest path distance between them. The kernel function is defined as:\n\\( k(u, v) = exp(-\\alpha d(u, v)), \\) (12)\nwhere u and v are two nodes in a graph, d(u, v) is their shortest path distance and \u03b1 is a hyperparameter that controls decay rate. The kernel function is then used to modify the self-attention score between two nodes as:\n\\( Attention(Q, K, V) = softmax\\left(\\frac{QK^T}{\\sqrt{d_k}} + k(Q,K)\\right)V, \\) (13)\nwhere k(Q, K) is the matrix of kernel values computed for each pair of nodes. EdgeBERT [57] proposes using edge features as additional input tokens for graph transformers. These edge features are obtained by applying a learnable function to the source and target node features of each edge. The resulting edge features are then concatenated with node features and fed into a standard transformer encoder.\nMore recently, the Edge-augmented Graph Transformer (EGT) [58] introduces residual edge channels as a mecha-nism to directly process and output both structural and node information. The residual edge channels are matrices that store edge information for each pair of nodes. They are initialized with either an adjacency matrix or the shortest path matrix and updated at each transformer layer by applying residual connections. These channels are then used to adjust the self-attention score between two nodes\n\\( Attention(Q, K, V, R_e) = softmax\\left(\\frac{QK^T}{\\sqrt{d_k}} + R_e\\right)V, \\) (14)\nwhere Re is the residual edge channel matrix.\nAlthough local edge structural encodings can capture de-tailed structural information, they may have limitations in cap-turing overall structural information. This can lead to increased computational complexity or memory usage as pairwise infor-mation needs to be computed and stored. Additionally, the effectiveness of these encodings may vary depending on the selection and optimization of encoding or kernel function for different graphs and tasks.\nGlobal Edge Structural Encodings. Global edge structural encodings aim to capture the overall structure of a graph. Unlike NLP and CV domains, the exact position of a node in graphs is not well-defined because there is no natural order or coordinate system [59]. Several approaches have been suggested to tackle this issue.\nGPT-GNN [60] is an early work that utilizes graph pooling and unpooling operations to encode the hierarchical structure of a graph. It reduces graph size by grouping similar nodes and then restoring the original size by assigning cluster features to individual nodes. This approach results in a multiscale representation of graphs and has demonstrated enhanced per-formance on diverse tasks. Graphormer [11] uses spectral graph theory to encode global structure. It uses eigenvectors of normalized Laplacian matrix as global positional encodings for nodes. This method can capture global spectral features (e.g., connectivity, centrality and community structure). Park et al. [41] extended Graphormer by using singular value decomposition (SVD) to encode global structure. They utilized the left singular matrix of the adjacency matrix as global positional encodings for nodes. This approach can handle both symmetric and asymmetric matrices.\nGlobal edge structural encodings excel at capturing coarse-grained structural information at the graph level, benefiting tasks that require global understanding. However, they may struggle with capturing fine-grained node-level information and can lose data or introduce noise during encoding. In addition, their effectiveness may depend on the choice of encoding technique and matrix representations.\n3) Message-passing Bias: Message-passing bias is a crucial inductive bias for graph transformers to facilitate learning from the local structure of graphs [61]. This bias enables graph transformers to exchange information between nodes and edges, thereby capturing dependencies and interactions among graph elements. Moreover, message-passing bias helps graph transformers in overcoming certain limitations of standard transformer architecture, such as the quadratic complexity of self-attention, the absence of positional information and chal-lenges associated with handling sparse graphs [39]. Formally, message-passing bias can be expressed as follows:\n\\( h_v^{(t+1)} = f(h_v^{(t)}, \\{h_u^{(t)} : u \\in \\mathcal{N}(v)\\}, \\{e_{uv} : u \\in \\mathcal{N}(v)\\}). \\) (15)\nHere, \\( h_v^{(t)} \\) is the representation of node v at layer t, u is a neighboring node of node v, euv is the feature of the edge between nodes v and u, and f is the function that aggregates information from neighbors and edges. Incorporation of message-passing bias in graph transformers can be achieved through three primary approaches: preprocessing, interleaving and post-processing . These approaches vary in how they combine message-passing operations with self-attention layers within transformer architecture.\nPreprocessing. Preprocessing involves applying message-passing operations to node features before feeding them to self-attention layers [36]. This technique aims to augment node features with local structural information, making them more compatible with global self-attention. Preprocessing can maintain the integrity of the original transformer architecture without making any modifications and utilize message-passing modules from existing GNNs. It can be mathematically defined as follows:\n\\( h_v^{(t)} = f(h_v^{(t-1)}, \\{h_u^{(t-1)} : u \\in \\mathcal{N}(v)\\}, \\{e_{uv} : u \\in \\mathcal{N}(v)\\}), \\)\n\\( h_v^{(t+1)} = SelfAttention(h_v^{(t)}, \\{h_u^{(t)} : u \\in V\\}). \\) (16)\nHere, \\( h_v^{(0)} = x_v \\), xv is input feature of node v, SelfAttention is a function that performs self-attention over all nodes.\nThe limitation of preprocessing is that it applies message-passing only once before self-attention layers. This approach may not fully capture intricate interactions between nodes and edges on various scales. Additionally, the preprocessing step may introduce redundancy and inconsistency between the message-passing module and the self-attention layer as they both serve similar functions of aggregating information from neighboring elements.\nInterleaving. Interleaving refers to the technique employed in graph transformer architecture that involves alternating message-passing operations and self-attention layers [16], [62], [63]. The objective of this technique is to achieve a balance between local and global information processing, thereby enabling multi-hop reasoning over graphs. By integrat-ing message-passing modules into core components of graph transformers, interleaving enhances their expressive power and flexibility. It can be mathematically defined as follows:\n\\( h_v^{(t+1)} = \\Theta + SelfAttention(h_v^{(t)}, \\{h_u^{(t)} : u \\in V\\}), \\)\n\\( \\Theta = f(h_v^{(t)}, \\{h_u^{(t)} : u \\in \\mathcal{N}(v)\\}, \\{e_{uv} : u \\in \\mathcal{N}(v)\\}). \\) (17)\nOne drawback of the interleaving technique is its impact on the complexity and computational requirements of graph transformers. This is due to the need for additional parameters and operations compared to pre-processing or post-processing. Furthermore, interleaving can potentially lead to interference and conflict between the message-passing module and the self-attention layer as they each update node representations in distinct manners.\nPost-processing. Post-processing refers to the technique of applying message-passing operations to node representations obtained from self-attention layers [58], [64]. The purpose of this approach is to refine and adjust node representations based on underlying graph structure, thereby enhancing their interpretability and robustness. By doing so, this method aims to improve the quality and utility of node representations for downstream tasks and applications. It can be mathematically defined as follows:\n\\( h_v^{(t+1)} = SelfAttention(h_v^{(t)}, \\{h_u^{(t)} : u \\in V\\}), \\)\n\\( h_v^{(T+1)} = f(h_v^{(T)}, \\{h_u^{(T)} : u \\in \\mathcal{N}(v)\\}, \\{e_{uv} : u \\in \\mathcal{N}(v)\\}). \\) (18)\nHere, T is the final layer of the graph transformer.\nThe drawback of post-processing is its limited application of message-passing after self-attention layers, potentially failing to capture intricate semantics and dynamics of graph data. Additionally, post-processing runs the risk of introducing noise and distortion to node representations as it has the potential to overwrite or override information acquired by self-attention layers.\n4) Attention Bias: Attention bias enables graph transform-ers to effectively incorporate graph structure information into the attention mechanism without message-passing or posi-tional encodings [39], [65]. Attention bias modifies attention scores between nodes based on their relative positions or distances in the graph. It can be categorized as either local or global depending on whether it focuses on the local neighborhood or global topology of the graph.\nLocal Attention Bias. Local attention bias limits the atten-tion to a local neighborhood surrounding each node. This con-cept is analogous to the message-passing mechanism observed in GNNs [66]. It can be mathematically defined as follows:\n\\( A_{ij} = \\frac{exp(g(x_i, x_j). b_{ij})}{\\sum_{k \\in \\mathcal{N}(v_i)} exp(g(x_i, x_k) \\cdot b_{ik})}, \\) (19)\nwhere aij is the attention score between node vi and node vj, xi and xj are their node features, g is a function that computes the similarity between two nodes, such as dot-product and linear transformation. bij is a local attention bias term that modifies attention score based on the distance between node vi and node vj. The local attention bias term can be either a binary mask that only allows attention within a certain hop distance [36], [42], [67] or a decay function that decreases attention score with increasing distance [64], [68].\nGlobal Attention Bias. Global attention bias integrates global topology information into the attention mechanism independent of message-passing and positional encodings [69]. It can be mathematically defined as follows:\n\\( A_{ij} = \\frac{exp(g(x_i, x_j) + c(A, D, L, P)_{ij})}{\\sum_{k=1}^{N} exp(g(x_i, x_k) + C(A, D, L, P)_{ik})}, \\) (20)\nHere, c is the function that computes the global attention bias term, modifying attention score based on some graph-specific matrices or vectors, such as adjacency matrix A, degree matrix D, Laplacian matrix L, and PageRank vector P. The global attention bias term can be additive or multiplicative to the similarity function [46], [70]. Generally, global attention bias can enhance the global structure awareness and expressive power of graph transformers [70].\nB. Graph Attention Mechanisms\nGraph attention mechanisms play an important role in the construction of graph transformers [71]. By dynamically as-signing varying weights to nodes and edges in the graph, these mechanisms enable transformers to prioritize and emphasize the most relevant and important elements for a given task [72]. Specifically, a graph attention mechanism is a function that maps each node vi \u2208 V to a vector hi \u2208 Rdk :\n\\( h_i = f_n(x_i, \\{x_j\\}_{v_j\\in\\mathcal{N}(v_i)}, \\{A_{ij}\\}_{v_j\\in\\mathcal{N}(v_i)}), \\) (21)\nwhere fn is a nonlinear transformation, xi is the input feature vector of node vi, xj is the input feature vector of node vj. The function fn can be decomposed into two parts: an attention function and an aggregation function.\nThe attention function computes a scalar weight for each neighbor of node vi, denoted by aij, which reflects the importance or relevance of node vj for node vi:\n\\( a_{ij} = softmax (LeakyReLU(W_a[x_i||x_j])), \\) (22)\nwhere Wa \u2208 R1\u00d72dn is a learnable weight matrix, || is the concatenation operation, and softmax\u00bfnormalizes weights over all neighbors of node vi. The aggregation function combines the weighted features of the neighbors to obtain the output representation of node vi:\n\\( h_i = W_h x_i + \\sum_{u_j \\in \\mathcal{N}(v_i)} A_{ij}x_j \\) (23)\nwhere Wh is another learnable weight matrix. Graph attention mechanisms have the potential to extend their application beyond nodes to edges. This can be achieved by substitut-ing nodes with edges and utilizing edge features instead of node features [73]. Furthermore, stacking of graph attention mechanisms in multiple layers allows for the utilization of output representations from one layer as input features for the subsequent layer [74].\n1) Global Attention Mechanisms: Global attention mecha-nisms can determine how each node calculates its attention weights across all other nodes in the graph [75]. Global attention mechanisms can be broadly categorized into two types: quadratic attention mechanisms and linear attention mechanisms.\nQuadratic Attention Mechanisms. Quadratic attention mech-anisms are derived from the conventional self-attention for-mula. This formula calculates attention weights by applying a softmax function to scale the dot product between the query and key vectors of each node:\n\\( A_{ij} = \\frac{exp(\\frac{q_i^T k_j}{\\sqrt{d_k}})}{\\sum_{n=1}^{N} exp(\\frac{q_i^T k_n}{\\sqrt{d_k}})}. \\) (24)\nThe computational complexity of this method is O(N2).\nOne of the pioneering studies that has introduced quadratic attention mechanisms for graph transformers is the work by Velickovic et al. [42]. The authors proposed the use of multiple attention heads to capture different types of rela-tions between nodes. Choromanski et al. [76] introduced the Graph Kernel Attention Transformer (GKAT), an approach that integrates graph kernels, structural priors, and efficient transformer architectures. GKAT leverages graph kernels as positional encodings to capture the structural characteristics of the graph while employing low-rank decomposition techniques to minimize the memory requirements of quadratic attention:\n\\( A_{ij} = \\frac{exp(\\frac{z_i^T + p_i p_j}{\\sqrt{d_k}})}{\\sum_{n=1}^{N} exp(\\frac{z_i^T + p_i p_n}{\\sqrt{d_k}})}, \\)\n\\( z_i = \\sum_{j=1}^{N} \\alpha_{ij} (UV^T)_{ij} \\) (25)\nwhere zi is the output vector of node vi, pi is the positional encoding vector of node vi, which is computed by applying a graph kernel function to node features. U and V are low-rank matrices that approximate value matrix in the GKAT model. GKAT also introduced a novel kernel-based masking scheme to control the sparsity of the attention matrix. Yun et al. [36] introduced Graph Transformer Networks (GTN), an approach that uses multiple layers of quadratic attention to acquire hierarchical representations of graphs. GTN further enhances attention computation by incorporating edge features through the use of bilinear transformations instead of concate-nation. This innovative technique demonstrates the potential for improved graph representation learning in the field of graph analysis.\nQuadratic attention mechanisms possess a significant advan-tage in capturing extensive dependencies and global informa-tion within graphs, thereby offering potential benefits for vari-ous graph learning tasks. Nonetheless, these mechanisms have some limitations, including their computationally expensive nature, high memory consumption, and susceptibility to noise and outliers. Additionally, quadratic attention mechanisms may not effectively preserve and leverage local information within graphs which holds significance in certain domains and tasks.\nLinear Attention Mechanisms. Linear attention mechanisms employ different approximations and hashing techniques to decrease the computational complexity of self-attention from O(N2) to O(N) [77]. These mechanisms can be categorized into two subtypes: kernel-based linear attention mechanisms and locality-sensitive linear attention mechanisms.\nKernel-based linear attention mechanisms leverage the con-cept of employing kernel functions to map query and key vectors into a feature space that allows efficient computation of their inner products [78], [79]. In the work by Katharopoulos et al. [80], Linear Transformer Networks (LTN) were introduced. LTN utilizes random Fourier features as kernel functions to approximate self-attention:\n\\( A_{ij} = \\frac{exp(\\frac{\\phi(q_i)^T \\phi(k_j)}{\\sqrt{d_k}})}{\\sum_{n=1}^{N} exp(\\frac{\\phi(q_i)^T \\phi(k_n)}{\\sqrt{d_k}})}, \\)\n\\( \\phi(x) = \\frac{1}{\\sqrt{m}} [cos(\\omega_1^T x + b_1),..., cos(\\omega_m^Tx + b_m)]^T. \\) (26)\nHere, \u03c6 is a random Fourier feature function that maps input vector into a m-dimensional feature space. \u03c9i and bi are randomly sampled from a Gaussian distribution and a uniform distribution, respectively.\nLocality-sensitive linear attention mechanisms are based on the concept of employing hashing functions to partition the query and key vectors into discrete buckets that facilitate local computation of their inner products [81]\u2013[83]. Kitaev et al. [84] introduced the Reformer model which uses locality-sensitive hashing (LSH) as a hashing function to approximate self-attention:\n\\( A_{ij} = \\frac{exp(\\frac{q_i^T k_j}{\\sqrt{d_k}})}{\\sum_{n \\in B_j} exp(\\frac{q_i^T k_n}{\\sqrt{d_k}})}, \\)\n\\( B_i = \\{j | h(q_i) = h(k_i)\\}, \\) (27)\nwhere h is an LSH function that maps the input vector into a bucket index and Bi is a set of nodes that share the same bucket index with node vi. RGN also incorporated edge features into the attention computation using a shared query-key projection matrix.\nLinear attention mechanisms offer a significant advantage in terms of reducing computational cost and memory us-age in self-attention, thereby enabling graph transformers to efficiently process large-scale graphs. However, they do come with certain limitations, including approximation error, hashing collision and loss of global information. Additionally, linear attention mechanisms may not be capable of capturing intricate or nonlinear relationships between nodes.\n2) Local Attention Mechanisms: Local attention mecha-nisms determine how each node computes its attention weights over a subset of nodes in the graph [85]. Local attention mech-anisms can be broadly categorized into two types: message-passing attention mechanisms and spectral attention mecha-nisms.\nMessage-passing Attention Mechanisms. Message-passing attention mechanisms are built on the foundational framework of message-passing neural networks (MPNNs) [86], which iteratively aggregate messages from neighboring nodes to compute node representations. To enhance MPNNs, message-passing attention mechanisms employ self-attention to com-pute messages or aggregation weights [39], [61], [87], [88]. The computational complexity of this method is O(E), where E represents the number of edges in the graph.\nOne of the pioneering works in introducing message-passing attention mechanisms for graph transformers is Graph-SAGE [89]. GraphSAGE proposes the use of mean pooling as an aggregation function and self-attention as a combination function. Additionally, GraphSAGE incorporates node sam-pling techniques to efficiently handle large-scale graphs. The function can be mathematically defined as:\n\\( h_v^{(l+1)} = ReLU(W^{(l)}[h_v^{(l)} ||MEAN\\left(\\left\\{h_u^{(l)}\\}, \\forall v_j \\in \\mathcal{N}(v)\\right\\}\\right)]) \\)\n\\( z_i = \\sum_{l=0}^{(L)} a_{il}h_i^{(l)}. \\) (28)\nHere, \\( h_i^{(l)} \\) is hidden vector of node vi at layer l, W(l) is a linear transformation matrix, || is the concatenation operator, MEAN indicates a mean pooling function. ait means an attention weight between node vi and layer l. More recently, Javaloy et al. [90] proposed L-CAT which uses self-attention as both an aggregation function and a combination function. L-CAT also incorporates edge features into the attention com-putation by using bilinear transformations. Message-passing attention mechanisms, while adept at preserving and lever-aging local graph information, are constrained by scalability issues, such as limited expressiveness and graph connectivity dependence. Their capacity to capture long-range dependen-cies and global graph information is also questionable.\nSpectral Attention Mechanisms. Spectral attention mech-anisms are founded on the concept of transforming node features into a spectral domain, where the graph structure is encoded by eigenvalues and eigenvectors of the graph Laplacian matrix [91]. Spectral attention mechanisms employ self-attention to calculate spectral coefficients and spectral filters [92]. The computational complexity of this approach is O(N).\nWang et al. [22] proposed graph isomorphism networks (GIN) as a spectral attention mechanism for graph transform-ers, employing an expressive aggregation function of sum pooling and self-attention. GIN also integrates a unique graph readout scheme via set functions to encapsulate global graph characteristics. The equations of GIN are as follows:\n\\( h_v^{(l+1)} = MLP^{(l)} ( (1+\\epsilon^{(l)}) \\cdot h_v^{(l)} + \\sum_{j \\in \\mathcal{N}(v)}h_j^{(l)} ), \\)\n\\( z_i = \\sum_{l=0}^{(L)} a_{il}h_i^{(l)}, \\)\n\\( z_G = READOUT(\\{z_i, \\forall v_i \\in V\\}). \\) (29)\nHere, MLP(l) is a multi-layer perceptron, \\( \\epsilon^{(l)} \\) is a learnable parameter, READOUT indicates a set function that aggregates node output vectors into a graph output vector. Zg is the output vector of graph G. In addition, Nguyen et al. [93] introduced UGformer, a self-attention-based method for the spectral coefficient computation of each node using eigenval-ues and eigenvectors of the Laplacian graph matrix. UGformer further integrates edge features into spectral computation via bilinear transformations.\nSpectral attention mechanisms possess the ability to incor-porate the structural information of a graph into the spectral domain, thereby offering potential benefits for certain tasks or domains. However, they are also accompanied by certain limitations, including high computational cost, memory con-sumption and sensitivity to graph size and density."}, {"title": "IV. TAXONOMY OF GRAPH TRANSFORMERS", "content": "The past few years have witnessed a surge of interest in graph transformers. This section dives into four key categories dominating the current literature: shallow, deep, scalable, and pre-trained graph transformers. By analyzing representative models within each category, we aim to establish valuable guidelines for designing effective graph transformers.\nA. Shallow Graph Transformers\nShallow graph transformers represent a class of GNNs that leverage the power of self-attention to acquire node representations from data structured in graphs. Inspired by transformer models, which effectively capture long-range de-pendencies in sequential data through self-attention, shallow graph transformers extend this concept to graph data by com-puting self-attention weights based on both node features and graph topology [94]. The primary objective of shallow graph transformers is to achieve exceptional performance while minimizing computational complexity and memory usage.\nShallow graph transformers can be seen as a generalization of graph attention networks (GAT) [42]. GAT use a multi-head attention mechanism to calculate node embeddings. However, GAT has some limitations, such as the inability to model edge features and lack of diversity among attention heads [95]. Several GAT extensions have been proposed in the literature to address these issues. For example, GTN by Yun et al. [36] introduces edge-wise self-attention to incorporate edge information into node embeddings. Ahmad et al. [63] proposed the graph attention transformer encoder (GATE), which applies a masked self-attention mechanism to learn different attention patterns for different nodes. GATE also uses a position-wise feed-forward network and dropout to enhance model capacity and generalization. The summary of shallow graph transformer methods is given in Table II.\nShallow graph transformers are efficient and adaptable capa-ble of handling various graph learning tasks and different types of graphs, but their lack of depth and recurrence may limit their ability to capture complex dependencies. Their performance can also be influenced by the choice of mask matrix and the number of attention heads, indicating a need for further research on their optimal design and regularization.\nB. Deep Graph Transformers\nDeep graph transformers consist of multiple self-attention layers stacked on top of each other, with optional skip con-nections, residual connections or dense connections between layers [102]. They are designed to achieve higher performance with increased model depth and complexity [103]. Deep graph transformers extend shallow graph transformers by applying self-attention layers to node features and graph topology hierarchically.\nHowever, deep graph transformers also face several chal-lenges that need to be addressed. One challenge is the dif-ficulty of training deeper models, which can be mitigated by employing techniques, such as PairNorm introduced in DeeperGCN [104]. Another challenge is the over-smoothing problem, which can be addressed by using a gated residual connection and a generalized convolution operator as proposed in DeeperGCN. Additionally, the disappearance of global attention capacity and the lack of diversity among attention heads are challenges that can be tackled by approaches like DeepGraph [94]. DeepGraph incorporates substructure tokens and local attention to improve the focus and diversity of global attention.\nDeep graph transformers, while complex, can achieve top-tier results on various graph learning tasks and adapt to different types of graphs and domains. However, their high computational cost, difficulty in optimization, and sensitivity to hyperparameters necessitate further research for optimal design and training.\nC. Scalable Graph Transformers\nScalable graph transformers are a category of graph trans-formers that tackle the challenges of scalability and efficiency when applying self-attention to large-scale graphs [39], [53], [114], [115]. These transformers are specifically designed to reduce computational cost and memory usage while main-taining or improving performance. To achieve this, various techniques are employed to reduce the complexity of self-attention, such as sparse attention, local attention, and low-rank approximation [12], [115]. Scalable graph transformers can be regarded as an enhancement of deep graph transformers addressing challenges, such as over-smoothing and limited capacity of global attention.\nSeveral scalable graph transformer models have been pro-posed to enhance the scalability and efficiency of graph transformers. For instance, Ramp\u00e1\u0161ek et al. [39] introduced GPS, use low-rank matrix approximations to reduce com-putational complexity, and achieve state-of-the-art results on diverse benchmarks. GPS decouples local real-edge aggre-gation from a fully-connected transformer and incorporates different positional and structural encodings to capture graph topology. It also offers a modular framework that supports multiple encoding types and mechanisms for local and global attention. Cong et al. [116] developed DyFormer, a dynamic graph transformer that utilizes substructure tokens and local attention to enhance the focus and diversity of global attention. DyFormer employs a temporal union graph structure and a subgraph-based node sampling strategy for efficient and scalable training.\nScalable graph transformers are an innovative and efficient category of graph transformers that excel in handling large-scale graphs while minimizing computational cost and mem-ory usage. However, scalable graph transformers face certain limitations, including the trade-off between scalability and expressiveness, the challenge of selecting optimal hyperpa-rameters and encodings, and the absence of theoretical anal-ysis regarding their convergence and stability. Consequently, further investigation is required to explore optimal designs and evaluations of scalable graph transformers for various applications.\nD. Pre-trained Graph Transformers\nPre-trained graph transformers utilize large-scale unlabeled graphs to acquire transferable node embeddings [135]. These embeddings can be fine-tuned for downstream tasks with scarce labeled data that address the challenges of data scarcity and domain adaptation in graph learning tasks [136], [137]. These transformers are similar to pre-trained large language models (LLMs) and are trained on graph datasets using self-supervised learning objectives, such as masked node prediction [138], edge reconstruction [139], and graph contrastive learn-ing [140]. These objectives aim to encapsulate the inherent properties of graph data independently of external labels or supervision [141]. The pre-trained model can be fine-tuned on a specific downstream task with a smaller or domain-specific graph dataset by incorporating a task-specific layer or loss function and optimizing it on labeled data [142]. This allows the pre-trained model to transfer the knowledge acquired from the large-scale graph dataset to the subsequent task, giving better performance compared to the training from scratch [142].\nPre-trained graph transformers face some challenges, such as the selection of appropriate pre-training tasks, domain knowledge incorporation, heterogeneous information integra-tion, and pre-training quality evaluation [143]. To address these issues, KPGT [144] and KGTransformer [145] have been proposed. KPGT leverages additional domain knowledge for pre-training, while KGTransformer serves as a uniform Knowledge Representation and Fusion (KRF) module in di-verse tasks. Despite their power and flexibility, pre-trained graph transformers encounter issues related to graph data heterogeneity and sparsity, domain adaptation, model gener-alization and performance interpretation.\nE. Design Guide for Effective Graph Transformers\nDeveloping effective graph transformers requires meticulous attention to detail and careful consideration. This guide pro-vides general principles and tips for designing graph trans-formers for various scenarios and tasks.\n\u2022 Choose the appropriate type of graph transformers based on the nature and complexity of your graph data and tasks. For simple and small graph data, a shallow graph transformer with a few layers may suffice. For complex and large graph data, a deep graph transformer with many layers can learn more expressive representations. For dynamic or streaming graph data, a scalable graph transformer is more efficient. Pre-trained graph trans-formers are more suitable for sparse or noisy graph data.\n\u2022 Design suitable structural and positional encodings for your graph data. These encodings capture the structure of graphs and are added to input node or edge features before feeding them to transformer layers. The choice of encodings depends on the characteristics of the graph data, such as directionality, weight, and homogeneity. The careful design of these encodings ensures their informa-tiveness.\n\u2022 Optimize the self-attention mechanism for your graph data. Self-attention mechanisms compute attention scores among all pairs of nodes or edges in the graph, captur-ing long-range dependencies and interactions. However, it introduces challenges like computational complexity, memory consumption, overfitting, over-smoothing, and over-squashing. Techniques like sampling, sparsification, partitioning, hashing, masking, regularization, and nor-malization can be employed to address these challenges and improve the quality and efficiency of the self-attention mechanism.\n\u2022 Utilize pre-training techniques to enhance the perfor-mance of graph transformers. Pre-training techniques leverage pre-trained models or data from other domains or tasks transferring knowledge or parameters to a specific graph learning task. Methods like fine-tuning, distillation, and adaptation can be used to adapt pre-trained models or data. Utilizing pre-training techniques is particularly beneficial when a large amount of pre-training data or resources are available."}, {"title": "V. APPLICATION PERSPECTIVES OF GRAPH TRANSFORMERS", "content": "Graph transformers are finding applications in various do-mains that involve interconnected data. This section delves into their applications for graph-related tasks, categorized by the level of analysis: node-level, edge-level and graph-level. Beyond these core tasks, graph transformers are also making strides in applications that handle text, images, and videos, where data can be effectively represented as graphs for analysis.\nA. Node-level Tasks\nNode-level tasks involve the acquisition of node represen-tations or the prediction of node attributes using the graph structure and node features [153].\n1) Protein Structure Prediction: In the field of bioin-formatics, graph transformers have demonstrated substantial potential in Protein Structure Prediction (PSP) [154]. Gu et al. [155] introduced HEAL, which employs hierarchical graph transformers on super-nodes that imitate functional motifs to interact with nodes in the protein graph, effectively capturing structural semantics. Pepe et al. [156] used Geometric Algebra (GA) modelling to introduce a new metric based on the relative orientations of amino acid residues, serving as an additional input feature to a graph transformer, assisting in the prediction of the 3D coordinates of a protein. Chen et al. [157] proposed gated-graph transformers integrating node and edge gates within a graph transformer framework to regulate information flow during graph message passing, proving beneficial in pre-dicting the quality of 3D protein complex structures. Despite the encouraging outcomes, various challenges persist, such as the complexity of protein structures, scarcity of high-quality training data, and substantial computational resource [158]. Further investigation is necessary to address these challenges and enhance the precision of these models.\n2) Entity Resolution: Entity Resolution (ER) is a crucial task in data management that aims to identify and link disparate representations of real-world entities from diverse sources [159]. Recent research has highlighted the efficacy of graph transformers in ER. For example, Yao et al. [160] proposed Hierarchical Graph Attention Networks (HierGAT) integrating the self-attention mechanism and graph attention network mechanism to capture and leverage the relationships between different ER decisions, leading to substantial en-hancements over conventional approaches. Ying et al. [161] extended the standard transformer architecture and introduced several straightforward yet powerful structural encoding tech-niques to enhance the modeling of graph-structured data. Despite facing challenges related to data complexity and structural information encoding, these techniques have exhibited promising outcomes in terms of enhanced performance, scal-ability, and accuracy. Furthermore, Dou et al. [162] proposed the Hybrid Matching Knowledge for Entity Matching (GTA) method improves the transformer for representing relational data by integrating additional hybrid matching knowledge acquired through graph contrastive learning on a specially designed hybrid matching graph. This approach has also demonstrated promising results by effectively boosting the transformer for representing relational data and surpassing existing entity matching frameworks.\n3) Anomaly Detection: Graph transformers are valuable tools for anomaly detection, especially in dynamic graphs and time series data [65], [163]. They tackle key challenges like encoding information for unattributed nodes and extract-ing discriminative knowledge from spatial-temporal dynamic graphs. Liu et al [164] proposed TADDY, a transformer-based Anomaly Detection framework, enhancing node encod-ing to represent each node's structural and temporal roles in evolving graph streams. Similarly, Xu et al. [165] proposed the Anomaly Transformer which uses an Anomaly-Attention mechanism to measure association discrepancy and employs a minimax strategy to enhance normal-abnormal differentiation. Chen et al. [166] proposed the GTA framework for multivariate time series anomaly detection incorporates graph structure learning, graph convolution, and temporal dependency mod-eling with a transformer-based architecture. Tuli et al. [167] developed TranAD, a deep transformer network for anomaly detection in multivariate time series data, showing efficient anomaly detection and diagnosis in modern industrial appli-cations. Despite their effectiveness, further research is needed to enhance their performance and applicability across different domains.\nB. Edge-level Tasks\nEdge-level tasks aim to learn edge representations or predict edge attributes based on graph structure and node features [168], [169].\n1) Drug-Drug Interaction Prediction: Graph transformers have been increasingly employed in the prediction of Drug-Drug Interactions (DDIs) owing to their capability to adeptly model the intricate relationships between drugs and targets [170]. Wang et al. [109] proposed a method which uses a line graph with drug-protein pairs as vertices and a graph transformer network (DTI-GTN) for the purpose of forecast-ing drug-target interactions. Djeddi et al. [171] proposed a novel approach named DTIOG for the prediction of DTIs, leveraging a Knowledge Graph Embedding (KGE) strategy and integrating contextual information derived from protein se-quences. Despite the encouraging outcomes, these approaches encounter challenges such as overlooking certain facets of the intermolecular information and identifying potential interac-tions for newly discovered drugs [172]. Nevertheless, findings from multiple studies indicate that graph transformers can proficiently anticipate DDIs and surpass the performance of existing algorithms [173].\n2) Knowledge Graph Completion: In the domain of Knowl-edge Graph (KG) completion, the utilization of graph trans-formers has been extensively investigated [174]. Chen et al. [175] proposed a novel inductive KG representation model, known as iHT, for KG completion through large-scale pre-training. This model comprises an entity encoder and a neighbor-aware relational scoring function, both parameterized by transformers. The application of this approach has led to remarkable advancements in performance, with a relative enhancement of more than 25% in mean reciprocal rank compared to previous models. Liu et al. [176] introduced a generative transformer with knowledge-guided decoding for academic KG completion, which incorporates pertinent knowledge from the training corpus to provide guidance. Chen et al. [177] developed a hybrid transformer with multi-level fusion to tackle challenges in multimodal KG completion tasks. This model integrates visual and textual representations through coarse-grained prefix-guided interaction and fine-grained correlation-aware fusion modules.\n3) Recommender Systems: Graph transformers have been effectively utilized in recommender systems by combining generative self-supervised learning with a graph transformer architecture [178]. Xia et al. [179] used the generative self-supervised learning method to extract representations from the data in an unsupervised manner and utilized graph trans-former architecture to capture intricate relationships between users and items in the recommendation system. Li et al. [180] introduced a new method for recommender systems that leverages graph transformers (GFormer). Their approach automates the self-supervision augmentation process through a technique called rationale-aware generative self-supervised learning. This technique identifies informative patterns in user-item interactions. The proposed recommender system utilizes a special type of collaborative rationale discovery to selectively augment the self-supervision while preserving the overall relationships between users and items. The rationale-aware self-supervised learning in the graph transformer enables graph collaborative filtering. While challenges remain in areas like graph construction, network design, model optimization, computation efficiency, and handling diverse user behaviors, experiments show that the approach consistently outperforms baseline models on various datasets.\nC. Graph-level Tasks\nGraph-level tasks aim to learn graph representations or predict graph attributes based on graph structure and node features.\n1) Molecular Property Prediction: Graph transformers are powerful tools for molecular property prediction, utilizing the graph structure of molecules to capture essential structural and semantic information [181]. Chen et al. [182] proposed Algebraic Graph-Assisted Bidirectional Transformer (AGBT) framework which integrates complementary 3D molecular information into graph invariants, rectifying the oversight of three-dimensional stereochemical information in certain machine learning models. Li et al [144] utilized Knowledge-Guided Pre-training of Graph Transformer (KPGT) in a self-supervised learning framework which emphasizes the impor-tance of chemical bonds and models the structural information of molecular graphs. Buterez et al. [183] proposed transfer learning with graph transformer to enhance molecular property prediction on sparse and costly high-fidelity data.\n2) Graph Clustering: Graph transformers have been in-creasingly utilized in the field of Graph Clustering, offering innovative methodologies and overcoming significant chal-lenges [184]. Yun et al. [185] proposed a graph transformer network to generate new graph structures for identifying useful connections between unconnected nodes on the original graph while learning effective node representation on the new graphs in an end-to-end fashion. Gao et al. [186] proposed a patch graph transformer (PatchGT) that segments a graph into patches based on spectral clustering without any trainable parameters and allows the model to first use GNN layers to learn patch-level representations and then use transformer to obtain graph-level representations. These methodologies have addressed issues such as the limitations of the local attention mechanism and difficulties in learning high-level information, leading to enhanced graph representation, improved model performance, and effective node representation.\n3) Graph Synthesis: Graph transformers have been applied in the field of graph synthesis to improve graph data mining and representation learning. Existing graph transformers with Positional Encodings have limitations in node classification tasks on complex graphs, as they do not fully capture the local node properties [16]. To address this, Ma et al. [187] introduced the Adaptive Graph Transformer (AGT). This model tackles the challenge of extracting structural patterns from graphs in a way that is both effective and efficient. AGT achieves this by learning from two different graph perspectives: centrality and subgraph views. This approach has been shown to achieve state-of-the-art performance on real-world web graphs and synthetic graphs characterized by heterophily and noise. Jiang et al. [188] proposed an Anchor Graph Transformer (AGFormer) that leverages an anchor graph model to perform more efficient and robust node-to-node message passing for overcoming the computational cost and sensitivity to graph noises of regular graph transformers. Zhu et al. [122] developed a Hierarchical Scalable Graph Transformer (HSGT) which scales the transformer architecture to node representation learning tasks on large-scale graphs by utilizing graph hierarchies and sampling-based training methods.\nD. Other Application Scenarios\nGraph transformers have a wide range of applications beyond graph-structured data. They can also be utilized in scenarios involving text, images, or videos. [189].\n1) Text Summarization: Text summarization is a crucial aspect of NLP which has been significantly advanced with the introduction of Graph transformers [190]. These models utilize extractive, abstractive, and hybrid methods for summarization. Extractive summarization selects and extracts key sentences or phrases from the original text to create a summary [191]. In contrast, abstractive summarization interprets the core con-cepts in the text and produces a concise summary. Hybrid summarization combines the advantages of both approaches [192]. Despite the progress, challenges remain in text com-prehension, main idea identification, and coherent summary generation. Nevertheless, the application of graph transformers in text summarization has demonstrated promising outcomes in terms of summary quality and efficiency [193].\n2) Image Captioning: Graph transformers have emerged as a potent tool within the domain of image captioning, offering a structured representation of images and efficiently processing them to produce descriptive captions [194]. Techniques such as Transforming Scene Graphs (TSG) leverage multi-head attention to architect graph neural networks for embedding scene graphs, which encapsulate a myriad of specific knowl-edge to facilitate the generation of words across various parts of speech [195]. Despite encountering challenges, such as training complexity, absence of contextual information, and lack of fine-grained details in the extracted features, graph transformers have exhibited promising outcomes. They have enhanced the quality of generated sentences and attained state-of-the-art performance in image captioning endeavors [196].\n3) Image Generation: Graph transformers have been ef-fectively utilized in image generation, as demonstrated by various research studies [197]. Sortino et al. [33] proposed a transformer-based method conditioned by scene graphs for image generation, employing a decoder to sequentially com-pose images. Zhang et al. [198] proposed StyleSwin which uses transformers in constructing a generative adversarial network for high-resolution image creation. Despite challenges like redundant interactions and the requirement for intricate architectures, these studies have exhibited promising outcomes in terms of image quality and variety [199], [200].\n4) Video Generation: Graph transformers have been ex-tensively applied in the field of Video Generation. Xiao et al. [201] proposed a Video Graph Transformer (VGT) model, which utilizes a dynamic graph transformer module to encode videos, capturing visual objects, their relationships, and dy-namics. It incorporates disentangled video and text transform-ers for comparing relevance between the video and text. Wu et al. [202] proposed The Object-Centric Video Transformer (OCVT) which adopts an object-centric strategy to break down scenes into tokens suitable for a generative video transformer and understanding the intricate spatiotemporal dynamics of multiple interacting objects within a scene. Yan et al. [203] developed VideoGPT, which learns downsampled discrete latent representations of a raw video through 3D convolutions and axial self-attention. A GPT-like architecture is then used to model the discrete latent in a spatiotemporal manner us-ing position encodings. Tulyakov et al. [204] proposed the MoCoGAN model which creates a video by mapping a series of random vectors to a sequence of video frames. Despite the challenges in capturing complex spatio-temporal dynam-ics in videos, these methodologies have exhibited promising outcomes across various facets of video generation, ranging from question answering to video summarization and beyond."}, {"title": "VI. OPEN ISSUES AND FUTURE DIRECTIONS", "content": "Despite their immense potential for learning from graph-structured data, graph transformers still face open issues and challenges that require further exploration. Here we highlight some of these open challenges.\nA. Scalability and Efficiency\nThe scalability and efficiency of graph transformers pose considerable challenges due to their substantial memory and computational requirements especially when employing global attention mechanisms to deal with large-scale graphs [114], [205]. These challenges are further amplified in deep architec-tures which are susceptible to overfitting and over-smoothing. To address these issues, several potential strategies can be proposed:\n1) Developing efficient attention mechanisms, such as lin-ear, sparse and low-rank attention, to reduce the com-plexity and memory usage of graph transformers.\n2) Applying graph sparsification or coarsening techniques to decrease the size and density of graphs while main-taining their crucial structural and semantic information.\n3) Using graph partitioning or sampling methods to divide large graphs into smaller subgraphs or batches for par-allel or sequential processing.\n4) Exploring graph distillation or compression methods to create compact and effective graph transformer models, which are suitable for deployment on resource-limited devices.\n5) Investigating regularization or normalization techniques, such as dropout, graph diffusion, convolution, and graph spectral normalization, to prevent overfitting and over-smoothing in graph transformer models.\nB. Generalization and Robustness\nGraph transformers often face challenges when it comes to generalizing to graphs that they have not encountered before or that fall outside of their usual distribution. This is especially true for graphs that have different sizes, structures, features, and domains. Additionally, graph transformers can be vulnerable to adversarial attacks and noisy inputs, which can result in a decline in performance and the production of misleading results. In order to improve the generalization and robustness of graph transformers, the following strategies could be taken into account:\n1) Developing adaptive and flexible attention mechanisms, such as dynamic attention, span-adaptive attention, and multi-head attention with different scales, to accommo-date varying graphs and tasks.\n2) Applying domain adaptation or transfer learning tech-niques to facilitate learning from multiple source do-mains and transfer the knowledge from source domains to target domains.\n3) Exploring meta-learning or few-shot learning techniques to enable learning from limited data and rapid adaptation to new tasks.\n4) Designing robust and secure attention mechanisms, such as adversarial attention regularization, attention mask-ing, and attention perturbation, to resist adversarial at-tacks and noisy inputs.\n5) Evaluating the uncertainty and reliability of graph trans-former models using probabilistic or Bayesian methods, such as variational inference, Monte Carlo dropout, and deep ensembles.\nC. Interpretability and Explainability\nGraph transformers commonly regarded as black box mod-els present significant challenges in terms of interpretability and explainability. The lack of sufficient justification and evidence for their decisions can undermine their credibility and transparency. To address this issue, several approaches can be considered:\n1) Developing transparent and interpretable attention mech-anisms, such as attention visualization, attention attribu-tion, and attention pruning, to highlight the importance and relevance of different nodes and edges in graphs.\n2) Applying explainable artificial intelligence (XAI) tech-niques, such as saliency maps, influence functions, and counterfactual explanations, to analyze and understand the behaviour and logic of graph transformer models.\n3) Exploring natural language generation techniques, such as template-based generation, neural text generation, and question-answering generation, to produce natural language explanations for outputs or actions of graph transformer models.\n4) Investigating human-in-the-loop methods, such as active learning, interactive learning, and user studies, to in-corporate human guidance in the learning or evaluation process of graph transformer models [31].\nD. Learning on Dynamic Graphs\nGraphs, which are frequently characterized by their dynamic and intricate nature, possess the ability to transform over time as a result of the addition or removal of nodes and edges, as well as the modification of node and edge attributes [206]. Moreover, these graphs may exhibit diverse types and modalities of nodes and edges. In order to empower graph transformers to effectively manage such dynamic graphs, it is advisable to explore the following strategies:\n1) Developing temporal and causal attention mechanisms, such as recurrent, temporal, and causal attention, to capture the temporal and causal evolution of graphs.\n2) Applying continual learning techniques on dynamic graphs to void forgetting previous knowledge and re-training.\n3) Exploring multimodal attention mechanisms, such as image-text, audio-visual, and heterogeneous attention, to integrate multimodal nodes and edges.\n4) Leveraging multi-level and multi-layer attention mecha-nisms, such as node-edge, graph-graph, and hypergraph attention, to aggregate information from different levels and layers of graphs.\nE. Data Quality and Diversity\nGraph transformers require a significant quantity of diverse and high-quality data to achieve effective learning. Never-theless, in the real world, data is frequently limited, noisy, incomplete, imbalanced, and biased. This has a detrimental effect on the performance and fairness of graph transformer models. To mitigate these challenges related to data quality and diversity, several potential strategies can be considered:\n1) Developing noise-tolerant attention mechanisms, such as denoising attention, error correction attention, and self-supervised attention, to filter out data noise and errors.\n2) Applying data augmentation or generation techniques, such as graph augmentation [207], graph completion, and graph generation, to enhance the quantity and vari-ety of data.\n3) Exploring data imbalance and bias mitigation tech-niques, such as data re-sampling, data re-weighting, and data debiasing, to improve data fairness and equity.\n4) Evaluating data quality and diversity using data quality assessment or data diversity analysis methods, such as data quality metrics, data diversity measures, and data quality visualization."}, {"title": "VII. CONCLUSION", "content": "Graph transformers are a novel and powerful class of neural network models, which can effectively encode and process graph-structured data. This survey provides a com-prehensive overview of graph transformers in terms of design perspectives, taxonomy, applications, and open issues. We first discuss how graph transformers incorporate graph inductive bias, including node positional encodings, edge structural encodings, message-passing bias and attention bias, to encode the structural information of graphs. Then, we introduce the design of graph attention mechanisms, including global and local attention mechanisms. Afterwards, a taxonomy of graph transformers is presented. This survey also includes a design guide for effective graph transformers, including the best practices and recommendations for selecting appropriate components and hyperparameters. Moreover, the application scenarios of graph transformers are reviewed based on various graph-related tasks (e.g., node-level, edge-level, and graph-level tasks), as well as tasks in other domains. Lastly, current challenges and future directions of graph transformers are identified. This survey aims to serve as a valuable reference for researchers and practitioners interested in graph transformers and their applications."}]}