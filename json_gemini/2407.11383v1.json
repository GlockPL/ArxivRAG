{"title": "TM-PATHVQA: 90000+ Textless Multilingual Questions for Medical Visual Question Answering", "authors": ["Tonmoy Rajkhowa", "Amartya Roy Chowdhury", "Sankalp Nagaonkar", "Achyut Mani Tripathi", "S R Mahadeva Prasanna"], "abstract": "In healthcare and medical diagnostics, Visual Question Answering (VQA) may emerge as a pivotal tool in scenarios where analysis of intricate medical images becomes critical for accurate diagnoses. Current text-based VQA systems limit their utility in scenarios where hands-free interaction and accessibility are crucial while performing tasks. A speech-based VQA system may provide a better means of interaction where information can be accessed while performing tasks simultaneously. To this end, this work implements a speech-based VQA system by introducing a Textless Multilingual Pathological VQA (TM-PathVQA) dataset, an expansion of the PathVQA dataset, containing spoken questions in English, German & French. This dataset comprises 98,397 multilingual spoken questions and answers based on 5,004 pathological images along with 70 hours of audio. Finally, this work benchmarks and compares TM-PathVQA systems implemented using various combinations of acoustic and visual features.", "sections": [{"title": "1. Introduction", "content": "In the realm of healthcare and medical diagnostics, Visual Question Answering (VQA) [1, 2] may emerge as a pivotal tool for analyzing complex medical images [3]. It may enable healthcare professionals to inquire about specific details within the visuals, fostering a deeper understanding. VQA may bridge complex medical visuals and human interpretation, thereby improving healthcare diagnosis. However, current VQA systems rely on text-based questions [4, 5], limiting their utility in scenarios where hands-free interaction and accessibility are crucial, particularly in healthcare settings. Hence, integrating speech may enhance the user experience by offering a more natural mode of interaction while performing tasks simultaneously [6, 7, 8, 9]. Thus, a speech-based VQA system would allow for hands-free operation where typing might be cumbersome. Hence, with this motivation, this work proposes a spoken VQA system that can accept multilingual spoken queries. The responses could still be displayed in textual form, enabling better perception and documentation for future references.\nThe development of a clinically significant spoken VQA system requires training using a dataset comprising spoken questions along with textual answers based on medical images. Without any such existing dataset, the PathVQA dataset [10] is extended by converting the textual questions into spoken form using a Direct Text-to-Speech Translation (DT2ST) [11] system. Subsequently, this resulted in the creation of a Textless Multilingual Pathological Visual Question Answering (TM-PathVQA) dataset, featuring spoken questions in English, German and French, tailored for this task. Notably, this paper also implements a Multi-Modal Learning (MML) framework to evaluate the efficacy of TM-PathVQA dataset by addressing the \u201cYes or No\u201d and open-ended type questions separately. Furthermore, this work also presents a comparative analysis between the frameworks implemented by incorporating various combinations of audio and image features extracted using various state-of-the-art models.\nDevelopment of VQA systems for the medical domain began with the introduction of the VQA-Med [12] dataset, which contains medical images and questions that require analysis of both visual content and medical context. VQA-RAD [13] incorporates Radiology reports into the VQA task for medical images. However, these two datasets were domain-specific. This led to the creation of PathVQA [10] covering diverse Pathological contents. Hence, these attributes motivated us to extend the PathVQA to contain spoken questions. To the best of our knowledge, this is the first-ever VQA dataset that incorporates speeches to facilitate the development of a spoken VQA system.\nThis paper presents three primary contributions. Firstly, it introduces the first-ever TM-PathVQA dataset featuring spoken questions in four languages viz. English, German, and French. Secondly, it also introduces a novel framework for implementing TM-PathVQA systems. Finally, this paper presents a diverse set of benchmark evaluations on this dataset by comparing TM-PathVQA systems that were implemented using various audio and image features extracted using various state-of-the-art models. The overview of this paper is as follows: Section 2 provides an overview of the TM-PathVQA dataset and its development process. Section 3 describes the details of different Multimodal Learning Frameworks (MML). Section 4 presents the benchmark results and discussions. Finally, Section 5 concludes the paper and suggests potential directions for future research."}, {"title": "2. TM-PathVQA Dataset", "content": "The TM-PathVQA dataset is created by extending the PathVQA dataset. PathVQA is recognized as the most extensive dataset developed for pathological VQA tasks. The textual questions in PathVQA were converted into English, German, and French speeches using the SeamlessM4T multimodal and multilingual AI translation model [11]. This resulted in a dataset containing medical visuals paired with spoken questions in three languages viz. English, German, and French, alongside answers in English text. This led to the inclusion of 98,397 question-answer pairs (32,799 questions for each language \u00d7 3 = 98,397 multilingual questions) derived from 5,004 pathological images along with 70 hours of audio containing spoken questions in those three languages."}, {"title": "3. Experimental Methodology", "content": "This section outlines the implementation of TM-PathVQA system in details."}, {"title": "3.1. Representations for Different Modalities", "content": "Various types of feature representations that were employed to implement the system are as follows:"}, {"title": "3.1.1. Text Representation", "content": "The Language-agnostic BERT Sentence Encoder (LaBSE) [14], a BERT-based model [15], was used to generate embeddings from texts. Inspired by the success of LaBSE, this work utilizes its capabilities to extract features from text-based questions."}, {"title": "3.1.2. Audio Representation", "content": "Audio features were extracted from the raw spoken questions, sampled at 16 kHz, using Wav2Vec2 [16] (having 1920 dimensions) and Hu-BERT [17] (having 768 dimensions) along with 80-dimensional Mel filterbanks having a window length of 400 and a hop length of 160 frames. For Wav2Vec2 features, the XLS-R 128 [18] pre-trained model was employed. Hu-BERT features were extracted from the 11th layer of the encoder and then normalized before feeding into the MML framework. Features were extracted from the last encoder layer using the Whisper large-V3 [19] pre-trained model."}, {"title": "3.1.3. Visual Representation", "content": "Image features were extracted using state-of-the-art models, including Vision Transformer (ViT) [20], ResNet-152 [21], VGG19 [22], and Faster-RCNN [23]. ViT divides the image into overlapping patches and feeds them into a Transformer encoder, utilizing the Attention Mechanism [24] to extract image features. Features were extracted from ResNet-152 from its final layer. VGG19, a Convolutional Neural Network (Conv-2D) [25] with 19 layers, employs smaller filters and strides to reduce memory complexity. Features were extracted from the final layer of VGG19 after max-pooling. Faster RCNN, an enhanced version of RCNN [26], is also utilized for feature extraction. Faster-RCNN incorporates a Regional Proposed Network (RPN) [27] on top of the ConvNet-extracted [28] features to generate object proposals. These proposals were then passed to a classification layer to return bounding boxes."}, {"title": "3.2. MML Framework for TM-PathVQA", "content": "The proposed MML framework comprises three modules: a feature extraction module for processing input images, another module for processing input raw audio signals, and a Transformer [29] encoder responsible for processing these inputs and generating responses. All these modules form the TM-PathVQA architecture. Speech input sequences were down-sampled using a series of stacked 2-dimensional Convolutional (Conv-2D) layers. Each Conv-2D layer had a stride of 4 with 5 channels, resulting in sequence reduction by four times. Then, these audio representations were concatenated with image representations before feeding as input to the Transformer encoder [29].\nThe encoder comprises two layers of Transformer blocks, each consisting of two Multi-Head Attentions [29] with an output dimension of 64. It is then followed by a Multi-Layer Perceptron (MLP) layer with 128 hidden units. The feed-forward block utilizes 256-dimensional inner states, followed by layer-normalization [30]. A dropout [31] rate of 0.2 is applied in the attention block and the MLP layer. The output is then passed to a classification layer represented as \u0177 \u2208 R, where C denotes the number of classes (unique words in the text containing answers). For open-ended questions, the number of classes were 4092, while for \u201cYes/No\u201d type questions, there were only two classes i.e. \u201cYes\u201d or \u201cNo.\u201d The final classification is then performed using the Softmax activation function.\nThe proposed MML TM-PathVQA framework was trained for 100 epochs with a batch size of 64. The model with the lowest validation loss was selected for performance evaluation on the test set. Adam [32] optimizer with a Learning Rate (LR) of 1 \u00d7 10, along with the ReduceLRonPlateau [33] scheduler,"}, {"title": "4. Results & Discussions", "content": "Tables 2, 3, 4 & 5 presents the performance comparison between various MML-based VQA systems. The performance differences against the system that has the best combination are highlighted in blue. From all the tables, Faster RCNN frameworks outperformed other image features for binary and multiclass classification tasks. From Tables 3 & 4, the speech-based VQA systems outperformed their text counterparts for both tasks when the best combinations were considered. For multiclass speech-based systems, all the languages achieved similar performance, with English marginally better. Audio features extracted using Hu-BERT also performed better than other acoustic features. From all these tables, it can be inferred that speech-based TM-PathVQA systems have better potential and utility compared to their text counterparts.\nAccess to the dataset and corresponding code is provided via the following link: https://github.com/\naquorio15/path_vqa.git"}, {"title": "5. Conclusion", "content": "This paper explored the implementation of a speech-based VQA system by introducing the TM-PathVQA dataset. The contributions of this paper include the creation of the first-ever Textless Multi-lingual Pathological VQA dataset in three diverse languages aimed at advancing multi-lingual VQA modeling research. Additionally, this work established baselines for TM-PathVQA systems implemented using various combinations of audio and image features. Speech-based VQA systems, employing Hu-BERT and Faster RCNN, demonstrated superior performance across the three languages compared to text-based systems. Hence, the proposed TM-PathVQA dataset and the"}]}