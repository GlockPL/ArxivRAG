{"title": "Discerning the Chaos: Detecting Adversarial Perturbations while Disentangling Intentional from Unintentional Noises", "authors": ["Anubhooti Jain", "Susim Roy", "Kwanit Gupta", "Mayank Vatsa", "Richa Singh"], "abstract": "Deep learning models, such as those used for face recognition and attribute prediction, are susceptible to manipulations like adversarial noise and unintentional noise, including Gaussian and impulse noise. This paper introduces CIAI, a Class-Independent Adversarial Intent detection network built on a modified vision transformer with detection layers. CIAI employs a novel loss function that combines Maximum Mean Discrepancy and Center Loss to detect both intentional (adversarial attacks) and unintentional noise, regardless of the image class. It is trained in a multi-step fashion. We also introduce the aspect of intent during detection that can act as an added layer of security. We further showcase the performance of our proposed detector on CelebA, CelebA-HQ, LFW, AgeDB, and CIFAR-10 datasets. Our detector is able to detect both intentional (like FGSM, PGD, and DeepFool) and unintentional (like Gaussian and Salt & Pepper noises) perturbations.", "sections": [{"title": "Introduction", "content": "Adversarial Attacks [5, 38] have been a well-posed threat against deep neural networks for a long time now. For different tasks, datasets, and architectures, the attacks are a serious security issue, even when the attacked images appear normal to human eyes. Different attacks have been proposed over the years that can be broadly classified as white-box, gray-box, black-box, and physical adversarial attacks [4, 13, 30]. Several defense techniques have been proposed in order to defend against these attacks, like adversarial training, distillation, and feature squeezing [39, 48]. Some of them are computationally heavy and some are class-dependent as well. One other branch of these defensive techniques lies in detecting the attacked images so they can be caught even before being sent to the model network. While some of these methods have shown impressive accuracies, not many highlight the effect on the models' performance on unseen attacks. Also, some of these methods do not work for attacks like CW [4] or DeepFool [35].\nFurther, there are some noises or corruptions that can be added during image processing, such as blurring or pixelation, and are considered important to increase classifier stability [8, 17, 44]. Another overlooked aspect is that of intent. As shown in Figure 1, we postulate that the noise can be further classified as intended and unintended noises. In the example, the adversarial perturbations, which are intentionally added, and the unintentional noise, such as Gaussian and salt & pepper noises, have a similar effect, that is, changing the attribute label from male to female. Existing research has shown that several unintentional noise additions or corruptions can affect the original decision countering the detection mechanisms in place [15, 18, 34]. Therefore, it is our understanding that we should not only be able to detect the unintended noises, but the approach should be able to disentangle intended adversarial perturbations from unintended noise patterns as well.\nIn this paper, we present a detector network called CIAI"}, {"title": "Related Work", "content": "Attacks and Corruptions: Adding some crafted and imperceptible noise can mislead an image classifier. In that regard, several gradient-based attacks have been proposed in the literature. FGSM (Fast Gradient Sign Method) [47] is one of the first and fastest attacks that showcased classifiers' vulnerability to adversarial attacks. There are white-box attacks wherein the attacker has entire information regarding the model under attack, from parameters to training data, while in the black-box setting, the attacker has no information regarding the target model with the gray-box setting lying somewhere in between. FGSM is a single step $L_\\infty$-distance-based attack. Under similar family falls attacks like PGD [30] which is iterative in nature, BIM [23], RFGSM [43], MIFGSM [10], SINIFGSM [26], and so on. Based on $L_2$-distance, some other attacks proposed are CW [4], DeepFool [35], Auto-Attack [7], and so on. Under $L_0$-distance, some proposed attacks are OnePixel [40], Pixle [37], SparseFool [33], and such. Other than these, there are physical, semantic-based, and patch attacks among others.\nThis varied range of adversarial attacks substantiates the vulnerability of neural networks and also shows how simple boundary manipulation can lead to almost complete failure of the well-trained models. Moreover, these attacks are often imperceptible by humans and can also be easily transferred under the black-box setting [31]. There are also universal attacks that require minimal or no knowledge of the model [49]. The careful formulation of such attacks ensures a grave drop in the model's performance, however, there is another way to reduce the model's performance. It comes in the form of corruption. While, many times, the corruptions happen unknowingly, like compressing images for effective storage, sometimes they can be added deliberately like improving the brightness of the image. In this regard, a benchmark [18] was provided for the ImageNet dataset as ImageNet-C and ImageNet-P including 15 different corruptions at five different severity levels."}, {"title": "Defense and Detection:", "content": "Be it adversarial attacks or corruptions, it is ideal to detect and defend against these modifications [9, 14, 32, 45, 48]. Several defense and detection techniques have been proposed like adversarial training, distillation, and feature squeezing among others. We focus on detection techniques where the goal is to detect the modified images before sending them to the model. It can be done in a white-box setting, that is training the detector on a known attack and evaluating the detector for the specific attack, or in a black-box setting, where a trained detector should be able to detect images for an attack it has not seen before. Nearest neighbors and graph methods have been explored for such techniques [1, 6, 19].\nDistribution-based methods have also been proposed like Local Intrinsic Dimensionality (LID) [29] that uses a Logistic Regression-based detector and Mahalanobis Distance (MD) [24]. MultiLID [28], built on LID, was recently proposed as a white-box detector that shows almost perfect detection when evaluated for binary classification between original and attacked images using non-linear classifiers Some techniques [25, 41] are specifically crafted for out-of-distribution detection. While these techniques can be extended for face datasets, more recently, an adversarial face detection [46] was proposed using self-perturbations with a focus on GAN-based attacks as well. Almost all these techniques perform a binary classification to differentiate between unattacked and attacked images. Our aim is to be able to add another dimension, either in the form of unintentional noises (corruptions) or another family of attacks, to perform more than a binary classification."}, {"title": "Proposed CIAI Network", "content": "The CIAI network is proposed in this section considering adversarial perturbations as well as unintentional noises that can change the predicted attributed from one class to another like one gender label to another in the case of gender prediction task as seen in Figure 1. The inspiration is to be able to divide the modified images based on their distribution space in a way that a similar group of noises, whether seen or unseen, can be detected. We also use vision transformers for the detector with the understanding that they are known to generalize better than the CNNs [50].\nWith the Intent to Attack: Detecting unintentional noises can help understand different adversarial attacks and the way they are designed. These unintentional noises do not affect the original accuracy of classifiers as much as the adversarial noises, but they still have an effect, and correcting the classifiers for such unintentional noises while discarding the intentionally attacked images can make the models more robust. These noises can occur at various stages while procuring the images or processing them including blurring and compression. Several of these unintentional noises are also used as data augmentation techniques, however, the idea here is to be able to detect the noises that lead to misclassification. The intended noises require a classifier to attack the images as they are crafted with the aim to fool the classifier but the unintended noises need no such network. They can be added by modifying the underlying distribution only slightly, like adding Gaussian noise, and still affect the model's performance [3]. We indicate the concept of intent on gender attribution and classification tasks using these unintentional noises. The same can be further used for multi-class classification as well with a group of different occurring noises or using two different families of unintended corruptions."}, {"title": "Proposed CIAI Detection Network", "content": "As shown in Figure 2, we propose a Class-Independent Adversarial Intent (CIAI) detection network. The training process is in two stages. For the first stage, detection training, we use a novel MMD and Center-based loss to train a Vision Transformer initialized with classifier weights trained for the original recognition task. MMD has been disputed to not be aware of adversarial noise, especially the CW attack as reported in [4]. However, [12] uses a deep-kernel-based MMD to show that it is aware of adversarial attacks when a specific kernel is used. We build based on this observation by extracting image embeddings from the trained classifier and building a detection model minimizing the proposed novel loss, $L_{det}$ between randomly initialized centers and image embeddings. For the second stage, detection classification, we use the trained detector for detection classification by freezing all the layers and adding three trainable layers for training.\nWe have a training set containing original images, $D$; a set of training images modified using adversarial attacks, $D^a_i$ where i is the number of adversarial attacks used during training; and a set of training images modified using unintentional noises, $D^u_i$ where i is the number of type of unintentional noises used. For our experiments, we use i = 2, that is, two types of adversarial attacks and two types of unintentional noises, for training the detection network, M. We have a paired example at the end for training where $<I_j, I^a_j, I^a_j, I^u_j, I^u_j>$ is the $j^{th}$ training example and $I_j\\in D, I^a_j\\in D^a_1, I^a_j \\in D^a_2, I^u_j \\in D^u_1, and I^u_j\\in D^u_2$. Five centers are initialized, one for each set of training images, $C_k$ where k = 5 for our experiments with k = 0 for D, k = 1 for $D^a_1$, k = 2 for $D^a_2$, k = 3 for $D^u_1$, and k = 4 for $D^u_2$. Further, for computing the loss, the detector model, M is used to get the embeddings for each batch of images leading to $<E_j, E^a_j, E^a_j, E^u_j, E^u_j>$ where M(I) gives the embedding for I; as $E_j$ and so on for all the other batches."}, {"title": "Original vs Modified Images", "content": "The first loss is formulated based on dividing the original image space and modified image space, that is, dividing D"}, {"title": "Intentionally Modified vs Unintentionally Modified Images", "content": "The next loss term $L_2$ is based on creating a separation between the intentionally modified and unintentionally modified images. The embeddings for different image settings are pulled apart for the detector network M and are formulated using two subterms, $L^{close}_o$ and $L^{far}_o$.\n$L^{close}_o = MMD(C_1, E^a_1) + MMD(C_2, E^a_2) + MMD(C_3, E^u_1) + MMD(C_4, E^u_2)$\n$L^{close}_o$, as presented in Equation 5 is meant for closing the distance between the image batches with their respective centers. That is, the embedding batch from the first adversarially attacked images, $E^a_1$, is pulled close to its respective assigned center, $C_1$ by calculating the MMD value between the two. This is done for each set of embeddings with their respective center. Since this distance needs to be minimized, the term is added as a positive factor in the entire loss formulation.\n$L^{far}_o = MMD(C_1, E^u_1) + MMD(C_2, E^u_2) + MMD(C_3, E^a_1) + MMD(C_4, E^a_2)$\nOn the other hand, $L^{far}_o$ is used for creating distance between the adversarially attacked images and unintentionally modified images. For that, the embeddings from the first unintentionally modified images are pulled towards the center assigned for the first adversarially attacked images and vice-versa. This is further done for the second attack and unintentional noise as well, as depicted in Equation 6. Since the centers are supposed to be pulled far from the embeddings in this formulation, the distance needs to be maximized here.\n$L_2 = \\alpha \\times L^{close}_o - (1-\\alpha) \\times L^{far}_o$\nThe two terms $L^{close}_o$ and $L^{far}_o$ are then combined as shown in Equation 7 with $\\alpha$ as the regularization factor. This term can not only be used to divide the intended and unintended noises apart but also can be used between two families of intended attacks or two families of unintended attacks as shown in experiments in the coming sections."}, {"title": "Groups within Modified Images", "content": "The third loss term, $L_3$ is used to create a separation within the subgroups of intentionally and unintentionally modified images. Since we use two types of noises in each group, a separation is created between these two noises. If we use FGSM and PGD attacks for the intentionally modified group, $L_3$ loss is used to pull the two groups away from each other. They can be further modified or removed depending on the number of noises used in each group. The loss is formulated using two subterms, $L^{close}_g$ and $L^{far}_g$.\n$L^{close}_g$, is formulated with the aim to bring the embedding of different image batches closer to their respective centers. Mathematically, the equation is the same as $L^{close}_o$ as seen in Equation 5.\n$L^{far}_g = MMD(C_1, E^a_j) + MMD(C_2, E^a_j) + MMD(C_3, E^u_j) + MMD(C_4, E^u_j)$\n$L^{far}_g$, as shown in Equation 8, is formulated to maximize the distance between embeddings from one attack from the center of another attack within the same group. That is, the images from the first adversarial attack are pulled away from the center dedicated to the second adversarial attack, and vice-versa.\n$L_3 = \\alpha \\times L^{close}_g - (1-\\alpha) \\times L^{far}_g$\nFinally, $L_3$ is combined using the two subterms as shown in Equation 9 with $\\alpha$ as the regularization term. All three loss terms together form the complete loss used to train the detector network, M."}, {"title": "Detector Network", "content": "The detector network, M outputs an embedding and is trained using the detection loss $L_{det}$. The network is similar to the original classifier in that it is initialized with the weights of the trained classifier. The network is then modified by changing the last classifier layer to get the embeddings with dimension d seen as stage 1 of detection training.\n$L_{det} = \\beta \\times L_1 + \\gamma \\times L_2 + \\delta \\times L_3$\nThe formulation for $L_{det}$ is depicted in Equation 10, where $\\beta + \\gamma + \\delta = 1$. The CIAI network, once trained, is then modified by adding 3 linear layers to train for 2-class, 3-class, or 5-class detection as depicted in Figure 2 seen as stage 2 of detection training. For the 2-class detector, the detection is between original and modified images; for the 3-class detector, the detection is between the original, modified with attacks, and modified with unintentional noises. The 5-class detector is for detection between original images, two types of adversarial attacks, and two types of unintentional attacks."}, {"title": "Experiments", "content": "For detection, the results are shown and discussed for different face datasets and the CIFAR dataset [22] (see supplementary), where classifiers are trained for attribute prediction and classification tasks, respectively. For the experiments, we first train the classifier for original tasks and then use them to train the proposed CIAI detection network. We use additional datasets to see how the CIAI detector performs across different face datasets. We then present the results for detection along with the tSNE plots with an attention map analysis."}, {"title": "Experimental Setting and Implementation Details", "content": "To showcase the workings of the proposed approach, we have used two case studies: (a) gender prediction using face images and (b) a standard image recognition task. CelebA [27], CelebA-HQ [21], LFW [20], AgeDB [36], and CIFAR-10 [22] datasets are used for the case studies here."}, {"title": "Gender Prediction using CelebA and LFW Dataset:", "content": "The CelebA dataset contains 162,770 training images, 20,367 validation images, and 19,962 testing images. The gender prediction is done between the two reported genders, male and female. Each image is resized to 3 \u00d7 224 \u00d7 224 for the transformer input. The classifier for attribute prediction is trained for 5 epochs on the entire training set with a learning rate of le-4, giving a final classification value of 99.58%. The CIAI network is trained with an embedding dimension, d = 128. It is initialized with the weights from the trained classifier. CIAIcel is first trained for 3 epochs with a learning rate of le - 4. Further, the linear layers are added to the detection network, and with frozen layers for the entire network except for the linear layers, the network is trained for 2-class and 3-class classification for another 3 epochs at a learning rate of le - 4. Further, we also train the CIAI detector for the LFW dataset which contains 13,233 images of 5749 people. For the gender labels, we refer to labels provided by Afifi and Abdelhamed [2] and train the classifier on the two reported genders, male and female. With 4272 males and 1477 females, the dataset can be seen as gender imbalanced. We randomly split the data into 10,000 training images, 1144 validation images, and 1144 testing images. Just like the CIAIcel detector, the CIAllfw detector is trained with the same training parameters at every step. Both 2-class and 3-class detectors are trained for the LFW dataset."}, {"title": "Cross Dataset Validation and other comparative results:", "content": "For evaluating the performance of the trained detectors across datasets, we use the AgeDB and LFW datasets. The dataset contains 12,240 images of 440 subjects with attribute information for identity, gender, and age. The entire dataset is used for validation on the CIAI detectors trained on the LFW dataset and CelebA dataset, respectively. Further, we use the CelebA-HQ dataset to compare against existing state-of-the-art detection methods. The CelebA-HQ dataset is a subset of the CelebA dataset with 30,000 high-quality images in totality, out of which 1000 images are a part of the testing set. For comparison, we use the CIAI detector trained on the CelebA dataset; the images included in the testing set of the CelebA-HQ dataset are intentionally removed from the training and validation set during the training of the attribute classifier as well as the CIAI detector. For comparison, we consider 5 best-performing methods: LID [29] or Local Intrinsic Dimensionality uses a k-nearest neighbor classifier for detecting adversarial attacks; SID [42] utilizes wavelet transformation to detect the adversarial attacks; ODIN [25] is specifically designed for detecting out-of-distribution examples; ReAct [41] is a post hoc method, also proposed for out-of-distribution detection and rectifies the internal activations of the neural networks; SPert [46] uses original datasets with their self-perturbations to train a detector.\nSeen and Unseen Noises: For the experiments, a number of attacks and noises are used to modify images. Seen noises are noises used to train the detector, and unseen"}, {"title": "Attention Map Analysis", "content": "For attention map analysis, attention weights are pulled from the last multi-head attention layer of the Transformer classifier and detector. The first row in Figures 5 and 6 indicate the original unmodified image along with the modified images. Further, attention maps for the attribute predictor, as shown in the middle row, indicate the features utilized to classify the images and how different noises affect the decision. Even with the unintended noises, the attention changes slightly, and empirically, the confidence level decreases during attribute prediction even if there is no misclassification. For both examples, we can see that the FGSM and PGD adversarial attacks successfully fool the classifier by predicting the wrong gender label. For Gaussian noise, in Figure 5, the modification fails to change the label and the detector also fails to identify the noise. However, we see a 25% decrease in the confidence level after the noise is added for the classification. We can also observe that the detector considers the entire image when detecting the unmodified images but focuses on particular regions when detecting intentional and unintentional noises as seen in the last row of two figures. The detector is thus able to learn the difference between these modifications."}, {"title": "Conclusion", "content": "In this paper, we introduced CIAI, a novel noise detection network that operates independently of the image class. CIAI not only distinguishes between original and modified images but also differentiates between intentional (adversarial) and unintentional noise, both of which can impact the performance of a model. Our results show that CIAI performs effectively on both known and unknown noise types, including those with similar characteristics. When Lp-norm-based attacks are used as seen noises, attacks based on similar formulations are detected with almost similar accuracy. As observed when FGSM is the seen noise, and FFGSM and RFGSM are unseen noises for the CelebA dataset. Additionally, it can be tailored to specifically target adversarial attacks. Importantly, our findings demonstrate that CIAI maintains robust detection capabilities even when classification accuracy is not significantly compromised by unintentional noise."}]}