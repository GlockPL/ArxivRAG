{"title": "Structural Generalization in Autonomous Cyber Incident Response with Message-Passing Neural Networks and Reinforcement Learning", "authors": ["Jakob Nyberg", "Pontus Johnson"], "abstract": "We believe that agents for automated incident response based on machine learning need to handle changes in network structure. Computer networks are dynamic, and can naturally change in structure over time. Retraining agents for small network changes costs time and energy. We attempt to address this issue with an existing method of relational agent learning, where the relations between objects are assumed to remain consistent across problem instances. The state of the computer network is represented as a relational graph and encoded through a message passing neural network. The message passing neural network and an agent policy using the encoding are optimized end-to-end using reinforcement learning. We evaluate the approach on the second instance of the Cyber Autonomy Gym for Experimentation (CAGE 2), a cyber incident simulator that simulates attacks on an enterprise network. We create variants of the original network with different numbers of hosts and agents are tested without additional training on them. Our results show that agents using relational information are able to find solutions despite changes to the network, and can perform optimally in some instances. Agents using the default vector state representation perform better, but need to be specially trained on each network variant, demonstrating a trade-off between specialization and generalization.", "sections": [{"title": "I. INTRODUCTION", "content": "We focus on the automation of cyber security incident response, where we imagine an automated agent that takes actions and modifies the computer network to address ongoing incidents. An example scenario is ransomware spreading in a corporate network, where action is required to prevent further damage. Following previous work in this domain [1]\u2013[6], we frame incident response as a stochastic decision problem, or a game between agents defending the network and agents that attempt to compromise it. A common method used to find a solution to this class of problem is reinforcement learning (RL) [7], which has been applied in the context of incident response by several authors [8], [9]. RL assumes the dynamics of the problem is not known, which is common for real-world problems, but that the agent can receive observations of the problem state and affect the state through a set of actions. Computer networks are often dynamic in structure, with hosts connecting and disconnecting over time. We consider it an important feature that a network management agent can handle changes to the network with no additional training, so called zero-shot generalization. There are several proposed methods for facilitating zero-shot generalization in reinforce- ment learning [10]. One approach is relational reinforcement learning, where knowledge about relations between objects in a domain is assumed to be transferable across problem instances [11]. We use an approach for relational reinforcement learning proposed by [12] and adapt it to a network security problem. The state of the computer network is represented as a relational graph, which is encoded with a message-passing neural network (MPNN). An agent then uses this encoding to take actions on objects represented in the graph. We also evaluate a modified version of the approach where the agent only incorporates information from the local neighborhood of the graph. We evaluate the MPNN agent on the second iteration of the Cyber Autonomy Gym for Experimentation (CAGE), a two-player cyber security incident simulator. Previous work has noted that agents trained in this environment have issues with structural generalization [6]. We create a number of variants of the original network topology used in CAGE 2 and use these to test the generalization capabilities of agents. Our results demonstrate that an agent which encodes the network state using an MPNN can generalize with no additional training on these variants, something an agent using a multi-layer perceptron (MLP) encoder and vector representations can not."}, {"title": "II. BACKGROUND", "content": "The Cyber Autonomy Gym for Experimentation (CAGE) is a collection of four cyber security scenario simulators developed by the Technical Cooperation Program (TTCP). All"}, {"title": "B. Learning from Graphs", "content": "Graphs are data structures that represent relationships or interactions between objects or entities. In a simple formulation, a graph consists of a set of nodes $V = \\{1,..., V_n\\}$, representing objects, and edges $E \\subseteq \\{(u,v) | (u, v) \\in V^2\\}$ representing relations between objects. Many functions and algorithms have been proposed to analyze aspects of graph- structured data [14]. We focus on message-passing algorithms, where node representations are at every iteration updated using an aggregation of messages from their immediate neighborhood. A general message passing update rule is\n$h_v^l = UPDATE(AGGREGATE(N_v^l), h_v^{l-1})$\nwhere AGGREGATE is a permutation invariant aggregation function, and UPDATE is a function of the aggregated neigh- borhood of $v$ and the node feature at step $l$. A graph repre- sentations can be obtained through an aggregation of the node representations. As long as all aggregations are permutation invariant, message passing algorithms create representations of the graph that are permutation invariant and equivariant [14, Ch. 5]. The update and aggregation functions of the message-passing function in Equation 1 can be implemented using neural networks, forming as class of methods called message-passing neural network (MPNN). MPNNs shares parameters across nodes, meaning that although the size of an input graph may change, the number of parameters in the model does not necessarily need to. There are several proposed architectures of MPNN, which use different schemes of aggregations and updates. These have different abilities to represent and discriminate graph structures, and some will fail to discriminate certain graph structures [15]. Representations produced by message passing are dependent on the number of iterations the algorithm runs, which is equivalent to the number of layers in an MPNN. This dictates the size of the receptive field of each node or, put in another way, how far information travel between nodes in the graph. With too few steps, node features may not be expressive enough to be distinguished. On the other hand, as the number of message-passing steps increase, node representations may become homogenized across the graph, a concept referred to as over-smoothing. The optimal number of message passing iterations is dependent on the graph and the downstream task. The term MPNN is sometimes used interchangeably with graph neural network (GNN). We prefer to use the term MPNN as not all neural networks applied on graphs use message- passing, making GNN a broader descriptor than the methods we use require."}, {"title": "C. Reinforcement Learning", "content": "Reinforcement learning is a field of machine learning for solving decision and control problems [7]. Problems solved with reinforcement learning can usually be expressed as a Markov decision process (MDP). An MDP can be described as a tuple of states, actions, rewards and state transition probabilities. The reward function R: A\u00d7S \u2192 R associates each state and action pair with a real value. For a finite MDP, the goal of an agent is to maximize the expected total reward $G = \\sum_{t=1}^T R_t$, where T is the step the MDP terminates at. In each state an agent decides on an action using a policy, a function from a state to an action or distribution over actions, \u03c0: S \u2192 \u0394(\u0391). The value of a policy is denoted as $V^\u03c0$, and is the expected total reward in a state s when following the policy \u03c0. An optimal policy \u03c0* is associated with the optimal value function $V^* := max V^\u03c0(s)$. Reinforcement learning methods attempt to approximate the optimal policy or value function through continuous interaction with the problem. Policy gradient reinforcement learning methods optimize the policy function towards the optimal policy through gradient descent. Actor-critic methods extend this method by also estimating the value function as part of the optimization process. For some problems, decisions can naturally be factorized into a series or set of action variables. This includes the CAGE 2 environment described in Section II-A, where actions are separated into hosts and commands. Given a problem with a joint action $a = (a_1,...,a_i)$, the probability of taking a in the state S can be expressed as a product of conditional probabilities,\n$\\pi(a|S) = \\prod_i \\pi_i(a_i | S, a_1,..., a_{i-1})$"}, {"title": "D. Symbolic Relational Deep Reinforcement Learning", "content": "We use an architecture for relational learning named Sym- bolic Relational Deep Reinforcement Learning (SR-DRL) [12]. SR-DRL is intended for problem domains that can be repre- sented using objects and actions are taken on those objects. Objects in the problem and their relations are represented as a relational graph, and an MPNN is used to encode the graph as input to a policy function. Policy factorization is used to separate joint actions into conditional parameters for additional flexibility in representation. SR-DRL generates a whole-graph representation in addition to node representations. The graph representation is updated sequentially after each message-passing step, incorporating the previous representation in the next iteration. The MPNN and policy function are trained end-to-end using reinforcement learning. An action- critic scheme is used when training agents, where the value function estimator is implemented as a neural network taking the graph representation as input."}, {"title": "III. IMPLEMENTATION OF MPNN AGENTS ON CAGE 2", "content": "To apply the SR-DRL approach on CAGE 2, we reshape the vector observation space into a relational graph. We also modify the action space. An agent has to make two decisions"}, {"title": "A. Local Message-Passing Scheme", "content": "The local message passing scheme is largely unchanged from the original implementation [12], albeit with the graph representation removed from the node update. The update to the representation h for a node v at layer l is thus defined as\n$h_v^l = h_v^{l-1} + \\phi_{agg}((V, U_{msg})),$ where\n$U_{msg} = max_{u \\in \\mathcal{N}(n)} \\phi_{msg}(u).$\nThe graph representation update at layer l is\n$g^l = g^{l-1} + \\phi_{gdl}((g^{l-1}, f(V)))$ where\n$f(V) = \\sum_{v \\in V} softmax(\\phi_{att}(v)) \\cdot \\phi_{feat}(v),$\nan attention-based aggregation scheme proposed by [17]. Each function \u03c6 is implemented as a single-layer neural network, \u03c6(x; W,b) = \u03c1(xWT + b), where \u03c1 is a non-linear activation function."}, {"title": "B. Policy Decomposition", "content": "The local approach decomposes the policy of the agent into two actions, where the agent first selects a node in the graph and then what action should be taken on it. In our notation, E denotes the size of the representation vectors and N the amount of nodes, |V|.  $H^{N \\times E} = \\{h_1,...,h_N\\}$ is a multiset"}, {"title": "C. Changes to CAGE 2", "content": "We introduced two changes to the CAGE 2 implementation, which we based on commit b5a71c44 of CybORG. Our options were to either work from the version of CAGE 2 in the most recent commit of CybORG, or the older version used for the challenge. We opted for the more recent version, as we found it easier to work with. The first change was reshaping the observation space to a graph, in order to use an MPNN agent. We converted the"}, {"title": "IV. EVALUATION", "content": "Our evaluation of the MPNN agent consisted of training agents in an environment configuration, and then testing the agent on a set of network variants. We only trained against the Meander agent, as it provided more varied observations than the B-Line agent. Previous work on CAGE 2 trained against a combination of attackers, switching between them at random between episodes [6], or switched between models depending on the attacker [4]. In our opinion, access to all forms of red team behavior during training should not be assumed. We thus withheld the B-Line policy during training and used it for evaluation only. The relational learning scheme we employ is not intended to generalize across opponent policies, and the ability to do so is only a fortunate side effect. Episodes were truncated after 50 steps during training. In the absence of blue team interference, this is enough for the Meander agent to be able to reach the operational server. To evaluate the ability of agents to generalize across structural changes, we created variants of the original network by removing different hosts. Host were selected on the condition that red team agents using both the B-Line and Meander policies could reach the operational host without them. Given this condition, we generated six network variants, each one with an additional randomly selected eligible host removed. This resulted in seven network variants, which includes the original network of 16 hosts. Changing the number of hosts in the network not only changes the observation space of the problem, but also the action space, as the number of machines under the control of the blue team changes. We trained MPNN agents on the network variant with 13 hosts, and evaluate on all variants. This setup gave three larger networks, and three smaller, to evaluate agents on. A specialized MLP agent was trained for each network variant for comparison with the general MPNN agent. We chose an MLP agent for comparison as they have been used in previous work on CAGE 2 [6]. Agents were rated by the scoring scheme described in Section II-A, that test agents with different episode lengths and red teams, as in previous work [4], [6]. The scale of the rewards change depending on the network size and episode length, making comparisons of agents for multiple problem instances difficult. As an alternative perfor- mance metric, we calculated the percentage of episodes where the agent did not receive any negative reward, which we call a \u201cperfect round\u201d. A perfect round thus means that the blue team managed to stop the red team without receiving a penalty. Agent training and evaluation was done on a machine equipped with 32 GB of RAM, an Intel Xeon Silver CPU with 24 1GHz cores and an NVIDIA Quadro RTX 4000 GPU. Training is also possible on more limited hardware, such as a laptop equipped with 16 GB of RAM and an 11th Gen. Intel i7 CPU. Our code implementation relies on PyTorch Geometric for the message-passing algorithm. We used proximal policy optimization to train agents, as was also done by [6]. The PPO and MLP implementations were sourced from Stable Baselines 3. The code for the MPNN policy is based on the implementation by [18], who in turn ported the code from the original authors [12] to work with Stable Baselines 3. Different selections of hyperparameters were not extensively evaluated, but we present results for MPNN agents with different numbers of layers. We do not include MPNN agents with a layer depth of one, as these were significantly worse than the layer variants. All agents were trained using 500 000 steps of each environment in total."}, {"title": "V. EVALUATION RESULTS", "content": "An immediate difference between agents using MLP and MPNN models was the training time. With our implementation, MPNN training was roughly 10 times slower than MLP training. In concrete numbers, training each MLP model took roughly 4 minutes, and each MPNN model 40 minutes. The difference was alleviated, however, by the fact that we trained less MPNN agents overall, as we did not have to train a separate agent for every scenario variant. We calculated average rewards and the total score following the challenge rules, meaning that each combination of attacker, episode length and agent was evaluated for 1000 episodes. The total score presented in Table II is the sum of average rewards"}, {"title": "VI. RELATED WORK", "content": "There exists several cyber incident simulators with different assumptions and associated solution methods [9]. In the interest of space, we limit works in described in this section to those that use CAGE 2 in some form. We make one exception to this limitation and highlight the work of [3], that combines reinforcement learning, relational graph learning and cyber security similarly to us. They evaluate their approach on another cyber incident simulator, Yawning Titan, which is more abstract than CybORG. Our implementation differs from theirs in that they use a graph embedding algorithm named Feather-G, a method for graph representation based on characteristic functions [19]. It is not explicitly stated in the text that the action space is generalized to different number"}, {"title": "VII. DISCUSSION", "content": "From the results of our experiments, we see that the MPNN agents perform better on the unseen network variants than the untrained MLP baseline. We also see that they can obtain a nonzero percentage of rounds where the red team does not capture any machines, and the blue team does not receive any penalty. We take this as evidence that the MPNN agents are capable of zero-shot generalization in this problem domain. The zero-shot performance is worse than that of MLP policies specifically trained on the network variant. This is in line with results obtained by [12], where the MPNN agents tend to be outperformed by one-shot planners on specific problem instances. The difference in results indicates a trade-off between generalization and specialization. We do not know if a fully generalized policy is possible to find for this problem, but work has been done to analyze this question in other domains [21]. We chose MLP agents as an upper bound of performance, as they are often used in previous work on the CAGE 2 environment. However, these are likely worse than a theoretical optimal policy, meaning that we do not know how close the zero-shot policy is to optimal. We can see from Table I that for some network variants, both the MLP and MPNN agents have few to no perfect rounds."}, {"title": "A. Limits of Relational Learning", "content": "An inductive bias of relational reinforcement learning is that the properties and relationships between objects remain constant. This may not be true, depending on the problem domain. In our problem, changing the red team policy will change the underlying dynamics of the problem, and the change in representation that the MPNN introduces does not directly address this. It may indirectly do so if the relational rules learned by the agent is general enough to counter any red team policy, but this is also true for a policy that can be learned by an MLP agent. Other methods of zero-shot learning would need to be applied to address this issue [10]."}, {"title": "B. Changes made to CAGE 2", "content": "We made a set of changes to the CAGE 2 environment. We removed the host that the red team starts at from the blue team action space. The starting host is different from other hosts in that blue team commands fail when used on it. This is to ensure the red team always has access to the network. If vector observations are used, there is no way to identify the starting host other than by its position in the vector. We chose to not include decoy actions for similar reasons. The decoy actions have specific sets of requirements that need to be fulfilled in order to execute them. This includes ports not being"}, {"title": "VIII. CONCLUSION", "content": "We implemented agents for automated network intrusion response that use message-passing neural networks to encode facts about the network. The agents were evaluated using a simulated network environment, the Cyber Autonomy Gym for Experimentation. Our results show that our agents can generalize across network variants without additional training but are outperformed by specially trained policies, indicating a trade-off between generalization and specialization. Our work addresses an issue present in previous work on automated incident response [4], [6]: That agents are bound to a specific size, structure and ordering of the network topology. This is in spite of computer networks being highly variable in structure, and a network operator needs to handle such changes while enacting security policies. We believe that by exploiting relational structure in the problem, agents for cyber incident response can be made more general and reusable. Reusing agents in problems with a different structure, but similar dynamics saves both time and energy, which we consider important for practical use."}]}