{"title": "OvSW: Overcoming Silent Weights for Accurate Binary Neural Networks", "authors": ["Jingyang Xiang", "Zuohui Chen", "Siqi Li", "Qing Wu", "Yong Liu"], "abstract": "Binary Neural Networks (BNNs) have been proven to be highly effective for deploying deep neural networks on mobile and embedded platforms. Most existing works focus on minimizing quantization errors, improving representation ability, or designing gradient approximations to alleviate gradient mismatch in BNNs, while leaving the weight sign flipping, a critical factor for achieving powerful BNNs, untouched. In this paper, we investigate the efficiency of weight sign updates in BNNs. We observe that, for vanilla BNNs, over 50% of the weights remain their signs unchanged during training, and these weights are not only distributed at the tails of the weight distribution but also universally present in the vicinity of zero. We refer to these weights as \"silent weights\", which slow down convergence and lead to a significant accuracy degradation. Theoretically, we reveal this is due to the independence of the BNNs gradient from the latent weight distribution. To address the issue, we propose Overcome Silent Weights (OvSW). OvSW first employs Adaptive Gradient Scaling (AGS) to establish a relationship between the gradient and the latent weight distribution, thereby improving the overall efficiency of weight sign updates. Additionally, we design Silence Awareness Decaying (SAD) to automatically identify \"silent weights\" by tracking weight flipping state, and apply an additional penalty to \"silent weights\" to facilitate their flipping. By efficiently updating weight signs, our method achieves faster convergence and state-of-the-art performance on CIFAR10 and ImageNet1K dataset with various architectures. For example, OvSW obtains 61.6% and 65.5% top-1 accuracy on the ImageNet1K using binarized ResNet18 and ResNet34 architecture respectively.", "sections": [{"title": "1 Introduction", "content": "Deep neural networks (DNNs) have shown tremendous success in various computer vision tasks, including image classfication [15,23], object detection [10,14,"}, {"title": "2 Related Work", "content": "Great efforts have been put into reducing the performance gap between BNNs and their real-valued counterparts. The pioneering BNN work dates back to Courbariaux et al. [5], which binarizes weights and activations to +1 or -1 by sign function. However, this aggressive approach limits the representation ability of BNNs to the binary space, resulting in a significant accuracy degradation. To reduce the quantization error and improve their accuracy, XNOR-Net [42] introduces a scaling factor obtained through the l\u2081-norm of weights or activations. Furthermore, XNOR-Net++ [3] merges two scale factors from the weights and activations into a trainable parameter and optimizes them via backpropagation. ABC-Net [31] employs a linear combination of multiple binary weight sets to approximate the real-valued weights and alleviate the information loss. Bi-Real Net [34] connects the real-valued activations to the consecutive block via an identity shortcut, which significantly enhances network representation ability while incurring negligible computational overhead. AdaBin [48] introduces adaptive binary sets to fit different distributions, enhancing binarized representation ability. UaBNN [59] introduces an uncertainty-aware BNN to reduce these weights' uncertainties. INSTA-BNN [24] controls the quantization threshold in an input instance-aware manner, taking higher-order statistics into account.\nApart from enhancing the representation ability of BNNs, gradient estimation is also one of the critical research directions, since gradients in the sign function are almost zero everywhere. Straight through estimator (STE) [1] is the most widely used function to enable the gradient to backpropagate. However, the gradient error is huge for STE and will accumulate during backpropagation, leading to instability in training and severe accuracy degradation. To alleviate this, Bi-Real Net [34] utilizes a piecewise polynomial function as the approximation function. IR-Net [40] proposes an error decay estimator and RBNN [30]"}, {"title": "3 Background", "content": "We briefly review the optimization process of BNNs in this section. Given a DNN, we denote $W_j \\in \\mathbb{R}^{C_{out} \\times C_{in} \\times K_h \\times K_w}$ as the real-valued weight in the j-th layer, $C_{out}$, $C_{in}$, $K_h$ and $K_w$ are the number of output channels, input channels, kernel height, and kernel width, respectively. Let the real-valued activation be $A_j$, then the convolution process can be formulated as\n$A_{j+1} = W_j * A_j$, (1)\nwhere * represents the standard convolution operation.\nBNNs aim to binarize weights $W_j$ and activations $A_j$ to discrete values ({+1, -1}) through sign function:\n$x = sign(x) = \\begin{cases} +1, \\text{ if } x \\ge 0, \\\\ -1, \\text{ otherwise}. \\end{cases}$ (2)\nTo reduce the quantization error in BNNs, XNOR-Net [42] introduces two scale factors to approximate the binarized weights $W$ and activation $A$. Furthermore, XNOR-Net++ [3] proposes fusing the activation and weight scaling factors into one and optimizing it via backpropagation. This approach significantly outperforms XNOR-Net within the same computational budget and has been widely adopted by recent works [33,34,40,53]. Following them, we denote the scaling factor as $a_j$. Then the binary convolution operation can be formulated as\n$A_{j+1} = (A_j \\circledast W_j) \\odot a_j$, (3)\nwhere $\\circledast$ is the efficient XNOR and Bitcount operation and $\\odot$ is the hadamard product. In the implementation of BNNs, $A_{j+1}$ will be processed through several layers, e.g., Batch Normalization (BN) layer, non-linear activation layer, and max-pooling layer. In this section, we omit these operations for simplicity.\nTo train a BNN, the forward propagation includes Eq. (2) and Eq. (3), where their real-value counterparts $W_j$ and $A_j$ are used for calculating gradients and updating during the backpropagation. However, the $sign$ is not differentiable, thereby gradient estimation is important in BNNs. Following the previous studies, we use straight-through estimator (STE) [1] to approximate the gradient of the loss w.r.t. the weight $w \\in W$:\n$\\frac{\\partial L}{\\partial w} = \\frac{\\partial L}{\\partial \\hat{w}} \\cdot \\frac{\\partial \\hat{w}}{\\partial w}$ (4)"}, {"title": "4 Methodology", "content": "In this section, we show that the distribution of gradients and weights for BNNs is independent by systematical and theoretical analysis. Then, we propose Adaptive Gradient Scaling (AGS) to scale the gradient and introduce Silence Awareness Decaying (SAD) to detect \u201csilent weights\", moving them towards zero. Both of them can enhance the efficiency of weight sign flipping."}, {"title": "4.1 The Independence of the Gradient and Weight Distribution", "content": "Figure 2 demonstrates forward and backward computation graph for binary convolutional (Bin-Conv) operation. As seen, Bin-Conv is often followed by a BN layer. We can conclude that once two BNNs N and N' satisfy:\n$W_j = W'_j, a_j = a'_j, BN_j = BN'_j, \\forall j$, (7)\nfor the same input $A_i$, the output $A_{j+1}$ and $A'_{j+1}$ can be formulated as:\n$A_{j+1} = BN ((A_j \\circledast W_j) \\odot a_j) = BN' ((A'_j \\circledast W'_j) \\odot a'_j) = A'_{j+1}$. (8)\nThrough mathematical induction, we can know that the $N$ and $N'$ will have the same output and loss, i.e. $L = L'$. Therefore, the backpropagation for them is:\n$\\frac{\\partial L}{\\partial W_j} = \\frac{\\partial L}{\\partial A_{j+1}} \\frac{\\partial A_{j+1}}{\\partial W_j} = \\frac{\\partial L'}{\\partial A'_{j+1}} \\frac{\\partial A'_{j+1}}{\\partial W'_j} = \\frac{\\partial L'}{\\partial W'_j}$ (9)"}, {"title": "4.2 Adaptive Gradient Scaling", "content": "To improve the efficiency of the update, an intuitive approach is to scale $\u03b2(t)$ to $\u03b3\u03b2(t)$. In this case, $W_j(t + 1)$ can be formulated as:\n$W_j(t + 1) = W_j(t) - \u03b3\u03b2(t) \\frac{\\partial L(t)}{\\partial W_j(t)}$\n$= W_j(t) - \u03b3\u03b2(t) \\frac{\\partial L(t)}{\\partial W_j(t)}$ (12)\n$= \u03b3W'_j(t + 1)$.\nHowever, selecting the appropriate scaling factor is tricky. Meanwhile, an inappropriate scaling factor may make weights with small magnitudes correspond to large gradients, leading them to oscillate frequently in {+1,-1} and introducing instability to BNNs training.\nTo overcome this issue, we introduce \"Adaptive Gradient Scaling\" (AGS). Let $G_j \\in \\mathbb{R}^{C_{out} \\times C_{in} \\times K_h \\times K_w}$ denote the gradient with respect to $W_j$, i.e., $\\frac{\\partial L}{\\partial W_j}$"}, {"title": "4.3 Silence Awareness Decaying", "content": "We propose another approach (Silence Awareness Decaying, SAD) orthogonal to AGS to detect and prevent \"silent weights\". Specifically, we track the flipping state of $W_j$ over time using an exponential moving average (EMA) strategy, which is formulated as:\n$S_j(t) = m \\cdot S_j(t - 1) + (1 - m) \\cdot \\frac{|sign (W_j (t)) - sign (W_j (t - 1)) |_{abs}}{2}$, (15)"}, {"title": "5 Experiment", "content": "In this section, we conduct extensive image classification experiments for OvSW and compare it to state-of-the-art (SOTA) methods on CIFAR10 and ImageNet1K with various architectures. Then, we discuss the hyperparameter settings for OvSW, including $\u03bb$ for AGS and $\u03c3$ for SAD and convergence speed on CIFAR100. We also conduct ablation study to demonstrate the compatibility"}, {"title": "5.1 Results on CIFAR10", "content": "We trained OvSW for CIFAR10 with 600 epochs, where the batch size is set to 256 and the initial learning rate to 0.1, decaying with Cosine Anealing. We adopt SGD optimizer with a momentum of 0.9 and weight decay of 5e-4 and employ the same data augmentation in ReCU [53]. \u03bb and \u03c3 are set to 0.04 and 9e-4 respectively. We compare OvSW with IR-Net [40], RBNN [30], CMIM [45], SiMaN [29], ReCU [53], SLB [55], FDA-BNN [52], DoReFa [60], RAD [6], DSQ [11], and Proxy-BNN [16]. As shown in Tab. 1, OvSW achieves the best performance among all methods. For ResNet18, OvSW obtains 93.2% top-1 accuracy, which"}, {"title": "5.2 Results on ImageNet1K", "content": "On ImageNet1K, OvSW is trained from scratch. We train OvSW with 200 epochs, where the batch size is set to 512 and the initial learning rate is set to 0.1, decaying with Cosine Annealing. We adopt SGD optimizer with a momentum of 0.9 and weight decay of le-4 and the data augmentation is the same as ReCU [53]. \u03bb and \u03c3 are set to 0.02 and 2e-5 respectively. We demonstrate the ImageNet1K performance of ResNet18/34 and compare OvSW with SOTA methods, including one-stage training methods XNOR [42], BiReal [34], IR-Net [40], RBNN [30], SiMaN [29], FDA-BNN [52], ReCU [53], CMIM [45], and two-stage training method [37] adopted by ReActNet [33]. As shown in Tab. 2, OvSW also achieves the best performance. For ResNet18, OvSW achieves 61.6% top-1 and 83.1% top-5 accuracy compared to ReCU's 61.0% top-1 and 82.6% top-5 accuracy, which demonstrates the efficiency of overcoming the \"silent weights\". For ResNet34,"}, {"title": "5.3 Ablation Analysis", "content": "We investigate the effectiveness of hyper-parameters, including $\u03bb$ and $\u03c3$, convergence speed, different components, and compatibility through ablation analysis. All the following results are based on binarized ResNet18 for CIFAR100.\n$\u03bb$ for AGS and $\u03c3$ for SAD. We first compare AGS with vanilla BNNs and analyze the $\u03bb$ for AGS. In Fig. 3a, we present the results for nine different settings of $\u03bb$, which vary from 0.01 to 0.09. As seen, the appropriate $\u03bb$ can significantly improve the performance of BNNs, and $\u03bb$ = 0.04 achieves the best performance. If $\u03bb$ is too small, the sign of the weights can flip inefficiently; while $\u03bb$ is too large, the sign of the weights can flip dramatically, introducing instability to the training. Based on this result, we further introduce SAD to AGS ($\u03bb$ = 0.04) to analyze $\u03c3$. Figure 3b demonstrates that the performance of the model can be further improved by identifying \"silent weights\" and applying additional penalties to them via SAD ($\u03c3$ = 0.0009).\nConvergence for OvSW. To verify that OvSW facilitates the flipping of weight signs and thus improves the efficiency of convergence, we fix the $\u03bb$ and $\u03c3$ to 0.04 and 0.0009 respectively, and record the top-1 accuracy over the training epoch from 60 to 500. As shown in Fig. 3c, OvSW effectively accelerates training convergence and achieves better performance. For example, OvSW achieves 66.37 \u00b1 0.35% top-1 accuracy with only 60 traning epochs, while vanilla BNNs only reaches 55.28\u00b10.07% and 65.23\u00b10.21% top-1 accuracy with 60 and 120 training epochs respectively. It indicates that OvSW can achieve significant performance gains in scenarios with limited training resources, such as training the network on edge devices. Meanwhile, although increasing epochs improve the final performance for both OvSW and vanilla BNNs, OvSW consistently outperforms the vanilla BNNs.\nComponents. AGS and SAD promote sign flipping for overall weights and \"silent weights\" respectively. To prove that they are orthogonal to each other, we conduct components ablation study for different modules and show the results"}, {"title": "5.4 Loss Landscape Visualization", "content": "BNNs restrict the weights and activations to discrete values, which naturally limits the representational capacity of the model and further result in disparate optimization landscapes compared to real-valued ones [32]. As illustrated in Fig. 4, we follow the method in [26] to plot the actual optimization landscape about our OvSW and compare it with the same architecture to real-valued and XNOR-Net++. As seen, our OvSW has a significantly smoother loss-landscape and minor loss elevation compared to XNOR-Net++, which confirms the effectiveness of OvSW in the BNN optimization."}, {"title": "5.5 Deployment Efficiency", "content": "We implement 1-bit models on the M1 Pro, which features 8 high-performance Firestorm cores and 2 efficient Icestorm cores in a hybrid design. The Firestorm cores can be clocked up to 3.2 GHz, while the Icestorm cores can reach 2.1 GHz. Our OvSW shows significant efficiency gains when deployed on real-world mobile devices, as evidenced by practical speed evaluations. To make our inference framework BOLT [9] compatible with OvSW, we leverage the ARM NEON SIMD instruction SSHL. We compare OvSW with 32-bit and 16-bit backbones. As shown in Tab. 4, OvSW inference speed is substantially faster with the highly efficient BOLT framework in a single thread. For instance, OvSW achieves an acceleration rate of about 3.47\u00d7 on ResNet18 compared to its 32-bit counterpart. For the ResNet34 backbone, OvSW achieves a 3.42\u00d7 acceleration rate with the BOLT framework on hardware, which is significant for computer vision applications on real-world edge devices. At the same time, OvSW can save memory by a factor of 16.64\u00d7 and 21.16\u00d7, which demonstrates its potential for applications with limited memory resources."}, {"title": "6 Conclusion", "content": "BNN is a crucial method to compress deep learning models and reduce inference overhead. In this paper, systematically and theoretically, we prove the distribution of gradients is independent of latent weights, which is the root cause of inefficient updating and performance degradation of BNNs. To this end, Adaptive Gradient Scaling (AGS) and Silence Awareness Decaying (SAD), are proposed to Overcome Silent Weights (OvSW) and achieve SOTA performance on BNNs. Specifically, AGS adaptively scales the gradient based on the distribution of weights, improving the efficiency of sign flipping for the overall weights. SAD can effectively measure the states of weight flipping, detect the \"silent weights\" and introduce additional penalty to them to facilitate their flipping. Extensive experiments demonstrate that OvSW can achieve notable performance gains over the SOTA methods on various datasets and networks. In addition, OvSW has better convergence efficiency and excellent compatibility, which can be combined with existing methods to further enhance the performance of BNNs."}, {"title": "Appendix", "content": "In this Appendix we provide the following material:\nAppendix A demonstrates more weight flipping information for vanilla BNNs and OvSW in additional to Fig. 1.\nAppendix B extrapolates the conclusions in Sec. 4.1 to a more generalized scenario.\nAppendix C conducts ablation analysis for different weight initialization methods, including kaiming normal [13] and kaiming uniform [13] and the same weight initialization methods with different std or range.\nAppendix D compares AGS with LARS.\nAppendix E disscusses the difference of OvSW with latent weights (Appendix E.1) and adam optimizer (Appendix E.2) for BNNs.\nAppendix F describes societal impact of OvSW."}, {"title": "A More Experimental Results", "content": "We further demonstrate the weight flip information of the relevant layers based on Fig. 1. As seen in Fig. 5 and Fig. 6, in addition to the weight flipping of layer4.conv2.weight, OvSW also promotes weight flip efficiency for other layers."}, {"title": "B A More Generalized Scenario", "content": "In this section, we further generalize the conclusion in Sec. 4.1 to the more general scenario, where N and N' have different $a_j$ and $a'_j$.\nFirstly, we rewritten Eq. (3) as:\n$A_{j+1} = BN ((A_j \\circledast W_j) \\odot a_j) = BN (A_j a_j)$\n$= BN (diag(a_j)A_j) = BN (\\Lambda_j A_j)$. (17)\nAssuming that for two networks $N$ and $N'$, where $A'_i = A_i$ and $sign(W'_j) = sign(W_j)$, it is easy to obtain that $A'_j$ is equivalent to $A_j$. Therefore, we pay our attention to $\\frac{\\partial L}{\\partial a_j}$ and $\\frac{\\partial L}{\\partial a'_j}$. Assuming that BN in both $N$ and $N'$ can estimate the mean and variance of $\\Lambda_j A_j$ and $\\Lambda'_j A'_j$ with relative precision, and learnable affine parameters $\u0393_j = \u0393'_j, \u03b2_j = \u03b2'_j$, we can know for the forward propagation:\n$A_{j+1} = BN (\\Lambda_j A_j)$\n$\\approx \u0393_j \\frac{\\Lambda_j A_j - \u03bc (\\Lambda_j A_j)}{\u03c3 (\\Lambda_j A_j)} + \u03b2_j$\n$\\approx \u0393'_j \\frac{\\Lambda'_j A'_j - \u03bc (\\Lambda'_j A'_j)}{\u03c3 (\\Lambda'_j A'_j)} + \u03b2'_j$ (18)\n$= BN' (\\Lambda'_j A'_j) = A'_{j+1}$,\nThus, $N$ and $N'$ will have the same output and loss, i.e. $L = L'$. For the back propagation:\n$\\frac{\\partial L}{\\partial a_j} = \\frac{\\partial L}{\\partial A_{j+1}} \\frac{\\partial A_{j+1}}{\\partial (\\Lambda_j A_j)} \\frac{\\partial (\\Lambda_j A_j)}{\\partial a_j}$\n$\\approx \\frac{\\partial L}{\\partial A_{j+1}} \\frac{\\partial A_{j+1}}{\\partial (\\Lambda_j A_j)} \\Gamma_j \\frac{\\partial A_{j+1}}{\\partial a_j}$ (19)"}, {"title": "C Ablation Analysis for Weight Initialization", "content": "We employ OvSW for two famous weight initialization methods, including kaiming normal (default in this paper) and kaiming uniform, as shown in Eq. (21) and Eq. (22) respectively:\n$W_j \\sim Normal(0, \\frac{1}{2}std^2)$ (21)\n$W_j \\sim Uniform( - Abound, Abound)$, (22)\nwhere $std = \\sqrt{\\frac{2}{C_{in} \\times K_h \\times K_w}}$ and $bound = gain \\times \\sqrt{\\frac{3}{C_{in} \\times K_h \\times K_w}}$ is computed via $C_{in} \\times K_h \\times K_w$. We train binarized ResNet18 for CIFAR100 with 120 epochs to demonstrate weight initialization analysis. Without loss of generality, we set \u03b3 to 1 and initialize $W_j$ via the stand kaiming initialization and then employ $W_j = \\gamma W'_j$ to simulate Eq. (21) and Eq. (22) in our implementations.\nWe present the epoch-wise flip rate for eight different settings of \u03b3, which vary from 0.0001 to 1000. The results in Fig. 10 to Fig. 17 and Fig. 18 to Fig. 25 are for kaiming normal and kaiming uniform respectively. As seen, because of the gradients of the BNN being independent of their latent weight distribution, the epoch-wise flip rate and \u03b3 have a significant negative correlation. When \u03b3 is set to 1000, the epoch-wise flip rate of both distributions undergoes a severe drop, greatly hurting the model's convergence as shown in Fig. 8 and performance as shown in Fig. 9. Meanwhile, we can observe that by reducing the variance or range of the weight distribution to some extent, a significant improvement can be achieved to the BNNs compared to the standard distribution, which demonstrates the reasonableness of AGS again."}, {"title": "D AGS vs LARS", "content": "In this section, we compare LARS and AGS (p = 0 in OvSW) and demonstrate their corresponding results in Fig. 26. As seen, by selecting the appropriate parameter \u03b7, LARS is also able to achieve impressive gains by scaling the local learning rate. However, the improvement of AGS is more significant over LARS.\nBy comparing the optimization processes of LARS and OvSW in Algorithm 2 and Algorithm 3, We find that the most essential difference between them is that LARS only scales the learning rate at a single step, and the scaling only acts on the current gradient descent and does not accumulate into the momentum; in contrast, AGS directly modifies the gradient through adaptive gradient scaling, and the gradient not only acts on the current, but also accumulates into the future optimization process through the momentum. From the experimental results in Fig. 26, we can see that compared to LARS, AGS is more conducive to the efficient training of BNNs."}, {"title": "E Discussion for Latent Weights and Optimizer", "content": "We first discuss the similarities and differences between ours and Helwegen et al. [17]. Similarly, both of us find that the gradient of the weights in the BNNS is independent of the magnitude of the weights. Subsequently, Helwegen et al. design a binary optimizer (BOP), which determines the flipping of the weight signs by comparing the exponential moving average of gradients with a pre-defined threshold, which is independent to the magnitude of weights and gradients. While this approach avoids the problem that inappropriate weight distributions can lead to inefficient weight signs flipping, they ignore the fact that the weight magnitudes also play a role in sign changes during the optimization process. OvSW constructed a correlation between the gradient distribution and the weight distribution via AGS, which is essentially the same as BOP in facilitating weight signs flipping. Meanwhile, the optimization takes the role of weight magnitude into account and thus achieves better results than BOP. Apart from these, another advantage of OvSW is that SAD detects \"silent weights\" to further enhance the efficiency of weight signs flipping."}, {"title": "E.2 Optimizer", "content": "OvSW emploies SGD to train BNNs, which is different from the previous state-of-the-art BNNs that uses Adam [21] as the optimizer, including ReActNet [33], AdamBNN [32], RoBNN [51], ReBNN [50]. From the perspective of weight signs flipping, we believe this is due to $\\frac{m_t}{\\sqrt{i_t} + e}$ in Adam adaptively scales the gradients and facilitates the flipping efficiency. However, Adam needs to preserve the first momentum and second momentum of the gradient during training, leading to additional storage. In the mixed precision training scenario, a model with parameter number \u03a8 and Adam optimizer will consume 16\u03a8 [41] storage for model and optimizer. Even though OvSW introduce an auxiliary variable S, it cause 14\u03a8 [41] storage, which is 12.5% less than Adam. At the same time, OvSW can be simply and effectively implemented on GPUs, improving the performance of BNN with little or no degradation of training speed."}, {"title": "F Societal Impact", "content": "Increasing model size can result in tremendous resource consumption and carbon emissions during both training end inference. OvSW can improve performance and accelerate the convergence efficiency of BNNs while introducing negligible memory and computational overhead. It can facilitate the deployment of BNNs. On the other hand, it also enables efficient training for BNNs under memory and computational resources constraints. Both have far-reaching potential for the promotion of green AI."}]}