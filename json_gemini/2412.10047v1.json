{"title": "Large Action Models: From Inception to Implementation", "authors": ["Lu Wang", "Fangkai Yang", "Chaoyun Zhang", "Junting Lu", "Jiaxu Qian", "Shilin He", "Pu Zhao", "Bo Qiao", "Ray Huang", "Si Qin", "Qisheng Su", "Jiayi Ye", "Yudi Zhang", "Jian-Guang Lou", "Qingwei Lin", "Saravan Rajmohan", "Dongmei Zhang", "Qi Zhang"], "abstract": "As Al continues to advance, there is a growing demand for sys-\ntems that go beyond language-based assistance and move toward\nintelligent agents capable of performing real-world actions. This\nevolution requires the transition from traditional Large Language\nModels (LLMs), which excel at generating textual responses, to\nLarge Action Models (LAMs), designed for action generation and\nexecution within dynamic environments. Enabled by agent systems,\nLAMs hold the potential to transform AI from passive language\nunderstanding to active task completion, marking a significant\nmilestone in the progression toward artificial general intelligence.\nIn this paper, we present a comprehensive framework for de-\nveloping LAMs, offering a systematic approach to their creation,\nfrom inception to deployment. We begin with an overview of LAMs,\nhighlighting their unique characteristics and delineating their dif-\nferences from LLMs. Using a Windows OS-based agent as a case\nstudy, we provide a detailed, step-by-step guide on the key stages of\nLAM development, including data collection, model training, envi-\nronment integration, grounding, and evaluation. This generalizable\nworkflow can serve as a blueprint for creating functional LAMs\nin various application domains. We conclude by identifying the\ncurrent limitations of LAMs and discussing directions for future\nresearch and industrial deployment, emphasizing the challenges\nand opportunities that lie ahead in realizing the full potential of\nLAMs in real-world applications.", "sections": [{"title": "1 INTRODUCTION", "content": "In recent years, large language models (LLMs) have demonstrated\nremarkable advancements across a range of natural language pro-\ncessing (NLP) tasks [4, 69, 77]. These models, often incorporating\nmultiple modalities such as language, vision, and speech, have be-\ncome foundational in numerous AI-driven applications [26, 55, 61,\n66]. Their success is evident in systems like question answering\nin conversational agents [43], code generation in GitHub Copi-\nlot [81], and improved search capabilities in platforms like Bing\n[62]. The key strengths of LLMs-namely their vast knowledge,\nability to support multimodal inputs, and capacity for human-like\nresponses-have propelled them to the forefront of AI research [45].\nTheir capability to generalize via zero-shot learning has further\nexpanded the horizons of what Al systems can achieve, making\nsignificant contributions to the productivity of both everyday tasks\nand specialized professional activities. These innovations mark an\nimportant milestone on the path toward artificial general intelli-\ngence (AGI) [14].\nHowever, while LLMs excel in generating intricate textual re-\nsponses, they are often constrained by their inability to directly\ninteract with or manipulate the physical world [65]. In many real-\nworld applications, intelligent systems need to perform tasks that\ngo beyond conversational exchanges-tasks that involve tangible\nactions [16]. The maxim \u201cactions speak louder than words\u201d [50]\nunderscores the limitations of purely text-based interactions, as\nusers increasingly expect intelligent agents to go beyond passive\nresponses and engage in real-world actions. For instance, a truly\ntransformative AI assistant could automate tasks in software appli-\ncations, manage household chores, or even engage with children in\nmeaningful ways. The realization of such capabilities would mark\na revolutionary shift in how we integrate AI into our daily lives, en-\nabling widespread automation and augmenting human capabilities\nacross diverse environments [54].\nAchieving this vision requires LLMs to extend their expertise\nfrom language processing to action generation. However, this tran-\nsition is not straightforward. While leading LLMs from industry\ngiants have demonstrated impressive performance in language-\nbased tasks, they encounter substantial limitations when tasked\nwith action generation [79]. Completing a task in the real world\ninvolves a sequence of complex steps: accurately understanding\nuser intent, devising a plan, and executing the necessary actions"}, {"title": "2 LARGE ACTION MODELS 101", "content": "Large Action Models (LAMs) represent a significant advancement\nin artificial intelligence, extending the capabilities of Large Lan-\nguage Models (LLMs) [85]. While LLMs are proficient at generating\nhuman-like text based on user inputs, LAMs go beyond text gener-\nation by performing actions in both physical and digital environ-\nments [82]. These models interpret user intentions from various\ndata forms, automate entire processes as per user requirements,\nplan for task completion, and interact with the world. This evo-\nlution signifies a shift from mere language interaction to action\nsequences that are grounded in real-world contexts."}, {"title": "2.1 Large Language Models", "content": "LLMs are neural networks with billions to hundreds of billions of\nparameters, trained on extensive text corpora to address general-\npurpose language tasks [31, 40, 75, 84, 92]. These models demon-\nstrate exceptional capabilities in natural language understanding\nand generation, allowing them to perform complex tasks such as an-\nswering questions [27], generating code [86], and providing human-\nlike textual responses [10] with minimal task-specific training,\nknown as zero-shot [69] or few-shot [4] learning. Unlike tradi-\ntional language models, which required extensive task-specific data\nand training, LLMs leverage their vast knowledge base to generalize\nacross diverse tasks with minimal supervision.\nWhile LLMs possess significant language understanding and\ngeneration capabilities, they are primarily limited to generating text-\nbased outputs. They excel at interacting with users and generating\ntext, but they lack the ability to directly interface with environments\nto execute actions. This limitation restricts their applicability in\nscenarios that require tangible interaction with digital or physical\nenvironments.\nTo extend their utility, LLMs are often embedded within agent\nframeworks [65]. These agent systems augment LLMs, enabling\nthem to interact with dynamic environments by collecting data from\nvarious sources [72], structuring it into meaningful inputs [32], and\nprompting the LLM for inference [72]. The agent then interprets\nthe model's output-whether in the form of code [67] or tool-based\nactions [54]-and grounds it within the environment by executing\nactions and collecting feedback [58]. Agents equipped with LLMs\ntypically function in a loop, continuously gathering environmental\ninformation, using LLM inference to form plans, executing those\nplans, and refining future actions based on feedback. This iterative\nprocess can incorporate external memory systems, enabling the\nagent to track historical actions and environmental states, further\nimproving the decision-making process over time [24, 89]."}, {"title": "2.2 From LLMs to LAMS", "content": "LAMs build upon the foundational capabilities of LLMs but are\nspecifically optimized for action-oriented tasks. They are designed\nto perform actions in both physical and digital environments, in-\nterpreting user intentions from various data forms, automating\nprocesses as per user requirements, planning for task completion,\nand interacting with the world [82]. This evolution signifies a shift\nfrom passive language interaction to generating action sequences\nthat are grounded in real-world contexts.\nFurthermore, due to their specialization in specific domains or\ntasks, LAMs can be smaller in scale compared to general-purpose\nLLMs while achieving comparable or superior performance within\ntheir operational scope. By focusing on a narrower range of tasks,\nLAMs prioritize efficiency and effectiveness, leveraging targeted"}, {"title": "2.3 Key Characteristics of LAMs", "content": "LAMs are distinguished by advanced capabilities that enable them\nto perform complex tasks effectively. These characteristics include:"}, {"title": "2.3.1 Interpretation of User Intentions", "content": "A fundamental capability\nof LAMs is the ability to accurately interpret user intentions from\ndiverse forms of input. These inputs may include natural language\nrequests, voice commands, images, or videos, such as device screen-\nshots or instructional videos [8]. User inputs are often abstract or\nimplicit [6], requiring LAMs to leverage their internal knowledge\nand complementary information to discern the true intent behind\nthe input. This process involves understanding nuances, disam-\nbiguating instructions, and inferring unstated objectives. LAMS\nmust translate these user intentions into actionable plans and steps,\nfacilitating subsequent interactions with the environment to fulfill\nthe user's objectives. This requires a robust foundation in LLMs,\nparticularly those with multi-round conversational capabilities [57],\nenhancing LAMs' proficiency in engaging with users to accurately\nunderstand and execute their requests."}, {"title": "2.3.2 Action Generation", "content": "The hallmark feature of LAMs is their\ncapacity for action generation grounded in the environment. LAMs\ntranslate user intentions into actionable steps that can be executed\nwithin specific contexts. These actions can take various forms: op-\nerations on graphical user interface (GUI) elements, API calls for\nsoftware applications, physical manipulations performed by robots,\ninvoking other Al agents or models, or autonomously generat-\ning code or combining meta-actions [5]. By incorporating detailed\nknowledge of the environment, including available actions, system\nstates, and expected inputs, LAMs can select appropriate actions\nand apply them correctly to meet user requests. This involves not\nonly executing predefined actions but also adapting to new situa-\ntions by generating novel action sequences when necessary."}, {"title": "2.3.3 Dynamic Planning and Adaptation", "content": "LAMs exhibit a sophisti-\ncated capability for dynamic planning and adaptation, which is cru-\ncial for handling complex user requests that span multiple steps [19].\nThey can decompose a complex task into several subtasks, each\nfurther broken down into specific action steps. This hierarchical\nplanning enables LAMs to approach task execution with a forward-\nlooking perspective, anticipating future requirements and potential\nobstacles. Moreover, as the execution of each action alters the state\nof the environment, LAM will react to these changes, adapting and\nrevising their plans and actions accordingly [58]. This flexibility\nensures robustness in dynamic scenarios where deviations from\ninitial expectations are common. For instance, if an unexpected"}, {"title": "2.3.4 Specialization and Efficiency", "content": "LAMs are fine-tuned for ex-\necuting specialized sequences of actions within specific environ-\nments [8]. By focusing on particular domains, LAMs achieve a\nhigh degree of accuracy and adaptability, outperforming general-\npurpose LLMs in targeted applications. This specialization allows\nLAMs to encode comprehensive knowledge about the environment\ndeeply into their architecture, including available actions, system\nconstraints, and contextual nuances. As a result, LAMs can operate\nmore efficiently, reducing computational overhead and improving\nresponse times. Furthermore, since LAMs are expected to com-\nplete actionable tasks within a more limited scope, their scale can\nbe smaller compared to general-purpose LLMs while achieving a\ncomparable level of performance within that specific domain. This\nmakes LAMs more practical for deployment in real-world applica-\ntions, including resource-constrained environments such as edge\ndevices or local systems."}, {"title": "2.3.5 Summary", "content": "In summary, LAMs transcend the basic function-\nality of converting user requests into a series of steps by compre-\nhending the underlying logic that interconnects and contextual-\nizes these actions. They understand sequence dependencies-why\ncertain steps must precede or follow others-and recognize when\nto adapt the plan to accommodate changing circumstances. LAMs\nextend Al systems into the realm of actionable intelligence. This sig-\nnificantly enhances their ability to autonomously perform complex,\nreal-world tasks, making them invaluable in applications requiring\nprecise interaction and manipulation within defined operational\ncontexts."}, {"title": "2.4 From Inception to Implementation", "content": "LAMs have the potential to significantly extend the impact of LLMs\nby enabling tangible interactions with real-world environments.\nTo harness this potential, an LAM must be developed from the\nground up and deployed within a real-world application, allowing\nit to operate effectively in a physical environment. This process\ninvolves 5 critical steps, as shown in Figure 3:\n(1) Data Collection and Preparation (Section 3): The first\nstep involves gathering and curating the necessary data for\nthe specific use case. This includes not only user queries but\nalso environmental context, potential actions, and any other\nrelevant data required to train the LAM effectively. The data\nmust undergo cleaning and pre-processing before it is used\nfor training or fine-tuning a LAM.\n(2) Model Training (Section 4): Using the prepared data, the\nnext step is to train the LAM. This training process can\ninvolve various techniques such as supervised fine-tuning\nand reinforcement learning to ensure the model can perform\nthe desired actions accurately and efficiently.\n(3) Offline Evaluation (Section 5): After obtaining the LAM,\nwe evaluate its performance using an offline dataset to verify\nits reliability in a controlled, static environment.\n(4) Integration and Grounding (Section 6): The LAM is inte-\ngrated into an agent framework that serves as its operational"}, {"title": "3 DATA COLLECTION AND PREPARATION", "content": "Data is a cornerstone in training LLMs, where high-quality data\nsignificantly enhances their performance [35, 68]. Similarly, LAMs\nrequire well-prepared, high-quality action-oriented data during\nthe supervised fine-tuning phase. Off-the-shelf LLMs often face\nchallenges when interacting with real-world environments. These\ndifficulties typically arise from either a lack of domain-specific\nknowledge or the generation of hallucinated outputs that fail to\nbe actionable. To mitigate these issues, we adopt a two-phase data\ncollection approach: task-plan collection and task-action collection,\nas shown in Figure 4. Specifically:\n(1) Task-Plan Data Collection: In this phase, we collect data\nconsisting of tasks and their corresponding plans. Tasks are\nuser requests expressed in natural language, while plans are\ndetailed, step-by-step procedures designed to fulfill these\nrequests. For example, a task such as \"How to change the\nfont size in Word?\" would have a corresponding plan out-\nlining the steps required to complete the task. This data is\nused to fine-tune the model to generate effective plans and\nimprove its high-level reasoning and planning capabilities.\nHowever, task-plan data cannot be directly executed in the\nenvironment, requiring the following data conversion phase.\n(2) Task-Action Data Collection: In this phase, the task-plan\ndata is converted into task-action data, which includes tasks,\nplans, and the associated action sequences needed to execute\nthose plans. Tasks and plans are refined to become more con-\ncrete and grounded within a specific environment. Action\nsequences are generated at this stage, such as select_text(\ntext=\"hello\") or click(on=Button(\"20\"), how=\"left\",\ndouble=False), which represent actionable instructions ca-\npable of directly interacting with the environment. This en-\nriched data provides the necessary granularity for training\nan LAM to perform reliable and accurate task executions in\nreal-world scenarios.\nThe task-plan data aims at enhancing the model's high-level\nplanning capabilities, allowing it to generate detailed, step-by-step\nplans based on user requests. Meanwhile, the task-action data fo-\ncuses on refining the model's ability to execute these plans by\nconverting each planned step into a concrete, executable step or\nsequence while considering environmental feedback. The data col-\nlection and preparation pipeline ensures that the model is capable\nof both high-level planning and low-level action execution, thereby\nbridging the gap between natural language plans and executable\nactions.\nIn the following sections, we detail the methodologies employed\nfor data collection, pre-processing, and integration of task-plan and\ntask-action data. We illustrate how these steps enable the LLM to\nLAM transformation."}, {"title": "3.1 Task-Plan Data", "content": "Figure 5 outlines a multi-step pipeline for collecting and processing\ntask-plan data, essential for training LAMs. The process begins with\ngathering raw data from diverse sources, including application\ndocumentation, WikiHow, and historical search queries. This is\nfollowed by structured pre-processing to ensure that the data is\nhigh-quality and relevant to specific tasks."}, {"title": "3.1.1 Data Sources", "content": "(1) Application Documentation: Documentation and usage man-\nuals for software applications provide authoritative task de-\nscriptions. These resources, maintained by product teams, are\nconsidered highly reliable. Relevant documentation, such as\nM365 documentation, is crawled, with outdated or inaccessi-\nble pages being filtered out. The HTML content is converted\ninto markdown format, and GPT-40 is used to extract task-plan\npairs in the desired structured format.\n(2) WikiHow: WikiHow hosts a wide range of how-to articles,\nincluding application-specific operational guides. Webpages\nrelated to Windows platform applications are crawled, and GPT-\n40 extracts task and plan components, ensuring the resulting\ndata aligns with the desired structured format.\n(3) Historical Search Queries: Search engine logs provide insight\ninto real user demands, addressing gaps not covered by formal"}, {"title": "3.1.2 Data Extraction and Pre-Processing", "content": "The initial step in pro-\ncessing raw data involves parsing to extract task-relevant content\nwhile filtering out unnecessary or irrelevant information. This in-\ncludes removing non-English entries, samples that are excessively\nshort or long based on predefined heuristics, and data unrelated to\nactionable tasks (e.g., content focused on smartphone operations).\nThe filtered data is then standardized into a unified format for\nfurther processing."}, {"title": "3.1.3 Data Construction", "content": "To create structured JSON samples, GPT-\n4o is employed to extract and format tasks along with their associ-\nated plans. For historical search queries, synthetic data is generated\nto enrich the raw input, addressing the common issue of insuffi-\ncient context. GPT-40 reformulates these queries into complete,\nsentence-like user requests, ensuring consistency across all data\nsources and facilitating effective downstream processing.\nThe resulting dataset contains structured JSON samples, with\neach entry including a unique task identifier (task_id), the task\ndescription (task), and a step-by-step plan (plan). An example is\nshown below:"}, {"title": "3.1.4 Data Evolving", "content": "With the initial dataset processed, we employ\ndata augmentation techniques to enhance its diversity and complex-\nity. Inspired by WizardLM [74] and AgentGen [23], we use GPT-40\nto evolve the raw task to generate new task-plan pairs, improving\nthe model's ability to follow instructions and handle more complex\ntasks.\nThe data evolving process generates new tasks from the original\nones by introducing additional complexity, constraints, or steps\nwhile preserving relevance. The guidelines for task evolution are\nas follows:\nThe evolved task must be executable step-by-step on a Win-\ndows OS or application.\nThe evolved task should include additional requirements,\nincreasing its complexity without exceeding 20 extra words.\nThe evolved task must remain concise and related to the\noriginal task.\nFor each evolved task, GPT-40 generates a corresponding plan\nadhering to the following guidelines:\nThe plan must provide correct and actionable steps for Win-\ndows environments or applications.\nThe plan should be concise and highlight critical action ob-\njects using bold emphasis.\nThis augmentation process results in a richer dataset where tasks\nbecome progressively more challenging, and plans incorporate\ndomain-specific knowledge."}, {"title": "3.2 Task-Action Data", "content": "The task-plan data collected in the previous stage provides high-\nlevel, step-by-step plans for resolving user-requested tasks, serving\nas general guidelines. However, these plans are textual and not\ndirectly executable in a real-world environment. For instance, a\ntask-plan data sample for the task \"Highlight text in document\"\noutlines the necessary steps but does not translate into actionable\ninstructions for interacting with the application's GUI. This gap\nhighlights the need for actionable task-action data to bridge the\ndivide between planning and execution. To enable LAMs to pro-\nduce actionable outputs, we generate task-action data derived from\nthe previously collected task-plan data. Task-action data captures\nthe granular interactions required to complete a task in the appli-\ncation environment, including GUI navigation, button clicks, and\nresponding to environmental feedback.\nTraditional approaches for action data collection often involve\nmanual or agent-based annotation for each task, which is both\ncostly and labor-intensive. To address these limitations, we propose\nan efficient, fully automated, and low-cost pipeline that leverages\nLLMs and real-world application interactions. This pipeline consists\nof four stages, as depicted in Figure 6: Instantiation, Execution,\nEvaluation, and Post-Processing. Specifically,\n(1) Instantiation: In this stage, the task-plan data is trans-\nformed into an executable trajectory. Using an LLM, each\ntask is instantiated with specific operational objects, and re-\nlated high-level plan is instantiated into a concrete sequence\nof actions that can be directly executed in the application\nenvironment.\n(2) Execution: The instantiated trajectory is then executed\nwithin the real-world application environment. During this\nstage, the system interacts with the application's GUI to\ncarry out the specified actions. For example, the instantiated\ntrajectory for highlighting text would involve selecting the\nappropriate text, navigating to the highlight tool, and apply-\ning the highlight. The result of this execution is the captured\nexecuted trajectory, including any feedback or environmen-\ntal changes observed during the process.\n(3) Evaluation: Once the execution is complete, the trajectory\nis evaluated for correctness using an LLM. The evaluation\nstage verifies whether the executed trajectory successfully\naccomplishes the intended task. This involves comparing\nthe observed outcomes with the expected results outlined\nin the task-plan data. Tasks that fail to meet the criteria are\nflagged for review, while successful executions are retained\nfor further processing.\n(4) Post-Processing: In the final stage, successful task-action\ntrajectories undergo post-processing to ensure consistency,\ncompleteness, and readiness for training. This includes refin-\ning the data format, ensuring compatibility with the training\npipeline, and annotating the data with relevant metadata\n(e.g., task IDs, execution time, and step-by-step feedback).\nThe post-processed task-action data is then added to the\ntraining dataset, enabling the LAM to learn from real-world\ninteractions.\nThe pipeline minimizes human intervention and reduces the num-\nber of LLM calls required, significantly improving scalability and\nefficiency."}, {"title": "3.2.1 Instantiation", "content": "The task-plan data are primarily collected from\nhelp documents or public websites, creating a gap between the\ngeneralized task-plan data and the specific requirements needed\nfor execution within a particular environment. A common issue\nis the lack of specificity. For instance, the task \"highlight text in\ndocument\" does not specify actionable objects, such as \"which text\""}, {"title": "3.2.2 Execution", "content": "To ensure that the steps in the instantiated task-\nplan data are accurate and truly actionable, the execution stage\nverifies the action sequence by matching control items with the\nreal application environment and performing the specified actions.\nThis process validates the task-action data, ensuring its correctness\nand compatibility with the application GUI.\nFor instance, as shown in Figure 7, the control item \"Text High-\nlight Color\" with its associated control label is retrieved using the\naction text \"Highlight\" from the control item pool. The correspond-\ning task-action data is then executed in the application without\nfurther intervention from the LLM. During execution, if an error\noccurs (e.g., a mismatch between the predicted control item and the"}, {"title": "3.2.3 Evaluation", "content": "Even if the task-action data is successfully ex-\necuted in the real application without errors, further evaluation\nis required to ensure its validity. Some tasks may be incorrectly\ninstantiated from the task-plan data, resulting in trajectories that,\nwhile executable, do not fulfill the original task description. Simi-\nlarly, the executed results might fail to align with the intended task\noutcomes. For evaluation, we utilize the instantiated task along\nwith its execution trajectory, which includes:\nConsecutive actions performed during execution.\nScreenshots captured before and after each action.\nEnvironmental changes observed between the initial and\nfinal states\u00a7.\nUsing this comprehensive trajectory, we prompt GPT-40 to evaluate\nwhether the executed task aligns with the original task description\nand achieves successful completion. The evaluation considers both\nthe sequence of actions and the resulting application state. The\nprocess assigns a \"task-complete\" key to indicate the outcome as"}, {"title": "3.2.4 Post-Processing", "content": "As noted in Section 3.2.2, a trajectory was\nrecorded during the execution process. This trajectory includes:\nScreenshots captured at each step.\nEnvironment states before and after each action.\nPlans and corresponding actions for every step.\nDuring the post-processing stage, these trajectories are combined\nwith the original task requests to generate synthetic step-wise\ntraining data. The resulting data format uses the task request as\ninput and LAM's plan and actions as output. This structured format\nis critical for training LAMs to map task requests to actionable\nsequences effectively."}, {"title": "4 MODEL TRAINING", "content": "Our objective is to develop an LAM from scratch that can map\nuser inputs to appropriate plans and executable actions, ultimately\nenabling complex task completion. To achieve this, we adopt a\nstaged training strategy consisting of four phases, each building\nupon the previous one. As illustrated in Figure 8, these phases guide\nthe model from learning structured task plans, to imitating expert\ndemonstrations, to self-boosting from its own successes, and finally\nleveraging reward-based optimization. Throughout these stages,\nthe model progressively evolves from LAM\u00b9 to LAM4.\nAt a high level, Phase 1: Task-Plan Pretraining provides a\nstrong foundation by teaching the model to generate coherent, step-\nby-step plans for various tasks. Phase 2: Learning from Experts\nthen introduces action trajectories labeled by GPT-40, enabling\nLAM\u00b2 to align its plan generation with actionable steps. However,\nrelying solely on expert successes limits diversity and adaptability.\nTo address this, Phase 3: Self-Boosting Exploration encourages\nthe model to tackle tasks that even GPT-40 failed to solve, au-\ntonomously generating new success cases and evolving into LAM\u00b3.\nFinally, Phase 4: Learning from a Reward Model incorporates\nreinforcement learning (RL) principles, allowing LAM\u00b9 to learn\nfrom both successes and failures, refining its decision-making in\ncomplex, previously unseen scenarios. Table 1 summarizes the data\nused in each phase. Each phase uses different training objectives,\nnamely (i) task-plan pretraining (phase 1) and (ii) decision-making\ntraining (phase 2-4), as detailed in Appendix E."}, {"title": "4.1 Phase 1: Task-Plan Pretraining", "content": "The initial stage focuses on imparting a broad understanding of\nhow tasks can be decomposed into logical steps. We start with\nMistral-7B [25] as the base model. A total of 76,672 task-plan pairs\n$(t_i, P_i)$ are collected from various sources, including application\nhelp documentation, WikiHow, and historical search queries. Of\nthese, 29,182 pairs are sourced directly, while 47,490 are gener-\nated via data evolution techniques (as described in Section 3.1.4),\nenriching the dataset with more complex and diverse tasks."}, {"title": "4.2 Phase 2: Learning from Experts", "content": "While LAM\u00b9 can produce structured plans, it lacks the ability to\nexecute them. In Phase 2, we introduce expert-labeled task-action\ntrajectories from GPT-40 (Section 3.2) to teach the model how to\nperform actions. The illustrative application in this paper is the Mi-\ncrosoft Word environment, where we have 2,192 successful expert\ntrajectories. Each trajectory consists of a sequence of state-action\npairs $(s_t, a_t)$, representing observed UI states and the corresponding\nactions to progress the task.\nWe split these 2,192 trajectories into a training set of 1,757 and\na test set of 435 trajectories, providing a total of 3,959 steps for\ntraining. By imitation learning LAM\u00b9 on these successful action\nsequences, we obtain LAM\u00b2. The objective is to minimize:\n$L_{SFT}(LAM) = \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{t=1}^{T_i} L_{CE}(LAM(s_t), a_t)$,\nwhere $N$ is the number of trajectories and $T_i$ is the number of\nsteps in trajectory $i$. By imitating the expert's policy, LAM\u00b2 trans-\nforms from a passive planner into a model capable of executing\nactions aligned with its plans, grounding its reasoning in the real\napplication environment."}, {"title": "4.3 Phase 3: Self-Boosting Exploration", "content": "Up to Phase 2, LAM\u00b2 only learns from successful trajectories pro-\nvided by GPT-40. This limits diversity and adaptability, as the model\nnever sees how to handle situations that even GPT-40 could not deal\nwith. To overcome this limitation, Phase 3 introduces self-boosting\nexploration.\nHere, we revisit failed GPT-40 trajectories, i.e., tasks that GPT-40\ndid not complete successfully, and let LAM\u00b2 attempt them. Using the\nReAct mechanism [58, 80], LAM\u00b2 interacts with the environment\nand tries alternative strategies for these challenging tasks. From\nthese attempts, we sampled 2284 GPT-40 failed tasks and then\ncollect 496 newly successful trajectories generated by LAM\u00b2 itself.\nThese self-labeled successes, combined with the original 2,192 GPT-\n4o successes, form an augmented dataset.\nWe then fine-tune LAM\u00b2 on this enriched data, yielding LAM\u00b3:\n$L_{SFT}(LAM) = \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{t=1}^{NT} L_{CE}(LAM(s_t), a_t)$.\nThis self-boosting step allows the model to learn from its own\nnewly discovered solutions, overcoming previous limitations and\nimproving adaptability. By leveraging planning knowledge from\nPhase 1 and expert strategies from Phase 2, LAM\u00b3 becomes more\nresourceful, even in scenarios with sparse or absent expert guidance."}, {"title": "4.4 Phase 4: Learning from a Reward Model", "content": "Despite the improvements, Phases 1-3 focus on successes or expert-\nlike behavior. They offer limited insights into intermediate decision\nquality and fail to exploit learning opportunities presented by failed\nattempts. In Phase 4, we integrate reinforcement learning (RL) to\naddress these shortcomings.\nTo this end, we design a two-stage approach, where we first\nThe reward model (RM) is built using LAM\u00b3 as the base model,\nwith an additional output layer added to produce scalar values\nrepresenting the quality of actions. Using the trained RM, we fine-\ntune LAM\u00b9 in an offline RL setting. Here, the model refines its\npolicy without additional environmental interactions, leveraging\npreviously collected trajectories to learn from failures and improve\naction selection."}, {"title": "4.4.1 Reward Model Training", "content": "First, we train a reward model (RM)\non both LAM\u00b3's successful (496) and failed (1788) trajectories and\nGPT-40's successful trajectories (2192) gathered in previous phases.\nAll steps in successful trajectories are assigned a reward of +1, and\nall steps in failed trajectories a reward of -1. This uniform, binary\nlabeling of outcomes ensures the RM consistently captures overall\ntrajectory quality. Formally:\n$r_t = RM(s_t, a_t; \\phi)$,\nwhere $\\phi$ presents the RM parameters, and $r_t \\in \\{+1,-1\\}$ is the\nassigned reward. The RM is trained via mean squared error (MSE)\nto approximate these ground-truth rewards.\nThe training dataset for the RM includes both failed and suc-\ncessful task-action trajectories generated by LAM\u00b3, as well as the\nsuccessful trajectories from the collected task-action data. All steps\nin successful trajectories receive a reward of +1, while every step in\nfailed trajectories is assigned a reward of -1. This uniform labeling\nstrategy ensures that the RM consistently reflects overall trajectory\nquality and effectively guides policy optimization."}, {"title": "4.4.2 Optimizing with Offline PPO", "content": "Armed with the RM to\nevaluate intermediate actions", "is": "n$L_{PPO}(LAM) = \\frac{1}{N} \\sum_{i=1}^{NT_i} min(\\frac{LAM_{\\theta}(a_t|s_t)}{LAM_{\\theta^{old}}(a_t|s_t)} A_t, clip(\\frac{LAM_{\\theta}(a_t|s_t)}{LAM_{\\theta^{old}}(a_t|s_t)}, 1 - \\epsilon, 1 + \\epsilon)A_t)$,\nwhere $A_t$ denotes the advantage derived from RM-generated re-\nwards, and $\\epsilon$ is a clipping parameter to ensure stable updates.\nBy incorporating signals from both successes and failures, LAM4\ngains a deeper understanding of action quality. This RL-"}]}