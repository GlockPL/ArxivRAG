{"title": "Outcome-Refining Process Supervision for Code Generation", "authors": ["Zhuohao Yu", "Weizheng Gu", "Yidong Wang", "Zhengran Zeng", "Jindong Wang", "Wei Ye", "Shikun Zhang"], "abstract": "Large Language Models have demonstrated remarkable capabilities in code generation, yet they often struggle with complex programming tasks that require deep algorithmic reasoning. While process supervision through learned reward models shows promise in guiding reasoning steps, it requires expensive training data and suffers from unreliable evaluation. We propose Outcome-Refining Process Supervision, a novel paradigm that treats outcome refinement itself as the process to be supervised. Our framework leverages concrete execution signals to ground the supervision of reasoning steps, while using tree-structured exploration to maintain multiple solution trajectories simultaneously. Experiments demonstrate that our approach enables even smaller models to achieve high success accuracy and performance metrics on competitive programming tasks, creates more reliable verification than traditional reward models without requiring training PRMs. Our approach achieves significant improvements across 5 models and 3 datasets: an average of 26.9% increase in correctness and 42.2% in efficiency. The results suggest that providing structured reasoning space with concrete verification signals is crucial for solving complex programming tasks.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have demonstrated remarkable capabilities in code generation tasks (Brown et al., 2020; OpenAI, 2023; Touvron et al., 2023a; Guo et al., 2024). However, when implementing algorithms with multiple components or handling intricate logic flows, these models often struggle to maintain reliability and consistency (Jiang et al., 2024b; Jimenez et al., 2023). This limitation becomes particularly apparent in problems requiring deeper algorithmic insights and careful consideration of trade-offs."}, {"title": "2 Preliminaries", "content": null}, {"title": "2.1 Process Supervision vs Outcome Supervision", "content": "Outcome supervision, the traditional paradigm in machine learning, assesses performance solely on final outputs (Chen et al., 2021b; Brown et al., 2020). This approach has evolved through LLM-as-a-judge methods, where language models are prompted (Zheng et al., 2023; Yu et al., 2024a) or trained (Wang et al., 2023c) to evaluate model outputs (Chang et al., 2023). While providing richer feedback than simple metrics, these methods still focus exclusively on final results.\nProcess supervision represents a paradigm shift by evaluating the quality of intermediate reasoning steps (Lightman et al., 2023; Luo et al., 2024). This paradigm employs specially trained Process Reward Models (PRMs) - an extension of LLM-as-a-judge - to evaluate each solution step (Lightman et al., 2023; Ma et al., 2023; Luo et al., 2024; Jiang et al., 2024a). PRMs have proven particularly effective in domains requiring complex reasoning, such as mathematical problem-solving and logical deduction, where they guide search algorithms toward better solution paths (Wang et al., 2024c,b).\nFormally, outcome supervision evalu=\nates only the final output: $R_{outcome}(y) = 1[output \\text{ is correct}]$. Process supervision, in\ncontrast, aggregates step-wise scores:\n$R_{process}(s_1,..., s_T) = \\sum_{t=1}^T PRM(s_t | s_{1:t-1})$,\nwhere $PRM(s_t|s_{1:t-1})$ evaluates the quality of step\n$s_t$ given the previous steps."}, {"title": "2.2 Execution-Driven Code Generation", "content": "Code generation is typically formulated as a sequence-to-sequence problem: given input specification x (including natural language description and test cases), generate a program y that correctly implements the required functionality (Jiang et al., 2024b). While most existing approaches treat this as a single-step generation process (Chen et al., 2021a), recent work has explored using execution feedback to guide code generation (Zhong et al., 2024; Zhang et al., 2023) or use CoT prompting to improve correctness (Shinn et al., 2024).\nAlthough these execution-guided approaches show promise, our experiments indicate they are insufficient for complex programming tasks that require deeper reasoning. While execution feedback is easy to measure, it alone provides little guidance on how to improve solutions that fail or how to make working solutions more efficient. More importantly, it offers no feedback during the intermediate stages of development, when early course corrections could prevent cascading errors.\nConsider implementing an efficient sorting algorithm: a model might write code that passes all test cases but uses an inefficient O(n\u00b2) approach. Outcome supervision would mark this as a success, missing the opportunity to guide the model toward a more optimal O(nlogn) solution. Similarly, if the code fails, a sparse \"fail\" signal provides no insight into whether the error lies in the algorithmic approach, the implementation details, or edge case handling. These limitations of both process and outcome supervision highlight the need to rethink how to supervise the development of complex programs, where both theoretical understanding and practical implementation must evolve together."}, {"title": "3 Methodology", "content": "We formulate code generation as a step-by-step reasoning process, where each state on our reasoning tree contains a combination of theoretical reasoning, code implementation, execution outcomes and step-level reward. This unified framework generalizes both outcome and process supervision by treating the refinement of outcomes as the process to be supervised. The overall approach is outlined in Figure 2 and formalized in Algorithm 1."}, {"title": "3.1 Outcome-Refining Process Supervision", "content": "Our framework introduces a novel supervision paradigm where the refinement of outcomes serves as the process to be supervised. Unlike traditional approaches that rely on linear Chain-of-Thought structures, we use beam search over tree structure where each state embodies the dual nature of code generation - theoretical understanding and practical implementation. The states evolve through a self-refining process: reasoning chains capture the changes and observations on theoretical approach and implementation strategy, while execution outcomes provide concrete signals for refinement. This interaction between reasoning and execution creates a refinement process: execution outcomes inform theoretical improvements, while enhanced reasoning guides better implementations.\nThe tree structure, combined with beam search, enables deeper reasoning exploration crucial for complex programming tasks. By maintaining multiple promising trajectories simultaneously, our framework allows diverse exploration of different strategies - a capability particularly important for problems requiring extensive reasoning and algorithmic insights. Through comprehensive state representation including reasoning chains, code, and execution feedback, inferior solution paths are naturally replaced when better alternatives are discovered. Our experimental results in Figure 4 demonstrate this outcome-guided exploration scales effectively with increased computational budget, which suggests the importance of providing sufficient reasoning space for complex programming tasks."}, {"title": "3.2 Self-Critic with Generative Rewarding", "content": "Traditional PRMs are trained in a discriminative manner, producing only numerical scores without explaining their evaluation rationale. This design fails to leverage language models' inherent ability to reason about their judgments. Our framework takes a different approach by treating process reward generation as a reasoning task itself. Before assigning scores, the model first articulates its analysis of the reasoning chain and execution outcomes, considering factors like algorithmic complexity, code structure, and performance metrics. This generative approach allows the model to benefit from its own chain-of-thought process when making evaluations.\nOur experiments Table 4 also demonstrate that this generative reward mechanism outperforms traditional discriminative PRMs, even for explicitly trained ones. The improvement stems from the model's ability to perform detailed analysis before making judgments, rather than directly mapping inputs to numerical scores."}, {"title": "3.3 Execution-Guided Process Reward Model", "content": "While traditional PRMs rely on expensive training data, we leverage concrete execution signals to ground the model's judgment. Existing methods use PRMs that are trained on human-annotated data to learn evaluation criteria for reasoning steps. Ours eliminates this training requirement through a key insight: since PRM training essentially grounds the model's judgment through human-defined criteria, we can achieve similar grounding by providing concrete execution signals directly to the language model. This allows execution outcomes to serve as objective anchors for evaluation while preserving the model's inherent reasoning capabilities. The process rewards emerge from holistic analysis across multiple dimensions: the soundness and coherence of reasoning steps, the relationship between reasoning and practical implementation, and concrete execution metrics including correctness, performance, and resource usage. This grounded approach offers eliminates PRM training costs, reduces hallucination through concrete verification, and enables reliable evaluation through objective metrics."}, {"title": "4 Experiments", "content": "Our experimental evaluation aims to address three key questions: (1) How effective is our framework"}, {"title": "4.1 Experimental Setup", "content": "Datasets. We evaluate on 3 programming benchmarks as shown in Table 2. LBPP is a recent complex programming dataset manually curated by human experts with competitive programming experience. HumanEval and MBPP are popular code generation benchmarks but could be too easy for current LLMs (Matton et al., 2024). Moreover, a significant proportion of their data is leaked in popular pre-training corpora (Riddell et al., 2024). To ensure reproducibility, we report our detailed hyperparameters in Appendix A, we also open-source all our code and scripts in the aforementioned URL.\nBaselines. For outcome supervision, Reflexion (Shinn et al., 2024) is a recent self-improvement approach that leverages execution feedback for"}, {"title": "4.2 Main Results", "content": "Table 1 shows the comparative results of our method and baselines, Figure 3 provides detailed multi-dimensional profiling of the performance of generated solutions with different methods.\nOur results indicate significant improvements in both correctness and code quality metrics, especially on harder benchmarks. Even a smaller model (Qwen 7B), when paired with our method, could surpass its larger variant (Qwen 14B) without our method, suggesting that providing sufficient reasoning space can be more effective than solely scaling model parameters - which is significantly more computationally expensive. This finding has important implications for practical applications where computational resources are limited.\nWhen compared to other execution-feedback and outcome reward based methods like Reflexion and LDB, our approach consistently demonstrates superior performance regardless of test case access. This improvement stems from a fundamental difference in approach: while these outcome-based methods focus primarily on local information like resolving execution errors and reasoning in chain structure, our method provides LLMs with broader reasoning space to reflect on higher-level aspects such as algorithm selection and problem properties by using process reward guided search. For"}, {"title": "4.3 Component-wise Ablation Study", "content": "We conducted experiments on the challenging LBPP dataset using the Qwen-7B model to inves-"}, {"title": "4.4 Analysis of Process Reward Model", "content": "Our framework uses an implicit process reward model (PRM), which provides outcome-level supervision during beam search without additional training. Most existing work on process supervision generates line-level process reward signals and relies on explicitly trained PRMs. This motivates us to explore two questions: (1) Are outcome-level rewards more effective than line-level rewards? (2) Is an implicit PRM that does not require training better than an explicitly trained PRM?\nTo address these questions, we conduct experiments on the LBPP dataset using the Qwen-7B model. For the outcome-level process supervision method, the implementation details are consistent with the corresponding parts of ORPS. For the line-level method, the model generates step-by-step thoughts for the coding problem, with numerical process rewards assigned to each step. The final code is then generated based on the best thought trace. For methods requiring explicit training, we randomly select half of the LBPP dataset as a training set to avoid data leakage. To simulate human-annotated process feedback, we filter data from GPT-4's outputs.\nResults in Table 4 confirm that our framework substantially outperforms the other three method, validating our design choices. Overall, outcome-level reward signals prove to be more effective than line-level signals. Intuitively, line-level signals can only provide feedback for incomplete thought processes, which undoubtedly lack more information compared to the outcome-level.\nAdditionally, the implicit PRM shows greater effectiveness than the explicit PRM. This suggests that external process supervision feedback may not always be reliable. We consider that LLMs already have strong self-reflection capabilities and only require execution outcomes to activate this ability. This also indicates that spending extra data and time on training reward models might be unnecessary."}, {"title": "4.5 Scaling Analysis", "content": "In addition, we wanted to explore how the performance of ORPS changes as reasoning overhead increases. For comparison, we chose BoN as the baseline. This is because BoN allows easy control of reasoning overhead with linear growth. We conducted experiments on two models using the most challenging LBPP dataset. The results are shown in Figure 4. With the same model, ORPS improved much faster as reasoning overhead increased. This shows that ORPS has strong scaling potential. It can effectively use more computational resources to improve reasoning. In comparison, BoN showed slower improvements, suggesting it does not fully utilize the increased reasoning capacity."}, {"title": "4.6 Case Studies", "content": "We also analyzed the improvements of ORPS across different problem categories. As shown in Figure 5, on the competitive programming dataset LBPP, our method shows significant improvements over the CoT Baseline, especially in more difficult categories. For instance, in complex algorithmic tasks such as dynamic programming, loops, and graphs, our method correctly solves nearly twice as many problems as CoT. This further confirms that high-quality intrinsic reasoning can help models avoid logical pitfalls when tackling difficult coding tasks.\nThrough detailed case studies, we demonstrate how our framework enhances code generation by improving reasoning. As shown in Appendix C, the response generated by the traditional CoT method for the Minimum Greatest Common Divisor problem in LBPP demonstrates that while the model provides a detailed step-by-step thought process during solution generation, the complexity of the task results in an imperfect code implementation. For instance, in CoT's approach, the reliance on nested loops and pairwise GCD calculations introduces inefficiencies and fails to address scalability for larger datasets. Similarly, our method's initial implementation demonstrates a lack of robustness in handling edge cases and unnecessary redundancies in subset formation.\nHowever, ORPS achieves a more accurate solution through finer reasoning. The code initially generated by our model contains redundancies and erroneous logic. Nevertheless, with the feedback from the critic on the execution outcomes, the programmer successfully refines the code to reach a correct implementation. This iterative process not only eliminates logical errors but also optimizes performance, demonstrating the advantage of integrating structured feedback into code generation."}, {"title": "5 Conclusion", "content": "In this study, we introduced Outcome-Refining Process Supervision (ORPS), a novel paradigm for enhancing code generation through structured reasoning and execution-driven feedback. By leveraging a tree-structured exploration framework, ORPS facilitates diverse solution trajectories, enabling models to refine both theoretical reasoning and practical implementation simultaneously. Our approach demonstrated significant improvements in correctness and efficiency across various benchmarks, including an average Pass@1 increase of 26.9% and a 42.2% reduction in runtime, outperforming traditional outcome-based and process-based supervision methods.\nKey findings reveal that structured reasoning space and concrete feedback signals are pivotal for solving complex programming tasks. ORPS proved effective even with smaller models, underscoring the importance of reasoning capabilities over mere model scaling. Furthermore, our framework's reliance on execution feedback eliminates the need for expensive, annotated training data, making it a cost-efficient alternative.\nThese contributions highlight the potential of process supervision to enhance complex problem-solving abilities in LLMs. Future work could extend this framework to other domains requiring rigorous reasoning and verification. By bridging the gap between reasoning quality and execution fidelity, ORPS paves the way for more au-"}, {"title": "6 Limitations", "content": "While ORPS demonstrates significant advancements in code generation, it has notable limitations. First, its reliance on execution outcomes as primary feedback restricts its applicability to tasks where execution is feasible and well-defined. Ambiguous problem descriptions or creative tasks beyond executable code can hinder the framework's performance. Additionally, this reliance may lead to a bias toward solutions optimizing immediate execution success, potentially overlooking broader algorithmic considerations.\nThe tree-structured beam search also introduces significant computational overhead, requiring substantial memory and processing power. While scalable with increased resources, this approach may be impractical for real-time or resource-constrained applications, limiting its utility in some environments.\nFinally, the quality of ORPS depends heavily on the underlying language model. If the model lacks domain knowledge or reasoning ability, it may generate incoherent reasoning chains or fail to explore promising solutions effectively, especially when feedback mechanisms rely solely on execution outcomes without additional validation."}, {"title": "7 Ethical Considerations", "content": "While ORPS advances code generation capabilities, we acknowledge important ethical considerations. Like all LLM-based systems, our framework inherits potential biases from the underlying models. However, our execution-guided approach provides an advantage: concrete verification helps detect and mitigate certain failure modes through objective testing. The framework's ability to explore multiple solution paths also reduces the risk of being stuck with problematic implementations.\nWe emphasize that ORPS is designed as a development aid rather than a replacement for human programmers. The execution feedback and reasoning traces make the code generation process more transparent and auditable. We encourage using this framework in conjunction with established software development practices, including code review and testing, particularly for applications in sensitive domains."}, {"title": "A Experimental Setup and Hyperparameter Details", "content": "This appendix provides a comprehensive description of the experimental setup, encompassing the hyperparameters, software, and hardware configurations employed in this study."}, {"title": "A.1 Search Algorithm Hyperparameters (ORPS)", "content": "The following hyperparameters were used for the search algorithm in ORPS:\n\u2022 Search Depth (num_rounds): 5. This parameter defines the maximum depth of the search tree, representing the number of iterative steps in the search process.\n\u2022 Beam Width (top_k): 3. This parameter specifies the number of highest-scoring candidate solutions (traces) retained at each step of the beam search.\n\u2022 Expansion Factor (num_samples): 20. This represents the number of new states (candidate solutions) explored from each state during the search process."}, {"title": "A.2 Inference Configuration", "content": "All inference experiments were conducted on a single machine using the FreeEval (Yu et al., 2024b) codebase, integrated with Hugging Face's text-generation-inference toolkit for efficient model serving. The following inference settings were applied:\n\u2022 Maximum Context Length (max_tokens): 18,000 tokens. This parameter defines the maximum number of tokens allowed in the input sequence to the model.\n\u2022 Generated Tokens per Round: 1,500 tokens. This specifies the number of new tokens generated by the model in each round of inference."}, {"title": "A.3 Execution Constraints", "content": "To ensure consistent and reproducible results, the following execution constraints were enforced during inference:\n\u2022 Timeout per Test Case: 5 seconds. This limits the maximum execution time allowed for each test case."}, {"title": "A.4 Model Training Configuration", "content": "This section outlines the hyperparameters and settings used during the training phase of the model, which was pertinent to the analysis experiments (subsection 4.4). While ORPS itself does not require training, these details are provided for completeness and reproducibility.\n\u2022 Training Framework:\nllamafactory (Zheng et al., 2024)\n\u2022 Optimization Framework: DeepSpeed\nZeRO3 (Rajbhandari et al., 2020) (Zero Re-dundancy Optimizer Stage 3). This enables\nefficient training of large models by partitioning optimizer states, gradients, and model parameters across data parallel processes.\n\u2022 Base Model:\nqwen-2.5-coder-7b-instruct.\nThis is the pre-trained language model upon which\nfurther training was conducted.\n\u2022 Batch Size per Device: 2. This defines the number of training examples processed on each GPU before a gradient update step.\n\u2022 Gradient Accumulation Steps: 4. This allows simulating a larger effective batch size by accumulating gradients over multiple forward and backward passes before updating model weights. The effective batch size is therefore 8 (2 per device * 4 steps).\n\u2022 Learning Rate: 2 \u00d7 10\u22125. This parameter controls the step size taken during gradient-based optimization.\n\u2022 Learning Rate Scheduler: Cosine decay. This gradually reduces the learning rate over the course of training, following a cosine function.\n\u2022 Number of Training Epochs: 2.0. This specifies the number of complete passes through the entire training dataset."}, {"title": "A.5 Hardware Environment", "content": "All experiments were performed on NVIDIA A800 GPUs, each equipped with 80GB of GPU memory."}, {"title": "BAI Usage in Code Development", "content": "During the development of ORPS and the design of its experiments, LLMs were employed to assist with coding. All AI-assisted code were reviewed and refined by the authors to ensure correctness and alignment with the research goals."}, {"title": "C Example Model Outputs", "content": "To illustrate the effectiveness of our approach, we present a representative example from the LBPP"}]}