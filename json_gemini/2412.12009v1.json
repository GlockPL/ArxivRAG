{"title": "SPEECHPRUNE: Context-aware Token Pruning for Speech Information Retrieval", "authors": ["Yueqian Lin", "Yuzhe Fu", "Jingyang Zhang", "Yudong Liu", "Jianyi Zhang", "Jingwei Sun", "Hai \"Helen\" Li", "Yiran Chen"], "abstract": "We introduce Speech Information Retrieval (SIR), a new long-context task for Speech Large Language Models (Speech LLMs), and present SPIRAL, a 1,012-sample benchmark testing models' ability to extract critical details from approximately 90-second spoken inputs. While current Speech LLMs excel at short-form tasks, they struggle with the computational and representational demands of longer audio sequences. To address this limitation, we propose SPEECHPRUNE, a training-free token pruning strategy that uses speech-text similarity and approximated attention scores to efficiently discard irrelevant tokens. In SPIRAL, SPEECHPRUNE achieves accuracy improvements of 29% and up to 47% over the original model and the random pruning model at a pruning rate of 20%, respectively. SPEECHPRUNE can maintain network performance even at a pruning level of 80%. This approach highlights the potential of token-level pruning for efficient and scalable long-form speech understanding.", "sections": [{"title": "I. INTRODUCTION", "content": "Speech Large Language Models (Speech LLMs) represent a significant advancement in speech language understanding and processing, as they leverage contextual reasoning capabilities of large language models to process audio inputs [1]. Unlike traditional cascaded pipelines, where automatic speech recognition (ASR) and language modeling are handled by separate modules, Speech LLMs unify audio processing, cross-modal fusion, and language modeling in a single architecture [2]. These unified models can perform multiple tasks like speech recognition, speech translation, speaker identification and emotion recognition, while maintaining end-to-end trainability [3-6].\nDespite the broad applications of Speech LLMs, there is one desirable functionality for these models that remains largely unexplored by existing work. Specifically, it is the capability of extracting crucial information within long-context audio, which we term Speech Information Retrieval (SIR). SIR is particularly relevant to real-world scenarios, which often require extracting key information from extended audio content, such as meetings, lectures, interviews, and customer service calls. For instance, the user may want the model (as an AI assistant) to accurately note down the time for a future event mentioned in a long conversation, so as to help them optimize their schedule. While straightforward to be accomplished by us humans, SIR is non-trivial and challenging for Speech LLMs. First, the target information is very likely to exist only in one short audio segment among the full, extensively long audio inputs. Precisely recognizing the relevant parts and ignoring the irrelevant parts is intuitively challenging for the models. Second, as we will discuss later, a more prohibitive limitation for Speech LLMs to perform SIR is their significant computational inefficiency when processing long audio token sequences.\nTo fill the research gap for SIR, our first contribution is a concrete task formulation and a rigorously constructed benchmark. Note that this effort is necessary and valuable because existing benchmarks for Speech LLMs mostly focus on tasks such as basic speech recognition, translation, and emotion detection, which all emphasize short-term capabilities. For example, 93% of the audio files in the Dynamic-superb phase- 2 benchmark [7] have a duration of less than 30 seconds. More recent benchmarks such as MMAU [8] (for complex reasoning) and AudioBench [9] (for instruction following) are still limited to short audio inputs (averaging 14.22 and 12.60 seconds respectively). These benchmarks contain only short audio clips and thus do not reflect the complexity of achieving long-context understanding and extracting precise information from lengthy audio sequences. To systematically assess the unique challenges posed by SIR, we present SPIRAL (Speech Informational Retrieval and Lookup), a 1,012-sample benchmark specifically crafted to evaluate Speech LLM performance on long-form audio sequences (around 90 seconds in duration). On a high level, SPIRAL constructs SIR questions by embedding a critical piece of information within lengthy and potentially distracting dialogues, thereby assessing the model ability to pinpoint and retrieve essential content from long-form inputs.\nPreliminary experiments on SPIRAL reveal limitations of current models in handling SIR tasks, due to fundamental archi- tectural constraints. Regardless of how audio inputs are encoded, Speech LLMs concatenate the derived audio tokens/embeddings with text tokens for later processing. However, audio signals typically yield substantially longer token sequences than text inputs, dominating the computational cost and leading to significant inefficiency due to the quadratic complexity of attention with respect to the input length [10]. In fact, most existing models limit the length of input audio files to only 30 seconds [7] (about 1500 raw tokens when using Whisper [11] for speech encoding, and models typically add adapters to downscale the number of tokens), as otherwise the audio token sequence could easily cause out-of-memory error on GPU. Obviously, such a limitation is restrictive for Speech LLMs to handle long-form audio inputs longer than 30 seconds.\nTo address the limitation, our second technical contribution"}, {"title": "II. SPEECH INFORMATION RETRIEVAL", "content": "We propose the SIR task to evaluate the ability of Speech LLMs to identify and extract critical information from extended spoken dialogues. This task addresses the practical challenge of finding key details within lengthy conversations, akin to finding a \"needle in a haystack,\" which is particularly challenging given most models' constraint of processing only 30-second audio segments.\nThe task is formulated as follows. Inputs include (1) a long- form speech input $A = a_1, a_2,..., a_n$ comprising sequential audio segments $a_i$, where each $a_i$ represents a continuous segment of the spoken dialogue, and (2) a textual query q that targets a specific piece of information mentioned or discussed at some unknown time within the speech. The model must process the entire sequence A to locate the relevant information that answers the query q.\nThis can be formally expressed as\n$r^* = f(A, q),$ (1)\nwhere $r^*$ stands for the correct response, f represents the model's function of processing speech, identifying salient information, and reasoning about the query. The critical information is contained within some segment $a_l$ at position l, but this location is not provided to the model explicitly, it must learn to identify and attend to relevant segments while processing the complete sequence.\nTo ensure accurate evaluation without ambiguity, we structure all queries as multiple-choice questions, following the estab- lished practice of multiple existing benchmarks [7\u20139]. Note, however, that the proposed SIR task can be easily generalized to open-ended questions as well. For each query q, the model selects from four possible responses $R = \\{r_1, r_2, r_3, r_4\\}$. This format allows for an objective evaluation of the model's dual capabilities: identifying relevant information in extended audio and understanding its semantic meaning."}, {"title": "B. Benchmark Construction", "content": "We introduce SPIRAL (Speech Information Retrieval And Lookup), a novel benchmark designed to evaluate Speech LLMs' ability to process long and realistic spoken inputs.\nThe samples in our dataset features three representative scenarios, including lectures, meetings, and daily conversations. Within each scenario, there are various fine-grained and specific topics that ultimately form a diverse and hierarchical topic structure for SPIRAL"}, {"title": "III. SPEECHPRUNE", "content": "Audio Encoder Speech LLMs typically consist of an audio encoder (such as Whisper [11]) which transforms raw audio with high sampling rates into lower-dimensional embeddings. Taking Whisper as an example, an audio input (with maximum length) is first processed and transformed into a 80-channel melspectrogram in the time-frequency domain. This 80-channel melspectrogram, generated with a window size of 25 ms and a hop size of 10 ms, is then fed into the Transformer-based encoder. A pooling layer with a stride of two follows to reduce the length of the audio representation. As a result, each frame of the encoder output approximately corresponds to a 40ms segment of the original audio signal. Thus, a 30-second audio yields 750 encoding embeddings. This temporal correspondence between audio frames and encoder outputs provides a natural foundation for our frame-level pruning strategy, as we can leverage the inherent structure of how speech information is encoded to maintain temporal coherence during pruning.\nLanguage Modeling After extracting the audio token, it is typically projected by an MLP [15] or Q-Former [16] to align the feature-wise dimensionality with text tokens. The audio token is then concatenated with the text token and other system prompts before being input to the LLM backbone [17]. In transformer-based models, the self-attention mechanism for each layer is computed as\n$Attention(Q, K, V) = softmax(\\frac{Q K^T}{\\sqrt{d_k}})V,$ (2)\nwhere Q, K, and V are the query, key, and value derived from the input sequence X through learnable projections:\n$Q = XW_Q,  K=XW_K, V=XW_V.$ (3)\nThe quadratic complexity $O(n^2)$ of self-attention mecha- nisms [18, 19] makes the length of audio tokens a critical computational bottleneck. For instance, a 10-minute conversa- tion with approximately 15,000 tokens requires 58.66 TFLOPS for Qwen-2 network [3], highlighting the need for efficient pruning strategies [20, 21]."}, {"title": "B. SPEECHPRUNE Methodology", "content": "We propose a two-stage token pruning approach, as shown in Fig. 1 and the following parts.\nFirst Level Pruning by Token-Text Similarity The first stage utilizes the correlation between audio and text tokens to identify semantically important audio segments. Recent research has shown that such audio-text token alignment enables effective cross-modal reasoning in speech language models [4]. More formally, we process the input to get speech embedding $S \\in R^{N \\times D}$ and text embedding $T \\in R^{L \\times D}$, where N is the number of speech tokens before pruning, L is the number of text tokens, and D is the embedding dimensionality. Here we only consider real text query as T and exclude system prompt and special tokens. The token-level similarity matrix $F \\in R^{N \\times L}$ between speech and text tokens is computed using cosine similarity:\n$F = \\frac{ST^T}{||S||_2 ||T||_2}$ (4)\nWe introduce an adaptive frame-level approach to enhance natural continuity and temporal correspondence. This method evaluates speech segments as one-second frames, aligning with the delta-band oscillations (1-2 Hz) that naturally process lexical and phrasal units in speech perception [22]. Given the speech embedding $S_t$, we obtain $m = [N/f]$ frames, where f is the frame size per second. For each frame i, the mean similarity score across text tokens is first computed, followed by frame-wise accumulation:\n$F_i = \\sum_{j=0}^{f-1} mean(F_{i.f+j,:, axis = 1}),$ (5)\nwhere $F_{i.f+j,:}$ represents the similarity scores between the j-th token in frame i and all text tokens. Token retention within each frame is determined by a softmax function applied to the accumulated frame scores:\n$p = softmax(F).$ (6)\nThe expected number of tokens to retain from each frame is\n$n_i = [Np_i],$ (7)\nwhere N denotes the overall number of tokens to be retained. For each frame i, we select the top-$n_i$ tokens based on their mean similarity scores:\n$indices_{first,i} = topk(mean(F_{i.f:(i+1).f,:, axis = 1), n_i),$ for i = 1,...,m, (8)\nwhere $F_{i.f:(i+1).f,;}$ represents the similarity scores of tokens within frame i across all text tokens.\nThe speech token remained after first level pruning is:\n$S_{p1} = S[\\cup_{i=1}^m indices_{first,i}].$ (9)\nBuilding on the first-level pruning results, we introduce a second pruning stage to further select important tokens based on approximated attention scores. This stage exclusively focuses on speech tokens, as the text-speech relationships have already been captured in the first pruning stage, enabling efficient mod- eling of internal dependencies within speech segments while minimizing computational overhead. The second stage utilizes the binarized attention from the network's first transformer layer. Specifically, we compute the scores using the signed binarized Query and Key weights, and also the pruned speech embeddings:\n$(W_Q^b, W_K^b, S') = sign(W_Q, W_K, S_{p1}).$ (10)\nThen the approximate attention scores are computed through binarized matrix operations:\n$Q' = S'W_Q^b,  K' = S'W_K^b,$ (11)\n$A = softmax(\\frac{Q'K'^T}{\\sqrt{d_k}}) \\in R^{N \\times N}.$ (12)\nThe final token selection is determined by\n$S_{p2} = S_{p1} [topk(mean(A, axis = 1), k)].$ (13)\nThis simplified attention mechanism accounts for less than 1% of the network's total computational complexity, which is highly efficient. The final pruned input merges selected audio tokens $S_{p2}$ with other essential tokens."}, {"title": "IV. EXPERIMENTS", "content": "We conduct our main experiments using Qwen-2 Audio [3], a state-of-the-art Speech LLM with extensive speech understanding task coverage. Our primary results are presented in Section IV-A, with qualitative analyses discussed in Sec- tion IV-B. Additionally, we perform ablation studies examining the performance impact of each pruning level in Section IV-C. Finally, Section IV-D demonstrates the generalizability of our proposed method across different models and benchmarks."}, {"title": "A. Main Experiments", "content": "We evaluate our method using Qwen-2 Audio, comparing our SPEECHPRUNE method against several baselines, compar- ing our two-level pruning strategy (SPEECHPRUNE) against three baselines: (1) Original: full audio trimmed at 30 seconds (750 tokens); (2) RAP: random audio pruning that selects non-contiguous segments to reach target rate; and (3) RAC: random audio cropping that selects a single contiguous segment at target rate. Our SPEECHPRUNE's two-level pruning strategy is set as follows: the first level prunes the input tokens to match the original method's input length (which is 750 tokens), while the second level removes additional tokens according to the specified pruning rate. We assess computational efficiency via TFLOPS, prefill time measured on a Quadro RTX6000 GPU, and memory usage (total and activations), estimated with LLM-Viewer [23]."}, {"title": "B. Qualitative Analysis", "content": "To visualize the effectiveness of our pruning strategy, we project token embeddings from one sample in the SPIRAL dataset into a 2D space using t-SNE visualization, comparing distributions between SPEECHPRUNE and RAP. Our method demonstrates more structured token selection, where preserved audio tokens (blue) exhibit stronger clustering around text tokens (red) compared to the scattered distribution in random pruning, suggesting effective retention of semantically relevant audio information. This visualization corroborates our quantitative results, showing SPEECHPRUNE's capability to maintain semantic relationships in the pruned representation."}, {"title": "C. Ablation Studies", "content": "To evaluate the effectiveness of our two-level pruning approach, we conduct ablation studies on the SPIRAL-H dataset."}, {"title": "D. Generalization Analysis", "content": "To evaluate the generalization capability of our method, we test SPEECHPRUNE on both different benchmarks and a different Speech LLM model. For additional benchmarks, we select two representative long-form speech understanding datasets: DREAM-TTS and CN-College-Listen. DREAM- TTS is derived from the text-based dialogue comprehension dataset DREAM [24], converted to speech using state-of-the-art TTS technology with 60 different speakers while maintaining gender consistency by [9]. CN-College-Listen is sourced from WavLLM's [25] test set, comprising English listening com- prehension questions from China's national college entrance examinations. For both datasets, we specifically use test samples that exceed 60 seconds in length to evaluate long-form speech understanding capabilities.\nWe also evaluate our method on DiVA [4], a recently proposed Speech LLM trained without instruction data using text-only LLM responses as self-supervision. As shown in demonstrates consistent improvements across all benchmarks and models. For Qwen-2 Audio with 0.2 pruning rate, our method improves accuracy from 53.69% to 65.19% on DREAM-TTS and from 52.91% to 62.86% on CN-College-Listen. When applied to DiVA with 0.15 pruning rate, SPEECHPRUNE similarly enhances performance across all three benchmarks, demonstrating its effectiveness even on models trained with different paradigms. These results suggest that our pruning strategy generalizes well across different types of speech understanding tasks and model architectures, even though these benchmarks were not originally designed specifically for SIR tasks."}, {"title": "V. CONCLUSION", "content": "In this work, we introduced the SIR task to target long-form speech comprehension, presented SPIRAL as a benchmark for evaluating such capabilities, and proposed SPEECHPRUNE, a training-free token pruning method leveraging speech-text similarity and approximate attention. Experimental results showed that SPEECHPRUNE not only reduces computational costs but can also enhance model performance, achieving nearly 29% and up to 47% network accuracy improvements over the original model and the random pruning model, respectively. While promising, further exploration is needed to improve robustness to diverse audio conditions, explore more nuanced token selection methods, and adapt pruning strategies to specific input characteristics or fine-tuned models."}, {"title": "APPENDIX", "content": "The dataset encompasses a hierarchical topic structure across three main categories: lectures, meetings, and conversations. The detailed topic list is shown in Fig. 4-6."}]}