{"title": "Integrating Causality with Neurochaos Learning: Proposed Approach and Research Agenda", "authors": ["Nanjangud C. Narendra", "Nithin Nagaraj"], "abstract": "Deep learning implemented via neural networks, has revolutionized machine learning by providing methods for complex tasks such as object detection/classification and prediction. However, architectures based on deep neural networks have started to yield diminishing returns, primarily due to their statistical nature and inability to capture causal structure in the training data. Another issue with deep learning is its high energy consumption, which is not that desirable from a sustainability perspective.\nTherefore, alternative approaches are being considered to address these issues, both of which are inspired by the functioning of the human brain. One approach is causal learning, which takes into account causality among the items in the dataset on which the neural network is trained. It is expected that this will help minimize the spurious correlations that are prevalent in the learned representations of deep neural networks. The other approach is Neurochaos Learning, a recent development, which draws its inspiration from the nonlinear chaotic firing intrinsic to neurons in biological neural networks (brain/central nervous system). Both approaches have shown improved results over just deep learning alone.\nTo that end, in this position paper, we investigate how causal and neurochaos learning approaches can be integrated together to produce better results, especially in domains that contain linked data. We propose an approach for this integration to enhance classification, prediction and reinforcement learning. We also propose a set of research questions that need to be investigated in order to make this integration a reality.", "sections": [{"title": "I. INTRODUCTION", "content": "Deep learning [1] comprises computational models that are composed of multiple processing layers to learn representa-tions of data with multiple levels of abstraction. These layers are combined together to form what are called artificial neural networks (ANNs) or neural networks (NNs) for short. Deep learning has proved to be remarkably successful in complex tasks such as object detection, speech recognition, classifica-tion and prediction, and has been applied to various domains such as medical informatics, robotics, computer vision, etc. [2]\nHowever, it has become apparent that deep learning has started to yield diminishing returns [3]. There are a few reasons for this. First, deep learning relies on statistically derived correlations to produce its results, many of which could be spurious. Second, neural networks trained on a particular dataset are found to perform poorly on datasets from a different distribution. Third, since they comprise multiple processing layers, and need to be trained on large datasets, their energy consumption has already raised concerns [4].\nIn order to address the issues of accuracy and energy con-sumption, alternative approaches are being considered. Promi-nent among them are causal machine learning and Neurochaos Learning. Causality [5] is concerned with the possible factors that could affect any object/event. For example, a pavement could have become wet due to either rain or a sprinkler. An example of an event would be - the stock of XYZ went up because of a policy that was implemented in country ABC. Causality is increasingly being integrated into traditional deep learning methods to enhance their accuracy [6], [7]. This is because of the realization that humans think causally [8]. Neurochaos Learning (NL) [9], [10] is based on the nonlinear chaotic firing exhibited by neurons in the brain. NL has been shown to be effective at classification tasks, even with a low number of training samples (sometimes with just a single sample/class for training). Therefore both causality and NL are brain-inspired. Indeed, in an earlier work [11], it has been shown that NL preserves the inherent causal structures present in input timeseries data.\nTo that end, in this position paper, we investigate how these two newly discovered approaches - causality and NL can be integrated together so that fullest advantage can be taken of both. Our particular emphasis is on domains containing linked data [12], i.e., data which is represented in the form of a graph. Prominent examples of such domains are Internet of Things (IoT), manufacturing, healthcare and life sciences, media and publishing and telecommunications. It is to be noted that the brain typically perceives data better when it is in a graph-like form, given the visual appeal of graphs.\nWe therefore view our position paper as one of the initial steps toward enriching the burgeoning area of neuromorphic computing [13] that is being investigated as enhancing the field of Artificial Intelligence (AI) via brain-inspired techniques."}, {"title": "II. BACKGROUND", "content": "Causality is the principle that determines the factors and mechanisms that relates how one event or object influences another event or object [14]. It is derived from the intuition that humans often think in terms of \"cause and effect\" and how we perceive as the world operates under such a causal model. In [15]. Causality has been defined via the \"ladder of causation\" which has the following levels: association, intervention and counterfactual. Association defines the effect of $v_i$ on $v_j$, thus: $v_i \\rightarrow v_j$, implying that $v_i$ is a cause of $v_j$. $v_i$ is typically referred to as an endogenous variable.\nSometimes other variables, also called exogenous variables or confounding variables, are also present, which could affect the causal relationship $v_i \\rightarrow v_j$. Such variables need to be accounted for in order to accurately model $v_i \\rightarrow v_j$, and this is done by fixing the value of the confounding variable, and then evaluating $v_i \\rightarrow v_j$. This refers to the second rung of the causation ladder, i.e., intervention. Whereas, the third rung of the causation ladder, i.e., counterfactual, is about \u201clooking back\" and analyzing what $v_i \\rightarrow v_j$ would have been had one or more confounding variables been intervened on, i.e., a type of \"what if\" reasoning.\nCausal relationships for a set of variables can be combined together into a structural causal model (SCM) [16] that is essentially a directed acyclic graph with variables such as $v_i$ and $v_j$ as nodes and edges modeled via the causal relationship $v_i \\rightarrow v_j$. Hence a SCM is a graph $G = (V, E)$, where V is the set of variables, including exogenous variables, and each edge $e \\in E$ is of the form $v_i \\rightarrow v_j$. Edges can also be attributed via a probability value $p_{ij}, 0 < p_{ij} < 1$, which refers to the relative strength of the causal relationship $v_i \\rightarrow v_j$. The $p_{ij}$ value can be fixed or drawn from a probability distribution, enhancing the SCM into a Bayesian network [17].\nA related concept to SCM is that of knowledge graph (KG), which is a representation of real-world entities and their relationships. A KG can therefore serve as an ontology of the domain in question. A KG does not model causal relationships, but can be augmented with causal relationships to form a causal knowledge graph (causal KG) [18]."}, {"title": "B. Neurochaos Learning", "content": "Neurochaos Learning (NL) [9] arose out of the observation that chaos and noise are inherent in the brain. Hence, like causality, NL is also a brain-inspired idea. The brain consists of \u2248 86 billion neurons that interact with each other to form a complex network [19]. These neurons are known to possess fluctuating responses to stimuli. This is partly due to their inherently chaotic nature and also due to noise, which is usually referred to in the literature as \u201cstochastic resonance (SR)\" [20].\nIn our earlier work, therefore [9], [21], [22], we have shown how SR can be used in NL. Also, the brain is known to be energy efficient and robust to noise, unlike computers.\nOur architecture, ChaosNet, consists of a layer of chaotic neurons which is the 1D GLS (Generalized L\u00fcroth Series) map, $CGLS: [0, 1) \\rightarrow [0, 1)$, is defined as follows:\n$CGLS(x) =\n\\begin{cases}\n\\frac{x}{1-x} \\qquad \\text{if } 0 \\le x < b \\\\\n\\frac{1-b}{1-x} \\qquad \\text{if } b \\le x < 1\n\\end{cases}$\nwhere x \u2208 [0,1) and 0 < b < 1.\nFig. 2 of Ref. [22] depicts the ChaosNet (NL) architecture. Each GLS neuron starts firing (from an initial neural activity of q units) upon encountering a stimulus (normalized input data sample) and halts when the neural firing matches the stimulus (the topological property of the GLS maps guarantees this for almost every initial q). From the neural trace of each GLS neuron, we extract the following features: firing time, firing rate, energy and entropy. These parameters are then fed into a classifier such as support vector machine (SVM) [23] (any other machine learning classifier, or even a neural network, can also be used).\nFiring time: Time taken for the chaotic trajectory to recognize the stimulus\nFiring rate: Fraction of time the chaotic trajectory is above the discrimination threshold b so as to recognize the stimulus\nEnergy: For the chaotic neural trace (trajectory) x(t) with firing time n, energy is defined as:\n$E = \\sum_{t=1}^n |x(t)|^2.$    (1)\nEntropy: For the chaotic neural trace (trajectory) x(t), we first compute the binary symbolic sequence S(t) as follows:\n$S(t_i) =\n\\begin{cases}\n0 \\qquad \\text{x(t_i) < b,} \\\\\n1 \\qquad \\text{b \\le x(t_i) < 1.}\n\\end{cases}$    (2)\nwhere i = 1 to n (firing time). We then compute Shannon Entropy of S(t) as follows:"}, {"title": null, "content": "$H(S) = - \\sum_{i=1}^2 P_i log_2(P_i)bits,$   (3)\nwhere $p_1$ and $p_2$ refer to the probabilities of the symbols 0 and 1 respectively.\nGLS neurons obey the topological transitivity property of chaos [22] and stochastic resonance [21] that enables them to perform classification tasks. We have demonstrated in an earlier work [11] that NL preserves Granger causality of timeseries data, unlike Deep Neural Network architectures (LSTM, CNN) which destroy causal structure in the input training data during learning. This makes NL a very desirable candidate for integrating causal learning.\nThis further strengthens our intuition that NL and causality can be integrated to produce better results, especially when working with linked data [12], i.e., data that is typically represented in a graph-like form. In what follows we will expand on this theme and present our ideas on causality-NL integration, as well as the research questions that need to be solved to make this integration a reality.\nIn addition, our earlier work on NL has produced other interesting and useful results. Firstly, we have shown in [9] that intermediate levels of noise provide the best classification performance. Secondly, as shown in [10], NL can be combined with other machine learning classifiers to significantly enhance the performance of the latter."}, {"title": "III. INTEGRATING CAUSALITY AND NL", "content": "In this section, we present an overall mindmap (see Fig. 1) of the various aspects of causality-NL integration.\nThe key aspects of our mindmap are as follows:\n1) The core of the mindmap is the integration between the \u201cCAUSALITY\" and \"NL\" nodes. We capitalize on the fact that neural networks are also graphs. In particular, since our focus is on linked data, we posit that graph neural networks (GNNs) [24] would be most appropri-ate for working with linked data. Hence we build on preliminary work presented in [24] that relates SCMs and GNNs. In particular, the notion of intervention in a GNN is introduced in [24]. A useful result from [24] is that any GNN can be seen as a neural SCM variant, and [24] also presents an algorithm to construct a GNN based on an SCM.\n2) The other key aspects of the mindmap, are extensions of NL to predictive modeling [25] and reinforcement learning [26], hitherto unexplored areas.\n3) Another key aspect is the enrichment of the SCM itself using ideas from ontology engineering [27]. It is expected that such enrichment would result in more accurate SCMs, thereby enhancing causality-NL inte-gration. This has two parts: using semantic web and data spaces to enrich causal models [28]; and using knowledge graphs to extend causal models into causal knowledge graphs as already mentioned above [18]."}, {"title": "B. Causality Modeling with Graph Neural Networks", "content": "A detailed review of integration of causality and GNNs is presented in [29]. GNNs that incorporate causality are referred to as Causal GNNS (CLGNNs). That paper identifies primar-ily two classes of CLGNNs: resolution-based and learning method-based. Within the resolution-based class, CLGNN-based methods have been developed for various types of input data: spectral, spatial, temporal and mixed. Within the learning method-based class, three types of learning methods have been addressed in the CLGNN literature:\nRepresentation learning \u2013 building features that represent the structure\nMeta learning - solution for data scarcity by exploiting previously learned experiences towards learning an algo-rithm that generalizes across various tasks\nAdversarial learning - examining and devising defenses against adversarial attacks on models\nReinforcement learning causal mechanisms for agent learning process towards improved decision making\n1) Graph Neural Network Overview: But first we will describe what a GNN actually is. As described in [29], a GNN is a type of neural network that operates on data that is represented as a graph, i.e., linked data. It comprises two functions, AGGREGATE and COMBINE, which are modeled as follows:\n$a_k = Aggregate_k \\{H_u^{k-1} : u \\in N(V)\\}$\n$H_v^k = Combine_k (H_v^{k-1}, a_k)$    (4)\nwhere the node representation is initialized as $H^0 = X$, N(v) is the set of neighbors of node v, K is the total number of GNN layers, k = 1,2,..., K and $H_v^K$ is the finalized node representations. $a_k$ is the aggregated node feature of the neighborhood $H_u^{k-1}$. In most GNN implementations, es-pecially involving Graph Convolution Networks (GCNs), the COMBINE function usually involves applying a non-linear activation function such as ReLU, sigmoid or tanh [30].\nGraph learning happens at the following levels:\nNode level: node properties are predicted. Node-level features may be importance-based or structure-based.\nGraph level: the features of the entire graph structure are captured for computing graph similarities.\nLink or Edge level: existing links are used to make predictions on new or missing links. Some link-level features are distance-based, local neighborhood overlap-based or global neighborhood overlap-based.\nGraph learning tasks are of the following types:\nNode Classification is the task of determining the class of a node based on classes of neighboring nodes.\nGraph Classification is the task of classifying an entire graph into various groups.\nNode Regression is the task of predicting a continuous value for a node.\nLink Prediction predicts potential relationships between two nodes in a graph."}, {"title": "C. Integrating Neurochaos Learning into Graph Neural Net-works", "content": "Based on the above discussion, we envisage the following (non-exhaustive) list of possibilities for integrating NL into GNNS:\nReplace the COMBINE function of Equation 4 with a neural spike such as 1D GLS map (Equation II-B) or Logistic Map [36]. Another alternative that could be investigated, is the SR function presented in [37], which is actually a stochastic ordinary differential equation.\n(Indeed, [37] has demonstrated robustness of neural net-works with noisy datasets using its SR function.)\nA variant of the above, in a manner similar to that presented in [38], is to model the neural spikes so that the neurons can themselves estimate their causal effects. The neuron can then use this effect to estimate its gradients and calculate its synaptic strengths. As shown in [38], this approach enables the neural network to learn to maximize reward, especially in the presence of confounded inputs.\nIn a similar vein, spike-induced GNNs, as introduced in [39], can also be investigated. The approach in that paper uses a variant of the well-known leaky integrate-and-fire neuron [40] as the activation function in order to implement graph representation learning.\nDirectly introduce SR in the edge weights of the GNN, turning the graph of the GNN into a stochastic graph [41]. Neural spikes such as those introduced above can be used to introduce stochasticity in the weights. It is to be noted, however, that the method in [41] confines itself to calculating network measures in social networks. Hence investigating how it can be extended to other domains based on linked data, would be needed.\nTailor the approach of SR introduction to the edge weights in a causal manner. That is, considering that, as presented in [24], a GNN can be represented as a variant of a causal graph, coordinate the introduction of SR into the weights so as to correspond to a causal path among the nodes of the graph. Intuitively, this is actually quite feasible, since a causal relationship between two nodes in a causal graph is already represented using Bayesian probability, which anyway models uncertainty. Moreover, this structure would also mimic how multiple neurons would interact with each other, via synapses. The key here would be determine the appropriate method for coordinated SR introduction.\nThe technique presented in [7] can also be considered here, where the emphasis is on eliminating confounders in timeseries data via random sampling and recombining variant patterns to create an intervention distribution. The objective is to eliminate the effect of hidden confounders. The key idea behind [7] is to use a spatio-temporal GNN, with the following modules: a causal feature learning module, a multi-layer spatiotemporal convolution mod-ule, a causal intervention module, and a predictive output module. Of special interest is the causal feature leaning module, which separates causal and non-causal features of the timeseries data.\nIn this context, the Graph Neural Stochastic Differen-tial Equations (GN-SDE) technique presented in [42] is worth noting. This enhances the traditional Graph Neural Ordinary Differential Equation (GN-ODE) technique by adding a diffusion function to the differential equation that models the stochasticity. This approach allows for the inclusion of prediction uncertainty, and [42] has shown via empirical studies that it can handle out-of-distribution datasets as well. This approach could be investigated as an alternative or supplement to the SR function approach presented in [37].\nAnother approach to consider, could be built on the model proposed in [43]. That model works with leaky integrate-and-fire neurons running in spiking neural network, and proposes that neuron firing be coordinated with the causal graph that represents the data. That is, given two nodes A and B in the causal graph, with the causal connection A \u2192 B, each node represents a population of neurons. This causal connection is then transformed into whether the neuron population discharges, in the order defined by the cause-and-effect relationship defined in the causal graph. Extending this approach to GNNs could be inves-tigated."}, {"title": null, "content": "Based on the aforementioned possibilities, and the earlier introduction to the prior work, the following research questions arise:\nRQ1: How would traditional graph embeddings, used in general GNNs so far, be modified to accommodate SR-based approaches such as NL?\nRQ2: As already posed in [29], how can causality in dynamic environments be integrated with NL? In particular, capturing causal dynamics in a continuous environment is quite difficult, due to the difficulty in gathering timeseries data to account for the changes in the system. Also, if the causality has a spatial component, incorporating spatial causality also poses a challenge. Perhaps the method proposed in [43] can be considered here.\nRQ3: How would SR be introduced directly into GNNs? One approach could be our strawman approach introduced above. Alternate approaches that could be explored, involve introducing random features [44] or stochastic aggregation [45].\nRQ4: Causal graphs typically come with Bayesian probabilities that define the possible cause of a variable on another. How could they be incorporated into our approach? In this regard, would probabilistic graphical models as described in [46] be useful? If so, how?\nRQ5: Causal graphs can be quite large as already stated in [29], especially if augmented to become causal knowledge graphs [18]. How can they be partitioned so that the overall problem can be solved via a \"divide and conquer\" approach? And if such an approach is indeed possible, how would the results be combined? Would it be perhaps via the approach suggested in [29], i.e., consolidate information from multiple graphs via representation learning?\nRQ6: Alternatively, how to solve the reverse problem of RQ5, i.e., how to use NL-based techniques to uncover these probabilities themselves? Could the fact that NL preserves causality [11] be useful here, and if so, how?"}, {"title": "IV. EXTENSIONS", "content": "In this section, we discuss some extensions beyond the usual classification-based neural network-based approach. Our focus will be on predictive modeling and reinforcement learning."}, {"title": "A. Extensions to Predictive Modeling", "content": "As discussed in [25], there are plenty of heuristic solutions for link prediction for network-structured (i.e., linked) data. First-order heuristics typically involve one-hop neighbors of the target nodes [47]. Second-order heuristics [48] are also sometimes considered. This can also be extended to higher-order heuristics that require knowledge of the entire network.\nHowever, the approach in [25] adopts a different strategy. From the subgraphs of the graph representing the data, it extracts graph structure features using a neural network. This is subsequently used for link prediction.\nA well-known example of prediction related task would be to predict drug-drug interactions [49], a common requirement in pharmacology. For such a task, drugs are represented as nodes and their interactions as edge in the graph, and the aim is to predict any missing edges, i.e., drug-drug interactions based on the existing data encoded in the graph. One such example subgraph from [49], is depicted in Fig. 5. In Fig. 5, to predict the interaction between Aspirin and Warfarin, it can be seen that the therapeutic efficacy of Aminosalicylic Acid can decrease when combined with Aspirin, suggesting that War-farin and Aspirin could be similar. Also, since Aminosalicylic acid may increase the anticoagulant capability of Warfarin and also since Aminosalicylic Acid resembles Aspirin, the method in [49] infers that Aspirin may also increase the anticoagulant capability of Warfarin. Therefore this predicts the link between Aspirin and Warfarin.\nBased on the above discussion on link prediction, the following research questions arise:\nRQ7: How would heuristics such as those described in [25], [49] be incorporated into an NL-based neural network to perform predictive modeling in graph-based data?\nRQ8: Here too, similar to RQ4, how would Bayesian probabilities be incorporated here to provide predictions with a certain degree of confidence?"}, {"title": "B. Extensions to Reinforcement Learning", "content": "Neural networks have been used in reinforcement learning (RL) for a long time [50]. The motivation for using neural networks for RL instead of the traditional approach using Q-tables, is that, for large data sizes, Q-table size could become large and unwieldy. In recent times, GNNs have also been used for reinforcement learning, as documented in [26]. The typical process of using GNNs in RL, is as follows. First, the local observation of agents is encoded into the feature vector in the embedded layer. Second, a graph attention network is used to define the edge weights as the strength of the connection in the coordination graph between each agent and its neighbors. Thirs, graph convolution is applied to perform message across all agents. Finally, the deep Q-network is used to approximate the Q-function.\nThe next action for the agents is determined based on the maximum output of the Q-network.\nAs an illustrative example of RL, remaining in the pharma-cological domain as per our earlier two examples, the graph-based method in [51] uses RL in drug discovery. Building on a pre-trained generative model, this method uses RL to guide the model to create molecules that obey a particular profile, even if the molecules were not present in the training set. The pre-trained model is generated based on GraphINVENT [52], which uses GNNs to generate molecular graphs.\nBased on the above discussion on using GNNs for reinforcement learning, the following research questions arise:\nRQ9: How can NL-enhanced GNNs be used to automatically generate Q-values, via SR? Perhaps the method presented in [52] could be used as a basis for this.\nIn addition, a method for reinforcement learning using spiking neurons, has been presented in [53]; perhaps that could be enhanced by adapting it for NL-enhanced GNNs.\nRQ10: How would the graph-based nature of the data in GNNs be represented here?"}, {"title": "V. ONTOLOGY MODELING TO ENRICH CAUSAL MODELS", "content": "Ontology modeling is the basis of building causal models, and helps represent causal models accurately. Hence an on-tology model can be seen as a schema of a causal graph. Thus we believe that focus on ontology modeling is needed to develop accurate causal models that can in turn help improve causality-NL integration.\nIntegrating ontologies and causal models is an active area of research. In [54], the authors present CausalLP, an approach that formulates the issue of incomplete causal networks as a knowledge graph completion problem. More specifically, the task of finding new causal relations in an incomplete causal network is mapped to the task of knowledge graph link prediction. The use of knowledge graphs to represent causal relations enables the integration of external domain knowledge; and as an added complexity, the causal relations have weights representing the strength of the causal association between entities in the knowledge graph.\nCausalLP has four primary phases: 1) encoding the causal associations in data as a causal network, 2) translating the causal network into a causal knowledge graph, 3) learning knowledge graph embeddings from the causal knowledge graph, and 4) using the knowledge graph embeddings for causal link prediction tasks.\nOn similar lines, an inductive link prediction approach using semantics is presented in [55]. This approach, via a random walk strategy and a graph attention network, calculates the combined structural and semantic scores of neighbors of nodes. Through this, the approach forms a subgraph with key nodes, which integrates and represents information about node neighbors.\nOn a related but different note, a higher-order link pre-diction for hypergraphs is presented in [56]. The reason for considering hypergraphs instead of just graphs, is that causal knowledge graphs would be hypergraphs, with multiple edge types between any pair of nodes. In [56], the solution to this problem is presented via hypergraph neural networks (HGNNs), which are variant of GNNs. Since typical HGNNs could get unwieldy, the approach in [56] presents a model of a light HGNN (LHGNN), performs hybrid aggregation to obtain hyperlink embeddings at a level higher than link embeddings.\nBased on the above discussion, the following research questions arise:\nRQ11: Can NL-enhanced GNNs help uncover hidden links/hyperlinks faster?\nRQ12: How can the causal knowledge graph be partitioned, in a manner posed in RQ5, to allow for faster hyperlink prediction using less memory and storage?\nRQ13: How can these techniques be extended in the case of temporal knowledge graphs [57], i.e., those that model streaming data such as in IoT or social media domains?\nRQ14: How can these techniques be used to evolve knowledge graphs, so that they in turn can provide graph-based inputs to NL-enhanced GNNs to perform classification and prediction?"}, {"title": "VI. CONCLUSIONS", "content": "In this position paper, we investigated two crucial machine learning techniques, viz., causal learning and neurochaos learning (NL), that can help enhance deep learning by ad-dressing its key issues, viz., statistical nature, reliance on large datasets and high energy consumption. Motivated by our recent results that show that NL does preserve causality in timeseries data, we investigated how causality and NL can be integrated together for better results. By employing graph neural networks which can model linked data, we showed how causality and NL can be integrated together. And to demonstrate the richness of the topic of causality-NL integration, we presented and discussed several research questions that arise. We believe that solving these research questions will help make causality-NL integration a reality.\nOur future work would be to investigate the posed research questions, and develop and demonstrate techniques that can help solve them. This would also involve further investigat-ing and taking forward our proposed approach described in Section III-C."}]}