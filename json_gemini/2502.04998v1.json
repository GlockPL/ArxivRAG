{"title": "On Sequential Fault-Intolerant Process Planning", "authors": ["Andrzej Kaczmarczyk", "Davin Choo", "Niclas Boehmer", "Milind Tambe", "Haifeng Xu"], "abstract": "We propose and study a planning problem we call Sequential Fault-Intolerant Process Planning (SFIPP). SFIPP captures a reward structure common in many sequential multi-stage decision problems where the planning is deemed successful only if all stages succeed. Such reward structures are different from classic additive reward structures and arise in important applications such as drug/material discovery, security, and quality-critical product design. We design provably tight online algorithms for settings in which we need to pick between different actions with unknown success chances at each stage. We do so both for the foundational case in which the behavior of actions is deterministic, and the case of probabilistic action outcomes, where we effectively balance exploration for learning and exploitation for planning through the usage of multi-armed bandit algorithms. In our empirical evaluations, we demonstrate that the specialized algorithms we develop, which leverage additional information about the structure of the SFIPP instance, outperform our more general algorithm.", "sections": [{"title": "1 Introduction", "content": "Multi-stage decision making problems are ubiquitous. Models for capturing these problems mostly assume additive utility or rewards across stages. The validity of these models relies on the premise that a fault at one stage does not affect the overall utility \"too much.\"\nWhile this is natural for a wide range of applications, in many multi-stage real-world scenarios, a fault at any single stage can immediately lead to the failure of the overall process. For instance, during the development of quality-critical products, a bad supply choice for any part of the products will lead to a product failure [JBD09]; during automated drug or material discovery which often has multiple stages of design, a wrong choice during any stage will fail the development [SDK+21]; in security-sensitive applications (e.g., transporting goods across regions monitored by adversaries [BHXT24]), the detection by the adversary at any step will lead to the failure of the entire mission; finally, in many popular computer games (e.g., Super Mario Bros and Frogger), winning is declared only when the player passed all barriers, otherwise the game restarts from the beginning. A key characteristic of these problems is the element of fault intolerance \u2014 that is, a reward is only given when the planner does not encounter any faults during the entire multi-stage process.\nThis work introduces a natural and basic planning model for such multi-stage fault-intolerant decision-making processes under uncertainty, coined Sequential Fault-Intolerant Process Planning (SFIPP). Each SFIPP process has m stages, and the planner needs to pick an action is \u2208 [k] to take at any stage s \u2208 [m], which has some success probability ps,is \u2208 [0,1]. The planner receives reward 1 if and only if their chosen action at every stage succeeded (with probability ps,is each), and receives reward 0 otherwise. Therefore, an SFIPP process is fully specified by a probability matrix $P\\in [0,1]^{m\\times k}$ where Ps,i = ps,i. We study the situation where P is unknown, but the planner plays this process repeatedly for T times and can learn P over time. Adopting the convention of online learning, we measure the planner's performance via regret the difference between the algorithm's summed reward and the optimal reward given P."}, {"title": "1.1 Our Contributions", "content": "1. We introduce and formalize SFIPP to model sequential multi-stage process planning with fault-intolerant reward structures, which naturally captures a wide range of important real-world problems including product quality control, drug discovery, security and computer games. SFIPP is closely related to several prior models, which we compare and discuss in Section 2.\n2. A foundational special case of SFIPP is when actions have deterministic success, i.e., P is a binary matrix containing only {0,1} entries. For this setting, we design a randomized algorithm whose performance depends directly on the unknown number of 0 entries in P, and attains the worst-case optimal expected regret.\n3. In the general SFIPP process with random stage successes, we design an algorithm with a tight regret bound inspired by the classic multi-armed bandit (MAB) problem. Our algorithm's analysis hinges on a lemma that upper bounds the regret in the fault-intolerant setting by the regret encountered in the additive utility case. We then study how the planner could leverage possible additional knowledge about the types of stages to provably improve the expected regret.\n4. Finally, we evaluate our algorithms empirically in various SFIPP settings with different P. We observe that our specialized algorithms that exploit knowledge of the P matrix outperform the general-purpose algorithm.\nOutline of paper. After formalizing SFIPP and comparing it with some prior work in Section 2, we study the SFIPP with deterministic success/failure in Section 3 and the more general probabilistic setting in Section 4, and then develop algorithms with improved regret under knowledge of stage types in Section 5. Our algorithms are evaluated empirically in Section 6 before we conclude in Section 7 with some future directions. Some proof details are deferred to the appendix.\nNotation. For any natural number $n \\in \\mathbb{N}_{>0}$, we let $[n] := \\{1,..., n\\}$. We use $\\mathbb{I}_{predicate}$ to denote indicator variables that are 1 if the predicate is true and 0 otherwise. We also employ standard asymptotic notations."}, {"title": "2 Formalizing the SFIPP Problem", "content": "A Sequential Fault Intolerant Process Planning (SFIPP) problem has m stages and the planner needs to pick an action i \u2208 [k] for each stage. At each stage s \u2208 [m], action i \u2208 [k] succeeds with some probability ps,i \u2208 [0, 1]. Hence, an SFIPP instance is fully characterized by a probability matrix P with Ps,i = Ps,i \u2208 [0,1]. Crucially, succeeding at any stage is independent of any successes at earlier stages. If P is known in advance, this is a trivial planning problem as the planner should always pick $\\text{argmax}_{i\\in [k]} P_{s,i}$ at any stage s.\nIn this paper, we study the situation with unknown P. Hence, the planner needs to learn to plan on the fly, and carefully balance exploration and exploitation. The player plays the SFIPP process over a horizon of $T \\in \\mathbb{N}_{>0}$ rounds. We denote by it,s \u2208 [k] the action picked by the planner in stage s \u2208 [m] of round t \u2208 [T]. This results in a sequence of m actions (it,1,...,it,m) across all m stages from which we receive a reward of 1 for round t \u2208 [T] with probability $\\prod_{s=1}^m P_{s,i_{t,s}}$, and 0 otherwise. In more detail, we generate an independent binary outcome at each stage and then return the product of these outcomes as the reward, i.e., the outcome at stage s in step t is 1 (\"success\" / \"pass\") with probability $p_{s,i_{t,s}}$ and 0 (\"failure\") otherwise. If the process fails and returns a reward of 0, we observe the first stage $s \\in [m]$ that the process failed at, i.e., the minimal s for which the generated outcome was 0. Our rationale for this is that in real-world applications if one encounters failure at one stage, one does not move on to (and observes the outcome of) the next stage, as the process has already failed and needs to be restarted. In sum, at each step $t \\in T$ the planner either knows that all m stages succeeded, or is told the first stage at which their selected action failed. The planner's goal is to design an algorithm that maximizes the total reward accumulated across all T rounds.\nWe assume that T > k, as otherwise the problem is uninteresting since one is not guaranteed to even play the optimal action in the worst case. Furthermore, as common in the literature, we assume that k and mare constants while T is treated as a variable, and we analyze how our algorithms perform over time as a function of T.\nFormally, we measure the performance of any algorithm solving SFIPP by its regret. Suppose an algorithm ALG produces a sequence of actions it,1,...,it,m for round t. The expected regret for the algorithm ALG is"}, {"title": "2.1 Connections to Prior Work", "content": "Sequential planning. Sequential planning problems are a foundational topic in artificial intelligence (AI) where one is often concerned with finding optimal sequences of actions to transition a system from an initial state to a desired goal state. Starting as early as A* search [HNR68], the study of sequential planning evolved to incorporate formal models such as STRIPS [FN71], probabilistic reasoning [KHW95], and domain-independent heuristics [HN01]. Modern methods of sequential planning also seek to address computational challenges posed by large state space [Bel66]; see also [GNT04]. Unfortunately, most sequential planning methods such as Probabilistic GraphPlan [BL99] assume known transition probabilities while the key difficulty in SFIPP lies in the fact that the action probability matrix P is unknown and has to be learned.\nReinforcement Learning (RL). RL is a sequential decision-making paradigm where agents learn an optimal policy for interacting with an environment. The latter is often modeled as a Markov Decision Process (MDP). While the SFIPP problem bears similarity to the more general RL framing, RL research often emphasizes convergence analysis. Contrary to that, the objective in SFIPP is to achieve low regret. While our m-stage SFIPP can be framed as an MDP with \u2248 m states (see, e.g., a survey by [CKSO22]), the customized methods we develop aiming for achieving a provably low regret do not apply to standard MDP formulations. For instance, our collapsing bandit algorithm in Section 5 enforces that the same action must be chosen for identical stage types, effectively collapsing states and violating the Markov property of MDPs. However, it is this violation, that lets the algorithm offer both theoretical and empirical improvements in regret bounds over standard approaches. Furthermore, our work explores SFIPP as a simpler yet powerful specialization of broader models like goal-oriented RL, allowing us to leverage problem-specific insights to design simpler algorithms with strong guarantees, outperforming generic approaches.\nStochastic multi-arm bandits (MAB). Stochastic MABs are closely related to the SFIPP problem. Con-cretely, in the special case of only one stage (i.e. m = 1), SFIPP degenerates precisely to the Bernoulli bandit problem where the reward of each arm (i.e. action in SFIPP) is a Bernoulli distribution. Therefore, SFIPP is a strict generalization of Bernoulli bandits to many stages. Note that the binary reward assumption in Bernoulli bandits does not intrinsically simplify the MAB problem as regret lower bounds for MABs all hold for (and in fact, are mostly proven under) Bernoulli bandits, showing that they already contain the most difficult MAB in-stances [BCB+12, Sli19, LS20]. Therefore, there is strong theoretical evidence to believe that SFIPP is strictly more challenging than standard MAB in the multi-stage setting.\nCombinatorial stochastic bandits. The planner picks a sequence of m actions in the SFIPP problem. That is similar to combinatorial bandits, where the learner can pick a subset of actions in one step [CBL12, CWY13, CTMSP+15, CXL18]. However, there are two key differences between SFIPP and combinatorial bandits. First, actions are simultaneously chosen in combinatorial bandits, whereas in SFIPP a failure at one stage effectively stops the planner from playing and learning from any actions for future stages. The second key difference lies in the reward structure - in combinatorial bandits, rewards are assumed to be additive (or sometimes submodular) across actions [HK12, QCZ14, CXL18], whereas our reward is a product of outcomes.\nEscape Sensing. [BHXT24] recently introduced and studied escape sensing games that feature the same reward structure as our SFIPP. Motivated by transporting peacekeeping resources by a convoy of ships, they study a model where a planner transports resources through a channel monitored by sensors of an adversary. A resource is successfully transported if and only if it escapes the sensing of all sensors, just like how the success"}, {"title": "3 SFIPP with Deterministic Success/Failure", "content": "In this section, we study the foundational special case of SFIPP in which actions are deterministic, i.e. the matrix P is binary containing only values 0 and 1 and at a stage, an action will either always or never succeed. We say an action i \u2208 [k] is successful at a stage s \u2208 [m] if pi,s = 1 and failing otherwise (i.e., Pi,s = 0). In this setting, the expected regret term in Eq. (1) simplifies to\n$\\mathbb{E}[R(ALG)] = \\sum_{t=1}^T [(\\prod_{s=1}^m \\mathbb{I}_{p_{s,i^*}=1}) - (\\prod_{s=1}^m \\mathbb{I}_{p_{s,i_{t,s}}=1})]$\nWithout loss of generality, we may assume that for every stage there is at least one successful action, i.e. $\\text{argmax}_{i\\in [k]} P_{s,i} = 1$ for all s \u2208 [m] (otherwise the regret becomes 0). The challenge now is to spend as few queries as possible to identify a successful action for each stage. In the rest of this section, we work towards constructing an algorithm that attains the optimal expected regret for this problem.\nTheorem 2. Consider the SFIPP problem with deterministic success/failure where $z = |\\{(s,i) \\in [m] \\times [k] : P_{s,i} = 0\\}|$ denotes the number of zero entries in P. Then, there is a randomized algorithm achieving expected regret (Eq. (1)) of at most $\\frac{z}{2}$. Furthermore, any algorithm incurs an expected regret of at least $\\frac{z}{2}$ in the worst case.\nTo show Theorem 2, we first consider the single-stage m = 1 setting and aim to understand the lower and upper bound on the number of actions that need to be tried to find the first successful action when there are z failing actions for any 0 <z<k-1; these are achieved in Lemma 3 and Lemma 4 respectively. These bounds will be helpful for the general problem with m > 1 because the action probabilities across stages are uncorrelated, which means that stages need to be learned separately. Accordingly, we will extend our one-stage results to the more general m \u2265 1 setting by first understanding the hardest multi-stage distribution of zeroes in Lemma 5 and then using it to prove that our proposed algorithm UNIFORMTHENFIXED is optimal in expectation.\nUsing Yao's lemma [Yao77], we first show a lower bound on the number of queries a randomized algorithm needs to find a successful action in the single-stage case:\nLemma 3. Fix some z \u2208 {0,..., k \u2212 1} and let Az be the set of all binary arrays of length k with exactly z O entries. In expectation, any randomized algorithm needs to query at least $ \\frac{k}{k+1-z}$, indices before it locates a 1 within an array sampled uniformly at random from Az.\nAs subsequently demonstrated, a simple randomized algorithm that uniformly selects from unknown entries has an optimal query complexity matching the bound from Lemma 3.\nLemma 4. Fix some z \u2208 {0,..., k \u2212 1} and let A be some binary array of length k with exactly z Os and k-z 1s. If we uniformly select an unexplored entry in A until we discover the first 1 in the array, then in expectation we select $\\frac{k-z+1}{2}$ zeros before we encounter a 1 entry.\nProof Sketch. Order the z Os arbitrarily and define Zi as the indicator whether the ith 0 was chosen before any 1. Then, one can show that $\\mathbb{E}(\\sum_{i=1}^z Z_i) = \\frac{k+1-z}{2}$.\nUsing Lemmas 3 and 4, we give the UNIFORMTHENFIXED algorithm (Algorithm 1) which at each stage uniformly selects an unselected action until it found a successful one which is later always selected. This algorithm achieves optimal expected regret at each stage and thereby also overall."}, {"title": "4 SFIPP with Probabilistic Successes", "content": "We now consider the SFIPP problem in full generality, where actions are probabilistic and P is an arbitrary (unknown) matrix in [0,1]m\u00d7k. We develop an algorithm working for arbitrary P and in particular do not make any assumptions on the connection between different columns of P. As discussed in Section 2, this setting constitutes a generalization of multi-armed bandits and we will present a general bandit-based algorithm. Note that it is not fruitful to directly model SFIPP into a bandit problem since that would result in km arms as each action sequence results in a different joint success probability. At the foundation of our algorithm lies the fact that when success probabilities of actions are uncorrelated across stages, the best we can do is to learn the success probabilities for each stage separately. To do so, at each stage of the SFIPP we utilize an arbitrary classic multi-arm bandit algorithm as a black-box. Following this reasoning, this section shows:\nTheorem 6. Suppose there is a MAB algorithm that achieves a regret bound of $O(R_{T,p_1,\\ldots,p_k})$ on the set of Bernoulli arms with success probabilities p1,...,pk over a horizon of T rounds. Given a SFIPP instance with probability matrix $P \\in \\mathbb{R}^{m\\times k}$ with rows P1,...,Pm, Algorithm 2 achieves a regret bound of $O(\\sum_{i=1}^m R_{T,P_i})$.\nThe crux of our analysis relies on the next lemma which will allow us to bound the regret generated in one round of a SFIPP instance (Eq. (1)) by the sum of the regret encountered at each stage when interpreting each stage as an individual Bernoulli bandit.\nLemma 7. Let a1,..., am, b1,..., bm \u2208 [0,1] such that 0 \u2264 bi < ai \u2264 1 for all i \u2208 [m]. Then,\n$0\\leq \\bigg( \\prod_{i=1}^m a_i \\bigg) - \\bigg( \\prod_{i=1}^m b_i \\bigg) \\leq \\sum_{i=1}^m (a_i - b_i)$\nProof sketch. Perform induction on m.\nBy applying Lemma 7, any existing classic bandit algorithm can be repeatedly employed at each stage independently in a black-box manner, resulting in a linear multiplicative increase in the regret bound. We refer to the resulting algorithm as STAGEDBANDIT (Algorithm 2).\nProof of Theorem 6. Consider the STAGEDBANDIT algorithm (Algorithm 2) which runs m instances of a MAB algorithm BANDIT, one BANDIT, for each stages \u2208 [m]. BANDIT, achieves an instance-dependent regret of $O(R_{T,P_s})$ over T rounds on the success probabilities Ps = (ps,1,...,Ps,k) at stage s. We can bound the regret"}, {"title": "5 Improved Regret with Known Stage Types", "content": "Given additional knowledge about which stages are known to be of the same type, we can modify the algorithm in Section 4 to obtain stronger results. Two stages are said to be of the same type if they share the same optimal action; a special case of this is when there are two identical stages resulting in two identical rows in P.\nWe will sketch two ways in which STAGEDBANDIT can be modified to \u201cmerge\u201d stages of the same type to provably attain lower regret and defer pseudocode and theoretical guarantees to Appendix A. Our first \"collapsing\" idea is to \"collapse\" all stages of the same type into one single \"meta\" stage, which means that we will always perform the same action throughout all stages of the same type and will only track whether we made it through all of the stages of the type. By doing so, instead of needing to learn the success probability of an action at each stage, we learn the success probability of a collapsed action as the product of the success probability across the collapsed stages. As an example, consider the following SFIPP matrix\n$P=\\begin{bmatrix} 0.9 & 0.8 \\\\ 0.6 & 0.5 \\\\ 0.5 & 0.7 \\end{bmatrix}$\nWe see that the optimal sequence is (1,1,2). Suppose we now collapse the first two rows of P into P':\n$P' = \\begin{bmatrix} 0.9 \\cdot 0.6 & 0.8 \\cdot 0.5 \\\\ 0.5 & 0.7 \\end{bmatrix} = \\begin{bmatrix} 0.54 & 0.4 \\\\ 0.5 & 0.7 \\end{bmatrix}$\nIn this collapsed matrix P', the optimal action sequence is (1, 2) which corresponds to performing (1, 1, 2) in P (as we perform the same action for all stages of the same type). As we only collapse rows that have the same optimal action index, optimally solving the collapsed SFIPP problem will recover the optimal action sequence of the original SFIPP. Furthermore, notice that the probability gap between the best action for the first two stages has widened from 0.9-0.8 = 0.1 and 0.6-0.5 = 0.1 to 0.54-0.4 = 0.14. The increase in probability gap is provably helpful in reducing incurred regret in bandit algorithms: there are well-known instance-dependent regret bounds for bandit algorithms which inversely relate the expected regret incurred by an algorithm to the success probability gaps between the optimal and suboptimal arms, with smaller gaps requiring more exploration to identify the best arm. For instance, given a bandit instance with arm success probabilities P1 > ... > pk, it is known that UCB achieves expected instance-dependent regret of $O(\\text{log}(T) \\cdot \\sum_{i=2}^k \\frac{1}{p_1 - p_i})$"}, {"title": "6.1 Power of Simplicity for Deterministic SFIPP", "content": "We first look at the deterministic setting, where we will observe that our specialized algorithm UTF outperforms the more general SB algorithm. For this, we constructed 100 deterministic SFIPP instances with m = 20 stages and k = 5 actions across a horizon of T = 10,000. We independently set each entry of P to 1 with some fixed probability p. We set one randomly selected index per stage to 1, thus ensuring that each stage contains at least one successful action."}, {"title": "6.2 More Stages Brings More Collapsing Gains", "content": "We empirically validate our theory from Section 5 that \"collapsing\" multiple stages of the same time helps drastically as the number of stages m increases. We generated 100 probabilistic SFIPP instances with m = {5, 10, 20} stages and k = 5 actions. Each entry of P is generated i.i.d. using the Beta distribution with parameters a = 10 and \u03b2 = 1 resulting in large success probabilities. By relabeling the actions, we ensured that in each generated instance, the same single action is optimal for every stage; see Appendix C.1 for details."}, {"title": "6.4 Invalid Collapsing Increases Regret", "content": "We demonstrate a strong detrimental effect of unbounded regret resulting from collapsing stages that are of different type. To illustrate this effect on an example first, suppose we collapse the last two rows of P from the beginning of Section 5 into P\":\n$P''= \\begin{bmatrix} 0.9 & 0.8 \\\\ 0.6 \\cdot 0.5 & 0.5 \\cdot 0.7 \\\\ 0.3 & 0.35 \\end{bmatrix}$\nIn the collapsed matrix P\", the optimal action sequence (1,2) corresponds to performing (1, 2, 2) in P, which is suboptimal and will always incur regret even as T\u2192\u221e.\nTurning to our experiments, as before, we construct 100 probabilistic SFIPP instances with m = 20 stages and k = 5 actions across a horizon of T = 10,000. Each entry of P is generated i.i.d. using the Beta distribution, either with parameters a = 10 and \u03b2 = 1 to encourage large success probabilities or with parameters a = 1 and \u03b2 = 1 to sample probability values from the uniform distribution. By relabeling the actions if necessary, we ensured that in each generated instance, odd-numbered and even-numbered stages are of the same stage type, i.e., all odd-numbered stages share a common optimal action, as do all even-numbered stages, but the optimal actions for odd and even stages are distinct; see Appendix C.1 for details.\nIn Fig. 1(e,f), we see that SCB-1 and SCFGB-1 incur ever-increasing regret due to misalignment of stage type information; (e) shows results for instances generated with parameters a = 10 and \u03b2 = 1 while (f) shows results for those generated with parameters a = 1 and \u03b2 = 1. Since they combined both odd and even type stages into a single one, SCB-1 and SCFGB-1 would never be able to learn and choose the true optimal action across all m stages. Meanwhile, we see that SCB_2 and SCFGB_2 outperform SB as expected, due to a similar reasoning as in Section 6.2."}, {"title": "7 Conclusions", "content": "In this work, we formalized a novel sequential planning problem called SFIPP and proposed algorithms for solving it under various settings. We supplemented our theoretical guarantees with empirical evaluation, highlighting interesting trade-offs between the algorithms depending on the unknown underlying success prob-abilities.\nIn future work, it would be interesting to consider different generalizations of our new model, for instance, the case where a different set of actions is available at different timesteps or stages, thereby arriving at a generalization of sleeping multi-armed bandits [KNMS10]."}, {"title": "B Deferred proofs", "content": "In this section, we give the deferred proof details.\nLemma 3. Fix some z \u2208 {0,..., k \u2212 1} and let Az be the set of all binary arrays of length k with exactly z O entries. In expectation, any randomized algorithm needs to query at least $\\frac{k}{k+1-z}$, indices before it locates a 1 within an array sampled uniformly at random from Az.\nProof. Without loss of generality, by relabeling indices, we may assume that any deterministic algorithm queries the array indices in order from 1 to k when searching for the first index containing a 1. Now, consider the uniform distribution of all $ \\binom{k}{z}$ binary arrays over Az. In general, for 0 \u2264 i \u2264 z, there will be $ \\binom{k-i-1}{z-i}$ arrays starting with i Os followed by a 1. Each of these arrays require (i + 1) queries by the deterministic algorithm to locate the first index containing a 1. So, in expectation over the uniform distribution, the deterministic algorithm will require\n$\\frac{1}{\\binom{k}{z}} \\sum_{i=0}^z \\binom{k-i-1}{z-i} (i + 1) = \\frac{k}{k+1-z}$\nqueries. Subtracting 1 from the above, we get the expected number of queries that the deterministic algorithm performs before it queries the index with 1. By Yao's lemma [Yao77], this means that any algorithm requires at least $\\frac{k}{k+1-z}$ index queries before it identifies an index with 1 within arrays with exactly z ones in the worst case.\nLemma 4. Fix some z \u2208 {0,..., k \u2212 1} and let A be some binary array of length k with exactly z Os and k - z 1s. If we uniformly select an unexplored entry in A until we discover the first 1 in the array, then in expectation we select $\\frac{k-z+1}{2}$ zeros before we encounter a 1 entry.\nProof. Let us define a random variable Y counting the number of selections until, but excluding, an 1 was selected. We will show that E(Y) = $\\frac{k+1-z}{2}$. To begin, let us number the z Os in any order. For each i \u2208 [z], we introduce an indicator random variable Zi, that is, Zi takes value 1 if the i-th 0 was selected before any 1, or value 0 otherwise. Over the uniform distribution of all possible k! sequences, E[Zi] is exactly the probability that the ith 0 occurs before all the k z 1s, which happens with proportion $\\frac{1}{k-z+1}$, and so E(Zi) = $\\frac{1}{k-z+1}$; see Example 12 for an example of this counting argument. Thus, by linearity of expectation, we see that\n$E(Y) = E(\\sum_{i=1}^z Z_i) = \\sum_{i=1}^z E(Z_i) = z \\cdot \\frac{1}{k-z+1} = \\frac{k+1-z}{2}$"}, {"title": "C Details of Experiments", "content": "C.1 Generation of Staged Experiment Instances\nTo generate P with j different stage types, we swap the ((s%j) + 1)-th action probability with the largest action probability for each stage so that the the optimal action for s-th stage always occurs at index (s%j) +1. For example, suppose we generated the following SFIPP matrix\n$P = \\begin{bmatrix} 0.124 & 0.357 & \\mathbf{0.432} & 0.291 & 0.085 \\\\ 0.214 & 0.076 & \\mathbf{0.389} & 0.407 & 0.153 \\\\ 0.265 & 0.178 & 0.099 & \\mathbf{0.314} & 0.348 \\\\ 0.428 & 0.067 & 0.209 & 0.134 & \\mathbf{0.275} \\end{bmatrix}$\nwhere the optimal actions are bolded for each stage. With 2 different stage types, we would modify it to\n$P' = \\begin{bmatrix} \\mathbf{0.432} & 0.357 & 0.124 & 0.291 & 0.085 \\\\ 0.214 & \\mathbf{0.407} & 0.389 & 0.076 & 0.153 \\\\ \\mathbf{0.348} & 0.178 & 0.099 & 0.314 & 0.265 \\\\ 0.067 & \\mathbf{0.428} & 0.209 & 0.134 & 0.275 \\end{bmatrix}$\nwhere the colors indicate the values that are swapped. After modification, the optimal action for stages 1 and 3 will be action 1 while the optimal action for stages 2 and 4 will be action 2. In this way, every odd or even stage will be of the same type whilst having differing optimal action indices across types."}]}