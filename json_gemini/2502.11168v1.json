{"title": "KNOWING YOUR TARGET: TARGET-AWARE TRANSFORMER MAKES Better SPATIO-TEMPORAL VIDEO GROUNDING", "authors": ["Xin Gu", "Yaojie Shen", "Chenxi Luo", "Tiejian Luo", "Yan Huang", "Yuewei Lin", "Heng Fan", "Libo Zhang"], "abstract": "Transformer has attracted increasing interest in spatio-temporal video grounding, or STVG, owing to its end-to-end pipeline and promising result. Existing Transformer- based STVG approaches often leverage a set of object queries, which are initialized simply using zeros and then gradually learn target position information via iterative interactions with multimodal features, for spatial and temporal localization. Despite simplicity, these zero object queries, due to lacking target-specific cues, are hard to learn discriminative target information from interactions with multimodal features in complicated scenarios (e.g., with distractors or occlusion), resulting in degrada- tion. Addressing this, we introduce a novel Target-Aware Transformer for STVG (TA-STVG), which seeks to adaptively generate object queries via exploring target- specific cues from the given video-text pair, for improving STVG. The key lies in two simple yet effective modules, comprising text-guided temporal sampling (TTS) and attribute-aware spatial activation (ASA), working in a cascade. The former focuses on selecting target-relevant temporal cues from a video utilizing holistic text information, while the latter aims at further exploiting the fine-grained visual attribute information of the object from previous target-aware temporal cues, which is applied for object query initialization. Compared to existing methods leveraging zero-initialized queries, object queries in our TA-STVG, directly generated from a given video-text pair, naturally carry target-specific cues, making them adaptive and better interact with multimodal features for learning more discriminative in- formation to improve STVG. In our experiments on three benchmarks, including HCSTVG-v1/-v2 and VidSTG, TA-STVG achieves state-of-the-art performance and significantly outperforms the baseline, validating its efficacy. Moreover, TTS and ASA are designed for general purpose. When applied to existing methods such as TubeDETR and STCAT, we show substantial performance gains, verifying its generality.", "sections": [{"title": "1 INTRODUCTION", "content": "Spatio-temporal video grounding (or STVG) aims at spatially and temporally localizing the target of interest from an untrimmed video given its textual description (Zhang et al., 2020b). As a multimodal task, it requires to understand the spatio-temporal content from videos and accurately connect it to the corresponding textual expression. Owing to its importance in multimodal video understanding and key applications such as content-based video retrieval, robotics, etc, STVG has attracted increasing interest with many models proposed (Zhang et al., 2020a;b; Tang et al., 2021; Su et al., 2021).\nRecently, inspired by the compact end-to-end pipeline of Transformer-based detection (Carion et al., 2020), researchers have introduced the Transformer (Vaswani et al., 2017) for STVG given that they both are localization task, and achieved previously unattainable result (Yang et al., 2022a; Jin et al.,"}, {"title": "2 RELATED WORK", "content": "Spatio-Temporal Video Grounding. Spatio-temporal video grounding (STVG) (Zhang et al., 2020b) aims to spatially and temporally localize the target of interest, given its free-form of textual description, from an untrimmed sequence. Early STVG approaches (Zhang et al., 2020b;a; Tang et al., 2021) mainly consists of two stages, first generating candidate proposals from the video with a pre-trained detector and then selecting correct proposals based on the textual expression. To eliminate the heavy dependency on the pre-trained detection model, recent methods (Su et al., 2021; Yang et al., 2022a; Jin et al., 2022; Lin et al., 2023b; Talal Wasim et al., 2024; Gu et al., 2024a), inspired by Transformer, switch to the one-stage pipeline that directly generates a spatio-temporal tube for target localization, without relying on any external detectors. Owing to the compact end-to-end training pipeline, such a one-stage framework demonstrates superior performance compared to previous two-stage algorithms. Our TA-STVG also belongs to the one-stage Transformer-based type. However, different from the aforementioned Transformer-based approaches that simply follows (Carion et al., 2020) to leverage zero-initialized object queries for target localization, TA-STVG innovatively exploits target-specific cues from the video-text pair for object query generation, making it adaptive to various scenarios and better interact with multimodal features in the decoder for more accurate localization.\nTemporal Grounding. Temporal grounding focuses on localizing specific targets or events from the video given the textual expression. Being relevant to STVG, it requires temporally localizing the target of interest, but the difference is that temporal grounding does not require to perform the spatial bounding box localization. In recent years, many methods (Cao et al., 2022; Chen et al., 2021; Mun et al., 2020; Hao et al., 2022; Wang et al., 2023b; Zhang et al., 2023; Barrios et al., 2023; Lin et al., 2023a) have been proposed for temporal grounding. For instance, the work of (Wang et al., 2023b) introduces a pre-training approach for improving video temporal grounding. The method in (Lin et al., 2023a) presents a unified framework for various video temporal grounding tasks. The approach in (Chen et al., 2021) proposes to learn complementary features from different modalities including"}, {"title": "3 OUR APPROACH", "content": "Overview. In this work, we propose TA-STVG, which aims to generate target-aware object queries for improving STVG. Motivated by DETR (Carion et al., 2020), TA-STVG utilizes the encoder-decoder framework which consists of a multimodal encoder and a spatio-temporal grounding decoder. As shown in Fig. 3, the multimodal encoder (\u00a73.1) extracts and interacts visual and textual features, while the decoder (\u00a73.4) learns target position information using target-aware object queries generated with TTS (\u00a73.2) and ASA (\u00a73.3) and their interaction with the multimodal feature for target localization."}, {"title": "3.1 MULTIMODAL ENCODER", "content": "Given a video and the textual expression, the multimodal encoder aims at obtaining vision-language features for target-aware query generation and the subsequent decoder for localization. It comprises visual and textual feature extraction and multimodal feature fusion as described in the following.\nVisual and Textual Feature Extraction. For the video, we extract its both appearance and motion features to leverage rich static and dynamic information. Specifically, we first sample $N_1$ frames $I = \\{I_i\\}_{i=1}^{N_1}$ from the video. Afterwards, ResNet-101 (He et al., 2016) and VidSwin (Liu et al., 2022) are respectively utilized for appearance and motion feature extraction. The appearance feature is denoted as $F^a = \\{f^a_i\\}_{i=1}^{N_1}$, where $f^a_i \\in R^{H \\times W \\times C_a}$ with $H$, $W$ and $C_a$ the height, width and channel dimensions, and the motion feature is represented using $F^m = \\{f^m_i\\}_{i=1}^{N_1}$, where $f^m_i \\in R^{H \\times W \\times C_m}$ with $C_m$ the channel dimension."}, {"title": "3.2 TEXT-GUIDED TEMPORAL SAMPLING (TTS)", "content": "To generate target-specific object queries, we first develop a text-guided temporal sampling (TTS) module to identify and sample frames relevant to the target guided by holistic textual features. Specifically, it will predict a relevance score between each frame and text and then sample video frames with their relevance scores above a predefined threshold. To enhance the frame selection in TTS, both appearance and motion are considered in computing the relevance, as shown in Fig. 4 (a).\nMore concretely, given multimodal feature $F$ from the multimodal encoder (\u00a73.1), we first extract the appearance, motion and textual features, respectively denoted by $F^a$, $F^m$, and $F^t$, from $F$ via deconcatenation $[F^a, F^m, F^t] = DeConcat(F)$. Afterwards, we perform average pooling on $F^a$ and $F^m$ and channel-average operation on $F^t$ to obtain frame-level appearance and motion features and the holistic textual feature,\n$G^a = AvgPooling(F^a) \\\\ G^m = AvgPooling(F^m) \\\\ G^t = Avg(F^t)$ (3)\nwhere $G^a$, $G^m$ (both $\\in R^{N_2 \\times D}$) and $G^t$ ($\\in R^{1 \\times D}$) are averaged appearance, motion, and text features.\nTo measure the relevance of each frame with the text, we fuse the textual feature into appearance and motion features via two cross-attention blocks, respectively. Specifically, the appearance feature $G^a$ or motion feature $G^m$ serves as query, while the textual feature $G^t$ as key/value. After this, MLP is performed on the appearance and motion features to predict relevance scores for each frame in terms of appearance and motion. As shown in Fig. 4 (a), this process can be expressed as follows,\ns_a = MLP(CA(CA(G^a, G^t), G^t)) \\\\ s_m = MLP(CA(CA(G^m, G^t), G^t))$ (4)"}, {"title": "3.3 ATTRIBUTE-AWARE SPATIAL ACTIVATION (ASA)", "content": "To explore more fine-grained information of the target, we design an attribute-aware spatial activation (ASA) module, which mines fine-grained visual semantic information from previous coarse-grained temporal features $R^a$ and $R^m$ for object queries. Particularly, ASA considers appearance attribute like color (e.g., \"green\", \"red\", etc) and shape (e.g., \u201csmall\u201d, \u201ctall\u201d, etc) and motion attribute such as action (e.g., \"walk\", \u201cride\u201d, etc) to generate fine-grained spatial features for the target. The intuition is that, appearance attribute is important for spatial target localization, while motion attribute is crucial in temporal localization, i.e., locating the start and end of the tube. In certain cases, even if spatial localization of target is good enough in each frame, the accuracy is low due to the incapability of temporally recognizing target action. Considering the specificity of queries for spatial and temporal decoders, appearance and motion activation features based on specific attributes are generated.\nSpecifically, as shown in Fig. 4 (b), given the textual feature $F^t$, we first extract the feature $T$ of the subject (usually the target to localize) in the textual sentence from it. Due to limited space, please refer to the Sec. D in supplementary material for subject extraction. Then, we repeat the subject feature $T_t$ to match the number of frames in $R^a$ and $R^m$, and the resulted feature is denoted as $T_t$. After that, we learn the fine-grained spatial features from $R^a$ and $R^m$ by interacting them with $T_t$ respectively using two cross-attention blocks. In specific, the subject feature $T_t$ serves as the query, while $R^a$ and $R^m$ as the key/value. By doing so, we can allow the subject feature to learn relevant spatial features from appearance and motion. To enforce the learning of attribute-aware"}, {"title": "3.4 DECODER FOR SPATIO-TEMPORAL LOCALIZATION", "content": "To learn target position information, we employ spatial and temporal decoders, similar to existing methods (Jin et al., 2022; Gu et al., 2024a). In both decoders, object queries $Q^a$ and $Q^m$ will iteratively interact with the multimodal feature $F$ from the encoder for target information learning, as follows,\n$Q^{a'} = SpatialDecoder(Q^a, F^a, F^t) \\\\ Q^{m'} = TemporalDecoder(Q^m, F^m, F^t)$ (10)\nwhere $Q^{a'}$ and $Q^{m'}$ are learned spatial and temporal queries after decoders, and $SpatialDecoder(\\cdot, \\cdot)$ and $TemporalDecoder(\\cdot, \\cdot)$ denote spatial and temporal decoders with each containing $K$ ($K = 6$)"}, {"title": "3.5 OPTIMIZATION", "content": "In TA-STVG, we predict: (1) the temporal relevance scores $s_a$ and $s_m$ in terms of appearance and motion in TTS, (2) the appearance and motion attribution classification scores $c_a$ and $c_m$ in ASA, and (3) the spatial boxes $B = \\{b_i\\}_{i=1}^{N_v}$ in the spatial decoder, start timestamps $H^s = \\{h^s_i\\}_{i=1}^{N_v}$ and end timestamps $H^e = \\{h^e_i\\}_{i=1}^{N_v}$ in the temporal decoder. During training, with the groundtruth of motion attribute $c_m^*$, appearance attribute $c_a^*$, start timestamps $H^{s*}$, end timestamps $H^{e*}$, and the bounding box $B^*$, we can calculate the total loss $L$ as\n$L = \\lambda_{TTS}(L_{BCE} (s_a, (H^{s+}, H^{e+})) + L_{BCE}(s_m, (H^{s+}, H^{e+})))) + \\lambda_{ASA}(L_{BCE}(c_m, c^*_m) + L_{BCE}(c_a, c^*_a)) \\\\loss\\ of\\ TTS \\hspace{3.5cm} loss\\ of\\ ASA \\\\ + \\lambda_{K} (L_{KL} (H^s, H^{s*}) + L_{KL}(H^e, H^{e*})) + \\lambda_l L_1(B^*, B) + \\lambda_u L_{IoU}(B^*, B)$ (11)\n$loss\\ of\\ temporal\\ decoder \\hspace{2.5cm} loss\\ of\\ spatial\\ decoder$\nwhere $L_{KL}$, $L_1$, $L_{IOU}$ and $L_{BCE}$ are KL divergence, smooth L1, IoU, and binary cross-entropy losses."}, {"title": "4 EXPERIMENTS", "content": "Implementation. TA-STVG is implemented in Python using PyTorch (Paszke et al., 2019). Similar to (Gu et al., 2024a), we use pre-trained ResNet-101 (He et al., 2016) and RoBERTa-base (Liu et al., 2019) from MDETR (Kamath et al., 2021) as 2D and text backbones, and VidSwin-tiny (Liu et al., 2022) as 3D backbone. The number of attention heads is 8, and the hidden dimension of the encoder and decoder is 256. The channel dimensions $C_a$, $C_m$, $C_t$, $D$ are 2,048, 768, 768 and 256. $\\delta$ and $\\theta$ are empirically set to 0.5 and 0.7. We use random resized cropping as augmentation method, producing an output with a short side of 420. The video frame length $N_2$ is 64, and the text sequence length $N_t$ is 30. During training, we use Adam (Kingma & Ba, 2015) with an initial learning rate of 2e-5 for the pre-trained backbone and 3e-4 for the remaining modules (note that the 3D backbone is frozen). The loss weight parameters $\\lambda_{TTS}$, $\\lambda_{ASA}$, $\\lambda_{K}$, $\\lambda_{l}$, and $\\lambda_{u}$ are set to 1, 1, 10, 5, and 3, respectively.\nDatasets and Metrics. We use three datasets, i.e., HCSTVG-v1/v2 (Tang et al., 2021) and Vid- STG (Zhang et al., 2020b), for experiments. HCSTVG-v1 contains 5,660 untrimmed videos, with 4,500 and 1,160 video-text pairs in training and testing sets. HCSTVG-v2 expands upon HCSTVG-v1, and comprises 10,131 training, 2,000 validation, and 4,413 testing samples. As test set annotations are not available, results are reported based on the validation set on HCSTVG-v2 as in other methods (Jin et al., 2022; Yang et al., 2022a; Lin et al., 2023b). VidSTG contains 99,943 video-text pairs. It includes 44,808 declarative and 55,135 interrogative sentences, with training, validation, and test sets containing 80,684, 8,956, and 10,303 sentences, and 5,436, 602, and 732 videos, respectively.\nFollowing (Yang et al., 2022a), we employ m_tIoU, m_vIoU, and vIoU@R as the evaluation metrics. m_tIoU measures the ability of temporal grounding by computing the average tIoU score across all the test set. m_vIoU compares the spatial grounding performance by calculating the average of vIoU scores, and vIoU@R measures the performance using ratios of samples with vIoU greater than R in test sets. For detailed metrics, please kindly refer to (Yang et al., 2022a)."}, {"title": "4.1 STATE-OF-THE-ART COMPARISON", "content": "HCSTVG Datasets. To validate the efficacy of our method, we compare it against other approaches on HCSTVG-v1/v2 datasets. Tab. 1 shows the comparison results on the HCSTVG-v1 test set. As shown in Tab. 1, TA-STVG achieves state-of-the-art performance on all four metrics, especially on the vIOU@0.3 metric, where it improves the score by 1.6% absolute gains compared to CG- STVG (Gu et al., 2024a). Compared to our baseline that does not employ TTS and ASA modules for"}, {"title": "4.2 ABLATION STUDY", "content": "Impact of TTS and ASA. The TTS and ASA are key components of TA-STVG. To vali- date their effectiveness, we conducted ablation experiments on HCSTVG-v1. As shown in Tab. 4, without TTS and ASA (0), our base- line achieves a m_tIoU score of 49.9%. When TTS is applied alone, the m_tIoU score is im- proved 52.2% with 2.3% gains (0 v.s. 2). When ASA is used alone, m_tIoU is improved to 51.4% with 1.5% gains ( v.s.). Working together, the best result of 53.0% is achieved with 3.1% absolute gains (1 v.s. 3). All these results clearly validate the efficacy of TTS and ASA, working either alone or together, for improving STVG.\nImpact of different branches in TTS. In TTS, we consider both appearance and motion, guided by text, for temporal selection. To further analyze TTS, we conduct experiments on its different choices on HCSTVG-v1. The results are reported in Tab. 5. As in Tab. 5, without using TTS (but using ASA), TA-STVG achieves 51.4% m_tIoU score (\u25cf). With the help of text-guided appearance branch, the m_tIoU score is improved to 51.8% with 0.4% gains (1 v.s. ). When using text-guided motion branch, the m_tIoU score is improved to 52.3% with 0.9% gains ( v.s. 3). When considering both appearance and motion, the m_tIoU score is boosted to 53.0% with 1.6% gains (1 v.s. 4), showing the best performance. It is worth noting that, if not using text as a guidance, the m_tIoU score will be decreased from 53.0% to 51.8% (6 v.s.), validating the importance of textual guidance in TTS.\nImpact of different attributes in ASA. In ASA, we consider both appearance and motion attributes for fine-grained feature learning. For in-depth analysis, we conduct ablations for ASA on HCSTVG- v1. As in Tab. 6, without using ASA (but using TTS), TA-STVG achieves 52.2% m_tIoU score (0). When considering appearance attribute, m_tIoU is improved to 52.3% (0 v.s. 2). When using motion attribute, the m_tIoU score is improved to 52.7% (1 v.s. 3). When applying both appearance and motion attributes, best result of 53.0% m_tIoU score is achieved with 0.8% gains (1 v.s. 6)."}, {"title": "4.3 VALIDATION OF GENERALITY", "content": "To validate the generality of our approach, we conduct experiments by applying TTS and ASA to other Transformer-based methods, in- cluding TubeDETR and STCAT. Since these two methods only use appearance features, we retain only the appearance branch to accom- modate them."}, {"title": "5 CONCLUSION", "content": "This paper proposes a novel Target-Aware Transformer for STVG (TA-STVG) by exploiting target- specific cues for query generation. The key lies in two effective modules of TTS and ASA. TTS aims to sample target-relevant temporal features using text as a guidance, while ASA explores fine-grained spatial features from coarse-grained temporal features. Working in a cascade, they allow exploiting crucial target-specific cues for generating target-aware object queries, which learn better target position information for STVG. Experiments on three datasets validate the efficacy of TA-STVG."}, {"title": "SUPPLEMENTAL MATERIAL", "content": "For better understanding of this work, we offer additional details, analysis, and results as follow:\n\u2022 A Details and More Results for the Oracle Experiments\nIn this section, we display more detailed architecture for the Oracle experiment and report results on more datasets.\n\u2022 B Architectures for Self-attention Encoder, Spatial and Temporal Decoders\nIn this section, we show the details for the architecture of the self-attention encoder used for multimodal feature fusion. In addition, we display the architectures of spatial and temporal decoders.\n\u2022 C Different Architectures of Activation Learning Strategies\nIn this section, we present details of architectures for activation learning strategies in ASA.\n\u2022 D Text Pre-processing and Attribute Label Construction\nThis section will illustrate the necessary text pre-processing for subject extraction. In addition, we will detail the construction of vocabulary for attribute labels.\n\u2022 E Clarification for Multi-label Classification in ASA\nIn this section, we offer clarification for the multi-label classification in the ASA module.\n\u2022 F Additional Ablation Experiments\nIn this section, we show additional ablation experiments on different loss weights and the robustness of our method.\n\u2022 G Additional Discussion\nIn this section, we provided additional discussions on our method.\n\u2022 H Efficiency and Complexity Analysis\nIn this section, we provide analysis of efficiency and model complexity for our proposed method and its comparisons with other state-of-the-art approaches.\n\u2022 I Qualitative Analysis and Results\nIn this section, we display qualitative analysis including relevance score learned by TTS, attribute-specific spatial activation learned by ASA, attention maps of object queries in the decoder, and qualitative results for spatial-temporal target localization.\n\u2022 J Limitation\nWe discuss the limitation of our method."}, {"title": "A DETAILS AND MORE RESULTS FOR THE ORACLE EXPERIMENTS", "content": "To verify whether target-aware queries benefit localization, we conduct an Oracle experiment by generating spatial and temporal object queries from the ground-truth boxes. Similar to our TA-STVG, the architecture in this Oracle experiment consists of a multimodal encoder, query generation, and a decoder, as shown in Fig. 7. The difference with our method is that, in the Oracle experiment, object queries are directly generated from the groundtruth boxes of the target using Rol pooling. In contrast, our method generates object queries from the trained TTS and ASA modules.\nWe also conduct Oracle experiments on VidSTG (Zhang et al., 2020b) and HCSTVG-v2 (Tang et al., 2021) datasets, as shown in Fig. 8. The results reveal that, compared to zero-initialized queries, the groundtruth-generated queries have significantly improve performance on these two datasets, which once again demonstrates the importance of using target-aware queries in enhancing the performance of STVG."}, {"title": "BARCHITECTURES FOR SELF-ATTENTION ENCODER, SPATIAL AND TEMPORAL DECODERS", "content": "B.1 ARCHITECTURE OF SELF-ATTENTION ENCODER\nThe self-attention encoder SelfAttEncoder(\u00b7), composed of L (L = 6) standard self-attention encoder blocks, is used to fuse features from multiple modalities. The structure is shown in Fig. 9.\nB.2 ARCHITECTURE OF SPATIAL AND TEMPORAL DECODER\nSimilar to previous methods (Gu et al., 2024a; Jin et al., 2022), we use a spatial decoder to learn spatial locations and a temporal decoder to learn temporal locations. The spatial decoder and the temporal decoder each consist of K (K = 6) blocks, with each block comprising a self-attention module and a cross-attention module. Fig. 10 illustrates the architectures of the spatial and temporal decoders."}, {"title": "C DIFFERENT ARCHITECTURES OF ACTIVATION LEARNING STRATEGIES", "content": "In ASA, we compare two different strategies for learning spatial activation of the target. One is the proposed attribute-aware spatial activation which learns attribute-specific information by attribute classification, and the other one is to learn the naive instance-level spatial activation through binary classification. Fig. 11 (a) and (b) show the detailed architectures of these strategies. Specifically, for the instance-level spatial activation in (b), the classification results can be directly employed as the spatial activation maps Ma and Mm to generated the spatially activated features. Compared to the naive instance-level activation method, our attribute-specific spatial activate enables learning more discriminative fine-grained features, which are crucial to distinguish the target from the background."}, {"title": "DTEXT PRE-PROCESSING AND ATTRIBUTE LABEL CONSTRUCTION", "content": "Subject Extraction. In the ASA module, we guide the learning of activation using the subject from the text. To this end, we employ the Stanford CoreNLP tool to conduct syntactic analysis on the text, transforming it into a structured graph. From the generated graph, we are able to extract the subject along with the related adjectives for appearance such as color and shape and verbs for motion such as various actions based on the part-of-speech tagging. The adjectives and verbs extracted are respectively used to produce the appearance and motion attribute labels. It is worth noticing that, since current STVG task focuses on localizing a single target (and thus there is only one subject in the query), we extract only one subject representation from the textual sentence.\nVocabulary and Attribute Label Construction. The ASA module is trained by predicting attribute classes. For this purpose, we have constructed two vocabularies for each dataset, including an appearance vocabulary and a motion vocabulary. Please note that, all information is from the dataset itself, without using extra information. Specifically, we first count the frequency of attribute words (adjectives or verbs related to the subject.) in each textual description in the dataset, and then eliminate words that appear less than 50 times. The remaining words constitute the vocabulary. The lengths of the appearance vocabulary and motion vocabulary for HCSTVG-v1/v2 are 25 and 40, respectively, while for VidSTG, they are 36 and 45, respectively. With the vocabulary, we can construct the multi-class attribute label for each dataset based on the words in the textural expression."}, {"title": "E CLARIFICATION FOR MULTI-LABEL CLASSIFICATION IN ASA", "content": "In this section, we provide more clarifications for the multi-label classification in our ASA module. Specifically, we answer the following questions: (a) Are the extracted subject features for appearance"}, {"title": "F ADDITIONAL ABLATION EXPERIMENTS", "content": "Impact of different loss weights. Similarly to the previous approaches, our model is trained with multiple losses, including TTS loss, ASA loss, spatial decoder loss, and temporal decoder loss. The weights for the spatial and temporal decoder losses are kept consistent works such as STCAT (Jin et al., 2022) and CG-STVG (Gu et al., 2024a). The weights for TTS and ASA losses are empirically set. We have conducted an ablation on the weights A\u03c4\u03c4\u0455 and AASA for TTS and ASA losses, and the results are shown in Tab. 11. From Tab. 11, we observe that when setting Arts and dasA to 1 and 1 (see ), respectively, we achieve the best performance.\nAblation on the robustness of TA-STVG. In order to show the robustness of our TA-STVG again challenges, we manually select videos from a current benchmark HCSTVG-v1, in which the objects suffer from heavy occlusions, similar distractors, or noisy text descriptions. The resulted subset is called Hard-HCSTVG-v1 (it will be released together with our source code), containing 300 videos selected from the test set of HCSTVG-v1. We show our performance on this Hard-HCSTVG-v1 and compare it with other state-of-the-art models in Tab. 12. From Tab. 12, we observe that, due to increased complexity, all the models degrade on the more difficult Hard-HCSTVG-v1 (please see Tab. 12 here and Tab. 1 in the paper). Despite this, our proposed TA-STVG still achieves the best performance (45.9/32.6/54.7/22.3 in m_tIoU/m_vIoU/vIoU@0.3/vIoU@0.5, see ) by outperforming CG-STVG (45.5/31.4/50.3/20.0 in m_tIoU/m_vIoU/vIoU@0.3/vIoU@0.5, see ), STCAT (42.9/28.8/47.3/19.3 in m_tIoU/m_vIoU/vIoU@0.3/vIoU@0.5, see), and TubeDETR (40.4/28.3/42.3/14.3 in m_tIoU/m_vIoU/vIoU@0.3/vIoU@0.5, see 0). Further more, TA-STVG significantly improves its baseline under these challenges (6 v.s. 4). All of these are attributed to our TTS and ASA modules that mine discriminative information of the target, even in challenging scenarios, for localization, showing their efficacy for improving STVG."}, {"title": "GADDITIONAL DISCUSSION", "content": "Discussion on construction of frame-specific object queries. In this work, the feature output by the ASA module undergoes a pooling and repetition to generate the final object queries. The reasons"}, {"title": "H EFFICIENCY AND COMPLEXITY ANALYSIS", "content": "To analyze the efficacy and complexity of our model, we conduct a comparative analysis involving the number of parameters, training consumption, inference consumption, and FLOPS against other existing methods. As shown in Tab. 13, we can see that, our proposed maintains similar computational complexity (206/234M trainable/total parameters and 2.97T FLOPS) to existing state-of-the-art models such as STCAT (207M/207M trainable/total parameters and 2.85T FLOPS) and CG-STVG (203M/231M trainable/total parameters and 3.03T FLOPS), while achieving better performance with the best m_tIoU of 51.7 and m_vIoU of 34.4 on VidSTG. Moreover, compared to the baseline method, TA-STVG brings negligible parameter and computational increments (Baseline v.s. TA-STVG: 202M v.s. 206M in trainable parameters, 230M v.s. 234M in total parameters, and 2.88T v.s. 2.97T in FLOPS), yet significantly enhances the performance (m_tIoU improved from 49.5 to 51.7 in and m_vIoU from 32.3 to 34.4), underscoring its superiority."}, {"title": "I QUALITATIVE ANALYSIS AND RESULTS", "content": "I.1 VISUALIZATION OF TEMPORAL RELEVANCE SCORE AND ATTRIBUTE-SPECIFIC ACTIVATION\nIn this section, we show more visualizations of the temporal relevance scores learned by TTS and attribute-specific spatial activation learned by ASA in Fig. 12. From Fig. 12, we can see that the frames, selected by the temporal relevance score s, will be within the groundtruth, which shows the selected features are target-relevant. In addition, the attribute-specific spatial attention maps Ma and Mm can well localize the related attributes such as color and different actions of the target. F instance, in the first example, the text is \"The man in green clothes walks behind the man in brown\". The highly relevant areas in the appearance attention map are mainly concentrated around the green clothes, while in the motion attention map are focused on the legs. These attribute-specific spatial activation helps with learning more discriminative fine-grained features for improving STVG.\nI.2 ATTENTION MAPS OF OBJECT QUERIES IN THE SPATIAL DECODER\nTo show the role of target-aware queries, we visualize its attention maps in the spatial decoder with comparison to the attention maps of zero-initialized queries, as shown in Fig. 13. As in Fig. 13, the"}, {"title": "J LIMITATION", "content": "Despite achieving state-of-the-art performance on multiple datasets, our method has two main limitations"}]}