{"title": "Diffusion Meets Options: Hierarchical Generative Skill Composition for Temporally-Extended Tasks", "authors": ["Zeyu Feng", "Hao Luan", "Kevin Yuchen Ma", "Harold Soh"], "abstract": "Abstract\u2014Safe and successful deployment of robots requires\nnot only the ability to generate complex plans but also the\ncapacity to frequently replan and correct execution errors.\nThis paper addresses the challenge of long-horizon trajectory\nplanning under temporally extended objectives in a receding\nhorizon manner. To this end, we propose DOPPLER, a data-\ndriven hierarchical framework that generates and updates\nplans based on instruction specified by linear temporal logic\n(LTL). Our method decomposes temporal tasks into chain of\noptions with hierarchical reinforcement learning from offline\nnon-expert datasets. It leverages diffusion models to generate\noptions with low-level actions. We devise a determinantal-\nguided posterior sampling technique during batch generation,\nwhich improves the speed and diversity of diffusion generated\noptions, leading to more efficient querying. Experiments on\nrobot navigation and manipulation tasks demonstrate that\nDOPPLER can generate sequences of trajectories that progres-\nsively satisfy the specified formulae for obstacle avoidance and\nsequential visitation. Demonstration videos are available online\nat: https://philiptheother.github.io/doppler/.", "sections": [{"title": "I. INTRODUCTION", "content": "Robots and autonomous agents are increasingly expected\nto safely perform complex tasks specified by high-level,\ntemporally extended instructions. Linear Temporal Logic\n(LTL) provides a formal language to specify such tasks,\nenabling agents to reason about sequences of events over\ntime [1], [2]. However, planning under LTL constraints\nposes significant challenges, especially in offline settings\nwhere agents must learn from fixed datasets without active\nexploration. Traditional data-driven policy learning methods\nstruggle with the non-Markovian nature of LTL rewards, and\noffline RL further complicates the issue due to distributional\nshifts.\nIn this work, we propose Diffusion Option Planning by\nProgressing LTLs for Effective Receding-horizon control\n(DOPPLER), an offline hierarchical reinforcement learning\nframework that generates receding horizon trajectories to\nsatisfy given LTL instructions. Our key insight is to leverage\ndiffusion models to represent options within the hierarchical\nRL framework. By integrating diffusion-based options, we\ncan generate diverse and expressive behaviors while ensuring\nthat the generated trajectories remain within the support\nof the offline dataset. To address the challenges of option\nselection and policy regularization in the offline setting, we\nintroduce a diversity-guided sampling approach that pro-\nmotes exploration of different modes of the data distribution\nwithout straying into out-of-distribution actions.\nCompared to prior work, the most closely related methods\nare those that use diffusion models for trajectory planning\nunder temporal constraints [3] and hierarchical diffusion\nframeworks for subgoal generation [4], [5]. However, these\napproaches either lack the ability to perform closed-loop,\nhierarchical planning or require expert-level demonstrations,\nlimiting their applicability in offline settings with non-expert\ndata. In contrast, DOPPLER combines hierarchical RL with\ndiffusion-based options to enable closed-loop control under\nLTL constraints using only offline data. To our knowledge,\nDOPPLER is the first work to integrate diffusion models into\na hierarchical RL framework for data-driven LTL planning\nin an offline setting.\nOur experiments demonstrate that DOPPLER significantly\noutperforms strong baselines on tasks specified by LTL for-\nmulas, achieving higher success rates in satisfying temporal\nspecifications. In simulation, DOPPLER effectively handles\ncomplex, long-horizon navigation tasks that prior methods\nstruggle with. On the real robot, DOPPLER exhibits robust\nbehavior in the presence of control noise and external pertur-\nbations, successfully accomplishing LTL tasks where other\nmethods fail.\nDOPPLER represents a step toward enabling autonomous\nagents to perform complex, temporally extended tasks speci-\nfied by LTL instructions in offline settings. In summary, this\npaper makes three key contributions:"}, {"title": "II. PRELIMINARIES", "content": "In this work, we aim to train an agent using an offline\ndataset to learn policies that generate trajectories satisfying\nLinear Temporal Logic (LTL) specifications. Our approach\nemploys offline hierarchical reinforcement learning, where\noptions are represented using diffusion models. In this sec-\ntion, we provide a succinct review of LTL, hierarchical\nreinforcement learning, and diffusion policies."}, {"title": "A. Formal Task Specification via LTL", "content": "To unambiguously define the task, we employ Linear\nTemporal Logic (LTL). LTL is a propositional modal logic\nwith temporal modalities [6] that allows us to formally\nspecify properties or events characterizing the successful\ncompletion of a task. These properties and events are drawn\nfrom a domain-specific finite set of atomic propositions P.\nThe set of LTL formulae \u03a8 is recursively defined in Backus-\nNaur form as follows [7], [8]:\n\u03c6 := p | \u00ac\u03c6 | \u03c6\u2227\u03c8 | \u25cb\u03c6 | \u03c6U\u03c8,\nwhere p\u2208 P and \u03c6, \u03c8 \u2208 \u03a8. \u00ac (negation) and \u2227 (and)\nare Boolean operators while \u25cb (next) and U (until) are\ntemporal operators. These basic operators can be used to\nderive other commonly-used operators, true = \u03c6 \u2228 \u00ac\u03c6,\n\u03c6\u2228 \u03c8 = \u00ac (\u00ac\u03c6 \u2227 \u00ac\u03c8) and \u25c7\u03c6 = true U \u03c6 (eventually \u03c6).\nLTL formulae are evaluated over infinite sequences of truth\nassignments \u03c3 = (\u03c3\u2080, \u03c3\u2081, \u03c3\u2082, ...), where \u03c3\u209c \u2208 {0,1}^{|P|} and\n\u03c3\u209c,\u209a = 1 iff p is true at time step t. (\u03c3,t) |= \u03c6 denotes\n\u03c3 satisfies \u03c6 at time t > 0, which can be initially defined\nover atomic propositions and then extended to more logical\noperators according to their semantics. Given a \u03c3, an LTL\n\u03c6 can be progressed to reflect which parts of \u03c6 have been\nsatisfied and which remain to be achieved [1]. The one-step\nprogression of \u03c6 on \u03c3 is defined as follows:\n\u2022 prog (\u03c3, p) = true if \u03c3\u209a = 1, where p \u2208 P\n\u2022 prog (\u03c3, \u00ac\u03c6) = \u00ac prog (\u03c3, \u03c6)\n\u2022 prog (\u03c3, \u03c6\u2227 \u03c8) = prog (\u03c3, \u03c6) \u2227 prog (\u03c3, \u03c8)\n\u2022 prog (\u03c3, \u25cb\u03c6) = \u03c6\n\u2022 prog (\u03c3, \u03c6U\u03c8) = prog (\u03c3, \u03c8) \u2228 (prog (\u03c3, \u03c6) \u2227 \u03c6U\u03c8)\nWe also overload prog (\u03c3\u2080:k, \u03c6) to indicate multistep pro-\ngression over \u03c3\u2080:k. To facilitate effective LTL learning on a\ndataset of trajectories, we restrict our consideration to LTL\nformulas that can be satisfied or falsified within a finite"}, {"title": "B. Hierarchical RL with options", "content": "We formulate RL within the framework of Markov deci-\nsion processes (MDPs) [13]. An MDP M = (S, A, p, r, \u03b3)\nconsists of state space S, action space A, transition prob-\nability function p:S\u00d7A \u2192 P(S)\u00b9, reward function\nr:S\u00d7A \u2192 R and discount factor \u03b3\u2208 [0,1). Given a\npolicy \u03c0 : S \u2192 P (A) and an initial state s\u2080 ~ \u03bc\u2080 \u2208\nP(S), an agent generates a trajectory T = (s\u209c, a\u209c)\u209c=\u2080 based on the distributions specified by \u03c0 and p. The agent\nreceives a discounted return R(T) = \u2211\u209c=\u2080\u03b3\u1d57r (s\u209c, a\u209c)\nalong its trajectory and aims to maximize the expected\nreturn \u0395\u03bc\u2080,\u03c0,\u03c1 [R (T)]. The action-value function Q^(\u03c0) (s, a) =\n\u0395\u03c0,\u03c1 [R(T) | s\u2080 = s, a\u2080 = a] represents this expected return\nconditioned on a specific initial s and a. In RL, p and r are\nunknown [14] to the learner.\nThe option-based framework incorporates temporally ex-\ntended behaviors termed options into MDPs [15]. An option\no = (I, \u03c0\u2092, B) is available to the agent in s iff s belongs\nto the initialization set I \u2286 S. Once the agent takes o, it\nfollows \u03c0\u2092 until o terminates, which occurs after t steps\naccording to the probability B : S\u1d57 \u2192 [0,1]. Policies\nare defined over both primitive actions A and options O,\ni.e., \u03c0 : S \u2192 P(A\u222aO). The corresponding option-\nvalue function is Q^(\u03c0) (s, o) = \u0395\u2092,\u03c0,\u03c1 [R (T) |E (\u03bf\u03c0, s)], where\n\u0395 (\u03bf\u03c0, s) denotes the event that o is initiated in s at t = 0,\nterminates at t with probability B (s\u2081, ..., s\u209c), and \u03c0 is\nfollowed after o terminates."}, {"title": "C. Diffusion-based Policies", "content": "Existing diffusion-based policies directly generate a finite\ntrajectory \u03c4\u2080:\u2096 = (s\u209c, a\u209c)\u209c\u2208[\u2096+\u2081] by training on a pre-\ncollected dataset of trajectories. These policies can be trained\nusing expert demonstrations for imitation learning [16],\nor with offline datasets guided by a Q function for pol-\nicy improvement [17]. Formally, a step-dependent neural\nnetwork s\u2091 is trained to approximate the score function\n\u2207\u03c4\u2080:\u2096 log p\u03b8 (\u03c4\u2080:\u2096) with denoising score matching objec-\ntive [18]:\nmin E\u03c4\u2080:\u2096 \u223c p\u2080(\u03c4\u2080:\u2096) E\u1d62\u223c U{1,2,...,N} [\u2225s\u03b8 (\u03c4\u2080:\u2096, i) - \u2207\u03c4\u2080:\u2096 log p(\u03c4\u2080:\u2096 | \u03c4\u2070:\u2096 )\u2225\u2082\u00b2], (1)\nin which \u03c4\u2080:\u2096 ~ q(\u03c4\u2080:\u2096 | \u03c4\u2070:\u2096) is the data trajectory \u03c4\u2080:\u2096 ~ p\u2080 (\u03c4\u2080:\u2096)\ncorrupted with noise by an N-step discrete ap-\nproximation of forward diffusion process q(\u03c4\u2070:\u2096 | \u03c4\u2070:\u2096) and\ni ~ U{1,2,...,N}. For example, in Denoising Diffu-\nsion Probabilistic Models (DDPM) [19], p (\u03c4\u2070:\u2096 | \u03c4\u2070:\u2096) =\nN (\u221a(\u03b1) \u0304\u1d62\u03c4\u2070:\u2096, (1 \u2212 (\u03b1) \u0304\u1d62) I), (\u03b1) \u0304\u1d62 := \u220f<\u2c7c=\u2081\u03b1\u2c7c, \u03b1\u2c7c := 1 \u2212 \u03b2\u2c7c\nand {\u03b2\u1d62} is a sequence of positive noise scales 0 <\n\u03b2\u2081, \u03b2\u2082,..., \u03b2\ud835\udc41 < 1. Diffusion models can generate plans\nconditioned on a start state by inpainting [17] or classifier-\nfree guidance [20]."}, {"title": "III. METHOD: DOPPLER", "content": "In this section, we introduce DOPPLER, our offline hierar-\nchical reinforcement learning framework for Linear Tempo-\nral Logic (LTL) instructions. At a high level, DOPPLER in-\ntegrates hierarchical RL with diffusion models to effectively\naddress the challenges of planning under LTL constraints in\nan offline setting (see Figure 1).\nWe begin by presenting our offline hierarchical RL ap-\nproach for LTL instructions. This includes the construction\nof a product MDP to handle the non-Markovian nature of\nLTL rewards, the definition of options and rewards, and the\nlearning of the option-value function from an offline dataset.\nNext, we describe how we represent options using diffusion\nmodels and introduce a novel diverse sampling technique.\nThis approach allows us to obtain a rich set of options that are\nboth expressive and within the support of the offline dataset,\nensuring effective policy regularization."}, {"title": "A. Offline Hierarchical RL for LTL Instructions", "content": "Recall that we adopt an options framework for hierar-\nchical reinforcement learning and our agent is tasked with\ngenerating behavior that satisfies a given Linear Temporal\nLogic (LTL) formula \u03c6\u2208 \u03a8. The first challenge is that LTL\nsatisfaction is evaluated over an entire trajectory. As such,\ndefining a standard Markov Decision Process (MDP) M over\nthe native states S is problematic since a non-Markovian\nreward function cannot be defined over single states.\nTo address this issue, we construct a product MDP M\u03c8\nsuch that the optimal Markov policies in M\u03c8 recover\nthe optimal policies for the non-Markovian reward func-\ntion in M [21], [22]. More formally, we define M\u03c8 =\n(S\u03c8, \u0391, \u03c1\u03c8, \u03b3\u03c8,\u03b3), where S\u2081 = S \u00d7 \u03a8. We assume that\nthe properties or events related to a temporal task can be\ndetected from the environmental state/action information and\ndefine a labeling function L : S \u00d7 A \u2192 {0,1}^P that\nmaps each state-action pair (s\u209c, a\u209c) to a truth assignment\n\u03c3\u209c = L(s\u209c,a\u209c). We consider LTL progression [1], [21] as\npart of the transitions, and thus, the transition probability\nin M\u03c8 is defined as p\u03c8 ((s\u2032,\u03c6\u2032)|(s,\u03c6),\u03b1) = p(s\u2032|s, a) if\n\u03c6\u2032 = prog (L(s, a), \u03c6) (and 0 otherwise).\nOptions and Rewards. We incorporate options into M\u03c8\nas described in Section II-B. We defer the detailed option\ndefinition to the next section and it suffices to assume that\neach option is a trajectory o = \u03c4\u2080:\u2096 = (a\u209c, s\u209c\u208a\u2081)\u2096<\u207b\u2081 . At\nstate s\u2080, the agent follows option o, which terminates after\nk steps. We assume that the options have an initiation set I\ncovering the entire state space.\nNext, we develop an option reward function such that the\nagent can either (i) accomplish the instructed LTL formula\n\u03c8 within the k steps of an option or (ii) transition to a state\nthat enables completion of \u03c6 in the future. In our product\nMDP M\u03c8, the agent receives a reward at (s\u209c, \u03c6) given by:\nr\u1d67 (s\u209c, \u03c6, a\u209c) = { 1, if prog (\u03c3\u209c, \u03c6) = true and \u03c6 \u2260 true\n                 -1, if prog (\u03c3\u209c, \u03c6) = false and \u03c6 \u2260 false\n                  0, otherwise\nwhere ot = L(st, at). The option reward is then defined as\n\u03b3\u03c8 (So, 4, 0) = \u03a3\u03c4= vtrt, where rt = r\u0173 (st, Ot, at) and\n\u03c6t = prog (\u03c3\u03bf:\u03c4, \u03c6).\nLearning an Option Critic. With the above definitions, we\ncan now proceed to learn an option-value function using\nan offline dataset. A high-level summary of our learning\nalgorithm is shown Algorithm 1. According to the Bellman\nequation, the optimal option-value function has the form:\nQ* ((s, \u03c6), 0) = r\u03c8 (s, \u03c6, \u03bf) +\n                         \u03b3^k \u2211 s\u2032,\u03c6\u2032 p (s\u2032, \u03c6\u2032|s, o) max o\u2032 Q^* ((s\u2032, \u03c6\u2032), o\u2032)\nwhere r\u03c8 (s, \u03c6, \u03bf) = E\u03c1\u2092[r\u2080 + \u2026 + \u03b3^{k-1} r{k-1}] and\np (s\u2032, \u03c6\u2032|s, o) is the probability of transitioning to s\u2032 and\n\u03c6\u2032 when executing option o under \u03c1\u2092. We aim to obtain\nan option critic Q ((s, \u03c6), o) that approximates Q*. In our\noffline setting, we are provided with a dataset of non-\nexpert trajectories D and a non-exhaustive dataset of LTL\nspecifications D\u0173; in our experiments, we use tasks from\nprior work [3], [22].\nWe represent Q using a neural network that takes in state-\noption pairs and LTL formula embeddings. LTL embeddings\ncan be obtained by using Graph Neural Networks (GNNs)\nthat process graph representations of LTLs [23]\u2013[25]. In\nthis work, the LTL formula embedding is computed using\na Relational Graph Convolutional Network (R-GCN) [26],\nwhich we found performs well on new LTL formulae with\nthe same template structure seen during training."}, {"title": "B. Options with Diffusion", "content": "Thus far, we have not yet fully defined our option set.\nIdeally, we would like a rich option set that is expressive\nenough to cover the diverse requirements of LTL instructions.\nHowever, one crucial issue is that in the offline setting, the\ndataset available to the learner is fixed and exploration is not\npermitted. As such, the options should generate trajectories\nwithin the support of the offline dataset to ensure reliable\nQ-value estimation.\nTo address this problem, we propose to regularize the\npolicy space using diffusion options, i.e., we represent our\noption set O using a diffusion model and limit the policy's\nco-domain to be O (instead of AUO). Diffusion models form\na rich policy class that can capture multimodal distributions,\nwhile generating samples within the training dataset.\nOnce a diffusion model P\u03b8(\u03c4\u2080:\u2096|s) is trained on the dataset,\nit can be used to sample options o = \u03c4\u2080:\u2096 ~ P\u03b8(\u03c4\u2080:\u2096|s).\nHowever, during both training (Eq. 4) and deployment, we\nneed to search over options to maximize the Q function. With\ndiffusion models, finding the optimal option by sampling\nmay require a large sample size to reduce approximation\nerror. In addition, the samples may lack diversity, with many\nsimilar trajectories from a few modes of the distribution.\nQ-Guidance. One potential way forward is to leverage the\ntrained Q-network to perform conditional sampling. This\nform of classifier guidance can be achieved by methods such\nas posterior sampling. However, we find that in practice, Q-\nguided generation tends to produce low-quality trajectories\n(see Section V-B).\nDiversity Sampling. We propose an alternative sampling\napproach that covers different modes of data distribution,\nwhile keeping the sample batch size M as small as possible.\nThe key idea is that each generated trajectory should be\ndistinct from the others in the batch. A standard diffusion\nmodel generates options using an approximate conditional\nscore s\u03b8 (\u03c4\u2080|s) \u2248 \u2207\u03c4\u2080 log p\u03b8 (\u03c4\u2080|s\u2080). We instead pro-\npose to sample from a posterior that conditions on the\nM \u2212 1 other samples with the conditional score function\n\u2207\u03c4(\u00b9) log p\u03b8 (\u03c4(\u00b9)|s\u2080) + \u2207\u03c4(\u00b9) log p({\u03c4(\u1d50)}\u1d50=\u2082,\u22ef,M |\u03c4(\u00b9)). In the following text\nwe omit the subscripts 0: k and conditioned state s\u2080 for\nclarity.\nWith this change, the M samples are no longer indepen-\ndent. We leverage Bayes\u2019 rule and sample estimation [31]\nto express the conditional score as the sum of two terms:\n\u2207\u03c4(\u00b9) log p\u03b8 (\u03c4(\u00b9)) and \u2207\u03c4(\u00b9) log p({\u03c4(\u1d50)}\u1d50=\u2082,\u22ef,M | \u03c4(\u00b9)),\nwhere the noiseless trajectory \u03c4\u0302(\u00b9) is estimated via Tweedie\u2019s\nformula [32] \u03c4\u0302(\u00b9) = \u2207\u03c4(\u00b9) (\u03c4(\u00b9)) + (I \u2212 \u2207\u03c4(\u00b9)) \u2207\u03c4(\u00b9) log p\u03b8 (\u03c4(\u00b9)))\nand p\u03b8({\u03c4(\u1d50)}\u1d50=\u2082,\u22ef,M | \u03c4(\u00b9)) is approximated by point\nestimation p\u03b8({\u03c4(\u1d50)}\u1d50=\u2082,\u22ef,M | \u03c4(\u00b9)).\nWe model this conditional probability using a differen-\ntiable probabilistic model. To encourage sample diversity,\nthe conditional probability should be high if \u03c4(\u00b9), ..., \u03c4\u0302(M)\nare pairwise dissimilar, and low if any of them are similar.\nThus, the unnormalized probability of the generated set can\nbe modeled by a similarity matrix analogous to determinantal\npoint processes [33],\np (\u03c4(\u00b9), {\u03c4(\u1d50)}\u1d50=\u2082 | s\u2080) = p (\u03c4(\u00b9)) det (L\u1d39)^{1/2}, (5)\nwhere L\u1d39 is the similarity matrix with elements indexed\nby integers (u, v), 1 < u,v \u2264 M measuring the similarity\n(e.g., cosine similarity) between \u03c4(\u1d58) and \u03c4(\u1d5b), and Z =\n\u222b p (\u03c4(\u00b9)) det (L\u1d39) d \u03c4(\u00b9) is a normalizing constant [34].\nSince the normalizing constant Z does not depend on \u03c4(\u00b9),\nthe gradient of the point estimate becomes\n\u2207\u03c4(\u00b9) log p\u03b8({\u03c4(\u1d50)}\u1d50=\u2082 | \u03c4(\u00b9)) = \u2207\u03c4(\u00b9) log det (L\u1d39).\n(6)\nThis gradient can be plugged into the reverse process for\nposterior sampling as summarized in Algorithm 2. Our\napproach maximizes the determinant of the similarity matrix,\nbiasing the generated trajectories to be distinct from one\nanother, thus promoting diversity."}, {"title": "IV. RELATED WORK", "content": "LTL is widely-used to specify high-level, temporally ex-\ntended requirements in robot tasks [1], [2], [35]. Many\nexisting data-driven methods for LTL planning learn in an"}]}