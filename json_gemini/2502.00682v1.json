{"title": "Guidance Source Matters: How Guidance from AI, Expert, or a Group of Analysts Impacts Visual Data Preparation and Analysis", "authors": ["Arpit Narechania", "Alex Endert", "Atanu R Sinha"], "abstract": "The progress in generative Artificial Intelligence (AI) has fueled AI-powered tools like co-pilots and assistants to provision better guidance, particularly during data analysis. However, research on guidance has not yet examined the perceived efficacy of the source from which guidance is offered and the impact of this source on the user's perception and usage of guidance. We ask whether users perceive all guidance sources as equal, with particular interest in three sources: (i) \u201cAI,\u201d (ii) \u201chuman expert,\" and (iii) \"a group of human analysts.\" As a benchmark, we consider a fourth source, (iv) \"unattributed guidance,\" where guidance is provided without attribution to any source, enabling isolation of and comparison with the effects of source-specific guidance. We design a five-condition between-subjects study, with one condition for each of the four guidance sources and an additional (v) \"no-guidance\" condition, which serves as a baseline to evaluate the influence of any kind of guidance. We situate our study in a custom data preparation and analysis tool wherein we task users to select relevant attributes from an unfamiliar dataset to inform a business report. Depending on the assigned condition, users can request guidance, which the system then provides in the form of attribute suggestions. To ensure internal validity, we control for the quality of guidance across source- conditions. Through several metrics of usage and perception, we statistically test five preregistered hypotheses and report on additional analysis. We find that the source of guidance matters to users, but not in a manner that matches received wisdom. For instance, users utilize guidance differently at various stages of analysis, including expressing varying levels of regret, despite receiving guidance of similar quality. Notably, users in the AI condition reported both higher post-task benefit and regret. These findings strongly indicate the need to further understand how different guidance sources impact user behavior for designing effective guidance systems.", "sections": [{"title": "1 INTRODUCTION", "content": "The prospect of offering guidance to users of a system has received fillip over the past two years as talks of Artificial Intelligence (AI)- powered tools like co-pilots and assistants caught imaginations of users and businesses alike. Hitherto, the guidance literature has paid attention to the important dimensions of \"why,\u201d \u201chow,\u201d \u201cwhat,\" and \"when\" in guided interactions [29, 79]-contributing theories, models, frameworks, and tools on what guidance to provide, why, how, and when. In this work, we investigate another dimension-\"from whom\"- focusing on the source of guidance-such as humans or an AI-which remains a gap in research. Today, guidance is already being sought from a variety of sources across various domains. For instance, in the analytics domain, guidance is sought from human experts (e.g., an expert analyst or consultant) or groups of peers (e.g., via community forums such as Stack Overflow\u00b9, HuggingFace2, GitHub\u00b3). Relevant research on advice-taking has already studied and compared such guidance from human experts and peers [57, 58, 65, 68, 69, 98]. Recently, there has been a growing expectation for guidance to come from an AI [22, 62, 95], even though this guidance must itself rely on data from human experts or groups of peers, or other systems, to train the models and align subsequent recommendations with user preferences. Thus, studying this \"from whom\" dimension is important because the effect of source-attribution in providing guidance carries significant implications for offering guidance systems.\nIn response, in this work, we focus on how users' perception and utility of guidance coming from a particular source impacts their performance during attribute selection as part of a visual data preparation task. In particular, we study three guidance sources-AI, human Expert, and a Group of human analysts-to answer our research question, \"Does the source of guidance matter to users even when the quality of guidance is held constant?\" As a benchmark, we also consider a fourth source, Unattributed, wherein guidance is provided without attribution to any source, enabling isolation of and comparison with the effects of source-specific guidance. We design a five-condition between-subjects study, with one condition for each of the four guidance sources and an additional no-guidance condition, Control, which serves as a baseline to evaluate the influence of any"}, {"title": "2 RELATED WORK", "content": ""}, {"title": "2.1 Visual Analytics and Guidance", "content": "Visual Analytics (VA) is a human-in-the-loop approach that combines automated analysis techniques with interactive visualizations for an effective understanding, reasoning, and decision-making based on large, complex datasets [54]. VA systems incorporate concepts from mixed-initiative systems to \"enable users and intelligent agents to collaborate efficiently\" [49] by taking initiatives on behalf of each other during analysis. More recently, VA systems have embraced a 'human is the loop' perspective-which emphasizes the central role of the user-by enabling the system to implicitly infer their workflow(s), and seamlessly integrating analytics into it [28]. However, automated actions by the system can be mistimed or a result of misinterpreted user intent; whereas, users may need to provide feedback on these automated actions or configure (feedforward) their intent to the system upfront, requiring a continuous, effective dialogue between the two to ensure smooth and effective analytic progress. Guidance is one such computer-assisted process that aims to actively resolve this \"knowledge gap\" between the user's understanding and the system's capabilities during an interactive analysis session [17, 18, 23]. In addition, guidance also aims to ensure effective system operation [82], enhance usability [25], improve analysis efficiency, validate insights, build confidence, prevent bias, and improve clarity of findings [23].\nThere have been several approaches to conceptualize and apply guidance in visual analytics to improve the quality of interactions between users and systems. Engels [29] characterized guidance into a \"what\" dimension (that defines the problem) and a \"how\" dimension (that defines mechanisms to solve the problem). P\u00e9rez- Messina et al. [79] proposed a typology of guidance tasks covering the \"why,\" \u201chow,\u201d \u201cwhat,\" and \"when\" aspects of guided interactions. Ceneda et al. [17, 18] characterized guidance based on the user's knowledge gap, the input and output of the guidance process, and its degree, later formalizing a methodology for designing effective guidance systems [16]. Sperrle et al. [85\u201387] introduced the concept of co-adaptive guidance wherein the user and the system teach and\""}, {"title": "2.2 Studies on Utilization of and Reliance on Guidance", "content": "Understanding how guidance is utilized or relied on is critical for designing effective guidance systems, and has also been a key focus of many prior studies. For instance, Wall et al. [96], Narecha- nia et al. [73], and Paden et al. [77] studied how presenting visual traces of a user's interaction history during analysis can help in- crease their awareness of analytic behavior and mitigate exploration biases. Sperrle et al. [84] conducted a Wizard of Oz study to in- vestigate the interaction dynamics between users and systems in co-adaptive guidance scenarios, focusing on the impact of guidance timing, contextualization, and adaptation, as well as the effects of misguidance on user confidence. Sperrle et al. [88] also studied how context-dependent user preferences and feedback during topic model refinement can enable the system (not the user) to learn and adapt its subsequent guidance, fostering effective co-adaptive guidance and human-machine collaboration.\nIn terms of how people utilize and rely on guidance recommen- dations originating from different sources, we found a complex interplay between human and algorithmic judgments. Some stud- ies found that users trust and rely on human partners more than \u0391\u0399 [7, 27, 31, 44, 56, 61], whereas some others found the oppo- site [38, 62, 64]. For example, Logg et al. [62] found people often trust algorithms more than human expertise, despite not fully un- derstanding the algorithm's intricacies. Several studies found that people's reliance on AI depends on various contextual factors, such as their Al literacy [50], domain expertise [39], and amount of feed- back [38]. For example, Gajos et al. [38] found that people make more accurate decisions by actively engaging with detailed explanations of AI recommendations-rather than just viewing the recommen- dations. Among human partners, studies have revealed differences between guidance from experts versus groups. Chen et al. [20] show that online book purchasing decisions are heavily influenced by consumer recommendations rather than expert opinions. Similarly, Vedadi et al. [94] find that in information security decisions, users tend to imitate others when faced with uncertainty, impacting their choices more than their personal assessments. This limitation ef- fect extends to software adoption, where user reviews influence lower-ranked products' adoption but not top ones [26].\nIn our study, we examine the utilization and perception of guidance from an AI, a human Expert, or a Group of analysts as source, which represents what users do, complementing prior work that investigates trust and reliance, which represent what users say. Our study also collects typed textual responses about trust and reliance to make the set of measures comprehensive in having both objective measures of utilization and subjective metrics of perception. We next describe relevant metrics available in prior work that quantify users' utilization of and reliance on guidance."}, {"title": "2.3 Metrics to Model Utilization of and Reliance on Guidance", "content": "Several metrics have been proposed that measure people's reliance on AI guidance, quantifying people's \"agreement\u201d and \u201cdisagreement\u201d with AI recommendations [64], people's \"acceptance\" of incorrect AI recommendations [14], people's \"change\" in behavior based on AI recommendations [55, 62], and people's propensity to \"delegate\" eventual decision-making to AI [21]. For instance, Lu et al. [64] proposed the \"agreement\u201d and \u201cdisagreement\u201d metrics, which assess how often user predictions are the same as, or different from, AI recommendations, respectively, when users make predictions before seeing AI recommendations. Buccinca et al. [14] measured how often users accept incorrect AI recommendations and how often users make mistakes when their predictions differ from the AI's, with both the user's prediction and the AI's recommendation being wrong. Kim et al. [55] proposed \"Switch Fraction\" to assess how often users completely change their answers to match AI recommendations [55]. Logg et al. [62] proposed \"Weight of Advice\" (WOA) to measure the proportional change in user predictions relative to the change in AI recommendations. Chiang et al. [21] proposed the \"delegation\" metric to measure how often users let an Al system fully make decisions on their behalf. These metrics provide a framework for understanding user interactions with guidance systems. In our study, we task participants to select relevant attributes from unfamiliar datasets, but there is no known ground truth in terms of number of attributes to select. Thus, we created new metrics based on these existing ones that better fit our study design."}, {"title": "2.4 Data Preparation and Subset Selection in Visual Analytics", "content": "Data preparation involves analyzing the data to ensure high-quality results through collection, integration, transformation, cleaning, reduction, and discretization [100]. As organizations follow a \"load- first\" philosophy and \u201cdump\u201d their data into centralized reposito- ries [40], the volume of data often overwhelms users, creating chal- lenges in data navigation, discovery, and monitoring [24, 33, 75, 76]. To mitigate these challenges, prior work has utilized several tech- niques based on the raw data [24, 70], meta-data [3, 46], and users' queries [11, 15, 101]. For example, Goods infers metadata from billions of datasets within an organization, making them searchable using keywords [46]. Similarly, there exist several proprietary [1, 2, 4] and open-source [2, 5] tools that provide data profile, quality, and lineage information for data observability, monitoring, and pipeline optimization. For example, Profiler [53] uses data mining to automat- ically detect quality issues in tabular data and offers coordinated visu- alizations for context; whereas, Tableau Prep [90], OpenRefine [47], and Wrangler [52] provide interactive affordances to explore, clean, structure, and shape the data before analysis.\nOur study focuses on subset selection [59], which can be achieved through \"feature set reduction\" to decrease the number of attributes or \"sample set reduction\" to reduce the number of records. Feature set reduction is often used in machine learning to eliminate irrelevant features [51] or perform dimensionality reduction [36]; whereas sam- ple set reduction is commonly applied in market segmentation [92] to identify specific consumer groups. Several existing tools facilitate this process of subset selection. For example, DataPilot [74] presents"}, {"title": "3 TASK: DATA PREPARATION AND SUBSET SELECTION FOR VISUAL ANALYTICS", "content": "In this section, we describe the source conditions of our study, the study prototype, the study task, and how we ensure internal validity."}, {"title": "3.1 Source (Study) Conditions", "content": "We designed a between-subjects study wherein participants were randomly assigned to one of five experimental conditions. In each condition, guidance was presented as if it came from an attributed- source or from an unattributed-source, holding the quality of guidance same across all guidance sources. The five experimental conditions along with their definitions are as follows:\nAI Guidance comes from an AI model trained on large information for data analysis tasks.\nExpert Guidance comes from a human expert analyst who is well regarded in industry for acumen in data analysis.\nGroup Guidance comes from a group of human analysts in your organization well versed in data analysis.\nUnattributed Guidance comes without an explicit mention of a source.\nControl No guidance is available in the interface."}, {"title": "3.2 Study Prototype", "content": "With no existing system that satisfies our goals of experimental control and quality control of guidance, we developed one wherein users can inspect a dataset, select relevant attributes, request guidance, and create visualizations. We used Angular [41] as our frontend framework and interfaced it with a Python [37] backend over the REST [81] and websocket [34] protocols. We persisted logged user interactions on the cloud via Google Cloud Logging [42].\nUI Tab 1: Explore and Analyze\n(A) Data Attributes shows the list of dataset attributes along with their definitions and (de)select affordances.\n(B) Data Records shows the entire dataset in an interactive table along with sorting and pagination affordances.\n(C) Guidance shows one attribute (up to 10) each time the user requests guidance via a button.\nIn this tab, users can Search attributes by keywords or Sort attributes (by their name) to organize their search space. For each visible data attribute, users can Inspect it to see a brief description, Click on it to see detailed records, Select it to be in their subset, Deselect it from their subset, and Request Guidance (except for Control participants who do not receive any guidance). For the data records, users can Paginate or Sort the interactive table.\nUI Tab 2: Create Dashboard\n(D) Selected Data Attributes, similar to (A) Data Attributes, shows the list of attributes selected by the user.\n(E) Mark and Encodings shows affordances to create visual- izations: a visualization mark type (bar, point, line), visual encodings (x, y, fill color, size, shape), and encoding aggrega- tions (sum, mean, max, min).\n(F) Visualization shows the visualization based on the Mark and Encodings along with an affordance to save it.\n(G) Saved Visualizations shows the list of all saved Visualiza- tions, including affordances to delete one or all.\nIn this tab, users can Inspect and Deselect selected attributes from the Selected Data Attributes view. Additionally, users can Create visualizations by changing the mark type, changing and resetting visual encodings and aggregations, and swapping the attributes mapped to xy axes. Lastly, users can Save visualizations or Delete one or all saved visualizations."}, {"title": "3.3 Main (Study) Task", "content": "Using the study prototype, we tasked participants to perform the following analysis task:\nAnalyze the provided dataset of attributes derived from online customer behaviors. Design multiple visualiza- tions indicating meaningful drivers of dollar ($) sales revenue for the company.\nSelect between 5 to 15 attributes (both inclusive); and make at 3 visualizations. It is important for you to pay attention to the attributes you select and the visualiza- tions you create from them.\nNote that these attributes and visualizations will be examined by your boss, who will then decide whether to use them for their own report. Spend around 20 minutes for this task.\nFor the conditions that received guidance (AI, Expert, Group, Unattributed) there were additional instructions:\nTo help you perform this complex task, guidance is on the way. This guidance may be helpful to perform the task well. The guidance will appear on the right-hand side panel on the UI. Your judgment is important in accepting or in rejecting any guidance you receive. It is your decision whether and which guidance to use. You will receive guidance:\n(For AI) from an AI model trained on large information for data analysis tasks.\n(For Expert) from a highly regarded industry ex- pert in data analysis.\n(For Group) from a group of data analysts in your organization well versed in data analysis.\n(For Unattributed) that may be helpful to perform the task well.\nWe note that our primary focus is on participants' selection of relevant data attributes, aligned with our study goals and hypotheses. Our analyses, presented later, emanate from this primary focus. To"}, {"title": "3.4 Internal Validity", "content": "Concerned with the internal validity of the study, we hold the quality of guidance constant across the four guidance conditions: Unattributed, AI, Expert, and Group. Of the total 33 attributes in the dataset that participants can select, we carefully identify 14 attributes that are important and relevant to our study task. From these 14 relevant attributes, we select 7, and then from the remaining 19 irrelevant attributes, we select 3, to create a list of 10 total attributes. This 7:3 ratio of relevant to irrelevant attributes was intentionally chosen to encourage participants to exercise judgment when using the provided guided attributes, rather than using them indiscriminately (carte blanche). Our study description emphasizes the importance of this judgment in deciding among guided attributes. Finally, we randomize the order in which these 10 attributes are recommended in the user interface, to enforce experimental validity by avoiding an order effect. By using the same proportion of relevant and irrelevant attributes that are guided, we achieve internal validity. However, to the extent the quality of guidance can vary with source, we lose some external validity. To achieve both internal and external validity requires a much larger study, which is beyond the scope of this paper."}, {"title": "4 HYPOTHESES", "content": "A primary goal of guidance is to minimize the \"knowledge-gap\" between the user and the system to enhance analytic workflows [17, 23]. Received wisdom suggests that the framework of guidance for visual analytics is commonly conceptualized agnostic of the source of guidance [16, 19]. In guidance without source-attribution, the focus for the beneficiary-user is on the guidance provided in the system, devoid of the entity behind the guidance. Thus, we find it useful to establish user behavior with this kind of source-agnostic guidance, termed, Unattributed guidance. Comparing users' behaviors in Unattributed guidance with that of the condition of no-guidance, or Control, allows a direct assessment of the impact of guidance on users' behaviors, un-confounded by any source-attribution (i.e., AI, Expert, Group). This comparison is the basis of the first three hypotheses. Only then, we examine the incremental impact of source-attribution in a set of two additional hypotheses. Below, we discuss the source-agnostic hypotheses followed by the source- specific hypotheses. All five hypotheses were preregistered and can be accessed at https://osf.io/q6ca5."}, {"title": "4.1 Source-Agnostic Hypotheses about Guidance", "content": "The three hypotheses in this section compare Unattributed guidance to no guidance (Control). The premise of guidance is that a user has a knowledge gap with respect to the system being used. Knowledge gap manifests in higher uncertainty the user faces when considering whether to select certain attributes. Our first hypothesis, H1, pro- poses that Unattributed guidance can aid in uncertainty reduction relative to the Control condition. The reduction in uncertainty under guidance gives higher confidence in selection of attributes, which can result in selecting more number of attributes for the task at hand, especially when those attributes come from guidance. Thus, our second hypothesis, H2, proposes that more attributes are likely to be selected under Unattributed guidance as compared to Control. Also, the reduction in uncertainty makes decision making about attribute selection quicker, giving our third hypothesis, H3, that total time for task-completion is likely to be lower in Unattributed guidance relative to Control condition. We next present objective metrics that form the basis for statistically testing these hypotheses.\nH1 Participants who receive guidance will be less uncertain about their attribute selections.\nMetric(s). We define uncertainty as the variance in the number of interactions with attributes. A higher variance indicates greater uncertainty, as it suggests the participant is repeatedly interacting with an attribute, likely due to difficulty in deciding whether to select it or not.\nUncertainty = $\\frac{\\sum_{i=1}^{n}(X_i-\\mu)^2}{n}$   (1)\nwhere n = total number of attributes in the dataset, $x_i$ = number of interactions with attribute i, and $\\mu = \\frac{\\sum_{i=1}^{n} xi}{n}$ = mean number of interactions across all attributes.\nH2 Participants who receive guidance will select more attributes.\nMetric(s). Attribute selection refers to shortlisting an attribute to include in the final phase of task.\nH3 Participants who receive guidance will take lesser time to complete the task.\nMetric(s). We measure time in two ways:\n(a) Duration of the task (in minutes).\n(b) Total number of interactions performed during the task.\nThe duration measures the total time spent performing the task, while the number of interactions is proxy for the amount of engagement with the UI."}, {"title": "4.2 Source-Specific Hypotheses", "content": "Among guidance sources, human Expert guidance is valued for its credibility and specialized knowledge [32], while an AI model offers efficient and objective suggestions [13], and a human Group provides"}, {"title": "5 USER STUDY", "content": ""}, {"title": "5.1 Participants", "content": "We recruited participants by emailing relevant mailing lists within a public U.S. university. Each interested applicant was screened based on their self-reported visualization literacy (at least 3 out of 5), age (at least 18 years), and physical location (in the U.S.), as per our study protocol approved by the ethics board. We invited 109 screened applicants to participate in the study of whom 90 participants completed the study. We discarded data of three of these participants from the resultant analysis due to suspicious/insufficient interaction behavior, resulting in 87 valid participants4.\nOur valid participants were either pursuing or had received professional (n=2), associates (1), bachelors (24), masters (38), doctoral (21) degrees or a technical certificate in computer sci- ence (n=49), human-centered computing (4), human-computer in- teraction (8), mechanical engineering (4), analytics, applied and computational mathematics, business administration (2), chartered fi- nance analyst, cybersecurity, digital media, economics, electrical and computer engineering, history, industrial engineering, information management, mechatronics, robotics and automation engineering, pharmaceutical sciences, physics, public health, public policy, ro- botics, and statistical science. Demographically, they were in the 18-24 (n=47), 25-34 (38), 35-44 (1), or 45-54 (1) age groups in years and of female (42), male (44), or preferred not to say (1) genders. They reported their level of experience looking at data in a visual form (e.g., scatterplot, bar chart) on a scale from 1 (non- expert) to 5 (expert): 3 (n=36), 4 (37), 5 (14). They also reported their level of experience working with analysis tools such as Excel (n=85), Programming (75), Tableau (48), PowerBI (12), QlikView (3), D3 (3), SQL, SAS, Plotly, Kibana, Colaboratory, .NET, matplotlib, MicroStrategy, Looker, Grafana, Sheets, and Gephi."}, {"title": "5.2 Study Session", "content": "We conducted the study remote asynchronously, providing each participant with a unique link to the study's user interface and giving a maximum two weeks to complete the task, albeit in a single, focused"}, {"title": "6 RESULTS", "content": "We first describe our choice of statistical analysis, then present find- ings related to all hypotheses, followed by additional analyses of usage behavior and responses from the pre- and post-study question- naires. We include summary visualizations and statistical findings related to the hypotheses and the questionnaires in the Appendix."}, {"title": "6.1 Statistical Analysis - Non-parametric", "content": "A common analysis to compare study conditions, as in this paper, is a statistical test for comparison of means of responses, which relies upon the assumption of normal distribution of responses. The number of participants in each of our five conditions is 17, or 18, which falls well below the number 30, at which the distribution of the sample mean of a metric in a condition approaches the normal distribution. Also, the measures in our study are of two types: subjective, stated responses to questions, and objective metrics of usage. The distribu- tion of several metrics, within each condition, depicts considerable skewness, violating assumption of normal distribution. Thus, we use non-parametric analysis and tests, which do not rely on inherent assumptions about distribution and provide robustness to compare conditions for responses and metrics alike. This allows a consistent approach to both sets of measures. In particular, we use median, instead of mean, and perform non-parametric statistical tests. Addi- tionally, to comprehensively examine statistical relationships among a large set of responses, we do a parametric, multivariate regression analysis. We conducted all statistical analyses using well-established Python libraries: scikit-learn5 and statsmodels6."}, {"title": "6.2 Results of Source-Agnostic Hypotheses", "content": "We report the results of our three source-agnostic hypotheses (H1- H3) involving Unattributed and Control conditions, using one-sided Mann-Whitney U tests (one-sided as the hypotheses are one-sided)."}, {"title": "H1 Unattributed participants who receive guidance will be less un- certain about their attribute selections than Control participants who do not receive guidance.", "content": "Metric: Variance in the number of interactions with attributes.\nThe median variance in the number of interactions across all at- tributes for Unattributed participants (median=19.18) was smaller than Control participants (median=21.82). These results direc- tionally favor our hypothesis, but are not statistically significant (p-value=0.34)."}, {"title": "H2 Unattributed participants who receive guidance will select more attributes than Control participants who do not receive guidance.", "content": "Metric: Total number of attributes selected at task end.\nUnattributed participants selected more attributes (median=10.5) than Control (median=6.0). These results directionally favor our hypothesis and are statistically significant (p-value=0.001)."}, {"title": "H3 Unattributed participants who receive guidance will take lesser time to complete the task than Control participants who do not receive guidance.", "content": "Metric: Total duration of the task (in minutes).\nUnattributed spent less time (median=12.76 minutes) than Con- trol (median=13.57 minutes). These results directionally favor our hypothesis but are not statistically significant (p-value=0.67).\nMetric: Total number of interactions made during the task.\nUnattributed performed less interactions (median=141) than Control (median=145.5). These results directionally favor our hypothesis but are not statistically significant (p-value=0.72)."}, {"title": "6.3 Results of Source-Specific Hypotheses", "content": "We report results of our two source-specific hypotheses (H4-H5) involving AI, Expert, and Group conditions, using pairwise one- sided Mann-Whitney U tests with Bonferroni correction (one-sided tests as both hypotheses are one-sided).\nH4 Participants will find guidance to have more utility when it comes from Expert > AI > Group.\nMetric: Total # attributes selected at task end.\nAI (median=9) and Expert (median=9) both selected more at- tributes than Group (median=7). These results directionally favor our hypothesis, albeit partially; the pairwise comparisons between the three conditions revealed statistical significance between Expert and Group (p-value=0.01) and AI and Group (p-value=0.04).\nNotably, all three source-specific conditions selected fewer at- tributes than Unattributed (median=10.5) and more attributes than Control (median=6) (H2).\nH5 Participants will verify the guidance more when it comes from AI > Expert > Group.\nMetric: Difference in the number of interactions with guided attributes after and before guidance."}, {"title": "6.4 Results of User Behavior Analysis", "content": "Besides metrics for the analysis of hypotheses, we compute other metrics that shine light on users' analytic behaviors while performing the task and can enrich our understanding of utilization of guidance. Specifically, user behavior analyses indicate interesting new hypothe- ses that draw from the rich psychology literature and set valuable future research directions."}, {"title": "6.4.1 Exploration of Unique Attributes", "content": "Figure 4 shows the dis- tribution of the total number of unique attributes interacted by users. Group shows the highest exploration (median=25) followed by Unattributed (20.5), AI (20), Expert (20), and Control (16). Directionally, among attributed sources, less exploration under AI and Expert suggests a belief in more prescriptive guidance from these two sources relative to Group."}, {"title": "6.4.2 Exploitation of Availability of Guidance", "content": "Figure 5 shows the distribution of the number of times users requested guidance, when available. Unattributed and AI find the most exploitation (median=5) followed by Expert and Group (median=3). Four participants in each of the AI, Expert, Group, and Unattributed conditions did not request guidance at all. Among attributed sources, these results may indicate a belief of more to be gained from exploitation of AI than from others; that is, more salience [48] of AI."}, {"title": "6.4.3 Eagerness to Access Guidance", "content": "Figure 6 shows the dis- tribution of two metrics - time taken (in minutes) and number of interactions performed - before requesting guidance for the first time.\nMetric: Time taken. Analyzing for the shortest time, Group finds the most eagerness to request guidance (median=0.28 minutes into the task) followed by Unattributed (0.45), Expert (2.19), and AI (2.81).\nMetric: Interactions performed. Analyzing for the least number of interactions, Group sees the most eagerness to request guidance (me- dian=0 interactions into the task) followed by Unattributed (2), Expert (22), and AI (24). The high eagerness to access guidance under Group may indicate a belief that the stored knowledge from this source is more psychologically accessible [48], given that the source comprises analysts who are peers and thus at the same level of hierarchy as users, unlike AI and Expert who are perceived at a higher level. This aspect of accessibility along with salience is central to users' knowledge activation [48], and is well recognized in information processing in psychology."}, {"title": "6.4.4 Behavioral Decision Making about Attributes", "content": "Given our conceptualization of guidance as behavioral decision making about attributes, we analyzed how participants' interaction behavior with respect to attribute selection and deselection changed during the task relative to the received guidance (Figure 7).\n(1) [Attribute Guided \u2192 Not Selected] Users received an attribute as guidance and never selected it. Group (median=2.0) exhibited this behavior more than AI, Expert, and Unattributed (me- dian=1.0). This directionally points to users' higher disagreement with guidance coming from Group than from AI or Expert, suggesting more benefit from latter among attributed sources. This metric is similar to Lu et al.'s \"disagreement\" metric [64].\n(2) [Attribute Guided \u2192 Selected] Users received an attribute as guidance and selected it. Unattributed (median=3.0) exhib- ited this behavior more than AI (median=2.0) than Group (median=1.5) than Expert (median=1.0). This measure of agree- ment with guidance, shows no directional advantage among any of the attributed sources. This metric is similar to Lu et al.'s \u201cagreement\u201d metric [64]."}, {"title": "6.5 Analysis of Stated Responses", "content": "To complement the objective measures for utilization, we report findings from analyzing the subjective response(s) in Pre-Study Questionnaire 2 and Post-Study Questionnaire. For detailed statis- tics, refer to Table 6 in Appendix C.\n6.5.1 Opinion about Guidance before and after analysis. Figure 8 shows how participants' opinions about guidance (\u201cReliability", "Confidence": "Value Add\") changed after the task, on a scale from 1 (Disagree) to 7 (Agree), along with best fit regression lines. For each factor, we compute the difference between relevant participants' after and before scores. Then, to test for statistical significance between the four conditions that received guidance (AI, Expert, Group, Unattributed), we utilize pairwise two-sided Mann Whitney U tests with Bonferroni correction.\nMetric: Reliability. Unattributed (median=-2.0) scored lowest followed by Group (median=-1.0), then Expert and AI (median=0.0), with no statistically significant differences.\nMetric: Confidence. Unattributed and Expert (median=-2.0) scored lowest followed by Group and AI (median=-1.0), with no statistically significant differences.\""}, {"title": "6.5.2 Reliance on- and regret using guidance.", "content": "Figure 9 shows users' response scores, on a scale from 1 (None at all) to 7 (A lot), on how much they relied on (\"Reliance\") and regretted relying on (\"Regret\") guidance during the task.\nMetric: Reliance. AI and Unattributed reported higher reliance on guidance (median=3.0) than both Expert and Group (median=2.0).\nMetric: Regret. Al reported higher regret relying on guidance (median=5.0) than both Expert and Group (median=2.0), with Unattributed in between (median=3.5)."}, {"title": "6.5.3 Regret in Guidance: A Deep-dive.", "content": "In Section 1", "systems": "post- task regret may impact continued utilization of guidance, anticipatory regret may distort use of guidance [93"}]}