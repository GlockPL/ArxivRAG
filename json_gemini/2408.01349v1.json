{"title": "PC2: Pseudo-Classification Based Pseudo-Captioning for Noisy Correspondence Learning in Cross-Modal Retrieval", "authors": ["Yue Duan", "Zhangxuan Gu", "Zhenzhe Ying", "Lei Qi", "Changhua Meng", "Yinghuan Shi"], "abstract": "In the realm of cross-modal retrieval, seamlessly integrating diverse modalities within multimedia remains a formidable challenge, especially given the complexities introduced by noisy correspondence learning (NCL). Such noise often stems from mismatched data pairs, which is a significant obstacle distinct from traditional noisy labels. This paper introduces Pseudo-Classification based Pseudo-Captioning (PC2) framework to address this challenge. PC2 offers a threefold strategy: firstly, it establishes an auxiliary \"pseudo-classification\" task that interprets captions as categorical labels, steering the model to learn image-text semantic similarity through a non-contrastive mechanism. Secondly, unlike prevailing margin-based techniques, capitalizing on PC2's pseudo-classification capability, we generate pseudo-captions to provide more informative and tangible supervision for each mismatched pair. Thirdly, the oscillation of pseudo-classification is borrowed to assistant the correction of correspondence. In addition to technical contributions, we develop a realistic NCL dataset called Noise of Web (NOW), which could be a new powerful NCL benchmark where noise exists naturally. Empirical evaluations of PC2 showcase marked improvements over existing state-of-the-art robust cross-modal retrieval techniques on both simulated and realistic datasets with various NCL settings. The contributed dataset and source code are released at https://github.com/alipay/PC2-NoiseofWeb.", "sections": [{"title": "1 Introduction", "content": "Cross-modal retrieval, a cornerstone of multimodal learning, is a vibrant domain tasked with bridging diverse modalities in the vast realm of multimedia [37, 53]. Yet, the tangible success of these methods hinges on a critical presumption: the training data must be in harmonious alignment across modalities. The hitch, however, lies in obtaining such perfectly matched data pairs. Manual annotation is not only a huge task but also prone to subjective errors. A potential alternative, often adopted, is mining co-occurring image-text pairs from the vast expanse of the internet [27, 39, 46]. But this convenience comes at a cost: the introduction of noise in the form of mismatched data pairs. This brings us to the crux of our discourse noisy correspondence [26]. Unlike traditional noisy labels, which are about incorrect category labels [33, 36, 49], noisy correspondence is the mismatch between different modalities in paired data (an example is shown in the upper part of Fig. 1). The collected data, riddled with a mix of clean and noisy data pairs, can diminish the effectiveness of cross-modal retrieval techniques [26, 38, 58]. Noisy correspondence learning (NCL) mentioned above still holds vast potential for development. Since it is first introduced by NCR [26], only a handful of works have ventured further exploration and they are mainly evaluated on artificially simulated NCL datasets [26, 58]. Thus, we collect 100K website image-meta description pairs from the web to construct a large-scale NCL-specific dataset: Noise of Web (NoW), which has more complex, natural, and challenging noisy correspondences. Back to the main topic, the previous NCL solutions can be summarized as adjusting the correspondence labels, which can be recasted as the soft margin of triplet loss,"}, {"title": "2 Related Work", "content": "Bridging the semantic divide between diverse modalities is the cornerstone in multimedia research [25, 29, 54]. Such cross-modal endeavors predominantly revolve around mapping these disparate modalities into a unified, learnable space, ensuring measurable semantic correlations. However, the methodologies and challenges associate with this goal vary based on the data modalities and the alignment strategies in play, e.g., image captioning [40, 59], video captioning [52]. For the focus of this article, image-text matching, the crux lies in deriving representations from images and aligning these with their textual counterparts [10, 16, 42]. Although previous image-text matching work has achieved considerable success [8, 12, 34], a recurrent concern in these studies is the assumption of perfectly aligned training data pairs, which is hard to guarantee due to extensive collection and annotation expenses. Noisy correspondence learning (NCL), a relatively novel problem, delves into this issue [21, 26, 38, 58]. It addresses the mismatched pairs inaccurately considered positive. Initial research in this domain is NCR [26], which trains image-text matching models robustly with adaptively rectified soft correspondence label. Subsequent to NCR, its successors have ushered in enhancements on NCL. For instance, BiCro [58] introduces an innovative approach to rectify noisy correspondence labels by leveraging the bidirectional cross-modal similarity consistency. This methodology capitalizes on the inherent consistency present within paired data. On the other hand, DECL [38] exploits cross-modal evidential learning to estimate the uncertainty brought by noise to isolate the noisy pairs. A salient feature uniting these methodologies is their conciliatory strategy towards handling misaligned image-text pairs; their designs primarily revolve around mitigating the detrimental impacts of mismatched pairs by isolating them or adjusting a smaller margin in triplet ranking loss. Contrasting these approaches, PC2 furnishes direct supervisory signals for images in mismatched pairs, which enriches the learning process."}, {"title": "3 Dataset Contribution: Noise of Web", "content": "The aim of noisy correspondence learning (NCL) is building robust models based on large-scale noisy data, which can be easily obtained on website and apps. However, although there exist some noisy correspondence learning datasets such as MS-COCO [35] and Flickr30K [60] as the benchmarks, the noise in them is human generated and picked, which limits noisy correspondence models' generalization ability towards real-world applications. Randomly replacing some images' caption with others in one dataset is not a perfect choice for noise generating since there may be multiple positive and reasonable captions to one image. Another disadvantage of existing datasets is the huge human labor for writing meaningful captions for images with various different representations. For example, MS-COCO has 616,435 captions for 123,287 images, and all these captions are given by human. Although Conceptual Captions [46] (a realistic datasets) is used for NCL [26], but its low noise ratio (3%~20%) makes it insufficient for a comprehensive evaluation. Motivated by the above mentioned, we develop a new dataset named Noise of Web (NoW) for NCL. It contains 100K cross-modal pairs consisting of website images and multilingual website meta-descriptions (98,000 pairs for training, 1,000 for validation, and 1,000 for testing). NoW has two main characteristics: without human annotations and the noisy pairs are naturally captured."}, {"title": "3.1 Motivation", "content": "The aim of noisy correspondence learning (NCL) is building robust models based on large-scale noisy data, which can be easily obtained on website and apps. However, although there exist some noisy correspondence learning datasets such as MS-COCO [35] and Flickr30K [60] as the benchmarks, the noise in them is human generated and picked, which limits noisy correspondence models' generalization ability towards real-world applications. Randomly replacing some images' caption with others in one dataset is not a perfect choice for noise generating since there may be multiple positive and reasonable captions to one image. Another disadvantage of existing datasets is the huge human labor for writing meaningful captions for images with various different representations. For example, MS-COCO has 616,435 captions for 123,287 images, and all these captions are given by human. Although Conceptual Captions [46] (a realistic datasets) is used for NCL [26], but its low noise ratio (3%~20%) makes it insufficient for a comprehensive evaluation. Motivated by the above mentioned, we develop a new dataset named Noise of Web (NoW) for NCL. It contains 100K cross-modal pairs consisting of website images and multilingual website meta-descriptions (98,000 pairs for training, 1,000 for validation, and 1,000 for testing). NoW has two main characteristics: without human annotations and the noisy pairs are naturally captured."}, {"title": "3.2 Data Collection", "content": "The source image data of NoW is obtained by taking screenshots when accessing web pages on mobile user interface (MUI) with 720\u00d71280 resolution, and we parse the meta-description field in the HTML source code as the captions. In NCR [26] (predecessor of NCL), each image in all datasets are preprocessed using Faster-RCNN [41] detector provided by [1] to generate 36 region proposals, and each proposal is encoded as a 2048-dimensional feature. Thus, following NCR, we release our the features instead of raw images for fair comparison. However, we can not just use detection methods like Faster-RCNN [41] to extract image features since it is trained on real-world animals and objects on MS-COCO. To tackle this, we adapt APT [19] as the detection model since it is trained on MUI data. Then, we capture the 768-dimensional features of top 36 objects for one image. Using local objects' feature could contribute more to the contrastive learning and pseudo-caption generating, as explained in [12, 26, 32]. Due to the automated and non-human curated data collection process, the noise in NoW is highly authentic and intrinsic. For example, semantic inconsistencies between page content and descriptions (e.g., the third column in Fig. 3), nonsensical garbled description resulting from improper website maintenance (e.g., the fourth column in Fig. 3). The estimated noise ratio of this dataset is nearly 70%. More details of NoW can be found in Sec. A of Supplementary Material."}, {"title": "4 Method", "content": "In the domain of cross-modal retrieval, ensuring accurate correspondence between different modalities, such as images and text, is crucial. To comprehensively study this challenge, we take image-text retrieval as a representative task to delve into the issue of noisy correspondence. At the heart of this task is a training set denoted as $D = \\{(I_i, T_i, c_i)\\}_{i=1}^N$, where each tuple represents an image-text pair. Here, $I_i$ and $T_i$ are the image and text components of the $i$-th pair, respectively. The label $c_i \\in \\{0, 1\\}$ signifies whether the pair is matched ($c_i = 1$) or mismatched ($c_i = 0$). $N$ represents the total count of data pairs in the training set. In the conventional setting"}, {"title": "4.1 Overview", "content": "In the domain of cross-modal retrieval, ensuring accurate correspondence between different modalities, such as images and text, is crucial. To comprehensively study this challenge, we take image-text retrieval as a representative task to delve into the issue of noisy correspondence. At the heart of this task is a training set denoted as $D = \\{(I_i, T_i, c_i)\\}_{i=1}^N$, where each tuple represents an image-text pair. Here, $I_i$ and $T_i$ are the image and text components of the $i$-th pair, respectively. The label $c_i \\in \\{0, 1\\}$ signifies whether the pair is matched ($c_i = 1$) or mismatched ($c_i = 0$). $N$ represents the total count of data pairs in the training set. In the conventional setting of image-text retrieval, it is often assumed that all image-text pairs in the dataset are matching (i.e., $\\forall i \\in \\{1,\\dots, N\\}, c_i = 1$). However, multimodal datasets might be imprecisely annotated in real-world, especially if they are sourced from the internet or created using cost-effective methods (i.e., $\\exists i \\in \\{1,\\dots, N\\}, c_i = 0$), which we refer to as noisy correspondence learning (NCL). In general, we do not have sufficient resources to accurately identify the matching status of all image-text pairs, as $c_i$ can be considered inaccessible. Given $D$, we use two modal-specific encoder $f(.)$ and $g(.)$ to respectively compute the feature embedding $f(I)$ and $g(T)$. The fundamental aim of cross-modal retrieval is to map different modalities into a unified feature space, where positive pairs should exhibit higher feature similarities, while negative pairs should manifest lower similarities. The similarity between given image-text pairs is determined using the function $S(I, T)$, which is a shorthand for $S(f(I), g(T))$. Generally, the primary objective is to optimize $f$ and $g$ by minimizing a triplet ranking loss function, which is influenced by the similarity measure and a distance margin $\\alpha$:"}, {"title": "4.2 Pseudo-Classification", "content": "In NCL, addressing mismatched data is paramount. However, many approaches often overlook the protection of learning from clean data. As previously discussed in Sec. 1, once mismatched pairs are introduced into training, the efforts invested in learning from clean data can be significantly compromised. To enhance the robustness of training on clean data, we propose an auxiliary training task that reinforces the learning of such data. A key insight we offer is that in image-text pairs, the caption of an image can be considered as a classification label $y \\in \\{1, ..., K\\}$, where $K$ is a pre-defined hyper-parameter. Hence, training on image-text pairs can be conceptualized as an $K$-way classification task. For instance, we can categorize the captions in the dataset into two main classes (i.e., $K = 2$): descriptions of natural landscapes and descriptions of biological actions. We aim to train the model to group images of natural landscapes and images containing living organisms into their respective classes. To achieve this goal, we set up a pseudo-classifier $C(.)$ and utilize the captions in clean data to generate pseudo-labels for the training of $C$. Specifically, given a mini-batch of clean data $\\{(I_i,T_i)\\}_{i=1}^B$ with batch size $B$, we firstly compute pseudo-predictions $\\mathbf{p} = C(f(I_i))$ and $\\mathbf{q} = C(g(T_i))$, where $\\mathbf{p}, \\mathbf{q} \\in \\mathbb{R}^K$ are probability vectors (i.e., soft label). Next, we conduct cross-entropy loss between the hard pseudo-labels $\\mathbf{q}^* = \\arg \\max(\\mathbf{q})$ and the pseudo-predictions of images (i.e., $\\mathbf{p}$):"}, {"title": "4.3 Pseudo-Prediction Based Pseudo-Captioning", "content": "The framework of PC2 is shown in Fig. 4. With pseudo-classifier C, we design a simple and effective approach to assign pseudo-captions to $I_n$. Given a mini-batch of data $\\{(I_i, T_i^c), (I_i, T_i^n)\\}_{i=1}^B$, we first compute their pseudo-predictions $\\mathbf{p} = C(f(I_i^c))$ and $\\mathbf{p} = C(f(I_i^n))$ for $I_i^c$ and $I_i^n$. Then, for each $I_i^n$, we assign the pseudo-caption $\\hat{T}_i^p$ by"}, {"title": "4.4 Prediction Oscillation Based Correspondence Rectification", "content": "In addition to paying special attention to noisy data, learning from clean data cannot be taken lightly, because we cannot guarantee that mismatched pairs have not been erroneously included in DC. Thus, we introduce a correspondence correction module with the following core idea: the pseudo-classification results of images, learned from pseudo-labels based on captions with correct correspondences, should be stable, i.e., oscillating pseudo-predictions indicate low correspondence in the image-caption pair."}, {"title": "5 Experiment", "content": ""}, {"title": "5.1 Experimental Setup", "content": "Datasets. We mainly conduct experiments on two prominent image-text retrieval datasets and our proposed realistic NCL benchmark: (1) Flickr30K [60]: This dataset encompasses 31,000 images, each coupled with five captions. The data is partitioned into 29,000 image-text pairs for training, 1,000 for validation, and 1,000 for testing. (2) MS-COCO [35]: Consisting of 123,287 images, each image in this dataset is accompanied by five captions. The division is as follows: 113,287 image-text pairs for training, 5,000 for validation, and 5,000 for testing. (3) Noise of Web: Please refer to Sec. 3 for details. Moreover, the additional result on realistic dataset Conceptual Captions [46] can be found in Sec. C.1 of Supplementary Material. Performance Metrics. The primary metric for assessing retrieval performance is the recall rate at k (R@k). We use both images and text as query entities and report on R@1, R@5, and R@10 for the evaluation. For the well-annotated datasets Flickr30K and MS-COCO, we introduce artificial noise by randomly mixing the training images and captions at five noise levels: 0%, 20%, 40%, 50%, and 60%. For all evaluations, the best checkpoint is selected based on the validation set, and its test set performance is reported. Baselines. For a comprehensive comparison, we extensively employ the following baselines: (1) generic image-text matching ap-proaches: SCAN [32], VSRN [34], IMRAM [8], SASGR, SGRAF [12] (specially, SGR* and SGR-C [26] are SGR pre-training without hard negatives and SGR training on clean data without noisy data, re-spectively) and (2) noisy-correspondence-resistant techniques: NCR [26], DECL [38], BiCro [58] and L2RM [22]."}, {"title": "5.2 Implementation Details", "content": "Just like the previous state-of-the-art (SOTA) NCL methods [26, 58], PC2 can also be universally extended to various cross-modal retrieval models. For a fair comparison, we adopt the same cross-modal retrieval backbone, SGR [12], as used in [26, 58], i.e., a full-connected layer is adopted for f(.), Bi-GRU [43] is adopted for g(.) and a graph reasoning technique proposed in [31] is adopted for S(,). Similarly, the training details (e.g., batch size B = 128, threshold \u03c4 = 0.5, margin \u03b1 = 0.2, m = 10) are kept consistent with [26, 58]. For the additional hyper-parameters in PC2, we set K = 128 for pseudo-classification and adopt cosine similarity for Sp used in pseudo-captioning. For loss weight, we set \u03bb n = \u03bb pse = 1 and \u03bb ent = 10. Following [26], we firstly warm up the model for 5, 10 and 10 epochs for Flickr30K, MS-COCO and NoW, respectively. Then, we train the model for 50 epochs in all experiments. We use the same Adam optimizer [28] with the default parameters for training as in [26, 58]. The complete list of hyper-parameters can be found in Sec. B of Supplementary Material."}, {"title": "5.3 Results and Analysis", "content": "Main Results. We summarize the main comparisons in Tab. 1, where SoC shows promising results on both Flickr30K and MS-COCO. In the most NCL settings, PC2 outperforms all baseline methods on the indicator Rsum by a tangible margin, e.g., PC2 outperforms the best baseline method on Flickr30K at noise ratios of 40%, 50%, and 60% by 3.3, 10.2 and 5.9, respectively. Further, it is noteworthy that even in settings without noisy cor-respondences, PC2 still achieves competitive performance, which to some extent outperform the best generic method: SGRAF (504.8 vs. 499.6 on Flickr30K). Conversely, NCR may be defeated by SGRAF (522.5 vs. 524.3 on MS-COCO). From the perspective of general image-text matching methods, they all suffer a significant setback at high noise ratios (e.g., NCR with 60%), highlighting the impor-tance of NCL methods. From the viewpoint of noise-robust methods, margin-based approaches are generally weaker than the pseudo-caption-based PC2. The core enhancement of our method lies in its ability to provide the correct supervisory signal for mismatched pairs as much as possible, enabling the model to make better use of noisy data. This offers a richer imagination space for NCL. Al-though the pseudo-captions assigned by PC\u00b2 may not be completely consistent with the semantics of the noisy images, a certain degree of semantic overlap is sufficient to provide effective supervision. Additionally, we show the comparison with BiCro* [58], a variant of BiCro that uses mismatch thresholds to filter out mismatched pairs (the performance of PC2 can also benefit from this technique), in Sec. C.2 of Supplementary Material. Results on NoW. Results on our challenging NCL benchmark, NoW, in Tab. 2, show our method's consistent performance advantage. Since a significant portion of captions in NoW are in Chinese, we first consider using JiebaTokenizer [24] to conduct tokenization. Moreover, we provide the additional results on BPETokenizer [45] and BertTokenizer [11, 56] that can be applied to multilingual texts in Sec. C.3 of Supplementary Material. Compared to metic-ulously organized datasets like MS-COCO, NoW better mirrors real-world industry scenarios. The lower success of existing meth-ods on NoW reveals NCL research gaps, opening new exploration avenues for the community. Challenges of NoW are twofold: (1) high noise levels and sparse visual elements in images (web pages), with overly verbose or less informative captions; (2) overly abstract"}, {"title": "6 Discussion and Future Work", "content": "Methodology. The design of batch-level pseudo-caption search is a trade-off between ease of implementation, efficiency, and performance. A larger batch size could make it easier for PC2 to find the appropriate pseudo-captions, thereby improving the performance. Likewise, global search for pseudo-captions could further enhance PC2. In our future work, improving the caption search space of PC2 or using image captioning solution for noisy data is our focus. In addition, it is an indisputable fact that vision-language model [39, 61] and multimodal large language model [30, 50, 55] have great potential as backbones in cross-modal retrieval tasks, but their application in NCL has not been fully explored [26]. This will also be our future direction of progress. Dataset. In the future, we will increase the overall size of our dataset, and improve the validation and test sets by manually re-annotating the captions of the images, rather than just picking pairs that are manually considered to match from the original dataset."}, {"title": "7 Conclusion", "content": "In this paper, we introduce Pseudo-Classification based Pseudo-Captioning (PC2) framework to enhance cross-modal retrieval in the presence of noisy correspondence learning. PC2 innovatively employs pseudo-classification and pseudo-captions for richer su-pervision of mismatched pairs and experiments showcases PC2's superiority over existing techniques. This study further contributes by open-sourcing Noise of Web (NoW) dataset, a new powerful benchmark for NCL. In the future, we will explore PC2's potential in other areas of multimodal learning."}]}