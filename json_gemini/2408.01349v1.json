{"title": "PC2: Pseudo-Classification Based Pseudo-Captioning for\nNoisy Correspondence Learning in Cross-Modal Retrieval", "authors": ["Yue Duan", "Zhangxuan Gu", "Zhenzhe Ying", "Lei Qi", "Changhua Meng", "Yinghuan Shi"], "abstract": "In the realm of cross-modal retrieval, seamlessly integrating di-\nverse modalities within multimedia remains a formidable challenge,\nespecially given the complexities introduced by noisy correspon-\ndence learning (NCL). Such noise often stems from mismatched\ndata pairs, which is a significant obstacle distinct from traditional\nnoisy labels. This paper introduces Pseudo-Classification based\nPseudo-Captioning (PC2) framework to address this challenge. PC2\noffers a threefold strategy: firstly, it establishes an auxiliary \"pseudo-\nclassification\" task that interprets captions as categorical labels,\nsteering the model to learn image-text semantic similarity through\na non-contrastive mechanism. Secondly, unlike prevailing margin-\nbased techniques, capitalizing on PC2's pseudo-classification capa-\nbility, we generate pseudo-captions to provide more informative\nand tangible supervision for each mismatched pair. Thirdly, the\noscillation of pseudo-classification is borrowed to assistant the cor-\nrection of correspondence. In addition to technical contributions,\nwe develop a realistic NCL dataset called Noise of Web (NOW),\nwhich could be a new powerful NCL benchmark where noise exists\nnaturally. Empirical evaluations of PC2 showcase marked improve-\nments over existing state-of-the-art robust cross-modal retrieval\ntechniques on both simulated and realistic datasets with various\nNCL settings. The contributed dataset and source code are released\nat https://github.com/alipay/PC2-NoiseofWeb.", "sections": [{"title": "1 Introduction", "content": "Cross-modal retrieval, a cornerstone of multimodal learning, is a\nvibrant domain tasked with bridging diverse modalities in the vast\nrealm of multimedia [37, 53]. Yet, the tangible success of these meth-\nods hinges on a critical presumption: the training data must be in\nharmonious alignment across modalities. The hitch, however, lies in\nobtaining such perfectly matched data pairs. Manual annotation is\nnot only a huge task but also prone to subjective errors. A potential\nalternative, often adopted, is mining co-occurring image-text pairs\nfrom the vast expanse of the internet [27, 39, 46]. But this conve-\nnience comes at a cost: the introduction of noise in the form of\nmismatched data pairs. This brings us to the crux of our discourse\nnoisy correspondence [26]. Unlike traditional noisy labels, which are\nabout incorrect category labels [33, 36, 49], noisy correspondence\nis the mismatch between different modalities in paired data (an\nexample is shown in the upper part of Fig. 1). The collected data,\nriddled with a mix of clean and noisy data pairs, can diminish the\neffectiveness of cross-modal retrieval techniques [26, 38, 58].\nNoisy correspondence learning (NCL) mentioned above still holds\nvast potential for development. Since it is first introduced by NCR\n[26], only a handful of works have ventured further exploration and\nthey are mainly evaluated on artificially simulated NCL datasets\n[26, 58]. Thus, we collect 100K website image-meta description\npairs from the web to construct a large-scale NCL-specific dataset:\nNoise of Web (NoW), which has more complex, natural, and chal-\nlenging noisy correspondences. Back to the main topic, the previous\nNCL solutions can be summarized as adjusting the correspondence\nlabels, which can be recasted as the soft margin of triplet loss,"}, {"title": "2 Related Work", "content": "Bridging the semantic divide between diverse modalities is the\ncornerstone in multimedia research [25, 29, 54]. Such cross-modal"}, {"title": "3 Dataset Contribution: Noise of Web", "content": "Motivation\nThe aim of noisy correspondence learning (NCL) is building robust\nmodels based on large-scale noisy data, which can be easily ob-\ntained on website and apps. However, although there exist some\nnoisy correspondence learning datasets such as MS-COCO [35]\nand Flickr30K [60] as the benchmarks, the noise in them is human\ngenerated and picked, which limits noisy correspondence models'\ngeneralization ability towards real-world applications. Randomly\nreplacing some images' caption with others in one dataset is not a\nperfect choice for noise generating since there may be multiple pos-\nitive and reasonable captions to one image. Another disadvantage\nof existing datasets is the huge human labor for writing meaningful\ncaptions for images with various different representations. For ex-\nample, MS-COCO has 616,435 captions for 123,287 images, and all\nthese captions are given by human. Although Conceptual Captions\n[46] (a realistic datasets) is used for NCL [26], but its low noise ratio\n(3%~20%) makes it insufficient for a comprehensive evaluation.\nMotivated by the above mentioned, we develop a new dataset\nnamed Noise of Web (NoW) for NCL. It contains 100K cross-modal\npairs consisting of website images and multilingual website meta-\ndescriptions (98,000 pairs for training, 1,000 for validation, and 1,000\nfor testing). NoW has two main characteristics: without human\nannotations and the noisy pairs are naturally captured."}, {"title": "3.1", "content": "The source image data of NoW is obtained by taking screenshots\nwhen accessing web pages on mobile user interface (MUI) with\n720\u00d71280 resolution, and we parse the meta-description field in\nthe HTML source code as the captions. In NCR [26] (predecessor\nof NCL), each image in all datasets are preprocessed using Faster-\nRCNN [41] detector provided by [1] to generate 36 region proposals,\nand each proposal is encoded as a 2048-dimensional feature. Thus,\nfollowing NCR, we release our the features instead of raw images for\nfair comparison. However, we can not just use detection methods\nlike Faster-RCNN [41] to extract image features since it is trained\non real-world animals and objects on MS-COCO. To tackle this,\nwe adapt APT [19] as the detection model since it is trained on\nMUI data. Then, we capture the 768-dimensional features of top 36\nobjects for one image. Using local objects' feature could contribute\nmore to the contrastive learning and pseudo-caption generating,\nas explained in [12, 26, 32]. Due to the automated and non-human\ncurated data collection process, the noise in NoW is highly authen-\ntic and intrinsic. For example, semantic inconsistencies between\npage content and descriptions (e.g., the third column in Fig. 3),\nnonsensical garbled description resulting from improper website\nmaintenance (e.g., the fourth column in Fig. 3). The estimated noise\nratio of this dataset is nearly 70%. More details of NoW can be\nfound in Sec. A of Supplementary Material."}, {"title": "4 Method", "content": "Overview\nIn the domain of cross-modal retrieval, ensuring accurate corre-\nspondence between different modalities, such as images and text,\nis crucial. To comprehensively study this challenge, we take image-\ntext retrieval as a representative task to delve into the issue of noisy\ncorrespondence. At the heart of this task is a training set denoted\nas D = {(Ii, Ti, ci)}_i=1^N, where each tuple represents an image-text\npair. Here, I\u00a1 and Ti are the image and text components of the i-th\npair, respectively. The label ci \u2208 {0, 1} signifies whether the pair\nis matched (c\u2081 = 1) or mismatched (c\u2081 = 0). N represents the total\ncount of data pairs in the training set. In the conventional setting"}, {"title": "4.1", "content": "of image-text retrieval, it is often assumed that all image-text pairs\nin the dataset are matching (i.e., \u2200i \u2208 {1,\u2026, N}, c\u2081 = 1). However,\nmultimodal datasets might be imprecisely annotated in real-world,\nespecially if they are sourced from the internet or created using\ncost-effective methods (i.e., \u2203i \u2208 {1,\uff65\uff65\uff65, N}, c\u2081 = 0), which we refer\nto as noisy correspondence learning (NCL). In general, we do not\nhave sufficient resources to accurately identify the matching status\nof all image-text pairs, as ci can be considered inaccessible.\nGiven D, we use two modal-specific encoder f(.) and g(.) to\nrespectively compute the feature embedding f(I) and g(T). The\nfundamental aim of cross-modal retrieval is to map different modal-\nities into a unified feature space, where positive pairs should exhibit\nhigher feature similarities, while negative pairs should manifest\nlower similarities. The similarity between given image-text pairs\nis determined using the function S(I, T), which is a shorthand for\nS(f(I), g(T)). Generally, the primary objective is to optimize f and\ng by minimizing a triplet ranking loss function, which is influenced\nby the similarity measure and a distance margin \u03b1:\n$$L\u207a (Ii, Ti) = [\u03b1 \u2212 S(Ii, T\u00ec) + S(Ii, Th)]+ + [a \u2212 S(Ii, T\u2081) + S(\u00cen, Ti)]+,$$\nwhere [x]+ = max(x, 0), (Ii, Ti), (Ii, Th) and (In, Ti) are the posi-\ntive pair, negative pair treating image as query and negative pair\ntreating text as query, respectively. \u00cen = arg max1;\u22601; S(Ij, Ti) and\nTh = arg maxT;\u2260T; S(Ii, Tj) are the hardest negatives in the mini-\nbatch [16]. Dynamic margin plays a crucial role in NCL. Previ-\nous margin-based approaches [26, 38, 58] mitigate the impact of\nmismatched pairs on model training by cleverly adjusting it. The\ngeneral adjustment strategy is to set a larger a for matched pairs\nand a smaller a for mismatched pairs. However, our focus is on Ti,\ni.e., we aim to ensure that all images in the pairs have the correct\ncorresponding captions., as optimizing this loss will help the model\nconverge towards a better direction.\nIn NCL, both noisy and clean data are intermixed. Therefore, the\nfirst thing we need to consider is how to distinguish the two as\ncorrectly as possible. For simplicity, we directly utilize the mem-\norization effect\u00b9 based co-dividing module in [26] to predict the\nclean probability wi of (Ii, Ti, ci) \u2208 D. Setting a threshold \u03c4, we\ndivide D into clean subset D\u00ba = {(I, T, ci)}_i=1^Nc, and noisy subset\nD = {(I, T, ci)}_i=1^Nn, i.e., D = D\u00ba U D\" and D\u00ba \u2229 D\" = 0. For"}, {"title": "4.2 Pseudo-Classification", "content": "In NCL, addressing mismatched data is paramount. However, many\napproaches often overlook the protection of learning from clean\ndata. As previously discussed in Sec. 1, once mismatched pairs are\nintroduced into training, the efforts invested in learning from clean\ndata can be significantly compromised. To enhance the robustness\nof training on clean data, we propose an auxiliary training task\nthat reinforces the learning of such data. A key insight we offer is\nthat in image-text pairs, the caption of an image can be considered\nas a classification label y \u2208 {1, ..., K}, where K is a pre-defined\nhyper-parameter. Hence, training on image-text pairs can be con-\nceptualized as an K-way classification task. For instance, we can\ncategorize the captions in the dataset into two main classes (i.e.,\nK = 2): descriptions of natural landscapes and descriptions of bi-\nological actions. We aim to train the model to group images of\nnatural landscapes and images containing living organisms into\ntheir respective classes. To achieve this goal, we set up a pseudo-\nclassifier C() and utilize the captions in clean data to generate\npseudo-labels for the training of C.\nSpecifically, given a mini-batch of clean data {(I,T)}_i=1^B with\nbatch size B, we firstly compute pseudo-predictions p = C(f(I))\nand q = C(g(T)), where p,q \u2208 R are probability vectors\n(i.e., soft label). Next, we conduct cross-entropy loss between the\nhard pseudo-labels q = arg max(q) and the pseudo-predictions\nof images (i.e., p):\n$$Lpse = \\frac{1}{B} \\sum_{i=1}^{B} H(P_{I_i}, q_{T_i}^*),$$\nwhere H(P, Q) denotes the standard cross-entropy loss between\ndistribution Q and P. The hard pseudo-label is widely leveraged\nin semi-supervised learning [13, 48] to achieve entropy minimiza-\ntion [18], which encourages the model to make highly confident\npredictions. Moreover, to avoid C from assigning all samples to a\nsingle class, we minimize an entropy loss to spreads the pseudo-\npredictions uniformly across the all classes [3, 4, 51]:\n$$Lent = \\frac{1}{B} \\sum_{i=1}^{B} \\frac{1}{K} \\sum_{k=1}^{K} p_{I_i}^{(k)} \\log(p_{I_i}^{(k)}).$$"}, {"title": "4.3 Pseudo-Prediction Based Pseudo-Captioning", "content": "The framework of PC2 is shown in Fig. 4. With pseudo-classifier\nC, we design a simple and effective approach to assign pseudo-\ncaptions to In. Given a mini-batch of data {(I, T\u00ba), (I, T^)}_i=1^B, we\nfirst compute their pseudo-predictions p = C(f(I)) and p =\nC(f(I)) for I and I. Then, for each I, we assign the pseudo-\ncaption TP by\n$$T_n^* = T_j^* \\text{ with } j = \\arg \\max_{b \\in \\{1,...,B\\}} (S_P(p_n^c, p_b^*)),$$\nwhere SP (,) is a function that can be used to compute the similarity\nbetween two distributions. Then, we assemble In and T into a\npseudo-pair (I, T) and substitute them into Eq. (1), aiming to\nprovide more accurate supervision signals for model training. As\nwe cannot guarantee that the found pseudo-caption accurately\nreflects the semantic information of In, we dynamically adjust the\nmargin to ensure that the model benefits from a more accurate\nlevel of correspondence during training. For specific, we adaptively\nadjust a in Eq. (1) with selected j in Eq. (4) :\n$$\u03b1 = \\frac{m S_P(p_n^c, p_j^*) - 1}{m-1} \u03b1,$$\nwhere m is a pre-defined curve parameter. The underlying principle\nhere is that if the similarity of the pseudo-predictions S\u00ba (p, p) is\nhigher, then the similarity between I and If (i.e., the image in the\noriginal pair where T\u00ba is present) should also be higher, indicating\na stronger correspondence between In and TP.\nThen, for the noisy data {(I, T^)}_i=1^B in the given mini-batch,\nwe train the model by minimizing the following loss:\n$$L^{n} = \\sum_{i=1}^{B} ([\u03b1_i^n - S(I_i^n, T_i^*) + S(I_i^n, \\hat{T}_i^n)]_+ \\\n+[\u03b1_i^n - S(I_i^n, T_i^*) + S(\\hat{I}_i^n, T_i^*)]_+)."}, {"title": "4.4 Prediction Oscillation Based\nCorrespondence Rectification", "content": "In addition to paying special attention to noisy data, learning from\nclean data cannot be taken lightly, because we cannot guarantee\nthat mismatched pairs have not been erroneously included in DC.\nThus, we introduce a correspondence correction module with the\nfollowing core idea: the pseudo-classification results of images,\nlearned from pseudo-labels based on captions with correct cor-\nrespondences, should be stable, i.e., oscillating pseudo-predictions\nindicate low correspondence in the image-caption pair.\nWe define prediction oscillation as the difference between pre-\ndictions for the same sample between adjacent epochs. A larger\ndifference indicates a higher oscillation, indicating that the model\nis less confident about the sample and is resisting the supervision\nprovided by the caption-based classification labels, i.e., implying a\nweaker correspondence between the image and caption. This pat-\ntern is very similar to the DNN's memorization effect mentioned\nin Sec. 4.1. Let p_{c_i}^{(e)} represent the pseudo-prediction at epoch e,\nand its prediction oscillation oe) is evaluated by:\n$$o_i^{(e)} = D_{KL}(p_{c_i}^{(e-1)} || p_{c_i}^{(e)}),$$"}, {"title": "5 Experiment", "content": "Experimental Setup\nDatasets. We mainly conduct experiments on two prominent image-\ntext retrieval datasets and our proposed realistic NCL benchmark:\n(1) Flickr30K [60]: This dataset encompasses 31,000 images, each\ncoupled with five captions. The data is partitioned into 29,000 image-\ntext pairs for training, 1,000 for validation, and 1,000 for testing. (2)\nMS-COCO [35]: Consisting of 123,287 images, each image in this\ndataset is accompanied by five captions. The division is as follows:\n113,287 image-text pairs for training, 5,000 for validation, and 5,000\nfor testing. (3) Noise of Web: Please refer to Sec. 3 for details. More-\nover, the additional result on realistic dataset Conceptual Captions\n[46] can be found in Sec. C.1 of Supplementary Material.\nPerformance Metrics. The primary metric for assessing retrieval\nperformance is the recall rate at k (R@k). We use both images\nand text as query entities and report on R@1, R@5, and R@10\nfor the evaluation. For the well-annotated datasets Flickr30K and\nMS-COCO, we introduce artificial noise by randomly mixing the\ntraining images and captions at five noise levels: 0%, 20%, 40%, 50%,\nand 60%. For all evaluations, the best checkpoint is selected based\non the validation set, and its test set performance is reported.\nBaselines. For a comprehensive comparison, we extensively em-\nploy the following baselines: (1) generic image-text matching ap-\nproaches: SCAN [32], VSRN [34], IMRAM [8], SASGR, SGRAF [12]\n(specially, SGR* and SGR-C [26] are SGR pre-training without hard\nnegatives and SGR training on clean data without noisy data, re-\nspectively) and (2) noisy-correspondence-resistant techniques: NCR\n[26], DECL [38], BiCro [58] and L2RM [22]."}, {"title": "5.1", "content": "Implementation Details\nJust like the previous state-of-the-art (SOTA) NCL methods [26,\n58], PC2 can also be universally extended to various cross-modal"}, {"title": "5.2", "content": "retrieval models. For a fair comparison, we adopt the same cross-\nmodal retrieval backbone, SGR [12], as used in [26, 58], i.e., a full-\nconnected layer is adopted for f(.), Bi-GRU [43] is adopted for\ng(.) and a graph reasoning technique proposed in [31] is adopted\nfor S(,). Similarly, the training details (e.g., batch size B = 128,\nthreshold \u03c4 = 0.5, margin a = 0.2, m = 10) are kept consistent\nwith [26, 58]. For the additional hyper-parameters in PC2, we set\nK = 128 for pseudo-classification and adopt cosine similarity for SP\nused in pseudo-captioning. For loss weight, we set \u03bb = pse = 1\nand ent = 10. Following [26], we firstly warm up the model for 5,\n10 and 10 epochs for Flickr30K, MS-COCO and NoW, respectively.\nThen, we train the model for 50 epochs in all experiments. We\nuse the same Adam optimizer [28] with the default parameters for\ntraining as in [26, 58]. The complete list of hyper-parameters can\nbe found in Sec. B of Supplementary Material."}, {"title": "5.3 Results and Analysis", "content": "Main Results. We summarize the main comparisons in Tab. 1", "method": "SGRAF (504.8 vs.\n499.6 on Flickr30K). Conversely, NCR may be defeated by SGRAF\n(522.5 vs. 524.3 on MS-COCO). From the perspective of general\nimage-text matching methods, they all suffer a significant setback\nat high noise ratios (e.g., NCR with 60%), highlighting the impor-\ntance of NCL methods. From the viewpoint of noise-robust methods,\nmargin-based approaches are generally weaker than the pseudo-\ncaption-based PC2. The core enhancement of our method lies in its\nability to provide the correct supervisory signal for mismatched\npairs as much as possible, enabling the model to make better use\nof noisy data. This offers a richer imagination space for NCL. Al-\nthough the pseudo-captions assigned by PC\u00b2 may not be completely\nconsistent with the semantics of the noisy images, a certain degree\nof semantic overlap is sufficient to provide effective supervision.\nAdditionally, we show the comparison with BiCro* [58", "24": "to conduct tokeniza-\ntion. Moreover, we provide the additional results on BPETokenizer\n[45", "56": "that can be applied to multilingual\ntexts in Sec. C.3 of Supplementary Material. Compared to metic-"}]}