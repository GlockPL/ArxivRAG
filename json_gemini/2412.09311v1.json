{"title": "Advancing Attribution-Based Neural Network Explainability through Relative Absolute Magnitude Layer-Wise Relevance Propagation and Multi-Component Evaluation", "authors": ["DAVOR VUKADIN", "PETAR AFRI\u0106", "MARIN \u0160ILI\u0106", "GORAN DELA\u010c"], "abstract": "Recent advancement in deep-neural network performance led to the development of new state-of-the-art approaches in numerous areas. However, the black-box nature of neural networks often prohibits their use in areas where model explainability and model transparency are crucial. Over the years, researchers proposed many algorithms to aid neural network understanding and provide additional information to the human expert. One of the most popular methods being Layer-Wise Relevance Propagation (LRP). This method assigns local relevance based on the pixel-wise decomposition of nonlinear classifiers. With the rise of attribution method research, there has emerged a pressing need to assess and evaluate their performance. Numerous metrics have been proposed, each assessing an individual property of attribution methods such as faithfulness, robustness or localization. Unfortunately, no single metric is deemed optimal for every case, and researchers often use several metrics to test the quality of the attribution maps. In this work, we address the shortcomings of the current LRP formulations and introduce a novel method for determining the relevance of input neurons through layer-wise relevance propagation. Furthermore, we apply this approach to the recently developed Vision Transformer architecture and evaluate its performance against existing methods on two image classification datasets, namely ImageNet and PascalVOC. Our results clearly demonstrate the advantage of our proposed method. Furthermore, we discuss the insufficiencies of current evaluation metrics for attribution-based explainability and propose a new evaluation metric that combines the notions of faithfulness, robustness and contrastiveness. We utilize this new metric to evaluate the performance of various attribution-based methods. Our code is available at: https://github.com/davor10105/relative-absolute-magnitude-propagation", "sections": [{"title": "1 INTRODUCTION", "content": "Deep neural networks (DNNs) have become essential for processing diverse data types, enabled by recent advances in deep learning hardware. These developments allow training models with unprecedented scale in both parameters and training data size. Scaling these variables often leads to state-of-the-art models surpassing human-level performance in tasks like image classification [40], reinforcement learning [44] and natural language understanding [24]. The presence of an extensive number of parameters and multiple non-linear layers contributes to the opaque nature of these models, rendering them challenging to interpret and understand. This fact encourages many researchers, especially in areas where model explainability is crucial, such as aviation, medicine or banking, to use smaller, often linear models that are transparent to their decision process.\nIn the realm of explainable artificial intelligence (XAI), two critical concepts are model interpretability and interpretations. Model interpretability assesses the intrinsic properties of a deep model, gauging the humans comprehensebility of its inference results. Interpretations involve algorithms explaining how deep models make decisions, such as highlighting discriminative features for a specific classification or identifying influential training examples. Given the paper's goal to unveil the inner workings of black-box models, the primary focus is on the concept of interpretations.\nVarious interpretation tools aim to explain DNN decision-making, but no method suits every task and model, making this area of research ongoing. Different principles create interpretability methods, such as highlighting input features on which the deep model mainly relies for a particular inference, either by using gradients [53] [46] [5] [48] [42], perturbations [35] or proxy explainable models [37]; visualizing intermediate features [53] or visualizing counterfactual examples [50]; analyzing training data to assess each data point's contribution to a particular inference result [27]. This paper\u2019s focus is on highlighting the input features that the model relied on most during inference, by propagating and tracking layer activations through the deep network.\nSubjective evaluation of attribution maps can be conducted by humans, but a quantitative measure is crucial for objective comparison and ranking of different attribution methods. Our focus is on evaluating explanation quality for specific examples and neural networks, acknowledging the complexity of this task. Explanation quality depends not only on the attribution method but also on classifier performance influenced by factors like network architecture and training data.\nAttributions should reflect the model's perspective rather than strictly adhering to human intuition. A segmentation map of an object may not qualify as an attribution map, potentially overlooking crucial evidence. Evaluation methods categorize different aspects like faithfulness, robustness, and localization. However, there's no established approach in the literature to effectively combine scores across classes for determining the best attribution method.\nOur work addresses current evaluation method limitations, proposing a novel approach that integrates faithfulness, robustness, and localization into a single score. This comprehensive assessment aims to inform researchers and practitioners about the performance of attribution methods, facilitating informed decisions in their selection and application.\nTo summarize, the main contributions of this work are:\n\u2022 The development of a novel Layer-Wise Propagation rule, referred to as Relative Absolute Magnitude Layer-Wise Relevance Propagation (absLRP). This rule effectively addresses the issue of incorrect relative attribution between neurons within the same layer that exhibit varying absolute magnitude activations. We apply this rule to three different architectures, including the very recent Vision Transformer, and demonstrate clear advantages over existing work.\n\u2022 The proposal of a new evaluation method, Global Attribution Evaluation (GAE), which offers a novel perspective on evaluating faithfulness and robustness of an attribution method by"}, {"title": "2 RELATED WORK", "content": "In the following section, we offer a comprehensive overview of the existing literature on layer-wise relevance propagation, transformer explainability, and attribution-based evaluation metrics."}, {"title": "2.1 Layer-Wise Relevance Propagation", "content": "Bach et al. [10] presented an algorithm for determining the relevance of a particular input neuron to the output of a non-linear network called layer-wise relevance propagation (LRP). LRP assumes that the classifier can be decomposed into several layers of computations, which can be parts of the feature extraction phase or parts of a classification algorithm that runs on the computed features. Given an input x and a neural network f the aim of LRP is to assign each input position p (for example, in the case of images, to each pixel) a relevance score $R_p^0$, where the superscript 0 indicates the first, input layer. Assuming the knowledge of the relevance map of the last layer $R^l$, the goal of a particular LRP formulation is to describe how to disperse output neuron j relevance to each of the input neurons i in the layer before - $R_i^{(l-1,l)}$, such that the following equation holds:\n$R_i^{(l-1)} = \\sum_{j \\in c(i)} R_{i \\rightarrow j}^{(l-1,l)}$\nFor a particular layer of a neural network with an activation function g and input x, the activation of an output neuron in the following layer is defined as:\n$a_j = g(\\sum_i x_i W_{ij})$\nThe term $x_0$ is set to 1 so that $w_{0j}$ represents the bias of the output neuron j. Bach et al. [10] proposed several formulas for computing $R_i^{l-1}$ from relevance scores of the following layer $R^l$:\n\u20ac-rule(LRP-\u20ac):\n$R_i^{l-1} = \\sum_j \\frac{x_i W_{ij}}{\\sum_k x_k W_{kj} + \\epsilon} R_j^l$\nThis rule redistributes the relevance of the following layer to the layer before based on the proportion of the contributions of each input neuron to the activation of the output neuron.\n\u03b1\u03b2-rule (LRP-\u03b1\u03b2):\n$R_i^{l-1} = \\sum_j \\alpha \\frac{(x_i W_{ij})^+}{\\sum_k (x_k W_{kj})^+ } - \\beta \\frac{(x_i W_{ij})^-}{\\sum_k (x_k W_{kj})^- } R_j^l$\nLRP-a\u00df treats positive and negative activations separately, offering new terms, hyperparameters of this method, a and \u00df to specify the relative importance between the two categories. Bach et al. [10] have found that values of \u03b1 = 2 and \u00df = 1 produce sharp relevance maps.\nHowever, the authors of Relative Attributing Propagation (RAP) [33] highlight an issue with commonly used LRP rules when propagating relevance towards the input layer. If a neuron receives conflicting large relevance values (e.g., one large positive and one large negative) from its subsequent layer, these values tend to cancel out, despite their significant impact on the model's final output."}, {"title": "2.2 Transformer Explainability", "content": "Transformers have become a staple in machine learning research and state-of-the-art approaches in almost all fields, such as image [19] and natural language processing [18], image generation [36] and reinforcement learning [15]. Their widespread use necessitates the development of explainability tools which can aid in model debugging and verifying fairness and unbiasedness of such approaches.\nThe main building blocks of Transformers are the self-attention layers, mapping a query and key-value pairs to an output. Several works exploited these attention scores as relevancy scores [49] [12] [51], but only using the final attention layer to compute these scores. Another option is"}, {"title": "2.3 Attribution-based Evaluation Metrics", "content": "The evaluation of attribution-based explanation methods can be categorized into several classes, each assessing different properties of these methods. The following classes are notable.\nFaithfulness - consists of importance correlation - the magnitude of attribution weights should reflect the importance of input components and polarity consistency - the sign of attribution weights should correctly indicate the polarity of input impact, in other words, the contribution or suppression effects to the model's prediction, as defined by Liu et al. [31]. Metrics in this category are based on observing the change in the model's output when permuting the input based on a given attribution map. Several metrics aim to compute the correlation between the model's original probability output and the perturbed one [11] [3] [34] [32] [5] [38] [52] or the correlation between probability drops and attribution scores on various points [3] [11], while others observe the change in model performance after perturbing the input [9] [39] [30].\nRobustness - measures the stability of attributions when subject to slight perturbations of the input, assuming that model output approximately stayed the same. These can be further divided in metrics that look for similar examples that should produce similar explanations [52], capture the strongest variation in the attribution map when perturbing the input [32] or measures the probability that the inputs with the same attributions have the same prediction label [16].\nLocalization - quantify the alignment between the highest scores of the generated attribution map and the target object outlined by the ground-truth mask [54] [28] [8].\nWe outline three most relevant approaches for our evaluation metric and address their drawbacks.\nApproach 1. Samek et al. [41] propose a metric for evaluating an attribution-based explanation method as a generalization of the approach presented by Bach et al. [10], where the authors use attributions generated by different methods to guide region perturbation of the input image. Concretely, for a model f, in each of the L steps indexed by k, the first k 9 \u00d7 9 regions of the input image sorted by attribution scores undergo perturbation by replacing them with randomly sampled values from a uniform distribution. At each step, a difference in the output neuron's value is observed between the original value and the value gotten by passing the perturbed image at step $k: x_{k}^{MoRF}$, where MoRF is an abbreviation of the used process - most relevant first. Those differences are then added together for every step and averaged over the entire dataset. An ordering of regions such that the most sensitive regions are ranked first would imply a steep decrease of the output neuron's value, this resulting in a larger area over the MoRF perturbation curve (AOPC):\nAOPC = $\\frac{1}{L+1 N} \\sum_n^N \\sum_k^L (F(x_n) - f(x_{n_{k}}^{MoRF}))$\nAs a further extension of this metric, the authors propose another metric, area between perturbation curves (ABPC). This metric takes in account the gap between the output neuron scores for an image modified by perturbing the most relevant first (MoRF), and the least relevant first (LeRF):\nABPC =$\\frac{1}{L+1 N} \\sum_n^N \\sum_k^L (f(x_{n_{k}}^{LeRF}) - f(x_{n_{k}}^{MoRF}))$\nIn the case of LeRF, the information in the image should be very stable and close to the original value for small value of k, and only drops quickly as k approaches L. Thus, this metric gauges how good an attribution method is at selecting both the most relevant parts of the image, so that the classification score drops the most, while also providing information about the least relevant parts of the image, whereby perturbing them, the classification score changes the least. The authors perturb the first 100 regions of the image, resulting in 15.7% of the image being exchanged.\nRecently, Rong et al. [39] expanded upon this approach. They followed the same principle of masking input features, which are either highly relevant (MoRF) or lowly relevant (LeRF), based on the initial attribution map. However, they introduced a new perturbation method by utilizing a"}, {"title": "3 METHOD", "content": "This section delves into a thorough description of our novel attribution approach, elucidating its intricacies and highlighting its applicability to complex architectures, including ResNets and Vision Transformers. Additionally, we introduce a novel evaluation metric for attribution methods, which effectively assesses their local and contrastive properties."}, {"title": "3.1 Relative Absolute Magnitude Layer-Wise Relevance Propagation", "content": "Motivated by the aforementioned issues with current LRP propagation rules, we propose a new rule called Relative Absolute Magnitude Layer-Wise Relevance Propagation (absLRP). It solves the issue of conflicting relevance found in most LRP rules by observing only the positive parts of neuron activation, similarly to LRP-\u03b11\u03b20. However, to address incorrect relative attribution due to varying magnitudes within neurons of the same layer, we use the absolute final output of each neuron as the normalizing factor. To produce contrastive attribution maps, we utilize the idea from Gu et al. [22] and we set the last layer's starting attribution as 1 for the target class, and -$\\frac{1}{N-1}$ for the other classes, N being the total number of classes. Applying this new rule yields sparse and contrastive attribution maps with no noise. The new LRP rule is defined as:\n$R_i^{l-1} = \\sum_j \\frac{(x_i W_{ij})^+}{\\sum_k | x_k W_{kj} | + \\epsilon} R_j^l$\nLooking back to the example in Figure 1, we already discussed how LRP-\u03b11\u03b2o assigns the same relevance score to each of the input neurons, regardless of the relative absolute magnitudes of activations in the first and the second hidden neuron. By applying our rule on the left network, we obtain the same attribution scores as the LRP-a1\u00dfo method, since both of the branches have equal absolute magnitudes of 3. However, when applying our method to the right network, the resulting attributions become [0.25, 0.75], a 50% difference attribution difference for each of the input neurons compared to the LRP-a1\u00dfo's attribution, since the second branch now has a much larger absolute magnitude of 11 which leads to its greater impact on the attribution map. The magnitude difference between the two branches amplifies this effect. If we make the difference even larger, for example, by setting the second hidden neuron's weights as [\u221217, 18], while LRP-\u03b11\u03b2\u03bf keeps the same attributions, our method now assigns 9 times more relevance to the second input neuron, since its absolute positive influence in the network is 18, compared to the first neuron's 2."}, {"title": "3.2 Global Attribution Evaluation", "content": "To address existing issues in attribution map evaluation, we propose a comprehensive metric for assessing the quality of a given attribution method $m_A$ by observing several factors at once and combining them into a single score. These factors are Local consistency and Contrastiveness.\nTo begin the process, we randomly sample four images from the dataset. Among these four images, we select one at random and designate it as the positive image. Subsequently, we calculate the Local consistency score for the positive image.\nLocal consistency - this is a factor unexplored in the existing literature, as the majority of research has primarily concentrated on two desirable aspects of attribution methods: faithfulness and robustness. In our work, however, we measure both of those factors in a novel way.\nIn contrast to previous studies that either relied on the ranking of input features provided by the initial attribution map or used a random subset of features for perturbation, our approach employs a gradient-based method at each step to identify areas that will result in the most significant or least significant change in the output score. Specifically, at each step (denoted as t), we select the top k percent of input features based on the gradient-based approach.\nIn our experiments, we utilize zero masking as the perturbation method, employing the absolute value of the product between the input value and the gradient of our masking loss with respect to the input. The masking loss is simply defined as the absolute value of the target output. This strategy leverages the insight that larger gradient values with respect to the input correspond to areas of the image that have the greatest impact on the model's output. Multiplying these values with the input provides an estimate of the impact of zeroing out those positions on the model's output. This alternative approach addresses a limitation in existing research on evaluation metrics. In previous studies, the attribution map, which is expected to explain relevant features for a given input example, is also employed for an unrelated purpose \u2013 identifying input features that cause the largest change in the model's output. Although these notions may partially overlap for some input features, they are distinct and should be treated as such. By using our proposed method, we can alleviate this issue and ensure a clear distinction between the two objectives.\nWe perform T steps of perturbation by selecting the features with the most significant effect first - Most Relevant First approach (MoRF). Conversely, we also perform an equal number of steps, but this time selecting features with the lowest impact first - Least Relevant First approach (LeRF). As a consequence, two sequences of model output values are generated. The first sequence corresponds to the step-wise perturbation of the most impactful features, and it is anticipated that the output value or at each step will exhibit a rapid decrease from the initial output value $O_{init}$. Conversely, the second sequence represents the step-wise perturbation of the least impactful features, where the output value is expected to decrease at a slower pace."}, {"title": "4 EXPERIMENTS", "content": "In the Experiments section, we employ our novel metric to comprehensively evaluate the performance of our attribution method, alongside state-of-the-art and commonly used methods, across various models and datasets. To provide a comprehensive analysis, we include the results obtained from existing evaluation metrics in the domains of faithfulness, robustness, and localization. Additionally, we conduct an ablation study specifically for Vision Transformers, aiming to elucidate the key advantages of our attribution method compared to existing approaches. Furthermore, we"}, {"title": "4.1 Quantitative experiments", "content": "We evaluate our proposed LRP method on two publicly available image classification datasets ImageNet (Deng et al. [17]) and PascalVOC2012 (Everingham et al. [21]), using the GAE evaluation metric. Step number for the local consistency masking procedure is set to T = 10 in all experiments.\nFor both datasets, we assess various attribution methods across three pre-trained image classification models: VGG16 [29], ResNet50 [23] and ViT-Base [19]. The significance of the difference in the mean values of the total scores was verified using the Wilcoxon signed-rank test with p = 0.05.\nIn the first quantitative experiment, the VGG network, we compare our approach to commonly used, and state-of-the-art explanation approaches for pure convolutional networks: Saliency [45], Input*Gradient [43], Deconvolution [53], LRP-e, DeepLIFT [43], LRP-\u03b11\u03b2\u03bf, LRP-02\u1e9e1, Integrated Gradients [48], SmoothGrad [47], GradCAM [42], HiResCAM [20], LayerCAM [25], GradCAM++ [13], contrastive-LRP (cLRP) [22], GuidedGradCAM [42], Relative Attributing Propagation (RAP) [33] and contrastive-RAP (cRAP) (an augmented version of RAP using the same method of obtaining contrastive maps described in contrastive-LRP paper). We also include two baseline attribution methods: a constant attribution that always produces ones as the attribution map and a random attribution sampled from a normal distribution.\nLastly, we assess the performance of various attribution methods for Vision Transformers, including GradCAM, GradCAM++, HiResCAM, LayerCAM, Last Layer Attention, Rollout, Transformer Interpretability Beyond Attention Visualization (TIBAV), and our proposed method."}, {"title": "4.2 Ablation study", "content": "We examine three ablated variants of our method specifically designed for Vision Transformers:\n1. Patch-level propagation: In this variant, we propagate the attributions solely to the patch-level, disregarding the relevance to individual input pixels.\n2. Values relevance: This variant involves propagating only the values' relevances through the self-attention block, while excluding the relevances of queries and keys. This approach aligns with the implementation of self-attention relevance propagation by Ali et al. [2].\n3. Queries and keys relevance: In this variant, we focus on propagating only the relevance of queries and keys, neglecting the relevance of values. This mirrors the methodology of TIBAV, utilizing only the self-attention scores derived from the multiplication of queries and keys.\nBy exploring these ablated variants, we aim to investigate the impact of different levels of attribution propagation on the performance of our method for Vision Transformers."}, {"title": "4.3 Qualitative experiments", "content": "Lastly, in this section, we present the qualitative results of our methods in various settings. The presented results are representative of the general performance of our methods.\nIn the first qualitative experiment, we simply visualize the attribution maps of several randomly sampled images from the ImageNet and PascalVOC datasets for each of the evaluated models. The"}, {"title": "5 CONCLUSION", "content": "In conclusion, this work makes two significant contributions. Firstly, we introduce a novel Layer-Wise Propagation rule, named Relative Absolute Magnitude Layer-Wise Relevance Propagation (absLRP), that effectively addresses the issue of incorrect relative attribution between neurons within the same layer exhibiting varying absolute magnitude activations. We also apply the new rule without major modifications to three different architectures, including the recent Vision Transformer. Moreover, we propose the potential extension of this approach to diverse data types and tasks, such as text classification, opening up ample opportunities for future research and exploration.\nSecondly, we propose a new evaluation method, Global Attribution Evaluation (GAE), which provides a fresh perspective on assessing the faithfulness, robustness and localization of attribution methods. In contrast to previous studies employing multiple metrics without a clear methodology for combining scores, our metric combines gradient-based masking and localization, providing a comprehensive evaluation using a single score. Through extensive experiments, we evaluate various attribution methods, revealing their individual strengths and weaknesses.\nBy quantitatively evaluating multiple attribution methods on diverse architectures and datasets, we establish the superiority of our approach over state-of-the-art and commonly used methods"}]}