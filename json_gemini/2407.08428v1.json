{"title": "A Comprehensive Survey on Human Video Generation: Challenges, Methods, and Insights", "authors": ["Wentao Lei", "Jinting Wang", "Fengji Ma", "Guanjie Huang", "Li Liu"], "abstract": "Human video generation is a dynamic and rapidly evolving task that aims to synthesize 2D human body video sequences with generative models given control conditions such as text, audio, and pose. With the potential for wide-ranging applications in film, gaming, and virtual communication, the ability to generate natural and realistic human video is critical. Recent advancements in generative models have laid a solid foundation for the growing interest in this area. Despite the significant progress, the task of human video generation remains challenging due to the consistency of characters, the complexity of human motion, and difficulties in their relationship with the environment. This survey provides a comprehensive review of the current state of human video generation, marking, to the best of our knowledge, the first extensive literature review in this domain. We start with an introduction to the fundamentals of human video generation and the evolution of generative models that have facilitated the field's growth. We then examine the main methods employed for three key sub-tasks within human video generation: text-driven, audio-driven, and pose-driven motion generation. These areas are explored concerning the conditions that guide the generation process. Furthermore, we offer a collection of the most commonly utilized datasets and the evaluation metrics that are crucial in assessing the quality and realism of generated videos. The survey concludes with a discussion of the current challenges in the field and suggests possible directions for future research. The goal of this survey is to offer the research community a clear and holistic view of the advancements in human video generation, highlighting the milestones achieved and the challenges that lie ahead.", "sections": [{"title": "I. INTRODUCTION", "content": "HUMAN video generation task aims to synthesize natu-ral and realistic 2D human video sequences with genera-tive models given control conditions such as text [1], [2], audio [3]\u2013[6] and pose [7], [8]. These generated video sequences fea-ture full-body or half-body human figures, including detailed motion representations of body parts and faces. Recently, this field has gained significant attention due to a wide range of potential applications, including film production, video games, AR/VR, human-robot interaction, digital humans, and accessible human-machine interaction.\nRecently, human video generation has achieved rapid progress benefiting from advancements in generation methods, i.e., Variational Autoencoders (VAE) [9], Generative Adversar-ial Networks (GAN) [10], and Diffusion Models [11]. How-ever, studying such a video synthesis problem is known to be challenging for the following reasons. Firstly, the appearance consistency of humans along the time sequence is a significant obstacle in this task. Secondly, the deformation of the human body that people are sensitive to in a synthesized video is hard to avoid, i.e., finger abnormalities, as shown in Fig. 1. Thirdly, the complexity of human motion video extends beyond just modeling the face; it also involves accurately modeling body motion and maintaining background consistency and harmony with body parts. Additionally, the demand for human motion generation often includes a context as the condition, such as text description, audio signals, pose sequences, ensuring temporal alignment with these conditional signals is crucial for producing a coherent and realistic human video.\nIn response to the rapid development and emerging chal-lenges of human video generation, we present a comprehensive survey of this field to help the community keep track of its progress.\nIn summary, the main contributions of this survey are fourfold:\nWe have carefully specified the boundaries of human video generation, offering a comprehensive analysis of recent advancements within this domain. We have catego-rized these advancements into three primary groups based on the modality driving the generation process: text-driven, audio-driven, and pose-driven. To our knowledge, this is the first survey that provides a systematic and focused examination of this particular field.\nWe thoroughly examine the challenges and hurdles in human video generation through massive related methods and an extensive inventory of relevant datasets, chal-lenges, evaluation metrics, and commercial projects. This paper guides readers in selecting suitable baselines or solutions for their unique applications. Additionally, our findings offer valuable insights to enhancing current methodologies.\nDrawing from our detailed literature review and in-depth analysis, we have identified several promising directions for future development in human motion generation.\nWe also provide a continuously updated GitHub repos-itory that includes the latest developments in the field, as well as links to awesome works and datasets. We aim to provide the research community the most cutting-edge information and provide easy access to important research works, datasets, and applications.\nThe survey is organized as follows. In Section II, we discuss the comparison with the previous survey. Section III covers the fundamentals of the task, including the commonly used datasets and the evaluation metrics. In Sections IV-VI, we summarize existing approaches for human motion generation based on different conditional signals, respectively, including text, audio, and pose. Finally, we draw conclusions and provide insights for this field in Section VIII."}, {"title": "II. COMPARISONS WITH PREVIOUS SURVEYS", "content": "To the best of our knowledge, this survey is the first to focus directly on the human video generation task. Although several surveys have been conducted on video or motion generation, the differences between our survey and existing ones are mainly in the following three aspects.\n1) Different Scope. This survey focuses on human video generation, which is a 2D video generation task that uses a generative model to input text, audio, posture, or other modal data and uses full-body or half-body characters, including hands and faces as generated subjects. Compared with the general video generation task that many previous surveys [12]\u2013[15] have focused on, this paper details the unique chal-lenges and developments of human generation. Additionally, surveys [16], [17] concentrated solely on the talking head task, which focuses only on the generation of the head. However, the scope of this survey pays additional attention to hands, thus extending to the generation of half-body and full-body. Furthermore, the work by Zhu et al. [18] explicitly addresses motion generation, emphasizing human poses rather than video generation.\n2) Video Perspective. This paper especially discusses hu-man generation challenges from a video perspective. In con-trast, previous human generation surveys [19], [20] focused on the problems in image generation.\n3) New Insight. To explore and solve the special challenges in human video generation and improve the generation quality, this paper provides a comprehensive analysis of the human video generation task through detailed methods and challenge discussion, as well as summarizes extra relevant datasets, evaluation metrics, and existing commercial projects. Our goal is to offer readers a clear and concise insight into the factors contributing to a successful human video generation and to answer the question, \"What Makes a Good Human Video Generation?\""}, {"title": "III. DATASET AND MATRIX", "content": "Comparing different methods in this field requires appro-priate and comprehensive evaluation metrics. However, evalu-ating generated human videos presents significant challenges due to factors such as the one-to-many mapping nature, the subjectivity inherent in human evaluations, and the complexity of high-level conditional signals. To address these challenges, this section provides an overview of the most commonly used evaluation metrics, highlighting their advantages and limitations.\nWe summarize that the evaluation of generated human videos in this field covers several critical aspects: Image Quality, Video Quality, Consistency, Diversity, Aesthetics, and Action Accuracy. Each of these categories is essential for a comprehensive assessment of the performance of different methods.\nImage Quality focuses on the visual fidelity of individual frames, evaluating pixel-level differences, structural similarity, and perceptual similarity to ensure frames closely match real ones.\nVideo Quality extends this evaluation to the temporal do-main, assessing the coherence and realism of frame sequences to capture the dynamic nature of real-world actions.\nTemporal Consistency is to ensure that the generated content maintains a natural flow and synchronization over time, which is crucial for applications involving synchronized audio and video.\nDiversity is to evaluate the variety and richness of the generated content, ensuring the model can produce a wide range of realistic videos.\nAction Accuracy is to assess the precision of human actions and movements within the videos, which is vital for appli-cations where the correctness of these actions is paramount. Together, these metrics provide a comprehensive framework for evaluating the performance and quality of methods in human video generation."}, {"title": "IV. TEXT TO HUMAN VIDEO GENERATION", "content": "In the following sections IV-VI, we will focus on the meth-ods of human video generation based on different condition signals. Firstly, we will introduce the text-driven human video generation methods.\nText can describe specific appearances, scenes, and styles, providing a rich source of information for generative models to control the generated content. Recent generative methods such as stable diffusion [82] and Sora [14] have shown that using text as input to generate images and videos has achieved impressive results.\nHowever, different from the general video generation tasks which focus on the coherence of the video, human video generation requires precise control over both the appearance and movement of the human body. Existing methods approach this challenge from two main angles: using text to maintain appearance and extracting semantic information from text to control poses. The overview of existing research in text-driven human video generation is shown in Fig. 3."}, {"title": "A. Text-driven Human Appearance Control", "content": "To control the appearance of the human body in the gen-erated video, there are two approaches: one is to directly provide reference images, and the other is to use input text descriptions to control the generated human appearance. Here, we discuss the text-driven human appearance control methods.\nTo ensure the consistency of appearance in generated videos with the textual descriptions while preserving identity details during frames, ID-Animator [1] leverages a pre-trained text-to-video (T2V) model with a lightweight face adapter to encode identity-relevant embeddings. Text descriptions guide the generation of human videos and control the character's appearance in the video. Similarly, [2] uses text descriptions to provide semantic information about the content of the characters, ensuring the generated videos align with the textual descriptions."}, {"title": "B. Text-driven Human Motion Control", "content": "Existing methods for precisely controlling the motion of the human body in generated videos typically follow two approaches:\n1) One approach follows a two-stage pipeline. It first generates corresponding poses based on the semantics of the input text according to the task and then uses these generated poses to guide the motion. More details about the pose-guided generation methods in the second stage can be referred in Section VI. For this type of task, it is necessary to establish a connection between text and poses to control motion in a video. HMTV [83] uses descriptive text to generate initial 3D human motion and control camera angles, ensuring dynamic and realistic video outputs. The text guides the actions and camera movements in the video, providing precise control over the character's movements and the viewer's perspective. For the Sign Language Production task, SignSynth [84] uses a Gloss2Pose network to generate sign language poses and a GAN to create high-quality sign language videos. Similarly, H-DNA [85] translates spoken sentences into sign language videos by first generating sign gesture poses and then using a GAN to produce the final video. In SignLLM [86], text descriptions are converted into gloss (an intermediate sign language representation) and then mapped to poses, which are rendered into Sign Language videos. Here, the semantics of the text are captured to align with the described human pose. In Cued Speech [87], [88] Generation task, [89] first leveraged a Large Language Model (LLM) to convert text into a descriptive gloss and then used the gloss to generate a fine-grained pose.\n2) The other approach directly uses text as a prompt to guide the generation of video actions. For instance, Text2Performer [53] involves the motion text and a motion encoder. motion text describes the movement, such as \"She is swinging to the right.\" The model implicitly models these descriptions by separately representing appearance and motion, thereby generating high-quality videos with consistent appearance and actions."}, {"title": "V. AUDIO TO HUMAN VIDEO GENERATION", "content": "In addition to textual descriptions, human video generation from audio signals has also been explored in this survey. In this section, we mainly discuss two main subtasks: speech-driven human video and music-driven human video. Speech-driven human video generation aims to generate a sequence of human gestures based on input speech audio, which requires the generated human motion to be harmonious with the audio, not only in terms of high-level semantics but also emotion and rhythm. While music-driven human video generation focuses on synthesizing the video of a person dancing or playing a certain instrument guided by a given music clip, which especially lies in the low-level beat alignment. In this scenario, the direct conversion of audio into video poses a complex challenge. Previous research has often followed a two-stage pipeline, including audio-to-motion and motion-to-video, as illustrated in Fig. 4."}, {"title": "A. Speech-driven Human Video Generation", "content": "Many existing works have concentrated on generating talk-ing videos, primarily focusing on the head region [95], [96]. In contrast, our review focuses on works that include body ges-tures [3], [4], [61], [92]\u2013[94]. To the best of our knowledge, all of these works fall under the field of co-speech gesture video generation. Given the importance of motion representation for the final video, we review these works from the perspective of motion generation.\nIn speech-driven human video generation, some methods [61], [92], [93] synthesize talking videos from sequences of 2D skeletons [3], [61] or 3D models [92], [93], with the rendering process being separate from the generation of the gestures. However, hand-crafted structural human priors like 2D/3D skeletons completely discard appearance information around key points, making precise motion control and video rendering highly challenging. Additionally, the pre-training of pose estimators relies on hand-crafted annotations, leading to error accumulation and often resulting in jitters. To alleviate these issues, ANGIE [62] utilizes an unsupervised feature, MRAA [64], to model body motion. A VQ-VAE [97] is then used to quantize common patterns, followed by a GPT-like network that predicts discrete motion patterns to generate gesture videos. However, MRAA, being a coarse modeling of motion, is linear and fails to represent complex-shaped regions, limiting the quality of gesture videos generated by ANGIE. Additionally, directly associating covariance with speech is inappropriate. To address these challenges, DiffTED and He al. propose decoupling motion from gesture videos while preserving critical appearance information of body regions. They use the learned 2D keypoints of the Thin-plate Spline (TPS) motion model [37] as targets for generation and leverage the TPS motion model to render the keypoints into images. Additionally, motivated by the success of recent diffusion models [11], DiffTED and He al. propose a diffusion-based approach to generate diverse gesture sequences."}, {"title": "B. Music-driven Human Video Generation", "content": "Music-driven human video generation uniquely intersects motion synthesis and music interpretation, aiming to create human motions synchronized with input music beat. This extends beyond general motion synthesis, as beat-aligned motions are complex to animate [90]. We have explored two sub-tasks, i.e., music-to-dance and music-to-performance. To achieve beat sensing motion generation, some music-to-dance video generation works [6], [90] explicitly detect beat from music audio, or design a matching phase learns the relationship between these two different modalities [47]. Islam al. [6] perform beat detection and repeated pattern extraction from input music first and then generate mathematical models of a person dancing and convert them into realistic images of the target person. Dabfusion [90] applies a beat extractor to explicitly disentangle beat features from music. These beat features are then used to guide the production of latent optical flows, followed by backward flow estimation to generate the output video. Differently, DanceIt [47] learns the relationship between these two different modalities at the first matching phase, then retrieves a sequence of pose fragments for each music audio and performs spatial-temporal alignment at the generation phase.\nFor music-to-performance video generation, it is challeng-ing to generate high-dimensional temporal consistent videos from low-dimensional audio modality. Zhu al. [5] propose a multi-staged framework that first generates the coarse video from given audio and then makes refinements by integrating intra-frame structure information from predicted keypoints and temporal information for final performance video generation. Music2Play [91] gains a sequence of poses in an auto-regressive way and, estimates the dense flow field information from the pair of poses, finally fuses multi-modal information (audio, flow, and image) to synthesize the output frame."}, {"title": "VI. POSE TO HUMAN VIDEO GENERATION", "content": "As illustrated in Fig. 5, existing research in pose-driven hu-man video generation has often followed a common pipeline. In the task of pose-driven human video generation, various pose types, including skeleton pose, dense pose, depth, mesh, and optical flow (as shown in Tab. IV), serve as common guiding modalities along with the more traditional text and speech inputs. According to the number of conditional poses, we can divide the existing pose guided human video generation methods into two categories. The first category uses only a single type of pose, which is recorded as single-condition pose-guided methods. The second category uses different types of pose signals, which are referred to as multi-condition poses-guided methods."}, {"title": "A. Single-condition Pose-guided Methods", "content": "Among all types of conditional signals, the most common are skeleton pose and dense pose. Early pose-guided human video generation methods [26], [52], [75], [99]\u2013[106] based on GANs primarily utilized conditional adversarial networks such as CGAN [107], pix2pix [108], and pix2pixHD [109]. These methods extracted skeleton poses using OpenPose [65] or StackPose [69] methods, or extracted dense pose using the DensePose method, and used the extracted skeleton pose or dense pose as conditional signal into CGAN or pix2pix generation models.\nWith the development of conditional generation models, current methods [8], [29], [110]\u2013[112] mostly utilize stable diffusion (SD) [113] or Stable Video Diffusion (SVD) [114], [115] as the backbone for video generation models. For in-stance, the MagicPose [29] injects pose features into the diffu-sion model by ControlNet [116]. In contrast to directly utiliz-ing ControlNet, methods such as MotionFollower [110], Mim-icMotion [111], AnimateAnyone [8], and UniAnimate [112] extract skeleton poses from targvideo frames using Dw-Pose [117] or OpenPose [65]. To align the extracted skeleton poses with the noise in the latent space and effectively leverage pose guidance during denoising processing, they design lightweight neural networks (composed of only a few convolutional layers) as pose guider.\nUnlike the above skeleton pose-guided video generation diffusion models, methods like DreamPose [118] and Mag-icAnimate [119] utilize the DensePose [81] method to extract dense pose and directly concatenate dense pose and noise into the denoising UNet by ControlNet. Different from these types of 2D poses (skeleton pose and dense pose), Human4DiT [120] extracts corresponding 3D mesh maps using SMPL [121]. Inspired by the work of Sora and other variants [14], [122], Human4DiT [120] regards Diffusion Transformer as the back-bone for video generation."}, {"title": "B. Multi-condition Poses-guided Methods", "content": "In addition to the single conditional pose-based human video generation, the recent success of SD [113] and SVD [114], [115] has laid the foundation for multi-conditional pose-guided human video generation. Most existing pose-guided methods use either skeleton pose or dense pose as the conditional input. However, these single-condition pose-guided methods often exhibit poor generalization to complex backgrounds and suffer from occlusion issues between differ-ent bodies and parts of the same individual.\nTo address the poor generalization considering the com-plex backgrounds, DISCO [48] presents an innovative model architecture featuring disentangled control over background and skeleton pose, thereby improving the compositionality of dance generation. This architecture enables the integration of both seen and novel subjects, backgrounds, and poses from di-verse sources. Follow-Your-Pose v2 [138] integrates an optical flow guide with other condition guiders to enhance background stability. Liu et al. [125] separates the motion representations of the foreground and background, animating human figures with pose-based motion while modeling background motion using sparse tracking points to capture natural interactions between the figure's activity and environmental changes.\nTo tackle the occlusion issues, Follow-Your-Pose v2 [138] addresses occlusions in multi-character animation with a depth guider, and improves character appearance learning with a reference pose guider. VividPose [98] introduces depth and mesh information, particularly in conjunction with the SMPL-X [137] model, which helps the system to better handle occlu-sions and complex movements that are common in human pose sequences. DreaMoving [131] integrates depth information and skeleton pose, helping the model to understand the spatial relationships between different parts of the body and the environment. The depth information is useful for handling occlusions as it allows the model to determine which body parts are in front of or behind others."}, {"title": "VII. CHALLENGES", "content": "In this section, we summarize the key challenges in the human video generation task, discuss the special challenges existing in the models guided by the particular modality, and explain the common problems faced by this task and related video generation tasks. Representative challenges include:\n1) Occlusion Issue. In the collected videos, overlapping body parts or multiple people occlusion is common, but most models cannot handle the problem of mutual influence well [98], [138].\n2) Body Deformation. Ensuring that generated video fea-tures such as body shape, face, and hands adhere to typical human characteristics is a significant obstacle in this task. One common example of this issue is the occurrence of malformed hands [139].\n3) Appearance Inconsistency. The generation of human videos also requires that the various features of the human appearance, including face, body, clothing, accessories, etc., be consistent in the generated videos. However, most models cannot achieve utterly satisfactory consistency.\n4) Background Influence. When generating videos with the human body in the foreground, the consistency of the background and the harmony with the foreground human body is also a major challenge. Poor background control will affect the quality of human generation and bring additional jitter and distortion.\n5) Temporal Inalignment. In models guided by tempo-ral signals, especially the audio-to-human video generation models, the synchronization of lips and voice is a significant challenge to improving the quality.\n6) Unnatural Pose. Current generated human video often suffers from the unnatural pose problem. The specific mani-festations of this problem include the inconsistency between the generated video and the inputted guided pose, as well as the naturalness of the movements in the generated videos.\nIn addition to the representative challenges mentioned above, in text- or audio-driven models, due to the one-to-many mapping nature in the dataset, meaning that a single input text or audio can correspond to several valid outputs. As a result, attempting to directly match the input with a single 'correct' gesture can lead to an unreliable and biased association. This approach hinders the model's ability to capture and learn the variations present within the data [3].\nIt should be noted that since human video generation is essentially a branch of video generation, the efficiency challenges brought by the common use of diffusion models, the challenges of multi-view generation, and the challenges of high-resolution generation still have a significant impact on the generation quality."}, {"title": "VIII. CONCLUSION AND DISCUSSION", "content": "In this survey, we provide a comprehensive overview of recent advancements in human video generation. Despite the rapid progress in this field, significant challenges remain that warrant further exploration. We summarize available dataset resources and commonly used evaluation metrics. Subsequently, we classify the existing researches based on conditional signals (i.e. text, audio and pose) and discuss each category in detail."}, {"title": "B. Discussion", "content": "In this section, we aim to discuss in detail the factors influencing the quality of human video generation, excluding dataset scale. To this end, we will focus on three aspects: generation paradigm, backbone, and condition pose.\n\u2022 Generation Paradigm. Compared to pose-driven meth-ods (which can be regarded as one-stage methods), text and audio-driven methods can be divided into one-stage and two-stage approaches. The former directly uses input text or audio as prompts to guide human video generation, while the latter generates poses from the input text or audio and then uses these generated poses as signals to guide human video generation. The introduction of various pose types, such as skeleton poses, in two-stage methods, provides additional geometric and semantic information, enhancing the accuracy and realism of video motions. This makes two-stage methods significantly more effective than one-stage methods, albeit at the cost of some efficiency.\n\u2022 Backbone. Diffusion models, such as SD and SVD, are widely used in various generative tasks, including human video generation, due to their superior performance and diversity. However, unlike GANs, which generate sam-ples in a single sampling step, diffusion models require multiple sampling steps, thereby increasing the time cost for training and inference.\n\u2022 Condition Pose. Different types of conditional poses work because they provide complementary information. The most common skeleton pose accurately describes the spatial information of the human body in the frame and the relative positions of body parts. However, it captures discrete pose changes rather than continuous motion details, providing limited temporal coherence. In contrast, optical flow inherently includes temporal infor-mation, capturing changes between consecutive frames and providing continuous motion trajectories in the fea-ture space. This allows the model to generate videos with smooth transitions between frames, avoiding jumps or discontinuities. Moreover, the skeleton pose does not include background and detail modeling, whereas depth maps capture distance information between human body and the background, along with surface details and depth changes. 3D meshes offer detailed geometric structures of object surfaces that skeleton poses lack. In sum-mary, different types of poses provide complementary spatiotemporal information, and there is no unified pose type that fulfills all requirements. Different scenarios and problems may require different poses."}, {"title": "C. Future Work", "content": "We outline several promising future directions from various perspectives, aiming to inspire new breakthroughs in human video generation research.\n\u2022 Large-Scale High-Quality Human Video Datasets. Ex-isting public datasets, including those in the fields of human action and human dance, are relatively small in scale. Collecting high-quality human video datasets is both challenging and expensive. However, a large-scale, high-quality human video dataset is crucial for developing a foundational model for human video generation.\n\u2022 Long Video Generation. Current human video genera-tion methods typically produce videos lasting only several seconds. Generating videos that extend to several minutes or even hours presents a significant challenge. Therefore, future research should focus on the generation of long-duration human videos.\n\u2022 Photorealistic Video Generation. As previously men-tioned, challenges such as occlusion, body deformation, pose unnaturalness, and appearance inconsistency can result in low-quality video generation. Resolving these visual and aesthetic issues to ensure that the generated human body movements follow real-world physical laws is a major challenge. Creating videos with highly realistic visual effects remains a difficult task.\n\u2022 Human Video Diffusion Efficiency. Diffusion models have become the backbone for human video generation tasks. However, the heavy training costs and deployment requirements of video diffusion models pose significant challenges. Reducing training costs and scaling down model size are crucial issues. Therefore, exploring the efficiency of video diffusion models is a valuable direc-tion for future research.\n\u2022 Fine-Grained Controllability. Existing multimodal-driven human video generation methods, even when incorporating additional, conditional signals such as 3D mesh and depth map alongside skeleton pose, still lack fine-grained control over specific body parts, particularly hands, and face. Future research could focus on achieving fine-grained, controllable generation of these detailed human body regions.\n\u2022 Interactivity. In addition to exploring fine-grained con-trollability, future work could further investigate interac-tive controllability. This would allow users to manipulate elements such as arm movements or facial expressions through simple actions like clicking, ultimately generat-ing human videos that meet user satisfaction."}]}