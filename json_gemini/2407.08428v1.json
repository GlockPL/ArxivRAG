{"title": "A Comprehensive Survey on Human Video\nGeneration: Challenges, Methods, and Insights", "authors": ["Wentao Lei", "Jinting Wang", "Fengji Ma", "Guanjie Huang", "Li Liu"], "abstract": "Human video generation is a dynamic and rapidly\nevolving task that aims to synthesize 2D human body video\nsequences with generative models given control conditions such\nas text, audio, and pose. With the potential for wide-ranging\napplications in film, gaming, and virtual communication, the\nability to generate natural and realistic human video is critical.\nRecent advancements in generative models have laid a solid\nfoundation for the growing interest in this area. Despite the\nsignificant progress, the task of human video generation remains\nchallenging due to the consistency of characters, the complexity\nof human motion, and difficulties in their relationship with the\nenvironment. This survey provides a comprehensive review of the\ncurrent state of human video generation, marking, to the best of\nour knowledge, the first extensive literature review in this domain.\nWe start with an introduction to the fundamentals of human\nvideo generation and the evolution of generative models that have\nfacilitated the field's growth. We then examine the main methods\nemployed for three key sub-tasks within human video generation:\ntext-driven, audio-driven, and pose-driven motion generation.\nThese areas are explored concerning the conditions that guide the\ngeneration process. Furthermore, we offer a collection of the most\ncommonly utilized datasets and the evaluation metrics that are\ncrucial in assessing the quality and realism of generated videos.\nThe survey concludes with a discussion of the current challenges\nin the field and suggests possible directions for future research.\nThe goal of this survey is to offer the research community a clear\nand holistic view of the advancements in human video generation,\nhighlighting the milestones achieved and the challenges that lie\nahead.", "sections": [{"title": "I. INTRODUCTION", "content": "HUMAN video generation task aims to synthesize natu-\nral and realistic 2D human video sequences with genera-\ntive models given control conditions such as text [1], [2], audio\n[3]\u2013[6] and pose [7], [8]. These generated video sequences fea-\nture full-body or half-body human figures, including detailed\nmotion representations of body parts and faces. Recently, this\nfield has gained significant attention due to a wide range\nof potential applications, including film production, video\ngames, AR/VR, human-robot interaction, digital humans, and\naccessible human-machine interaction.\nRecently, human video generation has achieved rapid\nprogress benefiting from advancements in generation methods,\ni.e., Variational Autoencoders (VAE) [9], Generative Adversar-\nial Networks (GAN) [10], and Diffusion Models [11]. How-\never, studying such a video synthesis problem is known to be\nchallenging for the following reasons. Firstly, the appearance\nconsistency of humans along the time sequence is a significant\nobstacle in this task. Secondly, the deformation of the human\nbody that people are sensitive to in a synthesized video is hard\nto avoid, i.e., finger abnormalities, as shown in Fig. 1. Thirdly,\nthe complexity of human motion video extends beyond just\nmodeling the face; it also involves accurately modeling body\nmotion and maintaining background consistency and harmony\nwith body parts. Additionally, the demand for human motion\ngeneration often includes a context as the condition, such\nas text description, audio signals, pose sequences, ensuring\ntemporal alignment with these conditional signals is crucial\nfor producing a coherent and realistic human video.\nIn response to the rapid development and emerging chal-\nlenges of human video generation, we present a comprehensive\nsurvey of this field to help the community keep track of its\nprogress.\nIn summary, the main contributions of this survey are\nfourfold:\n\u2022 We have carefully specified the boundaries of human\nvideo generation, offering a comprehensive analysis of\nrecent advancements within this domain. We have catego-\nrized these advancements into three primary groups based\non the modality driving the generation process: text-\ndriven, audio-driven, and pose-driven. To our knowledge,\nthis is the first survey that provides a systematic and\nfocused examination of this particular field.\n\u2022 We thoroughly examine the challenges and hurdles in\nhuman video generation through massive related methods\nand an extensive inventory of relevant datasets, chal-\nlenges, evaluation metrics, and commercial projects. This\npaper guides readers in selecting suitable baselines or\nsolutions for their unique applications. Additionally, our\nfindings offer valuable insights into enhancing current\nmethodologies.\n\u2022 Drawing from our detailed literature review and in-depth\nanalysis, we have identified several promising directions\nfor future development in human motion generation.\n\u2022 We also provide a continuously updated GitHub repos-\nitory that includes the latest developments in the field,\nas well as links to awesome works and datasets. We aim\nto provide the research community the most cutting-edge\ninformation and provide easy access to important research\nworks, datasets, and applications. For more details, please\nvisit our repository link\nThe survey is organized as follows. In Section II, we discuss\nthe comparison with the previous survey. Section III covers"}, {"title": "II. COMPARISONS WITH PREVIOUS SURVEYS", "content": "To the best of our knowledge, this survey is the first to focus\ndirectly on the human video generation task. Although several\nsurveys have been conducted on video or motion generation,\nthe differences between our survey and existing ones are\nmainly in the following three aspects.\n1) Different Scope. This survey focuses on human video\ngeneration, which is a 2D video generation task that uses a\ngenerative model to input text, audio, posture, or other modal\ndata and uses full-body or half-body characters, including\nhands and faces as generated subjects. Compared with the\ngeneral video generation task that many previous surveys [12]\u2013\n[15] have focused on, this paper details the unique chal-\nlenges and developments of human generation. Additionally,\nsurveys [16], [17] concentrated solely on the talking head task,\nwhich focuses only on the generation of the head. However,\nthe scope of this survey pays additional attention to hands,\nthus extending to the generation of half-body and full-body.\nFurthermore, the work by Zhu et al. [18] explicitly addresses\nmotion generation, emphasizing human poses rather than video\ngeneration.\n2) Video Perspective. This paper especially discusses hu-\nman generation challenges from a video perspective. In con-\ntrast, previous human generation surveys [19], [20] focused\non the problems in image generation.\n3) New Insight. To explore and solve the special challenges\nin human video generation and improve the generation quality,\nthis paper provides a comprehensive analysis of the human\nvideo generation task through detailed methods and challenge\ndiscussion, as well as summarizes extra relevant datasets,\nevaluation metrics, and existing commercial projects. Our goal\nis to offer readers a clear and concise insight into the factors\ncontributing to a successful human video generation and to\nanswer the question, \"What Makes a Good Human Video\nGeneration?\""}, {"title": "III. DATASET AND MATRIX", "content": "Comparing different methods in this field requires appro-\npriate and comprehensive evaluation metrics. However, evalu-\nating generated human videos presents significant challenges\ndue to factors such as the one-to-many mapping nature, the\nsubjectivity inherent in human evaluations, and the complexity\nof high-level conditional signals. To address these challenges,\nthis section provides an overview of the most commonly\nused evaluation metrics, highlighting their advantages and\nlimitations. \nWe summarize that the evaluation of generated human\nvideos in this field covers several critical aspects: Image\nQuality, Video Quality, Consistency, Diversity, Aesthetics, and"}, {"title": "IV. TEXT TO HUMAN VIDEO GENERATION", "content": "In the following sections IV-VI, we will focus on the meth-\nods of human video generation based on different condition\nsignals. Firstly, we will introduce the text-driven human video\ngeneration methods.\nText can describe specific appearances, scenes, and styles,\nproviding a rich source of information for generative models\nto control the generated content. Recent generative methods\nsuch as stable diffusion [82] and Sora [14] have shown that\nusing text as input to generate images and videos has achieved\nimpressive results.\nHowever, different from the general video generation tasks\nwhich focus on the coherence of the video, human video\ngeneration requires precise control over both the appearance\nand movement of the human body. Existing methods approach\nthis challenge from two main angles: using text to maintain\nappearance and extracting semantic information from text to\ncontrol poses. The overview of existing research in text-driven\nhuman video generation is shown in Fig. 3.\nTo control the appearance of the human body in the gen-\nerated video, there are two approaches: one is to directly\nprovide reference images, and the other is to use input text\ndescriptions to control the generated human appearance. Here,\nwe discuss the text-driven human appearance control methods.\nTo ensure the consistency of appearance in generated videos\nwith the textual descriptions while preserving identity details\nduring frames, ID-Animator [1] leverages a pre-trained text-\nto-video (T2V) model with a lightweight face adapter to\nencode identity-relevant embeddings. Text descriptions guide\nthe generation of human videos and control the character's\nappearance in the video. Similarly, [2] uses text descriptions\nto provide semantic information about the content of the\ncharacters, ensuring the generated videos align with the textual\ndescriptions.\nExisting methods for precisely controlling the motion of\nthe human body in generated videos typically follow two\napproaches:\n1) One approach follows a two-stage pipeline. It first\ngenerates corresponding poses based on the semantics of the\ninput text according to the task and then uses these generated\nposes to guide the motion. More details about the pose-guided\ngeneration methods in the second stage can be referred in\nSection VI. For this type of task, it is necessary to establish\na connection between text and poses to control motion in a\nvideo. HMTV [83] uses descriptive text to generate initial 3D\nhuman motion and control camera angles, ensuring dynamic\nand realistic video outputs. The text guides the actions and\ncamera movements in the video, providing precise control over\nthe character's movements and the viewer's perspective. For\nthe Sign Language Production task, SignSynth [84] uses a\nGloss2Pose network to generate sign language poses and a\nGAN to create high-quality sign language videos. Similarly,\nH-DNA [85] translates spoken sentences into sign language\nvideos by first generating sign gesture poses and then using\na GAN to produce the final video. In SignLLM [86], text\ndescriptions are converted into gloss (an intermediate sign\nlanguage representation) and then mapped to poses, which\nare rendered into Sign Language videos. Here, the semantics\nof the text are captured to align with the described human\npose. In Cued Speech [87], [88] Generation task, [89] first\nleveraged a Large Language Model (LLM) to convert text\ninto a descriptive gloss and then used the gloss to generate\na fine-grained pose.\n2) The other approach directly uses text as a prompt to guide\nthe generation of video actions. For instance, Text2Performer\n[53] involves the motion text and a motion encoder. motion\ntext describes the movement, such as \"She is swinging to\nthe right.\" The model implicitly models these descriptions\nby separately representing appearance and motion, thereby\ngenerating high-quality videos with consistent appearance and\nactions."}, {"title": "V. AUDIO TO HUMAN VIDEO GENERATION", "content": "In addition to textual descriptions, human video generation\nfrom audio signals has also been explored in this survey. In this\nsection, we mainly discuss two main subtasks: speech-driven\nhuman video and music-driven human video. Speech-driven\nhuman video generation aims to generate a sequence of human\ngestures based on input speech audio, which requires the\ngenerated human motion to be harmonious with the audio,\nnot only in terms of high-level semantics but also emotion and\nrhythm. While music-driven human video generation focuses\non synthesizing the video of a person dancing or playing\na certain instrument guided by a given music clip, which\nespecially lies in the low-level beat alignment. In this scenario,\nthe direct conversion of audio into video poses a complex\nchallenge. Previous research has often followed a two-stage\npipeline, including audio-to-motion and motion-to-video, as\nillustrated in Fig. 4.\nMany existing works have concentrated on generating talk-\ning videos, primarily focusing on the head region [95], [96]. In\ncontrast, our review focuses on works that include body ges-\ntures [3], [4], [61], [92]\u2013[94]. To the best of our knowledge, all\nof these works fall under the field of co-speech gesture video\ngeneration. Given the importance of motion representation for\nthe final video, we review these works from the perspective\nof motion generation.\nIn speech-driven human video generation, some methods\n[61], [92], [93] synthesize talking videos from sequences of\n2D skeletons [3], [61] or 3D models [92], [93], with the\nrendering process being separate from the generation of the\ngestures. However, hand-crafted structural human priors like\n2D/3D skeletons completely discard appearance information\naround key points, making precise motion control and video\nrendering highly challenging. Additionally, the pre-training of\npose estimators relies on hand-crafted annotations, leading to\nerror accumulation and often resulting in jitters. To alleviate\nthese issues, ANGIE [62] utilizes an unsupervised feature,\nMRAA [64], to model body motion. A VQ-VAE [97] is then\nused to quantize common patterns, followed by a GPT-like\nnetwork that predicts discrete motion patterns to generate\ngesture videos. However, MRAA, being a coarse modeling of\nmotion, is linear and fails to represent complex-shaped regions,\nlimiting the quality of gesture videos generated by ANGIE.\nAdditionally, directly associating covariance with speech is\ninappropriate. To address these challenges, DiffTED and He\nal. propose decoupling motion from gesture videos while\npreserving critical appearance information of body regions.\nThey use the learned 2D keypoints of the Thin-plate Spline\n(TPS) motion model [37] as targets for generation and leverage\nthe TPS motion model to render the keypoints into images.\nAdditionally, motivated by the success of recent diffusion\nmodels [11], DiffTED and He al. propose a diffusion-based\napproach to generate diverse gesture sequences.\nMusic-driven human video generation uniquely intersects\nmotion synthesis and music interpretation, aiming to create\nhuman motions synchronized with input music beat. This\nextends beyond general motion synthesis, as beat-aligned\nmotions are complex to animate [90]. We have explored two\nsub-tasks, i.e., music-to-dance and music-to-performance."}, {"title": "VI. POSE TO HUMAN VIDEO GENERATION", "content": "As illustrated in Fig. 5, existing research in pose-driven hu-\nman video generation has often followed a common pipeline.\nIn the task of pose-driven human video generation, various\npose types, including skeleton pose, dense pose, depth, mesh,\nand optical flow (as shown in Tab. IV), serve as common\nguiding modalities along with the more traditional text and\nspeech inputs. According to the number of conditional poses,\nwe can divide the existing pose guided human video generation\nmethods into two categories. The first category uses only a\nsingle type of pose, which is recorded as single-condition\npose-guided methods. The second category uses different\ntypes of pose signals, which are referred to as multi-condition\nposes-guided methods.\nAmong all types of conditional signals, the most common\nare skeleton pose and dense pose. Early pose-guided human\nvideo generation methods [26], [52], [75], [99]\u2013[106] based\non GANs primarily utilized conditional adversarial networks\nsuch as CGAN [107], pix2pix [108], and pix2pixHD [109].\nThese methods extracted skeleton poses using OpenPose [65]\nor StackPose [69] methods, or extracted dense pose using\nthe DensePose method, and used the extracted skeleton pose\nor dense pose as conditional signal into CGAN or pix2pix\ngeneration models.\nWith the development of conditional generation models,\ncurrent methods [8], [29], [110]\u2013[112] mostly utilize stable\ndiffusion (SD) [113] or Stable Video Diffusion (SVD) [114],\n[115] as the backbone for video generation models. For in-\nstance, the MagicPose [29] injects pose features into the diffu-\nsion model by ControlNet [116]. In contrast to directly utiliz-\ning ControlNet, methods such as MotionFollower [110], Mim-\nicMotion [111], AnimateAnyone [8], and UniAnimate [112]\nextract skeleton poses from targvideo frames using Dw-\nPose [117] or OpenPose [65]. To align the extracted skeleton\nposes with the noise in the latent space and effectively\nleverage pose guidance during denoising processing, they\ndesign lightweight neural networks (composed of only a few\nconvolutional layers) as pose guider.\nUnlike the above skeleton pose-guided video generation\ndiffusion models, methods like DreamPose [118] and Mag-\nicAnimate [119] utilize the DensePose [81] method to extract\ndense pose and directly concatenate dense pose and noise into\nthe denoising UNet by ControlNet. Different from these types\nof 2D poses (skeleton pose and dense pose), Human4DiT [120]\nextracts corresponding 3D mesh maps using SMPL [121].\nInspired by the work of Sora and other variants [14], [122],\nHuman4DiT [120] regards Diffusion Transformer as the back-\nbone for video generation.\nIn addition to the single conditional pose-based human\nvideo generation, the recent success of SD [113] and\nSVD [114], [115] has laid the foundation for multi-conditional\npose-guided human video generation. Most existing pose-\nguided methods use either skeleton pose or dense pose as\nthe conditional input. However, these single-condition pose-\nguided methods often exhibit poor generalization to complex\nbackgrounds and suffer from occlusion issues between differ-\nent bodies and parts of the same individual.\nTo address the poor generalization considering the com-\nplex backgrounds, DISCO [48] presents an innovative model\narchitecture featuring disentangled control over background\nand skeleton pose, thereby improving the compositionality of\ndance generation. This architecture enables the integration of\nboth seen and novel subjects, backgrounds, and poses from di-\nverse sources. Follow-Your-Pose v2 [138] integrates an optical\nflow guide with other condition guiders to enhance background"}, {"title": "VII. CHALLENGES", "content": "In this section, we summarize the key challenges in the\nhuman video generation task, discuss the special challenges\nexisting in the models guided by the particular modality, and\nexplain the common problems faced by this task and related\nvideo generation tasks. Representative challenges include:\n1) Occlusion Issue. In the collected videos, overlapping\nbody parts or multiple people occlusion is common, but\nmost models cannot handle the problem of mutual influence\nwell [98], [138].\n2) Body Deformation. Ensuring that generated video fea-\ntures such as body shape, face, and hands adhere to typical\nhuman characteristics is a significant obstacle in this task. One\ncommon example of this issue is the occurrence of malformed\nhands [139].\n3) Appearance Inconsistency. The generation of human\nvideos also requires that the various features of the human\nappearance, including face, body, clothing, accessories, etc.,\nbe consistent in the generated videos. However, most models\ncannot achieve utterly satisfactory consistency.\n4) Background Influence. When generating videos with\nthe human body in the foreground, the consistency of the\nbackground and the harmony with the foreground human body\nis also a major challenge. Poor background control will affect\nthe quality of human generation and bring additional jitter and\ndistortion.\n5) Temporal Inalignment. In models guided by tempo-\nral signals, especially the audio-to-human video generation\nmodels, the synchronization of lips and voice is a significant\nchallenge to improving the quality.\n6) Unnatural Pose. Current generated human video often\nsuffers from the unnatural pose problem. The specific mani-\nfestations of this problem include the inconsistency between\nthe generated video and the inputted guided pose, as well as\nthe naturalness of the movements in the generated videos.\nIn addition to the representative challenges mentioned\nabove, in text- or audio-driven models, due to the one-to-many\nmapping nature in the dataset, meaning that a single input text\nor audio can correspond to several valid outputs. As a result,\nattempting to directly match the input with a single 'correct'\ngesture can lead to an unreliable and biased association. This\napproach hinders the model's ability to capture and learn the\nvariations present within the data [3].\nIt should be noted that since human video generation"}, {"title": "VIII. CONCLUSION AND DISCUSSION", "content": "In this survey, we provide a comprehensive overview of\nrecent advancements in human video generation. Despite the\nrapid progress in this field, significant challenges remain\nthat warrant further exploration. We summarize available\ndataset resources and commonly used evaluation metrics.\nSubsequently, we classify the existing researches based on\nconditional signals (i.e. text, audio and pose) and discuss each\ncategory in detail.\nIn this section, we aim to discuss in detail the factors\ninfluencing the quality of human video generation, excluding\ndataset scale. To this end, we will focus on three aspects:\ngeneration paradigm, backbone, and condition pose.\n\u2022 Generation Paradigm. Compared to pose-driven meth-\nods (which can be regarded as one-stage methods), text\nand audio-driven methods can be divided into one-stage\nand two-stage approaches. The former directly uses input\ntext or audio as prompts to guide human video generation,\nwhile the latter generates poses from the input text or\naudio and then uses these generated poses as signals\nto guide human video generation. The introduction of\nvarious pose types, such as skeleton poses, in two-stage\nmethods, provides additional geometric and semantic\ninformation, enhancing the accuracy and realism of video\nmotions. This makes two-stage methods significantly\nmore effective than one-stage methods, albeit at the cost\nof some efficiency.\n\u2022 Backbone. Diffusion models, such as SD and SVD, are\nwidely used in various generative tasks, including human\nvideo generation, due to their superior performance and\ndiversity. However, unlike GANs, which generate sam-\nples in a single sampling step, diffusion models require\nmultiple sampling steps, thereby increasing the time cost\nfor training and inference.\n\u2022 Condition Pose. Different types of conditional poses\nwork because they provide complementary information.\nThe most common skeleton pose accurately describes\nthe spatial information of the human body in the frame\nand the relative positions of body parts. However, it\ncaptures discrete pose changes rather than continuous\nmotion details, providing limited temporal coherence. In\ncontrast, optical flow inherently includes temporal infor-\nmation, capturing changes between consecutive frames\nand providing continuous motion trajectories in the fea-\nture space. This allows the model to generate videos\nwith smooth transitions between frames, avoiding jumps\nor discontinuities. Moreover, the skeleton pose does not\ninclude background and detail modeling, whereas depth\nmaps capture distance information between human body\nand the background, along with surface details and depth\nchanges. 3D meshes offer detailed geometric structures\nof object surfaces that skeleton poses lack. In sum-\nmary, different types of poses provide complementary\nspatiotemporal information, and there is no unified pose\ntype that fulfills all requirements. Different scenarios and\nproblems may require different poses.\nWe outline several promising future directions from various\nperspectives, aiming to inspire new breakthroughs in human\nvideo generation research.\n\u2022 Large-Scale High-Quality Human Video Datasets. Ex-\nisting public datasets, including those in the fields of\nhuman action and human dance, are relatively small in\nscale. Collecting high-quality human video datasets is\nboth challenging and expensive. However, a large-scale,\nhigh-quality human video dataset is crucial for developing\na foundational model for human video generation.\n\u2022 Long Video Generation. Current human video genera-\ntion methods typically produce videos lasting only several\nseconds. Generating videos that extend to several minutes\nor even hours presents a significant challenge. Therefore,\nfuture research should focus on the generation of long-\nduration human videos.\n\u2022 Photorealistic Video Generation. As previously men-\ntioned, challenges such as occlusion, body deformation,\npose unnaturalness, and appearance inconsistency can\nresult in low-quality video generation. Resolving these\nvisual and aesthetic issues to ensure that the generated\nhuman body movements follow real-world physical laws\nis a major challenge. Creating videos with highly realistic\nvisual effects remains a difficult task.\n\u2022 Human Video Diffusion Efficiency. Diffusion models\nhave become the backbone for human video generation\ntasks. However, the heavy training costs and deployment\nrequirements of video diffusion models pose significant\nchallenges. Reducing training costs and scaling down\nmodel size are crucial issues. Therefore, exploring the\nefficiency of video diffusion models is a valuable direc-\ntion for future research.\n\u2022 Fine-Grained Controllability. Existing multimodal-\ndriven human video generation methods, even when\nincorporating additional, conditional signals such as 3D\nmesh and depth map alongside skeleton pose, still lack\nfine-grained control over specific body parts, particularly\nhands, and face. Future research could focus on achieving\nfine-grained, controllable generation of these detailed\nhuman body regions.\n\u2022 Interactivity. In addition to exploring fine-grained con-\ntrollability, future work could further investigate interac-\ntive controllability. This would allow users to manipulate\nelements such as arm movements or facial expressions\nthrough simple actions like clicking, ultimately generat-\ning human videos that meet user satisfaction."}]}