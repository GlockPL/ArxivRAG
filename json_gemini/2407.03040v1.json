{"title": "Raw Text is All you Need: Knowledge-intensive Multi-turn Instruction Tuning for Large Language Model", "authors": ["Xia HOU", "Qifeng Li", "Jian Yang", "Tongliang Li", "Linzheng Chai", "Xianjie Wu", "Hangyuan Ji", "Zhoujun Li", "Jixuan Nie", "Jingbo Dun", "Wenfeng Song"], "abstract": "Instruction tuning as an effective technique aligns the outputs of large language models (LLMs) with human preference. But how to generate the seasonal multi-turn dialogues from raw documents for instruction tuning still requires further exploration. In this paper, we present a novel framework named R2S that leverages the CoD-Chain of Dialogue logic to guide large language models (LLMs) in generating knowledge-intensive multi-turn dialogues for instruction tuning. By integrating raw documents from both open-source datasets and domain-specific web-crawled documents into a benchmark K-BENCH, we cover diverse areas such as Wikipedia (English), Science (Chinese), and Artifacts (Chinese). Our approach first decides the logic flow of the current dialogue and then prompts LLMs to produce key phrases for sourcing relevant response content. This methodology enables the creation of the GINSTRUCT instruction dataset, retaining raw document knowledge within dialogue-style interactions. Utilizing this dataset, we fine-tune GLLM, a model designed to transform raw documents into structured multi-turn dialogues, thereby injecting comprehensive domain knowledge into the SFT model for enhanced instruction tuning. This work signifies a stride towards refining the adaptability and effectiveness of LLMs in processing and generating more accurate, contextually nuanced responses across various fields.", "sections": [{"title": "1 Introduction", "content": "The evolution of large language models (LLMs), including advancements seen in the generative pre-trained Transformer (GPT) series by OpenAI, marks a significant milestone in the field of natural language processing (NLP). A pivotal strategy in their development has been instruction-tuning, which leverages human-curated prompts, feedback, and benchmark datasets to tailor LLMs' adaptability to specific domains, such as complex reasoning (Wei et al., 2022; Chai et al., 2024; Li et al., 2024) and coding (Rozi\u00e8re et al., 2023; Li et al., 2023a). Instruction tuning (Wang et al., 2023) emerges as an innovative approach, creating new tasks with bespoke instructions, thus enhancing model performance and cost-effectiveness. The diversity and scope of instruction data are critical for the model's ability to generalize and excel in previously unseen tasks, underpinning the continuous advancement and specialization of LLMs in various fields.\nInstruction tuning is a groundbreaking technique designed to fine-tune these powerful LLMs for better task-specific performance without intensive re-training on massive datasets, which delicately re-calibrates the response of LLMs by giving them instructions or prompts that are carefully crafted to elicit more accurate, contextually appropriate, or nuanced outputs. Many recent works (Sun et al., 2023; Luo et al., 2023) try to synthesize instructions, input, and output samples from an LLM and then filter invalid or similar ones. However, these methods focus on generating diverse single-turn dialogues based on the query or response. In Figure 1, continuing pre-training with raw documents and instruction tuning with human-annotated SFT data can inject domain-specific knowledge, but the process requires two-stage training and large-scale data. Therefore, How to produce reasonable multi-turn dialogues for instruction turning to inject the knowledge of raw documents into LLMs only with instruction tuning.\nIn this paper, we propose a framework to construct the Supervised fine-tuning dataset from the Raw documents (R2S), which leverages the Chain of Dialogue logic (CoD) to guide the LLM to create the reasonable knowledge-intensive multi-turn dialogues for instruction tuning. Specifically, we collect the existing documents from open-source dataset (Katyayan and Joshi, 2022; Yang et al., 2023) and crawl the domain-specific documents from websites (in the field of the artifacts) to create a knowledge-intensive benchmark K-BENCH, comprised of three tasks, including Wikipedia (English), Science (Chinese), and Artifacts (Chinese). Then, we introduce the chain of dialogue logic to first decide the type (e.g. Opinion Exchange Q&A, Informational Q&A, or Task-oriented Q&A) of the current turn in multi-turn dialogue. subsequently, we prompt the LLM to generate key phrases to search relevant spans for response generation. In this way, we successfully create the instruction dataset GINSTRUCT, which can keep the knowledge of the raw documents as much as possible in the style of the dialogue. Based on the raw documents, GINSTRUCT, we can fine-tune GLLM based on open-source LLMs, which aims at converting raw documents into multi-turn dialogues. Finally, we can use GLLM to generate the multi-turn SFT data to inject the knowledge into the SFT model.\nExtensive experiments of R2S are evaluated on our created benchmark K-BENCH. The results demonstrate that the synthetic instruction corpora can effectively inject the knowledge of raw documents into the SFT model, notably getting excellent performance under multiple evaluation metrics. The fine-tuned generator GLLM further verifies the effectiveness of CoD, leading to a reasonable and natural multi-turn dialogue. The contributions in this work are summarized as follows:\n\u2022 We propose the chain of dialogue logic (CoD), which ingeniously guides LLMs to produce reasonable and natural knowledge-intensive multi-turn dialogues for instruction tuning. It allows the LLMs to generate dialogues that are coherent and contextually relevant but also embed rich, domain-specific knowledge into these conversations.\n\u2022 We create a knowledge-intensive benchmark (K-BENCH) to facilitate the training and evaluation of the proposed methods, this work contributes a comprehensive knowledge-intensive benchmark, K-BENCH, covering a diverse range of topics including Wikipedia (English), Science (Chinese), and Artifacts (Chinese)-K-BENCH serves as a vital resource for assessing the effectiveness of CoD and the overall framework in handling complex, knowledge-driven dialogue tasks.\n\u2022 The work outlines the creation of GINSTRUCT, a synthetic instruction dataset that retains an extensive amount of knowledge from the raw documents in a dialogue format, which is used to fine-tune an open-source LLM, referred to as GLLM, which is specifically designed to transform raw documents into cohesive multi-turn dialogues. The experimental results from evaluating R2S on the K-BENCH benchmark demonstrate that this synthetic instruction approach is highly effective in enhancing the SFT model, enabling it to excel across various performance metrics. The fine-tuned generator GLLM is further proof of the efficiency of the CoD methodology, producing dialogues that are both logical and rich in domain-specific knowledge."}, {"title": "2 Problem Definition", "content": "Pre-training During pre-training, the LLM learns to predict the next word in a document based on the prefix context, enabling the LLM to capture linguistic patterns, syntax, semantics, and even some world knowledge embedded in the text. Given the raw document x \u2208 Dd, the pre-training can be described as:\n$$P(x) = \\prod_{k=1}^{K}P(x_k|X_{:k-1})$$\nwhere x is the document with K tokens.\nInstruction Tuning Supervised fine-tuning (SFT) is the process of taking a pre-trained model, like GPT, and adapting it to a specific task by training on a labeled dataset, which is typically much shorter than the initial pre-training phase since the model has already learned a wealth of language features. Given the query and response (q, a) \u2208 Ds, the single-turn instruction tuning can be described as:\n$$P(a) = \\prod_{i=1}^{n} P(a_i|a_{:i-1}, q; M)$$\nInstruction Tuning with Raw Text If we only have the dataset Dd with a limited number of raw documents, which are unable to support the domain-specific pre-training and following SFT, we can create muli-turn dialogues from Da to inject domain-specific knowledge into LLMs by instruction tuning. We create the multi-turn dialogue {q(t), a(t)}Jt=1\u2208 Ds from Da with raw documents to fine-tune the LLM M as:\n$$P(\\lbrace a^{(t)}\\rbrace_{t=1}^{T}) = \\prod_{t=1}^{T} P(a^{(t)}|a^{(:t-1)}, q^{(t)}; M)$$\nwhere a(t) and q(t) are t-th turn of the dialogures. a(:t-1) and q(t) are previous t \u2212 1 queries and t answers."}, {"title": "3 Benchmark", "content": "3.1 Data Collection & Statisites\nIn Table 1, our created benchmark comprises three distinct sections categorized by topics. we create 10, 428 English instruction instances and a test size of 1,041 items from the documents of the Squad V2.0 dataset. Then, the science dataset contains 9, 540 instruction instances and 955 test items from 10, 000 documents. The dataset in Artifacts utilizes data extracted from a website crawler, comprising 6, 152 instructions, and 615 test items from 6,872 documents. This dataset reflects a diverse collection strategy, spanning different languages (English and Chinese), content types (General and Artifacts), and sources (Squad V2, RefGPT, and a website crawler), aiming to provide comprehensive resources for domain-specific research.\n3.2 Evaluation Metric\nTo evaluate the generated multi-turn dialogues, we design the following metrics using the LLM (GPT-4) with the score range ({1: 'very bad', 2: 'bad', 3: 'neutral', 4: 'good', 4: 'very good' }).\nInformativeness (Info): Info is used to accurately whether the query articulates their problem or viewpoint, including the amount of key information, word count, and precision of expression.\nUnderstanding (US): US is used to evaluate the relevance between queries and the corresponding responses. For each turn of the dialogue, we calculate the score of US and average them to get a final score.\nUsefulness (UF): UF is introduced to decide whether the responses help the user solve their problem or confirm or correct their viewpoints (this may be based on facts or external documents).\nFidelity (FD): FD judges where the response involves factual document information and factual knowledge used in the document.\nFlexibility (FL): For the dialogue without the specific knowledge, FL is used to judge whether the response can correctly handle the query using the common knowledge in LLMs.\nConsistency (CS): Whether the behavior of conversation participants aligns with their roles and the behavior of the same speaker is consistent, involving the consistency and accuracy of expression.\nCohesion (CO): : Whether transitions in the conversation, topic deepening, and switches between topics are natural.\nInteractivity (IA): Whether the user and assistant express and share emotions during the exchange, including the accuracy of emotional expression and accurate understanding of emotions.\nCoverage Rate (CR): For multi-round dialogs constructed on the basis of documents, CR is used to evaluate the proportion of content covered by the dialogs over the content of the documents."}, {"title": "4 Framework of R2S", "content": "Figure 2 describes the overall framework of our method.\n4.1 Logic Definition\nTo create the multi-turn dialogue, we consider the following characteristic of the multi-turn dialogue: (1) Contextual Relevance: In multi-turn dialogues, each round of conversation is related to the context before and after it. Each answer is based on the previous round's question, and the next round's question may be based on the last answer. This coherence is a critical characteristic of multi-turn dialogues. (2) Continuity: In multi-turn dialogues, the exchange between the user and the assistant is continuous, not isolated single questions and answers, but a series of questions and answers. (3) Dialogue Depth: In multi-turn dialogues, users may delve deeper into a topic with their inquiries or follow up on the responses received with further questions. This requires the assistant to have the capability to manage complex dialogues and understand the depth of the conversation. (4) Topic Transition: In multi-turn dialogues, users may switch topics during the conversation. This demands the assistant to be flexible in response and able to answer on new topics. (5) Naturalness of Dialogue: In multi-turn dialogues, the assistant's responses need to be as natural and fluid as possible, making the user feel as though they are communicating with a person, not a machine.\nInspired by these characteristics of the multi-turn dialogue, we pre-define the six logic types of each turn: (1) Question-Answer: This is the most common logical sequence in dialogue, where Character A asks a question and Character B provides an answer. For example, 'How old are you?', 'I'm 23 years old.'. (2) Question-Question: This is a logical process where an intent to ask is completed. Character A asks a relatively vague question, and if Character B needs to clarify the intent of Character A's question, Character B can continue by asking another question. For example, 'How old are you?', 'Are you asking about my age?'. (3) Statement-Inquiry: Character A makes a statement, and Character B asks for or requires more information. For example, \"A: I went to the museum today.\" \"Oh, what interesting exhibitions did you see?\" (4) Statement-Explanation: Character A states a fact, and then Character B explains or elaborates on the information related to that fact. For example, 'Yao Ming won the CBA championship in 2002.', 'B: He was also named CBA Rebound King three times and Block King, and twice the CBA Dunk King.'. (5) Opinion-Rebuttal: Character A presents an opinion, and Character B counters with facts or presents a different viewpoint. For example, 'A: I think this movie is really good.', 'No, this movie scored a 9.5 rating, it is a good movie.'. (6) Opinion-Agreement: Character A expresses an opinion, and Character B either agrees or disagrees based on facts. For example, 'A: That singer performed terribly.', 'Indeed, the judges gave very low scores.'.\nEach link in the dialogue logical chain contains the current turn's dialogue type, progress, the logical process of the dialogue participants, and the purpose of the dialogue, achieving the effect of guiding the generation of multi-turn dialogue.\n4.2 Chain of Dialogue Logic\nThe chain of dialogue logic (DTC) encourages large language models (LLMs) to mimic the human thought process between rounds of dialogue, which includes identifying the type of dialogue, searching for relevant information, and discovering pertinent details, aiming to guide the generation of dialogue between turns. The design of the DTC format in this paper is inspired by chain of thought (CoT) and React. CoT is an improved prompting strategy intended to enhance the performance of LLMs in complex reasoning tasks such as arithmetic reasoning, common sense reasoning, and symbolic reasoning. React represents a new prompting paradigm based on repeated thought-action observation cycles until the current knowledge suffices to derive an answer, employed to facilitate reasoning and action within language models to tackle general tasks. DTC benefits from the \u201cThought\u201d paradigm of Dialogue-Search-Find, generating responses in dialogue that are more accurate and richer compared to CoT and React, as the Dialogue Types component leads the type of dialogue response, determining the direction for generating answers. Meanwhile, the Search-Find component can introduce factual knowledge into the dialogue, enhancing answer accuracy. Furthermore, unlike the simple prompt construction approach of input-output pairs, DTC incorporates intermediate reasoning steps, guiding the model towards the direction of the final output.\n4.3 GLLM\nGiven the synthetic multi-turn dialogue {q(t), a(t)}Jt=1 \u2208 D, generated by the teacher LLM (e.g. GPT-4), we can fine-tune the open-source LLMs to obtain the generator GLLM based on Qwen-2-7B and Llama-3-8B. To reduce the cost, we can use the GLLM with fewer parameters to inference more samples by adjusting the sampling temperature and sampling. In our work, we create the sampled dataset {qgt), ag)}=1 \u2208 Ds from GLLM to augment the original dataset."}, {"title": "5 Experiments", "content": "We conduct three types of experiments to evaluate the ability and effectiveness of our proposed R2S framewor to inject document knowledge into multi-turn dialogues:\n\u2022 K-BENCH Evaluation: we assess the quality of K-BENCH by conducting a comprehensive assessment involving both automated and human evaluation methods, subsequently K-BENCH Evaluation To verify the quality of of K-BENCH and further assess the effectiveness of the proposed CoD compared to CoT and direct-instruction prompting approaches, we employ GPT-4 Turbo as the judge to evaluate the K-BENCH. GPT-4 Turbo was tasked with scoring each dialogue turn based on the evaluation metrics and scoring standard described in 3.2. We adopt GPT-3.5 Turbo (GPT 3.5), Deepseek V2 Chat (Deepseek), and Qwen-2-72B-Instruct (Qwen) as different variants of raw text (documents) to multi-turn SFT data generators. Results of experiments on the three different types of SFT data generated: artifacts, science, and wikipedia are shown in Table 2, 3, and 4 repectively. By examing the results, we find that the LLMs with our proposed CoD design outperforms other models on all metrics, except for the coverage rate (CR) metric evaluated on the original dialogues created by RefGPT (Yang et al., 2023), using the same evaluation standards in Table 3. Yang et al. (2023) designed specific knowledge-injecting methods to ensure the dialogue data they construct include most of the facts from the documents, while our method not only ensures high factual accuracy, but also exhibits a high degree of coherence and contextual relevance. Moreover, the SFT data generated by Qwen with the CoD design still outperforms RefGPT on the CR metric, which demonstrates the potential of our method reaching high limits on different backbome LLMs. Since the GINSTRUCT dataset proposed for training GLLM is created using the same methods as creating K-BENCH, the evaluation results also indicate that the GINSTRUCTdataset exhibits high quality across different metrics.\nGLLM Evaluation In Table 2, 3, and 4, we also present results of GLLM initialized on both Llama-3-8B and Qwen-2-7B. In detail, we evaluate on the SFT data generated by these two GLLMs using the same metrics and standards introduced in \"K-BENCH Evaluation\". Evaluation results of the SFT data generated by GPT-3.5 only slightly is only slightly higher than the data generated by GLLM, which indicates that the GLLMs intialized on both LLMs obtained a high degree of data generation ability from GPT-3.5 with CoD. By leveraging the chain of dialogue logic and knowledge-intensive documents, the SFT data produced by GLLM surpasses traditional retrieval-based and direct response methods, establishing new high ground for knowledge-intensive instruction tuning in large language models.\nFine-tuned LLM Evaluation To evaluate the effectiveness of LLMs fine-tuned on the SFT data produced by GLLM, we conduct experiments comparing different generators, fine-tuned LLMs, and model settings on 3 dataset sources seperately: artifacts, RefGPT and SquadV2. We exam an-swers produced by Qwen-2-7B and Llama3-8B fine-tuned on multi-turn dialogue data generated by GPT-3.5 and GLLM, respectively. To further investigate the effectiveness of CoD, we compare LLMs fine-tuned on the training data generated by generators with and without CoD (refers as Direct model in 5) guidance. Experiment results are presented in Table 5. We find that all CoD models outperform direct models without CoD, demonstrating the effectiveness our chain-of-logic design. The LLMs trained on data produced by GLLM generators only slighly underperform those trained on data produced by GPT-3.5. This also matches the results from the GLLM evaluation experiment, where GLLM acquired most of the dialogue generation abilities from its upper-boundary GPT-3.5.\n5.3 Analysis\nHuman Evaluation To ensure the robustness of the GPT-4-based evaluation, we sampled 100 examples from K-BENCH created by GPT-3.5 and conducted human evaluation. The human evaluators were provided with the evaluation metrics and detailed instructions to score each dialogue turn from 1 to 5. The scores were aggregated to provide an overall assessment of the dataset quality. The evaluators were encouraged to provide qualitative feedback on the naturalness and factual accuracy of the dialogues. Additionally, we performed a correlation analysis between the scores assigned by GPT-4 and human evaluators. Specifically, we adopt Pearson and Spearman correlation coefficients to measure the association between the automated and human evaluations. The Pearson and Spearman correlation coefficients between GPT-4 and human evaluations were 0.89 and 0.87, respectively, indicating a strong alignment between automated and human assessments.\nThe effectiveness of R2S Figure 3 is a case study comparing dialogue responses generated by direct models and R2S. By integrating the chain of dialogue logic, GLLM effectively maintains factual accuracy and produces coherent, contextually relevant dialogues that mimic human-like interactions. Such approach not only improves the performance of the generated dialogues but also provides a robust framework for instruction tuning of large language models using raw text documents. Owing to the CoD design: 1) the dialogues generated by GLLM were significantly more informative and detailed, providing users with comprehensive responses, 2) the understanding and coherence of the dialogues were markedly improved, with responses that logically followed the user's queries, and 3) the dialogues exhibited higher loyalty to the reference documents, ensuring factual accuracy and reducing the occurrence of hallucinations."}, {"title": "6 Related Work", "content": "Large Language Model Large language models (LLMs) (Touvron et al., 2023a,b; Frantar et al., 2022; Bai et al., 2023; Du et al., 2021; Rozi\u00e8re et al., 2023), leveraging the Transformer architecture, represent a significant leap in natural language processing (NLP) (Li et al., 2022, 2023b; Qin et al., 2024a). LLMs undergo rigorous training on extensive textual datasets, enabling them to grasp a wide range of linguistic nuances and contexts. LLMs follow a two-stage process involving pre-training on large-scale corpora followed by instruct tuning for specific tasks, significantly improving performance across downstream understanding and generation challenges. Notably, the GPT series, starting from GPT-1 and evolving through GPT-4, (Radford et al., 2018; Black et al., 2022; Brown et al., 2020; OpenAI, 2023) showcases a progressive increase in model complexity and capacity, with GPT-3 comprising a staggering 175 billion parameters. The introduction of instruction tuning further amplifies the capabilities of LLMs, unlocking emergent abilities for intricate reasoning tasks, such as math and code. LLMs with instruction tuning garner attention from researchers and make significant impacts across various industry scenarios.\nInstruction Tuning LLMs refine their ability to follow and understand user commands more accurately fine-tuned on an instruction dataset (Ouyang et al., 2022; Zan et al., 2023; Qin et al., 2024b), consisting of various instructions and their corresponding desired outputs. Early research in constructing conversational datasets largely relies on manually annotated sets (e.g. QuAC (Choi et al., 2018) and CoQA (Reddy et al., 2019)), but the limited scale and high annotation costs restrict model performance and generalizability. Simulation-based approaches are adopted to generate synthetic dialogues through mimicking user-system interactions, thus reducing dependence on manual annotations. Recent advancements (Sun et al., 2023) emphasize the capabilities of LLMs like GPT-4 in autogenerating extensive, high-quality datasets such as the SODA dataset (Kim et al., 2023), with 1100 million utterances and 3 billion tokens. These works highlight LLMs' data generation prowess and the importance of human intervention in enhancing the accuracy of generated data, for instance, by applying basic, safety, and commonsense filters to GPT-3.5-generated dialogues (Chiang and Lee, 2023; Lotfi et al., 2023)."}, {"title": "7 Conclusion", "content": "In this paper, we introduce R2S, a framework for constructing a supervised fine-tuning (SFT) dataset from raw documents utilizing the chain of dialogue logic (CoD) to guide LLMs in creating knowledge-intensive multi-turn dialogues for instruction tuning. By aggregating existing documents from open-source websites/datasets, we establish a comprehensive benchmark, K-BENCH, featuring topics in Wikipedia (English), Science (Chinese), and Artifacts (Chinese). Utilizing CoD, we categorize dialogue turns (e.g., Opinion Exchange Q&A and Informational Q&A) and prompt the LLM to identify key phrases for generating relevant responses, thus retaining raw document knowledge within dialogue-style instruction datasets (GINSTRUCT) and enabling the fine-tuning of open-source LLMs (GLLM) for transforming raw documents into multi-turn dialogues, further enriching the SFT model with domain-specific knowledge. Extensive experiments on K-BENCH validate the efficacy of R2S, showing substantial improvements in performance across various metrics. The SFT generator GLLM confirms the effectiveness of CoD by creating reasonable and coherent multi-turn dialogues."}, {"title": "Limitations", "content": "Due to limited computing resources, the supervised fine-tuning (SFT) dataset generated by GLLM is evaluated on Qwen-2-7B and Llama-3-8B only, while more backbone large language models should be tested, ensuring the robustness of R2S to inject document knowledge into SFT data. Besides, the large-scale training scenario with more powerful LLMs still requires further exploration."}, {"title": "Ethical Considerations", "content": "The dataset used for evaluation in this paper is obtained from open data sources and has been manually verified and screened to eliminate any data with ethical risks and sensitive content. This ensures that the content is compliant with existing regulations and laws."}]}