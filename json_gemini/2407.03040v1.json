{"title": "Raw Text is All you Need: Knowledge-intensive Multi-turn Instruction Tuning for Large Language Model", "authors": ["Xia HOU", "Qifeng Li", "Jian Yang", "Tongliang Li", "Linzheng Chai", "Xianjie Wu", "Hangyuan Ji", "Zhoujun Li", "Jixuan Nie", "Jingbo Dun", "Wenfeng Song"], "abstract": "Instruction tuning as an effective technique aligns the outputs of large language models (LLMs) with human preference. But how to generate the seasonal multi-turn dialogues from raw documents for instruction tuning still requires further exploration. In this paper, we present a novel framework named R2S that leverages the CoD-Chain of Dialogue logic to guide large language models (LLMs) in generating knowledge-intensive multi-turn dialogues for instruction tuning. By integrating raw documents from both open-source datasets and domain-specific web-crawled documents into a benchmark K-BENCH, we cover diverse areas such as Wikipedia (English), Science (Chinese), and Artifacts (Chinese). Our approach first decides the logic flow of the current dialogue and then prompts LLMs to produce key phrases for sourcing relevant response content. This methodology enables the creation of the GINSTRUCT instruction dataset, retaining raw document knowledge within dialogue-style interactions. Utilizing this dataset, we fine-tune GLLM, a model designed to transform raw documents into structured multi-turn dialogues, thereby injecting comprehensive domain knowledge into the SFT model for enhanced instruction tuning. This work signifies a stride towards refining the adaptability and effectiveness of LLMs in processing and generating more accurate, contextually nuanced responses across various fields.", "sections": [{"title": "1 Introduction", "content": "The evolution of large language models (LLMs), including advancements seen in the generative pre-trained Transformer (GPT) series by OpenAI, marks a significant milestone in the field of natural language processing (NLP). A pivotal strategy in their development has been instruction-tuning, which leverages human-curated prompts, feedback,"}, {"title": "2 Problem Definition", "content": "Pre-training During pre-training, the LLM learns to predict the next word in a document based on the prefix context, enabling the LLM to capture linguistic patterns, syntax, semantics, and even some world knowledge embedded in the text. Given the raw document $x \\in D_d$, the pre-training can be described as:\n$$P(x) = \\prod_{k=1}^{K} P(x_k|X_{:k-1})$$  (1)\nwhere x is the document with K tokens.\nInstruction Tuning Supervised fine-tuning (SFT) is the process of taking a pre-trained model, like GPT, and adapting it to a specific task by training on a labeled dataset, which is typically much shorter than the initial pre-training phase since the model has already learned a wealth of language features. Given the query and response (q, a) \u2208 Ds, the single-turn instruction tuning can"}, {"title": "3 Benchmark", "content": "In Table 1, our created benchmark comprises three distinct sections categorized by topics. we create 10, 428 English instruction instances and a test size of 1,041 items from the documents of the Squad V2.0 dataset. Then, the science dataset contains 9, 540 instruction instances and 955 test items from 10, 000 documents. The dataset in Artifacts utilizes data extracted from a website crawler, comprising 6, 152 instructions, and 615 test items from 6,872 documents. This dataset reflects a diverse collection strategy, spanning different languages (English and Chinese), content types (General and Artifacts), and sources (Squad V2, RefGPT, and a website crawler), aiming to provide comprehensive resources for domain-specific research."}, {"title": "3.2 Evaluation Metric", "content": "To evaluate the generated multi-turn dialogues, we design the following metrics using the LLM (GPT-4) with the score range ({1: 'very bad', 2: 'bad', 3: 'neutral', 4: 'good', 4: 'very good' }).\nInformativeness (Info): Info is used to accurately whether the query articulates their problem or viewpoint, including the amount of key information, word count, and precision of expression.\nUnderstanding (US): US is used to evaluate the relevance between queries and the corresponding responses. For each turn of the dialogue, we calculate the score of US and average them to get a final score."}, {"title": "4 Framework of R2S", "content": "Figure 2 describes the overall framework of our method."}, {"title": "4.1 Logic Definition", "content": "To create the multi-turn dialogue, we consider the following characteristic of the multi-turn dialogue: (1) Contextual Relevance: In multi-turn dialogues, each round of conversation is related to the context before and after it. Each answer is based on the previous round's question, and the next round's question may be based on the last answer. This coherence is a critical characteristic of multi-turn dialogues. (2) Continuity: In multi-turn dialogues, the exchange between the user and the assistant is continuous, not isolated single questions and answers, but a series of questions and answers. (3) Dialogue Depth: In multi-turn dialogues, users may delve deeper into a topic with their inquiries or follow up on the responses received with further questions. This requires the assistant to have the capability to manage complex dialogues and understand the depth of the conversation. (4) Topic Transition: In multi-turn dialogues, users may switch topics during the conversation. This demands the assistant to be flexible in response and able to answer on new topics. (5) Naturalness of Dialogue: In multi-turn dialogues, the assistant's responses need to be as natural and fluid as possible, making the user feel as though they are communicating with a person, not a machine.\nInspired by these characteristics of the multi-turn dialogue, we pre-define the six logic types of each turn: (1) Question-Answer: This is the most common logical sequence in dialogue, where Character A asks a question and Character B provides an answer. For example, 'How old are you?', 'I'm 23 years old.'. (2) Question-Question: This is a logical process where an intent to ask is completed. Character A asks a relatively vague question, and if Character B needs to clarify the intent of Character A's question, Character B can continue by asking another question. For example, 'How old are you?', 'Are you asking about my age?'. (3) Statement-Inquiry: Character A makes a statement, and Character B asks for or requires more information. For example, \"A: I went to the museum today.\" \"Oh, what interesting exhibitions did you see?\" (4) Statement-Explanation: Character A states a fact, and then Character B explains or elaborates on the information related to that fact. For example, 'Yao Ming won the CBA championship in 2002.', 'B: He was also named CBA Rebound King three times and Block King, and twice the CBA Dunk King.'. (5) Opinion-Rebuttal: Character A presents an opinion, and Character B counters with facts or presents a different viewpoint. For example, 'A: I think this movie is really good.', 'No, this movie scored a 9.5 rating, it is a good movie.'. (6) Opinion-Agreement: Character A expresses an opinion, and Character B either agrees or disagrees based on facts. For example, 'A: That singer performed terribly.', 'Indeed, the judges gave very low scores.'.\nEach link in the dialogue logical chain contains the current turn's dialogue type, progress, the logical process of the dialogue participants, and the purpose of the dialogue, achieving the effect of guiding the generation of multi-turn dialogue."}, {"title": "4.2 Chain of Dialogue Logic", "content": "The chain of dialogue logic (DTC) encourages large language models (LLMs) to mimic the human thought process between rounds of dialogue, which includes identifying the type of dialogue, searching for relevant information, and discovering pertinent details, aiming to guide the generation of dialogue between turns. The design of the DTC format in this paper is inspired by chain of thought (CoT) and React. CoT is an improved prompting strategy intended to enhance the performance of LLMs in complex reasoning tasks such as arithmetic reasoning, common sense reasoning, and symbolic reasoning. React represents a new prompting paradigm based on repeated thought-action observation cycles until the current knowledge suffices to derive an answer, employed to facilitate reasoning and action within language models to tackle general tasks. DTC benefits from the \u201cThought\u201d paradigm of Dialogue-Search-Find, generating responses in dialogue that are more accurate and richer compared to CoT and React, as the Dialogue Types component leads the type of dialogue response, determining the direction for generating answers. Meanwhile, the Search-Find component can introduce factual knowledge into the dialogue, enhancing answer accuracy. Furthermore, unlike the simple prompt construction approach of input-output pairs, DTC incorporates intermediate reasoning steps, guiding the model towards the direction of the final output."}, {"title": "4.3 GLLM", "content": "Given the synthetic multi-turn dialogue {$q^{(t)}, a^{(t)}$}$_{t=1}^{T}$ \u2208 $D_s$, generated by the teacher LLM (e.g. GPT-4), we can fine-tune the open-source LLMs to obtain the generator GLLM based on Qwen-2-7B and Llama-3-8B. To reduce the cost, we can use the GLLM with fewer parameters to inference more samples by adjusting the sampling temperature and sampling. In our work, we create the sampled dataset {$q^{(t)}, a^{(t)}$}$_{t=1}^{T}$ \u2208 $D_s$ from GLLM to augment the original dataset."}, {"title": "5 Experiments", "content": "We conduct three types of experiments to evaluate the ability and effectiveness of our proposed R2S framewor to inject document knowledge into multi-turn dialogues:\n\u2022 K-BENCH Evaluation: we assess the quality of K-BENCH by conducting a comprehensive assessment involving both automated and human evaluation methods, subsequently"}, {"title": "5.2 Main Results", "content": "K-BENCH Evaluation To verify the quality of of K-BENCH and further assess the effectiveness of the proposed CoD compared to CoT and direct-instruction prompting approaches, we employ GPT-4 Turbo as the judge to evaluate the K-BENCH. GPT-4 Turbo was tasked with scoring each dialogue turn based on the evaluation metrics and scoring standard described in 3.2. We adopt GPT-3.5 Turbo (GPT 3.5), Deepseek V2 Chat (Deepseek), and Qwen-2-72B-Instruct (Qwen) as different variants of raw text (documents) to multi-turn SFT data generators. Results of experiments on the three different types of SFT data generated: artifacts, science, and wikipedia are shown in Table 2, 3, and 4 repectively. By examing the results, we find that the LLMs with our proposed CoD design outperforms other models on all metrics, except for the coverage rate (CR) metric evaluated on the original dialogues created by RefGPT  , using the same evaluation standards in Table 3. Yang et al. (2023) designed specific knowledge-injecting methods to ensure the dialogue data they construct include most of the facts from the documents, while our method not only ensures high factual accuracy, but also exhibits a high degree of coherence and contextual relevance. Moreover, the SFT data generated by Qwen with the CoD design still outperforms RefGPT on the CR metric, which demonstrates the potential of our method reaching high limits on different backbome LLMs. Since the GINSTRUCT dataset proposed for training GLLM is created using the same methods as creating K-BENCH, the evaluation results also indicate that the GINSTRUCTdataset exhibits high quality across different metrics.\nGLLM Evaluation In Table 2, 3, and 4, we also present results of GLLM initialized on both Llama-3-8B and Qwen-2-7B. In detail, we evaluate on"}, {"title": "5.3 Analysis", "content": "Human Evaluation To ensure the robustness of the GPT-4-based evaluation, we sampled 100 examples from K-BENCH created by GPT-3.5 and conducted human evaluation. The human evaluators were provided with the evaluation metrics and detailed instructions to score each dialogue turn from 1 to 5. The scores were aggregated to provide an overall assessment of the dataset quality. The evaluators were encouraged to provide qualitative feedback on the naturalness and factual accuracy of the dialogues. Additionally, we performed a correlation analysis between the scores assigned by GPT-4 and human evaluators. Specifically, we adopt Pearson and Spearman correlation coefficients to measure the association between the automated and human evaluations. The Pearson and Spearman correlation coefficients between GPT-4 and human evaluations were 0.89 and 0.87, respectively, indicating a strong alignment between automated and human assessments.\nThe effectiveness of R2S Figure 3 is a case study comparing dialogue responses generated by direct models and R2S. By integrating the chain of dialogue logic, GLLM effectively maintains factual accuracy and produces coherent, contextually relevant dialogues that mimic human-like interactions. Such approach not only improves the performance of the generated dialogues but also provides a robust framework for instruction tuning of large language models using raw text documents. Owing to the CoD design: 1) the dialogues generated by GLLM were significantly more informative and detailed, providing users with comprehensive responses, 2) the understanding and coherence of the dialogues were markedly improved, with responses that logically followed the user's queries, and 3) the dialogues exhibited higher loyalty to the reference documents, ensuring factual accuracy and reducing the occurrence of hallucinations."}, {"title": "6 Related Work", "content": "Large Language Model Large language models (LLMs) (Touvron et al., 2023a,b; Frantar et al., 2022; Bai et al., 2023; Du et al., 2021; Rozi\u00e8re et al., 2023), leveraging the Transformer architecture, represent a significant leap in natural language processing (NLP) (Li et al., 2022, 2023b; Qin et al., 2024a). LLMs undergo rigorous training on extensive textual datasets, enabling them to grasp a wide range of linguistic nuances and contexts. LLMs follow a two-stage process involving pre-training on"}, {"title": "7 Conclusion", "content": "In this paper, we introduce R2S, a framework for constructing a supervised fine-tuning (SFT) dataset from raw documents utilizing the chain of dialogue logic (CoD) to guide LLMs in creating knowledge-intensive multi-turn dialogues for instruction tuning. By aggregating existing documents from open-source websites/datasets, we establish a comprehensive benchmark, K-BENCH, featuring topics in Wikipedia (English), Science (Chinese), and Artifacts (Chinese). Utilizing CoD, we categorize dialogue turns (e.g., Opinion Exchange Q&A and Informational Q&A) and prompt the LLM to identify key phrases for generating relevant responses, thus retaining raw document knowledge within dialogue-style instruction datasets (GINSTRUCT) and enabling the fine-tuning of open-source LLMs (GLLM) for transforming raw documents into multi-turn dialogues, further enriching the SFT model with domain-specific knowledge. Extensive experiments on K-BENCH validate the efficacy of R2S, showing substantial improvements in performance across various metrics. The SFT generator GLLM confirms the effectiveness of CoD by creating reasonable and coherent multi-turn dialogues."}, {"title": "Limitations", "content": "Due to limited computing resources, the supervised fine-tuning (SFT) dataset generated by GLLM is evaluated on Qwen-2-7B and Llama-3-8B only, while more backbone large language models should be tested, ensuring the robustness of R2S to inject document knowledge into SFT data. Besides, the large-scale training scenario with more powerful LLMs still requires further exploration."}, {"title": "Ethical Considerations", "content": "The dataset used for evaluation in this paper is obtained from open data sources and has been manually verified and screened to eliminate any data with ethical risks and sensitive content. This ensures that the content is compliant with existing regulations and laws."}]}