{"title": "Teaching Smaller Language Models To\nGeneralise To Unseen Compositional\nQuestions", "authors": ["Timothy John Hartill"], "abstract": "We are inspired by recent progress with pretrained large Language Models\n(LLMs), that are able to answer questions that are unlikely to have been\nencountered during training. However a diversity of potential applications\nexist in the broad domain of reasoning systems and considerations such\nas latency, cost, available compute resource and internet connectivity are\nrelevant in determining an appropriate approach. We consider the setting\nwhere some local compute capacity is available at inference time but internet\nconnectivity is not.\nSimilar to a general-purpose LLM, we assume that our much smaller\nReasoning Models may be asked arbitrary questions from unknown distri-\nbutions, hence we focus on evaluation in an unseen setting where our evalu-\nation datasets are disjoint from our training datasets. We equip our models\nto answer diverse questions through multitask training focused on instilling\nan ability to reason over a provided context to an answer. We acquire this\ncontext from two knowledge sources; a local Wikipedia corpus queried using\na multi-hop dense retrieval system with novel extensions, and from ratio-\nnales generated from a larger Language Model optimised to run in a lower\nresource environment.\nOur main contributions to the study of question-answering in this setting\nare as follows: We propose novel methods to evaluate whether our model is\ncapable of answering contextualised questions without memorisation, and\nshow that it is. We establish a comprehensive set of baseline results on\nunseen evaluation datasets. We show that the addition of novel retrieval-\naugmented training datasets (RATD) to the training regime of the Reason-\ning Model in conjunction with our retrieval system significantly improves", "sections": [{"title": "1 Introduction", "content": "When prompted with task demonstrations (Brown et al., 2020), instructions\n(Sanh et al., 2021; Wei et al., 2021; Ouyang et al., 2022) or reasoning chains\n(Wei et al., 2022), large Language Models (LLMs) have shown an ability to\nanswer diverse questions unlikely to have been encountered during training\n(Brown et al., 2020; Sanh et al., 2021; Wei et al., 2021; Du et al., 2022;\nChowdhery et al., 2022). While impressive, this performance has required\naccess to considerable computational resource, typically centralised and ac-\ncessed over a network that is assumed to be continuously available. In this\nthesis, we consider the implications and opportunities that an alternative\nscenario might present; one in which internet connectivity is assumed to be\nunreliable, unavailable, or merely prohibitively expensive. To make progress\nin this scenario, utilising technology widely available at the time of writing,\nwe assume that some local compute capacity is available at inference time,\nnamely the equivalent of a single workstation with a large consumer-grade\nGPU card. Such resource-constrained environments are abundant, ranging\nfrom vehicles and fixed locations without continuous internet access, to sen-\nsitive applications involving highly confidential information not shareable\nover the internet.\nIn our constrained environment, we utilise a smaller Language Model\nthat can be run locally on our workstation to answer questions. We define"}, {"title": "1.1 Background and Motivation", "content": "smaller Language Models as generative Transformer models (Vaswani et al.,\n2017) with 400 million to 1 billion trainable parameters, i.e those that are\nlarge enough to be effective at answering questions whilst being able to\nperform training and inference with reasonable latency, cost and energy effi-\ncency. We boldly assume that like a general-purpose LLM, our smaller Lan-\nguage Models will be expected to answer arbitrary questions from unknown\ndistributions. This is uncommon in that, excepting Khashabi et al. (2020b),\nfew papers have reported zero-shot results for smaller Language Models, fo-\ncusing instead on optimising performance via finetuning for particular tasks.\nHowever, duplication between test and training splits in natural language\nprocessing (NLP) datasets is frequent (Lewis et al., 2021; Lee et al., 2022;\nKrishna et al., 2021; Kambhatla et al., 2023), which leads to conjecture as\nto what exactly a model has learned in the fine-tuned setting. In addition to\nthe possibility of answer leakage from directly memorised training samples,\nit has been shown that models are able to utilise more subtle cues, such as\nthe writing style of a particular annotator who contributed to both train and\ntest splits, for better results than are achievable where the test split is truly\nindependent of the training split (Geva et al., 2019). To minimise such issues\nas well as to facilitate comparison in a similar setting as other zero/few shot\nstudies, we define an unseen question as one from an evaluation dataset that\nis disjoint from our training datasets.\nLLMs have been shown to have strong performance in answering ques-\ntions that are input without any supporting context i.e. open domain ques-\ntions (Roberts et al., 2020). By contrast, smaller Language Models, such as\nthe BART model (Lewis et al., 2020a) that we use throughout our experi-\nments, are poor at answering such uncontextualised questions, particularly\nwhen the evaluation question is not a paraphrase of a memorised training\nsample (Lewis et al., 2021). An alternative approach, which we follow and\nextend, has been to use the question text to query a knowledge source and\nretrieve information pertinent to answering the question. The problem is\nthen transformed into a reading comprehension (RC) challenge whereby the\nquestion and the acquired context are input into a Language Model that\nwould preferably reason over the question and the provided context to infer\nan answer (hereafter, called a Reasoning Model)."}, {"title": "", "content": "In the belief that regardless of how comprehensive any available knowl-\nedge source may be, there will be many questions that cannot be answered\nusing information from a single retrieved document, we focus our study on\ncompositional questions. The classical Partee (1984) definition of composi-\ntionality as an ability to build up the meaning of a complex expression by\ncombining the meanings of its parts has been challenging in practice to use-\nfully apply to natural language tasks such as machine translation and our\nquestion-answering topic (Dankers et al., 2022; Hupkes et al., 2020). Re-\ncent work has alternatively described compositional or \u201ccomplex\u201d questions\nas those where answering requires decomposition into multiple reasoning\nsteps (Talmor and Berant, 2018; Geva et al., 2021), or reasoning (sometimes\ntermed composition) over more than one piece of information (Yang et al.,\n2018; Min et al., 2019; Khot et al., 2020; Zhao et al., 2023). The skills in-\nvolved in such reasoning are diverse and multidimensional (Rogers et al.,\n2023), encompassing for example fact composition (Khot et al., 2020), nu-\nmerical reasoning (Dua et al., 2019; Zhao et al., 2023), logical operations\n(Clark et al., 2020b; Sinha et al., 2019) or set operations (Sen et al., 2022).\nNoting that the complexity of reasoning needed is a function of both the\nquestion and the available evidence (Min et al., 2019), and that Language\nModel training data is itself a source of evidence, we offer a modestly revised\ndefinition of a compositional question as follows:\nA question is compositional if it is unlikely to be answerable by\nour Reasoning Model with a memorised answer from a similar\ntraining example, or by retrieving any single document from any\navailable knowledge source.\nHere, a knowledge source refers to training data for any Language Model\nwe utilise or the textual corpus accessed by our retrieval system. A document\nrefers to an individual training sample, or corpus paragraph respectively.\nOur first knowledge source is a corpus consisting of English Wikipedia\nparagraphs. Methods for retrieving information from such textual cor-\npora have a long history in the information retrieval domain generally e.g.\nSp\u00e4rck Jones (1972), and more recently for augmenting open domain ques-\ntions (Chen et al., 2017; Karpukhin et al., 2020). In regard to the latter, early"}, {"title": "", "content": "studies focused on the single-hop case where a single document from the cor-\npus typically provides sufficient evidence to enable answering the question\nin a deductively valid fashion. This work has subsequently been extended to\nretrieval for multi-hop questions where multiple documents from the corpus\nare necessary to answer the question (Qi et al., 2021; Xiong et al., 2021).\nHere studies have focused on datasets such as HotpotQA (Yang et al., 2018)\nwhere the necessary number of documents, henceforth n, has often been lim-\nited to two. In our work, we extend n to an arbitrary number of documents\nand introduce an Evidence Set Scoring model whose purpose is to quantify\nthe sufficiency of the information accumulated up to each hop for answering\na question.\nCorpora such as Wikipedia contain large amounts of factual information\nand it might be expected that effective retrieval from such sources would pro-\nvide good information for answering questions of a factual nature. However\nsuch knowledge sources have been shown to be less effective for identify-\ning other types of information such as commonsense, or \u201cworld\u201d knowledge\n(Piktus et al., 2021). We therefore evaluate a second knowledge source in\ncombination with the first; rationales generated by larger Language Mod-\nels conditioned on the question text. We define a rationale as a free-text\nexplanation (Wiegreffe and Marasovi\u0107, 2021) of approximately one to three\nsentences that aims to provide sufficient evidence from which to deduce an\nanswer. Querying a LLM over the internet to generate rationales would of\ncourse defeat our purpose, but we study the case where a larger Language\nModel can be optimised to run in our constrained environment."}, {"title": "1.2 Research Problem", "content": "The setting defined above poses a number of under-explored challenges that\nform the basis of our research. These can be summarised as:\nSmaller Language Model Viability As Reasoning Models\n\u25a0 The extent to which RC questions can be answered by smaller Lan-\nguage Models without reference to one or more memorised training\nsamples has not previously been documented."}, {"title": "", "content": "\u25a0 How well smaller Language Models can perform the reasoning function\nin the unseen setting, and how performance can be improved has not\nbeen comprehensively studied.\n\u25a0 Few studies quantify the LLM performance gap to smaller Language\nModels when both are considered in similar unseen settings.\nKnowledge Retrieval Limitations\n\u25a0 Even the most comprehensive set of knowledge sources is unlikely to\nyield sufficient information to enable answering any question deduc-\ntively. This could be due to any combination of (1) incompleteness\nof the knowledge source, (2) incompleteness of the question specifica-\ntion, (3) sub-optimality in the retrieval algorithm, or (4) information\nretrieved being false. It is therefore desirable to consider the situation\nwhere information retrieved is partially evidential, contains irrelevant\ndistractions, or false information. We evaluate novel mitigations for\nthese challenges.\n\u25a0 Research on improving performance in dense retrieval from textual\ncorpora where the retrieval components are not fine-tuned on the same\ndatasets as the target questions is limited (exceptions and alternatives\nto our approach in this regard are discussed in Section 3.2).\nKnowledge Source Strengths and Weaknesses\n\u25a0 As we discuss in Section 3.3, a number of studies consider LLMs as\nknowledge sources, but these generally assume that the LLM is the sin-\ngle, or primary source. Perhaps because of this assumption there has\nnot been much focus on quantifying the detailed strengths or weak-\nnesses of LLMs as knowledge sources in contrast to other possible\nsources of contextual information.\n\u25a0 Conversely, approaches focusing on retrieval from textual corpora tend\nto benchmark themselves against LLMs in a closed book setting where\nthe LLM is the Reasoning Model as well as the knowledge source. This\nhas the effect of conflating LLM reasoning ability with LLM viability"}, {"title": "", "content": "as a knowledge source. We offer an evaluation in a setting where these\nare disentangled.\n\u25a0 Few other studies have considered approaches to combining knowledge\nfrom disparate sources in constrained settings. Section 3.4 discusses\nthose studies that we have been able to identify."}, {"title": "1.3 Contributions", "content": "In the setting discussed above, we address our research questions and make\nthe following contributions to the research community:\n1. We demonstrate that a smaller Language Model is capable of perfor-\nmance beyond simple memorisation in deriving correct answers to chal-\nlenging compositional questions. To achieve this we propose a method\nof identifying overlap between evaluation and training samples based\nupon semantic similarity of input and output tokens. We utilise this\napproach in conjunction with a technique to intervene with additional\ntraining datasets to create a Reasoning Model versus a baseline Rea-\nsoning Model with no intervention. Our approach enables us to miti-\ngate effects of pretraining on results and to avoid comparing disparate\npopulations of evaluation subsets as some prior studies have done. Af-\nter demonstrating the effectiveness of our methods in identifying both\nmemorisable, and unmemorisable samples we are able to show that\nimproved performance on unmemorisable samples is not attributable\nto the effect of memorisation.\n2. We offer what is to our knowledge the most comprehensive set of base-\nlines evaluating smaller Language Model zero-shot reasoning abilities\nversus LLM and other approaches published to date. Here our baseline\n(Base) is a multitask-trained Reasoning Model that is trained in two\nstages on a large number of tasks, both existing and those that we\ndevelop.\n3. We propose the \u201cIterator\", a dense retrieval, reranking and evidence\nset scoring system that aims to identify the relevant n documents\""}, {"title": "", "content": "necessary to answer n-hop questions, where n is arbitrary but we use\n= 4.\nn\n4. We use the Iterator against a corpus of English Wikipedia paragraphs\nboth to develop contexts for unseen evaluation questions and to de-\nvelop retrieval-augmented training datasets (RATD) which are added\nto the existing Base training regime in training the Base+RATD\nmodel. RATD datasets are intended to impart diverse reasoning strate-\ngies, such as an ability to identify and weigh partially evidential facts\nin long, noisy contexts. We show that when used in conjunction with\nour retrieval-augmented evaluation samples the Base+RATD model\nsignificantly outperforms the Base model on the established baselines.\n5. We evaluate methods for combining information from two knowledge\nsources to develop contexts that are more helpful in answering ques-\ntions. The first knowledge source is the above Iterator with Wikipedia\nwhile the second involves rationale generation from larger Language\nModels that are optimised to run locally in a resource-constrained\nenvironment. We propose \u201cRationale Ranking\" (RR), a method that\nboth selects context components by relevance, and filters components\nthat may be false. This is accomplished by training a Rationale Rank-\ning model to score LLM-generated rationales and Iterator-generated\ncontexts for truthfulness in addition to the more common practice of\nquantifying relevance. A number of strategies are then evaluated for\nusing the resulting scores to develop contexts that combine information\nfrom both knowledge sources. We show that the RR method signifi-\ncantly outperforms the earlier Base+RATD baselines. We also show\nthat models trained using the earlier RATD training method are able\nto generalise sufficiently such that they can successfully utilise com-\nbined contexts both in isolation from, and in conjunction with, RR\nscoring.\n6. We show that smaller Language Models trained for reasoning can\nmanifest comparable or stronger performance on unseen questions to\nLLMs, when provided with the same knowledge to reason over that\nthe LLM is capable of generating for itself.\""}, {"title": "", "content": "7. We present evidence to illustrate the respective strengths and weak-\nnesses of LLMs and n-hop retrieval from a Wikipedia corpus as knowl-\nedge sources. The LLM tends to offer better performance when consid-\nering questions requiring commonsense knowledge (e.g. \u201cI'm crossing\nthe river, my feet are wet but my body is dry, where am I?\u201d). Retrieval\nfrom the Wikipedia corpus tends to be better at extracting knowl-\nedge necessary to answer n-hop factual questions where n is higher\nthan two (e.g. \"The Rhine forms a border between Aschenbr\u00f6del's\ncomposer's country and another country where women got the vote\nwhen?\"). Moreover, we show that combining information from these\nsources significantly improves the average performance over evalua-\ntion datasets versus using a single source, and on individual evalua-\ntion datasets the combined context performance is often beyond what\neither knowledge source in isolation can deliver.\nPortions of this thesis have been published in a peer-reviewed interna-\ntional journal. In particular, our RATD paper was accepted by Transactions\non Machine Learning Research (TMLR) in August 2023 (Hartill et al., 2023).\nAnother paper of which portions are also contained in this thesis has been\nsubmitted to a well-regarded venue for peer review and is awaiting review\ncompletion.\""}, {"title": "1.4 Thesis Overview", "content": "The remainder of this work is organized in the following chapters.\nChapter 2 provides preliminary explanations relevant to discussion in the\nfollowing chapters, specifically the models we use and the unseen evaluation\ndatasets we choose.\nChapter 3 reviews related research on the various topics that we utilise or\nextend in our research. We highlight the differences and similarities to our\nproblem formulation."}, {"title": "", "content": "Chapter 4 proposes a set of methods for determining whether a smaller\nLanguage Model is capable of reasoning over a provided question and con-\ntext to an answer or whether it is only capable of providing a memorised\nanswer from a similar training input.\nChapter 5 introduces a set of baselines for performance on challenging\nunseen compositional questions, comparing our approach of augmenting\nquestions with a retrieved context using the Iterator against LLM and other\napproaches. We then discuss our method for improving performance via\nthe addition of RATD datasets to the training regime of our Reasoning\nModel and demonstrate that this significantly improves performance when\ncombined with our retrieval method.\nChapter 6 presents a set of methods for combining the retrieval knowledge\nsource developed in the prior chapter with a second knowledge source con-\nsisting of rationales generated by larger Language Models. Here we show\nthat further significant improvement against the baselines are possible and\nexplore the strengths and weaknesses of each knowledge source with re-\nspect to the different types of questions encapsulated in each of our baselines.\nChapter 7 concludes the thesis. Here, we discuss limitations and potentially\nfruitful avenues to be explored in future research."}, {"title": "2 Preliminaries", "content": "The purpose of this chapter is to provide necessary definitions and back-\nground explanations relevant to the thesis. For the interested reader, Section\n2.1 provides a very brief history of computational approaches to answering\nquestions. Since it does not contain novel ideas, it may be skipped. Sec-\ntion 2.2 provides summary background on Language Models and introduces\nnomenclature used later in this thesis. Finally, to avoid duplication, Section\n2.3 provides a description of each dataset we use in evaluation as different\nsubsets of these are utilised in Chapters 4, 5 and 6. Since we reuse or develop\na large number of training datasets, the reader is referred to Chapter 5 for\nthe Reasoning Model training process, and to Appendix E for further details\non the individual training datasets."}, {"title": "2.1 Computational Approaches to\nQuestion-Answering", "content": "Excepting the recent trend towards using LLMs to answer questions di-\nrectly using knowledge encoded in the model parameters, computational\napproaches to the question-answering challenge have relied upon external\nsources of knowledge. The earliest question answering system was BASE-\nBALL (Green et al., 1961) which parsed a question into a structured rep-\nresentation which was then used to iteratively query a structured database.\nAnother very early system is described in Simmons et al. (1964). It used"}, {"title": "", "content": "content words extracted from each question to query an index of such terms\nand retrieve sentences that could be relevant to answering the question. The\nquestion and each sentence were then parsed using a dependency parser\nand sentences were scored with respect to the similarity of structure to the\nquestion parse. The highest scoring sentence was selected as most likely to\ncontain the answer.\nThese two studies are representative of the two main historical themes in\nquestion-answering: Semantic parsing methods such as BASEBALL convert\na question into a structured representation capable of being used as an exact\nquery against a database to return an answer. Information Retrieval-based\nmethods use some (not necessarily structured) representation of the question\nto retrieve a set of candidate documents, and then as in our case, use diverse\nRC mechanisms to extract or compute an answer from them (Bordes et al.,\n2014a; Jurafsky and Martin, 2023).\nExplorations of classical methods for RC Mechanisms where the con-\ntext has been provided rather than retrieved can be found in Hirschman\net al. (1999); Riloff and Thelen (2000). These both rely on lexical overlap\nbetween question and context sentences. Ng et al. (2000) claims to be the\nfirst machine learning method that is competitive for RC. They use a logis-\ntic regression model to score each question-context sentence pair where each\npair is represented as a vector of 20 specifically constructed features such as\na count of the number of words in common between the question and the\nsentence.\nIn 1999 the Text REtrieval Conference (TREC) question answering track\nwas launched with a goal of bringing together work being performed on\nInformation Retrieval with work being done on RC (Voorhees, 2001). Falcon\n(Harabagiu et al., 2000) is one such resulting project encompassing both of\nthese aspects.\nMore recently Bordes et al. (2014a,b) use neural models to embed bag-\nof-words representations of the question and subgraphs from the Freebase\nknowledge graph into a vector space such that the dot product of the re-\nsulting vector representations are higher where the subgraph contains the\nanswer. Since that time, many different approaches to question-answering\ninvolving neural models have been studied. Prominent amongst these are"}, {"title": "2.2 Language Models", "content": "approaches utilising Language Models, discussed in the next section, and\napproaches using graph neural networks (Kipf and Welling, 2017). In the\nlatter, a Language Model is typically used to create contextualised vec-\ntor representations of the question and retrieved (or provided) contextual\ninformation. A graph is then constructed over both with novelty being in-\ntroduced in the specification of nodes and edges. These representations are\nthen passed through a graph neural network for further refinement. The final\nrepresentations are subsequently used as input into further neural models\nfor performing tasks such as answering the question and predicting which\nsentences in the retrieved context are relevant (Fang et al., 2020).\nLanguage Models estimate a probability function P for a word, or token, in\na sequence of such tokens (Manning and Schutze, 1999). Given a sequence\nof s words w\u2081, ..., w, denoted as w\u2081, the task has been formalised as learning\nthe joint probability of the sequence from the product of the conditional\nprobability of each word conditioned on the subsequence preceding it:\n$$P(w_s) = \\prod_{i=1}^{s} P(w_i|w_1^{i-1})\\qquad(2.1)$$\nAccording to (Jurafsky and Martin, 2023), the mathematics of a tractible\napproximation of this was first formalised by Markov (Markov, 1913). Such\nn-gram models restrict the historical context considered in estimating the\nprobability of the ith word to n - 1 words by substituting the $P(w_i|w_1^{i-1})$\nterm in Equation 2.1 with $P(w_i|w_{i-n+1}^{i-1})$ where n is typically one (substitut-\ning $P(w_i)$ for $P(w_{i-n+1}^{i-1})$), two (bigrams) or three (trigrams). The con-\nditional probability for each n-gram is estimated based on a count of the\nnumber of occurrences of it in the corpus.\nIn 2000, (Bengio et al., 2000) proposed a neural version of a Language\nModel where the probability distribution over possible next words from an\ninput sequence is estimated by a feed-forward neural network. Each word\nin the vocabulary was represented by a dense vector $C(i) \u2208 R^d$ in which\nfeatures are learned during training. The vector was stored in a matrix and"}, {"title": "", "content": "accessed via the simple strategy of assigning each word in the vocabulary an\nindex number. This is readily identifiable with the embedding tables used\nas the first layer in modern neural Language Models. In 2013 Mikolov et al.\n(2013a,b) improved upon the utility of such word embeddings by propos-\ning the Continuous-Bag-Of-Words (CBOW) model (where the embedding\nparameters are learned from predicting the current word from both prior\nand future words), and the Skip-gram model (where the training objective\nwas to predict prior and future words from the current word). Embeddings\ncreated with models such as these and similar were commonly used as input\nrepresentations in the next generation of neural Language Models that were\nbuilt using recurrent neural networks (RNNs).\nIn 2014 Sutskever et al. (2014) proposed a sequence-to-sequence Lan-\nguage Model built for the task of neural machine translation (NMT). It\nwas built using the LSTM (Hochreiter and Schmidhuber, 1997) version of a\nRNN and featured an encoder-decoder architecture where at each timestep\nup to a maximum input sequence length t, the embedding for a word from\nthe input sequence $q : \\{x_i\\}_1^t\\}$ is input into the encoder, which outputs a\nhidden representation $h \u2208 R^d$ where d is the dimensionality of each input\nembedding as well as the hidden state. During training, the decoder takes\nthe final h as input along with the desired translation (or answer in our\nquestion-answering case) $a : \\{y_i\\}_1^m\\}$. As with Bengio et al. (2000) the decoder\nis trained to estimate the probability distribution over possible next words.\nThis is applied autoregressively to generate a word per iteration:\n$$P(a|q) = \\prod_{i=1}^m P(y_i|h, y_1^{i-1}) \\qquad(2.2)$$\nExtending the RNN architecture, Bahdanau et al. (2015) proposed an\nattention mechanism that uses a softmax function to produce a weighting\nover the sequence of all hidden states $H \u2208 R^{d\u00d7t}$ produced by the encoder\nwith the aim of weighting the most relevant parts of the corresponding\ninput representations higher than others. This was shown to substantially\nimprove performance on NMT tasks, and subsequently on other tasks such\nas question-answering as well. Adding the attention enhancement results in\nan update to the probability estimation function:"}, {"title": "", "content": "$$P(a|q) = \\prod_{i=1}^m P(y_i|H, y_1^{i-1}) \\qquad(2.3)$$\nIn the question-answering domain, Iyyer et al. (2014) and Hermann et al.\n(2015) applied RNN architectures to RC tasks. Chen et al. (2017) also used\nRNN models but here information retrieval was used to augment each ques-\ntion $q_i$ with a retrieved context $c_i$ where i denotes the ith sample in a dataset.\nFor brevity, throughout this thesis, we will denote input into a model using\nangle brackets e.g. in the Chen et al. (2017) case the encoder input would\nbe $(q_i, c_i)$, the decoder input would be $(H_i, a_i)$ and we will omit the batch\ndimension for readability.\nVaswani et al. (2017) proposed the first Transformer model, which\ndemonstrated improved performance on NMT tasks. Similar to (Sutskever\net al., 2014), this was an encoder-decoder model that estimates the prob-\nability function as per Equation 2.3. However the model differs greatly in\nthat each of the encoder and decoder components primarily consist of alter-\nnating layers of self-attention and feed-forward layers. Self-attention relates\neach position in a single sequence to each other. Vaswani et al. (2017) for-\nmalised this in the well-known equation:\n$$Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V \\qquad(2.4)$$\nHere each input embedding is linearly projected onto query and key\nvectors $q, k \u2208 R^{d_k}$ and a value vector $v \u2208 R^{d_v}$. These are packed into matrices\nQ, K and V. $\\frac{1}{\\sqrt{d_k}}$ is used as a scaling constant. Simplifying for brevity by\nignoring positional encoding, multiple attention heads, layer normalisation\nand residual connections, the resulting weighted output is input into the\nsubsequent feed forward layer. In the encoder, the process repeats until the\nfinal feed forward layer outputs $H_i \u2208 R^{d\u00d7t}$.\nIn 2019 Devlin et al. (2019) proposed BERT, which is an implementa-\ntion of the encoder component of the original Transformer. This paper in-\ntroduced the masked Language Modelling (MLM) pretraining task in which\nthe next-word modelling task introduced in Equation 2.1 is replaced with\na bi-directional cloze-style objective (Taylor, 1953) reminiscent of that in\nthe Mikolov et al. (2013a) CBOW model. In the MLM version of the cloze"}, {"title": "", "content": "objective, tokens in the input sequence are randomly masked and the model\nis able to consider both prior and future tokens in estimating the probability\ndistribution over possible tokens that each masked token could be. In this\nthesis we utilise later variants of BERT, namely RoBERTa (Liu et al., 2019)\nand ELECTRA (Clark et al., 2020a) as described in Chapters 5 and 6.\nSeveral variations of the MLM objective have seen wide adoption in\nencoder-decoder Transformer Language Models. Of particular note, Raffel\net al. (2020) evaluate a number of MLM styles and finally propose T5, a\nfamily of models that are pretrained using the version of MLM where the\nobjective is to predict variable-length spans of text that have each been re-\nplaced by a single masking token. Similar to GPT (Radford et al., 2018),\ndescribed below, they then perform further training using supervised objec-\ntives over a variety of NLP tasks, and show that the resulting model has\nstrong performance over all of them. At about the same time Lewis et al.\n(2020a) proposed BART, a similar model to T5 except that here the MLM\npretraining objective was to predict the entire input sequence with all mask\ntokens substituted with the correct text. We use BART as our main Reason-\ning Model throughout this thesis. One difference to the original is that in our\nwork, where we include a MLM task, we substitute the T5-style objective of\npredicting the unmasked answer spans in preference to the original BART\nobjective of predicting the entire input sequence as it is less computationally\nintensive.\nAnother line of Transformer model evolution has been the emergence\nof decoder-only Transformer Language Models. Unlike the encoder-decoder\nvariants, these generally estimate the probability function using the original\nnext-word objective similar to Equation 2.1. GPT (Radford et al., 2018)\nwas the first of these. In this study they showed that pretraining on a large\ncorpus using the next-word objective followed by task-specific finetuning was\neffective in producing strong performance on individual tasks. A subsequent\nmodel, GPT2 (Radford et al., 2019), was the first to show that a sufficiently\nlarge Language Model (1.5 billion trainable parameters) trained on a large\ncorpus could become proficient on evaluation tasks in a zero-shot (unseen)\nsetting. The GPT3 study (Brown et al., 2020) showed further improvement\nwas possible by hugely scaling the model size to 175 billion parameters along"}, {"title": "2.3 Unseen Evaluation Datasets", "content": "with increasing the pretraining corpus size. This paper also introduced the\nidea of few-shot prompting where several exemplars of the problem to be\nsolved along with the query are provided to the model as a prompt. In\nChapter 6 we utilise two such decoder-only LLMs", "criteria": 1}]}