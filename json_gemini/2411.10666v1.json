{"title": "SAM Decoding: Speculative Decoding via Suffix Automaton", "authors": ["Yuxuan Hu", "Ke Wang", "Jing Zhang", "Xiaokang Zhang", "Cuiping Li", "Hong Chen"], "abstract": "Large Language Models (LLMs) have revolutionized natural language processing by unifying tasks into text generation, yet their large parameter sizes and autoregressive nature limit inference speed. SAM-Decoding addresses this by introducing a novel retrieval-based speculative decoding method that uses a suffix automaton for efficient and accurate draft generation. Unlike n-gram matching used by the existing method, SAM-Decoding finds the longest suffix match in generating text and text corpuss, achieving an average time complexity of O(1) per generation step. SAM-Decoding constructs static and dynamic suffix automatons for the text corpus and input prompts, respectively, enabling fast and precise draft generation. Meanwhile, it is designed as an approach that can be combined with existing methods, allowing SAM-Decoding to adaptively select a draft generation strategy based on the matching length, thus increasing the inference speed of the LLM. When combined with Token Recycling, evaluations show SAM-Decoding outperforms existing model-free methods, achieving a speedup of 2.27\u00d7 over autoregressive decoding on Spec-Bench. When combined with EAGLE2, it reaches a speedup of 2.49\u00d7, surpassing all current approaches. Our code is available at ours repository.", "sections": [{"title": "1. Introduction", "content": "The Transformer-based Large Language Model (Brown et al., 2020; Dubey et al., 2024; Yang et al., 2024) has become a foundation model in the field of natural language processing. Through the large language model (LLM), all natural language processing tasks are unified as text generation tasks, which greatly improves the convenience of solving natural language processing tasks. While LLM has shown good performance in various tasks and is easy to use, the large-scale parameters and text generation-based paradigm contained in LLM have brought challenges to the practical deployment, especially in terms of the inference speed of the LLM. LLM generates tokens step by step in an autoregressive manner. At each round of generation, LLM generates one new token based on the tokens that has been generated. In addition, for the GPUs used for deploy LLM, this process requires reading the huge number of parameters contained in the LLM from the High-Bandwise Memory (HBM) to the on-chip memory, but these parameters only operate on a small number of parameters corresponding to the new token, resulting in the inability to fully utilize the parallel capabilities of the modern GPU, thereby limiting the inference speed of the LLM.\nTo address the above problem, one effective strategy is to enhance the arithmetic intensity of the large language model (LLM) generation process, which involves reducing the number of generation rounds while increasing the number of tokens generated in each round. Building upon this concept, the method of speculative decoding has been introduced and has spurred a series of follow-up studies (Leviathan et al., 2023; Miao et al., 2024; Cai et al., 2024; Fu et al., 2024). Rather than generating a single token per step, these approaches employ a draft model to predict token sequence as draft. The predicted sequence are then verified against the LLM, and only the accurate tokens are adopted.\nMost speculative decoding techniques employ various neural networks, such as smaller language models (LLMs) or embedding layers, as draft models, which we categorize as model-based methods. While these model-based methods effectively enhance processing speeds, they are not without limitations. Firstly, the application of model-based speculative decoding algorithms necessitates the training of draft models, and this training must often be domain-specific, adding a layer of complexity to their practical deployment. Secondly, these preliminary models compete with the main LLMs for GPU memory resources. Additionally, utilizing a transformer architecture as a preliminary model introduces extra key-value caching requirements, further complicating GPU memory management within LLM inference systems (Kwon et al., 2023; Zheng et al., 2024).\nIn addition to model-based speculative decoding methods,\nthere is an alternative approach known as retrieval-based methods. These methods involve retrieving potential subsequent tokens from previously generated content or a text database during each generation step. Unlike model-based approaches, retrieval-based methods do not necessitate the training of draft models or the use of GPUs for inference. They are especially advantageous in tasks like summarization and code rewriting.\nAfter examining the current retrieval-based methods, we identify several limitations. Firstly, these approaches typically rely on a single retrieval source. For instance, PLD (Saxena, 2023) focuses solely on prompts, whereas REST (He et al., 2024) is limited to text databases. Secondly, the retrieval techniques employed are often inefficient; the reliance on n-gram matching restricts the maximum length of matches, which can result in the production of less optimal drafts. Lastly, while retrieval-based methods excel in tasks like summarization, they generally underperform compared to model-based methods in other areas, limiting their piratical use.\nTo address the limitations of the retrieval-based approach mentioned earlier, this paper introduces SAM-Decoding, an innovative retrieval-based speculative decoding technique. This method boasts more efficient and precise retrieval capabilities compared to traditional retrieval-based techniques. Unlike conventional approaches that rely on n-gram matching, SAM-Decoding employs a suffix automaton (SAM) to solve the longest suffix match problem for the generating text within both the historical generated text and the text database. This process yields a more accurate draft compared to n-gram matching. Moreover, during the text generation phase, the suffix automaton effectively captures the relationships between adjacent suffixes, ensuring that the average time complexity for identifying the longest suffix match at each step remains at O(1). This represents a significant improvement over the computational efficiency of n-gram matching. Meanwhile, the length of the longest suffix match serves as an indicator of the alignment between the generating text and retrieval sources. By exploiting this metric, our approach can integrate existing speculative decoding methods as an auxiliary and dynamically decide whether to generate a draft based on the matched content, or utilize an auxiliary decoding method. This adaptability leads to a notable increase in the overall speed of the text generation process.\nSpecifically, SAM-Decoding first constructs a static suffix automaton for the existing text corpus. For each request, SAM-Decoding then builds a dynamic suffix automaton based on the input prompt. The suffix automaton contains"}, {"title": "2. Background", "content": "In this section, we introduce background information relevant to this work."}, {"title": "2.1. Suffix Automaton", "content": "Suffix Automaton is an efficient data structure for representing the substring index of a given string, which allows the storage, processing, and retrieval of compressed information about all its substrings. The time complexity of constructing a suffix automaton is O(L), where L is the length of the string and it can be constructed incrementally.\nA suffix automaton contains a series of nodes and two types of state transfer edges, next and link, where a node in the automaton represents a state and corresponds to a number of substrings that have the same ending position in the string. Meanwhile, two types of transfer edges are used to add a new token to the current state (next) and to take the suffix of the substring corresponding to the current state (link), thus transferring to a new state.\nBased on the two types of transfer edges, for a progressively generated token sequence, we can find the longest suffix that matches the sequence in a suffix automaton at each step of the generation with an average O(1) time complexity."}, {"title": "2.2. Speculative Decoding", "content": "LLM generates tokens in an autoregressive manner. Assuming that the input to the model is x = (x\u2081, x\u2082,..., x\u209c), an LLM generates a new token at each generation step as follows:\nx\u209c\u208a\u2081 = Sample(LLM(x)[t]).\nSpeculative decoding improves the generation speed of LLMs by increasing the number of tokens generated per step and reducing the total number of generation steps. At each generation step, the speculative decoding method first generates a draft sequence xdraft = (x\u209c\u208a\u2081, x\u209c\u208a\u2082,..., x\u209c\u208a\u2099) containing multiple tokens based on the draft model and the input x. The drafts are then verified by the LLM, and the correct tokens Xaccept = (x\u209c\u208a\u2081, x\u209c\u208a\u2082,..., x\u209c\u208a\u2098) are accepted:\nXdraft = Draft(x),\nXaccept = Verify(LLM(x, xdraft)[t : t + m]).\nBy checking whether each token in the draft is the predicted result of the previous tokens in the LLM, we can filter out the accepted tokens. Specifically, we find the maximum number of consecutive tokens maccept such that:\nSample(LLM(x, Xdraft)[t + i - 1]) = Xdraft[i]  i \u2208 {1, 2, ..., Maccept}.\nIn the above, we assume that the draft is a sequence of tokens. Recent works on speculative decoding have also proposed the use of tree-structured drafts. Specifically, these works introduce a tree mask in the attention module to compute tree-structured drafts without changing the existing framework. Using tree-structured drafts allows the LLM to validate multiple possible token sequences simultaneously, thereby increasing the acceptance rate of the drafts."}, {"title": "3. SAM-Decoding", "content": "In this section, we introduce our proposed method, SAM-Decoding. SAM-Decoding is a retrieval-based speculative decoding method designed to address three key limitations found in existing retrieval-based speculative methods: 1. The use of less comprehensive retrieval sources. 2. The employment of less efficient retrieval methods and restrictions on matching lengths. 3. Subpar performance outside specialized domains.\nTo tackle the first two limitations, SAM-Decoding leverages suffix automaton, which significantly enhance the comprehensiveness and efficiency of the retrieval process while allowing for more flexible matching lengths. Following this, we detail how SAM-Decoding can be integrated with both model-free and model-based methods. By utilizing the precise matching information provided by the suffix automaton, our method not only overcomes the third limitation but also ensures effective performance improvements across a wide range of tasks."}, {"title": "3.1. Suffix Automaton Construction", "content": "SAM-Decoding facilitates rapid draft generation by leveraging suffix automaton. Unlike traditional methods that rely on a single retrieval source, our approach utilizes both a text corpus and the generating text itself as retrieval sources. To accommodate these dual sources, we construct two distinct types of suffix automaton: a static suffix automaton and a dynamic suffix automaton. For the text corpus, we pre-build a static suffix automaton offline. During the inference phase, state matching is performed exclusively on this static automaton. Conversely, for the generating text, we create a dynamic suffix automaton that incrementally expands as the text generation progresses. State matching is also carry out concurrently with this expansion.\nA suffix automaton can be constructed in linear time using Blumer's algorithm (Blumer et al., 1984). Since the suffix automaton is designed for a single reference string, when dealing with multiple strings in the text corpus, we concatenate them into a single long string using special symbols, such as an End-of-Sentence (EOS) token. We then construct a static suffix automaton for this concatenated string.\nAt the same time, we also made several modifications to the suffix automaton to better suit draft generation. Specifically, at each generation step, the generating text corresponds to a node in the automaton, representing the longest suffix match. We need to identify the matching position in either the text corpus or the generating text. Based on this position, we can use the subsequent tokens as potential drafts. To achieve this, at every node of the suffix automaton, we record the earliest position of all substrings corresponding to that node in the reference string. We define this position as min_endpos. By using the min_endpos, we can directly map node to the position in the reference string. The construction process of the suffix automaton is detailed in Algorithm 3 in Appendix A.1."}, {"title": "3.2. Drafting with Suffix Automaton", "content": "After constructing the suffix automaton, it can be utilized as a draft model to generate draft at each step of the generation process. Specifically, let the current input to the LLM be denoted as x = (x\u2081, x\u2082,...,x\u209c), with the predicted next token being x\u209c\u208a\u2081. We define the suffix automaton as S and its associated reference text as T. The state within the suffix automaton corresponding to the sequence x is denoted as s\u209c.\nThe transition to the next state is performed based on the newly generated token x\u209c\u208a\u2081 and the current state s\u209c:\ns\u209c\u208a\u2081 = Transfer(S, x\u209c\u208a\u2081, s\u209c).\nSubsequently, we extract n consecutive tokens from the reference text T to form a draft, using the min_endpos value stored in node corresponding to state s\u209c\u208a\u2081. If the min_endpos value in s\u209c\u208a\u2081 is Pt\u208a\u2081, then the draft d is defined as:\nd = T[P\u209c\u208a\u2081: P\u209c\u208a\u2081 + n],\nwhere d represents the generated draft and n denotes the length of the draft.\nIn practical use, we utilize two types of automaton: a static automaton for the text corpus and a dynamic automaton for generating text. To distinguish between the two, we explicitly record the attribute l\u209c\u208a\u2081 for the state s\u209c\u208a\u2081. This attribute represents the longest suffix length matched by the generating text in the automaton. This helps us determine whether the draft should be generated using the static suffix automaton or the dynamic suffix automaton. Our experimental findings indicate that drafts generated from the dynamic automaton often outperform those from the static text corpus. Consequently, we prioritize drafts from the dynamic automaton. Specifically, let l\u2081 and l\u2082 represent the matching lengths of the static automaton and the dynamic automaton, respectively. We use the draft provided by the static automaton only if l\u2081 > l\u2082 + lbias; otherwise, we use the draft provided by the dynamic automaton. Here, lbias is a predefined constant greater than 0.\nWe illustrate the state transfer process of the suffix automaton in Algorithm 1. Using amortized analysis, we can prove that the average complexity of state transfer is O(1), while the worst-case time complexity is O(L), where L represents the length of the generating text. The detailed proof is provided in Appendix A.2. Reviewing existing methods, PLD uses n-grams and performs a brute-force search for matches in the prompt, resulting in a time complexity of O(n\u00b2L). REST also employs n-grams but searches for matches in the text using suffix arrays, leading to a time complexity of O(n\u00b2 log L). Here, n represents the predefined maximum matching length, and L represents the length of the prompt and the total text in the corpus, respectively. In comparison, our proposed model (SAM-Decoding) has a lower time complexity. Additionally, SAM-Decoding can find the exact longest suffix match without any limit on the matching length. This means that SAM-Decoding can generate drafts both faster and more accurately."}, {"title": "3.3. Update of Suffix Automaton", "content": "After the draft is generated, we verify the draft using the large language model (LLM) and accept the correct to-"}, {"title": "3.4. Adaptive Draft Selection", "content": "A straightforward idea is that the length of the suffix match can indicate the quality of the draft produced by the automaton. A longer match suggests that more tokens from the draft are likely to be acceptable, whereas a shorter match implies that the draft contains less useful information. Consider that besides the retrieval-based speculative decoding approach, there exist various alternative speculative decoding techniques. Consequently, if the retrieval-based method fails to produce a satisfactory draft, we can resort to these alternatives to enhance the draft's effectiveness.\nTo implement this, we concurrently employ an additional speculative decoding technique alongside the suffix automaton. During each iteration of the generation process, we adaptively select between the draft offered by the automaton and that provided by the supplementary speculative decoding method, based on the match length of the generated text within the automaton. The adaptive draft selection procedure is detailed in Algorithm 2, where threshold denotes a predetermined constant that determines whether to utilize the automaton or the auxiliary speculative decoding for draft generation. In our study, we incorporate two cutting-edge speculative decoding methods as auxiliary options: the model-free Token Recycling and the model-based EAGLE. Among them, Token Recycling maintains an adjacency list of the top k probable next tokens for each token. During each generation step, a draft tree is built using this adjacency list via breadth-first search, with the list being continuously updated according to the latest generated tokens. Conversely, EAGLE constructs a draft tree in each generation cycle by leveraging a Transformer decoder layer that integrates the hidden states output by the LLM."}, {"title": "4. Experiments", "content": "In this section, we first introduce our experimental setup, then present the experimental results, and finally present the ablation experiments.\nModels and tasks. We conducted experiments on Vicuna-7B-v1.3 (Zheng et al., 2023). We evaluated SAM-Decoding on Spec-Bench (Xia et al., 2024), HumanEval (Chen et al., 2021), MBPP (Austin et al., 2021), and HARGID (Kamalloo et al., 2023). Spec-Bench is a comprehensive benchmark designed for assessing Speculative Decoding methods across diverse scenarios. It is based on six commonly used datasets, MT-Bench (Zheng et al., 2023), WMT14 DE-EN, CNN/Daily Mail (Nallapati et al., 2016), Natural Question (Kwiatkowski et al., 2019), GSM8K (Cobbe et al., 2021), and DPR (Karpukhin et al., 2020), including six aspects: Multi-turn Conversation, Translation, Summarization, Question Answering, Mathmatical Reasoning, and Retrieval-augmented Generation. In addition, MBPP, HumanEval, and HARGID are used to evaluate the speed of decoding methods in code generation tasks and contextual question answering tasks, respectively.\nBaselines. We considered the following baseline methods, including the model-based method EAGLE (Li et al., 2024a;b), the model-free methods Token Recycling (Luo et al., 2024) and Lookahead Decoding (Fu et al., 2024), and the retrieval-based methods PLD (Saxena, 2023) and REST (He et al., 2024).\nMetrics. We evaluated speculative decoding methods from the following aspects (Li et al., 2024a)\n\u2022 Speedup Ratio: The wall-time speedup ratio of speculative decoding methods compared to autoregressive generation methods.\n\u2022 Mean Accepted Tokens: The average number of tokens accepted per generation step.\n\u2022 Throughput: The average number of tokens generated per second.\nExperiment setup. We conducted experiments on a server equipped with a 20-core CPU and a single NVIDIA RTX A6000 GPU (48GB). The experiments were implemented using PyTorch 2.3.0 and CUDA 12.1. For the models, we used the float16 data type and applied greedy decoding with a batch size of 1. Regarding hyperparameters, unless otherwise specified, lbias and Ithreshold were set to 5, and the size of the draft generated by the automaton was set to 40 tokens. For the auxiliary speculative decoding algorithms, we used the default configurations as described in their respective original papers. One exception was made for Token Recycling: instead of the draft tree structure used in the original paper, we employed a draft tree with a depth of 6 and containing 60 nodes.\nFor SAM-Decoding, we constructed a static suffix automaton based on the Stanford-alpaca, python-code-instruction-18k, and GSK8k. To enhance our model, we incorporated two auxiliary approaches: the model-free method Token Recycling and the model-based method EAGLE. Here, SAM-Decoding[T], SAM-Decoding[E], and SAM-Decoding[E2] denote the combinations of our base model with Token Recycling, EAGLE, and EAGLE2, respectively.\nExperiment Results. Experiment results on Spec-Bench are shown in Table 1. In the Spec-Bench, which consists of six tasks, multi-turn conversation, summarization, and"}, {"title": "Ablation Experiments", "content": "To further understand the contributions of various components of SAM-Decoding and the influence of different hyperparameters on inference speed, we conducted a series of ablation studies. Unless otherwise stated, these experiments were carried out on Spec-Bench using the vicuna-7B-v1.3 model, with SAM-Decoding[T] serving as the decoding strategy.\nFirstly, we examined the effects of lbias and Ithreshold on inference speed through a grid search. These parameters control the preference for generating text from the generating text during retrieval and the preference for using retrieval results over the draft model when creating drafts. The findings are summarized in Figure 2. As shown, both the mean accepted tokens (MAT) and the speedup ratio increase with Ibias and Ithreshold up to a point. Beyond a value of 5 for both parameters, these metrics start to decline.\nAdditionally, we investigated how the draft size utilized by SAM-Decoding affects inference speed. Figure 3 illustrates the mean accepted tokens and the inference speed of SAM-Decoding[T] at varying draft sizes. The data indicate that as the draft size grows, the mean number of accepted tokens and inference speed initially rise. However, once the draft size surpasses 40, these metrics begin to decline, and the decrease becomes more pronounced when the draft size reaches 70.\nFinally, we investigated the impact of different modules within SAM-Decoding on inference speed. SAM-Decoding comprises three draft generation modules: the static suffix automaton, the dynamic suffix automaton, and the auxiliary speculative decoding module. We measured the inference speed of SAM-Decoding after removing each of these three modules individually. The results are presented in Table 3. From the experimental results, it is clear that each module contributes to the acceleration of the decoding process. Notably, the dynamic suffix automaton has a significantly greater impact compared to the static suffix automaton. This suggests that, in many cases, generating drafts from the dynamic context is more effective than retrieving drafts from a pre-existing text corpus. Additionally, since not all scenarios are well-suited for retrieval-based methods, the auxiliary speculative decoding module plays a crucial role in enhancing the overall performance of SAM-Decoding."}, {"title": "5. Related Work", "content": "Speculative Decoding. Speculative decoding is an approach that can significantly speed up large language models (LLMs) without compromising the quality of their outputs. This is achieved by increasing the number of tokens produced in each inference step while simultaneously decreasing the total number of inference steps required. The majority of speculative decoding techniques rely on smaller neural networks to create initial drafts during the inference process. These techniques are referred to as model-based speculative decoding methods. Early implementations of model-based speculative decoding, such as those described in Speculative Decoding (Leviathan et al., 2023), primarily focused on generating draft sequences using pre-existing, smaller-scale LLMs. Subsequently, advancements like Medusa (Cai et al., 2024) and SpecInfer (Miao et al., 2024) introduced tree-based speculative methods and initiated the development of dedicated draft models tailored for speculative decoding. For instance, Medusa developed a set of specialized decoding heads designed to serve as draft models. More recently, EAGLE (Li et al., 2024a;b) highlighted the significance of incorporating the hidden state of the model's output into the speculative decoding process. In line with this insight, EAGLE trained a draft model that integrates the hidden state information from the model's output, leading to more efficient acceleration across a range of tasks.\nIn contrast to model-based methods, certain approaches focus on generating drafts by leveraging the model's recent prediction outcomes, such as the output from the decoding head. Examples of such methods include Lookahead Decoding (Fu et al., 2024) and Token Recycling (Luo et al., 2024). Lookahead Decoding keeps track of the n-grams of the generated text, whereas Token Recycling maintains the top k probable next tokens for each tokens.\nMoreover, some strategies generate drafts directly through the retrieval of previously generated texts or text corpora, utilizing n-gram matching. Notable among these are PLD (Saxena, 2023) and REST (He et al., 2024). Our proposed SAM-Decoding method also employs retrieval for draft generation. However, it stands out due to its superior speed and accuracy, and it can be seamlessly integrated with other speculative decoding techniques.\nAdditionally, beyond the aforementioned methods, research also conducted on speculative decoding that relies either on the model itself (Kou et al., 2024) or on sub-models within the larger architecture (Elhoushi et al., 2024).\nEfficient LLM Architecture. There is also work to improve the model's inference speed from the perspective of model structure. This part of the work includes model distillation, quantization and pruning. Model distillation (Sreenivas et al., 2024; Muralidharan et al., 2024) distills the knowledge of a large model into a small model, thereby speeding up inference while maintaining the model's performance. Quantization (Frantar et al., 2022; Xiao et al., 2023; Lin et al., 2024; Liu et al., 2024; Ashkboos et al., 2024b) reduces the number of bits required to store parameters and reduces the data transmission time from HBM to on-chip memory during inference, thereby achieving effective inference acceleration. Pruning (Frantar & Alistarh, 2023; Ashkboos et al., 2024a; Men et al., 2024; Chen et al., 2024; Hu et al., 2024; Sun et al., 2024; Zhang et al., 2024) is used to remove unimportant parameters in the model. For structured pruning, it can be combined with model distillation to train efficient small models, while semi-structured pruning can reduce the model's memory access and computing overhead and improve the inference speed by combining special hardware, i.e., sparse tensor core."}, {"title": "6. Conclusion", "content": "In this work, we propose SAM-Decoding, an speculative decoding method via suffix automatons constructed from both generated text and text corpus. SAM-Decoding can efficiently retrieve drafts from retrieval sources, thereby accelerating inference. SAM-Decoding is also designed to seamlessly integrate with existing methods. Consequently, in scenarios where retrieval is not feasible, SAM-Decodingcan adaptively switch to alternative methods for draft generation. Experimental results demonstrate that when combined with state-of-the-art techniques, SAM-Decodingcan significantly enhance performance in multi-turn conversation, summarization, and retrieval-augmented generation tasks."}, {"title": "A. Suffix Automaton", "content": ""}, {"title": "A.1. Construction Process of Suffix Automaton", "content": ""}, {"title": "A.2. Time Complexity of Suffix Automaton", "content": "Consider a suffix automaton S with an initial state s\u2080, which corresponds to the root node of the automaton (representing the empty string). Suppose that state s\u2080 undergoes transitions through a sequence of L tokens x = (x\u2081, x\u2082,...,x\u2097):\ns\u1d62 = Transfer(S, x\u1d62, s\u1d62\u208b\u2081),  i \u2208 {1, 2, ..., L}.\nWe aim to demonstrate that the average time complexity of each state transition is O(1), while the worst-case time complexity is O(L).\nFirst, let us define the matching length associated with state s\u1d62 as l\u1d62. Given that each state transition can increase the matching length by at most 1, it follows that 0 \u2264 l\u1d62 < i. Next, we introduce the concept of energy \u03c6 for each state s\u1d62, defined as \u03c6(s\u1d62) = l\u1d62. Let c\u1d62 represent the time cost of the i-th state transition. We then define the amortized cost \u0109\u1d62 as:\n\u0109\u1d62 = c\u1d62 + \u03c6(s\u1d62) \u2013 \u03c6(s\u1d62\u208b\u2081).\nWe can now express the total amortized cost over all transitions as:\n\u2211\u1d62\u208c\u2081\u1d38 \u0109\u1d62 = \u2211\u1d62\u208c\u2081\u1d38 (c\u1d62 + \u03c6(s\u1d62) \u2013 \u03c6(s\u1d62\u208b\u2081)) = \u2211\u1d62\u208c\u2081\u1d38 c\u1d62 + \u03c6(s\u2097) \u2013 \u03c6(s\u2080).\nSince \u03c6(s\u1d62) \u2265 0 and \u03c6(s\u2080) = 0, it follows that:\n\u2211\u1d62\u208c\u2081\u1d38 \u0109\u1d62 \u2265 \u2211\u1d62\u208c\u2081\u1d38 c\u1d62.\nNext, we analyze the upper bound of \u0109\u1d62. Each state transition involves moving through the link edge zero or more times, followed by a move through the next edge. Transitioning via the link edge incurs a cost of 1 but decreases the potential by at least 1. Conversely, transitioning via the next edge also incurs a cost of 1 but increases the potential by 1. Consequently, the amortized cost \u0109\u1d62 is bounded above by 2, leading to:\n\u2211\u1d62\u208c\u2081\u1d38 \u0109\u1d62 \u2264 2L.\nThus, the average time complexity of state transitions is:\n(\u2211\u1d62\u208c\u2081\u1d38 \u0109\u1d62) / L \u2264 2L/L = 2,\nwhich is O(1). In the worst case, a single operation may require up to l\u1d62 transitions through the link edge, followed by one transition through the next edge, resulting in a worst-case time complexity of O(L)."}]}