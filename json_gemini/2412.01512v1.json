{"title": "ARTBRAIN: AN EXPLAINABLE END-TO-END TOOLKIT FOR CLASSIFICATION AND ATTRIBUTION OF AI-GENERATED ART AND STYLE", "authors": ["Ravidu Suien Rammuni Silva", "Ahmad Lotfi", "Isibor Kennedy Ihianle", "Golnaz Shahtahmassebi", "Jordan J. Bird"], "abstract": "Recently, the quality of artworks generated using Artificial Intelligence (AI) has increased signif-\nicantly, resulting in growing difficulties in detecting synthetic artworks. However, limited studies\nhave been conducted on identifying the authenticity of synthetic artworks and their source. This\npaper introduces AI-ArtBench, a dataset featuring 185,015 artistic images across 10 art styles. It\nincludes 125,015 AI-generated images and 60,000 pieces of human-created artwork. This paper also\noutlines a method to accurately detect AI-generated images and trace them to their source model. This\nwork proposes a novel Convolutional Neural Network model based on the ConvNeXt model called\nAttentionConvNeXt. AttentionConvNeXt was implemented and trained to differentiate between the\nsource of the artwork and its style with an F1-Score of 0.869. The accuracy of attribution to the\ngenerative model reaches 0.999. To combine the scientific contributions arising from this study, a\nweb-based application named ArtBrain was developed to enable both technical and non-technical\nusers to interact with the model. Finally, this study presents the results of an Artistic Turing Test con-\nducted with 50 participants. The findings reveal that humans could identify AI-generated images with\nan accuracy of approximately 58%, while the model itself achieved a significantly higher accuracy of\naround 99%.", "sections": [{"title": "1 Introduction", "content": "A key advancement of recent generative AI models is their ability to produce visually appealing and realistic art [1, 2, 3].\nGenerative AI models have become very advanced; an art generated using the model won a fine art competition meant\nfor human artists [4]. A recent example is the 'Sony World Photography Award' [5], which was won by an AI-generated\nimage. Taking into account these recent developments, the future of the internet can be envisioned to be filled with\nsynthetic artwork generated using Image Generative Models (IGMs) and other synthetic media content. The ownership\nand responsibility aspects of this synthetic content are still an ongoing debate and a heavily understudied topic. Due to\nthis, the detection of synthetic data is crucial, especially for art, where due credit should always be given to the artist\nand the artist's skill.\nIGMs were inspired by the ideas published in [6]. One of the first generative models published was Generative\nAdversarial Models (GAN), a type of IGM that generates images using two competing models [7]. Variational Auto\nEncoder (VAE) has been explored for image generation using an Encoder-Decoder-based model setup [8]. The encoder\nencodes the input image in a vector-based form, while the decoder uses the vector to recreate the image. GANs rapidly\nimproved with new variants including StyleGAN [9], generating increasingly more realistic images."}, {"title": null, "content": "Diffusion models [10] are a type of IGM that generates images by learning to destroy visual information and trying\nto recreate them. Later, it was improved to exceed the potentiality and capabilities of GANs [11, 12]. The diffusion\nmodels were based on the mathematical stochastic model called Markov Chains, which explains the probabilistic\nbackground of a linked chain of events. These types of models have significantly improved the generation quality of\nsynthetic images, including photorealistic images [13] and artistic images [14]. Standard Diffusion [2] and DALL-E\n[3] are two of the most popular Diffusion-based Models, especially for fine art image generation. These models are\nalso called text-to-image models because text prompts can be provided as input explaining details of the expected\noutput image. The capabilities of diffusion models have recently become so advanced that it has become increasingly\nhard to differentiate between AI-generated art and human-drawn art by only using a digital image, creating a startling\ndebate between what is real and what is not. These abilities of generative models also pose a serious issue regarding the\noriginality of digitalised art, as authors in [15] ask, \u201cWho Gets Credit for AI-Generated Art?\u201d.\nThis study does not seek to provide a definitive answer to the mentioned question but aims to offer an effective toolkit\nfor the broader research community. This toolkit will assist in making informed decisions about artworks and help\nevaluate their authenticity. This toolkit includes a web-based application integrated with a model developed as a part of\nthis study and an AI-generated artistic dataset of 10 major art styles and 180,000+ samples. Furthermore, the study\ncompares human abilities to distinguish between human and AI-generated art with the AI's performance in this task.\nThe main scientific contributions of this work are as follows;\n1. A concise review of current research on AI-driven image generation and existing studies focused on detecting\nsuch images.\n2. Introducing AI-ArtBench - a novel AI artistic dataset, which includes samples generated using two diffusion-\nbased models. This is the only dataset of its kind available as of this document submission.\n3. Novel CNN-based architecture, AttentionConvNeXt, based on ConvNeXt [16] and Attention mechanism [17].\n4. Web-based applications that can be used to verify the authenticity of artistic images via the developed models.\n5. Evaluate the human ability to identify AI-generated art images through an 'Artistic Turing Test'. This is the\nfirst Artistic Turing Test conducted using complete AI art images generated by diffusion models.\nThe remainder of this paper is initiated with a review of the existing work in Section 2 concisely discussing the existing\nstatus of generation, detection and attribution of AI art. Then in Sections 3 and 4 the theories and techniques used\nin the implementation of the model and developed web-based application are explained followed by the evaluation\nresults of the models in Section 5. The paper concludes with a discussion of the limitations of the current study and an\nexploration of future work."}, {"title": "2 Related Work", "content": "This section reviews the existing work on AI art generation and detection while investigating its performance. The\nsection also presents major datasets that can potentially be used to train models for tasks of differentiating AI art from\nHuman art."}, {"title": "2.1 Using AI to Analyse Art", "content": "Many artistic styles have been identified throughout history that have been created by various artists. These are often\ncalled art movements. Art movements represent the artistic philosophies and ideas of contemporary artists who similarly\nsaw art. Cubism [18], Surrealism [19], Impressionism [20], Romanticism [21], Minimalism [22] and Expressionism\n[23] are some examples of these styles. Most of these artistic styles were pioneered in the 19th and 20th centuries,\npredominantly based in the European region, except for movements like Ukiyo-e [24] which was formed in Japan.\nDatasets like [25, 26, 27, 28, 29, 30, 31] which includes artworks with various styles mentioned above, can be used\nto train AI models for a wide range of tasks, from classification to artistic emotion analysis. Convolutional Neural\nNetworks (CNNs) [32] are a well-suited model for this task mainly because of their ability to extract spatial features on\ntheir own.\nCNNs made image understanding through AI methods very effective. Authors LeCun et al. [32] also present a method\nto produce heatmap-like gradient maps, which makes the results and the knowledge of CNNs explainable. Later,\nSelvaraju et al. [33] further improved the gradient maps making them more comprehensive. However, all these gradient\nmap generation methods focus only on a single class. This nature of singular class representation makes it less suitable\nfor understanding an artwork. Fused Multi-class Gradient-weighted Class Activation Map (FM-G-CAM) [34] attempts\nto overcome this limitation by producing a saliency map that represents multiple top-predicted classes. FM-G-CAM is"}, {"title": "2.2 Can AI Generate art?", "content": "The use of computer software to generate visual images was initiated long ago [35]. However, computers could not\ncreate art similar to human paintings until recently. This was possible due to three main factors: Large Scale Datasets,\nComputer Hardware Advancements, and Increased research interest in generative AI models.\nRecently, deep neural networks have been used to generate painting-like images by introducing GANs [7, 36, 9] and\nVAEs [8]. Neural Style Transfer [37] presents a technique to manipulate features learnt by CNNs to transfer 'style' and\n'content' from a given image to another. This could turn any image into an artwork created by a selected artist or style.\nSurpassing GANs, Diffusion models [10] marked a significant shift in AI art generation. Although initial results were\nless impressive compared to GANs, subsequent enhancements [11, 12] enabled diffusion models to perform better than\nGANs. Diffusion models have also become particularly advanced in generating synthetic art images [14]. Standard\nDiffusion [2] and DALL-E [3] are two of the most popular and high-performing diffusion models. Art generated using\nthese models is very realistic and convincing and sometimes very hard to differentiate from real human-drawn art.\nAuthors Song and Ermon [38] published the Noise-Conditioned Score Network (NCSN), which was an adapted version\nof the original diffusion model. NCSN mainly consists of Score Matching [39] and Langevin Dynamics [40], a\nmathematical system for modelling molecular systems. The Denoising Diffusion Probabilistic Model (DDPM) [11],\nunlike NCSN, takes a series of steps, adding noise into the original images stepwise and learning how to remove the\nnoise at each step. Then, finally, with a study published by OpenAI [12], GANs were surpassed by diffusion-based\nmodels. Rombach et al. [2] take a further step by conditioning the forward and backward diffusion processes using\ntext embeddings, introducing an attention-based text-to-image diffusion model. This allowed the generation of images\nusing textual prompts. The Mask Diffusion Model [41] is one of the recent studies that introduced transformer and\nattention-inspired variants of diffusion models. The model shows the highest FID score on the 256 \u00d7 256 version of\nImageNet, among other generative models.\nOpenAI recently published a new kind of generative model, the Consistency Model [42], which was inspired by the\nconcepts in diffusion models. Unlike diffusion models, the Consistency Model does all the steps in the forward and\nreverse diffusion process at once via a single-shot methodology. However, this means the model will require more\nparallel processing power.\nStable Diffusion XL 1.0 is another new development introduced to the original Stable Diffusion model. The new model\narchitecture comes with a 3.5 billion parameter base model and a 6.6 billion parameter refined model that enlarges the\nlatent image generated by the base model. Compared to the previous versions, the model boasts the capability to create\nvery high-quality photorealism-level images with simple and shorter prompts.\nTaking a completely different approach, Refik Anadol \u00b9 produces a new kind of art using generative image models\nand real-world data. He visualises real-world datasets in the form of art. The artworks are also a form of a dynamic\nNon-fungible Token (NFT), adding value to it."}, {"title": "2.3 Who owns AI Art?", "content": "Prior to verifying the authenticity of an artwork, detecting whether the given art is AI-generated is essential, to prevent\nmisusing AI-generated art specifically in situations like competitions [4, 5]. There are a few predominant ways of\ndetecting AI-generated art: Analysing the metadata of the image, Comparing features derived from the image, Training\na CNN to classify the real and synthetic images, and Performing a Reverse Image Search using the provided image.\nIn the existing literature, virtually no published study can be found that presents a method to detect AI-generated art.\nHowever, similar tasks have been carried out in a few different domains. The study [43] was done on the detection\nof synthetic human face images. The paper also presents a dataset that includes fake and real images. The study was\ndone in three steps: Data Collection, Manipulation and Detection. Although the dataset is acceptably balanced in terms\nof gender, diversity factors like skin tones have not been considered for the approach. Hence, the presented accuracy\nmight not be consistent for unseen data distributions collected from different ethnicities. The study only used synthetic\nimages created by GANs.\nThe study [44] uses synthetic images generated by both GANs and diffusion-based models to create a detection\nmechanism for synthetic images. The mechanism involves a novel approach using wavelet transformation. The study"}, {"title": "3 Theory and Design", "content": "This section presents the theoretical background and the design of the utilised and developed components of this\nresearch. The developed components, like models, are visualised, and their mathematical nature is also explained in\ndetail in this section."}, {"title": "3.1 Data Generation using Diffusion Models", "content": "To realise the research objectives, a dataset of AI-generated art was created in this study using two diffusion-based\nmodels: Standard Diffusion and Latent Diffusion [2]. Standard Diffusion is based on Latent Diffusion and generally\nproduces realistic, higher-quality images. The core equation behind diffusion models is as follows:\n$X_t = \\sqrt{\\alpha_t} x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon,$"}, {"title": null, "content": "where, $x_t$, represents the input image at a time $t$ and $\\alpha_t$ represents the learned transformation matrix that transforms\nthe original image $x_0$ into $x_t$. Building upon this theory, Stable Diffusion and Latent Diffusion models condition this\nprocess with textual inputs. T is generally chosen as 50 in Latent Diffusion models.\nThe loss function L used in the training process of the diffusion models is as follows:\n$L = E_{z,\\epsilon \\sim N(0,1),t} [||\\epsilon - \\epsilon_\\theta (z_t, t)||_2^2],$"}, {"title": null, "content": "where, $z = E(x)$. E is the encoder function that encodes the input x into its latent representation z. e represents the\nexpected noisy image at timestep t and the function $e_\\theta (z_t, t)$ denotes the predicted noise given the latent representation\n$z_t$ of input $x_t$ at timestep t."}, {"title": "3.2 Detection Model Design", "content": "The novel model developed in this study, AttentionConvNeXt, is primarily based on Convolutional Neural Networks\n(CNN) [32] and uses ConvNeXt model [16] as its backbone. The model was designed to improve the ConvNeXt model\nby introducing an attention mechanism using the concepts of SE-Blocks [46]. The core idea behind the design is to\nget the low, mid and high-level feature maps effectively utilised for the final prediction. Figure 1 shows the high-level\nmodel architecture of the AttentionConvNeXt model.\n'Low feature block' and 'Mid feature block' are pre-trained using ImageNet [47] dataset and are frozen during the\nmodel's training process retaining low-level features extracted from the ImageNet dataset. Unlike Low and Mid-feature\nblocks, the 'High feature block' is trainable and aims to extract higher-level features of the training images. This block"}, {"title": "3.3 Attention Module", "content": "The Attention Module in Figure 2 uses the concept of SE-Block [46] and re-purposes it similarly to the author's research\nParallelXNet [48]. Instead of a single block of feature maps, it focuses on multiple blocks of feature maps taken from"}, {"title": null, "content": "different depths of the backbone CNN network, ensuring that 'attention' is given to the mid-feature and end-feature\nmaps. H \u00d7 W \u00d7 C is the size of the concatenated block $Z_{concat}$ where $C = C_{low} + C_{mid}+C_{high}$. $C_{low}, C_{mid}, C_{high}$\nrespectively represents the channel count of the low, mid, and high feature blocks inputted into the Attention Module,\nas explained in Detection Model Design Section 3.2:\n$Z_{concat} \\in R^{H\\times W \\times C}$\n$\\bar y_c = \\frac{1}{I\\times J} \\sum_i^I\\sum_j^J z_{i,j,c}, where \\bar y_c \\in R^{I\\times J},$\n$Y = [\\bar y_1 ... \\bar y_c ...], where Y \\in R^{I\\times J\\times C},$\nwhere, $\\bar y_c$, represents the averaged value for each channel c. Then, the channel importance, w, is calculated as in the\nEquation 6 by passing Y through two fully connected layers where training parameters, $W_1$ and $W_2$, respectively:\n$w = sigmoid [W_2 (ReLU (W_1Y))], where w \\in R^{1\\times C}.$\nIn Equation 6, $W_1\\in R^{\\mu\\times C}, W_2\\in R^{C\\times \\mu}$, where, $\\mu$, is the feature reduction factor that \u2018squeezes' the feature impor-\ntance matrix that exists in the first FCN of the SE-Block. Then, the channel importance is used to weight the feature\nmaps (channels) of the original feature map block as in Equation 7. Note that the FCN layers here do not include a bias\nparameter:\n$Z^{'} = (w) (Z_{concat}), where Z^{'} \\in R^{I\\times J},$\n$Z_{weighted} = [Z^{'}_1 ... Z^{'}_{C}]^T, where Z_{weighted} \\in R^{I\\times J\\times C^{'}},$\n$Z_{weighted}$ represents the final output of the Attention Module, which is then connected to the Classifier Block."}, {"title": "4 Implementation", "content": "This section discusses the implementation decisions and the techniques used in developing the prototypes related to this\nstudy including the novel AI-art dataset. The three predominant implementation tasks carried out in this project are:\nData Generation, CNN Model Development, and ArtBrain Application development."}, {"title": "4.1 Novel Dataset: AI-ArtBench", "content": "This study presents a novel dataset containing AI-generated art images related to 10 major art styles: AI-ArtBench. The\nsamples were created using two main diffusion-based models with official pre-trained weights: Latent Diffusion and\nStable Diffusion. Using this, together with the real art dataset ArtBench, a new dataset of 185,015 images, including\n125,015 AI-generated images, AI-ArtBench7, was compiled. The summary of the compiled dataset is provided in Table\n1 and Figure 3. The dataset is split into two splits: Train and Test. Train split is to be used for the training purposes of\nthe model, whereas the Test split is for evaluation purposes. The testing split is perfectly balanced even among the\nsubclasses under each source, allowing it to be used as an unbiased validation set in evaluating the models. Each split is\ndivided into 30 sub-classes representing the ten art styles for each source. Appendix B contains the image count per\neach class. A sample from the dataset is displayed in Figure 4."}, {"title": null, "content": "The following sections contain the configurations, prompts, and parameters set in the image generation for each model."}, {"title": null, "content": "Latent Diffusion The same prompt was used for generating all the art images, treating all the artistic styles equally.\nNo further conditioning was added to the prompts, allowing the generated images to be generalisable. Each sample\ngeneration iteration is done using a generated random seed between 0 and 999999999, and the seed is stored in the\nfilenames of the images. Configurations of the Latent diffusion model used to generate the images are as follows:\nPrompt: [\"A painting in  art style\"], Model Configuration: [Diffusion Steps - 50, Image Size - 256 \u00d7 256 |\nType: JPEG, Parallel Samples - 4, Diversity Scale - 5.0, CLIP Model - ViT-B-32, Sampler - PLMS]\nStable Diffusion Similarly to the Latent Diffusion model, the same prompt was used for generating all the art images,\ntreating all the artistic styles equally. However, since Stable Diffusion v2-18 allows inputting a negative prompt, this was\nused to remove the photo frame that was observed in most of the generated AI art images in the pre-tests. xFormers\nwas used for efficient image generation. Each sample generation is done using a generated random seed between 0 and\n999999999, and the seed is stored in the filenames of the images. An identical prompt to the latent diffusion model was\nused with a the following Negative Prompt: [\"photo frame\"]. Model configurations were set as follows: [Diffusion\nSteps - 50, Image Size: 768x768 | Type: JPEG, Parallel Samples - 4, Guidance Scale - 9, Sampler - DPMS Multistep\nScheduler]\nIntel Xenon 2.0 GHz with 13 GB of RAM and Nvidia Tesla T4 with 16GB VRAM were used for the sample generation.\nSamples were named as: [<class index>-<generation seed>-<random number>.jpg]"}, {"title": "4.2 CNN Model Development", "content": "Mainly, two CNN models were implemented and trained in this study: AttentionConvNeXt, explained in Section 3.2\nand MobileNetV2 introduced in [49]. AttentionConvNeXt was developed, focussing on accuracy, and MobileNetV2\nwas developed, focussing on performance and on-device ML. Kaggle Notebooks with Intel Xenon 2.0 GHz, 13 GB\nRAM, and Nvidia Tesla P100 GPU with a VRAM of 16 GB were used for all the model training.\nAttentionConvNeXt is developed using ConvNeXt as the primary network. The following sections contain technologies\nused, training configurations, and other technical details of the model."}, {"title": null, "content": "Preprocessing The images of the compiled dataset were preprocessed according to the parameters of the ImageNet\ndataset[47]. Figure 5 compares the visual appearance after preprocessing the image.\nModel Development and training The explained model in Section 3.2 was developed using PyTorch and the code is\navailable open-source10. PyTorch's recommended object-oriented design was followed in all the implementations. The\nimplementation was done in two PyTorch modules: AttentionModule (Section 3.3) and AttentionConvNeXt (Section\n3.2). Firstly, all the feature map blocks were concatenated along the channel axis. Then, the block was traversed\nthrough an FCN, retrieving the channel importance. Then, it was used to weight the original feature map blocks in\nAttentionConvNeXt. Then, the weighted new feature block is inputted to the classifier FCN. Classifier FCN mainly\ncontains two linear layers, with dropout layers in between to prevent overfitting. Hyperparameters were selected after a\nseries of optimisation experiments. Batch size was set to 32 and the model was trained for 18 epochs until the learning"}, {"title": "5 Evaluation", "content": "This section evaluates all the prototypes built as part of this study. In this section, the accuracy of the developed models\nwill be evaluated and compared with the current state-of-the-art. A consistent testing set was used for all the tests\nconducted in this section. The same set used for validation is used as the test set. However, the batch size was set to 1\nin the inferencing to ensure the tests simulate the real-world environment. Each sample was preprocessed identically\nto the training introduced in Section 4.2. Accuracy is measured in terms of both the art style and the source model.\nF1-Score was used as a measure of accuracy for the classes."}, {"title": "5.1 Model Accuracy: Classification", "content": "Table 2 presents the classification accuracy of the trained models comparatively to the current state-of-the-art. The\nproposed approach maintains the best F1-score for 28 classes out of 30, making it the most accurate model. MobileNet\nV2 model also maintains a reasonable accuracy of 84% while having the ability to process samples comparatively faster.\nRegarding class F1-scores, Human-Realism has the lowest score, and SD-Ukiyo-e has the highest score in both the\ntrained models. This could be due to the significant visual differences in Ukiyo-e style and the variety of the visual\nstructures exist in Human-Realism style."}, {"title": "5.2 Model Accuracy: Attribution", "content": "Table 3 presents the attribution accuracy of the trained models. Attribution scores of main classes are derived by\naveraging their subclasses' prediction scores. The scores represent how well the models can attribute the images\ngenerated using diffusion models. Both models are highly accurate in attribution."}, {"title": "5.3 Ablation Tests", "content": "A series of ablation tests were conducted to measure the effectiveness of the techniques used. The results in Table\n4 shows that using ImageNet weights significantly improves the model's overall accuracy. This also implies the\nrequirement for more training data. The newly introduced architecture AttentionConvNeXt increases the accuracy by\naround 3% overall. It also maintains the highest F1-score on 29 of 30 total classes across all three models. Further,\nAttentionConvNeXt with transfer learning scores 10% better in some classes when compared to ConvNeXt with transfer\nlearning. The accuracy improvement shows the benefit of using the Attention Module in the AttentionConvNeXt Model."}, {"title": "5.4 Quantitative Evaluation", "content": "This section evaluates the system based on the qualitative factors related to AI art detection and attribution."}, {"title": "5.4.1 ArtBrain on popular artworks", "content": "The following section presents the ArtBrain model prediction results on a few artworks the model has never seen.\n\"Mona Lisa\" by Leonardo Da Vinci is one of the most famous artworks ever created by an artist shown in Figure A-1 of\nthe Appendix. It was created in 1503 and is a well-known Renaissance-style artwork, and the model accurately detects\nit as 'Renaissance - Human' with 85% confidence. Also, as correctly shown in the heatmap, a few spots of the image\nshow Baroque-style visual structures. \u201cStarry Night\" by post-impressionist artist Vincent Van Gogh is used in Figure\nA-2 of the Appendix. The model correctly identified it, and some features that bring expressionist characteristics were\nalso accurately highlighted.\nA controversial artwork by Jason Michael Allen called \u201cSpace Opera\u201d, which won an art competition14 that was meant\nfor human artists, was used on the ArtBrain model as in Figure A-3 of the Appendix. The model identifies it as\nRomanticism art created using Standard Diffusion. The art used here was generated using Midjourney15, , an application\nwith a Diffusion-based model derived from Stable Diffusion. Hence, the ArtBrain model is successful in this scenario\nas well. This provides an indication of the models performance against the unseen data distributions. However, further\nresearch is needed on the capability of the model on identifying samples generated from generative models other than\nthe ones used in the training."}, {"title": "5.4.2 Effect of image contrast on the prediction", "content": "In this test, as in Figure 6, the parameters of digital images were investigated on their the effect to the model predictions.\nThe chosen parameter was the 'Contrast' of the image and it can be concluded that changes to the contrast can\nsignificantly impact the model predictions."}, {"title": "5.4.3 Predictions on replicated images", "content": "For this test, as in Figure 7, \"Starry Nights\" is compared to its replicated version. The replicated version on the right\nwas created by capturing the original version of the image displayed on a 2K laptop screen. The capture was taken with\na Samsung Galaxy Note 20 mobile phone with a 1.8f aperture, 9MP quality, and ISO 200 conditions. As it is evident\nfrom the prediction results, even though both images are very similar to those of the human eye, the prediction results\nhave changed drastically."}, {"title": "5.5 Artistic Turing Test", "content": "In this study, \"Artistic Turing Test\" was conducted resting the practical effectiveness of the system. The test was based\non the original \"Turing Test\" [50], built to evaluate general machine intelligence. Authors Daniele et al. [51] have\nperformed a similar test but only using brush strokes rather than whole images. This test in this study is the first test\nconducted using diffusion-generated full art images.\nIn the test, 25 human-drawn images and 25 Standard Diffusion generated images were used from the AI-ArtBench\ntest set. The test initiates with a choice question in Figure 8 to understand the participant's knowledge of AI art and\nHuman art. Then, images were displayed in a list, and the respondents were asked to guess the origin of the image, as\nin Figure 9. The images were selected randomly through all ten art style classes and shuffled randomly in each response\ncollection. A 20-minute time limit was imposed per response session to overcome any unfair advantages. The test was\nshared only among a selected group to ensure the reliability of the results, collecting 50 responses. All responses were\ncollected anonymously. Then, the same image set was used in the ArtBrain AI application."}, {"title": null, "content": "Table 5 represents the results of the conducted Turing test in a matrix form grouping on the respondents' knowledge\nlevels. The maximum accuracy reached is 80%. However, that is by an expert respondent. The overall accuracy of the\nrespondents is around 54%, slightly higher than the random chance, which is in this case, 50%. AI got 98% on the\nsame set of images, detecting only one AI art wrongly as a Human art, as in Figure 10. The explainable AI heatmap\nshows that mostly the Japanese-like letters in the image have deceived the ArtBrain model into thinking it is human\nart. Also, note that ArtBrain's AI model has never been trained on any of these images since they were taken from"}, {"title": null, "content": "the test set of the AI-ArtBench dataset. Figure 9 shows the art, which only 16% of respondents correctly identified as\n'machine-generated'.\nThe results presented in Table 5 show that the accuracy increases with the knowledge of AI-generated art. In contrast, the\naccuracy remains low for respondents with advanced knowledge of human art but with novice or beginner knowledge of\nAI art. This implies that knowledge regarding AI-generated art is more effective and influential in identifying AI-human\nart than Human art knowledge."}, {"title": "6 Discussion", "content": "This chapter drives an in-depth discussion regarding notable aspects of this research, including the authors' opinions\nbased on the evaluation results."}, {"title": null, "content": "Lack of AI Art One of the key obstacles faced in initiating this research project is the lack of reliable data sources.\nVirtually no published studies were found to be done on understanding the nature and investigating the issues behind\nAI art generation. Also, there were no AI-generated artistic datasets available. To overcome this, the study presents"}, {"title": null, "content": "AI-ArtBench, a sufficiently sized dataset with more than 120,000 AI-generated artworks and 60,000 real art images\nfrom the ArtBench dataset, totalling 180,000+ images altogether (see Table 1). However, the dataset can be further\nimproved by adding more modifiers to the generation prompts. In this dataset, only the style was used in the generation\nprompt in image generation focusing on the capability of diffusion models to generate art based on a given style and the\npossibility of backtracking their source."}, {"title": null, "content": "Accuracy of ArtBrain As shown in Section 5, the accuracy of the trained model is very reliable. The human-drawn\nart classification accuracy surpasses the current state-of-the-art. The attribution accuracy levels are also high, implying\nthat there is a distinct separation in the data distributions from the three sources that human eye cannot perceive. This\nconfirms the findings of Sha et al. [52] regarding the idea of \"fingerprint\" that exists in the data distributions, making\nit possible to attribute the data to its original source. However, the study [52] was done on general images and this\nresearch, ArtBrain, proves its relevance to AI art images."}, {"title": null, "content": "Grad-CAM Vs Author's Approach This study uses FM-G-CAM [34] to improve the Explainability of CNN\npredictions. Currently, the prevalent approach is Gradient-weighted Class Activation Map - Grad-CAM [33]. However,\nit was identified that even though Grad-CAM is better at localising the objects that may have had an influence on the\nfinal prediction, the information it provides is not useful when there are spread-out visual structures that cause the\nprediction outcome. This is the case for ArtBrain as well since art images are not classified based on a contained\nobject(s) in the image but by the multiple complex visual structures and colour palettes. This study utilises the Fused\nMulti-class Gradient-weighted Class Activation Map - FM-G-CAM to overcome this issue. As visualised in Figure\n11, FM-G-CAM produces a multi-colour map corresponding to multiple classes. Unlike Grad-CAM, FM-G-CAM\nnormalises the weighted activation maps before passing into a final ReLU activation, making low activations visually\napparent."}, {"title": null, "content": "Authors Collins et al. [53"}]}