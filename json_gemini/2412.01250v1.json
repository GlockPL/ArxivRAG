{"title": "Collaborative Instance Navigation: Leveraging Agent Self-Dialogue to Minimize User Input", "authors": ["Francesco Taioli", "Edoardo Zorzi", "Gianni Franchi", "Alberto Castellini", "Alessandro Farinelli", "Marco Cristani", "Yiming Wang"], "abstract": "Existing embodied instance goal navigation tasks, driven by natural language, assume human users to provide complete and nuanced instance descriptions prior to the navigation, which can be impractical in the real world as human instructions might be brief and ambiguous. To bridge this gap, we propose a new task, Collaborative Instance Navigation (CoIN), with dynamic agent-human interaction during navigation to actively resolve uncertainties about the target instance in natural, template-free, open-ended dialogues. To address CoIN, we propose a novel method, Agent-user Interaction with UncerTainty Awareness (AIUTA), leveraging the perception capability of Vision Language Models (VLMs) and the capability of Large Language Models (LLMs). First, upon object detection, a Self-Questioner model initiates a self-dialogue to obtain a complete and accurate observation description, while a novel uncertainty estimation technique mitigates inaccurate VLM perception. Then, an Interaction Trigger module determines whether to ask a question to the user, continue or halt navigation, minimizing user input. For evaluation, we introduce CoIN-Bench, a benchmark supporting both real and simulated humans. AIUTA achieves competitive performance in instance navigation against state-of-the-art methods, demonstrating great flexibility in handling user inputs.", "sections": [{"title": "1. Introduction", "content": "Object-Goal navigation (ObjectNav) [1, 2] aims to locate any instance of a category c (i.\u0435., \u201cFind a picture\") within an unknown 3D scene. Initially restricted to scenarios with only a few predefined categories (6 \u2013 21) to find [2, 42], ObjectNav has advanced towards more challenging tasks, e.g., locating specific instances [11, 14] in an open-vocabulary [45] and multi-modal [10] setting. This evolution has been driven by breakthroughs in Large Language Models (LLMs) and Vision-Language Models (VLMs).\nIn this work, we focus on the language-guided Instance Navigation task [10, 14] where agents rely on natural language description of both intrinsic (e.g., color, material) and extrinsic attributes (e.g., context, spatial relations) of the target object, such as \"the black-and-white picture depict-ing a shirtless person, located near a bed.\" This task better reflects real-world agent-user interaction, where users typically cannot provide images or visual references of the target. Despite recent advances, current methods assume that full instructions are provided before navigation begins [10, 35], which might be inconvenient in practice, as the human user may be unable or unwilling to provide all the nuanced details upfront. However, as the agent navigates the environment, specific details become crucial for accurately identifying the target, particularly in settings with multiple instances co-existing in the same scene that are visually similar.\nTo address this, we introduce a more realistic setting, Collaborative Instance Navigation (CoIN), that allows the agent to ask questions to the user during the navigation via template-free, open-ended natural-language dialogues. COIN enables users to initiate the instance navigation task without providing extensive instance description. For ex-ample, the user might only specify the instance category, e.g., \"Find the picture\", a challenging minimal-guidance scenario. Notably, the agent-user interactions in COIN differ significantly from prior scenarios [4, 7, 22]. In CoIN, we allow template-free, open-ended interactions in natu-ral language, based on the agent's understanding acquired during navigation, while prior works either perform image-only interaction for users for target verification [22], or use templated question-answer pairs [7], relying on annotated ground-truth data for instance differentiation [4], or simpler sub-goals to reach the target [20, 23, 24, 26].\nWithin CoIN, two key research questions arise: 1) When and 2) How should agent-user interaction occur? In the first case, the agent must develop an internal model of its per-ceived environment to determine when to seek assistance from an external user to resolve ambiguities effectively. In the second, the agent should formulate the most informative questions to maximize its chances of locating the target.\nWe introduce a novel training-free approach called Agent-user Interaction with Uncertainty Awareness\""}, {"title": "(AIUTA). This method incorporates a Self-Questioner module, which leverages a VLM and an LLM, forming self-dialogues within the agent to inquire and verify details of potential target detections. As shown in Fig. 1, the LLM first prompts the VLM to obtain an initial detection description which can be incomplete and inaccurate. To enrich the relevant details, the LLM further generates questions for the VLM, whose responses complement the initial description. However, since VLMs cannot guarantee accurate responses grounded in the visual coun-terpart [18, 27, 40], we further prompt the LLM to generate sanity-check questions about all relevant details (e.g., \u201cIs the person shirtless?\u201d). We force the VLM's response to be either Yes, No or I don't know, proposing a Normalized-Entropy-based technique to quantify the VLM uncertainty. By filtering out uncertain details, we obtain a more accurate refined description. Finally, the Interaction Trigger predicts an alignment score between the known target object's attributes (i.e., facts we gathered from previous agent-human interactions, if any) and the refined detection description: based on this score, the module decides whether to continue navigation, terminate it, or ask a clarifying question to the user.\nTo facilitate the study of CoIN, we propose the first benchmark, CoIN-Bench, including a curated dataset, a new evaluation protocol supporting both simulated and real human users, and a novel performance metric that accounts for the agent-user interactions. The dataset is created on top of GOAT-Bench [10], only considering episodes involv-ing multiple instances in a scene. During the navigation, agents can query the user for details regarding the target instance. Our benchmark supports on-line evaluation with real humans. However, as real-human evaluation struggles with scaling-up, and cannot be replicated, our benchmark also supports large-scale experiments by simulating the user responses via a VLM with access to the target instance image. When evaluated on CoIN-Bench, with both real and VLM-simulated humans, AIUTA, while being training-free, outperforms state-of-the-art navigation methods which are trained on the dataset, in terms of success rate and path efficiency. AIUTA is also flexible with arbitrary user re-quest inputs as demonstrated by the real human evaluation.\nFurthermore, we prove with an ablation study that the Normalized-Entropy-based technique for estimating VLM uncertainty in the Self-Questioner module can effectively improve navigation performance and reduce agent-user in-teractions, as it contributes to more accurate detection de-scription. We also introduce a novel dataset, I Don't Know Visual Question Answering (IDKVQA), to facilitate a fair comparison among various techniques for VLM uncertainty estimation, proving that our proposal results in the highest reliability among recent competitors.", "content": "Paper Contributions are summarized as follows:"}, {"title": "2. Related Works", "content": "Object-Goal Navigation. ObjectNav policies are typically divided into two categories: training-based [10, 21, 30, 35] and zero-shot policies [6, 12, 37, 44, 46, 48, 50]. Trained policies rely exclusively on reinforcement learning [10, 21, 35] or in conjunction with behavioral cloning [30], both of which are computationally demanding and often struggle to generalize to unseen object categories. Vision-language-aligned embeddings offer a promising alternative by en-abling policies to incorporate detailed natural language de-scriptions as input. For instance, GOAT-Bench [10] em-ploy CLIP embeddings as the goal modality, while methods like [21, 35] train on image-goal navigation [51] episodes and evaluate on the object-goal navigation task. Among zero-shot policies, several methods extend the seminal frontier-based exploration [43], by incorporating LLM rea-soning [12, 46, 48, 50], CLIP-based localization [6] or vision-language value maps for frontier selections [44].\nInteractive Embodied AI. Common approaches for Human-agent interaction involve agents asking users for assistance, with responses typically consisting of shortest-path actions for reaching target objects [3, 34] or simpler sub-goals expressed in natural language to guide naviga-tion [20, 23, 24, 26, 38, 39]. In [31], authors proposed a framework to measure the uncertainty of an LLM-based planner, enabling the agent to determine the next action or ask for help. Alex Arena [7] is a platform designed for user-centric research, which includes a Dialog-guided Task Completion benchmark, using human-annotated tem-plated question-answer pairs collected via Amazon Me-chanical Turk. FindThis [22] requires locating a specific object instance through dialogue with the user. However, the agent only responds with images of candidate objects, lacking the ability to ask questions or engage in full natu-ral language interactions, limiting its interactivity. In [4], the Zero-Shot Interactive Personalized Object Navigation is proposed, where agents must navigate to personalized ob-jects (e.g., \"Find Alice's computer\"). However, personal-ized goals are manually annotated, and the user, simulated"}, {"title": "3. Collaborative Instance Navigation", "content": "Collaborative Instance Navigation (CoIN) introduces a novel scenario for the Instance Navigation task, where an agent navigates in an unknown environment to locate a target instance in collaboration with a human user via template-free and open-ended natural language interaction. The agent decides whether an interaction is needed to gather necessary target information from the user during the navi-gation. The objective of CoIN is to successfully locate the target instance with minimal user guidance, reducing the effort for the user in providing a detailed description.\nInitially, the agent is positioned randomly in an unknown 3D environment [29]. The navigation starts upon receiv-ing a user request I in natural language, which can be as minimal as by only specifying an open-set category c, e.g., \u201cFind the <category>\". We assume that the user: (i) knows the instance target, and can provide any detailed description d about it, and (ii) is collaborative to provide the true response when being asked by the agent. At each time step t, the agent perceives a visual observation \\( O_t \\) of the scene, allowing it to guide a policy to pick an action \\( a_t \\) \u2208 A = {Forward 0.25m, Turn Right 15\u00b0, Turn Left 15\u00b0, Stop, Ask}, where Ask is the novel action that comes with our COIN task. When in-voked, the agent asks the user a template-free open-ended question \\( q_{a\\rightarrow u} \\) in natural language to gather more informa-tion about the target. With the human response \\( r_{u\\rightarrow a} \\), the agent updates the set of facts (set of attributes and charac-teristics) \\( F_t \\), representing information derived exclusively from the interaction. Formally, the updated set of facts is represented as \\( F_t = F_{t-1} \\) \u222a \\( r_{u\\rightarrow a} \\). The navigation termi-\""}, {"title": "4. Proposed Method", "content": "Our proposed Agent-user Interaction with Uncertainty Awareness (AIUTA), a module that enriches the agent, is illustrated in Fig. 2. Upon receiving an initial user request I with minimal guidance that only specifies the category, e.g., \"Find the picture\" (\u2460 in Fig. 2), AIUTA updates the known facts regarding the target instance, i.e., \\( F_{t=0} = \\{ I \\} \\). Then, it activates a zero-shot navigation method, VLFM [44], per-ceiving the scene observation \\( O_t \\) and providing the naviga-tion policy (\u2461 in Fig. 2). VLFM constructs an occupancy map to identify frontiers in the explored space, and a value map that quantifies the semantic relevance of these frontiers for target object localization using the pre-trained BLIP-2 [13] model. Object detection is performed by Grounding-DINO, an open-set object detector [17]. More details about VLFM [44] in the Supp. Mat. (Sec. B.1).\nAIUTA is triggered upon the detection of an object be-longing to the target class (\u2462 in Fig. 2), executing two key components sequentially. First, the Self-Questioner (Sec. 4.1) leverages a Vision Language Model (VLM) and"}, {"title": "4.1. Self-Questioner", "content": "Upon detection, the Self-Questioner component aims to obtain a thorough and accurate description of the detected ob-ject. As suggested by previous studies [18, 27, 40], gen-erative VLMs may produce descriptions that are not fully grounded on the visual content, leading to inaccuracy or hallucination. To mitigate this issue, we leverage an LLM to automatically generate attribute-specific questions for the VLM. In particular, we propose a novel technique for es-timating uncertainty in VLM perception, enabling the re-finement of detection descriptions. The technique has three steps: (i) generating an initial detection description with de-tailed information relevant to target identification; (ii) esti-"}, {"title": "5. COIN-Bench", "content": "To facilitate the evaluation of COIN, we introduce COIN-Bench, which includes a novel evaluation protocol with both simulated and real human users, along with a new per-formance metric that accounts for agent-user interactions. \nDataset. Our dataset is based on GOAT-Bench [10], where targets are specified either by the category name, a description in natural language d, or by an image, in an open vocabulary manner. GOAT-Bench uses the HM3DSem [29] scene datasets and the Habitat simula-tor [32]. To adapt GOAT-Bench for agent-user interactions, we select episodes from multiple GOAT-Bench splits, in-cluding Train, Val Seen, Val Seen Synonyms and Val Unseen, organized by scene. To ensure the presence of multiple target instances of the same category within a scene (i.e., distractors), we apply a filtering proce-dure discarding episodes with less than \\( d_{min} = 2 \\) dis-tractors objects. Moreover, we exclude episodes that do not contain an associated language description d. After filtering, the simulator [32] sets a random starting posi-tion for the agent. To diversify the navigation difficulty, we set the geodesic distance between the target position"}, {"title": "6. Experiments", "content": "We first benchmark AIUTA, in comparison with state-of-the-art (SOTA) methods [10, 35, 44] on CoIN-Bench. Our evaluation with VLM-simulated users allows for quanti-tative comparisons at a large scale, proving that CoIN is a challenging and complex task to address. Moreover, our evaluation with real humans, despite using a subset of CoIN-Bench, demonstrates that AIUTA can flexibly pro-cess arbitrary user request inputs. Finally, ablation stud-"}, {"title": "7. Conclusion", "content": "We introduced the CoIN task, where an agent collabo-rates with the user during navigation to resolve uncertain-ties regarding the target instance. We presented AIUTA, the first method addressing CoIN with a novel self-dialogue between an LLM and a VLM, demonstrating state-of-the-art navigation performance and effective user-agent interac-tions on our benchmark CoIN-Bench with VLM-simulated users. Our experiment with real humans showed that AIUTA has great flexibility in handling arbitrary length user input. Finally, ablation studies confirm the effectiveness of key components, including our Normalized-Entropy based technique for VLM uncertainty estimation.\nLimitations & Future work. AIUTA is dependent on the capabilities of current LLMs. Often larger models lead to more satisfactory performances due to better prompt han-dling. However, the high computational cost prohibits on-board processing, thus limiting real-world deployment as it requires cloud-based services and raises privacy concerns. Future work could focus on optimizing LLMs/VLMs with smaller, efficient models suitable for onboard operations."}]}