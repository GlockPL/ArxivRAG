{"title": "A METHOD TO BENCHMARK HIGH-DIMENSIONAL PROCESS DRIFT DETECTION", "authors": ["EDGAR WOLF", "TOBIAS WINDISCH"], "abstract": "Process curves are multi-variate finite time series data coming from manufacturing processes. This paper studies machine learning methods for drifts of process curves. A theoretic framework to synthetically generate process curves in a controlled way is introduced in order to benchmark machine learning algorithms for process drift detection. A evaluation score, called the temporal area under the curve, is introduced, which allows to quantify how well machine learning models unveil curves belonging to drift segments. Finally, a benchmark study comparing popular machine learning approaches on synthetic data generated with the introduced framework shown.", "sections": [{"title": "1. INTRODUCTION", "content": "Manufacturing lines are typically decomposed into a sequential arrangement of individual steps, called processes, each utilizing one or more manufacturing techniques, such as casting, forming, separating, joining, or coating, to develop the component in accordance with its final specifications. Modern and encompassing sensor technology allows to monitor process behavior precisely, typically by measuring key performance indicators, like force, pressure, or temperature, over time. In modern IoT-enabled manufacturing lines, persisting these process curves has become the general case, allowing to monitor not only the process behavior when working on a single component, but also globally over multiple sequential executions [32]. Malign events like anomalous component batches, tool wear, or wrong calibrations can affect the line performance badly. Such events proceed subtle and often lead to a slow deformation of the resulting process curve iteration by iteration. Detecting such process drifts, that is time periods where machine behavior changes, is key to keep unplanned downtimes and costly bad parts at bay [19].\nSpecifically in high-volume production settings, where processes yield curves with multiple variables at a small rate and with short cycle times between subsequent executions, detecting process drifts is challenging due to the vast amount of data that is generated in short time. Thus, these settings have been an ideal application for machine learning methods [9, 15, 19, 30, 31, 2, 21]. Process drifts should not confused with concept or data drifts, where the goal is typically to analyse the declining performance of a trained machine learning model when new data starts to differ from the train data [22]. Thus, methods to detect concept drifts often have access to train data and trained machine learning models that can be used.\nIn our setting, however, we are interested in machine learning models that detect the drift itself. Generally speaking, process curve datasets are datasets holding finitely many multi-variate finite time series. Although process curves are time series by definition, many analysis techniques to analyse time series data do not apply because many methods, like ARMA [7] methods, assume infinite time series, often with strong assumptions on their stationarity.\nWhen working with high-dimensional data, multivariate functional data analysis [16, 24] is the statistical workhorse. Deep learning techniques, particularly dimensionality reduction methods like autoencoder [25, 14], become increasingly popular [20]. A popular approach in practice is to learn an low-dimensional embedding of the high-dimensional input and then analyse the learned latent variables with classic machine learning models. There exist some architectures that also can directly consume temporal context, like the index of the process iteration, as an auxiliary variable to learn causal meaningful latent variables, like the iVAE [12] or the independent component analysis [11]. In [13], a derivate of an variational autoencoder has been developed which particularly deals with process drifts. In [31], Bayesian autoencoders where used to detect drifts in industrial environments. Moreover, more general neural network based systems, like for casting [15] or milling [9] have been developed to analyse process curves. Often, also expert knowledge can be utilized, like by extracting deterministic features which then are further analyzed by machine learning methods, like in [2] for process curve analysis of wire bounds. Once the dimensionality has been reduced, either by hand-crafted feature extraction of representation learning, classic methods for drift detection apply, like a sliding KS-test [23]. There are also methods that work in high-dimensional settings for tabular data, like hellinger-distance based techniques [5].\nA common challenge when applying machine learning methods on applications from manufacturing is data imbalance as interesting events like scrap parts or drifts are rare. In [27], a deep generative model was used to synthetically generate process curves for casting processes, like to increase observations from the minority class.\nOverall, the analysis of process curves with machine learning techniques has been studied in depth for lots of applications from manufacturing. However, some inherent challenges remain. First, whether a model that works well in one application also does in another is by"}, {"title": "2. STATISTICAL FRAMEWORK TO ANALYSE PROCESS DRIFTS", "content": "A process curve is a finite time-series $C = (Y(x))_{x\\in I}$ with $Y(x) \\in \\mathbb{R}^c$ and $I \\subset \\mathbb{R}$ a finite set, where $Y: \\mathbb{R} \\rightarrow \\mathbb{R}^c$ represent physical properties of the process to be measured and $x$ an independent variable, often the time (see Remark 2.1 for concrete examples). We call $c\\in\\mathbb{N}$ the dimension of the process curve. Whenever a manufacturing process finishes its work on a component, a process curve is yielded. Thus, when the same process is executed on multiple parts sequentially, a sequence $C_1,...,C_T$ is obtained where each $C_t$ arises under slightly different physical conditions, i.e., $C_t = (Y_t(x))_{x\\in I_t}$. The different conditions can be due to different properties of the components or due to degradation and wareout effects of tools\nwithin the machine running the process. Also, the elements in $I_t$ can vary from execution to execution, for instance due to different offsets.\nRemark 2.1. In staking processes $Y$ is a measured force and $x$ the walked path of the press. In pneumatic test stations, $Y$ might be a pressure where $x$ might be time. In bolt fastening processes, $Y$ holds torque and angle and $x$ is the time [21].\nAs new components are assembled each time the process is executed, the effect of a part only affects a given curve, whereas wareout of the tool, which is used in every process, affects all curves. Let $C_1,...,C_T$ be process curves from $T$-many executions and $Y_1,..., Y_T$ the corresponding physical relations. We write $[T]$ for the set $\\{1,...,T\\}$. In our approach, we assume that there exists a function $f : \\mathbb{R}^k \\times \\mathbb{R} \\rightarrow \\mathbb{R}^c$ as well as a function $w : [T] \\rightarrow \\mathbb{R}^k$, such that for all $t \\in [T]$ and $x \\in I_t$:\n$f(w(t), x + \\epsilon_x) + \\epsilon_y = Y_t(x)$\nwhere the function $f$ is a proxy for the physics underneath the process and $\\epsilon_x$ and $\\epsilon_y$ denote white noise relating to measurement inaccuracies that is independent of $t$. The vector $w(t)$ represents environmental properties of the $t$-th execution, and some of its coordinates correspond to component properties, some to properties of the machine.\nAssuming no tool degradation but only component variance, we could assume that $w(t)$ is sampled in each process from a fixed but unknown distribution on $\\mathbb{R}^k$, like $w(t) \\sim N_{\\mu,\\sigma}$ with fixed $\\mu\\in \\mathbb{R}^k$ and $\\sigma\\in \\mathbb{R}^{k\\times k}$ for all $t \\in [T]$. Tool degradation, however, affects the curve substantially, often by letting certain support points of the curve move. For instance, in a staking process, the position and value of the maximal force, i.e., where the first derivative is zero, starts shifting (see Figure 1). Often, these support points can be formulated in terms of derivatives of $f$. More formally, let $d$ be the $(i)$-th derivative of $f$ according to the second argument and let for $t \\in [T], x^i(t), y^i(t) \\in \\mathbb{R}^{n_i}$ be such support points of the $t$-curve $C_t$, i.e.:\n$d^i f(w(t), x^i(t)) = y^i(t)$\nfor all $j \\in [n_i]$. Particularly, the properties $w(t)$ deform in a way such that the derivatives of $f(w(t), \\cdot)$ satisfies all support points. As mentioned before, if underlying physical properties of the process or components change, these support points start to change their position. More formally, at the $t$-th process execution, the support points are sampled according to a distribution, whose statistical parameter depend on $t$, for instance, for a particular support point $j \\in [n_i]$ of $y^i$, the following temporal change could happen, i.e. $y_t^i \\sim N_{\\mu_t^i, \\sigma}$, with\n$\\mu_t^i =\\begin{cases}b, & \\text{if } t < t_0\\\\ \\frac{t-t_0}{t_1-t_0} d + b, & \\text{if } t_0 \\leq t \\leq t_1\\\\ d, & \\text{if } t_1 < t\\end{cases}$\nwith $b \\neq d$. That is, the mean of the support point drifts linearly from $b$ to $d$ between $t_0$ and $t_1$ (see also Figure 1)."}, {"title": "3. DATA GENERATION", "content": "In this section, we explain the synthetization method of process curves. Here, we neither focus on how the $w(t)$ behave in latent space, nor on how $f$ is formulated exactly. Instead, our key idea is to model the behavior of support points over time and seek for parameters $w(t)$ using non-linear optimization satisfying the support point conditions in (2.1). That way, we obtain a generic and controllable representation of the process curves that is capable of modelling physical transformations over time by letting the physically motivated support points drift over time. Without restricting generality and to keep notation simple, we will assume for the remainder that $c = 1$. The multivariate case is a straight-forward application of our approach by modeling each variable in $Y$ individually (see Remark 3.1). For this, let $f : \\mathbb{R}^k \\times \\mathbb{R} \\rightarrow \\mathbb{R}$ be an $l$-times differentiable map and let for each $i \\in [l], x^i \\in \\mathbb{R}^{n_i}$ and $y^i \\in \\mathbb{R}^{n_i}$ be two $n_i$-dimensional vectors. We want to find $w^*$ such that\n$d^i f(w^*, x) \\approx y \\quad \\forall j\\in [n_i] \\quad \\forall i\\in [l]$.\nParticularly, $x^i$ and $y^i$ can be considered as support points surpassed by the graph of the map $f(w^*,\\cdot) : \\mathbb{R} \\rightarrow \\mathbb{R}$ (see Figure 2). Assuming that $f \\in C^{l+2}(\\mathbb{R}^k \\times \\mathbb{R})$, i.e. $f$ is $l+2$-times\ndifferentiable, and given $x^i \\in \\mathbb{R}^{n_i}$ and $y^i \\in \\mathbb{R}^{n_i}$ for each $i \\in [l]$, we can solve (3.1) using second-order quasi-Newton methods [6, Chapter 3] for the objective function\n$\\arg \\min_{w \\in \\mathbb{R}^k} \\sum_{i=1}^l\\sum_{j=1}^{n_i} D_i ||d_j f(w,x) - y_j^i||^2$\nwhere $D_1,...,D_l$ are constants to account for the different value ranges of the functions $d_j f$. Now, let $x^i(1),...,x^i(T) \\in \\mathbb{R}^{n_i}$ and $y^i(1),...,y^i(T) \\in \\mathbb{R}^{n_i}$ be sequences of $T$-many $n_i$-dimensional vectors respectively. Solving the optimization problem (3.2) for the each $t \\in [T]$"}, {"title": "4. THE TEMPORAL AREA UNDER THE CURVE", "content": "In this section, we construct a score to measure the predictive power of machine learning models for process drift detection. In order to do so, we first formalize what we understand as a process drift and which assumptions we require. Let $C_1, ..., C_T$ be a sequence of process curves and let $D \\subset [T]$ be the set of curve indices belonging to drifts. Our first assumption is that drifts, different than point anomalies, appear sequentially and can be uniquely decomposed into disjoint segments:\nDefinition 4.1 (Drift segments). Let $D\\subset [T]$. Then a series of subsets $D_1, ..., D_k \\subset D$ is a partition of drift segments if there exists $1 < l_1 < h_1 < l_2 < h_2 < ..., <l_k < h_k < T$ such that for all $i$, we have $D_i = [l_i, h_i]$ and $D = \\bigcup_{i=1}^k D_i$.\nThe drift segments can be considered as a partition of the smallest consecutive drifts which cannot decomposed any further into smaller segments. Now, assume we also have the output $s\\in \\mathbb{R}^T$ of a detector where each coordinate $s_t$ quantifies how likely the curve $C_t$ of the $i$-t-h process execution belongs to a drift, that is, the higher $s_t$ the more likely the detector classifies $t \\in D$. By choosing a threshold $\\tau\\in \\mathbb{R}$, we can construct a set\n$D(s,\\tau) := \\{t \\in [T] : s_t > \\tau\\}$\nwhich serves as a possible candidate for $D$. Clearly, if $\\tau_1 \\geq \\tau_2$, then $D(s, \\tau_1) \\subseteq D(s, \\tau_2)$. Its also straight-forward to see that for every $\\tau$, the set $D(s,\\tau)$ decomposes uniquely into drift segments $D_1,..., D_l$ as defined in Definition 4.1 and that the length and number of these atomic segments depends on $\\tau$. Now, to quantify the predictive power of the detector yielding $s$, one needs to quantify how close $D(s, \\cdot)$ is to $D$ when $\\tau$ varies. There are many established set-theoretic measurements that are widely used in practice to quantify the distance between two finite and binary sets $A$ and $B$, like the Jaccard index $\\frac{|A \\cap B|}{|A \\cup B|}$, the Hamming distance $\\frac{|A\\setminus B|+|B\\setminus A|}{|T|}$, or the Overlap coefficient $\\frac{|A\\cap B|}{min(|A|, |B|)}$ just to name a few. Most scores, however, have as a build-in assumption that the elements of the set are iid and hence the temporal context is largely ignored making them unsuitable for process drift detection. Moreover, for most detectors we have to select a discrimination threshold $\\tau$, making evaluation cumbersome as it requires to tune the threshold on a separate held-out dataset. Moreover, in most practical\nscenarios, $D$ is only a small subset and thus the evaluation metric has to consider highly imbalanced szenarios as well.\nClearly, detectors are required where all true drift segments $D_i$ are overlapped by predicted drift segments. For this, let $T_i := \\{j \\in [l] : D_j \\cap D_i \\neq \\emptyset\\}$. Clearly, $T_i \\cap T_{i+1} \\neq \\emptyset$ if $D_i$ and $D_{i+1}$ both intersect with a predicted drift segment. Now, the set $T_i := \\bigcup_{j\\in T_i}D_j$ which is the union of all predictive segments intersecting with $D_i$ serves as a candidate for $D_i$. To measure how well $D_i$ is covered - or overlapped - by $T_i$ we define the soft overlap score inspired by the Overlap coefficient as follows:\n$sOLS(D_i, s, \\tau) := \\frac{|T_i|}{\\max(|T_i \\cup D_i|) - \\min(|T_i \\cup D_i|) + 1}$\nObviously, an sOLS of 1 is best possible and this is reached if and only if $T_i = D_i$. It is easy to see that for fixed $D_i$, the enlargement of $T_i$ beyond the boundaries of $D_i$ improves the overlap score, as $|T_i|$ increases and one of either $\\max(|T_i\\cup D_i|)$ or $ - \\min(|T_i \\cup D_i|)$ increases as well. A special case is if $D_i$ is completely covered by $T_i$, i.e. $D_i \\subseteq T_i$, then it follows that $T_i$ is an interval as well and thus $sOLS(D_i, s, \\tau) = 1$. When $T_i$ enlarges, then the number of false positives, i.e. the time points $t$ contained in some $D_i$ and in the complement $D := [T] \\setminus D$ of the ground truth $D$, enlarges as well. Thus, the predictive power of a detector is shown in the overlap score as well as the created false positive rate\n$FPR(D, s, \\tau) := \\frac{|D(s, \\tau) \\cap \\overline{D}|}{|\\overline{D}|}$\ninto account. To also take false negatives into account, the enumerator in (4.2) could be changed as follows, yielding our final definition of the overlap score:\n$OLS(D_i, s, \\tau) := \\frac{|T_i\\cap D_i|}{\\max(|T_i \\cup D_i|) - \\min(|T_i \\cup D_i|) + 1}$\nOur score considers both, the OLS as well as the FPR, which mutually influence each other. In the computation of the AUC, any threshold $\\tau$ from $[\\min_t(s_t), \\max_t(s_t)]$ yields a pair of false positive rate $FPR(D, s, \\tau)$ and true positive rate $TPR(D, s, \\tau)$ which can be drawn as a curve in the space where FPR is on the x-axis and TPR on the y-axis. Similarly, we define the temporal area under the curve, or just TAUC, as the area under the FPR-OLS curve"}, {"title": "5. EXPERIMENTS", "content": "Next, we benchmark existing algorithms on data generated with our framework driftbench and reporting the TAUC. The goal of the benchmark is to provide a proof of concept for our score and data generation method, not to be very comprehensive on the model side. Thus, based on our literature research in Section 1 we have hand-selected a small set of typically used model patterns drift detectors used in practice consists of (see Section 5.1).\nThe basic evaluation loop follows a typical situation from manufacturing, where process engineers have to identify time periods within a larger curve datasets where the process has drifted. Thus, all models consume as input a process curve dataset $C_1,..., C_T$ and do not have access to the ground truth $D$, which is the set of curves belonging to a drift (see Section 5.3). Afterwards, each model predicts for each curve $C_t$ from this dataset, a score $s_t$ on which then the TAUC, sTAUC, and AUC are computed. To account for robustness, we generate each dataset of a predefined specification multiple times with different random seeds each, leading to slightly different datasets of roughly same complexity.\n5.1. Algorithms. The algorithms used can be decomposed into multiple steps (see also Figure 5), but not all algorithms use all steps. First, there are may some features extracted from each curve. Afterwards, a sliding window collects and may aggregates these such that a score is computed.\n5.1.1. Feature extraction. In this steps, we use autoencoders [25] to compute a $k$ dimensional representation $e_t \\in \\mathbb{R}^k$ for each high-dimensional process curve $C_t$ with $k$ small. The indention behind is to estimate an inverse of the unknown function $f$ and to recover information about the support points used. Moreover, we also apply deterministic aggregations over the x-information of each curve $C_t$,\n5.1.2. Windowing and aggregation. In this step, the algorithms may aggregate the data from the previous step using a fixed window of size $m$ that is applied in a rolling fashion along the\nprocess iterations. One aggregation we use is to first compute for each coordinate $j\\in [k]$ of $e_t \\in \\mathbb{R}^k$ with $t > m$ the rolling mean:\n$a_{t,j} = \\frac{1}{m} \\sum_{i=t-m+1}^t e_{i,j}$\nThese values can then further be statistically aggregated, like by taking the maximum $a_t := \\max\\{a_{t,j} : j \\in [k]\\}$.\n5.1.3. Score computing. Goal of this step is to compute a threshold which correlates with the ground truth, that is, the larger the higher the possibility of a drift. Here, we may also aggregate previous features in a rolling fashion. The simplest aggregation we use is to compute the euclidean distance of subsequent elements $s_t = ||a_t - a_{t-1}||_2$ which is just the absolute difference if $a_t$ and $a_{t-1}$ are scalars. If $a_t$ is a scalar, we also can compute the rolling standard deviation, again over a window of size $m$, like this:\n$s_t = \\sqrt{\\frac{1}{m-1} \\sum_{j=t-m+1}^t \\Big(a_j - \\big(\\frac{1}{m} \\sum_{i=t-m+1}^t a_i\\big)\\Big)^2}.$\nAnother approach follows a probabilistic path by testing if a set of subsequent datapoints $\\{a_{t-m+1},...,a_t\\}$ come from the same distribution as a given reference set. In our study, we use a windowed version [23] of the popular Kolmogorov-Smirnov test [26], often called KSWIN, which makes no assumption of the underlying data distribution. However, this can only be applied when $a_t$ is a scalar. More particularly, we define two window sizes, $m_r$ for the reference data and $m_o$ for the observation. The windows are offset by constant $\\delta > 0$. We then invoke the KS-test and receive a p-value $p_t$, which is small if the datasets come from different distributions. Thus, one way to derive a final score is to compute $s_t = \\log(\\frac{p_t}{1+p_t})$. We also evaluate algorithms that derive their score based on a similarity search within $\\{a_1,...,a_t\\}$. Here, we use clustering algorithms, like the popular k-means algorithm, and use the euclidean distance to the computed cluster center of $a_t$ as $s_t$. Another way is to fit a probability density function on $s_t$, like a mixture of Gaussian distributions, and to set $s_t$ as the log likelihood of $a_t$ within this model.\n5.2. Algorithm Overview. Here is a short summary of the algorithms used in our benchmark study:\n\u2022 RollingMeanDifference(mr) First, the rolling mean over a window of size $m_r$ is computed over all values for the respective curves in the window. Afterwards, the maximum value for each curve is taken and the absolute difference between two consecutive maximum values is computed.\n\u2022 RollingMeanStandardDeviation(mr) First, the rolling mean over a window of size $m_r$ is computed over all values for the respective curves in the window. We also choose the maximum value of these computed values per curve. Then, we compute the standard deviation using the same window for this one-dimensional input.\n\u2022 SlidingKSWIN($m_r,m_o, \\delta$): We compute the mean value for each curve and apply a sliding KS-test on this aggregated data. We use two windows of size $m_r$ and $m_o$ where the windows are offset by $\\delta$.\n\u2022 Cluster(nc): A cluster algorithm performed on the raw curves using $n_c$ clusters where score is distance to closest cluster center.\n\u2022 AE(k)-mean-KS($m_r, m_o, \\delta$): First, an autoencoder is applied extracting computing $k$ many latent dimensions. Afterwards, the mean across all $k$ latent dimensions is computed. Finally, a sliding KS-test is applied with two windows of sizes $m_r$ and $m_o$, where the windows are offset by $\\delta$.\n5.3. Datasets. We benchmark the algorithms listed in 5.1 on three different datasets (see Figure 6) created with our framework driftbench, all designed to comprise different inherent challenges. To generate dataset-1 and dataset-2, we used\n$f(w,x) = \\sum_{i=0}^7 w_ix^i$\nas function. The dataset dataset-1 consists of $T = 10.000$ curves, each called on $|I| = 100$ equidistant values between [0,4]. On the other hand, dataset-2 consists of $T = 30.000$ curves each having $|I| = 400$ values. Both datasets have drifts that concern a movement of the global maximum together with drifts where only information of first order changes over time. In the generation process of dataset-3, we used\n$f(w,x) = w_0\\cdot x \\cdot \\sin(\\pi \\cdot x - w_1) + w_2 \\cdot x$\nand generated $T = 10.000$ many curves, each having $|I| = 100$ datapoints. It only holds a single drift, where the global minimum at drifts consistently over a small period of time along\nthe x-axis. In all datasets, the relative number of curves belonging to a drift is very small: roughly 2 percent in dataset-1, 0.1 percent in dataset-2 and 1 percent in dataset-3.\n5.4. Results. The result of our benchmark study is shown in Figure 7. We again want to mention that our benchmark study is not to show whether one model is superior over another. Instead, its purpose is to highlight how well the TAUC and our synthetization system allows to explore their performance. Surprisingly, the RandomGuessDetector reached the highest AUC score on dataset-3, where it ranges on all three datasets among the last ranks in the TAUC score. The respective predictions of the best detectors are plotted in Figure 9. Moreover, cluster based systems reach good AUC scores, but are not competitive when using the TAUC as quantification measurement (see also Figure 8). Unsurprisingly,"}, {"title": "APPENDIX A. DATA GENERATION WITH POLYNOMIALS", "content": "In this section, we demonstrate the data generation method introduced in Section 3 along an example involving a polynomial $f : \\mathbb{R}^6 \\times \\mathbb{R} \\rightarrow \\mathbb{R}$ of degree five, i.e.\n$f(w,x) = \\sum_{i=0}^5 w_ix^i$\nwith $w \\in \\mathbb{R}^6$. We simulate 2000 process executions and thus sample 2000 process curves. The shape of each curve is defined by its support points. We are only interested in its curvature in $I = [0, 4]$. First, we want to add a condition onto the begin and end of the interval, namely that $f(w, 0) = 4$ and $f(w, 4) = 5$. Moreover, we would like to have a global maximum at $x = 2$, which means the first order derivative\n$\\frac{d}{dx}f(w, 2) = \\sum_{i=1}^4 i \\cdot w_i \\cdot 2^{i-1}$\nshould be zero and its second order derivate\n$\\frac{d^2}{dx^2}f(w, 2) = \\sum_{i=1}^3i \\cdot (i - 1) \\cdot w_i \\cdot 2^{i-2}$\nshould be smaller than zero. Here, we want it to be -1. Finally, we want to the curve to be concave at around $x = -1$. All in all, these conditions result into the following equations,\nsome of them are visualized in Figure 10:\n$\\frac{d}{dx}f(w, 2) = 7$\\n$\\frac{d^2}{dx^2}f(w, 2) = -1$\\n$\\frac{d}{dx}f(w, 0) = 4$\\n$\\frac{d}{dx}f(w, 4) = 5$\\n$\\frac{d^2}{dx^2}f(w, 1) = -1$\\nThen, we let the data drift at some particular features. We simulate a scenario, where the peak at $x_l$ and $x_d$ moves from the x-position 2 to 3 during the process executions $t = 1000$ until $t = 1300$. Thus, we let $x_l$ and $x_d$ drift from 2 to 3, resulting in a change of position of the peak. We let the corresponding y-values $y_l = 7$ and $y_d = 0$ unchanged. Now, we can solve each of the 2000 optimization problems, which results in 2000 sets of coefficients for each process curve, such that the conditions are satisfied. By evaluating f with the retrieved coefficients in our region of interest [0, 4], we get 2000 synthesized process curves with a drift present at our defined drift segment from $t = 1000$ until $t = 1300."}, {"title": "APPENDIX B. TAUC vs AUC", "content": "In this section we explore in depth the similarities and differences of the TAUC introduced in Section 4 and the established AUC. This is done along synthetic predictions.\nB.1. Lagged prediction. The first example we look at is a typical scenario that appears if window-based approaches are used, namely that the prediction lags a bit behind of the true window, but still the detector overlaps a significant proportion of the drift segment (see Figure 12. Other than the TPR, the sOLS rewards these predictors and thus the sTAUC shows a larger value than the AUC.\nB.2. Change point detection. Another typical scenario is that a detector shows significantly large values at the start and end of the true drift segment, but sag in between (see Figure 13). This could appear when using methods based on detecting change points. In principal, the detector correctly identifies the temporal context of the drift segment, although showing lower scores while the curves drift. Such predictions also score higher values in the sTAUC than the AUC.\nB.3. Varying length and position of predicted segments. A situation where the sTAUC coincides with the AUC mostly is in when only one true and predicted drift segment exist (see Figure 14). In cases where the center of the predicted segment coincides with the center of the true segment, the AUC and sTAUC match almost exactly when the length of the predicted segment is varied (see left graphic in Figure 15). If the predicted segment has fixed"}, {"title": "APPENDIX C. TAUC FOR TRIVIAL DETECTOR", "content": "To get a better understanding of the TAUC, we showcase the behavior on trivial detectors based on the structure of the ground truth. Suppose two pair of points $(FPR_i, OLS_i)$ and $(FPR_{i+1}, OLS_{i+1})$ of the constructed curve. Then the two methods for computing the TAUC are the following:\n\u2022 Trapezoidal rule:\nConstruct the curve by linearly interpolating $OLS_i$ and $OLS_{i+1}$ in between $FPR_i$ and $FPR_{i+1}$ and then calculate the area under the curve by using the trapezoidal integration rule.\n\u2022 Step rule:\nConstruct the curve by filling the values in between $FPR_i$ and $FPR_{i+1}$ with a constant value of $OLS_i$ and then calculate the area under the curve by using the step rule.\nFor example, take the trivial detector that always predicts a drift, called AlwaysGuesser. Then we receive the two points (0,0) and $(1,\\frac{P}{2k})$ as the only two points of the curve, where\n$P$ denotes the portion of drifts in $y$ and $k$ denotes the number of drift segments in $y$. In case of the step function, the computed score will always be 0, since the constructed curve only contains one step from [0, 1) with a OLS-value of 0, and only reaches a OLS-value of $\\frac{P}{2k}$ when reaching a FPR of 1 on the x-axis. Hence, the area under this constructed curve is always 0. When using the trapezoidal rule, we linearly interpolate the two obtained trivial points of the curve, thus constructing a line from (0,0) to (1, $\\frac{P}{2k}$). The TAUC is then given by the area under this line, which is equal to $\\frac{1}{2} \\cdot \\frac{P}{2k}$. Now suppose a detector which never indicates a drift, called NeverGuesser. Then we receive (0,0) as our only point, which does not construct a curve and thus does not have an area under it. Hence, the TAUC for this trivial detection is 0 in both cases.\nThis decreasing behaviour can be approximated by $\\lim_{k \\to \\infty} \\frac{P}{2k} = 0$, since the TAUC for a trivial detection with $k$ segments in case of the trapezoidal rule can be computed with $\\frac{P}{2k}$ and $0 < P \\leq 1$. Thus, the limit of the TAUC computed with the trapezoidal integration rule with increasing k follows as:\n$\\lim_{k \\to \\infty} \\frac{P}{2k} = 0$"}]}