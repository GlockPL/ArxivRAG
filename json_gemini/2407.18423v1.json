{"title": "HDL-GPT: High-Quality HDL is All You Need", "authors": ["Bhuvnesh Kumar", "Saurav Nanda", "Ganapathy Parthasarathy", "Pawan Patil", "Austin Tsai", "Parivesh Choudhary"], "abstract": "This paper presents Hardware Description Language Generative Pre-trained Transformers (HDL-GPT), a novel approach that leverages the vast repository of open-source High Definition Language (HDL) codes to train superior quality large code models. The core premise of this paper is the hypothesis that high-quality HDL is all you need to create models with exceptional performance and broad zero-shot generalization abilities. The paper elucidates the methods employed for the curation and augmentation of large corpora from open-source HDL code, transforming highly variable quality data into high-quality data through careful prompting and context maintenance. We demonstrate that the careful selection, filtering, and augmentation of data across HDLS can yield powerful models that surpass current state-of-the-art models. We also explore the impact of different fine-tuning methods on the quality of results. We describe experimental results across a range of fine-tuned SOTA LLMs, substantiating our claims. We demonstrate improvements of 50% to 200% over SOTA HDL models on current benchmarks in tasks ranging from HDL circuit explanations, code generation, formal and simulation testbench creation, triaging bugs, and fixing them. HDL-GPT opens new avenues for the development of advanced model training techniques for circuit design tasks.", "sections": [{"title": "INTRODUCTION", "content": "The relentless pace of Moore's law [1] has ushered in a surge of technological advancements in the realm of integrated circuit (IC) design. EDA algorithms and tools have led this revolution, facilitating the creation of feature-rich System-on-Chip (SoC) designs with billions of transistors. However, both Moore's law and increasing complex functionality requirements also necessitate constant advancements in chip design productivity. Recently, Large Language Models (LLMs) have emerged as a promising tool to augment EDA processes. Researchers have focused on automating language-related chip design tasks, which often involve time-consuming interfacing with natural or programming languages. LLMs, both commercial (e.g. OpenAI [2]) and open source (e.g. Mixtral [3], LLaMA2 [4]), have opened new avenues for automating these tasks. These LLMs can generate code, conduct analysis, and respond to engineering queries via a natural language interface.\nHowever, there are significant challenges in deploying LLMs in Hardware Description Language (HDL) code generation. For example, Tsai et al. [5] found that up to 55% of errors generated by LLMs in Verilog code are syntax errors. The complexity of HDL design extends beyond generating syntactically correct code and achieving precision often necessitates multiple iterations. The challenge lies in meeting performance, power, and area (PPA) requirements for the overall IC design, which requires eliminating syntactic, semantic, or tool-specific errors. This flow dependency of errors presents a significant obstacle to automating chip design tasks.\nA crucial part of solutions to these challenges is the quality and diversity of training data for LLMs to understand both syntax and semantics and enable robust zero-shot learning. Additionally, exposure to diverse types of errors in training data helps LLMs learn to detect and rectify such errors in the generated code.\nThere is significant prior art describing why high-quality training data is integral to the LLM training and fine-tuning. These include the following categories of motivating reasons:\n1) Quality of Generated Code: LLM performance is directly tied to the quality of the training data. High-quality data aids in producing more accurate, efficient, and robust code. This also exposes LLMs to real-world coding scenarios and challenges, enabling practical solutions with LLMs. For example, Gunasekar et. al [6] demonstrated the influence of high-quality data on enhancing a language model's efficiency in code-generation tasks.\n2) Understanding Context: Training data facilitates comprehension of context in code use, crucial for generating semantically appropriate code.\n3) Syntax and Semantics: Training data assists LLMs in understanding both syntax and semantics of the programming language, enabling them to learn from examples [7], [8].\n4) Zero-shot Learning: High-quality training data enables effective generalization in zero-shot learning scenarios, helping LLMs generalize to code for untrained tasks [9].\n5) Error Handling: Exposure to diverse errors through training data equips LLMs with the ability to manage and rectify errors in the generated code [5].\n6) Dataset Size: The size of the dataset also plays a crucial role in acquiring high-quality HDL data [10]. However, manually gathering a sufficiently large training corpus is a daunting task, both in terms of cost and effort. Therefore, the process needs to be automated for efficient data acquisition.\nThis paper introduces a systematic automated approach to generating high-quality data for HDL generation and analysis tasks to address these challenges and harness the potential of LLMs in HDL-based IC design. We explain how we curate and augment large corpora from open-source HDL code, thereby transforming highly variable quality data into high-quality data through careful prompting and context maintenance. We use augmented data to train high-quality large code models with exceptional performance and broad zero-shot generalization abilities. We also explore the impact of different fine-tuning methods on the quality of results and compare the results with the current state of the art in the field."}, {"title": "AUTOMATED HDL DATA AUGMENTATION", "content": "Our approach to data augmentation involves a systematic process known as chain of thought prompting [11]. The overall process is shown in Figure 1 as a series of task-specific pipelines with feedback.\nThe figure shows a conceptual breakdown of the process into a Data Pipeline, an LLM Finetune Pipeline, an Evaluation Pipeline, and a feedback enabling Validation Pipeline.\nThe Data Pipeline begins with data curation where we collect HDL code data from GitHub. We initially filter code repositories to remove code with non-permissible licenses. We also remove any code that matches the code or expected code in the evaluation datasets to prevent data leakage during quality evaluations. The filtered data forms our initial raw data corpus. We go through well-known techniques such as de-duplication, and custom filters to remove objectionable material from the raw data corpus. Once we have a curated data corpus, we can address the ten major functional steps in the augmentation process shown in Table I as S1 through S10."}, {"title": "FINE-TUNING METHODOLOGY", "content": "This section outlined a series of fine-tuning experiments that we conducted to explore and enhance the performance of various code-generation LLMs. Our experiments involve the use of multiple fine-tuned candidate models, including StarCoder (SC), StarCoder2 (SC2), and CodeLlama (CL-FT) utilizing the PEFT technique. We replicated the CodeGen-16B model (CG) following [7] and verified results versus published results in [15] for our comparisons. The QoR enhancement of each candidate model was then tested after performing fine-tuning using the Neftune technique.\n1) Experiment 1 - Base Model Selection: The first experiment involved the application of PEFT LORA fine-tuning to four distinct models: StarCoder, StarCoder2, Codegen, and CodeLLama. The initial version of augmented data was used for this experiment.\n2) Experiment 2 - Fine-Tuning with PEFT: The second experiment centered around PEFT-based fine-tuning of SC and SC2 using the final version of augmented data. This data version embodied all data pipeline steps as depicted in Figure 1. The goal was to enhance the models' capability across four crucial tasks: code generation, code explanation, bug finding/fixing, and writing testbenches.\n3) Experiment 3 - Fine-Tuning with Neftune: The third experiment further fine-tuned the SC model using the final augmented dataset along with an additional technique, Neftune, to bolster the Quality of Results (QoR). Neftune was introduced to add more variability during the fine-tuning phase and to assess the model's capacity for handling intricate code generation tasks.\nThe StarCoder variants, SC and SC2, demonstrated better performance than CodeGen and CodeLlama. Therefore, we chose the StarCoder variants as base LLMs for HDL-GPT in our work. We derived three fine-tuned models at the end of the model selection experiments, which are used in the benchmarking and analysis experiments described in Section IV:\n1) HDL-GPT (HG) is a fine-tuned version of StarCoder.\n2) HDL-GPT2 (HG2) is a fine-tuned version of StarCoder2.\n3) CL-FT is a fine-tuned version of CodeLlama."}, {"title": "EXPERIMENTS AND RESULTS", "content": "We evaluated HDL-GPT on various tasks including HDL code generation, explanation, bug finding, bug fixing, and test-bench (TB) generation. For the code generation task, we use NYU [15] and NVIDIA [16] benchmarks and pass@k as the evaluation metric. For all other tasks, we use student-teacher grading [17] as the quality metric with GPT-4 acting as the teacher. All experiments were conducted on NVIDIA A100 machines using 8 X 80GB GPUs.\nHDL-GPT scored maximum points in Basic and Advanced levels with an average score of 0.92. HDL-GPT2 outperformed others across all levels, achieving the highest average score of 0.96. CodeLlama FT performed comparably at the Basic level and better at higher levels than CG, yielding an average score of 0.58."}, {"title": "Code Analysis tasks", "content": "We use a student-teacher grading evaluation scheme on the same benchmarks as in the prior experiments for code analysis, bug finding, bug fixing, and testbench generation tasks. We evaluated four LLMs: Mixtral, Llama, OpenAI GPT-3.5, and GPT-4 as teacher candidates. The teacher model is prompted to perform scoring on a scale of 0-5 on the given HDL code based on detailed description, code quality, and code correctness. We then calculate the average grading scores (0-5) for each evaluation set and normalize to a range between 0 and 1 as shown in Equation 1. This is the normalized score reported in experiments in Figure 2.\n$Normalized Score = \\frac{Average \\ Score \\ for \\ a \\ Eval \\ Set}{Maximum \\ Score}$ (1)\nWe selected the best teacher model (GPT-4) by hand-curating grades across all teacher candidates on a random sample of 150 tasks from the evaluation dataset. Figure 2 shows the grading results for the Student-Teacher grading in various categories. The HDL code generation task received a normalized score of 0.77. The functional description task of undocumented HDL code had a normalized score of 0.59 and 0.57 with the actual description. The task of identifying and describing errors received a normalized score of 0.47 while the Error fixing task achieved a normalized score of 0.55. The test-bench generation task had the lowest normalized score of 0.34 showing room for improvement for HDL-GPT."}, {"title": "CONCLUSION", "content": "The work in this paper underscores the critical role of high-quality training data, input prompts, and fine-tuning methods (PEFT and Neftune) in creating a SOTA LLM, called HDL-GPT, for HDL code generation, explanation, bug detection/repair, and testbench creation. The experimental results against public benchmarks demonstrate that the model's performance, as well as the quality of the generated code, rely heavily on the training data's quality. Our future research will delve into optimizing HDL-GPT's performance for IC design flows, creating representative benchmark suites for IC design tasks, and enhancing its potential for high-quality code generation and task performance in the IC design flows."}]}