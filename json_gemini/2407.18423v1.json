{"title": "HDL-GPT: High-Quality HDL is All You Need", "authors": ["Bhuvnesh Kumar", "Saurav Nanda", "Ganapathy Parthasarathy", "Pawan Patil", "Austin Tsai", "Parivesh Choudhary"], "abstract": "This paper presents Hardware Description Language Generative Pre-trained Transformers (HDL-GPT), a novel approach that leverages the vast repository of open-source High Definition Language (HDL) codes to train superior quality large code models. The core premise of this paper is the hypothesis that high-quality HDL is all you need to create models with exceptional performance and broad zero-shot generalization abilities. The paper elucidates the methods employed for the curation and augmentation of large corpora from open-source HDL code, transforming highly variable quality data into high-quality data through careful prompting and context maintenance. We demonstrate that the careful selection, filtering, and augmentation of data across HDLS can yield powerful models that surpass current state-of-the-art models. We also explore the impact of different fine-tuning methods on the quality of results. We describe experimental results across a range of fine-tuned SOTA LLMs, substantiating our claims. We demonstrate improvements of 50% to 200% over SOTA HDL models on current benchmarks in tasks ranging from HDL circuit explanations, code generation, formal and simulation testbench creation, triaging bugs, and fixing them. HDL-GPT opens new avenues for the development of advanced model training techniques for circuit design tasks.", "sections": [{"title": "I. INTRODUCTION", "content": "The relentless pace of Moore's law [1] has ushered in a surge of technological advancements in the realm of integrated circuit (IC) design. EDA algorithms and tools have led this revolution, facilitating the creation of feature-rich System-on-Chip (SoC) designs with billions of transistors. However, both Moore's law and increasing complex functionality requirements also necessitate constant advancements in chip design productivity. Recently, Large Language Models (LLMs) have emerged as a promising tool to augment EDA processes. Researchers have focused on automating language-related chip design tasks, which often involve time-consuming interfacing with natural or programming languages. LLMs, both commercial (e.g. OpenAI [2]) and open source (e.g. Mixtral [3], LLaMA2 [4]), have opened new avenues for automating these tasks. These LLMs can generate code, conduct analysis, and respond to engineering queries via a natural language interface.\nHowever, there are significant challenges in deploying LLMs in Hardware Description Language (HDL) code generation. For example, Tsai et al. [5] found that up to 55% of errors generated by LLMs in Verilog code are syntax errors. The complexity of HDL design extends beyond generating syntactically correct code and achieving precision often necessitates multiple iterations. The challenge lies in meeting performance, power, and area (PPA) requirements for the overall IC design, which requires eliminating syntactic, semantic, or tool-specific errors. This flow dependency of errors presents a significant obstacle to automating chip design tasks.\nA crucial part of solutions to these challenges is the quality and diversity of training data for LLMs to understand both syntax and semantics and enable robust zero-shot learning. Additionally, exposure to diverse types of errors in training data helps LLMs learn to detect and rectify such errors in the generated code.\nThere is significant prior art describing why high-quality training data is integral to the LLM training and fine-tuning. These include the following categories of motivating reasons:\n1) Quality of Generated Code: LLM performance is directly tied to the quality of the training data. High-quality data aids in producing more accurate, efficient, and robust code. This also exposes LLMs to real-world coding scenarios and challenges, enabling practical solutions with LLMs. For example, Gunasekar et. al [6] demonstrated the influence of high-quality data on enhancing a language model's efficiency in code-generation tasks.\n2) Understanding Context: Training data facilitates comprehension of context in code use, crucial for generating semantically appropriate code.\n3) Syntax and Semantics: Training data assists LLMs in understanding both syntax and semantics of the programming language, enabling them to learn from examples [7], [8].\n4) Zero-shot Learning: High-quality training data enables effective generalization in zero-shot learning scenarios, helping LLMs generalize to code for untrained tasks [9].\n5) Error Handling: Exposure to diverse errors through training data equips LLMs with the ability to manage and rectify errors in the generated code [5].\n6) Dataset Size: The size of the dataset also plays a crucial role in acquiring high-quality HDL data [10]. However, manually gathering a sufficiently large training corpus is a daunting task, both in terms of cost and effort. Therefore, the process needs to be automated for efficient data acquisition.\nThis paper introduces a systematic automated approach to generating high-quality data for HDL generation and analysis tasks to address these challenges and harness the potential of LLMs in HDL-based IC design. We explain how we curate and augment large corpora from open-source HDL code, thereby transforming highly variable quality data into high-quality data through careful prompting and context maintenance. We use augmented data to train high-quality large code models with exceptional performance and broad zero-shot generalization abilities. We also explore the impact of different fine-tuning methods on the quality of results and compare the results with the current state of the art in the field."}, {"title": "II. AUTOMATED HDL DATA AUGMENTATION", "content": "Our approach to data augmentation involves a systematic process known as chain of thought prompting [11]. The overall process is shown in Figure 1 as a series of task-specific pipelines with feedback.\nThe figure shows a conceptual breakdown of the process into a Data Pipeline, an LLM Finetune Pipeline, an Evaluation Pipeline, and a feedback enabling Validation Pipeline.\nThe Data Pipeline begins with data curation where we collect HDL code data from GitHub. We initially filter code repositories to remove code with non-permissible licenses. We also remove any code that matches the code or expected code in the evaluation datasets to prevent data leakage during quality evaluations. The filtered data forms our initial raw data corpus. We go through well-known techniques such as de-duplication, and custom filters to remove"}, {"title": "III. FINE-TUNING METHODOLOGY", "content": "This section outlined a series of fine-tuning experiments that we conducted to explore and enhance the performance of various code-generation LLMs. Our experiments involve the use of multiple fine-tuned candidate models, including StarCoder (SC), StarCoder2 (SC2), and CodeLlama (CL-FT) utilizing the PEFT technique. We replicated the CodeGen-16B model (CG) following [7] and verified results versus published results in [15] for our comparisons. The QoR enhancement of each candidate model was then tested after performing fine-tuning using the Neftune technique.\n1) Experiment 1 - Base Model Selection: The first experiment involved the application of PEFT LORA fine-tuning to four distinct models: StarCoder, StarCoder2, Codegen, and CodeLLama. The initial version of augmented data was used for this experiment.\n2) Experiment 2 - Fine-Tuning with PEFT: The second experiment centered around PEFT-based fine-tuning of SC and SC2 using the final version of augmented data. This data version embodied all data pipeline steps as depicted in Figure 1. The goal was to enhance the models' capability across four crucial tasks: code generation, code explanation, bug finding/fixing, and writing testbenches.\n3) Experiment 3 - Fine-Tuning with Neftune: The third experiment further fine-tuned the SC model using the final augmented dataset along with an additional technique, Neftune, to bolster the Quality of Results (QoR). Neftune was introduced to add"}, {"title": "IV. EXPERIMENTS AND RESULTS", "content": "We evaluated HDL-GPT on various tasks including HDL code generation, explanation, bug finding, bug fixing, and test-bench (TB) generation. For the code generation task, we use NYU [15] and NVIDIA [16] benchmarks and pass@k as the evaluation metric. For all other tasks, we use student-teacher grading [17] as the quality metric with GPT-4 acting as the teacher. All experiments were conducted on NVIDIA A100 machines using 8 X 80GB GPUs.\nB. Code Generation - NVIDIA Benchmark\nTable IV compares various models on NVIDIA Human Eval [16] including OpenAI GPT-3.5, GPT-4, verilog-sft [16], HDL-GPT, and HDL-GPT2 at a temperature of 0.8.\nWe have directly cited the verilog-sft model's metrics on these benchmarks as published in [16] for comparisons as we do not have direct access to the model. The remaining models are evaluated on three metrics: pass@1, pass@5, and pass@10. HDL-GPT significantly outperformed OpenAI GPT3.5, GPT4, and verilog-sft models at all pass rates. HDL-GPT2 delivered the highest performance surpassing HDP-GPT, with pass rates of 60.6% at pass@1, 78.5% at pass@5, and 81.4% at pass @10.\nTable V compares all the models at a temperature of 0.8 on the machine evaluation benchmark. HDL-GPT showed better performance with 48.6% for pass@1, 71.9% for pass@5, and 77.6% for pass@10. Similarly, HDL-GPT2 shows even better performance and outperformed all with pass rates of 67.0% for pass@1, 86.0% for pass@5, and 90.2% for pass@10.\nC. Code Analysis tasks\nWe use a student-teacher grading evaluation scheme on the same benchmarks as in the prior experiments for code analysis, bug finding, bug fixing, and testbench generation tasks. We evaluated four LLMs: Mixtral, Llama, OpenAI GPT-3.5, and GPT-4 as teacher candidates. The teacher model is prompted to perform scoring on a scale of 0-5 on the given HDL code based on detailed description, code quality, and code correctness. We then calculate the average grading scores (0-5) for each evaluation set and normalize to a range between 0 and 1 as shown in Equation 1. This is the normalized score reported in experiments in Figure 2.\nNormalized Score = Average Score for a Eval Set / Maximum Score  (1)"}, {"title": "V. CONCLUSION", "content": "The work in this paper underscores the critical role of high-quality training data, input prompts, and fine-tuning methods (PEFT and Neftune) in creating a SOTA LLM, called HDL-GPT, for HDL code generation, explanation, bug detection/repair, and testbench creation. The experimental results against public benchmarks demonstrate that the model's performance, as well as the quality of the generated code, rely heavily on the training data's quality. Our future research will delve into optimizing HDL-GPT's performance for IC design flows, creating representative benchmark suites for IC design tasks, and enhancing its potential for high-quality code generation and task performance in the IC design flows."}]}