{"title": "Zero-Shot Visual Reasoning by Vision-Language Models: Benchmarking and Analysis", "authors": ["Aishik Nagar", "Shantanu Jaiswal", "Cheston Tan"], "abstract": "Vision-language models (VLMs) have shown impressive zero- and few-shot performance on real-world visual question answering (VQA) benchmarks, alluding to their capabilities as visual reasoning engines. However, the benchmarks being used conflate \"pure\" visual reasoning with world knowledge, and also have questions that involve a limited number of reasoning steps. Thus, it remains unclear whether a VLM's apparent visual reasoning performance is due to its world knowledge, or due to actual visual reasoning capabilities.\nTo clarify this ambiguity, we systematically benchmark and dissect the zero-shot visual reasoning capabilities of VLMS through synthetic datasets that require minimal world knowledge, and allow for analysis over a broad range of reasoning steps. We focus on two novel aspects of zero-shot visual reasoning: i) evaluating the impact of conveying scene information as either visual embeddings or purely textual scene descriptions to the underlying large language model (LLM) of the VLM, and ii) comparing the effectiveness of chain-of-thought prompting to standard prompting for zero-shot visual reasoning.\nWe find that the underlying LLMs, when provided textual scene descriptions, consistently perform better compared to being provided visual embeddings. In particular, ~18% higher accuracy is achieved on the PTR dataset. We also find that CoT prompting performs marginally better than standard prompting only for the comparatively large GPT-3.5-Turbo (175B) model, and does worse for smaller-scale models. This suggests the emergence of CoT abilities for visual reasoning in LLMs at larger scales even when world knowledge is limited. Overall, we find limitations in the abilities of VLMs and LLMs for more complex visual reasoning, and highlight the important role that LLMs can play in visual reasoning.", "sections": [{"title": "I. INTRODUCTION", "content": "The development of vision-language models or VLMs [1, 2, 3, 4, 5, 6] has gained considerable attention in recent years given their application in developing general-purpose multimodal intelligence. Similar to the zero-shot abilities observed in large language models or LLMs [7, 8, 9, 10] for language tasks, VLMs such as Flamingo [4] and BLIP-2 [5] have shown impressive zero- or few-shot reasoning abilities for language-vision tasks. Notably, they have been shown to surpass task-specific state-of-the-art models [4, 5] when finetuned on common visual question answering (VQA) benchmarks, including VQAv2 [11] and OK-VQA [12]. Furthermore, recent works [13, 14] have also shown how multimodal chain-of- thought (CoT) reasoning, wherein both language and vision modalities are used to elicit multi-step inference, improves the performance of models on multimodal question answering benchmarks such as ScienceQA [13]. These findings suggest that similar to LLMs, with increases in model size [15] and advanced prompting techniques [16, 17, 18], VLMs can exhibit stronger reasoning capabilities and operate as instruction-prompted zero- or few-shot visual reasoning engines.\nHowever, the current VQA benchmarks [11, 12, 19] used to evaluate the visual reasoning abilities of VLMs predominantly contain questions requiring only a few reasoning steps, and they often conflate visual reasoning with factual or world knowledge. While open-world visual reasoning certainly relies on knowledge of the world, it is important to recognize that visual reasoning at its core encompasses a wide range of cognitive processes including scene interpretation, memory manipulation, spatial reasoning or attention, and logical or semantic inference. To illustrate the above points further, consider the example question \"Who is wearing glasses?\" (given an image of two individuals) in the popular VQAv2 benchmark. A VLM's accurate answer to this question may simply be due to world knowledge about \u201cglasses\u201d and different categories of \"persons\", and not necessarily due to better visual reasoning capabilities. Similarly, the OK-VQA dataset is particularly designed to test how well models can utilize general world knowledge for VQA, and contains questions such as \"What phylum does this animal belong to?\" (given an animal image).\nAs such, based on the evaluation benchmarks and analysis in existing works, it is uncertain whether a model's apparent visual reasoning performance is due to its knowledge of the world, or its actual visual reasoning capabilities.\nThus, in this work, we propose to systematically analyze and benchmark zero-shot visual reasoning capabilities of VLMs through the usage of synthetic datasets. Specifically, we utilize the CLEVR [20] and PTR [21] datasets, which contain questions requiring minimal world knowledge, but a broader range of \"reasoning steps\" and primitive visual reasoning operations. Moreover, these datasets provide detailed meta-information for each (question, image) pair, including a complete symbolic scene description, as well as a step-by-step functional program"}, {"title": "A. Summary of Experiments and Findings", "content": "We focus on investigating two novel aspects of zero-shot visual reasoning in VLMs. Firstly, we compare the performances of VLMs versus LLMs. Specifically, we compare a \"traditional VLM\" (i.e. an LLM receiving scene information as visual embeddings from a base vision model) against an LLM simply receiving a completely textual representation of the scene. We find that LLMs consistently outperform VLMs that utilize the same base LLMs. Specifically, in the case of the BLIP2-Flan-T5 [5] model, using only its base LLM, i.e. Flan- T5 [8], without the visual front-end achieves ~18% higher accuracy on the PTR dataset, while GPT-4 was ~17% more accurate than GPT-4V on CLEVR. One key takeaway is that for questions which can be solved in 2 to 5 \u201creasoning steps\", LLMs show performance levels which are significantly above chance, suggesting that LLMs may in fact possess reasonable capabilities as zero-shot visual reasoning engines.\nSecondly, we study how CoT prompting compares to standard prompting for zero-shot application of these models in the context of VQA. We find that CoT prompting for visual reasoning in LLMs only obtains better results than standard prompting at large model scales (in our case for the 175B GPT- 3 turbo model) and performs worse for smaller models. For LLMs and VLMs, we observe trends of emergence of CoT rea- soning in zero shot settings even when the model's knowledge and context about the world is restricted. Furthermore, owing to the use of synthetic datasets to benchmark VLMs which are not explicitly trained on reasoning on synthetically rendered scenes, we also observe than increase model scale shows signs of improving CoT reasoning capabilities. This indicates that model scaling and CoT could potentially be used to extend and improve zero-shot reasoning performance for multimodal models on previously unseen settings."}, {"title": "B. Contributions", "content": "(1) To our knowledge, we are the first to systematically benchmark zero-shot visual reasoning capabilities of VLMs using synthetic datasets, disentangling the impact of world knowledge, so as to assess \u201cpure\u201d visual reasoning by models.\n(2) We compare the zero-shot VQA performance of VLMs against LLMs, and find that LLMs receiving only ground- truth textual scene information consistently perform better than when provided with visual embeddings.\n(3) Consistent with previous studies on CoT for language tasks [16], we find CoT for visual reasoning in LLMs also seems to emerge for larger model sizes even when the model's world knowledge is limited.\n(4) We analyze the visual reasoning performance of VLMs and LLMs under various factors including the number of \"reasoning steps\", question types and model scale. Our overall analysis indicates the limitations of VLMs and LLMs for complex visual reasoning and highlights the important role LLMs can play in enhancing visual reasoning capabilities."}, {"title": "II. RELATED WORK", "content": "Benchmarking reasoning abilities of LLMs and VLMs.\nSince the initial demonstration of LLMs as being effective few-shot learners [7], multiple works [7, 8, 10, 18, 22] have sought to refine the design and training of LLMs, besides comprehensively benchmarking [23, 24, 25] their reasoning abilities on language-specific tasks. More recently, the development of VLMs [1, 2, 3, 4, 5, 6] has drawn on advancements in both LLMs and vision-foundation models leading to their prompt- based application for vision-language tasks [4, 5, 6, 26] such as image captioning, text-guided image editing and general VQA. These works have evaluated the performances of VLMs on prominent VQA benchmarks including VQA-v2 [11], OK- VQA [12], GQA [19] and VizWiz [27] in zero-shot, few- shot and fine-tuned settings. However, these analyses are not sufficient to conclude the \u201ctrue\u201d visual reasoning capabilities of VLMs since the datasets typically conflate world knowledge with visual reasoning and require only a limited number of \"reasoning steps\" [28]. Further, these works have not assessed whether LLMs by themselves when provided textual scene representations are capable of visual reasoning. Thus, our work aims to more comprehensively evaluate the zero-shot visual reasoning capabilities of VLMs and their underlying LLMS by utilizing synthetic datasets.\nSynthetic datasets to disentangle reasoning capabilities from world knowledge. There are several synthetic datasets which can disentangle world knowledge from reasoning in different ways. [29] is a dataset designed for visual reasoning tasks. The images are synthetic and often involve simple shapes and layouts, ensuring the focus is on reasoning rather than world knowledge. [30] generates abstract visual scenes and accompanying textual descriptions designed to test various linguistic and visual phenomena. [31] is a synthetic visual reasoning dataset inspired by the structure of Raven's Progres- sive Matrices, a popular human IQ test. This format ensures that success on the task requires true visual reasoning and pattern recognition, rather than relying on learned associations or world knowledge. [20] and [21], the datasets used in this study, are tailored for disentangling world knowledge from visual reasoning. Their machine-generated questions ensure controlled complexity to test visual reasoning without relying on pre-trained visual or linguistic biases. Their rich annotations and scene metadata are ideal for testing reasoning abilities not only the visual and spatial aspects of the scene, but also the physical interactions in a wide range of reasoning types.\nCoT prompting for zero- or few-shot reasoning. The development of CoT techniques [16, 17, 32], wherein models are elicited to reason in multiple steps, has been shown to significantly benefit zero- or few-shot performance of LLMs"}, {"title": "III. EXPERIMENTS", "content": "A. Experimental Design\nOur experiment design philosophy was primarily guided by the major benchmarks and analysis which we wanted to perform in this study. Our first goal was to analyze the impact of scene information representation in the form of text or images on the model's zero-shot reasoning capabilities. Based on this, we provided the complete scene information in text format to the LLM (the Flan-T5 model family) using the scene metadata, while providing the scene image to the model's VLM counterpart, which was the BLIP-2 Flan-T5 model family [5]. To gauge the impact of the text-based scene metadata on VLM performance, we also ran a set of experiments providing both the scene metadata and the image to the VLM. Through this setup, we could study areas where the VLM might fall short in terms of information extraction and reasoning, and also identify if there were specific reason- ing categories where direct visual representation might be a clear advantage. The second goal was to identify the impact of Chain-of-Thought prompting on the reasoning abilities of LLMs and VLMs as well as its performance trends over scale, when the models world knowledge is limited. To achieve this we designed experiments which could benchmark different scale models of the same LLM and their counterpart VLM families on CoT and Standard Prompts.\nB. Experimental Setup\nDatasets. We use two datasets: (1) CLEVR [20], a synthetic VQA dataset containing images of 3D-rendered objects; each image comes with a number of compositional questions of various types, and (2) PTR [21], a dataset for part-based conceptual, relational and physical reasoning. Since the scene metadata was only provided for the images in the train and validating sets (and not the test sets), we use the validation sets of each of these datasets for testing. This allowed us to automatically generate text descriptions of the scenes to compare performance of Visual Language Models (VLMs) with the pure LLMs. There is neither training nor validation per se, since our experiments are in a zero-shot setting.\nStandard prompting. Our standard prompting procedure included providing the models with the relevant scene infor- mation (the image in the case of VLMs, or the scene metadata in the case of pure LLMs), a setup prompt and instructing the model to provide the final answer directly in one word. Since the models were tested in a purely generative setting, the models would often generate the correct answer, but not use the correct terminology, e.g. calling a cyan object light blue. To maintain the generative setting but align the model answers to match the scene terminology, it was provided with the setup prompt, which gave information on the possible attributes, colors, shapes etc. which could be present.\nChain-of-Thought Prompting. To elicit CoT reasoning in a zero-shot setting, we follow the prompt template of [17]. In addition to the same information and setup prompt provided in the standard prompt, we add \"Let's think step by step\" before each answer. We also developed a format prompt to force the model to give a final one-word answer at the end.\nVisual Language Models. We used three VLMs tuned for instructed generation for the experiments. These are BLIP2- Flan-T5-XL (3B) and BLIP2-Flan-T5-XXL (11B) [5] and GPT-4V. Using these models allowed us to compare the performance of the VLMs against the pure LLM versions of these models. Pretrained weights from LAVIS [33] were used for the BLIP-2 Flan-T5 model family. Language Models. For comparing VLMs to LLMs, we used three LLMs: Flan-T5-XL (3B), Flan-T5-XXL (11B) [8] and GPT-4. For CoT reasoning, such abilities have been shown to emerge at a scale of more than 100B [16]. Thus, we also tested our setup on GPT-3.5- Turbo (175B) [22] and smaller versions of GPT to analyse the impact of scale on CoT."}, {"title": "IV. RESULTS AND ANALYSES", "content": "A. Comparing LLMs with scene descriptions versus VLMs\nLLMs with scene descriptions outperform VLMs: Figure 2 shows the impact of visual grounding using BLIP-2 on the reasoning effectiveness of the models. Pure LLMS generally outperform or have similar performance to their counterpart VLM models across both scales and datasets. A t-test was performed to test if the pure LLMs performed better than VLMs. A p-value of 0.0088 indicates that the difference is statistically significant. This might seem counter-intuitive, as one might expect the VLM to be able to effectively utilize the \"visual frontend\" provided by the image encoder used in the BLIP-2 setup for querying the relevant aspects of the image. There are 2 possible explanations: 1) There are underlying issues in the VLM architecture which prevent the visual front- end from providing relevant information to the model. 2) The complexity of the tasks is not enough that a visual front-end which queries only the relevant information from the scene can be better than providing the complete, unfiltered information to the reasoning engine (the LLM).\nTo guard against data contamination (i.e. LLMs exposed to CLEVR or PTR during training), we ran image-free base- lines. Model performance was 36.85% for CLEVR (chance = 36.86%) and 10.16% for PTR (chance = 8.03%). The models performed at chance, indicating no contamination.\nLLM advantage for CLEVR versus PTR: The difference in performance between the LLM and the VLM is more pronounced in PTR than CLEVR. For CLEVR, the LLM outperforms the VLM by roughly 6-7%, while for PTR the gap is roughly 17-18%. One possible explanation is that the objects in PTR are more complex, with multiple parts, hence the task for the VLM's visual frontend is more challenging, and more errors and uncertainty are introduced. Providing the ground- truth scene description to the LLM eliminates this challenging"}, {"title": "B. Chain-of-Thought (CoT) Reasoning", "content": "Overall results: Figure 6 presents a concise summary of the main outcomes of Chain-of-Thought reasoning on the two datasets. Interestingly, the open source Flan-T5-XXL (11B) model with standard prompting achieves the best performance, outperforming even GPT-3.5-Turbo (175B), which is over 15x larger. This is true for both datasets, and regardless of CoT or standard prompting for GPT-3.5-Turbo. Flan-T5-XL (3B) only performed marginally worse than its larger 11B cousin.\nAnalysis by number of \"reasoning steps\": As expected, performance generally drops with more \u201creasoning steps\" (Fig. 7). For CLEVR, CoT prompting produced a small but consistent performance gain over standard prompting. For PTR, the CoT advantage is less consistent, with standard prompting sometimes performing better.\nAnalysis by question family (CLEVR): CoT prompting shows a noticeable improvement in the \"count\" question fam- ily, with some improvement in \u201ccompare attribute\", \"compare numbers\u201d and \u201cexist\u201d categories. \u201cQuery attribute\" questions in CLEVR typically involve direct queries about object prop- erties, often solvable in a single step consistent with the fact that overall accuracy is highest for this question family. This could explain why CoT does not provide a significant advantage in this simple, often one-step question family.\nAnalysis by question family (PTR): CoT prompting leads to improvements for \u201crelation\u201d and \u201carithmetic\u201d questions. For \"analogy\" questions, CoT lowers performance. CoT assists in \"relation\" and \"arithmetic\" questions by breaking down the task into simpler steps, aiding in the understanding of sequen- tial operations. On the other hand, for \"analogy\u201d questions,"}, {"title": "V. CONCLUSION", "content": "We benchmarked zero-shot visual reasoning capabilities of VLMs and LLMs. We used synthetic datasets to mitigate the impact of world knowledge on visual reasoning performance, and to also evaluate reasoning over a broad range of \"reasoning steps\". We studied two novel aspects of zero-shot visual reasoning: i) evaluating how a VLM's base LLM performs when provided textual scene description, compared to when provided with a visual embedding, and ii) the effectiveness"}]}