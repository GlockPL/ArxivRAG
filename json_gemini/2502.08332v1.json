{"title": "MODIFICATION AND GENERATED-TEXT DETECTION: ACHIEVING DUAL DETECTION CAPABILITIES FOR THE OUTPUTS OF LLM BY WATERMARK", "authors": ["Yuhang Cai", "Yaofei Wang", "Donghui Hu", "Gu Chen"], "abstract": "The development of large language models (LLMs) has raised concerns about potential misuse. One practical solution is to embed a watermark in the text, allowing ownership verification through watermark extraction. Existing methods primarily focus on defending against modification attacks, often neglecting other spoofing attacks. For example, attackers can alter the watermarked text to produce harmful content without compromising the presence of the watermark, which could lead to false attribution of this malicious content to the LLM. This situation poses a serious threat to the LLMs service providers and highlights the significance of achieving modification detection and generated-text detection simultaneously. Therefore, we propose a technique to detect modifications in text for unbiased watermark which is sensitive to modification. We introduce a new metric called \"discarded tokens\", which measures the number of tokens not included in watermark detection. When a modification occurs, this metric changes and can serve as evidence of the modification. Additionally, we improve the watermark detection process and introduce a novel method for unbiased watermark. Our experiments demonstrate that we can achieve effective dual detection capabilities: modification detection and generated-text detection by watermark.", "sections": [{"title": "1 Introduction", "content": "The powerful generative capabilities of LLMs have greatly enhanced the human capabilities to create text. Whether in literary creation, news writing, or technical documentation, people can complete work more efficiently and quickly with the help of LLMs. However, this technological advance has also raised concerns about the abuse of LLMs. LLMs can generate creations that are difficult to distinguish from human works and potentially create misleading statements or false information[1, 2]. Therefore, implementing practical detection tools to determine whether a text is generated by AI becomes particularly important [3, 4].\nWatermark is a promising method to reduce the risks of LLM abuse [5, 6, 7, 8, 9, 10]. Previous watermarking methods[11, 12, 13, 14, 15, 16, 17] often identify machine-generated text based on statistics, which counts the number"}, {"title": "2 Related Works", "content": ""}, {"title": "2.1 Watermarking for LLM", "content": "Kirchenbauer et al. [11] introduced a pioneering watermarking framework tailored for LLMs that embeds watermarks with minimal text quality impact. It creates \"green\" token list randomly before generating tokens, and encouraging the model to choose from them. This type of watermarking method increases robustness, but it disrupts the output distribution of the model[20]. To reduce the impact on the quality of text generation, Lee[13] considered text entropy to modify logits adaptively. To enhance the robustness of the watermark, Zhao[12] improved the robustness by fixing the division of the red and green lists. However, the above methods cannot avoid affecting the quality of the text generated by the LLM. To ensure the quality of text generation, Hu et al.[17] and Wu et al. [14] proposed unbiased watermark algorithms that can maintain probability distribution while embedding watermark."}, {"title": "3 Method", "content": ""}, {"title": "3.1 Preliminary", "content": "In LLMs generation, $P_M$ represents the probability distribution generated by pre-trained LLM, and V is the overall vocabulary set. In a typical LLM generation task, LLM receives a prompt $X_{-n_p:0}$ and outputs a sequence $X_{1:n}$ according to the prompt and the generated tokens $X_{-n_p:i-1}$ by gradually generating the next token $x_i$. When generating the token $x_i$, the probability of the token in the vocabulary set V is given by the conditional probability distribution $P_M(X_i | X_{-n_p:i-1})$.\nWhen embedding watermark by d-reweight[17], the output probability distribution is adjusted from $P_M(x_i | X_{-n_p:i-1})$ to $P_{M,w}(X_i | X_{-n_p:i-1}, \\theta_i)$. The cipher $\\theta_i$ is usually generated by a secret key $k \\in K$ and a fragment of the previous context, named texture key, $c_t$. Each $\\theta_i$ is independent and follows the same distribution $P_\\theta$."}, {"title": "3.2 Observations of Discarded Tokens", "content": "There is no existence research for modification detection on text by watermark. To achieve modification detection, we must observe the significant differences in the generated watermarked text before and after modification. One type of difference can be reflected in discarded tokens, which are not involved in detection and fail to function as evidence for detection results. However, for watermarks that pursue robustness [11, 13, 12], they are insensitive to modification because tokens with robust watermark are hard to be affected by modified tokens, resulting in inapparent difference because the number of discarded tokens increases slowly. Therefore, they are not suitable for modification detection. Meanwhile, we turn to another type of watermark: watermarks that are sensitive to modification. When face modification, the modified token influences the next several tokens even if these tokens are not modified, which causes obvious changes on discarded tokens and function as evidence for the existence of modification."}, {"title": "3.3 Inconsistent Distortion", "content": ""}, {"title": "3.4 Modification Detection", "content": "We described the modification detection method IDD in Algorithm 1. Modification score $n_{it}$ is introduced to represent the number of inconsistent tokens that $x' \\ne x_i$, which means token $x'$ in the text is inconsistent with token $x_i$ sampled by watermark method. According to $\\delta$ method, we constructed the same sampling function as the generation to restore the scene at the time of generation. Only one watermarked token can be generated based on sampling functions, and tokens different from the watermarked token can be classified as inconsistent tokens. We judge the tokens by $f_{it}(x_i)$ as follows:\n$f_{it}(X_i) = \\begin{cases}\n1, \\text{ if } x' \\ne x_i \\\\\n0, \\text{ if } x' = x_i\n\\end{cases}$\nwhere 1 represents an inconsistent token, and 0 confirms a consistent token. We accumulate the number of inconsistent tokens to get the final result $n_{it}$:\nn_{it} = \\sum_{i=1}^{N} f_{it}(X_i)$\nThen, we set a threshold. If $n_{it}$ is larger than the threshold, confirm the modification."}, {"title": "3.5 Enhance Watermark Detection", "content": "For the watermarked text generated by d-reweight, the inconsistent tokens significantly affect the detection score of the original method LLR and fail detection. To enhance the robustness and mitigate the impact of inconsistent tokens on watermark detection, we use an improved LLR for watermark extraction, named drLLR, as shown in Algorithm 1. Traditional LLR score results in LLR($x_i$) = -$\\infty$ when the token $x_i$ is modified, then makes the detection fail. However, if a token is modified and results in a negative infinite score, it should not be used as a detection basis but discarded directly. Therefore, we set the score of inconsistent tokens to 0 to drop these tokens:\ndrLLR(x_i) = \\begin{cases}\nlog \\frac{P_{M,w}(i-n_p:i-1, \\theta_i)}{P_M(X_i | x_{-n_p:i-1})}, \\text{ if } x_i = x_i\\\\\n0, \\text{ if } x_i \\ne x_i\n\\end{cases}$\nwhere $x' \\ne x_i$ indicates that token $x'$ in the text is inconsistent with token $x_i$ sampled by watermark method.\nWe only calculate the sum of the score of other tokens and divide it by the total number of tokens to reflect the strength of the watermark. If the average score drLLR is greater than the threshold, we detect the watermark in the text and"}, {"title": "4 EXPERIMENTS AND RESULTS", "content": ""}, {"title": "4.1 Settings and Datasets", "content": "To simulate a realistic low-entropy environment, we obtain 1,000 questions from the PubMedQA dataset [21] and use them as prompts. We use the model OPT-6.7B proposed by Zhang \u201cunpublished\u201d [22] and set the sampling method to Top-p (p = 0.9) and Top-k (k=50) to generate two datasets, one without watermark and the other with watermark, each containing 1000 pieces of text and max length of each text is 30. We choose three types of modification that are commonly used in actual use: addition, deletion, and replacement. We use a random perturbation parameter \u20ac to create datasets with different modification strengths. For example, e = 0.1 means 10% of tokens are modified in the datasets."}, {"title": "4.2 Baseline and Evaluation Metrics", "content": "Baseline For modification detection, we improve the KGW method[11] and use it as the baseline. We implement discarded tokens (which means these tokens are not involved in detection and fail to function as evidence for detection result) in KGW by counting the number of tokens in the red token set and Z-score, which reflects the number of red tokens to detect modification, then evaluate the performance of modification detection. Besides, we build the test based on a hypothesis: modifications damage part of watermarks, but there are enough identifiable watermarks to detect. If score < |threshold|, we report watermarked but modified. For generated-text detection, we choose KGW and \u03b4- reweight[17] with the original LLR method maximin variant of the LLR (mmLLR) as watermark baselines. Specifically, we set KGW with a fixed green list proportion y = 0.5 and diverse logit increments 8 =1. The hyper-parameter grid_ size of mmLLR is set to 10.\nEvaluation Metrics Currently, there are no studies about modification detection based on watermark for LLM-generated text, so we design our own watermark modification detection indicators. We compute TPR, FPR, Recall, F1-score to evaluate the ability of modification detection on different datasets. Especially, we set watermarked and modified text as positive examples, while watermarked and non-modified text as negative examples. For generated-text detection, we evaluated the performance of different watermarking methods on watermark strength. We report the Area Under Curve (AUC) of watermark detection."}, {"title": "4.3 Modification Detection Studies", "content": "As shown in Fig. 3, after different types of attacks, the number of discarded tokens (in the red list) from the KGW method remains relatively unchanged, meaning this method is insensitive to modification. Conversely, discarded tokens (inconsistent tokens) from d-reweight with IDD method is nearly 0 on the unaltered dataset, but it rapidly increases on the tampered dataset, showing a significant difference. This illustrates the potential of achieving modification detection: d-reweight watermark is easily broken after being tampered with and affects the extraction of subsequent tokens, causing obvious changes in discarded tokens. For d-reweight, the impact can be reported by the number of inconsistent tokens, demonstrating the feasibility of using the IDD method to detect modification effectively."}, {"title": "4.4 Experiments for Generated-Text Detection", "content": "Apart from modification detection, generated-text detection is an indispensable function for watermarks. So we show the result of AUC under different perturbation strength and perturbation methods in the Table 3. The drLLR method performs well in low entropy scenarios, achieving an AUC of 0.98 on the original watermarked dataset, surpassing both the KGW and mmLLR method (orignal LLR method). Compared to the KGW method, the AUC of drLLR slightly decreases when the attack strength reaches 0.1, and the decrease accelerates when it reaches 0.2. This is attributed to the characteristic of inconsistent distortion: the watermark carried by the text is prone to be broken, affecting the extraction of watermark on subsequent tokens, resulting lower detection score. Therefore, its robustness against modification is challenging to match the watermark that pursues robustness, yet it still achieves a certain level of robustness under modification attacks. This demonstrates that our detection method achieves dual detection capabilities for the output of LLM by effectively detecting modification in text and exhibiting certain robustness against modification."}, {"title": "5 Conclusion", "content": "We propose a novel conception of modification detection based on watermark for LLM-generated text to defend against spoofing attacks. We leverage the inconsistent distortion characteristic of unbiased watermark 8-reweight and address the shortcomings of traditional robust watermark methods that cannot detect modification effectively. Based on the inconsistent distortion, we introduce a modification detection method called IDD, which detects inconsistent tokens beside modified tokens. We propose an improved watermark detection method named drLLR, which enhances the robustness by dropping the score of inconsistent tokens. Experimental results demonstrate the effectiveness of IDD in detecting various types of modifications with high accuracy. At the same time, the drLLR method shows satisfactory performance in generated-text detection, which means we achieve effective dual detection capabilities by watermark. We hope this work can alleviate the potential threat of spoofing attacks and provide new references for designing LLM watermarks."}]}