{"title": "Leveraging Large Language Models in Code Question Answering: Baselines and Issues", "authors": ["Georgy Andryushchenko", "Vladimir Ivanov", "Vladimir Makharev", "Elizaveta Tukhtina", "Aidar Valeev"], "abstract": "Question answering over source code provides software engineers and project managers with helpful information about the implemented features of a software product. This paper presents a work devoted to using large language models for question answering over source code in Python. The proposed method for implementing a source code question answering system involves fine-tuning a large language model on a unified dataset of questions and answers for Python code. To achieve the highest quality answers, we tested various models trained on datasets preprocessed in different ways: a dataset without grammar correction, a dataset with grammar correction, and a dataset augmented with the generated summaries. The model answers were also analyzed for errors manually. We report BLEU-4, BERTScore F1, BLEURT, and Exact Match metric values, along with the conclusions from the manual error analysis. The obtained experimental results highlight the current problems of the research area, such as poor quality of the public genuine question-answering datasets. In addition, the findings include the positive effect of the grammar correction of the training data on the testing metric values. The addressed findings and issues could be important for other researchers who attempt to improve the quality of source code question answering solutions. The training and evaluation code is publicly available\u00b9.", "sections": [{"title": "1 Introduction", "content": "Generative AI plays a significant role in software engineering, and Code Question Answering is an emerging application within this domain. Question Answering over source code involves generating textual answers to code-related questions. These questions can be formulated by software engineers, project managers, or even by the other generative model. Integrating Generative AI in the form of"}, {"title": "2 Related Work", "content": "Advancements in natural language processing have significantly impacted code representation, analysis, and generation. This progess focuses on transformer-based models, which treat source code as token streams enriched with structural information. At inference, these models generate outputs autoregressively, producing text, code, or both. Notably, methods such as Code2Text have emerged in automatic code documentation. Such methods focused on code summarization, comment generation, and question answering over source code. This focus leverages large-scale pre-trained models to understand and interact with code, supporting applications such as legacy code comprehension and code snippet evaluation. Novel approaches in this domain use agent-driven LLMs for repository-level documentation generation [1].\nSeveral state-of-the-art datasets and models have contributed to this field. For example, the CodeQA dataset [2] includes Java and Python question-answer pairs derived from code comments. This inclusion facilitates code comprehension through models such as CodeBERT [12] and Transformer-based architectures. Some other examples include the CS1QA [3] dataset, originating from an introductory programming course, and the CodeQueries dataset [5], with its diverse set of queries and spans. These datasets provide valuable resources for training models on code-related questions. In addition, CodeBERT [12], RoBERTa [8], and GraphCodeBERT [11] have demonstrated their effectiveness in understanding and generating code-related responses by integrating semantic and syntactic features into their training.\nAdditionally, techniques for code summarization and representation have seen significant improvements. Indeed, several approaches have incorporated various pre-training objectives thus enchancing the ability to generate accurate and meaningful summaries of code. Examples of such approaches include Code2Seq [6] and DeepCom [10] that use code graphs for embedding representations, as well as multi-task models such as CodeT5 [14] and SPT-Code [9]. Meanwhile, methods for parameter-efficient fine-tuning, including prompt tuning and adapters, offer efficient ways to adapt large pre-trained models to specific Code Q&A tasks without requiring extensive computational resources. Together, these developments establish a robust foundation for exploring and improving Code Q&A with LLMs in practice."}, {"title": "3 Datasets", "content": null}, {"title": "3.1 Training Datasets", "content": "Unified Dataset To train a question-answering model on code, we compiled a unified dataset\u00b2 from the public datasets: CodeQA [2], CS1QA [3] and a subset of PCSD [13]. Given Python's clear syntax and its widespread use in academia and industry, we focused exclusively on Python code from both datasets to enhance the relevance of our research. Our dataset choices were limited by the availability of suitable public data.\nCodeQA comprises functions, questions, and short answers generated from a code-comments corpus using syntax parsing, with subsets in both Java and Python. The dataset contains \"What\", \"Where\", \"When\", \"How\", \"Why\" questions (\"Wh-questions\") and Yes/No questions. The detailed information about CodeQA dataset may be found in the original paper.\nCS1QA contains Python code sourced from an online introductory programming course, including chat logs, questions, answers, and question types. The authors of the dataset split the questions in the dataset into the following types: \"variable\", \"code understanding\", \"reasoning\", \"code explain\", \"error\", \"algorithm\", \"usage\", \"task\", \"logical\". The original paper provides the other details of the dataset.\nPCSD comprises the samples of Python code with their summaries. Since this dataset contains only the code summaries, all samples have the same question: \"What does this code do?\".\nThe initial datasets contained grammatical errors and noisy examples, such as answers suggesting internet searches. During data exploration, we found cases where the code, questions, or answers were poorly aligned in content or structure, and removed examples with undesirable formatting. Some answers were excessively long, spanning multiple lines, which could lead to ambiguity in model responses, so we excluded those exceeding four lines. Certain types of questions, particularly, \"error\", \"algorithm\", \"usage\", \"task\", and \"logical\" were excluded from the unified dataset to avoid potential factual inaccuracies. We also eliminated answers containing irrelevant code or information, identified by specific keywords (see Appendix A.4 for details), to prevent confusion and improve training effectiveness. After filtering 12% of the initial data, we derived a unified dataset, presented in Table 1."}, {"title": "Unified Dataset with Grammatical Corrections", "content": "Given that the dataset contains grammatical errors, these errors should be naturally corrected to see whether the results improve. Therefore, we applied Language Tool\u00b3 to identify the grammatical errors in questions. However, we did not correct the answers because it would have involved correcting the reference answers, and thus, making the comparison less fair. Finally, we classified the grammatical errors using the Python wrapper for Language Tool, manually labeled the error types that occur more than twice whether corrected or not, and automatically corrected the selected types if the tool suggested a correction. The dataset is publicly accessible at Hugging Face4."}, {"title": "Unified Dataset with Generated Summaries", "content": "We hypothesized that including short textual summaries of the code may enhance answer. To develop an effective summarization model, we focused on utilizing the CodeT5+ [14] model with 220 million parameters. The weights of this model are publicly available5. For training and evaluation, the Docstring [4] dataset was used. This dataset consisted of a collection of summaries for Python functions. After the summarization model was developed, we generated the summaries for the code samples of the unified question answering dataset. These summaries were added to the samples of the unified dataset, providing additional contextual information about the code6."}, {"title": "3.2 Testing Datasets", "content": null}, {"title": "Testing Subset of Unified Dataset", "content": "We used the unified dataset as one of our testing datasets, and extracted a subset that was not included in the training phase, i.e. the testing subset. This testing dataset comprised 8,228 samples after filtering. Refer to Appendix A.1 for a sample example."}, {"title": "Testing Dataset Based on ClassEval Dataset (ClassEvalQA)", "content": "As an additional testing dataset, we created a dataset consisting of code excluded from both the pre-training and fine-tuning phases of the models that we experimented with. The ClassEval dataset [24] was manually curated 100 Python coding tasks at the class level, accompanied by human-written solutions. This ensures that the code in ClassEval has not been encountered by the LLM during its pre-training phase. Given the constraints of human resources, leveraging state-of-the-art LLMs like GPT-3.5 for data generation is a common practice. To establish a benchmark for evaluating our Code Q&A models at the class level, we generated question-answer pairs using GPT-3.57, producing approximately 20"}, {"title": "High Quality Subset of Unified Dataset", "content": "Manual inspection of the testing subset from the unified dataset revealed quality issues, including grammatical errors, unclear question formulations, irrelevant answers, and overly verbose answers. Such problems might threaten the validity of our experiments. To mitigate this risk, we curated a subset of the initial testing dataset, manually selecting 100 high-quality examples that do not exhibit the aforementioned quality problems. For a sample example, see Appendix A.3."}, {"title": "4 Modeling", "content": "LLMs demonstrate an impressive power of text understanding and generalization [17]. The pre-training procedure makes the LLMs understand a vast set of source code snippets. These code snippets can be written in different ways and connected to different domains, such as backend, data science, game development, etc. Thus LLMs can perceive highly varying code.\nAs the models for experimenting we have chosen StarCoder [19] and DeepSeek-Coder [20]. StarCoder10 has a GPT-2 architecture with 15.5 billion parameters. It was pre-trained on a dataset of 1 trillion tokens. DeepSeek-Coder model is a decoder-only Transformer. We selected the 6.7 billion model version11. DeepSeek-Coder was pre-trained made on a dataset of 2 trillion tokens collected by the authors of the model.\nWe fine-tuned StarCoder and DeepSeek-Coder with the qLORA [18] approach with various low-rank adaptation (LoRA) hyperparameters. Usually, the fine-tuning of the model took about 24 hours on a single A100 GPU with 80 GB of video memory. We fine-tuned all the models on the different versions of the dataset: Unified Dataset, Unified Dataset with Grammar Correction, and Unified Dataset with Summaries."}, {"title": "5 Evaluation", "content": "This section describes the metrics that we used to evaluate our fine-tuned versions of StarCoder and DeepSeek-Coder models on the collected datasets. In addition, this section discusses the model results and provides error analysis."}, {"title": "5.1 Metrics", "content": "To compare the generated text with the true answer we evaluated the values of the following automated metrics:\nBLEU [21] is an algorithm to compare the similarity between generated and reference sentences. The score represents the n-gram precision between the machine-generated sentence and the reference translations with a brevity penalty. We chose n = 4 since the metric correlates well with human judgements with such n. BLEU scores range from 0 to 1, where the perfect match of the candidate with the reference was indicated by 1.\nBERTScore [22] is a semantic similarity evaluation metric for text generation that leverages the embeddings from pre-trained BERT [7] model. This metric compares machine-generated and reference sentences using cosine similarity, and considers the cases with synonyms. As a backbone model we used microsoft/deberta-xlarge-mnli checkpoint as recommended by the authors of the metric. BERTScore produces three values: Precision, Recall, and F1. The F1 value was considered since it includes both Precision and Recall values. All the values ranged from 0 to 1, where 1 was the prefect result.\nBLEURT [23] is a BERT-based evaluation metric for text generation. The metric uses a BERT-based regression model trained on publicly available collection of ratings to obtain the value. As a backbone model we used BLEURT-20 checkpoint as recommended by the authors of the metric. The metric values were mostly between 0 and 1, where 1 indicates the best similarity. However, sometimes the values may be below 0 and over 1, as the authors of the metric state.\nFl-score (F1) is the harmonic mean of token-based precision and recall. Precision is the fraction of common tokens in the candidate that appear in the reference text, while recall is the fraction of common tokens in the reference text that appear in the candidate. The F1-score ranged from 0 to 1 where 1 means the highest correspondence of the candidate to the reference.\nExact Match (EM) calculates the percentage of generated answers that exactly match the reference answers. The higher the percentage, the more similar the candidate is to the reference.\nWe chose BLEU-4, BERTScore F1, BLEURT metrics for our experiments based on their popularity and robustness [15]. In addition to these metrics, we considered F1-score and Exact match scores that are often used in question answering tasks [16]. However, when analyzing the results, we excluded F1-score from the further analysis due the high deviation of the values as shown in Figure 2.\nNote that in our experiments we calculated the average values only for BERTScore F1 and BLEURT metrics, while BLEU-4 and Exact Match values were calculated over the entire testing set."}, {"title": "5.2 Results", "content": "This subsection discusses the results of the experiments regarding the model hyperparameters. The subsection provides detailed results with the metrics values obtained from the experiments.\nModel Hyperparameters StarCoder and DeepSeek-Coder were trained with different sequence length and LoRA parameters. Evaluating the training results showed that the optimal sequence length was 1024, the optimal LoRA rank was 32 with LoRA alpha 64, and the dropout was 0.05 for both models.\nDecoding Strategy We also tried different decoding strategies to find out whether these strategies affected the quality of the generated answers. The tried decoding strategies were the following: greedy, sampling, sampling with temperature 0.6, top three sampling, sampling with top probability 0.99. Our experi-"}, {"title": "6 Discussion of Results and Open Issues", "content": null}, {"title": "6.1 Metrics Analysis", "content": "As Table 2 shows fine-tuning StarCoder on a dataset with corrected grammar improved all the metrics, while adding summaries to the model decreased the"}, {"title": "6.2 Error Analysis and Further Improvement", "content": "To determine the reasons of model mistakes, we manually reviewed the model answers on the high quality testing dataset. As a result of this review, we determined the following possible reasons:\nUnclear question. The question was formulated vaguely and ambiguously, so the resulting answer may not be determined.\nCode lacks necessary information. Answering the question required the information that the code did not contain.\nIrrelevant true answer. In fact, the true answer did not correspond to the question.\nExternal usage question. The question corresponded to the possible usages of the provided code. For example, the question might ask in which cases the code may be useful.\nRedundant information in the true answer. The true answer contains the redundant text unrelated to the question.\nOur findings revealed several common issues that affected the quality of the answers. The most recurring problems were related to insufficient context in the prompt and unclear question formulation. Such issues may be addressed through prompt improvement."}, {"title": "6.3 Limitations of Our Code Q&A Solution", "content": "As the final model for the solution, we preferred StarCoder model over DeepSeek-Coder. This preference was due to the metric values in Table 2, Table 3, and Table 4. Generally, StarCoder model happened to have higher values.\nThe proposed solution currently supports only Python due to lack of data in other programming languages. However, the underlying StarCoder model is a multilingual model, so zero-shot capabilities are possible. However, evaluating such capabilities is left for future work.\nOur model inherited the context length of StarCoder model that was equal to 1024 tokens. This length corresponds to approximately 700-800 words for code, question and answer altogether. If we assume the average code line is 15 words long, this assumptions leads to a maximum of 50 lines of code. A well-regarded programming style suggests that functions should be up to 20 lines long, so this context should be enough for function-level question answering. However, classes and files were longer than 50 lines in average, so class- or project-level question answering are left for future work.\nAnother open issue was the evaluation of the question answering. This issue stemmed from the lack of human-labeled datasets and the weaknesses of n-gram based metrics. Metrics like BLEU, which rely on n-grams, only reward the presence of correct words while disregarding synonyms and alternative expressions of the same meaning. Consequently, these metrics lose their correlation with human judgment beyond a certain threshold.\nThe final issue is the data quality. As discussed above, one of our datasets was synthesized from the code-summaries dataset, and another one was the history of a chat between students and teaching assistants containing grammatical errors and irrelevant answers. In fact, we obtained the most promising results from the manually collected and curated datasets such as ClassEvalQA and High Quality testing datasets. Therefore, for future studies we need to collect a new human-labeled dataset, as well as feedback for our solution in working scenarios.\nChatGPT12 could be a better baseline for code question-answering. However, our research study was funded with a purpose of developing a solution that could be run locally. This requirement is critical for the vast majority of companies for security and code safety reasons.\nCurrent solution is also subject to general language models issues, such as hallucinations, speaking on sensitive topics, variable-name dependence. The usual"}, {"title": "7 Conclusion", "content": "Question answering over source code is a tough but highly relevant problem. Indeed, a solution could enhance developers' code comprehension, streamline onboarding, and facilitate working with legacy code. In our approach, we tackled this problem using a classical method: collecting training data, cleaning them up, and fine-tuning pre-trained encoder-decoder and decoder-only models. Preliminary experiments indicate that enhancing the model-generated answers is feasible through several avenues. Simultaneously, experiments reveal that utilizing StarCoder and DeepSeek-Coder requires modest initial training set, but the quality of the data remains crucial. Therefore, future steps aim at refinement of the data collection and development of annotation tools."}]}