{"title": "Initial Study On Improving Segmentation By Combining Preoperative CT And Intraoperative CBCT Using Synthetic Data", "authors": ["Maximilian E. Tschuchnig", "Philipp Steininger", "Michael Gadermayr"], "abstract": "Computer-Assisted Interventions enable clinicians to perform precise, minimally invasive procedures, often relying on advanced imaging methods. Cone-beam computed tomography (CBCT) can be used to facilitate computer-assisted interventions, despite often suffering from artifacts that pose challenges for accurate interpretation. While the degraded image quality can affect image analysis, the availability of high quality, preoperative scans offers potential for improvements. Here we consider a setting where preoperative CT and intraoperative CBCT scans are available, however, the alignment (registration) between the scans is imperfect to simulate a real world scenario. We propose a multimodal learning method that fuses roughly aligned CBCT and CT scans and investigate the effect on segmentation performance. For this experiment we use synthetically generated data containing real CT and synthetic CBCT volumes with corresponding voxel annotations. We show that this fusion setup improves segmentation performance in 18 out of 20 setups investigated.", "sections": [{"title": "1 Introduction", "content": "To establish computer-assisted interventions, precise and reliable imaging, especially intraoperative imaging, is crucial. Mobile robotic medical imaging systems, like cone-beam computed tomography (CBCT) [7], enable intraoperative medical imaging with real time capabilities. CBCT is an imaging method that utilizes a cone-shaped X-ray beam and a flat-panel detector to capture detailed, three-dimensional images of a patient's anatomy using a potentially mobile system [4]. However, using CBCT as an intraoperative imaging system often comes with the disadvantage of suffering from more artifacts than preoperative CT imaging, affecting the performance of downstream tasks like segmentation [11].\nWhile this degraded image quality can affect medical imaging tasks, the availability of high quality preoperative scans represents potential to integrate highly detailed information based on the idea of multimodal learning [13,6,12]. Multimodal learning is an approach that involves fusing information from multiple domains to improve machine learning models for a downstream task like segmentation. In 3D medical imaging, a common approach is to enrich computed"}, {"title": "2 Methodology", "content": "We investigate multimodal learning of intraoperative CBCT volumes and misaligned, high quality, preoperative CT volumes using early-fusion, similar to Ren et al. [8] with both scans showing approximately the same section of the body. Evaluation is performed on the CBCT Liver Tumor Segmentation (CBCTL-iTS) [10] dataset with the two different semantic segmentation targets, liver and liver tumor segmentation.\nTo perform segmentation we used a holistic 3D unet introduced by \u00c7i\u00e7ek et al. [2] as the basis of our segmentation model and our baseline for all investigated settings. The 3D unet was adapted to process multimodal data by adding a paired and misaligned, preoperative CT as a second channel, resulting in a 4d data structure (early fusion). The 3D unet used for segmentation is shown in Fig. 1 and consists of an encoder with 3 double convolution layers and 3\u00d73\u00d73"}, {"title": "2.1 Data", "content": "CBCTLITS is an addition to the well studied Liver Tumor Segmentation (LiTS) [1] dataset. LiTS is an abdominal CT dataset with liver and liver tumor segmentations. CBCTLITS [10] expands on that by adding perfectly aligned, synthetic CBCT volumes. Fig 2 shows exemplar CT and corresponding CBCT volumes (with the varying parameter $a_{np}$) of subject 28, and the provided segmentation mask.\nThe factor $a_{np}$ corresponds to the undersampling of Digitally Reconstructed Radiographs (DRRs) in the CBCT reconstruction process of CBCTLITS. In detail, Tschuchnig et al. [10] synthetically generated CBCTLiTS by simulating Digitally Reconstructed Radiographs (DRRs) from the LiTS CT volumes, followed by filtered backprojection to generate the CBCT scans in CBCT geometry. By adjusting undersampling based on the parameter $a_{np} \\in {32, 64, 128, 256, 490}$,\nthe quality of the synthetic CBCT can be adapted.\nSince we aim to generate a realistic evaluation scenario, we synthetically generate misalignment between the perfectly aligned CT and CBCT pairs. We further aim to reduce the number of parameters controlling misalignment to one. Misalignment is established by the factor $\u03b1_\u03b1$. Misalignment augmentation was performed using TorchIO RandomAffine and RandomElasticDeformation. In detail, affine misaligned was performed using random (non-isotropic) scaling, with the scaling parameter sampled from U(1 \u2013 0.5\u00b7\u03b1\u03b1, 1+0.5\u00b7 \u03b1\u03b1), rotation, parameters sampled from U(-22.5\u00b7\u03b1\u03b1, 22.5\u00b7 \u03b1\u03b1), and translation, with the parameter sampled from U(0,0.5\u00b7 aa) with tri-linear interpolation. Elastic misalignment was applied with a maximum displacement sampled from U(0,20 aa) with 7"}, {"title": "2.2 Experimental Details", "content": "The models were trained on a Ubuntu server using NVIDIA RTX A6000 graphics cards. Due to the large data size and 48 GB VRAM memory limit, volumes were downscaled (isotropic) by the factor of two [9]. To binarize the masks, a threshold of 0.5 was applied to each channel of the unet output. The data was separated into training-validation-testing data. The separation was performed using the ratios 0.7 (training), 0.2 (validation), 0.1 (testing). All experiments were trained and evaluated 4 times to facilitate stable results with the same random splits as well as the same random CT misaligned for comparable result. Adam was used as an optimizer with a learning rate of 0.005 for liver and liver tumor segmentation"}, {"title": "3 Results", "content": "Experimental results are shown as boxplots in Fig. 4. The top row shows the results of liver and the bottom row of liver tumor segmentation. The left column shows results evaluated on affine misalignment, while the right columns show combined affine and elastic misalignment. The undersampling factor anp is shown on the x axis and Dice similarity coefficients on the y axis. Blue boxes show Baseline results and green boxes the multimodal learning approach results. Additionally the mean values are plotted in the corresponding box, for easy comparison."}, {"title": "4 Discussion", "content": "The results show that enriching intraoperative CBCT with roughly aligned, preoperative CT improves medical imaging tasks like semantic segmentation. Most multimodal setups improved segmentation performance with the only exception in liver tumor segmentation with $a_{np}$ = 490 in the combined affine and elastic misaligned case.\nSeveral trends are notable. First, the worse the CBCT quality, the more can be gained by adding high quality CT, leading to a maximum increase of Dice from 0.78 to 0.88 for liver and from 0.03 to 0.17 for liver tumor segmentation. This shows that adding the preoperative CT partly mitigates the decreasing image quality of the intraoperative CBCT. This also shows that there might be some implicit registration taking place, however, further experimentation with different degrees of misalignment are needed to investigate this effect further.\nSince we generate a dynamic dataset for both training and testing through the misalignment process, the misalignment itself can be seen as data augmentation. We assume that this effect can further improve the positive effect of the proposed multimodal setup however, this has to be evaluated on real, imperfectly aligned volume pairs. Furthermore, this form of multimodal learning is theoretically applicable to other medical imaging applications and architectures. Due to the positive results of this study, further experimentation into other architectures, such as the Segment Anything Model [5] or UNETR [3] present high potential."}]}