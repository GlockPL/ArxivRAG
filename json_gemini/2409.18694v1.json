{"title": "Learning from Pattern Completion: Self-supervised Controllable Generation", "authors": ["Zhiqiang Chen", "Guofan Fan", "Jinying Gao", "Lei Ma", "Bo Lei", "Tiejun Huang", "Shan Yu"], "abstract": "The human brain exhibits a strong ability to spontaneously associate different visual attributes of the same or similar visual scene, such as associating sketches and graffiti with real-world visual objects, usually without supervising information. In contrast, in the field of artificial intelligence, controllable generation methods like ControlNet heavily rely on annotated training datasets such as depth maps, semantic segmentation maps, and poses, which limits the method's scalability. Inspired by the neural mechanisms that may contribute to the brain's associative power, specifically the cortical modularization and hippocampal pattern completion, here we propose a self-supervised controllable generation (SCG) framework. Firstly, we introduce an equivariant constraint to promote inter-module independence and intra-module correlation in a modular autoencoder network, thereby achieving functional specialization. Subsequently, based on these specialized modules, we employ a self-supervised pattern completion approach for controllable generation training. Experimental results demonstrate that the proposed modular autoencoder effectively achieves functional specialization, including the modular processing of color, brightness, and edge detection, and exhibits brain-like features including orientation selectivity, color antagonism, and center-surround receptive fields. Through self-supervised training, associative generation capabilities spontaneously emerge in SCG, demonstrating excellent generalization ability to various tasks such as associative generation on painting, sketches, and ancient graffiti. Compared to the previous representative method ControlNet, our proposed approach not only demonstrates superior robustness in more challenging high-noise scenarios but also possesses more promising scalability potential due to its self-supervised manner. Codes are released on Github.", "sections": [{"title": "Introduction", "content": "In contrast to artificial intelligence, the human brain exhibits a remarkable characteristic: the sponta-neous emergence of associative generation[18, 24, 19], such as associating real-world visual scenes through pictures, sketches, graffiti, and so on. With the development of controllable generation technology[60, 1, 55], many excellent works have recently been able to achieve similar associative generation capabilities, such as ControlNet[56]. However, existing controllable generation models often require supervised training to achieve this function, such as edge maps [54], semantic segmentation maps[33], depth maps[32], and pose[51]. The brain's ability to associative generation is completely self-emergent and does not require deliberate supervised training. This ability may be related to two mechanisms: one is the modularization of the cerebral cortex in terms of structure and function [15], such as the visual area, auditory area, etc., and the visual area has sub-areas that are sensitive to color, lines, etc.. Although previous works such as group convolution[58, 34], capsule networks[44, 22], and mixture of experts models[61, 43] have designed modular grouping structures, and often show good advantages in model parameter efficiency and computational efficiency, they cannot spontaneously specialize reliable functional modules. Another is the pattern completion ability of hippocampus[47], such as associating the visual signal of a pepper with its taste, associating a sketch with a real visual scene, etc. If we regard edge maps, semantic segmentation maps, depth maps, etc. as general modalities or patterns, then existing controllable generation works can also be regarded as a kind of general pattern completion process[6]. Due to the lack of the brain's ability to spontaneously and reliably specialize functional modules [10], controllable generation often adopts a supervised approach. This brings two problems: one is the need for heavily manual defined supervision conditions, and the other is the need for a large amount of manually annotated corresponding data. This supervised approach may limit the scalability of associative generation, which is one of the most important capabilities in the era of large models [13, 40, 20]. Therefore, it is meaningful to enable the network to spontaneously specialize functional modules and perform pattern completion for controllable generation in a self-supervised manner[11].\nThe key challenge is how to guide the network to spontaneously form functional specialization[14]. An important characteristic of the cerebral cortex is the modularization, which is defined as a subset of highly inter-connected nodes which are relatively sparsely in other modules[36]. This allows modules to perform specific functions relatively independently, while different modules together achieve composite generalizations[5]. In the primary visual cortex of some mammals with strong"}, {"title": "Related Works", "content": "Controllable Generation: Controllable generative models enhance the generation process by in-corporating control conditions[60, 1, 55], leading to images that more closely align with desired outcomes. Compared to using text as control conditions[30, 26, 29, 27, 50, 38], image-based con-trol conditions[7, 37, 56, 59, 35] afford finer-grained controllability, which often need annotated paired images for supervised training. While some methods leverage structural similarity for addi-tional constraint[2, 62], enabling style transfer[42] while maintaining structural consistency, these approaches typically rely on GANs[17] rather than stronger diffusion models[23].\nModular Neural Networks: The most direct approach to modular network design is structural modularity, achieved by techniques such as dividing convolutional kernels into distinct groups within each layer[58, 34, 57, 46] or designing specialized blocks[48, 49]. Capsule networks[44, 22, 41, 28] and Mixture of Experts (MoE)[61, 43] both adopt dynamic routing mechanism between diverse blocks. These methods often improve parameter and computational efficiency but struggle to achieve reliable and stable functional specialization. Group equivariance[12, 45, 53, 52, 25, 8, 9] offers a novel approach for functional specialization. By imposing equivariant constraints, the network automatically learns orientation-selective features akin to simple cells, resulting in clear functional specialization[16]. However, this approach is limited to translational equivariance."}, {"title": "Self-supervised Controllable Generation", "content": "The whole framework of our Self-supervised Controllable Generation (SCG) is shown in Figure 1c. We first train a special autoencoder to encode the input image into disentangled feature spaces or modalities called Modular Autoencoder. Then we use part of the modules as control conditions to complete the missing information and reconstruct the input image. This self-supervised approach does not require manually designed feature extractors or manual annotations, making it easier to train and more scalable compared to supervised approaches.\nModular Autoencoder: The main principle of the modular autoencoder is to enhance the indepen-dence between modules and the correlation within modules. Inspired by the primary visual cortex of biological systems, we design a modular equivariant constraint to promote functional specialization. Specifically, for a typical autoencoder, it can be formally represented as:\n$E(I) = z, D(z) = I$\nwhere I is the input image, z is the latent representation, E and D are the encoder and decoder, respectively. For a modular autoencoder, we define the independence between modules as:\n$z = [z^{(0)}, z^{(1)}, ..., z^{(k-1)}], z^{(i)} (L_{\\delta}(I)) = P_{\\delta}(z^{(i)} (I)).$\nz can be devided into several modulars as $[z^{(0)}, z^{(1)}, ..., z^{(k\u22121)}]$, where $z^{(i)}$ is the ith modular. $L_{\\delta}$ is a kind of change determined by a low dimension parameter \u03b4. $P_{\\delta}$ is a prediction function in the latent space with parameter d. It means that each module can independently predict its state by the specific change d, which indicates that each module is equivariant on the corresponding change group. For the correlation within a module, we define\n$z^{(i)}_{n} = P(z^{(i)}_{m})$,\nwhere $z_{n}^{(i)}$ and $z_{m}^{(i)}$ denote one-hot representations with 1 in the mth and nth dimensions, respectively. For any two dimensions $z_{m}$ and $z_{n}^{(i)}$ in $z^{(i)}$, there always exist corresponding prediction parameters $\\delta_{mn}$ such that above equation holds.\nEquivariant Constraint: In practical implementation, we use simple translation and rotation transformations as the change $L_{\\delta}$, and a learnable linear transformation matrix $M^{(i)}(\\delta)$ as the prediction function.\nSpecifically, we utilize convolution operation as the encoder E with kernel W. We train the autoen-coder in homologous images I and I', where I' = $L_{\\delta}(I)$. The image encoding process is denoted as $f = I * W$ and $f' = I' * W$, where W is learnable convolutional kernels, * denotes convolution, and f and f' represent the extracted feature maps. The decoding process is $f \\otimes W$ and $f' \\otimes W$, where $W$ shares kernels used in the encoding process, and $\\otimes$ denotes deconvolution. The objective of the autoencoder is to make the reconstructed image as similar as possible to the original image. So we design the reconstruction loss as follows:\n$L_{recon} = ||f \\otimes W \u2013 I||^{2} + || f' \\otimes W \u2013 I' ||^{2}$\nThen, we construct an equivariant loss to drive modules in the networks to be more independent. We devide the convolutional kernels W into k groups: $W = [W^{(0)}, W^{(1)}, ..., W^{(k-1)}]$, and the corresponding feature maps f are also devided into k groups: $f = [f^{(0)}, f^{(1)}, ..., f^{(k-1)}]$. Within each module, we apply an equivariant constraint such that\n$L_{equ} = \\sum_{i} || f'^{(i)} \u2013 M^{(i)} (\\delta) f^{(i)}||^{2}, i \\in \\{0, 1, ..., k \u2212 1\\}$\n, where $M^{(i)} (\\delta)$ is a learnable prediction matrix determined by \u03b4. Next, we construct a symmetry loss to enhance the relationship within each module:\n$\\delta_{mn}^{(i)} = \\frac{e^{-M_{nm}^{(i)}(\\delta)/T}}{\\sum_{n}e^{-M_{vn}^{(i)}(\\delta)/T}} \\delta, m, n \\in \\{0, 1, ..., l - 1\\}$\n$L_{sym} = \\sum_{i} \\sum_{m} \\sum_{n} ||f_{m}^{(i)} \u2013 M_{mn}^{(i)} (\\delta) f_{n}^{(i)} ||^{2}, i \\in \\{0, 1, ..., k \u2212 1\\}$\nwhere l is the module length, and $M_{mn}^{(i)}(\\delta)$ is the m row and n column of the prediction matrix, which indicates the relationship between mth and nth dimensions module $f^{(i)}$ corresponding to \u03b4. $\\delta_{mn}^{*}$ is the best transformation parameter from nth dimension to mth dimension calculated by a softmax function with temperature T. $f_{m}^{(i)}$ and $f_{n}^{(i)}$ are two one-hot representation with 1 in the mth and nth dimensions, respectively.\nFinally, we take above three losses together as our Equivariant Constraint:\n$L_{EC} = L_{recon} + \\lambda_{1}L_{equ} + \\lambda_{2}L_{sym}$\nwhere $\\lambda_{1}$and $\\lambda_{2}$ are the weights of the equivariant loss and symmetry loss, respectively.\nPattern Completion: Based on the modular autoencoder trained in the previous section, we can treat each module as a general modality or pattern and then perform conditional generation through pattern completion. This process can be formulated as:\n$z(I) = C(z^{(i)} (I), i)$,"}, {"title": "Experiments and Results", "content": "Modular Autoencoder: We trained our modular autoencoder mainly on two typical datasets. One is MNIST, which is a small dataset with gray digits. The other is ImageNet, which is a relatively large dataset with color natural images. On both MNIST and ImageNet, modular autoencoder can achieve obvious function specialization as shown in Figure 2. Many learned kernels have significantly gabor-like orientation selectivities as shown in Figure 2a and b, which is a brain-like characteristic. On datasets consisting of gray images, such as MNIST, modules primarily specialize into selectivity for different spatial frequency orientations, with each module having the same spatial frequency and complete orientation selectivity (Figure 2a). On datasets consisting of natural color images, modules not only specialize into selectivity for different spatial frequency orientations but also specialize into modules that are sensitive to color and brightness (Figure 2b). Unlike imposing translation equivariant constraints on each kernel, Figure 2c imposes both translation equivariant and trans-rot equivariant constraints on the entire module. This allows it to learn brain-like center-surround receptive fields, and generate center-surround antagonistic and color-antagonistic modules. Similarly, Figure 2c also forms obvious\nSelf-supervised Controllable Generation on MS-COCO: We implement our SCG based on our trained modular autoencoder in Figure 2c (see more details in A.4.2). Specifically, We devided the 16 modules into 6 groups named from HC0 to HC5 (see details in A.4.1), and generated images conditioned by them respectively as shown in Figure 3. The original images with text prompts are randomly selected on MS-COCO in the first line. The condition images are in the second line. In Figure 3, some modules provide the color information, some provide the brightness information, and some provide the edge information with different spatial frequencies. The last condition image is the edge map extracted by the Canny detector, which is used in ControlNet. The third line is the generated images by different conditions. With incomplete control information as input, the diffusion model learns to complete the missing information to obtain a complete image. Due to the different control information, the generated images also have their own characteristics. The bottom shows more generation results. The three lines are original images, condition images, and generated images, respectively. HC0 mainly extracts color information, and the generated image is also closely functional specialization such as color, brightness, and edges, which can also be observed from the reconstruction visualization of each module in Figure 2d.\nAssociative Generation: Associating real-world scenes from different styles of paintings and abstract graffiti is a capability that everyone possesses. And this ability does not require supervised training, it is completely self-emergent and has zero-shot generalization ability. The proposed SCG can also emerge with association generation ability. We use SCG trained on COCO dataset to test its zero-shot generalization capabilities on assoviative generation with sketches, oil paintings, wash and ink paintings and more challenging ancient graffiti on rock.\nSketches: We first test it on manual sketches. We tested both ControlNet conditioned by Canny edges and SCG conditioned by HC3. In Figure 5, ControlNet can generate images from sketches in excellent fidelity and aesthetics to the original sketches. Basing on the Canny edge detector, ControlNet possesses a natural advantage when dealing with sketches, resulting in high-quality generation outcomes. For SCG, sketch is an entirely novel domain with a significant divergence from the training dataset distribution. Nevertheless, SCG can still generate images with remarkable fidelity and aesthetics by an automatic specialized feature extractor, demonstrating its excellent generalization capabilities.\nPainting: We also tested two typical painting styles: Western-style oil paintings and Eastern-style wash and ink paintings. Figure 6 shows the results of associative generation on oil paintings on the top part. The first row is the original oil painting images with text prompts, and the second row is the condition image Canny edge and generated images of ControlNet. Compared to clean"}, {"title": "Discussion and Limitation", "content": "The proposed equivariant constraint enables the network to spontaneously specialize into modules with distinct functionalities. Further experiments, detailed in the supplementary material, reveal features within each module exhibit similar spatial frequency selectivity, while different modules possess distinct spatial frequency preferences. Within each module, features exhibit varying orientation selectivity, covering the entire orientation space through a population coding scheme, forming a continuous and closed independent manifold space. These findings suggest the proposed equivariant constraint effectively promotes both intra-module correlation and inter-module independence, facilitating functional specialization. The close brain-inspired mechanisms and emergence of brain-like phenomena provide a novel perspective for gaining deeper insights into the learning mechanism of the brain. However, the current equivariant constraint is a simplified and preliminary version, resulting in only low-level feature specialization. Nevertheless, it validates the efficacy of using equivariant constraints to achieve modularization. Future endeavors will focus on inducing richer functional specialization encompassing semantic hierarchies.\nInspired by the hippocampal pattern completion in the brain, we propose a self-supervised control-lable generative framework based on automatically specialized functional modules. We demonstrate that this self-supervised approach can spontaneously emerge associative generation capabilities, mim-icking human-like abilities. This self-supervised learning paradigm, which predicts and completes missing information through masking, is fully analogous or compatible with current large models. Furthermore, our method extends the masking of information from low-semantic spatial dimensions to more abstract modal dimensions of images, providing a novel approach for enhancing existing large models. Similar to the self-supervised training methods of large models, this suggests that our self-supervised pattern completion framework also possesses scalability potential. While our current work focuses on validating the framework's effectiveness, we will explore its scalability in future."}, {"title": "Conclusion", "content": "Inspired by the visual hypercolumn, we propose a modular autoencoder framework with equiv-ariant constraints to automatically achieve functional specialization by promoting inter-module independence and intra-module correlation. Experimental results on multiple datasets demonstrate the effectiveness of this approach in generating functional specialization, exhibiting characteristics reminiscent of the biological primary visual cortex. Building upon these differentiated modules, we develop a self-supervised controllable generative framework inspired by hippocampal modal com-pletion. Experiments reveal that this framework can spontaneously emerge human-like associative generative abilities, exhibiting strong generalization even in scenarios involving significant distribu-tional differences, such as associating sketches and ancient graffiti with natural visual scenes. This self-supervised completion approach is fully compatible with existing large model training methods, suggesting its scalable potential and implications for inspiring novel large model architectures."}, {"title": "Appendix / supplemental material", "content": "Equivariant constraints are crucial for modular autoencoders. In this work, we designed simple equivariant constraints to validate the proposed modular autoencoder framework. We primarily em-ployed two equivariant constraints: translational equivariant constraints and translational-rotational equivariant constraints. These constraints were applied to different module partitions. Since transla-tional equivariance is encompassed by translational-rotational equivariance, modules corresponding to translational equivariance should be included within those corresponding to translational-rotational equivariance. When multiple translational modules are combined into a translational-rotational module, the learned features exhibit pronounced orientation selectivity, as illustrated in Figure Sla. Each convolutional kernel constitutes a translational equivariant module, and every 8 translational equivariant modules (a row) combine to form a translational-rotational equivariant module, termed a"}]}