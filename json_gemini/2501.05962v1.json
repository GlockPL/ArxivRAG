{"title": "Effective faking of verbal deception detection with target-aligned adversarial attacks", "authors": ["Bennett Kleinberg", "Riccardo Loconte", "Bruno Verschuere"], "abstract": "Background: Deception detection through analysing language is a promising avenue using both human judgments and automated machine learning judgments. For both forms of credibility assessment, automated adversarial attacks that rewrite deceptive statements to appear truthful pose a serious threat.\nMethods: We used a dataset of 243 truthful and 262 fabricated autobiographical stories in a deception detection task for humans and machine learning models. A large language model was tasked to rewrite deceptive statements so that they appear truthful. In Study 1, humans who made a deception judgment or used the detailedness heuristic and two machine learning models (a fine-tuned language model and a simple n-gram model) judged original or adversarial modifications of deceptive statements. In Study 2, we manipulated the target alignment of the modifications, i.e. tailoring the attack to whether the statements would be assessed by humans or computer models.\nResults: When adversarial modifications were aligned with their target, human (d=-0.07 and d=- 0.04) and machine judgments (51% accuracy) dropped to the chance level. When the attack was not aligned with the target, both human heuristics judgments (d=0.30 and d=0.36) and machine learning predictions (63-78%) were significantly better than chance.\nConclusions: Easily accessible language models can effectively help anyone fake deception detection efforts both by humans and machine learning models. Robustness against adversarial modifications for humans and machines depends on that target alignment. We close with suggestions on advancing deception research with adversarial attack designs.", "sections": [{"title": "INTRODUCTION", "content": "Detecting deception matters in daily life and in the courtroom. However, deception detection is also very hard and has explored a range of instruments, methods and approaches (Docan- Morgan, 2019; Granhag et al., 2015). The most often applied means of deception detection is through analysing language, commonly performed by trained humans and practiced by forensic psychologists, police, and business in several countries worldwide. In recent years, several computer-automated approaches have been developed, and Al tools for deception detection are increasingly marketed (e.g., liarliar.ai). But an emerging threat to the validity of deception detection approaches is largely glanced over: can artificial intelligence fool these deception detection techniques? In this paper, we introduce and empirically examine automated adversarial text modifications as threat for verbal deception detection."}, {"title": "Verbal deception detection", "content": "The central difference between the verbal approach to deception detection and other approaches (e.g., polygraphy, voice stress analysis, behaviour analysis) is that it relies on the content of statements that are either truthful or deceptive (Vrij, 2019; Vrij et al., 2022). Decisions about veracity are based on the information that a participant or suspect provides. On the one hand researchers develop techniques to maximize the information value of the statements (Vrij & Granhag, 2012). On the other hand, there is extensive research trying to improve how the credibility assessment is made; which cues to extract, how to extract them, and how to combine them (Levine, 2014; Verschuere et al., 2023). Investigative interviewing techniques borrowed from cognitive interviewing which encourage interviewees to provide a rich statement aim to elicit more diagnostic information than interviews without specific techniques (Mac Giolla & Luke, 2021). Research on the detectability of deception contained in a verbal statement (e.g., transcripts or typed narratives) has consistently shown that humans perform at the chance level when tasked to directly make a deception judgment (e.g., asking \"How deceptive is this statement?\"; for two meta-analyses, see Hartwig & Bond, 2011, 2014). For a long time, therefore, this task was outsourced to experts trained in extracting cues to deception (Nahari et al., 2019). Recently, however, the notion of poor human deception detection ability by lay people has been challenged with the introduction of a heuristics-based approach (Verschuere et al., 2023). When attention was directed to focusing on a single yet empirically supported cue \u2013 the detailedness of a statement lay people's ability to detect deception improved (65-70%) compared to a standard deception judgement (50-52%). While that promising avenue of research could help improve human deception detection, another research area relies on computational methods for verbal deception detection to increase the scale, reliability and potential performance of detecting deception."}, {"title": "Computer-automated verbal deception detection", "content": "Similar to human verbal deception detection, the data used to arrive at a computational judgment are verbal statements. The key difference is how the automated approaches make a credibility assessment (Fitzpatrick et al., 2015). Rather than using human judgment, the computational approach relies on machine learning and natural language processing (NLP). First, textual data are quantified (e.g., word frequencies, named entities, psycholinguistic variables, embedding representations) with NLP methods to arrive at a numerical representation of the data that can be used for further statistical analysis. The second step then involves using that"}, {"title": "Adversarial attacks", "content": "Adversarial machine learning pertains to a subfield of computer science that assesses the robustness of classification models (e.g., image and text classification). An original classification model becomes the target of an attack when an adversarial attack model seeks to modify input in such a way that the target model misclassifies the data. Importantly, the input data modification occurs in such a way that the changes are imperceptible to humans. The classic design of adversarial learning is image recognition. For example, minute changes to individual pixel values can trick the system into misclassifying dog images as cats (Elsayed et al., 2018). Recent work has brought the problem to NLP research (Bartolo et al., 2020; Morris, Lifland, Yoo, et al., 2020).\nFor example, one study (Mozes, Stenetorp, et al., 2021) used a fine-tuned language model (ROBERTa) that classified positive and negative movie reviews with an accuracy of 94.9%. The adversarial attack first queried the model to understand how it weighed input information to make a positive versus negative movie review classification, then bespoke modifications of movie reviews were created by substituting words with low-frequency synonyms (e.g., \"wonderful\" became \"tremendous\"). By modifying the reviews, these adversarial attacks were successful and more than halved the classification accuracy (from 94.9% to 40.8%). Adversarial attacks are a central method to assess the robustness of machine learning models but have not yet reached deception research.\nFor deception detection, adversarial modifications pose a threat when deceptive text data are purposefully modified to appear to a human or a classifier as truthful (Fig. 1). Devising adversarial attacks has since recently necessitated advanced machine learning. But the rapid availability of large language models (e.g., ChatGPT, Llama) has lowered the barrier for devising"}, {"title": "Aims of this paper", "content": "We test how robust both human judgments and machine learning classifiers are against adversarial modifications for deception detection. We compare deception detection performance on original statements with the performance achieved on adversarial modifications of deceptive statements. These modifications are crafted by a state-of-the-art large language model. In Study 1, we test two modification attack variations. The attack was developed with or without guidance on how to make a statement appear more truthful. We further examine the efficiency of the attack on two human and two automated deception detection approaches. For the human judgments, we compared a direct deception judgment as a control condition with the novel use-the-best heuristic decision-making strategy (Lob et al., 2024; Verschuere et al., 2023). We used machine learning classifiers of two levels of complexity: a fine-tuned language model and a simple word frequency-based model. In Study 2, we expand the adversarial design by devising more targeted automated rewriting modifications (i.e., specifically targeted at a model from study 1 versus targeted at humans) and evaluate them on both humans and a machine learning model."}, {"title": "STUDY 1", "content": "Method\nData availability statement\nAll data, analysis code and LLM-generation code to reproduce the findings of this paper are available at https://osf.io/7qz94/?view_only=73ef083d0a8b44cb9333ab17401d062d.\nTruthful and deceptive statements\nWe used the Hippocorpus dataset (Sap et al., 2020), which contains stories written by a total of 5,047 participants about remembered and imagined events after removal of missing values. Participants were first asked to recall a salient event that they had experienced in the past six months and write a story as well as a 2-3 sentence summary about that event. The summaries were then provided to a new sample of participants who had to imagine that event and write a fabricated story about it.\nWe evaluated the effect of adversarial modifications on both humans and a deception classifier. That classifier used a train/test split, so to avoid testing in already seen data, we used the test dataset from the classifier (i.e., the subsample of stories not used in the training phase) as the relevant dataset for the textual modifications. That test dataset consisted of 505 stories (262 deceptive, 243 truthful). The training set consisted of the remaining 4542 statements.\nAdversarial modifications\nThe adversarial versions were obtained by prompting a large language model (here: GPT-4-turbo) to rewrite the deceptive statements so that they appear truthful to humans (Table 1). We used two variations of that procedure: in the unguided modification condition, the model was instructed to \"[r]ewrite the following deceptive statement so that it may appear truthful to humans. Keep almost the same length as the original statement\u201d.\nIn the guided modification condition, we explained that liars try to mislead others by providing details that cannot be easily checked (Hartwig et al., 2007; Nahari et al., 2014) and directed the model to use unverifiable details with the following instruction: \"We know from research that liars prefer to avoid providing details that can be verified whereas truth-tellers prefer to provide details that can be verified. Verifiable details are (i) activities carried out with identifiable or named persons who the interviewer can consult, (ii) activities that have been witnessed by identifiable or named persons who the interviewer can consult, (iii) activities that the interviewee believes may have been captured on CCTV, and (iv) activities that may have been recorded and documented, such as using debit cards, mobile phones, or computers. Rewrite the following deceptive text by adding UNVERIFIABLE DETAILS so that it may appear truthful to humans. Pay attention to add only unverifiable details. Do not add any verifiable detail. Keep almost the same length as the original statement\".\nThe completion requests from the large language model were sent with a randomly sampled temperature parameter (i.e., the randomness involved in selecting next words in the completion) and with a maximum length that was 20 tokens longer than the original statement. For the interaction with the LLM, we used the rgpt3 R package (Kleinberg, 2024)."}, {"title": "Human deception judgment", "content": "We recruited participants via the online participant pool Prolific and asked them to assess ten randomly selected statements each. We aimed to have each statement assessed by at least three participants and used the mean of the judgments per statement for further analysis. Upon providing informed consent, participants were provided instructions about their task, which created two judgment conditions.\nIn the heuristic judgment condition, participants' focus was directed solely on judging the detailedness of the statements using a definition of detailedness as \"the degree to which the message includes details such as descriptions of people, places, actions, objects, events and the timing of events; the degree to which the message seemed complete, concrete, striking or rich in details\" (Verschuere et al., 2023). In the control condition, participants were asked to make a direct deception judgment.\nWithin each judgment condition, participants were further allocated to one of the three modification conditions. They either saw the original statements from the Hippocorpus test set, the unguided adversarial modifications or the guided adversarial modifications. In each modification condition, the truthful statements were not modified but were judged anew so that participants were exposed to the same distribution of truthful and deceptive statements in all conditions.\nAll participants made their judgment on an 11-point scale from 0=not detailed at all [completely deceptive], 5=moderate/neutral, to 10-absolutely detailed [completely truthful]. Participants were aware that the study was about detecting truthful and deceptive stories about recollected or fabricated events from another dataset. After completing the task, participants were debriefed and informed about the adversarial modification that was included in some statements."}, {"title": "Training machine learning models", "content": "Two machine-learning approaches were used. First, we retrained a model from previous research (Loconte et al., 2023). Specifically, we fine-tuned a FLAN-T5 base model' on the training set for a binary classification task. A 10-fold cross-validation was employed to assess the model's performance. After cross-validation, the model reached an average accuracy of 79.87% (SD=2.07), which was sufficient to then retrain it once on the whole training set to obtain a single, final model for use on the test set. Model training was done on Google Colaboratory Pro+ using an NVIDIA A100 Tensor Core GPU.\nSecond, to include a simpler model, we retrieved a representation of n-grams (unigrams, bigrams and trigrams, representing sequences of n consecutive words) after removing stop words and punctuation and applying stemming to the words. We further only included words that occur in at least 1% of the statements as a means to reduce the risk of overfitting. From the retained 1621 n-grams as predictors, we removed those with a near-zero variance in the training phase, resulting in a final set of 321 predictors that were used for training the model. The classification algorithm was a support vector machine model trained on 80% of the data with 5-fold cross-validation in the training set. The model was evaluated on the remaining unseen 20% of the data. Since such an n-gram model does not consider the order in which words occur, this is also known as a bag-"}, {"title": "Analysis plan", "content": "We examine the robustness of two deception detection modalities against adversarial rewriting. First, we test how human judges who either use a standard deception judgment or the decision"}, {"title": "Results", "content": "Corpus descriptives\nThe original dataset contained 505 statements (262 deceptive, 243 truthful). For each statement, we obtained six aggregate judgments resulting from the three variations (two adversarial modifications plus the original) and the two human judgment conditions (detailedness vs veracity judgments). Only statements for which we had at least two independent human judgments were included (number of judges per statement: M=3.09, SD=0.59), resulting in a final dataset of 2998 judged statements. The average word count was 278.02 (SD=96.97). There were significant differences in the length of the statements both in original form and in exacerbated form in the adversarial modifications (Table 2).\nHuman judgments\nThe 2 (Statement Veracity: truthful vs deceptive) by 3 (Statement Modification: unguided modification vs guided modification vs not modified) by 2 (Human Judgment: heuristic vs control) ANOVA revealed a significant main effect of Judgment, F(1, 2986)=46.00, p<.001, eta-sq=0.02, that subsumed under a significant three-way interaction, F(2, 2986)=10.99, p<.001, eta-sq=0.01 (see Table 3). We unpack that effect by Modification and conduct follow-up 2 (Veracity) by 2 (Judgment) ANOVAs for each modification type (Table 3).\nFor the original texts, a significant Veracity by Judgment interaction, F(1, 991)=8.02, p=.005, eta- sq=0.01 indicated that when using the heuristic, human truthfulness judgments were significantly higher for truthful than deceptive statements, d=0.30 [0.07; 0.54] and resulted in a\nMachine learning deception classification\nBoth deception classifiers the fine-tune language model and the bag-of-words model performed significantly better than the chance level (Table 4). While the bag-of-words model showed accuracies that were 7-14 percentage points lower (accuracies: 0.64-0.65) than those of the fine-tuned language model (accuracies: 0.71-0.79), the variability across statement modification conditions was lower than for the language model. In particular, the unguided modification resulted in a drop of seven percentage points for the language model (0.78 to 0.71) but did not affect the bag-of-words model (0.64 remained 0.64). Differences between the two models can largely be attributed to the bag-of-words model's poor recall values (i.e., detection rate) for truthful statements of 0.46."}, {"title": "Discussion Study 1", "content": "Our findings suggest that humans used the use-the-best heuristic effectively to improve their deception detection ability \u2013 albeit to a smaller effect size than originally reported. However, when statements were modified, the heuristic became ineffective, bringing judgments back to the chance level. In contrast, when statements are judged by machine learning classifiers, the drop in deception detection performance as a result of adversarial modifications was less substantial and remained above the chance level. A simple word frequency-based model performed 14 percentage points below the larger, fine-tuned language model but was more robust against adversarial modifications.\nThe modifications in Study 1 targeted a presumed mechanism in human deception (i.e., providing unverifiable details to appear credible) and may thus have put humans at an unfair disadvantage. To test whether the target of the modifications affects the judgment robustness, we manipulated the target in Study 2. Specifically, we devised modifications targeted either at humans or at the bag-of-words model from Study 1. If target alignment played a role, we would expect modifications to be more effective when modification target and actual judgment modality are the same (human target and human judgment; model target and model judgment) than if they are divergent."}, {"title": "STUDY 2", "content": "Method\nTargeted adversarial modifications\nThe key difference to Study 1 was how we devised the adversarial modifications. We used the same procedure with the key differences that the attack was now tailored to either humans or the bag-of-words model from Study 1. The bag-of-words model was chosen because, in contrast to the fine-tuned LM, we can trace which features it used and provide - at least superficially - some explainability for the modification attack. Also, Study1 shows that this bag-of-words model was most resistant to adversarial attacks as neither the guided not he unguided attacked lowered its accuracy. Two other changes were made to the design: i) since running Study 1, a more capable LLM has been released (GPT-40) which we used to reflect the upper boundary of what we can expect an LLM to do under our research design; ii) we chose a recommended default sampling temperature for model of 0.7; and iii) we changed the modification instructions to specifically target humans or the bag-of-words model from Study 1.\nFor Study 2, the original statements were rewritten again, this time tailored either to fool humans or to fool the classifier that proved to be most robust in Study 1. For the human-targeted instructions, the prompt to the LLM read: \"Below is a deceptive statement that was written by a human. Your task is to rewrite that statement in such a way that it appears more truthful to a human. The humans who read and assess the statement will be focusing on the detailedness and use detailedness as an indicator of truthfulness. For them, more details suggest a higher probability of truthfulness. Your task is to rewrite the statement so that it appears more truthful to humans. Finish your statement with a complete sentence and adhere to a similar length as the original statement when you rewrite it.\"\nThe model-targeted instructions, in turn, included details (highlighted below in bold) about the kind of model, the model's confidence in the current statement being truthful (based on the"}, {"title": "Human judgments", "content": "The procedure followed the one from Study 1. This time, all participants used the detailedness heuristic, dropping direct veracity judgments where humans performed at chance level even on the unaltered statements. All participants judged 10 statements. All modified statements were judged in two separate tasks, so that for each modification target, truthful and deceptive statements were presented in a random selection. The original, unmodified deceptive statements were not judged."}, {"title": "Machine learning models", "content": "We used the same bag-of-words model as in Study 1 (i.e., there was no additional training of the model with new data)."}, {"title": "Analysis plan", "content": "For human judgments, we use a 2 (Statement Veracity: truthful vs deceptive) by 2 (Statement Modification: human-target vs model-targeted) ANOVA on the average human judgment per statement. Machine learning performance will be compared by accuracy and AUCs."}, {"title": "Results", "content": "Participants in the human judgment task\nThe human judgments were crowdsourced from n=302 participants with an average age of 34.68 years (SD=11.97), of whom 52.31% were male, 46.36% female and 1.33% without gender data. Each participant was paid GBP 1.50 for a median task completion time of 11 minutes.\nCorpus descriptives\nFor statements modified for humans as targets, the truthful ones (M=359.82, SD=194.07) were significantly longer than deceptive ones (M=241.84, SD=64.66), d=0.83 [0.59; 1.03]. This effect was somewhat more pronounced when the target was the machine learning model (truthful: M=358.09, SD=192.41; deceptive: M=214.94, SD=92.97), d=0.96 [0.72; 1.21]. As in Study 1, these length differences are larger than in the unmodified statements. The human-targeted deceptive modifications were also longer than the model-targeted ones, d=0.34 [0.11; 0.56].\nHuman judgments\nThe 2 (Veracity) by 2 (Modification target) ANOVA indicated a significant main effect of condition, F(1, 984)=6.77, p=.009, eta-sq=0.01. The main effect of modification was not significant, F(1, 984)=1.38, p=.240, eta-sq=0.01. The significant condition by modification interaction, F(1,"}, {"title": "Semantic similarity constraints", "content": "In adversarial machine learning on textual data, a commonly used constrained is that the adversarial modification must preserve the meaning of the original statement. This is typically measured computationally as the cosine similarity between embeddings representations of the original and modified text (Morris, Lifland, Lanchantin, et al., 2020). Embeddings capture semantic information in textual data (Mikolov et al., 2013; Pennington et al., 2014). The cosine similarity between two embeddings representations ranges from -1.00 (complete opposites) to +1.00 (identical). If the similarity is too low, the modifications can be deemed invalid, so it merits attention to test whether this criterion is met and how it affects the modification effectiveness.\nFor the modifications in Study 2, for the human-targeted modifications, 97.69% had a similarity with the original more than 0.80 and 65.77% a similarity of 0.90 or higher. These values were higher for the model-targeted modification: 100% and 90.77% had a similarity to the original of 0.80 and 0.90, respectively. These observations manifest in an average similarity for human- targeted modifications (M=0.91, SD=0.04) that was significantly lower than those for the model- targeted modifications (M=0.95, SD=0.03), d=0.95 [0.72; 1.19]. These findings suggest that the deviations from the original were rather minimal but still somewhat more pronounced when the"}, {"title": "Exploring vocabulary complexity differences between modification targets", "content": "To understand better the potential mechanisms through which the modifications achieve their goal, we explored a candidate linguistic variable reported elsewhere when a language model were tasked to mislead humans (Kleinberg et al., 2024). In that study, when instructed to appear human, a language model generated texts with a markedly simpler vocabulary compared to a condition without specific instructions to mislead readers. We measured the vocabulary complexity by comparing the average word frequency ranks obtained from a ranked list of the most common 10,000 words in Google's Trillion Word Corpus (https://github.com/first20hours/google-10000-english). Lower ranks imply a more common and hence less complex vocabulary.\nThere was a marked effect in vocabulary complexity. The average word frequency rank was lower when the target was a model (M=1010.77, SD=199.52) than when humans were the target (M=1188.52, SD=177.53), d=0.94 [0.70; 1.18]. That effect suggests that the vocabulary used by the adversarial modification to mislead humans was significantly more complex."}, {"title": "Discussion Study 2", "content": "In the second experiment, we assessed whether alignment between target and modality mattered. The findings showed that when these were aligned, the adversarial modification is significantly more effective, both on humans and on a machine learning model. Conversely, when the judgment modality and the target were misaligned, the modifications were ineffective. Under misalignment, humans were able to apply the detailedness heuristic (d=0.36) to a similar degree as in the unmodified statements in Study 1 (d=0.30). Similarly, the machine learning model's accuracy (0.63) was close to that on the unmodified statements from Study 1 (0.64)."}, {"title": "GENERAL DISCUSSION", "content": "The availability and capabilities of large language models present not just a promise but also a challenge for verbal deception detection. Language model technology provides easy access to Al tools that may help to deliberately deceive in language. This work provided a first glimpse at this novel challenge known as adversarial attacks and examined the vulnerability of humans and automated classifiers to targeted efforts that invoke misclassifications of deceptive statements as truthful."}, {"title": "Human judgment", "content": "When humans judged the unmodified statements, we found evidence for the use-the-best heuristic in both studies. Focusing participants on the detailedness of the statements \u2013 despite making them aware that they were reading truthful and deceptive stories \u2013 resulted in truth-lie differences in human judgment, while a simple, deceptive judgment did not. Compared to other studies on that heuristic, the effect was substantially smaller here (d=0.30-0.36, compared to d=0.97 in Study 5 of Verschuere et al., 2023). While it is not uncommon to find smaller effects in"}, {"title": "Machine learning models", "content": "When the adversarial modifications were used for automated deception classification with machine learning models, there was clear evidence of target alignment. When the modification did not target a machine learning model (study 1 and one condition in study 2), there was no substantial effect. Both classifiers remained well above the chance level (64% and 78%). There was a drop in performance for the language model when modifications were unguided (i.e., not following the inverse verifiability approach logic) from 78% to 71%. It is noteworthy that such a drop was not observed for a simpler n-gram model, which remained consistently between 64- 65%. These findings are somewhat surprising, considering previous work that suggests that practically all text classification models are vulnerable when presented with adversarial examples (Alzantot et al., 2018; Bartolo et al., 2020; Morris, Lifland, Lanchantin, et al., 2020; Mozes, Stenetorp, et al., 2021). Moreover, the model that relied solely on the term frequency (n- grams) was less susceptible than the more complex one that uses an entire language model and embedding representations. However, when we devised adversarial modifications specifically targeted at the n-gram model, we observe a similar back-to-chance-level effect as in humans when they were targeted (i.e., an accuracy of 0.51 for aligned modifications and 0.63 for misaligned modifications)."}, {"title": "Aligned grey-box adversarial modifications", "content": "Our data provide two sides of the same coin. On the one hand, when an adversarial attack (here: textual modifications) is aligned with the target, a simple language model query is sufficient to render human and machine learning judgments ineffective with significant drops of performance. In machine learning terminology, these attacks resemble white-box attacks (Papernot et al., 2016) where details about the target model's workings are available to the attacker. We also provided the attacker (here: a language model tasked with re-writing text to appear more truthful) information about the targets: for humans, we revealed that they pay attention to the detailedness and for the target model, we provided basic model information, class probabilities and the most diagnostic features. In strict white-box attacks, the attacker has access to the gradient and receives constant feedback on the effect of input modifications. In contrast, black- box attacks provide no information about the target. Our approach can thus best be described as a grey-box attack."}, {"title": "Limitations", "content": "The focal point of this work is adversarial modifications. We operationalised attacks through an easily available large language model that possesses text manipulation capabilities that far exceed those of other non-generative machine learning models (Chang et al., 2024). The limitation inherent to that approach is that the modifications are much less constrained compared to a model built solely for substituting individual words. Consequently, the validity of modifications is less clear. While our analysis on semantic similarity suggests that the modified statements were sufficiently similar to the originals, other criteria could be examined in future work. These could include grammatical correctness and non-suspiciousness (Morris, Lifland, Lanchantin, et al., 2020). While such language quality criteria are arguably easy to meet for large language models, a clear evaluation of class preservation would be desirable. Future work could involve participants who write deceptive stories directly and ask them to evaluate an adversarial modification of their own statement.\nAnother angle worth exploring is whether the modifications were good enough. Adversarial machine learning typically involves training an attack model on the weights of the target model (in the case of white box attacks) or relies on significant amounts of trial-and-error with constant feedback from the model, and allows for thousands of iterations until an effective perturbation is found. The approach in our current study, in contrast, relied on a single-shot modification without any training (i.e., a single-shot grey-box attack). Future work on adversarial attacks in verbal deception research could compare existing attack frameworks (Morris, Lifland, Yoo, et al., 2020) and test a range of attack types (e.g., strictly word-level substitutions versus character-level attacks).\nLastly, none of the targets in our study were warned of potential faking attempts through adversarial modifications. Possibly, telling human participants and implementing a filtering mechanism for machine learning models about faking attempts, could attenuate attack effectiveness. Future work could test this experimentally and further our understanding of robustness against targeted modification attacks."}, {"title": "Looking ahead: Understanding deception with adversarial attacks", "content": "Aside from the needed assessment of the robustness of deception detection approaches, the introduction of adversarial attacks into deception research holds exciting potential. Early research on human adversarial attacks (Bartolo et al., 2020; Mozes, Bartolo, et al., 2021; Mozes et al., 2022), showed that humans craft adversarial examples rather differently than automated attacks. Humans are more efficient than the best attack models (e.g., 11 attempts when querying a model interactively versus 140,000 queries for some automated attacks), better at preserving semantics in their attack, and have a more targeted approach to selecting candidate words for replacement. These are in line with expectations that humans can rely on a nuanced language understanding, while automated models need to rely on scale and iterative trial-and-error more often (Mozes, Bartolo, et al., 2021).\nWhat these studies suggest is that putting humans in the shoes of an adversary tasked to mislead a model can results in fundamentally new insights about how humans think that a model makes its decision. These insights are derived indirectly from contrasting the strategies employed by automated and human attacks. For deception research, this avenue could break new ground. Not only can we adopt the human adversary design to understand how humans think a deception classifier works, which can inform research on explanability needs (Oswald et al., 2018) and human-Al interaction (Kleinberg & Verschuere, 2021; Von Schenk et al., 2024). More substantively, we can expand this research to a human-on-human attack design: by letting humans attack human judgments (i.e., rewriting text to mislead a human in their credibility assessment), we can infer how an attacking human thinks another human \u2013 and oneself by proxy - will arrive at a credibility assessment. Contrasting that with machine-on-human and human-on- machine attacks will enable us to understand different implicit representations held by humans and Al models about deception and could potentially prove a fruitful avenue to inform deception theory."}, {"title": "Conclusions", "content": "Deception detection remains a hard problem but recent advances in human heuristics-based judgment and in automated verbal deception detection are promising. However, when aligned automated adversarial attacks seek to rewrite deceptive statements to appear truthful, human judgments and machine learning classifications dropped to the chance level. Incorporating adversarial attacks in deception research may offer exciting potential to improve deception detection robustness and understand human and machine-held representations of deception."}]}