{"title": "AdaEAGLE: Optimizing Speculative Decoding via Explicit Modeling of Adaptive Draft Structures", "authors": ["Situo Zhang", "Hankun Wang", "Da Ma", "Zichen Zhu", "Lu Chen", "Kunyao Lan", "Kai Yu"], "abstract": "Speculative Decoding (SD) is a popular lossless technique for accelerating the inference of Large Language Models (LLMs). We show that the decoding speed of SD frameworks with static draft structures can be significantly improved by incorporating context-aware adaptive draft structures. However, current studies on adaptive draft structures are limited by their performance, modeling approaches, and applicability. In this paper, we introduce AdaEAGLE, the first SD framework that explicitly models adaptive draft structures. AdaEAGLE leverages the Lightweight Draft Length Predictor (LDLP) module to explicitly predict the optimal number of draft tokens during inference to guide the draft model. It achieves comparable speedup results without manual thresholds and allows for deeper, more specialized optimizations. Moreover, together with threshold-based strategies, AdaEAGLE achieves a 1.62\u00d7 speedup over the vanilla AR decoding and outperforms fixed-length SotA baseline while maintaining output quality.", "sections": [{"title": "Introduction", "content": "Auto-regressive (AR) models are effective in language modeling but face latency issues due to their sequential token generation, particularly in Large Language Models (LLMs) (Brown et al., 2020; Ouyang et al., 2024; Touvron et al., 2023a,b). Speculative decoding (SD) mitigates this by dividing decoding into two stages: a draft stage and a verification stage (Chen et al., 2023; Leviathan et al., 2023). A smaller draft model predicts multiple tokens, which are verified by the larger model in one forward pass. By iterating these two stages, SD accelerates LLM while maintaining the original model's output distribution (Chen et al., 2023).\nMost existing methods generate draft tokens using static structures, such as fixed-length sequences (Chen et al., 2023; Leviathan et al., 2023)"}, {"title": "Analysis of Adaptive Draft Length", "content": "In this section, we provide a systematic analysis of the huge potential gain of integrating adaptive draft length control into SD. We start with introducing SD and its SotA variation, EAGLE (Li et al., 2024b). Then we analyze the optimal draft length under the EAGLE framework. Finally, based on pilot experiments, we discuss the significant benefit of adaptive drafts."}, {"title": "Speculative Decoding", "content": "Let $T_{1:j}$ denote the sequence of tokens generated so far $t_1, t_2,..., t_j$ by an AR model (typically a LLM), and $p(t_j)$ be the probability assigned to token $t_j$ by the target model. Speculative decoding introduces a smaller draft model that guesses a sequence of candidate tokens $T_{j+1:j+k}$ along with their probabilities $p(t_{j+i})$ for $i \\in [1,k]$. The target model then computes the actual probabilities $p(t_{j+i})$ for these tokens in parallel and decides the acceptance of the tokens in order. The acceptance of each draft token $t_{j+i}$ is determined by:\n$P_{accept}(t_{j+i}) = min(1, \\frac{p(t_{j+i})}{p'(t_{j+i})})$.\nIf token $t_{j+i}$ is rejected, all successor tokens $T_{j+i+1:j+k}$ are discarded, and a new token is resampled from the distribution: $norm(max(0, p(t_{j+i}) \u2013 p'(t_{j+i})))$.\nThis process allows SD to validate multiple tokens in one pass, significantly reducing the number of sequential steps required in AR decoding. This method ensures that the output distribution of the SD is consistent with vanilla AR decoding of the target LLM (Chen et al., 2023)."}, {"title": "EAGLE", "content": "To improve draft quality, instead of training a separate small model, EAGLE (Extrapolation Algorithm for Greater Language Model Efficiency) reuses the target model's input embedding and LM head, adding a trainable draft head in between (Li et al., 2024b). EAGLE's first version used a static tree for draft validation in one forward pass, while the second version dynamically selected candidates based on confidence values (Li et al., 2024a).\nDDD (Brown et al., 2024) refined this method but still relied on confidence-based acceptance with manually set thresholds, as discussed in \u00a7 1."}, {"title": "Draft-Length Oracle for EAGLE", "content": "To facilitate the discussion, unless otherwise stated, the following paper is based on sequential draft generation and greedy decoding (sample temperature is set as 0). We start by illustrating a new EAGLE-Oracle model with ideal adaptability via a draft-length oracle. The EAGLE-Oracle model uses the same architecture and parameters as the original EAGLE model, but during inference, a draft-length oracle module can tell the optimal draft length. In the context of greedy decoding, the optimal draft length is defined as the length that minimizes the number of forward passes required by the target model. In the context of greedy decoding and EAGLE, the optimal draft length is exactly the acceptance length (need an additional assumption, see the proof in Appendix A). By telling the draft model the optimal length before generating, EAGLE-Oracle ensures that the draft model outputs the necessary number of tokens, minimizing waste and maximizing efficiency."}, {"title": "Benefit of Adaptive Draft Lengths", "content": "By using the draft length oracle, EAGLE-Oracle achieves the theoretically optimal speedup under the given model parameters. We evaluated the acceleration effects of both fixed-length EAGLEs and EAGLE-Oracle. The experiment settings follow \u00a7 4.1, with results shown in Table 2. The first five rows represent EAGLE with exactly 2 to 6 fixed-length draft token sequences, respectively, while the sixth row corresponds to EAGLE-Oracle.\nFor EAGLE-Oracle, $N_{waste} = 0$ since it generates exactly the optimal number of tokens at every draft starting position. As a result, the average acceptance length and the throughput of EAGLE-Oracle are much higher than those of EAGLE models with fixed-length drafts (improved by up to 42%), demonstrating the huge potential reward of pursuing perfect adaptive draft lengths."}, {"title": "AdaEAGLE", "content": "In this section, we start by introducing the overall architecture of AdaEAGLE, the first SD framework that explicitly models the adaptive draft structure (\u00a7 3.1). To achieve this, we carefully design a Lightweight Draft Length Predictor (LDLP) which functions like a prophet by directly estimating the optimal draft length in advance during inference. More details of the LDLP are presented in \u00a7 3.2. Finally, we demonstrate how to train the LDLP (\u00a7 3.3)."}, {"title": "SD with Adaptive Draft Length", "content": "Denote the target model as $M_T$ and the draft model as $M_D$. The draft model is responsible for autoregressively generating drafts, while the target model verifies the correctness of all tokens in the draft in a single forward pass. The validated prefixes\u00b9 from the generated drafts are fed back into the draft model to continue decoding until the entire decoding process is complete.\nFormally, for the interaction between $M_T$ and $M_D$ in the r-th iteration, let $T^r = (t_1, t_2,..., t_j, t_{j+1},..., t_{j+k})$ denote the input sequence of $M_T$, where $T^r_{1:j}$ is the generated formal sequence till last iteration, $T^r_{j+1:j+k}$ is the draft at current iteration, and $k \\in N+$ is the draft length."}, {"title": "Lightweight Draft Length Predictor", "content": "In designing LDLP, we follow the \u201cKISS\u201d (Keep It Simple and Stupid) principle. our goal is to ensure that its output closely approximates the number"}, {"title": "Training of LDLP", "content": "The training of LDLP involves two main problems: training data collection and optimization criteria design.\nTraining data collection As discussed in \u00a7 3.2, the training data for LDLP consists of pairs of $([e_j; f_j], k_i)$, where $e_j$ and $f_j$ represent the embedding and last hidden state after final layer normalization in the target model for the j-th token, respectively. Here, $k_i$ denotes the optimal draft length starting from the j-th token. For a given prompt $T_{in}$ with m tokens, we collect such pairs in the following three steps:\n1) generate the output sequence $T_{out}$ (n tokens) by the target model $M_T$ based on $T_{in}$ (Figure 3-(c)-1)\n2) continue drafting $k_{max}$ tokens (denoted as $T'_{out}$) by the draft model $M_D$ based on $T_{in} + T_{out}$ (for any $1 \\leq j \\leq n$, see Figure 3-(c)-2)\n3) compare $T'_{out}$ to $T_{out}$ and calculate the length $k_i$ of the longest common prefix between them (Figure3-(c)-3)\nOptimization criteria design Let $k^o$ and $\\hat{k}$ denote the ground truth and the prediction of LDLP, respectively. On the one hand, we desire $\\hat{k}$ and $k^o$ to be as close as possible and we adopt $L_1$ loss to achieve this. On the other hand, we observe that the required time for a single forward pass of the target model is approximately 20 times that of the draft model. Therefore, we hope that $\\hat{k}$ is not less than $k^o$ as much as possible. To achieve this, we introduce a penalty coefficient $\u03bb > 1$ to scale up the $L_1$ loss when $\\hat{k} < k^o$. Formally, the criteria is defined as\n$L = {\\lambda \\cdot |\\hat{k} - k^o|   \\hat{k} < k^o\\ |\\hat{k} - k^o|  otherwise$ (4)"}, {"title": "Incorporate AdaEAGLE with Other Adaptive Draft Techniques", "content": "Our explicit draft length modeling via LDLP can be easily combined with other approaches that utilize inherent model outputs (e.g., logits, entropy) for draft boundary modeling. By evaluating the per-step condition, the draft model can terminate generation based on both the predicted length and inherent threshold criteria. In experiments, we discuss the combination of AdaEAGLE and DDD (Brown et al., 2024), referred to as AdaEAGLE-DDD."}, {"title": "Experiments", "content": "Model We conduct the experiments with Vicuna-7B-v1.3 (Chiang et al., 2023) and the paired EAGLE (Li et al., 2024b) draft model. Our LDLP model is a three-layer MLP with residual connections, and the output hidden states are mapped to the length scalar through a linear transform."}, {"title": "Main Results", "content": "Baselines We select the original EAGLE method with a fixed draft length as our baseline. During each decoding step, EAGLE autoregressively generates a fixed number of draft tokens (ranging from 2 to 6). We also compare our method with the recently proposed adaptive draft length approach, DDD (Brown et al., 2024), applied to EAGLE.\nAdaEAGLE-DDD We also incorporate our draft length prediction method with the threshold-based method of DDD, resulting in AdaEAGLE-DDD.\nTable 3 compares our method with other baseline methods in terms of average accepted tokens and throughput (Tok/s). On average, AdaEAGLE achieves the highest throughput, outperforming all variations of the fixed draft length with the standard EAGLE decoding method."}, {"title": "Loss Penalty", "content": "We train AdaEAGLE using L1 regression loss to predict the adaptive draft length logits. As discussed in Section 3.3, shorter lengths are given greater tolerance compared to longer predictions. To address this, we apply a penalized L1 loss during training, which penalizes predictions that are shorter than the ground truth label. Table 4 compares the impact of incorporating the penalized loss, showing that it improves both the average accepted length and throughput. Figure 4 further illustrates the draft length distribution with and without the loss penalty, highlighting that the loss penalty encourages longer draft lengths."}, {"title": "Regression v.s. Classification", "content": "We also explore alternative approaches for draft length prediction, including regression and classification. In the regression approach, the model maps hidden states to a continuous score, whereas in the classification approach, it maps hidden states to a predefined set of classes. As shown in Table 4, regression outperforms classification. This is because regression can naturally model the ordinal relationships between different labels."}, {"title": "AdaEAGLE-DDD thresholds", "content": "Table 5 presents the performance for various threshold values for the DDD component of AdaEAGLE-DDD, showing that the throughput remains stable and insensitive across different settings. Based on these results, we select a threshold value of -0.6 for the experiments reported in Table 3."}, {"title": "Related Works", "content": "LLM is popular and capable. However, its AR inference is slow and costly. Significant methods for efficient LLM inference have been proposed (Zhou et al., 2024). This section focuses on the technique of speculative decoding (SD) and adaptive draft structure for SD."}, {"title": "Speculative Decoding", "content": "Speculative decoding (SD) uses a draft-verify paradigm to realize lossless acceleration. SD frameworks can be roughly divided by the type of draft models (Xia et al., 2024): (1) Independent draft models. SpecDec (Xia et al., 2023) introduced a non-AR Transformer to generate multiple tokens simultaneously. Alternatively, many works (Leviathan et al., 2023; Chen et al., 2023; Spector and Re, 2023; Sun et al., 2023) propose using smaller pre-trained models from the same LLM series (e.g., Llama2-7B and Llama2-70B (Touvron et al., 2023b)) to accelerate inference, which avoids extra training and preserves alignment in prediction behaviors due to shared tokenizers and training corpora. (2) Self-drafting eliminates the need for external draft models by leveraging the target LLM itself for drafting (Stern et al., 2018; Santilli et al., 2023; Cai et al., 2024; Li et al.,"}, {"title": "Adaptive Draft Structure", "content": "An adaptive draft structure means that the draft model can adjust its generation structure (e.g., the length of the draft sequence, or the depth/width/shape of the draft tree) dynamically based on the speculation context. Although some studies have recognized the need to make the draft structure dynamic, these methods still lack specialization and systematic approaches, as listed in Table 1."}, {"title": "Conclusion", "content": "In this paper, we present a novel speculative decoding framework, AdaEAGLE, which is the first to implement adaptive draft control by explicitly modeling the optimal draft structure. By incorporating the Lightweight Draft Length Prophet (LDLP) module, AdaEAGLE predicts the optimal draft length to guide token generation, minimizing the number of forward passes for the large target model and maximizing overall decoding speed. Experimental results demonstrate that AdaEAGLE avxchieves decoding throughput closer to the oracle topline than existing SD methods. It provides a lossless speedup of 1.61% over vanilla AR de-"}, {"title": "Limitations", "content": "Unlike EAGLE (Li et al., 2024b) and EAGLE-2 (Li et al., 2024a) which use tree-based decoding, the method presented in this paper is currently limited to a sequential draft structure, so we only provide results under a greedy (top-1) decoding strategy.\nAdditionally, further exploration could be conducted on how LDLP can improve accuracy. Although this paper presents improvements based on EAGLE, the concept of directly modeling adaptive draft structures can, in fact, be applied to many other frameworks, such as (Zhang et al., 2024). Therefore, future research is needed to optimize LDLP, evaluate different SD frameworks, and examine the interactions between these elements."}, {"title": "Oracle-Guided Analysis of Optimal Draft Length", "content": "Assume that when the input is token sequence $T_{a:b}$, output hidden states from the target model's second-to-top layer and the draft model's AR head are $F(T_{a:b}) = {f(t_a), f(t_{j+1})..., f(t_b)}$ and $F'(T_{a:b}) = {f'(t_a), f'(t_{j+1})..., f'(t_b)}$, respectively. Let the generated token sequence so far be denoted as $T_{1:j}$ and the draft token sequence based on $T_{1:j}$ be $T_{j+1:j+k|j}$. Under the greedy decoding strategy, given the sequence history $T_{1:j}$, no matter what draft model outputs, the target model's output results are deterministic. We refer to the token sequence generated by the original target model (without drafts) as the formal token sequence.\nIn SpecDec++ (Huang et al., 2024), the optimal draft length $k = optK(j)$ is defined as the length such that minimizes the number of forward passes required by the target model.\nAssumption 1. Let j and j' (j' < j) be the length"}]}