{"title": "Smoothness Really Matters: A Simple yet Effective Approach for Unsupervised Graph Domain Adaptation", "authors": ["Wei Chen", "Guo Ye", "Yakun Wang", "Zhao Zhang", "Libang Zhang", "Daxin Wang", "Zhiqiang Zhang", "Fuzhen Zhuang"], "abstract": "Unsupervised Graph Domain Adaptation (UGDA) seeks to bridge distribution shifts between domains by transferring knowledge from labeled source graphs to given unlabeled target graphs. Existing UGDA methods primarily focus on aligning features in the latent space learned by graph neural networks (GNNs) across domains, often overlooking structural shifts, resulting in limited effectiveness when addressing structurally complex transfer scenarios. Given the sensitivity of GNNs to local structural features, even slight discrepancies between source and target graphs could lead to significant shifts in node embeddings, thereby reducing the effectiveness of knowledge transfer. To address this issue, we introduce a novel approach for UGDA called Target-Domain Structural Smoothing (TDSS). TDSS is a simple and effective method designed to perform structural smoothing directly on the target graph, thereby mitigating structural distribution shifts and ensuring the consistency of node representations. Specifically, by integrating smoothing techniques with neighborhood sampling, TDSS maintains the structural coherence of the target graph while mitigating the risk of over-smoothing. Our theoretical analysis shows that TDSS effectively reduces target risk by improving model smoothness. Empirical results on three real-world datasets demonstrate that TDSS outperforms recent state-of-the-art baselines, achieving significant improvements across six transfer scenarios. The code is available in https://github.com/cwei01/TDSS.", "sections": [{"title": "Introduction", "content": "Graph Neural Networks (GNNs) have become a powerful tool for processing graph-structured data (Zhu et al. 2023; Wang et al. 2024; Zhu et al. 2024; Liao et al. 2024; Chen et al. 2024b,c; Liang et al. 2024a,b). Despite their notable success, GNNs often struggle with generalizing to distribution shifts, leading to unsatisfactory performance in the new domains (Li et al. 2022; Gao et al. 2023; Luo et al. 2024).\nTo tackle the challenge of distribution shifts across domains, Unsupervised Graph Domain Adaptation (UGDA) (Wilson and Cook 2020; Shi et al. 2024) provides an effective strategy by focusing on minimizing domain discrepancies, enabling the transfer of knowledge from a"}, {"title": "Related Work", "content": "Unsupervised Graph Domain Adaptation. UGDA aims to transfer knowledge from a well-labeled source graph to an unlabeled target graph, tackling the distribution shifts inherent in graph-structured data. Unlike traditional domain adaptation techniques (Zhuang et al. 2020), UGDA encounters greater challenges due to the non-IID nature of graph data, which complicates the alignment of node embeddings between the source and target graphs and can lead to mismatches and inconsistencies in the transferred knowledge. Consequently, many approaches have explored various techniques to achieve effective knowledge transfer, such as MMD (Shen et al. 2020b), graph subtree discrepancy (Wu, He, and Ainsworth 2023), and adversarial learning (Xiao et al. 2023; Li et al. 2024). However, these methods often miss the specific challenges posed by structural shifts in graphs, which can result in less effective solutions. Recently, a few studies have begun addressing distributional changes in graph structures by designing methods for structural alignment to reduce variations. For example, StruRW (Liu et al. 2023) reweights edges in the source graph to mitigate neighborhood shifts, while PairAlign (Liu et al. 2024b) recalibrates node influence through edge weights and adjusts classification loss with label weights to manage both structural and label shifts. Yet, when there is a significant disparity in the number of nodes and edges between the source and target graphs, direct edge reweighting may not fully capture the target graph's intricate structure and may still result in suboptimal performance.\nModel Smoothness. Model smoothness refers to designing a model that produces consistent and stable predictions across similar data points, ensuring that the prediction outputs are uniformly flat within certain neighborhoods (Rosca et al. 2020; Wang and Manchester 2023). This concept has already been both empirically and theoretically validated to improve model generalization and robustness in independent and identically distributed scenarios (Wang and Manchester 2023). In the UGDA setting, several approaches have sought to improve UGDA performance by incorporating additional model generalization constraints into their objectives. For example, SpecReg (You et al. 2023) derives a generalization bound for single-layer GNNs and enhances it by introducing spectral regularization. Similarly, A2GNN (Liu et al. 2024a) applies a Lipschitz formulation for k-layer GNNs, demonstrating that the target risk bound can be tightened by decreasing propagation layers in the source graph while increasing them in the target graph, though this approach can easily lead to over-smoothing. Different from these approaches, our work focuses on directly reducing target risk by leveraging model smoothness, providing a novel perspective to enhance the effectiveness of UGDA."}, {"title": "Problem Formulation", "content": "Consider an undirected, unweighted graph $G = (V, E)$, where V is the set of nodes connected by the edge set E. The adjacency matrix of G is a binary symmetric matrix A such that $A_{i,j} = 1$ if the i-th and j-th nodes are connected by an edge, and $A_{i,j} = 0$ otherwise.\nGiven a source graph $G_s = (V_s, E_s)$ with m labeled nodes and a target domain graph $G_t = (V_t, E_t)$ with n unlabeled nodes, we assume that both graphs share the same feature and label space but have different marginal distributions, i.e., $P(X_s) \\neq P(X_t)$ but $P(Y_s | X_s) = P(Y_t | X_t)$. In this paper, our goal is to develop a GNN-based classifier that can predict the node labels ${y_i}_{i=1}^n$ in the target domain, where $y \\in V_t$. We structure the model as a composition $g \\circ \\varphi$, where the mapping $\\varphi : X \\rightarrow H$ transforms the features into a latent space H, and the function $g : H \\rightarrow Y$ performs classification in the label space Y."}, {"title": "Methodology", "content": "In this section, we begin by presenting target domain structural smoothing tailored for UGDA. Following this, we provide a theoretical analysis showing how target error is influenced by model smoothness, demonstrating the method's effectiveness in enhancing knowledge transfer. Lastly, we offer a detailed analysis of the model's complexity."}, {"title": "Target Domain Structural Smoothing", "content": "As previously discussed, even minor structural differences between the source and target domains can significantly alter node representations. To counteract this, we introduce a straightforward yet powerful approach called target domain structural smoothing, designed to mitigate the effects of structural shifts. The key concept is to enhance the smoothness of node representations in the target graph by constraining the variations between neighboring nodes, thereby minimizing the adverse impact of motif-related structural changes on the model's predictive performance.\nThe overall process comprises two main steps: neighboring node generation and Laplacian smoothness constraint. In the first step, semantically similar nodes are sampled as neighbors for each node. The sampling method is flexible, accommodating various strategies such as random sampling, k-hop sampling, or random walk sampling. In this context, we present two specific techniques for neighbor sampling.\nk-hop Sampling: In the k-hop sampling, the neighbors of each node are selected by traversing up to k edges in the graph, where k determines the size of the neighborhood. Formally, let $G = (V,E)$ represent an undirected graph, where V is the set of nodes and E is the set of edges. For a given node $v_i$, its k-hop neighbors $N_k(v_i)$ are defined as:\n$N_k(v_i) = \\{v_j \\in V | shortest\\_path(v_i, v_j) \\leq k\\}, \\quad(1)$\nwhere $shortest\\_path(v_i, v_j)$ denotes the shortest distance between nodes $v_i$ and $v_j$.\nRandom Walk Sampling: The effectiveness of k-hop sampling is highly dependent on the parameter k. An inappropriate k can compromise the distinguishability of the learned embeddings. For example, a large k might lead to different nodes sharing identical subgraphs, causing their embeddings to collapse into a single point, which is a characteristic of over-smoothing in GNNs.\nInstead, the random walk sampling enables flexible exploration of the graph structure during sampling, helping to avoid over-smoothing and maintain meaningful distinctions between nodes (Nikolentzos and Vazirgiannis 2020). Formally, for a given node $v_i \\in V$, the set of neighboring nodes $N_r(v_i)$ is defined as:\n$N_r(v_i) = \\{v_j \\in V | RW_\\lambda(V_i, v_j) = True\\}, \\quad(2)$\nwhere $RW_\\lambda(V_i, v_j)$ indicates if node $v_j$ is visited during a random walk of length $\\lambda$ starting at node $v_i$.\nNext, let A be the original adjacency matrix of graph G. Here, we use random walk sampling as an example to illustrate the subsequent process. The updated adjacency matrix $\\hat{A}$, reflecting the sampled neighborhood, can be defined as:\n$\\hat{A}_{ij} = \\begin{cases}\n1, & \\text{if } v_j \\in N_r(v_i) \\text{ and } (v_i, v_j) \\in E \\\\\n0, & \\text{otherwise}.\n\\end{cases}\\quad(3)$\nThis updated matrix $\\hat{A}$ ensures that only edges between node $v_i$ and its sampled neighbors $N_r(v_i)$ are retained, thereby emphasizing relevant connections within the graph.\nFinally, we apply a smoothness constraint for each node to ensure consistency among neighboring nodes in target graph. In this context, we focus on applying this constraint solely to the target graph, as explained in Remark 2. Let $f(x_i)$ denote the feature representation of node i, and $d_i$ represent the degree of node i. The Laplacian smoothing regularization loss $L_{SR}$ is defined as follows:\n$L_{SR} = \\frac{1}{2} \\sum_i \\sum_j \\hat{A}_{ij} \\left( \\frac{f(x_i)}{\\sqrt{d_i}} - \\frac{f(x_j)}{\\sqrt{d_j}} \\right)^2. \\quad(4)$\nThis constraint minimizes feature differences between nodes and their neighbors, normalized by the square root of their degrees. This normalization ensures that high-degree nodes do not overly dominate the smoothing process."}, {"title": "Target Risk Bound with Model Smoothness", "content": "In this part, we show that how the target risk is bounded above by the model's smoothness, providing a theoretical foundation for the effectiveness of our approach. Formally, we define the model smoothness (Rosca et al. 2020) and total variation distance (TVD) (Villani et al. 2009).\nDefinition 1. Model Smoothness: A model f is $(\\kappa,r)$-cover with $\\Phi$ smoothness on graph G, if\n$E_{G} \\left[ \\underset{d(i,j)<\\kappa, ||x_i-x_j||_{\\infty}\\leq r}{\\sup} |f(\\Theta,x_{i}) - f(\\Theta,x_{j})| \\right] < \\Phi, \\quad(5)$\nwhere 'sup' refers to the supremum, $\\kappa$ is the maximum distance between neighboring nodes, and r denotes the maximum difference in the feature space between node features, and $\\Theta$ is the model parameters. It should note that a lower $\\Phi$ value indicates a higher level of model smoothness."}, {"title": "Definition 2. Total Variation Distance", "content": "Given two graphs $G_1$ and $G_2$, $G_1(v)$ and $G_2(v)$ represent the attribute distributions of node v on graphs $G_1$ and $G_2$ respectively. The total variation distance is defined as:\n$TVD(G_1, G_2) = \\frac{1}{2} \\sum_{v \\in V} |G_1(v) - G_2(v)|, \\quad(6)$\nwhere it measures the maximum difference between the attribute distributions of two nodes on the graph.\nAs we proceed, let's assume that the graphs $G_1$ and $G_2$ from the source and target domains have a compact support, denoted as $X \\subset \\mathbb{R}^d$, where d is the feature dimension. This implies there exists a constant $\\Gamma > 0$ such that for any two nodes $x_i, x_j \\in X$, the difference between their features, $||x_i - x_j||$, is also constrained by $\\Gamma$. Consider $L(f(x), y)$ as the loss function, which is continuous and differentiable. Here, $f(x)$ is the output of the GNNs predicting the node label, and y is the ground truth of node. We define the expected risk for the model f across the distribution S as $E_S(f) = E_{\\{x, y\\} \\sim S} [L(f(x), y)]$. Additionally, we assume that the loss is bounded, i.e., $0 \\leq L(f(x), y) \\leq \\Upsilon$. By combining our understanding of model smoothness and the concept of TVD, we can establish the following theorem."}, {"title": "Theorem 1.", "content": "Consider two distributions S and T, if a model f is $(\\kappa, 2r)$-cover with $\\Phi$ smoothness over distributions S and T, then with probability at least 1 - $\\xi$, the following holds:\n$E_T(f) \\leq E_S(f) + 2TVD(S, T) + \\Phi_S + \\Phi_T + K, \\quad(7)$\nwhere\n$K = \\frac{\\Upsilon Z}{\\sqrt{m}} + \\Upsilon \\frac{Z}{\\sqrt{n}} \\frac{\\log(1/\\xi)}{2}, \\quad(8)$\nand\n$Z = \\sqrt{2d} \\cdot \\frac{2^{42} \\Gamma^2 + 1}{4} \\cdot (\\log 2 + 2 \\log(1/\\xi)). \\quad(9)$\nProof. Refer to the Appendix for the details.\nIn this theorem, K and Z are intermediate variables, $\\Gamma$ and $\\Upsilon$ are constants specific to the given dataset, and m and n denote the number of nodes in the source and target domains, respectively. It is important to note that, unlike previous methods that derive upper bounds for target error (You et al. 2023; Liu et al. 2024a), we propose that the target risk $E_T(f)$ is constrained by the source risk $E_S(f)$, domain discrepancy TVD(S, T), and the model's smoothness $\\Phi$. This relationship is encapsulated in Theorem 1, which provides a more comprehensive understanding of the factors impacting the target risk. Therefore, by reducing model's smoothness $\\Phi$, we can effectively decrease the target error, offering a novel perspective for addressing the UGDA problem. It is also the core of our proposed TDSS approach."}, {"title": "Overall Optimization", "content": "The overall training process of our proposed TDSS consists of three main components:\n$\\mathcal{L} = \\mathcal{L}_{GC} + \\alpha \\mathcal{L}_{DA} + \\beta \\mathcal{L}_{SR}, \\quad(13)$\nwhere the first term, $\\mathcal{L}_{GC}$, represents the GNNs classifier loss. In this paper, we employ the state-of-the-art A2GNN (Liu et al. 2024a) to optimize this component. The $\\mathcal{L}_{DA}$ is the domain alignment loss, where we utilize the MMD method to align the source and target domains, aiming to minimize domain discrepancies. Finally, $\\mathcal{L}_{SR}$ is the smoothness loss, designed to ensure node smoothness by constraining feature variations between neighboring nodes. The $\\alpha$ and $\\beta$ act as trade-offs to balance the contributions of the domain alignment and smoothness losses, respectively.\nComplexity Analysis. In this section, we analyze the time complexity of TDSS, using the random walk sampling method as an example. Firstly, for each node performing $\\gamma$ random walks with a walk length of $\\lambda$, the time complexity is $O(n \\gamma \\lambda)$, where n is the node number in target graph. The values of $\\gamma$ and $\\lambda$ are chosen to be small, thus not incurring significant computational overhead. Secondly, for each node calculating the smoothing constraint with its random walk sampled neighbors, assuming that each node samples on average p neighbors (i.e., $p < \\gamma \\lambda$), the time complexity is O(npd), where d is the number of features. Therefore, the overall time complexity is $O(n(\\gamma \\lambda + pd)).$"}, {"title": "Experiment", "content": "In this section, we conduct experiments on three real-world datasets to address the following five research questions:\n(RQ1): How does the proposed method TDSS perform when compared to state-of-the-art baseline methods?\n(RQ2): How is the effectiveness of the proposed model evaluated across different backbone GNNs architectures?\n(RQ3): Which part of the model primarily contributes to the effective prediction of optimal UGDA?\n(RQ4): How do the hyper-parameters affect the performance of the proposed approach?\n(RQ5): How about the intuitive effect of TDSS?\nExperimental Setups\nDatasets We conduct experiments utilizing three real-world graphs from the ArnetMiner dataset (Tang et al. 2008): ACMv9 (A), Citationv1 (C), and DBLPv7 (D). To address the discrepancies in node attributes across these graphs, we integrate their attribute sets and standardized the attribute dimension to 6775, as elaborated in (Qiao et al. 2023). Each node signifies a paper, while each edge denotes a citation between two papers. Our objective is to categorize all the papers into five distinct research topics: Databases, Artificial Intelligence, Computer Vision, Information Security, and Networking. This study focuses on six key transfer tasks: A \u2192 C, A \u2192 D, C \u2192 A, C \u2192 D, D \u2192 A, and D \u2192 C. The statistics of three datasets are summarized in Table 1.\nBaselines For performance evaluation, we compare TDSS with three categories of baseline methods: (1) Unsupervised graph learning: DeepWalk (Perozzi, Al-Rfou, and"}, {"title": "Results for Various GNNs (RQ2)", "content": "Since TDSS2 is designed to be model-agnostic and seamlessly integrable into existing GNNs frameworks, it can be effectively integrated not only into state-of-the-art models like A2GNN but also into standard GNNs. To validate its broad applicability, we conduct experiments with GCN, GAT, and SGC. As shown in Table 3, the results demonstrate significant performance gains with our method compared to the original models without the smoothing constraint. Overall, these findings clearly indicate that our proposed TDSS approach enhances the generalization capabilities of various GNNs models. The consistent improvements across different tasks and backbones underscore the effectiveness and versatility of TDSS in advancing graph domain adaptation techniques.\nAblation Study (RQ3). To investigate the specific contributions of different components, we conduct an in-depth analysis and ablation studies on TDSS. The results are shown in Figure 2, and the variants of TDSS are: (1) w/o DA: which eliminates the alignment loss to assess the impact of domain alignment (i.e., setting $\\alpha = 0$). (2) w/o SR: which eliminates the smooth regularization loss to evaluate the impact of model smoothness (i.e., $\\beta = 0$). (3) w/o RW:"}, {"title": "Visualization Analysis (RQ5)", "content": "To provide an intuitive understanding of TDSS, we visualize the node representations learned in the target domain for the D \u2192 A task, comparing it with three models. Node embeddings are visualized using Stochastic Neighbor Embedding (SNE) (Hinton and Roweis 2002), as shown in Fig 4. TDSS achieves the best clustering performance with an NMI of 0.4175, indicating clear separation between clusters and tighter groupings of nodes. In contrast, SpecReg and A2GNN exhibit more overlap and dispersion, with NMIs of 0.2911 and 0.3651, respectively. The red circles highlight areas where TDSS reduces inter-cluster mixing, demonstrating superior discriminative power and feature representation in complex domains."}, {"title": "Conclusion", "content": "In this paper, we present target-domain structural smoothing, a simple yet effective method for enhancing model transferability in UGDA tasks. By integrating Laplacian smoothing with a neighborhood sampling mechanism, TDSS preserves consistent node representations and strengthens model robustness against structural distribution shifts. Our theoretical analysis underscores the importance of addressing model smoothness for successful domain adaptation. Empirical results on three real-world datasets validate the superiority of TDSS over state-of-the-art baselines, and demonstrate significant improvements across a range of transfer scenarios."}, {"title": "Appendix", "content": "Summary. In this section, we theoretically reveals that target error is influenced by source graph error, total variation distance, and model smoothness. In detail, we first establish a term to measure the robustness of a function f under distribution shift constrained by the Wasserstein distance on the source distribution S and target distribution T. Then, we can deduce that robustness of f in each domain is bounded by its model smoothness. Finally, by applying the domain transfer theorem, we can get the transfer error is bounded by the variation distance, and model smoothness of each domain.\nProof. Consider the set $B_W(\\mathcal{S}, r) = { \\mathcal{S} : W_\\infty(\\mathcal{S}, \\mathcal{S}) \\leq r }$, where $W_\\infty$ denotes the $\\infty$-th Wasserstein distance (R\u00fcschendorf 1985), a metric commonly used to quantify the difference between two probability distributions. The set $B_W(\\mathcal{S}, r)$ encompasses all distributions $\\mathcal{S}$ that lie within a radius r of the reference distribution $\\mathcal{S}$, under the $W_\\infty$ metric. Next, define $\\mathcal{S}_r$ as the distribution within $B_W(\\mathcal{S}, r)$ that minimizes the expected loss $E_{\\mathcal{S}}(f)$. Mathematically (Li and Chen 2022), this can be expressed as:\n$\\mathcal{S}_r = \\arg \\underset{\\mathcal{S} \\in B_W(\\mathcal{S}, r)}{\\min} E_{\\mathcal{S}}(f). \\quad(14)$\nSimilarly, define $\\mathcal{T}_r$ as the distribution within $B_W(\\mathcal{T}, r)$ that minimizes the expected loss $E_{\\mathcal{T}}(f)$, given by:\n$\\mathcal{T}_r = \\arg \\underset{\\mathcal{T} \\in B_W(\\mathcal{T}, r)}{\\min} E_{\\mathcal{T}}(f). \\quad(15)$\nIn essence, $\\mathcal{S}_r$ and $\\mathcal{T}_r$ denote the optimal distributions within their respective Wasserstein balls, $B_W(\\mathcal{S}, r)$ and $B_W(\\mathcal{T}, r)$, that yield the minimum expected loss for a given f. These distributions help assess the robustness of f under distributional shifts constrained by the Wasserstein distance. Thus, we have the following derivation:\n$E_T(f) = E_T(f) - E_{\\mathcal{T}_r}(f) + E_{\\mathcal{T}_r}(f) - E_{\\mathcal{S}_r}(f) + E_{\\mathcal{S}_r}(f) - E_{\\mathcal{S}}(f) + E_{\\mathcal{S}}(f)$\n$\\leq E_{\\mathcal{S}}(f) + |E_{\\mathcal{T}_r}(f) - E_{\\mathcal{T}}(f)| + |E_{\\mathcal{T}_r}(f) - E_{\\mathcal{S}_r}(f)| + |E_{\\mathcal{S}_r}(f) - E_{\\mathcal{S}}(f)|$\n$\\leq E_{\\mathcal{S}}(f) + |E_{\\mathcal{T}_r}(f) - E_{\\mathcal{T}}(f)| + |E_{\\mathcal{T}_r}(f) - E_{\\mathcal{S}_r}(f)| + |E_{\\mathcal{S}_r}(f) - E_{\\mathcal{S}}(f)|. \\quad(16)$\nThen, based on a recent study (Yi et al. 2021), for any model parameters $\\Theta$ and r, we have the following:\n$\\underset{\\mathcal{S} \\in B_W(\\mathcal{S}, r)}{\\sup} R_{\\mathcal{S}}(\\Theta) = E_{\\mathcal{S}} \\left[ \\underset{||\\delta||_\\infty \\leq r}{\\sup} f(\\Theta, x + \\delta) \\right], \\quad(17)$\n$\\underset{x \\sim \\mathcal{S}}{\\text{where}} \\ \\ \\$\nThis expression represents the supremum of the risk $R_{\\mathcal{S}}(\\Theta)$ over all distributions $\\mathcal{S}$ within a Wasserstein ball $B_W(\\mathcal{S}, r)$, which equals the expected value under $\\mathcal{S}$ of the supremum of $f(\\Theta, x + \\delta)$ for perturbations $\\delta$ bounded by r in the infinity norm. It shows that the distributional perturbation measured by $W_\\infty$ distance is equivalent to input perturbation. Hence, we can study $W_\\infty$ distributional robustness through $l_\\infty$-input-robustness (i.e., model smoothness).\nInspired by (Xu and Mannor 2012), we construct an r-cover for the metric space $(X, || \\cdot ||_2)$. The covering number $N(r, X, || \\cdot ||_2)$ denotes the minimum number of r-balls needed to cover X under the $l_2$-norm. This satisfies the inequality $N(r, X, || \\cdot ||_2) \\leq (2d)^{(2\\Gamma/r)^2 + 1} = N$, where X can be enclosed by a polytope with $l_2$-diameter smaller than $2\\Gamma$ and 2d vertices. For details, refer to Theorem 4 in (Vershynin 2018). Next, we consider the space X under the $l_\\infty$-norm. Due to the geometric properties of X, the covering number under the $l_\\infty$-norm satisfies a similar inequality $N(r, ...)$.\nTo construct such a cover explicitly, consider the collection of sets $(C_1, ..., C_N)$, where each $C_i$ is disjoint from the others and satisfies $||u - v||_\\infty \\leq r$ for any $u, v \\in C_i$. The sets $C_i$ can be constructed as $C_i = \\widehat{C_i} \\cap (\\cup_i \\{u\\})$, where $(\\widehat{C_1}, ..., \\widehat{C_N})$ is a covering of X under the $l_\\infty$-norm, and the diameter of each $\\widehat{C_i}$ is smaller than r, ensuring that each $C_i$ is a valid subset within the cover and remains disjoint from the others. Thus, we have:\n$|E_{\\mathcal{S}_r}(f) - E_{\\mathcal{S}}(f)|$\n$\\leq \\underset{\\mathcal{S} \\in B_W(\\mathcal{S}, r)}{\\sup} |R_{\\mathcal{S}}(\\Theta) - R_{\\mathcal{S}_m}(\\Theta)|$\n$= E_{\\mathcal{S}} \\left[ \\underset{||\\delta||_\\infty \\leq r}{\\sup} f(\\Theta, x + \\delta) \\right] - R_{\\mathcal{S}_m}(\\Theta)$\n$= \\sum_{j=1}^N E_{\\mathcal{S}} \\left[ \\underset{||\\delta||_\\infty \\leq r}{\\sup} f(\\Theta, x + \\delta) | x \\in C_{\\mathcal{S}}(C_j) \\right] - R_{\\mathcal{S}_m}(\\Theta)$\n$= \\sum_{j=1}^N E_{\\mathcal{S}} \\left[ \\underset{||\\delta||_\\infty \\leq r}{\\sup} f(\\Theta, x + \\delta) | x \\in C_j \\right] - \\sum_{j=1}^N \\frac{A_j}{m} \\sum_{i=1}^m f(\\Theta, x_i)$\n$\\leq N \\cdot E_{\\mathcal{S}} \\left[ \\underset{||\\delta||_\\infty \\leq r}{\\sup} f(\\Theta, x + \\delta) | x \\in C_{\\mathcal{S}}(C_j) \\right] - \\frac{A_j}{m} \\sum_{i=1}^m f(\\Theta, x_i)$\n$+ N \\cdot E_{\\mathcal{S}} \\left[ \\underset{||\\delta||_\\infty \\leq r}{\\sup} f(\\Theta, x + \\delta) | x \\in C_{\\mathcal{S}}(C_j) \\right] - \\frac{A_j}{m} \\sum_{i=1}^m f(\\Theta, x_i)$\n$\\leq \\frac{1}{m} \\sum_{j=1}^N A_j \\left[ \\underset{x \\in C_j, x^{\\prime} \\in C_j + B_\\infty(0, r)}{\\sup} |f(\\Theta, x^{\\prime}) - f(\\Theta, x_i)| \\right]$\n$+ \\Upsilon \\sum_{j=1}^N |\\frac{A_j}{m} - S(C_j)|$\n$\\leq \\sum_{j=1}^N \\frac{A_j}{m} \\left[ \\underset{x \\in C_j, x^{\\prime} \\in C_j + B_\\infty(0, r)}{\\sup} |f(\\Theta, x^{\\prime}) - f(\\Theta, x_i)| \\right]$\n$+ \\Upsilon \\sum_{j=1}^N |\\frac{A_j}{m} - S(C_j)|$\n$\\leq \\frac{1}{m} \\sum_{j=1}^N A_j \\left[ \\underset{||\\delta||_\\infty \\leq 2r}{\\sup} |f(\\Theta, x + \\delta) - f(\\Theta, x)| \\right]$\n$\\leq \\Phi_S + \\Upsilon \\sum_{j=1}^N |\\frac{A_j}{m} - S(C_j)|. \\quad(18)$\nMoreover, based on Proposition A6.6 in (Van Der Vaart et al. 1996), we have\n$P \\left[ \\sum_{j=1}^N |\\frac{A_j}{m} - S(C_j)| \\geq \\epsilon \\right] \\leq 2^N \\exp \\left( - \\frac{m \\epsilon^2}{2} \\right). \\quad(19)$"}]}