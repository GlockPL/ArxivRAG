{"title": "Towards Robust Speech Representation Learning for Thousands of Languages", "authors": ["William Chen", "Wangyou Zhang", "Yifan Peng", "Xinjian Li", "Jinchuan Tian", "Jiatong Shi", "Xuankai Chang", "Soumi Maiti", "Karen Livescu", "Shinji Watanabe"], "abstract": "Self-supervised learning (SSL) has helped extend speech technologies to more languages by reducing the need for labeled data. However, models are still far from supporting the world's 7000+ languages. We propose XEUS, a Cross-lingual Encoder for Universal Speech, trained on over 1 million hours of data across 4057 languages, extending the language coverage of SSL models 4-fold. We combine 1 million hours of speech from existing publicly accessible corpora with a newly created corpus of 7400+ hours from 4057 languages, which will be publicly released. To handle the diverse conditions of multilingual speech data, we augment the typical SSL masked prediction approach with a novel dereverberation objective, increasing robustness. We evaluate XEUS on several benchmarks, and show that it consistently outperforms or achieves comparable results to state-of-the-art (SOTA) SSL models across a variety of tasks. XEUS sets a new SOTA on the ML-SUPERB benchmark: it outperforms MMS 1B and w2v-BERT 2.0 v2 by 0.8% and 4.4% respectively, despite having less parameters or pre-training data. Checkpoints, code, and data are found in https://www.wavlab.org/activities/2024/xeus/.", "sections": [{"title": "1 Introduction", "content": "Our planet is home to over 7000 languages (Austin and Sallabank, 2011), yet most speech processing models are only capable of serving at most 100-150 of them (Barrault et al., 2023b; Radford et al., 2023). The biggest constraint in supporting more of languages is the lack of transcribed speech: only around half of the world's languages have a formal writing system (Ethnologue, 2017), and even fewer of them have the resources to support the scale of annotated data required for training neural models. A common approach to address this limitation is self-supervised learning (SSL) on large amounts of unlabeled multilingual speech (Zhang et al., 2023; Black, 2019; Li et al., 2022), which allows for strong downstream performance even when access to annotated data is limited.\nWhile SSL models have more relaxed data requirements relative to supervised models, few works have fully exploited this aspect to scale models to more languages. In fact, most multilingual SSL models remain in the 50-150 language range of coverage (Babu et al., 2022; Chen et al., 2023b; Chiu et al., 2022), reducing the benefits of this advantage. The MMS project (Pratap et al., 2023) sought to address this issue by directly crawling data for more languages, scaling SSL pre-training to 1,000+ languages. The authors collected speech across 1,406 languages and used it to train an SSL model, showing state-of-the-art (SOTA) results after fine-tuning on multilingual automatic speech recognition (ASR) and 3,900-way language identification (LID). While the MMS models were publicly released, the crawled data was not, preventing it from being used in future work and thus the models from being reproduced (Table 1).\nAn additional issue that is relatively unexplored in SSL research is robustness to noisy data. This aspect is important for multilingual models, since the available recordings of low-resource languages tend to be particularly noisy (Ardila et al., 2020). The issue is exacerbated by the fact that existing multilingual SSL corpora lack diversity not only in languages but also in speaking style and recording conditions. WavLM (Chen et al., 2022) and WavLabLM (Chen et al., 2023b) tackled this issue by simulating noisy conditions during training, overlapping utterances or adding acoustic noise to simulate multi-speaker and noisy environments respectively. While effective, we believe that this technique can be improved to cover an even wider variety of recording conditions.\nOur goal is to thus build a universal speech encoder that can handle both linguistically and acoustically diverse speech. To achieve this, we propose XEUS (pronounced Zeus) -- a Cross-lingual Encoder for Universal Speech. XEUS is an E-Branchformer (Kim et al., 2023) encoder pre-trained on over 1 million hours of publicly available data across a wide variety of recording conditions. We first curate the data from 37 existing corpora to ensure a diverse selection of speech and recording conditions not often found in standard ASR datasets, including but not limited to spontaneous speech, accented speech, code-switching, indigenous languages, and singing voices. We expand the language coverage of XEUS by introducing a new SSL corpus that uses data sources previously unseen in speech literature. This corpus, which will be publicly released, contains 7,413 hours of unlabeled audio across 4,057 languages, the widest coverage of any speech processing dataset.\nTo enhance the model's robustness, XEUS is also pre-trained with a novel SSL objective of acoustic dereverberation, which requires the model to predict clean discrete phonetic pseudo-labels from simulated reverberant audio. By combining this objective with HuBERT-style masked prediction (Hsu et al., 2021) and WavLM-style denoising (Chen et al., 2022), XEUS is designed to be the next step towards a truly universal speech encoder for any language or recording condition.\nIn our downstream evaluations, we find that XEUS consistently improves over SOTA SSL models across a wide variety of tasks. XEUS sets a new SOTA on the ML-SUPERB multilingual ASR benchmark, outperforming SSL models such as MMS (Pratap et al., 2023) and w2v-BERT 2.0 v2 (Barrault et al., 2023b) while having fewer parameters or less training data. Our speech translation (ST) results show the effectiveness of XEUS' wide language coverage, even for languages with less than 10 hours of data in the pre-training corpus. We also explore XEUS' potential in generative tasks and show its superiority on speech resynthesis when compared to other SSL encoders. Finally, we evaluate XEUS' representations on a variety of tasks through the English-only SUPERB benchmark, where it sets a new SOTA on 4 tasks despite XEUS' focus on multilingual performance.\nTo conduct SSL pre-training at such scale, we had to make significant optimizations to existing speech processing toolkits. To encourage further SSL research and reproducibility, we will publicly release this code, along with the training configurations and checkpoints for XEUS. We also release all 200+ intermediate checkpoints and training logs obtained throughout the pre-training for further research in the training dynamics of large-scale multilingual SSL models.\nTo summarize, our main contributions are as follows:\n1. We publicly release a new corpus that contains 7,413 hours of unlabeled speech across 4,057 languages, 25+ times wider coverage than current public datasets (Shi et al., 2023a).\n2. We introduce a new self-supervised task that improves model robustness by implicitly learning to clean reverberant audio.\n3. We publicly release XEUS, a SSL speech encoder trained on over 1 million hours of data across 4,057 languages.\n4. We evaluate XEUS on numerous downstream tasks, and show that it outperforms SOTA SSL models such as MMS (Pratap et al., 2023), w2v-BERT 2.0 v2 (Barrault et al., 2023b), and WavLM on tasks such as ASR, ST, and speech resynthesis."}, {"title": "2 Motivation and Related Work", "content": "2.1 Speech Representation Learning\nSSL has seen tremendous success in speech processing by having neural networks learn rich feature representations from large-scale unlabeled data (Baevski et al., 2020; Hsu et al., 2021; Chen et al., 2022), which can then be fine-tuned on various downstream tasks (Chen et al., 2023c; Zhang et al., 2023). Multilingual SSL is a natural extension of this technique (Conneau et al., 2021; Babu et al., 2022) and facilitates cross-lingual transfer learning at scale. However, almost no studies leverage this capability to scale multilingual SSL models to truly massively multilingual settings, with the exception of Meta's MMS capable of covering ~1,000+ languages (Pratap et al., 2023). However, MMS relies upon the older wav2vec 2.0 SSL objective, which has now been consistently outperformed by newer SSL objective (wen Yang et al., 2021; Hsu et al., 2021; Chiu et al., 2022). In our work, we scale to 4 times the language coverage of MMS while further boosting performance with more powerful model architectures (Kim et al., 2023) and training objectives (Chen et al., 2022).\n2.2 Robust Speech Representations\nAs most SSL speech encoders are trained solely on clean speech (Baevski et al., 2020; Hsu et al., 2021; Chung et al., 2021), they perform noticeably worse on noisy audio (Chang et al., 2021). A common approach to alleviate this issue is to perform continued pre-training of the SSL model in noisy conditions (Chang and Glass, 2023; Ng et al., 2023; Huang et al., 2023; Zhu et al., 2023). While computationally efficient, the performance of these methods is ultimately limited by the underlying SSL model. WavLM (Chen et al., 2022) solves this issue by introducing an implicit denoising task during SSL pre-training, where the model must predict clean phonetic pseudo-labels when given an utterance corrupted with acoustic noise. WavLabLM (Chen et al., 2023b) extends this approach to the multilingual setting. Unlike XEUS, however, neither model considers the impact of reverberation, and both are trained on much smaller corpora (40K-86K hours vs. 1+ million hours).\n2.3 Open Foundation Models\nState-of-the-art speech foundation models vary significantly in their degree of openness (Table 1). The best performing models like Whisper (Radford et al., 2023), Google USM (Zhang et al., 2023), w2v-BERT 2.0 v1 (Barrault et al., 2023a), and w2v-BERT 2.0 v2 (Barrault et al., 2023b) are all trained on fully closed data. Whisper and w2v-BERT 2.0 v1/v2 only report pre-training data quantity and the languages covered. The USM report includes much more information about their data sources, but the model checkpoints remain unreleased.\nSmaller scale multilingual speech models follow more open release practices. XLSR 53 (Conneau et al., 2021) and XLS-R 128 (Babu et al., 2022) came with checkpoints and only use publicly accessible datasets but did not release training code. Similarly, MMS (Pratap et al., 2023) released checkpoints but did not release their training code and crawled data. WavLabLM (Chen et al., 2023b) and MR-HuBERT (Shi et al., 2024) released code and checkpoints but operated on a smaller scale.\nSoftware infrastructure remains a critical barrier to democratizing speech SSL research. While there is plenty of infrastructure for large-scale training of text-based models (Workshop et al., 2022; Andonian et al., 2023; Liu et al., 2023; Groeneveld et al., 2024), no similar work has achieved speech pre-training at our scale. AR-HuBERT (Chen et al., 2023a) and the OWSM project (Peng et al., 2023b, 2024b,a) sought to reproduce SOTA speech models in an open manner but use more than 80% less data than our work. With XEUS, we release the entire pre-training framework. We only use publicly accessible datasets and release all of the additional pre-training data that we crawled. To facilitate research in the training dynamics of large-scale SSL models, we also release all model logs and are the first to also release all intermediate checkpoints. As we are the first to create an open SSL speech model at such data and model scale, we also release all of our heavily optimized training code."}, {"title": "3 Data", "content": "3.1 Existing Datasets\nWe begin by combining a large variety of pre-training data from 37 publicly accessible\u00b9 speech processing datasets across 150+ languages, which\nWe follow Peng et al. (2023b)and include licensed data such as BABEL (IARPA) as part of this definition, as exact copies can be obtained, unlike that of closed/unreleased data."}, {"title": "3.2 MMS-unlab v2", "content": "We first reproduce the MMS-unlab dataset (Pratap et al., 2023), which was not publicly released, and scale it to 200 more languages. Like the original, we crawl religious audiobooks from the Global Recordings Network.2 Our data, however, is processed in a different manner. Since we use it for SSL instead of language identification, we do not filter out languages with low amounts of data. We also perform VAD with an energy-based detector3 instead of a neural model, the latter of which is more computationally expensive and likely less robust to unseen languages. This leads to a total of 6,700 hours of data across 4,023 ISO3 languages, which is wider in coverage than the original with 3,809 languages. We obtain explicit written permission for the use and redistribution of this data, as the Global Recordings Network website did not include clear licensing information."}, {"title": "3.3 WikiTongues", "content": "We also create new unlabeled speech corpora by crawling data from 2 data sources previously unseen in the speech processing literature. The first\n2https://globalrecordings.net/en/us\n3https://github.com/wiseman/py-webrtcvad"}, {"title": "3.4 Jesus Dramas", "content": "The second corpus we collect is Jesus Dramas. The source of this data is Inspirational Films5, which released the \"Story of Jesus\" audio drama in 430 languages under a non-commercial license. Each multi-speaker audio drama is 90 minutes long, totalling 645 hours. We use the same VAD settings as in Section 3.2 to segment these dramas into utterance-level clips."}, {"title": "3.5 Final Pre-Training Corpus", "content": "The new datasets we collect from Sections 3.2, 3.3 and 3.4 total 7,413 hours of data across 4,057 ISO3 languages. After aggregating it with the data from Section 3.1, we obtain a total of 1.081 million hours of pre-training data. We filter out all utterances longer than 40 seconds due to memory constraints. An overview of all of our pre-training corpora with their licensing information is presented in Table 11 in the Appendix. Figure 1 shows an overview of the language distribution in our data on a log-scale. Overall, our pre-training dataset spans 189\nhttps://wikitongues.org\nhttps://www.inspirationalfilms.com"}, {"title": "4 Self-Supervised Pre-Training", "content": "XEUS' training, sketched in Figure 2, combines ideas from HuBERT's masked prediction, WavLM's denoising objective, and a new dereverberation objective. These components are described in the following sub-sections."}, {"title": "4.1 Masked Prediction and Denoising", "content": "To obtain the target phonetic pseudo-labels for HuBERT masked prediction, we first extract encoded representations from a pre-trained WavLabLM MS model (Chen et al., 2023b). The representations are then clustered using k-means, with k = 2,048."}, {"title": "Algorithm 1 Simulation of Reverberant Speech", "content": "Require: a batch of utterances B, a set of RIRS D, and reverberation probability pr.\nfor u \u2208 B do\nSample v from cont. dist. U(0, 1)\nif v < pr then\nSample a random RIR Un from D\ndt = min(argmax(un))\nr = u \u2217 Un\nRealign r to u using dt\nRescale r to have the same energy as u\nend if\nend for\nreturn B\nThe data used for the feature extraction and clustering is a subset of our training data. Specifically, we sample 6,000 hours from a combination of Common Voice, MLS, and Googlei18n, 6,000 hours from YODAS, and 6,000 hours from MMS-unlab v2, and combine it with the entirety of FLEURS and BABEL's training data. This leads to a total of 20K hours used for k-means.\nWe also integrate the acoustic denoising task proposed by WavLM into XEUS pre-training. During training, an input utterance has some probability p to be augmented with either random noise from the Deep Noise Suppression Challenge (Reddy et al., 2021) or another utterance in the batch as interference. As the target labels are obtained solely from uncorrupted speech, the model learns to implicitly clean the input audio."}, {"title": "4.2 Dereverberation", "content": "We extend the concept of acoustic denoising introduced by WavLM by proposing another speech enhancement task for SSL pre-training: dereverberation. Similarly to the WavLM dynamic mixing augmentation, we simulate reverberant conditions in the input audio during training while the target pseudo-labels are again left untouched. The model must thus implicitly learn to remove the reverberation from the audio to predict the clean pseudo-labels. We note that it is possible for both the noise and reverberation augmentation to be applied for a single utterance. For simplicity, the noise augmentation is always applied first.\nOur technique (Algorithm 1) consists of the following steps. First, each utterance in a mini-batch has a probability pr for the reverberation augmentation to be applied. If an utterance u is to be augmented, we then randomly sample a Room Impulse Response (RIR) Un to be used from the audio of Ko et al. (2017). We first estimate dt, the sample shift imposed on u after adding the reverberation, according to the highest peak in Un. The reverberant utterance r can then be simulated via convolution (\u2217) between u and Un. Finally, we realign r with u using dt and normalize it to have the same energy as u. This final realignment step is crucial for the effectiveness of this technique; otherwise the audio would be shifted and misaligned with the target pseudo-labels. Quantitative analyses of our method can be found in Appendix Section A.3."}, {"title": "4.3 Model Architecture", "content": "XEUS is based on the HuBERT architecture (Hsu et al., 2021), with several modifications. We replace the Transformer (Vaswani et al., 2017) with an E-Branchformer (Peng et al., 2022; Kim et al., 2023), as convolution-augmented models achieve superior SSL performance (Chung et al., 2021). We choose the E-Branchformer over the Conformer (Gulati et al., 2020) due to the former's relative ease of training and superior downstream performance (Peng et al., 2023a). We also replace the original HuBERT loss with cross entropy, which is faster and leads to better downstream performance (Yang et al., 2023). XEUS consists of a convolutional feature extractor and 19 E-Branchformer layers. Each of the latter has 8 attention heads, a hidden dimension of 1,024, feed-forward size of 4,096, and kernel size of 31. The model size is 577M parameters. Ablations on these modifications can be found in Appendix Section A.2."}, {"title": "4.4 Pre-Training Settings", "content": "XEUS is pre-trained on 64 40GB NVIDIA A100 GPUs using the ESPnet toolkit (Watanabe et al., 2018). Each GPU has a maximum batch size of 100 seconds, leading to a total batch size of 106 minutes. We use a noise augmentation probability p of 0.2, where there is an equal probability of the corruption being random noise or another utterance (Section 4.1). We use a dereverberation augmentation probability pr of 0.3 (Section 4.2). We perform a two passes through the training set, totalling 670K steps. More details and a breakdown of the training costs can be found in Appendix Section A.4.\nThe scale of XEUS' pre-training is unseen outside of a few other works (Radford et al., 2023; Zhang et al., 2023; Barrault et al., 2023b), with the amount of pre-training data being 5-25 times the size of those used in prior models trained with public toolkits (Peng et al., 2023b; Chen et al., 2023a; Hsu et al., 2021). To conduct training on such a scale, we made several optimizations to the open-source ESPnet toolkit, which was originally designed for standard academic-scale experiments. These optimizations will all be made publicly available. More details about these improvements are also reported in Appendix Section A.5."}, {"title": "5 Downstream Evaluation", "content": "We examine the capabilities of XEUS in various downstream applications. Section 5.1 evaluates the multilingual capabilities of XEUS in different settings. Section 5.2 evaluates the universality of XEUS' representations for a broad range of speech information, such as emotion and speaker content. Finally, Section 5.3 tests the acoustic representations of XEUS via speech resynthesis. We provide an overview of each downstream setup in its respective subsection. Detailed experimental settings can be found in Appendix Section A.6."}, {"title": "5.1 Multilingual Speech Processing", "content": "We primarily compare XEUS with 3 SOTA multilingual SSL models: XLS-R 128 (Babu et al., 2022), MMS (Pratap et al., 2023), and w2v-BERT 2.0 v2 (Barrault et al., 2023b) (Table 1). We use the ML-SUPERB benchmark (Shi et al., 2023a) as our main evaluation, as it tests each models across a diverse range of tasks and languages. We complement these experiments with additional analyses on FLEURS ASR+LID and low-resource ST.\n5.1.1 ML-SUPERB\nML-SUPERB benchmarks self-supervised speech representations on a variety of multilingual tasks across 143 languages. ASR performance is evaluated in terms of character error rate (CER \u2193), while accuracy (ACC \u2191) is used to evaluate LID. The benchmark is split across two data settings for each task: 10 minutes (min.) and 1 hour of data for each language. Each data setting contains 4 tasks: monolingual ASR in 9 languages, multilingual ASR, LID, and joint multilingual ASR+LID. In the multilingual tasks, 5 languages are reserved as a few-shot task, while the other 138 languages have the standard 10 min. / 1 hour of fine-tuning data. An overall SUPERB\u300f(\u2191) score for each model in each data setting is calculated following the benchmark rules (Shi et al., 2023a). Further details can be found in Appendix Section A.6.1.\nTable 3 shows that XEUS is the overall best performing model, with the highest SUPERB, score of 956 on both the 10 min. / 1 hour settings. XEUS achieves SOTA results on monolingual ASR with the best scores of 25.1 and 33.3 CER on the 1 hour and 10 min. tracks respectively. On multilingual ASR, XEUS is only outperformed by MMS 1B. For ASR+LID, XEUS achieves the best performance in the normal setting for both data tracks. While XEUS is worse than MMS in few-shot CER, it still achieves reasonable results and outperforms the other SSL models. Overall, XEUS outperforms the parameter-equivalent w2v-BERT 2.0 v2 across all task categories. This is accomplished using only accessible training data, which is 22% the size than that of w2v-BERT 2.0 v2.\n5.1.2 FLEURS\nFLEURS is a 102-language multilingual ASR benchmark, where each language has around 6-10 hours of training data. In this setting, we use heavier downstream probes that reflect SOTA ASR architectures. We adopt the same setup as (Peng et al., 2023a; Chen et al., 2023c), which remains the SOTA on FLEURS when not using additional labeled data. The downstream model consists of an E-Branchformer encoder paired with a Transformer decoder, totalling 100M parameters. Exact settings are shown in Appendix Section A.6.2.\nThe results of the FLEURS experiments are shown in the middle section of Table 4. We find that XEUS remains competitive with the SOTA w2v-BERT 2.0 v2 trained on much more data (8.9 vs 8.7 CER), and significantly outperforms both XLS-R and MMS 1B (9.6 and 9.2 CER respectively).\n5.1.3 Low-Resource Language Coverage\nWhile FLEURS and ML-SUPERB provide comprehensive multilingual benchmarks, their language coverage is far smaller than that of XEUS (102-143 vs 4,057). To understand if XEUS' wide language coverage was effective for languages with small (< 10 hours) amounts of pre-training data, we crawled additional labeled data for evaluation. We randomly chose 3 languages from the Jesus Film"}, {"title": "5.2 Task Universality", "content": "To test how well XEUS encodes other forms of speech content, we benchmark its capabilities on the English-only SUPERB (wen Yang et al., 2021). SUPERB tests self-supervised speech models across 5 broad task categories: Recognition (ASR and Phoneme Recognition (PR)), Detection (Keyword Spotting (KS) and Query By Example (QbE)), Semantics (Intent Classification (IC) and Slot Filling (SF)), Speaker (Speaker Identification (SID), Automatic Speaker Verification (ASV), and Speaker Diarization (SD)), and Paralinguistics (Emotion Recognition (ER)). Metrics for each task are: phoneme error rate (PR), WER (ASR), maximum term weighted value (QbE), F1 and concept error rate (SF), equal error rate (ASV), diarization error rate (SD), and accuracy (KS, IC, SID, and ER). Exact experimental settings are shown in Appendix Section A.6.5. We compare XEUS to WavLM (Chen et al., 2022), the SOTA model\nhttps://www.jesusfilm.org on the SUPERB leaderboard for almost all tasks. Our results in Table 5 show that XEUS consistently reaches if not surpasses SOTA scores across a variety of tasks, obtaining the highest score in 4 English-only tasks (KS, SD, ER, ASR), despite its curse of multilinguality (Conneau et al., 2020)."}, {"title": "5.3 Acoustic Representation Evaluation", "content": "We evaluate XEUS on its acoustic representation quality through the task of speech resynthesis. Here, we compare primarily against w2v-BERT 2.0 v2 as the SOTA multilingual model and WavLM Large as the SOTA English-only model. We train unit-to-speech HiFiGAN vocoders (Kong et al., 2020; Polyak et al., 2021) on the accented-English VCTK (Yamagishi et al., 2019) dataset with a discrete codebook vocabulary size of 100. We evaluate the quality of the resynthesized speech in terms of Mel-Cepstral Distortion (MCD), log-F0 Root Mean Square Error (F0), predicted Mean Opinion Score (Lo et al., 2019) (MOSNet), and Word Error Rate (WER). We obtain WER by transcribing the synthesized speech with a pre-trained Whisper medium (Radford et al., 2023). For each SSL model, we experiment with features extracted at 50% and 75% of the model depth (ie. layer 18 out of 24 in the latter case), and report the best performing configuration in Table 6. More details about this search can be found in Appendix Section A.6. Our results show that resynthesized speech from XEUS is higher quality than that from both WavLM and w2v-BERT 2.0 v2 across all metrics, whether it be perceptual or semantic, showcasing its strong performance in generative tasks along with its SOTA recognition capabilities."}, {"title": "6 Conclusion", "content": "This work presents XEUS, an SSL speech encoder trained on over 1 million hours of data across 4,057 languages. As a community contribution, we release a new dataset with 7,413 hours of unlabeled speech data across those 4,057 languages. We also introduce a novel joint dereverberation task for SSL pre-training, which we use to increase the robustness of XEUS. We show that XEUS can achieve comparable performance if not outperform other SOTA SSL models on various benchmarks, while having much stronger performance on long-tail languages. To make XEUS reproducible, we will release all training code and configurations, along with model weights. In the future, we hope to extend the downstream use of XEUS to a larger scale."}, {"title": "7 Limitations:", "content": "While the overall pre-training corpus of XEUS contains over 4,000 languages, many of these languages have less than 1 hour of speech data. While we are able to show that the presence of this small amount of data is still beneficial for these languages in downstream tasks (Section 5.1.3), the performance is still likely much worse than the performance on higher-resourced languages. Furthermore, due to the efforts required to collect and manually clean evaluation data, we only test on a subset of these truly low-resource languages in Section 5.1.3. While XEUS is one step towards speech recognition or translation systems for these tail languages, much work is still required before these tools can be deployed to end users.\nDue to the large number of tasks and domains that our evaluation covers, we mostly focus on relatively lightweight benchmarks such as the SUPERB suite and perform limited hyperparameter tuning. While this allows for fair comparisons between different models, the evaluation does not use the large-scale fine-tuning common in SOTA settings for downstream tasks."}, {"title": "8 Broader Impact and Ethics:", "content": "Broader Impact: In this work, we introduce XEUS, a new large-scale multilingual SSL speech encoder. Unlike previous foundation models that focus on a single aspect, XEUS obtains SOTA performance across a diverse range of both tasks and languages, further pushing towards the goal of truly universal models. By releasing both our data and model checkpoints, our goal is to provide foundations for more multilingual research, particularly in domains such as robust ASR and speech enhancement where evaluation is typically done solely on English.\nAnother major goal of our work is to democratize the development of speech foundation models. We believe that training infrastructure remains a significant barrier to entry for SSL research. This has two main aspects: software infrastructure and training configurations. Current speech toolkits such as ESPnet (Watanabe et al., 2018) and Speech-Brain (Ravanelli et al., 2021) focus on academic scale experiments, while general frameworks such as HuggingFace and Fairseq (Wang et al., 2020) are more limited in their implementation of different tasks and SOTA methods. By integrating our changes into ESPnet, our optimizations can allow users to scale speech models for other tasks such speaker representation learning. In the latter aspect, we note that the availability of training recipes and configurations pose the other major barrier to entry. Due to the computational cost of training, the development of foundation models poses a risk that is too high for most academic labs, as a single failed training run can be disastrous for the lab's budget. However, this can be mitigated by publishing training recipes and hyperparameters known to work well. The benefits of this is most visible in the OWSM project (Peng et al., 2023b, 2024b,a), where each successive work reported lower and lower computational expenses.\nEthics: We recognize the delicate nature of speech data, particularly when it involves the languages of indigenous and marginalized communities. Many authors of this work have long-term collaborations with indigenous communities and researchers. We are careful to crawl and release data only from sources that contain permissive licenses of the source data to avoid potential cases of misuse and violations of language ownership. For data sources that do not clearly indicate their usage/distribution terms, we obtained explicit permission from the corresponding stakeholders (such as in the case of the Global Recordings Network in Section 3.2). To follow the data's access conditions, we release all of our data under non-commercial licenses.\nWe partially anonymize our crawled datasets by removing speaker names from the meta-data. However, we do not alter the content of the speech itself. As such, we urge users of our released data to respect the privacy of the speakers and not attempt to identify them. It is also possible that portions of the speech content may be offensive in particular settings. With the diversity of over 4000 languages, it is likely that there are statements or views that are normative in one culture but offensive in another.\nWhile encoder-only speech models like XEUS have limited uses without any task-specific fine-tuning, the downstream models that are created after such processes are prone to the biases and misuse cases that all machine learning models are vulnerable to. For example, XEUS' capabilities in speech generation can be used for misinformation via audio deepfakes, which is an unintended use case of this model."}, {"title": "A Appendix", "content": "A.1 Data\nFor brevity, we provide an overview of these datasets in Table 11 and refer readers to the original papers for more details (Bu et al., 2017; Chen et al., 2021; Panayotov et al., 2015; O'Neill et al., 2021; Hernandez et al., 2018; Pratap et al.; Zhang et al., 2022; Carletta, 2007; Ardila et al., 2020; Godfrey et al., 1992; Conneau et al., 2022; Bang et al., 2020; Yang et al., 2022; Wang et al., 2021; Chen et al., 2023b; Li et al., 2023; Kahn et al., 2020; Galvez et al., 2021; Valk and Alum\u00e4e, 202"}]}