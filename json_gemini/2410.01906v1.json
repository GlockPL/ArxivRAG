{"title": "Social Media Authentication and Combating Deepfakes using Semi-fragile Invisible Image Watermarking", "authors": ["AAKASH VARMA NADIMPALLI", "AJITA RATTANI"], "abstract": "With the significant advances in deep generative models for image and video synthesis, Deepfakes and manipulated media have raised severe societal concerns. Conventional machine learning classifiers for deepfake detection often fail to cope with evolving deepfake generation technology and are susceptible to adversarial attacks. Alternatively, invisible image watermarking is being researched as a proactive defense technique that allows media authentication by verifying an invisible secret message embedded in the image pixels. A handful of invisible image watermarking techniques introduced for media authentication have proven vulnerable to basic image processing operations and watermark removal attacks. In response, we have proposed a semi-fragile image watermarking technique that embeds an invisible secret message into real images for media authentication. Our proposed watermarking framework is designed to be fragile to facial manipulations or tampering while being robust to benign image-processing operations and watermark removal attacks. This is facilitated through a unique architecture of our proposed technique consisting of critic and adversarial networks that enforce high image quality and resiliency to watermark removal efforts, respectively, along with the backbone encoder-decoder and the discriminator networks. This allows images shared over the Internet to retain the verifiable watermark as long as facial manipulations or any other Deepfake modification technique is not applied. Thorough experimental investigations on SOTA facial Deepfake datasets demonstrate that our proposed model can embed a 64-bit secret as an imperceptible image watermark that can be recovered with a high-bit recovery accuracy when benign image processing operations are applied while being non-recoverable when unseen Deepfake manipulations are applied. In addition, our proposed watermarking technique demonstrates high resilience to several white-box and black-box watermark removal attacks. Thus, obtaining state-of-the-art performance.", "sections": [{"title": "1 INTRODUCTION", "content": "Media authentication refers to the process of verifying the authenticity and integrity of digital media such as images, videos, audio recordings, or text documents [18]. With the advancement in generative models, combined with the widespread availability of vast datasets, there is a rise in digital manipulation tools and techniques that have enabled the creation of high-quality and convincing AI-generated synthetic media (such as face, audio, and text) known as Deepfakes [42, 59, 63]. Apart from many creative and artistic uses of deepfakes [11], many harmful uses range from non-consensual pornography to disinformation campaigns intended to sow civil unrest and disrupt democratic elections. Deepfakes have been flagged as a top AI threat to society [30, 42].\nIn this context, several deepfake generation techniques based on facial manipulation (forgery) have been proposed [42, 63]. These facial manipulations or forgery techniques depict human subjects with altered identities (identity swap), attributes, or malicious actions and expressions (face reenactment) in a given image or video. Specifically, identity or face swapping is the task of transferring a face from the source to the target image [43]. Attribute manipulation [13, 24] is a fine-grained facial manipulation obtained by modifying simple attributes (e.g., hair color, skin tone, and gender). Similarly to identity swap, face reenactment [43] involves a facial expression swap between source and target facial images. These facial manipulation tools are easily abused by malicious users, with little to no technical knowledge, to manipulate facial images of the user, resulting in a threat to privacy, reputation, and security. In fact, several smartphone-based applications have such attribute modifications in the form of filters. For example, FaceApp\u00b9, a popular smartphone application, modifies an uploaded image based on the selected attribute that can be edited using a slider to regulate the magnitude of the change. The entire process of facial modification can be easily accomplished within five minutes using these applications or other pretrained models available in the online repositories.\nConsequently, every year the volume of facial Deepfakes on social media has witnessed a significant rise. For instance, in 2023 alone, about 500, 000 deepfake videos were added to social media, marking a substantial rise from previous years. In 2021, there were approximately 14, 678 deepfake videos online, which itself was double the number from 2018 [65].\nWith this staggering growth of facial manipulation-based deepfake content in social media, it has become increasingly important to ensure the media's authenticity against malicious tampering. The classical forensic approach for media authentication against facial manipulation includes running an automated deepfake detector [16, 31, 38]. Common Deepfake detectors include pre-trained machine learning-based binary baselines that aim to distinguish between real and deepfake data based on visual artifacts, blending boundaries, attention module, and motion analysis [14, 21, 23, 30, 55, 67]. These passive Deepfake detection techniques, an ex-post forensics countermeasure, and are still in their early stage [61, 62] as these techniques suffer from poor detection accuracy [14, 45], cross-dataset generalizability [38, 67], obtain differential performance across demographic attributes such as gender and race [37, 60], and are vulnerable to adversarial attacks [37, 60]. Further, they fail to cope with ever-evolving deepfake generation techniques.\nAlternatively, watermarking is being actively researched as a proactive defense technique because it involves embedding invisible markers or signatures into authentic media content such as images or videos. These markers are unique to the creator and can help in identifying the authenticity of the content [25, 56, 70] by matching the watermark message retrieved from the media to the original embedded watermarked message. Thus, by watermarking content, before it gets shared or distributed, creators can take preventive measures against malicious use or alteration of their work (as indicated by alteration to the watermark retrieved from the media). Hence, promoting and regulating the responsible and ethical use of AI, an important initiative of government leaders and the legislature [68]. Invisible watermarks are preferred because they preserve the image quality and it is less likely for a layperson to tamper with it. Traditional image watermarking [6] techniques typically transform domain coefficients of an image, using various transforms such as the Discrete Cosine Transform (DCT) and Discrete Fourier Transform (DFT) for watermark embedding. Deep-learning-based techniques such as StegaStamp [56], and HiDDeN [70] have emerged as an efficient solution over traditional image watermarking in terms of an end-to-end solution for efficient message embedding [4, 22, 35, 56, 70]. However, these aforementioned watermarking techniques are also either fragile (i.e., watermark message is altered) to basic image processing operations such as compression and color adjustments [25, 70] or overly robust to malicious transform which deters manipulated\n\u00b9https://www.faceapp.com/"}, {"title": "2 RELATED WORK", "content": "2.1 Facial Manipulation Generation and Passive Deepfake Detection\nFacial manipulations are categorized primarily into three groups: identity swap [43], attribute manipulation [13, 24], and expression swap [57, 58]. Generative models such as auto encoders [5] (such as Faceswap), Generative Adversarial Networks (GANs) [19] (as FSGAN [43] and AttGAN [24]), and Diffusion Models (stable diffusion) [26] are used to create highly realistic fake content, including non-existent faces or altering existing ones [47]. Among all, GAN is the most commonly adopted model for the generation of facial manipulation because it excels at generating high-quality realistic images that mimic the distribution of the original dataset.\nThe majority of methods currently used for DeepFake detection are based on Convolutional Neural Networks (CNNs) (such as VGG16, ResNet50, ResNet101, ResNet152, and Xception) based binary baselines [32, 37\u201339, 59]. Other approaches include Long-Short-term Memory (LSTM) networks [12] to analyze spatio-temporal data, using facial and behavioral biometrics [1, 17, 49], examining inconsistencies in mouth movements [21], multi-attentional [67] models that focus on different parts of the image, and F\u00b3 -Net [48], which detects subtle manipulative patterns by analyzing frequency aspects of images. Additionally, an ensemble model [45] that combines two ConvNext models trained at varying epochs and a Swin transformer has recently been proposed for enhanced deepfake detection.\n2.2 Image Watermarking for Media Authentication\nThe existing digital watermarking techniques are utilized to embed three types of watermarks: fragile [8, 36], robust [9, 15, 46, 70], and semi-fragile [25, 33, 41, 54, 69]. Fragile watermarks are particularly sensitive, designed to invalidate the authentication of an image at the slightest modification, ensuring stringent authenticity checks. In contrast, robust watermarks are crafted to endure various forms of manipulation, thus allowing content creators to assert ownership over their media, even when it undergoes alterations. Semi-fragile watermarks combine features of both fragile and robust watermarks i.e., fragile to manipulations and robust to genuine transformations. Traditional embedding techniques for semi-fragile watermarks have manipulated both the spatial, such as least significant bits [64], and frequency domains (such as DCT [25] and DWT [7, 29]) of digital media. However, these conventional approaches can either make watermarks perceptible, distort the media, or render them susceptible to image transformations, particularly JPEG compression. Thus, rendering them inefficient for media authentication against tampering and alteration.\nDeep-learning-based watermarking techniques offer more efficient watermark encoding with high imper-ceptibility compared to traditional techniques. A robust watermarking technique called HiDDeN is proposed in [71], consisting of an encoder, a decoder, and a discriminator. However, this technique introduces distortions in the media and is not suitable for identifying manipulated media. Similarly, a watermarking technique called StegaStamp [56], that encodes hyperlinks into image pixels, using a trained neural network, imperceptible to human eyes. However, this model lacks vulnerability against malicious transformations and therefore is unsuitable for media authentication. A study in [41] introduces a semi-fragile deep-learning-based invisible watermark into the image pixels (FaceSigns) utilizes a U-Net-based encoder-decoder architecture designed to be robust against benign image-processing operations yet fragile to any facial manipulation, for media authentication. However,"}, {"title": "3 PROPOSED METHODOLOGY", "content": "Figure 2 illustrates an overview of our proposed proactive defense technique based on U-Net-based encoder-decoder architecture for invisible image watermarking. The five primary components of our proposed system are an encoder network Ea", "follows": "n$LRE(x) = L_1(b, b_{bt}) - L_1(b, b_{mt})$"}]}