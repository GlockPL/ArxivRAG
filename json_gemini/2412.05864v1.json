{"title": "CardOOD: Robust Query-driven Cardinality Estimation under Out-of-Distribution", "authors": ["Rui Li", "Jeffrey Xu Yu", "Kangfei Zhao", "Guoren Wang"], "abstract": "Query-driven learned estimators are accurate, flexible, and lightweight alternatives to traditional estimators in query optimization. However, existing query-driven approaches struggle with the Out-of-distribution (OOD) problem, where the test workload distribution differs from the training workload, leading to performance degradation. In this paper, we present CardOOD, a general learning framework designed to construct robust query-driven cardinality estimators that are resilient against the OOD problem. Our framework focuses on offline training algorithms that develop one-off models from a static workload, suitable for model initialization and periodic retraining. In CardOOD, we extend classical transfer/robust learning techniques to train query-driven cardinality estimators, and the algorithms fall into three categories: representation learning, data manipulation, and new learning strategies. As these learning techniques are originally evaluated in computer vision tasks, we also propose a new learning algorithm that exploits the property of cardinality estimation. This algorithm, lying in the category of new learning strategy, models the partial order constraint of cardinalities by a self-supervised learning task. Comprehensive experimental studies demonstrate the efficacy of the algorithms of CardOOD in mitigating the OOD problem to varying extents. We further integrate CardOOD into PostgreSQL, showcasing its practical utility in query optimization.", "sections": [{"title": "1 Introduction", "content": "Cardinality estimation is a fundamental task in query optimization. In the early years, RDBMSs adopted statistical methods [10, 32, 33] to estimate the cardinalities of SQL queries, where impractical statistical assumptions, i.e., uniform and independence of attributes, lead to inaccurate estimation results and suboptimal query plans [15]. Recent studies employ learning approaches for cardinality estimation, which are shifting from the Machine Learning (ML) approaches [7, 17, 28] to Deep Learning (DL) approaches [13, 20, 26, 27, 37, 40, 48, 52]. The shifting is driven by the powerful approximation capability of neural networks in end-to-end applications and high-end DL frameworks. All the DL approaches can improve the estimation accuracy as well as the quality of generated query plans.\nExisting DL-based cardinality estimation approaches are broadly categorized into data-driven and query-driven approaches. Data-driven approaches learn the joint probability distribution of all attributes by deep density models, i.e., deep autoregressive neural networks [13, 40, 48], Sum-Product Network [14], and cardinality estimation is implemented by local probability density estimation. It is difficult to deploy these learned estimators in DBMSs because of the huge training cost and inference inefficiency. Furthermore, these approaches typically do not support complex DB schema and queries such as cyclic joins or self-joins [30].\nAnother line of DL-based cardinality estimation is the query-driven approaches that build a regression model which maps features of a query to its corresponding cardinality in a training workload. In contrast to data-driven approaches, the training and inference of query-driven models are time and space-efficient due to low model complexity. And they are flexible to support complex SQL queries, by transforming the query into hidden vector representation by various neural network architectures. To the best of our knowledge, query-driven learned estimators are more practical to deploy in real DBMSs [30]. However, existing learned estimator are trained by optimizing the empirical error of the training workload under the assumption that the training and test workload are i.i.d. The largest challenge that query-driven approaches face is they are sensitive to workload shift, i.e., the distribution of test query is shifting from that of the training workload.\nRecent studies [24, 45] have proposed methods of statistical hypothesis testing to detect distribution shift of the workloads. Out-of-distribution (OOD) test queries are detected regarding the distribution shift of query features in the workload, lying in the literals in the filter predicates, the combinations of filter predicates/join conditions and the combinations of base tables, etc. For instance, the commonly used benchmark DSB [4] includes several hand-written query templates with different filtering predicates and join graphs. The test workload can be regarded as an OOD workload if it contains different query templates with the training workload. The OOD problem derives from the lack of a sufficiently large, diverse and balanced training query set, and the query join graphs and/or the predicates may change over time in a database. Since models trained by previous query logs are difficult to generalize well on the OOD test queries, these studies [24, 45] have developed incremental learning-based approaches to maintain the learned estimators, balancing the cost of model retraining and performance degeneration.\nIn this paper, our goal is to construct a general learning framework that can build robust query-driven cardinality estimators against the OOD problem. Distinguished from the online learning approaches that digest a dynamically incremental workload [24, 45], we concentrate on offline training algorithms that build one-off models from a stationary workload, which can be used for better model initialization and/or periodic retraining. In addition, these learning algorithms are orthogonal to the DL models for cardinality estimation. In other words, these learning algorithms can be applied to train any DL-based query-driven estimators.\nWe first extend typical transfer/robust learning techniques, beyond optimizing the empirical loss, to train cardinality estimators. These algorithms are categorized into three classes, i.e., representation learning, data manipulation, and new learning strategies. More specifically, representation learning approaches tend to align the hidden representations among sub-distributions in the training query workload. Data manipulation approaches utilize data augmentation to enlarge and enrich the training queries. The learning strategy-based approaches use new learning strategies, e.g., Distributionally Robust Optimization [34], self-supervised learning [16]. Although these learning techniques for OOD generalization have been evaluated in computer vision tasks, i.e., image classification, whether these techniques are also effective in building robust cardinality estimators remains unrevealed. The answer to this question is not straightforward. In image classification, the features of an image commonly drift in terms of color, style, and lightness etc., while the principal visual features of the object in an image are invariant to be exploited during learning. On the contrary, in cardinality estimation, query features drift anywhere, e.g., the predicates, the join condition, and the relations, which result in changes in the cardinality. Moreover, as a regression task, the prediction of query cardinality is more sensitive than the target image class, where any minor changes in query features may lead to changes in the cardinality of several magnitudes.\nTo further exploit the property of cardinality estimation, we propose a new learning algorithm based on self-supervised learning, in the category of learning strategy. In light of partial order embedding [29], the algorithm models the partial order of queries regarding their cardinality by optimizing a self-supervision learning objective. This partial order constraint, which naturally exists in SQL query workloads, promotes the query embeddings subject to the same partial order that implicitly copes OOD of query features. We develop an extensible learning framework called CardOOD that integrates all the learning algorithms we explore. In our experimental studies, these algorithms alleviate the OOD problem in cardinality estimation by different degrees. Finally, we deploy CardOOD in a real DBMS, PostgreSQL, for query optimization. The contributions of this paper are summarized as follows:\n\u2022 We extend classical transfer/robust learning algorithms to build robust DL-based cardinality estimators for SQL select-project-join (SPJ) queries. Here, some algorithms are originally domain adaption algorithms where features of the target domain are known while we assume the target test query workload is unknown.\n\u2022 We propose a novel self-supervised learning algorithm, called OrderEmb, which models the partial order of queries regarding the cardinality, for training robust cardinality estimators.\n\u2022 We build a DL framework, CardOOD, which integrates 6 robust learning algorithms and 2 types of DL models. We evaluate the characteristics of CardOOD in multiple facets.\n\u2022 We conduct a comprehensive experimental evaluation for CardOOD on two neural estimators for single relation range queries and multi-relation join queries.\n\u2022 We deploy CardOOD in PostgreSQL for query optimization. Test results on IMDB, DSB and JOB-light queries show that models constructed by CardOOD achieve up to 5.6%, 36.6% and 4.6% improvement in query execution time.\nRoadmap: The following of this paper is organized as follows. \u00a72 gives our problem statement. In \u00a73, we introduce 6 ML algorithms for establishing robust data-driven cardinality estimators, which fall into 3 categories. In the following, we elaborate on the implementation and characteristics of our CardOOD framework in \u00a74. \u00a75 reports the experimental results. We review the related works in \u00a76 and conclude the paper in \u00a77."}, {"title": "2 Problem Statement", "content": "A relational database consists of a set of relations, {R1, R2, RN}, where a relation Ri has di attributes such as Ri = (A1, Ad\u2081). Here, an attribute Aj is either a numerical attribute in a domain with a given range [minj, maxj] or a categorical attribute with a discrete finite domain {C1, C2,... Cm; }.\nWe study cardinality estimation for the select-project-join (SPJ) SQL queries with conjunctive conditions. A selection on an attribute is either a range filter (i.e., [lbj, ubj], denoting the condition lbj \u2264 Aj \u2264 ubj), if the attribute is numerical, or IN filter (i.e., Aj IN C, CC {C1, C2, \uff65\uff65\uff65 Cm; }, denoting the condition \u2203ck \u2208 C, Aj = ck), if the attribute is categorical. A projection can be on any attribute and a join we support is equi-join. The primary-foreign key join (PK/FK join), and foreign-foreign key join (FK/FK join), are treated as special equi-join with extra constraints.\nThe cardinality of an SQL query q is the number of resulting tuples, denoted as c(q). To learn a cardinality estimator, like existing works [20], we require a set of pre-defined joinable attribute pairs"}, {"title": "3 Methodology", "content": "We categorize the learning algorithms we explore into three classes: representation learning, data manipulation, and new learning strategies. We propose a novel learning algorithm by self-supervised learning, which is in the category of learning strategy.\n3.1 Representation Learning\nThe representation learning strategy aims to learn an invariant query embedding under distribution shift. We decompose the learned estimator ce as ce = go \u00b0 he, where he is the feature extraction function that transforms an initial query encoding into an intermediate vector embedding, and ge is the prediction function that predicts the cardinality from the intermediate embedding. The goal of the representation learning strategy can be formulated as Eq. (4), where Lreg denotes a regularization term and A is the trade-off parameter.\n$\\min_{h_{\\Theta}, g_{\\Theta}} \\mathbb{E}_{q, c(q)} [||g_{\\Theta} h_{\\Theta}(q) - \\log c(q) ||^2] + A L_{reg}$    (4)\nThe representation learning strategy is designed to better learn the feature extraction function he with corresponding Lreg."}, {"title": "4 CardOOD Framework", "content": "In this section, we elaborate on the CardOOD framework we establish, which integrates the 6 ML algorithms presented in \u00a73 as well as ERM training strategy. The framework contains 4 abstract modules, Query Encoder, Training Pipeline, Model and Algorithm, which are incorporated at a bottom-up level. All the 4 modules are extensible by implementing plug-and-play instances. Query Encoder is the module to transform raw SQL query statements into vectorized representations, which depends on the model type. Here, we implement a fixed-length encoder and set encoder for constructing the input vector representation for two representative query-driven models, Multi-layer Perceptron (MLP) and Multi-set Convolutional Network (MSCN), respectively.\nFixed-length Encoding: In existing work [6, 7], a SPJ query can be encoded by a fixed-length vector. The encoding consists of two parts: the selection conditions and the join conditions. The two parts are encoded separately and concatenated. The selection conditions are specified on numerical/categorical attributes. For a range filter, lbj \u2264 Aj \u2264 ubj, on a numerical attribute Aj, we normalize lb j and ubj to [0, 1] by mapping [lbj, ubj] to $\\left[\\frac{lb_j-min_j}{max_j-min_j}, \\frac{ub_j-min_j}{max_j-min_j} \\right]$, where [minj, maxj] is the domain of the attribute Aj. Thus, the representation is the interval of two real values. The equality predicates of categorical attributes are treated as range filters where lbj = ubj [44]. For an IN filter, Aj IN C, on a categorical attribute Aj, where C is a subset of the attribute Aj's domain, we adopt the lossless factorized bitmap encoding [47]. Regarding join conditions, we use an n-dim binary vector to represent equi-joins of total n joinable attribute pairs.\nSet Encoding: Following [20], we encode a SPJ query into 3 sets of vectors, i.e., a set of vectors of involved relations, selection conditions and join conditions, respectively. The 3 sets of vector are processed by set convolution, followed by average pooling layers. Subsequently, the output individual set representations are concatenated and fed into a final MLP."}, {"title": "5 Experimental Studies", "content": "In this section, we give the test settings in \u00a75.1, and report our comprehensive experiments in the following facets: 1 Test the prediction robustness of CardOOD (\u00a75.2) \u2461 Compare the training time of the algorithms in CardOOD (\u00a75.3) 3 Study the performance of"}, {"title": "6 Related Work", "content": "Classical Cardinality Estimators. Cardinality and selectivity estimation of relational queries have been studied in the database area for several decades. In the early approaches, multi-dimensional histograms are proposed to represent the joint probability distribution [10, 32, 33], where the assumption of attribute value independence is not necessary to be held. For join queries, various join"}, {"title": "7 Conclusion", "content": "DL-based query-driven cardinality estimators face the challenge of distribution shift, where queries to be predicted are OOD from the training query workload. In this paper, we build a framework CardOOD for training robust query-driven cardinality estimators. CardOOD integrates 6 learning algorithms over two DL models. Among these algorithms, 4 algorithms are the extensions of the transfer/robust learning techniques in ML areas. In addition, we propose a new algorithm that leverages the partial order of the cardinality among training queries to improve model generalization in OOD scenarios. Our extensive experiments on CardOOD show that the robust learned estimators achieve much lower 95% and 99% quantiles of the q-error than the estimators trained by plain ERM training paradigm. We also deploy CardOOD in a real DBMS, PostgreSQL, which improves the speed of the end-to-end query execution by up to 5.6%, 36.6% and 4.6% for IMDB, DSB and JOB-light queries, respectively."}]}