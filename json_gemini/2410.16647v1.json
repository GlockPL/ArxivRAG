{"title": "GE2E-KWS: GENERALIZED END-TO-END TRAINING AND EVALUATION FOR ZERO-SHOT KEYWORD SPOTTING", "authors": ["Pai Zhu", "Jacob W. Bartel", "Dhruuv Agarwal", "Kurt Partridge", "Hyun Jin Park", "Quan Wang"], "abstract": "We propose GE2E-KWS-a generalized end-to-end training and evaluation framework for customized keyword spotting. Specifically, enrollment utterances are separated and grouped by keywords from the training batch and their embedding centroids are compared to all other test utterance embeddings to compute the loss. This simulates runtime enrollment and verification stages, and improves convergence stability and training speed by optimizing matrix operations compared to SOTA triplet loss approaches. To benchmark different models reliably, we propose an evaluation process that mimics the production environment and compute metrics that directly measure keyword matching accuracy. Trained with GE2E loss, our 419KB quantized conformer model beats a 7.5GB ASR encoder by 23.6% relative AUC, and beats a same size triplet loss model by 60.7% AUC. Our KWS models are natively streamable with low memory footprints, and designed to continuously run on-device with no retraining needed for new keywords (zero-shot).", "sections": [{"title": "1. INTRODUCTION", "content": "Traditionally, the goal of keyword spotting(KWS) is to detect a relatively small set of predefined keywords such as device-waking phrases like \u201cHey Google\" or simple control commands like \u201cstop\u201d, \u201cturn on lights\", etc in a streaming audio environment. Research usually focuses on developing small-footprint models that can run continuously on device [1, 2, 3]. As devices become more intelligent and personalized, there is a growing demand from customers for the flexibility to specify their own triggering keyword via text or audio. A custom keyword spotting solution can provide a natural and seamless way for users to activate devices or software using voice.\nCustom keyword spotting is able to detect a custom target phrase from audio streams. While researchers have tried to use template matching approaches such as Dynamic time warping(DTW) [4] and phoneme classification approaches [5], they pose limitations on the keyword choices or require retraining for new keywords. A more efficient way to achieving open-vocabulary and zero-shot KWS is by 1) representing utterances by speech embeddings, and 2) comparing a speech embedding generated at enrollment time with a speech embedding generated at serving time. Detection of the keyword is triggered only when the cosine similarity of the two speech embeddings is higher than a chosen threshold. In both cases, embeddings are computed from a streaming audio window. However, we support the enrollment phrase specification either as audio or as text. In the case of text enrollment, we use Text To Speech (TTS) models [6, 7] to generate audio, which is then used to generate the embedding. Furthermore, if multiple enrollment embeddings are available, the centroid of the enrollment embeddings is used to compute the cosine distance.\nThere has been an increasing amount of research to optimize custom KWS by building high quality speech embeddings. The most straightforward way is using an Automatic Speech Recognition (ASR) encoder [8] to output a vector representation for an audio input window. The model usually has good accuracy for speech recognition and keyword matching, however the model size is usually too large to run on-device in a continuous manner.\nIn recent years, researchers have used transfer learning to extract speech embeddings from the hidden layers of models that are trained using classification tasks. In Lin et al. [9], a speech encoder with five convolution layers is connected to different and independent decoders that perform classification tasks. The encoder parameters are shared when training different decoders and the output of the shared encoder is used as speech embedding. Similarly, Berg et al. [10] explores self-attention based encoding and Ding et al. [11] built an encoder using a combination of convolution layers and a transformer architecture to perform classification tasks. Rybakov et al. [12] benchmarks the most popular architectures such as DNN, SVDF, LSTM, CRNN, MHAtt, etc. in the same setting. Although their models are device friendly, they are evaluated based on classification accuracy in the speech command dataset [13], which does not directly measure the quality of matching two separate utterances (i.e. test utterance and enrollment utterance). To evaluating keyword spotting quality in a setting closer to how users experience in the real world, we measure utterance matching accuracy directly by dividing the speech command dataset into enrollment dataset and testing dataset. In our experiments, the above classification model [9] does not perform well.\nTriplet loss gained popularity for efficiently solving face identification problems, e.g. FaceNet [14]. Since then, there has been more research on triplet loss in the speech domain. Bredin [15] and Song et al. [16] use triplet-loss based learning approaches for the speaker diarization. Zhang and Koshida [17] use triplet-loss for training speaker verification models. Recently, triplet loss has been applied to keyword spotting tasks. Sacchi et al. [18] and Chidhambararajan et al. [19] applied triplet loss on Open-Vocabulary Keyword Spotting, and built embeddings for texts and audio. However their sampling method is not efficient\u2014each batch only samples one positive and one negative pair with the anchor utterance. This slows down the training and increases convergence instability because the selection of a single anchor utterance introduces high variances. Furthermore, their experiments were based on only a two layer GRU architecture. We experiment with different sizes of LSTMs [20] and also apply state-of-the-art speech models including Conformer [21].\nThe main contributions of this paper include:\n\u2022 We apply generalized end-to-end loss from the domain of speaker verification [22] to custom KWS tasks, simulating the two-stage process of runtime enrollment and verification during the training. This allows the training process to simulate the end user journey in the real world application. Unlike triplet loss, which compares a single anchor utterance against one positive utterance and one negative utterance, we build one enrollment centroid per phrase(a.k.a. keyword) in the batch, and compare it with all positive and negative utterances. This allows us to optimize matrix operations when computing the loss for a training batch for more efficient training. Moreover, since the enrollment centroids are constructed from multiple utterances containing the same phrase, the reduced sampling variances improve convergence stability. Therefore we noticed a fast and stable convergence in our experiments.\n\u2022 Based on our literature review, to the best of our knowledge, we do not find a systematic and reliable way to evaluate custom KWS model performances. The triplet loss paper [18] uses fairly old WSJ dataset [23] collected in 1992, and Chidhambararajan's one-shot KWS [19] reports model results based on single threshold metrics from its TTS synthesized dataset and a small collected dataset with only 4 keywords. We design an end-to-end evaluation process that simulates the real world audio enrollment and custom keyword detections based on the commonly used speech command dataset [13] in clean and noisy conditions. We define a set of metrics to directly measure keyword matching quality. We detail the evaluation process in the paper so it can be used in other KWS research.\n\u2022 We apply and tune a Conformer [21] model for custom KWS, which beats the 7.5GB ASR encoder's AUC by 23.6% using a 419KB quantized model. Our quantization method in Sec. 3.3 allows a low memory footprint model efficiently running on device in a continuous manner, while preserving the high accuracy. Our models are natively streamable. They are trained and exported using framework mentioned in Sec. 4.1 with automatic streaming conversion features. Finally, we train and evaluate various sizes of LSTM and Conformer models, and show the trade-off between model quality and model size. This allows the flexibility to choose the most suitable model given device type and runtime constraints.\nThe rest of the paper is organized as follows: We discuss related work and use them for our benchmark experiments in Section 2. generalized end-to-end loss is applied to our different model architectures in Section 3. We design a new offline evaluation process simulating real world KWS environments in Section 4, and discuss experiment's results in Section 5. The paper concludes in Section 6."}, {"title": "2. MODEL BASELINES", "content": ""}, {"title": "2.1. ASR Encoder for Keyword Spotting", "content": "ASR systems are designed to transcribe speech into text. In the context of keyword spotting, researchers have applied ASR encoders to build utterance embeddings. In Prabhavalkar et al. [8], the context length of ASR encoder can be up to 2.56 seconds and thus capture enough of the audio sequence to build a representation without needing temporal pooling. Given that ASR encoders are trained with a large amount of data and large models, it works very well on keyword spotting tasks. We use a pre-trained 7.5GB ASR encoder from this paper to benchmark our models."}, {"title": "2.2. Speech Classification Encoder", "content": "Researchers have been using transfer learning to extract speech embeddings from hidden layers of models trained for other tasks such as classifications. In Lin et al. [9], a five layer convolution encoder is built and trained with 125 independent classifiers, sharing the encoder parameters during training. They use 200M short utterances (2s) mined from YouTube to train a 40-class model for each classifier. The output layer of the shared encoder is extracted as the speech embedding, published in site [24]. We use a pre-trained 1.4 MB speech classification encoder in our experiment baseline."}, {"title": "2.3. Triplet Loss Conformer", "content": "Triplet loss has been applied to optimize embedding representations for custom KWS from Sacchi et al. [18]. They construct the triplet loss by sampling an anchor utterance, and a"}, {"title": "3. PROPOSED METHOD", "content": ""}, {"title": "3.1. Generalized End-to-End (GE2E) Loss", "content": "We apply the generalized end-to-end (GE2E) solution from the domain of speaker verification [22], which splits all utterances in a batch into enrollment and test utterances. For each phrase, an enrollment centroid is built from all its enrollment utterances. This reduces training variance by building enrollment centroid from multiple utterances and by comparing it with all test utterance embeddings from the batch.\nFig. 1(a) gives an example of the process calculating GE2E loss. Formally, In a batch we have X different phrases $Pr_i$, i = 1, 2, .., X, each phrase has Y (Y is an even number) utterances from different speakers and speaking styles. Let $e_{ij}$ be the embedding for the ith phrase and the jth utterance. For each phrase $Pr_i$, we compute an embedding centroid $c_i$ using Y/2 utterances as enrollment while the remainder of the utterances for phrase $Pr_i$ are later used as test utterances. This yields X centroids, one for each phrase, with each centroid $c_i$ being defined as follows:\n$c_i = \\frac{1}{Y/2}\\sum_{j=1}^{Y} e_{ij}$\n                  $j (mod 2)\\neq0$                                             (1)\nEach centroid $c_i$ is paired with positive example embeddings ($p_i$) which consist of the held-out test utterances that include phrase $Pr_i$, and negative example embeddings ($n_i$) that consist of all held-out test utterances from other phrases. These can be formally defined as follows:\n$p_i = \\{e_{ij} \\mid j (mod 2) \\neq 1\\}$\n                                                (2)\n$n_i = \\{e_{kj} \\mid j (mod 2) \\neq 1, k = 1, 2, .., X and k\\neq i\\}$                                                              (3)\nWe compute the GE2E loss using cosine similarity scores such that we (a) maximize score between a centroid and its positive examples, and (b) minimize score between a centroid and its negative examples. This can be formalized as follows for a centroid $c_i$:\n$L(c_i) = log \\sum_{n\\in n_i} exp \\ cos(c_i, n) - log \\sum_{p\\in p_i} exp \\ cos(c_i, p)$\n                                                                                                      (4)\nGE2E has two advantages:\n\u2022 Increases convergence stability by reducing the sampling variance and building enrollment centroids from multiple utterances.\n\u2022 Increases computational efficiency by formatting embedding similarity computations of {enrollment, test utterance} pairs into matrix operations."}, {"title": "3.2. Model architectures", "content": ""}, {"title": "3.2.1. LSTM Model", "content": "LSTM [20] is a commonly used RNN to model sequential features like audio sequences. We used the standard three-layer LSTM architecture with different hidden layer and output layer dimensions to optimize the results and tradeoff model performance and model size."}, {"title": "3.2.2. Conformer Model", "content": "The Conformer model was proposed by Gulati et al. [21] to tackle ASR problems and has been the SOTA for ASR encoders. It combines the strengths of Transformers, which capture the global interactions of audio sequences, and strengths of Convolution Neural Networks (CNNs), which exploit local features effectively. In this paper, we used the standard model architectures from the above paper. We have tuned the number of Conformer blocks, the number of heads in Multi Head Self Attention (MHSA) and the output embedding sizes etc. to trade off between model performance and model sizes."}, {"title": "3.3. Quantization", "content": "We use the dynamic range quantization from TensorFlow Lite [25]. This dynamically quantizes weights and activations to 8-bits of precision based on the ranges of those weights and activations."}, {"title": "4. EXPERIMENT SETUP AND EVALUATION", "content": ""}, {"title": "4.1. Training Dataset and Infra", "content": "Our training data are processed from the MSWC open source dataset [26], which includes 38k different phrases and 5.3M training utterances from approximately 27k speakers. The audio sequences are processed into 40-dimensional spectral features with each frame length of 25ms. In a batch we randomly choose 8 phrases, and sample 10 utterances per phrase. For each phrase, we compute the GE2E loss detailed in Fig. 1(a) and Sec. 3.1. For training framework, our model developments are based on Tensorflow/Lingvo [27] with advantages of automatic streaming inference conversion."}, {"title": "4.2. Evaluation dataset and process", "content": "We use the test split of Speech command dataset [13], which has 11k utterances across 35 phrases from different speakers. Shown in Fig. 1(b), for a given phrase (e.g. \u201cup\u201d), we randomly select 10 utterances as an enrollment dataset and use the remaining utterances for testing purposes. The enrollment centroid is aggregated from 10 enrollment utterance embeddings and is used to compare with all testing utterance embeddings by cosine similarity. In this process, we aim to achieve the same goals as in real world serving environments:\nThe embedding similarity should be high if the enrollment centroid and test utterances are from the same phrase, and low otherwise. In addition, we apply 3-way MTR of 3db to 15db background noise to the utterances. Therefore our experiments are conducted in both clean and noisy conditions."}, {"title": "4.3. Evaluation metrics", "content": "As mentioned above we have 35 enrollment centroids corresponding to 35 phrases. Illustrated in Fig. 1(b), for each phrase, we compare its enrollment centroid against all testing utterance embeddings, and plot their cosine similarity scores into the histogram. Fig. 2 shows the histogram of different models for phrase \"up\". In each histogram, if a test utterance comes from the same phrase of enrollment utterances, we treat it as a true positive example and count its similarity score into red frequency bars. If the test utterance is from a different phrase, it is treated as true negative and the similarity score is counted into blue frequency bars. We normalize the frequency bars into probability distributions as there are many more negative pairs.\nA perfect model will have a threshold that can perfectly separate the scores from true positive examples and true negative examples. However in the overlapping areas, the model struggles to make the right decision. When applying different thresholds in the score histogram, from 0 to 1 with a stride of 0.01, we calculate FAR and FRR for each threshold and plot the Detection Error Tradeoff (DET) curves. Fig. 3 shows the DET curves for different models for the phrase \"up\".\nTo directly compare model performance using numeric metrics, we use the Area Under the DET Curve (AUC) and Equal Error Rate (EER) of the DET curves. In this context, EER is the value of FAR and FRR when they are equivalent. In both cases, lower AUC and lower EER indicate better model performance. Since each phrase has its own DET curve, the AUC and EER measure the model performance when matching a particular phrase. In order to have metrics that holistically evaluate a model independent of phrase, we compute aggregated AUC and EER by averaging metrics over different phrases. For the rest of the paper, unless otherwise noted, metrics such as AUC, EER and DET refer the aggregated version. In addition, AUC is a numeric value from 0 to 1. However for easier readings, AUCs are formatted with \"%\" in most places of the paper."}, {"title": "5. RESULTS", "content": ""}, {"title": "5.1. Results for Different Models", "content": "As mentioned in Sec. 2, we compare our GE2E conformer model with a pre-trained 7.5GB ASR encoder, a pre-trained 1.4MB speech classification encoder and a triplet loss based conformer model sharing the same model architecture, training data and similar training epochs. Our raw conformer model has size 2.8MB and is quantized into 419KB using techniques in Section.3.3. The evaluations are based on quantized models.\nThe above Fig. 2 and Fig. 3 show the score histograms and Detection Error Tradeoff (DET) curves for the phrase \"up\" across different models. Table.1 shows AUCs for few other individual phrases and also an all-words aggreated AUC defined in Section.4.3. To better visualize model performances, AUC values are plotted in Fig. 4 for all 35 words in the speech command dataset. The leftmost datapoints measure the overall model performance from \"All words AUC\". From the above table and plots, we can see the ASR model performs well with an AUC of 0.66%, but with a big size 7.5GB. The speech classification encoder has a device-friendly model size but it performs poorly with an AUC of 6.44%. The triplet loss conformer baseline with raw size 2.8MB and quantized model 419KB has 1.283% AUC. Noticeably, our GE2E loss model that has the same size as the triplet loss conformer performs the best here with an AUC of 0.504%, relatively 23.6% better than ASR encoder and 60.7% better than triplet loss conformer.\nIn Fig. 5, the DET curves for those models are plotted to show the accuracy at different operating points. The DET curves are aggregated over all evaluation phrases by averaging their FRRs and FARS."}, {"title": "5.2. Results for Different LSTM and Conformer Model Sizes", "content": "Given GE2E loss outperforms triplet loss model and other baseline models, we tune GE2E loss based LSTM and Conformer model architectures to understand how model performance varies with model size. For the LSTM model, we retain three stacking layers to capture the sequence dependencies, and increase the hidden layer and output layer dimensions. As shown in top four rows in Table. 2 and triangle decorated dotted curves in Fig. 6, the model performance achieves optimal result at 15MB size (1.2MB quantized) with our training data recipe. However the smaller model might still be preferred if a lower memory footprint or latency is required in the production environment.\nWe did further parameter tuning for the conformer models as well, including experimenting with 5-12 conformer blocks, various number of heads in MHSAs, and various positional encoding and attention output layer dimensions. As shown in bottom four rows in Table. 2 and solid curves in Fig. 6, the model performance approximates the optimal and plateaus when increasing the size to 18MB (1.8MB with quantizations). Note the 24MB model has slight better AUC but its DET curve is entangled with the 18MB model so we will not conclude who is better. Besides, the above results show that conformer has better result than LSTM at small model sizes(i.e. 2.8MB raw model size). As model size increases, the LSTM model performs slightly better in clean conditions, however the conformer models are better under noisy conditions because of the ability of detecting noise from local contexts."}, {"title": "6. CONCLUSION", "content": "This paper's main contributions include: (1) first time applying GE2E loss to simulate the two-stage process of runtime enrollment and verification during the KWS training and (2) design an end-to-end evaluation process that mimics the real world audio enrollment and custom keyword detection using the latest speech command dataset. On top of improved training speed and convergence stability, our GE2E loss and 419KB quantized conformer model beats the 7.5GB ASR encoder by 23.6% on phrase averaged AUC and beats the same size triplet loss baseline by 60.7%. Furthermore, our models are natively streamable and designed to run continuously on-device with no retraining needed for new keywords(zero-shot)."}]}