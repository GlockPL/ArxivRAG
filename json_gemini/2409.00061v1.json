{"title": "Enhancing Natural Language Inference Performance with Knowledge Graph for COVID-19 Automated Fact-Checking in Indonesian Language", "authors": ["Arief Purnama Muharram", "Ayu Purwarianti"], "abstract": "Automated fact-checking is a key strategy to overcome the spread of COVID-19 misinformation on the internet. These systems typically leverage deep learning approaches through Natural Language Inference (NLI) to verify the truthfulness of information based on supporting evidence. However, one challenge that arises in deep learning is performance stagnation due to a lack of knowledge during training. This study proposes using a Knowledge Graph (KG) as external knowledge to enhance NLI performance for automated COVID-19 fact-checking in the Indonesian language. The proposed model architecture comprises three modules: a fact module, an NLI module, and a classifier module. The fact module processes information from the KG, while the NLI module handles semantic relationships between the given premise and hypothesis. The representation vectors from both modules are concatenated and fed into the classifier module to produce the final result. The model was trained using the generated Indonesian COVID-19 fact-checking dataset and the COVID-19 KG Bahasa Indonesia. Our study demonstrates that incorporating KGs can significantly improve NLI performance in fact-checking, achieving a maximum accuracy of 0,8616. This suggests that KGs are a valuable component for enhancing NLI performance in automated fact-checking.", "sections": [{"title": "Introduction", "content": "COVID-19, also known as Coronavirus Disease 2019, is an acute inflammatory disease caused by SARS-CoV-2 that affects the human respiratory system. The signs and symptoms of COVID-19 include cough, fever, and shortness of breath. COVID-19 was first announced in late 2019 and has since become a worldwide pandemic. At that time, COVID-19 became the main global health concern due to its high contagiousness and the mortality rate it caused, with efforts to find a treatment still in progress. Therefore, every country was forced to formulate an effective strategy to overcome the pandemic [1]. One of the strategies at the public health level was to ensure that people received accurate information. In such conditions, accurate information can help people understand the current situation, and therefore, proper action can be taken [2].\nWith the advancement of the internet, people now tend to seek information, including health-related information, online [3]. Online news portals and social media have become popular places for seeking such information [4]. These trends have brought advantages for people in finding reliable information faster. Furthermore, a study by Manika et al. revealed that exposure to reliable online health information has given positive impacts to the health-related behavior changes [5]. This affirms the positive advantages of seeking health information online. However, despite their advantages, these information-seeking behavior trends have also made people vulnerable to receiving misinformation [6].\nMisinformation is simply defined as information that contradicts the facts [7]. Another definition of misinformation refers to information that is \"explicitly false\" compared to what has been determined or believed by expert consensus [8]. Misinformation cannot be neglected, as it can cause serious consequences, especially in the context of public health [2], [5], [8]. Misinformation can create distrust among people towards public health efforts, leading to failures in combating certain public health-related problems [2]. For example, misinformation about the COVID-19 vaccine has built negative sentiments in the public towards the vaccine [9], leading to lower adoption among the population. The widespread dissemination of misinformation through the internet can be explained by the abundance of unvalidated information spread through online channels, such as social media and news portals [6]. Therefore, attention must be given to overcoming this issue. One of the solutions is verifying the truthfulness of information through a process known as fact-checking [10], [11].\nFact-checking is a journalistic process to verify the truthfulness of information [12]. At the beginning, fact-checking is a human labor- and time-intensive process [12] involving collecting supporting evidence and verifying the truthfulness of information according to the collected and supported evidence [11]. However, with the abundance of user-generated content on the internet, it is almost impossible to do it manually [11], [13]. Thanks to the advancement of artificial intelligence and natural language processing, the fact-checking process paradigm has shifted towards automated fact-checking systems [11].\nAn automated fact-checking system leverages the power of deep learning [11], usually involving Natural Language Inference (NLI) [11], [14], [15] using existing Pre-trained Language Models (PLMs), to verify the truthfulness of information based on collected supporting evidence. NLI can be simply defined as a task of determining the relationship between a premise sentence and a hypothesis sentence [16], [17], [18], where, in the context of fact-checking, the hypothesis is the information being verified (claim) and the premise is the supporting evidence. The resulting relationships can be entailment (fact), contradiction (misinformation), or neutral (cannot be determined)."}, {"title": "Relevant Works", "content": "Injecting external knowledge into a model through KGs is still a fascinating open research question. Many researchers are conducting studies to find the optimal method (both in terms of performance and resulting complexity) to inject external knowledge into a model. To simplify, Yang et al. further categorize these methods into six categories: feature-fused, embedding-combined, knowledge-supervised, data-structure unified, retrieval-based, and rule-guided [24]. Among these, data-structure unified, embedding-combined, and retrieval-based methods gain our interest.\nOne challenge in injecting knowledge from KGs arises from the nature of KGs, which are represented as graphs. Therefore, the main idea behind a data-structure unified method is to transform and unify the input format into a defined, standardized structure. This unified data structure can then be used for downstream tasks [24]. K-BERT [29] is a well-known architecture that employs this method. The advantage of this approach is that it standardizes the input format. However, the drawback is the increased complexity of input processing, which can lead to reduced performance if not properly handled.\nWhile data-structure unified methods standardize inputs into a new data structure, they have the drawback of increasing input complexity. In contrast, embedding-combined methods take advantage of embedding representations. The idea behind this approach is to encode the input from the KG through a representation learning module and then fuse the resulting representations with the token representations from the main input. This fused representation can then be used for downstream tasks [24]. KnowBERT [30] is known to use this method, which allows models to gain knowledge through the provided representation embeddings.\nAnother method of injecting knowledge is the retrieval-based method. This approach involves retrieving, selecting, and encoding the most relevant knowledge from extensive KG sources. The advantages of this method lie in its interpretability and practical application of knowledge [24]. KT-NET is one example of this method in use [31]."}, {"title": "Methodology", "content": "We approached the integration of knowledge from KG into models from a different perspective. In this study, we proposed a model architecture that leverages the strengths of both embedding-based and retrieval-based methods.\nFrom the embedding-based method, we adopted the key concept of using fused embedding representations as input for downstream tasks. Meanwhile, from the retrieval-based method, we incorporated the concept of retrieving and selecting as much relevant information from the KG as possible, enabling the model to access extensive knowledge. Our approach allowed for straightforward knowledge integration while maintaining the simplicity of the model architecture."}, {"title": "Model Architecture", "content": "Our proposed model architecture consists of three modules: the NLI module, the fact module, and the classifier module. The NLI module is responsible for processing the semantic relationship between the given premise and hypothesis sentence, while the fact module handles the information from the fact paragraph. The resulting representation vectors from both modules are then fused (concatenated) into a single vector, which serves as the input for the classifier module. The classifier module then produces the final output (entailment, contradiction, or neutral). Both the NLI and fact modules are essentially PLMs, while the classifier module is a multi-layer perceptron network.\nWe define a \"fact paragraph\" as a collection of \"fact sentences\" combined to form a single paragraph. Each \"fact sentence\" is derived from a triplet retrieved from a KG, represented as {$e_s$,r, $e_t$}, where $e_s$ and $e_t$ represent the source and target entities (nodes), respectively, and r represents the relationship between them. These elements are combined to form a single sentence. For example, given the triplet {\"COVID-19\", \"HAVE_SYMPTOM\", \"cough\"}, the fact sentence would be \"COVID-19 have symptom cough.\" Figure 3 illustrates this straightforward process.\nTo generate a fact sentence from the retrieved triplet, we used a word-matching retrieval mechanism approach. This mechanism is implemented in the knowledge processor part of the model. Given a knowledge graph (KG) as the source of external knowledge and a hypothesis sentence as the input query to retrieve the relevant triplet, the mechanism steps are as follows (Figure 3, Table 2):\n1.  The input sentence is split into words using a certain delimiter (in this case, white space). Words considered as stop words are removed. The stop words list used in this study was for the Indonesian language [32].\n2.  Each resulting word is then used as a query to find matched entities $e_s$ in the KG.\n3.  Each matched entity $e_s$ is then used to find the corresponding entity $e_t$ and its relationship r, forming a triplet {$e_s$, r, $e_t$}.\n4.  Each retrieved triplet is then joined together to form a fact sentence.\n5.  Lastly, each formed fact sentence is joined together to form a fact paragraph."}, {"title": "Dataset Generation", "content": "A dataset is needed to train and evaluate the model. In this case, we require a COVID-19 fact-checking dataset in the Indonesian language. To the best of our knowledge, there are currently no COVID-19 fact-checking or general fact-checking datasets available in Indonesian. Therefore, in this study, we generated our own fact-checking dataset with the help of ChatGPT. Specifically, we used ChatGPT 3.5 Turbo to create our synthetic dataset. ChatGPT has been proven in many studies to be capable of generating high-quality synthetic datasets for various downstream tasks at a lower cost [33], [34], [35]. Moreover, using generative large language models (LLMs) such as ChatGPT to generate synthetic datasets offers several advantages. It results in diverse and rich contextual datasets, which can lead to improved model performance [36]. Figure 4 illustrates our dataset generation workflow in detail.\nThe process began by collecting factual sentences related to COVID-19 in the Indonesian language. These sentences were gathered from credible sources, such as journals, books, national (expert) consensus documents, and official government websites. These factual sentences served as the premises in the dataset. The sentences then underwent a paraphrasing process. During this stage, each premise was duplicated multiple n times and paraphrased to increase both the number and variation of premise sentences. Afterward, the premise sentences were processed by the hypothesis generator, where pairs of hypothesis sentences were generated. For each premise, multiple hypothesis sentences were generated, each labeled as entailment, contradiction, or neutral. Both the premise paraphraser and hypothesis generator processes used a zero-shot prompting technique. Finally, any possible duplicates were removed to ensure the uniqueness of the dataset."}, {"title": "Experiment Design", "content": "The key focus of this study was the PLM, where the NLI and fact modules were replaced by the selected PLM. The experiment was designed to identify the PLM that resulted in the best performance compared to the baseline. The baseline referred to a model that did not use knowledge from a KG and was defined as a PLM directly connected to the classifier module. The PLMs evaluated included indolem/indobert [37] and indobenchmark/indobert (p1 and p2) [38] as monolingual models, as well as mBERT [39] and XLM-ROBERTa [40] as multilingual models. All PLMs included in this study were of the case-insensitive (uncased) type and based on the transformer base architecture. Meanwhile, the KG used in this study was COVID-19 KG Bahasa Indonesia [27].\nThe experiments were divided into two phases. The first phase aimed to train the model and identify the best hyperparameter configuration, while the second phase focused on testing the model. During the first phase, the model was trained using the training dataset, and validation was conducted using the validation dataset. In the second phase, testing was performed using the testing dataset. The models were trained with a learning rate of 2e-5, a batch size of 16, and 16 epochs, employing an early stopping strategy with a patience of 5. The loss function used was cross-entropy loss, and the optimizer was Adam. Training was conducted on an Intel\u00ae Xeon\u00ae Silver 4208 processor and an Nvidia Quadro RTX 5000 GPU with 16 GB of RAM. The evaluation metrics included precision, recall, accuracy, and F1-score. The Wilcoxon Signed-Rank test was used to assess the statistical significance of the resulting accuracy."}, {"title": "Result and Discussion", "content": null}, {"title": "Generated Dataset", "content": "From our dataset generation workflow, we created 18,750 premise-hypothesis sentence pairs, with each label (entailment, contradiction, neutral) having 6,250 sentence pairs. The dataset was then divided into training and testing datasets with a ratio of 80:20. The training set was further divided into training and validation datasets with a ratio of 80:20. Evaluation of correctness was conducted on 100 randomly selected samples. The first evaluator gave a score of 90%, while the second evaluator gave a score of 87%. This resulted in an overall correctness score of 88,5%, indicating that the dataset is of good quality and suitable for this study."}, {"title": "Model Evaluation", "content": "Table 4 shows the results of the first phase experiment (the training phase). Although the model was run for 16 epochs, the experiments indicated that the model achieved the best results within the first 5 epochs. Most of the model's best results were obtained after just 2 training epochs. Among the models, the one using XLM-ROBERTa [40] required the longest training time, reaching its best performance at 5 epochs. This can possibly be explained by the fact that XLM-ROBERTa [40] had the largest number of parameters compared to the other models. From this, one can infer that an early stopping strategy can be used for an effective and efficient training process, reducing the need for longer epochs, which typically offer only marginal improvements and thus minimize the computer resources required."}, {"title": "Error Analysis", "content": "Error analysis was performed to understand where the model still fell short. In this case, error analysis was performed on the XLM-ROBERTa, the best PLM used in our proposed model. From the tables, it can be observed that the model attempted to return the most relevant fact paragraph information given the hypothesis sentence. The relevant keywords are marked with underscores. This provided additional information for the model to make better predictions.\nHowever, despite these improvements, one struggle identified was the need for a better algorithm to return the relevant information for the given hypothesis sentence. Our word-matching level mechanism relied heavily on word-to-word matching and did not consider the surrounding context. This resulted in non-relevant fact paragraphs being returned. Furthermore, another struggle arose when the information was not available in the KG, resulting in empty returned fact paragraphs. Therefore, further research is needed to improve the information retrieval algorithm and the completeness of the KG for fact-checking purposes."}, {"title": "Conclusion", "content": "In this study, we proposed the use of KG to enhance NLI performance for automated COVID-19 fact-checking in the Indonesian language. In our proposed model, we processed the semantic relationships between premise and hypothesis sentences and information from the KG in separate modules and used the concatenated representation vectors from both modules as input to the classifier. This approach allowed the model to integrate information from both semantic relationship data and the KG while maintaining a simple model complexity. The best model was achieved with the use of XLM-ROBERTa PLM trained with a learning rate of 2e-5 for 5 epochs using cross-entropy loss and the Adam optimizer. The accuracy was 0,8616, which was 1,65% higher compared to the baseline. Therefore, this study highlighted that KG can serve as a valuable component in an automated fact-checking system."}]}