{"title": "Improving Natural Language Understanding for LLMs via Large-Scale Instruction Synthesis", "authors": ["Lin Yuan", "Jun Xu", "Honghao Gui", "Mengshu Sun", "Zhiqiang Zhang", "Lei Liang", "Jun Zhou"], "abstract": "High-quality, large-scale instructions are crucial for aligning large language models (LLMs), however, there is a severe shortage of instruction in the field of natural language understanding (NLU). Previous works on constructing NLU instructions mainly focus on information extraction (IE), neglecting tasks such as machine reading comprehension, question answering, and text classification. Furthermore, the lack of diversity in the data has led to a decreased generalization ability of trained LLMs in other NLU tasks and a noticeable decline in the fundamental model's general capabilities. To address this issue, we propose Hum, a large-scale, high-quality synthetic instruction corpus for NLU tasks, designed to enhance the NLU capabilities of LLMs. Specifically, Hum includes IE (either close IE or open IE), machine reading comprehension, text classification, and instruction generalist tasks, thereby enriching task diversity. Additionally, we introduce a human-LLMs collaborative mechanism to synthesize instructions, which enriches instruction diversity by incorporating guidelines, preference rules, and format variants. We conduct extensive experiments on 5 NLU tasks and 28 general capability evaluation datasets for LLMs. Experimental results show that Hum enhances the NLU capabilities of six LLMs by an average of 3.1%, with no significant decline observed in other general capabilities.", "sections": [{"title": "Introduction", "content": "NLU is a subset of natural language processing in artificial intelligence, encompassing key tasks such as machine reading comprehension, text classification, question answering, and information extraction. Recently, LLMs (Dubey et al. 2024; Yang et al. 2023) have shown impressive performance in general chat, but their language understanding ability still has shortcomings (Xu et al. 2024a; Li et al. 2023a; Cheng, Huang, and Wei 2023). Supervised instruction fine-tuning is an effective method for enhancing specific capabilities of LLMs (Sainz et al. 2023; Zeng et al. 2024). Technical reports from Llama3.1 (Dubey et al. 2024) and Qwen2 (Yang et al. 2024) emphasize that high-quality instruction data is essential for effectively aligning LLMs, and producing such data is crucial for improving model performance significantly. Recently, there have been some works on high-quality instruction synthesis. Xu et al. (2024b) generates numerous queries through various templates, allowing current LLMs to produce responses. Cheng et al. (2024) has developed an instruction synthesis framework that converts raw pre-training text into instructional formats, substantially improving the performance of pre-trained models. Additionally, Zeng et al. (2024) has introduced an end-to-end framework that utilizes LLMs to create evolving synthesis instruction datasets. However, the instructions synthesized by these methods are domain-independent. Currently, there is a significant scarcity of instruction synthesis specifically for NLU tasks.\nRecently, LLMs have achieved impressive performance in unified information extraction tasks by synthesizing information extraction instructions (Xu et al. 2024a; Wang et al. 2023a; Gui et al. 2024). However, these synthesized instructions have several limitations. As illustrated in Figure 1, the language understanding performance of YAYI-UIE (Xiao et al. 2023), train on the Baichuan2-13B-Chat (Yang et al. 2023) and YAYI datasets, and Llama3-iepile (Gui et al. 2024), train on Llama-3-8B-Instruct (Dubey et al. 2024) and IEPILE, has noticeably decreased compared to their respective LLMs, with OneKE experiencing a 14.5% decline. An analysis reveals two main issues with the natural language understanding instructions developed in these cases: first, they primarily concentrate on information extraction tasks while overlooking machine reading comprehension, text classification, and question answering. Second, the instruction formats are too simplistic and fixed to make LLMs"}, {"title": "Related Work", "content": "Generative Natural Language Understanding In the field of natural language understanding, mainstream tasks encompass information extraction (NER, RE, EE, OpenIE etc.), text classification (topic classification, sentiment analysis, text similarity, natural language inference, etc.), and machine reading comprehension. For information extraction, the UIE (Lu et al. 2022) framework pioneered a generation-based unified approach, effectively addressing the challenges associated with redundant models and data construction. Building on this, InstructUIE (Wang et al. 2023a) and YAYI-UIE (Xiao et al. 2023) developed a suite of information extraction instructions, implementing an instruction-based extraction framework through fine-tuning of large language models. To further enhance generalization beyond previous extraction instructions, OneKE (Gui et al. 2024) has introduced a more comprehensive and diverse set of information extraction instructions. In the realm of text classification, Wang, Pang, and Lin (2023) and Sun et al. (2023) have innovatively utilized different prompting methods to facilitate zeroshot text classification and natural language inference, respectively. For machine reading comprehension, Cheng, Huang, and Wei (2023) achieved significant performance improvements by converting extensive amounts of raw text into QA pairs before fine-tuning.\nInstruction Synthesis Recent technical reports on the open-source large language models Llama 3.1 (Dubey et al. 2024) and Qwen2 (Yang et al. 2024) highlight that generating high-quality instructions is vital for training large models during both the pre-training and alignment stages. Wang et al. (2023b) proposes a framework for improving the instruction-following capabilities of pretrained language models by bootstrapping off their own generations. Li et al. (2024) exclusively utilizes a pre-curated taxonomy of human knowledge and capabilities as input and generates large-scale synthetic instruction data across all disciplines. Additionally, Dong et al. (2024) transforms the validation of instruction-following data quality into code verification, requiring LLMs to generate instructions. There are also efforts to argument instructions for IE tasks. By annotation guidelines, GOLLIE (Sainz et al. 2023) improves zero-shot information extraction, while ADELIE (Qi et al. 2024) constructs a high-quality alignment corpus for IE instructions."}, {"title": "Methodology", "content": "The architecture of our instruction synthesis framework is illustrated in Figure 2, which mainly consists of two parts. First, the basic instruction synthesis, which employs the structured instruction style with the field of \"instruction\u201d, \"schema\" and \"input\" from existing information extraction instructions (Lu et al. 2022; Gui et al. 2024; Xiao et al. 2023; Xu et al. 2024a) and extends to other NLU tasks, such as open information extraction, machine reading comprehension, and text classification. Second, the compound instruction synthesis, which diversifies the data from the basic instruction synthesis. The main strategies for this diversification include guidelines synthesis, preference rules synthesis, and format variants synthesis."}, {"title": "Guidelines Synthesis", "content": "Most of the previous methods (Xu et al. 2024a; Lu et al. 2022; Wang et al. 2023a; Xiao et al. 2023) focus solely on zero-shot learning for information extraction. The instructions they design are very rigid, leading to a significant loss of in-context learning ability in large language models trained on these instructions. To address these issues, a guidelines synthesis strategy is developed. We transform the basic instructions with multiple perspectives to synthesize instructions with guidelines, which effectively prevent LLMs from overfitting and improve their language understanding capabilities. The main perspectives are as follows:\n\u2022 Description: Add semantic explanations or typical values to a schema. For example, the schema \"date\" can refer to a certain date as \"2024-08-15\" or it can also refer to a particular month or day of the week, such as \u201cAugest\u201d, \u201cThursday\u201d.\n\u2022 Example: Providing the representative positive and negative examples in domain-specific tasks to help LLMs better follow and understand user instructions, thereby alleviating the issue of loss in-context learning capacity.\n\u2022 Format: Construct various structures and formats. The structures can be hierarchical or flat. The formats include JSON, text, markdown, code, etc. By specifying the output formats of an instruction and transforming the same sample into multiple corresponding structures and formats, we further reduce the overfitting of LLMs on monotonous format of instructions.\nGuideline Paraphrasing To improve the generalization ability of the instructions based on the guidelines mentioned above, we make additional confusion, introduce variations, and modify the guidelines. The specific strategies are as follows:\n\u2022 Label Name Variants : Utilizing synonyms to enhance the diversity of label name variants, for instance, the entity type \"Position\u201d may be substituted with terms such as \"Title\", \"Job\", and \"Occupation\".\n\u2022 Label Name Masking: Replacing a portion of the label names within the schema with placeholders in a randomized manner encourages the model to concentrate on and comprehend the schema guidelines more effectively.\n\u2022 Description Variants: Request LLM to generate explanations for a specific schema in a particular semantics using various expressions.\n\u2022 Representative Examples: For each schema, we generate five positive examples, five negative examples and various representative candidates, together with other guidelines (such as descriptions and name variants) to create a comprehensive scheme dictionary. Consequently, when synthesizing an instruction, the guidelines of a certain schema, including examples, can be randomly sampled from this dictionary."}, {"title": "Preference Rules Synthesis", "content": "While the synthesis of guidelines can enhance the diversity of instructions, the underlying semantics of these instructions remain almost unchanged, leading to minimal variation in model outputs. To address this limitation and synthesize samples with distinct semantics, we developed a strategy named \"preference rules synthesis\u201d as depicted in Figure 3. This approach leverages existing guidelines to implement a modification strategy that utilizes GPT-4 for generating a novel labeling rule, subsequently producing entirely new outputs based on this rule. In contrast to the direct utilization of a rule library for invoking GPT-4 to create labeled samples, this methodology yields labeled samples with greater semantic diversity, effectively mitigating the risk of overfitting in LLMs and improving their capability to understand fine-grained task requirements. The proposed modification strategy is outlined as follows:\n\u2022 Entity Boundaries: Handling of modifying prefixes and suffixes, such as President of the United States Biden or simply Biden.\n\u2022 Numerical: Including quantities, chronological order, logical sequence, etc. For example, extracting only the"}, {"title": "Format Variants Synthesis", "content": "The previous instruction structure (Li et al. 2023b; Gui et al. 2024; Xu et al. 2024a) is constrained to a single output format, such as JSON, code, or plain text. However, numerous NLU tasks do not adhere to a singular representational style. For example, tasks such as machine reading comprehension and text classification do not align well with the JSON format, as they often struggle to define appropriate \"keys\" within the JSON structure. This restriction to a sole JSON instruction format poses considerable limitations on the language understanding capabilities of LLMs. To address this challenge, as show in Figure 2, we extend the output formats for identical samples to include JSON, text, markdown, and other styles. Additionally, we integrate prompts for various output styles within the input instructions, thereby transforming a singular sample into multiple representations. To mitigate the risk of LLMs becoming overfitted to a specific style, we generate multiple outputs for each output style. For instance, in the NER task, we define different candidates for producing empty results, such as \u201c\u201d, \u201cNAN\", and []. By varying the format specification in the input instructions and selecting a diverse array of candidate outputs, we significantly enhance the variety of sample formats, ultimately alleviating the overfitting challenges faced by LLMs.\""}, {"title": "Instruction Statistics", "content": "Based on the framework mentioned above, a synthesized dataset of 2,812,832 instructions is generated. As illustrated in Figure 4, the entire dataset encompasses the following tasks: NER (23%), RE (29%), SPO (11%), EE (5%), EET (3%), EEA (2%), OpenIE (4%), KGE (12%), MRC (2%), and TC (1%), with an additional 8% IG (Instruction Generalist is included to prevent LLMs from losing its chat capability). All synthesis instructions are divided into two categories: basic instructions and compound instructions. Basic instructions account for 55% of the total. Compound instructions make up 45% and include at least one type of instruction diversity synthesis strategy (guidelines synthesis, preference rules synthesis, format variants synthesis). The total number of compound instructions is 1,261,658, in which 1,152,470 contain guidelines, 34,770 apply preference rules synthesis, and 108,091 use format variants synthesis. Due to overlaps among these strategies, the total data volume is less than the sum of the data for each individual strategy. The definitions, examples of instructions, and data source distributions for each task, along with examples of basic and compound instructions for each task, are detailed in the Appendix."}, {"title": "Experimental Settings", "content": "Datasets\nTo evaluate the effectiveness of the Hum dataset for natural language understanding, we perform zero-shot experiments on five NLU datasets: CrossNER (Liu et al. 2021) for named entity recognition, FewRel (Han et al. 2018) for relation extraction, CCF Law for event extraction, C3 (Sun et al. 2020)"}, {"title": "Hum For Natural Language Understanding", "content": "We fine-tune six different LLMs using Hum data and evaluate them across seven dimensions.\nAs illustrated in Table 5, models trained on the Hum dataset, such as Llama2, Llama3, Mistral, and Phi3, show an improvement in average performance across multiple dimensions. However, there is a noticeable decline in average performance for Qwen2 and Baichuan2. When comparing against models like YAYI-UIE (based on Baichuan2), Llama3-iepile (based on Llama3), and OneKE (based on Llama2), our synthesized data substantially outperformed these in multiple dimensions. Notably, tasks related to language understanding show significant improvements across all LLMs, with an average increase of 3.1%. The models, as shown in Table 4, improve significantly on tasks such as Lcsts, Lambada, Xsum, and WSC, which are similar to information extraction tasks as they require extracting answers from the original text. In contrast, C3 and Race are multiple-choice question-answering tasks, and the Hum dataset lacks this type of data, leading to less noticeable results. For other dimensions, results are mixed with some showing improvements and others showing declines. It is noteworthy that in"}, {"title": "Case Study", "content": "A typical compound instruction for relation extraction is shown in Figure 5. The LLMs are asked to extract instances of the relation \u201clocated in or next to body of water\", the description is given in schema to indicate the semantic range of the relation: the subject is a location and the object is the body of water. Two examples are provided (due to space limitations, the content of the examples is omitted) to describe the instances that should be extracted in practice. The output format can also be determined based on the output style of examples. The results of the same input instruction from GPT-4, Llama2, OneKE and HumLlama2 are listed. The Raz de Sein is a stretch of water, the La Vieille, Petite lighthouses lighthouses and le le de Sein are locations. Thus in the GPT-4 result, it has made a directional error of the subject and object. For OneKE, it may unable to understand the description and examples, thus it fails to extract and relation individuals from the text. The output of Llama2 is omitted since it is too long with the chain of thought, which also makes the result hard to be parsed. Thus we thought Llama2 is failed to understand the output format from the given examples. Finally for the result of HumLlama2, it extracts one valid relation instance and out put it in the required format."}, {"title": "Conclusion", "content": "In this paper, we propose a novel instruction synthesis framework to create high-quality instructions aimed at enhancing the language understanding capabilities of LLMs. We find that our synthesized Hum data significantly outperforms previous methods in NLU tasks, and notably improves the language understanding abilities of LLMs while incurring minimal knowledge loss in other dimensions. Through ablation experiments, we discover that our proposed methods (guidelines synthesis, preference rules synthesis, and format variants synthesis) significantly enhance the model's generalization ability. Our instruction synthesis method is simple to implement and can be easily adapted for instruction synthesis across various tasks."}, {"title": "Coding", "content": "To evaluate the coding capabilities of LLMs trained with Hum data, we conducted assessments using various coding datasets, such as MBPP (Austin et al. 2021) and HumanEval (Chen et al. 2021). The experimental results, displayed in Table 7, indicate that Hum data improved performance only in the case of Mistral, while other LLMs showed decreased performance. However, Hum-trained models still outperformed Llama3-iepile and OneKE significantly. Our analysis suggests that LLMs trained with Hum data tend to produce code with poorer formatting, likely due to the data's bias towards the JSON format. This formatting issue could stem from the structure of Hum data, which emphasizes consistency in JSON representation, potentially at the expense of more varied coding styles and best practices typically found in traditional programming datasets."}, {"title": "Reasoning", "content": "To evaluate the reasoning capabilities of LLMs trained with the Hum dataset, we conducted an assessment of their performance on several established reasoning benchmarks, including BBH (Suzgun et al. 2023), Drop (Dua et al. 2019), HellaSwag (Zellers et al. 2019), Ocnli (Hu et al. 2020), and PiQA (Bisk et al. 2020). The findings are summarized in Table 9. Our results indicate a significant performance improvement for the Mistral and Phi3 models when utilizing the Hum dataset, while the Qwen2, Llama2, and Llama3 models exhibited a modest decline that remains within the range of normal variability. In contrast to the Llama3-iepile and OneKE models, our models demonstrated substantial enhancements, although their performance was slightly below that of YAYI-UIE. It is important to highlight that the"}, {"title": "Tools", "content": "To evaluate the tool utilization capabilities of LLMs trained with Hum data, we used T-Eval (Chen et al. 2024) as a test set. As shown in Table 10, models such as Baichuan2, Llama3, and Mistral demonstrated some improvement in average metrics, whereas Qwen2, Llama2, and Phi3 showed a slight decline in performance. Overall, the performance changes across these models were modest. However, when compared to models like YAYI-UIE, Llama3-iepile, and OneKE, the improvements were more significant. This enhancement can be attributed to the complex and diverse format of the data we developed. Despite this, due to the specific instructions and data format used in T-Eval, the overall performance of our Hum data in this context did not exhibit a substantial improvement."}, {"title": "Professional Knowledge", "content": "To evaluate the professional knowledge question-answering capabilities of large language models (LLMs) trained on Hum data, we utilized several established datasets: C-Eval (Huang et al. 2023), CMMLU (Li et al. 2023a), and MMLU (Hendrycks et al. 2021a). The summarized results in Table 6 indicate that models like Llama3 and Phi3 demonstrated improvements with our data, whereas models such as Qwen2, Llama2, Baichuan2, and Mistral experienced declines in performance. We attribute these disparities mainly"}, {"title": "General Knowledge", "content": "In our evaluation of the general knowledge question-answering capabilities of LLMs trained on Hum data, we utilized a variety of established datasets, including in-cluding ARC (Clark et al. 2018), BoolQ (Clark et al. 2019), GaoKao-Bench (Zhang et al. 2023), AGIEval (Zhong et al. 2023), CommonsenseQA (Zhong et al. 2023), NQ (Kwiatkowski et al. 2019), OpenBookQA (Mihaylov et al. 2018), and TriviaQA (Joshi et al. 2017). The findings, presented in Table 11, reveal that Llama2, Llama3, Mistral, and Phi3 demonstrated some performance improvements with our curated data. Conversely, Qwen2 and Baichuan2 showed slight declines in performance that were consistent with normal statistical fluctuations. Importantly, these changes were not statistically significant. Our analysis indicates that the Hum data appears to have a stronger emphasis on answer retrieval from existing texts rather than the generation of new content, which likely impacted the observed results."}, {"title": "Dataset Robustness Analysis", "content": "In this study, we employ data synthesis techniques, including guidelines, preference rules, and format variants, to generate a dataset comprising approximately 2.8 million samples. We subsequently randomly select 10K, 100K, and 1M entries from this dataset for training the Qwen2. The experimental findings are detailed in Table 12 and 13. Our results demonstrate a positive correlation between the volume of data used for training and the NLU proficiency of the model. Furthermore, even with a modest training sample size of 10K, a notable enhancement in the model's NLU capabilities is observed."}, {"title": "Instruction Ablation Analysis", "content": "We design basic instructions and compound instructions to enhance the diversity of instructions, and the statistical analysis of the instructions is shown in Figure 4. We conduct an ablation analysis of the instructions on Qwen2, and the experimental results are presented in Table 12 and 15. As shown in the tables, the performance of compound instructions is superior to that of basic instructions, and mixing basic and compound instructions yields even better overall performance. Hum enables LLM to learn to understand prompts through diverse forms of instructions, rather than merely memorizing them."}, {"title": "Dataset and Instruction Explanation", "content": "All the dataset used for instruction synthesis in this work and the count of the instruction for each task is listed in Table 16."}, {"title": "Named Entity Recognition (NER)", "content": "For the NER task, we synthesized instructions based on 20 open-source datasets and 4 self-built datasets belonging to"}, {"title": "Reasoning", "content": "To evaluate the reasoning capabilities of LLMs trained with the Hum dataset, we conducted an assessment of their performance on several established reasoning benchmarks, including BBH (Suzgun et al. 2023), Drop (Dua et al. 2019), HellaSwag (Zellers et al. 2019), Ocnli (Hu et al. 2020), and PiQA (Bisk et al. 2020). The findings are summarized in Table 9. Our results indicate a significant performance improvement for the Mistral and Phi3 models when utilizing the Hum dataset, while the Qwen2, Llama2, and Llama3 models exhibited a modest decline that remains within the range of normal variability. In contrast to the Llama3-iepile and OneKE models, our models demonstrated substantial enhancements, although their performance was slightly below that of YAYI-UIE. It is important to highlight that the"}, {"title": "General Knowledge", "content": "In our evaluation of the general knowledge question-answering capabilities of LLMs trained on Hum data, we utilized a variety of established datasets, including in-cluding ARC (Clark et al. 2018), BoolQ (Clark et al. 2019), GaoKao-Bench (Zhang et al. 2023), AGIEval (Zhong et al. 2023), CommonsenseQA (Zhong et al. 2023), NQ (Kwiatkowski et al. 2019), OpenBookQA (Mihaylov et al. 2018), and TriviaQA (Joshi et al. 2017). The findings, presented in Table 11, reveal that Llama2, Llama3, Mistral, and Phi3 demonstrated some performance improvements with our curated data. Conversely, Qwen2 and Baichuan2 showed slight declines in performance that were consistent with normal statistical fluctuations. Importantly, these changes were not statistically significant. Our analysis indicates that the Hum data appears to have a stronger emphasis on answer retrieval from existing texts rather than the generation of new content, which likely impacted the observed results."}, {"title": "Dataset Robustness Analysis", "content": "In this study, we employ data synthesis techniques, including guidelines, preference rules, and format variants, to generate a dataset comprising approximately 2.8 million samples. We subsequently randomly select 10K, 100K, and 1M entries from this dataset for training the Qwen2. The experimental findings are detailed in Table 12 and 13. Our results demonstrate a positive correlation between the volume of data used for training and the NLU proficiency of the model. Furthermore, even with a modest training sample size of 10K, a notable enhancement in the model's NLU capabilities is observed."}, {"title": "Instruction Ablation Analysis", "content": "We design basic instructions and compound instructions to enhance the diversity of instructions, and the statistical analysis of the instructions is shown in Figure 4. We conduct an ablation analysis of the instructions on Qwen2, and the experimental results are presented in Table 12 and 15. As shown in the tables, the performance of compound instructions is superior to that of basic instructions, and mixing basic and compound instructions yields even better overall performance. Hum enables LLM to learn to understand prompts through diverse forms of instructions, rather than merely memorizing them."}, {"title": "Dataset and Instruction Explanation", "content": "All the dataset used for instruction synthesis in this work and the count of the instruction for each task is listed in Table 16."}, {"title": "Named Entity Recognition (NER)", "content": "For the NER task, we synthesized instructions based on 20 open-source datasets and 4 self-built datasets belonging to"}, {"title": "General Knowledge", "content": "In our evaluation of the general knowledge question-answering capabilities of LLMs trained on Hum data, we utilized a variety of established datasets, including in-cluding ARC (Clark et al. 2018), BoolQ (Clark et al. 2019), GaoKao-Bench (Zhang et al. 2023), AGIEval (Zhong et al. 2023), CommonsenseQA (Zhong et al. 2023), NQ (Kwiatkowski et al. 2019), OpenBookQA (Mihaylov et al. 2018), and TriviaQA (Joshi et al. 2017). The findings, presented in Table 11, reveal that Llama2, Llama3, Mistral, and Phi3 demonstrated some performance improvements with our curated data. Conversely, Qwen2 and Baichuan2 showed slight declines in performance that were consistent with normal statistical fluctuations. Importantly, these changes were not statistically significant. Our analysis indicates that the Hum data appears to have a stronger emphasis on answer retrieval from existing texts rather than the generation of new content, which likely impacted the observed results."}, {"title": "Dataset Robustness Analysis", "content": "In this study, we employ data synthesis techniques, including guidelines, preference rules, and format variants, to generate a dataset comprising approximately 2.8 million samples. We subsequently randomly select 10K, 100K, and 1M entries from this dataset for training the Qwen2. The experimental findings are detailed in Table 12 and 13. Our results demonstrate a positive correlation between the volume of data used for training and the NLU proficiency of the model. Furthermore, even with a modest training sample size of 10K, a notable enhancement in the model's NLU capabilities is observed."}, {"title": "Instruction Ablation Analysis", "content": "We design basic instructions and compound instructions to enhance the diversity of instructions, and the statistical analysis of the instructions is shown in Figure 4. We conduct an ablation analysis of the instructions on Qwen2, and the experimental results are presented in Table 12 and 15. As shown in the tables, the performance of compound instructions is superior to that of basic instructions, and mixing basic and compound instructions yields even better overall performance. Hum enables LLM to learn to understand prompts through diverse forms of instructions, rather than merely memorizing them."}]}