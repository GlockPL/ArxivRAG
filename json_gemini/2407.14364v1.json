{"title": "TOWARDS ASSESSING DATA REPLICATION IN MUSIC GENERATION\nWITH MUSIC SIMILARITY METRICS ON RAW AUDIO", "authors": ["Roser Batlle-Roca", "Wei-Hsiang Liao", "Xavier Serra", "Yuki Mitsufuji", "Emilia G\u00f3mez"], "abstract": "Recent advancements in music generation are raising mul-\ntiple concerns about the implications of AI in creative mu-\nsic processes, current business models and impacts related\nto intellectual property management. A relevant challenge\nis the potential replication and plagiarism of the training set\nin AI-generated music, which could lead to misuse of data\nand intellectual property rights violations. To tackle this is-\nsue, we present the Music Replication Assessment (MiRA)\ntool: a model-independent open evaluation method based\non diverse audio music similarity metrics to assess data\nreplication of the training set. We evaluate the ability of\nfive metrics to identify exact replication, by conducting a\ncontrolled replication experiment in different music gen-\nres based on synthetic samples. Our results show that the\nproposed methodology can estimate exact data replication\nwith a proportion higher than 10%. By introducing the\nMiRA tool, we intend to encourage the open evaluation\nof music generative models by researchers, developers and\nusers concerning data replication, highlighting the impor-\ntance of ethical, social, legal and economic consequences\nof generative AI in the music domain. Code and examples\nare available for reproducibility purposes.", "sections": [{"title": "1. INTRODUCTION", "content": "Significant advancements in generative algorithms for dig-\nital art creation are challenging the role of artificial intel-\nligence (AI) in artistic practices. Regarding generative AI\nin music, there is an increasing discussion related to the\nuse of computational tools in music creative processes [1],\nthe effects on artists' work, existing listening experiences\nand business models, and the impacts on intellectual prop-\nerty (IP) management [2, 3]. A key challenge is the po-\ntential replication and plagiarism of the training set in AI-\ngenerated music [3, 4], which can lead to data misuse and\nintellectual property violations."}, {"title": "2. BACKGROUND AND RELATED WORK", "content": "2.1 Implications of potential data replication in\nAI-generated music\nMusic-generative AI is advancing rapidly with novel high-\nquality models driven by a strong push from the indus-\ntry, which is encouraging a suitable environment for real-\nworld deployment. Yet, music generation algorithms bring\nsignificant concerns regarding their ethical, social, legal\nand economic implications. A key challenge is the po-\ntential data replication in AI-generated music-inquiring\nwhether a generative model extracts and copies fragments\nfrom the training data and whether AI-generated music can\nbe considered novel and original [3, 4]. This issue is fur-\nther complicated by the implications derived concerning\ndata misuse and IP violations such as copyright infringe-\nment. Moreover, diffusion models, one of the most pop-\nular architectures for generative AI, present high risks of\ndata replication as they tend to memorize their training\ndata [5-7]. In the image generation domain, Somepalli\net al. [9] demonstrate instances where generated images\nwith diffusion models contain object-level copies of their\ntraining data. Based on image retrieval frameworks, they\ncompare generated images with training samples and de-\ntect when content has been replicated. Similarly, Carlini\net al. [5] demonstrate that diffusion models memorize and\nreproduce images from their training data.\nMemorising training data and potential IP violations is\nhighly under-discussed in music generative models litera-\nture, despite being one of generative Al's main negative\nethical implications in the music domain [10]. However,\nthe recently proposed music generative model MusicLM\n[11] has been refrained from releasing due to the ethical\nrisks and potential work replication. In addition, Musi-\nCLDM [12] acknowledges potential issues linked to data\nreplication and plagiarism and, to address them, proposes\ntwo beat-synchronous mix-up strategies for data augmen-\ntation. The exemplified initiatives underscore the relevance\nof considering and addressing the ethical implications of\nthese algorithms.\n2.2 Evaluation methodologies in music generation\nXiong et al. [13] present a survey on music generation\nevaluation methodologies divided into objective, subjec-\ntive and combined approaches. They highlight a cur-\nrent claim in finding a standardised proper method that\naligns with all stakeholders, from developers to musicians\nand music listeners. However, even if multiple evaluation\nmethodologies exist for music generation models, the lit-\nerature highlights a lack of evaluation methodologies fo-\ncused on assessing data replication and the originality of\nAI-generated music [4, 14]. In the symbolic domain, Yin et\nal. [4] introduce the originality score to measure the extent\nto which an algorithm might be copying from the train-\ning set. Nonetheless, there is a growing interest in models\noutputting directly audio music instead of symbolic repre-\nsentations. Thus, a research gap exists in detecting data\nreplication in AI-generated music based on raw audio.\nA recent work by Barnett et al. [15] proposes a frame-\nwork based on two music audio embeddings to assess the\nsimilarity between the training data and AI-generated sam-\nples for understanding training data attribution. Their ap-\nproach, based on VampNet [16], computes cosine distance\non embeddings obtained from CLMR (Contrastive Learn-\ning of Musical Representations) [17] and CLAP (Con-\ntrastive Language-Audio Pretraining) [18].\nOur perspective is that combining metrics based on au-\ndio embeddings, acoustic qualities, and features capturing\nmusic characteristics, such as chord progression or tonal\nsimilarity, provides a comprehensive assessment of poten-\ntial data replication in AI-generated music. In this study,\nwe aim to validate the effectiveness of five music similarity\nmetrics and build an open tool to assess exact data replica-\ntion in AI-generated music using these metrics."}, {"title": "3. FORCED-REPLICATION EXPERIMENT", "content": "3.1 Audio Music Similarity Metrics\nFor this study, we consider five music similarly metrics:\nfour standard and a novel one, covering a diversity of char-\nacteristics, from audio embeddings to state-of-the-art met-\nrics. We here describe the metrics (summarised in Table 1)\nand methods used to implement them.\nCover Song Identification (CoverID) [19\u201321]: Cover\nsong identification is a task aiming to detect whether two\nmusic recordings are based on the same composition, ac-\ncounting for variations in tempo, structure, and instrumen-\ntation while keeping a similar melodic or harmonic line.\nCoverID relies on pitch-content features and local align-\nment. To obtain CoverID distance, we use the implementa-\ntion available in Essentia. A low CoverID value suggests\nsubstantial composition similarity between the two anal-\nysed music samples.\nKullback-Leibler (KL) divergence: This metric pro-\nvides a non-symmetric statistical measurement between\nreference and target probability distributions relative to\ntheir entropy. KL divergence has been employed in the\nliterature to estimate similarity in music (e.g. [22, 23]),\nand more recently, to assess automatic music generation\nprompt adherence (e.g. [24]). We aim to explore its capa-\nbilities to estimate data replication in music samples. To\nobtain probability distributions, we use the PaSST audio\nclassifier proposed in Koutini et al. [25], trained on Au-\ndioset. This methodology aligns with common practice in\nthe literature, such as in AudioGen [26] and MusicGen [27]\nto obtain the probabilities of the labels in their audio and\nmusic samples. To avoid the non-symmetry of KL diver-\ngence, we compute reference to target and target to ref-\nerence KL divergence and, subsequently, average both re-\nsults to obtain symmetric KL divergence. Low KL diver-\ngence indicates a closer similarity between distributions.\nContrastive Language-Audio Pretraining (CLAP)\nscore [18]: CLAP embeddings allow to obtain latent rep-\nresentations of audio or text by conditioning information.\nFor instance, MusicLDM [12] uses this metric to assess\nthe novelty in text-to-music generations. To compute the\nCLAP score between two music samples, we extract the\naudio embeddings from the pre-trained music model for\neach one and compute the cosine distance between them.\nA high CLAP score indicates a high similarity between the\ntwo music samples.\nDiscogs-EffNet (DEfNet) score: In addition to state-\nof-the-art distances between audio embeddings, we incor-\nporate a novel approach based on Essentia models [28].\nEssentia's Discogs-EffNet model provides music au-\ndio embeddings trained on Discogs metadata with con-\ntrastive learning purposes for music similarity. We con-\nsider DEfNet score to observe the effectiveness of embed-\ndings of a model trained for a music-related task on es-\ntimating data replication. Embeddings are extracted based\non track self-supervised annotations and compute the co-\nsine distance between reference and target samples. A high\nDEfNet score reveals high track similarity.\nFr\u00e9chet Audio Distance (FAD) [29, 30]: FAD is an\nadaptation of Fr\u00e9chet Inception Distance (FID) for music,\ncomparing embedding distributions of a reference and a\ntarget set, based on the ViGGish model [31]. Nonetheless,\na recent study by Gui et al. [30] questions whether VG-\nGish is the optimal model for FAD computation for music\ngeneration evaluation. They propose a tool kit with mul-\ntiple models to obtain more accurate embeddings to assess\nAI-generated music when calculating FAD. Consequently,\nwe implement the adapted version of FAD using the CLAP\naudio music pre-trained model. A low FAD score indicates\na high resemblance between the compared music samples."}, {"title": "3.2 Experimental Approach", "content": "To validate the effectiveness of the selected music similar-\nity metrics in detecting exact data replication, we carried\nout a controlled forced-replication experiment with syn-\nthetic data, i.e. replicating music excerpts into another\nsong under controlled conditions. Synthetic data guaran-\nteed that the analysed music samples contained copied in-\nstances, limiting our scope to exact data replication.\nFor this experiment, we use an in-house dataset of 30-\nsecond audio previews from the Spotify API, composed\nof over 18,000 samples and 24 music genre classes. We\nfocus on six music genre classes defined by Spotify API\ninternal class labels: heavy metal, afrobeats, techno, dub,\ncumbia and bolero. These genres were chosen for their di-\nverse musical compositions and elements, allowing us to\nexamine the metrics across multiple scenarios. This se-\nlection aligns with ChatGPT's recommendation to include\ngenres with distinct musical characteristics.\nWe divide data into three groups: (1) reference set: act-\ning as training data, (2) target set: composed of synthetic\ndata, representing AI-generated music, and (3) mixture\nset: containing different songs from the reference set but\nfrom the same music genre to build synthetic data. Syn-\nthetic data with replication contains a controlled percent-\nage of copy from a song in our reference set: 5% (1.5s),\n10% (3s), 15% (4.5s), 25% (7.5s) and 50% (15s). A syn-\nthetic sample is created by introducing the copied propor-\ntion at a random point of a music sample in the mixture\nset. We create 10 samples with a proportion of replication\nper song in the reference set. Figure 1 illustrates the pro-\ncedure to build synthetic data with 5% of replication. For\neach music genre, the reference and mixture sets are com-\nposed of 400 songs each. Thus, the target set comprises\n4,000 (400 x 10) songs per percentage of replication for\neach genre. Music samples are 30 seconds long as cur-\nrently it is the common length in full song composition\nmusic generative models.\nWe assess each metric for all the songs within the ref-\nerence set against themselves to establish a baseline (400\nx 400 = 160,000 per-pair evaluations). Then, we compute\nthem for each reference song and its copied instances to\nonly consider cases with exact data replication (4,000 per-\npair evaluations). Our experiment considers 120,000 sam-\nples of synthetic data (approximately 167h of music with a\nproportion of data replication)."}, {"title": "4. RESULTS", "content": "4.1 Analysing metric behaviour\nFigures 2, 3, 4, 5 and 6 depict the average \u00b5 and standard\ndeviations of the different metrics per degree of repli-\ncation and music genre. We observe a steady and similar\nbehaviour by three metrics (CoverID, CLAP and DEfNet)\nthrough all studied music genres, showing higher similar-\nity values for cases with higher replication levels (50%).\nStandard deviation decreases with increasing replication\nlevel, which suggests less disparity within the analysed\npairs. These three metrics show the sensitivity to es-\ntimate data replication. Instead, KL divergence presents\na surprising behaviour with very similar values of \u03bc and\n\u03c3 for different replication levels. We also observe a cer-\ntain degree of sensitivity in all music genres, except for\ndub, where the baseline mean \u03bc\u03b5 is smaller than in replica-\ntion cases \u03bcr, despite the standard deviation being higher\n(\u03bc\u03b5=0.757, \u03c3\u03b5=0.511; \u03bc\u03b3=0.862, \u03c3\u03b5=0.462). KL diver-\ngence demonstrates the capability of detecting replica-\ntion but is ineffective in distinguishing between degrees of\nreplication.\nContrasting with the other metrics, FAD based on\nCLAP music embeddings completely differs from them.\nOn the one side, its behaviour is inconsistent as it exhibits\nfluctuating trends for the different examined cases. On the\nother side, it fails to detect data replication. A higher sim-\nilarity value (low FAD) is always obtained for the base-\nline. Instead, for the different degrees of replication, higher\nFAD is achieved. Consequently, FAD based on CLAP mu-\nsic embeddings does not appear to be a suitable metric to\nassess exact data replication in music samples.\nBy analysing the metrics' behaviour, we could directly\nconclude that CoverID, KL divergence, CLAP and DEfNet\nare suitable for our posed research aim. However, further\nexploration is required before determining their ability to\ndetect replication and degree of replication. We delve into\nthis analysis in the next subsection.\n4.2 Assessing data replication detection sensitivity\nIn this section, we complement the previous analysis with\nan assessment of statistical differences. Because our data\nis not normally distributed and variance is heterogeneous,\nthe Kruskal-Wallis test [32] is the most adequate statistical\nanalysis to examine our results, as is non-parametric, does\nnot rely on normality and handles unequal sample sizes.\nWe perform the Kruskal-Wallis test on CoverID, KL di-\nvergence, CLAP and DEfNet. Significant statistical differ-\nences (p < 0.05) are observed across all music genres and\ndegrees of replication, consistent with our earlier findings.\nNonetheless, the insight of this analysis relies on the\npairwise comparisons between the baseline and different\ndegrees of replication. CoverID pairwise comparison re-\nveals a statistically significant difference between the base-\nline and the 5% replication degree for afrobeat, cumbia\nand techno. For the three other music genres, this happens"}, {"title": "5. MUSIC REPLICATION ASSESSMENT TOOL", "content": "Derived from the presented experiment, we implement the\nproposed methodology into an evaluation tool. We intro-\nduce the Music Replication Assessment (MiRA) tool: an\nopen evaluation method based on four diverse raw audio\nmusic similarity metrics.\nMiRA computes music similarity between reference\nand target samples to obtain global and per-pair distances,\nbased on CoverID, KL divergence, CLAP and DEfNet. It\ncan estimate data replication with a proportion higher than\n10% (3 seconds), but in most of the examined scenarios,\nit is sensible to 5% of replication. Per-pair distances are\nhighly beneficial for detecting close pairs, outliers and sus-\npicious cases with potential data replication. Considering\nthat replication detection requirements may vary depend-\ning on the evaluation, users are left to set their replication\nthreshold. In addition, MiRA is model-independent as no\ninformation about the model architecture or its characteris-\ntics is necessary. The evaluation is conducted directly with\nthe training (reference) and generated samples (target) of\nthe analysed generative model.\nHowever, designating a baseline value is encouraged to\naccurately interpret the music similarity between the refer-\nence and target samples. We propose a third comparison\ngroup of samples (control) based on songs related to the\nreference songs but unseen by the model (e.g. shared mu-\nsic genre). Again, this is a decision for the users condi-\ntioned to their evaluation scope. Note that using a control\ngroup allows us to understand and interpret the results ob-\ntained by acting as the baseline similarity level of indepen-\ndent songs with a shared characteristic.\nThe complete structure of the implemented system is\ndepicted in Figure 7. We release MiRA as an open-source\ntool, built into a PyPI package. Together with the code,\nwe provide examples and best practice recommendations\nfor using this methodology. With the release of MiRA, we\nhope to enhance transparency in music generation models\nand data replication assessment."}, {"title": "6. DISCUSSION AND CONCLUSIONS", "content": "This investigation focused on validating the use of mu-\nsic similarity metrics for assessing data replication in AI-\ngenerated music. We hypothesise that similarity metrics\nare effective in estimating data replication. Therefore, we\nframed the scope of our study to exact data replication\nin music samples, while conducting a controlled forced-\nreplication experiment with synthetic data.\nWe examined five diverse audio-based metrics: four\nstandard metrics (CoverID, KL divergence, CLAP and\nFAD) and a novel one (DEfNet). Our results indicate that\nfour of the five studied metrics can detect data replication\nto a certain extent. Instead, FAD based on CLAP music\nembeddings presented an opposite behaviour compared to\nthe other metrics. Higher similarity is obtained for the\nbaseline group and FAD shows unstable trends throughout\nthe diverse music genres. Thus, we do not find it suitable\nfor our case study. However, it must be acknowledged that\nthe recent publication by Gui et al. [30] offered multiple\nclassifiers to compute FAD. There is the possibility that\nwe did not consider the appropriate classifier for our task.\nThus, we should consider exploring other classifiers before\ndetermining the validity of FAD in detecting replication in\nmusic.\nRegarding the other four metrics, our results show in-\nteresting insights. First, we find CoverID to be sensible to\ndifferent replication degrees, establishing a robust thresh-\nold level at 10% of replication. Furthermore, in some of\nthe studied cases, replication sensitivity is lowered to 5%\nof replication. This is a substantial finding to validate the\nsuitability of metrics oriented to specific music character-\ntistics, such as tempo, structure and composition.\nNext, we observe that KL divergence can be sensitive to\nreplication as pairwise comparison between baseline and\ndegrees of replication is statistically significant. Neverthe-\nless, the other pairwise results reveal that KL divergence is\nineffective for differentiating between replication degrees.\nWe consider this an unexpected turnout in our analysis.\nConsidering CLAP and DEfNet scores, both\nembedding-based metrics, our experiment validates\ntheir suitability to detect data replication. Not only do\nthey show robustness by increasing their similarity value\nparallel to the replication degrees (i.e. higher similarity\nfor higher level of replication), but they also show high\nsensitivity for different degrees of replication. All results\nsuggest their sensitivity might be higher than we envi-\nsioned and might be able to detect replication in smaller\nsamples (i.e. < 1.5 seconds).\nAs a result of these findings, we achieve our second goal\nwithin the scope of this research: to build an open model-\nagnostic tool based on music similarity metrics on raw au-\ndio. In this article, we have introduced the MiRA tool,\nleveraging the four validated similarity metrics, which can\nbe used to evaluate any music-generative model with au-\ndio output. MiRA does not require any information about\nthe model architecture or its characteristics. Instead, sim-\nilarity evaluation relies on comparing reference and target\nsamples.\nBy introducing the MiRA tool, we are contributing to\nthe research gap of lack of evaluation methodologies di-\nrectly assessing potential data replication in AI-generated\nmusic. Our study validates the use of similarity metrics\nto estimate training data replication. We intend to encour-\nage the open evaluation of music generation models by re-\nsearchers, developers and users concerning data replica-\ntion. In addition, our research strives for the importance of\nethical, social, legal and economic consequences of gen-\nerative AI in the music domain, together with the need to\naddress their risks and issues.\n6.1 Limitations and Future Work\nDespite our contribution to advance towards data replica-\ntion assessment with music similarity metrics, there are\nmultiple opportunities to complement our investigation.\nFirst, we limited the scope of our experimental ap-\nproach to assessing the use of different music similarity\nmetrics for exact data replication, consequently reducing\nthe definition of plagiarism to exact replication of frag-\nments in the training set. We followed such an approach to\nvalidate our hypothesis and ensure an attainable method to\naddress this issue. While this reduced scope could poten-\ntially be solved using audio fingerprinting strategies [33],\nwe believe that by employing a diverse range of metrics\nwe can provide a more comprehensive assessment of data\nreplication.\nFraming our aim to exact data replication also intro-\nduced a limitation in considering typical perturbations that\nmusic samples experience when training the model or dur-\ning the model procedure to generate a music sample. Thus,\nit would be a key point for future work to validate the ro-\nbustness of these metrics towards typical data augmenta-\ntion techniques, such as pitch shifting and reverberation.\nProving them to be robust would also enhance the capa-\nbilities of MiRA for detecting potential replication in AI-\ngenerated music. At the same time, we intend to expand\nthe abilities of MiRA for data replication by incorporating\ncomplementary metrics, if necessary.\nIn addition, our experimental process was limited to the\nhigh computational costs of some of the metrics. In partic-\nular, we faced significantly large amounts of time to com-\npute FAD and KL divergence. This is a relevant concern as\nwe want MiRA to be an open tool that can be used by any\nresearcher or user. Thus, considering the computational\ncapacity required to compute the integrated metrics within\nis a relevant issue in our research.\nAnother limitation is the type of data that we use. We\nbase our experiment on synthetic data despite our goal be-\ning oriented to AI-generated music. We must use synthetic\ndata with a controlled percentage of replication to guaran-\ntee and assess the capabilities of detection and sensitivity\nof music similarity metrics. However, we would like to test\nthe validity of the introduced tool when used in a genera-\ntion context. To do so, we require not only a generative\nmodel but its details on training data and generation sam-\nples. We plan to expand our research in with AI-generated\ncontent in upcoming studies."}, {"title": "7. ETHICS STATEMENT", "content": "The late rapid popularity growth of generative AI in the\nmusic domain brings significant ethical implications. The\nmain challenges are linked to the role of Al within mu-\nsic creative processes, such as composition, potential mis-\nappropriation of data in AI-generated music, inquiries on\nthe novelty of generations, derived authorship attribution,\neffects on intellectual property rights and sustainability of\ncurrent business models. In addition, there are notable con-\ncerns about the cultural bias in these systems and their en-\nvironmental impact.\nOur research focused on the issue of assessing potential\ndata replication in AI-generated music. We observed a lack\nof evaluation methodologies to examine replication in raw\naudio. We contributed to this issue by proposing a method-\nology based on audio-based music similarity metrics. We\ndemonstrated its effectiveness and provided an open tool\nto evaluate AI-generated music. Our introduced approach\nis contributing to the transparency of music generation al-\ngorithms.\nDespite the positive contribution of our investigation,\nwe must be critical of some methodological aspects of our\nwork. Our principal ethical concern falls under the type\nof data used to conduct our forced-replication experiment.\nIn particular, we employ an internal dataset created with\nSpotify previews (30-second samples of music). Even if\nthese practices are common in the ISMIR community, we\nsee the need for guidelines for the legal assessment of MIR\ndata included in datasets, incorporating country dependen-\ncies, origin and intended use, personal data involved (from\nartists and listeners) and potential future consequences."}]}