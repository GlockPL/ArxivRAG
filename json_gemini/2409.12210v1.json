{"title": "Mixture of Diverse Size Experts", "authors": ["Manxi Sun", "Wei Liu", "Jian Luan", "Pengzhi Gao", "Bin Wang"], "abstract": "The Sparsely-Activated Mixture-of-Experts (MoE) has gained increasing popularity for scaling up large language models (LLMs) without exploding computational costs. Despite its success, the current design faces a challenge where all experts have the same size, limiting the ability of tokens to choose the experts with the most appropriate size for generating the next token. In this paper, we propose the Mixture of Diverse Size Experts (MoDSE), a new MoE architecture with layers designed to have experts of different sizes. Our analysis of difficult token generation tasks shows that experts of various sizes achieve better predictions, and the routing path of the experts tends to be stable after a training period. However, having experts of diverse sizes can lead to uneven workload distribution. To tackle this limitation, we introduce an expert-pair allocation strategy to evenly distribute the workload across multiple GPUs. Comprehensive evaluations across multiple benchmarks demonstrate the effectiveness of MoDSE, as it outperforms existing MoEs by allocating the parameter budget to experts adaptively while maintaining the same total parameter size and the number of experts.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have demonstrated remarkable performance in a variety of NLP tasks and have become valuable assistants through a wide range of applications. The scaling law (Kaplan et al., 2020) demonstrates that larger models exhibit superior performance. However, training larger models requires increased computational resources, posing a critical challenge. Mixture-of-Experts (MoE) (Fedus et al., 2022; Lepikhin et al., 2021) address this challenge by using sparse activation to scale up the trainable parameters while maintaining high training and inference efficiency. Recent MoE-based architectures, such as Mixtral of Experts (Jiang et al., 2024), DeepSeekMoE (Dai et al., 2024), and OpenMoE (Xue et al., 2024) have shown superior performance in various tasks.\nSpecifically, Dai et al. (2024) discuss two main issues in the design of the MoE Feed-Forward Networks (FFNs) architecture: Knowledge Hybridity, where each expert covers diverse knowledge due to the limited number of experts, and Knowledge Redundancy, where multiple experts share common knowledge. To address these issues, they propose Fine-Grained Expert Segmentation by splitting the FFN intermediate hidden dimension and Shared Expert Isolation by isolating certain experts to be always activated as shared experts. Additionally, Zhao et al. (2024) introduce Hypernetworks and HyperExperts modules to capture the cross-expert and cross-layer knowledge.\nHowever, almost all existing MoE architectures consist of experts with identical structures and sizes. This homogeneous architecture becomes a significant bottleneck when generating tokens with varying difficulty; some tokens are easier to predict, while others are more challenging. To deal with the varied difficulty, we propose the Diverse Size Experts structure for each FFN layer, where each expert has a different parameter size to handle generating tasks of varying difficulty. Note that we find a similar recent work called Heterogeneous the Mixture of Experts (Wang et al., 2024), which shares a similar motivation and utilizes parameter penalty loss and router entropy loss to control the size and number of activated experts.\nOur contributions are summarized as follows:\n\u2022 Diverse Size Experts We introduce the Mixture of Diverse Size Experts (MoDSE) in Section 3, a new type of FFN layer designed for the MoE framework. Unlike conventional MoEs, which consist of experts of the same size, MoDSE has experts of different sizes. It assigns each token to the expert that best matches its prediction needs in terms of capa-"}, {"title": "2 Preliminaries: Mixture of Experts", "content": "MoE models are usually constructed by replacing dense FFNs layers in the Transformer (Vaswani et al., 2017) with MoE layers. An MoE layer typically consists of multiple experts $E_1(\\cdot)\\cdots E_N(\\cdot)$ and the corresponding gate model $G_1(\\cdot)\\cdots G_N(\\cdot)$, N indicates the number of the experts. The gate model (Shazeer et al., 2017) with trainable weight matrices $W_g \\in \\mathbb{R}^{h_{input}\\times h}$ and $W_n \\in \\mathbb{R}^{h_{input}\\times h}$ selects the top k experts and combines the outputs of experts to produce the output $y \\in \\mathbb{R}^{h}$, where $h_{input}$ is the dimension of input x and h is the dimension of the hidden layer. Fedus et al. (2022) set k as one, while Lepikhin et al. (2021); Jiang et al. (2024) set as two. The outputs of experts are added with the noise to help with load balance. The noise generated from the input hidden vector x is multiplied by $W_n$ and processed by Softplus and the Root Mean Square Layer Normalization function RMSNorm, where \u03b3 is a learnable coefficient.\n$y = \\sum_{i=1}^{N} G_i(x)E_i(x)$ (1)\n$G(x) = Softmax(KeepTopK(H(x),k))$ (2)"}, {"title": "3 MODSE Architecture", "content": "Predicting the next token is easier within frequently appearing token pairs in the corpus. Tokens within the same word or phrase are easier to generate than those between two phrases or words. Analogous to the human brain, the amount of thought required to generate the next word varies among different words. Inspired by the fact that the difficulty of generating each next token varies, we propose MODSE as shown in Figure 1. In our work, the size of the expert parameters is used to quantify the amount of thinking involved. We assign experts a range of parameter sizes by setting the dimensions of the hidden layers to various lengths. However, the imbalance in expert size leads to an uneven workload. To address this issue, we propose a meticulously designed expert-pair allocation method to ensure each GPU node's workload is evenly distributed."}, {"title": "3.1 Diverse Size Experts", "content": "In a traditional MoE structure (Fedus et al., 2022; Lepikhin et al., 2021), the gating network combines a set of experts with the same size. We here adjust the scale of experts to ensure that different experts can handle tasks of varying difficulty. Note that we denote the designed Diverse Size Experts as $\\{\\hat{E}_1(\\cdot),\\dots, \\hat{E}_N(\\cdot)\\}$, and the dimension of the hidden layer for $\\hat{E}_i(\\cdot)$ is $h_i$.\n$\\hat{y} = \\sum_{i=1}^{N}\\hat{G}_i(x)\\hat{E}_i(x)$ (6)\n$\\{(\\hat{i}_1,\\hat{i}_1),\\dots, (\\hat{i}_n,\\hat{i}_n)\\}, with n = \\frac{N}{2}$ (7)\n$h_{\\hat{i}_k} + h_{\\hat{i}_{k'}} = 2 \\times h_,, with k \\in 1...n$ (8)\nTo maintain the overall parameter size, the experts are grouped into pairs $(\\hat{i},\\hat{i}')$, where $k \\in 1...n$ indicates the pair of the experts. The average value of $h_i$ within each pair equals h, with one expert being larger than the average size and the other smaller. Typically, the number of experts is even, ensuring the experts can be grouped into pairs, thus the total parameter size of the MODSE model matches that of the vanilla MoE model."}, {"title": "3.2 Load Balance Consideration", "content": "In MoDSE, experts with hidden layer sizes larger than the average have a higher workload due to the increased number of parameters, both during training and inference phrases. To address this load imbalance problem, we propose the expert-pair allocation strategy, which places each pair of experts on the same GPU and ensures that each GPU contains an equal number of parameters. For instance, in Figure 1, expert pairs are enclosed by dotted line frames, with expert 0 and expert 1 on the same GPU, and so forth.\nBesides the standard cross entropy (CE) loss, we use the auxiliary load balance loss $L_a$ from Switch Transformers (Fedus et al., 2022) to penalize the unbalanced routing distribution among experts. Consequently, each expert has the same frequency of being routed. In Section 4.3, we will demonstrate that after the entire training process, all tokens in the pre-training dataset are evenly spread across all experts. Along with the expert-pair allocation method, this ensures that the final workload of each GPU is balanced.\n$L_a = \\alpha \\cdot N \\cdot \\sum_{i=1}^{N} f_i \\cdot P_i,$ (9)\nwhere $\\alpha$ is a scalar hyperparameter. $f_i$ is the fraction of tokens routed to expert i, $i \\in \\{1,2,\\dots, N\\}$:\n$f_i = \\frac{1}{T}\\sum_{x \\in Batch} 1\\{\\text{argmax } p(x) = i\\},$ (10)\n$p(x) = [p_1(x), p_2(x),\\dots,p_N(x)],$ (11)\nwhere T is the number of tokens and $P_i$ is the fraction of the router probability for expert i:\n$P_i = \\frac{1}{T} \\sum_{x \\in Batch}P_i(x)$ (12)"}, {"title": "4 Experiments", "content": "4.1 Experimental Setup\nModels Our baseline MoE structure is based on the Llama 2 model (Touvron et al., 2023) with the dense FFNs layers replaced by expert layers. Table 1 summarises the model architecture parameters. For the MODSE setting, we adjust the expert sizes in baseline by modifying the dimensions of the hidden layers in 300M \u00d7 8 and 700M \u00d7 8 settings, as listed in Table 2. There are 8 experts grouped into 4 pairs, with the ratio to the input size as (4.5, 0.5), (4.0, 1.0), (3.0, 2.0), and (2.5, 2.5). We train byte pair encoding (BPE) (Sennrich et al., 2016) tokenizer with both English and Chinese datasets, and use it in the following experiments."}, {"title": "4.2 Main Results", "content": "Evaluations We evaluate models in downstream tasks using in-context learning including AGIEval (Zhong et al., 2024), MMLU (Hendrycks et al., 2021a), GSM8K (Cobbe et al., 2021), LAMBADA (Paperno et al., 2016), MATH (Hendrycks et al., 2021b), TriviaQA (Joshi et al., 2017), PIQA (Bisk et al., 2020), SIQA (Sap et al., 2019), and INTENT from Tech (2024) which contains 43 different user intention classes. The model with identical expert sizes is used as the baseline, all the evaluation results are listed in Table 3."}, {"title": "4.3 Analysis on Token Routing", "content": "We further conduct the experiments on 10B tokens data, to analyze the choices of tokens. The statistics from the 2nd to the 7th epoch are listed in Appendix A. The baseline model shows an even distribution of experts' workload. The ratio between the largest and the smallest number of tokens routed to the experts ranges from 1.2 to 3.0. The statistics for the MODSE setting show a non-uniform distribution, with ratios larger than 3.0 appearing, particularly in the first 2 layers of the model and for the experts with the second largest probability.\nHowever, after the entire training process, in the last epoch, only one ratio remains larger than 3.0, with the others ranging from 1.5 to 3.0, indicating that the token distribution among experts becomes more balanced by the end of the training."}, {"title": "4.4 Analysis on Difficult Tokens", "content": "We track the tokens in the MoDSE setting which exhibits a higher cross entropy (CE) loss than the mean value of 1.05 in the baseline, considering them having greater prediction difficulty. The average CE loss values in the MoDSE setting are lower than those in the baseline, indicating that MoDSE improves generating ability. This improvement is achieved by routing tokens that are more difficult to predict to the expert whose size better fits the token's generating task. Table 5 shows the results for the tokens with a higher CE loss than the mean loss value. The tokens in the higher loss threshold show a larger loss decline in the MoDSE setting, demonstrating that the MODSE model performs better on more difficult tokens."}, {"title": "4.5 Difficult Tokens Routing Distribution", "content": "To identify which experts handle the difficult tokens, further analysis is conducted on the 180 tokens with a CE loss greater than 2.0 in the baseline setting. We track the distribution of these 180 difficult tokens across the distinct experts in the 10B tokens data using the converged training model checkpoint. The full tracking results can be found in Appendix B.\nFor these difficult tokens, as shown in Figure 4 and Table 6, more tokens choose the larger experts, while fewer tokens select the smaller experts. This phenomenon is even more pronounced when only considering the top one expert. More than twice"}, {"title": "5 Related Work", "content": "5.1 FFNs Designs\nIn the field of FFNs structure designs, there have been several notable works. DeepSeekMoE (Dai et al., 2024) introduces two strategies, namely Fine-Grained Expert Segmentation and Shared Expert Isolation. By utilizing a finer granularity of expert size, experts can focus on more specific knowledge domains. In contrast, conventional expert sizes tend to cover a wider range of knowledge. The isolated shared experts handle common knowledge across various contexts, ensuring no shared parameters among experts, thereby compressing the parameter space. To tackle the issue of Knowledge Redundancy, HyperMoE (Zhao et al., 2024) also introduces HyperNetworks that contain Hyper-Experts to facilitate knowledge transfer between experts through conditional generation. In addition, DeLighT (Mehta et al., 2021) and Apple OpenELM (McKinzie et al., 2024) introduce block-wise scaling and layer-wise scaling, respectively. These modifications involve adjusting the width of the hidden dimension for FFNs and the number of attention heads on a per-layer basis, leading to more efficient parameter allocation and improved model performance.\nIn contrast to previous works, our research focuses on the allocation of expert parameters within a single MoE layer. This approach aims to equip experts with diverse predictive capacities while ensuring load balance across computational nodes."}, {"title": "5.2 Load Balance", "content": "The LSTM MoE (Shazeer et al., 2017) achieves load balance by incorporating the coefficient of variation of the load function as part of the auxiliary loss. This represents the probability of the gating network being non-zero. GShard, Switch Transformers, and ST-MoE (Lepikhin et al., 2021; Fedus et al., 2022; Zoph et al., 2022) also use a similar auxiliary load balance loss setting by introducing the average probability of each expert being routed across all tokens in the batch in the loss function. DeepSeekMoE (Dai et al., 2024) introduces the expert-level balance loss and the device-level balance loss to deal with the load imbalance issue caused by routing collapse. The expert-level balance loss adjusts the auxiliary loss in Switch Transformers by multiplying a coefficient to fit the different numbers of experts in DeepSeekMoE. The device-level balance loss changes the expert-level balance loss from being expert-wise to device-wise.\nIn our work, we utilize the balance loss from Switch Transformers (Fedus et al., 2022). Additionally, we propose an expert-pair allocation strategy to address the imbalance in expert sizes."}, {"title": "6 Conclusion", "content": "In this paper, we propose MoDSE, a novel structure for MoE layers. Inspired by the varying difficulties of next-token-generating tasks, we introduce the diverse size expert design, providing each expert with different prediction abilities. Our analysis of token routing distribution shows that MoDSE directs tokens to experts whose sizes are best suited for specific token generation tasks. This enhancement improves the MoE model's performance in auto-regression tasks and demonstrates superior results compared to the conventional MoE structure.\nAdditionally, we present the expert-pair allocation method to address the issue of load imbalances in the diverse size expert design, making the MoDSE design more practical."}, {"title": "Limitations", "content": "While MoDSE demonstrates superior performance, our work is subject to several limitations:\n\u2022 Due to limitations in computational and data resources, current experiments are conducted on small-scale MoE models, leaving the model's scalability to larger sizes unclear.\n\u2022 We obtain the aforementioned intriguing findings while training our own MoE LLM. Hence, the tokenizer and data utilized for pretraining are not available as open-source resources. We plan to apply this model design to open-source resources in our future work."}]}