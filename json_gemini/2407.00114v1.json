{"title": "OmniJARVIS: Unified Vision-Language-Action Tokenization Enables Open-World Instruction Following Agents", "authors": ["Zihao Wang", "Shaofei Cai", "Zhancun Mu", "Haowei Lin", "Ceyao Zhang", "Xuejie Liu", "Qing Li", "Anji Liu", "Xiaojian Ma", "Yitao Liang"], "abstract": "This paper presents OmniJARVIS, a novel Vision-Language-Action (VLA) model for open-world instruction- following agents in Minecraft. Compared to prior works that either emit textual goals to separate controllers or produce the control command directly, OmniJARVIS seeks a different path to ensure both strong reasoning and efficient decision-making capabilities via unified tokenization of multimodal interaction data. First, we introduce a self-supervised approach to learn a behavior encoder that produces discretized tokens for behavior trajectories \\( \\tau = \\{0_0, a_0, ... \\} \\) and an imitation learning (IL) policy decoder conditioned on these tokens. These additional behavior tokens will be augmented to the vocabulary of pretrained Multimodal Language Models (MLMs). With this encoder, we then pack long-term multimodal interactions involving task instructions, memories, thoughts, observations, textual responses, behavior trajectories, etc. into unified token sequences and model them with autoregressive transformers. Thanks to the semantically meaningful behavior tokens, the resulting VLA model, OmniJARVIS, can reason (by producing chain-of-thoughts), plan, answer questions, and act (by producing behavior tokens for the IL policy decoder). OmniJARVIS demonstrates excellent performances on a comprehensive collection of atomic, programmatic, and open-ended tasks in open-world Minecraft. Our analysis further unveils the crucial design principles in interaction data formation, unified tokenization, and its scaling potentials.", "sections": [{"title": "1. Introduction", "content": "Upon the success of pretrained Large Language Models (LLMs) (Brown et al., 2020; Cheng et al., 2024; Gong et al., 2023; OpenAI, 2023; Touvron et al., 2023) and Multimodal Langauge Models (MLMs) (Alayrac et al., 2022; Hinck et al., 2024; Liu et al., 2024; Man et al., 2024; Zhao et al., 2023), some recent works have been venturing into developing Vision-Language-Action (VLA) models (Brohan et al., 2023; Huang et al., 2023; Reed et al., 2022; Wang et al., 2023b), a promising pathway towards the ultimate goal of building autonomous agents that can follow and even self-generated instructions to fulfill various reasoning and acting tasks in open world environments. Among them, two most prominent architectures have been proposed: 1) Combining an off-the-shelf MLM (Alayrac et al., 2022; Liu et al., 2024) with separate goal-conditioned controllers (Cai et al., 2023a,b; Lifshitz et al., 2023), where MLM reasons, plans and pilots the controllers by producing textual goal instructions, e.g. DEPS (Wang et al., 2023c), JARVIS-1 (Wang et al., 2023b), voyager (Wang et al., 2023a); 2) Tuning a pretrained MLM into producing control commands directly, while maintaining the reasoning and language capabilities, e.g. RT-2 (Brohan et al., 2023), LEO (Huang et al., 2023). However, these two designs could still have significant drawbacks when it comes to open-world environments. First, an open world (e.g., Minecraft) usually teams up with an infinite number of complex and highly contextualized tasks (Fan et al., 2022; Lin et al., 2023), and it can be fairly challenging to depict them in text only. Therefore, VLA models that solely depend on text to communicate with the text-conditioned policies (Wang et al., 2023b,c) may fail to correctly pilot these controllers. On the other side, emitting the control command directly (Brohan et al., 2023; Huang et al., 2023) without invoking separate controllers could alleviate the aforementioned communication problem but given the long-horizon nature of open-world tasks, it is less practical to perform long-term control with a large VLA model as the context length requirement, computation cost and inference efficiency could become unaffordable.\nIn this paper, we aim to tackle the aforementioned issues of existing VLA models when facing open-world environments: complex & context-dependent tasks and long-term tasks. Our key insight originates from the observation of human decision-making: Given these open-world tasks, humans can make informed decisions via multi-round mental, verbal, and physical interactions (an illustration can be found in Figure 1). Therefore, if the VLA model can manage to learn from such interaction data, it may master the underlying human decision-making procedures. However, modeling interaction data is non-trivial: it is multi-modal, encloses vision (mostly observations), language (instructions, thoughts, etc.), and actions (behavior trajectories). Compared to the fruitful explorations on jointly tokenizing vision and language (Alayrac et al., 2022; Bavishi et al., 2023; Liu et al., 2024; Vinyals et al., 2015) into sequences for autoregressive modeling (Brown et al., 2020), tokenizing behavior trajectories (actions) is hard due to the following reasons. On the one hand, directly using low-level actions from the environment would pose huge challenges to the model's ability to process long sequences, which significantly hurts performance. It also hinders us from leveraging the planning ability of generative models. On the other hand, language-level action tokens require significantly more supervision and cannot accurately describe all possible actions."}, {"title": "2. A Tokenizer for Behaviors", "content": "As illustrated in Section 1, a key challenge for VLA is the mismatch between the action modality and other modalities such as the language instructions. A key insight is that a good amount of knowledge about the effects of actions can be learned directly from behavior trajectories \\( \\{t^{(i)}\\}_i \\). We propose to learn a behavior tokenizer in addition to the well-studied vision and language tokenizers to achieve unified tokenization of the vision, language, and actions in multimodal interaction data (Figure 1). We pose two main requirements to the behavior tokens. First, they should be able to express complete and diverse behavior from (short) trajectories. Further, the tokens should contain semantic information so that they are compatible with the other modalities, which enables the reasoning and planning ability of LLMs (e.g., by conducting chain-of-thought reasoning).\nSpecifically, we aim at producing a set of N discrete behavior tokens \\( \\{\\text{sbhv}_1, ..., \\text{sbhv}_N\\} \\) from a behavior trajectory \\( \\tau = \\{o_0, a_0, ...\\} \\). Further, a de-tokenizer is needed to map these tokens back to an action rollout in the environment that reproduces the goal achieved in \\( \\tau \\). GROOT (Cai et al., 2023b) explores a VAE-based approach to jointly learn a latent representation of behavior trajectories and an imitation learning policy decoder that conditions the latent as goal. However, the continuous latent cannot be used as the behavior tokens as they can be more difficult to learn and decode with the existing discrete tokens of pretrained MLMs (Huang et al., 2023; Ma et al., 2022). Therefore, we replace the Gaussian latent in GROOT with an improved vector quantized discrete latent called Finite Scalar Quantization (FSQ) (Mentzer et al., 2023). We adopt a quantization configuration of [8, 8, 8, 6, 5], which means a code with a length=5 and a codebook size of 8\\( \\times \\)8\\( \\times \\)8\\( \\times \\)6\\( \\times \\)5 = 15360 is produced. The configuration is selected by a simple grid search. Overall, the behavior tokenizer (behavior encoder) \\( e_{\\varphi}(o_{1:T}) \\) and the de-tokenizer (IL policy decoder) \\( \\pi_{\\theta}(a_t|o_{1:t}) \\) is learned with the following objective:\n\\[\\text{argmin}_{(\\varphi, \\theta)} \\mathbb{E}_{\\tau \\sim \\mathcal{D}} \\left[ \\sum_{t=1}^{T} -\\log \\pi_{\\theta}(a_t|o_{1:t}, f(e_{\\varphi}(o_{1:T}))) \\right], \\tag{1}\\]\nwhere \\( f(.) \\) denotes the finite scalar quantizer. We choose a non-causal (bidirectional) transformer and a causal transformer to parameterize the encoder \\( e_{\\varphi}(o_{1:T}) \\) and the policy decoder \\( \\pi_{\\theta}(a_t|o_{1:t}) \\), respectively. In practice, we set T = 128 as the trunk size of the behavior trajectory to be encoded. We will discuss how to handle trajectories longer than 128 in the next section.\nCompared to our behavior tokenization, most prior work in VLA models, either represents the behavior trajectories in interaction data as a textual goal description and invokes a separate goal-conditioned controller (Wang et al., 2023b,c), or represents the state-action sequence \\( \\{o_0, a_0, ...\\} \\) directly as in Decision Transformers (DT) (Brohan et al., 2023; Chen et al., 2021; Huang et al., 2023; Reed et al., 2022). Our approach offers a more compact but still informative representation of the actions part in multimodal interaction data. Moreover, the action readout, i.e. simply sending the behavior tokens to the policy decoder, is also more efficient than the DT-style direct control from VLA models (Brohan et al., 2023; Huang et al., 2023; Reed et al., 2022)."}, {"title": "3. Multimodal Interaction Data and OmniJARVIS", "content": "As illustrated in Figure 1, canonical multimodal interaction data comprises vision (observations), language (instructions, memories, thoughts), and actions (behavior trajectories). However, it can be difficult to directly collect such interaction data from human annotators. Therefore, we propose to convert an existing Minecraft gameplay dataset (Baker et al., 2022) into the multimodal interaction data required by OmniJARVIS. We begin with a formal definition of the interaction data, followed by our approach for data conversion and augmentation from existing datasets, and finish up with the architecture, formulation of learning on such interaction data, and inference procedure of OmniJARVIS. An overview of OmniJARVIS architecture and inference can be found in Figure 3."}, {"title": "3.1. Data Formation", "content": "An interaction sequence of decision-making \\( \\mathcal{D} = \\{D_t\\}_{t=0}^{T} \\) comprises T segments. Each segment \\( D_t \\) can be a sentence of text words \\( \\{w_i\\}_{i=1}^{I} \\), i.e. the language part such as instructions \\( D^{\\text{inst}} \\), memory \\( D^{\\text{mem}} \\) or thoughts \\( D^{\\text{tht}} \\). \\( D_t \\) can also be an image I, i.e. the vision part such as observations \\( D^{\\text{obs}} = I \\). Finally, \\( D_t \\) may belong to the action (behavior trajectory) part, i.e. \\( D^{\\text{bhv}} = \\{o_0, a_0, ...\\} \\). We assume these segments follow the ordering below (Figure 1):\n\\[D^{\\text{inst}}, D^{\\text{mem}}, D^{\\text{obs}}, \\underbrace{D^{\\text{tht}}, D^{\\text{bhv}}, D^{\\text{obs}}, D^{\\text{tht}}, D^{\\text{bhv}}, ..., D^{\\text{obs}}, D^{\\text{tht}}, D^{\\text{bhv}}}_{T}\\]\nWe tokenize such a sequence of segments into a series of tokens \\( \\{s_0, ..., s_M\\} \\) using the vision and language tokenizer offered by a pretrained MLM and the behavior tokenizer introduced in section 2."}, {"title": "3.2. Preparing Multimodal Interaction Data", "content": "In reality, many segments of the multimodal interaction \\( \\mathcal{D} \\) can be missing in public datasets. We consider the Minecraft contractor data released by OpenAI (Baker et al., 2022) and it only contains behavior trajectories \\( D^{\\text{bhv}} \\). Therefore, we need to properly augment the data with the additional textual segments including instructions \\( D^{\\text{inst}} \\), memory \\( D^{\\text{mem}} \\), and thoughts \\( D^{\\text{tht}} \\). We follow the prior practices (Huang et al., 2023; Liu et al., 2024) to synthesize the required text using LLMs. Below, we detail how each type of segment is constructed. More details can be found in appendix.\nSynthesis of instruction \\( D^{\\text{inst}} \\). The instruction is a high-level description of what task is being performed in the current interaction sequence. The considered OpenAI Minecraft data includes meta information of each gameplay video, which depicts fundamental events that happened during in Minecraft gameplay, e.g. what block was just destroyed, what entity was just killed, what item was just crafted, etc. Such meta-information can provide a basic overview of what the player has been through in the gameplay. We therefore prompt an LLM into summarizing the gameplay with the meta information. The summary will then be used as the instruction \\( D^{\\text{inst}} \\) of the current interaction trajectory.\nSynthesis of memory \\( D^{\\text{mem}} \\). The memory is the summary of what agents have finished in the previous interaction sequences. Due to the limited sequence length that the auto-regressive model can handle, the model needs to learn to summarize key information related to the task in historical interactions and ignore behaviors unrelated to instructions. The memory will be updated based on the results of each episode trunk and used for subsequent episode trunks. We therefore prompt an LLM into summarizing the gameplay with the meta information. The summary will then be used as the memory \\( D^{\\text{mem}} \\) of the current interaction trajectory. The memory prompt can be found in Appendix G."}, {"title": "3.3. Architecture, Training, and Inference of OmniJARVIS", "content": "As illustrated in Figure 3, OmniJARVIS is built upon a pretrained MLM. We augment the original vocabulary of the MLM with additional tokens from the behavior tokenizer. Specifically, as we adopted the [8, 8, 8, 6, 5] FSQ configuration (section 2), we augment with 8 + 8 + 8 + 6 + 5 = 35 new tokens as each behavior comprises 5 behavior tokens \\( \\text{sbhv}, ..., \\text{sbhv} \\) corresponding to 5 FSQ level. We formulate the learning objective of OmniJARVIS following (Brown et al., 2020; Raffel et al., 2020) in a prefix language modeling fashion. For a batch B of token sequence s, we optimize OmniJARVIS via:\n\\[\\mathcal{L}(\\theta, \\mathcal{B}) = - \\sum_{b=1}^{\\vert \\mathcal{B} \\vert} \\sum_{t=1}^{T}  \\log p_{\\theta} \\left( s_{t}^{(b)} \\vert s_{<t}^{(b)}, s_{\\text{prefix}} \\right),  \\ s_{\\text{prefix}} = \\text{tokenize}(s_{\\text{res}} \\vert s_{\\text{instruction}}, s_{\\text{memory}}, s_{\\text{observation}}), \\tag{3}\\]\nwhere \\( s_{\\text{prefix}} \\) denotes the prefix token, which is tokenized from the segments that served as context for reasoning and decision-making, i.e. instruction \\( D^{\\text{inst}} \\), memory \\( D^{\\text{mem}} \\) and observation \\( D^{\\text{obs}} \\) within the interaction sequence (Equation 2). The remaining tokens (tokenized from thought \\( D^{\\text{tht}} \\) and behavior trajectory \\( D^{\\text{bhv}} \\)) will be predicted in an autoregressive fashion. From a high level, OmniJARVIS is trained to reason (producing thought tokens) and act (producing behavior tokens) from contexts with task instructions, memory, and current observations. During inference, we begin with the feeding OmniJARVIS with a task instruction, an empty memory, and an initial observation. OmniJARVIS will produce a chain-of-thought as a means of reasoning and subsequently, emit behavior tokens for control. Every N steps, it is forced to reason again to produce new behavior tokens with the latest observation. We empirically set N = 32."}, {"title": "4. Capabilities and Analysis", "content": "4.1. Overview\nTraining details and Datasets. The training of the OmniJARVIS is divided into two stages. In the first step, we use a self-supervised training method to train a Behavior Tokenizer, including the Encoder and Decoder jointly. We use FSQ as a quantization method and build a codebook with 8*8*8*6*5 discrete codes. The training data for Behavior Tokenizer comes from Contractor Dataset (Baker et al., 2022), which is a collection of Minecraft gameplay videos. The training parameters and details remain consistent with GROOT, which can be found in Appendix A.\nIn the second stage, we use this behavior tokenizer to process Minecraft offline trajectories to obtain behavior token sequences. We add 35 (8+8+8+6+5) additional tokens to the MLM tokenizer as behavior tokens for unified representation, so each time the VLA needs to output a continuous sequence of 5 tokens to represent a complete behavior. We use GPT-3.5 to synthesize thought, memory, and instruction to raw offline datasets to build complete interaction data. The specific prompt can be found in Appendix F. These data collectively constitute the embodied instruction-following dataset of OmniJARVIS, including 600k trajectories and about 900M tokens."}, {"title": "6. Conclusion", "content": "We've presented OmniJARVIS, a novel VLA model that encompasses strong reasoning and efficient decision-making capabilities via unified tokenization of vision, language, and actions in multimodal interaction data. The key ideas are learning behavior tokenizer (trajectory encoder) and de-tokenizer (IL policy decoder) using self-supervised learning on behavior trajectories and autoregressive modeling of tokenized multimodal interaction data using a pretrained multimodal language model (MLM). Evaluations on the open-world Minecraft Universe demonstrate its impressive instruction-following capabilities. Possible future directions include a more in-depth investigation of behavior tokenization, language capabilities after VLA fine-tuning, and alignment concerns emerging from the unified interaction modeling and VLA capabilities."}]}