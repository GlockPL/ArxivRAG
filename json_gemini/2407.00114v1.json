{"title": "OmniJARVIS: Unified Vision-Language-Action Tokenization Enables Open-World Instruction Following Agents", "authors": ["Zihao Wang", "Shaofei Cai", "Zhancun Mu", "Haowei Lin", "Ceyao Zhang", "Xuejie Liu", "Qing Li", "Anji Liu", "Xiaojian Ma", "Yitao Liang"], "abstract": "This paper presents OmniJARVIS, a novel Vision-Language-Action (VLA) model for open-world instruction- following agents in Minecraft. Compared to prior works that either emit textual goals to separate controllers or produce the control command directly, OmniJARVIS seeks a different path to ensure both strong reasoning and efficient decision-making capabilities via unified tokenization of multimodal interaction data. First, we introduce a self-supervised approach to learn a behavior encoder that produces discretized tokens for behavior trajectories  $\\tau = \\{o_0, a_0, . . . \\}$ and an imitation learning (IL) policy decoder conditioned on these tokens. These additional behavior tokens will be augmented to the vocabulary of pretrained Multimodal Language Models (MLMs). With this encoder, we then pack long-term multimodal interactions involving task instructions, memories, thoughts, observations, textual responses, behavior trajectories, etc. into unified token sequences and model them with autoregressive transformers. Thanks to the semantically meaningful behavior tokens, the resulting VLA model, OmniJARVIS, can reason (by producing chain-of-thoughts), plan, answer questions, and act (by producing behavior tokens for the IL policy decoder). OmniJARVIS demonstrates excellent performances on a comprehensive collection of atomic, programmatic, and open-ended tasks in open-world Minecraft. Our analysis further unveils the crucial design principles in interaction data formation, unified tokenization, and its scaling potentials.", "sections": [{"title": "1. Introduction", "content": "Upon the success of pretrained Large Language Models (LLMs) (Brown et al., 2020; Cheng et al., 2024; Gong et al., 2023; OpenAI, 2023; Touvron et al., 2023) and Multimodal Langauge Models (MLMs) (Alayrac et al., 2022; Hinck et al., 2024; Liu et al., 2024; Man et al., 2024; Zhao et al., 2023), some recent works have been venturing into developing Vision-Language-Action (VLA) models (Brohan et al., 2023; Huang et al., 2023; Reed et al., 2022; Wang et al., 2023b), a promising pathway towards the ultimate goal of building autonomous agents that can follow and even self-generated instructions to fulfill various reasoning and acting tasks in open world environments. Among them, two most prominent architectures have been proposed: 1) Combining an off-the-shelf MLM (Alayrac et al., 2022; Liu et al., 2024) with separate goal-conditioned controllers (Cai et al., 2023a,b; Lifshitz et al., 2023), where MLM reasons, plans and pilots the controllers by producing textual goal instructions, e.g. DEPS (Wang et al., 2023c), JARVIS-1 (Wang et al., 2023b), voyager (Wang et al., 2023a); 2) Tuning a pretrained MLM into producing control commands directly, while maintaining the reasoning and language capabilities, e.g. RT-2 (Brohan et al., 2023), LEO (Huang et al., 2023). However, these two designs could still have significant drawbacks when it comes to open-world environments. First, an open world (e.g., Minecraft) usually teams up with an infinite number of complex and highly contextualized tasks (Fan et al., 2022; Lin et al., 2023), and it can be fairly challenging to depict them in text only. Therefore, VLA models that solely depend on text to communicate with the text-conditioned policies (Wang et al., 2023b,c) may fail to correctly pilot these controllers. On the other side, emitting the control command directly (Brohan et al., 2023; Huang et al., 2023) without invoking separate controllers could alleviate the aforementioned communication problem but given the long-horizon nature of open-world tasks, it is less practical to perform long-term control with a large VLA model as the context length requirement, computation cost and inference efficiency could become unaffordable.\nIn this paper, we aim to tackle the aforementioned issues of existing VLA models when facing open-world environments: complex & context-dependent tasks and long-term tasks. Our key insight originates from the observation of human decision-making: Given these open-world tasks, humans can make informed decisions via multi-round mental, verbal, and physical interactions (an illustration can be found in Figure 1). Therefore, if the VLA model can manage to learn from such interaction data, it may master the underlying human decision-making procedures. However, modeling interaction data is non-trivial: it is multi-modal, encloses vision (mostly observations), language (instructions, thoughts, etc.), and actions (behavior trajectories). Compared to the fruitful explorations on jointly tokenizing vision and language (Alayrac et al., 2022; Bavishi et al., 2023; Liu et al., 2024; Vinyals et al., 2015) into sequences for autoregressive modeling (Brown et al., 2020), tokenizing behavior trajectories (actions) is hard due to the following reasons. On the one hand, directly using low-level actions from the environment would pose huge challenges to the model's ability to process long sequences, which significantly hurts performance. It also hinders us from leveraging the planning ability of generative models. On the other hand, language-level action tokens require significantly more supervision and cannot accurately describe all possible actions."}, {"title": "2. A Tokenizer for Behaviors", "content": "As illustrated in Section 1, a key challenge for VLA is the mismatch between the action modality and other modalities such as the language instructions. A key insight is that a good amount of knowledge about the effects of actions can be learned directly from behavior trajectories  $\\left \\{ \\tau^{(i)} \\right \\}_i$. We propose to learn a behavior tokenizer in addition to the well-studied vision and language tokenizers to achieve unified tokenization of the vision, language, and actions in multimodal interaction data (Figure 1). We pose two main requirements to the behavior tokens. First, they should be able to express complete and diverse behavior from (short) trajectories. Further, the tokens should contain semantic information so that they are compatible with the other modalities, which enables the reasoning and planning ability of LLMs (e.g., by conducting chain-of-thought reasoning).\nSpecifically, we aim at producing a set of  $N$ discrete behavior tokens  $s^{\\text{bhv}}_1, ..., s^{\\text{bhv}}_N$ from a behavior trajectory  $\\tau = \\{o_0, a_0, . . . \\}$. Further, a de-tokenizer is needed to map these tokens back to an action rollout in the environment that reproduces the goal achieved in  $\\tau$. GROOT (Cai et al., 2023b) explores a VAE-based approach to jointly learn a latent representation of behavior trajectories and an imitation learning policy decoder that conditions the latent as goal. However, the continuous latent cannot be used as the behavior tokens as they can be more difficult to learn and decode with the existing discrete tokens of pretrained MLMs (Huang et al., 2023; Ma et al., 2022). Therefore, we replace the Gaussian latent in GROOT with an improved vector quantized discrete latent called Finite Scalar Quantization (FSQ) (Mentzer et al., 2023). We adopt a quantization configuration of [8, 8, 8, 6, 5], which means a code with a length=5 and a codebook size of 8\u00d78\u00d78\u00d76\u00d75 = 15360 is produced. The configuration is selected by a simple grid search. Overall, the behavior tokenizer (behavior encoder)  $e_\\varphi(o_{1:T})$ and the de-tokenizer (IL policy decoder)  $\\pi_\\theta(a_t|o_{1:t})$ is learned with the following objective:\n$\\text{argmin}\\_{e,\\theta} \\mathbb{E}\\_{\\tau \\sim D}  \\sum_{t=1}^T - \\text{log} \\pi\\_\\theta (a\\_t | o\\_{1:t}, f(e\\_\\varphi(o\\_{1:T})))   (1)$\nwhere  $f(.)$ denotes the finite scalar quantizer. We choose a non-causal (bidirectional) transformer and a causal transformer to parameterize the encoder  $e_\\varphi(o_{1:T})$ and the policy decoder  $\\pi_\\theta(a_t|o_{1:t})$, respectively. In practice, we set  $T = 128$ as the trunk size of the behavior trajectory to be encoded. We will discuss how to handle trajectories longer than 128 in the next section.\nCompared to our behavior tokenization, most prior work in VLA models, either represents the behavior trajectories in interaction data as a textual goal description and invokes a separate goal-conditioned controller (Wang et al., 2023b,c), or represents the state-action sequence  $\\left \\{ o\\_0, a\\_0, ... \\right \\}$ directly as in Decision Transformers (DT) (Brohan et al., 2023; Chen et al., 2021; Huang et al., 2023; Reed et al., 2022). Our approach offers a more compact but still informative representation of the actions part in multimodal interaction data. Moreover, the action readout, i.e. simply sending the behavior tokens to the policy decoder, is also more efficient than the DT-style direct control from VLA models (Brohan et al., 2023; Huang et al., 2023; Reed et al., 2022)."}, {"title": "3. Multimodal Interaction Data and OmniJARVIS", "content": "As illustrated in Figure 1, canonical multimodal interaction data comprises vision (observations), language (instructions, memories, thoughts), and actions (behavior trajectories). However, it can be difficult to directly collect such interaction data from human annotators. Therefore, we propose to convert an existing Minecraft gameplay dataset (Baker et al., 2022) into the multimodal interaction data required by OmniJARVIS. We begin with a formal definition of the interaction data, followed by our approach for data conversion and augmentation from existing datasets, and finish up with the architecture, formulation of learning on such interaction data, and inference procedure of OmniJARVIS. An overview of OmniJARVIS architecture and inference can be found in Figure 3."}, {"title": "3.1. Data Formation", "content": "An interaction sequence of decision-making  $D = \\{D\\_t\\}\\_{t=0}^T$ comprises  $T$ segments. Each segment  $D\\_t$ can be a sentence of text words  $\\left \\{w\\_i\\right \\}\\_{i=1}^n$, i.e. the language part such as instructions  $D^{\\text{inst}}$, memory  $D^{\\text{mem}}$ or thoughts  $D^{\\text{tht}}$.  $D\\_t$ can also be an image  $I$, i.e. the vision part such as observations  $D^{\\text{obs}} = I$. Finally,  $D\\_t$ may belong to the action (behavior trajectory) part, i.e.  $D^{\\text{bhv}} = \\{o\\_0, a\\_0, ...\\}$. We assume these segments follow the ordering below (Figure 1):\n$D^{\\text{inst}}, D^{\\text{mem}}, D^{\\text{obs}}, D^{\\text{tht}}, D^{\\text{bhv}}, D^{\\text{obs}}, D^{\\text{tht}}, D^{\\text{bhv}},\\dots   (2)$\nWe tokenize such a sequence of segments into a series of tokens  $\\left \\{ s\\_0, ..., s\\_M \\right \\}$ using the vision and language tokenizer offered by a pretrained MLM and the behavior tokenizer introduced in section 2."}, {"title": "3.2. Preparing Multimodal Interaction Data", "content": "In reality, many segments of the multimodal interaction  $D$ can be missing in public datasets. We consider the Minecraft contractor data released by OpenAI (Baker et al., 2022) and it only contains behavior trajectories  $D^{\\text{bhv}}$. Therefore, we need to properly augment the data with the additional textual segments including instructions  $D^{\\text{inst}}$, memory  $D^{\\text{mem}}$, and thoughts  $D^{\\text{tht}}$. We follow the prior practices (Huang et al., 2023; Liu et al., 2024) to synthesize the required text using LLMs. Below, we detail how each type of segment is constructed. More details can be found in appendix.\nSynthesis of instruction  $D^{\\text{inst}}$. The instruction is a high-level description of what task is being performed in the current interaction sequence. The considered OpenAI Minecraft data includes meta information of each gameplay video, which depicts fundamental events that happened during in Minecraft gameplay, e.g. what block was just destroyed, what entity was just killed, what item was just crafted, etc. Such meta-information can provide a basic overview of what the player has been through in the gameplay. We therefore prompt an LLM into summarizing the gameplay with the meta information. The summary will then be used as the instruction  $D^{\\text{inst}}$ of the current interaction trajectory.\nSynthesis of memory  $D^{\\text{mem}}$. The memory is the summary of what agents have finished in the previous interaction sequences. Due to the limited sequence length that the auto-regressive model can handle, the model needs to learn to summarize key information related to the task in historical interactions and ignore behaviors unrelated to instructions. The memory will be updated based on the results of each episode trunk and used for subsequent episode trunks. We therefore prompt an LLM into summarizing the gameplay with the meta information. The summary will then be used as the memory  $D^{\\text{mem}}$ of the current interaction trajectory. The memory prompt can be found in Appendix G."}, {"title": "3.3. Architecture, Training, and Inference of OmniJARVIS", "content": "As illustrated in Figure 3, OmniJARVIS is built upon a pretrained MLM. We augment the original vocabulary of the MLM with additional tokens from the behavior tokenizer. Specifically, as we adopted the [8, 8, 8, 6, 5] FSQ configuration (section 2), we augment with 8 + 8 + 8 + 6 + 5 = 35 new tokens as each behavior comprises 5 behavior tokens  $s^{\\text{bhv}}, ..., s^{\\text{bhv}}$ corresponding to 5 FSQ level. We formulate the learning objective of OmniJARVIS following (Brown et al., 2020; Raffel et al., 2020) in a prefix language modeling fashion. For a batch  $B$ of token sequence  $s$, we optimize OmniJARVIS via:\n$\\mathcal{L}(\\theta, B) = - \\sum\\_{b=1}^B \\sum\\_{t=1}^{T^{(b)}} \\text{log} p\\_\\theta(s\\_t^{(b)} | s\\_\\text{prefix}^{(b,t)})   (3)$\nwhere  $s\\_{\\text{prefix}}$ denotes the prefix token, which is tokenized from the segments that served as context for reasoning and decision-making, i.e. instruction  $D^{\\text{inst}}$, memory  $D^{\\text{mem}}$ and observation  $D^{\\text{obs}}$ within the interaction sequence (Equation 2). The remaining tokens (tokenized from thought  $D^{\\text{tht}}$ and behavior trajectory  $D^{\\text{bhv}}$) will be predicted in an autoregressive fashion. From a high level, OmniJARVIS is trained to reason (producing thought tokens) and act (producing behavior tokens) from contexts with task instructions, memory, and current observations. During inference, we begin with the feeding OmniJARVIS with a task instruction, an empty memory, and an initial observation. OmniJARVIS will produce a chain-of-thought as a means of reasoning and subsequently, emit behavior tokens for control. Every  $N$ steps, it is forced to reason again to produce new behavior tokens with the latest observation. We empirically set  $N = 32$."}, {"title": "4. Capabilities and Analysis", "content": "Training details and Datasets. The training of the OmniJARVIS is divided into two stages. In the first step, we use a self-supervised training method to train a Behavior Tokenizer, including the Encoder and Decoder jointly. We use FSQ as a quantization method and build a codebook with 8*8*8*6*5 discrete codes. The training data for Behavior Tokenizer comes from Contractor Dataset (Baker et al., 2022), which is a collection of Minecraft gameplay videos. The training parameters and details remain consistent with GROOT, which can be found in Appendix A.\nIn the second stage, we use this behavior tokenizer to process Minecraft offline trajectories to obtain behavior token sequences. We add 35 (8+8+8+6+5) additional tokens to the MLM tokenizer as behavior tokens for unified representation, so each time the VLA needs to output a continuous sequence of 5 tokens to represent a complete behavior. We use GPT-3.5 to synthesize thought, memory, and instruction to raw offline datasets to build complete interaction data. The specific prompt can be found in Appendix F. These data collectively constitute the embodied instruction-following dataset of OmniJARVIS, including 600k trajectories and about 900M tokens."}, {"title": "4.1. Overview", "content": "Interactive Dataset Format. We explore the crucial roles played by the different type of segments in interaction data, including the instruction, memory, thought, and caption tokens. The results can be found in Table 3, where we evaluate the loss on predicting the behavior tokens. It can be seen that instruction and thought can be more critical to the successful prediction of behavior tokens. This is consistent with our hypothesis \u2013 making informed decisions requires task instruction and reasoning."}, {"title": "4.2. Main Results I: Atomic Tasks", "content": "Atom tasks are various simple skills that agents in Minecraft need to master. They are basic tasks yet are fundamental skills that agents need to master during the learning process. We first evaluate OmniJARVIS with our learned behavior tokenizer on these tasks.\nWe select \"chopping trees\" \"digging dirt\" \"mining stones\" and \"collecting wheat seeds\" as the evaluation tasks. We directly take those short task descriptions as instructions for agents. We use text-conditioned VPT (Baker et al., 2022), Open-world Control (Cai et al., 2023a), STEVE-I (Lifshitz et al., 2023), and video-instructed GROOT (Cai et al., 2023b) as baselines. We compute the average rewards of different agents on every task in Figure 4 across 10 runs. By observing the environment and adjusting action tokens dynamically, OmniJARVIS effectively follows straightforward instructions across various scenarios. It consistently achieves a high average reward with minimal standard deviation."}, {"title": "4.3. Main Results II: Programmatic Tasks", "content": "To further verify the ability of OmniJARVIS to complete tasks with long sequences, we use 30 programmatic tasks to evaluate the performance of different agents. These tasks require the agent to start from an empty inventory in a new world until obtaining the final required items, which is usually a chain of atom tasks. These tasks are divided into five groups based on difficulty: wooden, food, stone, iron, and diamond. For example, the prompt for task \u201cObtain a diamond pickaxe\u201d is \u201cGive you nothing in the inventory, obtain a diamond pickaxe.\u201d This task requires more game time and more complex planning for up to 10 different intermediate items (Baker et al., 2022).\nBaselines are divided into two types. 1) directly outputs actions, namely the native behavior tokenizer, including STEVE-I (Lifshitz et al., 2023) and GROOT (Cai et al., 2023b). 2) using pretrained LLM as a planner to output language goals and connect the STEVE-I to execute these goals, including Zero-Shot Planner (GPT) (Huang et al., 2022a), ReAct (Yao et al., 2022), and DEPS (Wang et al., 2023c). We use success rate to evaluate the completion of tasks, that is, whether the task is completed within the specified time. The experimental results are listed in Table 1.\nProgrammatic Tasks usually require complex reasoning for planning. While STEVE-I and GROOT can only finish short skill-level tasks in atom tasks, is difficult to finish these programmatic tasks. Agents based on Language behavior tokenizer can complete complex tasks including diamond group ones, but with a low success rate. This is because these in-context learning methods leverage the pretrained LLM which may lack the necessary knowledge about this world. It is worth noting that in the Food group, agents based on Language Tokenizer have an average success rate of around 10%, as this set of tasks does not require complex reasoning. This indicates that Language-conditioned Tokenizers need additional language-conditioned trajectories as supervision for training while there was less such data available during STEVE-I's training phase leading to significant performance gaps. Meanwhile, OmniJARVIS uses a self-supervised trained behavior tokenizer which does not require extra language labels and hence receives more training resulting in good performance across a wider range of tasks. We will further prove this in the next set of Creative Task experiments."}, {"title": "4.4. Main Results III: Open-Ended Tasks", "content": "The open-ended tasks differ from programmatic tasks due to the lack of straightforward success criteria (Fan et al., 2022). We select the long-term creative tasks which usually need at least 5 minutes of human-playing time to finish. The task prompts can be found in Appendix D. Following image generation and video generation tasks (Heusel et al., 2017; Unterthiner et al., 2018), we take the Fr\u00e9chet Sequence Distance (FSD) metrics to evaluate the correlation between agent rollout video and creative instruction. Specifically, we first ask human experts to finish the creative task prompts."}, {"title": "4.5. Insights and Analysis", "content": "Vision Tokenization. We also evaluate training OmniJARVIS with different vision tokenization, including ImageCaptioner + LLaMA2-7B (Chen et al., 2023; Touvron et al., 2023) (basically converting the vision input into textual captions), fuyu-8b (Bavishi et al., 2023), and LLaVA-7B (Liu et al., 2024) architecture. For the ImageCaptioner+, we fix the ImageCaptioner models and only fine-tune the language model, i.e., LLaMA2-7B. We use the prediction loss of behavior tokens as the evaluation criterion, namely eval loss. We found that the model trained with LLaVA-7B architecture has the lowest evaluation loss, so we chose this model as the default model.\nBehavior Tokenizer. We explore OmniJARVIS with different behavior tokenizers, including the default setting using FSQ codebook, a variant of using VQ-VAE instead of FSQ (Van Den Oord et al., 2017), and simply using sub-goal language annotation as behavior \"tokens\". The evaluation results on 4 programmatic tasks are listed in Table 3. Using an FSQ tokenizer is generally better than a language goal, which confirms the advantages of using a tokenized behavior over language descriptions of behavior. The use of VQ-VAE as a quantized behavior tokenizer collapsed during the training process, so there were no results in all test tasks.\nBehavior Codebook. We conduct an in-depth investigation of behavior tokenizers with varying codebook sizes, utilizing recommended sets of FSQ levels to approximate specified codebook dimensions (Mentzer et al., 2023) as delineated in Table 4. We evaluate performance across multiple metrics for each codebook size. Codebook Usage is quantified as the proportion of codewords utilized at least once when encoding the validation datasets. Reconstruction FSD is measured by the FSD scores derived from the MineCLIP encoder (Fan et al., 2022), processing 1,000 different demonstration videos through the FSQ-GROOT and subsequent rollout in a randomly generated environment. Additionally, we measure Resampling FSD, which is the FSD score obtained when the environment rollout is conditioned on representations sampled from the codebook. Finally, we assess the average rewards for the task \u201ccollect wood\" using OmniJARVIS across varying codebook sizes. Our findings indicate that increases in codebook size correlate with enhanced average rewards and reduced FSD scores, suggesting a scalable performance in OmniJARVIS with larger codebooks.\nBehavior Semantics. We provide some qualitative analysis on the learned FSQ-based behavior tokenizer. In Figure 5, we tokenize several reference videos, then feed the behavior tokens to the policy decoder and see if it can accomplish the same task as in reference videos. The results indicate that our behavior tokenizer is able to capture such behavior semantics and offers rich task information."}, {"title": "4.6. Scaling Potential of OmniJARVIS", "content": "We investigate the scaling effect (Kaplan et al., 2020; Lin et al., 2024) of data and model in OmniJARVIS by monitoring the instruction-following loss on the validation set as the amount of data increases. In addition to fine-tuning from the default LLaVA-7B, we include two additional scales: OmniJARVIS-2B (fine-tuned from LLaVA-2B with Gemma-2B language models (Hinck et al., 2024)) and OmniJARVIS-13B (fine-tuned from LLaVA-13B with LLaMA2-13B language models (Liu et al., 2024)).\nThe validation loss curves in Figure 6 reveal the following insights: 1) When using Omni-Tokenizer, OmniJARVIS's instruction tuning aligns with the scaling law (Kaplan et al., 2020). All curves exhibit a log-linear decrease as the data scale increases. 2) Scaling up VLM consistently enhances performance. Notably, OmniJARVIS-7B demonstrates significantly lower losses compared to OmniJARVIS-2B. However, while improvements are consistent, the difference between OmniJARVIS-7B and OmniJARVIS-13B seems less pronounced, hinting at potential saturation when further scaling up VLM. This underscores both the scalability of OmniJARVIS and the importance of increasing data volume to match the model."}, {"title": "5. Related Works", "content": "Pretrained Language Models for Decision-making. Several works have explored leveraging LLMs to generate action plans for high-level tasks in embodied environments (Brohan et al., 2022b; Huang et al., 2022a; Liang et al., 2022; Zhang et al., 2023). To better perform complex planning in the environment, existing methods usually utilize chain-of-thought (Wei et al., 2022) or related methods (Yao et al., 2022). To better cope with uncertainties in open worlds, some LLM-based methods generate plans interactively with human and environmental feedback (Huang et al., 2022b; Shinn et al., 2023; Wang et al., 2023c) and retrieving from memory (Wang et al., 2023b) or internet corpus (Wang et al., 2024). However, those plans can only be executed in a language environment or require an additional controller or code executor to interact in an open world.\nVision-Language-Action Models. In order to better utilize the knowledge inside the language model for decision-making, some methods tend to use decision datasets to fine-tune pretrained language models (Driess et al., 2023; Durante et al., 2024). Gato (Reed et al., 2022) was among the first to tokenize environment-provided actions to enable joint sequential modeling across modalities. PaLM-E (Driess et al., 2023) generates high-level instructions as texts and uses dedicated controllers to perform the task described by the output instructions. The RT series focuses more on robotics settings. Specifically, RT-1 pairs a VLM with a language-conditioned controller; RT-2 extends the VLM to directly include control tokens; RT-X generalizes to new robots and environments. A recent VLA model LEO (Huang et al., 2023) expands the perception from 2D images to 3D world and enables rich scene-level reasoning and control tasks.\nOpen-world Agents in Minecraft. As LLMs have achieved remarkable reasoning results and un- derstanding capabilities across various domains, the year 2023 has witnessed researchers adopting multiple LLM-based approaches to create open-world agents in Minecraft (Wang et al., 2023a,b,c;"}, {"title": "6. Conclusion", "content": "We've presented OmniJARVIS, a novel VLA model that encompasses strong reasoning and efficient decision-making capabilities via unified tokenization of vision, language, and actions in multimodal interaction data. The key ideas are learning behavior tokenizer (trajectory encoder) and de-tokenizer (IL policy decoder) using self-supervised learning on behavior trajectories and autoregressive modeling of tokenized multimodal interaction data using a pretrained multimodal language model (MLM). Evaluations on the open-world Minecraft Universe demonstrate its impressive instruction-following capabilities. Possible future directions include a more in-depth investigation of behavior tokenization, language capabilities after VLA fine-tuning, and alignment concerns emerging from the unified interaction modeling and VLA capabilities."}, {"title": "A. Training Details", "content": "OmniJARVIS. We utilized the SFTTrainer class from the TRL library by Hugging Face to train the VLM model. The learning rate was set at 1.4e-5, and a cosine learning rate scheduler was employed. The weight decay parameter was set to 0 with a warm-up ratio of 0.03. Training took place on 8 A800 GPUs with FSDP, with a batch size of 2 and gradient accumulation steps of 4 using bf16 precision. The training lasted for one epoch on our generated dataset. The raw interaction dataset comes from the sections 6xx, 7xx, and 10xx of the contractor dataset provided by OpenAI (Baker et al., 2022) and the recording interactions of JARVIS-1 Agents (Wang et al., 2023b).\nBehavior Tokenizer. Each frame in our experiments has a resolution of 128x128 pixels. We segmented each episode into multiple trunks, with each trunk consisting of 128 frames. The learning rate was set at 0.00004, with a weight decay of 0.001. The batch size was configured to 2, and training was conducted on a cluster of eight NVIDIA 3090 Ti graphics cards. The training dataset comprised sections 6xx, 7xx, 9xx, and 10xx of the contractor dataset provided by OpenAI (Baker et al., 2022). The precision for training was set to bfloat16."}, {"title": "B. FSD Computation", "content": "This section outlines the computation of FSD score between the generated videos and human gameplay recordings. First, we divide the videos into trunks of 128 frames. For each segment, we sample 16 frames, with 8 frames in between each sampled frame. These sequences of 16 frames are then fed through the video encoder of MineCLIP (Fan et al., 2022) to obtain 512-dimensional video embeddings. Finally, the score is calculated according to (Heusel et al., 2017) between the embeddings of the generated videos and the reference videos.\nWe compute FSD scores between and within sets of videos using three distinct tasks, as illustrated in Figure B.1. A noticeable gap exists between the FSD scores calculated within the same set of videos and those calculated between different sets. Furthermore, the metric exhibits relative insensitivity to the number of videos used for computing the score, demonstrating the validity of our proposed metric."}, {"title": "C. Embodied Question Answering Benchmarks", "content": "The embodied question-answering benchmarks consist of questions and instructions for Minecraft benchmarks, consisting of over 100 questions on knowledge question answering, embodied planning, and math reasoning."}, {"title": "H. Examples of OmniJARVIS Interaction Process", "content": "The tokens with blue color are prompted from the environment observation and user, and the tokens with red color are generated by OmniJARVIS."}, {"title": "G. Prompt for Memory Summarization", "content": "Prompt G.1:Prompt for Memory Summarization\nA player is playing Minecraft. The situation of the player contains 4 parts: task, state, inventory and memory. Under this situation, the player will take a behavior. And after this behavior, the player's memory will be updated to \"Updated Memory\". I need you to give a subpart of the player's updated memory that is most relevant to its task.\nTask is the goal of the player. State describes the image the player is facing, Inventory is its current inventory of items. Memory contains its past behaviors, each item in memory is its past behavior and the number of this behavior. The memory is sorted by time, with the most recent behavior at the end. There are mainly 9 types of behavior:\n+ 'craft_item:x' means to craft an item x;\n+ 'drop:x' means to drop an item x;\n+ 'use_item:x' means to use an item x;\n+ 'pickup:x' means to pickup an item x;\n+ 'custom' means to custom its playing status;\n+ 'mine_block:x' means to mine a block x;\n+ 'kill_entity:x' means to kill an entity x;\n+ 'entity_killed_by:x' means the player is killed by an entity x;\n+ 'break_item:x' means an item x got broken.\nHere is the player's current situation:\nTask: {task}\nState: {state}\nInventory: {inventory}\nBehavior: {behavior}\nUpdated Memory: {updated_memory}\nI need you to summarize what the player has done to complete the task according to the updated memory. Please make sure every part in your summary is relevant to the task. The output format should be: \"The player first ..., then ..., and finally ...\" Then in a new line, try to summarize which stage of the task the player is in according to the memory."}, {"title": "F. Prompt for Thought Generation", "content": "Prompt F.1:Prompt for Thought Generation\nA player is playing Minecraft. I need you to give thought about what behavior it should take next given current situation. Here are some demonstrations:\nTask: {task}\nState: {state}\nInventory: {inventory}\nMemory: {memory}\nThought: {}\nBehavior: {behavior}\nGiven current situation and the behavior the player will take, output a simple thought that will directly lead to this behavior. Please carefully revise the need of the task, current inventory and recent memory of the player. Be sure to explain every part of the behavior. The output format should be \"Thought: reason...So the behavior should be {behavior}\"."}]}