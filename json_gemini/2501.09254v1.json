{"title": "Clone-Robust AI Alignment", "authors": ["Ariel D. Procaccia", "Benjamin Schiffer", "Shirley Zhang"], "abstract": "A key challenge in training Large Language Models (LLMs) is properly aligning them with human preferences. Reinforcement Learning with Human Feedback (RLHF) uses pairwise comparisons from human annotators to train reward functions and has emerged as a popular alignment method. However, input datasets in RLHF are not necessarily balanced in the types of questions and answers that are included. Therefore, we want RLHF algorithms to perform well even when the set of alternatives is not uniformly distributed. Drawing on insights from social choice theory, we introduce robustness to approximate clones, a desirable property of RLHF algorithms which requires that adding near-duplicate alternatives does not significantly change the learned reward function. We first demonstrate that the standard RLHF algorithm based on regularized maximum likelihood estimation (MLE) fails to satisfy this property. We then propose the weighted MLE, a new RLHF algorithm that modifies the standard regularized MLE by weighting alternatives based on their similarity to other alternatives. This new algorithm guarantees robustness to approximate clones while preserving desirable theoretical properties.", "sections": [{"title": "Introduction", "content": "As the reasoning capabilities of Large Language Models (LLMs) improve and as LLMs begin to play a larger role in society, it is increasingly important for LLMs to be aligned with human values. One common method used for AI alignment is Reinforcement Learning with Human Feedback (RLHF). In RLHF, a human annotator is typically shown two answers to a prompt, and asked to report which answer they prefer. This process is repeated across many annotators and potentially many types of questions and answers, and results in a large dataset of pairwise comparisons. An RLHF algorithm then takes the pairwise comparison dataset as input and outputs a reward function that assigns values to answers. One reason why RLHF is an appealing technique is because of the ease of data elicitation, as it is simpler for humans to pick a favorite between two answers than it is to rank many answers or provide good 'role model' answers for the LLM.\nFundamentally, the mandate of RLHF algorithms is to solve a preference aggregation problem, where the goal is to find a reward function that best aligns with the values of the general population, given pairwise comparison data. This goal is complicated by the fact that humans often have diverse preferences, and may not agree on the best answer to a question. Luckily, the study of how to aggregate diverse preferences is not a new area in computer science, but one that has been long explored by researchers in social choice theory. Classic social choice considers settings with sets of voters and sets of alternatives, where each voter provides a ranking over alternatives. These rankings are then provided as input to a voting rule, which outputs a summary of the voters' preferences (such as a single winner or an overall ranking). Social choice studies the design and analysis of such voting rules.\nIn the RLHF setting, the \u2018voters' are the human annotators, who provide pairwise preferences over the 'alternatives'. There are several reasonable choices for how to define an 'alternative' in RLHF. Perhaps the simplest definition is that an alternative is just a single answer. However, many RLHF datasets in practice contain answers to multiple questions. Therefore, an alternative could also be a question-answer pair. Finally, answers (and questions) are often generated by various LLMs, so an alternative could also be viewed as the LLM model which generated the answer. Whichever the case, an RLHF algorithm would then be the 'voting rule' which takes as input the preference data and outputs a summary of the preferences - in this case, a single reward function. The close relationship between RLHF and voting theory means that we can take inspiration from past work in social choice to both anticipate potential pitfalls in RLHF and design better RLHF algorithms.\nOne especially relevant potential pitfall is that input datasets in RLHF are not necessarily balanced in the types of questions and answers that are included. For example, at Anthropic, questions are generated by crowdworkers, who have the flexibility to communicate with LLMs on any topic of interest (Bai et al., 2022). In ChatGPT, answers are typically generated by LLMs, and depending on how the LLM was trained, some answers may look more similar than others (OpenAI, 2022). Therefore, it would be ideal for an RLHF algorithm to not be sensitive to near duplicates and to perform well even on unbalanced datasets. This is important for at least two reasons. First, RLHF algorithms which are not robust to near duplicates will require more careful design of the input dataset, which may increase cost and"}, {"title": "Our Results", "content": "To address the first goal, we show that the standard RLHF algorithm, which uses the regular-ized maximum likelihood estimator (MLE), is not robust to approximate clones. In response to the second goal, we propose a new algorithm for RLHF that we call the weighted MLE. Intuitively, the weighted MLE adjusts the objective function of the MLE by down-weighting alternatives that are similar to other alternatives (and therefore provide less new informa-tion) and up-weighting alternatives that are different than other alternatives (and therefore provide more new information). Our main result is that the weighted MLE is robust to approximate clones; we also demonstrate that it retains many of the clean interpretations of the regularized MLE.\nIn addition to our main result about independence of clones, we also prove an impossibility result for RLHF in the presence of diverse preferences. We show that for any RLHF algo-rithm, there exists a population such that even with constant scaling, the distance between the RLHF algorithm output and the mean rewards of the population is arbitrarily large. We show this for a population that consists of a mixture of only two Bradley-Terry-Luce (BTL) models, thereby highlighting the inherent difficulty of aggregating preferences of populations"}, {"title": "Related Work", "content": "RLHF has recently gained traction as a popular method of aligning LLMs with human preferences (Bai et al., 2022; Ouyang et al., 2022; Ziegler et al., 2019). The potential benefits of applying social choice theory to the RLHF setting have not gone unnoticed. Recent work has mapped classic social choice concepts to RLHF (Dai and Fleisig, 2024) and extended social choice axioms for RLHF (Ge et al., 2024), considered personalization as a way to address diversity (Poddar et al., 2024; Park et al., 2024), and studied other methods of aggregating diverse preferences (Zhong et al., 2024; Swamy et al., 2024).\nOur work particularly focuses on the social choice concept of independence of clones, which was first introduced by Tideman (1987). Follow-up work has studied manipulation by cloning and clone structures (Elkind et al., 2010, 2012), and recent work has studied an even stronger notion, obvious independence of clones (Berker et al., 2022). In a position paper, Conitzer et al. (2024) propose various high-level directions for applying social choice to RLHF, and in particular identify independence of clones as a desirable property for RLHF algorithms, because chatbot responses may be very similar to each other. They point out that Borda count, a voting rule which is implicitly used in current approaches to RLHF (Siththaranjan et al., 2023), is not independent of clones. In our work, we elaborate on this insight by considering approximate clones and providing specific instances for which standard RLHF algorithms are not robust to approximate clones.\nWe highlight several papers that are especially related to ours, and include more details in Appendix A. Like us, Xu et al. (2023) are concerned about duplicates in answers shown to annotators, but unlike us, their results are primarily for dichotomy models and three-way comparisons. Also like us, both Siththaranjan et al. (2023) and Chakraborty et al. (2024) give different forms of impossibility results for RLHF algorithms in the presence of diverse preferences.\nFurther afield, recent papers have considered other forms of robustness in RLHF, such as robustness to incorrect or corrupted data (Bukharin et al., 2024; Mandal et al., 2024). In"}, {"title": "Model", "content": "Suppose we have a set of annotators $N = [n]$ and an infinite set of all possible alternatives $S$, where $|S|$ is the volume of $S$. Each alternative $s \\in S$ has a context $c(s) \\in \\mathbb{R}^d$ which represents important features of the alternative. For notational convenience, we will often refer to the context $c(s)$ simply by $s$. We only observe a finite subset of alternatives $M = [m] C S$.\nEach annotator $i$ has a reward function $r_i: M \\rightarrow \\mathbb{R}$, where $r_i(x)$ represents the reward of annotator $i$ for alternative $x$.\nGiven two alternatives, an annotator expresses a preference over the two alternatives based on their reward function. As is common in RLHF, we assume that the expressed preferences of annotators follow a Bradley-Terry-Luce (BTL) model (Bradley and Terry, 1952; Bai et al., 2022), in that an annotator $i$ states a preference for alternative $x_1$ over alternative $x_2$ with probability\n$P_i(x_1 > x_2) = \\frac{e^{r_i(x_1)}}{e^{r_i(x_1)} + e^{r_i(x_2)}}.$\nThe BTL model takes into account the fact that annotator preferences may be noisy or inconsistent across queries, especially when the reward gap of two alternatives is small. When annotators are drawn randomly, we then denote the expected probability of seeing $x_1$ preferred to $x_2$ as $p^*(x_1 > x_2) = E_i [P_i(x_1 > x_2)]$.\nWe assume that annotator reward functions are Lipschitz continuous in the Euclidean dis-tance between the context of two alternatives, as stated below.\nAssumption 2.1. For all players $i \\in N$, the reward function $r_i$ is Lipschitz continuous with parameter $K > 0$. Formally, for any $i \\in N$ and any $x_1,x_2 \\in S$, $|r_i(x_1) - r_i(x_2)| \\leq K||x_1 - x_2||_2$.\nLet a query be a pairwise comparison $q = {x_1,x_2}$, where $x_1, x_2 \\in M$, and let a set of queries be denoted $Q$. For every $x_1, x_2 \\in M$, we assume that ${x_1,x_2}$ is included in $Q$ at least once. For $x_1, x_2 \\in M$ and $i \\in N$, define the random function $f_{BTL}({x_1,x_2},i) \\in {x_1,x_2}$ such that $Pr(f_{BTL}({x_1,x_2},i) = x_1) = P_i(x_1 > x_2)$. In other words, $f_{BTL}(q, i)$ is one sample of annotator i's preference between $x_1$ and $x_2$ according to annotator i's true rewards for $x_1$ and $x_2$ in the BTL model. Further let $f_{BTL}(q) = f_{BTL}(q, i')$ when $i'$ is drawn uniformly at random from $N$. A preference dataset $D(Q)$ is generated from $Q$ by choosing an annotator uniformly at random for each query and sampling that annotator's preference over the alternatives in that query, i.e. $D(Q) = {(q, f_{BTL}(q)) : q \\in Q}$.\nFor a given preference dataset $D$, define $P_D(x_1 > x_2)$ as the proportion of time that $x_1$ is preferred to $x_2$ in $D$. We say that a dataset $D$ is representative if $P_D(x_1 > x_2) = p^*(x_1 > x_2)$ for all $x_1, x_2 \\in M$. An RLHF algorithm ALG takes as input a preference dataset $D$ and returns a reward function $r(\\cdot)$ where $r : M \\rightarrow \\mathbb{R}$. Note that ALG does not know any"}, {"title": "MLE with Diverse Preferences", "content": "We first consider the setting with only one annotator (n = 1). When n = 1, the query data is generated from a single BTL model. A natural solution is to estimate the unknown reward function $r^*$ as the reward function that best matches the data in $D$. Using the Kullback-Leibler divergence $KL(\\cdot)$ as the distance metric, the reward function that best approximates the data in $D$ (minimizes the KL divergence) when n = 1 is\n$\\hat{r}^D := \\arg \\min_r - \\sum_{x_1,x_2 \\in M} P_D (x_1 > x_2) \\log (\\frac{e^{r(x_1)}}{e^{r(x_1)} + e^{r(x_2)}}).$\nWhen the number of samples for every pair of alternatives is the same, $\\hat{r}^1$ is exactly the MLE solution for RLHF. In RLHF, maximum likelihood estimation refers to finding the rewards for the single BTL model that has the highest probability of generating the observed data.\nFurthermore, because the true underlying distribution is a single BTL model when n = 1, standard MLE theory implies that $\\hat{r}^1$ will converge to $r^* := E_i[r] = r_1$ as the number of comparisons for each pair of alternatives goes to infinity (Zhu et al., 2023).\nHaving $\\hat{r}^D$ converge to $E_i[r]$ is a natural goal in RLHF, as this means that the reward function converges to the mean rewards for the underlying population. Unfortunately, when n > 1, no RLHF algorithm can accurately recover $E_i[r]$ for every possible population even if the algorithm is given infinite query data. This negative result holds even for n = 2 and m = 2 and is formally proven in Theorem 2.2 in terms of Euclidean distance. Note that there is an additive constant in this result because the BTL model is invariant to additive constants. Furthermore, Theorem 2.2 does not contradict the positive results of Zhang et al. (2022), as their results require sufficiently many alternatives in order to make the problem identifiable.\nTheorem 2.2. Let n = 2 and suppose D is a representative preference dataset over alter-natives in M. Then for any algorithm ALG and any C > 0, there exist $r_1$ and $r_2$ such that $r^D := ALG(D)$ satisfies\n$\\min_\\alpha (\\sum_{x \\in M} (\\frac{r_1(x) + r_2(x)}{2} - r^D(x) - \\alpha)^2) > C.$"}, {"title": "Average Win Rate and Borda Count", "content": "In addition to the interpretation discussed above, Siththaranjan et al. (2023) showed that the order in which the alternatives are ranked by the regularized MLE is the same as the order in which the alternatives are ranked by the average win rate. In Theorem 2.3, we show an even stronger relationship between the regularized MLE and the average win rate, which is that the regularized MLE is the unique solution to a system of equations involving the empirical average win rates. This gives additional interpretability to the regularized MLE, as the regularized MLE is therefore similar to an M-estimator where the m moments correspond to the win rates of the m alternatives.\nDefinition 2.2. For a dataset D over alternatives M, the average win rate of alternative $x \\in M$ is\n$AWR_D(x) := \\frac{1}{m} \\sum_{y \\in M} P_D(x > y).$\nTheorem 2.3. Let $\\hat{r}^D$ be the regularized MLE as defined in Equation (1) and define\n$AWR(x) = \\sum_{y \\in M} \\frac{e^{\\hat{r}^D (x)}}{e^{\\hat{r}^D (x)} + e^{\\hat{r}^D (y)}}.$\nThen $\\hat{r}^D$ is the solution to the system of equations\n$AWR_D(x) = A\\hat{r}(x) + AWR(x) \\forall x \\in M.$"}, {"title": "Robustness to Approximate Clones", "content": "In this section, we adapt the concept of independence of clones from social choice to the RLHF setting. In traditional social choice, independence of clones is a desirable characteristic of voting rules which intuitively states that the winner of an election remains the same when duplicates of candidates are added to the candidate pool. The Borda count voting rule, which is closely related to the MLE in RLHF (see Section 2.2), does not satisfy independence of clones. See Appendix C.1 for a formal definition of independence of clones in traditional social choice.\nWe adapt the traditional independence of clones definition for RLHF. Informally, we say that an RLHF algorithm is robust to approximate clones if adding new alternatives that are clones of existing alternatives does not significantly change the reward function that is output by the RLHF algorithm. Note that robustness to approximate clones in RLHF guarantees stability of the reward function instead of merely the winner, and is therefore a stronger notion. As an RLHF algorithm only has access to noisy observations regarding human preferences, we will also only require reward function stability when we have representative datasets, or in other words, in cases when the empirical pairwise win rates are the same as the true pairwise win rates. If the dataset is not representative, it is not necessarily desirable that the reward function is unchanged when a clone is added because there may be value in generating a larger dataset. When the dataset contains sufficiently many queries, the empirical pairwise win rates will approximately equal the true pairwise win rates by the law of large numbers. Additional justification of our definition of robustness to approximate clones in RLHF can be found in Appendix C.2.\nWe are now ready to formally present our definition of robustness to approximate clones for RLHF.\nDefinition 3.1 (Robust to Approximate Clones). An algorithm ALG is robust to approx-imate clones if for every $M \\subseteq S$ and $\\delta > 0$ there exists an $\\epsilon > 0$ such that the following holds. Suppose $M' = M \\cup {x'}$, where $x' \\in S$ and $\\exists x \\in M$ such that $||x - x'|| < \\epsilon$. Let D be a representative dataset of queries over the alternatives M and let D' be a representative dataset of queries over the alternatives in M'. Let $r = ALG(D)$ and let $r' = ALG(D')$. Then $|r'(x) - r'(x')| \\leq \\delta$ and for all $x \\in M$, $|r(x) - r'(x)| \\leq \\delta$.\nInformally, a RLHF algorithm satisfies Definition 3.1 if adding a new alternative whose context is \u201cvery close\u201d to that of an existing alternative does not significantly change the reward function. This is desirable because if the players' values are Lipschitz continuous,\na new alternative whose context is very similar to that of an existing alternative provides"}, {"title": "Weighted MLE", "content": "In this section, we propose a modified version of the regularized MLE which satisfies Defini-tion 3.1 while maintaining the interpretability inherent to the original MLE.\nThe main idea of the proposed algorithm is to modify the objective function by weighting each alternative by how unique that alternative is compared to the other alternatives in M. Therefore, an alternative with context very similar to the context of other alternatives will have a smaller weight, while an alternative with a context very different than the context of other alternatives will have a larger weight. Informally, the weight of an alternative y is the fraction of alternatives in S that are closer to y than to any other alternative in M (with ties split evenly among all tied alternatives). We define the weights formally in Definition 4.1.\nDefinition 4.1. For any set of alternatives $M \\subset S$ and any $x \\in S$, define $proj_M(x) = \\arg \\min_{y \\in M} ||x - y||_2 \\subset M$. For $y \\in M$, define\n$w_M(y) = \\frac{1}{|S|} \\int_{s \\in S} \\frac{I[y = proj_M(x)]}{|proj_M(x)|} dx.$\nNote that by this construction, $\\sum_{y \\in M} w_M(y) = 1$. Using these weights, we define the weighted MLE as $\\hat{r}^D_w = \\arg \\min_r f_D(r)$, where\n$f_D(r) = - \\sum_{x_1, x_2 \\in M} w_M(x_1)w_M(x_2)P_D(x_1 > x_2) \\times \\log (\\frac{e^{r(x_1)}}{e^{r(x_1)} + e^{r(x_2)}}) + \\frac{\\lambda}{2} \\sum_{x \\in M} w_M(x)r(x)^2.$\nIntuitively, $f_D(r)$ down-weights terms involving alternatives that provide less new informa-tion because they are very similar to other alternatives. Consequently, two alternatives that are approximate clones of each other will both have smaller weights.\nWe are now ready to state our main result, which is that the weighted MLE is robust to approximate clones\nTheorem 4.1. Under Assumption 2.1, the algorithm $ALG(D) = \\hat{r}^D_w$ is robust to approximate clones.\nWhile we defer the formal proof of Theorem 4.1 to Appendix G, we will next provide some intuition for why this result holds. Consider the Voronoi diagram of the alternatives in M, which consists of a partitioning of S into regions, where each region corresponds to all of the points in S that are closest to some alternative in M. For example, if S = [0, 1] \u00d7 [0, 1] and M = {(0,0), (1, 0), (1, 1)}, then we can draw the Voronoi diagram in Figure 1."}, {"title": "Case Study", "content": "Although our contributions are primarily theoretical, we supplement our results with a syn-thetic case study that highlights an instance where the weighted MLE performs better than the standard regularized MLE under diverse preferences. This case study moves our theory closer to practice in a few different ways. First, LLMs typically generate the responses seen by human annotators, and so our case study considers textual responses generated by the gpt-40-mini model. Note that this means each alternative has only one data point, unlike our theoretical results where we assume sufficiently many comparisons for every pair of alterna-tives. This better represents what happens in practice when each pair of responses is newly generated by an LLM at the point when an annotator is asked to report a preference Bai et al. (2022). LLMs have also been shown to be effective as implicit computational models of humans (Horton, 2023), and we therefore use an LLM as a stand-in for human annotators with diverse preferences rather than assuming humans make decisions using a BTL model.\nIn the case study, our goal is to train a reward function which evaluates answers to a single question: 'Describe Paris'. We use OpenAI's gpt-40-mini model both to generate textual descriptions of Paris and to simulate human annotators with diverse preferences. We consider a population with three types of annotators, each of which attach a different amount of importance to seeing the topics of food, art, and romance mentioned in a description of Paris. We then construct two preference datasets for this population, one which includes additional approximate clones (\u2018Clones') and one which does not ('Original'). More details on the annotator population and the dataset generation process can be found in Appendix I.\nWe approximate both the standard MLE algorithm and the weighted MLE algorithm using neural networks. Each neural network takes as input a context vector and outputs a reward value. To generate the context vectors, we use OpenAI's text-embedding-3-small model to extract embedding vectors from the textual descriptions of Paris. More details on the neural network training process can be found in Appendix G. We run each algorithm on both datasets described in the previous paragraph and observe how the reward function output by the algorithm changes across datasets. To visualize the change in reward function, we evaluate each reward function on all of the alternatives, and then plot the mean reward for three types of answers (food, art, and romance), with error bars corresponding to the sample standard deviations."}, {"title": "Discussion", "content": "In this section we discuss some limitations of our findings and outline potential directions for future research.\nThere is ample opportunity for further empirical research. Our case study is meant to highlight a specific instance where clones cause a problem for the standard RLHF methods, but does not imply any conclusions regarding how frequent or pervasive clones may be in"}, {"title": "Additional Related Work Details", "content": "In this section, we provide a more comprehensive comparison to several works that are especially related to ours. Like us, Xu et al. (2023) are concerned about the performance of current RLHF algorithms in the presence of duplicates. Like us, they show that there are simple models for human preferences under which the standard RLHF algorithms perform badly. Unlike us, their results are for a specific class of models they call dichotomy models. In such models, there are two types of messages and two types of individuals, and each type of individual has reward 1 for one type of message and reward 0 for the other. Their main results also focus on three-way comparisons, while our results deal with pairwise comparisons (which are standard in current RLHF algorithms).\nWe were inspired by Siththaranjan et al. (2023), who show that standard RLHF implicitly aggregates over hidden context according to Borda count. We build off their work to show that standard RLHF algorithms are not robust to approximate clones. Like Siththaranjan et al. (2023), we assume that humans have diverse preferences, but unlike them, we do not summarize these preferences by a hidden context. Rather, we directly model populations with different reward functions. Note that when we refer to \u2018context' in our paper, we are referring to the underlying context of alternatives, not of annotators as in Siththaranjan et al. (2023). Like us, Siththaranjan et al. (2023) give an impossibility result for RLHF algorithms in the presence of diverse preferences. They show that every RLHF algorithm fails to exactly recover the mean reward function for some population, while our result shows that every RLHF algorithm does arbitrarily badly at finding the mean reward function for some populations.\nChakraborty et al. (2024) also evaluate the efficacy of standard RLHF algorithms when there are diverse human preferences and give an impossibility result for when standard RLHF outputs a single reward function. However, their impossibility result is of a different form - they bound the gap between the optimal policy overall and the optimal policy for a subpopulation by the sum of total variation distances between preference distributions of subpopulations. By contrast, our impossibility result states that for any RLHF algorithm, there exists a population such that the distance between the RLHF algorithm output and the mean rewards of the population is arbitrarily large.\nFinally, we also note that because we study RLHF with diverse populations, this work is inherently related to the study of pluralistic alignment of AI systems, see e.g. the work of Sorensen et al. (2024) and Anwar et al. (2024)."}, {"title": "Proof of Theorem 2.2", "content": "We will prove the desired result by contradiction.\nWe will show the desired result for any $C > log^2(2)$ which will imply the desired result for all $C > 0$. For any $C > log^2 (2)$, define $\\kappa := e^{12\\sqrt{C}}$. Consider the following two populations, each of which consists of two types of annotators which are equally prevalent and two alternatives"}, {"title": "Adapting Independence of Clones", "content": "In this section, we provide further details and justification for our definition of independence of clones in the RLHF setting."}, {"title": "Independence of Clones in Traditional Social Choice", "content": "In traditional social choice, independence of clones (Tideman, 1987) is a desirable charac-teristic of voting rules which intuitively states that the winner of an election remains the same when duplicates of candidates are added to the candidate pool. More specifically, clas-sic voting theory considers settings with a set of n voters (denoted N) and m candidates (denoted M). Each voter then provides a full ranking over the M candidates. A voting rule takes as input the set of rankings and outputs a single candidate. A subset K\u2208 M of candidates is a set of clones if no voter ranks any candidate in M \\ K between any two candidates in K. Finally, a voting rule is Independent of Clones if and only if the following two properties hold. First, a candidate in M \\ K is output by the voting rule if and only if that same candidate is output by the voting rule after eliminating any candidate in K. Second, a candidate in K is output by the voting rule if and only if some other member of K is also output by the voting rule after eliminating any candidate in K."}, {"title": "Additional Justification for RLHF Definition", "content": "This section provides further justification for how we adapt the traditional independence of clones definition for RLHF. Here, we focus on how we develop a reasonable definition for exact independence of clones. In the next section, we will then explain why we further consider robustness to approximate clones.\nInformally, we say that an RLHF algorithm satisfies exact independent of clones if adding new alternatives that are clones of existing alternatives does not change the reward function that is output by the RLHF algorithm. There are a few major differences between the definition of independence of clones in the traditional setting and in RLHF. First, while traditional independence of clones guarantees that the winning alternative does not change, the RLHF version of independence of clones instead guarantees that the reward function does not change, which is a stronger notion. This is because in traditional voting theory the focus is on the winning alternative, while in RLHF we care about the reward function over all of the alternatives. Second, the input to an RLHF algorithm consists of query results over pairs of alternatives, rather than full rankings from every voter. Therefore, the definition of a clone from traditional social choice does not carry over. Instead, we will define a clone in RLHF as a new alternative with the same context as an existing alternative, which implies that every voter has the same reward for the new alternative as for the existing one. Finally, it is generally assumed that an RLHF algorithm has access to noisy observations regarding human preferences, rather than the true rewards of each voter for each alternative. Due to randomness, it may be the case that two alternatives for which all voters have the exact same value may still look different in the dataset of query results given as input to the RLHF algorithm.\nTherefore, we will say that an RLHF algorithm is independent of clones if when the empirical pairwise win rates are the same as the true pairwise win rates, then the reward function output by the algorithm is unchanged when a clone is added. Note that when the dataset contains sufficiently many queries, the empirical pairwise win rates will approximately equal the true pairwise win rates by the law of large numbers. Further note that it is not necessarily"}, {"title": "Exact Independence of Clones", "content": "In this section, we formally present our definition of exact independence of clones and explain why our work primarily focuses on robustness to approximate clones. The formal definition of exact independence of clones is below.\nDefinition C.1 (Exact Independence of Clones). An RLHF algorithm ALG satisfies in-dependence of clones if the following holds. Consider a set of alternatives [m + 1] such that the context of alternative m is the same as the context of alternative m + 1. Let $D_1$ and $D_2$ be representative datasets over the alternative sets [m] and [m + 1] respectively. Let $r_1 = ALG(D_1)$, and $r_2 = ALG(D_2)$. Then $r_2(m+1) = r_2(m)$ and for all $i \\in [m]$, $r_1(i) = r_2(i)$.\nWe note that independence of clones as defined in Definition C.1 is a very weak guarantee, as two alternatives are only clones if their contexts are exactly equal and reward functions only need to remain unchanged when there is sufficient data. Even so, as a result of the equivalence between the regularized MLE for RLHF and Borda count as discussed in Section 2.2, the regularized MLE algorithm does not satisfy Definition C.1. We formally prove this result in Appendix D. However, because we have the context of the alternatives (as defined in Section 2), we can easily adapt the regularized MLE to satisfy independence of clones by a simple pre-processing step. Recall that two alternatives have the same context if and only if they are clones. Therefore, we can combine the data of any two alternatives with the same context in order to remove clones. Note that the regularized MLE cannot be made to satisfy robustness of approximate clones with preprocessing, because Definition 3.1 must hold for any \u03b4 > 0.\nAs further motivation for moving beyond exact independence of clones, RLHF queries often ask annotators to compare textual responses generated by LLMs, where it is unlikely that an exact response will be duplicated. Therefore, it is more realistic in RLHF to consider robustness to approximate clones. For example, an approximate clone of a textual response may substitute an adjective for its synonym, or use a slightly different grammar structure."}, {"title": "Proof of Theorem 3.1", "content": "proof. Let M = {a,b,c} and M' = {a,b,c,c'}, where $||c \u2013 c'|| = 0$. Note that Mand M' differ only by c', and d' is an exact clone of an alternative c\u2208 M. Suppose that Dis generated by querying a population which consists of three types of individuals, where each type is represented by a BTL model.\nSuppose further that D' is generated from the same population after cloning alternative c, and can be represented by the following:"}, {"title": "Proof of Theorem 4.2", "content": "We begin with the following lemma", "w": "T \u2192 R+ such that $\\sum_{t \\in T"}, "w(y) = 1$. Let \u03bb \u2208 R+ and for any $x_1, x_2 \\in T$ let $p(x_1 > x_2) \\in [0,1"], "r": "arg min", "where\n$f(r)": "sum_{x_1,x_2 \\in T} w(x_1)w(x_2)p(x_1 > x_2) \\log (\\frac{e^{r(x_1)}}{e^{r(x_1)} + e^{r(x_2)}}) + \\frac{\\lambda}{2} \\sum_{y \\in T} w(y)r(y)^2.$\nThen f strongly convex with parameter $m = \\min_{x \\in T}w(x)$. Therefore, f has a unique global minimum r*, and for any r,\n$f(r) - f(r^*) \\geq \\frac{m}{2} ||r - r^*||^2.$\nproof. Note that the function $\\log (\\frac{e^{r(x_"}