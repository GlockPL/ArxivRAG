{"title": "Difficult Task Yes but Simple Task No: Unveiling the Laziness in Multimodal LLMs", "authors": ["Sihang Zhao", "Youliang Yuan", "Xiaoying Tang", "Pinjia He"], "abstract": "Multimodal Large Language Models (MLLMs) demonstrate a strong understanding of the real world and can even handle complex tasks. However, they still fail on some straightforward visual question-answering (VQA) problems. This paper dives deeper into this issue, revealing that models tend to err when answering easy questions (e.g., Yes/No questions) about an image, even though they can correctly describe it. We refer to this model behavior discrepancy between difficult and simple questions as model laziness. To systematically investigate model laziness, we manually construct LazyBench, a benchmark that includes Yes/No, multiple choice, short answer questions, and image description tasks that are related to the same subjects in the images. Based on LazyBench, we observe that laziness widely exists in current advanced MLLMs (e.g., GPT-40, Gemini-1.5-pro, Claude 3, LLaVA-1.5, LLaVA-1.6, and QWen-VL). We also analyzed the failure cases of LLaVA-1.5-13B on the VQA-v2 benchmark and discovered that about half of these failures are due to the model's laziness. This further highlights the importance of ensuring that the model fully utilizes its capability. To this end, we conduct a preliminary exploration of how to mitigate laziness and find that chain of thought can effectively avoid this issue. The data can be accessed at https://github.com/Akutagawa1998/LazyBench.", "sections": [{"title": "1 Introduction", "content": "Multimodal Large Language Models (MLLMs) (Liu et al., 2023c) integrate multimodal content such as images into large language models (LLMs) (Touvron et al., 2023). Represented by OpenAI's GPT-4 (OpenAI, 2023b), MLLMs have demonstrated impressive capabilities across various complex multimodal tasks (OpenAI, 2023a; Yang et al., 2023). However, existing research indicates that even state-of-the-art MLLMs still suffer from some straightforward visual questions (e.g., \u201cIs the door of the truck cab open?\" for an image of a truck.) (Tong et al., 2024). A natural question arises:\nWhy do MLLMs struggle with these easy\nquestions?\nIn this work, we dive deeper to explore this question and find that MLLMs often struggle with simple questions (like Yes/No questions) about an image, even though they can accurately describe the image itself. For example, as present in Figure 1, when we asked GPT-4V, \u201cIs this man wearing a beige cap?\u201d GPT-4V answered \"Yes\", which is incorrect. In contrast, when we asked it a similar but more difficult question, \u201cPlease describe the cap that the man is wearing\", GPT-4V correctly described its color. In this paper, we describe this phenomenon where MLLMs perform well on the description tasks but make mistakes on simpler tasks as model laziness\u00b9.\nTo systematically study model laziness, we manually construct a benchmark called LazyBench. We found image pairs encoded as \u201csimilar images\" by the pretrained Contrastive Language-Image Pre-Training (CLIP) (Radford et al., 2021) model and designed simple Yes/No questions on their visual differences. We collected images from where GPT-4V (website version) fails in the above-mentioned questions. Then for each image, we handcraft three different types of questions about the same subject of the Yes/No question: multiple choice, short answer question, and a description task. We use LazyBench to evaluate advanced closed-source models like GPT-40, GPT-4V (OpenAI, 2023a), Gemini-1.5-pro (Reid et al., 2024), and Claude 3 (Anthropic, 2024), and open-source models like\""}, {"title": "2 Related Work", "content": "2.1 Visual Question and Answering\nWith the success of LLMs, increasing attention has been given to integrating visual embeddings into language models. Initially, researchers applied transformers to connect visual encoders with LLMs, pretraining them on image-text matching datasets (Lin et al., 2014; Krishna et al., 2017; Changpinyo et al., 2021) and fine-tuning them on specific datasets (e.g., VQA (Antol et al., 2015), VQA-v2 (Goyal et al., 2017)). Then, to improve MLLMs' performances and generalization abilities, researchers began using VQA format data for instruction tuning (Liu et al., 2023c). Despite MLLMs showing considerable capabilities in some complex VQA tasks (Fu et al., 2022; Hu et al., 2022, 2023b; Fu et al., 2023a,b), these studies seem to focus primarily on the textual reasoning abilities of MLLMs (Wei et al., 2022), rather than on whether MLLMs are truly extracting information from the images. Our work bridges this gap by studying the model laziness.\n2.2 Benchmarks for Visual Perceptions\nIncreasing attention is being given to the evaluation of MLLMs' visual perception. Tong et al., 2024 suggest that due to encoding flaws in the CLIP pre-trained model, CLIP-based MLLMs might make mistakes on some simple questions. POPE (Li et al., 2023) and NOPE (Lovenia et al., 2023) designed questions about the presence or absence of objects in images to measure MLLM hallucination; however, these consist solely of Yes/No questions. Hallusibench (Liu et al., 2023a) provides a benchmark for evaluating MLLMs' hallucinations across different tasks. MathVerse (Zhang et al., 2024) is a benchmark for visual problems in mathematical domains such as tables and charts. It reveals that MLLMs may not be thoroughly reading these charts, but they lack analysis of simpler and more straightforward VQA tasks. LazyBench is the first benchmark to focus on the consistency of MLLMs' answers to the same question about the same subject in the same image when asked in different forms."}, {"title": "3 MLLMs Are Being Lazy", "content": "To thoroughly understand and analyze the lazy phenomenon, where MLLMs perform well on descriptive tasks but fail on simple tasks, we construct the LazyBench benchmark. Therefore, in this section, we first introduce the methods and steps for constructing LazyBench. Subsequently, we measure the extent of the lazy phenomenon of current state-of-the-art MLLMs on LazyBench. Finally, we use a CoT-based method to mitigate the MLLMs laziness.\n3.1 Samples of LazyBench\nEach item in LazyBench consists of an image, a ground truth statement, and 4 different questions (i.e., Yes/No, multiple-choice, short answer, description) together with their ground truth answers. For instance, in Figure 2, for the image, there are:\n\u2022 One Yes/No question: \"Is this motorcycle racer wearing a long-sleeved motorcycle racing suit?\" and its ground truth answer is \u201cNo\u201d.\n\u2022 One multiple-choice question has 3 options: \"sleeveless motorcycle racing suit\", \"Long-sleeved motorcycle racing suit\u201d, \u201cNothing, he is naked\" and the first one as its ground truth.\n\u2022 One short answer question: \"What is the motorcycle racer wearing on their upper body?\""}, {"title": "3.2 Constructing LazyBench", "content": "Inspired by Tong et al., 2024, we designed a series of questions targeting the visually obvious differences between image pairs that were encoded as similar by CLIP (Radford et al., 2021). Intuitively, if two images are encoded as similar vectors by CLIP but have clear visual differences, it indicates that at least one of the images had certain features incorrectly encoded or neglected. This step helps us quickly construct a set of visual questions that MLLMs are likely to get wrong. We collected images from ImageNet (Russakovsky et al., 2015) and MMVP (Tong et al., 2024). The specific steps are listed below:\nImage Selection We encoded each image using CLIP and compared their cosine similarities. Followed by Tong et al., 2024 and our observation, here we focused on \u201csimilar image pairs\" with a cosine similarity greater than 0.96 but smaller than 0.99. This similarity ensures that the images in the pair are considered \u201cvery similar\" by CLIP yet also easy to find obvious visual differences between the image pairs. We then identified images that appeared significantly different in human view.\nQuestion Construction Based on images from the previous step, we formulated Yes/No questions targeting their differences. We collected the images and questions that might be answered incorrectly\u00b2 and designed ground truth statements, multiple-choice questions, short answer questions, and descriptive request questions around the error points. The process is shown in Figure 3. The ground truth of Yes/No questions will always be \u201cno\u201d and the correct option for multiple-choice questions is shuffled randomly in A, B, and C.\nWhen designing the description request, we directly asked the model to describe the subject of our focus (e.g., in Figure 2, we requested the model to describe the motorcycle racer's outfit on his upper body). This means that the subject of the Yes/No questions and multiple-choice questions was equivalently addressed in the description request. So the description request does not include any additional information or prompt any CoT guidance.\n3.3 Experimental Result\nSetup For evaluating model laziness, we assessed the LazyBench questions on SOTA close-source MLLMs such as GPT-40, GPT-4-Vision-preview (OpenAI, 2023a), Gemini-1.5-pro (Reid et al., 2024), Claude-3-Opus-20240229 (Anthropic,"}, {"title": "4 Discussion", "content": "4.1 Laziness in Existing Benchmarks\nTo explore the impact of laziness on the evaluation of MLLMs in existing benchmarks for visual perceptions, we conducted case studies on several popular benchmarks (e.g., VQA-v2 (Goyal et al., 2017) and Hallusionbench (Liu et al., 2023a)). We evaluate LLaVA-1.5-13B on 1000 Yes/No questions in the VQA-v2 validation set. To automate this process, we design and propose Don't be lazy (Doby). Doby is a framework based on GPT-40 which can generate the ground truth statements and description requests like LazyBench from the Yes/No or multiple choice questions-answering pairs in the existing datasets, thereby expanding the original datasets. After the MLLMs respond to the descriptive tasks, Doby compares the generated statements with the model's descriptions to determine if the tested MLLMs' descriptions accurately convey the relevant information. This process allows for automatic monitoring and statistical analysis of the MLLMs' laziness phenomenon among the datasets.\nUsing Doby, we found that LLaVA-1.5-13B is lazy in 79 of 192 failure cases. Some examples are given in Figure 4. This indicates that the model's inability to sufficiently utilize internal knowledge under simple tasks is also an important reason for the model's insufficient accuracy. Namely, how to prevent the model from being lazy is an important part of improving the model's capabilities.\n4.2 Mitigating Laziness\nAs the above experimental results show that MLLMs are lazy in the simpler tasks. We also want to know:\nCan we mitigate this phenomenon by\nmaking the questions harder?\nTo answer this question, we used a CoT-based method that let MLLMs answer the Yes/No and multiple choice questions after letting them finish the description task. For example, we ask the MLLMs about the image shown in Figure 2: Please describe the motorcycle racer's outfit on his upper body, and then answer the question: Is this motorcycle racer wearing a long-sleeved motorcycle racing suit?\nAs Table 2 shows, after employing this CoT method, MLLMs exhibit significant improvements in both Yes/No and multiple-choice questions. The enhancement is more pronounced for Yes/No questions. Among the models, GPT-40, which had the highest accuracy in description tasks, showed the greatest improvement in Yes/No questions. Specifically, GPT-40's accuracy in Yes/No questions increases by 24.76%. There are 37.5% GPT-40 laziness cases among the original Yes/No questions that have been repaired, while Gemini-1.5-pro and LLaVA see the least improvements of 13.86% and 15.58%. Additionally, GPT-40's accuracy in multiple-choice questions improves by 5.94%, matching its performance in description tasks, while the accuracy for Claude 3 and LLaVA-1.5-13B even slightly exceeds their performance in description tasks.\nWe further hypothesize that fine-tuning MLLMs to provide explanations before giving answers, rather than answering first and then explaining (Chu et al., 2024), could also reduce MLLMs' laziness. Similarly, the method proposed by Yuan et al., 2024 allows models to correct themselves while generating unsafe outputs, which might also be effective in this context: when MLLMs realize that their first one or few tokens (e.g., \u201cYes\u201d, \u201cA\u201d, etc.) of their initial answer may have been incorrect while explaining, they can adjust and improve their response. The automatic prompt may also be useful (Pryzant et al., 2023).\n4.3 Doby Helps Find Noise Sample\nFurthermore, by checking the response to description request of Doby, we find that in addition to instances of laziness (Figure 11 in Appendix B), these datasets contain numerous issues like the textual information of the question is vague (Figure 11(d)), or the questions cannot be answered solely based on the images (Figure 11(c)). Ignoring these issues may lead to incorrect assessments of the model's capabilities. These issues are not apparent when solely examining the results of MLLMs on Yes/No questions and multiple choice questions, which also suggests that future researchers should take a deeper look into the description response.\n4.4 Further Discussion\nAs previous studies (Hu et al., 2023a; Liu et al., 2023b) have found imbalanced training data often causes many MLLMs to directly give affirmative answers like \"yes\" to any question. To further verify that MLLMs' laziness is different from option bias, we construct the conversed statement by another image in the \"similar image pairs\", (e.g., \"Statement for Image_1\" in Step 2 of Figure 3). The detailed information can be found in Appendix A.2.\n4.5 Why the MLLMs are Lazy?\nWe have a hypothesis about the reason why MLLMs are lazy: take Yes/No questions and descriptions as examples. For the former, the answer (MLLMs response) needs to be given within a few tokens or even a single token (i.e., \u201cYes\u201d, \u201cNo\u201d, or \"A\" etc.), which means the model can only \u201clook at the image a few times or even just once\" while decoding the answer. In contrast, when generating the description of a specific region in the image, MLLMs may need to look at the image many times throughout the decoding process. The \"quick glance\" for simple tasks versus the \"careful observation\" for complex tasks might be the reason behind laziness. We believe it is important to understand and explain laziness accurately with more experiments. However, since we are in the early stages of studying laziness, in this work we focus more on measuring, understanding its impacts, and finding solutions for laziness. We will leave the in-depth exploration of laziness for the future."}, {"title": "5 Conclusion", "content": "This paper highlights the laziness in MLLMs: a model can handle difficult tasks (e.g., describe the subject) but fails on simple tasks (e.g., a corresponding Yes/No question). We provide a benchmark LazyBench that systematically shows this discrepancy in model performance across advanced MLLMs. Our findings indicate that in addition to allowing the model to learn more knowledge, it is equally important to ensure that MLLM is fully utilizing the knowledge learned."}, {"title": "Limitations", "content": "This paper has the following limitations. First, laziness mainly occurs in powerful closed-source MLLMs where we cannot access their internals for further analysis of the root causes. Second, although our CoT-based method shows preliminary effectiveness, we regard the development and evaluation of laziness mitigation mechanisms as important future work. Third, the size of LazyBench is small. We will keep expanding it in the future."}, {"title": "A.1 Why Description is a \u201cMore Difficult Task\u201d?", "content": "We categorize binary (Yes/No) and multiple-choice questions as \"simple questions\u201d from the perspective of the probability of random guessing and the size of the solution space. For a Yes/No question, there is a 50% chance of guessing correctly, and for a multiple-choice question with three options, there is a 33% chance of guessing correctly. Conversely, an open-ended question such as \u201cDescribe the man's outfit of his upper body\" requires the model to generate a highly specific and correct response from an infinite combination of characters, such as \"He is wearing a sleeveless tank top\" or \"a vest,\" to be considered \"correct\". In this case, the probability of a correctly random guess is nearly zero. Therefore, intuitively, we believe that accurately describing an object is more challenging than selecting the correct answer from a limited set of options. Since we do not focus on \u201creasoning difficulty,\" all our questions are specifically designed to ensure that the \u201creasoning difficulty\" of Yes/No questions is comparable to that of descriptive questions. For example, in Figure 1(a), the descriptive task \"Please describe the cap that the man is wearing\" does not cover more information than the Yes/No question \u201cIs the man wearing a beige cap?\u201d and they are asking about the same thing. In a word, the term \"difficult task\" here refers solely to the type of question, not the specific question content."}, {"title": "A.2 Conversed & Irrelevant Question for Ablation Studies", "content": "We use GPT-4-turbo to generate a conversed Yes/No question and an irrelevant Yes/No question for each image, based on the statements. As shown in Figure 5, the irrelevant questions are about something unrelated to the images, and their ground truth answers are all \"No.\" If a model always fails in the irrelevant questions, we believe it tends to give \"yes\" responses without hesitation. All the ground truth answers to the conversed Yes/No questions are \"yes\". They test if MLLMs can correctly recognize the features which are indeed in this image."}, {"title": "A.3 Swapping Options", "content": "We use the swapping option to ensure that the result in multiple choice is not influenced by option bias of \u201cA\u201d, \u201cB\u201d and \u201cC\u201d. Empirically we find that LLaVA-1.5-7B obtains 91.21% accuracy when the correct answer is \u201cA\u201d, but 3.2% accuracy when the correct answer is \u201cB\u201d on LazyBench. Other MLLMs tend to have even performance while the order of the options shifts."}, {"title": "A.4 Evaluation Criterion of the Description", "content": "We employ a binary classification method to score the descriptions provided by the model as either correct or incorrect. As shown in Figure 6, when the MLLM gives a description identical to the statement, we judge it as correct (e.g., if the MLLM's description: \"This is a back view of a person\" and the statement: \u201cback view of a person\"). If the MLLM provides a description different from the statement, but we find it equivalent to the statement or containing the statement when considering the image, we also judge it as correct (e.g., GPT-40's description: \u201cThe camera captures the person from a low-angle, rear perspective, slightly to the left,\" which we consider a more detailed description based on the image). In all other cases, if the MLLM's description differs from the statement and is neither equivalent nor contains the same information, we judge it as incorrect (e.g., LLaVA-1.5-13B's description: \u201cThe camera perspective is a side view of the person running\u201d). Additionally, if the model's description is irrelevant to our question or refuses to answer the relevant question, we also consider it an incorrect description."}, {"title": "A.5 Prompts of Doby", "content": "In Doby, we first ask GPT-4-turbo to generate the statement by Yes/No question and answer pairs, here we use a few shot prompt strategy (Figure 7). Then we use GPT-4-turbo to generate the description request (Figure 8). After asking the MLLMs to answer the description request, Doby compared the statements and the MLLMs' descriptions (by using GPT-4-turbo, with the same criterion in Figure 6) to check if MLLMs can correctly describe the subject in the image."}]}