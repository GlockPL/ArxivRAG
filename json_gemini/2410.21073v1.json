{"title": "SKIP2-LORA: A LIGHTWEIGHT ON-DEVICE DNN FINE-TUNING\nMETHOD FOR LOW-COST EDGE DEVICES", "authors": ["Hiroki Matsutani", "Kazuki Sunaga", "Masaaki Kondo", "Radu Marculescu"], "abstract": "This paper proposes Skip2-LoRA as a lightweight fine-tuning method for deep neural networks\nto address the gap between pre-trained and deployed models. In our approach, trainable LORA\n(low-rank adaptation) adapters are inserted between the last layer and every other layer to enhance\nthe network expressive power while keeping the backward computation cost low. This architecture\nis well-suited to cache intermediate computation results of the forward pass and then can skip the\nforward computation of seen samples as training epochs progress. We implemented the combination\nof the proposed architecture and cache, denoted as Skip2-LoRA, and tested it on a $15 single board\ncomputer. Our results show that Skip2-LoRA reduces the fine-tuning time by 90.0% on average\ncompared to the counterpart that has the same number of trainable parameters while preserving the\naccuracy, while taking only a few seconds on the microcontroller board.", "sections": [{"title": "Introduction", "content": "On-device learning is an emerging research direction in edge AI aiming to reduce the gap between pre-trained and\ndeployed models. Since the available compute resources are limited in edge environments, full retraining of deep\nmodels is hardly feasible; thus, lightweight retraining methods of neural networks have been studied recently [1, 2, 3].\nSuch on-device learning methods can be broadly classified into 1) ELM (extreme learning machine) based retraining\nand 2) fine-tuning of some specific layers using a backpropagation algorithm. In the ELM-based on-device learning\n[4, 3], the OS-ELM (online sequential ELM) algorithm [5] is used for training the weight parameters of neural networks\nthat have a single hidden layer. Thus, the ELM-based approach cannot be applied to DNNs (deep neural networks)\nthat have multiple or many hidden layers; instead, the backpropagation-based approach can be used for such DNNs.\nA well-known method based on backpropagation fine-tunes the last layer of DNNs [6]; in this case, the backward\ncompute cost is very small compared to the full training, but the network expressive power remains limited since only\nthe last-layer weights can be updated. Another method freezes the weight parameters while only updating the bias\nmodules [7]. TinyTL introduces the lite residual module as a generalized bias module to be fine-tuned [7]. All these\nmethods update parts of the pre-train model. In addition, fine-tuning methods have been widely studied in the context\nof LLMs (large language models). A popular approach in LLMs is to add trainable adapters to pre-trained networks.\nTrainable adapter layers can be inserted in series to pre-trained networks [8], or attached to pre-trained weight matrixes\nin parallel [9]. LoRA (low-rank adaptation) [9] employs the latter approach; that is, trainable rank decomposition\nmatrixes are attached to each layer of a Transformer architecture [10]. This approach is portable, meaning that only\nthe adapters are updated while the original weights are untouched."}, {"title": "Preliminaries", "content": "Let N and M be the input and output dimensions of an FC (fully-connected) layer, respectively, in a DNN. A forward\npass of the FC layer is computed as follows:\n$y = G(x. W + b),$\nwhere x \u2208 $R^{B\u00d7N}$, y \u2208 $R^{B\u00d7M}$, W \u2208 $R^{N\u00d7M}$, b \u2208 $R^{M}$, and B represent the input feature map, output feature map, weight\nparameters, bias parameters, and batch size, respectively.\nA backward pass of the FC layer is computed as follows:\ngW =\n$x^{T}gy$ (2)\n$g^{b} = \u03a3gy$\n(3)\ngx = gy. $W^{T}$,\n(4)\nwhere gx \u2208 $R^{B\u00d7N}$, gy \u2208 $R^{B\u00d7M}$, gW \u2208 $R^{N\u00d7M}$, and gb \u2208 $R^{M}$ represent the gradients of x, y, W, and b, respectively.\nW and b are updated as follows:\nW \u2190 W-7.gW\n(5)\nb \u2190 b-\u03b7\u00b7gb,\n(6)\nwhere n represents a learning rate.\nA forward pass of a LoRA adapter of rank R for the FC layer is computed as follows:\n$y_{A} = xWA$\n(7)\n$y_{B} = y_{A} W_{B}$\n(8)\ny \u2190 y + y\u0432,\n(9)\nwhere ya \u2208 $R^{B\u00d7R}$ and yB \u2208 $R^{B\u00d7M}$ are intermediate outputs to update y. WA \u2208 $R^{N\u00d7R}$ and WB \u2208 $R^{R\u00d7M}$ are the\nweight parameters of the adapter.\nA backward pass of the LoRA adapter is computed as follows:\ngWB = ygy\n(10)\ngxB = gy $W_{B}^{T}$\n(11)\ngWA = $x^{T}$gxB\n(12)\ngxA = gxBWX\n(13)\ngx \u2190 gx + gxa,\n(14)\nwhere gxB \u2208 $R^{BXR}$ and gxa \u2208 $R^{B\u00d7N}$ are intermediate gradients. gWB \u2208 $R^{R\u00d7M}$ and gWa \u2208 $R^{N\u00d7R}$ represent the\ngradients of WB and WA, respectively.\nWA and WB are updated as follows:\nWA \u2190 WA-7gWA\n(15)\nWB \u2190 WB-7gWB.\n(16)"}, {"title": "Baseline Fine-tuning Methods", "content": "A forward pass of an FC layer computes y, while the backward pass computes gW, gb, and gx. In fine-tuning\nscenarios, not all are necessary; for example, gW and gb of an FC layer are not necessary when weight and bias\nparameters of the layer are not updated. Compute types of FC layers are classified as listed in the upper half of Table 1.\nThe number of floating-point operations and memory size can be modeled for each compute type, but they are omitted\ndue to the page limitation.\nAs basic fine-tuning methods, in this paper, FT-All, FT-Last, and FT-Bias are defined as follows:\n\u2022 FT-All: Weight and bias parameters of all layers are updated.\n\u2022 FT-Last: Weight and bias parameters of the last layer are updated.\n\u2022 FT-Bias: Bias parameters of all layers are updated.\nFigures 1(a), 1(b), and 1(c) illustrate FT-All, FT-Last, and FT-Bias methods, respectively, for DNNs consisting of three\nlayers. In these figures, the parameters to be updated are colored in red. The compute types of the first, second, and\nthird FC layers in FT-All are {FCywb, FCywbx, FCywbx}. Those in FT-Last are {FCy, FCy, FCywb}, and those in\nFT-Bias are {FCyb, FCybx, FCybx}. In the first layer, gx is not propagated any more and thus can be omitted.\nA forward pass of a LoRA adapter computes ya and yb, while the backward pass computes gWB, gWA, gxB, and\ngxA. Compute types of LoRA adapters are classified as listed in the lower half of Table 1. The compute and memory\ncost model for each compute type is omitted in this paper.\nAs fine-tuning methods of LoRA, LORA-All and LoRA-Last are defined as follows:\n\u2022 LoRA-All: LoRA adapters are added to all layers.\n\u2022 LoRA-Last: A LoRA adapter is added to the last layer.\nFigures 1(d) and 1(e) illustrate LoRA-All and LoRA-Last methods, respectively, for DNNs consisting of three layers,\nwhere the parameters to be updated are colored in red. The compute types of the first, second, and third LoRA adapters\nin LoRA-All are {LoRAyw, LoRAywx, LoRAywx}, and those of the FC layers are {FCy, FCyx, FCyx}. Similarly,\nthe compute types of the first, second, and third LoRA adapters in LoRA-Last are {\u03c6, \u03c6, LoRAyw}, and those of\nthe FC layers are {FCy, FCy, FCy}. The backward compute cost of LoRA-Last is thus much smaller than that of\nLoRA-All. On the other hand, LoRA-All introduces LoRA adapters to all the layers, while LoRA-Last introduces only\na single LoRA adapter to the last layer; thus, LoRA-All has a higher expressive power than LoRA-Last."}, {"title": "Performance Analysis", "content": "In this section, the compute costs of fine-tuning methods are analyzed. To analyze the compute costs, \"FT-All-LoRA\"\nis defined as a full fine-tuning method that combines FT-All and LoRA-All. We assume a simple 3-layer DNN that\nconsists of FC layer (FC1), LoRA adapter (LoRA1), batch normalization [11] (BN1), ReLU (Act1), FC2, LoRA2,\nBN2, Act2, FC3, LoRA3, and cross entropy loss (CEL) function. Table 2 shows the execution times breakdown of the\nforward and backward passes without CEL. Two datasets, Fan and HAR which will be explained in Section 5.1, are\nexamined. As shown, the first and second FC layers consume most of the compute costs. To reduce these compute\ncosts, Skip-LoRA and Skip2-LoRA are proposed in the next section."}, {"title": "Design and Implementation of Skip2-LoRA", "content": ""}, {"title": "Proposed Architecture: Skip-LoRA", "content": "Our first proposal is \u201cSkip-LoRA\u201d which aims to achieve a comparable expressive power to LoRA-All, yet with a\ncomparable backward compute cost to LoRA-Last. Skip-LoRA is defined as follows:\n\u2022 Skip-LoRA: LORA adapters are added between output nodes of the last layer and input nodes of the other\nlayers.\nAs shown in Figures 1(d) and 1(e), weight parameters of a LoRA adapter for the k-th layer are denoted as $W_{k\u22121,k}$.\nFor a DNN consisting of n layers, additional weight parameters of LoRA-Last and LoRA-All are denoted as $W_{n-1,n}$\nand $\\Sigma_{k=1}^{n}W_{k-1,k}$, respectively. On the other hand, those of Skip-LoRA are denoted as $\\Sigma_{k=1}^{n}W_{k-1,n}$. In this case, the\nforward pass of all the FC layers is computed normally with Equation 1. Then, the forward pass of n LoRA adapters\nis computed, and the results are added to the output feature map of the n-th FC layer as follows:\n$yn y + \\Sigma_{k=1}^{n}xk.W_{k-1,n}^{A}. W_{k-1,n}^{B}$\nwhere x and yk are the input and output feature maps of the k-th FC layer."}, {"title": "Proposed Cache: Skip-Cache", "content": "Skip-LoRA can reduce the backward compute cost, as well as LoRA-Last. The next bottleneck is the forward compute\ncost. Here, we aim to reduce the forward compute cost by reusing the forward compute results which have been already\ncomputed."}, {"title": "Implementation of Skip2-LoRA", "content": "Skip2-LoRA is implemented with the C language without any external libraries except for libm (\u201c-lm\" option).\nAlgorithm 1 shows the fine-tuning with Skip2-LoRA algorithm. T, |T|, E, and B are the training samples for\nfine-tuning, the number of training samples, the number of epochs, and the batch size, respectively.\nIn line 2, Skip-Cache Cskip is initialized. In line 5, a batch of training samples is randomly selected from T. In line\n6, the forward pass of all the FC layers is computed for the batch. During this computation, Cskip is examined and\nunnecessary computation is skipped. Algorithm 2 shows the forward pass of a single FC layer with Cskip. This is a"}, {"title": "Evaluations", "content": "The proposed Skip2-LoRA is compared with the counterparts in terms of accuracy and execution time using drifted\ndatasets. It is also compared with the state-of-the-art method."}, {"title": "Evaluation Setup", "content": "FT-All, FT-Last, FT-Bias, FT-All-LoRA, LoRA-All, LoRA-Last, Skip-LoRA, and Skip2-LoRA are executed on a\nRaspberry Pi Zero 2 W board [12], which is known as a $15 computer (Figure 2). The clock frequency is fixed at 1GHz\nto measure the execution time stably. Skip2-LoRA and its counterparts (except for TinyTL [7]) are implemented with\nthe C language and compiled with gcc version 8.3.0 with \u201c-O3\" option on the platform. They are further optimized\nwith SIMD (Neon) instructions with \u201c-mfpu=neon -ffast-math\" option.\nIn reality, the activation function and batch normalization are typically executed after each layer as in Table 2. In this case, the\noutputs after these functions should be cached except for the last layer (i.e., the outputs just after the FC layer should be cached in\nthe case of the last layer)."}, {"title": "Accuracy", "content": "Table 3 shows the baseline accuracy of the three datasets without fine-tuning on the 3-layer DNNs. In the \"Before\"\ncase, the model is trained with the pre-train dataset and then tested with the test dataset. In the \"After\" case, the model\nis trained only with the fine-tune dataset and then tested with the test dataset. In each case, the number of training\nepochs is set to a large enough value (i.e., E = 400 and 900 for the Damage1/Damage2 and HAR datasets). Table 3\nshows mean accuracy values over 20 trials. The accuracy is quite low in the Before case while it is significantly better\nin the After case. There is a significant accuracy gap between before and after the data drift; in this case, we can fill\nout the gap by the on-device fine-tuning as demonstrated below.\nTable 4 shows the accuracies of FT-All, FT-Last, FT-Bias, FT-All-LoRA, LORA-All, Skip-LoRA, and Skip2-LoRA on\nthe 3-layer DNNs. The test is conducted with the following three steps. In each case, the number of training epochs is\nset to a large enough value.\n1. The model is trained with the pre-train samples (E = 100 and 300 for the Damage1/Damage2 and HAR\ndatasets)."}, {"title": "Execution Time", "content": "Tables 6 and 7 show the execution times of FT-All, FT-Last, FT-Bias, FT-All-LoRA, LORA-All, LoRA-Last, Skip-\nLoRA, and Skip2-LoRA on a Raspberry Pi Zero 2 W with Neon instructions. The results on the Damage1 and\nDamage2 datasets are the same and thus reported as \u201cFan\u201d dataset in Table 6. The training batch size B is set to 20.\nAs shown in these tables, the training time of a batch consists of the forward pass, backward pass, and weight update.\nThe execution times are mean values over the entire fine-tuning process, where the number of epochs E is the same as\nthat in Section 5.2. Although Skip-LoRA and LoRA-All have the same number of trainable parameters, Skip-LoRA\nreduces the execution time of backward pass by 82.5% to 88.3% compared to LoRA-All, demonstrating benefits of the\nproposed Skip-LoRA architecture. In addition, Skip2-LoRA reduces the execution time of forward pass by 89.0% to\n93.5% compared to Skip-LoRA, demonstrating benefits of the proposed Skip-Cache. As a result, Skip2-LoRA reduces\nthe training time by 89.0% to 92.0% (90.0% on average) compared to LoRA-All that has the same number of trainable\nparameters. The training times are only 0.450msec and 0.595msec per batch in the Fan and HAR datasets, respectively.\nIn Skip2-LoRA, the training time is affected the number of epochs E, because a larger E can skip more forward pass\ncomputations. As mentioned in Section 5.2, E was set to a large enough value. Here, we estimate actual training time\nbased on practical E. Figure 3 shows the training curves of Skip2-LoRA with the three datasets. X-axis shows the\nnumber of trained epochs, and Y-axis shows the test accuracy. Mean accuracies over 10 trials are reported in these\ngraphs. Here, the number of required epochs in which the test accuracy first reaches within a 1% range of the reported\naccuracies in Table 4 is denoted as \"required epochs\". The required number of epochs are 100, 60, and 200 in the\nDamage1, Damage2, and HAR datasets, respectively. The number of their fine-tuning samples are 470, 470, and 1050;\nthus, the total fine-tuning times of Skip2-LoRA on a Raspberry Pi Zero 2 W are only 1.06sec, 0.64sec, and 2.79sec in\nthe Damage1, Damage2, and HAR datasets, respectively."}, {"title": "Power Consumption", "content": "Skip2-LoRA with the HAR dataset (E = 200) is executed on a Raspberry Pi Zero 2 W and the power consumption is\nmeasured with a current sensor INA219 (Figure 2). Figure 4 shows the variation of power consumption and temperature,\nwhere the fine-tuning starts at 9sec. Once the fine-tuning starts, the clock frequency increases from 600MHz to 1GHz\nand the power consumption increases. Although the net compute time for the forward and backward passes is 2.79sec\nas mentioned above, the results in Figure 4 include overheads for reading the dataset and loading the pre-trained weight\nparameters. The power consumption is at most 1,455mW for a short duration and the temperature does not exceed\n44.5 deg C."}, {"title": "Summary", "content": "In this paper, we extended LoRA adapters as a new lightweight on-device fine-tuning mehtod for resource-limited\nedge devices. The proposed Skip2-LoRA synergistically combines Skip-LoRA architecture to reduce the backward\ncompute cost and Skip-Cache to reduce the forward compute cost. Experimental results using three drifted datasets\ndemonstrated that Skip2-LoRA reduces the fine-tuning time by 90.0% on average compared to LoRA-All that has the\nsame number of trainable parameters while achieving comparable accuracies to the state-of-the-art method. The order\nof magnitude reduction of the compute cost enables a few seconds \u201cquick\u201d fine-tuning of DNNs on a Raspberry Pi\nZero 2 W board with modest power and temperature."}]}