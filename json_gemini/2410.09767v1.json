{"title": "LibEER: A Comprehensive Benchmark and Algorithm Library for EEG-based Emotion Recognition", "authors": ["Huan Liu", "Shusen Yang", "Yuzhe Zhang", "Mengze Wang", "Fanyu Gong", "Chengxi Xie", "Guanjian Liu", "Dalin Zhang"], "abstract": "EEG-based emotion recognition (EER) is garnering\nincreasing attention due to its potential in understanding and\nanalyzing human emotions. Recently, significant advancements\nhave been achieved using various deep learning-based techniques\nto address the EER problem. However, the absence of a con-\nvincing benchmark and open-source codebase complicates fair\ncomparisons between different models and poses reproducibility\nchallenges for practitioners. These issues considerably impede\nprogress in this field. In light of this, we propose a compre-\\hensive benchmark and algorithm library (LibEER) for fair\ncomparisons in EER by making most of the implementation\ndetails of different methods consistent and using the same single\ncodebase in PyTorch. In response to these challenges, we propose\nLibEER, a comprehensive benchmark and algorithm library\nfor fair comparisons in EER, by ensuring consistency in the\nimplementation details of various methods and utilizing a single\ncodebase in PyTorch. LibEER establishes a unified evaluation\nframework with standardized experimental settings, enabling\nunbiased evaluations of over ten representative deep learning-\nbased EER models across the four most commonly used datasets.\nAdditionally, we conduct an exhaustive and reproducible com-\nparison of the performance and efficiency of popular models,\nproviding valuable insights for researchers in selecting and de-\nsigning EER models. We aspire for our work to not only lower\nthe barriers for beginners entering the field of EEG-based emotion\nrecognition but also promote the standardization of research in\nthis domain, thereby fostering steady development. The source\ncode is available at https://github.com/ButterSen/LibEER.", "sections": [{"title": "I. INTRODUCTION", "content": "MOTION encompasses a series of subjective cognitive\nexperiences, serving as the primary framework for inter-\npersonal relationships and providing the criteria for evaluat-\ning human behavior [1]. Electroencephalography (EEG) is a\nwidely used physiological signal in emotion analysis due to\nits non-invasive nature, high temporal resolution, portability,\nand cost-effectiveness [29], [30]. Consequently, EEG-based\nemotion recognition (EER) has garnered increasing attention\nin recent years because of its robust capacity to understand\nemotions and has broad applicability in fields such as health-\ncare [31], [40], advertising [32], and education [33].\nTraditional approaches primarily focus on designing or\nlearning discriminative features, such as power spectral density\n(PSD) [39] and differential entropy (DE) [15], to enhance\nthe performance of EER. Recently, with the aid of advanced\nneural networks, a variety of deep learning-based models [16],\n[18] have sprung up to recognize emotion states from EEG\nsignals and have been one of the most promising research\ntopics [2]. These models often aim to capture the spatiotempo-\nral characteristics of EEG signals and can be categorized into\nfive types: deep neural network-based (DNN), convolutional\nneural network-based (CNN), recurrent neural network-based\n(RNN), graph neural network-based (GNN), and Transformer-\nbased methods. DNN-based methods [5], [6] are simple yet\neffective, employing fully connected layers and directly using\nthe EEG signal's features as input. CNN-based methods [8]-\n[10] treat EEG data as 2D or 3D images, where one dimension\ncorresponds to the electrodes and the other dimension(s) repre-\nsent temporal or frequency features. RNN-based methods [11],\nparticularly the long short-term memory (LSTM) model, are\nsuited for modeling crucial temporal information in EEG\nsignals. GNN-based methods [12]-[14] are employed to model\nthe spatial information of EEG signals across different chan-\nnels. Transformer-based methods [7] are the most powerful\nto date, segmenting EEG signals into multiple patches and\ncapturing dependencies between these patches using the self-\nattention mechanism.\nDespite the significant achievements claimed by various\ndeep learning-based models, some key issues remain underex-\nplored in the EER research field, hindering the further develop-\nment of actual progress in this area. In this paper, we identify\nthese issues as a lack of benchmark and codebase, which\nmakes fair comparisons difficult and leads to practitioners\nstruggling with reproducibility.\nThe most critical issue pertains to the absence of a com-\nprehensive benchmark for EER tasks, which results in non-\nunified, unreasonable, and ambiguous experimental settings\nacross different models. This situation makes fair compar-\nisons challenging and may even call into question certain\nconclusions. Specifically, with regard to the non-unified issue,\nvarious methods utilize differing datasets or operate under\ndifferent scenarios, employ distinct pre-processing techniques\n(e.g., whether to conduct artifacts removal or not), feature\nvarying ratios of training and test sets, and apply diverse data\nsegmentation (i.e., length of samples), among other discrep-\nancies. In terms of the unreasonable issue, certain studies\nemployed a fixed data split strategy [12], [15], [17] -for"}, {"title": "II. OVERVIEW OF EEG-BASED EMOTION RECOGNITION\nMETHODS", "content": "In this section, we begin by presenting the problem formula-\ntion of EEG-based emotion recognition, followed by a review\nof five kinds of EER methods, with an in-depth exploration of\nseveral representative approaches."}, {"title": "A. Problem Formulation", "content": "Most current datasets involve recording EEG signals while\nparticipants are exposed to specific stimuli, such as videos\nor music. In our framework, we define a participant in an\nexperiment as a subject. The complete set of EEG data col-\nlected from each subject under a particular stimulus condition\nis referred to as a trial, typically lasting between one to five\nminutes. Due to the high cost of acquiring EEG datasets, the\nnumber of trials within a complete dataset is generally limited.\nTo facilitate subsequent processing tasks, the data within each"}, {"title": "B. Representative Methods", "content": "We based our selection of baseline methods on a compre-\nhensive review of recent high-quality papers in the field of\nEER from the past few years. After thoroughly examining 60\nrecent papers, we identified and analyzed 205 distinct baseline\nmethods. Our analysis primarily focused on the frequency\nwith which these methods were used as baselines in the\nsurveyed papers, as this metric best reflects their authority\nand recognition within the field. Subsequently, we considered\nadditional factors, such as the citation count of each method\nand the impact factor of the journals (or the rank of confer-\nences) in which they were published, to refine our selection\nfurther. Lastly, we ensured a diverse range of methods by\ntaking into account their publication time and methodological\nvariety, including significant methods such as DNN, CNN,\nRNN, GNN, and Transformer. We filtered out models that\nwere below 10% of those reported in the papers to ensure\nthe validity of the algorithm library comparisons.\nConsidering the reproducibility of these methods and the\nlimitations of our current resources, we ultimately selected the\nten most representative works [5]\u2013[14] to include in LibEER.\nTable I presents detailed information and ranking of these ten\nmethods. These methods represent a well-considered balance\nof influence, methodological diversity, and publication re-\ncency, ensuring comprehensive coverage of the most impactful\nand up-to-date approaches. In future work, we will continue\nto maintain and expand this collection by incorporating more\nrecent and diverse methods."}, {"title": "1) CNN-based Methods:", "content": "Convolutional Neural Networks\n(CNN) are a type of feedforward neural network characterized\nby convolutional structure. When applied to EEG signals,\nCNN treats the EEG data as a 2D representation, where\none dimension corresponds to the electrodes and the other\ndimension represents either temporal or frequency features of\nthe EEG data, as shown in Figure 1 (a).\nThis paper discusses three key CNN-based methods. EEG-\nNet [8] employs depthwise and separable convolutions across\ntwo blocks. The first block features temporal convolution for\nfrequency information and depthwise convolution for spatial\nfilters. The second block uses separable convolution to further\nrefine the features, reducing dimensionality while preserving\nessential information. Channel-Fused Dense Convolutional\nNetwork (CDCN) [9] combines one-dimensional convolution\nwith dense structures to extract temporal and spatial features\nfrom EEG signals. Its dense blocks enhance information flow\nand feature reuse by connecting each layer directly to all pre-\nceding layers, effectively capturing temporal dependencies and\nspatial correlations. TSception [10] is a multi-scale CNN that\nincludes dynamic temporal, asymmetric spatial, and high-level\nfusion layers. The dynamic temporal layer captures temporal\nand frequency representations using multi-scale 1D kernels,\nwhile the asymmetric spatial layer learns spatial patterns. The\nhigh-level fusion layer integrates these features for efficient\nand accurate emotion classification."}, {"title": "2) RNN-based Methods:", "content": "Recurrent Neural Networks\n(RNNs) are designed for sequential data, where directed\nconnections between nodes allow them to maintain a mem-\nory of previous inputs. In EEG applications, RNNs extract\nspatial features by treating channels as sequence nodes and\ntemporal features by treating time slices as sequence nodes,\neffectively capturing the complex spatiotemporal dynamics in\nEEG signals (see Figure 1 (b)).\nThe Attention-based Convolutional RNN (ACRNN) [11]\nemploys channel-wise attention and extended self-attention\nmechanisms. Channel-wise attention allows the model to focus\non relevant channels for emotion recognition, while CNN\nextracts spatial features from these channels. ACRNN also\nuses Long Short-Term Memory (LSTM) units to capture and\nrefine temporal features with extended self-attention, assigning\nimportance to different time steps based on their relevance to\nthe recognized emotional state."}, {"title": "3) GNN-based Methods:", "content": "Graph Neural Network (GNN) is\na type of neural network designed to handle data structured\nas graphs. A graph consists of nodes and edges, which can\nrepresent complex relationships and structures. When pro-\ncessing EEG data, the EEG channels are typically treated as\nnodes in a graph, as shown in Figure 1 (c). Since EEG data\ndoes not inherently contain information about the edges or\nthe adjacency matrix, using graph neural networks to process\nEEG data often involves leveraging prior knowledge about the\nrelationships between channels or using learnable adjacency\nmatrices as the graph's adjacency matrix.\nWe introduced three GNN-based methods. Dynamical\nGraph CNN (DGCNN) [12] models multichannel EEG fea-\ntures using a dynamic adjacency matrix that adapts during\ntraining. It employs Chebyshev graph convolution for efficient\nfiltering calculations. Regularized GNN (RGNN) [13] uti-\nlizes a neuroscience-inspired adjacency matrix and introduces\nsimple graph convolution networks (SGC). RGNN enhances\nrobustness through node-wise domain adversarial training and\nemotion-aware distribution learning to address cross-subject\nvariations and noisy labels. Graph Convolutional Broad Net-\nwork (GCBNet) [14] combines graph convolutional layers\nwith traditional convolutional layers for effective feature ex-\ntraction. GCBNet uses Chebyshev graph convolution followed\nby stacked convolutional layers to abstract high-level features.\nThe GCBNet+BLS variant incorporates the Broad Learning\nSystem to enhance feature extraction and classification further."}, {"title": "4) DNN-based Methods:", "content": "Deep Neural Network (DNN) is\na feedforward artificial network consisting of fully connected\nneurons with a nonlinear activation function, including the\ninput layer, multiple hidden layers, and the output layer. In\nthe field of EER, DNN processes EEG data by transforming\nand mapping it into the feature space, enabling the extraction\nand utilization of complex patterns within the data, illustrated\nin Figure 1 (d).\nIn this paper, we have implemented two popular DNN-based\nmethods. Deep Belief Networks (DBN) [5] are probabilistic"}, {"title": "5) Transform-based Methods:", "content": "Transformer uses a self-\nattention mechanism to capture dependencies between ele-\nments in a sequence, effectively handling long-range depen-\ndencies. As shown in Figure 1 (e), the EEG data are typically\nsegmented into multiple patches on channels, regions, or\ntemporal segments when using transformers to process EEG\ndata. These patches are then fed into the transformer model\nfor further analysis.\nThe Hierarchical Spatial Learning Transformer (HSLT) [7]\nis a hierarchical transformer that extracts discriminative fea-\ntures by capturing EEG spatial dependencies from electrode\nto brain-region level. It inputs electrode-extracted patches\ninto the first layer of the transformer encoder to aggregate\ninformation within the same brain region. The output then\nserves as input for the second layer, which captures spatial\ndependencies between different brain regions and highlights\nthose with greater contributions."}, {"title": "III. BENCHMARK BUILDING", "content": "A comprehensive, rational, and transparent benchmark is\nessential for the equitable comparison of research method-\nologies within the field, a need currently unmet in the EER\ndomain. In this section, we will elaborate on the benchmark\nwe have devised, encompassing datasets, data preprocessing,\ndata partitioning, and evaluation methods. For convenience,\nwe summarize the key information of the EER benchmark in\nTable II."}, {"title": "A. Datasets", "content": "Research in the EER field exhibits variability in dataset\nselection and often lacks clarity regarding the specific data\nsegments utilized. This study evaluates the performance of\npopular models using four commonly employed datasets,\nwhich will be introduced along with the relevant data seg-\nments, as summarized in Table III.\n1) SEED: The SEED [22] dataset consists of 15 subjects\n(7 males, 8 females), each completing three sessions, with 15\nemotion-inducing video trials per session (positive, neutral, or\nnegative, 4 minutes each). EEG data were recorded using a\n62-channel cap at 200 Hz, following the 10-20 system. The\ndataset includes raw data and features such as PSD and DE\nacross five frequency bands. Data from all sessions are used\nfor subject-dependent tasks, while the first session is used for\ncross-subject tasks.\n2) SEED-IV: The SEED-IV [24] dataset comprises 15 sub-\njects (7 males, 8 females), each participating in three sessions\nwith 24 trials per session. Each 2-minute trial includes videos\nevoking happiness, sadness, fear, or neutrality. EEG data were\nrecorded at 1000 Hz using a 62-channel cap. The dataset offers\nraw data and preprocessed features such as PSD and DE. All\nsessions' data are used for subject-dependent tasks, while the\nfirst session is used for cross-subject tasks.\n3) DEAP: The DEAP [21] dataset consists of 32 subjects\n(16 males, 16 females) who participated in one session con-\ntaining 40 trials. Each trial involved a 1-minute music video\ndesigned to evoke specific emotions, followed by participant\nratings for arousal, valence, dominance, and liking on a 1-9\nscale. EEG data were recorded using a 32-channel cap at 512\nHz, and the dataset includes both raw and downsampled (128\nHz) versions. All data are used for both subject-dependent and\ncross-subject tasks.\n4) MAHNOB-HCI: The MAHNOB-HCI [23] dataset in-\ncludes 30 subjects (13 males, 17 females), though EEG data\nare completely missing for 2 subjects and partially missing\nfor 3. Each subject participated in a single session with\n20 trials, where videos lasting 34.9 to 117 seconds evoked\nemotions. After each video, participants rated arousal, valence,\ndominance, and predictability on a 1-9 scale. EEG data were\nrecorded using a 32-channel system at 512 Hz, and the dataset\nincludes both raw and downsampled (128 Hz) versions. Both\ncomplete and partial data are used for subject-dependent and\ncross-subject tasks."}, {"title": "B. Data Preprocessing", "content": "A significant challenge in reproducing extant studies on\nEER is the lack of a standardized data preprocessing protocol.\nPublicly available EER datasets are not immediately usable\nand require extensive preprocessing. Moreover, this critical\ninformation is rarely detailed in the literature, and many open-source packages do not provide the necessary data scripts. To\naddress this issue, our benchmark proposes a comprehensive\ndata preprocessing framework specifically designed for EER\nresearch. The procedure includes: (1) applying bandpass fil-\ntering between 0.3 and 50 Hz; (2) eliminating eye movement\nartifacts using Principal Component Analysis (PCA); (3) ex-\ntracting DE features across five frequency bands-[0.5, 4], [4,\n8], [8, 14], [14, 30], and [30, 50]-followed by processing with\na Linear Dynamic System (LDS); and (4) segmenting data\nusing 1-second non-overlapping sliding windows to enhance\nthe dataset."}, {"title": "C. Experimental Task", "content": "The field of EER encompasses various experimental tasks,\neach designed to address distinct challenges with correspond-\ning models. The primary experimental tasks are categorized\nas follows: (1) Subject-dependent: This task evaluates model\nperformance for individual subjects, necessitating both training\nand testing data from the same subject. (2) Cross-subject: This\ntask aims to generalize model performance across different\nsubjects, requiring training and testing data from different\nsubjects. (3) Subject-independent: This task assesses model\nperformance across a group of subjects, without needing\ntraining and testing data from the same or different subjects.\n(4) Cross-session: This task examines model performance\nacross different sessions of the same subject, requiring training\nand testing data from different sessions of the same individual.\nThe subject-independent task lies between the subject-\ndependent and cross-subject tasks, with the latter two being\nmore aligned with real-world applications. The cross-session\ntask necessitates datasets with multiple sessions, which many\ndatasets lack. Therefore, our benchmark focuses on subject-\ndependent and cross-subject tasks, as these are the most\ncommonly utilized and hold significant practical relevance.\nAdditionally, it is important to note that we treat data from\ndifferent sessions of the same subject as if they originate from\ndistinct subjects."}, {"title": "D. Data Splitting", "content": "Data splitting is a critical process that delineates how a\ndataset is partitioned into training and test sets, significantly\ninfluencing model performance. However, prevailing practices\nin this domain often exhibit a lack of rationality and the\nabsence of a coherent, standardized approach. For example,\nmany current data-splitting methodologies fail to incorporate\na validation set for model selection and do not consistently\nadhere to cross-trial configurations, resulting in unreliable\nexperimental outcomes.\nTo mitigate these shortcomings, we propose a systematic\nand rational data-splitting methodology in our benchmark.\nSpecifically, for tasks that are subject-dependent, we allocate\neach individual's data into training, validation, and test sets\nin a 0.6:0.2:0.2 ratio, adjusting as necessary for datasets\nthat do not divide evenly. In the case of cross-subject tasks,\nwe similarly partition subjects into training, validation, and\ntest sets following the same 0.6:0.2:0.2 ratio. Furthermore,\nour benchmark rigorously adheres to the cross-trial principle,\nensuring that samples from the same trial do not appear in\nboth the training and test sets."}, {"title": "E. Evaluation Methods", "content": "The evaluation method determines how the results reported\nin the paper are calculated. This part of the experimental setup\nis also where many studies encounter the most significant\nissues. Currently, many studies report the best results of the\nmodel on the test set for each epoch. This evaluation method\nis seriously flawed as it greatly exaggerates the performance\nof the model.\nTherefore, in our benchmark, our evaluation method reports\nthe performance on the test set of the model that achieved the\nhighest F1 score on the validation set across all epochs. We\nuse accuracy and F1 score, the most commonly used metrics\nin existing research, and report both the mean and standard\ndeviation. The methods for calculating accuracy and F1 score\nare presented in Eq. 5 and Eq. 6, respectively.\n$ACC = \\frac{TP + TN}{TP + TN + FP + FN}$\n$F1 Score = \\frac{2 \\times TP}{2 \\times TP + FP + FN}$"}, {"title": "IV. LIBEER TOOLKIT FRAMEWORK", "content": "In this section, we will present the framework of the\nalgorithm library to facilitate researchers' utilization of these\nmodels in the domain of EER. Specifically, we will detail\nthe coding procedures associated with the data loader, data\nsplit, and model training and evaluation within the LibEER.\nTo facilitate the utilization of the LibEER library in the field\nof EER, users can easily install via pip, which stremlines\nthe installation process and ensures that all necessary depen-\ndencies are managed automatically. For further details and\naccess to the source code, please visit the official repository\nat https://github.com/ButterSen/LibEER.\nLibEER offers a user-friendly and equitable platform for\nEER implemented using PyTorch. It comprises three primary\nmodules: the data loader, data splitting, and model training\nand evaluation. The data loader module standardizes the\ndata formats across various datasets and provides a range of\npreprocessing techniques. The data splitting module delivers\na cohesive solution applicable to different experimental tasks\nand partitioning methods. The model training and evaluation\nmodule presents a standardized and extensible framework for\nconducting model training and evaluation. Figure 2 illustrates\nthe framework of LibEER. Below, we provide detailed expla-\nnations and usage examples for each module."}, {"title": "1) Data loader:", "content": "The data loader module primarily com-\nprises two components: dataset reading and EEG data prepro-\ncessing. Currently, the data formats in EEG emotion recog-\nnition datasets lack uniformity, necessitating distinct data\nloading methods for different datasets, which ultimately re-\nduces research efficiency. To address this challenge, LibEER\noffers the get_uniform_data() function, which facilitates the\nreading of datasets through tailored methods for each dataset,\nsubsequently integrating the data into a standardized format\nof (session, subject, trial). This approach not only streamlines\nsubsequent preprocessing tasks but also enhances compatibil-\nity with various experimental tasks and data-splitting methods.\nAdditionally, LibEER provides the preprocess() function,\nwhich allows for the preprocessing of data based on user-\ndefined settings. The preprocess() function operates on a trial\nbasis and encompasses three primary steps: noise removal,\nfeature extraction, and sample segmentation. When utilizing\nraw data directly from the dataset, only sample segmentation is\nexecuted. Noise removal aims to eliminate electromyography\n(EMG) noise, electrooculography (EOG) artifacts, and other\nforms of interference from the EEG signals. The feature\nextraction process analyzes the data to derive relevant EEG\nfeatures, such as DE and PSD features, based on the specified\nfeature types and frequency bands provided by the user. Sam-\nple segmentation organizes the data into user-defined sample\nlengths. Ultimately, the processed data is integrated into the\nformat of (session, subject, trial, sample). This functionality is\nillustrated in the usage examples of LibEER for data loading."}, {"title": "2) Data split:", "content": "The data splitting module is primarily re-\nsponsible for partitioning the dataset into training, testing, and\nvalidation sets. LibEER performs this division based on the\nexperimental tasks and splitting methods selected by the user.\nFor the two experimental tasks, the merge_to_part() function\nis utilized to integrate the data into a standardized format.\nIn the context of subject-dependent tasks, the\nmerge_to_part() function organizes the data into the\nformat of (subject, trial, sample), wherein the data for each\nsubject is treated as an independent sub-task, allowing for\nsubsequent data splitting on a trial basis for each sub-task.\nConversely, for cross-subject and cross-session tasks, the\nmerge_to_part() function consolidates the data into (subject,\nsample) and (session, sample) formats, with data splitting\nconducted on a subject or session basis.\nTo accommodate various splitting methods, the\nget_split_index() function further divides the data based on\nthe specified formats and labels. The splitting methodologies\navailable in LibEER include fixed ratio splitting, which\npartitions the data into training and testing sets according to\na predetermined ratio; early stopping splitting, which divides\nthe data into training, validation, and testing sets while\nmaintaining balanced label distribution; and cross-validation"}, {"title": "3) Model training and evaluation:", "content": "Model training and\nevaluation are essential processes responsible for training the\ndesignated model and assessing its performance. LibEER\nconstructs models based on the selected architecture and pa-\nrameters, subsequently training them according to the specified\ntraining configurations. In scenarios where a validation set\nis not available, LibEER identifies the optimal performance\non the testing set during training as the final outcome for\nthat training iteration. Conversely, when a validation set is\nutilized, LibEER selects the network weights that yield the best\nperformance on the validation set for evaluation against the\ntesting set, thereby determining the final result of the training\nround.\nThe ultimate outcomes of the model will be reported\naccording to the performance metrics specified by the user."}, {"title": "V. EXPERIMENTAL VALIDATION", "content": "In this section, we thoroughly validate the importance of\nLibEER and our benchmark through extensive experimenta-\ntion. Initially, we utilize LibEER to replicate the selected\nmodels as closely as possible following the experimental\nconfigurations detailed in the original publications. Subse-\nquently, we employ LibEER to compare the performance of\nall models based on the established benchmark. Lastly, we\nconduct ablation studies to address specific issues identified\nduring the experimental phase."}, {"title": "A. Model Reproduction", "content": "To ensure the reliability of our benchmark, we conducted\na meticulous replication of the representative EER methods\nusing the original configurations and compared them with\nthe results reported in the original paper. Table IV presents\nboth our reimplementation outcomes and those reported in\nthe original study with the gap. Additionally, we detail the\nspecific original experimental setups, clearly illustrating the\ndifferences in experimental details among various models.\nFrom the results in the Table IV and the replication process,\nwe can draw the following conclusions:"}, {"title": "B. Model Comparison", "content": "As shown in Table IV, the implementation details of dif-\nferent methods vary significantly, making it difficult to fairly\ncompare their performance and draw reliable conclusions for\nthe advancement of the EER field. To this end, we conduct\na fair comparison of all methods based on the unified and\nreasonable benchmark proposed in Section III. The results\nfor the two experimental tasks are reported in Tables V\nand VI, respectively. We select SVM [28], one of the most\ncommonly used machine learning algorithms, as the baseline\nmodel. Its results are presented first. In the two tables, the\ntop two methods in each scenario are highlighted using bold\nand underlined formatting. From these results, we make the\nfollowing key observations.\nObservation 1. The graph-structured information between\nEEG signal channels is crucial for EER tasks. Several\nGNN-based models achieved superior performance.\nTo provide a clearer illustration of the performance of\neach method under the benchmark, we ranked each method's\nperformance in each scenario. The highest-ranking method\namong the n methods receives n points, while the lowest-\nranking method receives 1 point. The total score for each\nmethod across all scenarios is then accumulated, and the\nscoring results for both experimental tasks are presented in\nFigure 6 (a) and (b).\nFrom Figure 6 (a), it can be observed that in the subject-\ndependent task, DGCNN achieved the best performance with\na total score of 140. Following closely are GCBNet and its\nvariant GCBNet_BLS, both scoring 132 points. CDCN and\nRGNN ranked fourth and fifth with scores of 126 and 105,\nrespectively. In contrast, EEGNet, Tsception, and ACRNN\nperformed relatively poorly. In the cross-subject task (see\nFigure 6 (b)), the overall performance ranking of each model is\nsimilar to that in the subject-dependent scenario. Notably, the\nDBN model, which has a relatively simple architecture, rises\nto third place in the cross-subject scenario. In contrast, the MS-\nMDA model, specifically designed for cross-subject scenarios,\nshows significant variability across different datasets, resulting\nin a moderate ranking in the overall score.\nIn terms of model types, the four GNN-based methods\ndemonstrated superior performance on both tasks, providing\nstrong evidence that the graph-structured information between\nEEG signal channels is crucial for EER tasks. In contrast, the\nCNN-based methods exhibited varying levels of performance\non both tasks. The DNN and Transformer-based methods\nshowed average results on the subject-dependent task, while\nthe DNN-based methods performed better on the cross-subject\ntask. The RNN-based methods performed relatively poorly.\nHowever, for the Transformer and RNN-based methods, as\nonly one model was selected from each category, the results\nmay contain some occasionality. Based on the current results,\nwe recommend that researchers in the EER field focus more\non using GNN-based models when designing their models and\nemphasize the channel graph structure information inherent in\nEEG signals.\nObservation 2. The issue of significant variability in EEG\ndata among different subjects remains unaddressed, with\nall methods showing large standard deviations in subject-\ndependent scenarios and relatively low performance in\ncross-subject scenarios.\nTable V demonstrates that the standard deviations for each\nmethod across various scenarios are significantly high, av-\neraging approximately 15-20%. Table VI indicates that the\nperformance of the methods is relatively low, suggesting con-\nsiderable room for improvement. The pronounced variability\nin EEG data among subjects has long been a critical issue in\nthe field of EER, despite some methods asserting the inclusion\nof specific modules to tackle this challenge. However, under\nour unified and systematic benchmark, the models failed to\neffectively address the variability in data from different sub-\njects, as reported in the original studies. Consequently, further\nin-depth investigation is necessary to adequately confront the\nchallenges posed by the substantial differences in EEG data\namong various subjects.\nObservation 3: The scarcity of EEG data limits the\neffective representation learning capabilities of deep learn-\ning methods, leading to some deep learning approaches\nperforming even worse than traditional machine learning\nmethods, such as SVM.\nAs demonstrated in Tables V and VI, several carefully\ndesigned deep learning algorithms did not outperform the\nbaseline SVM, as reported in their original studies. A signif-\nicant contributing factor to this discrepancy is the scarcity of\nEEG data, characterized by both limited quantity and scale\nof datasets. Deep networks typically necessitate substantial\ndata volumes to fully leverage their potential for represen-\ntation learning. Additionally, many studies lack validation\nsets in their experimental designs, which can easily result in\noverfitting and inflated performance metrics in contexts with\nlimited data. In contrast, our rigorous experimental setup has\nhighlighted these shortcomings. Therefore, a prudent strategy\nwould be to utilize various data augmentation techniques [41]-\n[43] to expand the dataset and to implement strategies aimed\nat reducing model overfitting.\nObservation 4. The data collection methods for different\nEEG datasets vary, resulting in differences in data quality\nand task difficulty. Several methods performed better on\nthe SEED dataset, which has a more reasonable data\ncollection approach.\nTo visually illustrate the performance differences of various\nmodels across different datasets, we created radar charts for\nboth experimental tasks, as shown in Figure 7. From the results\npresented in Figure 7, we summarize that models performed"}, {"title": "C. Discussion of Ablation Study", "content": "In this section", "length": "In the ablation study on sample length", "types": "In the ablation study on feature types,\nwe utilized both DE and PSD features, along with their\nrespective variants processed by Locality Differential Sparse\n(LDS), as inputs. The results are presented in Figure 9. As\nillustrated in Figure 9, in the subject-dependent scenario, the\nDE and PSD features demonstrate comparable performance,\nwhile both feature types processed by LDS show significant\nimprovements. In the cross-subject scenario"}]}