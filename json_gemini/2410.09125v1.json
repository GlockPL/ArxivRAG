{"title": "Training on Fake Labels: Mitigating Label Leakage in Split Learning via Secure Dimension Transformation", "authors": ["Yukun Jiang", "Peiran Wang", "Chengguo Lin", "Ziyue Huang", "Yong Cheng"], "abstract": "Two-party split learning has emerged as a popular paradigm for vertical federated learning. To preserve the privacy of the label owner, split learning utilizes a split model, which only requires the exchange of intermediate representations (IRs) based on the inputs and gradients for each IR between two parties during the learning process. However, split learning has recently been proven to survive label inference attacks. Though several defense methods could be adopted, they either have limited defensive performance or significantly negatively impact the original mission. In this paper, we propose a novel two-party split learning method to defend against existing label inference attacks while maintaining the high utility of the learned models. Specifically, we first craft a dimension transformation module, SecDT, which could achieve bidirectional mapping between original labels and increased K-class labels to mitigate label leakage from the directional perspective. Then, a gradient normalization algorithm is designed to remove the magnitude divergence of gradients from different classes. We propose a softmax-normalized Gaussian noise to mitigate privacy leakage and make our K unknowable to adversaries. We conducted experiments on real-world datasets, including two binary-classification datasets (Avazu and Criteo) and three multi-classification datasets (MNIST, FashionMNIST, CIFAR-10); we also considered current attack schemes, including direction, norm, spectral, and model completion attacks. The detailed experiments demonstrate our proposed method's effectiveness and superiority over existing approaches. For instance, on the Avazu dataset, the attack AUC of evaluated four prominent attacks could be reduced by 0.4532\u00b10.0127.", "sections": [{"title": "Introduction", "content": "Deep learning has been applied in many areas of people's daily lives. However, the paradigm of data-centralized deep learning has been continuously questioned since people's concerns about their privacy rose. For a typical scene, consider that an online shopping company A owns the clients' purchase records, while an online video website owner B has the clients' advertisement data. To learn how the advertisement on B will impact clients' purchase tendency on A, A and B need to learn a global model using A's data labels and B's features. However, due to privacy concerns and regulations such as GDPR for clients' data, A and B cannot directly share their data to train a global model (Turina et al. 2021; Thapa, Chamikara, and Camtepe 2021; Zhang et al. 2023; Turina et al. 2020).\nSplit learning (Yang et al. 2019; Gupta and Raskar 2018; Poirot et al. 2019; Vepakomma et al. 2018a; Liu et al. 2024) is a solution to such a scenario, allowing the feature and label owners to train a machine learning model jointly. In split learning, the neural network's training process is split into the non-label and label parties (Langer et al. 2020). At the beginning of split learning, the two parties apply Private Set Intersection (PSI) protocols (Abuadbba et al. 2020) to find the intersection of their data examples and establish the alignment of data example IDs. During training, the non-label party will use a bottom model to obtain the intermediate layer of its data examples and send it to the label party. Then, the label party will complete the rest of the neural network training process. The label party will apply a top model to predict the non-label party's data examples. Next, in the backpropagation process, the label party computes the gradient from its loss function and sends the parameter gradient back to the non-label party. Thus, the non-label party can take the parameter gradient from the label party to train its bottom model. Split learning has been applied in many areas (Pham et al. 2023; Abuadbba et al. 2020; Lin et al. 2024; Wu et al. 2023b; Matsubara, Levorato, and Restuccia 2022; Samikwa, Di Maio, and Braun 2022; Wu et al. 2023a; Park et al. 2021).\nThough the goal of split learning is to preserve privacy for both parties, the challenge of privacy leakage still exists within split learning. The non-label party can establish a gradient inversion attack (Kariyappa and Qureshi 2021), a label leakage attack that allows an adversarial input owner to learn the label owner's private labels by exploiting the gradient information obtained during split learning. In the gradient inversion attack, the label leakage attack is treated as a supervised learning problem by developing a novel loss function using specific key properties of the dataset and model parameters instead of labels. The non-label party can derive the labels of data examples via a gradient inversion attack, thus harming the privacy of the label party. Split learning urgently requires a protection mechanism to guard the privacy of label parties. In this paper, we propose these contributions to achieve this goal:\n\u2022 We craft a dimension transformation module that could"}, {"title": "Background and Related Work", "content": "Privacy Leakage in Split Learning\nThough in split learning, the label party and non-label party will only share intermediate representation, privacy leakage threats still exist in this system (Geiping et al. 2020; Jin et al. 2021; Wu, Li, and He 2022; Jiang et al. 2022; Qi et al. 2022).\nFeature leakage. First, the forward intermediate representations can leak the private feature information in the non-label party. (Vepakomma et al. 2019) is the first to such feature leakage in split learning and provided a defense solution using distance correlation. By creating a novel loss function employing specific key properties of the dataset and model parameters, (Kariyappa and Qureshi 2021) created a Gradient Inversion Attack (GIA), which converts the attack into a supervised learning problem.\nLabel leakage. Second, the backward intermediate representation can leak the private label information in the label party. (Li et al. 2022) propose a norm-based method for leaking private labels in the conversion prediction problem. Their method is inspired by the high-class imbalance of the training dataset for the conversion prediction task. Due to this imbalance, the gradients' magnitude is more significant when the unusual class is encountered. So, an adversarial input owner can infer the private class labels by considering the gradients' norm. Instead, they investigate whether label information may be disclosed through backward interaction from the label party to the non-label party. (Fu et al. 2022) discover that a malicious participant can exploit the bottom model structure and the gradient update mechanism to gain the power to infer the privately owned labels. Worse still, by abusing the bottom model, he/she can infer labels beyond the training dataset. Based on their findings, they propose a set of novel label inference attacks. (Sun et al. 2022) uses SVD to find correlations between embeddings and labels. Analyzing the mean and singular vectors assigns scores, clusters samples, and infers labels, bypassing privacy protections. (Liu et al. 2021) first investigate the potential for recovering labels in the vertical federated learning context with HE-protected communication and then demonstrate how training a gradient inversion model can restore private labels. Additionally, by directly substituting encrypted communication messages, they demonstrate that label-replacement backdoor attacks may be carried out in black-boxed VFL (termed \"gradient-replacement attack\u201d).\nOur work SECDT focuses on defending label leakage threats.\nPrivacy Protection in Split Learning\nTechniques to protect communication privacy in FL generally fall into three categories: 1) cryptographic methods such as secure multi-party computation (Bonawitz et al. 2017; Zhang et al. 2020; Wan et al. 2024; Pereteanu, Alansary, and Passerat-Palmbach 2022); 2) system-based methods including trusted execution environments (Subramanyan et al. 2017); and 3) perturbation methods that shuffle or modify the communicated messages (Abadi et al. 2016; McMahan et al. 2017; Zhu, Liu, and Han 2019; Erlingsson et al. 2019; Cheu et al. 2019; Cheng et al. 2021). Defense against label leakage threats can be categorized into the 3rd category. Previous researchers have proposed several defense schemes to defend against label leakage threats. (Li et al. 2022) propose Marvell, which strategically determines the form of the noise perturbation by minimizing the label leakage of a worst-case adversary. The noise is purposefully designed to reduce the gap between the gradient norms of the two classes, which deters the attack. However, it requires a large amount of extra computation that slows down the speed of split learning. (Abadi et al. 2016) proposed using differential privacy in deep learning to protect privacy. We utilize differential privacy in vertical federated learning on the guest side to safeguard the confidentiality of data labels. (Wei et al. 2023) propose MixPro, an innovative defense mechanism against label leakage attacks in Vertical Fed-"}, {"title": "Label Inference Attack", "content": "System Model\nIn two-party split learning, considering a training dataset {X_i, Y_i}_{i=1}^N, the host (non-label party) has the features {X_i}_{i=1}^N and the guest (label party) has the corresponding labels {y_i}_{i=1}^N as shown in Figure 1. They jointly train a model based on the training dataset {Xi, Yi}_{i=1}^N without leaking {X_i}_{i=1}^N and {y_i}_{i=1}^N to each other. Specifically, the host trains a bottom model E while the guest trains a top model C. In each iteration, the host sends the forward embedding output z = E(X) of the bottom model E to the guest, then the guest will use z as the input of the top model C and send the backward gradient g to the host. After receiving the gradient g, the host uses g to update the bottom model code E. The host will not access the label {y_i}_{i=1}^N, while the guest will not access the data feature {X_i}_{i=1}^N, thus protecting the privacy of both the host and the guest.\nPotential Attacks\nThen we consider the malicious host who wants to get the label held by the guest. We consider five types of attacks in our threat model:\nDirection attack (Li et al. 2022) For a given example, all examples of the same class provide positive cosine similarity, but all examples of the opposite class produce negative cosine similarity. The non-label party can identify the label of each case if the problem is class-imbalanced and knows there are more negative examples than positive ones. The class is negative if more than half of the examples produce positive cosine similarity; otherwise, it is positive. In many real-world scenarios, the non-label party can reasonably guess which class contains most examples in the dataset without ever having any access to the data. For instance, in disease prediction, the prevalence of a given disease in the general population is almost always much lower than 50%.\nNorm attack (Li et al. 2022) The model tends to be less confident during training that a positive example will be positive than a negative example will be negative. Moreover, for both the positive and negative cases, the norm of the gradient vector ||g||_2 is on the same order of magnitude (has an equivalent distribution). As a result, the gradient norm ||g||_2 for positive examples is typically higher than for negative instances. The scoring function r_n(g) = ||g||_2 is a good predictor of the hidden label y. The privacy loss (leak AUC) compared to the attack r_n is known as the norm leak AUC.\nSpectral attack (Sun et al. 2022). The spectral attack utilizes singular value decomposition (SVD) to exploit the correlation between intermediate embeddings and private labels in machine learning models. By calculating the mean and top singular vector of the embeddings, the attack assigns scores to each sample, clusters them, and then infers the labels based on the distribution of these scores, effectively stealing private labels despite the presence of certain privacy-preserving techniques.\nModel competition attack (Fu et al. 2022). The model competition attack involves an adversary obtaining a trained bottom model, adding random inference layers to form a complete model, and then fine-tuning it with a small set of auxiliary labeled data. The adversary refines the model using semi-supervised learning algorithms tailored to the dataset's domain. The result is a fully-trained model that can predict labels for any data, allowing the adversary to infer labels for any sample of interest while adhering to the rules of VFL, thus conducting a passive attack without active interference."}, {"title": "The Proposed SECDT", "content": "Overview\nFigure 2 shows the training workflow of our proposed SECDT, which comprises three parts, i.e., dimension trans-"}, {"title": "Dimension Transformation", "content": "formation, gradient normalization, and randomization. In two-party split learning, there is a host (feature party) and a guest (label party), but only the guest has the training labels. However, the private label information could be inferred by the offensive host from the backward gradients {gi}_{i=1}^N. Generally speaking, existing label inference attacks against split learning are designed from two perspectives: direction and magnitude (Li et al. 2022; Sun et al. 2022; Fu et al. 2022). Therefore, the dimension transformation is crafted to achieve bidirectional mapping between original and increased K-class labels, mitigating label leakage from the directional perspective. Then, to remove the magnitude's divergence of gradients, we design a magnitude normalization algorithm to achieve this goal. Furthermore, we introduce a randomization module in which two random noises are proposed to mitigate privacy leakage and make our K unknowable to adversaries. All the defenses are conducted by the guest, which is invisible to the host.\nIt is intuitive that for the adversaries, the original classification tasks are more vulnerable to label inference attacks than the multi-class classification tasks, especially from the directional perspective.\nDimension-increased transformation Could we transform original labels to multi-class labels during model training? To achieve this goal, we craft the dimension transformation module. Transfer from binary to multi-class: Specifically, given dataset D = {Xi, yi}_{i=1}^N with labels {y_i}_{i=1}^N, we have\nOne-hot(yi) = \\hat{y_i} \\in One-hot({0,\u2026\u2026,k\u22121}), (1)\nwhere One-hot(\u00b7) converts numbers 0 to k \u2212 1 to one-hot encodings. For instance, if k = 3 (i.e., the task is a three-category task), we have One-hot(0, 1, 2) = {[1,0,0], [0, 1, 0], [0, 0, 1]}. Then, given labels {y_i}_{i=1}^N and targeted (increased) dimension K, we define a mapping M_{k,K} from k-dimension to K-dimension as\nM_{k,K}(Yi) = M_{K,yi}, (2)\nwhere M_{K,yi} are elements that randomly selected from corresponding mapping pool \\rho_{k,y_i} for label yi. To generate the required mapping pools \\rho_{K,y}, with specific K, an ordered set of one-hot encodings is defined as\n\u03a8 = {One-hot(0),\u2026\u2026, One-hot(K \u2212 1)}. (3)\nThen, we shuffle \u03a8 randomly or according to specific rules to get\n\u03a8^S = Shuffle(\u03a8) (4)\nand separate \u03a8_s into k disjoint mapping pools given by\n\\rho_{K,0},..., \\rho_{K,k-1}, (5)\nwhere U_{K,y = 0}^{k-1} \\rho_{K,y} = \u03a8^S. There could be many rules for dividing \u03a8_s to \\rho_{K,y}. For simplicity, in this work, unless otherwise mentioned, we divide \u03a8_s equally to \\rho_{K,0},..., \\rho_{K,k\u22121}. Indicating the length (number of elements) of \\rho_{k,y} by \\sigma_y = \\frac{K}{k}, for each \\rho_{k,y}, we have\n\\rho_{K,y} = {\\tau_{y,0},...,\\tau_{y,\\sigma_y-1}}. (6)\nThen, we have the dataset D_K = {X_i, M_{k,K}(Yi)}_{i=1}^N with increased the K-dimension labels to optimize the objective\nmin_{E,C} L(\\theta, D_K) = \\frac{1}{N}min \\sum_{i=1}^N L(C(E(X)), M_{k,K}(Yi)), (7)\nwhere L(,) is for computing the cross entorpy. Because dimension transformation works before training of the model and the dimension-increased labels are fixed, optimizing the Equation 7 could also optimize the objective\nmin_{E,C} L(\\theta, D) = min_{E,C} min \\frac{1}{N} \\sum_{i=1}^N L(E(C(X)), Yi). (8)"}, {"title": "Dimension-decreased transformation", "content": "Dimension-decreased transformation The K-dimension prediction could not be directly used as the final result during inference time. Intuitively, with the K-dimension prediction p^{(K)} = C(z) = C(E(X)), we could derive original k-dimension inference result p based on a maximum mapping function \\mathcal{MM}(\u00b7) given by\np = \\mathcal{MM}(p^{(K)}) = y, if p^{(K)} \\in \\rho_{K,y}. (9)\nHowever, as turning a k-classification task into a K-classification task will increase the difficulty of model training, the aforementioned function \\mathcal{MM}(\u00b7) results in compromised model performance compared with the original k-classification task. We believe this is because the data whose labels belong to the same mapping pool \\rho_{K,y} essentially have similar characteristics. We believe this is because the data samples used for inference are similar but not identical to the training samples, resulting in the p^{(K)} to be linear combinations of one-hot encodings in \\rho_{K,y} for feature X. To overcome this challenge, in our SECDT, we propose to realize performance-reserved dimension-decreased transformation based on a novel weighted mapping function \\mathcal{WM}(\u00b7) given by\np = \\mathcal{WM}(p^{(K)}) = arg max_{0<y<k-1}Wy.p^{(K)}, (10)\nwhere P^{(K)} is the weight. Here, w_{y}.p^{(K)} is the inner product of w_y and \u00b7p^{(K)}, where w_y represents the result of element-wise addition (+) of one-hot encodings in the mapping pool \\rho_{K,y} that\nWy = \\tau_{y,0} \\oplus \\tau_{y,1} \\oplus ... \\oplus \\tau_{y,\\sigma_u-1}. (11)\nWith function \\mathcal{WM}(\u00b7), we covert the linear combination of one-hot encodings in \\rho_{K,y} to the inner product as the confidence that P^{(K)} belongs to label y. Our results show that the proposed weighted mapping significantly guarantees the effectiveness of the task model."}, {"title": "Gradient Normalization", "content": "Inspired by (Cao et al. 2021) that normalizes gradients from suspicious clients in horizontal federated learning, in SECDT, we make the first attempt to normalize gradients in the cut layer of split learning, which could fully avoid label inference attacks conducted based on the gradients' magnitude. (Cao et al. 2021) normalize gradients with a trustworthy gradient locally computed by the server, but there is no single trust-worthy gradient in our SECDT. Hence, since all gradients in our SECDTare clean (i.e., have not been manipulated by adversaries), literally, each gradient could be used to normalize others. In our SECDT, the minimum, mean, and maximum l2-norm among all gradients in the current mini-batch could be used to realize normalization. In this paper, unless otherwise mentioned, the mean l2-norm is considered as the standard norm for normalization. Specifically, when the batch size is set to be B, during each iteration, B gradients {g_b}_{b=1}^B are computed by the guest. Then, we normalize these gradients as\n\\hat{g_b} = \\frac{g_b}{\\parallel g_b \\parallel} \\phi, (12)\nwhere \\parallel \u00b7 \\parallel represents l2-norm and \\phi is the selected standard l2-norm. For instance, considering the mean l2-norm as the standard, we have\n\\phi = \\frac{1}{B} \\sum_{b=1}^B \\parallel g_b \\parallel. (13)"}, {"title": "Noise-based Randomization", "content": "Considering potential adaptive attacks against our proposed SECDT, which may succeed after an attacker could infer our mapping pools \\rho_{K,y}. Hence, we aim to keep our increased dimension K confidential to adversaries, making mapping pools unknowable. In this work, we propose adding Softmax-normalized Gaussian noise (SGN) to make our increased dimension K agnostic to adversaries. Due to the task independence of introduced noise, the noise could also mitigate privacy leakage of split learning to existing attacks. We assume that during SECDT's model training phase, each sample's label (target) is a K-dimension one-hot encode \\tau = [\u03b6_1, \u03b6_2,..., \u03b6_K]. Then, we propose two noises as follows.\nSoftmax-normalized Gaussian noise For each sample's one-hot encoded label y, the guest generates a noise vector\n\\gamma = [\\omega_0, \\omega_1,...,\\omega_{K-1}], (14)\nwhere \\omega follows standard Gaussian distribution, i.e., \\varepsilon ~ \\mathcal{N}(0, 1). Moreover, to make this noise more controllable, the guest normalizes \\gamma based on the softmax function as\n\\hat{\\gamma} =Softmax(\\gamma) \\begin{cases} \\frac{e^{\\omega_0}}{\\sum_{j=0}^{K-1} e^{\\omega_j}}\\\\ \\frac{e^{\\omega_1}}{\\sum_{j=0}^{K-1} e^{\\omega_j}}\\\\ ...\\\\ \\frac{e^{\\omega_{K-1}}}{\\sum_{j=0}^{K-1} e^{\\omega_j}} \\end{cases} (15)\nAdding noise With generated noise for each sample's label \\tau, we add them into the initial label to obtain\n\\tau'= \\tau \\oplus (\\mu . \\hat{\\gamma}), (16)\nwhere \\mu is used to determine the noise level."}, {"title": "Evaluation Setup", "content": "In this section, we first discuss the datasets (\u00a7), model architecture, and environment () for evaluation. Then, we introduce the evaluated attacks (\u00a7) and the compared schemes (\u00a7) in our evaluation.\nDatasets\nWe selected three image datasets for the multiple classifications and two click prediction datasets for the binary classification:\n\u2022 Criteo (dat 2014b): Criteo is a CTR dataset provided by Criteo. The training set consists of some of Criteo's traffic over seven days. Each row corresponds to a display ad served by Criteo.\n\u2022 Avazu (dat 2014a): Avazu is one of the leading mobile advertising platforms globally. It consists of 10 days of labeled click-through data for training and one day of ads data for testing (yet without labels). Only the first ten days of labeled data are used for benchmarking.\nEvaluated Attacks\nIn our experiment, we evaluated four types of label inference attacks:\n\u2022 Direction attack (Li et al. 2022). When a non-labeled party knows the class imbalance, it can identify labels based on cosine similarity. If more negative examples exist, a higher proportion of positive similarity suggests a negative class, and vice versa.\n\u2022 Norm attack (Li et al. 2022). The model's confidence in positive predictions is lower than in negative ones. The gradient norm \\parallel g \\parallel_2 for positive examples is usually higher, making it a good predictor for the hidden label y. This attack's privacy loss is measured by the norm leak AUC.\n\u2022 Spectral attack (Sun et al. 2022). This attack uses SVD to find correlations between embeddings and labels. Analyzing the mean and singular vectors assigns scores, clusters samples, and infers labels, bypassing privacy protections.\n\u2022 Model competition attack (Fu et al. 2022). An adversary obtains a bottom model, adds layers, and fine-tunes it with labeled data. Using semi-supervised learning, they create a model that can predict any data label, enabling passive inference without active interference.\nEvaluated Schemes\nWe evaluated five label inference attack defense schemes in our experiments:\n\u2022 No defense. The pure classification task in split learning without any defense mechanisms.\n\u2022 Marvell. (Li et al. 2022) proposed a random perturbation technique, which strategically finds the structure of the noise perturbation by minimizing the amount of label leakage (measured through our quantification metric) of a worst-case adversary (called Marvell).\n\u2022 DP. (Abadi et al. 2016) proposed using differential privacy in deep learning to protect privacy. We utilize differential privacy in vertical federated learning on the guest side to protect the privacy of data labels.\n\u2022 MixPro. MixPro (Wei et al. 2023) is an innovative defense mechanism against label leakage attacks in Vertical Federated Learning (VFL) that employs a two-step approach: Gradient Mixup and Gradient Projection.\n\u2022 SECDT. We proposed a dimension transformation method called SECDT, which transforms the classification task into a fake label classification task on the guest"}, {"title": "Ablation Study", "content": "Impact of Expanded Dimension Size\nWe first evaluate the impact of expanded dimension size on the performance of split learning. We applied all four attack schemes for the Avazu and Criteo datasets, while for MNIST, Fashion-MNIST, and CIFAR-10 datasets, we only applied the model competition attacks.\nAs the results shown in Figure 5, the increasing dimension size does not affect the performance of the test utility. While the attack utility is strongly affected as the dimension size increases. For experiments on Avazu and Criteo, the norm, direction, and spectral attack utilization decrease soon and converge to the lower boundary as the dimension size reaches 8 (four times the original dimension size 2). The model completion attack stood steady until the dimension size reached 20 (10 times the original dimension size). This indicated that model completion attack requires SECDTto expand to a stronger dimension size to defend.\nImpact of Dimension Decrease\nFurthermore, we compared SECDTwithout dimension-decreased transformation and SECDTwith dimension-decreased transformation. Dimension decrease is vital in keeping the accuracy of split learning when applying SECDT.\nThe dimension decrease transformation is a critical component of the SECDTframework that facilitates the conversion of the increased K-class labels back to the original binary or k-class labels. This process is essential for the practical application of the model, as it ensures that the output is usable for the end-user. As detailed in \u00a7, SECDTemploys a weighted mapping function to achieve this transformation, designed to preserve the model's performance while safeguarding against label inference attacks.\nOur experiments were conducted on diverse datasets, including Avazu, Criteo, MNIST, FashionMNIST, and CIFAR-10, to evaluate the impact of the dimension decrease transformation. These datasets span various domains, comprehensively assessing the technique's effectiveness.\nThe experimental results, as depicted in Table 1 of the original document, offer profound insights into the significance of the dimension decrease transformation in SECDT. When SECDTwas applied without incorporating the dimension decrease, a notable decline in test accuracy was observed across all evaluated datasets. This decline underscores the importance of the transformation in maintaining the model's predictive power. Conversely, introducing the dimension decreased transformation and led to a substantial improvement in test accuracy. This enhancement was consistent across different datasets, demonstrating the technique's robustness. The results indicate that the weighted mapping function effectively navigates the increased dimensionality introduced during the training phase and accurately translates it back into the original label space. Furthermore, the results highlight that the dimension decrease transformation does not compromise SECDT's ability to defend against label inference attacks. The attack utility remained consistently low, even when the transformation was applied, validating the technique's efficacy as a privacy-preserving measure.\nThe dimension decrease transformation's impact was also assessed against various attack vectors, including norm attacks, direction attacks, spectral attacks, and model com-"}, {"title": "Impact of Normalization", "content": "pletion attacks. The results demonstrated that the transformation did not adversely affect SECDT's resilience against these attacks. This finding is particularly significant, as it suggests that the dimension decrease can be universally applied to enhance the security of split learning models without diminishing their defense capabilities.\nAn additional observation from the experiments is the interaction between the noise level and the effectiveness of the dimension decrease transformation. While a moderate noise level did not significantly impact test utility, higher noise levels did lead to a decline in accuracy. This observation suggests that the dimension decrease transformation can counteract the adverse effects of noise to a certain extent, providing an additional layer of protection for the model.\nImpact of Normalization\nWe compared SECDTwithout normalization and SECDTwith normalization. Normalization is vital in defending against norm attacks when applying SECDT.\nSECDT's approach to gradient normalization, as detailed in \u00a7, involves using the mean 12-norm of the gradients within a mini-batch as a standard for normalization. This method is particularly effective against attacks that rely on the magnitude of gradients to infer sensitive information. By enforcing a standard norm, SECDTdiminishes the adversaries' ability to discern differences in gradient magnitudes that could indicate the presence of certain classes within the dataset.\nThe use of gradient normalization in SECDTintroduces a trade-off between utility and privacy. While normalization can reduce the model's utility for adversaries, it may also affect the model's ability to learn complex patterns if not implemented carefully. However, our experiments, as shown in Table 2, demonstrate that SECDTsuccessfully strikes a balance, maintaining high test accuracy while significantly reducing the attack utility."}, {"title": "Impact of Noise", "content": "Finally, we evaluate the impact of noise scale on the performance of split learning. We applied all four attack schemes for the Avazu and Criteo datasets, while we only applied the model competition attacks for the MNIST, Fashion-MNIST, and CIFAR-10 datasets.\nThe incorporation of noise into the learning process is a fundamental strategy for enhancing privacy in machine learning, particularly in the context of split learning, where protecting sensitive labels is paramount. As outlined in \u00a7, SECDTintroduces noise-based randomization to obfuscate the mapping pools and to prevent adversaries from discerning the true dimensionality of the label space (K). This approach complements the dimension transformation and gradient normalization techniques within SECDT, offering a layered defense against label inference attacks.\nOur experiments were structured to meticulously evaluate the impact of noise-based randomization on the performance of SECDT. A spectrum of noise levels was applied to the label encoding process, and the outcomes were assessed against a backdrop of real-world datasets, including Avazu, Criteo, MNIST, FashionMNIST, and CIFAR-10. The objective was to determine how varying noise levels affect the model's utility for legitimate tasks and its resilience against adversarial attacks.\nThe experimental findings, as presented in Figure 6 of the original document, offer a nuanced perspective on the role of noise in SECDT. A key observation was that the introduction of noise did not significantly impair the model's performance on test tasks up to a certain threshold. This suggests that the noise was effectively integrated without disrupting the model's learning capabilities. However, test utility markedly declined beyond a noise level of 0.5. This decline indicates a critical threshold beyond which the noise erodes the model's ability to make accurate predictions. Identifying this threshold is crucial for practitioners seeking to balance privacy and utility in their models.\nIn contrast to the impact on test utility, the introduction of noise had a pronounced detrimental effect on attack utility. The results demonstrated that the model's utility for adversarial purposes decreased significantly as the noise level increased. Notably, for norm, direction, and spectral attacks, the attack utility plummeted as the noise scale surpassed 0.2, effectively neutralizing these attack vectors at moderate noise levels.\nAn interesting observation was the resilience of model completion attacks to noise up to a level of 0.4. This suggests that adversaries employing this type of attack may require a higher noise threshold to be deterred. The persistence of model completion attacks at lower noise levels underscores the need for a multi-faceted defense strategy, where noise is complemented by other techniques such as dimension transformation and gradient normalization."}, {"title": "Inference Attack towards K in SECDT", "content": "When an attacker knows our SECDTis used to learn the global model, it can adapt its attacks to SECDT. The attacker can guess the increased dimension K in SECDT. Since the mapping from 2-dimension to K-dimension M_{2,K} will reveal some clustering features. We evaluate whether the attacker can guess the real value of K in SECDT. For each data sample's backward gradient g, we iteratively set the dimension size from 2 to 2K. Then, we use the K-means clustering algorithm to cluster the gradient to the set dimension number of clusters. After that, the calinski harabasz score is used to evaluate the clustering result. The set dimension with the highest will be elected as the guessed dimension. We set the value of K/k to be 2, 5, and 10 in our experiment.\nWe evaluate SECDTw/ and w/o noise in Figure 7, and the result shows that the attacker can easily guess the dimension K of SECDTsince the frequency it guesses right is very high. Furthermore, we evaluate SECDTwith our proposed SGN and find out that the frequency that the attacker guesses the right dimension K becomes very low. Thus, the result indicates that our proposed noise can ease the proposed adaptive attack toward SECDT.\nTime Cost\nTo conduct a comprehensive assessment, we designed experiments that measured the time expenditure associated"}, {"title": "Proof of Convergence", "content": "First", "assumptions": "n1. The loss function L(\\theta) is Lipschitz continuous", "have": ""}]}