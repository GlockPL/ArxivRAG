{"title": "RDPI: A Refine Diffusion Probability Generation Method for Spatiotemporal Data Imputation", "authors": ["Zijin Liu", "Xiang Zhao", "You Song"], "abstract": "Spatiotemporal data imputation plays a crucial role in various fields such as traffic flow monitoring, air quality assessment, and climate prediction. However, spatiotemporal data collected by sensors often suffer from temporal incompleteness, and the sparse and uneven distribution of sensors leads to missing data in the spatial dimension. Among existing methods, autoregressive approaches are prone to error accumulation, while simple conditional diffusion models fail to adequately capture the spatiotemporal relationships between observed and missing data. To address these issues, we propose a novel two-stage Refined Diffusion Probability Impuation (RDPI) framework based on an initial network and a conditional diffusion model. In the initial stage, deterministic imputation methods are used to generate preliminary estimates of the missing data. In the refinement stage, residuals are treated as the diffusion target, and observed values are innovatively incorporated into the forward process. This results in a conditional diffusion model better suited for spatiotemporal data imputation, bridging the gap between the preliminary estimates and the true values. Experiments on multiple datasets demonstrate that RDPI not only achieves state-of-the-art imputation accuracy but also significantly reduces sampling computational costs. The implementation code is available at https://github.com/liuzjin/RDPI.", "sections": [{"title": "Introduction", "content": "Spatiotemporal data encompass both temporal and spatial dimensions, primarily including traffic flow data, meteorological data, environmental monitoring data, etc. playing a crucial role in scientific research and social development. However, due to equipment aging, sensor malfunctions, or communication issues, spatiotemporal data often experiences partial or even complete data loss from sensors, which severely affects the reliability and effectiveness of subsequent analyses. Therefore, effectively imputing spatiotemporal data has become a crucial research issue.\nCurrently, methods for addressing spatiotemporal data imputation issues are mainly categorized into deterministic methods, probabilistic methods, and diffusion methods. Deterministic methods attempt to impute values using rules or deterministic algorithms, such as Recurrent Neural Networks (RNNs) (Cao et al. 2018; Yoon, Zame, and van der Schaar 2018; Che et al. 2018), Transformer-based Networks (Vaswani et al. 2017; Du, C\u00f4t\u00e9, and Liu 2023; Shan, Li, and Oliva 2023), and Graph Neural Networks (GNNs) (Cini, Marisca, and Alippi 2021; Marisca, Cini, and Alippi 2022) have been widely employed. These methods leverage the powerful representation learning capabilities of neural networks to learn complex spatiotemporal patterns from data and use learned models to predict missing data. However, deterministic methods often lack modeling of data uncertainty, which can pose challenges when dealing with complex spatiotemporal dependencies.\nProbabilistic methods use statistical models or machine learning techniques to model the probability distribution of data and perform imputation based on these distributions. In addition to Variational Autoencoders (VAE) (Kim et al. 2023; Mulyadi, Jun, and Suk 2021; Fortuin et al. 2020) and Generative Adversarial Networks (GAN) (Luo et al. 2018; Liu et al. 2019; Miao et al. 2021), other methods such as generative models and Bayesian networks are also utilized. These methods not only learned distributions from existing data, but also leveraged the latent structures and patterns within the data to handle data uncertainty. For example, VAE learns a latent-space representation of the data, effectively imputing missing data.\nTo better simulate the diffusion process of data over time and space and capture the dynamic changes and complex dependencies of spatiotemporal data, researchers have proposed methods based on diffusion models (Ho, Jain, and Abbeel 2020). The Conditional Sore-based Diffusion model for Imputation (CSDI) (Tashiro et al. 2021) is widely adopted as a typical approach. However, traditional CSDI models often only consider conditional factors during denoising network training, neglecting them in the forward and imputation process. This limitation may result in conditional models that fail to fully capture the complex dependencies in spatiotemporal traffic data, thereby affecting their performance in practical data imputation tasks.\nTo address the key challenges in spatiotemporal data imputation, we propose a novel two-stage refined diffusion probability imputation (RDPI) framework . In the initial stage, a preliminary estimate is generated using an initial network. In the refinement stage, a novel conditional diffusion model is proposed to optimize and adjust the results. By integrating conditional information throughout the diffusion process, the framework significantly improves the accuracy and effectiveness of imputing missing data.\nKey contributions of this paper include:\n\u2022 Introduction of the RDPI framework, which employs a two-stage imputation strategy to achieve higher-precision data completion.\n\u2022 Incorporated observed values into the forward process, deriving a conditional diffusion model better suited for spatiotemporal data imputation and utilizing residuals as the diffusion target to effectively enhance imputation efficiency.\n\u2022 Extensive empirical validation demonstrates that the proposed RDPI method excels in traffic data imputation tasks, achieving state-of-the-art performance.\nThese achievements provide new methodologies and theoretical support for addressing urban traffic data imputation challenges, potentially offering more reliable data foundations for urban traffic management and planning."}, {"title": "Related Work", "content": "Deterministic Methods Deterministic methods typically rely on rules or algorithms to impute missing data in spatiotemporal data. These methods learn complex spatiotemporal patterns from data and use the learned models to predict missing data. Despite their widespread application across various domains, deterministic methods often struggle to capture the complex dependencies and uncertainties present in real-world spatiotemporal data. Early research in spatiotemporal data imputation primarily relied on simple historical or global information, such as methods like KNN (Hastie et al. 2009; Beretta and Santaniello 2016) and Kriging (Zheng et al. 2023; Appleby, Liu, and Liu 2020; Wu et al. 2021). With technological advancements, autoregressive methods such as ARIMA (Box et al. 2015) and VAR (Zivot and Wang 2006) have been introduced, which consider time periodicity and trend information for imputation. However, as data volumes grow and uncertainties in spatiotemporal data increase, researchers have shifted towards methods that focus on extracting temporal and spatial dependencies from the data. For instance, GRAPE (You et al. 2020) treats missing and observed data as nodes in a bipartite graph, transforming the imputation problem into a node prediction task within the graph. GRIN (Cini, Marisca, and Alippi 2021) uses message passing to reconstruct missing data across different channels, while BRITS (Cao et al. 2018) employs bidirectional recurrent neural networks to directly learn missing data within dynamic systems.\nProbabilistic Methods Probabilistic methods utilize statistical models or machine learning techniques to model the probability distribution of spatiotemporal data and perform imputation based on these distributions. Typical probabilistic methods include VAE, GAN, and other generative models and Bayesian networks. These methods excel in handling data uncertainty by effectively imputing missing data through learning latent structures and patterns within the data. For example, GP-VAE (Fortuin et al. 2020) employs a Gaussian process model for performing variational inference, capturing correlations between features using Gaussian processes. MIWVAE (Mattei and Frellsen 2019), based on Importance Weighted Autoencoders, maximizes the strict lower bound of the log-likelihood of observed data, avoiding additional costs due to missing data. SSGAN (Miao et al. 2021) introduces a novel semi-supervised adversarial network that utilizes both time and temporal label information to enhance imputation effectiveness. GAIN (Yoon, Jordon, and Schaar 2018) uses a generative adversarial approach to generate missing data by providing additional information to the discriminator through hint vectors, ensuring that the generator learns features of the real data distribution. Finally, GRUI (Luo et al. 2018) proposes an RNN unit that considers time lag effects and utilizes adversarial models to impute incomplete original time-series data. STGNP (Hu et al. 2023) utilizes graph neural processes and proposes a graph aggregator that accurately estimates uncertainty while capturing spatiotemporal correlations.\nDiffusion Methods Currently, the most effective methods for addressing spatiotemporal imputation issues are typically based on diffusion models. CSDI (Tashiro et al. 2021) was the first time-series imputation method based on diffusion models, leveraging the Diffusion Probabilistic Models (DPMs) (Ho, Jain, and Abbeel 2020; Han, Zheng, and Zhou 2022; Li et al. 2024; Song, Meng, and Ermon 2021)to generate missing data conditioned on observed data, demonstrating the advantages of deep probabilistic generative models. SSSD (Alcaraz and Strodthoff 2022) integrates state-of-the-art state-space models as denoising modules and combines them with Diffwave to achieve data imputation. PriSTI (Liu et al. 2023) extends CSDI by designing a specialized noise prediction model that extracts conditional features from enhanced observed data and calculates spatiotemporal attention weights using extracted global context priors. MIDM (Wang et al. 2023) considers a Gaussian process as the latent distribution if the end point of the diffuser in time is a standard Gaussian of missing data and observed data are paradoxes, so that the difficult priors are meaningful and easier to handle. Researchers have proposed many data imputation methods based on diffusion models, among which Conditional diffusion Data Imputation model is widely adopted as a typical approach. However, these diffusion models often only consider conditional factors during training and denoising stages, neglecting them in the imputation process. This limitation may result in conditional models that fail to fully capture the complex dependencies in spatiotemporal traffic data, thereby affecting their performance in practical data imputation tasks."}, {"title": "Preliminaries", "content": "We formalize a spatiotemporal data set $x \\in \\mathbb{R}^{N\\times D}$ as sequential data consisting of N time series of length L. Any data in the dataset can be missing, meaning their values are unknown. We divide the dataset into two parts: observed data $x^o$ and missing data $x^m$. Specifically, both observed and missing data are obtained from a binary mask matrix"}, {"title": "Method", "content": "The pipeline of our proposed Refine Diffusion Probability Imputation (RDPI) framework is illustrated in Figure 1. RDPI consists of two stages: initialization and refinement. In the initialization stage, initial imputation results for missing data are generated using a deterministic imputation model. Subsequently, in the refinement stage, these initial imputations are further enhanced through a novel conditional diffusion probability model. The goal of this refinement is to minimize the disparity between the initially imputed data and the true data.\nCurrent imputation methods based on conditional diffusion models often focus solely on conditions during the reverse process, neglecting the inherent relationship between missing data and observed values in the forward process. Therefore, we rigorously defined the conditional probability distributions for the forward and backward processes, and derived a novel ELBO that incorporates the forward conditions. Based on these theoretical frameworks, we developed a novel conditional diffusion imputation model. Thus, in the following sections, we will introduce the corresponding reverse process, forward process, the ELBO as well as the final training and imputation algorithms."}, {"title": "Initial Stage", "content": "In the proposed framework, an initial model generates preliminary missing data, which is then refined by a diffusion model that establishes a connection between these preliminary values and the true values. Probabilistic sampling of the diffusion model helps mitigate overfitting issues of the initial model on the training data while reducing the pressure of relying solely on the diffusion model for generating missing data. In theory, the initial model can employ any imputation method, including the diffusion model itself. However, using diffusion models in both stages increases randomness and the computational cost of iterative sampling. Therefore, employing a deterministic approach in the initial stage is more practical. The combination of deterministic and diffusion models preserves the rapid generation capability of deterministic models and maintains the precise likelihood calculation ability of diffusion models. Joint training in both stages further enhances the effectiveness of deterministic imputation methods. Using the deterministic model obtained through two-stage training as the final imputation model, the diffusion model can be seen as a module that improves the deterministic imputation model. Thus, our conditional diffusion model can be integrated with any deterministic missing data imputation method to enhance its imputation results.\nIn our experiments, we employed the deterministic method $f_{\\theta}$ (utilizing GRIN (Cini, Marisca, and Alippi 2021) as described in the experiment) with an initial imputation value, and the noise-disturbed object of diffusion model $g_{\\theta}$ was the residual $z_m^0$ of the true missing data $x^m$ and initial imputation $f_{\\theta}(x^o)$. We utilized a two-stage training approach with joint training of the deterministic model and the diffusion model, where the training loss in the initial stage was:\n$\\mathcal{L}_{init} = ||f_{\\theta}(x) - x_m||_1$ (1)\nIt is worth noting that the initial model may not necessarily require pretraining, as gradients from joint training flow into $g_{\\theta}$ through $f_{\\theta}$ (Whang et al. 2022). However, for ensuring the stability of the diffusion model, we recommend pretraining. In contrast to reference (Whang et al. 2022), our loss function calculates $|| f(x) - x_m||_1$ rather than $||x - f_{\\theta}(x)||$. Experimental results show that using $|| f(x) - x_m||_1$ can increase the initial model's imputation loss if $f_{\\theta} (x)$ is negative, leading to instability in the diffusion target as the diffusion loss decreases. Maintaining $f_{\\theta}(x)$ positive ensures that the imputation performance of the initial model remains stable during further optimization."}, {"title": "Reverse Process", "content": "Following the steps of DDPM (Denoising Diffusion Probabilistic Models) (Ho, Jain, and Abbeel 2020), the conditional diffusion model (Han, Zheng, and Zhou 2022) is a latent variable model of the form $p(z_{1:T}|z_0) = \\int p(z_{T}|z_0)dz_{1:T}$, where $z_t$ is a latent variable with the same dimension as the residual $z_m$ conditioned on observed data $z_0^o$. The joint probability $p(z_T|z_0^o)$ is referred to as the reverse process, defined on a Markov chain:\n$p(z_{0:T}|z_0^o) := p(z_T|z_0) \\prod_{t=1}^T p(z_{t-1}|z_t, z_0^o)$ (2)\n$p(z_{t-1}|z_t, z_0^o) := \\mathcal{N}(z_{t-1}; \\mu_{\\theta}(z_t, z_0^o, t), \\Sigma_{\\theta}(z_t, z_0^o, t))$ (2)\nwhere $p(z_T|z_0)$ represents the endpoint of the conditional diffusion process, which is assumed to follow a standard"}, {"title": "Forward Process", "content": "Given a data point $z_m^0$ sampled from the true spatiotemporal data distribution $p(z_m^0)$, we define the forward process or diffusion process as a Markov chain with learned Gaussian transitions that starts from the sampled data distribution:\n$q(z_{1:T}|z_m^0, z_0^o) := \\prod_{t=1}^Tq(z_t|z_{t-1}, z_0^o)$ (3)\n$q(z_t|z_{t-1}, z_0^o) := \\mathcal{N}(z_t; \\sqrt{1 - \\beta_t}z_{t-1} + \\sqrt{1 - \\beta_t}z_0^o, \\beta_tI)$ (3)\nwhere $\\beta_t$ is variance from a given variance schedule $\\beta_1, ..., \\beta_T$.\nAt each transition step, a small amount of Gaussian noise is added to the data through Gaussian transitions. The step size is controlled by a variable schedule {$\\beta_t \\in (0,1)$}$_{t=1}^T$. Let $\\bar{\\alpha}_t := 1 - \\beta_t$ and $\\alpha_t = \\prod_{i=1}^t \\bar{\\alpha}_i$, we sample $z_t$ directly from $z_m^0$ with an arbitrary timestep t:\n$q(z_t|z_m^0, z_0^o) = \\mathcal{N}(z_t; \\sqrt{\\alpha_t}(z_m^0 + z_0^o), (1 - \\alpha_t)I)$ (4)\nBenefiting from the reparameterization trick, the diffusion process can be implemented as:\n$z_t = \\sqrt{\\alpha_t}z_m^0 + \\sqrt{\\alpha_t}z_0^o + \\sqrt{(1 - \\alpha_t)}\\epsilon_t$ (5)\nwhere $\\epsilon \\sim \\mathcal{N}(0, I)$ is random noise sampled from a standard Gaussian distribution.\n$\\mu_t (z_t, z_0^o, t) :=\\frac{\\sqrt{\\alpha_{t-1}}\\beta_t}{1 - \\alpha_t}\\sqrt{\\bar{\\alpha}_t}(1 - \\alpha_{t-1})z_0^o$+\n$\\frac{\\bar{\\alpha}_t(\\sqrt{\\alpha_{t-1}}\\beta_t - \\alpha_t(1 - \\alpha_{t-1}))}{\\sqrt{\\alpha_t}(1 - \\alpha_t)}z_m^0$+\n$\\frac{\\bar{\\alpha}_t(\\sqrt{1 - \\alpha_t})}{\\sqrt{\\alpha_t}(1 - \\alpha_t)}\\epsilon_t$ (9)\nWe need to learn a neural network to estimate the conditioned probability distributions in the reverse diffusion process $p(z_{t-1}|z_t, z_0^o) =  \\mathcal{N}(z_{t-1}; \\mu_{\\theta}(z_t, z_0^o, t), \\Sigma_{\\theta}(z_t, z_0^o, t))$. We would like to train $\\mu_{\\theta}$ to predict $\\epsilon$. We can reparameterize the Gaussian noise term instead to make it predict $\\epsilon_{\\theta}$ from the input $z_t$ at time step t:\n$\\mu_{\\theta}(z_t, z_0^o, t) = \\frac{\\sqrt{\\alpha_{t-1}}\\beta_t}{1 - \\alpha_t} \\sqrt{\\bar{\\alpha}_t}(1 - \\alpha_{t-1})z_0^o$+\n$\\frac{\\sqrt{\\alpha_t}(1 - \\alpha_t)}{\\sqrt{\\alpha_t}(1 - \\alpha_t)} \\epsilon_{\\theta}(z_t, z_0^o, z_0^o, t)$ (11)"}, {"title": "Training", "content": "With the light of reverse process and forward process, a novel ELBO of the conditional diffusion model can be derived as follows (See Appendix for details):\n$\\log p(z_m^0|z_0^o) \\nonumber$+\n$= \\mathbb{E}_{q(z_T|z_m^0,z_0^o)} [D_{KL}(q(z_T|z_m^0, z_0^o)||p(z_T|z_0^o)) \\nonumber$+\n$+ \\sum_{i=2}^T D_{KL}(q(z_{T-1}|z_T, z_m^0, z_0^o)||p(z_{T-1}|z_t, z_0^o)) \nonumber$+\n$+ \\log p(z_1|z_t, z_0^o)]$ (6)\nUsing Bayes' rule, we have:\n$q(z_{t-1}|z_t, z_m^0,z_0^o) = \\frac{q(z_t|z_{t-1}, z_m^0,z_0^o)q(z_{t-1}|z_m^0,z_0^o)}{q(z_t|z_m^0,z_0^o)}$\n$\u221d exp(-\\frac{1}{2}(\\frac{(\\sqrt{\\bar{\\alpha}_tz - \\alpha_tz_0^0) + (2\\sqrt{\\alpha_t}z_m^0 + 2\\sqrt{\\alpha_{t-1}}z_t + 2\\sqrt{\\alpha_{t-1}}z_0^0)z_{t-1}}{\\beta_t}) + C(z_0^o, z_m^0, z_t)))$ (7)\n(See Appendix for details) where $C(z_t, z_m^0, z_0^o)$ is some function not involving $z_{t-1}$ and details are omitted.\nFollowing the standard Gaussian density function, the mean and variance can be parameterized as follows (See Appendix for details):\n$\\frac{1}{\\sigma_{z_t}} = \\frac{\\beta_t}{\\bar{\\alpha}_t} \\frac{1}{\\sigma_{z_{t-1}}} + \\frac{\\bar{\\alpha}_t}{\\sigma_1}$ (8)\nThe loss term can be written (See Appendix for details):\n$L_t =  \\mathbb{E}_{z_t,\\epsilon}[\\frac{(1 - \\bar{\\alpha}_t)^2}{2(1 - \\alpha_t)}||\\frac{\\sqrt{\\bar{\\alpha}_t}(1 - \\alpha_{t-1})}{\\bar{\\alpha}_t}z_0^o + \\frac{\\bar{\\alpha}_t\\sqrt{\\alpha_t(1 - \\alpha_{t-1})}}{1 - \\alpha_t}z_m^0 + \\sqrt{(1 - \\alpha_t)}\\epsilon - \\epsilon_{\\theta} (z_t^o, z_0^o, z_m^0, t) || ]$ (12)\nThe Simplified training objective can be written:\n$\\mathcal{L}_{simple} = \\mathbb{E}_{z_m^0, \\epsilon}[|| \\epsilon_t - \\epsilon_{\\theta} (z_t^o, z_0^o, z_m^0, t) ||]$ (13)\nWe follow the convention to assume it will be close to zero by carefully diffusing the observed response variable $z_t^o$ towards a pre-assumed distribution $p(z_m^0|z_0^o)$.\nAt last, joint loss of RDPI becomes:\n$\\mathcal{L}_{joint} = \\mathcal{L}_{simple} + \\lambda \\mathcal{L}_{init}$ (14)\nwhere $\\lambda$ is the hyperparameter that trades off between the initial loss and diffusion loss. The complete training procedure has been displayed in Algorithm 1."}, {"title": "Imputation", "content": "After training denoising networks in each diffusion step, we can derive the likelihood of residual distributions according to the Eq (11). Missing data can be generated by sampling from standard Gaussian noise, as outlined in Algorithm 2. Similarly to DDPM, the significant computational demand of iterative sampling is a primary drawback of diffusion models. To expedite sampling, RDPI can employ any unconditional DDPM accelerated sampling scheme, such as DDIM (Song, Meng, and Ermon 2021). However, it's crucial to note that unlike the original DDIM, a version without randomness may compromise the quality of final data imputation. Therefore, RDPI values the specific sampling approach by retaining the accelerated scheme with intact random components.\nFollowing the steps of DDIM, we can get\n$P(z_{t-1}|z_t^m, z_0^o) = \\mathcal{N}(z_{t-1};$ $\\frac{\\alpha_{t-1}}{\\alpha_t}(\\sqrt{1-\\alpha_{t-1}}-\\frac{\\beta_t}{\\sqrt{1-\\alpha_t}})\n1-at-1-d2\\\n$\\epsilon_{\\theta}, \\frac{\\beta_t}{\\alpha_t})$\n(See Appendix for details). The pipeline of the accerlerate imputation method has been displayed in Algorithm 3."}, {"title": "Denoising Model", "content": "In this section, we introduce the proposed denoising model. The specific process is illustrated in Figure 2. Our denoising model consists of four components: embedding module, temporal self-attention module, graph neural network module and spatial self-attention module.\nTo keep the native information in the raw data, we first concatenates observed and missing data, then utilize a 1D convolutional layer to obtain a feature embedding $E_f \\in \\mathbb{R}^{T\\times N\\times d}$.\n$E_f = CNN(z_0^o||z_m^0)$ (16)\nIn spatiotemporal sequences, data from the same node exhibit temporal heterogeneity over time, whereas data from different nodes at the same time point show spatial heterogeneity across space. Therefore, it is essential to encode the time information and spatial features into the data. The embedding module is responsible for converting time information, spatial information, and diffusion step information into trainable embedding tables. Traffic flow data exhibit distinct periodic patterns, with weekdays and weekends showing temporal heterogeneity within the same week, and significant variations occurring throughout the day. Thus, we encode the time window into a learnable embedding table $E_{tem} \\in \\mathbb{R}^{N_w\\times d}$, where $N_w$ and d is the lenth of time window and the dimension of the feature embedding. Furthermore, we designed a spatial embedding $E_{spa} \\in \\mathbb{R}^{N_s\\times d}$ to simultaneously capture temporal and spatial heterogeneity, where $N_s$ and d denote the number of nodes and the dimension of the feature embedding, respectively. The final representation of the spatiotemporal embedding $Z^{emb}$ is obtained by summing up all the embeddings:\n$Z^{emb} = E_f + E_{tem} + E_{tem} + E_{spa}$ (17)\nSince the attention mechanism performs exceptionally well in modeling sequential data, we use self-attention to model the spatial and temporal dependencies of $Z_t^m$, which can be formulated as follows:\n$Attn(Z^{emb}) = softmax(\\frac{Q K^T}{\\sqrt{d}})$ (18)\n$Q=W_QZ^{emb}, K=W_K.Z^{emb}, V = W_V. Z^{emb}$ (18)\nwhere $W_Q$, $W_K$ and $W_V \\in \\mathbb{R}^{d\\times d}$ are learnable projection parameters. To capture the relationships between nodes in spatiotemporal data, we employ graph neural networks to further model spatial dependencies which can be formalized as follows:\n$Z^{emb} = GNN(Z^{emb}, A)$ (19)"}, {"title": "Experiments", "content": "In this section, we first introduce the experimental dataset, followed by the comparison methods, evaluation metrics, and experimental settings. We evaluated the proposed RDPI framework through extensive spatiotemporal imputation experiments, aiming to answer the following questions:\nRQ 1: Can RDPI demonstrate stronger imputation capabilities compared to state-of-the-art methods?\nRQ 2: How does the imputation performance of RDPI vary under different missing-rate conditions?\nRQ 3: Does the imputation of missing data benefit from conditional observational data and the initial imputation model?\nRQ 4: Does RDPI exhibit good imputation performance even when the entire node is missing?\nRQ 5: Does hyperparameter tuning significantly impact on the model's performance?"}, {"title": "Datasets and Experimental Settings", "content": "Datasets. We conducted comparative experiments on four real datasets: PEMS-BAY (Li et al. 2017), METR-LA (Li et al. 2017), AQI (Yi et al. 2016)and AQI36 (Yi et al. 2016). PEMS-BAY contains traffic speed data from 207 nodes on California highways over four months. METR-LA collected traffic speed data from 325 nodes on Los Angeles highways over six months. Both PEMS-BAY and METR-LA are sampled every 5 minutes. AQI contains hourly sampled air quality data from 437 observation nodes in 43 Chinese cities over twelve months. AQI36 contains 36 observation nodes from AQI in Beijing over twelve months. \nSettings. For fairness, as done in previous work (Cini, Marisca, and Alippi 2021; Liu et al. 2023), we split PEMS-BAY and METR-LA into training, validation, and test sets in a ratio of 70%/10%/20%. For AQI and AQI36, the 3th, 6th, 9th and 12th months are used as test sets and the other months as the training sets. \nTraining Strategies. In air quality datasets, missing data and their corresponding true data are already annotated in the dataset. As in previous works (Cini, Marisca, and Alippi 2021; Liu et al. 2023), we employed two different training scenarios: (1) In-sample, where model was trained according to the specified missing positions in the dataset; (2) Out-of-sample, where the training set combined the missing data specified with the observed data, and new missing positions were randomly generated. In traffic datasets, missing data is sparse and lacks ground truth values. Therefore, in addition to the actual observed data, we employed a method to manually design target data imputation using random masking. Similar to prior studies, the imputation targets were divided into two scenarios: (1) Point missing, where 25% of observed data was randomly masked; (2) Block missing, where initially 5% of observed data was masked, and then for each node, data within 1 to 4 hours was randomly masked at a probability of 0.15%."}, {"title": "Baseline", "content": "In our study, we conducted a comparative analysis of various baseline methods, include: (1) MEAN, imputation using the node-level average; (2) KNN, imputation by averaging values of k = 10 highest weight neighboring nodes; (3) MICE (White, Royston, and Wood 2011), with 100 maximum iterations and 10 nearest features limitation; (4) VAR, a vector autoregressive single-step predictor; (5) GRIN (Cini, Marisca, and Alippi 2021), a bidirectional GRU based framework with graph neural network for multivariate time series; (6) BRITS (Cao et al. 2018) using recurrent dynamics to impute the missing data; (7) rGAIN, GAIN with a bidirectional recurrent encoder and decoder; (8) GP-VAE (Fortuin et al. 2020), a deep probabilistic model by combining VAE and Gaussian process; (9) CSDI (Tashiro et al. 2021), a probability imputation method based on conditional diffusion models; (10) MIDM (Wang et al. 2023), a diffusion based model with noise sampling and denoising mechanism for multivariate time series imputation."}, {"title": "Metrics", "content": "We apply three evaluation metrics to measure the performance of spatiotemporal imputation: Mean Absolute Error (MAE), Mean Squared Error (MSE), and Mean relative error (MRE), which are defined as:\n$MAE(X, \\hat{X}) = mean(sum(|\\hat{X} - X|))$ (20)\n$MSE(X, \\hat{X}) = mean(sum((\\hat{X} - X)^2))$ (20)\n$MRE(X, \\hat{X}) = mean(sum(|\\frac{\\hat{X} - X}{\\hat{X}}|))$ (20)"}, {"title": "Results (RQ1)", "content": "We first evaluate the spatiotemporal imputation performance of RDPI compared with other baselines. \nWe contend that the MRE may not fully capture the true performance of the model. This discrepancy arises because the diffusion model smooths the prediction residuals through probabilistic sampling, thereby reducing the magnitude of residuals between the true values and the initial imputed values. Since MSE evaluates the overall discrepancy between predicted and true values, this smoothing effect significantly improves it. In contrast, MRE focuses on relative error and may not adequately reflect the improvements resulting from this smoothing. This improvement is primarily due to our proposed conditional diffusion model, which leverages an initial model to reduce the computational cost of the denoising model. The initial model provides a quick rough imputation, whereas the conditional diffusion model efficiently estimates the residual distribution between the rough imputation and the true data. This approach demonstrates that the RDPI framework effectively smooths the residuals between the initial imputation and the true data, thereby reducing extreme predictions and leading to more accurate and reliable results."}, {"title": "Sensitivity Analysis (RQ2)", "content": "For spatiotemporal data, the sparsity of the data significantly affects the performance of models. We use MATR-LA to evaluate the robustness of the model under different data missing rates. The evaluation methods include BRITS, GRIN, and CSDI. We test the imputation performance of different models with data missing rates ranging from 10% to 90% for both patterns. For block-missing patterns, we assess the imputation performance of different models with varying missing rates over time intervals ranging [12, 48]; for point-missing patterns, we drop the observed data according to the missing rate.\nAs illustrated in the figure, the results generated by diffusion models (RDPI and CSDI) significantly outperform those of deterministic models (BRITS and GRIN). Notably, RDPI consistently exhibits the best imputation performance even as the missing rate increases. This can be attributed to RDPI make use of observed data as a condition during the denoising and diffusion processes, which effectively captures the dependencies between observed and missing data."}, {"title": "Abaliation Study (RQ3)", "content": "We design ablation experiments on datasets METR-LA and AQI36 to verify the effectiveness of the conditional diffusion model", "variants": "n\u2022 w/o cond-forw: the observational condition was not used in forward process.\n\u2022 w/o residual: the same denoising network was used for the true missing data.\n\u2022 w/o joint: the initial model was frozen", "pre-train": "the initial model was not pretrained.\n\u2022 predicting $\\epsilon_{\\theta"}, "the training strategy is predicting $\\epsilon_{\\theta}$ rather than $\\epsilon_{\\theta}$.\n\u2022 -f_\\theta(x): the loss function calculates $||\\hat{X} - f_\\theta(x)||$. The diffusion target is $\\hat{X} - f_\\theta(x)$.\nThe result of two datasets are shown in Table 5. As shown in the table, we have the following observations: (1) From the results of w/o cond-forw, it can be observed that the imputation performance is the worst when the process is not conditioned on the observed values. This phenomenon can be explained by the fact that the lack of observed values as conditions means that the model cannot utilize the relationship between observed and missing data, thus failing to effectively capture the internal dependencies in the data."]}