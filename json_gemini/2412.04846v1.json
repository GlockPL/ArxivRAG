{"title": "eXpath: Explaining Knowledge Graph Link Prediction with Ontological Closed Path Rules", "authors": ["Ye Sun", "Lei Shi", "Yongxin Tong"], "abstract": "Link prediction (LP) is crucial for Knowledge Graphs (KG) completion but commonly suffers from interpretability issues. While several methods have been proposed to explain embedding-based LP models, they are generally limited to local explanations on KG and are deficient in providing human interpretable semantics. Based on real-world observations of the characteristics of KGs from multiple domains, we propose to explain LP models in KG with path-based explanations. An integrated framework, namely eXpath, is introduced which incorporates the concept of relation path with ontological closed path rules to enhance both the efficiency and effectiveness of LP interpretation. Notably, the eXpath explanations can be fused with other single-link explanation approaches to achieve a better overall solution. Extensive experiments across benchmark datasets and LP models demonstrate that introducing eXpath can boost the quality of resulting explanations by about 20% on two key metrics and reduce the required explanation time by 61.4%, in comparison to the best existing method. Case studies further highlight eXpath's ability to provide more semantically meaningful explanations through path-based evidence.", "sections": [{"title": "1 INTRODUCTION", "content": "Knowledge graphs (KGs) [1, 5, 17] commonly suffer from incompleteness, such that link prediction (LP) becomes a crucial task for KG completion, aiming to predict potential missing relationships between entities within a KG. In the deep learning era, advanced KG embedding models (KGE) such as ComplEx [28], TransE [29], and ConvE [9] have been applied to perform the LP task successfully. Yet, due to the inherent black-box nature of deep learning, how to interpret these LP models remains a daunting issue for KG applications. For example, in financial KGs used to make high-stake decisions such as fraud or credit card risk detection, interpretability is required not only for customer engagement purpose [21], but also by the latest law enforcement [8].\nVarious methods have been developed to interpret the behaviour of LP models, e.g., to explain graph neural network (GNN) based predictive tasks [6, 30, 34], embedding-based models [3, 32], and providing subgraph-based explanations [31, 33, 36]. On KG, the recently proposed adversarial attack methods [3, 24, 27] become a major class of approaches for explaining LP results. The adversarial method captures a minimal modification to KG as an optimal explanation if only a maximal negative impact is detected on the target prediction. In particular, Kelpie [27] introduces entity mimic and post-training techniques to quantify the model's sensitivity to link removal and addition. Despite the success of LP explanation models on KG, they have key limitations in at least two aspects. First, in most methods, only local explanations related to the head or tail entity of the predicted link are considered without exploring the full KG. Second, the explanations generally focus on maximizing computation-level explainability, e.g., the perturbation to predictive power when adding/removing the potential explanation link. They mostly lack semantic-level explainability, which is extremely important for human understanding.\nIn this work, we are motivated by several observations during the real-world deployment of LP models on KG. For instance, in a material knowledge graph of Fig. 1, to explain the fact of a material synthesized within a particular solvent (dotted red link), classical methods only excerpt single-hop links representing certain properties of the material (thickened blue links). In reality, the material expert favours path-based explanations such as the blue paths on the middle/bottom of Fig. 1. The middle path indicates material/solvent sharing the same sub-structure, while the bottom path indicates two materials reported by the same paper/team that potentially share the same solvent environment. These path-based explanations represent fundamental semantics, such as causal relationships with the predicted link. Building on these observations, we propose a path-based explanation framework, namely eXpath, to address the interpretability problem of LP models on KG. Our method not only suggests minimal KG modifications similar to adversarial attack explanations but also highlights semantically meaningful link paths supporting each modification.\nNote that the idea of path-based explanation has also been studied in the recent work of Power-Link [6] and PaGE-Link [34]. However, these works focus on explaining GNN-based embedding models and extracting all the potential KG paths up to thousands for a single explanation. In comparison, we consider the explanation of factorization-based embedding models, a mainstream method for KGs. The follow-up adversarial explanation evaluates only a few KG modifications at a time, and it is computationally costly to select the best set of paths from thousands of candidates. Moreover, another pragmatic challenge lies in the evaluation of individual path explanation. While the adversarial method works well in quantifying the effectiveness of a single-link explanation, adding/deleting an entire path can bring more significant change to the KG, hard to evaluate by the same method. The contribution of this work is to address the above challenges as summarized below:\n\u2022 Based on the attributed characteristics of KG, we introduce the concept of relation path, which aggregates individual paths by their relation types. The explanation analysis then works on the level of relation paths, greatly reducing the computational cost while augmenting the semantics of explanations;\n\u2022 On the evaluation of path-based explanations, we propose to borrow ontology theory, particularly the closed path rule and property transition rule, which not only reassures the path-based semantics but also guarantees high-occurrence explanations within the whole KG dataset;\n\u2022 Through extensive experiments across multiple KG datasets and embedding models, we demonstrate the effectiveness of our method, which significantly outperforms existing LP explanation models. Case study also reveals the consistency of path-based explanations with ground-truth semantics."}, {"title": "2 RELATED WORK", "content": "2.1 The Explanation of Knowledge Graph Link\nPrediction (KGLP)\nExplainability in Knowledge Graph Link Prediction (KGLP) is a critical area of research due to the increasing complexity of models used in link prediction tasks. General-purpose explainability techniques are widely used to understand the input features most responsible for a prediction. LIME [25] creates local, interpretable models by perturbing input features and fitting regression models, while SHAP [15] assigns feature importance scores using Shapley values from game theory. ANCHOR [26] identifies consistent feature sets that ensure reliable predictions across samples. These frameworks have been widely adopted in various domains, including adaptations for graph-based tasks, although their application in link prediction for knowledge graphs remains limited.\nGNN-based LP explanation primarily focuses on interpreting the internal workings of graph neural networks for link prediction. Techniques like GNNExplainer [30] and PGExplainer [16] identify influential subgraphs through mutual information, providing insights into node and graph-level predictions, although they are not directly applicable to link prediction tasks. Other methods, such as SubgraphX [31] and GStarX [33], use game theory values to select subgraphs relevant to link prediction. At the same time, PaGE-Link [34] argues that paths are more interpretable than subgraphs and extends the explanation task to the link prediction problem on heterogeneous graphs. Additionally, Power-Link [6], a path-based KGLP explainer, leverages a graph-powering technique for more memory-efficient and parallelizable explanations. However, GNN-based explainability techniques are limited to GNN-based LP models and do not extend to embedding-based approaches.\n2.2 Adversarial Attacks on KGE\nAdversarial attacks on KGE models have gained attention for assessing and improving their robustness. These attacks focus primarily on providing local, instance-level explanations. The goal is to introduce minimal modifications to a knowledge graph that maximizes the negative impact on the prediction. Methods are typically categorized as white-box or black-box approaches.\nWhite-box methods propose algorithms that approximate the impact of graph modifications on specific predictions and identify crucial changes. Criage [24] applies first-order Taylor approximations for estimating the impact of removing facts on prediction scores. Data Poisoning [3, 32] manipulates embeddings by perturbing entity vectors to degrade the model's scoring function, highlighting pivotal facts during training. ExamplE [13] introduce Example heuristics, which generate disconnected triplets as influential examples in latent space. KE-X [36] leverages information entropy to quantify the importance of explanation candidates and explains KGE-based models by extracting valuable subgraphs through a modified message-passing mechanism. While these white-box methods offer valuable insights, they often require full access to model parameters, making them impractical for real-world applications.\nRecent research has also focused on black-box adversarial attacks, which do not require knowledge of the underlying model architecture. KGEAttack [2] uses rule learning and abductive reasoning to identify critical triples influencing predictions, offering a model-agnostic alternative to white-box methods. While this study is closely related to ours, it employs simpler rules and does not consider the support provided by multiple or long rules for the facts. Kelpie [27] explains KGE-based predictions by identifying influential training facts, utilizing mimic and post-training techniques to sense the underlying embedding mechanism without relying on model structure. However, these methods are limited to fact-based explanations that focus only on local connections to the head or tail entity without capturing the multi-relational context needed for full interpretability.\n2.3 Ontological Rules for Knowledge Graph\nOntological rules for knowledge graphs have been a prominent area of research, as they provide symbolic and interpretable reasoning over knowledge graph data. AMIE [10, 11] and AnyBURL [19, 20]"}, {"title": "3 BACKGROUND AND PROBLEM DEFINITION", "content": "3.1 KGLP Explanation\nKnowledge Graphs (KGs), denoted as KG = (E, R, G), are structured representations of real-world facts, where entities from & are connected by directed edges in G, each representing semantic relations from R. These edges G \u2286 & \u00d7 R \u00d7 &, represent facts of the form f = (h, r, t), where h is the head entity, r is the relation, and t is the tail entity. Link Prediction (LP) aims to predict missing relations between entities in a KG. The standard approach to LP is embedding-based, where entities and relations are embedded into continuous vector spaces, and a scoring function, fr(h, t), is used to measure the plausibility of a fact. Evaluation of LP models is typically performed using metrics such as mean reciprocal rank (MRR), which measures how well the model ranks the correct entities when predicting missing heads or tails in the test set Gtest.\n$MRR = \\frac{1}{2 |G_{test}|} \\Sigma_{f \\in G_{test}} (\\frac{1}{rkt(f)} + \\frac{1}{rkh(f)})$ (1)\nwhere $rkt(f)$ represents the rank of the target candidate t in the query (h, r, ?), and $rkh(f)$ the rank of the target candidate h in the query (?, r, t).\nWhile embedding-based LP provides accurate predictions, understanding the reasoning behind these predictions is essential for model transparency and trust. To address this, explanation methods for embedding-based LP focus on providing instance-level insights into predictions, revealing underlying features like proximity, shared neighbors, or similar latent factors. However, directly perturbing the model's architecture or embeddings is challenging. As a result, explanation methods often rely on adversarial perturbations within the training data, such as modifications to the neighborhood of the target triple, to assess the robustness of KGE models.\n3.2 Adversarial Attack Problem\nAdversarial attacks in the context of KGLP explanations are designed to assess a model's vulnerability to small changes and evaluate the stability of LP models by intentionally degrading their performance through targeted perturbations in the training data. These attacks provide instance-level adversarial modifications as explanations. Given a prediction (h, r, t), an explanation is defined as the smallest set of training facts that enabled the model to predict either the tail t in (h, r, ?) or the head h in (?, r, t). For example, to explain why the top-ranked tail for (Barack_Obama, nationality, ?) is 'USA', we identify the smallest set of facts whose removal from the training set Gtrain would cause the model to change its prediction for (h, r, ?) from 'USA' to any entity e \u2260 t, and for (?, r, t) from h to any entity e' \u2260 h. These facts involve the head and tail entities, as they are crucial to the prediction.\nWe evaluate the impact of the adversarial attack by comparing standard metrics, such as MRR, before and after the attack. Specifically, we train the model on the original training set and select a small subset of the test set T C Ge as target triples for which the model achieves good predictive performance. After removing the attack set from the training set, we retrain the model and measure the degradation in performance on the target set.\nSince we focus on small perturbations, the attack is restricted to deleting a small set of triples. To make this process computationally feasible, we adopt a batch mode where the deletion of one target triple may affect others. If the target sets are small and the predicates contain disjoint entities, dependencies between triples are rare and can typically be neglected. The explanatory capability of the attack is measured by the degradation in MRR, defined as:\n$\\delta MRR(T) = 1 - \\frac{MRR_{new}(T)}{MRR_{original}(T)}$ (2)\n3.3 Path-Based Exaplanation\nWhile adversarial attacks focus on identifying critical facts for each prediction, they often lack a clear rationale for why specific facts are considered critical. We observe that certain knowledge graphs, as shown in Fig. 1, exhibit semantically meaningful paths that can enhance the interpretability of predictions.\nIn this work, we tackle the adversarial attack problem with path-based explanations. Given a prediction (h, r, t), an explanation consists of the smallest set of training facts that support the prediction, as well as the rationale for each fact's inclusion in the explanation, specifically that one or more critical paths support it.\nA critical path is represented as a relation path from the head to the tail entity: (h, r1, A1) ^ (A1, r2, A2) \u06f8\u06f0\u06f0\u06f0\u06f8 (An\u22121, rn, t), where h and t represent the head and tail entities, ri denotes relations, and Ai represents placeholder of any intermediate entity. This sequence of triples forms a path from the head to the tail entities. Each critical path corresponds to a high-confidence Closed Path (CP) rule, which describes the relationship between entities X and Y via alternative paths and consists of one or more relations without considering intermediate entities."}, {"title": "4 EXPATH METHOD", "content": "The expath method is designed to explain any given prediction <h, r, t) by identifying a small yet effective set of triples whose removal significantly impacts the model's predicted ranking of h and t. Additionally, eXpath provides the rationale for its explanations by presenting the critical paths associated with each selected fact.\nThe eXpath method follows a three-stage pipeline: path aggregation, path-based rule mining, and critical fact selection. In the path aggregation stage (Figure 2(a)), we use breadth-first search (BFS) on the training facts (Gtrain) to identify paths from h to t, limiting the maximum path length to 3 to ensure interpretability. These paths are then compressed into relation paths (Pr) by removing intermediate entities, reducing the candidate paths while preserving essential semantic structure. In the path-based rule mining stage (Figure 2(b)), we prune the candidate relation paths to retain only the highly relevant ones (Prelevant) using a local optimization technique based on head and tail relevance. These relevant paths form the body of candidate closed path (CP) rules, evaluated with a matrix-based approach to compute their confidence. Simultaneously, we construct Property Transition (PT) rules from the facts linked to the head and tail entities in Fh and Ftrain, retaining high-confidence CP and PT rules for fact selection. Finally, in the critical fact selection stage (Figure 2(a)), we score the candidate facts based on the number and confidence of rules they belong to, selecting the highest-scoring facts to form the final explanation.\nNotably, while our method efficiently extracts path-based explanations, experiments (Section 5) show that not all KGLP explanations require path-based semantics. In sparser KGs, simple one-hop links can score higher in evaluations. To leverage both approaches, we propose a fusion model that combines eXpath's explanations with those from non-path methods (e.g., Kelpie). By evaluating explanations from both methods, the highest-scoring ones are selected as the final explanation. This fusion model highlights the complementary strengths of different explanation types and demonstrates its potential as a superior overall solution.\n4.1 Relation Path and Ontological Rules\nWhen providing path-based explanations for a prediction f =\n<h, r, t), the number of simple paths from h to t grows exponentially with the path length, making even 3-hop paths computationally prohibitive. To mitigate this issue, we focus not on the specific entities traversed by a path but rather on the sequence of relations along the path. This abstraction, referred to as a \"relation path,\" drastically reduces the number of candidate paths while preserving their semantic meaning. This concept is inspired by using closed path rules (CP) in ontological rule learning. By aggregating multiple simple paths into relation paths, we significantly reduce path count while retaining the interpretability crucial for explanations.\nFigure 3 illustrates examples of CP and PT rules, which are inspired by the definitions of binary and unary rules with an atom ending in a constant in ontological rule mining. While PT rules can be generalized into CP rules by adding relationships between constants and replacing constants with variables, they remain essential for scenarios where two constant entities are strongly correlated (e.g., male and female) but cannot be described by simple paths. These interpretable rules offer insight into link predictions, providing a solid foundation for generating explanations. Formally, we distinguish between two types of rules:\nCP: $r(A_{0}, A_{n}) \\leftarrow \\bigwedge_{i=1}^{n} r_{i}(A_{i-1}, A_{i})$ (3)\nPT : $r(X, c) \\leftarrow r_{0}(X, c') \\text{ or } r(c, Y) \\leftarrow r_{0}(c', Y)$\nwhere r and ri denote relations (binary predicates), A0, Ai, An, X, Y are variables, and c, c' are constants (entities). We use $ to denote a rule, where the atoms on the left (h) form the head of the rule (head($)), and the atoms on the right (r) form the body of the rule (body($)). To simplify the notation, in the following part, we use r \u2190 r1, 2, ..., rn to symbolize CP rules, and relations can be reversed to capture inverse semantics (noted with a single quote, r'). For example, the relation hypernym (X, Y) can also be expressed as hypernym' (\u03a5, \u03a7).\nCP rules are termed \"closed paths\" because the sequence of relations in the rule body forms a path that directly connects the subject and object arguments of the head relation. This characteristic establishes a strong connection between CP rules and relation paths. Both concepts focus on capturing the structured relationships between entities in a knowledge graph, and their forms are inherently aligned. This alignment allows relation paths to serve as direct candidates for CP rule bodies. In fact, every CP rule can be viewed as a formalized and generalized representation of a relation path, enriched with additional confidence and support. Moreover, the structured nature of CP rules makes them well-suited for explaining embedding-based predictions, as they encapsulate the critical relational patterns that underpin the model's reasoning.\nTo assess the quality of rules, we recall measures used in some major approaches to rule learning [7, 10]. Let & be a CP rule of the form $. A pair of entities r (e, e') satisfies the head of & and there exist entities e\u2081,..., en-1 in the KG such that (e, r\u2081, e\u2081), ..., (en-1, rn, e') are facts in the KG, so the body of R are satisfied. Then, the support degree (supp), standard confidence (SC), and head coverage (HC) of & are defined as:\n$supp(\\phi) = \\#\\{(e, e') : body(\\phi)(e, e') \\wedge r(e, e') \\}$ (4)\n$SC(\\phi) = \\frac{supp(\\phi)}{\\#\\{(e, e') : body(\\phi)(e, e')\\}}, HC(r) = \\frac{supp(\\phi)}{\\#\\{(e, e') : r(e, e')\\}}$\n4.2 Path-based Rule Mining\nA critical step for generating path-based explanations is constructing a rule set I, which includes both closed path (CP) and Property Transition (PT) rules, as defined in Section 4.1. We do not mine all possible rules across the entire knowledge graph (KG) but instead focus on extracting relevant rules for each prediction from a localized graph relevant to the specific prediction f = (h, r, t). PT rules relevant to a given prediction arise from other facts related to h and t (f' \u2208 Ftrain Ftrain). These rules are constructed by replacing common entities in f and f' with variables, which serve as the rule head and body, respectively. For example, for f = <Porco_Rosso, language, Japanese) and f' = (Porco_Rosso, genre, Anime), the corresponding PT rule is: (X, language, Japanese) \u2190 (X, genre, Anime). This rule, similar to the \"sufficient scenario\" proposed by Kelpie [27], captures whether different entities in the same context satisfy the same prediction.\nCalculating metrics for PT rules is relatively straightforward. Based on Equation 4, we simply count the number of facts in Gtrain that satisfy (X, language, Japanese) and (X, genre, Anime) as the head and body counts, respectively. The number of facts satisfying both conditions serves as the support count. Finally, we set a threshold: only rules for which SC($) > minSC and HC($) > minHC are selected to form the PT rule set OPT.\nCP rules relevant to a prediction, on the other hand, arise from relation paths (Pr) connecting h and t. CP rule mining is more complex than PT rule mining due to the potentially large number of CP rules for a single prediction and the computational expense of evaluating CP rules across the entire knowledge graph. As detailed in Algorithm 1, we first filter P, using local optimization, ensuring that only relation paths relevant to the prediction Prelevant are considered for evaluation.\nDuring the pruning process, each relation path is assigned a head relevance score and a tail relevance score, which reflect its importance to the prediction. Relation paths with positive head and tail relevance (Relh > 0 and Relt > 0) scores are considered relevant to the prediction and retained as candidate rule bodies (prelevant) for further evaluation. This filtering approach assumes that a relation path can only serve as a valid rule body if both its head and tail relations are critical to the prediction.\nTo compute relevance scores, eXpath adopts an efficient local optimization approach inspired by the Kelpie mimic strategy [27]. Mimic entities for the head and tail, denoted as h' and t' (see Fig. 2(b)), are created. These mimic entities retain the same connections as the original head or tail entities, except that all facts associated with the evaluated relation are removed. The embeddings of the mimic entities, along with those of the original head and tail entities, are then independently trained using their directly connected facts.\nThree predictive scores are computed: fr(h, t), fr(h', t), and fr(h, t'), where fr (h, t) represents the model's scoring function for the triple (h, r, t). The relevance of a relation is defined as the reduction in the predictive score after removing all facts associated with a specific relation:\n$Rel_{h} = 1 - \\frac{f_{r}(h', t)}{f_{r}(h, t)}, Rel_{t} = 1 - \\frac{f_{r}(h, t')}{f_{r}(h, t)}$ (5)\nHere, Rely and Relt quantify the importance of relations connected to the head and tail entities. Relative changes in scores are used instead of rank reductions, as scores provide a more robust metric. Rank reductions can be unreliable, especially in local optimization scenarios where mimic entities may overfit, resulting in consistent ranks of 1. This relevance score effectively captures the impact of facts on the prediction by simulating the model's underlying embedding mechanisms.\nFinally, eXpath constructs a CP rule set OCP for each prediction based on the relevant relation paths Prelevant to select high-quality rules that have strong support and confidence. Confidence is computed as $conf(\\phi) = SC(\\phi) \\frac{supp(\\phi)}{supp(\\phi)+minSupp}$ which prevents the overestimation of rules with insufficient support (e.g., supp < 10), inadequate for generalizing into a rule. High-confidence CP and PT rules (PCP and OPT) are retained for fact selection. Strong support and confidence ensure that the selected rules are robust for causal reasoning, enabling eXpath to generate accurate and interpretable path-based explanations.\nHowever, efficiently computing metrics for CP rules presents a significant challenge. To address this, we adopt the matrix-based approach from RLvLR [23]. The method verifies the satisfiability of the body atoms in candidate rules to compute the metrics for CP rules. Given a KG represented as a set of S matrices, where each nxn binary matrix S(rk) corresponds to a relation rk, the adjacency matrix S(rk) has an entry of 1 if the fact (ei, rk, ej) exists in the KG, and 0 otherwise.\nThe product of adjacency matrices is closely related to closed path rules. For instance, consider the rule : r\u2190 r1, r2. A fact rt (e, e') is inferred by f if there exists an entity e\" such that r\u2081 (e, e\") and r2(e\", e') hold. The product S(r1) S(r2) produces the adjacency matrix for the set of inferred facts. The binary transformation S(r1, r2) = binary(S(r1)\u00b7S(r2)) is then used to generalize this computation. The metrics for this CP rule are calculated as:\n$supp(\\phi) = sum(S(r_{1}, r_{2}) & S(r))$\n$SC(\\phi) = \\frac{supp(\\phi)}{sum(S(r_{1}, r_{2}))}, HC(\\phi) = \\frac{supp(\\phi)}{sum(S(r))}$ (6)\nwhere sum aggregates all matrix entries, and & represents the element-wise logical AND operation. While this example involves rules with its body of length 2, the method extends straightforwardly to any length. This matrix-based approach offers a scalable solution for efficiently computing rule metrics in large knowledge graphs.\n4.3 Critical Fact Selection\nThis section details the method for selecting an optimal set of facts to explain a given prediction triple (h, r, t), leveraging the rules extracted in the previous step. The core idea is to identify the most critical fact or a combination of facts within the paths connecting the head and tail entities. Each fact is evaluated based on its contribution to the prediction, and those with higher scores are considered more pivotal. The final explanation set is constructed by selecting the highest-scoring facts.\nSeveral key factors are taken into account to determine the significance of a fact: (1) Facts that satisfy a larger number of rules are given higher priority, as this indicates their broader relevance within the prediction. (2) Rules with higher confidence are weighted more heavily, reflecting their more robust causal support. (3) The position of a fact within a rule (e.g., whether it connects to the head or tail entity) is adjusted based on the relation relevance scores determined earlier.\nConsidering all these factors, the scoring system provides a robust metric for evaluating each fact's importance. To model the contribution of a fact that satisfies multiple rules, we adopt a confidence degree (CD) aggregation approach inspired by rule-based link prediction methods [22]. The CD of a fact f is calculated using the confidence values of all the rules that infer f in a Noisy-OR manner. For explanation tasks, which reverse the link prediction perspective, we define the CD of f as follows:\n$CD(f) = 1 - \\prod_{\\phi \\in \\Phi(f)} (1 - conf(\\phi) \\cdot w(f, \\phi))$ (7)\nwhere \u03a6(f) is the set of rules inferred from the prediction, conf ($) is the confidence of rule 6, and w(f, f) represents the importance of fact f within rule 4. This importance score, ranging from 0 to 1, reflects the proportion of f 's appearances in the rule and its relative importance based on the relevance of the rule's head and tail relations. The importance score w(f, f) is calculated as:\n$r_{h}(\\phi) = \\frac{Rel_{h}(\\phi)}{Rel_{h}(\\phi) + Rel_{t}(\\phi)}$ (8)\n$w(f, \\phi) = r_{h}(\\phi) \\cdot p_{h}(f, \\phi) + (1 - r_{h}(\\phi)) \\cdot p_{t}(f, \\phi)$\nwhere Relh ($) and Relt ($) are the relevance scores of the rule's head and tail relations, respectively. The term ph (f, 4) represents the proportion of f 's appearances in the head of all paths related to rule . This formulation ensures that facts appearing more prominently in rules are scored higher. In PT rules, the importance score for a fact w(f, $) is simplified to 1, as the rule corresponds to a unique fact for a given prediction.\nWe rank all candidate facts by their CD scores and select the top-ranked facts to form the explanation. This approach ensures that the selected facts are those most strongly supported by high-quality, relevant rules, providing robust and interpretable explanations for the given prediction."}, {"title": "5 EXPERIMENT", "content": "5.1 Experimental Setup\nWe assessed eXpath on the KG LP task using four benchmark datasets: FB15k, FB15k-237 [14], WN18, and WN18RR [4]. These datasets' detailed statistics and link prediction metrics are provided in Table 1. We adhered to the standard splits and training parameters to ensure consistency across comparisons and maintain identical training parameters before and after removing facts.\nWe compared the performance of eXpath against four contemporary systems dedicated to LP interpretation: Kelpie [27], Data Poisoning (DP) [32], Criage [2], and KGEAttack [2]. These implementations are publicly available, and we tailored the code sourced from their respective Github repositories. Since the explanation framework is compatible with any Link Prediction (LP) model rooted in embeddings, we conduct experiments on three models with different loss functions: CompEx [28], ConvE [9], and TransE [29].\nIn adversarial attacks, each explanation framework recommends one or more facts, which are removed before retraining the model with the same parameters. The drop in performance metrics is used to assess the quality of the explanations. The baseline frameworks, including DP, Criage, and Kelpie, focus solely on facts directly related to the head entity (i.e., attributes of the head entity). KGEAttack randomly selects a fact in the extracted rule, while eXpath focuses on facts related to either the head or tail entity, each supported by relevant CP and PT rules. To ensure fairness between the explanation systems, we restrict the number of facts that can be removed. Specifically, DP, Criage, Kelpie(L1), and expath(L1) limit the removal to at most one fact, whereas Kelpie and eXpath can remove up to four facts. Based on experiments and existing literature, we set the thresholds minSC = 0.1, minHC = 0.01, minSupp = 10. These parameters are adapted from the definitions of high-quality rules in prior work [10].\nBased on the problem formulation outlined in Section 3.3, we randomly select a small subset TC Ge from the test set, where the model demonstrates relatively good predictive performance. Specifically, we choose 100 predictions that exhibit strong performance. These predictions are not required to rank first for both head and tail predictions, as enforcing such strict criteria could overly limit the selection process and reduce the applicability of the scenarios. To evaluate model performance, we focus on the relative reduction in reciprocal rank rather than the absolute reduction since the predictions in T are not necessarily top-ranked, and lower-performing predictions are assigned smaller weights. The model's explanatory capability is measured by the relative reduction in H@1 (Hits@1) and MRR (Mean Reciprocal Rank), defined as:\n$H@1(M_{x}, f) = \\frac{1}{2}(1(rk_{h}(M_{x}, f) = 1) + 1(rk_{t}(M_{x}, f) = 1))$\n$RR(M_{x}, f) = \\frac{1}{2}(\\frac{1}{rk_{h}(M_{x}, f)} + \\frac{1}{rk_{t}(M_{x}, f)})$ (9)\n$\\delta H@1(M_{x}, T) = 1 - \\frac{\\Sigma_{f \\in T} H@1(M_{x}, f)}{\\Sigma_{f \\in T} H@1(M_{0}, f)}$\n$\\delta MRR(M_{x}, T) = 1 - \\frac{\\Sigma_{f \\in T} RR(M_{x}, f)}{\\Sigma_{f \\in T} RR(M_{0}, f)}$\nwhere Mx represents the model trained on the dataset excluding the candidate explanations extracted by the explanation framework x, and Mo denotes the original model trained on the entire dataset, 1(.) is the indicator function that returns 1 if the condition inside holds and 0 otherwise.\nWhile both SH@1 and 8MRR are useful, SMRR proves more robust. The stochasticity of model training and small dataset size (100 predictions) can cause significant variability in SH@1 values. This issue is exacerbated for fragile models like TransE, where ranks fluctuate even without attacks. We address this by averaging results over five experimental runs. To ensure that the prediction to be explained is of high quality, we restrict the MRR to greater than 0.5, which ensures that the prediction ranks first in at least one of the head or tail predictions. To evaluate the overall explanatory power, we sum the MRR values of new model Mx and original model Mo across all facts in the numerator and denominator, respectively. This approach ensures that better predictions contribute more significantly to the evaluation, avoiding bias toward selecting only predictions with head and tail ranks 1. Moreover, this method allows for negative explanations, where the rank decreases after removing a fact.\n5.2 Explanation Results\nTables 2 and 3 demonstrate the overall effectiveness of the eXpath method in generating explanations for link prediction tasks, evaluated using the SH@1 and 8MRR metrics as defined in Equation 9. For a fair comparison, explanation methods are categorized based on explanation size (i.e., the number of facts provided). The first"}, {"title": "6 CONCLUSION", "content": "In this work, we introduce eXpath, a novel path-based explanation framework designed to enhance the interpretability of LP tasks on KG. By leveraging ontological closed path rules, eXpath provides semantically rich explanations that address challenges such as scalability and relevancy of path evaluation on embedding-based KGLP models. Extensive experiments on benchmark datasets and mainstream KG models demonstrate that eXpath outperforms the best existing method by 12.4% on 8MRR in terms of the most important multi-fact explanations. A higher improvement of 20.2% is achieved when eXpath is further combined with existing methods. Ablation studies validate that the CP rule in our framework plays a central role in the explanation quality, with its removal leading to a 20.3% average drop in performance.\nWhile our method currently utilizes a small subset of key ontological rules, other rule types, such as unary rules with dangling atoms, are found to have less impact on LP results. This suggests that broader language biases may not always align with the strengths of embedding-based models. Future work can explore the potential of general rule learning on KG and adapt them to the eXpath's overall framework. Additionally, the semantically rich explanations supported by eXpath can benefit from interactive visualization tools, offering enhanced accessibility and understanding of the explanations for both KG experts and non-expert users."}, {"title": "5.3 Fact Position Preferences", "content": "To evaluate the effect of restricting facts to the head or tail entity, we analyzed their impact on explanation performance, focusing on the relative significance of head and tail attributes. Table 4 presents results across explanation sizes (1, 2, 4, 8), where all allows unrestricted fact selection, head restricts facts to those connected to the head entity, and tail restricts facts to those connected"}]}