{"title": "DNF: Unconditional 4D Generation with Dictionary-based Neural Fields", "authors": ["Xinyi Zhang", "Naiqi Li", "Angela Dai"], "abstract": "While remarkable success has been achieved through diffusion-based 3D generative models for shapes, 4D generative modeling remains challenging due to the complexity of object deformations over time. We propose DNF, a new 4D representation for unconditional generative modeling that efficiently models deformable shapes with disentangled shape and motion while capturing high-fidelity details in the deforming objects. To achieve this, we propose a dictionary learning approach to disentangle 4D motion from shape as neural fields. Both shape and motion are represented as learned latent spaces, where each deformable shape is represented by its shape and motion global latent codes, shape-specific coefficient vectors, and shared dictionary information. This captures both shape-specific detail and global shared information in the learned dictionary. Our dictionary-based representation well balances fidelity, contiguity and compression \u2013 combined with a transformer-based diffusion model, our method is able to generate effective, high-fidelity 4D animations.", "sections": [{"title": "1. Introduction", "content": "3D shape representations have seen significant research in computer vision and graphics, with notable recent generative developments in neural field based representations [17, 18, 24], which enable efficient capturing of high-fidelity detail in such a high-dimensional setting. However, realistic generation requires not only 3D generation, but 4D as the world is dynamic, encompassing and requiring motion to enable interactions, across wide-ranging applications such as content creation, mixed reality, simulation, and robotics.\nTraditionally, template-based parametric models have been used to represent category-specific deforming objects, such as bodies [15, 21], faces [2], and hands [28, 29], where a fixed template mesh can be employed. Advances in coordinate-MLP reprsentations to represent neural implicit fields have enabled a compact representation encompassing high-fidelity detail, with the ability arbitrarily query the coordinate-MLP for high resolutions. Such coordinate-MLPs can be optimized to fit single shapes independently, reconstructing very high detail but lacking any shared structure across multiple shapes due to single-shape optimization. These coordinate-MLP neural fields can also be learned across multiple objects, each represented by a la-"}, {"title": null, "content": "tent code, which captures shared global structures but easily loses high-fidelity detail of individial shape details. Additionally, 4D deforming shapes encompasses complexity in both shape and motion, and are more efficiently represented with disentangled shape (which remains the same over a deforming sequence) and the motion of that shape.\nWe thus propose DNF, a new dictionary-learning based 4D representation that compactly represents deforming shapes while maintaining high fidelity; our representation enables unconditional 4D generation through diffusion generative modeling. We introduce dictionary learning into a neural field representation. Inspired by template-based [2, 15] and neural [22, 33] parametric models, we first learn a neural field representation for both shape and motion of deforming 4D objects. We learn coarse latent shape and motion fields: the coarse latent shape space is learned for object shapes in their initial frame poses, and the motion field is conditioned on the shape latent feature to produce temporal flow from the initial frame to its following deformations.\nHowever, learning a single global latent-based representation for shape and motion tends to suffer from loss of detail, both in shape and temporal evolution. Thus, we construct a shared dictionary based on these representations, by decomposing the learned shape and motion MLPs using a singular value decomposition (SVD) [9], and using the singular vector matrices as a shared dictionary. We then fine-tune the singular values on each object; since the singular values can be viewed as the coefficient values of the linear combination of different elements in the dictionary, they are continuous and enable interpolation. Furthermore, to reduce the redundancy and improve the representation capabilities of the dictionary, we compress the dictionary by dropping the singular vectors with small singular values and then expand the dictionary with residual learning in row-rank form. This enables learning a powerful dictionary during the fine-tuning process, capable of representing 4D animations in a disentangled, compact fashion. Our dictionary-based representation characterizes 4D data in the form of latent and coefficient vectors per shape, accompanied with a shared dictionary, which effectively balances quality, contiguity and compression.\nWe then train a transformer-based diffusion model on this representation, enabling unconditional generation of high-fidelity sequences. Due to our flexible dictionary, motions can also be generated for a given shape of a category not seen during training. With the diffusion out-painting, our generations can also be extended to a longer sequence with plausible motion. Experiments on the DeformingThings4D [14] dataset demonstrate the effectiveness of our approach. The main contributions of our work are summarized as follows:\n\u2022 We propose a novel, dictionary-based representation for"}, {"title": null, "content": "4D deforming shapes. A deforming shape is characterized by both shape-specific encodings (shape and motion latent, along with fine-tuned singular value coefficient vectors), along with a shared global dictionary, yield its 4D neural field representation.\n\u2022 We construct a dictionary by decomposing globally-optimized shape and motion MLPs through singular value decomposition to enable a compact representation for fine-tuning shape-specific shape and motion parameters for high-fidelity 4D representations.\n\u2022 Our dictionary-based representation enables effective unconditional 4D generation by employing a transformer-based diffusion model on the learned dictionary representations, achieving state-of-the-art generation of deforming objects."}, {"title": "2. Related Work", "content": "Representing 4D Deformable Shapes Inspired by the success of various advances in 3D representations for expressing static 3D objects, various approaches have been proposed for representing 4D deformable shapes. For domain-specific modeling, such as for human bodies, heads, or hands, template-based parametric models have become widely used [2, 13, 15, 28]. While a fixed template enables robust representation for specific category types, this limits shape expressivity and does not capture general deforming shapes across various categories. Various methods have also recently been introduced to extend mesh-based parametric models to neural formulations [1, 12, 22, 23, 34, 37, 40], while continuing to leverage domain-specific knowledge, thereby constraining the approaches to domain-specific settings.\nRecently, coordinate-field based MLP representations of neural fields have enabled a more flexible representation, capable of representing objects with arbitrary topologies and high resolution. For instance, Occupancy Flow [20] leverages an occupancy field based representation, incorporating Neural-ODE [36] to simulate the velocity field for motion. LPDC [32] replaces the Neural-ODE with an MLP and learns local spatio-temporal codes, representing both shape and deformations. NPMs [22] disentangles the shape and pose into separate latent spaces via two MLP networks, using a shape latent representing an SDF of the shape geometry and a pose latent representing the flow field from the canonical shape. While these methods show strong potential in the neural field representation, it remains challenging to accurately capture complex 4D dynamics, especially in non-rigid objects when using either ODE solvers or a single global latent vector coupled with an MLP network. In contrast, our method constructs a dictionary to encompass both global MLP-based optimization to fit general coarse shapes, along with per-shape specific fine-tuned parameters to achieve high-fidelity detail for each deforming shape se-"}, {"title": "3. Method", "content": "We introduce our dictionary-learning based 4D shape representation, which allows a deforming object to be represented in the form of a shared dictionary, along with a shape-specific latent vector and coefficient vectors. We then use this representation for unconditional 4D generation, employing a diffusion model on our dictionary-based neural fields to generate new, high-fidelity 4D deforming shape sequences."}, {"title": "3.1. Dictionary-based 4D Neural Fields", "content": "To handle the challenges faced by 4D representations, we propose a novel neural field representation to balance shape fidelity, representation contiguity and compression. This representation is learned from a training set comprising S 4D sequences, containing M deforming shape identities. Inspired by Neural Parametric Models [22], we first decompose a 4D sequence of a deforming shape into its shape and motion, using MLP-based coordinate fields. Each canonically-posed shape identity i in the training set is encoded in a D-dimensional latent shape code si through an auto-decoder. Note that we do not assume that shapes are given in canonical poses, and simply use the initial shape in a train sequence as the canonical shape. The shape MLP fe predicts the implicit SDF d for shape identities based on"}, {"title": null, "content": "the shape code si assigned to each i-th identity in S shapes:\n\n$f_\\theta(s_i, x) = d$\t\t(1)\n\nBased on the learned shape space, we then train a motion MLP conditioned on both the identity's latent shape code and the corresponding $D_m$-dimensional latent motion code m to predict a flow vector $\\Delta x$ for the motion of the t-th frame of i-th identity:\n\n$f_{\\theta_m}(s_i, m, x) = \\Delta x$\t\t(2)\n\nThis produces coarse representations of shape and motion, but tends to lack detail when trained across diverse objects. We thus also fine-tune these MLPs to better fit to individual 4D sequences while maintaining shared structure across different train elements.\nDictionary-based fine-tuning. To improve the representation power of the shape and motion MLPs, we introduce dictionary learning into their MLP fine-tuning. We perform a singular value decomposition (SVD) on the MLP parameters, freezing the singular vectors and only fine-tuning the singular values. To reduce redundancy while further improving the representation power, we compress the dictionary (removing small singular values), and then extend it with low-rank residual learning. This representation learning is shown in Figure 2.\nMore specifically, to improve the representation power of the shape and motion MLPs $f_\\theta$ and $f_{\\theta_m}$, we then keep the corresponding optimized shape and motion latents $s_i$ and m fixed while fine-tuning the MLP parameters $\\Theta_s$ and"}, {"title": null, "content": "$\\Theta_m$ for each 3D shape and deformation. For simplicity of notation, we formulate the following for general MLP weights $\\Theta$ and apply the same process for both $\\Theta_s$ and $\\Theta_m$. Note that if we directly fine-tune the whole MLP parameters on each shape to obtain shape-specific MLP weights, there would be no consistency in their weight space, resulting in poor continuity of the underlying representation for generative modeling. Thus, prior to fine-tuning, given an MLP network with layers l = 1,..., L, and weights $\\Theta$ = {$W_l \\in \\mathbb{R}^{J \\times F}$}$_{l=1}^L$, we perform a layer-wise singular value decomposition (SVD) on the MLP parameters:\n\n$W_l = U_l \\Sigma_l V_l^T,$\t\t(3)\n\nwhere $U_l \\in \\mathbb{R}^{J \\times J}$ and $V_l \\in \\mathbb{R}^{F \\times F}$ are matrices of singular vectors and $\\Sigma_l \\in \\mathbb{R}^{J \\times F}$ = diag($\\sigma_l$) is a diagonal matrix with descending non-negative singular values on its diagonal.\nThe singular value decomposition can be written as\n\n$W_l = \\sum_{i=1}^{r} \\sigma_{l,i} (u_{l,i}v_{l,i}^T),$\t\t(4)\n\nwhere r < min(J, F) is the rank of $W_l$, $u_{l,i}$ and $v_{l,i}$ are the i-th column of $U_l$ and $V_l$, singular vectors of $W_l$.\nEach weight layer $W_l$ of the MLP parameters can be viewed as a linear combination of elements in a dictionary, where $\\sigma$ = {$\\sigma_l$}$_{l=1}$ is the coefficient vector and the products of singular vectors in U = {$U_l$}$_{l=1}$ and V = {$V_l$}$_{l=1}$ form the dictionary. We further let $\\sigma$ = $\\sqrt{\\sigma}$ to make sure the non-negativity of the coefficient values. We use the singular"}, {"title": null, "content": "vector matrices U and V to form our dictionary decoder $f_d$ and instantiate a copy of $\\sigma$ for each sample as {$\\sigma_i$}$_{i=1}^N$.\nFor shape fitting, we decompose $\\Theta_s$ to form the shape dictionary decoder $f_d$, fix the dictionary in $f_d$ and only fine-tune {$\\sigma_i$}$_{i=1}^S$ for each shape with a reconstruction loss:\n\n$L_{rec}(d, \\hat{d}) = |clamp(d, \\delta) - clamp(\\hat{d}, \\delta)|,$\t\t(5)\n\nwhere $clamp(x, \\delta) = min(\\delta, max(-\\delta, x))$ uses the parameter $\\delta$ to control the distance from the surface, focusing learning on regions nearby and enhancing surface detail.\n$\\Theta_m$ is decomposed analogously: we build a motion dictionary decoder $f_m$ and fine-tune {$\\sigma_i$}$_{i=1}^M$ for each frame with an $l_1$-loss $L_m$ on the flow prediction:\n\n$L_m = |\\Delta x - \\Delta \\hat{x}|.$\t\t(6)\n\nSince the global latent space is continuous in its nature, we attach a list of coefficient vectors which is also continuous to the latent vector, ensuring the contiguity of their weight space and enabling to generalize to new samples.\nDictionary compression and extension A dictionary decomposed from a pre-trained MLP is capable of representing most cases by a linear combination of existing elements, but cannot fully represent all fine-scale local details, particularly for more complex shapes or deformations. Furthermore, not all the elements in the dictionary play an important role, making the full dictionary relatively inefficient. Thus, rather than using the dictionary directly obtained from the SVD, we first compress to reduce redundancy, and then extend the dictionary to enable more expressivity of detail.\nWe compute an approximation to the matrix $\\Theta$ by simply using only the most important components. Due to the nature of SVD, the data in the matrices U, $\\Sigma$ and V are sorted by their contribution to the matrix $\\Theta$. We can then directly take $U_k$, $V_k^T$ and $\\Sigma_k$, corresponding to the first k columns of U and V and the upper left (kxk)-square of $\\Sigma$, to obtain the approximation:\n\n$\\Theta \\approx \\tilde{\\Theta} = {\\sum_{i=1}^{k} \\sigma_{l,i} (u_{l,i}v_{l,i}^T)}_{l=1}$ \t\t(7)\n\nAfter removing the superfluous elements in the dictionary, we then extend the dictionary with new, more relevant elements learned as residual offsets from the network parameters to further improve its representation capabilities:\n\n$\\Theta' = \\tilde{\\Theta} + \\Delta \\Theta,$\t\t(8)\n\nwhere $\\Delta \\Theta$ can also be written in the same form of SVD with low-rank matrices:\n\n$\\Delta \\Theta = {U_{res} \\Sigma_{res} (V^T)_{res}}_{l=1},$\t\t(9)"}, {"title": null, "content": "with $U_{res} \\in \\mathbb{R}^{J \\times r_k}$, $\\sigma_{res} \\in \\mathbb{R}^{r_k}$, $V_{res} \\in \\mathbb{R}^{F \\times r_k}$, and $r_k \\ll J,F$. Assuming that the residual vector matrices $U_{res}$ and $V_{res}$ can also serve as dictionaries, we aim to use them as singular vector matrices; that is, they should form orthogonal bases. To this end, we add an additional orthogonalization loss when optimizing the residual matrices:\n\n$L_{orth} = |U_{res}^T U_{res} - I| + |V_{res}^T V_{res} - I|.$\t\t(10)\n\nWith this orthogonalization loss, dictionary elements minimize redundancy and enhance interpretability, stability, and computational efficiency, resulting in more distinct and generalizable feature representations.\nDuring this fine-tuning, we freeze $U_k$ and $V_k$ and optimize $U_{res}$ and $V_{res}$ among all train objects, in addition to their individual coefficient vectors {$\\sigma_i$}$_{i=1}^S$ for shapes and {$\\sigma_i$}$_{i=1}^M$ for motion of subsequent frames.\nAs a result, we can represent each shape or motion with its original $D$ or $D_m$-dimensional latent code, concatenating a L-length coefficient vector list {$\\sigma_l \\in \\mathbb{R}^{(k+r_k)}$}$_{l=1}$, which learned during fine-tuning. We denote them as $\\theta_s$ for shape features and $\\Theta_m$ for motion features. This representation is designed to maintain quality, contiguity in the representation space, and support a compact encoding. We can then further use it for generative modeling to synthesize new, high-quality 4D motions."}, {"title": "3.2. Weight-Space Diffusion", "content": "We then learn a generative model on our dictionary-based 4D representation, leveraging diffusion modeling."}, {"title": "Shape Diffusion", "content": "We then model the weight space of $s_i$ through a diffusion process. Using a transformer backbone, our representation which is a (L + 1)-length latent vector list, combining the shape code $s_i$ and L coefficient vectors {$\\sigma_l$}$_{l=1}^L$, naturally split into L + 1 tokens.\nDuring diffusion modeling, as shown in Figure 3, we gradually add gaussian noise t times to $\\theta_s$. A linear projection is then applied to the noised vector and the sinusoidal embedding of t. Afterwards, the projections are summed up with the position encoding vector on each token position and fed into a transformer decoder. The transformer decoder consists of multiple self-attention layers, and predicts the denoised tokens with the simple objective [10]:\n\n$L_{simple} = E_{\\theta \\sim q(\\sigma_s),t \\sim [1,T]} [||\\theta_s - \\hat{\\theta_s}||^2].$\t\t(11)\n\nThen the denoised tokens are passed through a final output projection to produce the predicted denoised vectors $\\hat{\\theta_s}$.\nDuring inference, we can then sample new $\\theta_s$ from random noise. We then split $\\theta_s$ into a latent vector $\\hat{s}$ and a list of coefficient vectors $\\hat{\\sigma_l}$ for each MLP layer. With the shape dictionary decoder $f_d$, we can obtain a neural field"}, {"title": null, "content": "with the generated $\\hat{\\sigma_s}$, to present the generated shape with the predicted SDF:\n\n$f_d(\\hat{s}, x, \\hat{\\sigma_s}) = \\hat{d}.$\t\t(12)\n\nMotion Diffusion In order to model the motion sequence of a deforming shape, we train a diffusion model on windowed sequences of length t. That is, for a motion sequence with T original frames, we randomly pick subsequences with t frames for training. These subsequences are constructed by concatenating the t motion features in an extra time dimension $\\Theta_m = {\\theta_t$}$_{t=1}^T$, conditioning on the shape code of the canonical shape. To introduce the shape conditions, we use a conditional cross-attention in addition to the self-attention layers in the transformer decoder.\nTo further maintain the frame order and improve coherence in the time dimension, we add an extra temporal self-attention on the time dimension afterwards. The temporal self-attention is performed among tokens from different frames, but with the same positions in the {$\\theta_m$}$_{t=1}^t$ (e.g., motion codes for different frames).\nTrained on a random subsequences of the original motion sequence, our motion diffusion is capable of generating sequences longer than t frames through diffusion out-painting with a sliding window. We first generate a t-frame sequence, using the last k frames as the context, and let the diffusion model in-paint the following (t - k) frames, and iteratively repeat this process. In practice, our diffusion model is trained to generate 6-frame motions and uses the last 2 frames as context to in-paint the subsequent 4 frames, thus extending the generated motion sequence."}, {"title": "4. Experiments", "content": "We evaluate our approach on unconditional 4D motion generation, and demonstrate its ability to generalize to synthesizing new motions for unseen animal species."}, {"title": "4.1. Experimental Setup", "content": "Datasets. We use the DeformingThings4D [14] dataset to train our approach and all baselines. DeformingThings4D contains 38 different shape identities for a total of 1227 animations, divided into training (75%), validation (5%), and test (20%) subsets. The test sets are divided into unseen motions and unseen shapes, including unseen species. We use the first frame of each train sequence to represent shape identities for shape training. Shape SDFs are computed by sampling 200,000 points around the object surface and uniformly in the unit sphere. For motion sequences, we sample the first 16 frames of each sequence and sample 200,000 corresponding points for each frame of the sequence.\nImplementation details. For our shape and motion MLPs, we use 384-dimensional latent codes along with an"}, {"title": "4.2. Unconditional Motion Generation", "content": "We compare with state of the art on unconditional 4D generation, generating 16-frame animal motion sequences. For our method, we first generate 6 frames and then continually extend by 4 frames, using the last 2 frames as context. As shown in Tab 1, we achieve notably improved results compared to the baselines. Fig. 4 shows a visual comparison, illustrating our improved visual quality and temporal consistency in the generated motions.\nIn particular, HyperDiffusion suffers from poor shape quality with broken or missing legs during movements, due to the lack of continuity in the weight space of HyperDiffusion. Motion2VecSets preserves finer shape details, but"}, {"title": "4.3. Ablations", "content": "To evaluate the effectiveness of our representation in capturing finer local details, we conduct ablation studies focusing on the impact of dictionary-based fine-tuning and the decoupling of shape and motion spaces."}, {"title": "Effect of the dictionary-based fine-tuning.", "content": "To verify the effectiveness of our dictionary-based fine-tuning in capturing finer local details, we compare our method with NPMS in terms of shape reconstruction quality. As shown in Tab. 2, we compare the average Chamfer Distance between the ground truth meshes and the reconstructed meshes over the first 16 frames of each motion. NPMs uses only a disentangled global latent representation for each deforming shape, which struggles to capture fine details, in contrast to our instance-specific compressed, weight-space fine-tuning."}, {"title": "Effect of decoupling shape and motion.", "content": "Another approach to fitting deforming shapes is to directly fine-tune the shape code (with or without the coefficient vector list $\\sigma_l$) of the initial shape to accommodate subsequent deformations, while keeping the global MLP fixed (denoted as Sft and $\\text{S-}\\sigma_{ft}$). As shown in Tab. 2, when fine-tuning with $\\sigma_s$, the reconstruction performance surpasses NPMs', but remains inferior to our decoupled method. Additionally, decoupling shape and motion space not only improves the reconstruction quality, but also enables our diffusion model to generate motions for unseen animal species."}, {"title": "4.4. Generating Motions for Unseen Shape Species", "content": "Our generative model can even generalize to generate plausible motions for unseen shape identities, including unseen species. Given a mesh of a new shape identity, we leverage our learned shape dictionary to obtain a neural field that represents the shape by optimizing a new shape code and coefficient vector list. As shown in Fig. 6, for animal species not seen during training, our method significantly improves reconstruction quality compared to using only the shape code for fitting, which captures the general shape but struggles to represent fine local details. Moreover, our motion diffusion model can also generalize to these new shape conditions, producing realistic global and local deformations for unseen animal species."}, {"title": "Limitations.", "content": "While our DNF demonstrates potential for a more expressive, compact 4D representation space, vari-"}, {"title": "5. Conclusion", "content": "We have presented a new, dictionary-based representation for 4D deforming objects that maintains a compact, contiguous latent representation to disentangle shape and motion for high-fidelity unconditional 4D generation. We leverage a weight-space representation of shape and motion for 4D objects, using compressed dictionary-based fine-tuning to maintain local detail across a diverse array of shapes. This enables training a diffusion model on our dictionary-based representation to synthesize new deforming sequences. We believe this will enable new opportunities in generative modeling for high-dimensional, complex data."}, {"title": "6. Network Architecture Details", "content": "With a pre-trained shape and motion MLP, we first conduct SVD to each linear layer of the MLP and compress the matrices $U\\in\\mathbb{R}^{J\\times J}$, $\\Sigma\\in \\mathbb{R}^{J\\times F}$ and $V\\in \\mathbb{R}^{F\\times F}$ to $U_k \\in \\mathbb{R}^{J\\times k}$,$V_k \\in \\mathbb{R}^{F\\times k}$ and $\\Sigma_k \\in \\mathbb{R}^{k\\times k}$. For each layer in the MLP, we then use two linear layers $N_U \\in \\mathbb{R}^{J\\times k}$ and $N_V \\in \\mathbb{R}^{F\\times k}$ to play the role as U and V, replacing the original linear layer $N \\in \\mathbb{R}^{J\\times F}$. To extend the dictionary, we use another two linear layers $N_{U,\\iota} \\in \\mathbb{R}^{J\\times r_k}$ and $N_{V,\\iota} \\in \\mathbb{R}^{F\\times r_k}$ to learn the residual. During the fine-tuning, we freeze the parameters of $N_U$ and $N_V$ and optimize $N_{U,\\iota}$, $N_{V,\\iota}$, and $\\sigma$."}, {"title": "6.2. Shape Diffusion", "content": "For each shape feature, consisting of nine (L + 1) vectors (one original latent code and eight coefficient vectors corresponding to eight MLP layers), we naturally split them into nine tokens. Each token is projected to the same dimension, set to 1280 in our implementation. The projected tokens are then summed with positional encoding vectors corresponding to their positions and fed into a transformer decoder. The transformer decoder, composed of 32 self-attention layers, predicts the denoised tokens."}, {"title": "6.3. Motion Diffusion", "content": "The overall architecture of motion diffusion is similar to that of shape diffusion but operates on a sequence of motions with t frames as input. The t motion features are concatenated along an additional time dimension, and a positional encoding is added in this dimension to ensure the correct order of the generated motions. Similarly, we project these (t x L) tokens to an inner dimension and add positional encoding vectors based on their token positions. In the motion diffusion model, each layer of the transformer decoder contains three attention layers:\n1. A spatial self attention layer to aggregate tokens within each frame,"}, {"title": null, "content": "2. A condition cross-attention layer to incorporate shape conditions, and\n3. A temporal self-attention layer to aggregate tokens from the same position across different frames (e.g., motion codes of different frames).\nIn the sampling stage, our motion diffusion is capable of generating sequences longer than t frames through diffusion out-painting with a sliding window. We first generate a t-frame sequence, using the last k frames as the context, and let the diffusion model in-paint the following (t-k) frames, and iteratively repeat this process.\nTo be more specific, given the motion features {$\\theta^t_{m}$}$^k_0$ of the last k frames, we append (t \u2013 k) vectors, {$\\theta^{t-k}_{m}$}, which are initialized as random noise of the same size as the motion features. The goal is to denoise {$\\theta^{t-k}_{m}$} using the context provided by {$\\theta^t_{m}$}.\nFor each denoising time step d, we aim to denoise {$\\theta^{t-k}_{m}$}$^d$ into {$\\theta^{t-k}_{m}$}$^{d-1}$. To achieve this, we first apply a d-step diffusion process to {$\\theta^t_{m}$}, obtaining a noised version, {$\\theta^t_{m}$}$^d$, which is then concatenated with {$\\theta^{t-k}_{m}$}$^d$. Subsequently, our motion diffusion model denoises the combined vectors, producing {$\\theta^{t-k}_{m}$}$^{d-1}$ using a DDIM sampler.\nIn practice, our diffusion model is trained to generate 6-frame motions and uses the last 2 frames as context to in-paint the subsequent 4 frames, thus extending the generated motion sequence."}, {"title": "7. Implementation Details", "content": "7. Data processing\nShape space. For each shape identity in the train dataset, we sample 200k points on the given mesh. We then calculate its grid SDF with resolution equals to 256, sampling 50k points uniformly within the unit bounding box and 150k random near-surface points within a distance of 0.02 from the surface of the shape.\nPose space. Following previous work [22], we sample 200k surface points on each shape identity and store the barycentric weights for each sampled point at the same time. Each point is then randomly disturbed with a small noise $\\mathcal{N}(0, \\Sigma)$ along the normal direction of the corresponding triangle in the mesh, with $\\Sigma \\in \\mathbb{R}^3$ a diagonal covariance matrix with entries $\\Sigma_{ii} = \\sigma$. Then, for each t-th deforming shape for the identity, we compute corresponding points by using the same barycentric weights and the noise to sample the deformed mesh. In our experi-"}, {"title": null, "content": "ments, we sample 50% surface points ($\\sigma \\approx 0$) and 50% with $\\sigma = 0.002$."}, {"title": "7.2. Data augmentation", "content": "When training the motion diffusion model, we apply data augmentation techniques to enhance the model's robustness. Specifically, for each motion subsequence, we reverse the frame order to create a new training sample, which significantly improves the continuity of the generated motions. Additionally, we distribute the shape condition S using a few-step diffusion process, defined as\n\n$S_t = f(S_{t-1}, \\epsilon_t)$, for t = 1, ...,T,\n\nwhere f represents the diffusion forward function, $\\epsilon_t$ is the added noise, and T is the total number of steps. Here, we choose T randomly from the range [0, 50]."}]}