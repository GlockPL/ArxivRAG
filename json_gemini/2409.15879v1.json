{"title": "Machine Translation Advancements of Low-Resource Indian Languages by Transfer Learning", "authors": ["Bin Wei", "Jiawei Zhen", "Zongyao Li", "Zhanglin Wu", "Daimeng Wei", "Jiaxin Guo", "Zhiqiang Rao", "Shaojun Li", "Yuanchang Luo", "Hengchao Shang", "Jinlong Yang", "Yuhao Xie", "Hao Yang"], "abstract": "This paper introduces the submission by Huawei Translation Center (HW-TSC) to the WMT24 Indian Languages Machine Translation (MT) Shared Task. To develop a reliable machine translation system for low-resource Indian languages, we employed two distinct knowledge transfer strategies, taking into account the characteristics of the language scripts and the support available from existing open-source models for Indian languages. For Assamese(as) and Manipuri(mn), we fine-tuned the existing IndicTrans2(Gala et al., 2023) open-source model to enable bidirectional translation between English and these languages. For Khasi (kh) and Mizo (mz), We trained a multilingual model as a baseline using bilingual data from these four language pairs, along with an additional about 8kw English-Bengali bilingual data, all of which share certain linguistic features. This was followed by fine-tuning to achieve bidirectional translation between English and Khasi, as well as English and Mizo. Our transfer learning experiments produced impressive results: 23.5 BLEU for enas, 31.8 BLEU for en\u2192mn, 36.2 BLEU for as\u2192en, and 47.9 BLEU for mn\u2192en on their respective test sets. Similarly, the multilingual model transfer learning experiments yielded impressive outcomes, achieving 19.7 BLEU for en\u2192kh, 32.8 BLEU for en\u2192mz, 16.1 BLEU for khen, and 33.9 BLEU for mz en on their respective test sets. These results not only highlight the effectiveness of transfer learning techniques for low-resource languages but also contribute to advancing machine translation capabilities for low-resource Indian languages.", "sections": [{"title": "1 Introduction", "content": "In the realm of machine translation, Neural Machine Translation (NMT) has become the dominant technology, as confirmed by previous research. However, training NMT models requires large amounts of data, which presents a significant challenge when dealing with low-resource languages. To tackle this challenge, we employed transfer learning, a well-established approach that enhances model performance by transferring knowledge gained from one task to other related tasks. To improve translation capabilities for low-resource languages, we faced the challenge of limited bilingual resources for Indian languages. To overcome this issue, we trained a multilingual model using not only all the bilingual data provided for the task but also additional Bengali data. Additionally, we examined the languages supported by the existing IndicTrans2(Gala et al., 2023) open-source model and conducted a comparative analysis. Based on our findings, we selected different baseline models for knowledge transfer depending on the language pair: for Assamese and Manipuri, we used the IndicTrans2 model as the baseline, while for Khasi and Mizo, we trained multilingual model by ourselves as the baseline. This approach enabled us to effectively leverage existing resources while addressing the specific challenges associated with each language pair.\nIndicTrans2 is the first open-source transformer-based multilingual NMT model that supports high-quality translations across all the 22 scheduled Indic languages. It was trained on the extensive Bharat Parallel Corpus Collection (BPCC), a publicly accessible repository encompassing both pre-existing and freshly curated data for all 22 scheduled Indian languages, this model boasts a comprehensive understanding of the linguistic diversity within the Indian subcontinent. To enhance its linguistic prowess, IndicTrans2 has undergone auxiliary training utilizing the rich resource of back-translated monolingual data. The model was then trained on human-annotated data to achieve further improvements. We used this model in the first two subtasks and fine-tuned it on the training data provided by WMT24. By adopting this approach, we aim to capitalize on the acquired knowledge during training to significantly bolster the performance of the model in the specific translation task at hand. The fine-tuned IndicTrans2 achieves good scores, so we are using it for our final submission in the first two subtasks.\nFor the multilingual model, we first utilized resources from Bengali. The choice of Bengali was based on its belonging to the Indo-Aryan branch, its linguistic feature similarities with some of the target low-resource languages, and its relatively rich available data. By introducing Bengali data, we aimed to enable the model to learn features potentially shared with the target languages, thereby laying a foundation for processing other related languages. Next, we integrated all available bilingual data from Indic language MT track. This included parallel corpora between various Indian languages and English. Although the data for each language pair might be limited individually, the combined dataset offered diverse learning samples. We believe that this integration of multilingual data helps the model capture both the commonalities and differences among different Indian languages. Based on this carefully selected and integrated data, we trained a multilingual model. The design goal of this model was to handle translation tasks for multiple Indian languages simultaneously, using the commonalities between languages to compensate for the scarcity of data in any single language. Through this approach, we expect the model to learn more generalized language representations and translation knowledge under resource constraints, leading to improved performance on Khasi and Mizo translation tasks.\nUltimately, we adopted a differentiated strategy for knowledge transfer. This approach thoroughly considered the characteristics of each language to achieve optimal transfer effects. In Section 2, we will discuss the details of the data, the methods and processes used for data pre-processing. Section 3 will cover the overall architecture and training strategies of the NMT system, including a detailed account of the various optimization methods. In Section 4, we will present the experimental parameters, results, and their analysis. The final section will summarize the key findings of the paper."}, {"title": "2 Data", "content": ""}, {"title": "2.1 Data Details", "content": "We have fine-tuned the model using the WMT24 corpus. Additionally, we used 2M monolingual english dataset to do BT and FT. The amount of data we used is shown in Tables 1."}, {"title": "2.2 Data Pre-processing", "content": "Our data pre-processing methods for NMT include:\n\u2022 Remove duplicate sentences or sentence pairs.\n\u2022 Convert full-width symbols to half-width.\n\u2022 Use fasttext\u00b9 (Joulin et al., 2016) to filter other language sentences.\n\u2022 Use mosesdecoder\u00b2 (Koehn et al., 2007) to normalize English punctuation.\n\u2022 Filter out sentences with more than 150 words.\n\u2022 Use fast-align (Dyer et al., 2013) to filter sentence pairs with poor alignment.\n\u2022 Sentencepiece\u00b3 (SPM) (Kudo and Richardson, 2018) is used to perform subword segmentation, and the vocabulary size is set to 32K.\nSince there may be some semantically dissimilar sentence pairs in bilingual data, we use LaBSE\u2074 (Feng et al., 2022) to calculate the semantic similarity of each bilingual sentence pair, and exclude bilingual sentence pairs with a similarity score lower than 0.75 from our training corpus."}, {"title": "3 NMT System", "content": ""}, {"title": "3.1 System Overview", "content": "We use Transformer (Vaswani, 2017) as our neural machine translation (NMT)(Bahdanau et al., 2014) model architecture. For the first two subtasks(en-as, en mn), we use the IndicTrans2(Gala et al., 2023)"}, {"title": "3.2 Regularized Dropout", "content": "Regularized Dropout (R-Drop)\u2075 (Wu et al., 2021) is a simple yet more effective alternative to regularize the training inconsistency induced by dropout (Srivastava et al., 2014). Concretely, in each mini-batch training, each data sample goes through the forward pass twice, and each pass is processed by a different sub model by randomly dropping out some hidden units. R-Drop forces the two distributions for the same data sample outputted by the two sub models to be consistent with each other, through minimizing the bidirectional Kullback-Leibler (KL) divergence (Van Erven and Harremos, 2014) between the two distributions. That is, R-Drop regularizes the outputs of two sub models randomly sampled from dropout for each data sample"}, {"title": "3.3 Data Diversification", "content": "Data Diversification (DD) (Nguyen et al., 2020) is a data augmentation method to boost NMT performance. It diversifies the training data by using the predictions of multiple forward and backward models and then merging them with the original dataset which the final NMT model is trained on. DD is applicable to all NMT models. It does not require extra monolingual data, nor does it add more parameters. To conserve training resources, we only use one forward model and one backward model to diversify the training data."}, {"title": "3.4 Forward Translation", "content": "Forward translation (FT) (Abdulmumin, 2021), also known as self-training, is one of the most commonly used data augmentation methods. FT has proven effective for improving NMT performance by augmenting model training with synthetic parallel data. Generally, FT is performed in three steps: (1) randomly sample a subset from the large-scale source monolingual data; (2) use a \"teacher\" NMT model to translate the subset data into the target language to construct the synthetic parallel data; (3) combine the synthetic and authentic parallel data to train a \"student\" NMT model."}, {"title": "3.5 Back Translation", "content": "An effective method to improve NMT with target monolingual data is to augment the parallel training data with back translation (BT) (Sennrich et al., 2016; Wei et al., 2023). There are many published works that expand the understanding of BT and investigate methods for generating synthetic source sentences. Edunov et al. (2018) find that back translations obtained via sampling or noised beam outputs are more effective than back translations generated by beam or greedy search in most scenarios. Caswell et al. (2019) show that the main role of such noised beam outputs is not to diversify the source side, but simply to tell the model that the given source is synthetic. Therefore, they propose a simpler alternative strategy: Tagged BT. This method uses an extra token to mark back translated source sentences, which generally outperforms noised BT (Edunov et al., 2018). For better joint use with FT, we use sampling back translation (ST) (Edunov et al., 2018)."}, {"title": "3.6 Denoise", "content": "In machine translation, denoising improves translation quality by removing noise from the training data, such as inaccurate translations, grammatical errors, or unnatural sentence structures, allowing the model to focus on high-quality data and produce more accurate and fluent translations. Additionally, denoising enhances the model's robustness by eliminating noisy data, which helps the model better learn the target language's patterns, reducing errors and leading to more stable and reliable performance across diverse inputs. It also optimizes training efficiency by decreasing the amount of data the model needs to process, particularly by filtering out low-quality data, which results in a cleaner and more consistent dataset and can shorten the overall training time. Moreover, denoising reduces error propagation by preventing the model from learning incorrect language patterns, thereby minimizing the accumulation and spread of errors in generated translations. Finally, it enhances the model's generalization ability, as denoised data is more representative, enabling the model to better adapt to different types of input sentences and improve its performance in real-world applications. Through denoising, machine translation models can more effectively utilize high-quality data, leading to superior translation outcomes and greater overall model stability."}, {"title": "3.7 Transductive Ensemble Learning", "content": "Ensemble learning (Garmash and Monz, 2016), which aggregates multiple diverse models for inference, is a common practice to improve the performance of machine learning models. However, it has been observed that the conventional ensemble methods only bring marginal improvement for NMT when individual models are strong or there are a large number of individual models. Transductive Ensemble Learning (TEL) (Zhang et al., 2019) studies how to effectively aggregate multiple NMT models under the transductive setting where the source sentences of the test set are known. TEL uses all individual models to translate the source test set into the target language space and then fine-tune a strong model on the translated synthetic data, which significantly boosts strong individual models and benefits a lot from more individual models."}, {"title": "3.8 Transfer Learning", "content": "Transfer learning(TL) is a machine learning technique where a model trained on one task is adapted for a second related task. Instead of starting the training of a new model from scratch, transfer learning leverages the knowledge learned from the first task to improve learning on the second task. For Assamese(as) and Manipuri(mn), We have used IndicTrans2(Gala et al., 2023), a powerful model that performs well for English-to-Indic and Indic-to English translation for 22 scheduled Indian languages. This knowledge can be used to translate other Indian languages to and from English. Our approach entailed the fine-tuning of this model, leveraging the parallel corpus provided by the WMT24 for the Indic MT task. This fine-tuning process equipped the model with the expertise required to proficiently translate Assamese and Manipuri to and from English, ultimately yielding the most outstanding results. Similarly, for Khasi and Mizo, we trained a multilingual model as the baseline. We also applied transfer learning techniques to enhance the baseline model using data specific to these language pairs. The results on both the test and dev sets were highly encouraging."}, {"title": "4 Experiment", "content": ""}, {"title": "4.1 Settings", "content": "We use Transformer architecture in all the subtasks. For the first two subtasks, we use Indic Trans2 (Gala et al., 2023) as our baseline model, which is a deep Transformer architecture with 18-layers encoder and 18-layers decoder. With the latter subtasks, the model is also a Transformer architecture with 35-layers encoder and 3-layers decoder. For the first two subtasks, our models apply Adam (Kingma and Ba, 2014) as optimizer to update the parameters with \u03b2\u2081 = 0.9 and B2 = 0.98. We employ a warmup learning rate of 10\u207b\u2077 for 2000 update steps and a learning rate of 3 * 10\u207b\u2075. For normalization, we use a dropout value of 0.2 and normalize the probabilities using smoothed label cross-entropy. We use GeLU activations (Hendrycks and Gimpel, 2016) for better learning. For the latter subtasks, parameter update frequency is 2, and learning rate is 5e-4. The number of warmup steps is 4000, and model is saved every 1000 steps. R-Drop (Wu et al., 2021) is used in model training for all subtasks, and we set A to 5."}, {"title": "4.2 Results", "content": "Regarding this four language pair directions, we use Regularized Dropout, Bidirectional Training, Data Diversification, Forward Translation, Back Translation, Alternated Training, Curriculum Learning, and Transductive Ensemble Learning. The evaluation results of four language pair directions NMT system on WMT24 Indic MT test and dev set are shown in Tables 2 and Tables 3.\nAs shown in Table 2, IndicTrans2(Gala et al., 2023) provides a strong baseline. Fine-tuning the model with FT, BT, and bitext data leads to significant improvements, particularly in the en-mn direction, where the BLEU score increases by nearly 20 points over the baseline on the test and dev set. This improvement is largely attributed to Data Diversification. Table 3 further illustrates that FT and BT data contribute the most to model performance, especially in the en-mz direction, which sees an increase of nearly six BLEU points compared to the multilingual baseline. Even after enhancing the model with BT and FT data, adding filtered high-quality bilingual data results in an average gain of about one BLEU point, highlighting the critical role of data quality. Finally, we all use TEL technique to obtain a good result, the improvement is very small, almost less than one bleu score."}, {"title": "5 Conclusion", "content": "This paper presents the submission of HW-TSC to the WMT24 Indic MT Task. For the first two sub-tasks, we use IndicTrans2 as our baseline model to fine-tune it with corpus provided by WMT24 on the en-as and en-mn language pairs, which achieves remarkable performance. For the latter two sub-tasks, we train a multilingual model on the en-kh and en-mz language pairs, and then use training strategies such as R-Drop, DD, FT, BT, denoise and TEL to train the NMT model based on the deep Transformer-big architecture. By applying these training strategies, our submission achieved a competitive result in the final evaluation."}]}