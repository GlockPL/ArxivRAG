{"title": "KAN4TSF: Are KAN and KAN-based models Effective for Time Series Forecasting?", "authors": ["Xiao Han", "Xinfeng Zhang", "Yiling Wu", "Zhenduo Zhang", "Zhe Wu"], "abstract": "Time series forecasting is a crucial task that predicts the future values of variables based on historical data. Time series forecasting techniques have been developing in parallel with the machine learning community, from early statistical learning methods to current deep learning methods. Although existing methods have made significant progress, they still suffer from two challenges. The mathematical theory of mainstream deep learning-based methods does not establish a clear relation between network sizes and fitting capabilities, and these methods often lack interpretability. To this end, we introduce the Kolmogorov-Arnold Network (KAN) into time series forecasting research, which has better mathematical properties and interpretability. First, we propose the Reversible Mixture of KAN experts (RMOK) model, which is a KAN-based model for time series forecasting. RMOK uses a mixture-of-experts structure to assign variables to KAN experts. Then, we compare performance, integration, and speed between RMoK and various baselines on real-world datasets, and the experimental results show that RMoK achieves the best performance in most cases. And we find the relationship between temporal feature weights and data periodicity through visualization, which roughly explains RMOK's mechanism. Thus, we conclude that KAN and KAN-based models (RMOK) are effective in time series forecasting. Code is available at KAN4TSF: https://github.com/2448845600/KAN4TSF.", "sections": [{"title": "1 Introduction", "content": "Time series forecasting (TSF) is the task of using historical data to predict future states of variables. This research area includes a broad scope of applications, such as financial investment, weather forecasting, traffic estimation, and health management Bi et al. [2023], Gao et al. [2023], Savcisens et al. [2024], Han et al. [2024a]. The machine learning community's progress has long inspired time series forecasting technology: the popularity of early statistical learning methods gave rise to SVR and ARMIA, while the development of deep learning introduced MLP and Transformer into time series forecasting. At present, various time series forecasting methods cover almost all deep learning network architectures, such as RNN, CNN, Transformer, and MLP Nie et al. [2023], Wu et al. [2023], Han et al. [2024b]. The forecasting models derived from different network architectures have their own advantages in forecasting performance, running speed, and resource usage.\nAlthough deep learning-based models have made notable progress in time series forecasting, there are still several challenges. The universal approximation theorem (UAT), which is the mathematical foundation of most mainstream forecasting models, cannot provide a guarantee on the necessary network sizes (depths and widths) to approximate a predetermined continuous function with specific accuracy. And this theory can only achieve an approximation rather than a representation. The limitations of UAT have become the sword of Damocles hanging over time series forecasting. Furthermore, the prediction mechanism of existing models is black-box, resulting in a lack of interpretability. These nontransparent methods are suspected of being suitable for tasks that require a low tolerance for errors, such as medicine, law and finance.\nKolmogorov-Arnold Network (KAN) Liu et al. [2024a], which is based on the Kolmogorov-Arnold representation theorem (KART), has become a novel approach to solving the above challenges. On the one hand, KART proves that a multivariate continuous function can be represented as a combination of finite univariate continuous functions. This theorem establishes the relationship between network size and input shape under the premise of representation. On the other hand, KAN offers a pruning strategy that simplifies the trained KAN into a set of symbolic functions, enabling the analysis of specific modules' mechanisms, thereby significantly enhancing the network's interpretability. In addition, KAN's function fitting idea is consistent with the properties of time series, such as periodicity and trend, which is conducive to embedding prior knowledge into the network structure and improving the performance of the network.\nDespite being a relatively recent proposal, KAN, which employs a trainable 1D B-spline functions to convert incoming signals, has already sparked numerous efforts to improve or broaden its capabilities. Some studies propose KAN's variants which replace the B-splines with Chebyshev polynomials SS [2024], wavelet functions Bozorgasl and Chen [2024], Jacobi polynomials Aghaei [2024], ReLU functions Qiu et al. [2024], etc., to accelerate training speed and improve network performance. Other studies introduce KAN with existing popular network structures for various applications. For example, ConvKAN Bodner et al. [2024] and GraphKAN Zhang and Zhang [2024], Xu et al. [2024] are proposed for image processing and graph processing. In summary, KANs have been extensively empirically studied in vision and language Azam and Akhtar [2024], Yu et al. [2024]. However, existing studies lack a KAN-based model that considers time series domain knowledge, making it impossible to verify whether KAN is effective in time series forecasting.\nTo this end, we aim to propose a KAN-based model for the time series forecasting task and evaluate its effectiveness from four perspectives: performance, integration, speed, and interpretability. First, we propose the Reversible Mixture of KAN Experts model (RMoK), a KAN-based time series forecasting model that uses multiple KAN variants as experts and a gating network to adaptively assign variables to specific experts for prediction. RMoK is implemented as a single-layer network because we hope that it can have similar performance and better interpretability than existing methods. Then, we use a unified training and evaluation setting to compare the performance of RMoK and current popular baselines on seven real-world datasets. The experimental results show that RMoK achieves state-of-the-art (SOTA) performance in most cases. Subsequently, we conduct a comprehensive empirical study on KAN-based models, including the comparison between KAN and Linear, the effect of integrating KANs with the Transformer, and the speed of the KAN-based models. Finally, we discuss the interpretability of RMoK using the example of weather prediction. We visualize the weights of temporal features at different time steps in KAN and find the correlation between the weight distribution and the periodicity of the data.\nTo sum up, the contributions of this work include:\n\u2022 To the best of our knowledge, this is the first work that comprehensively discusses the effectiveness of the booming KANs for time series forecasting.\n\u2022 To validate our claims, we propose the Reversible Mixture of KAN Experts model, which uses a single layer of the mixture of KAN experts to keep a balance between performance and interpretability.\n\u2022 We fairly compare the performance between RMoK and baselines on seven real-world datasets, and the experimental results show that RMoK achieves the best performance in most cases. And we also conduct a comprehensive empirical study on KAN-based models about integration and speed.\n\u2022 We mine the relationship between time feature weights and data periodicity through visual-ization, which roughly explains the mechanism of RMOK.\nIn summary, compared with the baselines in terms of performance, integration, speed, and inter-pretability, we conclude that KAN is effective in time series forecasting."}, {"title": "2 Problem Defintion", "content": "In multivariate time series forecasting, given historical data $X = [X_1,\u2026, X_T] \\in \\mathbb{R}^{T\\times C}$, where T is the time steps of historical data and C is the number of variates. The time series forecasting task is to predict $Y = [X_{T+1},\uff65\uff65\uff65, X_{T+P}] \\in \\mathbb{R}^{P\\times C}$ during future P time steps."}, {"title": "3 Related Work", "content": ""}, {"title": "3.1 Time-Series Forecasting Models", "content": "Although Transformer-based methods have almost become the standard in CV and NLP, various network architectures (such as Transformer, CNN, and MLP) are competing in time series forecasting recently.\nTransformer-based time series forecasting models have strong performance but high time and memory complexity. Informer Zhou et al. [2021] proposed ProbSparse self-attention to reduce the complexity from $O(T^2)$ to $O(T log T)$. Pyraformer Liu et al. [2022a] utilizes the pyramid attention mechanism to capture hierarchical multi-scale time dependencies with a time and memory complexity of O(T). PatchTST Nie et al. [2023] and Crossformer Zhang and Yan [2023] use the patch operation to reduce the number of input tokens, thereby reducing time complexity. The performance of early Multi-layer perception-based (MLP-based) models is generally weaker than transformer-based methods. However, NLinear Zeng et al. [2023] and RLinear Li et al. [2023] combine different normalization methods with a single-layer MLP, and achieve performance that exceeds Transformer-based models on some datasets with extremely low computational cost.\nRecurrent neural networks (RNNs) are suitable for handling sequence data, making them a favorable choice for time series analysis. The size of the hidden state in RNN is independent of the input time series length, so recent RNN-based time series prediction models, such as SegRNN Lin et al. [2023] and WITRAN Jia et al. [2024], apply the RNN structure to time series forecasting with longer input. Convolutional neural networks (CNNs) are frequently used in time series forecasting models in the form of 1D convolution, such as ModernTCN donghao and wang xue [2024] and SCINet Liu et al. [2022b], but TimesNet Wu et al. [2023] takes a different approach by converting the 1D time series into a 2D matrix through the Fourier transform and then using 2D convolution for prediction. In addition to the above four types of network structures, there is also time series forecasting work based on new network architectures such as Mamba Dao and Gu [2024], Wang et al. [2024] and RWKV Peng et al. [2023], Hou and Yu [2024]."}, {"title": "3.2 Kolmogorov-Arnold Network", "content": "The Kolmogorov-Arnold representation theorem (KART) is the mathematical foundation of the Kolmogorov-Arnold Network (KAN) Liu et al. [2024a], which makes KAN more fitting and inter-pretable than Multi-Layer Perceptrons (MLP) based on the universal approximation theorem. We show the different between KAN and MLP in Figure 1. Given an input tensor $x \\in \\mathbb{R}^{n_0}$, the structure of L layers KAN network can be represented as:\n$\u039a\u0391\u039d(x) = (\u03a6_1 \u25cb \u03a6_{L-1} \u25cb\uff65\uff65\uff65 \u25cb \u03a6_2 \u25cb \u03a6_1) X$,\n(1)"}, {"title": "4 RMOK", "content": ""}, {"title": "4.1 Mixture of KAN Experts Layer", "content": "Time series in real-world scenarios frequently exhibit non-stationarity, with their statistical properties (such as mean and variance) varying over time. Moreover, there are significant distribution discrepan-cies between variables in multivariate time series. This poses a significant challenge to time series forecasting techniques, inevitably impacting the KAN-based methods as well. Fortunately, when it comes to dealing with distribution shift, KAN has a unique characteristic compared to existing Linear and Transformer-based networks: KAN has many variants using different spline functions. Considering that the special spline function may be suitable for modeling certain data distributions, we try to combine several KANs into a single layer and adaptively schedule them according to the input data.\nFollowing this idea, we propose the mixture of KAN experts (MoK) layer, which is a combination of KAN and mixture of experts (MoE). MoK layer uses a gating network to assign KAN layers to variables according to temporal features, where each expert is responsible for a specific part of the data. KAN and its variants only differ in the spline function in Equation 3, so we use $K(\\cdot)$ to represent these methods uniformly in this paper. Our proposed MoK layer with N experts can be simple formed as:\n$\u00d7_{l+1} = \\sum_{i=1}^{N}G(x_l)_iK_i(x_l)$,\n(4)"}, {"title": "4.2 Reversible Mixture of KAN Experts Model", "content": "We can get a sophisticated KAN-based model by stacking multiple KANs or replacing the linear layers of the existing models with KANs. However, we try to design a simple KAN-based model that is easy to analyze while achieving comparable performance to the most state-of-the-art time series forecasting methods.\nInspired by several successful single-layer methods Li et al. [2023], Zeng et al. [2023], we propose a simple, effective and interpretable KAN-based model, Reversible Mixture of KAN Experts Network (RMoK), which uses RevIN Kim et al. [2022] and single MoK layer, as shown in Figure 2. First, RevIN+ (the normalization operation of RevIN) uses a learnable affine transformation to normalize the input time series of each variable. Then, the MoK layer obtains the prediction results based on the normalized time series features. Finally, the prediction results are denormalized to the original distribution space using the same affine transformation parameters in the first step by RevIN\u00af (the denormalization operation of RevIN).\nDuring the training stage, the gating network has a tendency to reach a winner-take-all state where it always gives large weights for the same few experts. Following the previous work Shazeer et al. [2017], we add an load balancing loss function to encourage experts have equal importance. First, we count the weight of experts as loads, and calcuate the square of the coefficient of variation of the load values as additional loss:\n$L_{load-balancing} = CV(loads)^2$.\n(8)\nAnd the total loss function is the sum of the prediction loss and load balancing loss with weight \u03c9\u03b9:\n$L = MSE(Y, \\hat{Y}) + w_\\iota \\cdot L_{load-balancing}$.\n(9)"}, {"title": "5 Experiments", "content": "In this section, we experimentally validate the effectiveness of our proposed KAN-based models, RMOK, on various time series forecasting benchmarks. Specifically, we conduct extensive experi-ments, including performance comparison, running speed comparison, the impact of integrating the KAN and MoK layers into other network structures (Transformer), and the interpretability of RMoK."}, {"title": "5.1 Experimental Settings", "content": "Dataset. We conduct extensive experiments on seven widely-used real-world datasets, including ETT(h1, h2, m1, m2) Zhou et al. [2021], ECL, Traffic and Weather Lai et al. [2018], whose statistical information is shown in Table 1. We follow the same data processing operations used in TimesNet Wu et al. [2023], where the training, validation, and testing sets are divided according to chronological order.\nEvaluation Metrics Following previous works Zhou et al. [2021], Wu et al. [2023], we use Mean Squared Error (MSE) and Mean Absolute Error (MAE) to evaluate the performance of time series forecasting.\nBaselines We select 6 well-acknowledged forecasting models as our baselines, including (1) Transformer-based methods: PatchTST Nie et al. [2023] and FEDformer Zhou et al. [2022]; (2) CNN-based methods: TimesNet Wu et al. [2023] and SCINet Liu et al. [2022b]; (3) Linear-based methods: RLinear Li et al. [2023] and DLinear Zeng et al. [2023]."}, {"title": "5.2 Can KAN-based Models get SOTA Performance?", "content": "In this section, we conduct extensive experiments to compare the forecasting performance of KAN-based models with advanced baselines.\nWe use two versions of our proposed RMOK: RMoK-S (the small version, which has four experts), and RMOK-B (the base version, which has eight experts). We compare RMoKs with six popular Transformer-based, CNN-based and Linear-based baselines on seven benchmarks. The experimental results are shown in Table 2, where both the best results of KAN-based models and baselines are highlighted in bold and the best results of all models are marked in red. The results of baselines come from published papers Wu et al. [2023], and the results of RMoKs are average of four times experiments with fixed seed in [0, 1, 2, 3]. Surprisingly, RMOK achieves the best results in most cases. Considering that RMoK is a simple single-layer method that does not model correlations among variates, this empirical finding adequately demonstrates that KAN-based models are effective in the TSF task.\nSpecifically, we can divide these seven datasets into two groups based on the number of variables. On the four ETT subsets with fewer variables, RMoK outperforms the baselines most, and RMOK-S and RMOK-B exhibit their own strengths under different forecasting length P. On the Weather, ECL, and Traffic datasets with more variables, RMoK-B is significantly better than RMoK-S, indicating that the mixture of experts approach is suitable for dealing with a large number of variables. In addition, due to the intricate spatiotemporal correlation among various variables in Traffic dataset, the Transformer-based method PatchTST achieves the best results, whereas our proposed simple RMOK achieves suboptimal results that far exceeded other baselines."}, {"title": "5.3 Does KAN Outperform Linear?", "content": "In this section, we conduct ablation experiments to compare KAN-based models with Linear-based models in three time series forecasting datasets.\nFor fair comparison, we replace the four KAN experts in RMOK-S with four Linear experts to obtain a Linear-based baseline with the mixture of expert structure, naming RMoL-S. And we replace the entire MoK layer with a single KAN or Linear to obtain RWavKAN, RTaylorKAN and RLinear to analyze the performance of KAN variants and Linear on the TSF task. The experimental results are reported in Table 3, where all the results are average of 4 times experiments and the best results are highlighted in bold. We can conclude three useful empirical experiences. First, the KAN-based model outperforms the linear-based model in most cases. We speculate this phenomenon is due to KAN's function representation idea, which is more efficient to capture the periodicity and trend in time series. Second, the mixture of expert structures is applicable to both KAN and Linear, which should be attributed to the fact that the gating network assigns variables to specific experts. Third, the performance of KAN-based models is affected by the specific function, which may be related to the intrinsic distribution of the time series."}, {"title": "5.4 Can KAN be Integrated into Other Methods?", "content": "In this section, we verify whether KAN can be integrated into the existing time series forecasting models as a plug-in to improve performance. We select iTransformer Liu et al. [2024b] as baseline and replace the linear projections of all the attention modules with different KAN variants and MoK layer. The experiments are conducted on the ETT datasets. For fair comparison, we set the same model hyperparameters in all experiments where hidden dim is 512 and layer number is 2, and search the best learning rate from 1e-2 to 1e-5 through a grid searching strategy, and repeat four times to report the average results in Table 4. While iTransformer with various KANs performs well only in ETTm1, iTransformer with MoK achieves the best performance in most cases on both ETTh1 and ETTm1. These experimental findings demonstrate that MoK is a successful form for integrating KAN into Transformer-based methods."}, {"title": "5.5 Are KAN-based Models Efficiency?", "content": "We report the size of model parameters, the training and infering speed of KAN-based methods and baselines on ETTh1 dataset with input length 96 and prediction length 720. We implement all methods in a unify code library with PyTorch Paszke et al. [2019], and the testing platform is a GPU server with NVIDIA A100 80GB GPUs. We set training batch size to 64, infer batch size to 1. The KAN, WKAN, TKAN, JAKN represent KANs with B-splines functions, Wavelet function (transform is mexican hat), Taylor polynomials (order=3), and Jacob polynomials (d is degree which sets to 4 or 6). As shown in Table 5, the running speed of KANs are affected by the specific implementation. KAN variant with Taylor polynomials can achieve close running efficiency to the Linear. With future hardware optimization, the efficiency of KANs will still be improved."}, {"title": "5.6 Are KAN-based Models Interpretable?", "content": "In this section, we aim to analyze the interpretability of RMoK for the time series forecasting task. First, we generate a heatmap to visualize the outputs of the gating network, which decide variable-expert assignment. This visualization demonstrates how the RMOK simplifies the multivariate time series forecasting task into multiple univariate forecasting subtasks. Then, we analyze what knowledge RMoK learns from real-world time-varying systems in each subtask.\nOur proposed RMoK model uses a single MoK layer, which consists of gating network and KAN experts. The gating network outputs the matching score between variables and experts based on the input feature, and the top-k matching experts are selected for each variable to predict the future state. We train an RMOK-S model with 4 experts on Weather dataset with 21 variables, then count the top-1 scores of all samples in the test set and generate a heatmap in Figure 3, where (x, y) = 1.0 means the y-th variable matches the x-th expert in all test samples. From this figure, although the matching score is related to the time-varying input data, the variable is still closely related to the specific expert. Therefore, we roughly approximate that the trained RMoK is a linear combination of several KANs, and RMoK treats the multivariate forecasting task as multiple univariate forecasting tasks. Although the above process is crude, this simplification helps us analyze the explainability of RMOK in complex real-world scenarios.\nAfter simplification, we try to analyze why KAN is effective in time series forecasting from the perspective of univariate and single-expert subtask. We use the temperature variable from Weather dataset which collects data every 10 minutes to train the KAN with B-splines function as forecasting model. We decompose and visualize data according to the trend, season, and resid terms through statsmodels \u00b9 library. As shown in Figure 4, the temperature series has obvious daily periodicity, which is consistent with life experience. Then, we input the complete daily period (the past 144 time steps) data to predict the state of the next time step. We visualize the weight of each feature dimension of the trained RMoK. As shown in the Figure 4b, there are three peaks in the feature weight, which are near 0, 72 and 144 time steps. O represents the temperature at the same time step of the previous day's period, 144 represents the temperature at the adjacent moment, and 70 represents half a period. These three weight peaks correspond to the three zero points of the cosine function in one period. Finally, we conclude that RMoK can learn the periodicity of time series, which can preliminary explain its effectiveness in time series prediction."}, {"title": "6 Conclusion", "content": "This work discusses the effectiveness of KANs in time series forecasting. Due to various variants with different spline functions of KAN, we propose a single-layer mixture of KAN experts model (RMOK) to alleviate the distribution variation in time series. We experimentally compared KAN with existing baseline methods of various network architectures in terms of performance, integration, efficiency, and interpretability. Experimental results on seven real-world datasets show that RMoK performs in most metrics, it is sufficient to conclude that KAN and KAN-based models are effective in the time series forecasting task, and KAN can gain a place in the increasingly fierce model structure competition. The single-layer KAN-based model proposed in this work is only a preliminary attempt to introduce KAN into time series forecasting. We hope that our work can help future studies improve the performance and interpretability of KAN-based models."}, {"title": "A Implementation Details", "content": ""}, {"title": "A.1 Datasets", "content": "We conduct extensive experiments on seven real-world datasets, including ETT(h1, h2, m1, m2) Zhou et al. [2021], ECL, Traffic and Weather Lai et al. [2018], whose detail information is shown in Table 6. We follow the same data processing operations used in TimesNet Wu et al. [2023], where the training, validation, and testing sets are divided according to chronological order. The datasets we used cover multiple domains (electricity, transportation, and weather), which are sufficient to verify the generalizability of our method."}, {"title": "A.2 Baselines", "content": "To comprehensively compare KAN with other baselines under different network structures, we select six well-known forecasting models as our baselines, including (1) Transformer-based methods: PatchTST and FEDformer; (2) CNN-based methods: TimesNet and SCINet; (3) Linear-based methods: RLinear and DLinear. We report the details of these baselines in Table 7."}, {"title": "A.3 Experimental Settings", "content": "We implement our method and baselines in a unify code library with PyTorch 2 and Pytorch-lightning 3 (codes will be released after acceptance), and we conduct all experiments on a GPU server with NVIDIA A100 80GB GPUs. To prevent the sample drop in the test phase where the samples of the last batch are discarded, we set the test batch size to 1. Following previous works Zhou et al. [2021], Wu et al. [2023], we use Mean Squared Error (MSE) and Mean Absolute Error (MAE) to evaluate the performance of time series forecasting.\n$\\begin{aligned}\\text { M A E }(\\mathbf{x}, \\hat{\\mathbf{x}}) & =\\frac{1}{N} \\sum_{i=1}^{N}\\left|x_{i}-\\hat{x}_{i}\\right| ;\\\\ \\text { M S E }(\\mathbf{x}, \\hat{\\mathbf{x}}) & =\\frac{1}{N} \\sum_{i=1}^{N}\\left(x_{i}-\\hat{x}_{i}\\right)^{2} ;\\end{aligned}$\n(10)\nwhere xi denotes the i-th ground truth, \u00c2\u1d62 represents the i-th predicted values, and N represents the number of testing samples."}, {"title": "B Experimental Analysis", "content": "We report hyperparameter sensitivity and performance robustness experimental results to further evaluate the effectiveness of KAN-based modes in time series forecasting."}, {"title": "B.1 Hyperparameter Sensitivity", "content": "We conduct hyperparameter experiments to compare the performance between MoK and KAN layers. Results are shown in Table 8, where WavKAN, TaylorKAN and JacobKAN are KANs with wavelet functions, Taylor polynomials, and Jacobi polynomials. The type of spline function affects KAN's ability to model time series, but there is currently a lack of theoretical guidance on how to choose the best suitable spline function, which can only be determined through experimental methods. Thus, MoK becomes a best practice for KAN, which does not require a lot of experiments to select the optimal spline function. This cocktail-like solution achieves better performance than a single KAN layer in most cases and achieves overall performance improvements on datasets with a large number of variables (such as Weather, Traffic, ECL)."}]}