{"title": "How Much Do LLMs Hallucinate across Languages? On Multilingual Estimation of LLM Hallucination in the Wild", "authors": ["Saad Obaid ul Islam", "Anne Lauscher", "Goran Glava\u0161"], "abstract": "In the age of misinformation, hallucination\u2014 the tendency of Large Language Models (LLMs) to generate non-factual or unfaithful responses-represents the main risk for their global utility. Despite LLMs becoming increasingly multilingual, the vast majority of research on detecting and quantifying LLM hallucination are (a) English-centric and (b) focus on machine translation (MT) and summarization, tasks that are less common \"in the wild\" than open information seeking. In contrast, we aim to quantify the extent of LLM hallucination across languages in knowledge-intensive long-form question answering. To this end, we train a multilingual hallucination detection model and conduct a large-scale study across 30 languages and 6 open-source LLM families. We start from an English hallucination detection dataset and rely on MT to generate (noisy) training data in other languages. We also manually annotate gold data for five high-resource languages; we then demonstrate, for these languages, that the estimates of hallucination rates are similar between silver (LLM-generated) and gold test sets, validating the use of silver data for estimating hallucination rates for other languages. For the final rates estimation, we build a knowledge-intensive QA dataset for 30 languages with LLM-generated prompts and Wikipedia articles as references. We find that, while LLMs generate longer responses with more hallucinated tokens for higher-resource languages, there is no correlation between length-normalized hallucination rates of languages and their digital representation. Further, we find that smaller LLMs exhibit larger hallucination rates than larger models.", "sections": [{"title": "1 Introduction", "content": "Generalizing seamlessly to (seemingly) arbitrary language understanding, reasoning, and generation tasks, Large Language Models (LLMs) (Kojima et al., 2022; Dubey et al., 2024; Aryabumi et al.,"}, {"title": "2 Background and Related Work", "content": "We provide a brief overview of the body of related work on (1) hallucination detection models and (2) benchmarks for evaluating LLM hallucination.\nHallucination Detection. Coarsely, LLM halluci- nations fall into two categories. Intrinsic halluci- nation are content contradicts some reference in- formation source. The reference may be explicitly given to the LLM as part of the task (e.g., the text to be summarized in summarization or source lan- guage text in machine translation) or it may implicit (e.g., general world knowledge in question answer- ing). In contrast, extrinsic hallucination refers to content that does not contradict the reference but is unnecessary or superfluous with respect to the task (e.g., additional facts in fact-based question answering) (Ji et al., 2023). Recent work intro- duced finer-grained taxonomies for both categories. For example, Mishra et al. (2024) distinguish be- tween several types of intrinsic hallucinations (e.g., entity-based hallucinations or relation-based hal- lucinations). In a similar vein, extrinsic halluci- nations are split into subtypes such as invented,"}, {"title": "3 Hallucination Detection", "content": "We first describe how we obtained multilingual hallucination detection datasets (\u00a73.1) and then re- port on training and evaluation of a multilingual hallucination detection model (\u00a73.2)."}, {"title": "3.1 MFAVA Benchmark", "content": "Evaluation Datasets. We start from the English FAVA (Mishra et al., 2024) dataset and its respec- tive set of fine-grained hallucination types. FAVA'S evaluation portions were created by (1) eliciting information-seeking prompts (i.e., questions) from various sources,2 (2) generating responses with three LLMs and (3) having human annotators label hallucinated spans. We follow a similar protocol to create evaluation datasets for 30 languages.3 We start from 300 information-seeking prompts, 150 from evaluation portion of FAVA and 150 from the Natural Questions dataset (Kwiatkowski et al., 2019). We then ask GPT-4 (Achiam et al., 2023) to (1) first create answer passages in a target lan- guage and then to (2) explicitly introduce the hallu- cinations of the fine-grained FAVA types into the answer. We refer to these synthetically labeled hal- lucination evaluation datasets, comparable across the 30 target languages, as MFAVA-Silver.\nFor five linguistically diverse high-resource languages-Arabic, Chinese, German, Russian, and Turkish we also collect human hallucina- tion annotations. To this end, we provide to the"}, {"title": "3.2 Multilingual Hallucination Detection", "content": "Models. Using the translations of ca. 30K FAVA training instances in our 30 target languages, we train the following models: (1) MONO denotes monolingual models trained on data of one lan- guage (and evaluated for the same language on the respective MFAVA portion), i.e., we train 30 MONO models, one for each of our target lan- guages; (2) MULTI refers to a single multilingual model trained on concatenated training data of all 30 languages. We train the Mono models and the Multi model for both tasks, Binary and Category. We follow the recent body of work that successfully converts generative decoder LLMs into encoders for discriminative tasks (Li et al., 2023b; Duki\u0107 and \u0160najder, 2024; BehnamGhader et al., 2024; Schmidt et al., 2024) and fine-tune Llama-3-8B- base (Dubey et al., 2024) by removing future-token masking, i.e., allowing for bidirectional contextu- alization (Bidirect). For comparison, for the Multi model, we also fine-tune the decoder as-is, using"}, {"title": "4 Estimating Hallucination in the Wild", "content": "We next propose a protocol for estimating hallu- cination rates of LLMs (for a wide range of lan- guages) in the wild, based (1) on the number of hallucinated tokens detected by a hallucination de- tection (HD) model in the wild and (2) estimates of HD model's performance (precision and recall)."}, {"title": "4.1 From Model Performance to Hallucination Rates Estimates", "content": "Estimating Hallucination Rates in the Wild. Let Pi and R\u2081 be the estimates of token-level precision and recall of a HD model for some language l and let Hdet,l be the number of hallucination tokens that the HD model detected (i.e., predicted) on some corpus C of LLM generations in language l, which serves as an approximation of the LLM outputs in the wild. We then posit that the estimate of the true hallucination rate of the LLM in the wild for language l, HRest,l, is given as follows:\n$H_{Rest,l} = \\frac{P_1 H_{det,1}}{R_i N_i} \\times 100(\\%)$ (1)\nwhere N\u2081 is the total number of tokens in Cl, i.e., the total number of tokens generated by the LLM across answers to all user prompts. Intuitively, mul- tiplying the number of model's detections Hdet,l with its estimated precision P\u012b discounts Hdet,l by the number of tokens falsely detected as halluci- nated by the model-while we do not know ex- actly which token predictions are false positives, the expected rate of false positives is, by defini- tion, exactly captured by P\u2081. Analogously, dividing Hdet, with Ri accounts for the tokens that are hal- lucinated, but will (falsely) not be detected by the model and R\u2081 is exactly the estimate of the rate of such false negatives. We divide the estimate of the absolute number of truly hallucinated tokens (i.e., Pi Hdet,1/R\u2081) with N\u2081, making HRest, a rela- tive measure, that is, a rate (i.e., proportion) of all generated tokens that are hallucinated (multiplied by 100 and expressed as %). We provide a more detailed explanation/justification of Eq. (1) in \u00a7A.4.\nEstimation Dataset. We next create corpora Ci (one corpus for each of our 30 target languages) of free-text LLM answers to knowledge-intensive queries, as approximations of the LLM usage in the wild. We start by randomly selecting articles from the language-specific Wikipedia, to serve as ground"}, {"title": "4.2 Final Estimates", "content": "Figure 4 shows our in-the-wild hallucination rate estimates HRest,1 for all 30 MFAVA languages and 11 LLMs. The average rate across all languages varies between 7% and 12%, with both Gemma models offering the lowest rates. Smaller Qwen-2.5"}, {"title": "5 Conclusion", "content": "We presented the first effort towards understanding how much multilingual LLMs hallucinate \u201cin the wild\". To this end, we proposed a novel framework for hallucination rate estimation, which adjusts the number of detected hallucinations based on the detector's performance resulting in more reliable rate estimates. We trained a series of multilingual detection models, and measured their precision and recall scores on our newly created MFAVA datasets across 30 languages. To estimate halluci- nations, we build a novel synthetic open-domain knowledge-intensive QA dataset for which we col- lected answers from eleven open-source LLMs. Our findings indicate that smaller models and mod- els that cover more languages hallucinate signifi- cantly more, and that model response-length does not correlate with hallucination rate."}, {"title": "Limitations", "content": "We acknowledge the following limitations of our work: (1) we adopted the common translate-train approach and thus used MT to translate the original FAVA to our 30 target languages. While one may argue that we thus add some noise to the training process resulting in unreliable detectors, recall that we are not opting for the highest possible detection performances, but rather interested in obtaining reli- able performance estimates. (2) We only have gold annotations for 5 languages. Here, one might argue that, thus, our performance estimates might be un- reliable. This is why in \u00a74.1, we compare estimates obtained on MFAVA-Silver with ones obtained on MFAVA-Gold and show that silver annotations can serve as a reliable proxy. (3) For our hallucination evaluation, we only manually check a subset of the Arabic, Chinese, German, Russian, and Turkish queries to ensure that the answers to the synthetic prompts are present in the Wikipedia references. The high rate of 98% we observed makes us confi- dent that the potential error we introduce via such \"non-grounded\u201d questions for other languages is negligible. We still acknowledge, however, that the Wikipedia articles we use might be limited in terms of the knowledge they cover (Kim et al., 2024), this is why we carefully filter via minimum length and collaborative Wikipedia depth towards higher- quality articles with high coverage. (5) Finally, we deliberately limited the scope of this work to assessing factual correctness and we do not cover factual coverage. We decided to do so as quan- tifying hallucinations in long-form generation is already difficult for English Xu et al. (2023); Min et al. (2023); Wei et al. (2024b) and more so in non-english languages (Kim et al., 2024), and cur- rently, resources for assessing multilingual factual coverage are still lacking."}, {"title": "A Appendix", "content": "A.1 Choice of languages\nInitially, we wanted to cover all 14 language fami- lies based on Glottolog 5.0 (Nordhoff and Ham- marstr\u00f6m, 2012)), however, as we progressed through the languages, we found that even the best closed-source LLMs like GPT-4 and Gemini are bad at generating text in low-resource languages (e.g. Amharic, Aymara, Hausa and Tamil) and we could not employ LLMs to generate and annotate a silver hallucination detection dataset in these lan- guages. See Table 7 for 30 languages.\nA.2 Annotation Process\nCost of Silver Annotations The total cost for generating silver data for 30 languages using GPT-4 was ~$2,310 with ~$77 per language. Distribution of categories across 30 languages is provided in Table 4 and and per language label distribution is provided in Figure 6."}, {"title": "A.3 Training Details", "content": "All the classifiers were trained utilizing the Bi- LLM (Li et al., 2023b) and transformers (Wolf, 2019) library. The models were trained with three seeds (42, 47, 49) on 4xH100 until convergence. Seeds are set for torch.manual_seed() and ran- dom.seed(). The exact hyper-parameters are given in the Table 5. Total GPU hours: 1134."}, {"title": "A.4 Adjusting for P\u2081 and Ri", "content": "The hallucination rate HRest,l for a given language l, is defined as the ratio of hallucinated tokens de- tected by the model (Hdetected,l) to the total number of generated tokens (N\u2081):\n$HRI \\frac{H_{det, l}}{\u039d_{l}}$ (2)\nTo refine this rate, we adjust for the detection model's precision (P1) and recall (R\u2081). Precision is defined as:\n$P = \\frac{TP1}{TP1 + FP1}$ (3)\nwhere (TP\u2081) FP\u2081 denote true and false positives respectively. Rearranging this equation gives the number of true positives:\n$TP1 = Pi HR_{det, l}$ (4)\nRecall is defined as:\n$R = \\frac{TP1}{TP1 + FN1}$ (5)\nwhere FN denotes false negatives. The total number of corrected hallucinations (HRest,1) can thus be expressed as:\n$HRest,1 = TP1 + FN1 = \\frac{TP1}{Ri}$ (6)\nSubstituting Equations 4 in 6, we derive the Hest,l as:\n$H_{est, l} = \\frac{Pi Hdet.l}{Ri}$ (7)"}, {"title": "A.5 Hallucination Evaluation Dataset", "content": "To construct the hallucination evaluation dataset, we aimed to scrap ~ 1000 articles per language with more than 2000 characters. However, problem with non-English languages (especially moderate- low resource) is that \u2265 2000 character articles can be scarce. Furthermore, sometimes Wikipedia has articles tagged as Unreferenced, Failed Verification, or Under Construction which flag the article as unfinished or not factually verified. Such tags are very prominent in languages other than English and we do not include such articles in our dataset.\nWe use Wikipedia article summary (text before the first heading) as references and prompt gpt- 4 to generate 2 knowledge-intensive queries per article. Sometimes, it generated only one query even though we explicitly state to generate two queries. We did not prompt the GPT-4 again to generate the second query due to budget constraints. The total cost to generate prompts for 31 languages is $192. Per language statistics can be found in Table 9. We will release hallucination evaluation data under an open scientific licensing."}, {"title": "A.6 Response Collection for Hallucination Evaluation Dataset", "content": "We collect LLM responses on 5 seeds: 42, 43, 44, 47, 49 for 6 LLM model. The generation configurations that we used are provided in the generation_config.json in model repositories on huggingface. Seeds are set for torch.manual_seed() and random.seed()."}, {"title": "A.7 Manual Analysis", "content": "To further check the quality and informativeness (See \u00a7A.8 for definitions of informativeness) of the responses, we manually analyze 60 responses from Aya-23-8B for German and Arabic, two languages for which we have gold annotations. Overall, 10% of the responses had repetitive words and sentences and 5% of the responses were I don't know re- sponses. For the remainder of the samples, the responses were fluent and long and were relevant to the input prompt."}, {"title": "A.8 Informativeness", "content": "Currently, there is no agreed-upon definition of informativeness. Lin et al. (2022) considers a re- sponse to be informative if it is potentially relevant to the question and Wei et al. (2024b) considers a response to be informative if it has a certain number of supporting facts from the reference text."}]}