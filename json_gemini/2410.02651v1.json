{"title": "CAX: CELLULAR AUTOMATA ACCELERATED IN JAX", "authors": ["Maxence Faldor", "Antoine Cully"], "abstract": "Cellular automata have become a cornerstone for investigating emergence and self-organization across diverse scientific disciplines, spanning neuroscience, artificial life, and theoretical physics. However, the absence of a hardware-accelerated cellular automata library limits the exploration of new research directions, hinders collaboration, and impedes reproducibility. In this work, we introduce CAX (Cellular Automata Accelerated in JAX), a high-performance and flexible open-source library designed to accelerate cellular automata research. CAX offers cutting-edge performance and a modular design through a user-friendly interface, and can support both discrete and continuous cellular automata with any number of dimensions. We demonstrate CAX's performance and flexibility through a wide range of benchmarks and applications. From classic models like elementary cellular automata and Conway's Game of Life to advanced applications such as growing neural cellular automata and self-classifying MNIST digits, CAX speeds up simulations up to 2,000 times faster. Furthermore, we demonstrate CAX's potential to accelerate research by presenting a collection of three novel cellular automata experiments, each implemented in just a few lines of code thanks to the library's modular architecture. Notably, we show that a simple one-dimensional cellular automaton can outperform GPT-4 on the 1D-ARC challenge.", "sections": [{"title": "INTRODUCTION", "content": "Emergence is a fundamental concept that has captivated thinkers across various fields of human inquiry, including philosophy, science and art (Holland, 2000). This fascinating phenomenon occurs when a complex entity exhibits properties that its constituent parts do not possess individually. From the collective intelligence of ant colonies to the formation of snowflakes, self-organization and emergence manifest in myriad ways. The study of self-organization and emergence holds the promise to unravel deep mysteries, from the origin of life to the development of conciousness.\nCellular automata (CA) are models of computation that exemplify how complex patterns and sophisticated behaviors can arise from simple components interacting through basic rules. Originating from the work of Ulam and von Neumann in the 1940s (Neumann & Burks, 1966), these systems gained prominence with Conway's Game of Life in the 1970s (Gardner, 1970) and Wolfram's systematic studies in the 1980s (Wolfram, 2002). The discovery that even elementary cellular automata can be Turing-complete underscores their expressiveness (Cook, 2004). CAs serve as a powerful abstraction for investigating self-organization and emergence, offering insights into complex phenomena across scientific domains, from physics and biology to computer science and artificial life. In recent years, the integration of machine learning techniques with cellular automata has opened new avenues for research in morphogenesis (Mordvintsev et al., 2020), self-organization (Randazzo et al., 2020; 2021), and developmental processes (Najarro et al., 2022). The advent of Neural Cellular Automata (NCA) has significantly broadened the scope of CA research, yielding profound biological insights and showcasing the power of gradient-based optimization in studying emergence and self-organization. NCAs extend traditional CAs by incorporating neural networks to learn update rules, allowing for more complex and adaptive behaviors. This approach enables the modeling of sophisticated phenomena such as pattern formation in biological systems and the evolution of"}, {"title": "BACKGROUND", "content": ""}, {"title": "CELLULAR AUTOMATA", "content": "A cellular automaton is a simple model of computation consisting of a regular grid of cells, each in a particular state. The grid can be in any finite number of dimensions. For each cell, a set of cells called its neighborhood is defined relative to the specified cell. The grid is updated at discrete time steps according to a fixed rule that determines the new state of each cell based on its current state and the states of the cells in its neighborhood.\nA CA is defined by a tuple $(L, S, N, \\phi)$, where $L$ is the $d$-dimensional lattice or grid with $c$ channels, $S$ is the cell state set, $N \\subset L$ is the neighborhood of the origin, and $\\phi : S^N \\rightarrow S$ is the local rule."}, {"title": "CONTROLLABLE CELLULAR AUTOMATA", "content": "A controllable cellular automaton (CCA) is a generalization of CA that incorporates the ability to accept external inputs at each time step. CCAs formalize the concept of Goal-Guided NCA that has been introduced in the literature by Sudhakaran et al. (2022). The external inputs can modify the behavior of CCAs, offering the possibility to respond dynamically to changing conditions or control signals while maintaining the fundamental principles of cellular automata.\nA CCA is defined by a tuple $(L, S, I, N, \\phi)$, where $I$ is the input set and $\\phi : S^N \\times I^N \\rightarrow S$ is the controllable local rule. A mapping from the grid to the input set $I : L \\rightarrow I$ is called the input. $I(x)$ represents the input of a cell $x \\in L$. Similarly to the state, we denote $I(N_x) = \\{I(n), n \\in N_x\\}.\nThe controllable global rule $\\Phi : S^L \\times I^L \\rightarrow S^L$ is defined such that, for all $x$ in $L, \\Phi(S, I)(x) = \\phi(S(N_x), I(N_x))$. A controllable cellular automaton is initialized with an initial state $S_0$. Then, the state is updated according to the controllable global rule $\\Phi$ and a sequence of input $(I_t)_{t>0}$ at each discrete time step $t \\in N$, to give,\n$\\Phi(S_0, I_0) = S_1, \\Phi(S_1, I_1) = S_2, ...$\nAs discussed in Section 2.1, CAs can be conceptualized as recurrent convolutional neural networks. However, traditional CAs lack the ability to take external inputs at each time step. CCAs extend the capabilities of traditional CAs by making them responsive to external inputs, akin to recurrent neural networks processing sequential data. CCAs bridge the gap between recurrent convolutional neural networks and cellular automata, opening up new possibilities for modeling complex systems that exhibit both autonomous emergent behavior and responsiveness to external control."}, {"title": "RELATED WORK", "content": "The field of CA has spawned numerous tools and libraries to support research and experimentation, with CellPyLib (Antunes, 2021) emerging as one of the most popular and versatile options. This Python library offers a simple yet powerful interface for working with 1- and 2-dimensional CA, supporting both discrete and continuous states, making it an ideal baseline for comparative studies and further development. While it provides implementations of classic CA models like Conway's Game of Life and Wireworld, CellPyLib is not hardware-accelerated and does not support the training of neural cellular automata. Golly is a cross-platform application for exploring Conway's Game of Life and many other types of cellular automata. Golly's features include 3D CA rules, custom rule loading, and scripting via Lua or Python. While powerful and versatile for traditional CA, Golly is not designed for hardware acceleration or integration with modern machine learning frameworks.\nThe recent surge in artificial intelligence has increased the availability of computational resources, and encouraged the development of sophisticated tools such as JAX (Bradbury et al., 2018), a high-performance numerical computing library with automatic differentiation and JIT compilation. A rich ecosystem of specialized libraries has emerged around JAX, such as Flax (Heek et al., 2024) for neural networks, RLax (DeepMind et al., 2020) for reinforcement learning, and EvoSax (Lange, 2022), EvoJax (Tang et al., 2022) and QDax (Chalumeau et al., 2023) for evolutionary algorithms."}, {"title": "CAX: CELLULAR AUTOMATA ACCELERATED IN JAX", "content": "CAX is a high-performance and flexible open-source library designed to accelerate cellular automata research. In this section, we detail CAX's architecture, design and key features. At its core, CAX leverages JAX and Flax (Heek et al., 2024), capitalizing on the well-established connection between CA and recurrent convolutional neural networks. This synergy, discussed in Section 2), allows CAX to harness advancements in machine learning to accelerate CA research. CAX offers a modular and intuitive design through a user-friendly interface, supporting both discrete and continuous cellular automata across any number of dimensions. This flexibility enables researchers to seamlessly transition between different CA types and complexities within a single, unified framework (Table 1). We have made our anonymized repository available at github.com/b769eb6f/cax. We invite readers to experience CAX's capabilities firsthand by accessing our curated examples as interactive notebooks in Google Colab, conveniently linked in the repository's README."}, {"title": "ARCHITECTURE AND DESIGN", "content": "CAX introduces a unifying framework for all cellular automata types, encompassing discrete, continuous, and neural models across any number of dimensions (Table 1). This flexible architecture is built upon two key components: the perceive module and the update module. Together, these modules define the local rule of the CA. At each time step, this local rule is applied uniformly to all cells in the grid, generating the next global state of the system, as explained in Section 2.1. This modular approach not only provides a clear separation of concerns but also facilitates easy experimentation and extension of existing CA models."}, {"title": "PERCEIVE MODULE", "content": "The perceive module in CAX is responsible for gathering information from the neighborhood of each cell. This information is then used by the update module to determine the cell's next state. CAX provides several perception mechanisms, including Convolutional Perception, Depthwise Convolutional Perception and Fast Fourier Transform Perception. The perceive modules are designed to be flexible and can be customized for different types of cellular automata."}, {"title": "UPDATE MODULE", "content": "The update module in CAX is responsible for determining the next state of each cell based on its current state and the information gathered by the perceive module. CAX provides several update mechanisms, including MLP Update, Residual Update and Neural Cellular Automata Update. Like the perceive modules, the update modules are designed to be flexible and can be customized for different cellular automata models."}, {"title": "FEATURES", "content": ""}, {"title": "PERFORMANCE", "content": "CAX leverages JAX's powerful vectorization and scan capabilities to achieve remarkable speed improvements over existing implementations. Our benchmarks, conducted on a single NVIDIA RTX A6000 GPU, demonstrate significant performance gains across various cellular automata models. For Elementary Cellular Automata, CAX achieves a 1,400x speed-up compared to CellPyLib. In simulations of Conway's Game of Life, a 2,000x speed-up is observed relative to CellPyLib.\nFurthermore, in the domain of Neural Cellular Automata, specifically the Self-classifying MNIST Digits experiment, CAX demonstrates a 1.5x speed-up over the official TensorFlow implementation. These performance improvements, illustrated in Figure 3, are made possible by JAX's efficient vectorization and the use of its scan operation for iterative computations. The following code snippet exemplifies how CAX utilizes JAX's scan function to optimize multiple CA steps:\ndef step (carry: tuple [CA, State), input: Input | None) -> tuple [tuple [CA,\nState], State]:\nca, state = carry\nstate = ca.step(state, input)\nreturn (ca, state), state if all_steps else None\n(_, state), states = nnx.scan(\n step,\n in_axes=(nnx.Carry, input_in_axis),\n length=num_steps,\n)((self, state), input)\nThis optimized approach allows for rapid execution of complex CA simulations, opening new possibilities for large-scale experiments and real-time applications."}, {"title": "UTILITIES", "content": "CAX offers a rich set of utility functions to support various aspects of cellular automata research. A high-quality implementation of the sampling pool technique is provided, which is crucial for training stable growing neural cellular automata Mordvintsev et al. (2020). To facilitate the training of unsupervised neural cellular automata and enable generative modeling within the CA framework, CAX incorporates a variational autoencoder implementation. Additionally, the library provides utilities for handling image and emoji inputs, allowing for diverse and visually engaging CA experiments. These utilities are designed to streamline common tasks in CA research, allowing researchers to focus on their specific experiments rather than reimplementing standard components."}, {"title": "DOCUMENTATION AND EXAMPLES", "content": "CAX prioritizes user experience and ease of adoption through comprehensive documentation and examples. The entire library is thoroughly documented, with typed classes and functions accompanied by descriptive docstrings. This ensures users have access to detailed information about CAX's functionality and promotes clear, type-safe code. To help users get started and showcase advanced usage, CAX offers a collection of tutorial-style interactive Colab notebooks. These notebooks demonstrate various applications of the library and can be run directly in a web browser without any prior setup, making it easy for new users to explore CAX's capabilities.\nFor easy access and integration into existing projects, CAX can be installed directly via PyPI, allowing users to quickly incorporate it into their Python environments. The library maintains high standards of code quality, with extensive unit tests covering a significant portion of the codebase. Continuous Integration (CI) pipelines ensure that all code changes are thoroughly tested and linted before integration. These features collectively make CAX not just a powerful tool for cellular automata research, but also an accessible and user-friendly library suitable for both novice and experienced researchers in the field."}, {"title": "IMPLEMENTED CELLULAR AUTOMATA AND EXPERIMENTS", "content": "To showcase the versatility and capabilities of the library, we show that CAX supports a wide array of cellular automata, ranging from classical discrete models to advanced continuous CAs and including neural implementations. In this section, we provide an overview of these implementations, demonstrating the library's flexibility in handling various dimensions and types (Table 1).\nWe begin with three classic models that highlight CAX's ability to support both discrete and continuous systems across different dimensions. The Elementary CA, a foundational one-dimensional discrete model studied extensively by Wolfram (2002), demonstrates CAX's efficiency in handling simple discrete systems. Conway's Game of Life (Gardner, 1970), a well-known two-dimensional model, showcases CAX's capability in simulating complex emergent behaviors in discrete space. Lenia (Chan, 2019), a continuous, multi-dimensional model, illustrates CAX's flexibility in supporting more complex, continuous systems in arbitrary dimensions.\nFurthermore, we have replicated four prominent NCA experiments that have gained significant attention in the field. The Growing NCA (Mordvintsev et al., 2020) demonstrates CAX's ability to handle complex growing patterns and showcases the implementation of the sampling pool technique, crucial for stable growth and regeneration. The Growing Conditional NCA (Sudhakaran et al., 2022) utilizes CAX's Controllable CA capabilities, as introduced in Section 2.2 allowing for targeted pattern generation. The Growing Unsupervised NCA (Palm et al., 2021) highlights CAX's versatility in incorporating advanced machine learning techniques, specifically the use of a Variational Autoencoder within the NCA framework. The Self-classifying MNIST Digits (Randazzo et al., 2020) showcases CAX's capacity for self-organizing systems with global coordination via local interactions, contrasting with growth-based tasks.\nThese implementations not only validate CAX's performance and flexibility but also serve as valuable resources for researchers looking to build upon or extend these models. We complement these implementations with three novel experiments, which will be detailed in the following section."}, {"title": "NOVEL NEURAL CELLULAR AUTOMATA EXPERIMENTS", "content": ""}, {"title": "DIFFUSING NEURAL CELLULAR AUTOMATA", "content": "In this experiment, we introduce a novel training procedure for NCA, inspired by diffusion models. Traditionally, NCAs have predominantly relied on growth-based training paradigms, where the state is initialized with a single alive cell and trained to grow towards a target pattern (Sudhakaran et al., 2022; Mordvintsev et al., 2020; Palm et al., 2021). However, this approach often faces challenges in maintaining stability and achieving consistent results (Mordvintsev et al., 2020). The conventional"}, {"title": "SELF-AUTOENCODING MNIST DIGITS", "content": "In this experiment, we draw inspiration from Randazzo et al. (2020) where a NCA is trained to classify MNIST digits through local interactions. In their work, each cell (pixel) of an MNIST digit learns to output the correct digit label through local communication with neighboring cells. The NCA demonstrates the ability to reach global consensus on digit classification, maintain this classification over time, and adapt to perturbations or mutations of the digit shape. Their model showcases emergent behavior, where simple local rules lead to complex global patterns, analogous to biological systems achieving anatomical homeostasis.\nBuilding upon this concept, we propose a novel experiment that could be termed \u201cSelf-autoencoding MNIST Digits\". In this setup, we utilize a three-dimensional NCA initialized with an MNIST digit on one face, see Figure 6. The objective of the NCA is to learn a rule that will replicate the MNIST digit on its opposite face (red face). However, we introduce a critical constraint: in the middle of the NCA, there is a mask where cells cannot be updated, effectively preventing direct communication between the two faces. Crucially, we allow for a single-cell wide hole in the center of this mask, creating a minimal channel for information transfer.\nTo successfully replicate the MNIST digit on the opposite face, the NCA must develop a sophisticated rule set that accomplishes two key tasks. First, it must encode the MNIST image into a compressed form that can pass through the single-cell hole. Second, it must then decode this information on the other side to accurately reconstruct the original digit. A notable aspect of this result is that each cell in the NCA performs an identical local update rule, contributing to the system's overall emergent behavior. As shown in Figure 7, the NCA successfully reconstructs MNIST digits on"}, {"title": "1D-ARC NEURAL CELLULAR AUTOMATA", "content": "In this experiment, we train a one-dimensional NCA on the 1D-ARC dataset (Xu et al., 2024). The 1D-ARC dataset is a novel adaptation of the original Abstraction and Reasoning Corpus (Chollet, 2019) (ARC), designed to simplify and streamline research in artificial intelligence and language models. By reducing the dimensionality of input and output images to a single row of pixels, 1D-ARC maintains the core knowledge priors of ARC while significantly reducing task complexity. For example, the tasks in 1D-ARC include \"Static movement by 3 pixels\", \"Fill\", and \"Recolor by Size Comparison\". For a full description of the dataset, see the project page. Our experiment focuses on training an NCA to solve the 1D-ARC tasks. Each input sample consists of a single row of colored pixels and a corresponding target row. The NCA's objective is to transform the input into the target through successive applications of its rule. We consider a task successful if all pixels in the NCA's output match the target pixels after a predetermined fixed number of steps."}, {"title": "CONCLUSION", "content": "In this paper, we introduce CAX: Cellular Automata Accelerated in JAX, an open-source library, designed to provide a high-performance and flexible framework to accelerate cellular automata research. CAX provides substantial speed improvements over existing implementations, enabling researchers to run complex simulations and experiments more efficiently.\nCAX's flexible architecture supports a wide range of cellular automata types across multiple dimensions, from classic discrete models to advanced continuous and neural variants. Its modular design, based on customizable perceive and update components, facilitates rapid experimentation and development of novel CA models, enabling efficient exploration of new ideas.\nCAX's comprehensive documentation, example notebooks, and seamless integration with machine learning workflows not only lower the barrier to entry but also promote reproducibility and collaboration in cellular automata research. We hope this accessibility will accelerate the pace of discovery by attracting new researchers.\nIn the future, we envision several exciting directions, such as expanding the model zoo to implement and optimize a wider range of cellular automata models, and exploring synergies between cellular automata and other approaches, such as reinforcement learning or evolutionary algorithms."}, {"title": "HYPERPARAMETERS", "content": "In this section, we detail the hyperparameters for the three novel neural cellular automata experiments presented in Section 5."}, {"title": "EXAMPLE NOTEBOOK", "content": "## Import\nimport jax\nimport jax.numpy as jnp\nimport mediapy\nimport optax\nfrom cax.core.ca import CA\nfrom cax.core.perceive.depthwise_conv_perceive import\nDepthwiseConvPerceive\nfrom cax.core.perceive.kernels import grad_kernel, identity_kernel\nfrom cax.core.state import state_from_rgba_to_rgb, state_to_rgba\nfrom cax.core.update.nca_update import NCAUpdate\nfrom cax.nn.pool import Pool\nfrom cax.utils.image import get_emoji\nfrom flax import nnx\nfrom tqdm.auto import tqdm\n## Configuration\nseed = 0\nchannel_size = 16\nnum_kernels = 3\nhidden_size = 128\ncell_dropout_rate = 0.5\npool_size = 1_024\nbatch_size = 8\nnum_steps\n= 128\nlearning_rate = 2e-3\nemoji = \"gecko\"\ntarget_size = 40\ntarget_padding = 16\nkey = jax.random.key (seed)\nrngs = nnx.Rngs (seed)\n## Dataset\ntarget = get_emoji (emoji, size=target_size, padding=target_padding)\n## Init state\ndef init_state():\nstate_shape = target.shape[:2] + (channel_size,)\nstate = jnp.zeros(state_shape)\nmid = tuple (size // 2 for size in state_shape[:-1])\nreturn state.at [mid [0), mid [1), -1).set(1.0)\n## Model\nperceive = DepthwiseConvPerceive (channel_size, rngs)\nupdate = NCAUpdate (channel_size, num_kernels channel_size, (hidden_size\n,), rngs, cell_dropout_rate=cell_dropout_rate)\nkernel =\njnp.concatenate ([identity_kernel (ndim=2), grad_kernel (ndim=2)],\naxis=-1)\nkernel\n=-1),\n= jnp.expand_dims(jnp.concatenate ([kernel] * channel_size, axis\naxis=-2)\nperceive.depthwise_conv.kernel = nnx.Param(kernel)\nca = CA (perceive, update)\n## Train\nstate = jax.vmap(lambda : init_state())(jnp.zeros(pool_size))\npool = Pool.create({\"state\": state})"}]}