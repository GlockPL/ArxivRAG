{"title": "Empirical evaluation of LLMs in predicting fixes of Configuration bugs in Smart Home System", "authors": ["Sheikh Moonwara Anjum Monisha", "Atul Bharadwaj"], "abstract": "This empirical study evaluates the effectiveness of Large Language Models (LLMs) in predicting fixes for configuration bugs in smart home systems. The research analyzes three prominent LLMS - GPT-4, GPT-40 (GPT-4 Turbo), and Claude 3.5 Sonnet using four distinct prompt designs to assess their ability to identify appropriate fix strategies and generate correct solutions. The study utilized a dataset of 129 debugging issues from the Home Assistant Community, focusing on 21 randomly selected cases for in-depth analysis. Results demonstrate that GPT-4 and Claude 3.5 Sonnet achieved 80% accuracy in strategy prediction when provided with both bug descriptions and original scripts. GPT-4 exhibited consistent performance across different prompt types, while GPT-40 showed advantages in speed and cost-effectiveness despite slightly lower accuracy. The findings reveal that prompt design significantly impacts model performance, with comprehensive prompts containing both description and original script yielding the best results. This research provides valuable insights for improving automated bug fixing in smart home system configurations and demonstrates the potential of LLMs in addressing configuration-related challenges.", "sections": [{"title": "I. INTRODUCTION", "content": "The field of smart home automation has seen significant growth in recent years, with an increasing number of households adopting various Internet of Things (IoT) devices and systems to enhance their living environments. However, as these systems become more complex, users often encounter configuration bugs that can hinder the proper functioning of their smart home setups. This paper presents an empirical evaluation of the effectiveness of Large Language Models (LLMs) in predicting fixes for configuration bugs in smart home systems, focusing on the use of advanced AI models to address common issues faced by users[21].\nThe global smart home market is projected to reach USD 338.28 billion by 2030, growing at a 20.1% CAGR. Key drivers include increas- ing internet use, rising disposable incomes, the need for remote home monitoring, and demand for energy-efficient solutions. Home Assistant (HA) stands out among various platforms for its open-source, privacy-focused design, with over 332,000 active installations as of March 2024[15]. The widespread adoption of HA presents a unique opportunity for software engineering research, as (1) its data and programs offer a comprehensive representation of smart home systems, and (2) our research can contribute to the future development of smart home technologies[6][12]."}, {"title": "B. Configuration Bugs", "content": "Configuration bugs in smart home systems refer to errors or inconsistencies in the setup and programming of automation rules, device interactions, and system behaviors. These bugs can arise from various sources, including:\n\u2022 Incorrect syntax in automation scripts\n\u2022 Mismatched trigger conditions\n\u2022 Improper use of variables and templates\n\u2022 Inconsistent state definitions\n\u2022 Incompatible device integrations\nSuch bugs can lead to unintended system behaviors, re- duced efficiency, or complete failure of automated processes, significantly impacting the user experience and the overall functionality of the smart home system."}, {"title": "C. Large Language Models (LLMs)", "content": "Large Language Models are advanced artificial intelligence systems trained on vast amounts of text data to understand and generate human-like language [21]. These models have demonstrated remarkable capabilities in various natural lan- guage processing tasks, including:\n\u2022 Text generation\n\u2022 Language translation\n\u2022 Question answering\n\u2022 Code generation and analysis\nIn the context of this study, we focus on three prominent LLMs:\n\u2022 GPT-4: Developed by OpenAI, GPT-4 is known for its advanced reasoning capabilities and versatility across a wide range of tasks[2]."}, {"title": "D. Home Assistant Community (HAC)", "content": "The Home Assistant Community[12] is a platform where users of the popular open-source home automation system, Home Assistant, share knowledge, discuss issues, and seek solutions for their smart home setups. It serves as a valuable resource for both novice and experienced users, providing a wealth of real-world examples of configuration challenges and their resolutions."}, {"title": "E. Prompt Engineering", "content": "Prompt engineering refers to the process of designing and optimizing input prompts for LLMs to elicit desired outputs. In this study, we explore four distinct prompt designs:\n\u2022 Description Only\n\u2022 Description + Original Script\n\u2022 Description + Original Script + Specific Issue\n\u2022 Issue Only\nThese prompts are carefully crafted to provide varying levels of context and information to the LLMs, allowing us to assess their performance under different input conditions."}, {"title": "F. Configuration Bug Resolution Strategies", "content": "The study builds upon previous research that identified eight representative resolution strategies for common configuration bugs in smart home systems. These strategies encompass various aspects of automation configuration, including:\n\u2022 Proper use of quotes and string literals\n\u2022 Correct indentation and formatting\n\u2022 Appropriate trigger selection\n\u2022 Accurate state reading and specification\n\u2022 Efficient handling of multiple triggers and entity values\nBy mapping these strategies to the dataset and using them as a framework for evaluation, we can assess the LLMs' ability to identify and apply appropriate fixes to configuration issues."}, {"title": "G. Significance of the Study", "content": "This research aims to bridge the gap between advanced AI technologies and practical smart home automation challenges. By evaluating the capability of LLMs to predict and generate fixes for configuration bugs, we seek to:\n\u2022 Enhance the user experience for smart home enthusiasts by providing more accessible troubleshooting solutions\n\u2022 Reduce the time and effort required to resolve common configuration issues\n\u2022 Explore the potential for AI-assisted automation in smart home system maintenance and optimization\n\u2022 Contribute to the broader field of AI applications in IoT and home automation\nThrough a rigorous empirical evaluation of different LLMs and prompt designs, this study offers valuable insights into the effectiveness of AI-driven approaches to solving real-world smart home configuration challenges. The findings presented here have implications for both end-users and developers in the smart home ecosystem, potentially paving the way for more intelligent and user-friendly automation systems in the future."}, {"title": "II. APPROACH", "content": ""}, {"title": "A. Dataset and Resolution Strategies", "content": "1) Dataset Extraction: We extracted the dataset from anik at el's work titled \"Programming of Automation Configuration in Smart Home Systems: Challenges and Opportunities\" [7]. They crawled Home Assistant Community (HAC)[12][6] and manually reviewed the discussion threads under the cate- gory \"Configuration\". They considered threads tagged with \"automation\" from 03/01/2022 to 08/31/202222. Then they retrieved a dataset of 438 candidate discussion threads based on the HAC ranking and timestamp range. After a manual inspection of each thread, they finally selected 190 threads. Among them,129 (68%) examined issues concerning debug- ging. Although existing tools can detect at most 14 issues and fix none. We collected their 129 debugging issues and did further investigation utilizing Large Language Models and different prompt engineering. The overview of the debugging dataset is presented in Figure 1.\nThe dataset comprises the debugging data related to automa- tion configuration issues in smart home systems. As we man- ually evaluated different Large Language Models considering different prompt designs (Evaluated on 3 models generating 4 prompts), we randomly selected 21 data from them.\n2) Representative Resolution Strategies: Anik at el[7] ob- served something insightful among the data they have crawled from HAC. They identified some strategies (eight) that can be repetitively applied to resolve multiple issues of con- figuration bugs. They listed each strategy in terms of its focus, the relevant technical concepts, the number of is- sues it resolved, and a detailed description of the strat- egy's content shown in Figure 2. They also ranked the eight strategies in ascending order based on the number of relevant concepts. This structured information ensured clarity in understanding the purpose and applicability of each strategy, which we subsequently used as rules to map the strategies to the dataset. Our dataset is available here: Configuration bug dataset with predicted fix and strategies."}, {"title": "B. Prompt Engineering", "content": "Prompt Design: To evaluate how LLMs respond to differ- ent levels of contextual information and to test the models' ability to generalize across various configuration bug scenar- ios, we considered four types of prompts. These prompts were designed to guide LLMs in predicting strategies for resolving configuration bugs.\nWe designed different prompts to provide information to the LLMs, enabling them to generate accurate predictions. The four distinct prompts are as follows:"}, {"title": "C. Selection of Large Language Models", "content": "The application of Large Language Models (LLMs) in solving coding-related problems has been increasing significantly[21]. In our approach, we considered three ad- vanced and popular LLMs for our empirical evaluation, which"}, {"title": "D. Mapping to Representative Strategies", "content": "We utilized the representative strategies suggested by Anik et al.[7], which contain structured information to aid in un- derstanding the purpose and applicability of each strategy, to map the strategies to the dataset. Based on these eight listed strategies, we relabeled the dataset strategies accordingly."}, {"title": "E. Generating Configuration Fix Strategies and Fixes Using Prompts and LLMs", "content": "Our approach utilized Large Language Models (LLMs) to generate strategies for fixing configuration bugs in smart home automation by leveraging our designed prompts. We also applied the same approach to generate fixes for configuration bugs utilizing Large Language Models (LLMs). These prompts were carefully crafted to guide the models in effectively identifying and resolving configuration issues."}, {"title": "F. Compare different models and prompts", "content": "After generating bug-fix strategies and the fixes for all 21 data points using four prompts and three large language models, we compared the outputs of the different models. Additionally, we analyzed the predictions of the models when using the same prompts."}, {"title": "III. IMPLEMENTATION", "content": ""}, {"title": "A. Dataset Collection", "content": "To conduct our work we considered the extracted 129 de- bugging issues and 8 representative resolution strategies from previous research on \"Programming of Automation Configura- tion in Smart Home Systems: Challenges and Opportunities\u201d [7]. They collected the dataset from the Home Assistant Community(HAC)[12] and provided the eight representative resolution strategies with relevant information such focus of the strategy, the relevant technical concept(s), the number of issues it resolved, and a detailed description of the strategy's content."}, {"title": "B. Reform Dataset", "content": "We applied the eight representative resolution strategies, along with relevant information as rules, to map the strategies from the extracted dataset to these commonly used resolution strategies. The mapping for each issue was finalized through consensus between both authors."}, {"title": "C. Generate prompts", "content": "We generated four types of prompts based on the data avail- able to us. These prompts were designed to include attributes that could provide insightful information for predicting the fix strategies and generating fixed scripts."}, {"title": "D. Select Large Language Models", "content": "We selected the large language models based on their advanced capabilities to read and generate code, and also their availability. We have considered three models, GPT4, GPT40, and Claude 3.5 Sonnet, and implemented our approach using all four generated prompts. Selecting the Large Language Models for predicting strategies to fix configuration bugs."}, {"title": "E. Configuration Fix Strategies Generation", "content": "1) Input to LLMs: We provided the models with the config- uration bug details and the representative strategy list as input, using our predesigned prompts Figure 4a.\n2) Output from LLMs: The models analyzed the informa- tion provided through prompts. Then suggested fixing strate- gies from the provided strategy list as output."}, {"title": "F. Configuration Fixes Generation", "content": "1) Input to LLMs: Using our predesigned prompts, we provided the models with the configuration bug details as input. We specifically designed these prompts to provide the necessary context and guide the models generating fixed scripts figure 4b.\n2) Output from LLMs: The models analyzed the informa- tion provided through different prompts. They generated fixed scripts for the given configuration bugs and presented them as output along with reasoning."}, {"title": "G. Compare Prompts", "content": "After generating the fix strategies and solutions for con- figuration bugs using three models and all four prompts, we compared the performance of the prompts. We also analyzed the output from different models for the same prompt. While evaluating the suggested fixes, we focused only on cases where the models correctly predicted the strategies."}, {"title": "H. Compare LLMs", "content": "We evaluated the outcomes of the three models against the original strategies and fixes. Additionally, we compared the models' predictions for the same prompt.\nThe strategies of different prompts."}, {"title": "IV. EVALUATION", "content": "We completed all the implementation steps and collected the suggested strategies and fixes generated by different prompts for each configuration bug. Subsequently, we manually eval- uated all the fix strategies and suggested fixes from GPT4, GPT40, and Claude 3.5 Sonnet against the strategies derived from the dataset."}, {"title": "A. High-level evaluation of the models", "content": "The applied large language models each have distinct ad- vantages. GPT4 demonstrates superior reasoning capabilities compared to GPT40. On the other hand, Claude 3.5 Sonnet offers better reasoning for suggesting strategies and fixes for configuration bugs than GPT40, though it is not as compre- hensive as GPT4."}, {"title": "B. Model Performance", "content": "We evaluated all the large language models considered in this study based on the fix strategies and fixes they generated for configuration bugs in smart home systems.\n1) Predicting Fix Strategies: Overall, GPT4 and Claude 3.5 Sonnet demonstrated the strongest performance with Prompt 2 (Description + Original Script), achieving 80% accuracy in strategy prediction for fixing smart home system configuration bugs. GPT-4, however, exhibited more consistent results across different prompt types. The performance of GPT-4 was strong with Prompt 2 and Prompt 3 but poor with Prompt 1 and Prompt 4. Claude 3.5 Sonnet performed particularly well with Prompt 3. All three models demonstrated similar performance when using Prompt 4.\nGPT-4 and GPT-40 provided 52% similar strategies when using Prompts 1 and 2. The similarity dropped to 38% for Prompts 3 and 4."}, {"title": "2) Predicting Fixes", "content": "By observing the predicted strategies for fixing configuration bugs across different models and prompts, we measured the number of matched strategies and the proportion of correct fixes generated by the models. During the evaluation, some fixes could not be definitively assessed for correctness and were labeled as Do Not Know.\nFrom the it can be concluded that the overall performance of GPT4 is better than the other two models. While GPT4o performed poorly with Prompt 1, it showed good performance with the other three prompts. Claude 3.5 Sonnet performed well with Prompts 3 and 4; however, it provided some incorrect fixes in all cases."}, {"title": "C. Prompt Evaluation", "content": "1) Predicting Fix Strategies: We evaluated the different prompts based on the number of fix strategies the models pre- dicted using them. Overall, Prompt 3 and Prompt 4 provided better suggestions than the other two prompts for selecting strategies to fix configuration bugs.\nTable 3 shows that all three models performed well with Prompt 3, where each model provided the correct strategy for more than 67% of the total data. Among them, Claude 3.5 Sonnet outperformed GPT4 and GPT4o, even though it did not perform as well with the other prompts.\nGPT4 performed well with Prompt 2 but provided poor suggestions with Prompt 1 and Prompt 4. Prompt 4 did not perform well with any of the three models. None of the models were able to correctly predict even half of the strategies for fixing bugs.\n2) Predicting Fixes: We measured the number of matched strategies and how many of these included correct fixes generated by the models. During the evaluation of the fixes predicted by the models, there were instances where we could not determine their correctness, and we labeled these cases as Do Not Know.\nFor Prompt 2, GPT4 correctly predicted 85.71% of the fixes from the correct strategies it provided, with only one fix labeled as Do Not Know. On the other hand, GPT-40 produced more than 27% incorrect fixes among the correct strategies it provided.\nGPT-40 outperformed the other models when using Prompt 2.  Table VIII shows that GPT-40 correctly predicted 90% of the fixes from the correct strategies it provided, with only one fix labeled as Do Not Know. However, GPT-4 and GPT-40 produced 11.76% and 10% incorrect fixes, respectively.\nFor Prompt 3, GPT-4 excelled in providing correct fixes for configuration bugs, achieving more than 93% accuracy among the fix strategies it predicted correctly. GPT-40 also performed well, delivering approximately 86% correct fixes, although some of its other suggested fixes were labeled as Do Not Know. Table IX presents a detailed comparison.\nThe table X shows that GPT4 and GPT4o performed equally well in predicting fixes for configuration bugs correctly. Both models provided approximately 78% of the fixes accurately from the correct strategies they predicted. However, GPT-40 also suggested 22.22% of the fixes incorrectly."}, {"title": "V. RELATED WORK", "content": "Several research efforts have explored ways to improve configuration challenges processes through automation and tool support.\nThe work of Duc et al. [13] examined the main failure patterns of smart home systems, this work focuses on hardware failure(wireless link loss, battery damage, power outage). Chen et al. [11] analyzed the fault symptoms and provided maintenance suggestions after modeling the IoT system with four layers: application, storage, communication, and data.\nIn the realm of natural language processing for code, [4] surveyed various techniques for learning from source code, including approaches for code summarization and bug detec- tion. Their work provides a foundation for our use of NLP and machine learning to analyze code changes and generate human-readable comments. [24]\nV. J. HELLENDOORN ET AL[25] explored the use of machine learning models(RNN, Transformer, GGRN etc.)[8], [16], [23] for code completion and bug detection. Their find- ings on the effectiveness of transformer-based models for code"}, {"title": "VI. CONCLUSION", "content": "GPT-4 and Claude 3.5 Sonnet demonstrated exceptional capabilities in predicting fix strategies for smart home configuration bugs, achieving 80% accuracy when provided with both bug descriptions and original scripts[1]. GPT-4 maintained consistent performance across different prompt types, while GPT-40 offered advantages in processing speed and cost-effectiveness despite slightly lower accuracy rates.\nThe research revealed that prompt design significantly influ- ences model performance. Prompt 2 (Description + Original Script) and Prompt 3 (Description + Original Script + Spe- cific Issue) consistently outperformed other designs across all models[1]. This finding suggests that providing comprehen- sive context is crucial for accurate bug fix prediction. GPT4 achieved remarkable accuracy in fix generation, reaching 93.33% success rate with Prompt 3, while GPT-40 achieved 90% accuracy with Prompt 2[1]. These results demonstrate the models' strong capability in not only identifying appropriate fix strategies but also generating correct solutions. This study introduces a novel framework for evaluating LLMs in the con- text of smart home configuration debugging. The development of four distinct prompt types and their systematic evaluation provides valuable insights for future research in automated bug fixing. The findings offer immediate practical value for smart home system developers and users. The high accuracy rates achieved by the models suggest that LLM-based tools could significantly streamline the debugging process in smart home configurations. Future research should focus on improving model performance for complex configuration scenarios and reducing the occurrence of incorrect fix suggestions. Particular attention should be paid to cases where models currently produce uncertain or incorrect results. The promising results suggest opportunities for developing automated debugging tools that integrate LLMs with existing smart home platforms. Such integration could provide real-time assistance to users encountering configuration issues. Further investigation into prompt design optimization could yield even better results. Special attention should be given to reducing the 22.22% failure rate observed in some scenarios with GPT40. While the study demonstrates significant progress in automated bug fixing for smart home systems, some limitations remain. The research was conducted on a dataset of 21 randomly selected cases from a larger pool of 129 debugging issues[1]. Future work with larger datasets could provide more comprehensive insights into model performance across a broader range of configuration scenarios."}]}