{"title": "Ensuring Medical AI Safety: Explainable AI-Driven Detection and Mitigation of Spurious Model Behavior and Associated Data", "authors": ["Frederik Pahde", "Thomas Wiegand", "Sebastian Lapuschkin", "Wojciech Samek"], "abstract": "Deep neural networks are increasingly employed in high-stakes medical applications, despite their tendency for shortcut learning in the presence of spurious correlations, which can have potentially fatal consequences in practice. Detecting and mitigating shortcut behavior is a challenging task that often requires significant labeling efforts from domain experts. To alleviate this problem, we introduce a semi-automated framework for the identification of spurious behavior from both data and model perspective by leveraging insights from explainable Artificial Intelligence (XAI). This allows the retrieval of spurious data points and the detection of model circuits that encode the associated prediction rules. Moreover, we demonstrate how these shortcut encodings can be used for XAI-based sample- and pixel-level data annotation, providing valuable information for bias mitigation methods to unlearn the undesired shortcut behavior. We show the applicability of our framework using four medical datasets across two modalities, featuring controlled and real-world spurious correlations caused by data artifacts. We successfully identify and mitigate these biases in VGG16, ResNet50, and contemporary Vision Transformer models, ultimately increasing their robustness and applicability for real-world medical tasks.", "sections": [{"title": "1 Introduction", "content": "In the past decade, Artificial Intelligence (AI) models have become ubiquitous in medical applications, often outperforming human experts in tasks like melanoma detection [14] and the prediction of cardiovascular diseases from electro cardiogram (ECG) data [68]. However, the non-transparent nature of Deep Neural Network (DNN) predictions poses challenges in safety-critical contexts, as their reasoning remains obscure to both clinicians and model developers. This opacity is concerning, particularly since DNNs are prone to exploit spurious correlations in the training data. This can lead to shortcut learning [39], where models rely on (medically) irrelevant features, yet correlating with the target label. Such shortcuts are not limited to protected attributes like gender or ethnicity, but include various confounders in the training data, such as objects (e.g., rulers or hair), color shifts, or watermarks added by scanning devices. A well-known example are band-aids in dermoscopic images for melanoma detection dominantly occurring beside benign lesions, causing AI models to associate the presence of band-aids with benign lesions with potentially severe consequences in practice. Similarly, DNNs trained to detect pneumonia from radiographs have predicted the hospital system used for the scan, as prevalence varied across hospitals in the study [78]. Moreover, confounding shortcuts were learned over intended signals from computed tomography scans for COVID-19 detection [23].\nThe field of XAI sheds light onto the previously non-transparent prediction behavior of DNNs, providing insights into their internal reasoning. While traditional local \u03a7\u0391\u0399 methods focus on feature importance for individual predictions, global XAI approaches and mechanistic interpretability aim to understand overall model behavior by explaining the roles of internal representations and encoded features [1, 34, 79]. Recently, these insights have been utilized to systematically uncover model weaknesses like shortcut behavior. Current methods either detect outlier explanations for biased data samples [48, 3, 27] or outlier model concept representations [16, 53, 59]. Beyond revealing model weaknesses, XAI is also ca-"}, {"title": "2 Related Work", "content": "Existing XAI-based bias identification approaches primarily focus on shortcut detection in pre-trained models on benchmark datasets like ImageNet [24]. One line of research aims to identify samples with outlier model behavior, measured via local attribution scores in input [48] or latent [3, 27] space. Other methods seek to pinpoint spurious model representations like neurons or circuits directly. For instance, Singla and Feizi [66] employ human annotators to label neurons as valid or spurious using feature visualization techniques, leveraging this information to detect spurious samples. To minimize manual labeling, they consider only a subset of neurons based on their mutual information with model failures. In contrast to individual neurons, Neuhaus et al. [53] define class-wise neural Principal Component Analysis (PCA) to identify meaningful latent directions for each class, and leverage human annotators to label a pre-selected subset as spurious. With this approach they identify spurious features, such as the presence of bird feeders impacting the prediction of hummingbirds. Another recent bias identification method is Data-agnostic Representation Analysis (DORA) [16], which generates samples maximizing the neurons activations to represent concepts and defines a distance measure based on how neurons respond to representations of other neurons to identify outlier concepts. This has resulted in the identification of neurons extracting spurious concepts, such as watermarks or background features. In a medical context, existing works study shortcut behavior related to sensitive attributes like gender or age by comparing the model performance in different sub-popluations [15], manually annotate the dataset for spurious features [9], or define heuristics to automate the detection of specific artifacts [62]. In contrast, our work presents a generic framework for the XAI-based identification of spurious correlations without prior knowledge and the (semi-)automatic generation of sample- and pixel-level annotations for spurious features. Closest to our work is Reveal2Revise [59], an iterative model correction framework with the steps (1) reveal for bias identification, (2) bias modeling, (3) revise for bias mitigation, and (4) (re-)evaluation. While the Reveal2Revise framework emphasizes bias mitigation (step 3), our work extends and examines the XAI-based bias identification and modeling (steps 1 and 2) in greater detail, as well as its applicability for data annotation to reduce manual labeling efforts."}, {"title": "3 From Bias Modeling to (Semi-)Automated Data Annotation", "content": "In recent years, the XAI community has shifted its focus from local to global explanations to better understand overall model behavior. This line of research, known as mechanistic interpretability, aims to interpret internal representations in terms of human-understandable concepts, encoded as individual neurons, model circuits, or directions in latent space. Understanding model internals enables the identification of model substructures that encode biases stemming from data artifacts (step (2) in Fig. 1). In this section, we discuss how biases are encoded within DNNs, and how this understanding can be utilized for data annotation tasks, such as the detection of biased samples and (spatial) bias localization. Note that while we assume knowledge on the existence of biases in this section, the identification thereof (step (1) in Fig. 1) is addressed in Sec. 4.\nConsidered Types of Data Artifacts In this work, we focus on data artifacts caused by spurious correlations, i.e., concepts unrelated to the (medical) task, yet correlating with the target label due to biases in the dataset curation process. Whereas some artifacts are entirely irrelevant to the task, e.g., watermarks from medical devices, other artifacts can have a medical meaning but no causal impact on the predicted outcome, such as skin markers from dermatologists. We further distinguish between well-localized objects, such as band-aids or rulers, and non-localizable artifacts, e.g., slight color or brightness shifts caused by the usage of different medical scanners. Data artifacts can spatially overlap task-relevant information, such that masking out artifactual regions might remove important information. Moreover, spurious features can be conceptually entangled with valid features. For example, in melanoma detection, model representations for specific color patterns indicative of lesions may be entangled with natural variations in skin tone."}, {"title": "3.1 Bias Modeling from an Explainable AI Perspective", "content": "We define a DNN as a function \\(f : \\mathbb{X} \\rightarrow \\mathbb{Y}\\) that maps input samples \\(x \\in \\mathbb{X}\\) to target labels \\(y \\in \\mathbb{Y}\\). We further assume that at any layer \\(l\\) with \\(m\\) neurons, \\(f\\) can be split into a feature extractor \\(a_l : \\mathbb{X} \\rightarrow \\mathbb{R}^m\\), computing latent activations at layer \\(l\\), and a classifier head \\(f : \\mathbb{R}^m \\rightarrow \\mathbb{Y}\\), mapping latent activations to target labels. Neurons in layer \\(l\\) are denoted as \\(n_i^l\\) with \\(i\\) indexing the neuron position in the respective layer. We further assume the existence of binary (bias) concept labels \\(t \\in \\{0,1\\}\\).\nRepresenting Concepts with Individual Neurons Traditionally, it has been assumed that neurons in robust models encode human-aligned concepts, particularly at layers close to the model head [55, 61, 32, 6]. Hence, there might exist a neuron \\(n_i\\) acting as feature extractor for a biased concept. Various feature visualization approaches aim to globally explain the concept represented by a neuron by identifying inputs that maximally trigger the neuron. Whereas one line of work generates inputs that maximally activate the selected neuron [33, 55, 35], other approaches select natural images from a reference dataset, e.g., the training set. Specifically, while Activation Maximization (ActMax) [69, 11] selects samples that maximally activate a given neuron, Relevance Maximization (RelMax) [1] selects samples for which the neuron is maximally relevant for the classification task, as computed by a local explainability methods. In contrast to activations, the relevance scores are directly linked to the model prediction, indicating the neuron's impact on a specified target label. However, limitations of the mapping of concepts to individual neurons are redundancy, i.e., multiple neurons representing the same concept [25], and polysemanticity [38, 56, 31, 29], i.e., neurons reacting to multiple, seemingly unrelated concepts. Recent works aim to overcome these challenges by disentangling learned concepts via Sparse Autoencoders (SAEs) [45, 13]. Assuming there are more concepts than neurons, SAEs leverage sparse dictionary learning to find an overcomplete feature basis, allowing the usage of encoder neurons as monosemantic concept representation.\nRepresenting Concepts with Directions Given the aforementioned limitations of neurons and the fact that there are typically more concepts than neurons, it is assumed that concepts are encoded as linear combinations of neurons, i.e., directions in latent space, referred to as superposition [56, 31]. As an al-"}, {"title": "3.2 Biased Sample Retrieval", "content": "With a precise bias representation via CAV \\(h_l\\) or neuron \\(n_i\\), the detection of artifactual samples can be further automated (step (3) in Fig. 1). Specifically, all samples from a dataset can be ranked by their similarity to the artifact representation, computed for example via cosine similarity, and presented to human annotators in that order, significantly supporting them in detecting artifact samples. Therefore, the data annotation process for concept representations based on a single neuron \\(n_i\\), as for example suggested by Singla and Feizi [66], is similar to global XAI methods explaining individual neurons via ActMax or RelMax, which retrieve reference samples with maximal activation or relevance for the given neuron. However, the limitations discussed above, namely redundancy and polysemanticity, affect the concept detection capabilities of individual neurons. To address this, extending the artifact representation to linear directions in latent space via CAVs is a viable solution. Specifically, given CAV \\(h_l\\) and sample \\(x\\), we can compute a bias score \\(s_\\text{bias}^\\text{acts}\\) by projecting latent activations \\(a_l(x)\\) for layer \\(l\\) onto the CAV:\n\\(s_\\text{bias}^\\text{acts} = h_l^\\top a_l(x).\\)\nAlternatively, inspired by RelMax, we can compute bias scores using relevance scores instead of activations, as outlined in Appendix A.2. Since relevance scores are computed class-specifically, this approach allows distinguishing concepts that are artifactual for"}, {"title": "3.3 Spatial Bias Localization", "content": "Beyond detecting artifact samples, XAI insights can further reduce human labeling efforts by automating the spatial localization of biased (and localizable) concepts within these samples (step (6) in Fig. 1). We assume the existence of a bias representation via CAV \\(h_l\\) or neuron \\(n_i\\). The latter can be represented as CAV \\(h_l\\) by setting all values to zero, except for the one for neuron \\(n_i\\), which is set to one. The targeted concept can then be localized in input space using local attribution methods, such as Layer-wise Relevance Propagation (LRP) [4]. Using singular neurons as concept representation, Singla and Feizi [66] leverage Class Activation Maps [80] to visualize the feature map for the given neuron in input space and"}, {"title": "4 Concept Validation: Detecting Spurious Behavior", "content": "Given the large number of model parameters, detecting biased model representations can be like searching for a needle in a haystack, especially without prior knowledge of spurious correlations. To address this challenge, a common strategy is to identify outlier model behavior using a reference dataset. Automated detection approaches typically focus on either analyzing post-hoc explanations for a set of reference images to find anomalous model behavior [48, 27] or identifying outlier representations within the model itself [16, 53]. For concept validation (step (1) in Fig. 1), we distinguish between the data perspective in Sec. 4.1, which focuses on detecting samples exhibiting outlier behavior, and the model perspective in Sec. 4.2, which aims to identify outlier concept representations within the model. However, it is to note that outlier model reasoning is not necessarily caused by spurious correlations, but can be (clinically) valid model behavior that is rarely used. Thus, detecting spurious correlations often requires manual inspection by human experts to determine whether outlier behavior is valid or caused by spurious correlations."}, {"title": "4.1 Data Perspective Detecting Spurious Samples", "content": "A first line of works assumes that models use a different behavior for spurious samples compared to \"clean\" samples. Concretely, model behavior can be estimated using local attribution methods, such as Input Gradients [51, 65], GradCAM [64], or LRP [4]. Note, that backpropagation-based attribution approaches distribute relevance scores from the output through all layers to the input, enabling the analysis of both input heatmaps and latent relevance scores."}, {"title": "4.2 Model Perspective Detecting Spurious Representations", "content": "In contrast to detecting spurious samples, recent work focuses on identifying spurious model internals directly. This aligns with mechanistic interpretability, which seeks to decipher the internal mechanics of DNNs [56, 31, 13]. As outlined in Sec. 3.1, various global XAIs methods aim to explain the role of individual neurons, and these insights can be leveraged to detect spurious model internals by clustering learned concepts and identifying outliers. Given input data \\(X\\) with \\(N\\) samples, Pahde et al. [59] compute spatially aggregated relevances \\(R_l \\in \\mathbb{R}^{N \\times C}\\). Subsequently, they compute the pairwise cosine distance per column (i.e., channel/concept) and embed the resulting distance matrix \\(D_l \\in \\mathbb{R}^{C \\times C}\\) in a low-dimensional space using dimension reduction techniques like t-Distributed Stochastic Neighbor Embedding (t-SNE) [71] or Uniform Manifold Approximation and Projection (UMAP) [49]. This low-dimensional embedding can be visualized to identify outliers through human inspection or anomaly detection algorithms, such as the Local Outlier Factor [12]. In summary, outlier representations can be identified in an embedding representation \\(E\\in \\mathbb{R}^{C \\times L}\\), obtained as\n\\(E = emb(d_p(D_l))\\)\nwhere \\(emb : \\mathbb{R}^{C \\times C} \\rightarrow \\mathbb{R}^{C \\times L}\\) reduces the dimension to \\(L\\), and the pairwise distance function \\(d_p()\\) is applied along all channel dimensions in the latent representation \\(D_l\\), either given by activations or relevance scores for layer \\(l\\). Note that this approach assumes over-parameterization resulting in redundant neurons, allowing to distinguish between similar and dissimilar concept representations. An example is shown in Fig. 4 (bottom), where latent relevance scores from a ResNet50 model trained for melanoma detection are used to identify outlier concepts, specifically a cluster focusing on band-aids rather than clinically relevant features. Notably, Eq. 3 can easily be extended to find outlier directions instead of neurons. Specifically, this involves a linear transformation of latent representations \\(D_l\\) using the directions of interest, e.g., obtained in unsupervised manner as described in Sec. 3.1.\nSimilarly, DORA embeds a pairwise distance matrix of neuron representations into 2D, but proposes a data-agnostic approach and a tailored distance function [16]. Specifically, they generate ActMax samples as concept representation for neurons, referred to as natural Activation-Maximization signals (n-AMS). Each neuron \\(n\\) is represented by a representation ac-"}, {"title": "5 Bias Mitigation", "content": "After detecting model biases and identifying biased samples, we aim to unlearn undesired behaviors through bias mitigation. A first line of approaches modifies the training data, e.g., by removing or manipulating biased samples, followed by retraining the model [77, 75]. While effectively mitigating biases,"}, {"title": "6 Experiments", "content": "We evaluate our framework with four medical datasets from two modalities, namely vision and time-series. After describing the experimental setup (Sec. 6.1), we demonstrate the capabilities of our framework for bias identification (Sec. 6.2), the detection of biased samples (Sec. 6.3), bias localization (Sec. 6.4) and mitigation (Sec. 6.5).\nExperimental Setup The considered datasets include ISIC2019 for melanoma detection [19, 70, 20], HyperKvasir for the identification of gastrointestinal abnormalities [10], CheXpert with chest radiographs [46], and the PTB-XL dataset [73] with 12-lead ECG (time series) data. All vision datasets contain real-world artifacts that DNNs may utilize as spurious correlation, i.e., features unrelated to the task, yet correlating with the target label. ISIC2019 is particularly known for various artifacts like colorful band-aids near benign lesions and rulers or skin markers beside malignant lesions [62, 3, 18, 59]. Moreover, HyperKvasir contains insertion tubes predominantly in samples without abnormal conditions, while CheXpert samples with cardiomegaly contain pacemakers in radiographs more frequently than in healthy patients [75]."}, {"title": "6.1\n7 Limitations", "content": "While all steps in our framework are semi-automated, they require human supervision from domain experts, e.g., to validate outlier concepts, inspect detected bias samples, and determine which concepts should be unlearned. Below, we discuss additional challenges associated with each step of our framework.\nConcept Validation/Bias Identification: When encoded biases dominate, they may not appear as outlier concepts or samples. Identifying prediction sub-strategies, e.g., using PCX, can overcome this challenge. Moreover, it is to note that if the discussed approaches are not applied per class label, detected clusters might resemble clusters of classes instead of different sub-strategies."}, {"title": "8 Conclusions", "content": "In this work, we introduce an XAI-based framework for the identification of spurious shortcut behavior in DNNs from both data and model perspective. We utilize concept-based bias representations for the semi-automated computation of sample- and pixel level bias annotations. Our work is integrated into the Reveal2Revise framework, providing valuable insights for bias mitigation and re-evaluation. We successfully demonstrated the applicability of the framework by identifying and mitigation spurious correlations caused by controlled and real-world data artifacts in four medical datasets across two modalities, using VGG16, ResNet50, and ViT model architectures. Future work may explore the identification and mitigation of biases in disentangled concept spaces, e.g., leveraging SAEs. Another promising direction is the integration of expected concepts for a more targeted search for unexpected concepts."}]}