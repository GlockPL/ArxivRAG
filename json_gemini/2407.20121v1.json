{"title": "EXIT: An Explicit Interest Transfer Framework for Cross-Domain Recommendation", "authors": ["Lei Huang", "Weitao Li", "Chenrui Zhang", "Jinpeng Wang", "Xianchun Yi", "Sheng Chen"], "abstract": "Cross-domain recommendation has attracted substantial interest in industrial apps such as Meituan, which serves multiple business domains via knowledge transfer and meets the diverse interests of users. However, existing methods typically follow an implicit modeling paradigm that blends the knowledge from both the source and target domains, and design intricate network structures to share learned embeddings or patterns between domains to improve recommendation accuracy. Since the transfer of interest signals is unsupervised, these implicit paradigms often struggle with the negative transfer resulting from differences in service functions and presentation forms across different domains. In this paper, we propose a simple and effective Explicit Interest Transfer framework named EXIT to address the stated challenge. Specifically, we propose a novel label combination approach that enables the model to directly learn beneficial source domain interests through supervised learning, while excluding inappropriate interest signals. Moreover, we introduce a scene selector network to model the interest transfer intensity under fine-grained scenes. Offline experiments conducted on the industrial production dataset and online A/B tests validate the superiority and effectiveness of our proposed framework. Without complex network structures or training processes, EXIT can be easily deployed in the industrial recommendation system. EXIT has been successfully deployed in the online homepage recommendation system of Meituan App, serving the main traffic.", "sections": [{"title": "1 Introduction", "content": "Recommendation systems have received long-term and widespread attention in both academia and industry. Traditional recommendation methods [5, 15, 27, 31, 35] often focus on a single domain, using data from a single domain to mine user interests. However, due to the sparsity of most user behaviors and incomplete user interests in a single domain, single-domain recommendation methods suffer from the long-standing data sparsity problem [9, 16] and biased interest estimation [17, 25]. Cross-Domain Recommendation (CDR) methods transfer knowledge from the source domain to the target domain to solve the problems existing in single-domain recommendation, which currently attract continuous exploration. For example, on large e-commerce platforms like Amazon and life service platforms like Meituan, there are multiple business domains to meet the diverse interests of users. CDR methods can be naturally applied to these platforms to improve recommendation accuracy.\nHowever, there are two key challenges when implementing CDR methods in Meituan. Firstly, there is a marked contrast between the service functions of the search domain, which addresses the user's explicit needs, and the recommendation domain, which predicts the user's latent needs. For instance, user's sporadic medicine orders in the search domain when ill, if misinterpreted as regular interest and transferred to the recommendation domain, could result in numerous medicines being recommended when the user is healthy. Such a negative transfer situation could greatly damage the user experience, potentially leading to user attrition or even negative publicity. Secondly, Meituan's diverse range of businesses, including food delivery, in-store services, hotel booking, and healthcare, amplifies the challenge in preventing negative transfer. As users' interests in each business may vary depending on specific contexts (e.g., location and time), it is crucial to differentiate the significance of these interests across various contexts when transferring them to the recommendation domain. For instance, user interest in food delivery typically spikes during weekday meal times at the office, while interest in in-store entertainment services increases on weekend afternoons in commercial districts. Therefore, the CDR method must be capable of filtering out beneficial source domain signals that are appropriate for the recommendation domain.\nEarly CDR methods mainly utilize the similar content information to link different domains for data augmentation [26, 30]. Recent CDR methods employ machine learning techniques to first learn user/item embeddings [12, 17, 33] or rating patterns [13, 32] from the source domain and then transfer them to the target domain. Despite promising progress, existing CDR methods are ineffective when applied to scenarios that have significant differences across domains and businesses, such as Meituan. Firstly, existing CDR methods typically facilitate knowledge transfer between similar domains (such as multiple recommendation domains under different scenarios [11, 23, 32, 36]) where the impact of negative transfer is minimal. However, they may struggle in scenarios where there is a marked contrast between the source and target domains. Secondly, existing CDR methods primarily focus on the implicit modeling paradigm, as the learning objectives of these methods are to fit the ground truth of the target or source domain interest, without directly supervising the cross-domain transfer of interest signals. As a result, the transfer of interests from the source domain to the target domain is implicit and uncontrollable. This implicit modeling of interest transfer makes it challenging to filter and differentiate interests in the source domain, so it is highly probable that inappropriate or interfering interests will be transferred to the target domain, leading to negative transfer.\nTo tackle the aforementioned challenges, we propose an Explicit Interest Transfer framework (EXIT) for cross-domain recommendation. As shown in Figure 2, EXIT is capable of selectively transferring interests from the source domain based on user's specific contexts (i.e. the fine-grained scenes), ensuring only those beneficial to the target domain are transferred. This enables EXIT to both utilize source domain knowledge to more accurately capture user interests and prevent negative transfer.\nThe main contributions of this work are summarized as follows:\n\u2022 To the best of our knowledge, we are the first to use supervised learning to model the cross-domain interest transfer process, which provides a novel explicit paradigm for cross-domain recommendation.\n\u2022 We implement an effective instance for the explicit paradigm. Specifically, we elaborately design the interest combination label, a novel approach for constructing labels for supervised learning of the interest transfer process. To collaborate with the interest combination label, we further propose a scene selector network to model the intensity of cross-domain interest transfer in fine-grained scenes.\n\u2022 We conduct extensive experiments on the industrial production dataset and online A/B tests. The consistent superiority validates the effectiveness of EXIT, which is now deployed in the homepage recommendation system of Meituan App. The deployment of EXIT brings 1.23% CTCVR (Click-Through Conversion Rate) and 3.65% GTV (Gross Transaction Value) lift, contributing significant improvement to the business and greatly improving the user experience."}, {"title": "2 Related Work", "content": "Single-domain recommendation. Single-domain recommendation involves training the recommender in a specific business domain and then using it for that domain. To enhance feature extraction, recent recommendation models are moving toward the direction of deep network and feature interaction. DNN [7] and PNN [20] are proposed to learn high-order feature representations. Wide&Deep [3] and DeepFM [5] integrate shallow techniques and deep network approaches to simultaneously extract both low-order and high-order features. DIN [35], DIEN [34], MINN [18] and SIM [19] model the user's historical behavior sequence to capture the diversity of user interests. DCN [28] and DCV-V2 [29] combine the cross network and deep network to enhance feature crossing. Despite their rapid development, single-domain recommendation methods still suffer from the long-standing problems of data sparsity [9, 16] and biased interest estimation [17, 25].\nCross-domain recommendation. CDR methods transfer source domain knowledge to the target domain, alleviating the data sparsity problem and improving recommendation performance in the target domain. Early content-based transfer methods tended to utilize data from source domains to enhance or augment the target domain data [1, 26, 30]. Rating pattern-based transfer methods such as MINDTL [6] and DARec [32] first learn the specific rating patterns of users from the source domain and then transfer the rating patterns to the target domain. Most CDR methods follow an embedding-based transfer paradigm that employs machine learning techniques to learn user/item embeddings and transfer these embeddings across domains. DTCDR [36] designs an effective embedding-sharing strategy to combine and share the embeddings of common users across domains. MiNet [17] employs item-level attention and interest-level attention to jointly model user interests from multiple domains. STAR [23] proposes a centered network shared by all domains to model domain commonalities and domain-specific networks tailored for each domain to capture domain characteristics. UniCDR [2] provides a unified framework that solves the data sparsity problem and cold-start problem simultaneously. All of these CDR methods use an unsupervised approach to model cross-domain knowledge transfer."}, {"title": "3 The Proposed Framework", "content": "In this paper, we focus on improving CTCVR prediction in recommendation systems by utilizing multi-domain purchase behavior data of users. The CTCVR prediction task in online recommendation systems involves constructing a prediction model to estimate the probability of a user purchasing an item recommended by the system. The predicted CTCVR value \u0177 of a user u purchasing an item m can be formulated as:\n$\\hat{y} = f ([E(u_1), .., E(u_i); E(m_1), .., E(m_j); E(c_1), .., E(c_k)]; \\Theta_f)$ (1)\nwhere f is the model function with learnable model parameters $\\Theta_f$, E() denotes the embedding layer which maps the raw features into dense vectors. $u_i, m_j, c_k$ respectively represent user feature, item feature, and context feature, while i, j and k denote feature indexes.\nThe cross-domain CTCVR prediction problem is defined as leveraging the data from source domain to better learn the model function f and improve the prediction performance in the target domain. In the industry scenario of this work, the source domain and the target domain share the same set of users and items, as they are different business domains on the Meituan App.\nThe overall framework of EXIT is shown in Figure 3. The overall idea is to first independently predict the user's source domain interest and target domain interest, then identify the portion of interests in the source domain that can be appropriately transferred to the target domain through explicit labels, and combine it with the target domain interest to represent the user's complete interest. EXIT is composed of three components: the Interest Prediction Network (IPN), the Interest Combination Label (ICL), and the Scene Selector Network (SSN). IPN models the user's interests in the target and source domains, as the basis for building a comprehensive representation of the user's interests. The role of ICL is to serve as the ground truth of the user's complete interests suitable for the target domain, and as the label for supervised learning in the interest transfer process. SSN helps the model learn in which scenarios it is suitable to carry out cross-domain interest transfer, which takes the scene feature embedding $E_{scene}$ as input and outputs the interest transfer probability $P_{trans}$. These three components work together to fit the user's complete interests suitable for the target domain and eliminate inappropriate interests. From a global perspective, the user's complete interest $P_{whole}$ is represented as the interest in the target domain plus the beneficial interest transferred from the source domain, which is formulated as:\n$P_{whole} = P_{target} + P_{source} * P_{trans}$ (2)\nwhere $P_{target}, P_{source}$ respectively represent the user's target domain interest and source domain interest predicted by IPN."}, {"title": "3.3 Interest Prediction Network", "content": "As the overlapping users and items between domains share the same basic features, so we model the user interests of different domains based on the multi-task framework.\nIPN takes user features (user profile, behavior sequence, etc.), item features (item attributes), and context features (hour, weekday, etc.) as inputs. The raw sparse features are mapped into dense embeddings through the embedding layer. For simplicity, user embedding, item embedding, and context embedding are represented as $E_U$, $E_M$ and $E_C$. The feature embeddings are finally transformed into a vector representation V as:\n$V = [E_U ||E_M||E_C]$ (3)\nWe aggregate interests from multiple source domains for a unified representation. Given the original n source domain labels: $y_1, y_2, ..., y_n$, where y denotes whether the user has purchased the item in the i-th source domain, the aggregate label is represented as:\n$y^s = Max(y_1, y_2, ..., y_n)$ (4)\nThe aggregation label represents the union interests of multiple source domains. When any source domain label is 1, the aggregate label is 1. As industrial apps typically have dozens of business domains, designing separate networks to consider each source domain would cause the model parameters to inflate, making it difficult to meet the online system's requirements for model memory and inference speed. Unified modeling of source domain interests allows us to only add a source domain interest prediction target in the multi-task network. When the domain expands, the model structure need not be altered. Only the sample labels need to be adapted, facilitating rapid implementation and deployment of the model.\nTaking the feature vector representation V as input, and utilizing the target domain label $y^t$ and the source domain aggregation label $y^s$ as supervision signals, which respectively indicate whether the user has purchased the item in the corresponding domain, the target domain interest tower and the source domain interest tower model the user's target domain interest $P_{target}$ and source domain interest $P_{source}$ respectively."}, {"title": "3.4 Interest Combination Label", "content": "Since we use supervised learning to model the cross-domain interest transfer process which requires explicit labels, we construct the ICL as a supervisory signal for whether the source domain signal can be transferred to the target domain. In our study, we use the interest transfer probability $P_{trans}$ to quantify whether the signal from the source domain can be transferred to the target domain. During the model training process, by minimizing the loss associated with the ICL, $P_{trans}$ achieves the effect of \"tending to 1 when the source domain interest is suitable for the target domain and tending to 0 when it is not\", thus achieving the filtering of the source domain signals, and only transferring beneficial signals to the target domain.\nGiven the target domain label $y^t$ and the source domain aggregate label $y^s$, the construction manner of the interest combination label $y^{icl}$ is shown in Table 1, where \u03b7 denotes the group consistency interest, which will be discussed in the next subsection. Here we explained why the ICL is constructed in the manner of Table 1.\n\u2022 When $y^t$ = 0, $y^s$ = 0 or $y^t$ = 1, $y^s$ = 0, there is no need to transfer interest from the source domain as the user has no interest in the source domain. Through the supervision of ICL, the interest transfer probability learned by the model will tend to 0 when training loss is minimized.\n\u2022 When $y^t$ = 1, $y^s$ = 1, it indicates that the user is likely to purchase the same item in both the source and target domains. In this case, the source domain interest is suitable for transferring to the target domain, which is a positive transfer situation. Thus the ICL is set to 2 to make the model learn a high interest transfer probability, ideally close to 1.\n\u2022 When $y^t$ = 0, $y^s$ = 1, it is unclear whether the user's source domain interest can be transferred to the target domain as user has different interests in the source and target domains. At this point, we extract the consistency of the user group's interest within the source and target domains as a supervised signal to determine whether the interest can be transferred across domains. Thus, the ICL is represented as $y^{icl} = y^t + \u03b7$, that is, user's target domain interest $y^t$ plus group consistency interest \u03b7. The interest transfer probability learned by the model will tend to n by the ICL.\nWhen a user's interests in the source and target domains are inconsistent, the group consistency interest is used to represent the interest that is suitable for transferring to the target domain. Consider the following scenario, many users have purchased the same item in both the source and target domains. Even if a user has not purchased the item in the target domain, it does not necessarily indicate a lack of interest in the item. This could be due to the interest estimation bias of the recommendation system, resulting in no exposure to the item for this user. In this case, the group consistency interest obtained by mining collaborative filtering information can more accurately reflect the transferability of cross-domain interests. The group consistency interest of item i can be calculated using the following formula:\n$N_i = \\frac{|U_{pay,i}^t \\cap U_{pay,i}^s|}{|U_{pay,i}^t \\cup U_{pay,i}^s|}$ (5)\nwhere $U_{pay,i}^t$ and $U_{pay,i}^s$ denote the set of users who have paid for item i in the target domain and source domain, respectively. Notably, the computation of Equation 5 is time-efficient with the Spark tool\u00b9 and only requires daily updates when deployed in real-world applications."}, {"title": "3.5 Scene Selector Network", "content": "Users' interest preferences in Meituan have a strong correlation with specific contexts, as most users prefer to purchase food delivery on weekdays and choose in-store services on weekends. Scene information plays a significant role in predicting user interests and is also an important factor affecting cross-domain interest transfer. Therefore, we propose SSN in the EXIT framework to learn the transfer intensity of different interests from the source domain to the target domain under fine-grained scenes, in order to ensure that the interest transfer probability matches the real changes in user interests. For instance, we expect a high interest transfer probability for food delivery in the scenario of <weekday, midday, office building, and white-collar>, and a relatively low transfer probability for in-store services in this scenario.\nThe SSN first concatenates the user embedding $E_U$, item embedding $E_M$, and context embedding $E_C$, and then uses a fully connected layer to compress the concatenated embedding to $E_{hid}$. SSN takes the compressed embedding $E_{hid}$ and the scene embedding $E_{scene}$ as inputs to enhance the utilization of scene information, where $E_{scene}$ is concatenated by the embeddings of various scene features such as user's age, item type, current location, and current time. Finally, SSN outputs the interest transfer probability $P_{trans}$, which is formulated as follows:\n$E_{hid} = FC([E_U ||E_M||E_C])$ (6)\n$E_{scene} = [E(F_{age})||...||E(F_{type})||...||E(F_{hour})]]$ (7)\n$H_{SSN} = MLP([E_{hid} ||E_{scene}])$ (8)\n$P_{trans} = Sigmoid(FC(H_{SSN}))$ (9)\nwhere || denotes the vector concatenation operation. E() denotes the embedding layer. $F_{age}$, $F_{type}$ and $F_{hour}$ represent three different scene features. FC denotes the fully connected layer that performs the dimensional transformation of the dense embedding. MLP refers to a set of DNN layers with activation functions, and Sigmoid is the activation function with output values between [0,1]."}, {"title": "3.6 Multi-Interest Joint Loss", "content": "The supervised labels for IPN modeling target domain interest and source domain interest are $y^t$ and $y^s$ respectively, which are true/false flags. For the classification problem, this paper employs the classical cross-entropy loss. The supervised signal for modeling the interest transfer intensity of SSN is provided by the ICL, and SSN is trained in coordination with the multi-task network. As shown in Table 1, ICL is a continuous value, so we use L1 loss to measure the distance between the predicted interest and ICL. Thus, the multi-interest joint loss is represented as follows:\n$Loss_{joint} = \u03bb_1Loss_{target} + \u03bb_2Loss_{source} + \u03bb_3Loss_{icl}$ (10)\n$Loss_{target} = \\frac{1}{N_t} \\sum_{x_i \\in X_t} CrossEntropy(y_i^t, P_i^t)$\n$Loss_{source} = \\frac{1}{N_t} \\sum_{x_i \\in X_t} CrossEntropy(y_i^s, P_i^s)$\n$Loss_{icl} = \\frac{\u03bb_3}{N_t} \\sum_{x_i \\in X_t} |y_i^{icl} - (P_i^t + P_i^s * p_{trans})|$ (11)\nwhere $\u03bb_1, \u03bb_2$ and $\u03bb_3$ are weights that control the importance of different losses. $x_i$ is the training sample from training set $X_t$ and $N_t$ is the number of training set samples. CrossEntropy denotes cross-entropy loss function. $y^t, y^s$ and $y^{icl}$ denote the target domain label, source domain aggregate label, and interest combination label, respectively. While $P^t, P^s$ respectively denote the user's target domain interest and source domain interest predicted by IPN, and $p_{trans}$ denotes the interest transfer probability output by SSN."}, {"title": "4 Experiments", "content": "We collect the user exposure and purchase logs in the homepage recommendation, search, and channel section from the Meituan App, to build the real-world industrial dataset. The target domain is the homepage recommendation domain and the source domains are the search domain and channel section domain. Although datasets such as Amazon\u00b2 are publicly available, their source domain and target domain do not share users and items simultaneously. Hence, we select this in-house industrial data to evaluate the proposed model, which is larger than the public datasets available.\nThe introduction of source domain knowledge causes the model to learn source domain interests that do not exist in the target domain, thus expanding the prediction interest space of the model and leading to the exposure of some items that could not be exposed before. For this reason, single-domain and cross-domain methods have different prediction interest spaces. Evaluating the model solely on the offline dataset in the target domain interest space does not fully reflect the advantages of the cross-domain methods. To ensure a more accurate evaluation of the performance of different methods, we deploy EXIT and the baseline methods in the Meituan homepage recommendation production environment, which contains hundreds of millions of user requests. The source and target domains correspond with the offline dataset. The users are randomly divided into different buckets of similar volume and are assigned our proposed model or baseline models for selecting items to be exposed in Meituan homepage. Each bucket involves about one million users. The size of the item candidate set is about 100 million, covering the most active products at Meituan."}, {"title": "4.2 Compared Baselines", "content": "To demonstrate the superiority of our EXIT framework, we compare it with various baselines, including single-domain and cross-domain methods. The single-domain baseline methods include LR [21], DNN [7], DeepFM [5], and DCN [28]. LR is a linear model, while DNN is a deep nonlinear model. DeepFM extracts both low-order and high-order features simultaneously, while DCN performs explicit feature crossing. The cross-domain baseline methods are MV-DNN [4], CoNet [8], MiNet [17], STAR [23] and UniCDR [2]."}, {"title": "4.3 Evaluation Metrics", "content": "For offline experiments, we use the widely used Area Under the ROC Curve (AUC) and Logloss as evaluation metrics [5, 17, 22, 24]. For online A/B tests, we use three most important evaluation metrics in the Meituan platform: Click-Through Conversion Rate (CTCVR), Gross Transaction Value (GTV), and Negative Feedback Rate (NFR). CTCVR represents the conversion efficiency of the online system, calculated as the total payment volume on the platform divided by the total exposure volume. GTV represents the total amount of business conducted on the platform. NFR evaluates user satisfaction with recommendation results and is a key indicator of user experience. For CTCVR and GTV, the larger the better. For NFR, the smaller the better."}, {"title": "4.4 Parameter Settings", "content": "We set the dimension of the embedding vectors for each feature as embedding dim = 8. The batch size is set to 384. All the methods are implemented in TensorFlow and optimized by the Adam algorithm [10]. Each method runs 3 times in the offline dataset and each online A/B test runs for a period of 7 days, with the average results being reported. For our proposed EXIT, we use MMOE [14] with two shared experts as the multi-task network. The target tower and source tower are both 2-layer DNNs with PReLU activation functions. In SSN, we utilize the following scene features: user-side scene features such as gender, occupation, and age; item-side features such as item level 1 category, item level 2 category, item level 3 category, and item business; and context-side scene features such as hour, day of the week, request page, and connection type. We conducted parameter sensitivity experiments on $\u03bb_1, \u03bb_2$ and $\u03bb_3$ in the multi-interest joint loss. We found it easy to adjust the parameters to appropriate values. In practice, we set $\u03bb_1, \u03bb_2$ and $\u03bb_3$ all to 1."}, {"title": "4.5 Overall Performance Comparison", "content": "We evaluate the performance of EXIT and baseline methods on both the offline dataset and online A/B tests. For the online experiments, we set DNN as the benchmark and report the performance improvements of other methods compared to DNN due to business privacy. As illustrated in Table 3, the consistent improvement validates the effectiveness of EXIT. It is observed that deep models have significant performance improvements than LR. DeepFM and DCN outperform DNN, indicating that feature combination and feature crossing can improve prediction performance. As to the cross-domain baseline methods, they show improvements over single-domain methods on AUC and Logloss, demonstrating the benefits of knowledge transfer. In terms of the online CTCVR and GTV, cross-domain methods achieve more significant improvements. This suggests that evaluating offline performance only in the target domain interest space cannot fully reflect the actual performance of CDR methods, verifying the necessity of conducting online comparison experiments. However, for the NFR metric that measures user experience, most cross-domain baselines perform worse than DeepFM and DCN. This is because these cross-domain methods do not explicitly consider the negative transfer issue, which results in the introduction of inappropriate interests into the target domain, consequently harming the user experience. Our proposed EXIT, by explicitly modeling effective interest signals that are suitable for transferring to the target domain, not only captures user interests more comprehensively but also prevents negative transfer. Therefore, EXIT significantly outperforms both the single-domain and cross-domain baseline methods on all offline and online evaluation metrics."}, {"title": "4.6 Ablation Study", "content": "We conducted several ablation studies and that every component in EXIT is essential.\nFirstly, we eliminated the ICL and indiscriminately incorporated all interest signals from the source domain into the target domain. As shown in Table 4, the performance of both offline and online experiments exhibits a significant decline. Notably, the NFR metric shows a substantial increase, which may be attributed to the introduction of interest signals that are inappropriate for the recommendation domain. The results suggest that the ICL not only assists in more accurately modeling user interests but also plays a crucial role in preventing negative transfer. Secondly, to figure out the effectiveness of SSN, we eliminate the additional scene features and scene embeddings. The experimental results convey that scene information is beneficial for modeling the interest transfer intensity. Moreover, to investigate the function of the multi-interest joint loss, we removed the cross-entropy losses associated with yt, ys and eliminated the stop gradient from the network in Figure 3, leaving only the L1 loss associated with the final ICL to train the model. However, the training process fails to converge without the multi-interest joint loss. This indicates that learning the source domain interest and target domain interest separately is the fundamental task of the model, which also aligns with the global perspective in Equation 2.\nWe study the role of group consistency interest through two variants of ICL: (1) ICL(\u03b7 = 0) indicates that ICL is constructed solely based on user's personalized interests, without considering group consistency interests; (2) ICL(y + n) indicates the use of group consistency interests to represent the transferability of cross-domain interests in all scenarios. The experimental results in Table 4 demonstrate that ignoring group consistency interests or relying solely on them both result in a decline in model performance. Additionally, user's personalized interests have a more significant impact on ICL. The results confirm the soundness of the ICL construction method. To accurately represent the transferability of cross-domain interests, we propose utilizing group consistency interests when the user's source and target domain interests are inconsistent. When interests align in both domains, the decision to transfer interests should be based on the user's personalized interests."}, {"title": "4.7 Online Deployment", "content": "We deployed EXIT on the homepage recommendation system of Meituan App and conducted online A/B tests over two weeks during Mar.2024. Since the base production model is a multi-task model that simultaneously models multiple objectives, we maintain the structure of all other objectives and only replace the modeling of the CTCVR target with the EXIT framework during online deployment. The introduction of EXIT brings 1.23% overall CTCVR lift, 3.65% GTV lift, and 6.62% NFR decrease. The results demonstrate the effectiveness of the EXIT in the actual production recommendation system. Currently, EXIT has been deployed for handling major online traffic at the homepage on Meituan App, which is one of the top mobile apps in our country."}, {"title": "5 Case Study", "content": "To further verify the working mechanism of our explicit framework, we provide case studies from the online recommendation system. Table 5 shows the recent behaviors of an actual Meituan user in the Meituan App and how the EXIT scores different candidate items during an online request from this user. The user exhibits diverse interests across different business domains, primarily purchasing food delivery in the target domain and purchasing group deals and medicines in the source domain. Therefore, it is difficult to accurately model user interests with only single-domain data. Furthermore, we examine how EXIT scores candidate items during the online request. For group deals like hot pot or barbecue that the user is interested in within the source domain, both $P_{source}$ and $P_{trans}$ are found to be quite high. According to Equation 2, higher values of both $P_{source}$ and $P_{trans}$ result in higher predicted CTCVR scores. Consequently, the model prioritizes and exposes these items to the user. This suggests that EXIT is capable of effectively transferring knowledge across domains, thus facilitating a more accurate understanding of user interests. As for the candidate items of cold medicine, although the user may have previously purchased cold medicine in the source domain due to sickness or other reasons, the group consistency interest for such items is minimal. By supervising the model learning with the ICL, it learns a lower $P_{trans}$, thereby omitting such source domain instant interests during interest transfer. By controlling the interest transfer probability, EXIT can effectively identify interest signals that are inappropriate for the target domain, thereby preventing negative transfer."}, {"title": "6 Conclusion", "content": "In this paper, we propose a simple and effective EXplicit Interest Transfer framework called EXIT, to tackle the challenges in CDR. EXIT constructs Interest Combination Label (ICL) for supervised learning of the cross-domain interest transfer, which provides a novel explicit paradigm for CDR and alleviates the issue of negative transfer. To collaborate with the ICL, we propose a scene selector network to model the intensity of interest transfer in fine-grained scenes. By this means, EXIT can accurately and completely capture user interests and improve the recommendation performance. Extensive offline and online experiments demonstrate the superiority of EXIT. EXIT has been successfully deployed to the Meituan homepage recommendation system, bringing significant business benefits to Meituan. We anticipate that EXIT will inspire more efforts in the future towards explicit cross-domain recommendations. In the future, we will explore how supervised learning can be used to model knowledge transfer in domains without item or user overlap, to achieve broader applications of the explicit paradigm."}]}