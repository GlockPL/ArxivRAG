{"title": "SE-GCL: An Event-Based Simple and Effective Graph Contrastive Learning for Text Representation", "authors": ["Tao Meng", "Wei Ai", "Jianbin Li", "Ze Wang", "Yuntao Shou", "Keqin Li"], "abstract": "Text representation learning is significant as the cornerstone of natural language processing. In recent years, graph contrastive learning (GCL) has been widely used in text representation learning due to its ability to represent and capture complex text information in a self-supervised setting. However, current mainstream graph contrastive learning methods often require the incorporation of domain knowledge or cumbersome computations to guide the data augmentation process, which significantly limits the application efficiency and scope of GCL. Additionally, many methods learn text representations only by constructing word-document relationships, which overlooks the rich contextual semantic information in the text. To address these issues and exploit representative textual semantics, we present an event-based, simple, and effective graph contrastive learning (SE-GCL) for text representation. Precisely, we extract event blocks from text and construct internal relation graphs to represent inter-semantic interconnections, which can ensure that the most critical semantic information is preserved. Then, we devise a streamlined, unsupervised graph contrastive learning framework to leverage the complementary nature of the event semantic and structural information for intricate feature data capture. In particular, we introduce the concept of an event skeleton for core representation semantics and simplify the typically complex data augmentation techniques found in existing graph contrastive learning to boost algorithmic efficiency. We employ multiple loss functions to prompt diverse embeddings to converge or diverge within a confined distance in the vector space, ultimately achieving a harmonious equilibrium. We conducted experiments on the proposed SE-GCL on four standard data sets (AG News, 20NG, SougouNews, and THUCNews) to verify its effectiveness in text representation learning. The accuracy achieved on the respective datasets is 91.56%, 86.76%, 98.03%, and 97.79%, demonstrating superior performance on most datasets compared to baseline methods.", "sections": [{"title": "1 Introduction", "content": "Text representation learning is a fundamental aspect of natural language processing that helps capture semantic and syntactic nuances of textual data. It enables adequate comprehension and generation by machine learning models [1, 22, 33, 34, 38, 39, 45, 48, 57]. Its paramount importance lies in its transformative capacity to bridge the gap between raw text data and computational models, paving the way for advancements in information retrieval, text mining, machine translation, sentiment analysis, and automated reasoning [7, 19, 21, 40-42, 62]. The pervasive nature of textual data in the digital age underscores the necessity for advanced text representation learning methods. Despite their significant progress, existing text representation methods grapple with several challenges.\nOne major flaw of the prevailing approach is its tendency to treat text as an undifferentiated sequence and extract only keywords or sentences as representatives of the entire text. At best, it connects other similar texts or additional information to enhance data. These approaches significantly oversimplify the inherent content complexity and discount its contextual richness. Traditional word-based representations such as BoW and TF-IDF effectively ignore the order of words, while others like Word2Vec [26], GloVe [29], and fastText [17] model the semantics of individual words but struggle with capturing the nuances of longer phrases or sentences. Numerous academics are diligently exploring sentence-level representations. For instance, MixCSE [65] forces the model to capture subtle sentence semantic features by introducing hard negative examples. However, the encapsulation of text that encompasses multiple sentences invariably results in the erosion of structural integrity and the dilution of long-range semantic coherence. Even more advanced Transformer-based [8, 24, 35, 36, 46, 49, 66] methods treat the entire text as a sequence of words and then employ the attention mechanism to understand the context. For example, Bert [13], pre-trained on a large-scale corpus, uses the multi-head attention mechanism to capture the dependencies between words and achieve competitive results in multiple natural language tasks. ELECTRA [12] designs the replaced token detection method to achieve better robustness with lower training costs. However, Transformer-based models mainly focus on token information and may ignore the complex interconnections and multiple-level hierarchy. In addition, it has a length limit on the input text, and truncation of the text may lead to unpredictable loss of text semantics. Its processing method is shown in Fig. 1(a). While powerful, such models may fail to consider the high-level semantic structure inside the document, limiting their effectiveness in text representation learning.\nThe advent of graph neural networks [2-6, 14, 23, 37, 43, 44, 47, 53] provides a new perspective for text representation, making it possible to model unstructured data such as text. Its processing method is shown in Fig. 1(b). For example, TextGCN [61] builds a corpus-level heterogeneous graph and uses word nodes as a bridge for message passing to learn the representation of document nodes. Although it solves the problem of converting a text corpus into a graph, it cannot take advantage of the rich contextual information in the text. TextING [64] builds a separate graph for text, reducing memory consumption, but it ignores the rich relationship information between entities in the text and lacks the grasp of semantic information. In summary, both sequence-based and graph-based methods do not fully utilize entities and their relationship information, which cannot represent semantically sparse text well. Therefore, designing a text representation method that can truly embrace natural language's semantic and structural complexity is still a problem worth exploring.\nUnsupervised graph contrastive learning has been applied in text representation learning in the ongoing pursuit of a more streamlined learning paradigm. The advantage of GCL-based methods is that they can autonomously identify underlying structural features in data without annotating the data. For example, CGA2TC [60] constructs a corpus-level graph with words and documents as nodes and designs a contrastive graph representation framework with an adaptive augmentation strategy, which can effectively remove graph noise and achieve promising performance. However, it does not consider the structural and semantic information in the text comprehensively, leading to less distinguishable representations of texts. To alleviate this problem, TGNCL [20] constructs a word graph for each text, which captures the rich contextual information of the text. Then, a contrastive learning regularization is developed based on the constructed text graphs to improve the robustness of text representation. Moreover, the efficiency of CGA2TC [60] and TGNCL [20] is notably reduced by their reliance on intricate graph data augmentation techniques, including the creation and encoding of contrastive views. Therefore, the delicate equilibrium between robustness and efficiency underscores the urgent need to develop more refined yet effective data augmentation techniques in graph contrastive learning to unlock its latent potential in text representation.\nIn response to these challenges, we present an event-based, simple, yet effective graph contrastive learning framework SE-GCL for text representation learning. This method diverges from traditional techniques by focusing on textual events as the primary unit of analysis rather than merely extracting keywords and sentences. SE-GCL captures the core intent of texts semantically and structurally by defining textual events and building internal relational graphs for each text. Further, we introduce a streamlined, unsupervised graph contrastive learning framework to leverage the complementarity between semantic and structural textual information for comprehensive feature extraction. Specifically, to improve text representation efficiency, we first mine the event skeletons in the internal relationship graph to preserve only the more essential semantics. Furthermore, we propose a simplification of the complex data augmentation process commonly found in existing graph contrastive learning. For anchor embeddings, instead of GCN, we use MLP to generate anchor embeddings infused with semantic information. For event skeletons, we adopt GCN for embedding representation. This approach explores the complementarity of semantic and structural information while effectively simplifying the strategy for generating embeddings. In another simplification step, we shuffle the anchor embeddings to generate negative embeddings, avoiding the need for more computationally expensive strategies. Lastly, we can achieve equilibrium by manipulating various embeddings using multiple loss functions to approach or diverge from each other within a finite distance in the vector space. This systematic yet innovative approach effectively addresses the challenges current text representation methods face, offering a more efficient and robust path forward. Our source code is available at https://github.com/KrisWongz/SEGCL. The main contributions of this paper can be summarized as follows:\n\u2022 We first propose the definition of textual events and construct an event-based internal relational graph to express the core intent of each text at both semantic and structural levels.\n\u2022 We propose an event-based graph contrastive text representation learning framework, which can explore the complementarity between semantic and structural information to obtain semantic-rich text representation and achieve better efficiency.\n\u2022 Experiments and analysis on real-world datasets show that our method outperforms existing methods in effectiveness and interpretation."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Word-sentence-based Text Representation", "content": "Primarily, the encoding of textual information hinges on the representation of words, a foundational pillar in the landscape of natural language processing that maintains its indelible significance. Conventional word-based representations like Bag of Words and Term Frequency-Inverse Document Frequency tend to overlook the sequential arrangement of words. In contrast, alternative techniques such as Word2Vec, GloVe, and fastText [17, 26, 29] endeavour to encapsulate the semantics of individual words yet grapple with capturing the subtle intricacies of elongated phrases or sentences. These methodologies have recorded noteworthy successes in the realm of word representation. However, their direct application for text representation poses a formidable challenge.\nAn advanced approach lies in the representation of sentences, a technique poised to assimilate more robust features. For instance, Zhang et al. [65] have proposed a contrastive model, which expands upon SimCSE [16] by iteratively crafting hard negatives through a blend of both positive and negative features. Similarly, Yan et al. [59] have introduced a Contrastive Framework for Self-Supervised Sentence Representation Transfer, employing contrastive learning to refine BERT [13] in an unsupervised yet efficacious manner. In another noteworthy research [68], a multilayer semantic representation network is explicitly devised for sentence representation, wherein a multi-attention mechanism garners the semantic information across varying sentence levels. Sentence representation has seen substantial advancements and has been effectively incorporated across multiple domains in recent years. Nevertheless, text representation spanning multiple sentences invariably invites the degradation of structural integrity and a concurrent dilution of long-range semantic coherence.\nWord-based and sentence-based models achieve superior results in short text sequence representation learning. However, when dealing with lengthy texts that have complex meanings, these models often struggle to grasp the deeper semantic features. This is because they tend to analyze words and sentences in isolation without considering how these elements interrelate or how they contribute to the overall coherence of the text."}, {"title": "2.2 Text Representation via Deep Learning", "content": "The field of text representation via deep learning methodologies has undergone a remarkable metamorphosis, marked by an exponential surge in model intricacy and the multifaceted representations they facilitate. Primitive undertakings centered predominantly on convolutional neural networks (CNNs) [18] and recurrent neural networks (RNNs) [63], along with their long short-term memory (LSTM) [32] offshoot. These frameworks, exploiting the inherently sequential characteristic of textual data, marked a considerable stride forward from their preceding non-contextual counterparts. Regrettably, the CNNs' focus remains tethered predominantly to local information, overlooking long-range semantic relationships. Concurrently, RNNs and their ilk possess the capacity to consider the sequence in its entirety but display diminishing effectiveness as the sequence length swells. Ultimately, none of these models demonstrate an efficacious capability in abstracting global semantics.\nWith the inception of attention mechanisms and transformer architectures, the domain of text representation underwent a significant paradigm shift. Transformer-centric models, such as BERT [13] and GPT [30], seized the potential of the attention mechanism to capture dependencies without regard to their proximity within the textual continuum, effectively circumventing the constraints of RNNs and LSTMs. For example, SWCC [15] utilizes document-level co-occurrence information of events to learn event representations without additional annotations.\nSimultaneously, the rise of graph neural networks (GNNs) [10, 51, 53] signaled a promising development in text representation. Uniquely endowed to grasp the structural nuances innate to text, GNNs address a critical gap often neglected by sequence-oriented models. For instance, TextGCN [61] erects a text graph predicated on word co-occurrence and document-word correlations, subsequently employing a Graph Convolutional Network to learn representations. TREND [54] proposed the concepts of events and dynamic nodes, which capture the individual and collective characteristics of events, respectively. TextFCG [52] builds a single graph for all words in each text, labels edges by fusing various contextual relations, and uses GNN and GRU for text classification.\nAlthough these deep learning-based methods are practical and widely used, they all face difficult problems. First, sequence-based models focus on local dependencies of text but cannot fully capture long-term dependencies. Second, although the graph-based models can construct the global structure of the text corpus, they ignore the rich entity information and relationship information within the texts."}, {"title": "2.3 Contrastive Representation Learning", "content": "Contrastive representation learning represents another significant frontier in developing of advanced text representation techniques. This branch of learning operates on the principle of learning representations by contrasting positive pairs (similar or related instances) against negative pairs (dissimilar or unrelated instances). Such learning mechanisms have shown remarkable success across various applications, including computer vision and natural language processing[55].\nSimCLR [11] extended InfoMax principles to multiple views and maximized Mutual Information (MI) by augmenting the resulting views with data. The InfoGCL [56] framework reduced mutual information between contrasting parts through the Information Bottleneck principle while maintaining the integrity of task-relevant information at the level of individual modules and the entire framework. Mo et al. [27] proposed a simple unsupervised graph representation learning method SUGRL, whose multiple loss explored the complementary information between structural and neighbor information to produce more minor generalization errors. GCNSS [25] effectively alleviates the negative sampling bias problem in graph contrastive learning by utilizing label information. NCLA [31] proposed a learnable graph augmentation strategy to produce safer contrasting views. For the field of text representation, CGA2TC [60] designed an adaptive data enhancement strategy to effectively filter graph noise information. TextGCL [67] uses contrastive learning loss to simultaneously train GCN and Bert to learn a more robust text representation. TGNCL [20] introduces contrastive learning regularization on text-level graphs to learn robust word representations.\nExisting graph data augmentation methods can lead to two potential issues. Firstly, graph data augmentation typically involves view generation and view encoding, which incurs significant computational costs. Secondly, modifying graph information in a random manner (such as node dropping and edge dropping) may result in unpredictable semantic loss. Consequently, there is a pressing need to develop a more efficient strategy for data augmentation in this context."}, {"title": "3 Proposed Method", "content": "In this paper, we introduce an event-based, simple yet effective graph contrastive learning (SE-GCL) framework, a novel approach to text representation that effectively captures both the semantic and structural intricacies inherent in natural language. The proposed SE-GCL method comprises four major steps, forming a systematic pipeline for comprehensive text representation, whose overall structure is shown in Fig. 2.\nThe first step is the construction of an intra-relation graph. Recognizing that textual events represent core semantic and structural information, we extract these event blocks and build a graph based on their semantic relationships, thereby retaining the most critical and central semantic information of the text.\nNext, we introduce the concept of event skeleton extraction. By defining event skeletons and applying them to the intra-relation graphs, we effectively compress and augment them, leading to a more efficient and enhanced representation of textual events.\nThe third step involves the generation of embeddings in the contrastive framework. We design a streamlined, unsupervised graph contrastive learning framework to exploit the complementarity between semantic and structural textual information for comprehensive feature extraction. We use less complex embedding generation strategies instead of complex data augmentation strategies common in existing graph contrastive learning.\nFinally, the SE-GCL method employs multiple loss functions to facilitate the convergence of our model. A harmonious balance is achieved by manipulating various embeddings to approach or diverge from each other within a finite distance in the vector space, ensuring the robustness and effectiveness of our model.\nIn the following sections, we delve into a detailed exposition of each step, elucidating the innovative mechanisms and strategies that underpin the SE-GCL method."}, {"title": "3.1 Intra-relation Graph Construction", "content": "In the first stage of our SE-GCL method, we convert the raw text into an intra-relation graph using a syntactic dependency-based Language Technology Platform (LTP) [9] event extraction tool. The overall process is shown in Fig. 3. This tool allows us to delve beyond surface-level syntactic structures of sentences and directly extract deep semantic information, thereby providing a more comprehensive and enriched understanding of the text.\nWe commence by processing the text into multiple triplet event blocks using the LTP tool. An example of such an event block is [\u201cPeter\u201d, \"eats\", \"apple\"]. Each element in these event blocks is referred to as an \"event element\". One of the key advantages of this approach is that we can describe the semantics of sentences through the semantic framework borne by the vocabulary without needing to abstract the vocabulary itself. This is crucial as the number of arguments is invariably smaller than the vocabulary.\nSubsequent to the event block formation, we retain the part-of-speech information for each word. This stage can be likened to the process of named entity recognition, wherein entity information, such as person names, place names, and institution names, is identified. The retention of part-of-speech information is crucial as it provides additional context and semantic information that aids in the construction of the intra-relation graph. It allows us to differentiate between entities and actions and to understand the roles different words play within the event blocks.\nBuilding upon the event blocks and the part-of-speech information, we then proceed to construct an intra-relation graph. Initially, we include event blocks with entities, establishing edge connections based on the relationships between these entities. Specifically, entities that appear simultaneously in the same sentence are connected through edges, such as entity 1 and entity 2 in Fig. 3. It is worth pointing out that the same entity in different event blocks will be treated as a node, which means that different event elements can be connected through the shared event entity.\nIn the next step, we preserve event blocks where an event element appears multiple times and establish connections based on the co-occurrence relationship of these event elements. It should be noted that the event elements need not be identical for a link to be established. A link is created if the similarity between event elements exceeds a predefined threshold ($\\gamma$). The similarity between event elements is measured using a semantic similarity metric, which takes into account both the semantic and syntactic similarities between the elements. The threshold is determined empirically, with a higher threshold leading to fewer but more confident connections and a lower threshold leading to more but potentially less confident connections.\nThrough the aforementioned process, we successfully convert the text into an intra-relation graph, capturing the intricate semantic and structural details inherent in natural language. This graph forms the foundation for the subsequent stages of our SE-GCL method."}, {"title": "3.2 Event Skeleton Extraction", "content": "The second stage of our SE-GCL method involves the extraction of event skeletons. Event skeletons provide a specific event composition architecture frequently observed in these types of articles. The main purpose of event skeleton extraction is to capture representative semantic information within the text by establishing a relation graph between event entities, which helps to mine the core event organization pattern of the text. Therefore, the event skeleton provides an effective way to represent the event structure of text and is an important method to explore rich text contextual semantics.\nTo extract these event skeletons, we employ the gSpan (graph-based Substructure pattern mining) algorithm [58], a seminal technique in the field of frequent subgraph mining. The gSpan algorithm operates by mapping graph data to a canonical order string and systematically exploring the search space using a depth-first search strategy. This technique enables us to efficiently identify frequent subgraphs, i.e., substructures that recur at a frequency above a given threshold.\nGiven a graph G, denote the intra-relation graph. In a more sophisticated and elegant manner, we assign identifiers to the nodes and connections within the intra-relation graph. The trio of node categories can be associated with a total of sextet edge types. Subsequently, we arrange the nodes and connections in accordance with the frequency of their identifiers and eliminate those nodes and connections that exhibit a lower frequency, thereby deriving a novel graph, denoted as $G_{new}$. The amalgamation of connections exhibiting a higher frequency into a set, denoted as E, can be perceived as the formation of a set E encompassing all connections within the graph $G_{new}$.\nArrange the edges in E in descending order of the minimum depth-first search (DFS) encoding order and frequency. Our objective is to discern the subgraph of the frequency within the intra-relation graph, and the connection can also be perceived as a unique subgraph. The connections within the set E can be viewed as the most rudimentary frequent subgraph. The ensuing frequent subgraph mining is predicated upon these frequent connections for recursive mining.\nSubsequently, we procure the initial frequent subgraph, denoted as A, predicated on E, and proceed to augment it recursively. The augmentation process is partitioned into a triad of steps. Initially, an assessment is made to determine whether DFS encoding is fulfilled. If this condition is met, an expansion is executed on the rightmost side. An evaluation of the newly augmented subgraph is conducted to ascertain whether it complies with the support degree. If it does, the recursive expansion continues predicated on the new subgraph. Ultimately, we succeed in obtaining frequent subgraphs.\nOnce these frequent subgraphs are extracted, they can be represented as event skeletons when applied to the intra-relation graph with event information. As such, the event skeletons encapsulate the representative structure of the events, providing a compact yet comprehensive snapshot of the most salient semantic elements within the text. This efficient representation paves the way for the subsequent stages of the SE-GCL method."}, {"title": "3.3 Embedding Generation", "content": "We focus on the generation of embeddings within the graph contrastive learning framework in the third phase, which includes the creation of anchor embeddings, positive embeddings, and negative embeddings.\nAnchor Embedding: Traditionally, embedding generation methods have relied heavily on deep learning techniques such as GCN, followed by the application of a readout function to obtain anchor embeddings. While effective, these methods can be computationally intensive and time-consuming, posing challenges for scalability.\nIn contrast, our approach in the SE-GCL method is to leverage the intra-relation graph directly to generate anchor embeddings with event information. Specifically, we employ a simple multi-layer perceptron (MLP) to transform the word nodes in the intra-relation graph into anchor embeddings. This approach effectively reduces the computational burden of the algorithm, thereby enhancing its scalability. By using the MLP, we can capture the event information inherent in the word nodes of the intra-relation graph. The formula is expressed as:\n$H^{l+1} = sigmod(H^lW^l + b)$, (1)\nwhere $H^l$ is the embedding of the l-th layer and $H^0$ is the input. $W^l$ is the weight of the l-th layer. M represents the number of neurons. b is the artificially set bias. Here, we regard the output $H^L$ of the last layer as the anchor embedding.\nNegative Embedding: In contrastive learning, the combination of negative samples containing significantly different features and anchor samples can promote the model to learn highly discriminative representations. In the context of generating negative embeddings, many previous methodologies have relied on intricate negative sampling strategies or have utilized GCN to obtain embeddings after distorting the original graph. While these methods can be effective, they are often complex and time-consuming, posing challenges for scalability and efficiency. In contrast, we adopt a simpler strategy: shuffle the anchor embeddings to generate the negative embeddings. This approach significantly reduces the computational burden of the algorithm. Through this strategy, our method destroys the original order of anchor embeddings, creating a set of negative embeddings with distinct characteristics from the anchor embeddings. This aligns with the objective of contrastive learning, which aims to minimize the similarity between negative pairs (i.e., the anchor and negative embeddings). Furthermore, this simple strategy of generating negative embeddings provides discriminative negative samples while significantly reducing the computational cost by removing the graph neural network. The negative embedding $\\bar{H}$ is defined as shown in Equation 2.\n$\\bar{H} = shuffle(H)$. (2)\nPositive Embedding: The generation of positive embeddings is a crucial aspect of the SE-GCL method. While there are diverse approaches to this task, many existing methods rely on GCN to extract the structural information of the graph or to perform further data augmentation. However, we propose two distinct strategies for generating positive embeddings, aiming to capture more complementary information. These strategies focus on two key types of information: structural information $H^+$ and event information $\\tilde{H}$.\nStructural Information: The first strategy is designed to capture the structural information inherent in the intra-relation graph. This involves leveraging the relationships between the nodes in the graph, as represented by the edges and their properties. By focusing on this structural information, we can capture the underlying architecture of the events in the text, which is crucial for understanding their context and semantics.\nEvent Information: The second strategy is focused on capturing the event information represented in the intra-relation graph. This involves leveraging the event blocks and event skeletons that we have extracted in the previous steps. By focusing on the event skeleton information, we can capture the specific compositions of events in the text, which are crucial for understanding the core semantics.\nFor each intra-relation graph G, we introduce a two-layer GCN as an encoder to get the structure embedding. Formally, Let A represents the adjacency matrix of G and D is the degree matrix, where $D_{ii} = \\sum_j A_{ij}$. Moreover, each node is connected to itself. Then, the neighbor information is aggregated into N to update the embedding of node v recursively by the aggregation function AGG. The steps can be expressed as follows:\n$H^{+1} = \\sigma(\\bar{A}.W^l.p.concat(H, AGG(H, v_j \\in N_v)),$ (3)\nwhere $\\sigma$ represents the activation function, such as Leaky ReLU. $\\bar{A} = D^{-\\frac{1}{2}}AD^{-\\frac{1}{2}}$ is the symmetric normalized adjacency matrix. $W^l$ is the trainable transformation matrix of the layer l. p is the event skeleton's weight.\nWe select nodes in the event skeleton to sample and obtain their average to obtain the positive embedding with the event information. The sampled positive event embeddings provide a new perspective to encourage the model to learn discriminative representations. The formula is expressed as follows:\n$\\tilde{H} = \\frac{1}{k}\\sum (H_i\\vert v_i \\in event\\_skeleton)$, (4)\nwhere k represents the number of nodes in the event skeleton.\nIn general, we design two positive embeddings from the structural and event levels to explore their complementarity. The structure embedding contains the information of the whole intra-relation graph, and the event embedding contains part of the nodes' information. They interpret the same graph from different perspectives. Therefore, it is considered separately, and we can obtain specific complementary information."}, {"title": "3.4 Multi-Loss Functions", "content": "The purpose of contrastive learning is to make positive embedding close to anchor embedding and negative embedding away from anchor embedding. Since a small generalization error may improve the generalization ability of contrastive learning, and reducing the intra-class variation or expanding the inter-class variation is an effective solution to reduce the generalization error, we design multi-loss functions for the resulting positive, anchor, and negative embeddings based on the event skeleton and intra-relation graph. Meanwhile, we introduce an upper bound loss to improve efficiency and replace the discriminator method. The multi-loss can be formulated as follows:\n$sim(H,H^+) < sim(H,\\bar{H}) - \\eta,$ (5)\nwhere $sim(\\cdot)$ is a similarity measure function, such as l2-norm distance, and $\\eta$ is a non-negative number to ensure that the distance between positive and negative embeddings is within a fixed range. Integrate all negative embeddings to get the loss $\\zeta$:\n$\\zeta_{multi} = \\frac{1}{k} \\sum_{i=1}^k \\{sim(H, H^+)^2 - (sim(H, \\bar{H_i})^2 - \\eta)\\} max,$ (6)\nwhere $\\{ \\cdot \\} max$ means taking the maximum value between $\\{ \\cdot, 0\\}$, k represents the number of negative embeddings.\nWe apply the loss to the two defined positive embeddings. The loss of structure embedding $H^+$ can be formulated as follows:\n$\\zeta_s = \\frac{1}{k} \\sum_{i=1}^k \\{sim(H, H^+_s)^2 - (sim(H, \\bar{H_i})^2 - \\eta)\\} max,$ (7)\nwhile the loss of event embedding can be formulated as follows:\n$\\zeta_e = \\frac{1}{k} \\sum_{i=1}^k \\{sim(H, \\tilde{H})^2 - (sim(H, \\bar{H_i})^2 - \\eta)\\} max.$ (8)\nThe implementation of such a multiplet loss, which is essentially a pair of triplet losses, can enhance the disparity between classes upon examining Eqs. (7) and (8), two potential scenarios emerge. The first scenario is one where the loss incurred in Eq. (8) is null, while that of Eq. (7) is non-zero. In this case, Eq. (7) continues to extend the vector representation of the negative sample in comparison to the positive sample of Eq. (7) further afield. The converse scenario is equally plausible. Eq. (8) also serves to distance the vector representation of negative samples. Collectively, these two equations contribute significantly to the differentiation between classes. In this way, we can effectively expand the inter-class variation in the case of one type of loss with poor effect to obtain complementary information of event information and structural information.\nIn order to avoid the situation where the gap between the anchor embedding and the positive embedding itself is very large, we set an upper bound for the negative pair to ensure that the distance between the negative embedding and the anchor embedding is limited, which can effectively reduce the intra-class variation. The upper bound loss is defined as follows:\n$\\zeta_u = \\frac{1}{k} \\sum_{i=1}^k \\{sim(H, H^+)^2 - (sim(H, \\bar{H_i})^2 - \\eta - \\theta)\\} min,$ (9)\nwhere $\\{\\cdot\\} min$ means taking the minimum value between $\\{\\cdot, 0\\}$. The final multi-loss $\\zeta$ is given by\n$\\zeta = W_e\\zeta_e + W_s\\zeta_s + \\zeta_u,$ (10)\nwhere $W_e$ and $W_s$ are the weights of $\\zeta_e$ and $\\zeta_u$, respectively. We average all final node embeddings in the graph to get the text representation."}, {"title": "4 Experiments", "content": "We conduct experiments on common datasets to evaluate the performance of the SE-GCL method. In this section, we will introduce the data sets and preprocessing, comparison of methods, experiment settings, results, and corresponding analysis."}, {"title": "4.1 Data Sets", "content": "In our experiments, we utilized four diverse datasets to evaluate the performance and robustness of the SE-GCL method. These datasets are:\nAG News\u00b9: This is a dataset of news articles from the AG's corpus of news articles on the web, pertaining to the four largest classes. The dataset contains 30,000 training examples and 1,900 test examples per class.\n20NG2: This dataset is a collection of approximately 20,000 newsgroup documents partitioned across 20 different newsgroups. It is a popular dataset for experiments in text applications of machine learning techniques, such as text classification and text clustering.\nSougouNews\u00b3: The data of SogouNews is compiled by Sogou Lab. It comes from a total of 1,245,835 news reports from 18 Sohu News channels, including domestic, international, sports, social, and entertainment, from June to July 2012. Considering the device factor and balancing the dataset, we randomly sample 3000 entries in each of the ten categories.\nTHUCNews4: The ThuCNews corpus is a news document generated by filtering the historical data of the Sina News RSS subscription"}, {"title": "4.2 Comparison of Methods", "content": "To evaluate the performance of our method, we compare it with different types of text representation learning methods. Covering different types of models ensures that the evaluation is not biased by specific model types, providing more balanced and representative evaluation results. These baseline methods can be divided into three groups, including Word Embedding Based Models, Sequence Deep Learning Models, and Graph Based Representation Learning Models. The selected methods are as follows:\n(1) Word Embedding Based Models\nTF-IDF+LR [61]: Bag-of-words model with word frequency inverse document frequency weighting. Use logistic regression as the classifier.\nfastText [17]: A simple yet efficient method for text classification (Joulin et al. 2017) that treats the average of word/n-gram embeddings as document embeddings and then feeds the document embeddings into a linear classifier.\n(2) Traditional Deep Learning Models\nCNN [18]: CNN is a type of traditional deep learning model that is commonly used for text classification tasks. It uses convolutional layers to learn spatial hierarchies of features from the input data automatically and adaptively.\nBert [13]: It is a transformer-based method that has achieved state-of-the-art results on a wide range of natural language processing tasks. It uses a masked language model objective to pre-train deep bidirectional representations from unlabeled text.\n(3) Graph Based Representation Learning Models\nTextGCN [61]: TextGCN is a graph-based method for text classification that constructs a single, large graph over all documents in the corpus. It then applies a Graph Convolutional Network (GCN) to this graph to learn document representations.\nGAT [51]: It uses attention mechanisms to capture the importance of neighbors in the graph, which has been used for various tasks, including node and graph classification.\nTextING [64]: TextING is a graph-based method for text classification that constructs a text information graph and applies a graph neural network to learn representations.\nDGI [50]: An unsupervised graph embedding algorithm based on mutual information whose goal is to maximize the mutual information between a local representation (patch) and the corresponding graph summary representation (summary)."}, {"title": "4.3 Experimental Setup", "content": "All experiments were conducted using the PyTorch framework, a popular open-source machine-learning library for Python. The experiments were run on a computer equipped with an i7-9700kf CPU and an RTX2080s GPU, ensuring sufficient computational resources for the tasks.\nDuring the model training phase, we trained all models until the loss value converged to ensure optimal results. To account for variability and randomness in the training process, each experiment was repeated ten times using different random seeds. The best precision and F1 scores obtained from each experiment were then averaged to provide the final result.\nFor large-scale datasets, we adopted a mini-batch strategy to address potential out-of-memory issues. This strategy involves dividing the dataset into smaller subsets or 'mini-batches' that are processed independently. This approach not only helps to manage memory usage but also can lead to faster and more stable convergence of the model.\nIn the SE-GCL method, we set the output dimension of each neuron in the hidden layer to 128. The learning rate, a critical parameter that determines the step size at each iteration while moving towards a minimum of a loss function, was set in the range of [0.005, 0.01]. The weight decay, a regularization technique that prevents the weights from growing too large, was set within the range of [0, 0.0001] for all datasets. The regularization factor was set to n = 1e - 6, and the dropout rate was set to 0.4.\nFor all datasets, we allocated 70% of the data for training and the remaining 30% for testing. This split ensures that the models have sufficient data to learn from while providing an independent subset of data to evaluate their performance.\nTo ensure a fair comparison, we used the parameter settings from the original models for all comparative analyses. This ensures that each model is evaluated under its optimal conditions, providing a reliable basis for comparison."}]}