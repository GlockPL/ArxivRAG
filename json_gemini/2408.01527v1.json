{"title": "Analyzing LLMs' Capabilities to Establish Implicit User Sentiment of Software Desirability", "authors": ["Sherri Weitl-Harms", "John D. Hastings", "Jonah Lum"], "abstract": "This study explores the use of several LLMs for providing quantitative zero-shot sentiment analysis of implicit software desirability expressed by users. The study provides scaled numerical sentiment analysis unlike other methods that simply classify sentiment as positive, neutral, or negative. Numerical analysis provides deeper insights into the magnitude of sentiment, to drive better decisions regarding product desirability.\nData is collected through the use of the Microsoft Product Desirability Toolkit (PDT), a well-known qualitative user experience analysis tool. For initial exploration, the PDT metric was given to users of ZORQ, a gamification system used in undergraduate computer science education. The PDT data collected was fed through several LLMs (Claude Sonnet 3 and 3.5, GPT4, and GPT40) and through a leading transfer learning technique, Twitter-Roberta-Base-Sentiment (TRBS), and through Vader, a leading sentiment analysis tool, for quantitative sentiment analysis. Each system was asked to evaluate the data in two ways, first by looking at the sentiment expressed in the PDT word/explanation pairs; and by looking at the sentiment expressed by the users in their grouped selection of five words and explanations, as a whole. Each LLM was also asked to provide its confidence (low, medium, high) in its sentiment score, along with an explanation of why it selected the sentiment value.\nAll LLMs tested were able to statistically detect user sentiment from the users' grouped data, whereas TRBS and Vader were not. The confidence and explanation of confidence provided by the LLMs assisted in understanding the user sentiment. This study adds to a deeper understanding of evaluating user experiences, toward the goal of creating a universal tool that quantifies implicit sentiment expressed.", "sections": [{"title": "I. INTRODUCTION", "content": "In product development, understanding implicit user sen-timent is crucial for creating products that truly appeal totheir intended audience. It is also important for improvingand marketing products. Sentiment analysis, a field dedicatedto automatically extracting emotional reactions expressed byusers [1], holds the potential to provide deeper insights intouser perceptions, satisfaction, and overall product desirability.\nRecent advancements in natural language processing (NLP),particularly the development of large language models(LLMs), have opened new possibilities for sentiment analysis,particular for qualitative data. These models have demon-strated capabilities in various tasks, including zero-shot sentiment classification [2].\nOften, user reviews or expressions on social media areused as a basis for sentiment analysis. Understanding implicitsentiment, a common linguistic phenomenon, where accuratejudgment often requires common sense or domain knowledge,is challenging [2]. Quantifying implicit user sentiment remainsa significant challenge, particularly when explicit user ratingsor reviews are unavailable [3].\nTo address situations in which sentiment data is lacking,tools such as surveys and the Microsoft Product DesirabilityToolkit (PDT) can be used to evaluate user experiences. ThePDT [4], [5] is recognized as a valuable qualitative tool forevaluating user experience and satisfaction.\nThis research aims to bridge the gap between qualitativesentiment data and quantitative analysis by applying recentLLMs to PDT data. The goal is to derive meaningful quan-titative insights from qualitative user responses. In addition,by comparing LLMs with existing approaches, the studyseeks to identify the most effective methods for quantifyingimplicit user sentiment in software desirability evaluations.The following research questions guide the study:\n1) How do various LLMs compare to known sentimentanalysis tools in providing sentiment analysis scores ofPDT data?\n2) Is there a particular algorithm (or combination of algo-rithms) that outperforms others and should be used as thebasis for the development of an implicit user sentimentanalysis tool?\nThe sections that follow provide background informationon the PDT and sentiment analysis techniques, detail themethodology, present results, discuss their implications, andoutline directions for future work."}, {"title": "II. BACKGROUND", "content": "A. Product Desirability Toolkit\nThe PDT is a well-known qualitative analysis tool used toevaluate user experience and satisfaction with products, suchas software [6]\u2013[13]. It aims to \u201cunderstand the illusive, intan-gible aspect of desirability resulting from a user\u2019s experiencewith a product\u201d [7].\nThe PDT asks users to select five adjectives from a givenset that best describe their feelings about the experiencealong with providing an optional explanation of their wordchoices. By gathering this group of word/explanation pairs,"}, {"title": "B. Sentiment Analysis", "content": "Sentiment analysis, a field dedicated to automatically ex-tracting emotional reactions expressed by users [1], holdsbroad applications, from influencing policy decisions, refiningproduct development [15], managing brand reputation, andhandling crisis communication [16]\u2013[18]. Businesses derivevalue from sentiment analysis for market research, understand-ing user perception, and customer satisfaction [17], [19].\nSentiment analysis employs NLP, statistical methods, andmachine learning algorithms to identify trends and patternsamong attitudes, emotions, and opinions expressed in text,subsequently classifying them into categories or sentimentscores [17], [20]\u2013[22]. Quantifying implicit user sentiment ischallenging, but important for understanding user experiences.\nTraditional sentiment analysis techniques often rely onlexicon-based approaches or machine learning models trainedon labeled data. However, these methods can struggle withcontext-dependent sentiments and implicit expressions of userfeelings [23], which are common in PDT responses."}, {"title": "C. LLMs in Sentiment Analysis", "content": "LLMs have shown promising results in various NLP tasks,including their ability to guage sentiment effectively [17],[24]\u2013[27]. LLMs, such as GPT models, are trained on vastamounts of text data and can generate human-like text basedon input prompts. As of late 2023, GPT (gpt-3.5-turbo-0301)demonstrated impressive zero-shot capabilities in sentimentclassification tasks, and could serve as a universal and well-behaved sentiment analyzer [2]. Generative AI is pioneered inzero-shot content analysis, such as automated textual analysis[25]. However, previous work found that LLMs generallyperform poorly on implicit sentiment analysis and domain-specific training was needed to improve performance [2]. Theapplication of LLMs to implicit sentiment analysis offers newpossibilities for understanding user experiences and productdesirability. Their ability to understand context and capturenuanced and implicit sentiments could potentially overcomesome of the limitations of traditional sentiment analysis tech-niques when applied to PDT data."}, {"title": "III. METHODS", "content": "This research experiment focuses on sentiment analysisof PDT survey datasets, each containing five words andexplanations from respondents. The goal is to evaluatethe effectiveness of various sentiment analysis technologies, par-ticularly LLMs, on this data. Unlike traditional methods thatclassify sentiment into three categories (positive, neutral, andnegative) as seen in [25], this study employs a scaled numericalsentiment analysis ranging from 0 to 1 (with 0 being themost negative and 1 the most positive). While numericalanalysis is more challenging, it offers significant advantagesby providing more detail and insights into the magnitudeof sentiment, which can help decision makers make betterdecisions regarding product desirability.\nA. Data Collection\nThe data for this study was previously collected [13] usingPDT from users of ZORQ [28], a gamification frameworkutilized in undergraduate computer science education in whichspace ships navigate a 2D game universe. The PDT datacollected utilizing the same set of 55 words as Barnum etal. [6] and shown in the original article [4]. Fifty sets of PDTRespondent Term Groupings (PRTG) were collected, whereeach PRTG is the group of five word/explanation pairs for onerespondent. In the dataset, 15 of the 50 users did not provideexplanations for the terms selected.\nPrior to analysis, the raw survey data was processed tocreate a more concise dataset in which each row consistedof a word choice and its corresponding explanation. The datawere cleaned to remove any inconsistencies or errors. Thisincluded checking for and addressing issues such as missingvalues, mismatched quotes, and non-text characters.\nB. Data Labeling\nTo establish a gold standard for evaluation, the authorsperformed manual data labeling. Many studies use LLMssuch as GPT4 for data labeling [29]. However, a baselineof manually labeled test data is necessary to understandthe accuracy of LLM-based labeling. In-house manual datalabeling secures the highest quality labeling possible and isgenerally considered the gold standard by data scientists andengineers [30]. It is a common method, especially for smalldata sets that require expertise that cannot be easily crowd-sourced [30] and to provide a baseline of labeled test data [2],[29], [31]. He et al. [29] found that GPT4 itself had higheraccuracy than crowd-sourcing (particularly Amazon Mechan-ical Turk (MTurk) workers pipeline [32]), highlighting the"}, {"title": "C. Sentiment Analysis Tools and Methods", "content": "This research explores the sentiment analysis performanceon PDT data of several LLMS: GPT4 [33] and GPT40 [34],Claude Sonnet 3 and 3.5 [35], along with Twitter-Roberta-Base-Sentiment (TRBS) [36], and Vader (Valence Aware Dic-tionary and sentiment Reasoner) [37]. The latter two werechosen for their established effectiveness in sentiment analysis tasks and their differing approaches, which provide a usefulcomparison to the LLMs. TRBS is a pre-trained model fine-tuned on Twitter data for sentiment analysis, based on theROBERTa architecture, and is a leading transfer learning tech-nique. Vader, a long standing tool in this domain, combinesa robust lexicon with heuristic rules for contextual nuance,making it user-friendly and highly accurate [36]. It is designedto perform well on social media text but effective across othertext forms as well.\nTwo approaches were attempted for running PDT datathrough the tools to produce PRTG-level scores: 1) havingthe tool produce sentiments scores for each of the fiveword/explanation pairs for a respondent and then manuallycalculating an average (referred to as 'Avg5' tests), and 2)having the tool produce one overall sentiment score forall of the word/explanation pairs for a respondent (referredto as 'Respondent' tests). From the potential combinations(tool \u00d7 scoringapproach), we ran the following tests:\n1) Claude3-Avg5\n2) Claude3.5-Respondent\n3) GPT4-Avg5\n4) GPT4-Respondent\n5) GPT4o-Avg5\n6) GPT40-Respondent\n7) TRBS-Avg5\n8) TRBS-Respondent\n9) Vader-Avg5\n10) Vader-Respondent\nFor the LLMs, the associated prompts appear in Table I.The prompts instruct the LLM to score the PDT data andprovide an explanation. Except for Claude3, all LLMs werealso asked to provide a confidence level (low, medium, high)for their scoring. These prompts were initially developedand refined through the web interfaces for Claude and GPT.The tests for Claude3-Avg5 and Claude3.5-Respondent wererun through the web interface by uploading a CSV file ofword/explanation pairs to the web interface, while the otherprompts were processed through the GPT API. For GPT4o-Avg5 and GPT40-Respondent, three runs were conducted andthe results were averaged.\nFor Avg5 scoring with TRBS and Vader, words and expla-nations were scored separately and averaged. For respondent-level scoring with TRBS and Vader, the word/explanation pairswere concatenated with a separating period and space \". \", andthe five pairs for the respondent were joined in the same waybefore being processed by the tool."}, {"title": "D. Method Evaluation", "content": "To assess each tool's performance, well-established metricsfor numerical data analysis, such as Pearson Coefficients (PC),Mean Squared Error (MSE) and Mean Absolute Error (MAE)are employed. These metrics provide a comprehensive analysisof sentiment classification, allowing for a thorough evaluationof the system's effectiveness [15]. Additionally, the Wilcoxonstatistical test [38] as well as the paired t-test are used, with thenull hypothesis that the mean difference between algorithmicresults and the gold standard is zero. The Wilcoxon test isparticularly useful when dealing small paired sets of datawhich do not follow a normal distribution."}, {"title": "IV. RESULTS", "content": "A. Overall Results\nResults are shown in Table II, with tools listed by decreasingorder of PC. All strong values are highlighted, and the GPT40-Respondent approach produced the best statistically significantresults, closely followed by Claude3-Avg5. The GPT4o-Avg5results had strong MAE, MSE, and PC results, but were notstatistically matched at the \u03b1 = .05 level. The other LLMs ran"}, {"title": "B. LLM Confidence", "content": "All GPT4o runs expressed high confidence for 36 andmedium on three PRTGs. On average, GPT40 expressed highconfidence on 42 of the 50 PRTGs, and medium confidenceon eight. GPT40 did not express low confidence on any PRTGduring any run. Claude3.5 expressed high confidence on 31,medium on 8, and low on 11 PRTGs. GPT4 expressed highconfidence on 42, and medium on eight PRTGs.\nAn example where GPT40 (0.82) and Claude3.5 (0.90)expressed high confidence for a PRTG with positive sentiment(0.90) was explained by GPT4o as: \u201cThe overall sentimentof the words is very positive, indicating that the projectwas seen as accessible, comprehensive, enjoyable, useful,and valuable. The explanations provided further strengthenthe positive sentiment by highlighting how the project wasorganized, enjoyable, and educational.\u201d. Claude3.5 explained:\"This respondent's feedback is overwhelmingly positive. Theydescribe the project as 'Accessible', \u2018Comprehensive', 'Fun',\u2018Useful\u2019, and 'Valuable'. Their explanations emphasize howenjoyable and beneficial the project was, even calling it one oftheir favorite projects. The high score reflects the consistentlypositive feedback with no negative aspects mentioned.\"\nAn example where GPT40 (0.09) and Claude3.5 (0.20)expressed high confidence for a negative sentiment datum wasfor a PRTG with sentiment of 0.20. One GPT4o explanationwas: \"The overall sentiment is overwhelmingly negative due toconsistent complaints about complexity, frustration, difficulty,stress, and the time-consuming nature of the task, all relatedto using an IDE that performed poorly.\u201d. Claude3.5 explained\"This respondent's feedback is predominantly negative. Theydescribe the project as 'Complex', 'Frustrating', 'Hard touse', 'Stressful', and \u2018Time-consuming'. Their explanationhighlight difficulties with the IDE, lack of code comments, andunclear expectations. The low score reflects the consistentlynegative feedback with no positive aspects mentioned.\"\nConsistent medium confidence was exampled on a PRTGwith sentiment 0.50, where GPT40 averaged 0.54 andClaude3.5 rated 0.50. A GPT4o explanation was: \u201cThe wordsused, such as 'complex', 'confusing', and 'time-consuming',have negative connotations, balanced by more neutral orpositive words like 'comprehensive' and 'fun'.\u201d However, theexplanations elucidate that while challenges existed, therewas also significant learning and sometimes enjoyment, con-tributing to a generally moderate sentiment score.\" Claude3.5explained: \"This respondent's feedback is mixed. While theyfound the project 'Fun' and 'Comprehensive', they also described it as 'Complex', 'Confusing', and \u2018Time-consuming'.They appreciated the challenge but noted the difficulty in un-derstanding the program initially. The moderate score reflectsthis balance of positive and negative aspects, with the funfactor slightly outweighing the challenges.\u201d\nOn average, GPT4o expressed high confidence in its senti-ment score for 200 of the 250 word/explanation pairs; mediumfor 47 pairs, and low confidence for 3 pairs. For 145 pairs, allthree runs expressed high confidence in its score, and matchedmedium confidence for 10 pairs. Only run2 had any lowconfidence ratings, and only for two sets of respondent datawhere the user had not provided an explanation for the wordsselected. On those cases, the algorithm explained \u201cPositive butno context provided to adjust.\u201d"}, {"title": "C. Individual Word-Explanation Pair Results", "content": "The 250 individual word-explanation pairs were also evalu-ated through the algorithms (without considering the respon-dent source), with the hypothesis that the mean differencebetween algorithmic results and the gold standard for the datais zero. The PC values for the algorithms are Claude3 0.93,GPT40 0.93, GPT4 0.85, TRBS 0.80, and Vader 0.64. Thepaired sample t-test and Wilcoxon test failed for all of thealgorithms. The results of the algorithmic distribution on the250 individual word-explanation pairs are shown in Figure 2."}, {"title": "V. DISCUSSION AND FUTURE WORK", "content": "The results presented in Table II demonstrate that the LLM"}]}