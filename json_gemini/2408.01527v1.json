{"title": "Analyzing LLMs' Capabilities to Establish Implicit User Sentiment of Software Desirability", "authors": ["Sherri Weitl-Harms", "John D. Hastings", "Jonah Lum"], "abstract": "This study explores the use of several LLMs for pro- viding quantitative zero-shot sentiment analysis of implicit soft- ware desirability expressed by users. The study provides scaled numerical sentiment analysis unlike other methods that simply classify sentiment as positive, neutral, or negative. Numerical analysis provides deeper insights into the magnitude of sentiment, to drive better decisions regarding product desirability. Data is collected through the use of the Microsoft Product Desirability Toolkit (PDT), a well-known qualitative user ex- perience analysis tool. For initial exploration, the PDT metric was given to users of ZORQ, a gamification system used in undergraduate computer science education. The PDT data collected was fed through several LLMs (Claude Sonnet 3 and 3.5, GPT4, and GPT40) and through a leading transfer learning technique, Twitter-Roberta-Base-Sentiment (TRBS), and through Vader, a leading sentiment analysis tool, for quantitative sentiment analysis. Each system was asked to evaluate the data in two ways, first by looking at the sentiment expressed in the PDT word/explanation pairs; and by looking at the sentiment expressed by the users in their grouped selection of five words and explanations, as a whole. Each LLM was also asked to provide its confidence (low, medium, high) in its sentiment score, along with an explanation of why it selected the sentiment value. All LLMs tested were able to statistically detect user sentiment from the users' grouped data, whereas TRBS and Vader were not. The confidence and explanation of confidence provided by the LLMs assisted in understanding the user sentiment. This study adds to a deeper understanding of evaluating user experiences, toward the goal of creating a universal tool that quantifies implicit sentiment expressed.", "sections": [{"title": "I. INTRODUCTION", "content": "In product development, understanding implicit user sen- timent is crucial for creating products that truly appeal to their intended audience. It is also important for improving and marketing products. Sentiment analysis, a field dedicated to automatically extracting emotional reactions expressed by users [1], holds the potential to provide deeper insights into user perceptions, satisfaction, and overall product desirability. Recent advancements in natural language processing (NLP), particularly the development of large language models (LLMs), have opened new possibilities for sentiment analysis, particular for qualitative data. These models have demon- strated capabilities in various tasks, including zero-shot senti- ment classification [2]. Often, user reviews or expressions on social media are used as a basis for sentiment analysis. Understanding implicit sentiment, a common linguistic phenomenon, where accurate judgment often requires common sense or domain knowledge, is challenging [2]. Quantifying implicit user sentiment remains a significant challenge, particularly when explicit user ratings or reviews are unavailable [3]. To address situations in which sentiment data is lacking, tools such as surveys and the Microsoft Product Desirability Toolkit (PDT) can be used to evaluate user experiences. The PDT [4], [5] is recognized as a valuable qualitative tool for evaluating user experience and satisfaction. This research aims to bridge the gap between qualitative sentiment data and quantitative analysis by applying recent LLMs to PDT data. The goal is to derive meaningful quan- titative insights from qualitative user responses. In addition, by comparing LLMs with existing approaches, the study seeks to identify the most effective methods for quantifying implicit user sentiment in software desirability evaluations. The following research questions guide the study: 1) How do various LLMs compare to known sentiment analysis tools in providing sentiment analysis scores of PDT data? 2) Is there a particular algorithm (or combination of algo- rithms) that outperforms others and should be used as the basis for the development of an implicit user sentiment analysis tool? The sections that follow provide background information on the PDT and sentiment analysis techniques, detail the methodology, present results, discuss their implications, and outline directions for future work."}, {"title": "II. BACKGROUND", "content": "A. Product Desirability Toolkit The PDT is a well-known qualitative analysis tool used to evaluate user experience and satisfaction with products, such as software [6]\u2013[13]. It aims to \"understand the illusive, intan- gible aspect of desirability resulting from a user's experience with a product\" [7]. The PDT asks users to select five adjectives from a given set that best describe their feelings about the experience along with providing an optional explanation of their word choices. By gathering this group of word/explanation pairs, the approach is designed to capture rich, qualitative data about user experiences and perceptions. The advantages of using the PDT are \u201c1) it aims to avoid a bias toward the positive found in typical questionnaires (e.g., it has been found that if a respondent thinks that a survey intends to assess the quality of a product, they are likely to provide more positive answers about quality) and 2) it is able to more effectively uncover constructive negative criticisms in the guided interview\" [9]. The PDT is described as the closest tool that uses \"psy- chometric theory to create a user experience (UX)-relevant measure of product or service desirability\" [14]. The design of the PDT prompts users to tell a revealing story of their experience as users comment on their word choice [6] and provides a rich set of qualitative data related to the user's implicit desirability of the product in question. While the PDT is a great qualitative tool, it is a poor quantitative tool by itself [14]. The PDT provides a way to triangulate findings from other feedback mechanisms, with po- tential to produce more meaningful and substantive results of user experiences [6], but it necessary to think of improvements on the original method [12]. The PDT text from respondent word/comments groupings are ripe for sentiment analysis. B. Sentiment Analysis Sentiment analysis, a field dedicated to automatically ex- tracting emotional reactions expressed by users [1], holds broad applications, from influencing policy decisions, refining product development [15], managing brand reputation, and handling crisis communication [16]\u2013[18]. Businesses derive value from sentiment analysis for market research, understand- ing user perception, and customer satisfaction [17], [19]. Sentiment analysis employs NLP, statistical methods, and machine learning algorithms to identify trends and patterns among attitudes, emotions, and opinions expressed in text, subsequently classifying them into categories or sentiment scores [17], [20]\u2013[22]. Quantifying implicit user sentiment is challenging, but important for understanding user experiences. Traditional sentiment analysis techniques often rely on lexicon-based approaches or machine learning models trained on labeled data. However, these methods can struggle with context-dependent sentiments and implicit expressions of user feelings [23], which are common in PDT responses. C. LLMs in Sentiment Analysis LLMs have shown promising results in various NLP tasks, including their ability to guage sentiment effectively [17], [24]\u2013[27]. LLMs, such as GPT models, are trained on vast amounts of text data and can generate human-like text based on input prompts. As of late 2023, GPT (gpt-3.5-turbo-0301) demonstrated impressive zero-shot capabilities in sentiment classification tasks, and could serve as a universal and well- behaved sentiment analyzer [2]. Generative AI is pioneered in zero-shot content analysis, such as automated textual analysis [25]. However, previous work found that LLMs generally perform poorly on implicit sentiment analysis and domain- specific training was needed to improve performance [2]. The application of LLMs to implicit sentiment analysis offers new possibilities for understanding user experiences and product desirability. Their ability to understand context and capture nuanced and implicit sentiments could potentially overcome some of the limitations of traditional sentiment analysis tech- niques when applied to PDT data."}, {"title": "III. METHODS", "content": "This research experiment focuses on sentiment analysis of PDT survey datasets, each containing five words and explanations from respondents. The goal is to evaluate the effectiveness of various sentiment analysis technologies, par- ticularly LLMs, on this data. Unlike traditional methods that classify sentiment into three categories (positive, neutral, and negative) as seen in [25], this study employs a scaled numerical sentiment analysis ranging from 0 to 1 (with 0 being the most negative and 1 the most positive). While numerical analysis is more challenging, it offers significant advantages by providing more detail and insights into the magnitude of sentiment, which can help decision makers make better decisions regarding product desirability. A. Data Collection The data for this study was previously collected [13] using PDT from users of ZORQ [28], a gamification framework utilized in undergraduate computer science education in which space ships navigate a 2D game universe. The PDT data collected utilizing the same set of 55 words as Barnum et al. [6] and shown in the original article [4]. Fifty sets of PDT Respondent Term Groupings (PRTG) were collected, where each PRTG is the group of five word/explanation pairs for one respondent. In the dataset, 15 of the 50 users did not provide explanations for the terms selected. Prior to analysis, the raw survey data was processed to create a more concise dataset in which each row consisted of a word choice and its corresponding explanation. The data were cleaned to remove any inconsistencies or errors. This included checking for and addressing issues such as missing values, mismatched quotes, and non-text characters. B. Data Labeling To establish a gold standard for evaluation, the authors performed manual data labeling. Many studies use LLMs such as GPT4 for data labeling [29]. However, a baseline of manually labeled test data is necessary to understand the accuracy of LLM-based labeling. In-house manual data labeling secures the highest quality labeling possible and is generally considered the gold standard by data scientists and engineers [30]. It is a common method, especially for small data sets that require expertise that cannot be easily crowd- sourced [30] and to provide a baseline of labeled test data [2], [29], [31]. He et al. [29] found that GPT4 itself had higher accuracy than crowd-sourcing (particularly Amazon Mechan- ical Turk (MTurk) workers pipeline [32]), highlighting the unreliable quality in crowd-sourced labels; further supporting the necessity for manual in-house labeling in this study. The three authors, with over 60 years of combined software development experience, have the necessary expertise to man- ually annotated the dataset to create a gold-standard to measure the LLMs. The annotation was completed in two ways: 1) by assigning an overall sentiment score for each user's PRTG, and 2) by scoring each individual term and explanation pair. The inter-annotator agreement (Pearson's coefficients) between the three annotators for the ZORQ PRTGs was (0.92, 0.96, and 0.96) and (0.88, 0.89, 0.89) for the word/explanation pairs. The authors expect product usability sentiment to likely be skewed negatively or positively, rather than following a normal distribution. Based on the manual data labeling, the ZORQ PDT dataset had an average overall gold-standard sentiment rating of 0.76, for both the PRTGs and the word/explanation pairs, with standard deviations of 0.26 for the word/explanation pairs, and 0.19 for the PRTGs; showing that the data is skewed in the positive direction, and not normally distributed. C. Sentiment Analysis Tools and Methods This research explores the sentiment analysis performance on PDT data of several LLMS: GPT4 [33] and GPT40 [34], Claude Sonnet 3 and 3.5 [35], along with Twitter-Roberta- Base-Sentiment (TRBS) [36], and Vader (Valence Aware Dic- tionary and sentiment Reasoner) [37]. The latter two were chosen for their established effectiveness in sentiment analysis tasks and their differing approaches, which provide a useful comparison to the LLMs. TRBS is a pre-trained model fine- tuned on Twitter data for sentiment analysis, based on the ROBERTa architecture, and is a leading transfer learning tech- nique. Vader, a long standing tool in this domain, combines a robust lexicon with heuristic rules for contextual nuance, making it user-friendly and highly accurate [36]. It is designed to perform well on social media text but effective across other text forms as well. Two approaches were attempted for running PDT data through the tools to produce PRTG-level scores: 1) having the tool produce sentiments scores for each of the five word/explanation pairs for a respondent and then manually calculating an average (referred to as 'Avg5' tests), and 2) having the tool produce one overall sentiment score for all of the word/explanation pairs for a respondent (referred to as 'Respondent' tests). From the potential combinations (tool \u00d7 scoringapproach), we ran the following tests: 1) Claude3-Avg5 2) Claude3.5-Respondent 3) GPT4-Avg5 4) GPT4-Respondent 5) GPT4o-Avg5 6) GPT40-Respondent 7) TRBS-Avg5 8) TRBS-Respondent 9) Vader-Avg5 10) Vader-Respondent For the LLMs, the associated prompts appear in Table I. The prompts instruct the LLM to score the PDT data and provide an explanation. Except for Claude3, all LLMs were also asked to provide a confidence level (low, medium, high) for their scoring. These prompts were initially developed and refined through the web interfaces for Claude and GPT. The tests for Claude3-Avg5 and Claude3.5-Respondent were run through the web interface by uploading a CSV file of word/explanation pairs to the web interface, while the other prompts were processed through the GPT API. For GPT40- Avg5 and GPT40-Respondent, three runs were conducted and the results were averaged. For Avg5 scoring with TRBS and Vader, words and expla- nations were scored separately and averaged. For respondent- level scoring with TRBS and Vader, the word/explanation pairs were concatenated with a separating period and space \". \", and the five pairs for the respondent were joined in the same way before being processed by the tool. D. Method Evaluation To assess each tool's performance, well-established metrics for numerical data analysis, such as Pearson Coefficients (PC), Mean Squared Error (MSE) and Mean Absolute Error (MAE) are employed. These metrics provide a comprehensive analysis of sentiment classification, allowing for a thorough evaluation of the system's effectiveness [15]. Additionally, the Wilcoxon statistical test [38] as well as the paired t-test are used, with the null hypothesis that the mean difference between algorithmic results and the gold standard is zero. The Wilcoxon test is particularly useful when dealing small paired sets of data which do not follow a normal distribution."}, {"title": "IV. RESULTS", "content": "A. Overall Results Results are shown in Table II, with tools listed by decreasing order of PC. All strong values are highlighted, and the GPT40- Respondent approach produced the best statistically significant results, closely followed by Claude3-Avg5. The GPT4o-Avg5 results had strong MAE, MSE, and PC results, but were not statistically matched at the \u03b1 = .05 level. The other LLMs ran at the respondent level (Claude3.5 and GPT4) were statistically significant. The three GPT40 respondent-level runs were statistically matched, and had between-run PC values of 0.90, 0.92, and 0.92. For the three GPT4o Avg5 runs, the t-stats for run\u2081 and run2 means were not statistically matched, but the other runs had between-run t-stats that were significant. These runs had between-run PC values of 0.85, 0.82, and 0.95. B. LLM Confidence All GPT4o runs expressed high confidence for 36 and medium on three PRTGs. On average, GPT4o expressed high confidence on 42 of the 50 PRTGs, and medium confidence on eight. GPT40 did not express low confidence on any PRTG during any run. Claude3.5 expressed high confidence on 31, medium on 8, and low on 11 PRTGs. GPT4 expressed high confidence on 42, and medium on eight PRTGs. An example where GPT40 (0.82) and Claude3.5 (0.90) expressed high confidence for a PRTG with positive sentiment (0.90) was explained by GPT4o as: \u201cThe overall sentiment of the words is very positive, indicating that the project was seen as accessible, comprehensive, enjoyable, useful, and valuable. The explanations provided further strengthen the positive sentiment by highlighting how the project was organized, enjoyable, and educational.\u201d. Claude3.5 explained: \"This respondent's feedback is overwhelmingly positive. They describe the project as 'Accessible', \u2018Comprehensive', 'Fun', 'Useful', and 'Valuable'. Their explanations emphasize how enjoyable and beneficial the project was, even calling it one of their favorite projects. The high score reflects the consistently positive feedback with no negative aspects mentioned.\" An example where GPT40 (0.09) and Claude3.5 (0.20) expressed high confidence for a negative sentiment datum was for a PRTG with sentiment of 0.20. One GPT4o explanation was: \"The overall sentiment is overwhelmingly negative due to consistent complaints about complexity, frustration, difficulty, stress, and the time-consuming nature of the task, all related to using an IDE that performed poorly.\u201d. Claude3.5 explained \"This respondent's feedback is predominantly negative. They describe the project as 'Complex', 'Frustrating', 'Hard to use', 'Stressful', and \u2018Time-consuming'. Their explanations highlight difficulties with the IDE, lack of code comments, and unclear expectations. The low score reflects the consistently negative feedback with no positive aspects mentioned.\" Consistent medium confidence was exampled on a PRTG with sentiment 0.50, where GPT4o averaged 0.54 and Claude3.5 rated 0.50. A GPT4o explanation was: \u201cThe words used, such as 'complex', 'confusing', and 'time-consuming', have negative connotations, balanced by more neutral or positive words like 'comprehensive' and 'fun'.\u201d However, the explanations elucidate that while challenges existed, there was also significant learning and sometimes enjoyment, con- tributing to a generally moderate sentiment score.\" Claude3.5 explained: \"This respondent's feedback is mixed. While they found the project 'Fun' and 'Comprehensive', they also de- scribed it as 'Complex', 'Confusing', and \u2018Time-consuming'. They appreciated the challenge but noted the difficulty in un- derstanding the program initially. The moderate score reflects this balance of positive and negative aspects, with the fun factor slightly outweighing the challenges.\" On average, GPT4o expressed high confidence in its senti- ment score for 200 of the 250 word/explanation pairs; medium for 47 pairs, and low confidence for 3 pairs. For 145 pairs, all three runs expressed high confidence in its score, and matched medium confidence for 10 pairs. Only run2 had any low confidence ratings, and only for two sets of respondent data where the user had not provided an explanation for the words selected. On those cases, the algorithm explained \u201cPositive but no context provided to adjust.\u201d C. Individual Word-Explanation Pair Results The 250 individual word-explanation pairs were also evalu- ated through the algorithms (without considering the respon- dent source), with the hypothesis that the mean difference between algorithmic results and the gold standard for the data is zero. The PC values for the algorithms are Claude3 0.93, GPT40 0.93, GPT4 0.85, TRBS 0.80, and Vader 0.64. The paired sample t-test and Wilcoxon test failed for all of the algorithms. V. DISCUSSION AND FUTURE WORK The results presented in Table II demonstrate that the LLMS performed well to quantify implicit user sentiment of PDT data using both respondent-level and Avg5 scoring. This research is an initial step toward the creation of a tool for a broad user-base designed to provide rich quantitative sentiment analysis of implicit product desirability, especially in situations where no user rating system exists. Based on the results, either GPT40 or Claude3.5 or a combination of both would serve well as the basis for this tool. The small amount of text in the word/explanation pairs may have made it challenging for the tools to match sentiment, as noted by Hartmann et al. [39]. However, Claude3 and GPT4 were statistically significant in their match with the gold standard, with GPT4o having good MAE, MSE, PC values using Avg5 scoring, and all were statistically significant with respondent scoring. The results indicate that these LLMs have been able to overcome the challenge of conducting sentiment analysis on short snippets of text. Further work is needed to investigate the impact of data length on LLM performance. Table II also provides the sentiment distributions from strongly negative (x < 0.2) to strongly positive (x >= 0.80), for each tool. These values show the strength of the LLMs, as compared with the other tools, as do the MSE, MAE, and Avg values. A deeper exploration of the approaches for evaluating software desirability used in this paper as a generalized methodology is needed. One area of exploration is the algorithms' ability to match at a given context-level. The study also found that the PDT data, especially at the respondent level, provided enough data for LLMs to perform well, without domain-specific training. As a note on the applicability of the PDT as a survey tool for use in studying the effectiveness of gamification applications, it was quick and easy to construct and distribute and has a strong foundation in software product evaluation [9]. Further exploration on other PDT datasets would add verification and support. The confidence and explanations expressed by the LLMs add value in understanding user sentiment. For example, when GPT40, GPT4, and Claude3.5 agree in high confidence on their ratings, the authors' confidence in the rating increases, whereas medium or low confidence suggests the potential need for human review. Because the LLMs expressed high confidence a vast majority of the time (GPT4o and GPT4 84%; Claude3.5 62%), human review is likely only needed for a few cases. Further exploration is needed for confirmation. During initial prompt development, Claude3 was provided PDT data in an expansive spreadsheet exported from Qualtrics. Despite the relatively small size (250 rows), it overwhelmed Claude3, preventing it from producing desired behavior. For resolution, data was preprocessed into word/explanation pairs, allowing Claude to focus on the concisely formatted text. In addition, the order of data might be important in some cases. For example, with Avg5 scoring, Claude3 generated consistent base word scores only when pairs were sorted alphabetically by word, effectively grouping word/explanation pairs for the same word together. This observation could be relevant for other LLMs, and further work is needed to determine the impact of PDT data order on scoring. During initial prompt development, Claude3 demonstrated obvious sentiment analysis capabilities. In contrast, GPT 4 through the web interface struggled to produce consistent behavior. Requesting a sentiment score seemed to appeared to help with GPT4's behavior. Thus, starting with GPT4, a confidence score was also produced. Collectively, a confidence score and an explanation of scoring provided by the LLMs helps in understanding user sentiment. Future work could investigate the quantitative effect on sentiment accuracy of prompting for a confidence score. A final observation is that GPT4 through the web interface was cumbersome, and even through the API it had challenges."}, {"title": "VI. CONCLUSION", "content": "This study adds to a deeper understanding of evaluating user experiences. It explores the use of several LLMs (Claude Son- net 3 and 3.5, GPT4, and GPT40) along with TRBS and Vader on a set of PDT data, for providing quantitative numerical zero-shot sentiment analysis of implicit software desirability expressed by users. All LLM tools outperformed the other approaches and were statistically significant in performing as zero-shot sentiment analyzers on the PDT data. The confidence and explanation of confidence provided by the LLMs assist in understanding the user sentiment."}]}