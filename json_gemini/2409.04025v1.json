{"title": "BFA-YOLO: Balanced multiscale object detection network for multi-view building facade attachments detection", "authors": ["Yangguang Chen", "Tong Wang", "Guanzhou Chen", "Kun Zhu", "Xiaoliang Tan", "Jiaqi Wang", "Hong Xie", "Wenlin Zhou", "Jingyi Zhao", "Qing Wang", "Xiaolong Luo", "Xiaodong Zhang"], "abstract": "Detection of building facade attachments such as doors, windows, balconies, air conditioner units, billboards, and glass curtain walls plays a pivotal role in numerous applications. Building facade attachments detection aids in vbuilding information modeling (BIM) construction and meeting Level of Detail 3 (LOD3) standards. Yet, it faces challenges like uneven object distribution, small object detection difficulty, and background interference. To counter these, we propose BFA-YOLO, a model for detecting facade attachments in multi-view images. BFA-YOLO incorporates three novel innovations: the Feature Balanced Spindle Module (FBSM) for addressing uneven distribution, the Target Dynamic Alignment Task Detection Head (TDATH) aimed at improving small object detection, and the Position Memory Enhanced Self-Attention Mechanism (PMESA) to combat background interference, with each component specifically designed to solve its corresponding challenge. Detection efficacy of deep network models deeply depends on the dataset's characteristics. Existing open source datasets related to building facades are limited by their single perspective, small image pool, and incomplete category coverage. We propose a novel method for building facade attachments detection dataset construction and construct the BFA-3D dataset for facade attachments detection. The BFA-3D dataset features multi-view, accurate labels, diverse categories, and detailed classification. BFA-YOLO surpasses YOLOv8 by 1.8% and 2.9% in mAP@0.5 on the multi-view BFA-3D and street-view Facade-WHU datasets, respectively. These results underscore BFA-YOLO's superior performance in detecting facade attachments.", "sections": [{"title": "1. Introduction", "content": "Buildings play a pivotal role in urban settings, with their applications enhancing daily living, industrial processes, and public services (Binns et al., 2018; Rapoport, 1982). The detection of building facade attachments (e.g. doors, windows, balconies, air conditioner units, billboards, glass curtain walls) has a wide range of applications in downstream tasks (Durmus et al., 2022; Yang et al., 2022; zuway and Farkash, 2022). Buildings facade research finds key applications in smart city technologies, heritage conservation, precision navigation, and energy simulation, driving industry advancements (Apanaviciene et al., 2020; Nestic\u00f2 and Somma, 2019; Ribera et al., 2020; Jiang et al., 2021; Feng et al., 2020; V\u00e1zquez-Canteli et al., 2019). Detection of building facade attachments is critical for enhancing Building Information Modeling (BIM) and ensuring compliance with Level of Detail 3 (LOD3) standards during construction, providing substantial support for urban design, and also supporting the identification and repair of defects in the 3D model (Dore and Murphy, 2014; Biljecki et al., 2016; Wang et al., 2023; Becker, 2009; Arvanitis et al., 2022)."}, {"title": "2. Materials and Methods", "content": ""}, {"title": "2.1. Datasets", "content": "To detect building facade attachments from various viewpoints, this study generates the BFA-3D dataset by simulating scenes from different angles, using 1240 images (1200 \u00d7 1200 pixels) rendered from 3D models. The rendering strategy is depicted in Figure 1. In the horizontal dimension, we designed a fine-grained rotation strategy. The camera was rotated at fixed positions in 60\u00b0 intervals from 0\u00b0 to 300\u00b0, with each position offering a unique viewing direction. This multi-angle rotation strategy enriches viewpoint diversity in the dataset and enables more comprehensive feature learning by the model, enhancing its performance in complex scene detection. In the vertical dimension, we introduced a random camera tilt angle variation. Simulating real-world observation, the camera was randomly tilted downward from 0 to 30\u00b0. This tilting strategy not only diversifies the dataset but also reveals more facade details in the rendered images, offering valuable information for detecting attachments.\nTo accurately annotate the 1,240 images of building elevations captured from diverse viewpoints, we initially utilized the ISAT tool alongside the Segment Anything large model to efficiently generate masks (Ji and Zhang, 2023). Subsequently, these masks were transformed into bounding boxes, determined by the masks' maximum enclosing rectangles. In the domain of building facade attachment classification, we identified six primary categories: doors, windows, balconies, air conditioner units, billboards, and glass curtain walls. Furthermore, to account for varying installation styles of windows on facades, our classification was refined to differentiate windows set within walls from those extending outward. The classification and the item count for each category within the BFA-3D dataset are meticulously detailed in Tables 2. To guarantee the dataset's annotation quality, three annotators were tasked with the annotation process, emphasizing accuracy and consistency. Figure 2 illustrates the annotation procedure. In instances of category discrepancy between annotators 1 (P1) and 2 (P2), a third annotator (P3) was consulted to finalize category decisions. For discrepancies concerning the positioning of wireframes by Annotators 1 (P1) and 2 (P2), we calculated the average position of the wireframes' centers for the identical target with an intersection over union (IoU) exceeding 0.9, alongside averaging the dimensions of the wireframes. Targets with an IoU less than 0.9 for both wireframes were adjudicated by annotator 3 to ensure consistency and accuracy.\nWe distribute the BFA-3D dataset into training set, validation set and test set in the ratio of 8:1:1. The number and distribution of objects in the training set are shown in Figure3.\nIn deep network model training, sufficient and diverse data is crucial to improve the generalization ability of the model. However, it is often difficult to collect images of building facades from different viewpoints, and the collected data often face the problem of category imbalance. To overcome this challenge, this study employs data augmentation techniques to extend and enrich the training dataset. Data augmentation is an effective method to perform various transformations on raw data to generate more training samples (Shorten and Khoshgoftaar, 2019; Miko\u0142ajczyk and Grochowski, 2018). Building facade attachments are often characterized by different shapes, sizes, textures, and locations, and are easily affected by environmental factors such as illumination and occlusion. In this study, we make modifications to the training images that are suitable for this dataset, including rotation, panning, brightness adjustment, color transformation and random noise addition. Among them, the image rotation operation is based on the center point of the image and randomly selects an angle between 15\u00b0 and 30\u00b0 for clockwise or counterclockwise rotation. The image translation operation randomly selects a value between 100 and 300 pixels as the moving distance. In order to fill in the uninformative areas that may be created by the rotation and scaling operations, a random noise padding is introduced. The brightness adjustment operation then selects a random factor between 0.5 and 0.7 to increase or decrease the exposure of the image. During the random noise addition process, we use Gaussian noise that conforms to a normal distribution to further increase the diversity of the data. These data enhancement methods successfully provide more angles, more light variations, and more disturbances training data for the deep network model, which further enriches the training data and balances the class distribution gap; reduces the model's dependence on specific details and features, and reduces the risk of overfitting; exposes the model to more diverse data and learns from a wider range of more abstract features rather than relying on a particular detail or feature only, which helps to improve the model's performance in the task of detecting building facade attachments. Figure4 shows an example of data enhancement."}, {"title": "2.2. BFA-YOLO Network", "content": "The unique architectural characteristics of buildings result in a marked discrepancy in the abundance of facade attachment objects, posing considerable challenges for deep network model training. In this study, we use a deep learning-based object detection framework, BFA-YOLO, to detect building facade attachments. The model is based on the YOLOv8 (You Only Look Once v8) object detection algorithm (Varghese and M., 2024). The structure of the BFA-YOLO network proposed in this paper is shown in Figure 5. In the BFA-YOLO network model, we propose the Feature Balanced Spindle Module (FBSM). This module enhances the network's capacity to discern features from sparsely represented categories via a specialized resampling method, substantially bolstering their recognition. Additionally, to address the issue of relatively small facade attachments within larger images, we proposes the Target Dynamic Alignment Task Detection Head (TDATH). This head is effective for small object detection, ensuring the accurate identification of diminutive targets. Furthermore, we presents the Position Memory Enhanced Self-Attention Mechanism (PMESA) to minimize interference from complex urban background features. This mechanism significantly curbs the impact of such background elements on detection and enhances accuracy. Collectively, the FBSM, TDATH, and PMESA mechanisms offer a robust and precise system for detecting facade attachments. These solutions effectively address issues of object imbalance, small object detection, and background interference, thereby substantially improving the precision of facade attachments detection."}, {"title": "2.2.1. Feature Balanced Spindle Module (FBSM)", "content": "Owing to the distinctive architectural features of the building, there is a considerable variance in the number of targets pertaining to its facade attachments. This variation presents a considerable challenge in the training of deep neural networks. To address this challenge, this paper puts forth a novel feature equalization spindle module, the schematic of which is presented in Figure 6. The objective of this module is to strengthen the network's capability to perceive features from underrepresented categories more effectively, achieved through a process of feature resampling. This approach aims to markedly enhance the recognition of the aforementioned categories.\nIn FBSM, to enhance computational efficiency and alleviate complexity, the module utilizes individual convolution kernels for each channel of the input feature map, amalgamating the outputs to generate the final result. The FBSM consists of three input channels: one undergoes depthwise convolution (Howard, 2017), while the other two channels execute standard convolution processes. Post-convolution, the output tensors from these channels are concatenated, thereby augmenting the model's proficiency in capturing, integrating, and presenting diverse features. The resultant output $x_{out}$ is determined by the equation $x_{out} = x + DVConV_{n}(x)$, with n taking the values 5, 7, 9, 11. The multifaceted DWConV operations facilitate the fusion and diffusion of features by performing a series of processing steps. This strategy empowers the network to learn a more extensive and intricate array of features, especially for those that are underrepresented. This increased propagation and amalgamation further refine the network's responsiveness and its capacity for recognition within these specific domains."}, {"title": "2.2.2. Target Dynamic Alignment Task Detection Head (TDATH)", "content": "In practical applications of building facade attachments detection, certain elements (e.g. air conditioner units and small windows) are often very small compared to the overall image size. This feature poses a great challenge to the target detection task. To effectively address this challenge, we have developed a specialized detection mechanism called target dynamic alignment task detection head (TDATH), the structure of which is shown in Figure 7. The TDATH detection head greatly enhances the detection of small targets by focusing on proportional alignment, ensuring that even tiny targets can be accurately identified against complex backgrounds. This innovation not only enhances the model's ability to capture complex details, but also greatly improves the overall target detection accuracy and robustness.\nThe design of the TDATH meticulously considers the attributes of small objects. It incorporates three primary inputs that encapsulate information about the object at varying scales and levels of features. These inputs initially undergo two Convolution and Group Normalization (Conv_GN) operations for feature extraction and enhancement (Wu and He, 2018). These operations are pivotal in capturing the local intricacies and global contextual details of the object. Subsequently, the resulting feature maps from the Conv_GN layers are concatenated with the initial feature map, facilitating an effective fusion of information across different scales and feature depths. This process yields a more comprehensive set of feature representations for subsequent detection. Following concatenation, the composite feature maps are further refined through a module termed the Cross-Scale Refinement Module (CRCS), which may involve a custom convolution or pooling operation, exemplified here for demonstration. The CRCS module adeptly refines and enhances the feature maps, optimally preparing them for the subsequent detection tasks. The CRCS-processed feature maps are then subjected to task decomposition in conjunction with the concatenated results, a process that involves distinguishing and processing objects of varying categories or scales to ensure precise detection of each entity. The decomposed feature maps proceed through deformable convolution operations within the DCNV2 (Deformable Convolutional Networks v2) framework (Zhu et al., 2018). DCNV2 dynamically adapts the sampling locations of the convolution kernel, which is contingent upon the object's shape and scale, thereby capturing intricate details and contours more effectively. The DCNV2-processed feature maps subsequently enter a regression convolution operation to yield the object's bounding box position information. Concurrently, the concatenated feature map is channeled to a generator that performs element-wise multiplication with the task-decomposed tensor, followed by a classification convolution to output the object's category information. Through this intricately designed sequence of operations, the TDATH detection head achieves robust detection of small objects and rare categories. It harnesses information across multiple scales and feature levels, ensuring precise detection and handling of each object through dynamic alignment and task decomposition mechanisms."}, {"title": "2.2.3. Position Memory Enhanced Self-Attention Mechanism (PMESA)", "content": "The substantial resemblance between architectural facade attachments and the intricate spatial backgrounds of urban environments often leads to significant interference with the precision of detection tasks. To mitigate this interference and enhance the accuracy of target detection, this paper introduces the innovative Position Memory Enhanced Self-Attention Mechanism. Its structure is shown in Figure 8. By reinforcing the model's retention of location information and focus on target positions, this mechanism significantly diminishes the impact of complex background elements on detection outcomes. Consequently, the mechanism facilitates more precise and efficient recognition of facade attachments.\nThe Position Memory Enhanced Self-Attention (PME-SA) mechanism, rooted in the C2f module of YOLOv8, represents an innovative advancement. We have substituted the traditional bottleneck layer in the C2f module with an innovative RetBlock, as outlined in (Fan et al., 2023). This replacement incorporates RelPos relative position information into RetBlock, thereby providing crucial positional data for the target object being detected. Within RetBlock, Manhattan Self-Attention (MaSA) based on RetNet's retention mechanism forms the core. MaSA transforms the original unidirectional, one-dimensional temporal decay mechanism traditionally used for textual data into a bidirectional, two-dimensional spatial decay model, finely capturing intricate spatial relationships within image data. This bidimensional approach facilitates in-depth analysis of image information. By decomposing the self-attention mechanism and the spatial decay matrix along the image's horizontal and vertical axes, we significantly reduce computational demands while preserving the model's explicit spatial priors, maintaining efficiency without compromising performance. The inclusion of PME-SA position memory significantly enhances the self-attention mechanism's ability to capture contextual image information, thereby bolstering the model's capacity to process local details and overall performance, offering an effective solution for visual task processing. The architecture of the C2f module, RetBlock are illustrated in Figure 8.\nTo optimize computational efficiency and elevate model performance, the RetBlock design thoughtfully integrates several essential components. It initiates the process with Depthwise Convolution (DWConv) as a preprocessing step, a strategy that considerably cuts down on parameters and computational complexity while preserving robust feature extraction capabilities. Following this, RetBlock employs the Skip Connection mechanism, merging the input feature maps with those processed by DWConv. This fusion not only facilitates the seamless flow of information but also enhances the model's capacity for gradient backpropagation, addressing the challenge of gradient vanishing in deep networks. The merged feature maps are then directed to the Layer Normalization (LN) layer for standardization, which expedites the training process and stabilizes the model. Post-normalization, the feature maps undergo a Manhattan Self-Attention operation. This operation, encapsulated by Equations (1, 2).\n$D^{H}_{nm} = \\frac{\\|x_n - x_m\\|}{D_{max}}$\n$D^{W}_{nm} = \\frac{\\|y_n - y_m\\|}{D_{max}}$ (1)\n$Ret Block(X) = [Softmax(Q_H(K_H^T)) \\odot D^H] \\cdot [Softmax(Q_W(K_W^T)) \\odot D^W ]^T$ (2)\nLeverages the Manhattan distance to gauge feature similarity, thereby enabling an efficient self-attention mechanism that captures global information with minimal computational overhead. The outcome of the Manhattan self-attention is then concatenated with the LN layer's output, amalgamating original features with those derived from self-attention to enrich the feature representation. This amalgamated feature set is re-normalized through the LN layer and subsequently processed by a Feed-Forward Network (FNN) layer for additional feature refinement and extraction. Finally, the FNN layer's output is concatenated with the input features prior to LN, yielding RetBlock's final output. The PMESA seamlessly integrates image relative position information into RetBlock, ensuring the provision of precise positional data for the detection object. It can be represented as Equations (3, 4).\n$RelPos^{d}(X) = Softmax(p_x^d, p_y^d)$ (3)\n$PMESA(X) = \\frac{\\sum_{n=1}^{N}[Ret Block(X) + RelPos^{d}(X)]}{n}$ (4)\nwhere x, y denote the relative position, d is the step size of the sequence before and after the relative position, and n denotes how many PMESA operations were performed."}, {"title": "3. Experiments and Analysis", "content": ""}, {"title": "3.1. Experiment Settings", "content": ""}, {"title": "3.1.1. Experimental Design", "content": "In this study, we design a series of experiments on the BFA-3D dataset and the Facade-WHU dataset to validate the effectiveness of our proposed method. In order to ensure the objectivity of the cross-sectional comparison of each network model, we divided the BFA-3D dataset and Facade-WHU dataset into a training set, a validation set, and a test set, respectively.\nWe evaluated the detection effectiveness of these models to illustrate the superior performance of our BFA-YOLO model in identifying building facade attachments. Furthermore, in order to meticulously analyze the contribution of each component within the framework, we conducted a comprehensive ablation study. In the ablation experiment experiments, we progressively developed multiple iterative versions of the YOLOv8 network model based on the BFA-3D training set. This includes the baseline YOLOv8 model, the YOLOv8 model enhanced by FBSM, TDATH, and PMESA integration, and pairwise combinations of these enhanced models, leading to the final BFA-YOLO model. In addition, we also selected two types of building facade attachments, i.e., doors and windows, from the Facade-WHU dataset to validate the efficacy of our proposed BFA-YOLO network model in recognizing these attachments in street-view images."}, {"title": "3.1.2. Evaluating Indicator", "content": "To effectively measure the deep network model's capability in identifying building facade attachments, we employ two critical evaluation metrics: Average Precision (AP) and mean Average Precision (mAP). The AP metric enhances our understanding of the model's performance by calculating the average precision across all recall levels, illustrating the model's detection efficiency through the area under the Precision-Recall (P-R) curve. Notably, a superior AP value signifies the model's proficiency in achieving high precision across varying recall levels. In the context of multi-class detection tasks, mAP represents a normalized metric that aggregates the AP values across all classes to evaluate the model's consistent performance. This is crucial for assessing how well the model handles complex and diverse objects. AP, and mAP are computationally defined as Equations (5, 6). We utilize the mean Average Precision (mAP) with an Intersection over Union (IoU) threshold set at 0.5, denoted as mAP@0.5.\n$AP = \\int_{0}^{1} P(R)dR$ (5)\n$MAP = \\frac{\\sum_{n=1}^{N}AP}{n}$ (6)\nWithin the discussion section, to more thoroughly delineate the contributions of this research, we implement the TIDE detection error evaluation methodology (Bolya et al., 2020). This method serves to more distinctly highlight the advantages of our novel approach in accurately identifying attachments on building facades. The array of errors analyzed through the TIDE framework includes Classification Error, Localization Error, Combined Classification and Localization Error, Duplicate Detection Error, Background Error, and Missed Detection Error. To offer a more intuitive visualization of the model's detection capabilities, this study further employs heatmap visualizations. Such an approach not only demonstrates the efficacy of our cutting-edge solution in the detection of building facade attachments but also provides a visual representation of the model's effective receptive field, thereby enabling a comprehensive assessment of our model's superior performance (Ding et al., 2022; Luo et al., 2016)."}, {"title": "3.1.3. Experimental Settings", "content": "We conducted our experiments at the Wuhan University Supercomputing Center using the PyTorch deep learning framework and CUDA11.8. We adapted YOLOv8 from the official codebase of ultralytics (Varghese and M., 2024). We implemented Faster-CNN (Ren et al., 2015), TridentNet (Li et al., 2019), and Tood (Feng et al., 2021) using the MMDetection framework (Chen et al., 2019). We trained all models for 500 epochs using SGD optimizer with a learning rate of 0.01."}, {"title": "3.2. Experiments and Analysis", "content": "The execution of these experiments was driven by three primary objectives. Firstly, this research aimed to assess the performance of the BFA-YOLO model and conduct a comparative analysis with various other models dedicated to detecting attachments on building facades. The goal of this evaluation was to provide an in-depth comparison that elucidates the respective strengths and weaknesses of each model concerning their precision in identifying building facade attachments. Secondly, the experiments sought to examine the practical applicability of the BFA-YOLO model, with a particular emphasis on its implementation potential and its effectiveness in complex detection scenarios. Thirdly, the study aimed to confirm the effectiveness of the two newly proposed modules, along with the innovative attention mechanism introduced. In addition, TIDE error detection experiments were conducted to further explore and disclose how our innovations enhance performance in addressing error detections."}, {"title": "3.2.1. Comparative Experiment", "content": "To evaluate the effectiveness of our constructed network, we conducted experiments on the BFA-3D dataset and the Facade-WHU dataset. We compared the results with Faster R-CNN, TridentNet, Tood, YOLOv5, and YOLOv8. Referring to Table 3, our proposed BFA-YOLO model achieves a mAP@0.5 of 86.4% across all categories on the BFA-3D dataset, representing the highest performance among the compared network models. In a detailed comparison with YOLOv8, BFA-YOLO demonstrates enhancements in the AP metrics for door (Door), embedded window (EM_Win), protruding window (PR_Win), billboard (Bil), and glass curtain wall (Gla_Wal) by 1.9%, 1.3%, 2.6%, 1.4%, and 7.1%. Our method shows considerable advancements over established models such as Faster R-CNN, TridentNet, Tood, and YOLOv5. Figure 9 shows the visualized detection results of BFA-YOLO and YOLOv8 networks on the BFA-3D test set. The P-R curves of the above network models for mAP50 on all building facade attachment types in the BFA-3D dataset are shown in Figure10.\nWe focused on two categories of building facade attachments from the Facade-WHU dataset that overlap with those in the BFA-3D dataset: doors and windows. Experiments were conducted on the adapted Facade-WHU dataset, and the results are presented in Table 4. This includes a 2.8% and 3.0% improvement over YOLOv8 in AP for window and door detection, respectively. Our proposed network model, BFA-YOLO, achieved an mAP@0.5 of 54.7%, which represents an advancement over the performance of Faster R-CNN, TridentNet, Tood, YOLOv5, and. Figure 11 shows the visualized detection results of the BFA-YOLO and the YOLOv8 network on the Facade-WHU test set."}, {"title": "3.2.2. Ablation Experiment", "content": "In order to comprehensively evaluate the effectiveness of our proposed module in addressing category imbalance, small-object detection challenges, and background interference, which are the key challenges in building facade attachments detection, we have carefully designed and executed an exhaustive ablation study. The study focuses on three core components: FBSM, TDATH, and the PMESA. By systematically integrating these modules individually and in combination into the baseline model, we thoroughly analyze their individual and synergistic effectiveness. The baseline model was set to YOLOv8 without any of the aforementioned enhancement modules to ensure the fairness and accuracy of the evaluation. Subsequently, we constructed six variant models (M1 to M7), each of which integrates the three key modules mentioned above, either separately or jointly, to explore their specific impact on the detection performance. Specifically, M1 integrates the FBSM, which aims to balance the detection capabilities of different classes and scales of objects by optimizing the feature distribution. M2 introduces the TDATH, a mechanism that dynamically adjusts the detection frame to adapt to the object deformation, improving the detection accuracy for small objects and complex backgrounds. M3 applies PMESA to enhance the feature representation by utilizing spatial context information to effectively reduce background interference. M4 combines FBSM and TDATH, aiming to solve the feature equalization and small object detection problems simultaneously. M5 integrates FBSM and PMESA to explore the synergistic effect of feature equalization and background suppression. M6 integrates TDATH and PMESA, focusing on improving small object detection accuracy and background interference suppression. Finally, Model M7, as the core result of this study, integrates all three key modules and represents the complete form of the proposed method. Through the experimental results presented in Table5, we can clearly see that with the addition of the modules, the detection performance of the model under various types of challenges is significantly improved, which verifies the validity and necessity of the design of the modules and the superiority of their synergistic work.\nAccording to the AP evaluation metrics, the addition of the FBSM results in an improvement of 0.9, 1.7, and 5.3 relative to the original YOLOv8 for the categories of balcony (Bal), billboard (Bib), and glass curtain wall (Gal_Wal), respectively. The addition of TDATH resulted in 0.8, 2.5, and 1.3 AP improvements for embedded window (EM_Win), protruding window (PR_Win) and air conditioner unit (ACU), respectively. After adding the PMASA, door (Door), embedded window (EM_Win), protruding window (PR_Win), and glass curtain wall (Gal_Wal) have an improvement of 3.4, 2.0, 2.9, and 5.5, respectively, with respect to the original YOLOv8 model. In order to verify that there is no conflict in the individual modules, we merge them together and synergize them, and find that the three of them two-by-two can synergize, and there is no significant decrease in mAP@0.5(%). Finally, we combined the PMESA with the FBSM and the TDATH to become BFA-YOLO, and experimentally found that BFA-YOLO achieved a mAP@0.5(%) of 86.4 in all categories."}, {"title": "4. Discussion", "content": "From the perspective of model detection effect, the BFA-YOLO model detection effect is significantly improved over that of YOLOv8. This enhancement is mainly attributed to the modules designed in this paper. These modules specialize in building facade attachments inspection tasks. The introduction of these modules not only improves the detection accuracy of the model, but also enhances the model's ability to deal with complex scenes and objects. The results of the detection comparison between BFA-YOLO and YOLOv8 as well as the detection heat map are shown in Figure 12. We delve into the effective receptive field of BFA-YOLO and analyze how each module of the BFA-YOLO network model performs erroneously on the BFA-3D dataset, as well as comparing the effectiveness of BFA-YOLO as well as YOLOv8 for the detection of attachments on building facades. We note that different modules have different effects on different types of objects. For example, the PMESA achieves significant gains on objects such as door (Door), embedded window (EM_Win), protruding window (PR_Win), due to the fact that these objects usually have more complex shapes and textures in the image and require stronger spatial attention mechanisms to capture their details. Similarly, the performance improvement of the TDATH on the problem of detecting small objects in air conditioner unit (ACU), embedded window (EM_Win) and protruding window (PR_Win) demonstrates the effectiveness of the dynamic alignment strategy when dealing with small objects. The addition of the FBSM shows varying degrees of improvement relative to YOLOv8 in balconies (Bal), billboards (Bib), and glass curtain walls (Gal_Wal), which are three categories with a small number of categories, demonstrating the effectiveness of our proposed FBSM module. BFA-YOLO synthesizes the strengths of all three, and performs well on all categories. To validate the effectiveness of the improved method in this paper, we mapped the effective receptive fields of the BFA-YOLO network and compared it with YOLOv8. The results are shown in Figure13. Our proposed BFA-YOLO method outperforms YOLOv8 in terms of effective receptive fields.\nWe use TIDE's object detection error class accuracy rating metrics and compare the performance of different models on different error detection metrics on the BFA-3D dataset and the Facade-WHU dataset to better reflect the limitations of working with different modules. The experimental results are shown in Figure 14. Overall, BFA-YOLO has the best error performance. The results on the BFA-3D dataset show that after adding FBSM, although the Bkg background interference of the model is significantly reduced, the Dupe duplicate detection error detection metrics increase significantly, which is due to the increased risk of duplicate detection while the model tries to reduce the background interference. The addition of TDATH resulted in a significant decrease in the model's Cla error detection metrics, suggesting that this module helps to reduce classification errors. With the addition of PMESA, Bkg background interference error detection performance improved, but Dupe duplicate error detection increased substantially, suggesting that reducing background distractors comes at the cost of duplicate detections. Results on the Facade-WHU dataset show that our proposed BFA-YOLO network model has the smallest overall performance detection error metrics. These findings provide valuable clues for further model optimization."}, {"title": "5. Conclusions and Future work", "content": "In this paper, we propose an innovative object detection method for building facade attachments, BFA-YOLO, which is significantly improved on YOLOv8 to achieve more accurate detection of building facade attachments. Through a series of experimental analyses, we verify the excellent performance of BFA-YOLO in object detection. First, BFA-YOLO introduces the FBSM, which effectively addresses the challenge of the uneven number of objects on building facade attachments and improves the model's adaptability in diverse object scenarios. Secondly, we introduce the TDATH, which proposes an effective solution to the small object detection problem and significantly improves the detection accuracy of small objects. In addition, we introduced the PMESA, which effectively reduces the interference of the background and further improves the detection accuracy. In the quantitative evaluation, compared to YOLOv8 improves 1.8% in mAP@0.5, which fully In the quantitative evaluation, and mAP@0.5 shows an improvement of 2.9% on the Facad-WHU dataset. These experiments fully demonstrate the advantages of BFA-YOLO in building facade attachments detection.\nCompared with other existing models, BFA-YOLO also demonstrates significant performance advantages. To support this research, we constructed a building facade attachments dataset containing seven categories, which provides rich samples for model training and testing. As automation and intelligence become the trend in the field of object detection of building facade attachments, the proposal of BFA-YOLO provides strong support to realize this goal. After that we are going to optimize in the following aspects. We will continue to increase the number of datasets and explore a more comprehensive and detailed classification system to enrich the data volume of the BFA-3D dataset and improve the completeness of the data. We will also explore more effective methods to improve the performance of building facade attachments detection to meet the demand for high accuracy and efficiency in practical applications. We are exploring the potential of BFA-YOLO in practical applications. We apply BFA-YOLO in the reconstructed 3D model to detect building facade attachments and obtain the location information of building facade attachments objects in the 3D model to support downstream applications."}, {"title": "CRediT authorship contribution statement", "content": "Yangguang Chen: Conceptualization, Resources, Writing - Original Draft. Tong Wang: Validation, Writing - Review & Editing. Guanzhou Chen: Conceptualization, Resources, Writing - Original Draft. Kun Zhu: Validation, Writing - Review. Xiaoliang Tan: Validation, Writing - Review. Jiaqi Wang: Validation, Writing - Review. Hong Xie: Methodology, Data Curation, Visualization, Writing - Review & Editing. Wenlin Zhou: Writing - Original Draft, Visualization. Jingyi Zhao: Validation, Writing - Review & Editing. Qing Wang: Validation, Writing - Review. Xiaolong Luo: Validation, Writing - Review. Xiaodong Zhang: Supervision, Project administration, Funding acquisition."}]}