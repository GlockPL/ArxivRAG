{"title": "Enhancing Retrieval-Augmented Generation: A Study of Best Practices", "authors": ["Siran Li", "Linus Stenzel", "Carsten Eickhoff", "Seyed Ali Bahrainian"], "abstract": "Retrieval-Augmented Generation (RAG) systems have recently shown remarkable advancements by integrating retrieval mechanisms into language models, enhancing their ability to produce more accurate and contextually relevant responses. However, the influence of various components and configurations within RAG systems remains underexplored. A comprehensive understanding of these elements is essential for tailoring RAG systems to complex retrieval tasks and ensuring optimal performance across diverse applications. In this paper, we develop several advanced RAG system designs that incorporate query expansion, various novel retrieval strategies, and a novel Contrastive In-Context Learning RAG. Our study systematically investigates key factors, including language model size, prompt design, document chunk size, knowledge base size, retrieval stride, query expansion techniques, Contrastive In-Context Learning knowledge bases, multilingual knowledge bases, and Focus Mode retrieving relevant context at sentence-level. Through extensive experimentation, we provide a detailed analysis of how these factors influence response quality. Our findings offer actionable insights for developing RAG systems, striking a balance between contextual richness and retrieval-generation efficiency, thereby paving the way for more adaptable and high-performing RAG frameworks in diverse real-world scenarios.", "sections": [{"title": "1 Introduction", "content": "Language Models (LMs) such as GPT, BERT, and T5 have demonstrated remarkable versatility, excelling in a wide range of NLP tasks, including summarization (Bahrainian et al., 2022), extracting relevant information from lengthy documents, question-answering, and storytelling (Brown et al., 2020b; Devlin et al., 2019; Raffel et al., 2020). However, their static knowledge and opaque reasoning raise concerns about maintaining factual accuracy and reliability as language and knowledge evolve (Huang et al., 2024; Jin et al., 2024). As new events emerge, and scientific advancements are made, it becomes crucial to keep models aligned with current information (Shi et al., 2024a). However, continuously updating models is both costly and inefficient. To address this, RAG models have been proposed as a more efficient alternative, integrating external knowledge sources during inference to provide up-to-date and accurate information (Lewis et al., 2020; Borgeaud et al., 2022; Lee et al., 2024). RAG models augment language models by incorporating verifiable information, improving factual accuracy in their responses (Gao et al., 2023; Kim et al., 2023). This approach not only mitigates some conceptual limitations of traditional LMs but also unlocks practical, real-world applications. By integrating a domain-specific knowledge base, RAG models transform LMs into specialized experts, enabling the development of highly targeted applications and shifting them from generalists to informed specialists (Siriwardhana et al., 2023). In recent years, this advancement has led to many proposed architectures and settings for an optimal RAG model (Li et al., 2024; Dong et al., 2024). However, the best practices for designing RAG models are still not well understood.\nIn this paper, we comprehensively examine the efficacy of RAG in enhancing Large LM (LLM) responses, addressing nine key research questions:\n(1) How does the size of the LLM affect the response quality in an RAG system? (2) Can subtle differences in prompt significantly affect the alignment of retrieval and generation? (3) How does the retrieved document chunk size impact the response quality? (4) How does the size of the knowledge base impact the overall performance? (5) In the retrieval strides (Ram et al., 2023), how"}, {"title": "2 Related Works", "content": "RAG systems have emerged as a promising solution to the inherent limitations of LLMs, particularly their tendency to hallucinate or generate inaccurate information (Semnani et al., 2023; Chang et al., 2024). By integrating retrieval mechanisms, RAG systems fetch relevant external knowledge during the generation process, ensuring that the model's output is informed by up-to-date and contextually relevant information (Gao et al., 2023; Tran and Litman, 2024). Guu et al. (2020) show that language models could retrieve relevant documents in real time and use them to inform text generation, significantly enhancing factual accuracy without increasing model size. Shi et al. (2024b) demonstrate how retrieval modules can be applied even to black-box models without direct access to their internals. In-Context Retrieval-Augmented Language Models further dynamically incorporate retrievals into the generation process, allowing for more flexible and adaptive responses (Ram et al., 2023). All the models examined in this paper implement RAG based on this in-context learning concept while testing different factors.\nRecent research has focused on optimizing RAG systems for efficiency and performance. Several strategies for improving the system's retrieval components are outlined, such as optimizing document indexing and retrieval algorithms to minimize latency without compromising accuracy (Wang et al., 2024). Additionally, Hsia et al. (2024) examine the architectural decisions that can enhance the efficacy of RAG systems, including corpus selection, retrieval depth, and response time optimization. Furthermore, Wu et al. (2024) illustrate how optimization strategies can be designed to balance the model's internal knowledge with the retrieved external data, addressing the potential conflict between these two sources of information. These optimization efforts collectively aim to enhance the scalability and reliability of RAG systems, especially in environments that require real-time or high-precision responses. Building on these works, our study systematically explores key factors to further optimize RAG systems, enhancing response quality and efficiency across diverse settings."}, {"title": "3 Methods", "content": "Augmenting LLMs with real-time, up-to-date external knowledge bases, allows the resulting RAG system to generate more accurate, relevant, and timely responses without the need for constant retraining (Fan et al., 2024). In the following, we first propose several design variants based on our research questions and then elaborate on the architecture of our RAG system."}, {"title": "3.1 RAG Design Variations", "content": "To explore the strategy that influences the efficacy of RAG, we propose the following research questions to guide our investigation:"}, {"title": "3.2 Architecture", "content": "To address the above questions, we design a RAG system and conduct experiments with various configurations. Our RAG system combines three key components: a query expansion module, a retrieval module, and a text generation module"}, {"title": "A. Query Expansion Module", "content": "Inspired by the core principles of information retrieval, which start with a broad search and are followed by focused re-ranking (Carpineto and Romano, 2012), our first stage focuses on query expansion to define the search space. For Query Expansion, we employ a Flan-T5 model (Raffel et al., 2020), to augment the original user query.\nGiven an initial query q, the model generates a set of N expanded queries q' = {q_1, q_2, ..., q_n}, where each q' represents a keyword phrase relevant to answering the original query. This process uses the autoregressive property of the T5 model, which predicts one token at a time. The model encodes q into a hidden state h and generates each token yt at step t, conditioned on the previous tokens y<t and the hidden state h:\nP(y_t|h, y_{<t}) = Decoder(Encoder(q), y_{<t}) \\qquad (1)\nBy repeating this process, the model produces N relevant expanded queries."}, {"title": "B. Retrieval Module", "content": "For the retrieval module, we use FAISS (Douze et al., 2024) because it is computationally efficient, easy to implement, and excels at performing large-scale similarity searches in high-dimensional spaces. Documents are segmented into chunks C = {C_1, C_2,..., C_n }, and a pre-trained Sentence Transformer (Reimers and Gurevych, 2019) encoder generates embeddings E = {e_1, e_2, ..., e_n} based on C. The IndexBuilder class indexes these embeddings for retrieval. Given a query embedding qemb, from the same encoder, the top k chunks are retrieved based on the inner product similarity:\nSim(q_{emb}, e_i) = q_{emb}^T e_i \\qquad (2)\nThe retrieval process for RAG variants consists of three steps. Step 1: We retrieve a preliminary set of documents D(1) based on the expanded queries q' and the original query q, shown as D(1) = Retrieve((q, q'), D). Step 2: From D(1), we retrieve the relevant documents using the original query q, resulting in the final document set D(2) = Retrieve(q, D(1)). Step 3: We split the documents in D(2) into sentences, denoted as S, and retrieve the most relevant sentences, S(1) = Retrieve(q, S), based on the original query. Step 3 represents the Focus Mode, which we investigate in Q9. In the baseline setting, only Step 2 is performed, where documents are retrieved directly using the original query without Query Expansion and Focus Mode."}, {"title": "C. Text Generation Module", "content": "Upon receiving a query q, the retrieval module retrieves similar document chunks D(2) or sentences S(1), forming the context K. The LLM is prompted with q and K, generating responses. In the Retrieval Stride variant, the context K is dynamically updated at specific intervals during generation. At time step tk, the retriever updates K based on the generated text g<tk up to tk:\nK(t_k) = Retriever(q, D, g_{<t_k}) \\qquad (3)\nThis keeps K continuously updated with relevant information. The LLM generates tokens autoregressively, where each token gt is based on previous tokens g<t and context K. The final generated sequence g represents the response to the query q:\nP(g_t|g_{<t}, K) = LLM(g_{<t}, K) \\qquad (4)\nIn the baseline setting, the retrieval stride is not used, and K remains fixed during generation."}, {"title": "4 Experimental Setup", "content": "This section provides details about our experimental setup, including the evaluation datasets, knowledge base, evaluation metrics, and implementation specifics of our RAG approach."}, {"title": "4.1 Evaluation Datasets", "content": "To evaluate the performance of RAG variants, we use two publicly available datasets: TruthfulQA (Lin et al., 2022) 2 and MMLU (Hendrycks et al., 2021) 3. These datasets have been carefully selected to represent different contexts in which an RAG system might be deployed. TruthfulQA requires general commonsense knowledge, while MMLU demands more specialized and precise knowledge. Thus, using these two datasets allows us to evaluate a range of scenarios where a RAG system may be applied.\nTruthfulQA (Lin et al., 2022): A dataset of 817 questions across 38 categories (e.g., health, law, politics), built to challenge LLMs on truthfulness by testing common misconceptions. Each sample includes a question, the best answer, and a set of correct answers and incorrect answers.\nMMLU (Hendrycks et al., 2021): This dataset evaluates models in educational and professional contexts with 57 subjects across multiple-choice questions. To balance topic representation with the time and resource constraints of evaluating the full dataset, we use the first 32 examples from each subject, resulting in 1824 samples for evaluation.\nExamples from both datasets are shown in Table 1. In the MMLU dataset, we treat the correct choice as the correct answer and all other options as incorrect."}, {"title": "4.2 Knowledge Base", "content": "To ensure comprehensive topic coverage, we use Wikipedia Vital Articles 4 as the knowledge base for the RAG model. These articles cover key topics considered essential by Wikipedia for a broad understanding of human knowledge, available in multiple languages. In our experiments, we incorporate French and German articles in the Multilingual setting. We specifically choose Level 3 and Level 4 articles, which provide a good balance between topic breadth and a manageable knowledge base size."}, {"title": "4.3 Evaluation Metrics", "content": "To provide a comprehensive overview of the generative performance, our evaluation utilizes the following metrics:\nROUGE (Lin, 2004): is a set of metrics used to assess text generation quality by measuring overlap with reference texts. ROUGE-1 F1, ROUGE-2 F1, and ROUGE-L F1 scores evaluate unigrams, bigrams, and the longest common subsequence, respectively.\nEmbedding Cosine Similarity: is a metric used to compute the cosine similarity score between the embeddings of the generated and reference texts, both encoded by a Sentence Transformer (Reimers and Gurevych, 2019) model.\nMAUVE (Pillutla et al., 2021): is a metric for assessing open-ended text generation by comparing the distribution of model-generated text with that of human-written text through divergence frontiers. The texts are embedded using a Sentence Transformer(Reimers and Gurevych, 2019), and MAUVE calculates the similarity between their embedding features. Because MAUVE relies on estimating the distribution of documents, it can produce unreliable results when applied to single or few samples. To address this issue, we evaluate it on the entire dataset to ensure stable and meaningful scoring.\nFActScore (Min et al., 2023): is a metric designed to evaluate the factuality of responses generated by large language models (LLMs) by identifying and assessing atomic facts-concise sentences that convey individual pieces of information. Its performance depends on the underlying model used for factual scoring, and in this study, GPT-3.5-turbo (Brown et al., 2020a) serves as the base model."}, {"title": "4.4 Implementation Details", "content": "For Query Expansion, we utilize the T5 model (Raffel et al., 2020), specifically google/flan-t5-small, fine-tuned with FLAN (Chung et al., 2024), to generate relevant keywords. FAISS (Douze et al., 2024) is employed for vector indexing and similarity search, while a Sentence Transformer (all-MiniLM-L6-v2) serves as the text encoder for generating sentence embeddings to enable semantic comparison. For text generation, we employ models from the Mistral family (Jiang et al., 2023)5, including the Instruct7B model"}, {"title": "5 Experiments and Results", "content": "To identify effective setups for optimizing the RAG system, we evaluate the performance of different RAG variants across 3 aspects: relevance evaluation, factuality assessment, and qualitative analysis."}, {"title": "5.1 Relevance Evaluation", "content": "To address the 9 questions proposed in Section 3.1, we compare the relevance of the generated examples from model variants to the reference text and evaluate their performance differences.\n1. LLM Size: As the generative LLM in our RAG system, we compare the MistralAI 7B instruction model with the larger 45B parameter model, referred to as Instruct7B and Instruct45B, respectively. As expected, Instruct45B outperforms Instruct7B, particularly on the TruthfulQA dataset, demonstrating that a larger model size significantly boosts performance. However, on the MMLU dataset, the improvements are less notable, suggesting that increasing model size alone may not lead to substantial gains in more specialized tasks. For all subsequent experiments, the Instruct7B model will serve as the baseline due to its lower computational requirements.\n2. Prompt Design: We examine the impact of different system prompts on model performance, with details of each prompt provided in Appendix A.2. Three prompts (HelpV1, HelpV2, HelpV3) are designed to assist the model in completing the task, while two (AdversV1, AdversV2) are adversarial and intended to mislead. As shown in Table 2, the helpful prompts consistently outperform the adversarial ones across all metrics, with HelpV2 and HelpV3 achieving the highest scores. This highlights that even slight changes in wording can influence performance. Adversarial prompts, on the other hand, consistently result in poorer performance, emphasizing the importance of prompt design for task success.\n3. Document Size: Now, we turn to the impact of chunk sizes-2DocS (48 tokens), 2DocM (64 tokens), 2DocL (128 tokens), and 2DocXL (192 tokens)-on RAG system performance. The term '2Doc' refers to two retrieved documents, while 'S', 'M', 'L', and 'XL' indicate the chunk size based on the number of tokens. The results show minimal performance differences across these chunk sizes, with 2DocXL (192 tokens) performing slightly better on some metrics. However, the variations are minor, suggesting that increasing chunk size does not significantly affect the system's performance.\n4. Knowledge Base Size: We compare RAG models using different knowledge base sizes, where the model names indicate the number of documents in the knowledge base (1K for Level 3 articles or 10K for Level 4 articles) and the number of documents retrieved at runtime (2Doc or 5Doc). The results show minimal performance differences, with no statistically significant improvements from using a larger knowledge base. This suggests that increasing the knowledge base size or retrieving more documents does not necessarily improve the quality of the RAG system's output, possibly because the additional documents are either irrelevant or redundant for answering specific queries.\n5. Retrieval Stride: We analyze the impact of retrieval stride (Ram et al., 2023), as discussed in Section 3.2, which determines how frequently documents are replaced during generation. Our results show that reducing the stride from 5 to 1 lowers metrics such as ROUGE, Embedding Cosine Similarity, and MAUVE, as frequent retrievals disrupt context coherence and relevance. This contrasts with Ram et al. (2023), who reported better performance with smaller strides based on perplexity. However, we found perplexity to be inconsistent with other metrics and human judgment, making it unsuitable for our task, aligning with Hu et al. (2024), who highlighted perplexity's limitations. Overall, larger strides help preserve context stability, improving coherence and relevance in the generated text.\n6. Query Expansion: Next, we examine the impact of Query Expansion by varying the size of the retrieval filter in Step 1 of the retrieval module (Section 3.2), using 9 articles for ExpendS, 15 for ExpendM, and 21 for ExpendL, while keeping the number of retrieved documents constant at 2. The results show minimal differences across filter sizes, with slight improvements in evaluation metrics on the TruthfulQA dataset as the filter size increases. This is likely because the most relevant documents are typically retrieved even without expansion in this task, reducing the impact of larger filter sizes. Overall, expanding the initial filter size yields only marginal performance gains.\n7. Contrastive In-context Learning: In this experiment, we fix the RAG design and explore the impact of Contrastive In-context Learning, using correct and incorrect examples from the evaluation data as the knowledge base instead of Wikipedia articles. Model names indicate the number of examples retrieved (ICL1Doc for one, ICL2Doc for two), with '+' denoting the inclusion of contrastive (incorrect) examples (see Appendix A.3). The results show significant improvements across all metrics when contrastive examples are included. For example, the ICL1Doc+ design achieves a 3.93% increase in ROUGE-L on TruthfulQA and a 2.99% improvement in MAUVE on MMLU. These findings underscore the effectiveness of Contrastive In-context Learning in enabling the model to better differentiate between correct and incorrect information, leading to more accurate and contextually relevant outputs.\n8. Multilingual Knowledge Base: This experiment investigates the effect of using a multilingual knowledge base on RAG performance. In the MultiLingo and MultiLingo+ configurations, multilingual documents are retrieved, with MultiLingo+ additionally prompting the system to respond in English (see Appendix A.4). Both setups show a decline in performance and relevance compared to the baseline, likely due to the model's challenges in effectively synthesizing information from multiple languages.\n9. Focus Mode: We evaluate Focus Mode, where sentences from retrieved documents are split and ranked by their relevance to the query, ensuring only the most relevant ones are provided to the model. Model names reflect the number of documents and sentences retrieved (e.g., 2Doc1S retrieves one sentence from two documents). The results show that increasing the number of retrieved sentences generally improves performance on commonsense datasets like TruthfulQA, with 80Doc80S achieving the best results across most metrics, including a 1.65% gain in ROUGE-L. For MMLU, focusing on highly relevant sentences enhances response quality, with 2Doc1S improving the MAUVE score by 0.49% and 120Doc120S boosting Embedding Cosine Similarity by 0.81%. The Focus Mode is a text selection method that enhances retrieval in RAG architectures and may also prove effective in text summarization and simplification (Blinova et al., 2023)."}, {"title": "5.2 Factuality Assessment", "content": "The factuality performance of RAG variants on TruthfulQA and MMLU is summarized in Table 3. Key insights include: (1) w/o_RAG consistently underperforms, confirming that RAG systems enhance factual accuracy over the base LLM. (2) ICL1D+ outperforms all others, scoring 57.00 on TruthfulQA and 74.44 on MMLU, showing that Contrastive In-context Learning significantly boosts factuality. (3) On MMLU, Focus Mode variant 120Doc120S ranks second with 65.87, showing that focusing on relevant sentences boosts performance. 80Doc80S variant shows moderate improvements on TruthfulQA by effectively retrieving and ranking relevant sentences. (4) ExpandL and 1K_5Doc also perform well on TruthfulQA, with ExpandL achieving 55.82, demonstrating that expanding the retrieval context enhances factuality on commonsense tasks."}, {"title": "5.3 Qualitative Analysis", "content": "Examples generated by the model variants on the TruthfulQA and MMLU datasets are presented in Appendix A Table 5. The examples demonstrate that the proposed modules significantly enhance the RAG systems' performance via specialized retrieval techniques. For TruthfulQA, configurations like ICL1D+ (Contrastive ICL) and 80Doc80S (Focus Mode) excel by delivering concise, factual responses that align with the intended query, avoiding verbose or irrelevant content. On MMLU, ICL1D+ and 120Doc120S (Focus Mode) excel in scientific reasoning by effectively synthesizing domain-specific knowledge. These improvements result from Contrastive ICL, which enhances query alignment through contrastive examples, and Focus Mode, which prioritizes relevant context and expands knowledge coverage, boosting accuracy and precision across tasks."}, {"title": "6 Discussion and Key Findings", "content": "Based on a total of 74 experiment runs testing different RAG configurations, we present our key findings: (1) Empirical results confirm that our proposed Contrastive In-Context Learning RAG outperforms all other RAG variants, with its advantage becoming even more pronounced on the MMLU dataset, which requires more specialized knowledge. (2) Our proposed Focus Mode RAG ranks second, significantly outperforming other baselines, underscoring the importance of prompting models with high-precision yet concise retrieved documents. (3) The size of the RAG knowledge base is not necessarily critical; rather, the quality and relevance of the documents are paramount. (4) Factors such as Query Expansion, multilingual representations, document size variations, and retrieval stride did not lead to meaningful improvements in terms of Table 2 metrics. (5) In terms of factuality (Table 3), we observe similar patterns: Contrastive In-Context Learning RAG and Focus Mode RAG are the still the top models, but the Query Expansion method achieves the second place on the TruthfullQA dataset. (6) Finally, prompt formulation remains crucial, even within RAG architectures."}, {"title": "7 Conclusions and Future Work", "content": "In this paper, we comprehensively studied RAG architectures based on existing literature and then proposed four new RAG configurations. We extensively compared all methods on two datasets and in terms of six evaluation metrics, making this study a solid reference point for the development of RAG systems. Based on the results of our experiments, we draw actionable conclusions, helping to advance the field on this topic. Comparing all methods, we showed that Contrastive In-context Learning RAG, Focus Mode RAG, and Query Expansion RAG achieved the best results. Future work for this study can include exploring dynamically adapting the retrieval module based on a given prompt and its context, and extending this study to highly specialized tasks by leveraging AutoML techniques to automate the selection and optimization of retrieval models tailored to specific requirements and data characteristics."}, {"title": "8 Limitations", "content": "In this paper, we tested the effect of various RAG configurations including previous literature but also a few new approaches that we proposed.\n(1) While we extensively studied various RAG architectures and drew conclusions on the best practices, we did not test the effect of combining two or more of the approaches that we studied. This will remain an important future work. (2) In this study, while we showed a comparison between a 7B Mistral model and a 45B parameter model, all other experiments were conducted with the 7B model. Thus, we did not study different model sizes in depth. (3) The multilingual experiments we conducted, only considered English as the target language and French and German as the alternative language. This experiment can be extended with a few other languages."}, {"title": "A Appendix", "content": "We share the details of the evaluation datasets, knowledge base, and the settings used in the ablation study."}, {"title": "A.1 Statistical Analysis of Knowledge Base", "content": "Table 4 shows the statistical analysis of the knowledge base."}, {"title": "A.2 Prompt Variants", "content": "The prompts for the different variants in the Prompt Design section are shown as follows:\n\u2022 HelpV1: You are a truthful expert question-answering bot and should correctly and concisely answer the following question.\n\u2022 HelpV2: You are an accurate and reliable question-answering bot. Please provide a precise and correct response to the question following.\n\u2022 HelpV3: As an expert QA bot, your role is to offer precise, accurate responses. Ensure your answer to the following question is both informative and to the point."}, {"title": "A.3 Contrastive ICL Prompt", "content": "The prompt structures for the different variants in the Contrastive In-context Learning (ICL) section are outlined as follows:\n\u2022 Baseline Prompt: \"HelpV1. Considering this information: Retrieved Context K. Question: q, Answer:\"\n\u2022 ICL1D Prompt: \"HelpV1. Considering this example: Question: q, Correct Answer: Answer correct. Question: q, Correct Answer:\"\n\u2022 ICL2D Prompt: \"HelpV1. Considering these examples: Question: q, Correct Answer: Answer correct. Question: q, Correct Answer: Answer correct. Question: q, Correct Answer:\"\n\u2022 ICL1D+ Prompt: \"HelpV1. Considering these examples: Question: q, Correct Answer: Answer correct. Question: q, Incorrect Answer: Answer incorrect. Question: q, Correct Answer:\"\n\u2022 ICL2D+ Prompt: \"HelpV1. Considering these examples: Question: q, Correct Answer: Answer correct. Question: q, Incorrect Answer: Answerincorrect. Question: q, Correct Answer: Answer correct. Question: q, Incorrect Answer: Answer incorrect. Question: q, Correct Answer:\""}, {"title": "A.4 Multilingual Setting", "content": "In the multilingual setting, we randomly replace English documents with French or German documents before embedding them for the MultiLingo and MultiLingo+ variants. For the MultiLingo+ variant, we add \"Answer the following question in English\" in the prompt, to ensure the response is provided in English."}]}