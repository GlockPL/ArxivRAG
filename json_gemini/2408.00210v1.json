{"title": "A Prior Embedding-Driven Architecture for Long Distance Blind Iris Recognition", "authors": ["Qi Xiong", "Xinman Zhang", "Jun Shen"], "abstract": "Blind iris images, which result from unknown degradation during the process of iris recognition at long distances, often lead to decreased iris recognition rates. Currently, little existing literature offers a solution to this problem. In response, we propose a prior embedding-driven architecture for long distance blind iris recognition. We first proposed a blind iris image restoration network called Iris-PPRGAN. To effectively restore the texture of the blind iris, Iris-PPRGAN includes a Generative Adversarial Network (GAN) used as a Prior Decoder, and a DNN used as the encoder. To extract iris features more efficiently, we then proposed a robust iris classifier by modifying the bottleneck module of InsightFace, which called Insight-Iris. A low-quality blind iris image is first restored by Iris-PPRGAN, then the restored iris image undergoes recognition via Insight-Iris. Experimental results on the public CASIA-Iris-distance dataset demonstrate that our proposed method significantly superior results to state-of-the-art blind iris restoration methods both quantitatively and qualitatively, Specifically, the recognition rate for long-distance blind iris images reaches 90% after processing with our methods, representing an improvement of approximately ten percentage points compared to images without restoration.", "sections": [{"title": "Introduction:", "content": "As biometric technology becomes increasingly prevalent in security authentication, iris recognition has gained considerable attention for its unique features and resistance to forgery [1,2]. However, in certain challenging environments, such as when iris images are captured from a distance in uncontrolled settings, various factors\u2014such as low resolution, blur, and noise, or combinations of these issues\u2014often cause significant degradation in image quality, as shown in Figure 1.\nFrom Figure 1(a), it is evident that the iris image captured at a distance of three meters using specialized equipment has very clear textures, making it suitable for iris classification and recognition. However, in Figure 1(b), due to the influence of certain environmental factors, the captured iris image is very blurry, making the iris textures difficult to recognition. This situation will severely affect the accuracy of iris recognition [3,4].\nCurrent hardware devices are often constrained by cost and technological limitations, making it challenging to achieve satisfactory recognition results for degraded iris images. Consequently, enhancing algorithms to improve iris recognition rates has become a prominent research focus. Among these enhancement methods, iris image restoration algorithms are particularly noteworthy. Their objective is to convert low-quality, low-resolution iris images into high-quality, high-resolution ones, thereby increasing the robustness of iris recognition systems."}, {"title": "Related Works", "content": ""}, {"title": "Iris recognition.", "content": "Iris recognition technology distinguishes individuals by using algorithms to model the unique textures of their irises. Daugman was the first to propose a successful commercial iris recognition system [19]. This system uses calculus operators to detect the inner and outer boundaries of the iris, and 2D Gabor filters to extract iris features. Recognition is then performed by calculating the Hamming distance. While Daugman's method shows excellent performance with ideal iris images, its recognition rate diminishes significantly with non-ideal images. Since then, many researchers have proposed improved methods [20-22]. However, these enhancements are still primarily based on ideal iris images, limiting their effectiveness in real-world scenarios.\nIn recent years, the rapid development of deep learning technology has led to significant progress in iris recognition. Deep learning models can automatically extract features, enhancing the accuracy and robustness of iris recognition systems. Nguyen et al. [23] were among the first to investigate the performance of pre-trained CNNs in iris recognition. They discovered that although these CNNs were initially trained to classify general objects, they were also effective in representing iris images. This approach successfully extracted discriminative visual features and achieved satisfactory recognition results on two iris datasets. Minaee et al. [24] explored the application of deep features extracted from VGG-Net for iris recognition. Their method was tested on two well-known iris databases and demonstrated satisfactory results. Luo et al. [25] designed a deep learning model incorporating spatial attention and channel attention mechanisms. These mechanisms were directly integrated into the feature extraction module, enabling the model to efficiently learn the most important features while suppressing unnecessary ones. Hafner et al. [26] adapted the Daugman-defined iris recognition pipeline by using the DenseNet-201 convolutional neural network as the feature extractor. This adaptation achieved a recognition accuracy of 97.3%.\nThe iris features extracted through deep learning are often utilized in iris"}, {"title": "Iris Image Restoration.", "content": "We reviewed all literature on iris image enhancement from the past five years. The techniques discussed in these papers can be categorized into three types: traditional algorithms, CNN-based methods, and GAN-based methods. Traditional algorithms encompass methods that do not utilize deep learning. For instance, Liu M et al. [30] used a fuzzy filter on the region outside the iris boundary to reduce interference, allowing deep learning models to better focus on the iris features. While this method improved the recognition rate, it did not fundamentally enhance low-quality iris images. Additionally, papers [4] and [31] enhanced the CLAHE algorithm from different perspectives, essentially using interpolation methods.\nPapers [12] and [32] explore the use of CNN to achieve super-resolution in iris images. Paper [12] focuses on evaluating different CNN architectures, highlighting the importance of maintaining texture details to generate more realistic images. In contrast, Paper [32] investigates various CNN architectures and also examines how image reprojection can enhance the accuracy of iris recognition systems. Furthermore, Paper [13] introduces an efficient iris image super-resolution network (ESISR). This network significantly reduces computational costs by minimizing the number of parameters and employing a sharpness-based loss function, all while maintaining image quality. This makes ESISR particularly suitable for mobile device applications.\nPaper [33] proposes a method that utilizes a densely connected convolutional network as the generator. This approach combines adversarial learning with an identification loss function for joint training, thereby enhancing both the super-resolution quality and the recognition capability of iris images. Building on this, Paper [34] introduces the ocular super-resolution network (OSRCycleGAN), which is based on a cycle-consistent generative adversarial network (CycleGAN). This method aims to achieve super-resolution reconstruction from low-resolution ocular images."}, {"title": "Blind Image Restoration Techniques", "content": "The aforementioned literature primarily addresses non-blind image restoration. Currently, there is no research specifically focused on blind iris restoration. However, several techniques have been developed for blind face restoration, including HiFaceGAN [36], PSFR-GAN (Progressive Semantic-Aware Style Transformation for Blind Face Restoration) [37], PULSE (Photo Upsampling via Latent Space Exploration) [38], and GPEN (Generative Facial Prior-Embedded Network) [18], among others.\nHiFaceGAN is a GAN specifically designed for face image restoration. Its generator includes multiple sub-networks, each responsible for different levels of restoration tasks. PSFR-GAN progressively restores face images by utilizing semantic information and style transformation to improve the quality of restoration. PULSE is a super-resolution technique that does not directly optimize pixel-level losses. Instead, it finds the high-resolution output that best matches the low-resolution input through latent space exploration. GPEN is a method for face restoration and enhancement that employs a generative network embedded with facial prior knowledge. This approach effectively addresses various issues in face images, such as blurriness, noise, and compression artifacts. This paper can draw on the above-mentioned blind face restoration techniques to achieve blind iris restoration.\nThis paper aims to adapt the aforementioned Iris recognition technology and blind face restoration techniques to achieve blind iris recognition. We use a GAN as a Prior Decoder and a Pre-Trained Insight-Iris model to construct a new blind iris image restoration network. Insight-Iris is a robust iris classifier, which we developed by modifying the bottleneck module of InsightFace."}, {"title": "Proposed Method", "content": ""}, {"title": "System Framework", "content": "Given the unknown reasons for the degradation of blind iris images, this paper proposes to integrate a pre-trained GAN generator with an iris classifier into a single network. By freezing certain parts of the GAN network and fine-tuning the iris classifier, the method aims to achieve end-to-end recognition of low-quality iris images. This approach simplifies the process and has the potential to enhance the overall"}, {"title": "Implementation of Iris Image Restoration Network", "content": "Figure 2 clearly shows that the performance of the GAN network is pivotal to the recognition rate when restoring images. The restoration of blind iris images discussed in this chapter exemplifies a classic ill-posed inverse problem. The objective of this paper is to derive a high-quality iris image, denoted as y, from a low-quality blind iris image, denoted as x, as illustrated in Equation (1).\n$x = D(y)$ (1)\nwhere D represents the degradation function (i.e., blurring, adding noise, etc.).\nThe primary challenge in solving the blind image restoration problem is the non-uniqueness of the solution. Many different high-quality images (y') can satisfy (x = D(y')), meaning that various high-quality iris images (y') might degrade into the same low-quality iris image (x). This is especially true when the degradation process results in the loss of critical information.\nExisting methods [12,13,32] generally employ pixel-level loss functions to train DNNs for mapping x to y. As a result, the final output tends to be an average of all high-quality iris images, often lacking detailed features and textures."}, {"title": "Overall Network Structure", "content": "To address the lack of detail and texture in iris images generated by the GAN network, this paper first trains a GAN prior network using StyleGAN technology, as shown in Figure 3. This GAN prior network is then embedded into a DNN as the decoder for high-quality iris image restoration, with the DNN serving as the encoder,"}, {"title": "GAN-Based Prior Network", "content": "U-Net [39] has been successfully applied to various image restoration tasks and is effective in preserving image details. Consequently, our Iris-PPRGAN adopts a U-shaped encoder-decoder architecture, as depicted in Figure 4. The GAN prior network is designed to meet two requirements: firstly, it must be capable of generating high-definition iris images; secondly, it should be easily integrated into the U-shaped network structure to function as a decoder.\nInspired by recent GAN architectures like StyleGAN [40,41], this paper employs a mapping network to project the latent code (z) into a less entangled space, as shown in Figure 3. The intermediate code (w) is then distributed to each GAN block. When integrating the GAN prior network into a DNN for fine-tuning, feature maps generated from each encoder layer are directly passed to corresponding decoder layers as noise inputs for the GAN blocks, as illustrated in Figure 4. This integration helps restore image details and contextual information because these feature maps carry significant spatial information from the input data. Therefore, when embedding the GAN prior"}, {"title": "DNN Encoder with Multi-Head Attention Mechanism", "content": "Given that the input low-quality iris image has a size of 256\u00d7256\u00d73, a DNN comprising seven convolutional layers is employed as the encoder. The encoder extracts key features from the low-quality image to generate the input controls for the GAN, including the latent code and noise inputs. The network structure of the encoder is detailed in Table 1.\nThe design concept of the encoder is as follows:\n1) Use the output of the fully connected layers (i.e., deeper features) to replace the latent code z. These deep features encapsulate high-level abstract information extracted from the input image, which is used to govern the global structure of the generated iris image.\n2) Use the shallow outputs of the encoder to replace noise inputs. These shallow features contain more local and detailed information, which manage local aspects in the"}, {"title": "Discriminator Module and Loss Functions", "content": "The discriminator network used in the paper directly adopts the implementation of the StyleGAN2 discriminator. To fine-tune the proposed GAN model, we employ three loss functions: adversarial loss $L_a$, L1 smooth loss, and identity loss $L_{id}$. The adversarial loss $L_a$ is inherited from the GAN prior network.\n$L\u2081 = ming 1G max\u0189 E(x) log(1 + exp(-D(G(X)))) $ (2)\nWhere, X and X represent the real high-definition image and the degraded low-quality image, respectively. G denotes the generator during training, and D denotes the\ndiscriminator.\nSmooth L1 Loss, also known as Huber Loss, is commonly used in regression problems. It combines L1 and L2 loss to mitigate the impact of outliers on model training. Smooth L1 Loss uses the squared term for smaller errors and the absolute value for larger errors. This approach maintains the robustness of the loss function while reducing the risk of gradient explosion.\n$L1 =\\begin{cases}0.5\\cdot(X-G(X))^2 & \\text{if } | X-G(X)|<1\\\\| X-G(X)|-0.5 & \\text{otherwise}\\end{cases}$(3)\nwhere, X and X represent the real high-definition image and the degraded low-quality image, respectively. G denotes the generator during training.\nIdentity loss $L_{id}$ relies on the dot product between feature vectors to measure similarity. The larger the dot product, the more similar the two are.\n$L_{id}=1-F(X)\\cdotF(G(X))$\nWhere, F represents the pre-trained iris classifier used to extract feature vectors.\nThe final loss L is as follows:\n$L= L_a+L_1+L_{id}$ (5)\nSmooth L1 Loss enhances fine image details and preserves original color information. Introducing identity loss $L_{id}$ helps balance the adversarial loss $L_a$, leading to the restoration of more realistic and identity-consistent iris images."}, {"title": "Implementation of the Classifier", "content": "As illustrated in Figure 2, the classifier is a pivotal component of our system. An effective classifier can better extract features from iris images, leading to higher recognition rates. Commonly used classifiers include VGG, ResNet50, and other deep CNNs. Recent advancements in deep learning have introduced even more powerful classifiers, such as the InsightFace framework used in face recognition [42].\nIn this paper, we introduce a classifier specifically designed for iris recognition, named Insight-Iris, which is based on the InsightFace framework. The structure of Insight-Iris is detailed in Figure 6."}, {"title": "Experimental Results and Analysis", "content": ""}, {"title": "Experimental Dataset", "content": "In this experiment, we used the CASIA-Iris-Distance dataset from the Chinese Academy of Sciences . This dataset was created with an advanced biometric sensor capable of detecting iris and facial features up to 3 meters away. The high-resolution images include both irises and facial features and comprise 142 subjects with a total of 2,567 images. Each image has a resolution of 2352\u00d71728 pixels. Figure 7 shows three example images from the CASIA-Iris-Distance database.\nFrom this dataset, we selected 141 classes of images. Of these, 102 classes containing a total of 1,748 left-eye iris images were used to train the GAN's prior network. Corresponding low-quality iris images were then synthesized from these high-quality images to fine-tune the Iris-PPRGAN network. To evaluate the model's performance, 80% of the remaining 39 classes, totaling 624 images, were used to train"}, {"title": "Training Strategy", "content": "The training of the entire network is organized into four steps:\n1, Pre-train the GAN Prior Network: Select 102 classes of images from the experimental dataset and pre-train the GAN prior network following the training strategy of StyleGAN.\n2, Train the Iris Classifier: Select an additional 39 classes of images to train the Insight-Iris classifier.\n3, Fine-tune the Iris-PPRGAN Network: Embed the pre-trained GAN model into the proposed Iris-PPRGAN network and fine-tune the entire network using a set of synthesized low-quality (LQ) and high-quality (HQ) iris image pairs (the image synthesis process is detailed in section 4.3).\n4, Integrate and Fine-tune: Combine the trained Iris-PPRGAN network with the trained iris classifier to create a system for long-distance low-quality iris classification. Utilize the input low-quality iris images, freeze the Iris-PPRGAN network, and fine-tune the iris classifier.\nThis structured approach ensures a cohesive and efficient training process for the entire network."}, {"title": "Implementation of Low-Quality Iris Images", "content": "Since the pre-trained GAN prior network must be embedded into Iris-PPRGAN for fine-tuning, we need to construct low-quality (LQ) to high-quality (HQ) image pairs. To achieve this, we use the following degradation model to synthesize degraded iris"}, {"title": "Experimental Results of the GAN Network", "content": "Fine-tuning the model with numerous severely degraded images, such as those in Figure 8, enables the DDN encoder component of the Iris-PPRGAN to learn to generate appropriate latent codes and noise. These inputs are then fed into the GAN prior decoder network, which is simultaneously updated to effectively handle severely degraded iris"}, {"title": "Experimental Results of the Iris Classifier", "content": "The iris classifier in our method has a dual role: it enhances recognition accuracy and accurately extracts iris features to calculate the identity loss of the GAN network. Given that the input images are blind iris images similar to Figure 9(a), traditional methods that precisely locate the iris region are no longer feasible under such low-quality constraints. Therefore, we use the entire low-quality iris image as input. This approach not only simplifies computation but also enables the model to learn the global information of the iris image.\nThis study uses 80% of a total of 39 classes of iris images, amounting to 624 images, for the training set and employs transfer learning to train the iris classifier. The remaining 20%, or 156 iris images, are used as the test set. Table 3 presents the operating environments for the different deep convolutional neural networks, while Table 4 shows the classification recognition results. All experiments are conducted in a Windows 10 environment."}, {"title": "Recognition Experiment Results for Long-Distance Blind Iris", "content": "In the experiments, we use traditional image restoration algorithms and various GAN models to replace the GAN shown in Figure 2. Using the method described in section 4.2, low-quality iris images are generated from the 624 training images mentioned in section 4.1 and used as input to Figure 2 to fine-tune the Insight-Iris classifier. Table 5 presents the performance of the long-distance iris classifier under different image restoration algorithms used in Figure 2."}, {"title": "Ablation Experiments", "content": "To investigate the effectiveness of the components of the proposed model, two types of ablation experiments were designed.\n(1) Effect of Different Classifiers on Recognition Rate: Various classifiers were used to replace the iris classifier shown in Figure 2, in order to test the recognition rate of blind iris images. The results are documented in Table 6.\nTable 6 Effect of Different Classifiers on Recognition Rate for Blind Iris Images\n(2) Effect of the Proposed System on Original Iris Images: Although the proposed system is designed for blind iris images, the quality of input images cannot be determined in practical applications. To verify the robustness of BIRN, we compared the performance of original, non-degraded iris images under different restoration algorithms. The experimental results are documented in Table 7.\nTable 7 shows that the restoration algorithms have a slight impact on the recognition rate of original, non-degraded iris images . Unlike other restoration methods, our proposed Iris-PPRGAN not only maintains but actually improves the recognition rate of the original iris images by 0.62%. This demonstrates its strong robustness."}, {"title": "Conclusion", "content": "This paper introduces a prior embedding-driven architecture for the recognition of long-distance blind iris images. The system integrates two key components: a novel iris image restoration network called Iris-PPRGAN, and a new iris classifier named Insight-Iris. In developing Iris-PPRGAN, we achieve high-quality iris image restoration by embedding a pre-trained GAN into a U-shaped deep neural network as a decoder, and fine-tuning the entire GAN network using low-quality iris images extracted from artificially degraded facial images. Subsequently, different low-quality iris images are fed into Iris-PPRGAN to fine-tune the iris classifier. Experimental results demonstrate that the methods we proposed can achieve a recognition rate of up to 90%, improving the recognition rate by 10% and outperforming existing techniques for low-quality iris restoration. This ensures robustness and accuracy in recognizing long-distance, low-quality blind iris images."}]}