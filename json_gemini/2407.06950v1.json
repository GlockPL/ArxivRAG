{"title": "Spanish TrOCR: Leveraging Transfer Learning for Language Adaptation", "authors": ["Valentin Laurent", "Filipe Lauar"], "abstract": "This study explores the transfer learning capabilities of the TrOCR architecture to Spanish. TrOCR is a transformer-based Optical Character Recognition (OCR) model renowned for its state-of-the-art performance in English benchmarks. Inspired by Li et al.'s assertion regarding its adaptability to multilingual text recognition, we investigate two distinct approaches to adapt the model to a new language: integrating an English TrOCR encoder with a language specific decoder and train the model on this specific language, and fine-tuning the English base TrOCR model on a new language data. Due to the scarcity of publicly available datasets, we present a resource-efficient pipeline for creating OCR datasets in any language, along with a comprehensive benchmark of the different image generation methods employed with a focus on Visual Rich Documents (VRDs). Additionally, we offer a comparative analysis of the two approaches for the Spanish language, demonstrating that fine-tuning the English TrOCR on Spanish yields superior recognition than the language specific decoder for a fixed dataset size. We evaluate our model employing character and word error rate metrics on a public available printed dataset, comparing the performance against other open-source and cloud OCR spanish models. As far as we know, these resources represent the best open-source model for OCR in Spanish. The Spanish TrOCR models are publicly available on HuggingFace [20] and the code to generate the dataset is available on Github [25].", "sections": [{"title": "1 Introduction", "content": "Optical Character Recognition (OCR), is the process of reading an image. A computer cannot read an image because it only contains pixels. OCR is the technique that translates the text represented by those pixels into a machine-understandable format. An OCR system normally is composed of a text detection and a text recognition algorithm, where the text detection problem can be seen as an object detection task where the only object is the text and the text recognition problem can be seen as a multi-modal problem where we have the cropped image as the input and the transcribed text inside this image as the output. Most of OCR research focus on the scene text recognition problem [13,12,18,19,26] and/or English datasets [7,9,10,8]. In this paper, we will focus on the text recognition part of OCR. More specifically, we explore the multi language adaptation capabilities of Transformer based text recognition models applied to Visual Rich Documents (VRDs).\nThe end goal of an OCR system is to be able to read like a human, which means that does not matter the font, the background or the writing style, it will be able to read it. However, to evaluate the quality of an OCR model, we usually take a specific dataset, split it into two sets, train/fine-tune the model in one set and evaluate it in the other set, but both are part of the same initial dataset. This approach does not test the general reading capability of the model as it is in-sample data, despite it being split between train and test sets. In this study, we propose to evaluate the OCR model in an out-of-sample way, where we pre-train the model in a large language-specific synthetic corpus adapted to VRDs and then we evaluate this model out-of-the-box in an unseen dataset.\nIn 2021, Li et al. [15] released a paper presenting the TrOCR architecture, a transformer-based OCR model renowned for its exceptional performance on English benchmarks. TrOCR represents a state-of- the-art approach in optical character recognition, employing transformer architectures for robust text recognition. Unlike previous OCR models, TrOCR leverages transformers for both image interpretation and text generation, enabling more efficient and accurate recognition of textual content. Its transformer- based design allows for comprehensive language understanding, facilitating superior performance across"}, {"title": "2 Related Work", "content": "Optical Character Recognition is, by default, a multi-modal problem, where we have an image as input and text as output. There are two main approaches for OCR, CTC-based models and transformer-based models.\nShi, Bai and Yao [22] proposed CRNN, an end-to-end model that uses a CNN as to encode the visual information into columns and an RNN to decode it into text. They use a CTC decoding layer to post-process the output by removing the repeated symbols and all the blanks from the labels to achieve the final prediction. Multiple open-source OCR frameworks are using this architecture as their default recognition model, such as EasyOCR [11] and Paddle OCR [5].\nLi et al. [15] proposed TrOCR, an end-to-end transformer model that uses an image transformer as the encoder and a text transformer as the decoder. Relying fully on the transformer architecture allows the model to be flexible on the size of the architecture and the weights initialization from pre-trained checkpoints. In the paper, they propose three variants of the model: small (total parameters=62M), base (total parameters=334M) and large (total parameters=558M) versions. This diversity enables us to strike a balance between resource efficiency and parameter richness, thus enhancing the model's capability to understand language nuances and image details.\nWithin the TrOCR architecture, all variants leverage vision transformers in their encoders, each with its unique architectural components. The small version adopts the DeiT architecture [24], while the base and large versions incorporate a BeiT [2] model. Additionally, there is variation among the decoders: the small version integrates MiniLM [27], while the base and large versions use RoBERTa [16]. These three models have been trained on a dataset comprising hundreds of millions of English samples and are publicly available on the HuggingFace platform [17].\nAn important difference between transformer and non-transformer-based models is that for the first one, we need a huge amount of data to pre-train the model due to the lack of inductive bias in transformers [4]. Without a pre-training step on synthetic data, TrOCR performance decreases a lot and the model is not able to generalize for unseen images."}, {"title": "2.2 Datasets", "content": "Most of the available benchmarks in the literature are in English and for the problem of scene text recognition. As widely used datasets for scene text recognition we have IIIT5K [18], SVT [26], IC13 [13], IC15 [12], SVTP [19], and CT80 [21]. The biggest challenge of these datasets is the curved and rotated text present in the images and the different backgrounds of the text.\nFor documents, we have 3 widely used datasets in the literature, 2 for printed and 1 for handwritten data. For printed data, we have the SROI (Scanned Receipts OCR and Information Extraction) [9] and the FUNSD (Form Understanding in Noisy Scanned Documents) [7] datasets. These datasets contain scanned English documents of receipts and forms, that despite being real images, cannot capture the diversity of documents present in our world. For handwritten data, we have the IAM dataset containing 82,227 English words produced by 400 different writers.\nThe only publicly available multi-language OCR dataset on printed documents is the XFUND (Multi- lingual Form Understanding) dataset [28], which contains visually rich documents annotated in 7 different languages. However, for each language, we have only a few thousand examples, which is not enough for training an OCR system.\nAs the OCR models, especially the transformer ones, need millions of data in the pre-training step [15], synthetic datasets such as MJSynth [10] and SynthText [8] are commonly used. These two datasets together have a total of 16M image-text pairs, but the problem is that they contain only English words.\nTo overcome the problem of having only English words, there are open source tools to generate synthetic data such as TRDG (Text Recognition Data Generator), that can generate image-text pairs for every language as it relies on the text font. The tool also allows you to generate the images with multiple augmentation techniques such as rotation, gaussian blurring, dilation, erosion and others. However, these data augmentation methods are more general computer vision techniques and do not take into account the specificities of VRDs."}, {"title": "3 Methodology", "content": "For an effective OCR system, a robust architecture and a comprehensive dataset are essential. Especially, the dataset should be extensive and representative enough to enable the model to learn how to read accurately in a specific language. In this section, we will explore how to generate a synthetic dataset in any language in order to train our model for the VRD problem."}, {"title": "3.1 VRDs image-text generation dataset", "content": "Developing a dataset of labeled text images needs the availability of an existing text corpus from which the images can be derived. Numerous such corpora are accessible in the public domain [1,3]. However, to get a more diverse dataset that we have control over the content, we opted to extract our own corpus by scraping some Spanish Wikipedia pages. This approach also allows us to be free from existing datasets, thereby enabling us to tackle any other language. We also ensured a uniform distribution in terms of text length to prevent biasing the dataset. Throughout our experiments, we noticed that a corpus consisting of 2,000,000 sentences gives good enough generalization capabilities to the model.\nTo generate text images for our dataset, we explored the use of deep learning models, specifically pre- trained generative models such as ScrabbleGAN [6] and diffusion models, such as Diffusion-Handwriting- Generation [23]. However, a significant challenge lies in producing accurate images that genuinely mirror the intended content. This task is made even more difficult by a persistent issue known as mode collapse. This phenomenon occurs when the generative model learns only a limited subset of the probability distribution. Consequently, the model starts generating repetitive and unrepresentative samples. This issue becomes particularly pronounced when dealing with longer sentences, leading to outputs that may not fully represent the intended diversity and complexity of the data adapted to VRDs.\nThe most naive approach is to artificially create text on a blank image using a specific font style. While this method may seem straightforward, it presents multiple limitations, particularly in terms of diversity and realism. These limitations become even more pronounced when we consider the complex layouts, varied fonts, presence of artifacts and intricate designs often found in VRDs. Therefore, our focus is on developing a robust tool that can effectively handle the intricacies of these visually rich documents.\nTo address these challenges and better represent the complexities of VRDs, we first introduced an element of randomness into some critical parameters such as font style and size, color, and padding. In"}, {"title": "3.2 Models", "content": "To train the OCR model, we tried two different approaches. The first approach involves taking the English checkpoints in their three versions and fine-tuning them using the VRDs text generator to create a specialized Spanish dataset for OCR. However, it is crucial to note that this is only feasible if the initial tokenizer of the TrOCR can handle all the characters within the corpus. Therefore, this approach is only suitable for Latin languages, on which the tokenizer has been fitted. Essentially, we want the model that already knows how to read to be able to understand Spanish.\nThe second approach is based on the idea that the main component of the TrOCR responsible for language understanding is the text decoder. Therefore, the idea is to pair the encoder of the TrOCR with a decoder-like structure trained on Spanish data. The crucial components facilitating this merge are the cross-attention layers, which are responsible for establishing the connection between the encoder and the decoder. In our approach, we opted to link the output of the encoder directly to the input of the cross-attention layers of the decoder. Initially, the weights of these layers are randomly initialized, and subsequent training is required to optimize their performance. Essentially, this model knows Spanish and needs to learn how to read.\nThe primary goal of these two approaches is to compare both methods. To ensure a fair comparison, we need the models to have a similar number of parameters. To achieve this, we chose to create a small and"}, {"title": "4 Results", "content": "In this section, we evaluate the data augmentation techniques applied to the model training against a fixed test set. Additionally, we evaluate the trained models on the XFUND Spanish dataset and compare their performance with two other existing Spanish OCR models."}, {"title": "4.1 Image generation augmentation benchmarking", "content": "To fully leverage the dataset creation pipeline, it is essential to benchmark all data augmentation meth- ods. Table 2 presents the CER and WER results for the selected OCR models under various training methods and data augmentation scenarios. The test set has 10k generated images with random augmen- tations. In the creation of the test set, we made sure that the input text was not present in the training set. This benchmark offers clear insights into the impact of the different augmentation techniques on model performance."}, {"title": "4.2 XFUND Spanish dataset", "content": "To evaluate the capacity of our model in a real-world scenario, we used the XFUND Spanish dataset. XFUND [28] is a multi-language dataset that contains 7 different languages. We collected the Spanish data of this dataset which contains 11,449 images-text pairs in the training set and 3,413 in the test set. It is important to mention that the XFUND has multiple problems on the annotations, we will better explore this later in this section.\nTo assess the real capacity of an OCR model, we believe that it should be tested in an out-of-sample dataset. Therefore, we use the pre-trained models from the synthetically generated VRD dataset and we evaluate them directly in the test set of Spanish XFUND without further fine-tuning in the training set.\nWe compare our results against two other OCR solutions available in the market. We compare to the well-known open-source OCR library EasyOCR-spanish, which is a CNN-LSTM based model [11]. Additionally, we also compare with the Azure OCR cloud API, renowned for its efficiency. The comparison results on the XFUND dataset are presented in Table 3."}, {"title": "4.3 Limitations and future research", "content": "This model was not trained on handwritten data. We added handwritten fonts in the dataset generation but they do not capture the nuances of different written styles that people may have, so we do not suggest using this model for handwritten recognition without further fine-tuning.\nAs the English version of TrOCR, this model was trained to recognize single-line text. If you add multi-line text images, the model may hallucinate. Differently from the English TrOCR version, we did not add vertical text, so our model is not able to recognize those.\nAfter fine-tuning the models in Spanish, they lost their capacity to predict English text when there is a similar word in Spanish. For future research, we would like to study how to make TrOCR learn a new language without losing the first one and also to include multiple languages in the same model.\nFor future work, we would also like to focus more on handwritten recognition and non-Latin languages using a language-specific decoder."}, {"title": "5 Conclusion", "content": "In this study we explored the multi language adaptation capabilities of TrOCR. Facing the challenge of lacking a sufficiently robust dataset for model training, specially for Visual Rich Documents, we propose a language agnostic method to synthetically generate an enormous dataset of image-text pairs to train an OCR model. This method takes into account specificities of VRDs such as boxes between character, horizontal and vertical lines and artifacts coming from the text decoder propagation error.\nWe explore two training approaches, fine-tuning and English checkpoint of TrOCR into Spanish and substituing TrOCR's decoder to a pre-trained Spanish one and fine-tuning it. Both approaches were fine-tuned with your synthetic Spanish dataset and we evaluated them in an out-of-sample manner on the XFUND Spanish dataset without further fine-tuning. We analyse the results based on the character error rate (CER) and word error rate (WER), considering them as the most suitable metrics for OCR evaluation.\nOur findings indicate that the English TrOCR model, fine-tuned in Spanish, effectively captures information from images compared to models utilizing a mix of an English encoder and a Spanish decoder. Notably, models with an English encoder and a Spanish decoder failed to achieve metrics comparable to the fine-tuned English one. Consequently, we conclude that, for the set of hyper-parameters and architectures chosen, fine-tuning the TrOCR English version in Spanish was better than modifying the decoder to a Spanish version and then fine-tuning.\nAs a result, we have introduced a method for training a TrOCR in any language that can achieve high performance, comparable to professional OCR models on VRDs. This could have a significant impact on the industry, particularly considering the scarcity of publicly available OCR for languages other than English. We open-source the code to generate the dataset and the trained models."}]}