{"title": "TIME TRANSFER: ON OPTIMAL LEARNING RATE AND BATCH SIZE IN THE INFINITE DATA LIMIT", "authors": ["Oleg Filatov", "Jan Ebert", "Jiangtao Wang", "Stefan Kesselheim"], "abstract": "One of the main challenges in optimal scaling of large language models (LLMs) is the prohibitive cost of hyperparameter tuning, particularly learning rate \u03b7 and batch size B. While techniques like \u00b5P (Yang et al., 2022) provide scaling rules for optimal \u03b7 transfer in the infinite model size limit, the optimal scaling behavior in the infinite data size limit (T \u2192 \u221e) remains unknown. We fill in this gap by observing for the first time an interplay of three optimal \u03b7 scaling regimes: \u03b7 \u00d7 \u221aT, \u03b7 \u00d7 1, and \u03b7 \u00d7 1/\u221aT with transitions controlled by B and its relation to the time-evolving critical batch size Berit XT. Furthermore, we show that the optimal batch size is positively correlated with Bcrit: keeping it fixed becomes suboptimal over time even if learning rate is scaled optimally. Surprisingly, our results demonstrate that the observed optimal \u03b7 and B dynamics are preserved with \u00b5P model scaling, challenging the conventional view of Berit dependence solely on loss value. Complementing optimality, we examine the sensitivity of loss to changes in learning rate, where we find the sensitivity to decrease with T\u2192\u221e and to remain constant with \u00b5P model scaling. We hope our results make the first step towards a unified picture of the joint optimal data and model scaling.", "sections": [{"title": "1 INTRODUCTION", "content": "Large Language Models (LLMs) have increasingly become a prominent area of study in the field of Natural Language Processing (NLP) and beyond. They have demonstrated significant improve-ment in performance across a wide range of tasks, such as language understanding, text generation, translation, and summarization, showing results comparable or outperforming those of an average domain expert (Dubey et al., 2024; OpenAI et al., 2024; Team et al., 2023). The primary advan-tage of LLMs is their ability to scale well with increased computational resources, which results in predictive improved performance (Kaplan et al., 2020; Hoffmann et al., 2022).\nOne of the main challenges in LLM scaling lies in the proportional scaling of computational re-sources required for hyperparameter tuning. To remedy this, \u00b5Transfer (Yang et al., 2022) technique was proposed as a way to transfer hyperparameters from a small (proxy) model to a large (target) one by introducing scaling rules for learning rate, weight multipliers and initialization scale, altogether referred to as Maximal Update Parametrization (\u00b5P). While significantly reducing the hyperparam-eter tuning cost coming with model scaling, its applicability is limited by requiring both target and proxy models to share the same batch size and number of training iterations. With current pretrain-ing budgets surpassing trillions of tokens, it makes \u00b5Transfer computationally expensive to apply even with tuning a small proxy model.\nOne solution would be hyperparameter tuning performed both for the small proxy model and on the small dataset, followed by \u00b5Transfer to the larger model and larger dataset, under assumption of both datasets being sampled from the same underlying data distribution. This raises the question of \u00b5Transfer's applicability in the infinite data limit, which can be formalized as an increase in the size of the training dataset, which in the LLM case is measured by the number of tokens. Understanding training dynamics in this limit would unlock hyperparameter transfer not only across model scales, but also across data horizons, thus removing the largest limitation of \u00b5Transfer."}, {"title": "2 METHODOLOGY", "content": ""}, {"title": "2.1 TERMINOLOGY", "content": "Time (T): we often use the terms time, token budget, and data horizon interchangeably, both to specify the measure of the training data size in tokens, and to pinpoint the specific moment throughout the model training. From this perspective, an infinite data limit T \u2192 \u221e, as opposed to a fixed budget regime with T = const, refers to an (infinite) increase of the number of tokens seen by the model during pretraining.\n\u00b5P: we refer to a model with width $d_{base}^{model}$ as a base model if \u00b5P scaling multipliers for learning rates, weight multipliers and initialization scale (Sec. 2.2) are computed relative to this width. This brings us to a broader view on \u00b5P where the base model \"pinpoints\" the training dynamics for all the other models obtained either by scaling up or down the base $d_{base}^{model}$ width. Together with the base model, we refer to this set of models as a \u00b5P model family or as a \u00b5P trajectory if the direction of scaling is implied. We also slightly distinguish between the base and proxy models, where the former is used to define a \u00b5P model family, while the latter is a model used to tune hyperparameters to be transferred with \u00b5Transfer to a target model."}, {"title": "2.2 MODEL CONFIGURATION AND DATASETS", "content": "For all our experiments we use a default MPT model architecture (MosaicML, 2023) as implemented in the 1lm-foundry codebase (MosaicML, 2024), with all the models sharing the same training configuration (Appendix A.3). We use the Decoupled AdamW optimizer (Loshchilov & Hutter, 2019) with \u03b2\u2081 = 0.9, \u03b2\u2082 = 0.95, \u20ac = 1e-8, weight decay X = 0 and gradient clipping by the L2 norm value of 1.\n\u00b5P is implemented according to Table 8 of Yang et al. (2022), so that when $d_{model}$ is set to the base model width $d_{base}^{model}$, it replicates Standard Parametrization (SP). That makes our observations for the base models also applicable to setups that use SP rather than \u00b5P. Model weights are initialized from the normal distribution with the base model standard deviation $\u03c3_{init}^{base} = 1/\\sqrt{d_{model}^{base}}$. The models are scaled up/down only in width, with the head dimension $d_{head}$ being always fixed and the number of heads being scaled proportionally to the width scaling.\nThe models are trained with the causal language modeling task on the train split of the Colossal Clean Crawled Corpus (C4) dataset (Raffel et al., 2020), tokenized with the GPT2 tokenizer (Rad-ford et al., 2019) with a vocabulary size of 50257 and a context length of 1024 tokens. As a metric to evaluate model performance, we report the loss on the C4 validation split as $L_{val}$."}, {"title": "2.3 HYPERPARAMETER GRID", "content": "To investigate the interplay of learning rate and batch size in the infinite data limit T \u2192 \u221e, we define a 5D grid spanned by the following axes: \u03b7, \u0392, T, $d_{model}$, $d_{base}^{model}$ (see Appendix A.4 for exact definition). Fundamentally, we are interested in measuring how the loss profile $L_{val}(\u03b7)$ and its optimum value \u03b7* evolve in time T depending on the choice of batch size B. As this measurement is moreover conditioned on the \u00b5P trajectory and a specific point therein, we firstly study this evo-lution for a trajectory pinpointed by one specific base model with $d_{base}^{model}$. We train a set of models within the defined \u00b5P trajectory with different widths $d_{model}$, ranging in size from 32M up to 354M parameters, and measure for each of them the $L_{val}(\u03b7)$ profile at specific points in time T, ranging from 1B up to 275B tokens. Then, we repeat the same measurement for a new \u00b5P trajectory, pin-pointed by a different value of $d_{base}^{model}$. This grid approach allows us to interpret results from multiple perspectives, as we detail in Sec. 3."}, {"title": "2.4 LEARNING RATE SCHEDULE SCALING", "content": "Since we study the training dynamics in the infinite data limit, it necessarily implies training models across different data horizons. This raises the question of how one should adjust the learning rate schedule in this limit. Motivated by recent work of Hu et al. (2024); H\u00e4gele et al. (2024), in all our experiments we use a warmup-stable (WS) version of the warmup-stable-decay (WSD) schedule consisting of a warmup phase with a linear increase of learning rate from 0 to Nmax and a constant phase with learning rate fixed at Nmax, hereafter notated as \u03b7. Our version omits the decay phase to simplify experimentation as we observe that it does not affect the optimal \u03b7 position (Appendix A.6). The warmup duration is fixed across all horizons and across all experiments at an absolute value of $T_{warmup}$ = 219 = 524288 tokens. Whenever batch size is varied, we adjust the number of gradient steps in the warmup phase accordingly so that the total amount of tokens seen by the model during warmup equals 219. We also present additional experiments with different ways to scale the"}, {"title": "3 RESULTS", "content": ""}, {"title": "3.1 LEARNING RATE OPTIMUM DRIFTS IN TIME, WITH BATCH SIZE INTERPOLATING BETWEEN DIFFERENT SCALING RULES", "content": "First, we begin with setting $d_{base}^{model}$ = 1024 and scanning learning rate across different batch sizes and $d_{model}$. We present results for the \u03b7* optimum evolution in time T in Fig. 1a and full $L_{val}(\u03b7)$ pro-file scans in Appendix A.7. In order to reduce statistical uncertainties, in Fig. 1a each data point for budgets T\u2264 235 is an average \u03b7* value across \u03bcP models for a given batch size and horizon length $(\u03b7^*, B, T) = \\sum_{d_{model}} (\u03b7^*, B, T, d_{model})/3$, where $d_{model} \\in {256, 512, 1024}$ and all models share the same base model with $d_{base}^{model} = 10241$.\nFig. 1a illustrates how batch size serves as a parameter interpolating between various learning rate scaling regimes. For the smallest probed batch sizes B = {216, 218} we observe an initially perfect inverse square-root scaling of the optimal learning rate \u03b7* with increase of the data horizon T. The scaling, however, breaks down at some point and optimal learning rate plateaus. We speculate"}, {"title": "3.2 \u039f\u03a1\u03a4\u0399MALLY-TUNED BATCH SIZE INCREASES IN TIME", "content": "Second, we study how optimal hyperparameter values evolve in time to yield optimal loss values. For each batch size and horizon length, we select the best-performing run across the learning rate grid and plot model loss $L_{val}$ against batch size across time horizons for the configuration with ($d_{model}$ = 1024, $d_{base}^{model}$ = 1024). Results are presented in Fig. 2, with a full set of plots across various combinations of ($d_{model}$, $d_{base}^{model}$) in Appendix A.10.\nWe observe an approximate square-root scaling of the optimal batch size with increase of the pre-training token budget from $B^* |_{T=2^{30}} = 2^{18}$ to $B^* |_{T=2^{35}} = 2^{20}$ (Fig. 2a). Emergence of subopti-mality is more pronounced when transposing the token budget and batch size axes (Fig. 2b), where the smallest B = 216 batch size curve, with each point having learning rate scaled with the inverse"}, {"title": "3.3 CRITICAL BATCH SIZE REGION EVOLVES IN TIME, BUT IS UNCHANGED WITHIN \u03bc\u03a1", "content": "In Fig. 1b, we reinterpret Fig. 1a by transposing batch size and token budget axes and plotting the optimal learning rate and batch size scaling jointly per data horizon. This better emphasizes dynam-ics in the fixed token budget regime, rather than the trajectory of optimal learning rate evolution in time. As in Sec. 3.1, we perform an average across \u03bcP models sharing the same $d_{base}^{model}$ = 1024 and also plot the corresponding standard deviation. We include a similar plot for the other base model with $d_{base}^{model} = 256$ in Appendix A.8 and individual plots for each of the ($d_{model}$, $d_{base}^{model}$) configurations in Appendix A.9.\nWe observe that for a given time horizon, the (\u03b7*, B) curve has a bell-like shape, as predicted by Li et al. (2024). The left-hand side of the peak represents a previously known \u03b7 \u00d7 \u221aB scaling rule (Malladi et al., 2023; Shen et al., 2024). However, with our experiments, we uncover a previously unseen right-hand side of the curve, also referred to as \"surge\" by Li et al. (2024), where the optimal learning rate for a fixed token budget scales inversely proportionally to the batch size scaling via the \u03b7* \u03b1 1/\u221aB rule. Although statistical uncertainties are high for the experiment with $d_{base}^{model}$ = 1024, the square-root rules are more pronounced for the experiment with $d_{base}^{model}$ = 256 (Appendix A.8).\nFurthermore, the peak position of the fixed token budget, which we refer to as the critical batch size Bcrit (Sec. 2.1), is evolving in time via an approximately $B_{crit} \\sim T$ scaling rule, showing the same trend as in McCandlish et al. (2018). Moreover, we note that the optimal learning rate corresponding to Bcrit decreased by a factor of two with a horizon scaling from 230 to 237 tokens. This might be a statistical fluctuation since there is no significant decrease for the experiment with $d_{base}^{model}$ = 256 (Appendix A.8).\nLastly, there is a difference of Bcrit evolution between the T and \u00b5P infinite width limits. Specifically, for a fixed token budget, we observe no significant change of the curves' shapes and peak positions across $d_{model}$ values within the same \u00b5P trajectory, and also with the change of the base model (Appendix A.9). At the same time, there is a noticeable drift of Bcrit in the T\u2192\u221e limit with the model being fixed. As both limits are accompanied with a comparable change of the model performance2, this observation brings evidence that dependence of the critical batch size exclusively on the loss value suggested by Kaplan et al. (2020) (Eq. 7) is not entirely complete. Or, contrary to experimental results in Li et al. (2024), the two definitions of the critical batch size region (Ap-pendix A.2) are not the same and should be disentangled."}, {"title": "3.4 LEARNING RATE SENSITIVITY IS REDUCED IN TIME, AND IS UNCHANGED WITHIN \u00b5P", "content": "After having studied the learning rate optimum dynamics, we turn our attention to a broader structure around the optimum from the sensitivity perspective. Specifically, we are interested in how the shape of the $L_{val}(\u03b7)$ curve changes in the time T \u2192 \u221e and \u00b5P width limits. In Fig. 3, we present our observations for the two base models with $d_{base}^{model}$ = $d_{model} \\in {256, 1024}$, for token budgets \u03a4\u2208 {231, 233, 235}. We note that since we implement \u00b5P in a way that the base model is also SP-parametrized, the results should be applicable to this parametrization as well."}, {"title": "4 DISCUSSION", "content": "While originally, we were aiming to find a golden recipe for hyperparameter transfer in the infinite data limit, we show that there is no simple and straight-forward answer. However, we do believe that there exists a deeper underlying perspective on the problem, as opposed to the one of simply tuning learning rate and batch size."}, {"title": "5 RELATED WORK", "content": "(\u03b7, \u0392) scaling rules In efforts to accelerate model training, the \u03b7 \u00d7 B rule for the SGD optimizer was found necessary to avoid performance loss due to increased batch size (Goyal et al., 2018), known as generalization gap (Keskar et al., 2017). Afterwards, additional usage of momentum (Smith et al., 2018) and model scaling (Park et al., 2019) was incorporated, and a \u03b7 \u00d7 \u221aB rule for Adam was observed (Hilton et al., 2022). From the theoretical side, experimentally observed rules were verified with the framework of stochastic differential equations (SDEs) (Smith & Le, 2018; Malladi et al., 2023), loss curvature analysis (Zhang et al., 2019; McCandlish et al., 2018; Li et al., 2024) and random matrix theory (Granziol et al., 2021). While most of the studies were performed in the fixed epoch budget, Shallue et al. (2019) broadened the perspective to other target budget measures and studied the scope of the \u03b7 \u00d7 B rule applicability across various datasets and model architectures. Looking beyond fixed budgets, Smith & Le (2018) showed a linear relation between the optimal batch size and the dataset size (for fixed n), and Smith et al. (2020) similarly presented hints for a linear relation between the optimal learning rate and the dataset size (for fixed B), with both works considering the SGD optimizer. In the modern LLM pretraining context, Hu et al. (2024); DeepSeek-AI et al. (2024) approached this problem by deriving the joint (\u03b7, \u0392) scaling laws.\n\u00b5P Originally developed within the Tensor Program series studying feature learning in the in-finite width limit (Yang & Hu, 2022; Yang et al., 2022), \u00b5P has been gaining traction recently within the LLM community. It has been extensively tested and applied experimentally (Lingle, 2024; Blake et al., 2024; Gunter et al., 2024; Dey et al., 2024), as well as theoretically, with Yang et al. (2023); Bordelon et al. (2024) extending it to the infinite depth limit, and Yang et al. (2024); Bernstein et al. (2023) revisiting it from the spectral normalization perspective. Recently, Everett et al. (2024) showed that other model parametrizations also induce hyperparameter transfer if tak-ing weight alignment into account. Furthermore, they revealed that \u00b5Transfer does not work in the regime of Chinchilla-optimal scaling (Hoffmann et al., 2022). The most closely related work to ours, Shen et al. (2024) expanded on this observation and proposed a learning rate scheduler com-bining \u00b5P and experimentally measured (\u03b7, B) scaling rules to allow for the hyperparameter transfer in the T\u2192 \u221e limit, however only limited to the n* x 1/\u221aT scaling regime.\nSensitivity The topic of loss sensitivity to suboptimal hyperparameter choice is less thoroughly studied, focusing exclusively on learning rate as the most affecting hyperparameter. Wortsman et al. (2023) studied how various optimizer and model interventions, such as weight decay or \u00b5P usage, influence the learning rate sensitivity with the model size scaling. H\u00e4gele et al. (2024) investigated the impact of various learning rate schedule choices, such as length and functional form of the decay phase."}, {"title": "6 CONCLUSION", "content": "In this work, we studied joint model and data scaling in the LLM context from the perspective of optimal learning rate and batch size dynamics. We observed an intricate interplay of three optimal learning rate scaling regimes in the infinite data limit, controlled by the batch size in its relation to the critical batch size as it evolves in time. This dynamic is preserved during model scaling with \u00b5P, as well as the loss sensitivity to the learning rate variation, highlighting the intriguing difference in how \u00b5P infinite width and time limits evolve the critical batch size. Overall, we hope our observations"}, {"title": "A APPENDIX", "content": ""}, {"title": "A.1 HYPERPARAMETER OPTIMIZATION IN THE INFINITE DATA AND MODEL SIZE LIMIT", "content": "We believe our observations provide useful hints on how to scale learning rate and batch size jointly in the infinite data and model size limits. We take the general \u00b5Transfer approach of tuning hyper-parameters for a small proxy model and then transferring them either zero-shot or according to some scaling rules via extrapolation, across model sizes and data horizons.\n1. If one can afford tuning a \u00b5P proxy model on the data horizon of the target model, then it\nis sufficient to simply perform a grid search over learning rate and batch size values to find\nthe best combination, following \u00b5Transfer (Yang et al., 2022). As we describe in Sec. 5,\n\u00b5Transfer has been established to successfully transfer hyperparameters to O(10B) model\nsizes, albeit with potential limitations arising from very long range extrapolation in the\ninfinite width limit (Blake et al., 2024; Gunter et al., 2024).\n2. Otherwise, a proxy model has to be tuned on a shorter data horizon than the target one. In\nthat case, we suggest running a 2D grid search across learning rate and batch size values\nroughly around the optimal ones, where each training follows a WSD schedule (Sec. 2.4),\nfor as long as compute budget allows. We suggest both the warmup and decay of the\nschedule to be fixed to the one of the target model in absolute number of tokens, which in\nturn should be about 10-20% fraction of the target model horizon to be optimal (Kosson\net al., 2024; H\u00e4gele et al., 2024). This is due to the observed drift of the learning rate\noptimum with the change of the number of steps (Appendix A.6). It is still not yet clear\nhow scaling of warmup/decay length and Adam's \u1e9e1,2 parameters (which we keep constant\nin our experiments) can be incorporated into the total horizon scaling. We leave this as an\ninteresting direction for future work.\n3. After the grid search, one should be able to obtain a plot similar to Fig. 1b and Fig. 2a.\nProvided long enough WSD horizon, a drift in time of the critical batch size region, asso-ciated to the peak of the fixed token budget curve in Fig. 1b, should be visible. Likewise,\nthere should be a drift of the optimally tuned (i.e. assuming optimal learning rate is used)\nbatch size in time as in Fig. 2a. Since we observe a strong correlation but still a mismatch\nbetween the optimally-tuned batch size and the critical batch size, we suggest the following\napproach for selecting optimal hyperparameter values:\n(a) Derive scaling rule by extrapolating the batch size optimum drift in time T based on\nFig. 2a (in our case, approximately $B^* \\times \\sqrt{T}$). Estimate the expected optimal batch\nsize value $B_{target}^*$ for the target data horizon $T_{target}$ under assumption of the optimally\ntuned learning rate.\n(b) Extrapolate the drift of the fixed budget curve in time based on Fig. 1b (in our case,\napproximately $B_{crit} \\propto T$) and derive the expected critical batch size for the target\nhorizon $B_{crit}^{target}$.\n(c) Set optimal learning rate for the target horizon as:\n$\u03b7_{target}^* = \\begin{cases}\n\u03b7_{crit}^* \\sqrt{B_{target}/B_{crit}^{target}} & \\text{if } B_{target} < B_{crit}^{target}, \\\\\n\u03b7_{crit}^* \\sqrt{B_{crit}^{target}/B_{target}} & \\text{if } B_{target} > B_{crit}^{target}\n\\end{cases}$"}, {"title": "A.2 ON CRITICAL BATCH SIZE AND NOISE SCALE", "content": "There are two perspectives on the critical batch size Bcrit. Firstly, McCandlish et al. (2018) define it as a batch size which results in an optimal trade-off between data sample efficiency and gradient update step efficiency:\n$B_{crit} := \\frac{E_{min}}{S_{min}}$,\nwhere Emin (Smin) are the minimum possible number of training examples (steps) to reach a spec-ified level of performance. Additionally, they introduce a notion of a noise scale (for SGD-like optimizers):\n$B_{noise} := \\frac{tr(H \\Sigma)}{|G|^2 H G}$,\nwhere G is the noiseless true gradient, H is the true hessian of the loss function and \u2211 is the minbatch covariance. For $B \\leq B_{curv}^{noise}$, one obtains the linear learning rate scaling rule, while for B > Bruise increasing B does not yield any loss improvement.\nUnder assumption of the Hessian being a multiple of the identity matrix, one obtains a simplified version:\n$B_{simple}^{curv} := \\frac{tr(\u03a3)}{|G|^2}$,\nand McCandlish et al. (2018) argue that\n$B_{crit} = B_{simple}^{curv}$,\nthus bridging together mathematical loss curvature and pragmatical compute resource utilization views. Approximation with Bury simple, being computationally less expensive to estimate, is shown to be to a good degree applicable across multiple tasks, datasets and model architectures. Both the critical batch size and the noise scale are shown to grow in time as one progresses in the training, with the only dependence on the loss value via a power law, with parameters Bo and AB to be determined empirically (Kaplan et al., 2020):\n$B_{crit} = B_0 L^{1/a_B}$"}, {"title": "A.3 MODEL TRAINING CONFIGURATION (CONT.)", "content": "\u2022 24 layers, FFN expansion factor $f_{ffn}$ = $d_{ffn}/d_{model}$ = 4, multihead attention with the head dimension $d_{head}$ = 128.\n\u2022 GeLU activation function, Layer Normalization initialized with 1 (Ba et al., 2016), ROPE with \u03b8 = 10000 (Su et al., 2023).\n\u2022 Dropout is disabled and biases are included in all layers (initialized with 0), weights are shared between the input and output embedding layers.\n\u2022 FSDP parallelization scheme (Zhao et al., 2023), bfloat16 precision, FlashAttention-2 (Dao, 2023)."}, {"title": "A.4 HYPERPARAMETER GRID (CONT.)", "content": "The (\u03b7, B, T, $d_{base}^{model}$, $d_{model}$) grid is defined with the following values:\n\u2022 Learning rate \u03b7:\n {2-12, 2-11.5, ..., 2-7} for $d_{base}^{model} = 1024$\n {2-11, 2-10,..., 2-6} for $d_{base}^{model} = 256$\n\u2022 Batch size B = {216, 218, ..., 226} tokens\n\u2022 Data horizon T = {230, 231 ..., 235} tokens"}, {"title": "A.6 LEARNING RATE SCHEDULE SCALING (CONT.)", "content": "Conventionally, the learning rate schedule consists of a warmup phase, followed by either a constant phase or a decay phase. When all of the three phases are enabled, one obtains a warmup-stable-decay (WSD) schedule (Hu et al., 2024):\n$\u03b7(t) = \\begin{cases}\n\\frac{t}{T_{warmup}} \u03b7_{max} & \\text{if } t < T_{warmup} \\\\\n\u03b7_{max} & \\text{if } T_{warmup} \\leq t < T-T_{decay}, \\\\\n(1 - \\frac{t - (T-T_{decay})}{T_{decay}}) \u03b7_{max} & \\text{if } T-T_{decay} < t < T\n\\end{cases}$"}]}