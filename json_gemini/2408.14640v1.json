{"title": "Effect of Adaptation Rate and Cost Display in a Human-AI Interaction Game", "authors": ["Jason T. Isa", "Bohan Wu", "Qirui Wang", "Yilin Zhang", "Samuel A. Burden", "Lillian J. Ratliff", "Benjamin J. Chasnov"], "abstract": "As interactions between humans and AI become more prevalent, it is critical to have better predictors of human behavior in these interactions. We investigated how changes in the AI's adaptive algorithm impact behavior predictions in two-player continuous games. In our experiments, the AI adapted its actions using a gradient descent algorithm under different adaptation rates while human participants were provided cost feedback. The cost feedback was provided by one of two types of visual displays: (a) cost at the current joint action vector, or (b) cost in a local neighborhood of the current joint action vector. Our results demonstrate that AI adaptation rate can significantly affect human behavior, having the ability to shift the outcome between two game theoretic equilibrium. We observed that slow adaptation rates shift the outcome towards the Nash equilibrium, while fast rates shift the outcome towards the human-led Stackelberg equilibrium. The addition of localized cost information had the effect of shifting outcomes towards Nash, compared to the outcomes from cost information at only the current joint action vector. Future work will investigate other effects that influence the convergence of gradient descent games.", "sections": [{"title": "1 Introduction", "content": "Interactions between humans and AI are increasing in our daily lives [Gorecky et al., 2014; Hoc, 2000; Kun, 2018; Cannan and Hu, 2011]. To ensure that AI can be designed appropriately [Boy, 2017], it is important to have better predictors of human behavior in interactive tasks. In this paper, we leverage game-theoretic equilibria based on Nash [Nash Jr, 1950] and Stackelberg [Von Stackelberg, 2010] equilibria to predict the outcome of a human-AI game. Many human-AI interactions can be modeled as a game [Von Neumann and Morgenstern, 1947]. In particular, simultaneous play games can be modeled as two players performing gradient descent over their own actions, while Stackelberg game can be modeled as a hierarchical game between a leader and a follower, where the leader performs gradient descent assuming the follower plays its best response. In this work, we show how the adaptation rate of the AI's gradient descent algorithm affects the joint action vector outcome of the game, modeled by these two game-theoretic equilibria. We then show how these predicted outcomes can change when the human is presented with different feedback information.\nThis paper considers a two-player general-sum game between a human and an AI. The game is played repeated over two different user cost displays. In one display, human participants explored their input space, similar to seeing only a single point on their cost landscape, Figure la. While in the other display the human had additional information about their perceived localized cost landscape, from which one could potentially estimate cost differences or gradient information, Figure 1b. Within the game, participants were instructed to optimize an objective while the AI adapts behind-the-scene. This system serves as a test-bed for studying game-theoretic learning dynamics and behavior outcomes in human-AI interaction that can be generalized to various domains involving adaptive machine learning algorithms that learns from human data.\nThere are many prior works on human-AI interaction [March, 2021; Rosenfeld, 2018]. Some research on user interfaces require users to choose discrete actions [Andreoni and Miller, 1993; Crandall et al., 2018]. However, in our work, the user repeatedly chooses a continuous action, for example, like this related research on human-AI negotiation [Cao et al., 2020]. Other research are more similar to ours, such as [Nikolaidis et al., 2017; Li et al., 2019; Li et al., 2016], where players play dynamic games over continuous action spaces. Studying continuous action spaces offers advantages due to realistic modeling of real-world scenarios. Many real-world multi-agent settings involve continuous actions, such as in robotics, control systems, and learning algorithms. Continuous actions can also be generalized to discrete actions by using continuous variables to represent discrete actions by smooth approximations or mixed strategies.\nWhile many studies in this field have studied models based on cooperative or zero-sum games [Lai et al., 2021; Rastogi et al., 2022], our research adopts a general-sum framework, offering a more realistic representation of feedback loops in human-AI interactions [Ba\u015far and Olsder, 1998]. Moreover, unlike existing works that focus on how information about the AI's outputs is displayed to the human user [Bondi et al., 2022; Bu\u00e7inca et al., 2021], our study modifies the information that humans receive about their own objectives. There are also many prior works on studying equilibrium outcomes in games and learning dynamics [March, 2021]. Furthermore, behavioral studies [Camerer, 2010] and empirical studies [Chugunova and Sele, 2022] have shown that human adaptation in some settings can be modeled and empirically measured. In our study, we aim to empirically validate the game-theoretic model through crowd-sourced experiments. This research could be used to motivate possible design choices in human-AI systems.\nWe showed, through human subjects experiments and numerical simulation, that the adaptation rate had a robust effect of shifting the behavior outcome of human actions from the Nash equilibrium to the human-led Stackelberg equilibrium. As the Al's adaptation rate increased, human participants' cost decreased, leading the participant to a more human beneficial outcome while the AI's cost stayed relatively constant.\nIn what follows, we will first provide a formal definition of the games we study and the relevant game-theoretic equilibrium concepts. Following, we describe the quadratic cost model and cost feedback display setup for the experiments. We include algorithm definitions for the human experiments and simulations. Our results highlight the effect of varying the AI's adaptation rate while participants used different feedback displays. We find that both the rate and cost display had significant effects on the human action vector outcome of the game."}, {"title": "2 Preliminaries", "content": "We now formalize the games we studied and present equilibrium concepts accompanied by sufficient condition characterizations.\nConsider a non-cooperative game between two agents where the Human agent H plays the AI agent M. The Human agent has cost $C_H : H \\times M \\rightarrow \\mathbb{R}$ and the AI agent has cost $C_M: H \\times M \\rightarrow \\mathbb{R}$ where $H \\subseteq \\mathbb{R}^{d_H}$ and $M \\subseteq \\mathbb{R}^{d_M}$ denoting the action spaces of the respective agents such that $d_h, d_m$ denote the number of action inputs the respective agents have. We assume throughout that each $C_H, C_M$ is sufficiently smooth: $C_H, C_M \\in C^q(X, \\mathbb{R})$ for some $q \\ge 2$. In words, we consider the class of two-player smooth games on continuous, constrained actions spaces.\nA joint action $(h^*, m^*) \\in H \\times M$ is a Nash equilibrium if $C_H(h^*,m^*) \\le C_H(h,m^*), \\forall h \\in H$ and $C_M(h^*, m^*) \\le C_M(h^*, m), \\forall m \\in M$. A joint action $(h^s, m^s) \\in H \\times M$ is a Stackelberg equilibrium with the Human agent as the leader if $ \\forall h \\in H, \\sup_{m \\in M(h)} C_H(h^s, m^s) \\le \\sup_{m \\in M(h)} C_H(h,m)$ where $M(h) = \\{m' \\in M | C_M(h,m') \\le C_M(h,m) \\forall m \\in M\\}$. Refer to [Fiez et al., 2020] for the definition of Nash and Stackelberg equilibria.\nIn a simultaneous play game [Ba\u015far and Olsder, 1998], the agents aim to solve optimization problems,\n$h^N = \\underset{h \\in H}{\\text{arg min }} C_H(h, m^*)$,\n$m^N = \\underset{m \\in M}{\\text{arg min }} C_M(h^*, m)$\nwhere $(h^N, m^N) \\in H \\times M$ is the Nash equilibrium. In this setting, agents are modeled as making decisions simultaneously, in contrast to the sequential nature of Stackelberg games.\nIn the formulation of a Stackelberg game [Ba\u015far and Olsder, 1998], the leader makes the first move, then the follower responds. In the modern machine learning setting, recent work has shown that Stackelberg equilibria arise for gradient-based updates [Fiez et al., 2020]. In a Stackelberg game, the leader and follower aim to solve the following optimization problems:\n$h^s = \\underset{h \\in H}{\\text{arg min}} \\{C_H(h, m^s) | m^s = \\underset{m \\in M}{\\text{arg min }} C_M(h, m)\\}$,\n$m^s = \\underset{m \\in M}{\\text{arg min }} \\{C_M(h^s,m)\\}$.\nThe designation of 'leader' and 'follower' indicates the order of play between the agents, meaning the leader plays first and the follower second. In our experiments, the leader that emerges is the Human agent, and the AI agent is the follower.\nWe remark that Nash equilibria exist for convex costs on compact and convex action spaces [Rosen, 1965] while Stackelberg equilibria exist on compact action spaces [Ba\u015far and Olsder, 1998, Thm 4.3, Thm 4.8 & Section 4.9]. Predicated on existence, equilibria can be characterized in terms of sufficient conditions on agent costs. We denote $\\frac{\\partial C_H}{\\partial h}$ as the derivative of $C_H$ with respect to h and $\\frac{\\partial C_M}{\\partial m}$ as the derivative of $C_M$ with respect to m. The following gives sufficient conditions for a Nash equilibrium.\n[Sufficient conditions for differential Nash equilibrium [Ratliff et al., 2014]] The joint strategy $(h^*, m^*) \\in H \\times M$ is a differential Nash equilibrium if"}, {"title": "3 Experimental Setup", "content": "This section contains details about the two-player continuous game's cost model, the experiment design, the AI adaptive algorithm, and the two cost feedback displays. A link directing to a copy of our experiment's game code can be found in the Supplemental Document (Section C.5)."}, {"title": "3.1 Game and Cost Models", "content": "A Human and AI agent play a continuous game $(C_H, C_M)$ where cost functions $C_H$ and $C_M$ map from Human actions $h \\in H$ and AI actions $m \\in M$ to a real number. The costs are parameterized as quadratic functions given by\n$C_H(h,m) = \\frac{1}{2}h^T A_H h + h^T B_H m + \\frac{1}{2} m^T D_H m + h^T a_H + m^T b_H$  (5)\nand\n$C_M(h, m) = \\frac{1}{2}m^T A_M m + m^T B_M h + \\frac{1}{2} h^T D_H m + m^T a_m + h^T b_m$  (6)\nwhere $A_H, B_H, D_H, a_H, b_H$ are the Human's cost parameters and $A_M, B_M, D_M, a_M, b_m$ are the AI agent's cost parameters, based on [Chasnov et al., 2023]. The sizes of the matrices are $A_H, B_H, D_H \\in \\mathbb{R}^{d_H \\times d_H}, A_M, B_M, D_M \\in \\mathbb{R}^{d_M \\times d_H}, a_H, b_M \\in \\mathbb{R}^{d_H}$, and $a_M, b_H \\in \\mathbb{R}^{d_M}$. The square matrices $A_H, A_M$ are positive definite to ensure the existence of a Nash equilibrium, and $A_H - B_H A_M^{-1} B_M$ is positive definite to ensure the existence of a Stackelberg equilibrium.\nThe parameters for the games in our experiments were selected such that the human actions at the game-theoretic equilibria were distinct positions, $h^N = (-0.25, -0.25)$ and $h^s = (+0.25, +0.25)$ ($h^N = -0.25$ and $h^s +0.25$ for the one-dimensional Human agent input game). The parameters selected also had the Human and AI optimum cost at joint action vectors located at distinctly different locations in the joint action vectors, (h, m), resulting in the cost functions of the Human and AI to neither be fully aligned nor opposed."}, {"title": "3.2 Human Experiment Design", "content": "In this section, we describe the design of the experiment from the human participant's perspective.\nAll human participants were recruited from the crowdsourcing platform, Prolific [Palan and Schitter, 2018]. Participants had no prior training and were selected from the standard sample on the Prolific platform, distributed according to US and UK census data.\nIn the Prolific platform, participants were directed to an external link to our study. Studies either had the cost feedback display (Experiment 1) or the cost landscape display (Experiment 2), as shown in Figure 1, and had participants play either the 1x2, 2x1, or 2x2 version of the game ($d_H \\times d_M$ where $d_H$ is the dimensions of Human actions and $d_M$ is the dimensions of AI actions). Each participant was given only one type of feedback display and one type of joint action vector dimensions for the entirety of their participation.\nAt the start of the study, participants were presented with an introduction screen explaining the task description and user instructions. Within each trial, participants were provided feedback information depending on the experiment, as described in Section 3.4 and 3.5. Participants were only instructed with the task to move their cursor to \"keep the circle as small as possible\" (Experiment 1 cost feedback display) or \"keep the color inside the black circle cursor as light (close to white) as possible\u201d (Experiment 2 cost landscape display). This task can be thought as being similar to using a dial to tune a radio station, using sound feedback to minimize radio static. When one trial ended, participants self-selected when to begin the next trial, optionally taking breaks, as needed.\nFor all experiments, participants provided manual input via a computer mouse (horizontal and vertical position of cursor) to continuously determine the value of a 2-dimensional input action $h \\in [-1,1]^2 \\subset \\mathbb{R}^2$. For experiment versions where the human had only one-input, participants only provided horizontal manual input to determine the value of a one-dimensional input action $h \\in \\mathbb{R}$. For the one-dimensional case, vertical movement was neither used to calculate the human's action nor recorded in our data. Within the code, this cursor input $h(t,:) = (h(t,1), h(t,2))$, where t denotes the time and $h(t,1)$ and $h(t,2)$ denotes the horizontal and vertical input for the human, was scaled such that $h(t,:) = (-1,-1)$ corresponded to the bottom-left corner of the participant's game display and $h(t,:) = (+1,+1)$ corresponded to the top-right corner of the display.\nTo help prevent human participants from memorizing the location of game equilibria, at the beginning of each trial a variable $s_i$ was chosen uniformly at random from $\\{-1,+1\\}$ and the map $h(t,i) \\rightarrow s_i h(t,i)$ was applied to the participant's manual input for the duration of the trial, where i denotes the horizontal (i = 1) or vertical (i = 2) input. When the variable's value was $s_i = -1$, this had the effect of applying a \"mirror\" symmetry to the input. Our experiments had four"}, {"title": "3.3 AI Adaptation Rule", "content": "In this section, we describe the AI's adaptation rule.\nIn all experiments, the AI adapted its input actions using gradient descent,\n$m(t+1,:) = m(t,:) - \\alpha \\frac{\\partial C_M}{\\partial m}(h(t,:), m(t,:))$,  (7)\nwith one of the five different choices of adaptation rate\n$\\alpha \\in \\{0,0.001,0.01, 0.1, 1\\}$.\nAt the slowest adaptation rate $\\alpha = 0$, the AI implemented a constant policy, $m(t+1,:) = m^N$, which was the AI's action component of the game's Nash equilibrium. At adaptation rates $\\alpha > 0$, the Al's input actions were initialized at a point away from both equilibria, $m(0,:) = (0.1,0.1)$ ($m(0,:) = (0.1)$ for the 2x1 version of the game). At the fastest adaption rate $\\alpha = 1$, the AI implements the AI's best response to the human's action. Each adaptation rate condition was experienced once per symmetry (described in the previous section), in randomized order.\nFor the quadratic cost model (6), the AI's updates are linear in the players' actions. Given Human actions $h \\in \\mathbb{R}^{d_H}$, AI actions $m \\in \\mathbb{R}^{d_M}$, and AI cost parameters $A_M \\in \\mathbb{R}^{d_H \\times d_H}, B_M \\in \\mathbb{R}^{d_M \\times d_H}, a_M \\in \\mathbb{R}^{d_H}, b_m$, the AI adaptation rule is\nm^+ = \\begin{cases}\nm^N & \\text{if } \\alpha = 0, \\\\\nm - \\alpha(A_M m + B_M h + a_M) & \\text{if } 0 < \\alpha < 1, \\\\\n-A_M^{-1}(B_M h + a_M) & \\text{if } \\alpha = 1.\n\\end{cases} (8)\nAt each time step of the repeated interaction, the AI implemented a linear adaptation rule to update its action. The adaptation rule allowed the AI to adapt to the human's action in"}, {"title": "3.4 Cost Feedback Display", "content": "For Experiment 1, participants were given feedback on their current cost, via an expanding/shrinking circle. The radial length of the circle represented the participant's cost at the current human-AI joint action vector, $C_H(h(t,:), m(t,:))$. Before and throughout each trial, participants were instructed to \"keep this circle as small as possible\".\nThe cost feedback display was performed with the 1x2, 2x1, and 2x2 version of the game. Each version was tested with n = 30 participants."}, {"title": "3.5 Cost Landscape Feedback Display", "content": "For Experiment 2, participants were given feedback information on their localized cost landscape via a 7\u00d77 grid of shaded dots centered at the cursor. The shaded dots represented the Human cost at the human-AI joint action, $\\{C_H(h,m(t,:)) | h \\in B_\\epsilon(h(t,:))\\}$, where the AI's action, $m(t,:)$, was the AI's current action, and $B_\\epsilon(h) = \\{h' | |h \u2013 h'| < \\epsilon\\}$ is box with side length 2$\\epsilon$ centered at the participant's action h.\nFor the cost landscape feedback display, the lighter (closer to white) color of the dot represented positions of lower cost. Each colored dot represent the real-time position and Human cost for the participant's actions between \u00b10.15 of the participant's cursor, equally spaced by action values of 0.05. Participants were instructed to \"keep the color inside the black circle cursor as light (close to white) as possible\u201d.\nThe cost landscape feedback display was tested with n = 30 participants on only the 2x2 version of the game."}, {"title": "4 Results of Human Experiments", "content": "We conducted two experiments with different populations of naive human subjects recruited from a crowdsourcing research platform. The participants engaged in a two-player human-AI game defined by a pair of quadratic cost functions $C_H$ and $C_M$ given in (5) and (6). The AI adapted its actions within trials using gradient descent in (8) while human participants were tasked with keeping their cost as small as possible using one of our two feedback displays. The experiment parameters were designed to yield distinct game-theoretic equilibria in the players' action spaces. These analytically determined equilibria were compared with the empirical distributions of actions reached by human participants and AI over a sequence of trials in each experiment.\nWe tested five adaptation rates $\\alpha \\ge 0$ for the AI gradient descent algorithm, with multiple repetitions for each rate and the sequence of rates occurring in random order. In our experiments, we found that the observed behavior outcomes follow the predicted game-theoretic values. The following sections will focus on results from the 2x2 version of the experiment but similar behavior can be seen in the 1x2 and 2x1 versions of the experiment. We report figures demonstrating results of the 1x2 and 2x1 versions of the human participant experiment in the Supplemental Document (Section B). A link to our figure generation code and data collected can be found in the Supplemental Document (Section C.5)."}, {"title": "4.1 Cost Feedback Results", "content": "In Experiment 1, participants were provided feedback on their current cost via an expanding/shrinking circle. We found that distributions of median h and m action vectors of the last 5 seconds of each trial (n = 30) shifted from close to the Nash equilibrium (NE) at the slowest adaptation rate to close to the human-led Stackelberg equilibrium (SE) at the fastest adaptation rate (Figure 2(a,b)). For the AI adaptation rates that we selected, median h and m action vectors were along a line between the analytically calculate NE and SE for each player: human action behavior near the NE for $\\alpha = 0.001$, human action behavior near input [0,0] for $\\alpha = 0.01$, and human action behavior near the SE for $\\alpha = 0.1$. The shift we observed from Nash to Stackelberg was in favor of the human resulting in a decrease in human cost as the joint human-AI action vectors shifted from NE to SE (Figure 2(c)). The AI's costs displayed a U-shape trend from slower adaptation rates to faster adaptation rates, resulting in a AI cost close to 0 (the AI agent's analytically calculated optimum cost) at $\\alpha = 0.01$."}, {"title": "4.2 Cost Landscape Feedback Results", "content": "In Experiment 2, participants were provide information via a localized heat map of their cost landscape. This cost landscape display provided participants with additional information on the action dynamics of the game that was not available to participants in the cost feedback display. We found that distributions of median h action vectors of the last 5 seconds of each trial (n = 30) resulted in a similar shift as Experiment 1: from near the NE at the slowest adaptation rate to near the SE at the fastest adaptation rate. Compared to the Experiment 1, the cost landscape experiment resulted in a general shift of median action vectors for the human participants towards the NE (Figure 3(a)). In Figure 3(c), human cost decrease as the adaptation rate increased, with an increase of human cost at $\\alpha = 0.01$. For the AI, median m action vectors displayed a similar behavior as the cost feedback experiment: with a shift from near NE at slow adaptation rates to near SE at fast adaptation rates with the exception of the adaptation rate of $\\alpha = 0.001$, which was closer to SE than the faster adaptation rate of $\\alpha = 0.01$ (Figure 3(b)). The AI's costs in the Experiment 2 displayed a similar U-shape outcome as the cost feedback experiment. Comparing the two feedback displays, the Al's costs in Experiment 2 are larger and closer to the analytically calculated NE and SE AI cost for all adaptation rates."}, {"title": "5 Numerical Simulations", "content": "The results found in the experiments (Section 4.1 and 4.2) would not have been obtained if the Human agent also adapting its actions using gradient decent because changing adaptation rate alone in simultaneous gradient descent play does not change stationary points [Chasnov et al., 2020]. We found that simulating the human adaptation as a two-point zeroth-order approximation of the Human agent's gradient provided similar learning dynamics as our human experiments results.\nAlong with numerical simulations for the 1x2, 2x1, and 2x2 versions of our human experiments, we implemented numerical simulations to demonstrate the learning dynamics on a larger scale problem. The dimensions of players' action spaces are $d_h = 64$ and $d_m = 128$. The pseudocode of the simulation is listed in Algorithm 2, which calculated a two-point zeroth-order approximation of the Human agent's gradient [Nesterov and Spokoiny, 2017]. This section contains details about the 64x128 numerical simulations and results. A link to all numerical simulation code can be found in the Supplemental Document (Section C.5)."}, {"title": "5.1 Varying Adaptation Rate in Simulation", "content": "The 64x128 numerical simulations tested three AI adaptation rates slow ($\\alpha = 0.0001$), moderate ($\\alpha = 0.001$), and fast ($\\alpha = 0.01$) while the simulated Human agent had adaptation rate $\\eta = 0.01$. In the simulations, the number of iterations was T = 1000, representing to length of a trial in the experiment while the inner loop K was set to K = 10 to simulation the Human's strategy to test an action near their current action in order to gain cost information to estimate which action to take next. At each trial, the simulated Human selects a random variable $\\delta$ from a normal distribution with standard deviation $\\sigma$. The direction d is used in the model of the Human's gradient update. The cost parameters for the simulation were selected at random, following the same criteria as the human experiments."}, {"title": "5.2 Results of Numerical Simulation", "content": "Figure 5 shows results from our numerical simulation for the 64 x 128 game with $\\alpha \\in \\{0.0001, 0.001, 0.01\\}$. Without changing the adaptation rules defined in Algorithm 2, both the simulated Human and AI converged to the NE equilibrium at the slowest adaption rate and the SE equilibrium at"}, {"title": "6 Discussion", "content": "Our study focused on the effect of elements on both sides of human-AI interactions: adaption rate (the AI side) and user-feedback information (the human side).\nIn Experiment 1, we observed a correlation between adaptation rates and human cost: as the AI's adaptation rate increased, the human's cost decreased (Figure 2). These findings in the multi-dimensional game are consistent with that of the one-dimensional game in [Chasnov et al., 2023]. This suggests that quicker AI responses allow humans to better anticipate AI actions, improving human performance.\nExperiment 2 kept all elements of the game the same as Experiment 1 but explored the effects of providing the participant additional information which could be used to estimate the dynamics of their cost landscape. We observed a similar correlation between adaptation rate and human cost at the slow ($\\alpha = 0.001$) and fast ($\\alpha = 0.1$) adaptation rates but observed an increase in human cost at the moderate ($\\alpha = 0.01$) adaptation rate (Figure 3). Looking at the human's actions alone, median human action outcomes still follow the pattern shown in Experiment 1, shifting from NE at slower AI adaptation rates to SE at faster AI adaptation rates. Additionally, Experiment 2 showed a general shift of human action outcomes towards the Nash equilibrium in Experiment 2. This may be because the cost landscape feedback suggest to participants that h actions towards the NE will result in lower cost with lighter (closer to white) dots in the direction of the NE and darker dots in the direction away from NE, these results suggests a change in the human adaptation strategy between focusing on the cost landscape information or the current cost information (color of the cursor).\nDuring slower adaptation rates, participants may not be able to anticipate the Al's responses (Experiment 1), resulting in more trust in the heat map information. This is because the heat map information displays the localized Human cost landscape at the human's slice of the cost landscape constrained to the Al's current action. This means when participants moves their actions in the direction of lighter color in the heat map, the cursor color will reflect an expected change due to the AI adapting slowly to the human's new input. This is different for the faster adaptation rates because as the participant moves their actions in the direction of light color, the AI changes its input quickly, which creates an unexpected change in cursor color. Because of this, participants playing with Als at faster adaptation rates may choose to focus on their cursor color to achieve smaller cost.\nOur experiments suggest that humans may give priority for cost landscape feedback information when they are less able to anticipate AI actions. Future work can investigate use biomarkers like eye tracking, which has been shown to be predictors of human intent [Aronson and Admoni, 2022], to determine what feedback information the participant is using and actively to change AI adaptation rate to shift human behavior towards the Nash or Stackelberg equilibrium.\nThe cost model and adaptation rule used in our experiments aims to be representative of real-world scenarios where agents dynamically make decisions in continuous spaces. However, our experiments are limited to the specific user feedback and two-dimensional manual input chosen. In real-world scenarios, the action spaces may be larger and costs may not be observable. Future work will address these limitations by studying larger models and studying other classes action spaces, such as using electromyography or eye-tracking devices as continuous inputs for the human, and decoders or controllers as the AI."}, {"title": "7 Conclusion", "content": "In order to prevent undesirable behavior and maximize benefit to individuals and society, it is critical to develop better predictors of human behavior in the interactions between humans and AI. Here we showed that a specific expansion to the information available to the human, from single point cost feedback to multi-point cost landscape, resulted in a shift of median actions across different AI adaptation rates towards the Nash equilibrium while still following the same trend of a shift from the Nash equilibrium at slower adaptation rates to the human-led Stackelberg equilibrium at faster adaptation rates. This work highlights the importance of validating behavioral models over different user feedback displays and adaptation parameters, opening the door to future research of human-AI co-adaptive interactions."}, {"title": "A Cost Parameters", "content": "In this section, we provide the parameter values for the cost models used in our experiments."}, {"title": "A.1 2x2 experiment parameter values", "content": "The parameter values are chosen to be AH = \u0410\u043c = I are 2x2 identity matrices and\n$B_H = \\begin{bmatrix}\n0.0677 & 0.0677 \\\\\n0.1182 & 0.1182\n\\end{bmatrix}, D_H = \\begin{bmatrix}\n2.0006 & -0.9800 \\\\\n-0.9800 & 2.0644\n\\end{bmatrix}, a_H = [0.2254  0.2254], b_H = [0.9556  0.9300],$ (9)\n$B_M = \\begin{bmatrix}\n0.2650 & 0.2650 \\\\\n0.2654 & 0.2654\n\\end{bmatrix}, D_M = \\begin{bmatrix}\n11.1467 & 6.4300 \\\\\n6.4300 & 5.7645\n\\end{bmatrix}, a_M = [0  0], b_M = [0  0].$  (10)\nThese parameters were chosen such that the players' optima and the constellation of relevant game-theoretic equilibria were distinct positions:\nHuman optimum $(h_H^*, m_H^*) = ([-0.06, -0.06], [-0.90, -0.87]),$\nAI optimum $(h_M^*, m_M^*) = ([0, 0], [0, 0]),$\nNash equilibrium $(h^N, m^N) = ([-0.25, -0.25], [0.13, 0.13]),$\nHuman-led Stackelberg equilibrium $(h^s, m^s) = ([+0.25, +0.25], [-0.13, -0.13])$"}, {"title": "A.2 1x2 experiment parameter values", "content": "The parameter values are chosen to be AH = I", "0.2973": "b_H = [0.5290", "0.6082": "D_M = [4.0400", "0": "b_M = [0", "positions": "nHuman optimum $(h_H^*, m_H^*) = ([-0.89", "1.19": "nAI optimum $(h_M^*, m_M^*) = ([0"}, {"0": "nNash equilibrium $(h^N, m^N) = ([-0.25", "0.15": "nHuman-led Stackelberg equilibrium $(h^s, m^s) = ([+0.25"}]}