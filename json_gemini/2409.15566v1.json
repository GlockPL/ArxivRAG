{"title": "GEM-RAG: Graphical Eigen Memories For Retrieval Augmented Generation", "authors": ["Brendan Hogan Rappazzo", "Yingheng Wang", "Aaron Ferber", "Carla Gomes"], "abstract": "The ability to form, retrieve, and reason about mem-ories in response to stimuli serves as the cornerstone for generalintelligence - shaping entities capable of learning, adaptation, andintuitive insight. Large Language Models (LLMs) have proventheir ability, given the proper memories or context, to reasonand respond meaningfully to stimuli. However, they are stillunable to optimally encode, store, and retrieve memories - theability to do this would unlock their full ability to operate as AIagents, and to specialize to niche domains. To remedy this, onepromising area of research is Retrieval Augmented Generation(RAG), which aims to augment LLMs by providing them withrich in-context examples and information. In question-answering(QA) applications, RAG methods embed the text of interest inchunks, and retrieve the most relevant chunks for a promptusing text embeddings. Motivated by human memory encodingand retrieval, we aim to improve over standard RAG methodsby generating and encoding higher-level information and taggingthe chunks by their utility to answer questions. We introduceGraphical Eigen Memories For Retrieval Augmented Generation(GEM-RAG). GEM-RAG works by tagging each chunk of textin a given text corpus with LLM generated \"utility\u201d questions,connecting chunks in a graph based on the similarity of both theirtext and utility questions, and then using the eigendecompositionof the memory graph to build higher level summary nodes thatcapture the main themes of the text. As a result, GEM-RAG notonly provides a more principled method for RAG tasks, but alsosynthesizes graphical eigen memory (GEM) which can be usefulfor both exploring text and understanding which components arerelevant to a given question. We evaluate GEM-RAG, using bothUnifiedQA and GPT-3.5 Turbo as the LLMs, with SBERT, andOpenAI's text encoders on two standard QA tasks, showing thatGEM-RAG outperforms other state-of-the-art RAG methods onthese tasks. We also discuss the implications of having a robustRAG system and future directions.", "sections": [{"title": "I. INTRODUCTION", "content": "The ability to create intelligent machines has long occu-pied the fascination of humankind, from the automata of themedieval error, formalization of logic in the 17th century andevolving with the emergence of computing theory and artificialintelligence concepts in the 19th century. Now in the modernera, the possibility of generating Artificial General Intelligence(AGI) [1] seems as close as ever, in particular just within thepast three years with the advent of massive scale machinelearning systems, especially Large Language Models (LLMs),and Large Multimodal Models (LMMs).\nLLMs have emerged as remarkably powerful general knowl-edge stores, with the ability to perform impressively on a largevariety of tasks [2], [3], [4]. Further, given their large contextlengths, its has been shown that they have a powerful abilityto perform in-context learning, whether it be for chat-likeapplications where it can reference parts of the conversationdynamically, adapting to classification tasks [5], or reasoningtasks through chain-of-thought prompting [6]. LLMs appearto have solved one part of the general intelligence equation,given the proper context, much like a human mind given theproper working memory stream, they can reason and respondto questions reasonably. But without a way to encode, storeand retrieve information that extends outside of their context,LLMs are missing an extremely important part of generalintelligence, they are unable to form long-term, and ongoingmemories as AI agents, and are unable to adapt to new nichedomains.\nIf LLMs were able to perfectly encode, store and retrievememories it would open the possibility for AI agents toremember decades of conversations, research publications,literary works, and more, building hierarchies of memoriesand knowledge about the world. It would allow advancedQA models to read and encode huge amounts of niche textdocuments, and provide robust conversations or QA citingcontent in those documents. Expanding this idea for LMMs,this would enable the ability to retrieve text, memories, audio,imagery, video, and more, paving the way for more potentmodels and use cases.\nMotivated by this problem, retrieval augmented generation(RAG) has been an increasingly active area of research,which aims to adapt static LLMs to new and niche domainapplications [7], [8], [9], [10], [11]. While RAG methods canwork for any type of text-based information, whether it bemories [12] or images in the case of vision LMMs [13], one mem-standard setting is that concerning large corpora of text fromstories, articles, books, etc. In these settings, RAG generallyworks by first splitting the text into chunks, and obtaining anembedding for each chunk. Given a new prompt, the RAGsystem will embed it, return the top k nearest chunks inembedding space, and then prompt the LLM to answer theprompt given the context chunks [14].\nCurrent RAG systems empirically work reasonably well, butare far from optimal and have several issues, of which wewill discuss two. Firstly, while it is a good approximation,purely finding the similarity between the prompt and each"}, {"title": "III. METHODS", "content": "GEM draws its design and motivation from the intricaciesof human cognition, namely, the processes by which humansencode, store and retrieve information. Specifically, we drawinspiration from how it is believed the human brain prioritizesinformation based on its utility, and that information that ismost often retrieved together gets summarized together [15].\nOur first observation from psychology is the so called\"testing effect\", which observes that if humans are tested onsubject material, they are more likely to accurately rememberit [16]. This may be because, by testing, the information canbe associated to a specific utility in our cognition, and thus is\"tagged\" with information that makes it easier to retrieve. Tothis end, we aim to better tag each chunk of information, by\"tagging\" it with LLM generated utility questions, which help express what information a specific text chunk has, and whyit might be useful.\nOur second observation in that in human cognition, the moreoften memories are retrieved together, the more likely the willcontinue to be retrieved together [17]. In the context of ourutility questions method and their respective text embeddings,we can build a graph constituting all chunks of the text asnodes, and the strength of their connections as the similarityof their question embeddings. We can then, inspired by thisobservation of human cognition, try to build higher levelsummary nodes based off the modes of the graph, i.e. thethe nodes that are likely to be retrieved together given a prompt.Our intuition is that by performing an eigendecomposition,each eigenvector will capture different mode or themes in thetext, that would often be retrieved together, and thus shouldbe used to generate summary nodes.\nOur method at a high level involves several steps to firstconstruct the graph, by chunking the text, generating utilityquestions, building the initial graph, and finally using theeigenvectors from spectral decomposition of normalized graphLaplacian to build the summary nodes. Then, with the GEMproduced, we show how it can be used to perform RAG.\nThe specifics of our implementation are discussed in theexperiments section. A schematic of our method can be seenin Figure 1.\nChunking Text chunkingis standard practice in RAG where the corpus of text of interest$C$ is split apart into chunks, ${CHUNK_1...CHUNK_n}$, where eachchunk, $CHUNK_i$ has some number $T$ tokens. In practice the textcan be split by number of characters or number of tokens. Fora corpus of text we first chunk the text into $n$ chunks, whereeach chunk is $T$ tokens long, and $n=\\frac{N}{T}$, where $N$ is thetotal number of tokens in the text."}, {"title": "A. Connection to Human Cognition", "content": "GEM draws its design and motivation from the intricaciesof human cognition, namely, the processes by which humansencode, store and retrieve information. Specifically, we drawinspiration from how it is believed the human brain prioritizesinformation based on its utility, and that information that ismost often retrieved together gets summarized together [15].\nOur first observation from psychology is the so called\"testing effect\", which observes that if humans are tested onsubject material, they are more likely to accurately rememberit [16]. This may be because, by testing, the information canbe associated to a specific utility in our cognition, and thus is\"tagged\" with information that makes it easier to retrieve. Tothis end, we aim to better tag each chunk of information, by\"tagging\" it with LLM generated utility questions, which help express what information a specific text chunk has, and whyit might be useful.\nOur second observation in that in human cognition, the moreoften memories are retrieved together, the more likely the willcontinue to be retrieved together [17]. In the context of ourutility questions method and their respective text embeddings,we can build a graph constituting all chunks of the text asnodes, and the strength of their connections as the similarityof their question embeddings. We can then, inspired by thisobservation of human cognition, try to build higher levelsummary nodes based off the modes of the graph, i.e. thethe nodes that are likely to be retrieved together given a prompt.Our intuition is that by performing an eigendecomposition,each eigenvector will capture different mode or themes in thetext, that would often be retrieved together, and thus shouldbe used to generate summary nodes."}, {"title": "B. Graphical Eigen Memories", "content": "Our method at a high level involves several steps to firstconstruct the graph, by chunking the text, generating utilityquestions, building the initial graph, and finally using theeigenvectors from spectral decomposition of normalized graphLaplacian to build the summary nodes. Then, with the GEMproduced, we show how it can be used to perform RAG.\nThe specifics of our implementation are discussed in theexperiments section. A schematic of our method can be seenin Figure 1.\nText chunkingis standard practice in RAG where the corpus of text of interest$C$ is split apart into chunks, ${CHUNK_1...CHUNK_n}$, where eachchunk, $CHUNK_i$ has some number $T$ tokens. In practice the textcan be split by number of characters or number of tokens. Fora corpus of text we first chunk the text into $n$ chunks, whereeach chunk is $T$ tokens long, and $n=\\frac{N}{T}$, where $N$ is thetotal number of tokens in the text.\nGiven each text $CHUNK_i$, wethen prompt an LLM to generate some $m$ number of utilityquestions. This can be represented by a function $Q$, that takesa chunk of text, and an integer, $m$, and generates $m$ utilityquestions. Formally, given $CHUNK_i$, we compute $Q(CHUNK, m)$which give a set of utility questions ${q_{i1}, q_{i2},..., q_{im}}$.\nIn order to quantify the similarity of eachutility question to each other and/or to a prompt, we need toembed the text into a high dimensional feature space, givenby a text encoder. More specifically, given a text embeddingfunction $E$, for each node $i$, we compute the embedding $v_i$as follows: $v_i= E(CHUNK_i)$. Similarly, we also computethis embedding for each utility question by the same function.However, in order to best encode the information of the utilityquestion we make the utility question's embedding the averageof the questions embedding, and the base text's embedding,that is $v_{ij} = (E(q_{ij}) + V_i)/2$.\nGiven the embeddingof each chunk of text, as well as each chunk'scorresponding utility questions and respective embeddings, wethen generate the fully connected graph. For each node/chunkpair $i, j$, we consider the sum of the similarity metric betweenall of node $i$'s utility question embeddings, to that of node $j$'sbase text embedding. More formally, let $G = (N, E)$ be thememory graph we are constructing. Let the nodes be given by:$N = {CHUNK_1, CHUNK_2, ..., CHUNK_K}$, where each node $t$ hasutility questions $Q_t= {q_{t1}, q_{t2},...,q_{tm}}$, and base text em-bedding $v_t$. Then, For each $(t, v) \\in N\\timesN$, and for each $i \\in{1,2,..., m}$, generate an edge between $t$ and $v$ with weight$\\Sigma_{i=1}^{m} SIM(U_{ti}, v_v)$. Any function could be used to computethe similarity, but in all cases we use standard cosine similarity,i.e., $SIM(a, b) = \\frac{a\\cdot b}{\\lVert a \\rVert \\lVert b \\rVert}$.\nIn order to build an en-coding system that encodes this higher level information weformulate this as a random walk or spectral decompositionproblem. Intuitively, in this context, each eigenvalue andits corresponding eigenvector reveal a distinct 'theme' orconceptual dimension in the graph. By summarizing the topcomponent nodes of each eigenvector components, using anLLM, we can understand the most significant relationships andconceptual clusters within the graph.\nMore specifically, let $S = (S_{ij})_{n\\timesn}$ be the similarity matrixof the graph, where $s_{ij}$ is the sum weight between the $m$ SIMvalues of each utility questions of nodes $i$ and to node $j$. Byattaching each $s_{ij}$ to a weighted edge $e_{ij}$, we can map thesimilarity matrix $S$ onto the memory graph $G$. Thus, we canbetter understand the relationship between different text piecesof the document by analyzing the properties and behaviors of$G$.\nSince different documents possess different connectivityand node centrality, the spectrums will also be at differentscales. To better quantify how influential each node is withoutdegree bias, we transform $S$ to a variant of normalized graphLaplacian $L$, which is $L = D^{-1/2}(S \u2013 I)D^{-1/2}$, where$D = diag(d_i)$ is the degree matrix and $I$ is the identitymatrix. Then we conduct spectral decomposition by solving"}, {"title": "IV. EXPERIMENTS", "content": "Our method, and RAG methods in general can be used fora host of tasks, including AI agent memory, fine-tuning LLMsfor niche scientific domains etc. For purposes of comparisonwe evaluate the efficacy of our model in the context of QAfor medium length documents.\nIt should be noted that RAG methods in general aim tostudy the specific problem of retrieving optimal small contextwindows from large data sets and returning them to an LLMfor processing. However, this setting is of ongoing debate andchange within the research community, as large LLMs haveincreasingly large context lengths, making it possible to fitmassive amounts of data within a single prompt. We feel, thateven with larger and larger context windows, the problem ofretrieval is still of interest because in a real-world setting, thedata sets of interest are often still much larger than the currentcontext windows of the biggest models, and returning massiveamounts of context tokens is impractical and costly. Secondly,even with a larger context, it has been shown that LLMs canbe prone to forget information in the context, and can be proneto hallucinate information. Lastly, with more precise retrievalit makes it easier to verify and track the information that anLLM is using to answer a question. For these reasons, we feelthat the retrieval methods are still highly motivated despitethe evergrowing context length of large models. Retrieval alsorequires no finetuning, and uses LLMs \u201coff-of-the-shelf\u201d.\nTo properly compare our method to other recent workwe study datasets where often the entire data set can fit inthe context of an LLM. For this reason, while our methodoutperforms other retrieval methods, which are all limited to400 tokens of context, there has been work that uses thousands of tokens of context and outperforms retrieval metrics such as"}, {"title": "V. DISCUSSION AND CONCLUSION", "content": "We developed GEM-RAG, a method for RAG inspired byhuman cognition, that tags each memory or chunk by the spe-cific utility of its information and relation to other memories.Further, we use these utility questions to formulate a weightedfully connected graph. We perform an eigendecompositionon this memory graph to robustly extract \u201ceigenthemes\", andcreate summary nodes for each theme. We observe in mostcases, for multiple text embeddings and LLMs, our methodout-performs standard baselines. We also show that a producedGEM is a standalone object: it can be used with any LLMto be searchable and conversable, and provides a principledvisualization for understanding textual data. We believe anoptimal RAG method has the ability to greatly improve theability of LLMs, enabling real AI agents that can leveragemassive histories of conversations, or adapt to massive nichedata sets without fine-tuning. Further, these RAG methods canextend to LMMs, retrieving text, images, videos, sound, etc.,and bringing us closer to simulating human cognition."}]}