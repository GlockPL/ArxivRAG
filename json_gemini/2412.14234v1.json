{"title": "Syzygy: Dual Code-Test C to Rust Translation using LLMs and Dynamic Analysis", "authors": ["Manish Shetty", "Naman Jain", "Adwait Godbole", "Sanjit A. Seshia", "Koushik Sen"], "abstract": "Despite extensive usage in high-performance, low-level systems programming applications, C is susceptible to vulnerabilities due to manual memory management and unsafe pointer operations. Rust, a modern systems programming language, offers a compelling alternative. Its unique ownership model and type system ensure memory safety without sacrificing performance.\nIn this paper, we present Syzygy, an automated approach to translate C to safe Rust. Our technique uses a synergistic combination of LLM-driven code and test generation guided by dynamic-analysis-generated execution information. This paired translation runs incrementally in a loop over the program in dependency order of the code elements while maintaining per-step correctness. Our approach exposes novel insights on combining the strengths of LLMs and dynamic analysis in the context of scaling and combining code generation with testing. We apply our approach to successfully translate ZOPFLI, a high-performance compression library with ~3000 lines-of-code and 98 functions generating ~4500 lines-of-code in Rust. We validate the translation by testing equivalence with the source C program on a broad set of inputs. To our knowledge, this is the largest automated and test-validated C to safe Rust code translation achieved so far.", "sections": [{"title": "1 INTRODUCTION", "content": "C to Rust translation has seen tremendous interest in recent years owing to memory safety vulnerabilities in C [45] on the one hand and Rust's strong static type and ownership system guaranteeing compile-time eradication of these vulnerabilities on the other. It is further motivated by the fact that both C and Rust can target similar applications (low-level, performance-critical libraries) and are supported by Clang-based compiler toolchains. Though desirable, C-Rust translation is challenging: C and (safe) Rust employ different typing systems (strongly typed variables and no raw pointers in Rust) and different variable access rules (arbitrary accesses in C, while strict borrowing rules in Rust), amongst other differences. Manual migration of even moderately sized codebases requires multiple person-weeks of effort, motivating the need for automatic translation techniques.\nThere are two main approaches for code translation: rule-based/symbolic and LLM (Large Language Model)-based. Rule-based translation approaches often operate on a terse intermediate representation (for achieving full coverage with a limited rule set) and thus often produce uninterpretable target code. Symbolic program synthesis approaches (e.g., [2, 32]), on the other hand, often do not scale to multi-function codebases. LLMs shine in both these respects: they produce natural/interpretable code and have superior scaling capabilities. LLMs cannot, however, perform precise inference of semantic features of program executions such as aliasing and allocation sizes. This is especially important in languages with pointer casts and dynamic allocations such as C, where this information is often not syntactically available/interpretable.\nWe develop SYZYGY, an approach to convert medium-large (multi-file, multi-function) C codebases to equivalent safe Rust code automatically. As the name suggests, Syzygy exhibits two kinds of dualities. The first is a synergistic combination of superior generative capabilities of LLMs with semantic execution information collected by dynamic analyses [4] on the source C codebase. Secondly, as opposed to using the LLMs+Dynamic Analysis recipe to perform Rust code-generation alone, we also build a test translation approach for reliable equivalence testing.\nThe four tenets of our approach - LLMs + Dynamic Analysis and Code + Test Generation work together as follows. First, C constructs such as dynamic (heap) allocations of unknown sizes, aliasing between pointers, and pointer type casts (and the lack thereof in Rust) lead to challenging translation scenarios in which execution information (e.g., array sizes, aliasing, and types) must be inferred from the code. Our dynamic analysis extracts this semantic execution information and makes it available to the LLM, thus aiding translation. Second, to efficiently leverage the stochastic generative capabilities of LLMs for large codebases, we need tests to weed out and/or repair incorrect candidates. While tests for top-level functions are often available/easy to construct (due to documented APIs), reliably testing intermediate functions is challenging. Here we once again use our LLM+Dynamic Analysis recipe: we (a) use dynamic allocation analysis to mine intermediate function I/O examples from top-level C tests, and then (b) use LLMs to programmatically map these I/O examples to Rust to construct intermediate function equivalence tests.\nThis approach of combining incremental code generation with equivalence test generation (for repair and validation) draws inspiration from test-driven development (TDD) practices improving code design and reliability [6, 33]. Notably, just as TDD uses tests to drive implementation, our approach uses intermediate equivalence tests to greedily translate a C codebase while maintaining compatibility with an invariant: previously generated Rust code. This allows us to greatly improve the accuracy, and hence scalability, of translation. We compare our technique with other C-to-Rust approaches in Table 1."}, {"title": "2 BACKGROUND", "content": "C constructs: dynamic allocations, pointer casts, and aliasing. C allows dynamic memory management using malloc() and free(). While essential when allocation sizes are not known statically, dynamic (heap) allocations require careful reasoning to prevent memory safety issues. C provides low-level stack/heap accesses using pointers. Pointers can be manually constructed, modified, and cast to other types (including the void*). Further, multiple pointers can alias, i.e., point to/reference the same/overlapping memory. Dynamic allocations, casts, and aliases challenge translation since they require non-local program reasoning. Our dynamic analyses (for allocation sizes, types, and aliasing) help combat this."}, {"title": "3 PROBLEM FORMULATION", "content": "We aim to perform source-level translation from a C codebase to an equivalent safe Rust program. We formulate this as a program-synthesis task: given as specification a C program, synthesize a safe Rust implementation that satisfies it by preserving its functional behavior.\nGiven: a C program $P_c \\in C$, a set of test inputs $T \\subset I$,\nFind: a Rust program $P_R \\in R_{safe}$,\nSuch that: $\\forall t \\in T. P_c(t) = P_R(t)$.\nWe check conditions for a satisfactory synthesis using the notion of \u201cobservational equivalence\u201d [1], i.e., the generated Rust program produces equivalent outputs to the C program on a set of test inputs T in the space of inputs I. In addition to generating equivalent Rust code, our main motivation for C to Rust translation is eliminating memory safety vulnerabilities. To do so, we disallow any unsafe blocks in the generated Rust program. Notably, the two main requirements described above (equivalence and memory safety) may conflict: e.g., some input tests may exercise unsafe behaviour during execution (not compilation). As a consequence of enforcing the safe fragment of Rust, we allow undefined behaviors (possibly runtime panics) in such cases. Next, we discuss details about the scope of the input C and output Rust programs we consider:\nInput C Program. We restrict the input C program to the following conditions:\n(1) Acyclic data structures: data structures with pointer cycles require special care in Rust to avoid memory leaks (e.g., Weak pointers to free disconnected memory cycles).\n(2) No multithreading: requires Arc-wrapped references to thread-safely perform borrowing.\n(3) No type punning: performing raw memory accesses on untyped or multi-typed memory regions would hinder the best effort type analysis that our approach aims to perform.\nThese are soft restrictions on our Rust code generation pipeline since we use an LLM's capability to perform the task. However, our current testing and analysis infrastructure is limited to these cases."}, {"title": "4 APPROACH", "content": "Our high-level approach, SYZYGY is illustrated in Figure 1 with an overview provided in \u00a74.1. At a high level, our approach follows a dual-translation approach-we incrementally translate both code and tests and use execution filtering to ensure validity. This dual translation is guided by synergistically combining LLMs with dynamic analysis."}, {"title": "4.1 Overview", "content": null}, {"title": "4.1.1 Incremental Aligned Translation", "content": "Our approach performs translation at the granularity of translation units such as functions, structs, typedefs, and macros. That is, we decompose the C codebase into granular units (SLICER in Figure 1) and loop over them, translating each incrementally."}, {"title": "4.1.2 Semantic Type Alignment & Dynamic Analysis", "content": "Translation between imperative typed languages can be conceptually thought of as a combination of two operations: (1) converting/lifting types of variables and (2) translating expressions and statements on these variables. The key challenge here arises due to pointers in C. This is because pointers hide information regarding the underlying object structure and how it's used. (Safe) Rust does not allow C-like pointers, thus translation needs to recover this hidden information. In addition to all of these void pointers in C further obfuscate information by also hiding the type of the underlying object. We now discuss this challenge further through the following table.\nDynamic Analysis. While LLM-driven code generation is powerful, we observe that such aspects of programs are difficult to reason about purely syntactically. Thus, we develop dynamic analyses (SPECMINER, \u00a74.2) to collect properties of function arguments such as aliasing, nullability, and allocation sizes. We use these to guide the LLM during code and test generation. We use instrumented execution to collect this since static approaches require complex/infeasible program analysis."}, {"title": "4.1.3 Sampling for Reliable Code Translation", "content": "For each C translation unit, an LLM generates corresponding Rust code (CODEGENERATOR, \u00a74.3). However, LLMs can generate incorrect code, making their use unreliable. Prior work has shown that scaling the number of attempts LLMs take at problems can help increase performance [5, 30]. Following this, we use an intuitive approach by sampling multiple Rust code generations and testing them to filter valid generations. By relying on tests, we transfer the soundness of our approach to the reliability of our tests."}, {"title": "4.1.4 Reliable Testing by I/O Translation", "content": "However, tests take a black-box approach to verifying code and can have issues such as incompleteness, low code coverage, or even incorrectness. Simply generating tests using LLMs for arbitrary Rust code does not work due to complexities with invalid types, inter-argument constraints like aliasing, and incorrect assertions on expected outputs. We address this by observing that C codebases contain tests, particularly for top-level entry points"}, {"title": "4.2 Dynamic Specification Mining", "content": "We use dynamic analysis to assist the LLM when performing codegen-testgen-repair in our approach. This analysis produces different kinds of specifications (information) about the execution of the C program. For each C function $f_c$, we gather two kinds of information. Firstly, collect specifications/properties of the arguments to $f_c$, such as types, bounds, nullability, and aliasing (\u00a74.2.1). Secondly, we obtain input-output examples for $f_c$ (\u00a74.2.2).\nWe use the mined specifications in two ways. First, this information guides LLM-driven code generation by providing hints distilled into the LLM's prompt (\u00a74.3). Secondly, collected I/O samples are used in equivalence test generation with LLM generated tests (\u00a74.4). Below, we describe the kinds of specifications we collect using the function f in Fig. 3 as an example."}, {"title": "4.2.1 Mining Specifications from C Executions", "content": "Our approach relies on robust analyses to collect information about function arguments. This information is consumed by code and test generation (see \u00a74.2.2). We use dynamic analysis to collect this information (per execution) since static approaches require complex program analysis (and may even be infeasible).\nOur approach operates by (a) maintaining a runtime that tracks variables and their properties and (b) instrumenting the C codebase (using the LLVM compiler toolchain [27]) with handlers that interact with the runtime. Then, simply compiling and executing the instrumented codebase allows us to collect information about the types, bounds, nullability, and aliasing for function input and outputs. Depending on the property, we either aggregate or maintain context-sensitive information per execution (e.g., nullability is aggregated and bounds are not) Our approach is general and extensible also to collect other kinds of dynamic information which may be useful in code/test generation. We now discuss the particular specifications we collect along with how they are used in the pipeline, using the code in Fig. 3 as a running example.\nTypes. We first map inputs and outputs to their types. These can be recovered from the LLVM IR types with an instrumentation pass. In particular, we track the following types:\nTypes (T) = Primitives (int, uint, char,...) | Structs | Data pointers to T | Function pointers\nWe store a mapping from function arguments to their types in our instrumented runtime. For example, in function f (Fig. 3), we would extract the following type mapping:\nTypeInfo = {arr \u2192 pointer(float), brr \u21a6\u2192 pointer(float),\nkrr pointer(int), num \u2192 int, kernel \u2192 function}\nOur type inference process manages several challenging cases. When encountering a malloc call, the initial allocation returns a void pointer (void*) representing untyped memory. Our LLVM pass tracks subsequent bitcast instructions that convert this raw memory to typed pointers. Consider the example in Figure 3, where caller2 allocates arrays of floats and integers. For each allocation, we observe a bitcast from i8* (LLVM's representation of void*) to the target pointer type. By tracking these bitcast operations, we recover the intended type of the allocated memory. We employ a conservative approach for void pointers, mapping them to a base character type while maintaining pointer semantics. This strategy extends to function pointers, where we preserve the complete function signature during analysis. Finally, we treat arrays and pointers equivalently: singleton arrays are pointers (with size 1), and arrays are pointers with a larger size. The allocation size information captures this difference (see below).\nAllocation sizes. Dynamic allocations lead to a major challenge with C-based translation. Our dynamic analysis tracks allocation size by maintaining a bounds table at runtime.\nThe bounds table maintains comprehensive allocation information in an interval tree data structure. Each allocation record contains metadata, memory bounds, and inner element sizes for nested types. This information is collected through two mechanisms: compiler-inserted instrumentation for stack variables and allocation function interception for heap memory. We maintain type-aware size tracking for both allocation types, accounting for structure fields and array dimensions. Allocation size information is later used when collecting serialized I/O examples for functions. For our example function f, in the caller1 path, we would record the following bounds information:\nSizeInfo = {arr \u2192 100, brr \u2192 10,\u2026 }\nNullability. We also determine nullability by dynamically analysing argument values in various test (execution) contexts. Given a function with pointer arguments, e.g., f(int* a), we can"}, {"title": "4.2.2 CI/O Specification", "content": "Recall that we validate the correctness of each translated function through test-based equivalence (against the corresponding C function). This requires input-output test examples for each function $f_c$ in the codebase.\nValid inputs. Determining valid inputs to every function $f_c$ in the codebase is challenging. For example, functions can have complex inter-argument pre-conditions (e.g., two array arguments must have the same length), pointer arguments may have to be non-NULL or alias each other in specific ways. Inferring these pre-conditions (towards generating valid inputs) is hard and requires expensive program analysis. LLMs similarly struggle to generate valid inputs satisfying such pre-conditions. We address this by observing that calling the top-level (e.g., main) entry point results in calls to internal functions with valid inputs. Further, the top-level entry point often has much simpler input constraints and/or is better documented. Thus, we invoke (fuzz) the top-level function on multiple inputs and collect function call arguments for each resulting execution.\nInternal function input capture. To capture I/O examples for function $f_c$, we instrument the entry and exit points of $f_c$ with handlers that dump serialized input and output values. We serialize values of non-pointer objects (e.g., primitive types) directly. For pointers, we dereference the argument (possibly multiple times) to store the internal object. At the function's exit point, we dump both the output and the input arguments to ensure we capture side effects. This gives us an input-output test example: ($I_c$, $O_c$). We use these C I/O examples to generate equivalent Rust test inputs ($I_{RUST}$) and to check equivalence (see \u00a74.4)."}, {"title": "4.3 Rust Code Generation", "content": null}, {"title": "4.3.1 High-level strategy: slicing and greedy translation", "content": "As discussed in \u00a74.1, our approach performs incremental aligned translation: we use the program dependency graph to slice the C codebase into individual translation units and then translate them starting from the leaves (units with no dependencies) upwards in isolation. Translation units are top-level statements in the C program: functions, struct definitions, macros, typedefs. Operating on individual units at a time, rather than entire files or the full codebase, ensures focused translation and increases accuracy."}, {"title": "4.3.2 Per-unit translation", "content": "For each translation unit, we use LLMs to generate candidate Rust translations. We follow the following two principles to identify valid compiling translations.\nUsing information from dynamic analysis. We collect property specifications using SPECMINER module providing type, nullability, and aliasing information about the function arguments. These properties constrain the signature for Rust translation and we provide them to LLM as additional context. For example, in Figure 5, our nullability map allows LLM to infer non-intuitive properties and correctly generate the correct code. Aliasing information similarly directs the function signature in terms of using smart pointer (Rc<RefCell<RT>>) or leveraging argument collapsing strategies."}, {"title": "4.4 Rust Test Generation", "content": "Given a newly generated Rust function fr, the goal is to check for equivalence between fr and the source C function fc. We check equivalence using tests created in two steps. First, we translate input C arguments to equivalent Rust arguments (\u00a74.4.1). Then, functions fc and fr are invoked on these inputs, and their outputs are compared for equivalence (\u00a74.4.3)."}, {"title": "4.4.1 C to Rust I/O Translation", "content": "Consider an input-output example ($I_c$, $O_c$) for a C function $f_c$ gathered during dynamic specification mining (\u00a74.2.2). We now want to compare the behavior of fr against fc for this example. This requires translating the C argument objects $I_c$ and $O_c$ into equivalent Rust objects $I_{rust}$ and $O_{RUST}$. The challenge is that argument translation must adapt to the context-dependent signature of fr that is chosen by code generation (\u00a74.3).\nFor example, in Fig. 6, while all three C functions square, sum, and caps take a single pointer argument, they reference different objects: a singleton integer, an array of integers, and a string, respectively. Consequently, the Rust translations of these functions have different signatures. Particularly, as the String example shows, even hardcoding C array pointer to Rust array/Vec mappings may be inadequate. As such, it is very hard to cover all possible cases using rule-based mappings. In light of this, we rely on using the LLM itself to generate a translation mapping function, translateArgs, that maps C objects to Rust objects (both for inputs and expected outputs):\ntranslateArgs($I_C$) \u2192 $I_{RUST}$   translateArgs($O_c$) \u2192 (expected) $O_{RUST}$\nProgrammatically translating I/O by generating such translation mapping alleviates the stochastic nature of LLMs, providing more reliability when translating many tests."}, {"title": "4.4.2 Argument Construction LLM Tool", "content": "Certain aspects of the argument translation, however, require broader program reasoning. Examples include aliasing between pointer arguments and allocation sizes (e.g. when size is not explicitly available as another argument). In the sum example (Fig. 6), constructing a Vec from the array pointer would require size information. Our dynamic analysis (Allocation Sizes) comes to our aid!"}, {"title": "4.4.3 Equivalence Testing", "content": "With a valid translateArgs, we can execute fc and fr on equivalent inputs. Here, we observe that functional equivalence between fc and f\u20a8 can be elegantly demonstrated through a commutative relationship centered on the translateArgs function."}, {"title": "4.5 Multi-round & Rejection Sampling", "content": "Overall, with this pipeline, specification mining (\u00a74.2) \u2192 code generation (\u00a74.3) \u2192 testing (\u00a74.4), we can identify whether a generated f\u20a8 is a valid translation of fc. We use multiple rounds of this pipeline with error feedback to correctly translate all functions.\nRejection Sampling. As mentioned in \u00a74.1, we make use of sampling to scale LLM inference and verify generated solutions. We use this rejection sampling approach at each pipeline module that invokes an LLM, as illustrated in Figure 7. For each module, we first sample a set of solutions, i.e., rust translations for CODEGENERATOR, translateArgs for ARGTRANSLATOR, and equivalence tests for EQTESTER. We then use either compilation or execution as the signal to filter out bad generations and pass the filtered set to the next module.\nError Feedback. Finally, failing equivalence tests are used as error feedback. However, since these functions can produce large outputs, we perform a diff operation between the true and desired output to isolate the discrepancy. We provide these diffs and assertion messages to the LLM and ask it to regenerate the (incorrect) function. Consider an anecdotal example for equivalence-based feedback from our ZOPFLI case study (\u00a76.2). Below, the BoundaryPMFinal function grabs the next field from a pool of nodes (left). However, in the first round, the LLM interprets node->next as incrementing the index field (right), corresponding to the next iterator in the C code."}, {"title": "5 IMPLEMENTATION", "content": "Following, we describe the implementation details of the various components of our approach."}, {"title": "5.1 SLICER", "content": "We implement our program slicer using CLANG-17 that consumes an entire C codebase, analyzes the definitions and usages, and builds a topologically sorted dependency graph of all the code elements in the C program. Additionally, we implement an iterator over the graph that progressively traverses and yields code elements in dependency order. By performing a topological sort on this graph, we can identify an incremental translation unit and determine its dependency slice (the set of units it depends on). Additionally, we incorporate extra dependency edges between function types and the functions matching those types, as inferred via dynamic analysis."}, {"title": "5.2 SPECMINER", "content": "SPECMINER module of our pipeline is responsible for collecting I/O specifications for C functions. These I/O are later used for inferring various property specifications such as nullability related to the function. Since C does not support reflection and even primitive information such as pointer sizes and types are not directly available, we use dynamic analysis to collect this information. The SPECMINER module of our pipeline is responsible for collecting input/output (I/O) specifications for C functions. These I/O specifications are later used to infer various function properties, such as nullability and aliasing. Since C does not support reflection, and even primitive information like pointer sizes and types are not directly available, we employ dynamic analysis to collect this information. In particular, we implement an allocation tracking LLVM pass (using LLVM-143) to collect the bounds and type information for all allocations in the program. We then implement a C++ runtime to access this information for serializing the C function arguments and outputs. Because C functions can be stateful and may modify their arguments, we serialize the arguments both before and after function execution along with the return value."}, {"title": "5.3 CODE GENERATOR", "content": "We implement our Rust code translation module using an LLM based parallel sampling and filtering approach. In particular, for a given C translation unit, we use the SLICER to collect its immediate parents. We then prompt the LLM with the C source code elements, the corresponding Rust elements already translated, and the dynamic analysis annotations (such as nullability and aliasing information about arguments) to generate the translated Rust code."}, {"title": "5.4 ARGTRANSLATOR", "content": "We use bindgen to generate Rust function signatures from the C function signatures and employ FFI to access the C functions and their arguments. We then use LLMs to programmatically map the types and arguments between the C and Rust functions. Specifically, for a given C and its candidate Rust translation, translation, we provide the LLM with the aligned C and Rust function contexts and ask it to generate a program that maps the C arguments to appropriate Rust arguments. We use successful execution of the generated translation program (that is, we can load and map the C arguments to Rust arguments) as a filter to select the candidate Rust functions."}, {"title": "5.5 EQTESTER", "content": "For a given C function, its candidate translated Rust function, and the corresponding type alignment function, we prompt the model to generate a test function which checks whether the C and Rust functions are equivalent. We perform value-based equivalence checks by implementing the PartialEq traits to Rust types, enabling the use of the built-in equality operator to assert equivalence (performing nested equality comparisons internally). We use these generated tests to filter the candidate Rust functions that successfully compile and pass the equivalence tests. If none of the candidate functions pass the tests, we provide the model with execution error feedback and iteratively repair the translation over multiple rounds.\nWe run our experiments on a 96 core Intel Xeon machine with 720 GB of RAM. We use 01-PREVIEW and 01-MINI to perform all the steps unless specified otherwise."}, {"title": "6 EXPERIMENTS", "content": "In this section, we discuss two experimental results. First, in \u00a76.1, we present a case study on translating URLPARSER, a modest C program comprising over 400 lines, which was studied in prior work [29]. Next, in \u00a76.2, we present our results from translating ZOPFLI, an optimized C compression program with over 3000 lines."}, {"title": "6.1 UrlParser Case Study: Motivating Dual Translations", "content": "We translate the UrLParser program [26], a C application that parses URLs and extracts their components (protocol, domain, path, query, and fragment) from a URL string. To gather deeper insights, we run our approach with human intervention to identify failure models and solutions. This program has been used in prior work [29], where the authors applied existing LLM-based approaches, namely Flourine [13] and VERT [52], and reported challenges in generating correct translations. In particular, they faced difficulties in program decomposition, where inconsistent translations of decomposed functions resulted in compilation errors.\nIncremental sampling and filtering alleviate challenges. We use our SLICER and CODEGENERATOR modules to incrementally generate compilable translations. By providing dependency context to the"}, {"title": "6.2 ZOPFLI Translation", "content": "hWe translate the C program for the ZOPFLI compression algorithm [14], which is known to achieve superior compression ratios by extensively optimizing the compression process. The codebase consists of over three thousand lines of C code (excluding comments), comprising ninety-eight functions and ten structs, and spans over twenty-one files. It provides a challenging testbed for evaluating our approach due to the diversity of the source code constructs, including heap-based data structures (such as linked lists and array iterators), function pointers, and void* arguments while still agreeing with our assumptions about acyclic data structures, concurrency, and lack of void*\nabuse (type punning). The C code implements nuanced algorithms, including LZ77 compression, Huffman encoding, and block splitting. Following, we detail our implementation and results."}, {"title": "6.2.1 Implementation", "content": "We use the unifdef tool [37] to preprocess the codebase and standardize the #ifndef preprocessor configuration options. This process addresses a combination of system configurations (operating system, compiler) and optimization settings implemented in ZOPFLI. Next, we run Syzygy on ZopFLI, using ZopfliDeflate as the top-level entry point interfacing with the core deflate algorithm. This function receives the input string to be compressed, along with ZopfliOptions configurations that determine how block splitting is performed. We semi-automatically collect 26 test inputs for the ZopfliDeflate function. These inputs achieve 88% line coverage and 70% branch coverage on the original C program.\nWe use SPECMINER to collect I/O and property specifications for every function in the C code using the above-mentioned tests dynamic allocation analysis. Next, we iterate on the translation units in dependency order and run our pipeline to generate Rust translations for each unit and test for equivalence. Since we do not have equivalence tests for non-function translation units, such as structs, global assignments, and macros, we manually verify translation correctness. In particular, we manually construct the appropriate Rust structs and check other global assignments or macros as necessary. This process discovered a bug in the ZOPFLI_APPEND_DATA macro where LLM translation did not handle a corner case in the original code which we repaired manually."}, {"title": "6.2.2 Results", "content": "We run our approach and translate ZOPFLI in about 15 hours costing about 2500$. Equivalence Test Suite. To ensure that our translation is correct, we collect a comprehensive test suite for the ZopfliDeflate function comprising 1000000 compression inputs ranging between 1e1 and 1e7 characters (having a combined size over 20GB). We measure coverage of our larger tests and find that they achieve 95% line coverage and 83% branch coverage. Note that we identified a considerable portion of uncovered code regions (example 5.9% of the branches) as error regions (like assert statements, exit branches) and thus supposed to be unreachable. We use these to check equivalence in compression ratios between the C and Rust programs."}, {"title": "6.2.3 Analysis", "content": "Next, we analyze the quality and efficiency of translations.\nQualitative Analysis. Our translated Zopfli program ranges over 7k lines (including overly verbose comments and docstrings). Interestingly, we observe that LLMs eagerly modify the C program's logic (usually in the spirit of optimizing or simplifying), often leading to bugs. We address these issues by prompting the models with strict warnings to limit such behaviors. For example, since C does not provide native support for data structures like vectors, it maintains additional attributes like the length of an array to mimic the functionality. LLMs are eager to write Rust code that mitigates using external length (and using Vec length) but do so inconsistently causing bugs."}, {"title": "6.2.4 Ablations", "content": "Following, we describe ablations for our approach.\nChoice of Models. Recall (from \u00a75) that we use O1 models for our experiments. We also experiment with GPT-4O models to perform translation. Since GPT-4O models are less powerful, we increase the sampling budget to collect more candidate translations. Additionally, since we discovered that GPT-4O is insufficient for performing execution-feedback-based repair operations, we fell back to O1 models for repair. We run our pipeline, passing all intermediate tests, and generate an alternative translation for ZOPFLI. Notably, using significantly cheaper GPT-4O models, we can generate a correct translation for ZOPFLI in $ < 800. However, the generated translation is noticeably slower than the original C implementation and crashes on some long test inputs. We conclude that O1 models produce more reliable and higher-quality translations and hope to improve the efficiency of our pipeline by using robust combinations of O1 and GPT-4O models in future work.\nChoice of using dynamic analysis and Testing. Recall that we use dynamic analysis to construct intermediate tests and incrementally generate correct code. As an ablation, we remove the testing module directly to generate code without any intermediate filtering except compiler checks, resembling the approach from [42]. Using this approach, we can successfully generate a compiling version of ZOPFLI using LLM queries within 100$. However, we find that the generated code does not run even for trivial inputs such as \"Hello, World\" and crashes with index out of bounds error. This aligns with our findings from Figure 9 where compiling code solutions achieves only 80% median pass-rate with long-tail achieving < 20% pass-rate."}, {"title": "7 RELATED WORK", "content": null}, {"title": "7.1 C-to-Rust Interest and Applications", "content": "C to Rust full code translation has recently garnered much interest owing to memory safety vulnerabilities [45] in C, strong typing and ownership in Rust, and the relative ease of combining C and Rust through Clang compilation toolchains, and the C-Rust FFI (Foreign Function Interface). In particular, the NSA (US National Security Agency) has strongly advocated migration to Rust, and DARPA has launched the TRACTOR program [12] aimed at automating translation.\nWe based our main case study on ZOPFLI [14], a high-performance compression library by Google. We chose ZOPFLI because (a) compression libraries form a project milestone of the DARPA TRACTOR program, and (b) ZOPFLI has been manually translated to Rust [7] with the experience well-documented. We note that our auto-translation radically differs from the handwritten implementation (e.g., in terms of code architecture, organization, and Rust features used). There is also emerging user-study research comparing human expert-translated and auto-generated translations (e.g., using LLM-based techniques) [29].\nTranslation from C to Rust is considered especially relevant for sensitive codebases such as cryptographic and network libraries and OS kernel modules/drivers. However, the opinion is still divided on how much and what part of the source C codebase to migrate. For example, Li et al. [28] makes the case that certain low-level utilities (e.g., drivers) are best left in C since migration can lead to more subtle bugs (e.g., issues with memory mapping/address spaces)."}, {"title": "7.2 Automated C-to-Rust Translation", "content": "We provide a summary of C-to-Rust translation approaches in Table 1. The work on C-to-Rust translation closest to ours is [42]. They, too, perform LLM-driven C-to-Rust translation. However, unlike us, they focus on sampling alone, without intermediate testing (for internal functions). As a result, they achieve lower accuracy on the validation tests. One of our key insights is that incremental translation combined with internal function tests can result in major accuracy gains"}, {"title": "7.3 LLM-driven Code Translation", "content": "LLM driven code translation has received considerable interest in recent years. These works have explored training LLMs for code translation [38, 39, 47", "44": "and directing prompting based approaches [22, 24, 46, 54", "8": "used a rule-based test translation to translate small (single-"}]}