{"title": "EFFICIENT MATERIALS INFORMATICS BETWEEN ROCKETS AND ELECTRONS", "authors": ["Adam M. Krajewski"], "abstract": "The true power of computational research typically can lay in either what it accomplishes or what it enables others to accomplish. In this work, both avenues are simultaneously embraced across several distinct efforts existing at three general scales of abstractions of what a material is - atomistic, physical, and design. At each, an efficient materials informatics infrastructure is being built from the ground up based on (1) the fundamental understanding of the underlying prior knowledge, including the data, (2) deployment routes that take advantage of it, and (3) pathways to extend it in an autonomous or semi-autonomous fashion, while heavily relying on artificial intelligence (AI) to guide well-established DFT-based ab initio and CALPHAD-based thermodynamic methods.\n\nThe resulting multi-level discovery infrastructure is highly generalizable as it focuses on encoding problems to solve them easily rather than looking for an existing solution. To showcase it, this dissertation discusses the design of multi-alloy functionally graded materials (FGMs) incorporating ultra-high temperature refractory high entropy alloys (RHEAs) towards gas turbine and jet engine efficiency increase reducing CO2 emissions, as well as hypersonic vehicles. It leverages a new graph representation of underlying mathematical space using a newly developed algorithm based on combinatorics, not subject to many problems troubling the community. Underneath, property models and phase relations are learned from optimized samplings of the largest and highest quality dataset of HEA in the world, called ULTERA. At the atomistic level, a data ecosystem optimized for machine learning (ML) from over 4.5 million relaxed structures, called MPDD, is used to inform experimental observations and improve thermodynamic models by providing stability data enabled by a new efficient featurization framework.", "sections": [{"title": "Introduction", "content": "The discovery of novel materials that solve societal challenges or otherwise improve human lives is arguably one of the most critical components of building a modern world. Starting from the bronze age, humans were able to reliably combine raw materials in a structured fashion to achieve desired results, even though, at the time, there was no mechanistic understanding of why things happen. This has changed with gradual introduction of the scientific method, which standardized and systematized the discovery approach, with revolutionary advancements in materials happening every time a new technology for sharing and combining knowledge, such as propagation of the Greek language, printing press, or computer aided design (CAD), has been introduced and widely adopted.\n\nIn the current world, which went through the Internet revolution around 2000 and is currently going through the artificial intelligence (AI) revolution of the 2020s, one can point to the informatiztion of materials science as one such communication technology with a potential to revolutionize materials discovery by combining vast amounts of multidimensional data, intricate multidisciplinary domain knowledge, and ability to guide experiments beyond level achievable by a human being. In order to achieve this, one has to consider how to combine these efficiently, mitigating problems such as inhomogenities between data sources, computational challenges related to vast design spaces, hidden uncertainties in the reported values, and many flavors of errors, unavoidably present in the complex datasets involved.\n\nWhile creating an efficient, high-performance, fully general ecosystem for materials informatics appears nearly impossible even for a large group of researchers, a case-specific approach can be constructed in a fashion prioritizing generalizability, which can then be adjusted to other problems. This Dissertation builds such a case-specific approach, embedding a more general blueprint through the development of methods that are rarely limited to the target application but are rather biased towards it through design choices"}, {"title": "Flow of Material Discovery and This Work", "content": "Throughout this work, all topics raised in Section 1.1 will be discussed in a reversed order to progressively build from fundamentals to highly-specialized final applications, while retaining generality at every stage. This way, one will be able to build a holistic picture focused on how data flows within materials informatics research and converges together at consecutive length-scales to discover new materials in specific niches in a systematic, easy-to-automate approach, rather than build elaborate solutions that may happen to work well but can also break the fundamentals a common occurrence in our era of powerful computing and machine learning where tools always give an answer but it may hold negative value.\n\nAs shown in Figure 1.2, the first 4 chapters (colored blue) cover atomistic treatment of materials, discussing how data at this level is collected, featurized, managed and expanded. SIPFENN approach and the latest pySIPFENN featurization package are first developed. Then MPDD database with 4.5 million DFT-relaxed or experimentally observed entries is set up to serve as a highly efficient deployment vehicle. Lastly, crystALL approach automatically extends it into chemical areas of interest."}, {"title": "Executive Summary", "content": "First, Chapter 2 on Extensible Structure-Informed Prediction of Formation Energy with Improved Accuracy and Usability employing Neural Networks introduces fundamental concepts critical to structure-informed modeling of atomic configurations from the perspective of machine learning (ML) modeling and presents design of such models employing artificial neural networks for the prediction of formation energies of atomic structures based on elemental and structural features of Voronoi-tessellated materials. It provide a concise overview of the connection between the machine learning and the true material-property relationship, how to improve the generalization accuracy by reducing overfitting, how new data can be incorporated into the model to tune it to a specific material system, and preliminary results on using models to preform local structure relaxations.\n\nIt results in three final models optimized for achieving (1) highest test accuracy on the Open Quantum Materials Database (OQMD), (2) high extrapolative performance in the discovery of new materials, and (3) high performance at a low computational cost. On a test set of 21,800 compounds randomly selected from OQMD, these models achieves a mean absolute error (MAE) of 28, 40, and 42 meV/atom, respectively. The first model represented the state-of-the-art performance on this problem when released in 2020 [8] (see Table 2.2), the second model provides better predictions in a test case of interest not present in the OQMD, while the third one reduces the computational cost by a factor of 8 making it applicable to embedded and mobile applications. A transfer learning procedure was also demonstrated for the first time, showing dramatic improvements in extrapolation with just several datapoints (see Figure 2.10).\n\nThe results were implemented into a new open-source tool called SIPFENN or Structure-Informed Prediction of Formation Energy using Neural Networks, which not only improved the accuracy beyond existing models but also shipped in a ready-to-use form with pre-trained neural networks, which was first-of-a-kind at release, and a GUI interface allowing it be included in DFT calculations routines at nearly no cost.\n\nNext, Chapter 3 on Efficient Structure-Informed Featurization and Property Prediction of Ordered, Dilute, and Random Atomic Structures expands upon SIPFENN and implements a fully-featured machine learning focused analysis framework called pySIPFENN or python toolset for Structure-Informed Property and Feature Engineering with Neural Networks to fuel needs of structure-informed materials informatics a rapidly evolving discipline of materials science relying on the featurization of atomic structures or"}, {"title": "Software and Data Availability", "content": "As of 2022, the SIPFENN code has been deprecated in favor of highly improved pySIPFENN which was re-written with great stability, reliability, and deeper integration with other community tools. Chapter 3 describes it in detail and Section 3.7 describes its availability along related workshops and tutorial materials.\n\nFor archival purposes, the last version of SIPFENN code is available through Penn State's Phases Research Lab website at https://phaseslab.org/sipfenn in (1) a minimal version that can be run on pre-computed descriptors in CSV format as well as (2) ready-to-use version with pre-compiled Magpie [39]. SIPFENN contains hard-coded links to neural networks stored in the cloud that can be downloaded at a single-click (see Figure 2.13).\n\nAll neural networks were made available in (1) open-source MXNet format maintained by Apache Foundation, used within SIPFENN, (2) closed-source WLNet format maintained by Wolfram Research and having the advantage of even easier deployment, as well as guaranteed forward compatibility with future versions of Wolfram Language, and in (3) Open Neural Network Exchange (ONNX) format [58] distributed through pySIPFENN, as of April 2024.\n\nFor ensured longevity of results, original SIPFENN neural networks are stored through the courtesy of Zenodo.org service under DOI: 10.5281/zenodo.4006803 at the CERN's Data Centre."}, {"title": "Introduction", "content": "SIPFENN or Structure-Informed Prediction of Formation Energy using Neural Networks software, first introduced by the authors in 2020 [8], [9] and described extensively in Chapter 2, is one of several open-source tools available in the literature [48], [76]-[81] which train machine learning (ML) models on the data from large Density Functional Theory (DFT) based datasets like OQMD [10], [11], [59], AFLOW [14], [15], Materials Project [13], NIST-JARVIS[82], Alexandria [83], or GNOME [84] to predict formation energies of arbitrary atomic structures, with accuracy high enough to act as a low-cost surrogate in the prediction of thermodynamic stability of ground and non-ground state configurations at 0K temperature. The low runtime cost allows such models to efficiently screen through millions of different atomic structures of interest on a personal machine in a reasonable time.\n\nIn addition to high-accuracy neural network models trained on OQMD [10], [11], [59], SIPFENN included a number of features not found in competing tools available at the time, such as the ability to quickly readjust models to a new chemical system based on just a few DFT data points through transfer learning and a selection of models optimized for different objectives like extrapolation to new materials instead of overfitting to high-data-density regions or low memory footprint [9]."}, {"title": "General Structure Featurization Improvements", "content": "Being able to predict the thermodynamic stability of arbitrary atomic structures and their modifications is one of the most critical steps in establishing whether hypothetical candidates can be made in real life [90]; however, it is certainly not the only task of interest to the community [91], [92]. These diverse needs, combined with increasing interest in multi-property modeling, have shifted the focus of SIPFENN tool from model training [9] toward the development of reliable, easy-to-use, and efficient general-purpose featurizers existing in a framework, which can be used by researchers and companies to quickly develop and deploy property-specific models, or use features directly in exploring similarity and trends in materials.\n\nThus, while the acronym has been retained, the name of the software has been changed to python toolset for Structure-Informed Property and Feature Engineering with Neural Networks or pySIPFENN, and the software component has been carefully re-implemented in its entirety to make it as general as possible and enable the following core advantages:\n\nReliable featurization, which can be immediately transferred to other tools thanks to standalone submodule implementations based only on two common libraries (NumPy [93] and pymatgen [74]). These include completely re-written Ward2017 Java-based featurizer [76] (see Section 3.2.2) and 3 new ones, described in Sections 3.3, 3.4, and 3.5.\n\nEffortless plug-and-play deployment of neural network (and other) ML models (for any property) utilizing any of the defined feature vectors, enabled by the use of Open Neural Network Exchange (ONNX) open-source format [58] which can be exported from nearly every modern ML framework and is then loaded into pySIPFENN's PyTorch backend [94] through onnx2torch [95]. Furthermore, implementing custom predictors, beyond those supported by PyTorch, is made easy by design.\n\nDedicated ModelExporters submodule makes it easy to export trained models for publication or deployment on a different device while also enabling weight quantization and model graph optimizations to reduce memory requirements.\n\nThe ability to acquire data and adjust or completely retrain model weights through automated ModelAdjusters submodule. Its applications include:\n\n(a) Fine-tuning models based on additional local data to facilitate transfer learning ML schemes of the domain adaptation kind [96], where a model can be adjusted to new chemistry and specific calculation settings, introduced by SIPFENN back in 2020 [9], which is also being adopted by other models like ALIGNN [97]. Such an approach can also be used iteratively in active learning schemes where new data is obtained and added.\n\n(b) Tuning or retraining of the models based on other atomistic databases, or their subsets, accessed through OPTIMADE [98], [99] to adjust the model to a different domain, which in the context of DFT datasets could mean adjusting the model to predict properties with DFT settings used by that database or focusing its attention to specific chemistry like, for instance, all compounds of Sn and all perovskites.\n\n(c) Knowledge transfer learning [100] to adjust models to entirely new, often less available properties while harvesting the existing pattern recognition.\n\nThe resulting pySIPFENN computational framework is composed of several components, as depicted in Figure 3.1, and is available through several means described in Section 3.7, alongside high-quality documentation and examples."}, {"title": "Optimizations for Ordered Structures", "content": "Modeling of disordered materials is a critical area of research [107]; however, the vast majority of atomistic ab initio datasets used for ML studies focuses on highly symmetric ordered structures because of their high availability and ability to model complex phenomena in a holistic fashion if local ergodicity can be established [108], [109]. One evidence of the focus on such data is the fact that out of 4.4 million atomic structures in MPDD [110], which includes both DFT-based [10], [11], [13]-[15], [59], [82], [84] and experimental [85]-[87] data, only 54 thousand or 1.25% lack any symmetry. It is also worth noting that this number used to be much lower before the recent publication of the GNOME dataset by Google DeepMind [84], which accounts for around 4 of them.\n\nIn the case of remaining 98.75% structures, a 3-dimensional crystallographic spacegroup is defined for each of them along with corresponding Wyckoff positions (designated by letters) which are populated with either zero (empty), one (when symmetry-fixed), or up to infinitely many (typically up to a few) atoms forming a set of symmetry-equivalent sites called crystallographic orbits [111]. When these crystallographic orbits are collapsed into atoms occupying a unit cell, each is repeated based on the multiplicity associated with the Wyckoff position it occupies, which can range from 1 up to 192 (e.g., position I in Fm-3m/225), with values 1, 2, 3, 4, 6, 8, 16, 24, 32, 48, and 96 being typical [112] even in compositionally simple materials like one of the experimentally observed allotropes of pure silicon with atoms at the 8a, 32e, and 96g positions [113]. For certain crystal lattice types, the multiplicity can be somewhat reduced by redefining their spatial periodicity with so-called primitive unit cells, like in the case of the aforementioned Si allotrope, in which primitive unit cell has 4 times fewer (34) sites but still over 10 times more than the 3 unique crystallographic orbits.\n\nThis presents an immediate and previously untapped opportunity for multiplying the computational performance of most atomistic featurizers (e.g., Matminer [114]) and ML models [9], [48], [76]-[81], [115], [116], which nearly always process all atoms given in the input structure occasionally converting to primitive unit cell in certain routines (CHGNet [79]), unless they operate on different occupancies of the same structure [117]. This allows for a proportional decrease in both CPU/GPU time and memory footprint. The general-purpose KS2022 in pySIPFENN uses high-performance symmetry analysis library spglib [118] to automatically take advantage of this whenever possible, as depicted in the schematic in Figure 3.2. It shows an interesting example of a topologically close-packed \u03c3 phase, which is critical to model in a wide range of metallic alloys [119] but challenging in terms of combinatorics because of 5 unique sites that can be occupied by many elements [120], [121] making it a very active area of ML modeling efforts [117], [122] in the thermodynamics community.\n\nIn the case of KS2022 featurizer, running the same 30-atom test as in Section 3.2.3 but on \u03c3 phase takes on average 84ms or is 5.1 times faster thanks to processing 6 times less sites. Similar results should be (a) quickly achievable with any other featurizer processing individual sites, including most graph representations embedding local environments (e.g., MEGNet [77]) or deconstructing graphs into graphlets (e.g., minervachem molecule featurizer [123]), and (b) possible with convolution-based models operating on graphs (e.g., ALIGNN [78]) or voxels [80] through custom adjustments to the specific convolution implementation. In the case of voxel representations and any other memory-intense ones, it may also be beneficial to utilize this approach to compress them when transferring between devices like"}, {"title": "Optimizations for Dilute, Defect, and Doped Structures", "content": "The optimization strategy in Section 3.3 ensures that only the sites that are guaranteed to be crystallographically unique are processed through featurization or graph convolution and is directly applicable to the vast majority of both data points and literature methods. However, in the case of methods relying on describing the immediate neighbors, whether through Wigner-Seitz cell (see Fig. 3.2) or subgraph (see, e.g., [77]), one can achieve further efficiency improvements by considering which sites are guaranteed to be unique under the representation.\n\nThere are several classes of atomic structures where the distinction above makes a difference, but the room to improve is exceptionally high when one site in an otherwise highly symmetric structure is modified, leading to a structure that, depending on the context, will be typically called dilute when discussing alloys [125], doped when discussing electronic materials [126], or said to have defect in a more general sense [127]. Throughout pySIPFENN's codebase and the rest of this work, the single term dilute is used to refer to all of such structures due to authors' research focus on Ni-based superalloys at the time when optimizations below were made public in February 2023.\n\nTo visualize the concept, one can consider, for instance, a 3x3x3 body-centered cubic (BCC) conventional supercell (54 sites) and call it base structure. If it only contains a single specie, then KS2022 from Section 3.3 will recognize that there is only one crystallographic orbit and only process that one. However, if a substitution is made at any of the 54 equivalent sites, the space group will change from Im-3m (229) to Pm-3m (221), with 8 crystallographic orbits on 7 Wyckoff positions; thus, the default KS2022 featurizer will process 8 sites. At the same time, several of these crystallographic orbits will be differentiated only by the orientation and distance to the dilute (substitution) site, which does affect ab initio calculation results (e.g., vacancy formation energy vs supercell size [128]), but is guaranteed to have no effect on the model's representation because of the exact same neighborhood configuration (including angles and bond lengths) if conditions given earlier are met. Thus, it only requires adjustments to the site multiplicities or convolution implementation (simplified through, e.g., a Markov chain). In the particular dilute-BCC example at hand, depicted in Figure 3.3, there are 4 such representation-unique crystallographic orbits, i.e., 1 with the dilute atom, 2 neighboring the dilute atom sharing either large hexagonal (1st nearest neighbor shell) or small square face (2nd nearest neighbor shell), and 1 non af-fected by the dilute atom which is equivalent to the remaining 4 orbits; thus reducing number of sites that need to be considered by a factor of 2.\n\nThe KS2022_dilute featurization routine, schematically depicted in Figure 3.3, conveniently automates the above process for both simple cases like aforementioned substitution in pure element and complex cases like introducing a dilute atom at the 2a/2b orbit in \u03c3-phase (green cell in Fig. 3.2), by performing independent identification of crystallographic orbits in the dilute structure and base structure, followed by identification of the dilute site and its configuration to establish orbit equivalency under pySIPFENN's KS2022 representation, to finally reconstruct complete site ensemble of the dilute structure.\n\nIn the case of KS2022_dilute implementation run on the dilute BCC supercell shown in Figure 3.3, the efficiency is improved nearly proportionally to the reduction in the number of considered sites,"}, {"title": "Handling Millions of Atomic Structures", "content": "Traditionally, the field of materials science deals with highly complex data, in terms of both input and output descriptions, acquired through laborious experiments in the laboratories, such as mechanical or electrochemical tests performed according to professional standards or expensive computations on supercomputer clusters, including ones based on the density functional theory (DFT) [137] or finite element method (FEM) [138]. Because of this high-cost aspect, the number of data points being generated is usually very limited, especially after raw data is fitted into models and disseminated through reports and scientific publications as typeset tables, text, or figures, which are then carefully interpreted by other researchers.\n\nIn the last few decades, however, the community has become increasingly engaged in large-scale efforts to construct extensive collections of data points, especially after endorsement and special funding was provided by the Materials Genome Initiative in 2011 [89], [139], [140]. It significantly accelerated many efforts to combine materials-specific scientific literature sources into a homogeneous structure were attempted, with perhaps the most successful being the Pauling File developed since 1995 with as of 2024, was built with 1000 years of full-time academic effort [141] and underlies several databases, including ASM Phase Diagram Database [142] and Materials Platform for Data Science (MPDS) [141].\n\nFurthermore, several computational databases listed later in Section 4.2.2 were also created and, in some cases, grew beyond one million entries.\n\nWith the rise of such large-scale databases, it becomes critical to be able to efficiently operate on them and utilize them, as even seemingly fast calculations of 1s grow to over 8 weeks when performed 5 million times. Furthermore, this scale is likely to sharply increase in the near future due to a large volume of data coming from machine learning (ML) studies, which, even if filtered out, will likely accelerate search efforts using more traditional computation by constantly providing guidance.\n\nThree entities at the core of MPDD are treated as \"first-class citizens\" interacting with each other. Going counter-clockwise Materials cover our past sampling of the problem domain, Descriptors cover our understanding of it, and Properties determine utility. Going clockwise desired Properties guide analysis leading to understanding encoded in Descriptors, which inform us of unexplored regions of problem domain in their individual contexts."}, {"title": "Ergodic Ensemble Approach to the Material Discovery", "content": "The ability to determine atomic structure of existing or hypothetical solids formed by chemical elements is one of the fundamental challanges in theoritical crystallography and in materials science as a whole. For the last several decades [155], the two remaining challenges are predictions of (a) compositions forming compounds of interest and (b) candidate topologies or approximate atomic arrangements they can form. Given these, the exact structure can be solved experimentally, by matching X-ray diffraction profile to candidates using the method by Bail et al. [156] and interpreting the profile, or using deep learning [157]. Similar analysis can be done, e.g., on the Raman spectrum [158]. For hypothetical compounds in their pure states, the exact structure can also be solved using ab initio quantum mechanical methods, such as Kohn-Sham density functional theory (DFT) [159], which are computationally expensive but can provide accurate results and continue to improve [160].\n\nThe latter problem of proposing the topologies has been traditionally approached by finding geometries satisfying constraints, like the principles systematized by [@Pauling1929], which included then-novel concepts like co\u00f6rdination. Over decades, this has evolved into automated computational tools like the GRINSP by Bail [161] which can propose such structures. More recently, increasing computational power shifted focus towards testing many options, with some of the methods starting from random arrangements\n\nThus, efforts to generate ensembles of possible structure candidates need to propose diverse polymorph populations, such as one in Fig. 6 in Pickard et al. [162] generateed through AIRSS."}, {"title": "Creating an Efficient Database Infrastructure for Discovery of Real Materials Exemplified with High Entropy Alloys", "content": "Compared to atomic structures discussed in Chapters 2 through 5, which are precisely defined, \"real\" materials which were physically made in a lab tends to be (a) much less homogeneous in terms of how they are reported and (b) the data belonging to them is much less defined in terms of the completeness of description, i.e., many critical parameters like phases present can be missing or misreported due to a plethora of reasons including lack of equipment, limited precision, or human errors. Because of these, handling them generally requires much more flexible and elaborate ecosystem capable of:\n\nHandling voids in the knowledge with a structured, well-defined, reproducible procedures.\n\nFilling in the gaps using the prior knowledge and available domain coverage.\n\nDetect unavoidable errors coming from humans, machines, and miscommunications along a long path from when the data is conceived (e.g., \"This alloy should be strong and ductile\"), through initial report (e.g., \"Sample 17 (A17) was X [...] Tensile tests were performed. [...] A17 fractured at 0.172\"), to the database (e.g., \"X has tensile ductility of 17.2%\").\n\nSimplified schematic of automatic linear combination modeling of properties. For every chemical composition, a linear combination of elemental properties is calculated for BCC, FCC, and HCP structures based on best-matched elemental polymorph data coming from experiments and DFT-based pure element calculations. If an applicable structure set (e.g., FCC+FCC+HCP) has been reported for a given input datapoint, an average of the respective linear combinations is reported as structure-informed LC."}, {"title": "Detecting Abnormalities in Materials Data", "content": "In the past, these challenges were not as severe nor impactfull because the effort placed in the data analysis was much greater than data extraction; however, as our community moves towards this large-scale multi-source data collection, it increasingly starts to surface impacting ML efforts. While present in datasets on all alloy classes, this problem is particularly visible in high entropy alloys (HEAs) due to their novelty and complexity. A number of factors causes errors, including a lack of standardized notations and general parsing mistakes, such as typos, but most of them can be caught through detection of abnormalities in relation to the correct data and its patterns.\n\nIn this chapter, we present an open-source Python tool called PyQAlloy that can be run on alloy datasets to screen them for a variety of such abnormalities. It puts each data point in a variety of scopes ranging from individual datapoint analysis, through single-study meta-data, to the database as a whole."}, {"title": "Optimization of Compositional Dataset Domain towards Reliable Machine Learning Training and Deployment", "content": "Chapter 8 on Optimization of Compositional Dataset Domain towards Reliable Machine Learning Training and Deployment builds on top of ULTERA datasets curated by PyQAlloy and recognizes that selecting dimensions to model, or in this case, the selection of chemical elements to restrict the modeling effort, is a combinatorically hard problem for complex compositions existing in highly dimensional spaces due to the interdependency of components being present. Consequentially, optimizing the data availability and density for applications such as machine learning becomes a challenge.\n\nA novel tool, called nimCSO, has been introduced and implmenets several methods for performing this task over even millions of datapoints and extreme dimensionalities encountered, for instance, in materials science of Compositionally Complex Materials (CCMs) which often span 20-45 chemical elements, 5-10 processing types, and several temperature regimes, for up to 60 total data dimensions.\n\nIt achieves the extreme performance by leveraging the metaprogramming ability of the Nim language to optimize itself at the compile time, both in terms of speed and memory handling, to the specific problem statement and dataset at hand based on a human-readable configuration file. As demonstrated in the chapter, it reaches the physical limits of the hardware (L1 cache latency) and can outperform an efficient native Python implementation over 400 times in terms of speed and 50 times in terms of memory usage (not counting interpreter), while also outperforming NumPy implementation 35 and 17 times, respectively, when checking a candidate solution. It was designed to be both a user-ready tool and a scaffold for building even more elaborate methods in the future, including heuristics going beyond data availability."}, {"title": "Inverse Design of Compositionally Complex Alloys", "content": "Chapter 9 on Inverse Design of Compositionally Complex Alloys demonstrates how ULTERA dataset can be used to propose new alloys through inverse design. It leverages conditional Generative Adversarial Netowrks (cGAN) models to bias the predicted alloys in terms of (1) the property values by setting the conditioning and (2) the predicted compositions by recognizing concept vectors in the latent space established at the training step. The former ability is implemented within a demonstrator Python package called heaGAN which can be used to design HEAs by setting design criteria. It also allows advanced users to re-train cGAN models based on their own data or a third-party property surrogate model."}, {"title": "Efficient generation of grids and traversal graphs in compositional spaces towards exploration and path planning exemplified in materials", "content": "Chapter 10 on Efficient generation of grids and traversal graphs in compositional spaces towards exploration and path planning exemplified in materials departs from both data and machine learning to focus on the mathematical abstractions of the alloy design problem. It begins by considering that many disciplines of science and engineering deal with problems related to compositions, ranging from chemical compositions in materials science to portfolio compositions in economics. They exist in non-Euclidean simplex spaces, causing many standard tools to be incorrect or inefficient, which is significant in combinatorically or structurally challenging spaces exemplified by Compositionally Complex Materials (CCMs) and Functionally Graded Materials (FGMs). In this chapter, we explore them conceptually in terms of problem spaces and quantitatively in terms of computational feasibility.\n\nSeveral essential methods specific to the compositional (simplex) spaces are implemented, with most critical parts presented in chapter body as code listings, through a high-performance open-source library nimplex. Most significantly, we derive and implement an algorithm for constructing a novel n-dimensional simplex graph data structure, which contains all discretized compositions and all possible neighbor-to-neighbor transitions as pointer arrays. Critically, no distance or neighborhood calculations are performed, instead leveraging pure combinatorics and the ordering in procedurally generated simplex grids, keeping the algorithm O(N), so that graphs with billions of transitions take seconds to construct on a laptop. Furthermore, we demonstrate how such graph representations can be combined to express"}, {"title": "Infeasibility Gliding in Compositional Spaces", "content": "Chapter 11 on Infeasibility Gliding in Compositional Spaces leverages the novel graph-based representations of compositional spaces to dramatically improve our approach to screening and modifying compositionally complex alloys based on their structural constraint feasibility. It builds on the fact that the equilibrium existence of a particular phase in the phase-regions of chemical space (not necessarily corresponding to the present elements, as exploted in Chapter 10) is bound by a continuous surface of (generally) low curvature. Thus, performing the community-standard brute force screening type evaluation of all compositions will often calculate high numbers of points that cannot be reached.\n\nThus, nimplex-generated graphs are used to quickly re-implement the screening procedure into depth-first search traversals of the feasible regions that glide one the infeasibility boundaries, reducing computation by half in semi-randomly picked \"bad-case\" which was known to produce relatively large feasible regions. In more elaborate screenings not taking advantage of prior knowledge, this improvement may very well reach between one and several orders of magnitude."}, {"title": "Path Planing In Compositional Spaces through Graph Traversals", "content": "Lastly, Chapter 12 on Path Planing In Compositional Spaces through Graph Traversals takes additional advantage of the nimplex-generated graphs, which can be also extracted from feasible regions established in 11, to deploy path planning algorithms and find sets of compositions that allow dissimilar alloys to be continuously combined in the least number of steps. Additional considerations are made in relation to an example property field of Root Mean Square Atomic Displacement (RMSAD) related to the yield stress in HEAs. In particular, its value, gradient, and magnitude of gradient are used to modify the graph weights encoding different design objectives, which are effortlessly solved with the same out-of-the box pathfinding approach."}]}