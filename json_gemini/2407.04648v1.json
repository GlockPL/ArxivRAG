{"title": "EFFICIENT MATERIALS INFORMATICS BETWEEN\nROCKETS AND ELECTRONS", "authors": ["Adam M. Krajewski"], "abstract": "The true power of computational research typically can lay in either what it accomplishes or what it\nenables others to accomplish. In this work, both avenues are simultaneously embraced across several\ndistinct efforts existing at three general scales of abstractions of what a material is - atomistic, physical,\nand design. At each, an efficient materials informatics infrastructure is being built from the ground\nup based on (1) the fundamental understanding of the underlying prior knowledge, including the data,\n(2) deployment routes that take advantage of it, and (3) pathways to extend it in an autonomous or\nsemi-autonomous fashion, while heavily relying on artificial intelligence (AI) to guide well-established\nDFT-based ab initio and CALPHAD-based thermodynamic methods.\n The resulting multi-level discovery infrastructure is highly generalizable as it focuses on encoding prob-\nlems to solve them easily rather than looking for an existing solution. To showcase it, this dissertation\ndiscusses the design of multi-alloy functionally graded materials (FGMs) incorporating ultra-high tem-\nperature refractory high entropy alloys (RHEAs) towards gas turbine and jet engine efficiency increase\nreducing CO2 emissions, as well as hypersonic vehicles. It leverages a new graph representation of un-\nderlying mathematical space using a newly developed algorithm based on combinatorics, not subject to\nmany problems troubling the community. Underneath, property models and phase relations are learned\nfrom optimized samplings of the largest and highest quality dataset of HEA in the world, called ULTERA.\nAt the atomistic level, a data ecosystem optimized for machine learning (ML) from over 4.5 million re-\nlaxed structures, called MPDD, is used to inform experimental observations and improve thermodynamic\nmodels by providing stability data enabled by a new efficient featurization framework.", "sections": [{"title": "Introduction", "content": "The discovery of novel materials that solve societal challenges or otherwise improve human lives is\narguably one of the most critical components of building a modern world. Starting from the bronze age,\nhumans were able to reliably combine raw materials in a structured fashion to achieve desired results,\neven though, at the time, there was no mechanistic understanding of why things happen. This has\nchanged with gradual introduction of the scientific method, which standardized and systematized the\ndiscovery approach, with revolutionary advancements in materials happening every time a new technology\nfor sharing and combining knowledge, such as propagation of the Greek language, printing press, or\ncomputer aided design (CAD), has been introduced and widely adopted.\nIn the current world, which went through the Internet revolution around 2000 and is currently going\nthrough the artificial intelligence (AI) revolution of the 2020s, one can point to the informatiztion of ma-\nterials science as one such communication technology with a potential to revolutionize materials discovery\nby combining vast amounts of multidimensional data, intricate multidisciplinary domain knowledge, and\nability to guide experiments beyond level achievable by a human being. In order to achieve this, one has\nto consider how to combine these efficiently, mitigating problems such as inhomogenities between data\nsources, computational challenges related to vast design spaces, hidden uncertainties in the reported\nvalues, and many flavors of errors, unavoidably present in the complex datasets involved.\nWhile creating an efficient, high-performance, fully general ecosystem for materials informatics appears\nnearly impossible even for a large group of researchers, a case-specific approach can be constructed in a\nfashion prioritizing generalizability, which can then be adjusted to other problems. This Dissertation builds\nsuch a case-specific approach, embedding a more general blueprint through the development of methods\nthat are rarely limited to the target application but are rather biased towards it through design choices,\nassumptions, and helpful simplifications. In the process, it introduces several novel individual pieces of\nsoftware, including pySIPFENN, MPDD, crystALL, ULTERA, PyQAlloy, nimCSO, nimplex, and their\nderivatives to collectively bridge ab-initio methods, belonging to the domain of quantum physics, with\nengineering of devices placed in extreme environments, such as gas engine turbine blades or hypersonic\nvehicles, designed by aerospace engineers, through efficient materials informatics pipelines existing across\nscales, as summarized in the Figure 1.1 and described in detail in Section 1.2."}, {"title": "Flow of Material Discovery and This Work", "content": "Throughout this work, all topics raised in Section 1.1 will be discussed in a reversed order to progressively\nbuild from fundamentals to highly-specialized final applications, while retaining generality at every stage.\nThis way, one will be able to build a holistic picture focused on how data flows within materials informatics\nresearch and converges together at consecutive length-scales to discover new materials in specific niches\nin a systematic, easy-to-automate approach, rather than build elaborate solutions that may happen to\nwork well but can also break the fundamentals a common occurrence in our era of powerful computing\nand machine learning where tools always give an answer but it may hold negative value.\nAs shown in Figure 1.2, the first 4 chapters (colored blue) cover atomistic treatment of materials,\ndiscussing how data at this level is collected, featurized, managed and expanded. SIPFENN approach\nand the latest pySIPFENN featurization package are first developed. Then MPDD database with 4.5\nmillion DFT-relaxed or experimentally observed entries is set up to serve as a highly efficient deployment\nvehicle. Lastly, crystALL approach automatically extends it into chemical areas of interest.\nAll of MPDD is then harvested to model materials at the physical scale by (1) serving as inputs to ther-\nmodynamic model generation using pycalphad [6] and ESPEI [7] or training of pySIPFENN ML models\ngenerating needed data, and (2) informing experimental observations by, for instance, automatically com-\npiling a set of carbides stable in an alloy system at OK. At the same time, the largest experimental HEA\ndata infrastructure, called ULTERA, is compiled joining together over 6,800 property datapoints manually\nextracted from 555 literature publications.\nThe experimental database is curated through novel PyQAlloy package created to detect abnormalities\nand dramatically reduce fraction of erroneous data relative to other similar ones in the literature. Once\ncurated, the nimCSO package can guide ML efforts in terms of which components of the data (chemical\nelements) should be considered when modeling to optimize trade-off between applicability and data\ndensity available to the models. Lastly, compositional space representations generated through nimplex\nand inverse design workflows serve as deployment vehicles for the trained methods."}, {"title": "Executive Summary", "content": "First, Chapter 2 on Extensible Structure-Informed Prediction of Formation Energy with Improved Ac-\ncuracy and Usability employing Neural Networks introduces fundamental concepts critical to structure-\ninformed modeling of atomic configurations from the perspective of machine learning (ML) modeling\nand presents design of such models employing artificial neural networks for the prediction of formation\nenergies of atomic structures based on elemental and structural features of Voronoi-tessellated materials.\nIt provide a concise overview of the connection between the machine learning and the true material-\nproperty relationship, how to improve the generalization accuracy by reducing overfitting, how new data\ncan be incorporated into the model to tune it to a specific material system, and preliminary results on\nusing models to preform local structure relaxations.\nIt results in three final models optimized for achieving (1) highest test accuracy on the Open Quantum\nMaterials Database (OQMD), (2) high extrapolative performance in the discovery of new materials,\nand (3) high performance at a low computational cost. On a test set of 21,800 compounds randomly\nselected from OQMD, these models achieves a mean absolute error (MAE) of 28, 40, and 42 meV/atom,\nrespectively. The first model represented the state-of-the-art performance on this problem when released\nin 2020 [8] (see Table 2.2), the second model provides better predictions in a test case of interest not\npresent in the OQMD, while the third one reduces the computational cost by a factor of 8 making it\napplicable to embedded and mobile applications. A transfer learning procedure was also demonstrated for\nthe first time, showing dramatic improvements in extrapolation with just several datapoints (see Figure\n2.10).\nThe results were implemented into a new open-source tool called SIPFENN or Structure-Informed\nPrediction of Formation Energy using Neural Networks, which not only improved the accuracy beyond\nexisting models but also shipped in a ready-to-use form with pre-trained neural networks, which was\nfirst-of-a-kind at release, and a GUI interface allowing it be included in DFT calculations routines at\nnearly no cost.\nNext, Chapter 3 on Efficient Structure-Informed Featurization and Property Prediction of Ordered,\nDilute, and Random Atomic Structures expands upon SIPFENN and implements a fully-featured machine\nlearning focused analysis framework called pySIPFENN or python toolset for Structure-Informed Property\nand Feature Engineering with Neural Networks to fuel needs of structure-informed materials informatics\n- a rapidly evolving discipline of materials science relying on the featurization of atomic structures or"}, {"title": "Software and Data Availability", "content": "As of 2022, the SIPFENN code has been deprecated in favor of highly improved pySIPFENN which was\nre-written with great stability, reliability, and deeper integration with other community tools. Chapter\n3 describes it in detail and Section 3.7 describes its availability along related workshops and tutorial\nmaterials.\nFor archival purposes, the last version of SIPFENN code is available through Penn State's Phases\nResearch Lab website at https://phaseslab.org/sipfenn in (1) a minimal version that can be run on pre-\ncomputed descriptors in CSV format as well as (2) ready-to-use version with pre-compiled Magpie [39].\nSIPFENN contains hard-coded links to neural networks stored in the cloud that can be downloaded at a\nsingle-click (see Figure 2.13).\nAll neural networks were made available in (1) open-source MXNet format maintained by Apache Foun-\ndation, used within SIPFENN, (2) closed-source WLNet format maintained by Wolfram Research and\nhaving the advantage of even easier deployment, as well as guaranteed forward compatibility with future\nversions of Wolfram Language, and in (3) Open Neural Network Exchange (ONNX) format [58] distributed\nthrough pySIPFENN, as of April 2024.\nFor ensured longevity of results, original SIPFENN neural networks are stored through the courtesy of\nZenodo.org service under DOI: 10.5281/zenodo.4006803 at the CERN's Data Centre."}, {"title": "Introduction", "content": "SIPFENN or Structure-Informed Prediction of Formation Energy using Neural Networks software, first\nintroduced by the authors in 2020 [8], [9] and described extensively in Chapter 2, is one of several open-\nsource tools available in the literature [48], [76]\u2013[81] which train machine learning (ML) models on the\ndata from large Density Functional Theory (DFT) based datasets like OQMD [10], [11], [59], AFLOW [14],\n[15], Materials Project [13], NIST-JARVIS[82], Alexandria [83], or GNOME [84] to predict formation\nenergies of arbitrary atomic structures, with accuracy high enough to act as a low-cost surrogate in the\nprediction of thermodynamic stability of ground and non-ground state configurations at OK tempera-\nture. The low runtime cost allows such models to efficiently screen through millions of different atomic\nstructures of interest on a personal machine in a reasonable time.\nIn addition to high-accuracy neural network models trained on OQMD [10], [11], [59], SIPFENN included\na number of features not found in competing tools available at the time, such as the ability to quickly\nreadjust models to a new chemical system based on just a few DFT data points through transfer learning\nand a selection of models optimized for different objectives like extrapolation to new materials instead\nof overfitting to high-data-density regions or low memory footprint [9]."}, {"title": "General Structure Featurization Improvements", "content": "Being able to predict the thermodynamic stability of arbitrary atomic structures and their modifications is\none of the most critical steps in establishing whether hypothetical candidates can be made in real life [90];\nhowever, it is certainly not the only task of interest to the community [91], [92]. These diverse needs,\ncombined with increasing interest in multi-property modeling, have shifted the focus of SIPFENN tool\nfrom model training [9] toward the development of reliable, easy-to-use, and efficient general-purpose\nfeaturizers existing in a framework, which can be used by researchers and companies to quickly develop\nand deploy property-specific models, or use features directly in exploring similarity and trends in materials.\nThus, while the acronym has been retained, the name of the software has been changed to python\ntoolset for Structure-Informed Property and Feature Engineering with Neural Networks or pySIPFENN,\nand the software component has been carefully re-implemented in its entirety to make it as general as\npossible and enable the following core advantages:\n1. Reliable featurization, which can be immediately transferred to other tools thanks to standalone\nsubmodule implementations based only on two common libraries (NumPy [93] and pymatgen [74]).\nThese include completely re-written Ward2017 Java-based featurizer [76] and\n3 new ones, described in Sections 3.3, 3.4, and 3.5.\n2. Effortless plug-and-play deployment of neural network (and other) ML models (for any property)\nutilizing any of the defined feature vectors, enabled by the use of Open Neural Network Exchange\n(ONNX) open-source format [58] which can be exported from nearly every modern ML framework\nand is then loaded into pySIPFENN's PyTorch backend [94] through onnx2torch [95]. Fur-\nthermore, implementing custom predictors, beyond those supported by PyTorch, is made easy by\ndesign.\n3. Dedicated ModelExporters submodule makes it easy to export trained models for publication\nor deployment on a different device while also enabling weight quantization and model graph\noptimizations to reduce memory requirements.\n4. The ability to acquire data and adjust or completely retrain model weights through automated\nModelAdjusters submodule. Its applications include:\n(a) Fine-tuning models based on additional local data to facilitate transfer learning ML schemes\nof the domain adaptation kind [96], where a model can be adjusted to new chemistry and\nspecific calculation settings, introduced by SIPFENN back in 2020 [9], which is also being\nadopted by other models like ALIGNN [97]. Such an approach can also be used iteratively in\nactive learning schemes where new data is obtained and added.\n(b) Tuning or retraining of the models based on other atomistic databases, or their subsets,\naccessed through OPTIMADE [98], [99] to adjust the model to a different domain, which in\nthe context of DFT datasets could mean adjusting the model to predict properties with DFT\nsettings used by that database or focusing its attention to specific chemistry like, for instance,\nall compounds of Sn and all perovskites.\n(c) Knowledge transfer learning [100] to adjust models to entirely new, often less available prop-\nerties while harvesting the existing pattern recognition.\nThe resulting pySIPFENN computational framework is composed of several components, as depicted\nin Figure 3.1, and is available through several means described in Section 3.7, alongside high-quality\ndocumentation and examples."}, {"title": "Ward2017 Reimplementation", "content": "In their 2017 work Ward et al. [76] introduced a novel atomic structure featurization concept based\non establishing and weighting neighbor interactions by faces from 3D Voronoi tesselation to describe\nlocal chemical environments (LCEs) of atomic sites and then performing statistics over them to obtain a\nglobal feature vector. The original SIPFENN models [8] built on top of this while utilizing an improved,\ncarefully designed deep neural network models to obtain up to 2.5 times lower prediction error on the\nsame dataset [9]. A detailed description of the descriptor can be found in Section 2.1 of Krajewski et al.\n[9]. In general, the calculation of the Ward2017 descriptor consists of three parts:\nCalculation of attributes based upon global averages over the components of the structure.\nCalculation of attributes based upon local neighborhood averages for each site in the structure.\nCalculation of more complex attributes based upon averages over paths in the structure.\nWard et al. [76] implemented the above calculations in Java, which was popular at the time; while\nmost of the current machine-learning packages use almost exclusively Python (e.g., scikit-learn\n[101] and PyTorch [94]), making it cumbersome to use Java. Even more critically, the original Java\nimplementation was not computationally efficient (as explored in Sections 3.3, 3.4, and 3.5), and enabling\ntools were not supported in Java.\nIn the present work, authors have reimplemented Ward et al. [76] from scratch in Python as a standalone\nsubmodule for pySIPFENN, which calculates all 271 features within numerical precision, except for three\nperforming a random walk on the structure, which is stochastic in nature and results in slightly different\nfinal values due to a different seed. The Voronoi tessellation has been implemented with Voro++\n[102]\u2013[104] and all numerical operations were written using NumPy [93] arrays to greatly speed up the\ncalculations and make the efficient utilization of different computing resources, such as GPUs, easy to\nimplement."}, {"title": "KS2022 Feature Optimization", "content": "Typically, during feature engineering, researchers first attempt to collect all features expected to enable\ngood inference and then remove some based on the interplay of several factors:\n1. Low impact on the performance, which increases the representation memory requirements and\npossibly increases the risk of overfitting to both systematic and random trends.\n2. High computational cost, which limits the throughput of the method deployment.\n3. Unphysical features or feature representations which can improve model performance against\nwell-behaving benchmarks covering a small subset of the problem domain but compromise model\ninterpretability and extrapolation ability in unpredictable ways."}, {"title": "Optimizations for Ordered Structures", "content": "Modeling of disordered materials is a critical area of research [107]; however, the vast majority of atomistic\nab initio datasets used for ML studies focuses on highly symmetric ordered structures because of their\nhigh availability and ability to model complex phenomena in a holistic fashion if local ergodicity can\nbe established [108], [109]. One evidence of the focus on such data is the fact that out of 4.4 million\natomic structures in MPDD [110], which includes both DFT-based [10], [11], [13]\u2013[15], [59], [82], [84]\nand experimental [85]\u2013[87] data, only 54 thousand or 1.25% lack any symmetry. It is also worth noting\nthat this number used to be much lower before the recent publication of the GNOME dataset by Google\nDeepMind [84], which accounts for around 4 of them."}, {"title": "Optimizations for Dilute, Defect, and Doped Structures", "content": "The optimization strategy in Section 3.3 ensures that only the sites that are guaranteed to be crystallo-\ngraphically unique are processed through featurization or graph convolution and is directly applicable to\nthe vast majority of both data points and literature methods. However, in the case of methods relying on\ndescribing the immediate neighbors, whether through Wigner-Seitz cell or subgraph, one can achieve further efficiency improvements by considering which sites are guaranteed to\nbe unique under the representation.\nThere are several classes of atomic structures where the distinction above makes a difference, but\nthe room to improve is exceptionally high when one site in an otherwise highly symmetric structure is\nmodified, leading to a structure that, depending on the context, will be typically called dilute when\ndiscussing alloys, doped when discussing electronic materials , or said to have defect in a\nmore general sense [127]. Throughout pySIPFENN\u2019s codebase and the rest of this work, the single term\ndilute is used to refer to all of such structures due to authors\u2019 research focus on Ni-based superalloys at\nthe time when optimizations below were made public in February 2023.\nTo visualize the concept, one can consider, for instance, a 3x3x3 body-centered cubic (BCC) conven-\ntional supercell (54 sites) and call it base structure. If it only contains a single specie, then KS2022\nfrom Section 3.3 will recognize that there is only one crystallographic orbit and only process that one.\nHowever, if a substitution is made at any of the 54 equivalent sites, the space group will change from\nIm-3m (229) to Pm-3m (221), with 8 crystallographic orbits on 7 Wyckoff positions; thus, the default\nKS2022 featurizer will process 8 sites. At the same time, several of these crystallographic orbits will be\ndifferentiated only by the orientation and distance to the dilute (substitution) site, which does affect\nab initio calculation results (e.g., vacancy formation energy vs supercell size [128]), but is guaranteed\nto have no effect on the model\u2019s representation because of the exact same neighborhood configuration\n(including angles and bond lengths) if conditions given earlier are met. Thus, it only requires adjustments\nto the site multiplicities or convolution implementation (simplified through, e.g., a Markov chain). In the\nparticular dilute-BCC example at hand, depicted in Figure 3.3, there are 4 such representation-unique\ncrystallographic orbits, i.e., 1 with the dilute atom, 2 neighboring the dilute atom sharing either large\nhexagonal (1st nearest neighbor shell) or small square face (2nd nearest neighbor shell), and 1 non af-\nfected by the dilute atom which is equivalent to the remaining 4 orbits; thus reducing number of sites\nthat need to be considered by a factor of 2.\nThe KS2022_dilute featurization routine, schematically depicted in Figure 3.3, conveniently auto-\nmates the above process for both simple cases like aforementioned substitution in pure element and\ncomplex cases like introducing a dilute atom at the 2a/2b orbit in \u03c3-phase, by\nperforming independent identification of crystallographic orbits in the dilute structure and base struc-\nture, followed by identification of the dilute site and its configuration to establish orbit equivalency\nunder pySIPFENN\u2019s KS2022 representation, to finally reconstruct complete site ensemble of the dilute\nstructure.\nIn the case of KS2022_dilute implementation run on the dilute BCC supercell shown in Figure\n3.3, the efficiency is improved nearly proportionally to the reduction in the number of considered sites,"}, {"title": "Optimizations for Random Solid Solutions", "content": "Sections 3.3 and 3.4 have demonstrated how recognition of symmetry in ordered structures can guarantee\nequivalency of sites and how understanding the character of featurization can further extend that notion\nof equivalency so that the ML representations of all sites can be obtained efficiently up to an order of\nmagnitude faster. Random solid solutions are the conceptually opposite class of atomic structures, where\nthe lack of symmetry or site equivalency is guaranteed, yet featurizing them requires one to solve the\nsame problem of efficiently obtaining the ML representations of all sites present, which also happen to\nbe infinite.\nTypically, in the ab initio community, random solid solutions are represented using Special Quasirandom\nStructures (SQS) introduced in landmark 1990 work by Zunger et al. [63], which are the best structures\nto match neighborhood correlations in a purely random state given component ratios and number of\natoms to use, hence the name special. For many years, finding SQS structures required exponentially\ncomplex enumeration of all choices and was limited to simple cases until another critical work by Walle\net al. [129], which used simulated annealing Monte Carlo implemented through ATAT software to find\nthese special cases much faster, exemplified through the relatively complex \u03c3-phase and enabling the\ncreation of SQS libraries used in thermodynamic modeling [130].\nHowever, the direct use of an SQS may not be the optimal choice for structure-informed random\nsolid solution featurization due to several reasons. Firstly, as discussed by Walle et al. [129], SQS\ncan be expected to perform well on purely fundamental grounds for certain properties like total energy\ncalculations, but one has to treat them with caution because different properties will depend differently\non the correlation and selecting the SQS may be suboptimal. Building up on that, one could, for\ninstance, imagine a property that depends strongly on the existence of low-frequency, high-correlation\nregions catalyzing a surface reaction or enabling nucleation of a dislocation. In terms of ML modeling,\nthis notion is taken to the extreme with calculated features being both very diverse and numerous while\nbeing expected to be universal surrogates for such mechanistically complex properties.\nSecondly, SQSs that can be generated in a reasonable time are limited in terms of the number of atoms\nconsidered, causing quantization of the composition. This is not an issue if a common grid of results is\nneeded, e.g., to fit CALPHAD model parameters [130] or to train a single-purpose ML model [131], but\nit becomes a critical issue if one needs to accept an arbitrary composition as the ML model and SQS\nwould have to be obtained every time. This issue is further amplified by the rapidly growing field of\ncompositionally complex materials (CCMs), which exist in vast many-component compositional spaces\nprohibiting SQS reuse even at coarse quantizations [132] while being a popular deployment target for\nboth forward and inverse artificial intelligence methods [133]\u2013[135] due to their inherent complexity.\nBased on the above, it becomes clear that costly computing of an SQS structure would have to be\ndone for every ML model, and it would not be consistent between chemistries and complexities. At the\nsame time, the primary motivation for limiting the number of sites for ab initio calculations is gone since\nKS2022 can featurize over 1,000 sites per second on a laptop (Apple M2 Max run in parallel).\nThus, the objective of optimization is shifted towards consistency in convergence to feature vector values\nat infinity. To accomplish that, pySIPFENN goes back to random sampling but at a large scale and\nindividually monitoring the convergence of every feature during the expansion procedure, implemented\nthrough KS2022_randomSolutions and depicted in Figure 3.4, to ensure individual convergence."}, {"title": "Summary and Conclusions", "content": "pySIPFENN or python toolset for Structure-Informed Property and Feature Engineering with Neu-\nral Networks is a free open-source software (FOSS) modular framework extending authors\u2019 past\nwork [9] described in Section 3.1 by including many key improvements in the structure-informed\nfeaturization, machine learning model deployment, different types of transfer learning (connected\nto OPTIMADE API [99]), rewrite of key literature tools (e.g., Ward2017 Java-based featurizer\n[76]) into Python+NumPy [93], and optimizations of past feature set as described in Sections 3.2.1,\n3.2.2, and 3.2.3.\npySIPFENN framework is uniquely built from tightly integrated yet highly independent modules\nto allow easy use of essential functions without limiting advanced researchers from taking specific\ncomponents they need, like a specific featurizer, and simply copying it into their software, reducing\ndependencies to the minimum (including pySIPFENN itself).\nSection 3.3 discusses how featurization of atomic structures (or configurations) to construct vector,\nvoxel, graph, graphlet, and other representations is typically performed inefficiently because of\nredundant calculations and how their efficiency could be improved by considering fundamentals of\ncrystallographic (orbits) equivalency to increase throughout of literature machine learning model,\ntypically between 2 to 10 times. Critically, this optimization applies to 98.75% of 4.4 million\nstored in MPDD [110], which includes both DFT-based [10], [11], [13]\u2013[15], [59], [82], [84] and\nexperimental [85]\u2013[87] data, showing massive impact if deployed. KS2022 featurizer implements\nthese advances in pySIPFENN using spglib [118] and Voro++ , while retaining ability\nto process arbitrary structures.\nSection 3.4 explores how symmetry is broken in dilute, doped, and defect structures, to then\ndiscuss site equivalency under different representations and how this notion can be used to improve\nefficiency by skipping redundant calculations of sites which are not guaranteed to be equivalent\nbased on crystallographic symmetry alone but need to be contrasted with defect-free representation.\nKS2022_dilute featurizer implements these advances in pySIPFENN.\nSection 3.5 discusses featurization of perfectly random configuration of atoms occupying an arbi-\ntrary atomic structure and, for the first time, considers fundamental challenges with using SQS\napproach in the context of forward and inverse machine learning model deployment by extending\npast discussion on SQS limitations given by Walle et al. [129], which do not typically appear in ab\ninitio and thermodynamic studies. KS2022_randomSolutions featurizer has been developed to\nefficiently featurize solid solutions of any compositional complexity by expanding the local chemical\nenvironments (LCEs) ensemble until standardized convergence criteria are met."}, {"title": "Introduction", "content": "Traditionally, the field of materials science deals with highly complex data, in terms of both input and\noutput descriptions, acquired through laborious experiments in the laboratories, such as mechanical\nor electrochemical tests performed according to professional standards or expensive computations on\nsupercomputer clusters, including ones based on the density functional theory (DFT) [137] or finite\nelement method (FEM) [138]. Because of this high-cost aspect, the number of data points being\ngenerated is usually very limited, especially after raw data is fitted into models and disseminated through\nreports and scientific publications as typeset tables, text, or figures, which are then carefully interpreted\nby other researchers.\nIn the last few decades, however, the community has become increasingly engaged in large-scale efforts\nto construct extensive collections of data points, especially after endorsement and special funding was\nprovided by the Materials Genome Initiative in 2011 [89], [139], [140]. It significantly accelerated many\nefforts to combine materials-specific scientific literature sources into a homogeneous structure were\nattempted, with perhaps the most successful being the Pauling File developed since 1995 with as\nof 2024, was built with 1000 years of full-time academic effort [141] and underlies several databases,\nincluding ASM Phase Diagram Database [142] and Materials Platform for Data Science (MPDS) [141]."}, {"title": "The Material-Property-Descriptor Database", "content": "The Material-Property-Descriptor Database (MPDD) is an extensive (4.5M+) database of ab initio\nrelaxations of 3D crystal structures, combined with an infrastructure of tools allowing efficient descriptor\ncalculation (featurization), as well as the deployment of ML models like SIPFENN [75] described in\nChapter 2, and other developed by the community including ALIGNN [78] and CHGNet [79].\nThe most critical motivation behind MPDD is the retention of intermediate modeling data (atomistic\nfeatures), including structure-informed descriptors described in Chapter 3, which typically cost orders\nof magnitude more computational time than any of the other steps performed during ML model de-\nployment [9]. Thus, many ML models can be run at a small fraction of the original cost if the same\ndescriptor (or, more commonly, a subset chosen through feature selection) is used. This benefit applies\nregardless of whether a model is just another iteration, e.g., fine-tuned to a specific class of materials\nlike perovskites, or an entirely new model for a different property. Thanks to this, machine learning\nresearchers can effortlessly take advantage of MPDD to deploy numerous ML models directly to the\ncommunity without needing to construct individual deployment targets, which are typically both much\nsmaller and redundant relative to existing datasets.\nFurthermore, MPDD\u2019s access to stored atomic structures and associated metadata has been shown\nto be useful, for instance, in the fully data-driven prediction of atomic structures (validated with DFT\nand experiments). It, for instance allowed quick identification of unknown structures in Nd-Bi [35] and\nAl-Fe [36] systems, what is discussed in more detail in Chapter 5."}, {"title": "Open Databases Integration for Materials Design (OPTIMADE) API\nof MPDD", "content": "OPTIMADE or the Open Databases Integration for Materials Design consortium has been established to\nmake materials databases interoperable by developing a specification for a ubiquitous REST API [99].\nThat way, databases can remain independently maintained and tuned to specific needs, like ab initio data\nor ML data, while at the same time reporting on the contained knowledge so that redundant calculations\nare not performed, and the efficiency of the materials informatics community at large is dramatically\nimproved.\nMPDD has a stable OPTIMADE API that serves the entire core MPDD dataset, fully implementing\nv1.1.0 of the OPTIMADE standard, as of April 2024, through a cloud server based on optimade-\npython-tools [153]. Making the MPDD available via OPTIMADE was initially challenging, as MPDD\nstores and exchanges data in a way that prioritises high throughput and low storage requirements,\nincluding binary data, making it difficult or slow to make MPDD queryable as an OPTIMADE API on-\nthe-fly. However, issues have been resolved by establishing a self-updating mirror of the dataset where\nstructures are made OPTIMADE-compliant during transfer, which can occur within the same virtual\nmachine or other integrated computing environment. Most of the MPDD-specific data is available under\nthe mpdd namespace of OPTIMADE, including dictionaries of metadata, properties, and descrip-\ntors, described earlier in this chapter.\nThe base URL is available at:\nhttps://optimade.mpdd.org\nand one can see a sample dataset response by following the structures endpoint at:\nhttps://optimade.mpdd.org/v1/structures\nFurther discussion on MPDD\u2019s OPTIMADE API is given in Appendix Section A.4, with Figures A.10 and\nA.11 showing outputs of, respectively, base URL and structures endpoint being queried."}, {"title": "MPDD-eXchange", "content": "In addition to the OPTIMADE API serving as the primary endpoint for the end-users"}, {"title": "EFFICIENT MATERIALS INFORMATICS BETWEEN\nROCKETS AND ELECTRONS", "authors": ["Adam M. Krajewski"], "abstract": "The true power of computational research typically can lay in either what it accomplishes or what it\nenables others to accomplish. In this work, both avenues are simultaneously embraced across several\ndistinct efforts existing at three general scales of abstractions of what a material is - atomistic, physical,\nand design. At each, an efficient materials informatics infrastructure is being built from the ground\nup based on (1) the fundamental understanding of the underlying prior knowledge, including the data,\n(2) deployment routes that take advantage of it, and (3) pathways to extend it in an autonomous or\nsemi-autonomous fashion, while heavily relying on artificial intelligence (AI) to guide well-established\nDFT-based ab initio and CALPHAD-based thermodynamic methods.\n The resulting multi-level discovery infrastructure is highly generalizable as it focuses on encoding prob-\nlems to solve them easily rather than looking for an existing solution. To showcase it, this dissertation\ndiscusses the design of multi-alloy functionally graded materials (FGMs) incorporating ultra-high tem-\nperature refractory high entropy alloys (RHEAs) towards gas turbine and jet engine efficiency increase\nreducing CO2 emissions, as well as hypersonic vehicles. It leverages a new graph representation of un-\nderlying mathematical space using a newly developed algorithm based on combinatorics, not subject to\nmany problems troubling the community. Underneath, property models and phase relations are learned\nfrom optimized samplings of the largest and highest quality dataset of HEA in the world, called ULTERA.\nAt the atomistic level, a data ecosystem optimized for machine learning (ML) from over 4.5 million re-\nlaxed structures, called MPDD, is used to inform experimental observations and improve thermodynamic\nmodels by providing stability data enabled by a new efficient featurization framework.", "sections": [{"title": "Introduction", "content": "The discovery of novel materials that solve societal challenges or otherwise improve human lives is\narguably one of the most critical components of building a modern world. Starting from the bronze age,\nhumans were able to reliably combine raw materials in a structured fashion to achieve desired results,\neven though, at the time, there was no mechanistic understanding of why things happen. This has\nchanged with gradual introduction of the scientific method, which standardized and systematized the\ndiscovery approach, with revolutionary advancements in materials happening every time a new technology\nfor sharing and combining knowledge, such as propagation of the Greek language, printing press, or\ncomputer aided design (CAD), has been introduced and widely adopted.\nIn the current world, which went through the Internet revolution around 2000 and is currently going\nthrough the artificial intelligence (AI) revolution of the 2020s, one can point to the informatiztion of ma-\nterials science as one such communication technology with a potential to revolutionize materials discovery\nby combining vast amounts of multidimensional data, intricate multidisciplinary domain knowledge, and\nability to guide experiments beyond level achievable by a human being. In order to achieve this, one has\nto consider how to combine these efficiently, mitigating problems such as inhomogenities between data\nsources, computational challenges related to vast design spaces, hidden uncertainties in the reported\nvalues, and many flavors of errors, unavoidably present in the complex datasets involved.\nWhile creating an efficient, high-performance, fully general ecosystem for materials informatics appears\nnearly impossible even for a large group of researchers, a case-specific approach can be constructed in a\nfashion prioritizing generalizability, which can then be adjusted to other problems. This Dissertation builds\nsuch a case-specific approach, embedding a more general blueprint through the development of methods\nthat are rarely limited to the target application but are rather biased towards it through design choices,\nassumptions, and helpful simplifications. In the process, it introduces several novel individual pieces of\nsoftware, including pySIPFENN, MPDD, crystALL, ULTERA, PyQAlloy, nimCSO, nimplex, and their\nderivatives to collectively bridge ab-initio methods, belonging to the domain of quantum physics, with\nengineering of devices placed in extreme environments, such as gas engine turbine blades or hypersonic\nvehicles, designed by aerospace engineers, through efficient materials informatics pipelines existing across\nscales, as summarized in the Figure 1.1 and described in detail in Section 1.2."}, {"title": "Flow of Material Discovery and This Work", "content": "Throughout this work, all topics raised in Section 1.1 will be discussed in a reversed order to progressively\nbuild from fundamentals to highly-specialized final applications, while retaining generality at every stage.\nThis way, one will be able to build a holistic picture focused on how data flows within materials informatics\nresearch and converges together at consecutive length-scales to discover new materials in specific niches\nin a systematic, easy-to-automate approach, rather than build elaborate solutions that may happen to\nwork well but can also break the fundamentals a common occurrence in our era of powerful computing\nand machine learning where tools always give an answer but it may hold negative value.\nAs shown in Figure 1.2, the first 4 chapters (colored blue) cover atomistic treatment of materials,\ndiscussing how data at this level is collected, featurized, managed and expanded. SIPFENN approach\nand the latest pySIPFENN featurization package are first developed. Then MPDD database with 4.5\nmillion DFT-relaxed or experimentally observed entries is set up to serve as a highly efficient deployment\nvehicle. Lastly, crystALL approach automatically extends it into chemical areas of interest.\nAll of MPDD is then harvested to model materials at the physical scale by (1) serving as inputs to ther-\nmodynamic model generation using pycalphad [6] and ESPEI [7] or training of pySIPFENN ML models\ngenerating needed data, and (2) informing experimental observations by, for instance, automatically com-\npiling a set of carbides stable in an alloy system at OK. At the same time, the largest experimental HEA\ndata infrastructure, called ULTERA, is compiled joining together over 6,800 property datapoints manually\nextracted from 555 literature publications.\nThe experimental database is curated through novel PyQAlloy package created to detect abnormalities\nand dramatically reduce fraction of erroneous data relative to other similar ones in the literature. Once\ncurated, the nimCSO package can guide ML efforts in terms of which components of the data (chemical\nelements) should be considered when modeling to optimize trade-off between applicability and data\ndensity available to the models. Lastly, compositional space representations generated through nimplex\nand inverse design workflows serve as deployment vehicles for the trained methods."}, {"title": "Executive Summary", "content": "First, Chapter 2 on Extensible Structure-Informed Prediction of Formation Energy with Improved Ac-\ncuracy and Usability employing Neural Networks introduces fundamental concepts critical to structure-\ninformed modeling of atomic configurations from the perspective of machine learning (ML) modeling\nand presents design of such models employing artificial neural networks for the prediction of formation\nenergies of atomic structures based on elemental and structural features of Voronoi-tessellated materials.\nIt provide a concise overview of the connection between the machine learning and the true material-\nproperty relationship, how to improve the generalization accuracy by reducing overfitting, how new data\ncan be incorporated into the model to tune it to a specific material system, and preliminary results on\nusing models to preform local structure relaxations.\nIt results in three final models optimized for achieving (1) highest test accuracy on the Open Quantum\nMaterials Database (OQMD), (2) high extrapolative performance in the discovery of new materials,\nand (3) high performance at a low computational cost. On a test set of 21,800 compounds randomly\nselected from OQMD, these models achieves a mean absolute error (MAE) of 28, 40, and 42 meV/atom,\nrespectively. The first model represented the state-of-the-art performance on this problem when released\nin 2020 [8] (see Table 2.2), the second model provides better predictions in a test case of interest not\npresent in the OQMD, while the third one reduces the computational cost by a factor of 8 making it\napplicable to embedded and mobile applications. A transfer learning procedure was also demonstrated for\nthe first time, showing dramatic improvements in extrapolation with just several datapoints (see Figure\n2.10).\nThe results were implemented into a new open-source tool called SIPFENN or Structure-Informed\nPrediction of Formation Energy using Neural Networks, which not only improved the accuracy beyond\nexisting models but also shipped in a ready-to-use form with pre-trained neural networks, which was\nfirst-of-a-kind at release, and a GUI interface allowing it be included in DFT calculations routines at\nnearly no cost.\nNext, Chapter 3 on Efficient Structure-Informed Featurization and Property Prediction of Ordered,\nDilute, and Random Atomic Structures expands upon SIPFENN and implements a fully-featured machine\nlearning focused analysis framework called pySIPFENN or python toolset for Structure-Informed Property\nand Feature Engineering with Neural Networks to fuel needs of structure-informed materials informatics\n- a rapidly evolving discipline of materials science relying on the featurization of atomic structures or"}, {"title": "Software and Data Availability", "content": "As of 2022, the SIPFENN code has been deprecated in favor of highly improved pySIPFENN which was\nre-written with great stability, reliability, and deeper integration with other community tools. Chapter\n3 describes it in detail and Section 3.7 describes its availability along related workshops and tutorial\nmaterials.\nFor archival purposes, the last version of SIPFENN code is available through Penn State's Phases\nResearch Lab website at https://phaseslab.org/sipfenn in (1) a minimal version that can be run on pre-\ncomputed descriptors in CSV format as well as (2) ready-to-use version with pre-compiled Magpie [39].\nSIPFENN contains hard-coded links to neural networks stored in the cloud that can be downloaded at a\nsingle-click (see Figure 2.13).\nAll neural networks were made available in (1) open-source MXNet format maintained by Apache Foun-\ndation, used within SIPFENN, (2) closed-source WLNet format maintained by Wolfram Research and\nhaving the advantage of even easier deployment, as well as guaranteed forward compatibility with future\nversions of Wolfram Language, and in (3) Open Neural Network Exchange (ONNX) format [58] distributed\nthrough pySIPFENN, as of April 2024.\nFor ensured longevity of results, original SIPFENN neural networks are stored through the courtesy of\nZenodo.org service under DOI: 10.5281/zenodo.4006803 at the CERN's Data Centre."}, {"title": "Introduction", "content": "SIPFENN or Structure-Informed Prediction of Formation Energy using Neural Networks software, first\nintroduced by the authors in 2020 [8], [9] and described extensively in Chapter 2, is one of several open-\nsource tools available in the literature [48], [76]\u2013[81] which train machine learning (ML) models on the\ndata from large Density Functional Theory (DFT) based datasets like OQMD [10], [11], [59], AFLOW [14],\n[15], Materials Project [13], NIST-JARVIS[82], Alexandria [83], or GNOME [84] to predict formation\nenergies of arbitrary atomic structures, with accuracy high enough to act as a low-cost surrogate in the\nprediction of thermodynamic stability of ground and non-ground state configurations at OK tempera-\nture. The low runtime cost allows such models to efficiently screen through millions of different atomic\nstructures of interest on a personal machine in a reasonable time.\nIn addition to high-accuracy neural network models trained on OQMD [10], [11], [59], SIPFENN included\na number of features not found in competing tools available at the time, such as the ability to quickly\nreadjust models to a new chemical system based on just a few DFT data points through transfer learning\nand a selection of models optimized for different objectives like extrapolation to new materials instead\nof overfitting to high-data-density regions or low memory footprint [9]."}, {"title": "General Structure Featurization Improvements", "content": "Being able to predict the thermodynamic stability of arbitrary atomic structures and their modifications is\none of the most critical steps in establishing whether hypothetical candidates can be made in real life [90];\nhowever, it is certainly not the only task of interest to the community [91], [92]. These diverse needs,\ncombined with increasing interest in multi-property modeling, have shifted the focus of SIPFENN tool\nfrom model training [9] toward the development of reliable, easy-to-use, and efficient general-purpose\nfeaturizers existing in a framework, which can be used by researchers and companies to quickly develop\nand deploy property-specific models, or use features directly in exploring similarity and trends in materials.\nThus, while the acronym has been retained, the name of the software has been changed to python\ntoolset for Structure-Informed Property and Feature Engineering with Neural Networks or pySIPFENN,\nand the software component has been carefully re-implemented in its entirety to make it as general as\npossible and enable the following core advantages:\n1. Reliable featurization, which can be immediately transferred to other tools thanks to standalone\nsubmodule implementations based only on two common libraries (NumPy [93] and pymatgen [74]).\nThese include completely re-written Ward2017 Java-based featurizer [76] and\n3 new ones, described in Sections 3.3, 3.4, and 3.5.\n2. Effortless plug-and-play deployment of neural network (and other) ML models (for any property)\nutilizing any of the defined feature vectors, enabled by the use of Open Neural Network Exchange\n(ONNX) open-source format [58] which can be exported from nearly every modern ML framework\nand is then loaded into pySIPFENN's PyTorch backend [94] through onnx2torch [95]. Fur-\nthermore, implementing custom predictors, beyond those supported by PyTorch, is made easy by\ndesign.\n3. Dedicated ModelExporters submodule makes it easy to export trained models for publication\nor deployment on a different device while also enabling weight quantization and model graph\noptimizations to reduce memory requirements.\n4. The ability to acquire data and adjust or completely retrain model weights through automated\nModelAdjusters submodule. Its applications include:\n(a) Fine-tuning models based on additional local data to facilitate transfer learning ML schemes\nof the domain adaptation kind [96], where a model can be adjusted to new chemistry and\nspecific calculation settings, introduced by SIPFENN back in 2020 [9], which is also being\nadopted by other models like ALIGNN [97]. Such an approach can also be used iteratively in\nactive learning schemes where new data is obtained and added.\n(b) Tuning or retraining of the models based on other atomistic databases, or their subsets,\naccessed through OPTIMADE [98], [99] to adjust the model to a different domain, which in\nthe context of DFT datasets could mean adjusting the model to predict properties with DFT\nsettings used by that database or focusing its attention to specific chemistry like, for instance,\nall compounds of Sn and all perovskites.\n(c) Knowledge transfer learning [100] to adjust models to entirely new, often less available prop-\nerties while harvesting the existing pattern recognition.\nThe resulting pySIPFENN computational framework is composed of several components, as depicted\nin Figure 3.1, and is available through several means described in Section 3.7, alongside high-quality\ndocumentation and examples."}, {"title": "Ward2017 Reimplementation", "content": "In their 2017 work Ward et al. [76] introduced a novel atomic structure featurization concept based\non establishing and weighting neighbor interactions by faces from 3D Voronoi tesselation to describe\nlocal chemical environments (LCEs) of atomic sites and then performing statistics over them to obtain a\nglobal feature vector. The original SIPFENN models [8] built on top of this while utilizing an improved,\ncarefully designed deep neural network models to obtain up to 2.5 times lower prediction error on the\nsame dataset [9]. A detailed description of the descriptor can be found in Section 2.1 of Krajewski et al.\n[9]. In general, the calculation of the Ward2017 descriptor consists of three parts:\nCalculation of attributes based upon global averages over the components of the structure.\nCalculation of attributes based upon local neighborhood averages for each site in the structure.\nCalculation of more complex attributes based upon averages over paths in the structure.\nWard et al. [76] implemented the above calculations in Java, which was popular at the time; while\nmost of the current machine-learning packages use almost exclusively Python (e.g., scikit-learn\n[101] and PyTorch [94]), making it cumbersome to use Java. Even more critically, the original Java\nimplementation was not computationally efficient (as explored in Sections 3.3, 3.4, and 3.5), and enabling\ntools were not supported in Java.\nIn the present work, authors have reimplemented Ward et al. [76] from scratch in Python as a standalone\nsubmodule for pySIPFENN, which calculates all 271 features within numerical precision, except for three\nperforming a random walk on the structure, which is stochastic in nature and results in slightly different\nfinal values due to a different seed. The Voronoi tessellation has been implemented with Voro++\n[102]\u2013[104] and all numerical operations were written using NumPy [93] arrays to greatly speed up the\ncalculations and make the efficient utilization of different computing resources, such as GPUs, easy to\nimplement."}, {"title": "KS2022 Feature Optimization", "content": "Typically, during feature engineering, researchers first attempt to collect all features expected to enable\ngood inference and then remove some based on the interplay of several factors:\n1. Low impact on the performance, which increases the representation memory requirements and\npossibly increases the risk of overfitting to both systematic and random trends.\n2. High computational cost, which limits the throughput of the method deployment.\n3. Unphysical features or feature representations which can improve model performance against\nwell-behaving benchmarks covering a small subset of the problem domain but compromise model\ninterpretability and extrapolation ability in unpredictable ways."}, {"title": "Optimizations for Ordered Structures", "content": "Modeling of disordered materials is a critical area of research [107]; however, the vast majority of atomistic\nab initio datasets used for ML studies focuses on highly symmetric ordered structures because of their\nhigh availability and ability to model complex phenomena in a holistic fashion if local ergodicity can\nbe established [108], [109]. One evidence of the focus on such data is the fact that out of 4.4 million\natomic structures in MPDD [110], which includes both DFT-based [10], [11], [13]\u2013[15], [59], [82], [84]\nand experimental [85]\u2013[87] data, only 54 thousand or 1.25% lack any symmetry. It is also worth noting\nthat this number used to be much lower before the recent publication of the GNOME dataset by Google\nDeepMind [84], which accounts for around 4 of them."}, {"title": "Optimizations for Dilute, Defect, and Doped Structures", "content": "The optimization strategy in Section 3.3 ensures that only the sites that are guaranteed to be crystallo-\ngraphically unique are processed through featurization or graph convolution and is directly applicable to\nthe vast majority of both data points and literature methods. However, in the case of methods relying on\ndescribing the immediate neighbors, whether through Wigner-Seitz cell or subgraph (see,e.g., [77]), one can achieve further efficiency improvements by considering which sites are guaranteed to\nbe unique under the representation.\nThere are several classes of atomic structures where the distinction above makes a difference, but\nthe room to improve is exceptionally high when one site in an otherwise highly symmetric structure is\nmodified, leading to a structure that, depending on the context, will be typically called dilute when\ndiscussing alloys [125], doped when discussing electronic materials [126], or said to have defect in a\nmore general sense [127]. Throughout pySIPFENN\u2019s codebase and the rest of this work, the single term\ndilute is used to refer to all of such structures due to authors\u2019 research focus on Ni-based superalloys at\nthe time when optimizations below were made public in February 2023.\nTo visualize the concept, one can consider, for instance, a 3x3x3 body-centered cubic (BCC) conven-\ntional supercell (54 sites) and call it base structure. If it only contains a single specie, then KS2022\nfrom Section 3.3 will recognize that there is only one crystallographic orbit and only process that one.\nHowever, if a substitution is made at any of the 54 equivalent sites, the space group will change from\nIm-3m (229) to Pm-3m (221), with 8 crystallographic orbits on 7 Wyckoff positions; thus, the default\nKS2022 featurizer will process 8 sites. At the same time, several of these crystallographic orbits will be\ndifferentiated only by the orientation and distance to the dilute (substitution) site, which does affect\nab initio calculation results (e.g., vacancy formation energy vs supercell size [128]), but is guaranteed\nto have no effect on the model\u2019s representation because of the exact same neighborhood configuration\n(including angles and bond lengths) if conditions given earlier are met. Thus, it only requires adjustments\nto the site multiplicities or convolution implementation (simplified through, e.g., a Markov chain). In the\nparticular dilute-BCC example at hand, depicted in Figure 3.3, there are 4 such representation-unique\ncrystallographic orbits, i.e., 1 with the dilute atom, 2 neighboring the dilute atom sharing either large\nhexagonal (1st nearest neighbor shell) or small square face (2nd nearest neighbor shell), and 1 non af-\nfected by the dilute atom which is equivalent to the remaining 4 orbits; thus reducing number of sites\nthat need to be considered by a factor of 2.\nThe KS2022_dilute featurization routine, schematically depicted in Figure 3.3, conveniently auto-\nmates the above process for both simple cases like aforementioned substitution in pure element and\ncomplex cases like introducing a dilute atom at the 2a/2b orbit in \u03c3-phase, by\nperforming independent identification of crystallographic orbits in the dilute structure and base struc-\nture, followed by identification of the dilute site and its configuration to establish orbit equivalency\nunder pySIPFENN\u2019s KS2022 representation, to finally reconstruct complete site ensemble of the dilute\nstructure.\nIn the case of KS2022_dilute implementation run on the dilute BCC supercell shown in Figure\n3.3, the efficiency is improved nearly proportionally to the reduction in the number of considered sites,"}, {"title": "Optimizations for Random Solid Solutions", "content": "Sections 3.3 and 3.4 have demonstrated how recognition of symmetry in ordered structures can guarantee\nequivalency of sites and how understanding the character of featurization can further extend that notion\nof equivalency so that the ML representations of all sites can be obtained efficiently up to an order of\nmagnitude faster. Random solid solutions are the conceptually opposite class of atomic structures, where\nthe lack of symmetry or site equivalency is guaranteed, yet featurizing them requires one to solve the\nsame problem of efficiently obtaining the ML representations of all sites present, which also happen to\nbe infinite.\nTypically, in the ab initio community, random solid solutions are represented using Special Quasirandom\nStructures (SQS) introduced in landmark 1990 work by Zunger et al. [63], which are the best structures\nto match neighborhood correlations in a purely random state given component ratios and number of\natoms to use, hence the name special. For many years, finding SQS structures required exponentially\ncomplex enumeration of all choices and was limited to simple cases until another critical work by Walle\net al. [129], which used simulated annealing Monte Carlo implemented through ATAT software to find\nthese special cases much faster, exemplified through the relatively complex \u03c3-phase and enabling the\ncreation of SQS libraries used in thermodynamic modeling [130].\nHowever, the direct use of an SQS may not be the optimal choice for structure-informed random\nsolid solution featurization due to several reasons. Firstly, as discussed by Walle et al. [129], SQS\ncan be expected to perform well on purely fundamental grounds for certain properties like total energy\ncalculations, but one has to treat them with caution because different properties will depend differently\non the correlation and selecting the SQS may be suboptimal. Building up on that, one could, for\ninstance, imagine a property that depends strongly on the existence of low-frequency, high-correlation\nregions catalyzing a surface reaction or enabling nucleation of a dislocation. In terms of ML modeling,\nthis notion is taken to the extreme with calculated features being both very diverse and numerous while\nbeing expected to be universal surrogates for such mechanistically complex properties.\nSecondly, SQSs that can be generated in a reasonable time are limited in terms of the number of atoms\nconsidered, causing quantization of the composition. This is not an issue if a common grid of results is\nneeded, e.g., to fit CALPHAD model parameters [130] or to train a single-purpose ML model [131], but\nit becomes a critical issue if one needs to accept an arbitrary composition as the ML model and SQS\nwould have to be obtained every time. This issue is further amplified by the rapidly growing field of\ncompositionally complex materials (CCMs), which exist in vast many-component compositional spaces\nprohibiting SQS reuse even at coarse quantizations [132] while being a popular deployment target for\nboth forward and inverse artificial intelligence methods [133]\u2013[135] due to their inherent complexity.\nBased on the above, it becomes clear that costly computing of an SQS structure would have to be\ndone for every ML model, and it would not be consistent between chemistries and complexities. At the\nsame time, the primary motivation for limiting the number of sites for ab initio calculations is gone since\nKS2022 can featurize over 1,000 sites per second on a laptop (Apple M2 Max run in parallel).\nThus, the objective of optimization is shifted towards consistency in convergence to feature vector values\nat infinity. To accomplish that, pySIPFENN goes back to random sampling but at a large scale and\nindividually monitoring the convergence of every feature during the expansion procedure, implemented\nthrough KS2022_randomSolutions and depicted in Figure 3.4, to ensure individual convergence."}, {"title": "Summary and Conclusions", "content": "pySIPFENN or python toolset for Structure-Informed Property and Feature Engineering with Neu-\nral Networks is a free open-source software (FOSS) modular framework extending authors\u2019 past\nwork [9] described in Section 3.1 by including many key improvements in the structure-informed\nfeaturization, machine learning model deployment, different types of transfer learning connected\nto OPTIMADE API [99]), rewrite of key literature tools and\noptimizations of past feature set as described in Sections 3.2.1,\n3.2.2, and 3.2.3.\npySIPFENN framework is uniquely built from tightly integrated yet highly independent modules\nto allow easy use of essential functions without limiting advanced researchers from taking specific\ncomponents they need, like a specific featurizer, and simply copying it into their software, reducing\ndependencies to the minimum (including pySIPFENN itself).\nSection 3.3 discusses how featurization of atomic structures (or configurations) to construct vector,\nvoxel, graph, graphlet, and other representations is typically performed inefficiently because of\nredundant calculations and how their efficiency could be improved by considering fundamentals of\ncrystallographic (orbits) equivalency to increase throughout of literature machine learning model,\ntypically between 2 to 10 times. Critically, this optimization applies to 98.75% of 4.4 million\nstored in MPDD [110], which includes both DFT-based [10], [11], [13]\u2013[15], [59], [82], [84] and\nexperimental data, showing massive impact if deployed. KS2022 featurizer implements\nthese advances in pySIPFENN using spglib and Voro++, while retaining ability\nto process arbitrary structures.\nSection 3.4 explores how symmetry is broken in dilute, doped, and defect structures, to then\ndiscuss site equivalency under different representations and how this notion can be used to improve\nefficiency by skipping redundant calculations of sites which are not guaranteed to be equivalent\nbased on crystallographic symmetry alone but need to be contrasted with defect-free representation.\nKS2022_dilute featurizer implements these advances in pySIPFENN.\nSection 3.5 discusses featurization of perfectly random configuration of atoms occupying an arbi-\ntrary atomic structure and, for the first time, considers fundamental challenges with using SQS\napproach in the context of forward and inverse machine learning model deployment by extending\npast discussion on SQS limitations given by Walle et al. [129], which do not typically appear in ab\ninitio and thermodynamic studies. KS2022_randomSolutions featurizer has been developed to\nefficiently featurize solid solutions of any compositional complexity by expanding the local chemical\nenvironments (LCEs) ensemble until standardized convergence criteria are met."}, {"title": "Handling Millions of Atomic Structures", "content": "Traditionally, the field of materials science deals with highly complex data, in terms of both input and\noutput descriptions, acquired through laborious experiments in the laboratories, such as mechanical\nor electrochemical tests performed according to professional standards or expensive computations on\nsupercomputer clusters, including ones based on the density functional theory (DFT) [137] or finite\nelement method (FEM) [138]. Because of this high-cost aspect, the number of data points being\ngenerated is usually very limited, especially after raw data is fitted into models and disseminated through\nreports and scientific publications as typeset tables, text, or figures, which are then carefully interpreted\nby other researchers.\nIn the last few decades, however, the community has become increasingly engaged in large-scale efforts\nto construct extensive collections of data points, especially after endorsement and special funding was\nprovided by the Materials Genome Initiative in 2011 [89], [139], [140]. It significantly accelerated many\nefforts to combine materials-specific scientific literature sources into a homogeneous structure were\nattempted, with perhaps the most successful being the Pauling File developed since 1995 with as\nof 2024, was built with 1000 years of full-time academic effort [141] and underlies several databases,\nincluding ASM Phase Diagram Database [142] and Materials Platform for Data Science (MPDS) [141]."}, {"title": "The Material-Property-Descriptor Database", "content": "The Material-Property-Descriptor Database (MPDD) is an extensive (4.5M+) database of ab initio\nrelaxations of 3D crystal structures, combined with an infrastructure of tools allowing efficient descriptor\ncalculation (featurization), as well as the deployment of ML models like SIPFENN [75] described in\nChapter 2, and other developed by the community including ALIGNN [78] and CHGNet [79].\nThe most critical motivation behind MPDD is the retention of intermediate modeling data (atomistic\nfeatures), including structure-informed descriptors described in Chapter 3, which typically cost orders\nof magnitude more computational time than any of the other steps performed during ML model de-\nployment [9]. Thus, many ML models can be run at a small fraction of the original cost if the same\ndescriptor (or, more commonly, a subset chosen through feature selection) is used. This benefit applies\nregardless of whether a model is just another iteration, e.g., fine-tuned to a specific class of materials\nlike perovskites, or an entirely new model for a different property. Thanks to this, machine learning\nresearchers can effortlessly take advantage of MPDD to deploy numerous ML models directly to the\ncommunity without needing to construct individual deployment targets, which are typically both much\nsmaller and redundant relative to existing datasets.\nFurthermore, MPDD\u2019s access to stored atomic structures and associated metadata has been shown\nto be useful, for instance, in the fully data-driven prediction of atomic structures (validated with DFT\nand experiments). It, for instance allowed quick identification of unknown structures in Nd-Bi [35] and\nAl-Fe [36] systems, what is discussed in more detail in Chapter 5."}, {"title": "Open Databases Integration for Materials Design (OPTIMADE) API\nof MPDD", "content": "OPTIMADE or the Open Databases Integration for Materials Design consortium has been established to\nmake materials databases interoperable by developing a specification for a ubiquitous REST API [99].\nThat way, databases can remain independently maintained and tuned to specific needs, like ab initio data\nor ML data, while at the same time reporting on the contained knowledge so that redundant calculations\nare not performed, and the efficiency of the materials informatics community at large is dramatically\nimproved.\nMPDD has a stable OPTIMADE API that serves the entire core MPDD dataset, fully implementing\nv1.1.0 of the OPTIMADE standard, as of April 2024, through a cloud server based on optimade-\npython-tools [153]. Making the MPDD available via OPTIMADE was initially challenging, as MPDD\nstores and exchanges data in a way that prioritises high throughput and low storage requirements,\nincluding binary data, making it difficult or slow to make MPDD queryable as an OPTIMADE API on-\nthe-fly. However, issues have been resolved by establishing a self-updating mirror of the dataset where\nstructures are made OPTIMADE-compliant during transfer, which can occur within the same virtual\nmachine or other integrated computing environment. Most of the MPDD-specific data is available under\nthe mpdd namespace of OPTIMADE, including dictionaries of metadata (e.g., properties and descrip-\ntors, described earlier in this chapter."}, {"title": "MPDD-eXchange", "content": "In addition to the OPTIMADE API serving as the primary endpoint for the end-users, MPDD has recently\nbeen extended through an experimental platform available at contrib.mpdd.org based on GitHub cloud-\nbased automations to facilitate data exchange; thus, called MPDD-eXchange or MPDD-X. The central\nexchange happens when the user uploads their data, which gets validated and ingested into the data\necosystem, enriching the MPDD, while the user gets presented with machine learning predictions associated\nwith it and gets persistent credit associated with their account.\nFrom the user perspective, all work is being performed entirely within GitHub Issues of the MPDD-"}]}]}