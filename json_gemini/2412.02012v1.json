{"title": "INSIGHT: Explainable Weakly-Supervised Medical Image Analysis", "authors": ["Wenbo Zhang", "Junyu Chen", "Christopher Kanan"], "abstract": "Due to their large sizes, volumetric scans and whole-slide pathology images (WSIs) are often processed by extract-ing embeddings from local regions and then an aggregator makes predictions from this set. However, current methods require post-hoc visualization techniques (e.g., Grad-CAM) and often fail to localize small yet clinically crucial de-tails. To address these limitations, we introduce INSIGHT, a novel weakly-supervised aggregator that integrates heatmap generation as an inductive bias. Starting from pre-trained feature maps, INSIGHT employs a detection module with small convolutional kernels to capture fine details and a context module with a broader receptive field to suppress local false positives. The resulting internal heatmap high-lights diagnostically relevant regions. On CT and WSI benchmarks, INSIGHT achieves state-of-the-art classifica-tion results and high weakly-labeled semantic segmentation performance. Project website and code are available at: https://zhangdylan83.github.io/ewsmia/", "sections": [{"title": "1. Introduction", "content": "Advances in medical imaging technology have enabled clinicians to extract critical insights from increasingly large and complex datasets. However, the size of medical images presents computational and analytical challenges: whole-slide images (WSIs) of histopathology slides can contain billions of pixels [3], and volumetric scans, such as CT or MRI, are composed of hundreds of slices. Processing such data end-to-end with deep neural networks is computation-ally infeasible. Instead, pipelines rely on aggregators, which synthesize local embeddings extracted from tiles (WSIs) or slices (volumes) into global predictions [5, 6, 23]. While this divide-and-conquer strategy is efficient, current methods often discard spatial information during feature aggregation and depend on post-hoc visualization tools, such as Grad-CAM [33], to generate interpretable heatmaps. These visual-izations are prone to missing clinically significant features and introduce additional complexity.\nTo address these limitations, we propose INSIGHT (Integrated Network for Segmentation and Interpretation with Generalized Heatmap Transmission), a novel weakly-supervised aggregator that embeds interpretability directly into its architecture. INSIGHT processes pre-trained patch-or slice-level embeddings through two complementary mod-ules: a Detection Module, which captures fine-grained di-agnostic features, and a Context Module, which suppresses irrelevant activations by incorporating broader spatial in-formation. These outputs are fused to create interpretable internal heatmaps that naturally align with diagnostic re-gions. INSIGHT produces internal segmentation-quality heatmaps without requiring pixel- or voxel-level annotations as shown in Fig. 1."}, {"title": "2. Related Work", "content": "Weak supervision is essential in medical imaging, where the creation of detailed annotations is expensive and labor-intensive [3]. Fully supervised methods like U-Net [31] and DeepLab [7] rely on densely labeled data, which lim-its their scalability. In contrast, multiple instance learning (MIL) [3, 4] enables models to aggregate features from lo-cal regions using only image- or slide-level labels, avoiding the need for pixel-level annotations. This approach allows models to train on significantly larger datasets, as physicians typically diagnose patients without annotating individual im-age regions. For instance, the FDA-cleared Paige Prostate system demonstrates the clinical viability of weak super-vision by achieving high diagnostic accuracy for prostate cancer detection [3, 46]. Similarly, MIL-based methods have effectively aggregated patch- or slice-level features to make predictions on WSIs and volumetric scans [5, 6].\nHowever, current weakly supervised methods face crit-ical limitations. They require post-hoc visualization tech-niques, such as Grad-CAM [33] or class activation maps (CAM) [45], to produce interpretable saliency maps, adding complexity and often failing to localize small yet clinically essential details. These saliency maps can also be biased by dataset characteristics, such as organ co-occurrence, and often struggle to balance classification accuracy with spatial localization [10]. This leaves considerable room for improve-ment in designing aggregators that can better capture spatial dependencies and produce clinically meaningful outputs. IN-SIGHT addresses these challenges by directly embedding interpretability into its architecture, eliminating the reliance on post-hoc methods and bridging the gap between compu-tational efficiency and clinical relevance."}, {"title": "2.1. Weakly Supervised Medical Image Learning", "content": "Fully supervised methods such as U-Net [31] and DeepLab [7] have achieved high segmentation accuracy in medical imaging. However, their reliance on dense, pixel-or voxel-level annotations limits scalability in clinical work-flows [3]. Weakly supervised learning mitigates this issue by enabling models to train on coarser annotations, such as image- or slide-level labels, which are more practical in large-scale clinical datasets.\nMethods based on MIL have proven effective for WSIs and volumetric data, where detailed annotations are unavail-able. MIL aggregates features from local patches or slices to predict global outcomes, bypassing the need for pixel-level labels. For example, the FDA-cleared Paige Prostate sys-tem demonstrated the clinical viability of MIL by achieving high diagnostic accuracy for prostate cancer detection using only slide-level labels [3, 46]. MIL-based methods have also been successfully applied to breast cancer subtyping from WSIs [5] and chemotherapy response prediction from volumetric CT scans [6].\nWhile alternative weak labeling strategies, such as scrib-bles [19], bounding boxes [20], or point-level markers [16], reduce annotation demands, they still require explicit region annotations, which are rarely performed by physicians in clinical practice. INSIGHT builds on the MIL paradigm by utilizing only image-level labels for both classification and segmentation. Unlike existing methods, INSIGHT incorpo-rates interpretability directly into its architecture, generating fine-grained heatmaps that align with diagnostic regions without requiring post-hoc visualization techniques."}, {"title": "2.2. Aggregation Techniques in Medical Imaging", "content": "Aggregation techniques are essential for efficiently process-ing large-scale medical imaging data, such as WSIs and vol-umetric scans, without requiring dense annotations. In WSIs, MIL-based methods treat each slide as a collection of patches and aggregate features for global predictions. For instance, MIL has been successfully applied to cancer detection using slide-level labels [3], while advancements like STAMP [25] and tissue-graph approaches [28] have improved WSI analy-sis by refining aggregation techniques to detect patterns and segment tissues.\nIn volumetric imaging, slice-level features are aggre-gated to form volume-level predictions. For example, multi-resolution systems like [32] use DenseVNet-generated fea-tures to improve classification of chest CT scans, while [43] developed an aggregation framework with label correction to enhance outcome predictions.\nINSIGHT advances these techniques by preserving spa-tial resolution across slices or patches until the final pool-ing stage, enabling the generation of detailed, localized heatmaps. This addresses a key limitation of traditional aggregation methods, which often discard spatial informa-tion, reducing their ability to localize diagnostically relevant regions."}, {"title": "2.3. Explainable AI Methods in Medical Imaging", "content": "Explainable AI is crucial for ensuring model predictions in medical imaging are interpretable and clinically meaningful. Post-hoc methods such as class activation maps (CAM) [45] and Grad-CAM [33] are widely used to generate saliency maps that highlight regions associated with model predic-tions. For example, [42] developed high-resolution CAMs for thoracic CT scans, while [40] refined pseudo-labels for chest X-ray segmentation using CAMs.\nAttention mechanisms have further advanced inter-pretability. Self-attention, as introduced in [39], has been combined with Grad-CAM to localize disease-specific re-gions [44], while dual-attention mechanisms, such as DA-CMIL [9], produce interpretable maps for detecting condi-tions like COVID-19 and bacterial pneumonia.\nWhile these methods generate useful insights, they rely on post-hoc interpretability, which can misalign with the model's decision-making process. INSIGHT eliminates this reliance by embedding interpretability directly into its archi-tecture, producing calibrated, built-in heatmaps as part of its predictions. This ensures alignment between the model's outputs and diagnostic reasoning."}, {"title": "2.4. Pre-training in Medical Image Analysis", "content": "Pre-trained models have significantly advanced medical imaging by providing robust feature extractors. ResNet-based architectures, such as those used in [11, 17], effectively capture local and intermediate features in weakly supervised frameworks. However, CNNs face limitations in capturing long-range dependencies, which are critical for analyzing complex medical images.\nVision Transformers (ViTs) [12] address these limitations by leveraging self-attention to capture broader spatial rela-tionships. INSIGHT leverages pre-trained ViTs to extract tensor embeddings from WSI patches and radiology slices. By preserving spatial resolution, INSIGHT achieves robust performance across WSI and CT datasets, supporting fine-grained analysis and interpretability."}, {"title": "3. Method", "content": "Fig. 2 illustrates the architecture of INSIGHT, a weakly su-pervised framework for medical image analysis. INSIGHT processes pre-trained features from each slice in a volume or each patch in a WSI through dual modules\u2014Detection and Context\u2014to generate fine-grained heatmaps. These heatmaps are aggregated using a SmoothMax pooling strat-egy to produce categorical predictions, optimized using a combination of binary cross-entropy (BCE) and spectral de-coupling losses. Below, we describe each component in detail."}, {"title": "3.1. Feature Extraction", "content": "To preserve spatial resolution and capture both local and global contextual information, INSIGHT uses pre-trained models specific to the input modality. For CT scan volumes, we use a ViT pre-trained with DINOv2 [26] on ImageNet. For pathology WSIs, we use UNI [8], a DINOv2 model pre-trained on over 100 million WSIs. The extracted 2D feature maps are denoted as $F \\in \\mathbb{R}^{c \\times h \\times w}$, where c, h, and w are the feature depth, height, and width, respectively."}, {"title": "3.2. INSIGHT Architecture", "content": "Feature Adaptation. Pre-trained feature maps are trans-formed to domain-specific representations via a linear trans-formation layer:\n$F_1 = WF + b,                                                                                                                  (1)$"}, {"title": "Detection Module.", "content": "where $W \\in \\mathbb{R}^{c' \\times c}$ and $b \\in \\mathbb{R}^{c'}$ adapt the features to the target dataset. The transformed features $F_1 \\in \\mathbb{R}^{c' \\times h \\times w}$ are then fed into two parallel modules: Detection and Context.\n This module uses small-kernel convo-lutions to capture fine-grained details such as textures and edges, producing a heatmap $H_{Det}$ that highlights potential regions of interest:\n$H_{Det} = M_{Det}(F_t).                                                                                                                               (2)$"}, {"title": "Context Module.", "content": "To reduce false positives from the De-tection module, the Context module uses large-kernel con-volutions to capture broader spatial context and suppress irrelevant activations. Its output, $H_{Con}$, acts as a gating mechanism:\n$H_{Con} = M_{Con}(F_t).                                                                                                                               (3)$"}, {"title": "Heatmap Fusion.", "content": "The Detection and Context outputs are combined to generate a refined heatmap:\n$H = \\sigma((1 - \\sigma(H_{Con})) \\cdot H_{Det}),                                                                                                         (4)$\nwhere $\\sigma$ is the sigmoid activation, and $\\cdot$ denotes element-wise multiplication. The resulting heatmap H assigns rele-vance scores to regions, aligning with the target labels."}, {"title": "3.3. Pooling and Classification", "content": "To produce a categorical outcome, heatmaps are aggregated across all slices or patches using SmoothMax pooling [22], a Boltzmann-weighted operator that emphasizes regions with strong activations:\n$\\hat{y} = \\frac{\\sum_i H_i \\cdot exp(\\alpha \\cdot H_i)}{\\sum_i exp(\\alpha \\cdot H_i)},                                                                                                              (5)$\nwhere y is the predicted likelihood of a positive outcome, $H_i$ are heatmap activations, and $\\alpha$ controls the weighting. For multi-label classification, the heatmap is extended to multiple channels, one per category, enabling simultaneous optimization for all labels."}, {"title": "3.4. Training Objective", "content": "Binary Cross-Entropy with Label Smoothing. BCE loss with label smoothing improves generalization and prevents overconfidence in noisy or overlapping labels:\n$L_{BCE} = -\\frac{1}{N}\\sum_{i=1}^N [y_i log(\\hat{y}_i) + (1 - y_i)log(1 - \\hat{y}_i)],                                                                                 (6)$\nwhere $y'_i = (1 - \\epsilon)y_i + \\frac{\\epsilon}{2}$ is the smoothed label, $\\epsilon$ is the smoothing factor, and $\\hat{y}_i$ is the predicted probability.\nSpectral Decoupling. To address gradient starvation and encourage robust decision boundaries, we include spectral decoupling loss [29]:\n$L_{SD} = \\frac{ASD}{2} \\cdot ||z||_2^2,                                                                                                                               (7)$"}, {"title": "Total Loss.", "content": "where z are unnormalized logits, and $A_{SD}$ controls regular-ization strength. Spectral decoupling is one of the most effective methods for overcoming dataset bias without addi-tional sub-group labels [36, 37].\n The total loss combines these objectives:\n$L = L_{BCE} + L_{SD}.                                                                                                                                         (8)$"}, {"title": "3.5. Heatmap Visualization", "content": "Heatmaps are resized and reconstructed to match the original image dimensions. For WSIs, patches are stitched based on coordinates, while for CT volumes, slices are stacked se-quentially. Dynamic thresholding with Otsu's algorithm [27] isolates regions of interest:\n$H' = H \\cdot I(H > T),                                                                                                                                                (9)$\nwhere $T = arg \\underset{t}{min} \\sigma^2(t)$ minimizes intra-class variance."}, {"title": "4. Experiments", "content": "We evaluate INSIGHT on three publicly available datasets representing two imaging modalities: CT volumes and WSIs."}, {"title": "4.1. Datasets and Preprocessing", "content": "MosMed [24]. The MosMed dataset comprises 1, 110 chest CT studies for COVID-19 detection, split into two subsets: MosMed-A (1,060 volumes) and MosMed-B (50 volumes). MosMed-A is used for classification tasks, employing five-fold cross-validation with an 80/20 train-test split. MosMed-B, containing voxel-level annotations, is used solely for eval-uating segmentation performance by comparing INSIGHT-generated heatmaps H against ground truth using Dice score. CT volumes are normalized to the Hounsfield Unit (HU) range [-1000, 400], scaled to [0, 1], and resized to 518 \u00d7 518 x 32. To ensure compatibility with pre-trained ViTs, single-channel data is replicated across three channels. Lung parenchyma is isolated using Lungmask [14]."}, {"title": "4.1.2. Whole Slide Image (WSI) Datasets", "content": "CAMELYON16 [18]. This dataset contains 399 WSIs for detecting metastatic breast cancer in lymph nodes. We split the 269 training slides into 90/10 for training and validation. The model checkpoint with the highest validation Dice score is evaluated on the official test set of 129 slides.\nBRACS [1]. This dataset contains 547 WSIs annotated with 4, 539 bounding boxes for breast carcinoma subtype classification. Following BRACS's official splits, the final dataset comprises 347, 51, and 73 slides for training, valida-tion, and testing, respectively.\nBoth CAMELYON16 and BRACS use the CLAM tool-box [21] for patch extraction, preserving spatial embeddings for INSIGHT."}, {"title": "4.2. INSIGHT's Implementation Details", "content": "Architecture. INSIGHT uses ViT-L/DINOv2 for CT slices and UNI for WSI tiles. Pre-processed inputs are resized to 224 x 224, and embeddings from the last layer of the encoder are concatenated to retain spatial information. Di-mensionality is reduced from 1024 to 128 using a linear transformation. Detection and Context modules consist of three convolutional layers with GELU activations, layer nor-malization, and a final layer for label-specific outputs.\nTraining Configuration. INSIGHT is trained using AdamW with a learning rate of $10^{-4}$ and weight decay of $10^{-4}$. Training is capped at 50 epochs, with early stopping triggered after 8 epochs of no validation improvement. Hy-perparameters, including $\\alpha = 8$ for SmoothMax pooling, are tuned on validation sets. Label smoothing is applied for multi-label classification on BRACS, while Spectral Decou-pling is used for MosMed and CAMELYON16 to improve generalization."}, {"title": "4.3. Compared Baselines", "content": "We compare INSIGHT to several state-of-the-art methods across the MosMed, CAMELYON16, and BRACS datasets, evaluating both classification and segmentation performance.\nMosMed. For the MosMed dataset, we benchmark against methods specifically designed for COVID-19 CT analysis:\n\u2022 VIT-COVID Classifier [38]: A ViT pre-trained on a large-scale dataset of COVID-19 CT images, designed for binary classification.\n\u2022 Progressively Resized 3D-CNN [13]: A CNN that pro-gressively resizes input volumes to extract multi-scale information, enhancing analysis in volumetric CT images.\n\u2022 3D U-Net [2]: A fully supervised segmentation model trained with voxel-level annotations, providing a direct comparison compared to weakly supervised methods.\n\u2022 3D GAN [34]: A weakly supervised generative model that performs volume-level segmentation without requiring dense voxel-level annotations.\nWSI Datasets. For the CAMELYON16 and BRACS datasets, we compare INSIGHT against leading MIL-based methods. To ensure a fair comparison, all methods (includ-ing ours) use the UNI encoder [8] for feature extraction:\n\u2022 ABMIL [15]: An attention-based MIL approach that iden-tifies the most informative regions within a slide, com-monly used in weakly supervised WSI analysis.\n\u2022 CLAM-SB/MB [21]: A MIL method combining atten-tion mechanisms with clustering to improve localiza-tion. CLAM-SB uses a single branch for non-overlapping classes, while CLAM-MB employs multiple branches to target specific classes.\n\u2022 TransMIL [35]: A transformer-based MIL model that captures long-range dependencies within WSIs, enhancing contextual analysis for slide-level classification."}, {"title": "4.4. Quantitative Results", "content": "INSIGHT consistently outperformed baseline methods in classification and segmentation tasks across all datasets, demonstrating its adaptability to diverse medical imaging modalities. Performance metrics are reported in Table 1 and Table 2."}, {"title": "Key Results:", "content": "\u2022 On CAMELYON16, INSIGHT achieved a Dice score of 94.2%, outperforming the top competing model by 6.9%. This significant improvement highlights its ability to enhance boundary precision in segmentation tasks."}, {"title": "Spatial Embedding.", "content": "\u2022 On BRACS, INSIGHT demonstrated strong multi-label classification performance, achieving AUC improvements of 3.3% for both ADH and FEA subtyping. This show-cases its effectiveness in distinguishing subtle tissue varia-tions critical for breast carcinoma subtype classification.\n\u2022 On MosMed, INSIGHT achieved a classification AUC of 99.0%, nearly 5% higher than the best baseline, underscor-ing its robustness in volumetric CT analysis for COVID-19 detection.\nINSIGHT's superior performance is attributed to three key architectural innovations:\n By retaining spatial resolution dur-ing feature extraction, INSIGHT produces feature maps of 16 \u00d7 16 \u00d7 1024 for CT and 14 \u00d7 14 \u00d7 1024 for WSIs, preserv-ing diagnostically meaningful spatial structures. In contrast, baseline methods reduce feature maps to 1 \u00d7 1 \u00d7 1024 vec-tors via pooling, discarding fine-grained spatial information. This capability enables INSIGHT to detect small but critical"}, {"title": "Adjacent Context Focus.", "content": "patterns, particularly in datasets like BRACS, where distinct subtypes may co-exist within a single slide.\n INSIGHT's context module cap-tures localized dependencies through larger convolutional filters, avoiding the noise introduced by global self-attention mechanisms. This focus on adjacent spatial relationships is particularly effective for multi-label datasets like BRACS, where lesion subtypes often appear in close proximity."}, {"title": "Broader Foreground Activation Consideration.", "content": "Unlike conventional MIL-based models that focus only on the top k patches, INSIGHT's smooth pooling method considers all relevant foreground activations above a threshold. This approach preserves finer boundary details, significantly en-hancing segmentation accuracy. For example, on CAME-LYON16, this broader activation inclusion contributed to its 6.9% Dice score improvement."}, {"title": "4.5. Qualitative Analysis", "content": "We visualize and compare heatmaps generated by INSIGHT and baseline methods in Fig. 3. Unlike baseline methods, which often rely on post-hoc adjustments, INSIGHT directly integrates interpretability into its architecture, mapping each patch's probability to the heatmap. This eliminates the need for additional calibration steps, producing well-calibrated, fine-grained heatmaps that effectively highlight diagnosti-cally significant regions. Additional visualizations are pro-vided in Appendix A, which further illustrate the superior interpretability and precision of INSIGHT's heatmaps across both WSIs and CT datasets."}, {"title": "WSI Heatmap Comparisons:", "content": "\u2022 INSIGHT: The heatmaps generated by INSIGHT accu-rately capture subtle structures and ensure comprehensive coverage of ground-truth areas. Zoomed-in views reveal precise delineation of lesion boundaries, showcasing IN-SIGHT's ability to localize regions critical for diagnosis. Ground-truth areas exhibit high activation values, reflect-ing the model's robustness in spatially aligning predictions with annotations.\n\u2022 ABMIL: Heatmaps from ABMIL frequently produce false negatives, detecting only a limited number of patches and failing to capture a holistic view of lesion regions. This lack of coverage diminishes its diagnostic utility, particu-larly in cases with subtle abnormalities.\n\u2022 CLAM: While CLAM's heatmaps overlap with ground-truth regions, they often exhibit poor calibration, with sig-nificant portions of the lesion areas showing low activation values. This leads to suboptimal localization, especially for boundaries.\n\u2022 TransMIL: Heatmaps generated by TransMIL suffer from numerous false positives, marking non-lesion areas as relevant. This overactivation reduces diagnostic precision and makes the results less interpretable for clinical use.\nCT Heatmap Comparisons: INSIGHT demonstrates strong generalization across modalities, accurately highlight-"}, {"title": "4.6. Ablation Studies", "content": "ing infected regions in CT volumes. This is particularly evident in MosMed, where INSIGHT's heatmaps precisely localize areas of COVID-19 infection with minimal false pos-itives or negatives. These findings underscore INSIGHT's versatility and robustness compared to baseline methods.\n To validate the effectiveness of INSIGHT's architectural components, we perform an ablation study analyzing three key innovations: context suppression, SmoothMax pool-ing, and regularization. Results for the MosMed, CAME-LYON16, and BRACS datasets are shown in Table 3. The study evaluates both binary (MosMed, CAMELYON16) and multi-label (BRACS) tasks, revealing how each component contributes to classification and segmentation performance."}, {"title": "Key Ablation Study Findings:", "content": "\u2022 Context Suppression: Adding context suppression (Row 2) improves Dice scores across all datasets, with a notable gain of 16.6 on CAMELYON16. This enhancement high-lights the module's ability to improve spatial alignment be-tween predictions and ground truth by selectively focusing on relevant regions while suppressing background noise. However, context suppression alone does not maximize AUC, indicating that spatial coherence alone is insuffi-cient for capturing the discriminative features required for optimal classification performance.\n\u2022 SmoothMax Pooling: Incorporating SmoothMax pooling alongside context suppression (Row 3) results in a sub-stantial Dice score increase of 51.2 on CAMELYON16, emphasizing its role in refining lesion boundaries by lever-aging all pixel-level activations. This pooling strategy improves the model's ability to capture nuanced spatial details, enhancing boundary precision. However, Smooth-Max pooling slightly reduces AUC for binary tasks, likely due to the softened decision boundaries introduced by spatial averaging, which may blur class distinctions.\n\u2022 Regularization: Adding regularization (Row 4) improves both AUC and Dice scores across datasets, stabilizing AUC performance in MosMed and enhancing robustness in BRACS. Spectral decoupling mitigates overfitting in single-label tasks like MosMed, ensuring broader feature"}, {"title": "5. Discussion", "content": "utilization and improved generalization. For multi-label tasks like BRACS, label smoothing particularly benefits smaller classes such as ADH and FEA, improving classifi-cation consistency across imbalanced distributions.\nAccurately detecting and interpreting disease-related regions in medical imaging remains a critical challenge, particularly when only image-level annotations are available. INSIGHT addresses this gap by achieving state-of-the-art performance in both classification and segmentation tasks across CT vol-umes and WSIs using only image-level labels. This capa-bility is transformative, as it aligns with real-world clinical workflows where physicians rarely annotate images at the pixel or voxel level. By enabling the use of far larger datasets for training, INSIGHT unlocks the potential for scalable and efficient medical imaging solutions.\nINSIGHT's architectural innovations\u2014its detection and context modules\u2014allow it to combine local feature analy-sis with contextual understanding, generating interpretable and well-calibrated heatmaps. These heatmaps not only en-hance diagnostic transparency but also reduce reliance on extensive pixel-level annotations. Such capabilities could accelerate physician annotations [30], allowing clinicians to correct heatmaps rather than annotate from scratch, thereby streamlining annotation workflows where pixel-level labels are required."}, {"title": "Foundation Models and Future Potential.", "content": "For CT scans, we employed DINOv2 pre-trained on ImageNet due to the absence of publicly available radiology-specific pre-trained models. While this approach yielded strong results, we an-ticipate further improvements with embeddings tailored to radiology data, which could better capture salient features in CT scans. Similarly, for WSIs, we used UNI, one of the first foundation models in pathology. However, newer and more powerful models, such as Virchow [41], have recently emerged. Leveraging these advanced models for feature extraction could significantly enhance INSIGHT's perfor-mance. Unfortunately, due to computational constraints and the recent public release of these models, we were unable to perform a comparative analysis. Future work will ex-plore integrating these cutting-edge foundation models into INSIGHT to fully harness their potential."}, {"title": "Broader Applicability and Limitations.", "content": "While INSIGHT has been validated on CT and WSI data, its applicability to other imaging modalities, such as MRI and ultrasound, re-mains unexplored. In principle, INSIGHT's architecture should generalize well to these modalities due to its ability to process spatially embedded features and generate inter-pretable outputs. However, a significant barrier to extending INSIGHT lies in the scarcity of publicly available datasets for these modalities. Future work will focus on identify-ing or curating appropriate datasets to evaluate INSIGHT's generalizability and performance across a broader range of imaging modalities."}, {"title": "6. Conclusion", "content": "INSIGHT represents a significant advancement in weakly supervised medical imaging, demonstrating robust perfor-mance across diverse tasks and modalities. By effectively leveraging image-level labels, integrating interpretability directly into its architecture, and providing fine-grained di-agnostic insights, INSIGHT lays the foundation for scalable and clinically relevant AI solutions. Future efforts will aim to enhance its capabilities with advanced foundation models and extend its applicability to additional imaging modali-ties, furthering its impact on medical imaging research and practice."}, {"title": "B. Ablation", "content": "INSIGHT retains spatial resolution during the feature extraction process, and the output feature maps preserve spatial structure, resulting in dimensions of 16 \u00d7 16 \u00d7 1024 for CT slices and 14 \u00d7 14 \u00d7 1024 for WSI tiles. In contrast, compared methods typically apply a pooling strategy that reduces the feature map to a 1 \u00d7 1 \u00d7 1024 vector that discards spatial information. To evaluate the effectiveness of our spatial embedding, we conducted an ablation study where the spatial embedding was replaced with a non-spatial one. The results are summarized in Table 4, which highlights the impact of maintaining spatial resolution. Retaining spatial resolution increased the classification AUC by 4.5% and improved the Dice score for segmentation by over 15%. These substantial improvements underscore the critical role of spatial information in histopathological analysis, particularly for segmentation tasks that rely on fine-grained details and spatial continuity to localize regions of interest accurately. Additionally, compared to transformer-based aggregators, which face a quadratic increase in computational cost, INSIGHT leverages its lightweight CNN architecture to achieve a near-linear increase in computation, enabling it to fully capitalize on the benefits of spatial embedding."}]}