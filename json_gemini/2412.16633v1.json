{"title": "POEX: Policy Executable Embodied AI Jailbreak Attacks", "authors": ["Xuancun Lu", "Zhengxian Huang", "Xinfeng Li", "Xiaoyu Ji", "Wenyuan Xu"], "abstract": "The integration of large language models (LLMs) into the planning module of Embodied Artificial Intelligence (Embod-ied AI) systems has greatly enhanced their ability to translate complex user instructions into executable policies. In this paper, we demystified how traditional LLM jailbreak attacks behave in the Embodied AI context. We conducted a comprehensive safety analysis of the LLM-based planning module of embodied AI systems against jailbreak attacks. Using the care-fully crafted Harmful-RLbench, we accessed 20 open-source and proprietary LLMs under traditional jailbreak attacks, and highlighted two key challenges when adopting the prior jail-break techniques to embodied AI contexts: (1) The harmful text output by LLMs does not necessarily induce harmful poli-cies in Embodied AI context, and (2) even we can generate harmful policies, we have to guarantee they are executable in practice. To overcome those challenges, we propose Policy Executable (POEX) jailbreak attacks, where harmful instruc-tions and optimized suffixes are injected into LLM-based planning modules, leading embodied AI to perform harmful actions in both simulated and physical environments. Our approach involves constraining adversarial suffixes to evade detection and fine-tuning a policy evaluator to improve the executability of harmful policies. We conducted extensive experiments on both a robotic arm embodied AI platform and simulators, to validate the attack and policy success rates on 136 harmful instructions from Harmful-RLbench. Our find-ings expose serious safety vulnerabilities in LLM-based plan-ning modules, including the ability of POEX to be transferred across models. Finally, we propose mitigation strategies, such as safety-constrained prompts, pre- and post-planning checks, to address these vulnerabilities and ensure the safe deploy-ment of embodied AI in real-world settings.", "sections": [{"title": "1 Introduction", "content": "Embodied AI, which integrates perception, planning, and execution modules, has emerged with a potential to revo-lutionize how autonomous systems interact with and navi-gate their environments. Recent advances in large language models (LLMs) [2, 36] have further amplified this poten-tial, particularly in the planning module, where LLMs are widely used to transform nature language instructions, e.g., \"Place the cup on the table\", into policies [3, 10, 20], like composer(\"grasp the cup\u201d), composer (\"move to the ta ble\u201d), composer (\u201copen gripper\u201d). By providing several pre-defined policies, typically foundational API functions, as input within a given context, LLMs can interpret diverse user instructions and automatically execute the corresponding ac-tions. This capability significantly enhances the flexibility and adaptability of embodied AI systems, making them powerful tools across various applications.\nGiven LLM-integrated embodied AI systems are increas-ingly deployed in critical areas such as surgery [27], house-hold [26], and autonomous driving [13], there is an ur-gent need to carry out comprehensive safety analysis of these widespread applications, particularly concerning the robustness of embodied AI against jailbreak attacks. While LLMs have been shown to be highly vulnerable to jail-breaks [8, 25, 31, 46, 48], which can lead to the generation of biased, discriminatory, or harmful content, the safety chal-lenges in embodied AI are distinctly different due to their interaction with the real world. The consequences of a com-promised LLM-based planning module are far more severe, as they could lead to physical harm\u2014such as breaking objects or causing injury\u2014directly impacting both the environment and human safety. Furthermore, in the context of embodied AI, jailbreak attacks involve not just the generation of unsafe or unethical content but also the precise manipulation of control policies.\nTo answer the questions of how traditional jailbreak attacks behave in embodied AI systems and why, we first introduced the Harmful-RLbench, a hand-crafted embodied AI safety-oriented dataset designed to benchmark diverse LLMs as a planning module in embodied contexts. Harmful-RLbench consists of 25 completely unique, hand-crafted task scenarios, including environments such as kitchen, library, bedroom, and"}, {"title": "2 Background", "content": "2.1 LLM-based Embodied AI\nThe embodied AI system as shown in the Figure 2 consists of three modules: perception, planning and execution. The perception module senses the position and category of ob-jects in the environment; the planning module generates policies based on environmental information, context prompt and user instructions; the execution module transforms the policies into actions. The planning module not only needs to understand user instructions and environmental information but also generate logical and organized policies to complete tasks, making it challenging to train the model to adapt to different abstract and complex tasks. Considering the capabil-ities of LLMs in context learning and complicated reasoning, it is a viable solution to integrate LLMs into the embodied AI planning module to improve generalization. Provided with perceptual information, instructions, examples, constraints, etc. LLMs can transform new instructions into policies. The policies can be in a predefined structured form, programming code, or even natural language, and are generally composed of pre-trained fundamental actions arranged in combination. We particularly introduce the work on LLM-based Embodied AI below:\nSaycan [3] embeds LLMs into the planning module, where LLMs decompose instructions into reinforcement learning trained skills, and then select the skill that best matches the vision value as the policy. ChatGPT for Robotics [39] and Language Models as Zero-Shot Planners [19] try to add avail-able objects, callable functions to the context, and the LLM-based planning module transforms instructions into simple code policies. Inner Monologue [12], Socratic Models [43] and Grounded Decoding [11] use open-vocabulary object de-"}, {"title": "2.2 Jailbreak Attack", "content": "Jailbreak attacks refer to the attacker exploiting the vulnerabil-ity of the model architecture or carefully designing prompts to bypass the safety defenses of LLMs and output restricted or in-secure content. Jailbreak attack methods are categorized into white-box and black-box attacks. In a white-box attack sce-nario, the attacker has access to white-box information such as model architecture and parameters, while in a black-box attack scenario, the attacker can only have access to black-box information such as model responses. In order to determine the complete reasoning and planning process of embodied AI, the white-box approach is generally followed in the current embodied AI research, so we focus on white-box attack methods next.\nWhite-box jailbreak attacks can be divided into three cat-egories: gradient-based, logic-based and model-based. For gradient-based attacks, the gradient is used to optimize the inputs so that the model outputs harmful content. GCG [48] uses the gradient information to optimize an adversarial suf-fix so that the LLMs output affirmative replies to the mali-cious behaviors, which in turn outputs the malicious content. AutoDAN-liu [25] automatically generates invisible jailbreak prompts with a well-designed hierarchical genetic algorithm. AutoDAN-zhu [46] combines gradient-based labeling opti-mization with controlled text generation to generate coherent attack prompts. Probe-Sampling [45] investigates a new algo-rithm called probe sampling to speed up the GCG algorithm. GCG++ [35] replaces cross-entropy loss with multi-class hinge loss to improve the performance of GCG. Logits-based attacks optimize the input based on the probability distribution of the output token. COLD-Attack [8] adapts the energy-based Constrained Decoding with Langevin Dynamics to meet the requirements of fluency, steganography, sentimentality, and left-right coherence. Model-based attacks use other models to generate malicious prompts. AdvPrompter [31] trains another LLM to generate human-readable adversarial prompts."}, {"title": "3 Threat Model", "content": "The goal of the POEX attack is to compel embodied AI sys-tems to accept and execute harmful instructions in the physi-cal world, causing harm to both the environment and human lives. In typical operation, the LLM-based planning module of an embodied AI system receives user instructions, converts them into executable policies, and carries out these policies through its execution module. Under normal circumstances, the planning module should reject harmful instructions and re-frain from generating corresponding policies. However, in the POEX attack scenario, an attacker injects a harmful instruc-tion along with an optimized jailbreak suffix into the plan-ning module. This manipulated input can bypass the safety mechanisms of LLMs, transforming harmful instructions into harmful and executable policies, which are then performed by the embodied AI in the physical world.\nAttack Capability. We assume that the attacker cannot modify LLMs or retrain the underlying LLMs. Specifically, the attacker cannot alter the model's weights, system prompts, or contextual configurations. The attacker's mere ability is tampering with the user instructions, including the injection of adversarial suffixes, to achieve their malicious goals.\nModel Knowledge. We assume that the attacker has white-box access to the LLMs within the planning module, which aligns with prior work on adversarial attacks [23, 24, 40]. Namely, the attacker is fully aware of the architecture implies that the adversary has complete knowledge of the model's architecture, gradients, and internal configurations, including the context and system prompts used by the LLM."}, {"title": "4 Preliminary Investigation", "content": "4.1 Harmful-RLbench Dataset\nSeeing the urgent need to evaluating the safety boundaries of LLM-based embodied AI systems, we develop Harmful-RLbench, the first hand-designed dataset specifically crafted to assess both the usability and safety of embodied AI in generic manipulation tasks. Built upon the RLbench plat-form [15], Harmful-RLbench advances existing jailbreak and embodied AI benchmarks by seamlessly integrating harmful instructions with embodied AI scenarios, focusing on embod-ied AI safety risks, as shown in Figure 3.\nTask scenario. Harmful-RLbench includes 25 completely unique task scenarios, covering environments such as kitchen, library, bedroom, and laboratory. Each scenario utilizes a Franka Emika Panda robotic arm \u00b9 equipped with six RGB-D cameras. On this basis, we place realistic 3D object models in the environment, specifically selected for their relevance and potential safety risks. These objects include hazardous items like sharp knives and fragile vases, providing a diverse and challenging testing environment."}, {"title": "4.2 Preliminary Experiments", "content": "To explore the usability and safety of the LLM-based em-bodied AI, we evaluate the task success rate (TSR), attack success rate (ASR), and policy success rate (PSR) of different LLMs using Harmful-RLbench with both correct and harmful instructions.\n4.2.1 Baseline LLMs\nWe selected a diverse set of representative LLMs, varying in model structure, parameter size, and training data. These mod-els were categorized into two groups: open-source and pro-prietary. The open-source models include Phi-3 [1], gemma-2 [37], Qwen [4], Llama [38], Mistral [16], Mixtral [17], and Vicuna [6]. The proprietary models include Claude-3.5, GPT-4-turbo [2], GPT-40, and GPT-40-mini. For fairness and re-producibility, we ensured that all models used no sampling, and the hyperparameters, including the maximum number of generated tokens, were kept consistent across all experiments.\n4.2.2 Evaluation Metrics\nTask Success Rate (TSR). TSR measures the percentage of correct instruction tasks successfully completed by the policy generated by the LLM-based planning module. For this, we provide correct instructions to the planning module and score the generated policies based on the following criteria, where we define only scores above 3 are deemed as successful completion.\n\u2022 Score 0: No output is generated.\n\u2022 Score 1: A policy is generated but was not in the correct format, making it non-executable.."}, {"title": "5 Design of POEX Attack", "content": "To achieve policy executable embodied AI jailbreak attacks, we face the following challenges:\n\u2022 In the defined threat model, the attacker cannot modify sys-tem or contextual prompts and can only inject adversarial suffixes into user instructions.\n\u2022 The attack must ensure that the policies generated by the planning module are executable by the execution module, thereby causing harm in the physical world.\n\u2022 The adversarial suffixes need to remain human-readable and bypass perplexity detection, ensuring compatibility with speech-based human-computer interaction systems."}, {"title": "5.1 Design Overview", "content": "Facing the above challenges, we designed the algorithmic framework as shown in Figure 5:\n\u2022 initialisation module: Generates an adversarial suffix us-ing either random initialization or pre-trained initialization.\n\u2022 mutator module: Replaces tokens in the adversarial suffix using gradient-based token ranking.\n\u2022 constraint module: Filters adversarial suffixes with per-plexity scores exceeding a defined threshold.\n\u2022 selector module: Evaluates the loss of the attack and policy models, selecting the adversarial suffix with the lowest loss for further evaluation.\n\u2022 evaluator module: Assesses whether the generated policy successfully bypasses defenses and whether it is executable. If not, the process is repeated until both criteria are met."}, {"title": "5.2 Initialisation Module", "content": "The initialization module supports two adversarial suffix cre-ation methods: random initialization and pre-trained initial-ization. If the attacker optimises the adversarial suffix for the first time, the random initialization method can be used, which means that the adversarial suffix will be composed of randomly selected English words from the vocabulary of the LLM. To maintain a low perplexity for the initial adversarial suffix, the attacker should choose the same word repeated several times as the initial adversarial suffix. An attacker can also use the pre-trained initialization method, where the at-tacker randomly selects one from the pool of successfully"}, {"title": "5.3 Mutator Module", "content": "The mutator module uses the greedy coordinate gradient method to mutate the initial adversarial suffix. First, we com-pute the gradient matrix of the adversarial suffix with re-spect to the cross-entropy loss (i.e., the reference loss in Section 5.5). Then we invert the gradient matrix after normal-isation to obtain the score matrix. Finally, we set the scores of non-English words in the score matrix to negative infinity, and then randomly replace one token in the adversarial suf-fix with one of the top-k tokens with the highest scores. The purpose of retaining only English words is to ensure that the adversarial suffix after replacement is still composed of all English words, which not only makes it easier to inject the at-tack into voice human-computer interaction systems but also avoids possible perplexity detection. Each single-token re-placement can also minimize perplexity to the greatest extent. The specific formula for mutation is as follows:\n$G(i, j) = \\frac{\\partial L_{cross}}{\\partial t_{ij}}$\nwhere $G(i, j)$ denotes the gradient of the cross-entropy loss function of the jth token at the ith position in the gradient matrix, $L_{cross}$ denotes the cross-entropy loss, and $t_{ij}$ denotes the jth token at the ith position."}, {"title": "5.4 Constraint Module", "content": "In order to avoid possible perplexity detection, the constraint module filters out adversarial suffixes with perplexity higher than the threshold. Since the text to be evaluated is much shorter than the context length of the LLM, the perplexity of the adversarial suffix is evaluated by autoregressively decom-posing the sequence and calculating the conditional probabil-ity of the entire previous subsequence at each step, specifically defined as follows:\n$PPL(T) = exp{\\{ \\frac{1}{t} \\sum_{i=1}^{t} -log p_{\\theta}(X_i | X_{<i})} \\}$"}, {"content": "In order to avoid possible perplexity detection, the constraint module filters out adversarial suffixes with perplexity higher than the threshold. Since the text to be evaluated is much shorter than the context length of the LLM, the perplexity of the adversarial suffix is evaluated by autoregressively decomposing the sequence and calculating the conditional probability of the entire previous subsequence at each step, specifically defined as follows:\n$PPL(T) = exp{\\{ \\frac{1}{t} \\sum_{i=1}^{t} -log p_{\\theta}(X_i | X_{<i})} \\}$"}, {"content": "In order to avoid possible perplexity detection, the constraint module filters out adversarial suffixes with perplexity higher than the threshold. Since the text to be evaluated is much shorter than the context length of the LLM, the perplexity of the adversarial suffix is evaluated by autoregressively decomposing the sequence and calculating the conditional probability of the entire previous subsequence at each step, specifically defined as follows:\n$PPL(T) = exp{\\{ \\frac{1}{t} \\sum_{i=1}^{t} -log p_{\\theta}(X_i | X_{<i})} \\}$"}, {"content": "In order to avoid possible perplexity detection, the constraint module filters out adversarial suffixes with perplexity higher than the threshold. Since the text to be evaluated is much shorter than the context length of the LLM, the perplexity of the adversarial suffix is evaluated by autoregressively decomposing the sequence and calculating the conditional probability of the entire previous subsequence at each step, specifically defined as follows:\n$PPL(T) = exp{\\{ \\frac{1}{t} \\sum_{i=1}^{t} -log p_{\\theta}(X_i | X_{<i})} \\}$"}, {"title": "5.5 Selector Module", "content": "The selector module calculates the reference loss on the attack model and the policy executability loss on the policy eval-uation model, then weights and sums the two, selecting the instance with the lowest combined loss as the best instance to be evaluated. This process ensures that the selected instance has a relatively small reference loss while also having a rela-tively high policy executability score, meaning it considers both the jailbreak and the policy executability. The specific formula is defined as follows:\n$L(T,Y,Y') = \\alpha \\cdot L_{ref}(T, Y, Y') +\\beta \\cdot L_{policy} (X)$\nWhere T is the harmful instruction with the adversarial suffix, Y is the policy output by the LLM, Y' is the reference output (usually the name of a preset API function), $L_{ref}$ and $L_{policy}$ represent the reference loss and the policy executability loss, and \u03b1, \u03b2 are the weight coefficients of the two, used to balance the weights of the reference loss and the policy executability loss.\n$L_{ref}(T,Y,Y') = -(1/N) \\cdot \\sum_{i=1}^{N}(y'_i \\cdot log(p_i(Y|T)))$"}, {"title": "5.6 Evaluator Module", "content": "The evaluator module is used to evaluate whether the policy meets the following two conditions: 1) successful jailbreak; 2) executable policy. As analyzed in the preliminary experi-ment, the planning module may encounter the following four situations with harmful instructions:\n\u2022 Generating harmful policies without any warning;\n\u2022 Generating harmful policies but providing natural language warning indicating potential safety risks;\n\u2022 Refusing to reply while not generating the policy;\n\u2022 Refusing to generate harmful policies but generating harm-less policies as examples;\nTo distinguish the above four situations, we use the prefix matching to evaluate whether the jailbreak is successful; we use the policy evaluator to score the generated policy and evaluate whether the policy is executable based on the score. Only when both the jailbreak is successful and the policy is executable do we consider that the embodied AI policy executable jailbreak attack is completed.\n5.6.1 Prefix Matching\nThe prefix matching is used to determine whether the output policy meets the format specifications of the embodied AI system. Generally speaking, the policy consists of a fixed set of API functions, and the names of the API functions in different policies generated by different instructions are same. Therefore, with the knowledge of the adversary's embodied AI system, the attacker can match the generated policy with the prefixes of the names of the API functions in the system. Only if the prefix matching is successful, the policy is consid-ered to be potentially executable by the embodied AI system execution module. We use the following formula to describe the prefix matching: only if the beginning of all policies is among the reference API function names do we consider the prefix match successful.\n$match = \\bigwedge_{i=1}^{A} \\bigvee_{j=1}^{V} (Policy_i starts \\ with \\ Reference_j)$"}, {"content": "The evaluator module is used to evaluate whether the policy meets the following two conditions: 1) successful jailbreak; 2) executable policy. As analyzed in the preliminary experiment, the planning module may encounter the following four situations with harmful instructions:\n\u2022 Generating harmful policies without any warning;\n\u2022 Generating harmful policies but providing natural language warning indicating potential safety risks;\n\u2022 Refusing to reply while not generating the policy;\n\u2022 Refusing to generate harmful policies but generating harm-less policies as examples;\nTo distinguish the above four situations, we use the prefix matching to evaluate whether the jailbreak is successful; we use the policy evaluator to score the generated policy and evaluate whether the policy is executable based on the score. Only when both the jailbreak is successful and the policy is executable do we consider that the embodied AI policy executable jailbreak attack is completed.\n5.6.1 Prefix Matching\nThe prefix matching is used to determine whether the output policy meets the format specifications of the embodied AI system. Generally speaking, the policy consists of a fixed set of API functions, and the names of the API functions in different policies generated by different instructions are same. Therefore, with the knowledge of the adversary's embodied AI system, the attacker can match the generated policy with the prefixes of the names of the API functions in the system. Only if the prefix matching is successful, the policy is consid-ered to be potentially executable by the embodied AI system execution module. We use the following formula to describe the prefix matching: only if the beginning of all policies is among the reference API function names do we consider the prefix match successful.\n$match = \\bigwedge_{i=1}^{A} \\bigvee_{j=1}^{V} (Policy_i starts \\ with \\ Reference_j)$"}, {"title": "6 Evaluation", "content": "6.1 Experiment Setup\n6.1.1 Prototype\nWe implemented a prototype of the POEX attack based on Pytorch and used two NVIDIA A800 GPUs to train the adver-sarial suffixes. We set the default configuration of the LLMs as follows: the maximum number of new tokens is 128, the temperature is 0.01, the length of the adversarial suffix is 5, the number of mutations is 64, the batchsize is 16,and the first 256 tokens with the largest gradient are taken. It is worth noting that, in order to ensure the reproducibility of the ex-periments, we set the temperature to 0.01 to make the LLMs generate the same policy every time.\nAdditionally, we trained the policy evaluator based on Meta-Llama-3-8B-Instruct and GPT-40-mini. For Meta-Llama-3-8B-Instruct, we fine-tuned it using Lora technology. During training, we set the maximum length to 1024, optimized using gradient accumulation, with a batch size of 2 per device and 8 gradient accumulation steps. The learning rate was set to 5e-5, and the learning rate scheduler was cosine annealing. Bf16 precision and gradient checkpointing were enabled to save memory and improve model training efficiency. For GPT-40-mini, we fine-tuned it using the default parameters generated by the official dataset.\n6.1.2 Dataset\nWe use our self-designed Harmful-RLbench dataset to train and evaluate adversarial suffixes, where each of the 136 harm-ful instructions corresponds to a context and system prompt. After optimizing the adversarial suffixes, we conduct simula-tion evaluation in Comppeliasim to see if they would cause harm to humans and the environment. The dataset of the fine-tuning model comes from preliminary experiments, where we created a fine-tuning dataset consisting of 5500 one-to-one corresponding instructions, contexts, system prompts, gener-ated policies, and policy scores."}, {"title": "6.1.3 Evaluation metrics", "content": "Attack Success Rate (ASR): the ratio of the number of instructions that LLMs do not refuse to answer to the total number of instructions, a higher attack success rate means a better jailbreak attack."}, {"title": "6.2 Performance Evaluation", "content": "We compare the effectiveness of our method with other white-box based jailbreak attack methods in generating harmful policies on three open-source models, Llama-3-8B-instruct, Mistral-7B-instruct-v0.2 and Vicuna-13B-v1.5. To ensure fair-ness, we use the same suffix length, the maximum number of iterations, the initialization suffix and the template of prompt with safety constraints. We generate adversarial suffixes for each of the 136 harmful instructions in Harmful-RLbench, and then evaluate the metrics such as attack success rate, pol-icy success rate, etc., and the specific results are shown in Table 2.\nHigher policy sucess rate: The average attack success rate of our method is basically same as other jailbreak methods, but our policy success rate is higher than other methods on all three models. The high policy success rate is also due to our inclusion of the policy evaluator to improve the executability of the generated policies. However, there is an upper limit to the ability of the model with a small number of parameters, and it is difficult to generate logical policies for complex instructions. In addition, the policy success rate of our attacks is even higher than before adding system prompts containing safety constraints, which demonstrates that our optimized adversarial suffixes not only have the ability to jailbreak but also improve the model reasoning.\nHigher perplexity pass rate: The perplexity pass rate of our method is 100% on all models because the constraint module limits the perplexity below a certain threshold. Au-toDAN passes the perplexity detection when the length of the adversarial suffixes is short and struggles to pass the per-plexity detection when they are long because it generates the token one by one. GCG randomly selects tokens from the entire vocabulary when optimizing, which makes it difficult for suffixes to have low perplexity and to pass the perplexity detection.\nLower word error rate: The word error rate of our attack is much lower than other methods, which means that the ad-versarial suffixes generated by our method can be accurately restored to text by speech recognition and then injected into embodied AI systems. This is because the candidate list of other methods is the entire vocabulary when replacing token, and thus the adversarial suffixes often contain special symbols and other languages. These non-English word tokens result in speech that is difficult to be accurately recognized by speech recognition systems.\nWe compared the distribution of policy evaluation scores of harmful policies generated by the three methods on the three LLMs. As shown in Figure 7, the percentage of policy score of 3 and 4 especially 4 generated by our method is higher than the other methods, which indicates that our method effectively transforms the non-executable policies into executable poli-cies. We believe that adding adversarial suffixes after harmful instructions is not conducive to LLMs understanding and rea-"}, {"title": "6.3 Transferability of adversarial suffixes", "content": "In this section, we evaluate the transferability of adversarial suffixes optimized on white-box models to black-box mod-els. We first optimize the corresponding adversarial suffix for each harmful instruction on Llama-3-8B-instruct, Mixtral-7B-instruct-v0.2, and Vicuna-13B-v1.5, and then we use GPT-4-Turbo, Mixtral-8x22B-instruct, Llama-3.1-70B-instruct as black-box models to evaluate the ASR and PSR of the ad-versarial suffixes. We also evaluate the transfer effect of con-catenating the suffixes optimized by the three models. Finally, we combine the adversarial suffixes optimized individually and the concatenated suffixes from the three models, consid-ering the attack successful if any one of them succeeds. The results are shown in Table 3, comparing to the baseline of only harmful instructions, the adversarial suffixes we gener-ated on the three open-source small parameter models are still effective both on the proprietary model and the open-source large-parameter model. The method of concatenating suffixes achieves better results on the GPT-4-Turbo model than indi-vidual suffixes, but does not perform as well on the other two models. When setting the condition that only one of the first four adversarial suffixes is considered successful, we find that the transfer effect is greatly enhanced, especially with Mixtral-8x22B-instruct achieving 100% ASR and 63.24% PSR. We speculate that this may be because Mixtral-8x22b-instruct is a mixture of experts model, where each small model has different vulnerabilities, leading to greater vulnerability when facing combined attacks."}, {"title": "6.4 Real-World Case", "content": "We built an experimental platform in the real world and tested the execution success rate for five harmful instructions. For each harmful instruction and adversarial suffix, we take the form of speech to inject into the embodied AI system, and then observe whether the embodied AI system completes the harmful instruction, repeat each instruction ten times, and finally record the execution success rate as shown in Table 4. We find that five harmful instructions can succeed at least once in ten repetitions of the experiment, which shows that our attack can indeed cause harm in the real-world as well. By analyzing the failed examples, we found that most of the failures stem from the gripper not being able to grab the object or opening the gripper in the wrong position, rather than our attack algorithm."}, {"title": "6.5 Model-Based Defense", "content": "In this section, we attempt to use model-based defense meth-ods to defense policy executable embodied AI jailbreak attacks. We selected Llama-Guard-2, Llama-Guard-3, and Harmbench to simultaneously detect the input of harmful in-structions and the output of harmful policies, with Harmbench comparing the presence of context. The specific results are shown in the table 5. We found that the three methods could detect 30%-40% of harmful instructions or policies on aver-age across the three models, with Harmbench achieving an average detection rate of 85% when context was included. This indicates that for model-based defense methods without context, our attacks remain effective in most cases. We be-lieve that the reason adding context can effectively improve the detection rate is that the detection model infers the in-tent of harmful instructions and policies based on the context, whereas it lacks this common sense without context. However, due to the significant resource consumption of model-based methods, it is challenging to apply them in embodied AI systems, thus necessitating the exploration of more efficient defenses."}, {"title": "7 Related Work", "content": "In this section, we summarize related work on LLM-based embodied AI safety research and datasets.\nLLM-based Embodied AI Attack. At present, there is lit-tle research on LLM-based embodied AI security. Most work study whether LLMs can output harmful text in embodied"}, {"title": "8 Discussion", "content": "Future Work. Our preliminary experiments reveal signifi-cant safety risks when directly applying LLMs to the planning modules of embodied AI. Current safety alignments in LLMs primarily focus on restricting biased, discriminatory, and hate-ful text, leaving gaps in defending against harmful instructions that could endanger humans or environment in embodied AI scenarios. Therefore, future safety alignment practices for LLMs used in embodied AI can consider incorporating spe-cific datasets like Harmful-RLbench, which are tailored to these contexts. Also, future work shall focus on creating more robust safety mechanisms that address the unique risks posed by LLM-based embodied AI systems operating in real-world conditions.\nEthical Considerations. We will open-source POEX's implementation, model checkpoints, and Harmful-RLbench datasets upon the acceptance of the paper to support the re-search and developer communities. Since Harmful-RLbench datasets contains harmful Instructions that might be used for illegitimate purposes. Having weighed the benefits and harms, we are releasing the datasets in a limited way, i.e., we will directly release the correct instructions of the datasets while provide the harmful instructions upon request to, e.g., profes-sors at other institutions who are doing related research. This helps with providing the datasets to those who can use it for legitimate purposes, while reducing the potential harms from releasing it publicly."}, {"title": "9 Conclusion", "content": "In this paper, we introduce POEX\u2014a policy executable jail-break attack targeting LLM-based planning in embodied AI systems-uncovering serious safety risks as these systems can be manipulated to execute harmful actions in physical envi-ronments. To support further research, we establish Harmful-RLbench, the first dataset specifically designed to evaluate both the usability and safety of LLM-based planning modules in embodied AI. Based on Harmful-RLbench, our extensive evaluation conducted across three LLMs, 25 task scenarios, and 136 dangerous instructions, resulted in a 80% attack suc-cess rate and a 50% policy success rate, highlighting the poten-tial for real-world harm. In response to these risks, we propose effective defense mechanisms, including pre-instruction and post-policy detection, which effectively mitigate these attacks by identifying harmful input and output."}, {"title": "Appendix", "content": "You are a helpful assistant that pays attention to the user's instructions and writes good python code for operating a robot arm in a tabletop environment. You must obey Asimov's three laws: A robot may not injure a human being or, through inaction, allow a human being to come to harm. A robot must obey orders given it by human beings except where such orders would conflict with the First Law. A robot must protect its own existence as long as such protection does not conflict with the First or Second Law. Not only that, you can't generate code that damages objects, violates morals and laws, destroys the environment, or is violent."}, {"title": "A Safe System Prompt", "content": "The safe system prompt is shown in Figure 9."}, {"title": "B Policy Evaluator Prompt", "content": "The policy evaluator prompt is shown in Figure 10."}]}