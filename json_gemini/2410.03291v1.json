{"title": "Enhanced Transformer architecture for\nin-context learning of dynamical systems", "authors": ["Matteo Rufolo", "Dario Piga", "Gabriele Maroni", "Marco Forgione"], "abstract": "Recently introduced by some of the authors, the in-context identifi-\ncation paradigm aims at estimating, offline and based on synthetic data,\na meta-model that describes the behavior of a whole class of systems.\nOnce trained, this meta-model is fed with an observed input/output se-\nquence (context) generated by a real system to predict its behavior in\na zero-shot learning fashion. In this paper, we enhance the original\nmeta-modeling framework through three key innovations: by formulat-\ning the learning task within a probabilistic framework; by managing non-\ncontiguous context and query windows; and by adopting recurrent patch-\ning to effectively handle long context sequences. The efficacy of these\nmodifications is demonstrated through a numerical example focusing on\nthe Wiener-Hammerstein system class, highlighting the model's enhanced\nperformance and scalability.", "sections": [{"title": "Introduction", "content": "In system identification (SYSID), the primary objective is to model dynamical\nsystems, leveraging both measured input-output trajectories and prior knowl-\nedge of the system's dynamics. The SYSID setting is thus closely related to\nsupervised machine learning (ML), with a specific focus on dynamical systems.\nDue to this similarity, machine-learning techniques have become increasingly\npopular over the years for estimating dynamical systems through, e.g., neural\nnetwork architectures [1, 2, 3] and kernel-based methods [4, 5].\nDespite their success, standard supervised learning approaches do not ex-\nploit the insight that could be gained by repeatedly identifying similar systems.\nWhile humans improve at solving related tasks, traditional ML (and SYSID) al-\ngorithms rely on fixed, predefined procedures that are applied uniformly across\ndifferent datasets.\nThe concept of learning to learn, or meta-learning, was introduced in the late\neighties by the pioneering work [6] to overcome this limitation and is gaining"}, {"title": "Problem description", "content": "In [11], an in-context parametric learner $M_{\\phi}$, referred to as meta-model, has\nbeen introduced to describe a class of dynamical systems. The meta-model $M_{\\phi}$,\nwith tunable parameters $\\phi$, is trained on a \"meta-dataset\". To define the \"meta-\ndataset\", two probability distributions, one over dynamical systems and one over\ninput signals, are introduced. By sampling from these two distributions, it is\npossible to generate a sequence of systems and inputs that, together with the\ncorresponding outputs, result in \"usual\" input/output datasets. This leads to\na potentially infinite stream of datasets {$D^{(i)} = (u_1^{(i)}, Y_1^{(i)}), i = 1, 2, ..., \\infty$}\nwith $u \\in \\mathbb{R}^{n_u}$ and $y \\in \\mathbb{R}^{n_y}$, each sampled from the dataset distribution\n$p(D)$ induced by randomly sampling systems and inputs. The datasets $D^{(i)}$ are\nall different, but they are drawn from the same probability distribution, so it is\npossible to transfer the learned knowledge from one realization to another.\nEach dataset realization $D^{(i)}$ is split into an initial context segment of length\nm and a contiguous query segment of length $N-m$. The meta-model $M_{\\phi}$ is\ntrained to reconstruct the query output $Y_{m+1:N}$ from the query input $U_{m+1:N}$\nand the input/output context $(u_{1:m}^{(i)}, y_{1:m}^{(i)})$, namely:\n$Y_{m+1:N}^{(i)} = M_{\\phi}(U_{m+1:N}^{(i)}, u_{1:m}^{(i)}, y_{1:m}^{(i)}),$ (1)\nwhere $\\hat{Y}_{m+1:N}^{(i)}$ represents the estimate of the output $y_{m+1:N}^{(i)}$.\nTraining is performed in a supervised manner, by minimizing the regression\nloss\n$J = \\frac{1}{b} \\sum_{i=1}^{b} ||Y_{m+1:N}^{(i)} - M_{\\phi}(U_{m+1:N}^{(i)}, u_{1:m}^{(i)}, y_{1:m}^{(i)})||^2 ,$ (2)"}, {"title": "Advancing the Transformer Architecture", "content": "The Transformer architecture in [11] has been extended to address the three\nlimitations previously mentioned. In the next paragraphs, we will systematically\naddress each limitation, detailing the specific modifications made to the original\narchitecture. We anticipate the final extended architecture in Fig. 1, with the\nfollowing main changes:\n\u2022 to address L1, we modify the final output layer to provide both the mean\nand the standard deviation of the predicted output samples;\n\u2022 to address L2, we introduce an additional layer before the decoder to\nhandle arbitrary initial conditions of the query sequence;\n\u2022 to address L3, we split the context sequence into patches, which are in-\ndividually processed by a RNN before being fed into the multi-attention\nblocks of the Transformer's encode."}, {"title": "Learning Probability Distributions", "content": "To formulate the learning problem within a probabilistic setting, we introduce\nthe conditional probability distribution $p(y_{m+1:N}|U_{m+1:N}, U_{1:m}, Y_{1:m})$ of the fu-\nture output sequence $y_{m+1:N}$, given the query input $U_{m+1:N}$ and the context"}, {"title": "Handling arbitrary initial conditions", "content": "In order to consider distinct context and query sequences, it is necessary to\nprovide initial conditions along with a query input sequence. In this work, we\nfeed the meta-model with $n_{in}$ input-output samples preceding the query input as\nthe initial conditions. Thus, for supervised learning of this meta-model $M$, two\ninput/output sequences are generated from each system S: a context sequence\n$(u_{1:m}, y_{1:m})$ and a disjoint query sequence $(\\bar{u}_{1:N}, \\bar{y}_{1:N})$, whose initial samples\n$(\\bar{u}_{1:n_{in}}, \\bar{y}_{1:n_{in}})$ are taken as initial conditions.\nThen, given a query input $\\bar{u}_{n_{in}+1:N}$, initial conditions $(\\bar{u}_{1:n_{in}}, \\bar{y}_{1:n_{in}})$, and\ncontext $(u_{1:m}, y_{1:m})$, the meta-model returns the mean and standard deviation\nvectors $\\mu_{n_{in}+1:N}, \\sigma_{n_{in}+1:N}$ of the probability distribution of the future output\nsequence $y_{n_{in}+1:N}$:\n$\\mu_{n_{in}+1:N}, \\sigma_{n_{in}+1:N} = M_{\\phi}(\\bar{u}_{n_{in}+1:N}, \\bar{u}_{1:n_{in}}, \\bar{y}_{1:n_{in}}, u_{1:m}, y_{1:m}).$ (8)\nTo this end, we modify the Transformer architecture by introducing a linear\nlayer that processes the initial conditions, mapping each time step from $\\mathbb{R}^{n_u+n_y}$\nto $\\mathbb{R}^{d_{model}}$. Similarly, the query input is mapped from $\\mathbb{R}^{n_u}$ to $\\mathbb{R}^{d_{model}}$ through\na separate linear layer. The corresponding output sequences are then concate-\nnated to form a single sequence of length N, where all elements have dimension\n$d_{model}$. This sequence is combined with positional encoding before being passed\ninto the decoder backbone of the Transformer, as shown in the layers before the\ndecoder in Fig. 1.\nRemark 1 The length m of the input/output context sequence should be suffi-\nciently large to characterize the dynamics of the system. On the other hand, the\nnumber $n_{in}$ of input/output samples used as initial conditions is typically small,\nof the order of magnitude of the number of dynamical states of the system."}, {"title": "Patching for long context sequences", "content": "In order to handle Limitation L3, and thus considering long context sequences,\nwe adopt a patching approach tailored to the meta-learning problem introduced\nin Section 2, and described in the following paragraphs."}, {"title": "Numerical Example", "content": "The presented meta-model architecture is assessed for in-context learning of\ndynamical systems belonging to the Wiener-Hammerstein (WH) [22] class. For\nmeta-model learning, the negative log-likelihood (5) is minimized over 1 million\niterations of the AdamW algorithm [23], with minibatch size b = 32. Training is\nrepeated for different values of the context length $m \\in$ {400, 800, 16000, 40000}.\nAll computations are performed on a server equipped with an Nvidia RTX 3090\nGPU. For reproducibility of the results, all codes made are available in the\nGitHub repository [24]."}, {"title": "System Class and Dataset Distribution", "content": "We generate synthetic datasets from random stable WH systems having struc-\nture $G_1 - F - G_2$, where F is a static non-linearity sandwiched between two Linear\nTime-Invariant (LTI) dynamical blocks $G_1$ and $G_2$. Systems are drawn from"}, {"title": "Transformer architecture", "content": "The chosen Transformer architecture is characterized by: $N_{layers} = 12$ layers;\n$d_{model} = 128$ hidden units; and $n_{heads} = 4$ attention heads, both in the encoder\nand the decoder. The number of the initial conditions is set to $n_{in} = 10$ and\nthe query length to $N - n_{in} = 100$.\nAs previously mentioned, training is performed for context length $m \\in$\n{400, 800, 16000, 40000}. For the sake of comparison between the proposed\npatching-based approach and the non-patching method, for the shortest con-\ntext length m = 400 the recurrent patching approach is not implemented and\nthe sequence of embeddings $P_{1:M}$ (with M = m) is obtained simply by process-\ning the samples $u_{1:m}, y_{1:m}$ through a linear layer with $n_u + n_y$ inputs and $d_{model}$\noutputs, as in the original work [11]."}, {"title": "Results", "content": "The test root mean square error (RMSE), train time, and number of parameters\nof the different meta-models are reported in Table 1. Furthermore, the validation\nRMSE over the iterations of the AdamW optimization algorithm is visualized\nin Fig. 3.\nThe difference between the achieved RMSE and the noise floor $\\sigma_{noise} = 0.1$\nconsistently decreases with the context length up to m = 16000. We remark\nthat, although it does not result in additional improvements for the benchmark"}, {"title": "Out-of-distribution", "content": "To assess robustness of the trained meta-model against a shift in the input\ndistribution, we have generated test datasets from 256 different systems by\napplying as input a pseudo random binary signal (PRBS), thus with different\ncharacteristics with respect to the input used for meta-model training. The case\nof context length m = 16000 is reported. Output trajectories are visualized in\nFig 5a, for all systems (left panel) and for one particular realization (right\npanel). The average RMSE is 0.3094, about 2.4x larger than the in-distribution\ncase (see Table 1). It is interesting to observe that, as the performance decrease,\nthe estimated uncertainty bands get wider.\nFollowing the approach in [26], we performed a short fine-tuning of the meta-\nmodel for PRBS input over 40k iterations, corresponding to approximately 2\nhours of training. Results are visualized in Fig. 5b for the same test datasets\nconsidered in the previous paragraph. We remark that fine-tuning leads to a\nsignificant improvement in the RMSE from 0.3094 to 0.1058 (thus very close to\nthe noise floor $\\sigma_{noise} = 0.1$), along with a shrinkage of the uncertainty bands."}, {"title": "Conclusions", "content": "Major modifications to the Transformer-based meta-model architecture intro-\nduced in [11] for in-context system identification have been presented. Numeri-\ncal experiments show that using longer context windows results in a substantial\naccuracy improvement, with the trained meta-model approaching the noise floor.\nFurthermore, the meta-model maintains reasonable performance in the case of\nout-of-distribution inputs, with prediction capabilities that can be significantly\nimproved through fast fine-tuning.\nThe statistical formulation, besides providing uncertainty quantification for\nthe output predictions, opens up applications in other areas, such as meta-state\nestimation [27], or in classical system identification tasks where the meta-model\ncan be used to generate synthetic data as in [28], and the uncertainty can be used\nto properly weight synthetic samples. This is the subject of ongoing research,"}]}