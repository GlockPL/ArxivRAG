{"title": "SwinSF: Image Reconstruction from Spatial-Temporal Spike Streams", "authors": ["Liangyan Jiang", "Chuang Zhu", "Yanxu Chen"], "abstract": "The spike camera, with its high temporal resolution, low la-\ntency, and high dynamic range, addresses high-speed imaging challenges\nlike motion blur. It captures photons at each pixel independently, cre-\nating binary spike streams rich in temporal information but challenging\nfor image reconstruction. Current algorithms, both traditional and deep\nlearning-based, still need to be improved in the utilization of the rich\ntemporal detail and the restoration of the details of the reconstructed im-\nage. To overcome this, we introduce Swin Spikeformer (SwinSF), a novel\nmodel for dynamic scene reconstruction from spike streams. SwinSF is\ncomposed of Spike Feature Extraction, Spatial-Temporal Feature Ex-\ntraction, and Final Reconstruction Module. It combines shifted window\nself-attention and proposed temporal spike attention, ensuring a com-\nprehensive feature extraction that encapsulates both spatial and tem-\nporal dynamics, leading to a more robust and accurate reconstruction\nof spike streams. Furthermore, we build a new synthesized dataset for\nspike image reconstruction which matches the resolution of the latest\nspike camera, ensuring its relevance and applicability to the latest devel-\nopments in spike camera imaging. Experimental results demonstrate that\nthe proposed network SwinSF sets a new benchmark, achieving state-of-\nthe-art performance across a series of datasets, including both real-world\nand synthesized data across various resolutions. Our codes and proposed\ndataset will be available soon.", "sections": [{"title": "1 Introduction", "content": "The spike camera [7], which is an emerging neuromorphic camera, excels in\nhigh-speed motion capture with its high temporal resolution, low latency, low\npower consumption, and high dynamic range. These attributes are critical for\nhigh-speed imaging requirements. Aiming to solve the problems of motion blur\nand artifacts caused by the imaging mechanism that each pixel on the sensor of\ntraditional digital camera is exposed synchronously in high-speed motion scenes,\neach pixel on the sensor of spike camera records the external absolute light\nintensity independently and continuously to generate spike streams with high\ntemporal resolution. As a result, the spike camera can not only finely record\nthe high-speed motion process, but also provide rich lighting information to\nreconstruct the texture details in the scene.\nHowever, converting binary spike streams to images requires specialized algo-\nrithms. Traditional methods [35,34,33] perform basic reconstructions but strug-\ngle with noise and motion blur. Deep learning, notably CNNs and transform-\ners, has advanced spike camera image reconstruction significantly. For exam-\nple, Spk2ImgNet [31] directly reconstructs images but may miss global context.\nSpikeFormer [22], leveraging transformers, excels but demands high computa-\ntional resources for large images. WGSE [12], using discrete wavelet transforms,\nstruggles with nonlinear data patterns. As depicted in fig. 1, none fully restores\nimage details, highlighting the need to exploit spatial-temporal features in spike\nstreams effectively.\nIn this paper, we propose Swin Spikeformer(SwinSF), a spike camera im-\nage reconstruction model designed to reconstruct dynamic scenes clearly from\nspike streams. In detail, SwinSF is made up of three main modules: Spike Fea-\nture Extraction Module, Spatial-Temporal Feature Extraction Module and Final\nReconstruction Module. The Spike Feature Extraction Module uses convolution"}, {"title": "2 Related Works", "content": "2.1 Image Reconstruction of Spike Camera\nSpike cameras' advancement has spurred various applications, with image recon-\nstruction being pivotal. Recent methods, like TFI [35], TFP [35], Spk2ImgNet\n[31], SpikeFormer [22], and WGSE [12], have emerged. TFI leverages inter-spike\nintervals (ISIs) for reconstruction, foundational yet limited by quantization and\ndark current issues. TFP uses a moving time window to reduce noise but risks\nblur in fast scenes. Spk2ImgNet and SpikeFormer apply CNNs and transformers,\nrespectively, for direct reconstruction, improving visuals. WGSE innovates with\ntime-frequency analysis through discrete wavelet transforms. Despite progress,\nspatial-temporal representation learning remains an area for enhancement.\n2.2 Vision Transformers\nRecently, the Transformer architecture [25] has gained significant interest in\ncomputer vision due to its NLP successes. Many transformer-based methods"}, {"title": "3 Preliminaries: Mechanisms of Spike Camera", "content": "The core of spike camera sensor is an asynchronous, persistently active pixel ar-\nray. Each pixel independently accumulates photons; upon reaching a set thresh-\nold, it triggers a spike signal, resetting the count. This mechanism cycles con-\ntinuously. Mathematically, this process is expressed as:\n$A(i, j, t) = \\int_{t_{pre}}^{t} al(i, j, t) dt \\bmod \\Theta,$\n(1)\nwhere $A(i, j, t)$ denotes photon count at pixel (i, j) at time t, $\\alpha$ is the photoelec-\ntric rate, $I(i, j, t)$ is incident photon intensity. $\\Theta$ is the threshold.\nTheoretically, pixels fire spikes anytime thresholds are met, but physically,\nthe spike camera samples photon counts at high frequency, outputting discrete-\ntime signals $S(i,j,t)$ at intervals $t = nT$, where T is a short interval of mi-\ncroseconds. If a pixel (i, j) meets the threshold at time t = nT, it reads out\n$S(i, j, t) = 1$ and resets the count for the next cyclicality. Otherwise, it reads out\n$S(i, j, t) = 0$. The above can be expressed as:\n$S(i, j,t) = \\begin{cases} 1 & \\text{if } A(i, j, t) \\geq \\Theta \\\\ 0 & \\text{if } A(i, j,t) < \\Theta \\end{cases}$\n(2)"}, {"title": "4 Method", "content": "4.1 Overall\nTo fully learn spatial-temporal representation and reconstruct the dynamic scene\nfrom the spike streams, we propose a spike camera image reconstruction model\ncalled Swin Spikeformer(SwinSF). As shown in fig. 3, our network architecture\nis composed of three integral components: a Spike Feature Extraction Mod-\nule, a Spatial-Temporal Feature Extraction Module, and a Final Reconstruction\nUnder continuous light, pixels independently accumulate photons, with high-speed sampling validating each pixel's spike status, forming an $H \\times W$ spike\nframe. The sensor finally generates a sequence of spike frames called spike\nstreams as the output of spike camera.\nfig. 2 provides an illustrative example of a car moving through a scene. The\nbackground is darker than the car's surface. Intuitively, the frequency of the\nspike is directly proportional to the brightness level observed, which is clearly\nevidenced by the red box depicted in the figure."}, {"title": "4.2 Spike Attention Block(SAB)", "content": "As depicted in fig. 2, the special imaging mechanism of the spike camera leads\nto a special dependence on the long-distance spatial-temporal information in the\nprocess of image reconstruction. Therefore, we design the Temporal Spike At-\ntention(TSA) mechanism for spike streams to enhance the long-distance spatial-\ntemporal information extraction ability of the network. The TSA block is in-\nserted into the standard Swin Transformer Block [18], following the initial Lay-\nerNorm (LN) layer and in parallel with the shifted window-based multi-head\nself-attention (SW-MSA) module, as illustrated in fig. 3. The SW-MSA is em-\nployed periodically within consecutive TSA blocks, following a similar approach\nas described in [18,17,5]. To reconcile the optimization and visual representation\ndiscrepancies between TSA and SW-MSA, we introduce a small constant $\\beta$ as a\nscaling factor to the output of TSA. This adjustment harmonizes the interplay\nof the attention mechanisms, ensuring a more cohesive learning dynamic. The\nwhole process of SAB is computed as:\n$\\begin{aligned}\nX_l, X_m, X_r &= \\text{LN}(\\text{Split}(X)) \\\\\nY_m &= \\text{SW-MSA}(X_m) + \\beta\\text{TSA}([X_l, X_m, X_r]) + X_m \\\\\nY_m &= \\text{MLP}(\\text{LN}(Y_m) + Y_m) \\\\\nY &= \\text{Concat}(X_l, Y_m, X_r)\n\\end{aligned},$\n(3)\nwhere X denote the input feature, $X_l, X_m, X_r$, are three features split by X,\nrepresenting the features at frames l,m, and r respectively. $Y_{ms}$ and $Y_{mt}$ represent\nthe intermediate features computed by the SW-MSA and TSA, and Y is the\noutput of the SAB."}, {"title": "4.3 Temporal Spike Attention (TSA)", "content": "To address the inherent limitation of SW-MSA in capturing solely the spatial\nvisual representations within an isolated frame, and acknowledging the necessity\nfor spatial-temporal feature extraction in image reconstruction of spike camera\ntasks, we introduce a novel mechanism called Temporal Spike Attention (TSA).\nThis design is specifically tailored to distill spatial-temporal features across mul-\ntiple frames, thereby harnessing the rich spatial-temporal information embedded\nwithin spike streams.\nGiven the middle frame features and the left and right adjacent frame features\nof size $C \\times H \\times W$, they are first partitioned into local windows of size $M \\times M$,\nthen TSA is calculated among the middle frame window features $X_m \\in \\mathbb{R}^{M^2 \\times C}$\nand the left and right adjacent frame window features $X_l, X_r \\in \\mathbb{R}^{M^2 \\times C}$, $M^2$ is\nthe number of window feature elements and C is the channel number of feature.\nWe compute the Query Q, Key K, Value V from $X_l, X_m, X_r$, by linear projection\nas:\n$Q = X_lP_l, K = X_rP_r,V = X_mP_m,$\n(4)\nwhere $P_l, P_m, P_r \\in \\mathbb{R}^{M^2 \\times D}$ are projection matrices. D is the channel number of\nprojected features. Then we use Q to query K in order to generate the attention\nmap $A = \\text{Soft Max}(\\frac{Q K^T}{\\sqrt{D}}) \\in \\mathbb{R}^{M^2 \\times M^2}$, which is then used for weighted\nsum of V, This is formulated as:\n$\\text{TSA}(Q, K, V) = \\text{SoftMax} (\\frac{Q K^T}{\\sqrt{D}}) V,$\n(5)\nwhere SoftMax($\\cdot$) means the row softmax operation.\nFrom the above process, we know that the TSA mechanism is adept at cap-\nturing the spatial-temporal dynamics inherent in spike streams. As illustrated in\nfig. 4, by computing the query and key from the adjacent frames, TSA is able to\nassess the spatial-temporal correlation between these frames, which is critical for\nthe accurate reconstruction of the middle frame. More specifically, the attention\nmap A, generated from the interaction between Q and K, reflects the degree\nof temporal similarity between the left and right frames. The updated middle\nframe feature representation $Y^U$ is computed as:\n$Y^U = \\sum_{i=1}^{N} A_i V_i,$\n(6)\nwhere $A_i$ is the attention weight and $V_i$ is the value matrix. This formulation\nallows TSA to perform a weighted aggregation of features, where the weights are\ndetermined by the temporal correlation between the adjacent frames. By focusing\non the temporal consistency between frames, TSA can effectively reconstruct\nthe middle frame even in the presence of significant temporal sparsity. This\nis a distinct advantage over traditional attention mechanisms, which do not\naccount for the temporal aspect of the spike streams and may struggle with the"}, {"title": "4.4 Loss Function", "content": "Given that our network simultaneously reconstructs the dynamic scene of the\nmiddle spike frame and its adjacent spike frames, while effectively utilizing the\ninter-frame information in the reconstruction process, we employ a two-part loss\nfunction to optimize our network, which is formulated as:\n$\\begin{aligned}\n\\text{Loss} &= L_{adj} + L_{mid} \\\\\nL_{adj} &= ||I_l - G_l||_1 + ||I_r - G_r||_1, \\\\\nL_{mid} &= ||I_m - G_m||_1\n\\end{aligned},$\n(7)\nwhere $I_i$ is the reconstructed dynamic scene of frame i, $G_i$ is the ground truth\nimage of frame i and A is the parameter balancing these two losses."}, {"title": "4.5 Construction of New Dataset", "content": "To train our models, we need a dataset of spike stream inputs with correspond-\ning ground truth images. The spike-REDS dataset [31], derived from the REDS\ndataset [19], is limited by its resolution (250 \u00d7 400) and lack of high-speed mo-\ntion representation. To address this, we developed an advanced spike camera\nsimulator for 1000 \u00d7 1000 resolution. Using the X4K1000fps video dataset [23]\nfor high-resolution, fast-motion ground truth, we extract one or two 1000 \u00d7 1000\nregions per frame. We apply the Super-SloMo frame interpolation algorithm [13]"}, {"title": "5 Experiments", "content": "5.1 Datasets\nTo evaluate our network, we conduct experiments on synthesized and real-life\ndatasets with varying resolutions. For 250 \u00d7 400 resolution, models are trained on\nspike-REDS [31] and tested on spike-REDS and PKU-Spike-HighSpeed [35]. For\n1000 \u00d7 1000 resolution, models are trained and tested on spike-X4K, as shown\nin section 4.5.\n5.2 Details\nWe use 2 RSSB blocks, each with 6 SAB blocks. The temporal window sizes for\nadjacent and middle frames are 28 and 41, respectively. For training at 250 \u00d7 400\nresolution, the spatial local window size is 5 \u00d7 5, head size 2, patch size 1, and\nchannel size 96. At 1000 \u00d7 1000 resolution, the window size is 5 \u00d7 5, head size 1,\npatch size 4, and channel size 64.\nTraining is done with PyTorch using the Adam optimizer, starting with a\nlearning rate of 0.0001. Models are trained for 900 epochs, with the learning\nrate halved every 300 epochs. Training is performed on 4 NVIDIA RTX3090\nGPUs (24GB) with a batch size of 4 for 250 \u00d7 400 resolution, and on 1 NVIDIA\nTesla V100 GPU (32GB) with a batch size of 1 for 1000 \u00d7 1000 resolution."}, {"title": "5.3 Comparison With State-of-the-Art", "content": "To evaluate our proposed network SwinSF, we compare it in different datasets\nand resolutions with recent works, i.e., TFP, TFI, SpikeFormer, Spk2ImgNet,\nWGSE. To be specific, WGSE was the state-of-the-art method before this.\nQuantitative and Qualitative Comparison on the resolution of 250\n\u00d7 400. When evaluating our network at a resolution of 250 \u00d7 400, we train\nwith spike-REDS and test on both spike-REDS and the PKU-Spike-HighSpeed\nDataset. To quantitatively compare different reconstruction methods, we employ\ntwo full-reference image quality assessment (IQA) metrics, namely PSNR and\nSSIM, for synthesized data evaluation, and a no-reference IQA metric, NIQE,\nfor real-world data assessment. As depicted in table 1, our SwinSF outper-\nforms other methods on the spike-REDS dataset, achieving the highest PSNR\nat 39.34dB with a notable gain of over 0.46dB, underscoring its superior effec-\ntiveness. Our SwinSF also achieves the highest SSIM at 0.9803. Furthermore,\ntable 2 reveals that SwinSF consistently delivers the best NIQE scores on PKU-\nSpike-HighSpeed dataset.\nfigs. 5 and 6 present the reconstruction results from various methods for\nreal-world and synthesized data. The reconstructions by our SwinSF are visu-\nally superior to those of competing approaches. Our SwinSF consistently delivers\nhigh-quality reconstructions, effectively revealing crisp textures and intricate de-\ntails even within high-speed motion scenes. Due to the relatively simplistic con-\nstruction and outdated resolution of spike-REDS, we place particular emphasis\non the evaluation of spike-X4K."}, {"title": "6 Ablation Study", "content": "6.1 Effectiveness of SAB\nTo evaluate SAB's impact, we conducted ablation studies on spike-REDS. V1,\na baseline, mirrors a standard transformer layer [25] with only Multihead Self-\nAttention (MSA). V2 adopts Shifted Window Multihead Self-Attention (SW-\nMSA) [18], akin to a basic Swin Transformer layer. V3 focuses solely on Temporal\nSpike Attention (TSA), eschewing spatial attention. V4 combines MSA and TSA\nfor a dual spatial-temporal attention approach. V5, our proposed configuration\nof SAB, further builds upon V2 by adding TSA to the SW-MSA framework.\nAnalyzing table 4, V5 outperforms V2 by 0.69, while V4 exceeds the performance\nof V1 by 0.80. This confirms Temporal Spike Attention (TSA)'s effectiveness.\nV3's 0.05 improvement over V\u2081 stresses temporal information's importance in\nspike data reconstruction. V2's superiority over V\u2081 and V5 over V4 underscores"}, {"title": "6.2 Effectiveness of the scaling factor of TSA", "content": "We conduct experiments to explore the effects of the scaling factor $\\beta$ of TSA. As\npresented in the manuscript section 4.2, $\\beta$ is used to control the weight of TSA\nfeatures for feature fusion. A larger $\\beta$ means a larger weight of features extracted\nby TSA and B = 0 represents TSA is not used. As shown in table 5, the model\nwith B = 0.1 obtains the best performance. This observation suggests a nuanced\ninterplay between TSA and SW-MSA, potentially hinting at optimization chal-\nlenges when both are concurrently employed. Crucially, a minimal $\\beta$ weighting"}, {"title": "6.3 Effectiveness of the parameter of loss function", "content": "To investigate the impact of varying the parameter $\\lambda$ of the loss function, we\nestablished a range of $\\lambda$ values to assess changes in performance. As illustrated\nin table 6, it is observed that the loss function achieves optimal performance\nwhen $\\lambda$ is set to 0.1. In contrast, setting $\\lambda$ to 0.5 or 1 results in no significant\nimprovement in model performance, and may even lead to a decline."}]}