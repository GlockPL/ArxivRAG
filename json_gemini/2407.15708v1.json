{"title": "SwinSF: Image Reconstruction from Spatial-Temporal Spike Streams", "authors": ["Liangyan Jiang", "Chuang Zhu", "Yanxu Chen"], "abstract": "The spike camera, with its high temporal resolution, low latency, and high dynamic range, addresses high-speed imaging challenges like motion blur. It captures photons at each pixel independently, creating binary spike streams rich in temporal information but challenging for image reconstruction. Current algorithms, both traditional and deep learning-based, still need to be improved in the utilization of the rich temporal detail and the restoration of the details of the reconstructed image. To overcome this, we introduce Swin Spikeformer (SwinSF), a novel model for dynamic scene reconstruction from spike streams. SwinSF is composed of Spike Feature Extraction, Spatial-Temporal Feature Extraction, and Final Reconstruction Module. It combines shifted window self-attention and proposed temporal spike attention, ensuring a comprehensive feature extraction that encapsulates both spatial and temporal dynamics, leading to a more robust and accurate reconstruction of spike streams. Furthermore, we build a new synthesized dataset for spike image reconstruction which matches the resolution of the latest spike camera, ensuring its relevance and applicability to the latest developments in spike camera imaging. Experimental results demonstrate that the proposed network SwinSF sets a new benchmark, achieving state-of-the-art performance across a series of datasets, including both real-world and synthesized data across various resolutions. Our codes and proposed dataset will be available soon.", "sections": [{"title": "1 Introduction", "content": "The spike camera [7], which is an emerging neuromorphic camera, excels in high-speed motion capture with its high temporal resolution, low latency, low power consumption, and high dynamic range. These attributes are critical for high-speed imaging requirements. Aiming to solve the problems of motion blur and artifacts caused by the imaging mechanism that each pixel on the sensor of traditional digital camera is exposed synchronously in high-speed motion scenes, each pixel on the sensor of spike camera records the external absolute light intensity independently and continuously to generate spike streams with high temporal resolution. As a result, the spike camera can not only finely record the high-speed motion process, but also provide rich lighting information to reconstruct the texture details in the scene.\nHowever, converting binary spike streams to images requires specialized algorithms. Traditional methods [35,34,33] perform basic reconstructions but struggle with noise and motion blur. Deep learning, notably CNNs and transformers, has advanced spike camera image reconstruction significantly. For example, Spk2ImgNet [31] directly reconstructs images but may miss global context. SpikeFormer [22], leveraging transformers, excels but demands high computational resources for large images. WGSE [12], using discrete wavelet transforms, struggles with nonlinear data patterns. As depicted in fig. 1, none fully restores image details, highlighting the need to exploit spatial-temporal features in spike streams effectively.\nIn this paper, we propose Swin Spikeformer(SwinSF), a spike camera image reconstruction model designed to reconstruct dynamic scenes clearly from spike streams. In detail, SwinSF is made up of three main modules: Spike Feature Extraction Module, Spatial-Temporal Feature Extraction Module and Final Reconstruction Module. The Spike Feature Extraction Module uses convolution"}, {"title": "2 Related Works", "content": "Spike cameras' advancement has spurred various applications, with image reconstruction being pivotal. Recent methods, like TFI [35], TFP [35], Spk2ImgNet [31], SpikeFormer [22], and WGSE [12], have emerged. TFI leverages inter-spike intervals (ISIs) for reconstruction, foundational yet limited by quantization and dark current issues. TFP uses a moving time window to reduce noise but risks blur in fast scenes. Spk2ImgNet and SpikeFormer apply CNNs and transformers, respectively, for direct reconstruction, improving visuals. WGSE innovates with time-frequency analysis through discrete wavelet transforms. Despite progress, spatial-temporal representation learning remains an area for enhancement."}, {"title": "2.2 Vision Transformers", "content": "Recently, the Transformer architecture [25] has gained significant interest in computer vision due to its NLP successes. Many transformer-based methods [18,6,8,11,15,26,9] have been proposed for tasks like image classification [18,15,21,24], segmentation [26,2,10,27], and object detection [6,4,1]. While effective at capturing long-range dependencies [9,20], studies [14,28,29] suggest combining transformers with convolutions for better results. Transformers have also been applied to low-level tasks like image and video super-resolution [3,16,17,5], deblurring [32], and denoising [30]. However, these methods are not directly applicable to spike camera data, and their use in this field remains underexplored. This study aims to propose a transformer-based method for reconstructing high-quality images from continuous spike streams."}, {"title": "3 Preliminaries: Mechanisms of Spike Camera", "content": "The core of spike camera sensor is an asynchronous, persistently active pixel array. Each pixel independently accumulates photons; upon reaching a set threshold, it triggers a spike signal, resetting the count. This mechanism cycles continuously. Mathematically, this process is expressed as:\n$A(i, j, t) = \\int_{t_{pre}}^{t} \\alpha I(i, j, t) dt \\mod \\Theta,$\nwhere $A(i, j, t)$ denotes photon count at pixel $(i, j)$ at time $t$, $\\alpha$ is the photoelectric rate, $I(i, j, t)$ is incident photon intensity, $\\Theta$ is the threshold.\nTheoretically, pixels fire spikes anytime thresholds are met, but physically, the spike camera samples photon counts at high frequency, outputting discrete-time signals $S(i,j,t)$ at intervals $t = nT$, where $T$ is a short interval of microseconds. If a pixel $(i, j)$ meets the threshold at time $t = nT$, it reads out $S(i, j, t) = 1$ and resets the count for the next cyclicality. Otherwise, it reads out $S(i, j, t) = 0$. The above can be expressed as:\n$S(i, j,t) =\\begin{cases} 1 & \\text{if } A(i, j, t) \\geq \\Theta \\\\ 0 & \\text{if } A(i, j,t) <\\Theta \\end{cases}$"}, {"title": "4 Method", "content": "To fully learn spatial-temporal representation and reconstruct the dynamic scene from the spike streams, we propose a spike camera image reconstruction model called Swin Spikeformer(SwinSF). As shown in fig. 3, our network architecture is composed of three integral components: a Spike Feature Extraction Module, a Spatial-Temporal Feature Extraction Module, and a Final Reconstruction"}, {"title": "4.1 Overall", "content": "Module. This design draws upon the foundational elements established in prior research [18], yet innovates through its unique integration of these components.\nSpecifically, given a binary consecutive spike stream $S\\in B^{T\\times H\\times W}$ as input, we first split the spike stream into three parts in chronological order representing the left frame, middle frame, and right frame, respectively. Then, we extract the spike features $F_{sl},F_{sm},F_{sr} \\in R^{C\\times H\\times W}$ of each part by exploiting convolution layers, where $T$ denotes the temporal depth of the spike stream, $C$ denotes the channel number of the spike feature, and $H$ and $W$ denote the spatial dimensions. These extracted features are subsequently concatenated along the channel dimension, yielding $F_s \\in R^{3\\times C\\times H\\times W}$. Subsequently, a series of Residual Swin Spikeformer Blocks (RSSB) are utilized to perform the Spatial-Temporal Feature Extraction. Afterward, we add a global residual connection to fuse the concatenated spike features $F_s$ with spatial-temporal features $F_{ST} \\in R^{3\\times C\\times H\\times W}$. Finally, we reconstruct high-quality images via the Final Reconstruction module consisting of convolution layers, with each image representing the left frame, middle frame, and right frame respectively. As described in fig. 3, each RSSB contains several Spike Attention Blocks(SAB) and a convolution layer with a residual connection."}, {"title": "4.2 Spike Attention Block(SAB)", "content": "As depicted in fig. 2, the special imaging mechanism of the spike camera leads to a special dependence on the long-distance spatial-temporal information in the process of image reconstruction. Therefore, we design the Temporal Spike Attention(TSA) mechanism for spike streams to enhance the long-distance spatial-temporal information extraction ability of the network. The TSA block is inserted into the standard Swin Transformer Block [18], following the initial LayerNorm (LN) layer and in parallel with the shifted window-based multi-head self-attention (SW-MSA) module, as illustrated in fig. 3. The SW-MSA is employed periodically within consecutive TSA blocks, following a similar approach as described in [18,17,5]. To reconcile the optimization and visual representation discrepancies between TSA and SW-MSA, we introduce a small constant $\u03b2$ as a scaling factor to the output of TSA. This adjustment harmonizes the interplay of the attention mechanisms, ensuring a more cohesive learning dynamic. The whole process of SAB is computed as:\n$X_l, X_m, X_r = LN(Split(X))$\n$Y_m = SW-MSA(X_m) + \\beta TSA([X_l, X_m, X_r]) + X_m$\n$Y_m = MLP(LN(Y_m) + Y_m)$\n$Y = Concat(X_l, Y_m, X_r)$\n, where $X$ denote the input feature, $X_l, X_m, X_r,$ are three features split by X, representing the features at frames l,m, and r respectively. $Y_{ms}$ and $Y_{mt}$ represent the intermediate features computed by the SW-MSA and TSA, and $Y$ is the output of the SAB."}, {"title": "4.3 Temporal Spike Attention (TSA)", "content": "To address the inherent limitation of SW-MSA in capturing solely the spatial visual representations within an isolated frame, and acknowledging the necessity for spatial-temporal feature extraction in image reconstruction of spike camera tasks, we introduce a novel mechanism called Temporal Spike Attention (TSA). This design is specifically tailored to distill spatial-temporal features across multiple frames, thereby harnessing the rich spatial-temporal information embedded within spike streams.\nGiven the middle frame features and the left and right adjacent frame features of size $C \u00d7 H \u00d7 W$, they are first partitioned into local windows of size $M \u00d7 M$, then TSA is calculated among the middle frame window features $X_m \u2208 R^{M^2\u00d7C}$ and the left and right adjacent frame window features $X_l, X_r \u2208 R^{M^2\u00d7C}$, $M^2$ is the number of window feature elements and C is the channel number of feature. We compute the Query Q, Key K, Value V from $X_l, X_m, X_r,$ by linear projection as:\n$Q = X_lP_l, K = X_rP_r, V = X_mP_m,$\nwhere $P_l, P_m, P_r \u2208 R^{M^2\u00d7D}$ are projection matrices. D is the channel number of projected features. Then we use $Q$ to query $K$ in order to generate the attention map $A = Soft Max\\left( \\frac{QK^T}{\\sqrt{D}}\\right) \u2208 R^{M^2\u00d7M^2}$, which is then used for weighted sum of $V$, This is formulated as:\n$TSA(Q, K, V) = SoftMax \\left( \\frac{QK^T}{\\sqrt{D}} \\right) V,$\nwhere SoftMax(\u00b7) means the row softmax operation.\nFrom the above process, we know that the TSA mechanism is adept at capturing the spatial-temporal dynamics inherent in spike streams. As illustrated in fig. 4, by computing the query and key from the adjacent frames, TSA is able to assess the spatial-temporal correlation between these frames, which is critical for the accurate reconstruction of the middle frame. More specifically, the attention map $A$, generated from the interaction between $Q$ and $K$, reflects the degree of temporal similarity between the left and right frames. The updated middle frame feature representation $Y^U$ is computed as:\n$Y^U = \\sum_{i=1}^{N} A_iV_i,$\nwhere $A_i$ is the attention weight and $V_i$ is the value matrix. This formulation allows TSA to perform a weighted aggregation of features, where the weights are determined by the temporal correlation between the adjacent frames. By focusing on the temporal consistency between frames, TSA can effectively reconstruct the middle frame even in the presence of significant temporal sparsity. This is a distinct advantage over traditional attention mechanisms, which do not account for the temporal aspect of the spike streams and may struggle with the"}, {"title": "4.4 Loss Function", "content": "Given that our network simultaneously reconstructs the dynamic scene of the middle spike frame and its adjacent spike frames, while effectively utilizing the inter-frame information in the reconstruction process, we employ a two-part loss function to optimize our network, which is formulated as:\n$\\mathcal{L}oss = \\mathcal{L}_{adj} + \\mathcal{L}_{mid}$\n$\\mathcal{L}_{adj} = ||I_l - G_l||_1 + ||I_r - G_r||_1,$\n$\\mathcal{L}_{mid} = ||I_m - G_m||_1$\nwhere $I_i$ is the reconstructed dynamic scene of frame i, $G_i$ is the ground truth image of frame i and A is the parameter balancing these two losses."}, {"title": "4.5 Construction of New Dataset", "content": "To train our models, we need a dataset of spike stream inputs with corresponding ground truth images. The spike-REDS dataset [31], derived from the REDS dataset [19], is limited by its resolution (250 \u00d7 400) and lack of high-speed motion representation. To address this, we developed an advanced spike camera simulator for 1000 \u00d7 1000 resolution. Using the X4K1000fps video dataset [23] for high-resolution, fast-motion ground truth, we extract one or two 1000 \u00d7 1000 regions per frame. We apply the Super-SloMo frame interpolation algorithm [13]"}, {"title": "5 Experiments", "content": "To evaluate our network, we conduct experiments on synthesized and real-life datasets with varying resolutions. For 250 \u00d7 400 resolution, models are trained on spike-REDS [31] and tested on spike-REDS and PKU-Spike-HighSpeed [35]. For 1000 \u00d7 1000 resolution, models are trained and tested on spike-X4K, as shown in section 4.5."}, {"title": "5.1 Datasets", "content": "To evaluate our network, we conduct experiments on synthesized and real-life datasets with varying resolutions. For 250 \u00d7 400 resolution, models are trained on spike-REDS [31] and tested on spike-REDS and PKU-Spike-HighSpeed [35]. For 1000 \u00d7 1000 resolution, models are trained and tested on spike-X4K, as shown in section 4.5."}, {"title": "5.2 Details", "content": "We use 2 RSSB blocks, each with 6 SAB blocks. The temporal window sizes for adjacent and middle frames are 28 and 41, respectively. For training at 250 \u00d7 400 resolution, the spatial local window size is 5 \u00d7 5, head size 2, patch size 1, and channel size 96. At 1000 \u00d7 1000 resolution, the window size is 5 \u00d7 5, head size 1, patch size 4, and channel size 64.\nTraining is done with PyTorch using the Adam optimizer, starting with a learning rate of 0.0001. Models are trained for 900 epochs, with the learning rate halved every 300 epochs. Training is performed on 4 NVIDIA RTX3090 GPUs (24GB) with a batch size of 4 for 250 \u00d7 400 resolution, and on 1 NVIDIA Tesla V100 GPU (32GB) with a batch size of 1 for 1000 \u00d7 1000 resolution."}, {"title": "5.3 Comparison With State-of-the-Art", "content": "To evaluate our proposed network SwinSF, we compare it in different datasets and resolutions with recent works, i.e., TFP, TFI, SpikeFormer, Spk2ImgNet, WGSE. To be specific, WGSE was the state-of-the-art method before this.\nQuantitative and Qualitative Comparison on the resolution of 250 \u00d7 400. When evaluating our network at a resolution of 250 \u00d7 400, we train with spike-REDS and test on both spike-REDS and the PKU-Spike-HighSpeed Dataset. To quantitatively compare different reconstruction methods, we employ two full-reference image quality assessment (IQA) metrics, namely PSNR and SSIM, for synthesized data evaluation, and a no-reference IQA metric, NIQE, for real-world data assessment. As depicted in table 1, our SwinSF outperforms other methods on the spike-REDS dataset, achieving the highest PSNR at 39.34dB with a notable gain of over 0.46dB, underscoring its superior effectiveness. Our SwinSF also achieves the highest SSIM at 0.9803. Furthermore, table 2 reveals that SwinSF consistently delivers the best NIQE scores on PKU-Spike-HighSpeed dataset.\nfigs. 5 and 6 present the reconstruction results from various methods for real-world and synthesized data. The reconstructions by our SwinSF are visually superior to those of competing approaches. Our SwinSF consistently delivers high-quality reconstructions, effectively revealing crisp textures and intricate details even within high-speed motion scenes. Due to the relatively simplistic construction and outdated resolution of spike-REDS, we place particular emphasis on the evaluation of spike-X4K."}, {"title": "6 Ablation Study", "content": "To evaluate SAB's impact, we conducted ablation studies on spike-REDS. V1, a baseline, mirrors a standard transformer layer [25] with only Multihead Self-Attention (MSA). V2 adopts Shifted Window Multihead Self-Attention (SW-MSA) [18], akin to a basic Swin Transformer layer. V3 focuses solely on Temporal Spike Attention (TSA), eschewing spatial attention. V4 combines MSA and TSA for a dual spatial-temporal attention approach. V5, our proposed configuration of SAB, further builds upon V2 by adding TSA to the SW-MSA framework.\nAnalyzing table 4, V5 outperforms V2 by 0.69, while V4 exceeds the performance of V1 by 0.80. This confirms Temporal Spike Attention (TSA)'s effectiveness. V3's 0.05 improvement over V\u2081 stresses temporal information's importance in spike data reconstruction. V2's superiority over V\u2081 and V5 over V4 underscores"}, {"title": "6.1 Effectiveness of SAB", "content": "To evaluate SAB's impact, we conducted ablation studies on spike-REDS. V1, a baseline, mirrors a standard transformer layer [25] with only Multihead Self-Attention (MSA). V2 adopts Shifted Window Multihead Self-Attention (SW-MSA) [18], akin to a basic Swin Transformer layer. V3 focuses solely on Temporal Spike Attention (TSA), eschewing spatial attention. V4 combines MSA and TSA for a dual spatial-temporal attention approach. V5, our proposed configuration of SAB, further builds upon V2 by adding TSA to the SW-MSA framework.\nAnalyzing table 4, V5 outperforms V2 by 0.69, while V4 exceeds the performance of V1 by 0.80. This confirms Temporal Spike Attention (TSA)'s effectiveness. V3's 0.05 improvement over V\u2081 stresses temporal information's importance in spike data reconstruction. V2's superiority over V\u2081 and V5 over V4 underscores the Swin Transformer's [18] positive effect. It highlights the Swin Transformer's capacity to boost performance and collaborate with TSA for spatial-temporal information mining."}, {"title": "6.2 Effectiveness of the scaling factor of TSA", "content": "We conduct experiments to explore the effects of the scaling factor $\u03b2$ of TSA. As presented in the manuscript section 4.2, $\u03b2$ is used to control the weight of TSA features for feature fusion. A larger $\u03b2$ means a larger weight of features extracted by TSA and B = 0 represents TSA is not used. As shown in table 5, the model with B = 0.1 obtains the best performance. This observation suggests a nuanced interplay between TSA and SW-MSA, potentially hinting at optimization challenges when both are concurrently employed. Crucially, a minimal $\u03b2$ weighting for the TSA component appears to mitigate these challenges, facilitating a more harmonious integration of temporal and spatial attention mechanisms."}, {"title": "6.3 Effectiveness of the parameter of loss function", "content": "To investigate the impact of varying the parameter A of the loss function, we established a range of A values to assess changes in performance. As illustrated in table 6, it is observed that the loss function achieves optimal performance when A is set to 0.1. In contrast, setting A to 0.5 or 1 results in no significant improvement in model performance, and may even lead to a decline."}]}