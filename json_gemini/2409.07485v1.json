{"title": "Optimization and Deployment of Deep Neural Networks for PPG-based Blood Pressure Estimation Targeting Low-power Wearables", "authors": ["Alessio Burrello", "Francesco Carlucci", "Giovanni Pollo", "Xiaying Wang", "Massimo Poncino", "Enrico Macii", "Luca Benini", "Daniele Jahier Pagliari"], "abstract": "PPG-based Blood Pressure (BP) estimation is a challenging biosignal processing task for low-power devices such as wearables. State-of-the-art Deep Neural Networks (DNNs) trained for this task implement either a PPG-to-BP signal-to-signal reconstruction or a scalar BP value regression and have been shown to outperform classic methods on the largest and most complex public datasets. However, these models often require excessive parameter storage or computational effort for wearable deployment, exceeding the available memory or incurring too high latency and energy consumption. In this work, we describe a fully-automated DNN design pipeline, encompassing HW-aware Neural Architecture Search (NAS) and Quantization, thanks to which we derive accurate yet lightweight models, that can be deployed on an ultra-low-power multicore System-on-Chip (SoC), GAP8. Starting from both regression and signal-to-signal state-of-the-art models on four public datasets, we obtain optimized versions that achieve up to 4.99% lower error or 73.36% lower size at iso-error. Noteworthy, while the most accurate SoA network on the largest dataset can not fit the GAP8 memory, all our optimized models can; our most accurate DNN consumes as little as 0.37 mJ while reaching the lowest MAE of 8.08 on Diastolic BP estimation.", "sections": [{"title": "I. INTRODUCTION AND RELATED WORKS", "content": "Blood pressure (BP) is a crucial health parameter that necessitates continuous monitoring for a large population of vulnerable individuals, being linked to various heart-related diseases, such as hypertension, cardiomyopathy, and heart failure [1]. Various monitoring solutions exist, from cuffless to invasive procedures, but wearable technologies such as smart-watches would enable non-invasive monitoring of larger coorts of individuals at an affordable cost and without affecting their normal lives, thereby contributing to saving many lives.. In this domain, one of the most common monitoring techniques relies on Photoplethysmography (PPG).\nPPG uses a light-emitting diode (LED) to illuminate the skin. A photodiode then collects the reflected light, whose intensity depends on the blood volume variation due to heart activity [2]. While various medically relevant parameters can be derived from PPG, including heart rate (HR) and respiratory rate, this paper focuses on its usage for the estimation of Systolic Blood Pressure (SBP) and Diastolic Blood Pressure (DBP), reflecting the blood pressure during and in between heart muscle contractions, respectively.\nA broad set of machine learning techniques have been employed in the literature for this task, ranging from classical methods like Random Forest (RF) [3], and Support Vector Regression (SVR) [4] to Deep Neural Networks (DNNs) [5], [6]. Further, ensemble learning frameworks have been proposed to reduce the risk of overfitting [4], [7]. In comparison to classic methods, DNNs offer the advantage of not needing an often expensive feature extraction process and have been shown to generalize better on unseen data in several biosignal pro- cessing tasks [8]\u2013[10]. Several DNN architectures have been considered for PPG-based BP estimation [5], [6], [11], [12], with most recent works focusing on 1D Convolutional Neural Networks (CNNs) [10]. Some works train these networks as regressors to directly predict a scalar DBP or SBP value based on a time window of PPG readings. ResNet-like networks achieve state-of-the-art performance in this category [13]. Others adopt a signal-to-signal (sig2sig) approach, where the CNN is tasked to reconstruct the entire DPB/SBP time series starting from the PPG one. In this group, architectures based on UNet [14] are the best-performing ones.\nHowever, existing deep learning models for BP estima- tion have a large number of parameters and high computa- tional complexity. When pursuing continuous monitoring on resource-constrained, low-power devices such as wearables, those models either exceed the available memory or incur excessive latency and energy consumption.\nThis paper attempts to mitigate this issue through the use of a fully automated DNN design pipeline, which encompasses two main optimization steps, i.e., Neural Architecture Search (NAS) [15] and Quantization [16]. Starting from state-of-the- art regression and sig2sig CNNs, we first apply a gradient- based NAS to automatically select each layer's operation from a pool, and tune the network depth, discovering architectures that balance BP prediction error and model size. We then select a subset of the Pareto-optimal DNNs identified by the NAS and quantize them to int8 precision to further reduce their size, latency, and energy consumption. Lastly, we employ a DNN compiler [17] to automatically convert the quantized models to optimized C code, targeting GreenWaves' GAP8 [18], an ultra- low-power System-on-Chip (SoC) suitable to be embedded in a wearable device for practical, efficient and continuous BP monitoring. To the best of our knowledge, ours is the first work"}, {"title": "II. MATERIALS & METHODS", "content": "A. Blood Pressure Estimation using PPG Signal\nBlood pressure monitoring techniques can be continuous or intermittent and invasive or non-invasive. The invasive mon- itoring, usually performed through an intra-arterial catheter [23], directly measures the atrial blood pressure signal (ABP). Common cuff-based methods like sphygmomanometer, al- though being gold standard and minimally invasive, are cum- bersome and don't allow continuous monitoring.\nOn the other hand, PPG optical signal is strongly related to changes in blood volumes, and its effectiveness is already proven in various clinical applications. Although PPG is morphologically similar to ABP, and various studies have shown how the two signals share most of the informative features [24], extracting a blood pressure estimation from it is not a trivial task, given that the signal is subject to artifacts related to movements or air between the sensor and the skin.\nB. Datasets\nIn our study, we adopt the same four datasets, data pre- processing, and training protocols as the extensive survey in [25], which is state-of-the-art for this task. All datasets are resampled to 125 Hz. PPGBP [20] is the smallest dataset, with 619 total PPG segments, each lasting 2.1s, but it involves a large number of patients (218) with different cardiovascular diseases. BCG [19] is a bed-based ballistocardiography dataset comprising around 4 hours of cumulative measurements on 40 individuals, split into 5s windows. Sensors [21] is a subset of the MIMIC III dataset, comprising 11102 non-overlapping 5s data segments from 1195 patients. Lastly, UCI [22] is a subset of the MIMIC II waveform. It's considerably bigger than all the others, with \u2248411k segments from an unknown number of subjects. All datasets include only measurements on resting patients in a clinical setting. Therefore, motion artifact removal using acceleration data [8], [26] can be neglected. Along with PPG signals, BCG, UCI, and Sensors provide the complete blood pressure time series as ground truth for prediction. PPGBP, instead, only includes two scalar values (SBP and DBP) per sample. Thus, sig2sig models cannot be trained on this dataset. All model performances are evaluated using the test set MAE on SBP and DBP separately. The training protocol uses a 5-fold per-subject Cross-Validation for all datasets except UCI. Given its size, single-held-out validation and test sets are adopted for UCI.\nNotably, cross-patient inference following these protocols yields significantly higher estimation errors than medical- grade device requirements, which can only be reached through personalized fine-tuning [27]. However, this additional training is orthogonal to our work, which aims to demonstrate the feasibility of deploying efficient DNNs for BP estimation onboard wearable hardware.\nC. Network Optimization\nTo optimize our DNNs, we leverage the open-source library PLINIO [28], which provides an easy-to-use interface to im- plement various NAS and Quantization algorithms. Together with the training dataset, preprocessed and windowed as discussed above, the other key input of the pipeline is a seed network, i.e., an initial DNN, which serves as a blueprint to generate optimized models. We use the two best-performing DNNs from [25] as seeds. Both are 1D CNNs, but while the first belongs to the scalar regressors category and is derived from a ResNet [13], the second is a UNet-like [14] sig2sig model. We refer the reader to [25] for further details on the seeds.\nNotably, these two architectures have already been opti- mized in [25] for each dataset, albeit only for maximizing accuracy. In contrast, in our work, we perform cost-aware optimizations, showing that this permits us to find similarly or better performing models, which are additionally smaller and more efficient.\n1) NAS: For each seed, the first optimization step consists of the application of a gradient-based NAS called SuperNet, inspired from [29], whose working principle is depicted in Fig. 3. This method replaces each convolutional layer in the seeds with a pool of alternatives, all receiving the same input. The output of each layer is obtained as a linear combination of the various alternatives' outputs, weighted by softmax-ed trainable parameters \u03b8\u1d62 (Fig. 3b). Intuitively, finding a good architecture corresponds to setting, for each layer, one of the \u03b8\u1d62 = 1 (and the others 0). The NAS solves a continuous relaxation of this problem by inserting the multi-path DNN"}, {"title": "d) Output", "content": "in a standard training loop, where both the normal network weights W and the newly added @ are optimized jointly by gradient descent. This training uses the modified loss function shown in Fig. 3c, where L is the standard task loss, i.e., in our case, the Mean Squared Error (MSE) between the network's output f(W, X) and the ground truth \u00dd. The newly added term R, instead, is the expected cost of the network as a function of the layer selection parameters. An example of its calculation is shown in Fig. 3c. In this work, we use model size as a cost metric. At the end of the training, the output architecture is generated by selecting, for each layer, the alternative associated with the largest \u03b8\u1d62. Varying the scalar regularization strength \u03bb, which controls the balance between the two loss terms, allows the generation of multiple output DNNs with different error vs cost trade-offs.\nIn our work, we use the SuperNet to select, for each layer, between a standard 1D convolution (C), a Depthwise-separable block (DW), and an identity operation (ID). The original mod- els only include standard convolutions. The DW block, made of a sequence of a depthwise convolution and a pointwise layer, was first made popular by [30], and has been shown to provide a lower-size yet similar-accuracy approximation of standard convolutions, leading to tiny yet capable networks. The ID, instead, is added only when input and output tensors have the same shape, and lets the NAS modulate the network depth by skipping some layers.\n2) Quantization: In a second optimization step, we select some of the Pareto-optimal DNNs generated by the NAS and quantize them to int8 format. For this, we use PLINIO's Quantization-Aware Training (QAT) capabilities [16]. We use a standard min-max affine quantization format for weights and the Parametrized Clipping Activation (PaCT) method for layer's inputs and outputs [31]. Accumulation and biases are on 32 bits, as supported by our target inference library [32].\nNote that the adopted NAS and quantization methods are not new per se. However, to our knowledge, we are the first to apply HW-aware optimizations for BP estimation.\nD. Network Deployment\nWe deploy our networks on the GreenWaves GAP8 [18], a low-power, RISC-V-based multi-core IoT processor designed specifically for signal processing tasks on edge devices. GAP8 features a cluster of eight general-purpose cores used to accelerate compute-intense workloads. It also includes a 2- level scratchpad memory, with 512 kB of main memory, used to store the application code and DNN weights, and a 64 kB last-level cache with single-clock access latency for the cluster. A DMA engine moves the data between memory levels.\nTo convert our optimized DNNs into inference code for GAP8, we employ the DORY compiler [17]. DORY au- tomatically generates C code that handles the entire infer- ence process, including memory management, DMA trans- fers scheduling, and optimized AI primitives invocation. It can directly take as input quantized DNNs generated by PLINIO. As backend library for implementing each layer, we use [32]. We profile our deployed models on the GAP8 evaluation board, utilizing the internal performance counters for measuring latency, and the Nordic Power Profiler Kit II for power [33]."}, {"title": "III. RESULTS", "content": "We performed all trainings using the Adam optimizer with a learning rate of 0.001 for the network weights and a separate Adam optimizer with a learning rate of 0.01 for the NAS parameters (0). In each epoch, the network weights were optimized on the training set, while the NAS parameters were optimized on the validation set, as in [29]. For all datasets and both ResNet and UNet seeds, we tested 18 different values of \u03bb, evenly spaced on a log scale between 10\u207b\u00b9\u00b9 and 10\u207b\u2077. We compared our optimized models with the original ResNet and UNet from [25], as well as with two classic model families also considered in that paper, namely Random Forests (RFs) and Support Vector Regressors (SVR).\nA. Pareto Analysis\nFig. 4 depicts the results of NAS on all four datasets and for DBP and SBP prediction. All results are reported in a MAE vs model size (n. of parameters) plane. Red and green diamonds correspond to the optimization seeds (ResNet and UNet from [25] respectively). Correspondingly colored circles are the Pareto-optimal architectures found with NAS. All results refer to floating point DNNs, before quantization.\nOn all datasets, we obtain models that either dominate the seeds or are on the memory vs error Pareto front. On the smallest one, PPGBP, we obtain a rich curve of Pareto architectures starting from the ResNet. We are able to reduce the seed size by 16%, with a small increase in the MAE of only 3.9% and 3.5% on DBP and SBP prediction, respectively. As mentioned, we do not have results with the UNet seed for PPGBP, given that this dataset does not include the full BP signal ground truth. On BCG, we Pareto-dominate both seed networks, improving both MAE and size. Our best UNet- derived model obtains 11.139 mmHg MAE on SBP prediction and 7.52 mmHg MAE on DBP, being 6.7%/4.7% better than the best seed (ResNet). Simultaneously, this network reduces the total number of parameters by 3.8\u00d7. However, it shall be noted that for these two datasets, classical ML methods still outperform our optimized DNNs in both performance and size, as reported in [25]. SVR achieves the lowest MAE in DBP estimation for both PPGBP and BCG datasets (8.04 and 7.34 mmHg, respectively) and a MAE of 13.15 mmHg and 11.45 mmHg on SBP estimation, being outperformed by the UNet, solely on BCG."}, {"title": "IV. CONCLUSION", "content": "The efficient execution of PPG-based BP estimation algo- rithms is critical for the prevention of important diseases as- sociated, for instance, to hypertension. With our experiments, we demonstrated the possibility of embedding accurate DNN models on low-power wearable-class devices, achieving SoA performance. Future work will focus on the fine-tuning of our models on patient-specific data to reach competitive accuracy with the golden standard of non-intrusive PB."}]}