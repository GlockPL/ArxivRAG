{"title": "PERCORE: A Deep Learning-Based Framework for Persian Spelling Correction with Phonetic Analysis", "authors": ["Seyed Mohammad Sadegh Dashti", "Amid Khatibi Bardsiri", "Mehdi Jafari Shahbazzadeh"], "abstract": "This research introduces a state-of-the-art Persian spelling correction system that seamlessly integrates deep learning techniques with phonetic analysis, significantly enhancing the accuracy and efficiency of natural language processing (NLP) for Persian. Utilizing a fine-tuned language representation model, our methodology effectively combines deep contextual analysis with phonetic insights, adeptly correcting both non-word and real-word spelling errors. This strategy proves particularly effective in tackling the unique complexities of Persian spelling, including its elaborate morphology and the challenge of homophony. A thorough evaluation on a wide-ranging dataset confirms our system's superior performance compared to existing methods, with impressive F1-Scores of 0.890 for detecting real-word errors and 0.905 for correcting them. Additionally, the system demonstrates a strong capability in non-word error correction, achieving an F1-Score of 0.891. These results illustrate the significant benefits of incorporating phonetic insights into deep learning models for spelling correction. Our contributions not only advance Persian language processing by providing a versatile solution for a variety of NLP applications but also pave the way for future research in the field, emphasizing the critical role of phonetic analysis in developing effective spelling correction system.", "sections": [{"title": "1. Introduction", "content": "Spelling correction is an integral component of all text processing environments, particularly for languages with complex morphology and syntax like Persian. The task of spelling correction primarily involves the detection and correction of two types of errors: non-word errors and real-word errors.\nNon-word errors are instances where the misspelled words are not found in a dictionary and are meaningless. These errors often occur due to typographical mistakes or a lack of knowledge about the correct spelling of a word. While non-word errors in Persian are mostly similar to those in other languages, there are unique cases due to the specific features of the Persian language. For instance, consider the word \u201c\u0645\u06cc\u0646\u0648\u0627\u0632\u0645\u201d /min\u00e6vaz\u00e6m/ \u2018I play')\u00b9. If a user ignores the pseudo space, the word transforms to \u0645\u06cc\u0646\u0648\u0627\u0632\u0645,which might not be found in a standard dictionary. Conversely, if the user commits a split error and uses a white space instead of a pseudo space, then the word transforms into two different sequences of characters \u201c\u0645\u06cc\u201d and \u201c\u0646\u0648\u0627\u0632\u0645\u201d \u2014 neither of which can be attested in the dictionary.\nReal-word errors in spelling correction are a significant challenge. They occur when a correctly spelled word is used incorrectly in context. These errors can be due to accidental mistyping, confusion between similar sounding or meaning words [1], incorrect replacements by automated systems like AutoCorrect features [2], and misinterpretation of input by Automatic Speech Recognition (ASR) and Optical Character Recognition (OCR) systems [3-6].\nThe Persian language, with its extensive vocabulary and intricate properties, further complicates real-word error correction. Unique features of Persian such as homophony (words that sound the same but have different meanings), polysemy (words with multiple meanings), heterography (words with the same spelling but different meanings depending on pronunciation), and word boundary issues add to this complexity.\nDespite these inherent challenges, there have been concerted efforts to develop both statistical and rule-based methods for detecting and correcting both classes of errors in Persian. However, these methods have achieved only limited success. The task of word error detection and correction in Persian continues to be an active area of research. The advent of more advanced methods and tools holds promise for overcoming these challenges and enhancing the accuracy of error detection and correction. The objective of this study is to introduce an advanced method for detecting and correcting spelling errors in Persian. Our primary contributions are summarized as follows:\n\u2022\tGeneral Persian Corpus: We introduce a large general corpus of Persian text, consisting of 1.4 million formal documents.\n\u2022\tLanguage Representation Model: We present a language representation model that has been fine-tuned for the task of spelling correction. Our method utilizes the contextual score of words to determine the best replacement candidate for an error.\n\u2022\tError Generation Algorithm: We propose a novel error generation algorithm for populating original corpora of Persian text with artificially generated errors. This algorithm offers flexibility in specifying the proportion of real-word and non-word errors in a given corpus. Importantly, it can also determine the edit distance of generated errors, allowing NLP researchers to set up various test scenarios tailored to their specific research questions."}, {"title": "2. Related Works", "content": "Automatic word error correction is a pivotal component in Natural Language Processing (NLP) systems. Initial techniques were reliant on edit distance and phonetic algorithms [7-10], with subsequent enhancements demonstrating the effectiveness of incorporating context information to boost the efficiency of auto-correction systems [11]. Utilization of contextual measures such as semantic distance and noisy channel models based on N-grams has been widespread across various NLP applications [1, 2, 12-14]. Innovative strategies have been devised to correct multiple real-word errors in highly noisy contexts [15], including a notable model by Dashti for detecting and auto-correcting real-word errors within sequences containing multiple inaccuracies [16].\nThe adoption of neural word or sense embeddings, leveraging context information for spelling correction, marked a significant advancement [17]. The use of pretrained contextual embeddings for detecting and correcting real-word errors was a notable milestone [18]. By 2020, deep learning methodologies were applied to address context-sensitive spelling errors in English documents [19], with subsequent research developing a BERT-based model for similar purposes [20].\nThe rapid progression and innovation in pre-trained language models, exemplified by the development of BERT and the subsequent introduction of the GPT series\u2014including GPT-2.0, GPT-3.0, have markedly revolutionized the field of error correction. These advancements have ushered in an era of unparalleled adaptability and enhanced performance across a diverse array of languages, as documented in seminal works [21, 22]. Notably, the application of these models to English and Arabic has demonstrated their proficiency in leveraging vast knowledge bases for the effective correction of complex error types through fine-tuned, prompt-based methods [23, 24]. In parallel, the exploration and utilization of large language models for Chinese language processing, as seen with innovations like SpellBERT and methodologies that employ disentangled phonetic representations, highlight the bespoke adaptations necessary to address the unique challenges presented by Chinese orthography and phonology [25, 26]. Further contributions to the Chinese language processing field have introduced phonetic pre-training techniques [27], enhancing the model's linguistic comprehension. Additionally, NeuSpell emerges as a neural spelling correction toolkit, offering a suite of pre-trained models designed for straightforward, user-friendly implementation [28].\nIn clinical text, Tran et al. introduced a context-sensitive spelling correction model [29], and a contextual spelling correction approach tailored for end-to-end speech recognition systems was developed [30]. A multi-task detector-corrector framework for Chinese spelling correction was also proposed [31]. Liu et al. advanced Chinese spelling correction with CRASpell, a contextual"}, {"title": "2.1 Linguistic Challenges in Persian Automatic Spelling Correction", "content": "Persian, an integral member of the Indo-Iranian group within the Indo-European language family, serves as the official language in Iran, Tajikistan, and Afghanistan. Its deep historical roots contribute to a rich linguistic tapestry. This language, enriched by Arabic elements yet retaining its core structure over centuries, poses unique challenges in the field of Automatic Spelling Correction (ASC) [39, 42]. The complexity of Persian, especially apparent in its script and morphology, stands in stark contrast not only to the Latin alphabet of English, with its irregular phoneme-grapheme correspondences [43], but also to the cursive script of Arabic, which grapples with complexities in its system of vowelization [44].\nAdding to this linguistic diversity, Chinese introduces a set of challenges distinct from those of Persian, English, and Arabic. As a language with thousands of years of history, it plays a central role in East Asia, being the official language of China, Taiwan, and one of the official languages of Singapore [45]. Chinese is characterized by its logographic writing system, where each character represents a morpheme and can be a word on its own or part of a compound word [46]. This system is fundamentally different from the alphabetic and abjad systems used by English, Persian, and Arabic, respectively. The challenges in ASC for Chinese stem primarily from this logographic nature, the high degree of homophony due to its tonal system, and the absence of a clear boundary between words in written text [47]. These characteristics necessitate ASC approaches that are highly sensitive to the context and semantic content of text, as well as sophisticated algorithms for character recognition and word segmentation.\nThus, the diversity in linguistic structures across these languages-ranging from the complexity of scripts and morphology in Persian, the irregular phoneme-grapheme correspondences in English, the vowelization system in Arabic, to the logographic writing system and tonality of Chinese-underscores the necessity for ASC systems to be finely attuned to the phonetic, orthographic, and grammatical nuances of each language. This multifaceted landscape presents a compelling challenge for developing effective and nuanced ASC technologies that can accommodate the rich variety of human language."}, {"title": "3. Material and Methods", "content": "Our method detects and corrects two types of errors in Persian text: Non-word and Real-word errors. Figure 1 demonstrates the architecture of the proposed system. The system design consists of six distinct modules that exchange information through a databus. The INPUT module takes raw test corpora. The pre-processing component normalizes the text and handles word boundary issues. The error generation module generates errors based on a desired density at different edit-distance function values. The contextual analyzer module evaluates the contextual similarity within desired word sequences. Error detection uses a dictionary look-up method for non-word error detection and contextual similarity matching for real-word errors. The error correction module corrects both classes of errors using context information from fine-tuned contextual embeddings model, along with phonetic and edit-distance similarity measures. The corrected corpora or word sequence is delivered through the OUTPUT module."}, {"title": "3.1 Pre-processing step", "content": "Text pre-processing is a crucial step in most NLP applications, consisting of sentence segmentation, tokenization, normalization, and stop-word elimination. Sentence segmentation involves determining the boundaries of sentences, typically divided by punctuation marks such as periods, exclamation points, or question marks. Tokenization is the process of dividing a sentence into a sequence of terms that represent the sentence, which will be used to extract features. Normalization involves converting text into canonical forms and is an important step in Persian NLP applications, as it is in many other languages.\nOne of the key tasks in normalizing Persian text is converting pseudo and white spaces to regular forms, substituting whitespaces with zero width nonjoiners when necessary. For example, )\u2018\u0645\u06cc \u0628\u06cc\u0646\u0645 /mibin\u00e6m/ \u2018see\u201d) is replaced with )\u2018\u0645\u06cc\u0628\u06cc\u0646\u0645\u2019 / mibin\u00e6m / \u2018see\u2019). Persian and Arabic share many characteristics, and some Persian letters are often misspelled using Arabic forms. Researchers often find it beneficial to standardize these variations by replacing Arabic characters )\u06cc \u2018Y' /j/; \u06a9 \u2018k\u2019/k/;\u3002\u2018h\u2019/h/) with their Persian equivalents. For instance)\u0631\u0627\u06cc /rey/ vote) is transformed to )\u2018\u0631\u0627\u06cc /r\u00e6y/ \u2018vote'). Normalization also includes removing diacritics from Persian words; e.g. )\u062f\u0631\u0647 /d\u00e6rre/ \u2018valley') is changed to )\u2018\u062f\u0631\u0647 /d\u00e6re/ \u2018valley'). Additionally, Kashida(s( are removed from words; for instance)\u0628\u0627\u0646\u06a9 /bank/ bank') is transformed to )\u2018\u0628\u0627\u0646\u06a9 /bank/ 'bank'). To achieve the normalization goal, a dictionary including the correct typographic form of all Persian words named Dehkhoda is used to find the normal form of multi-shaped words [58]."}, {"title": "3.2 Damerau-Levenshtein distance", "content": "Our approach uses the Damerau-Levenshtein distance measure to generate non-word and real-word errors for detection and correction tasks. This measure considers insertion, deletion, substitution, and transposition of characters. For example, the Damerau-Levenshtein distance between \u201cKC\u201d and \u201cCKE\u201d is 2. It's found that around 80% of human-generated spelling errors involve these four error types [59]. Studies show that real-word errors account for about 25% to 40% of all spelling errors in English text [60, 61]. We choose an edit distance of up to 2 between the correct term and the error. When the edit distance is set to one, an average of five candidates are generated as replacements for a target context word. However, once the distance is increased to 2, the average number rises to 23. The computing time increases correspondingly. We ensure that the generated candidates are attested in the reference lexicon."}, {"title": "3.3 Error Generation", "content": "In the following two sections, we describe the error generation algorithm and its properties, as well as explain how we determine the density of errors."}, {"title": "3.3.1 Error Density", "content": "There are no publicly available Persian texts containing genuine word errors with reasonable density. Researchers address this issue by inserting artificially generated misspellings into free-running text. For instance, the authors of [62] randomly generated malapropisms into the test corpus in free-running English texts at a frequency rate of 0.005, replacing 1 word in every 200 words. Different frequency rates were used in [1, 15, 16]. The authors used an approach based on confusion sets for the Persian language [34, 63], although they did not specify the real-word error density in the test corpus. However, based on the size of the evaluation corpora and the number of test instances, it can be estimated that it amounts to about 15% of total sentences. In the evaluation of spelling correction systems, there is a unique constraint where the maximum number of errors per sentence is set to one when generating word errors, meaning that each distinct"}, {"title": "3.3.2 Error Generation Algorithm", "content": "We proposed a novel error generation algorithm for populating pre-processed corpora. Pseudocodel is used to generate errors in a corpus that is essentially free of misspellings. The method introduces artificially generated errors into the original test corpora within distances 1 and 2. The number of sentences with errors is determined by the error density E. For each target term in the text, we extract all probable occurrences of error that are within the appropriate distance in the dictionary. To produce errors and replacement candidates, the suggested model uses a large lexicon borrowed from the Vafa spell-checker for the Persian language dictionary, which contains 1,095,959 different terms. In the error generation process, we adhere to a strict rule: only those original corpora words that can be attested in the lexicon may be replaced with an instance of error, in order to avoid any Out-Of-Vocabularies(OOV).\nFurthermore, parameter D1 allows us to determine the number of errors within distance 1, with the remaining errors at distance 2 denoted as 1 \u2013 D1. For example, if there is a 1000-sentence test corpus with E = 0.20 and D1 = 0.60, then 200 sentences will contain misspellings, with 120 of them having distance 1 misspellings and 80 of them including errors within distance 2. Following the observations of [8] and [59] and keeping in mind the objective of the model, we classify word errors into two types and assign a specific density to each one. \u1e9e represents the total number of artificially generated errors in the erroneous corpora, while \u1e9e\u2081 and B2 are coefficient parameters representing the density of non-word and real-word errors, respectively. Equation 1 represents the relationship between the size of the corpora and the frequency of non-word and real-word errors.\n$\\beta_1 + \\beta_2 \\leq 1$ (1)\nOur novel error generation algorithm allows us to set up various scenarios and evaluate the robustness of our model in detecting and correcting real-word and non-word errors at the desired density and complexity."}, {"title": "3.4 Contextual embeddings", "content": "Word embeddings analyze large volumes of textual data to embed word meanings into low-dimensional vectors [64, 65]. They store valuable syntactic and semantic information [66] and are beneficial to many NLP applications [67]. However, they suffer from meaning conflation deficiency: the inability to distinguish between a word's multiple meanings.\nTo address this, state-of-the-art approaches represent specific word senses, referred to as contextual embeddings or sense representation. Contextualized word embedding methods like ELMO consider the context of the input sequence [64]. There are two ways to pre-train language representations: feature-based approaches like ELMo and fine-tuning approaches like OpenAI GPT [22]. Fine-tuning methods train a language model using massive datasets of unlabeled plain texts. The parameters of these models are then fine-tuned using task-specific data [22, 68, 69].\nHowever, pre-training an efficient language model requires significant data and computational resources [70-73]. Multilingual models have been developed for languages with identical morphology and syntactic organization. But non-Latin languages differ greatly from Latin-based languages and require a language-specific approach [74]. A similar issue exists in the Persian language. Despite the fact that some multilingual models include Persian, they may fall behind monolingual models that are specifically trained on language-specific vocabulary with larger volumes of Persian text data. To the best of our knowledge, ParsBert [75] is the only attempt to pre-train a Bidirectional Encoder Representation Transformer (BERT) [68] model for the Persian language."}, {"title": "3.4.1 PERCORE's Language Representation Model", "content": "Persian is considered an under-resourced language. Despite the existence of language models that support Persian, only one has been pre-trained on a large Persian corpus [75]. However, ParsBERT's data includes many informal documents, such as user reviews and comments, and many of the collected documents contain misspelled words, making the model unsuitable for spelling correction tasks. The absence of a high-performance language model in this field is a significant challenge. In this section, we will discuss our General Persian Corpus and the process of pre-training PERCORE's language representation model."}, {"title": "3.4.2 Data", "content": "While there are many freely available formal Persian texts, they have not been compiled into a single, error-free, large corpus. This is necessary for the purpose of pre-training a language representation model for spelling correction. As a result, to train an effective model for spelling correction in Persian, we had to gather a large collection of texts from available corpora of formal text. This corpus contains 1.4 million documents collected from various sources:\n\u2022\tBijankhan Corpus: The Bijankhan corpus\u00b2 is a collection of tagged texts, including daily news and common texts [76]. It comprises 4300 articles that have been categorized into different subject categories such as political, cultural, and others. With around 2.6 million words, this corpus provides a rich source of data for researchers working on Persian language processing"}, {"title": "3.4.3 Model Architecture", "content": "PERCORE's architecture uses the original BERTbase configuration with 12 hidden layers, 12 attention heads, 768 hidden sizes, and a total of 110M parameters. Our model has a maximum token capacity of 512. Model's architecture is shown in Figure 2.\nMany attribute BERT's success to its MLM pre-training task, where it masks or replaces tokens at random before predicting the original tokens [68]. This makes BERT well-suited to be a spelling checker, as it interprets the masked and changed tokens as misspellings. In BERT\u2019s embedding layer, each input token T\u00a1 is indexed to its embedding representation ER\u2081. Then, ER\u00a1 is forwarded to BERT's encoder layers to obtain the following representation of HR\u2081:\n$ER_i = BERT \u2013 Embedding(T_i)$ (2)\n$HR_i = BERT \u2013 Encoder(ER_i)$ (3)\nwhere ER\u00a1 and HR\u00a1 \u2208 R1*d and d is the hidden dimension. Following this, similarities between HR\u00a1 and all token embeddings will be estimated to predict the distribution of Yi over the existing vocabulary.\n$Y_i = Softmax(HR_i, E^T)$ (4)\nwhere E \u2208 RV*d and Yi \u2208 R1*V; here V represents the size of the vocabulary and E denotes the BERT embedding layer. The ith row of E corresponds to ER\u00a1 in Equation 2. The final rectification result for Ti is the Tk token, whose ERk has the highest similarity to HR\u2081."}, {"title": "3.4.5 Fine-tuning for Spelling Correction Task", "content": "We fine-tuned the language representation model on the Persian spelling correction task to achieve maximum performance for spelling correction. To fine-tune the model, we used TestSet1 from the Hamshahri corpus, which contains 103,840 sentences from the reserved articles. As input to the model, we used a single sentence that ended with a full stop. We only took one sentence at a time because our focus was on training the model for spelling correction. A closer examination of the test set indicated that several sentences were short, and masking a few tokens would eliminate a substantial amount of context. As a result, sentences with fewer than 20 words were excluded from the corpus. Finally, 91,420 sentences with a minimum length of 20 were"}, {"title": "3.5 Persian Soundex", "content": "Soundex is a phonetic algorithm that indexes names based on their pronunciation in English [80]. It is widely recognized as one of the most popular phonetic algorithms. The primary objective of Soundex is to encode homophones with identical representations, allowing them to be matched despite minor variations in spelling.\nIn the case of the Persian language, there are 32 alphabets (in written form) that were divided into 14 categories based on their sounds. This classification is informed by extensive research in the field of Persian phonology [81, 82]. Our proposed code length for Persian is fixed at 4 characters. In our approach, Persian alphabet elements are grouped based on their pronunciation, with characters that have identical pronunciation being placed in the same classes, as shown in Table 2. When assigning codes, we ignored vowels. Some alphabets, such as \u201cI\u201d, and \"\u06cc\", can function as both a vowel and a consonant in Persian. When these letters appear at the beginning of a word, they act as consonants. As a result, syllabification is necessary to determine whether a letter is functioning as a vowel or a consonant."}, {"title": "3.6 Error Detection Module", "content": "The error detection module employs two distinct methodologies, depending on the type of error being detected. Lexical lookup is used for non-word errors, while contextual analysis is used for real-word errors. The first step in error detection, regardless of the type of error, is boundary detection and token identification. When the model receives a sentence S as input, it first marks the beginning and end of the sentence with Beginning of Sentence (BoS) and End of Sentence (EoS) markers, respectively, and estimates the number of tokens in the sentence:\n< BoS > T1 T2 T3 ... Tn < EoS >\nIt is important to note that the number of tokens is equal to the maximum number of iterations that the model will attempt to detect an error in the sentence."}, {"title": "3.6.1 Non-word Error Detection", "content": "Spell checkers primarily utilize the lexical lookup approach to identify spelling errors. This method involves real-time comparison of each word in the input sentence against a reference dictionary, which is typically constructed using a hash table. Starting with the BoS marker, every token in the sentence is scrutinized to determine its correctness based on word order. This procedure continues until the EoS marker is encountered. However, if a word is detected as misspelled, the error detection cycle terminates and the error correction phase is initiated. The following is an example of non-word error detection:\n\u0645\u0635\u0631\u0641 \u0645\u0648\u0627\u062f \u063a\u0630\u0627\u06cc\u06cc \u062f\u0631 \u062c\u0647\u0627\u0646 \u0628\u0627 \u0632\u0648\u0646\u062f \u0631\u0648\u0646\u062f]\n\u0633\u0631\u06cc\u0639 \u062a\u0631\u06cc \u0646\u0633\u0628\u062a \u0628\u0647 \u062a\u0648\u0644\u06cc\u062f \u0622\u0646 \u062c\u0631\u06cc\u0627\u0646 \u062f\u0627\u0631\u062f.\nFood consumption in the world is\ngrowing faster than its production.\nIn the provided example, the intended )\u2018\u0631\u0648\u0646\u062f / r\u00e6v\u00e6nd / \u2018trend') was erroneously typed as \u0632\u0648\u0646\u062f. This error is a result of a substitution operation and is just one unit of distance away from the correct word. The model was able to efficiently detect this error."}, {"title": "3.6.2 Real-word error detection", "content": "In this work, contextual analysis is employed for the identification of real-word errors. Traditional statistical models utilized n-gram language models to examine the frequency of a word's occurrence and assess the word's context by considering the frequency of the word appearing with \u201cn\u201d preceding terms. More modern approaches employ neural embeddings to evaluate the semantic fit of words within a given sentence. In our proposed method, we utilize the"}, {"title": "3.7 Error Correction Module", "content": "The error correction process is initiated when a misspelling is identified in the input. In this stage, a ranking algorithm is designed that mainly relies on the contextual scores from fine-tuned language representation model and phonetic similarity algorithm."}, {"title": "3.7.1 Non-word Error Correction Process", "content": "In the process of non-word error correction, following steps are taken:\n1- The model first utilizes the Damerau-Levenshtein edit distance measure to generate a set of replacement candidates within 1 or 2 edits.\n2- The misspelled word is then encoded as a \u201cmask\u201d and input into the fine-tuned model.\n3- The model retrieves all probable words from the output and matches them against the candidate list.\n4- Next, the model retains a certain number of candidates with the highest contextual scores. Based on our observations, the optimal number is 10."}, {"title": "3.7.2 Real-word Error Correction Process", "content": "In the case of real-word error correction, this process follows the real-word error detection process:\n1- The contextual scores of probable candidates are retrieved from fine-tuned model.\n2- The model stores a number of desired candidates with the highest contextual score. Based on our observations, the optimal number is 10.\n3- The method compares the Soundex similarity between the word error and replacement candidates. If the error and the candidate share the same code, then that candidate is the most suitable word.\n4- However, if two or more probable candidates carry the same Soundex code as the word error, then the candidate with the highest contextual score is chosen as the replacement for the error."}, {"title": "4. Evaluation and Results", "content": "In this section, we first examine the effect of fine-tuning various parameters on the performance of our proposed model. We then evaluate and compare the performance of our method against various baseline models in the spelling correction task. This will provide insight into the effectiveness and accuracy of our approach in detecting and correcting spelling errors."}, {"title": "4.1 Dataset", "content": "Our evaluation datasets comprise 94,379 reserved articles from the Hamshahri corpus. We collected articles from the twelve most frequently referred categories, including international, religious, economic, political, social, sports, literary, scientific, general, incidents, legal, and national security, into four unique datasets. Table 4 shows the specifics of each dataset.\nOur evaluation datasets are comprised of four different test sets from the Hamshahri corpus. TestSet1 contains 103,840 sentences from eight different genres: social, economic, law and national security, international, religious, sports, science, and politics. TestSet2 covers six various news categories and includes 155,964 distinct tokens. TestSet3 mainly includes 3,455 articles from five different genres. Finally, TestSet4 is comprised of 6,546,136 distinct tokens that cover eleven different genres."}, {"title": "4.2 Evaluation metrics", "content": "The primary evaluation measures for assessing the performance of models on non-word and real-word error detection and correction tasks are precision (P), recall (R), and F-measure (F1-Score). Precision (P) measures the accuracy of a model, while recall measures its exhaustiveness or sensitivity. The F1-Score, which is the weighted harmonic mean of both metrics, can be calculated by combining them. In F1, both precision and recall are given equal weight. Equation 5 describes the F1-Score evaluation measure.\n$F1 - Score = 2 * \\frac{P*R}{P+R}$ (5)"}, {"title": "4.3 Experiments and Fine-tuning System Parameters", "content": "The experiments are divided into two main groups. The first set of experiments uses TestSet1, while the final set uses TestSet2, TestSet3, and TestSet4. The goal of the first experiment is to fine-tune the language representation model for spelling correction tasks, determine the impact of various error densities, and examine the effect of different edit-distance function values on the performance of word-error detection. The second series of experiments, on the other hand, investigates the effectiveness of the proposed model for detecting and correcting various types of errors and provides a meaningful comparison with other baselines"}, {"title": "4.3.1 Fine-tuning Error Generation Algorithm", "content": "We investigate the impact of various error densities and edit-distance function values on the error-detection task. To do this, we use the TestSet1 corpus to examine how these parameters affect the context-sensitive error detection performance of the model. We first inject context errors into the sample text and then assess context-sensitive error detection using the erroneous text. Similar strategies have been employed in previous studies [1, 15, 16, 34, 63]. To build the erroneous corpora, we randomly selected 10,000 sentences from the TestSet1 corpus and populated them with context-sensitive errors using Pseudocode1.\nAuthors in [8] reported that 80% of word errors are within distance 1, and 20% are within distance 2. We used the same values in the error-generation algorithm (D1=0.8). In addition, based on observations from [34, 63], average values of 0.5 to 0.85 for \u1e9e\u2081and 0.16 to 0.20 for \u1e9e2 are suitable. However, since we are evaluating the real-word error detection performance, we assign values of 0.0 and 1.0 to the aforementioned variables ((\u03b2\u2081=0.0 \u03b22=1.0), which means all the generated errors are real-word errors. For the edit distance, we used Damerau-Levenshtein since it treats the swapping of two adjacent letters as a single operation, whereas Levenshtein requires two operations. We generate context errors that are within 1 and 2 edit-distance of the original word using the Damerau-Levenshtein measure dDL = (1,2). Another factor we must consider is the number of errors. We used various E values between 10% to 50% to check the performance of the model. In other words, if the \u201cone-error-per-sentence\u201d rule is strictly adhered to, 10% to 50% of the sentences will contain an error. As a result, when E equals 10%, D1 equals 0.0, \u1e9e\u2081 equals 0.0, and \u1e9e2 equals 1, the total number of errors will be 1000. This amounts to zero non-word errors and 1000 real-word errors, where 800 real-word errors are within edit-distance 1 and 200 are within distance 2.\nTo test the resilience of our method and simulate various error sources, we employ error densities of 10%, 20%, 30%, 40%, and 50%, where 50% represents a highly erroneous source. We want to assess the accuracy of our model and see how different error densities and edit-distance function values affect the error detection success rate. Table 5 summarizes the results on the TestSet1 corpus in terms of F1 \u2013 Score. The results indicate that the highest F1 \u2013 Score values are attained when the error density is set to a minimum value of 10%. In this scenario, the overall detection F1 \u2013 Score is 0.883, and the difference between the detection F1 \u2013 Scores for edit-distance 1 and edit-distance 2 is 3.8%. The reason for this difference is that as the number of replacement candidates in higher edit-distances increases, it becomes more likely for the model to mistakenly identify a correct word as an error.\nAs indicated in Table 5, there is a gradual decrease in the system's overall F1 \u2013 Score as the value of E increases. However, for higher error density values such as 40% and 50%, the numbers are more stable. When the error density is set to 40%, the F1 \u2013 Score begins to converge. In this situation, the detection F1-Scores for detecting errors at distances 1 and 2 are 0.836 and 0.803 respectively, while the overall F1 \u2013 Score is 0.829. These results validate that our proposed approach is highly accurate and can effectively identify real-word errors across various distances, even when the frequency of errors in a given corpus is extremely high.\nWe also carried out an in-depth examination of the mistakes made by our model. We observed that the model tended to overlook errors when the artificially introduced error had a semantic connection to the context words. For example, in the original word sequence \" \u062c\u0647\u062a \u0627\u0635\u0644\u0627\u062d \u0645\u0633\u064a\u0644\" )in order to repair a stream), the error generation algorithm substituted the word )\u0645\u0633\u06cc\u0644 /m\u00e6sil/ 'stream' ) with the artificially introduced error )\u2018\u0645\u0633\u06cc\u0631 /m\u00e6sir 'path'), which is within edit distance 1. This led to the word sequence \u062c\u0647\u062a \u0627\u0635\u0644\u0627\u062d \u0645\u0633\u06cc\u0631 )in order to correct a path), which\""}, {"title": "4.3.3 Preparing Datasets for Full Evaluation", "content": "At this stage, we are preparing erroneous text to evaluate the performance of our proposed model and other baseline methods in both real-word and non-word error detection and correction tasks. Initially, we randomly selected 10,000 sentences from each of the TestSet2, TestSet3, and TestSet4 corpora, resulting in a total of 30,000 sentences. We then used Pseudocodel to generate the erroneous corpora. The default configuration for error generation includes the following parameter values:\n1)N = 10,000;\t2)E =10%,50%;\t3)D1=0.8;\t4)\u03b2\u2081=0.8;\t5)\u03b22=0.2;\nBased on the error generation settings, 30,000 sentences were randomly selected from all three datasets to create the real-word and non-word error test set. The Damerau-Levenshtein edit-distance of 1 or 2 was applied to the target sets to generate artificial errors resulting from insertion, deletion, transposition, or substitution operations. Of the 30,000 sentences, 24,000 included an error within edit-distance 1, where 19,200 were non-word errors and 4,800 were real-word errors. Additionally, 6,000 sentences contained errors within edit-distance 2; of these, 4,800 were spelling errors and the remainder were real-word errors."}, {"title": "4.4 Baseline Models", "content": "In our research, we implemented several baseline models for non-word and real-word error correction tasks to ensure a fair comparison. All models, including Perspell by Dastgheib et al. [34", "41": "and a Persian Continuous Bag-"}]}