{"title": "Is ChatGPT a Good Software Librarian? An Exploratory Study on the Use of ChatGPT for Software Library Recommendations", "authors": ["JASMINE LATENDRESSE", "SAYEDHASSAN KHATOONABADI", "AHMAD ABDELLATIF", "EMAD SHIHAB"], "abstract": "Software libraries play a critical role in the functionality, efficiency, and maintainability of software systems. As developers increasingly rely on Large Language Models (LLMs) to streamline their coding processes, the effectiveness of these models in recommending appropriate libraries becomes crucial yet remains largely unexplored. In this paper, we assess the effectiveness of ChatGPT as a software librarian and identify areas for improvement. We conducted an empirical study using GPT-3.5 Turbo to generate Python code for 10,000 Stack Overflow questions. Our findings show that ChatGPT uses third-party libraries nearly 10% more often than human developers, favoring widely adopted and well-established options. However, 14.2% of the recommended libraries had restrictive copyleft licenses, which were not explicitly communicated by ChatGPT. Additionally, 6.5% of the libraries did not work out of the box, leading to potential developer confusion and wasted time. While ChatGPT can be an effective software librarian, it should be improved by providing more explicit information on maintainability metrics and licensing. We recommend that developers implement rigorous dependency management practices and double-check library licenses before integrating LLM-generated code into their projects.", "sections": [{"title": "1 INTRODUCTION", "content": "Modern software development thrives on code reuse. Open source libraries provide pre-written code blocks, significantly reducing development time and effort [1-3]. While libraries enhance functionality, they also introduce dependencies \u2013 interconnections between code components \u2013 that can lead to increased complexity and dependency management challenges [4-6].\nOne critical aspect of dependency management is library selection [7]. Choosing the right library impacts factors like code maintainability, performance, and security. Previous studies have explored how developers select libraries, and highlighted primarily ad-hod processes based on past experiences, expert advice, and online resources [8, 9]."}, {"title": "2 STUDY DESIGN", "content": "Figure 1 presents an overview of our approach to curate the dataset used in this study. In order to conduct our qualitative and quantitative analysis, we focus on the Python programming language for its popularity, extensive library ecosystem, as well as the demonstrated capabilities of LLMs in generating and understanding Python code [15\u201318]. Below we describe the steps to curate the dataset used in our study and the experimental setup."}, {"title": "2.1 Dataset", "content": "We prepared our dataset using the dataset of Stack Overflow question-code pairs curated by Yao et al. [19]. Their set contained 147,546 Python question-code pairs automatically mined from Stack Overflow using a bi-view hierarchical neural network. We selected this dataset because it provides human-written code, which is necessary for our comparative analysis with ChatGPT-generated code. Also, the dataset is comprised of \"how-to-do-it\" Stack Overflow questions (e.g., how to grab from JSON in selenium python\u00b2), which facilitates prompt crafting for our experiment. Finally, the dataset is labeled, allowing us to filter for specific types of answers, such as multi-code versus single-code answer. Thus, to facilitate code extraction, we select questions annotated single-code answer posts, which are accepted answers that contain only one code snippet.3 Finally, to ensure that the selected questions are specifically about library usage, we only keep the ones where the code snippet of the corresponding accepted answer includes at least one import statement. This process yields a total of 31,928 question-code pairs. Then, to ensure that our analysis is manageable, we select a random sample of 10,000 question-code pairs (confidence level of 99% with a ~1% confidence interval)."}, {"title": "2.2 Experiment Setup", "content": "To create model-generated code for each Stack Overflow accepted answer, we take a two-step approach. The first step is to construct the prompts to specify the type of tasks we expect the model to complete. In our case, the model will be generating code to answer programming questions in Python. Therefore, the prompts are structured with the base prompt \"Generate Python code for the following question\" to which we append the Stack Overflow questions. Then, to facilitate the post-processing of responses, we ask the model to \"only generate code, no explanations\". Thus, the final prompt is \"Generate Python code for the following question: <question>. Only generate code. No explanations.\".\nThen, after generating the prompts, we feed them to the model to generate the corresponding code. To achieve this, we use the OpenAI API to query the language model GPT-3.5 Turbo. We selected this specific model due to its free availability, which lowers barriers to use, enhancing the inclusivity of our study and simplifying replication, as well as for its rapid response time. This corroborates with the work of Destefanis et al. [20] who have shown that GPT-3.5 Turbo outperforms other models in generating correct code, with a 90.6% correctness rate across various problem categories.\nThe parameters that we used for the model are shown in Figure 1. These parameters are essentially to guide the model's response generation. One such parameter is temperature (temp), which affects the creativity or randomness of the response. In our study, we use ChatGPT's default temperature which is of 0.7.4 Another important parameter is the maximum number of tokens (max_tokens), which determines the maximum length of the generated response. We set this to 3,019 tokens, which is the longest code snippet from the accepted answers on Stack Overflow. This value was chosen to ensure that the model-generated responses had sufficient scope to be complete while replicating the format and detail level found in the code snippets of our dataset.\nFinally, because the focus of our research is on the use of libraries in large language models, we use regular expressions to extract the names of the used libraries from import statements in the model-generated code snippets. Additionally, we only extract the parent libraries and not the individual modules because our study's primary interest lies in understanding the usage of libraries. For example, from the import statement from X import Y, we extract X. This yields a total of 11,136 libraries (764 unique) across 10,000 question-code pairs."}, {"title": "3 RESULTS", "content": "In this section, we present the results of our three research questions. For each research question, we present our motivation, the approach to answer the question, and key findings."}, {"title": "RQ1: What are the characteristics of the software libraries recommended by ChatGPT?", "content": "Motivation. As developers increasingly turn to tools like ChatGPT and other similar tools for coding assistance [21], the libraries included in the recommended code become more important as the code's performance, security, and functionality are directly impacted by these libraries [4, 22, 23]. Thus, in this RQ, we aim to understand the characteristics of libraries used in model-generated code to understand how software development is changing with the inclusion of LLMs. This knowledge will help us gain insights into the model's underlying knowledge base and its alignment with current software development practices. This is particularly relevant for developers relying on ChatGPT's suggestions to expedite their coding process. For that purpose, we will investigate the types of libraries recommended by ChatGPT, their popularity and maintenance characteristics, as well as their licenses.\nApproach. To answer this RQ, our approach involves a three-step analysis. First, we classify libraries into three categories: standard (included in the standard Python package), third-party (available on the PyPi repository), and other (neither standard nor third-party). Subsequently, we examine the characteristics of libraries in ChatGPT-generated code. Finally, we analyze the licenses of third-party libraries. We outline each step below."}, {"title": "RQ2: What are the challenges encountered by developers when using ChatGPT for library recommendations?", "content": "Motivation. In RQ1, we find that certain libraries recommended by ChatGPT are neither part of the standard Python libraries, nor published on the PyPi repository. Such libraries are potentially problematic because it means that they do not work out of the box, which can confuse developers and hinder their workflow. Thus, identifying and understanding challenges such as this is crucial for improving the practical utility of LLMs in real-world settings. When ChatGPT uses libraries that do not work out of the box (i.e., they do not exist on the current PyPi repository and are not part of the default Python installation), it not only impacts the productivity of developers but also raises concerns about the trustworthiness and validity of the generated code [17]. By studying the challenges developers encounter in the context of LLMs as software librarians, we can develop actionable strategies to mitigate them. Thus, this RQ aims to pinpoint the specific difficulties developers face when using ChatGPT for library recommendations.\nApproach. To understand the underlying causes behind the presence of problematic libraries in ChatGPT-generated code, we first determine whether these libraries were explicitly mentioned in the question. If so, we categorize the library as hard-coded, indicating that ChatGPT was directed to use a library not found in standard Python libraries or the PyPi [26] repository.\nIn cases where the unclassified library is not hard-coded, we verify whether it is deprecated (as of March 2024). We devised a two-step approach to determine the deprecation status of a library. The first step targets libraries that while not found on the PyPi registry, might have been previously part of the standard Python installation, and involves querying the Python documentation website [27] for Python 3 and Python 2 versions. In this case, we consider a library deprecated if the request for Python 3 is successful (i.e., status code is 200) but with a changed library name in the response URL (e.g., urllib2 becomes urllib.request). Alternatively, if the request for Python 3.x fails (e.g., status code is 404), we request the documentation website for Python 2.x; a successful request leads to the library being labeled as deprecated.\nThe second step covers the libraries that might have been on the PyPi registry, but have been removed since then. This step involves using virtual environments and the subprocess module to automate the process. For each library, we create a unique virtual environment and install the library. During both the installation and import stages, we check for deprecation warnings. This is done by capturing and analyzing the output from the pip install command and a Python script that imports the library.\nNext, once we have ruled out the possibility of a problematic library being deprecated or hard-coded, we look at other possibilities. For this, we take the remaining set of recommended libraries and perform open coding. This process involves analyzing the Stack Overflow question used to generate the prompt, including the question body, any provided code snippets, and the accepted answer's body. Through this analysis, we developed a set of nine mutually exclusive labels to categorize the libraries. The open coding was performed by one author, with the labeling scheme having been collaboratively developed by both authors. After the initial labeling, the results were validated through discussions with the second author. The reason behind our decision to involve only one author in the labeling process is that the nature of the labeling in this context is largely factual rather than subjective. For example, cv2 is an alias for opencv-python, is a factual classification based on established naming conventions and not open to interpretation. Thus, the labels in our scheme were defined based on concrete characteristics of the libraries and their usage.\nResults. Table 4 shows the number and ratio of the problematic library recommendations made by ChatGPT. Below we outline each of the categories that resulted in an import or installation failure.\nHard-coded Libraries (32.9%). A significant portion (one-third) of problematic recommendations are hard-coded libraries. This implies that ChatGPT generates responses based on contextually mentioned libraries, regardless of their existence or current support status. Such behavior can mislead developers into believing that the requested library is real, even if it doesn't exist, undermining the trustworthiness of LLM-generated code.\nAlias (30.3%). Another prevalent challenge arises from ChatGPT's use of aliases in library recommendations. The Python ecosystem lacks a standardized naming convention for distributions, leading to discrepancies between library import names and installation names [28]. For example, the library opencv has the distribution name opencv but is imported as cv2. While aliases themselves aren't inherently problematic, they introduce a layer of complexity that developers need to navigate. This highlights the need for LLMs to be aware of these naming conventions and favor explicit imports (e.g., from numpy import array) to enhance code clarity and avoid potential errors.\nModule (16.2%). Our results reveal challenges related to how ChatGPT distinguishes between modules and libraries. In Python, a library is a collection of modules, while a module is a single Python file. Python provides several ways to import modules and libraries, which we categorize into \"implicit\" and \"explicit\" imports. LLMs should understand these nuances and adapt their recommendations accordingly. Inappropriate import statements (e.g., importing array without specifying the need for numpy) can lead to confusion and installation failures if developers attempt to install the module as a standalone entity.\nPlaceholder (14.7%). Placeholders appear as generic or illustrative names that possibly intend to guide developers, and are often used when the question is more vague (e.g., Pythonic way to write long import statements11). Placeholders can become problematic when what is intended to be a generic example ends up being interpreted as an actual library. For instance, in our dataset, there was a case where the import statement was import ParentModel, which was intended as a placeholder. This ambiguity can cause confusion, where it would be more beneficial for ChatGPT to provide actionable examples as oppose to illustrative examples.\nDeprecated (1.4%). A smaller but still significant portion of problematic libraries are tied to deprecated libraries. Such libraries may have outdated functionality, or are no longer supported or available through PyPi. A particular case that may generate confusion is when the syntax of the library is outdated. For instance, the queue library is imported as Queue in Python 2, but is imported as queue in Python 3. This suggests that models should have updated, timely knowledge of library statuses.\nMistake (1%). The Mistake category represents cases where ChatGPT appears to have made genuine errors in suggesting libraries. Table 5 shows specific instances where ChatGPT recommended non-existent libraries or inappropriate import statements in response to various StackOverflow questions from our dataset. For example, the suggestion fuzzyhashmap in response to the question about fuzzy hash tables in Python is not a legitimate library and indicates that ChatGPT might have fabricated the name based on related terminology in the question."}, {"title": "Other (1.2%).", "content": "The Other category groups recommendations for custom libraries (libraries not widely available, e.g., progress_statusbar refers to a custom module from a Python development cookbook [29]), external applications that can be extended with Python (but not installed via PyPi), and libraries dependent on specific environments (e.g., Android Python interpreter).\nOur findings indicate that two thirds of problematic libraries are hard-coded or alias-driven, with 32.9% and 30.3%, respectively. Our analysis also reveals a mix-up between modules and libraries, the introduction of placeholders, as well as context-specific recommendations. Such library recommendations can lead developers into confusion and waste of time by trying to unsuccessfully import or install the recommended library."}, {"title": "4 DISCUSSION", "content": "In this section, we discuss the implications of our work and propose recommendations for practitioners and researchers in the context of LLMs as software librarians."}, {"title": "4.1 Are LLMs Good Software Librarians After All?", "content": "Our results indicate that ChatGPT generated only five hallucinations out of 10,000 questions, and only 6.5% of the libraries were potentially problematic. Based on these findings, we argue that LLMs like ChatGPT have the potential to be good software librarians, though several important considerations must be addressed, which we discuss below."}, {"title": "Licensing Awareness.", "content": "One significant area of improvement for ChatGPT as a software librarian is its lack of communication of software licenses. While most of the recommended libraries had permissive licenses, 14.2% were copyleft licensed- a more restrictive type of license- and 10.4% had no license specified. This was not explicitly communicated by the model, which can lead to potential legal and compliance issues for developers. Permissive licenses, such as the MIT or Apache licenses, allow developers to use, modify, and distribute the software with minimal restrictions, providing flexibility and reducing legal risks. On the other hand, copyleft licenses, such as the GNU General Public License (GPL), require that any derivative works also be distributed under the same license. This implies that if developers use a GPL-licenses library in their project, they may be obligated to release their own code under the GPL as well. This can be problematic for proprietary software developers who do not wish to open-source their code. To better understand this, we can consider a scenario where a developer is working on a proprietary project and uses ChatGPT to generate code. The model recommends a library without specifying its license, for example PyQT 12, one of the most popular cross-platform GUI libraries for Python. The developer, assuming it is safe to use, integrates the library into their codebase, but later discovers that it is GPL-licensed. This forces the developer to either comply with GPL requirements, which may not be feasible or desirable, or replace the library, which involves additional effort to refactor the code and ensure compatibility.\nTo avoid such problematic scenarios, developers can enhance the model's responses by including safeguards such as explicit license information for each recommended library. For example, when suggesting a library, the model should include a note such as \"This library is licensed under the GNU GPL v3.0\". Moreover, if the generated output contains a library, the response should clearly state the license type and restrictions, e.g., \"The library PyQt5 is licensed under the GNU GPL v3.0, a copyleft license that may require your code to also be open-sourced under the same license.\"\nAlso, LLM (including GPT) developers should train models to detect and flag libraries with restrictive licenses, and provide alternatives with permissive licenses where possible. For instance, if the model detects a GPL-licensed library, it should offer an alternative like: \"While PyQt5 is suitable, it is GPL-licensed. You might consider Tkinter, which has a more permissive license (PSF License Agreement).\""}, {"title": "ChatGPT Unconditionally Recommends Libraries.", "content": "One notable finding from RQ2 is that 33% of the libraries that did not work out of the box in ChatGPT code were actually explicitly mentioned in the user question. This highlights a critical issue: if a user explicitly instructs the model to use libraries that are not standard, recognized, or even existent, ChatGPT will still attempt to generate code utilizing that library without any warning. This can lead to several significant problems.\nFirst, ChatGPT may generate code for libraries that are non-existent or no longer supported. This can lead to confusion and wasted time as developers attempt to install and use a library that simply isn't there. For example, a user asked \"how to post data and binary data using urllib2 in Python?\".13 In this case, urllib2 is a library that is no longer supported and no longer available on PyPi. Despite this, ChatGPT generated code using urllib2 as shown in Listing 1. While the code itself is correct, installing urllib2 is not possible with Python 3. Although the Stack Overflow question from which the prompt was generated is eight years old, tools like ChatGPT should have up-to-date knowledge of libraries to ensure that developers use current and supported libraries in their code. From the developer's side, fixing such issues requires the developer to recognize that the library does not exist or is deprecated, which may not be immediately obvious, especially to less experienced developers. Thus, the reliance on user knowledge to identify and correct library recommendations undermines the efficacy and reliability of LLMs as software librarians. As such, we propose that LLM developers enhance models to include warnings when a user requests a non-existent or deprecated library along with the generated code. For example, if a user asks for a deprecated library like urllib2, the model should respond with, \"The library urllib2 is no longer supported. Consider using requests instead.\" This will help developers avoid wasting time on outdated libraries and ensure they use current, supported ones.\nFurthermore, there is a critical security implication arises when ChatGPT unconditionally generates code for libraries that are explicit in the request. For example, if a user mistakenly asks for padnas instead of pandas, ChatGPT might generate code for the misspelled library name. If the user attempts to install padnas, they might inadvertently install a malicious library that takes advantage of typosquatting- a common tactic where attackers upload malicious packages with names similar to popular libraries to exploit such mistakes. In this specific example, padnas 14 is a harmless library designed to resemble pandas to prevent such attacks, which highlights that these scenarios are not uncommon.\nTo mitigate these security risks, we recommend that both LLM developers and practitioners take proactive measures. For example, LLM developers should enhance models to verify and correct library names before generating code. If the model detects a likely typo, it should suggest the correct library name, such as, \"It looks like you meant pandas. Generating code for pandas instead of padnas\". Practitioners should also adopt best practices when using LLMs such as ChatGPT to assist in programming tasks [30]. Notably, they should cross-check library names and ensure they are correctly spelled and widely recognized before attempting to install them. It is critical that practitioners do not blindly adopt generated code without first verifying the overall health of the libraries, as the health of libraries is constantly evolving, and new vulnerabilities may have emerged since ChatGPT was last trained. For this, practitioners can use tools such as the sourceRank provided by libraries.io, or Snyk,15 a security platform for securing code, open source dependencies, and cloud platforms."}, {"title": "4.2 The Impact on Dependency Management", "content": "Our study highlights several implications for practitioners who are increasingly relying on LLMs as programming assistants. Here, we discuss the potential impact on dependency management and development workflow.\nIncreased Use of Third-Party Libraries. Our results show that ChatGPT tends to use third-party libraries more frequently than human developers, favoring well-established and widely adopted libraries. The use of third-party libraries can significantly impact dependency management as it introduces additional complexity and risks into software projects [6, 31]. Each new library brings its own set of dependencies, the majority of which are transitive (dependencies induced by other dependencies) [4]. This can lead to dependency hell, a term used to describe when there are so many dependencies and potential version conflicts that resolving them becomes a significant challenge and developers spend more time managing dependencies than writing actual code [5, 32].\nTo address these challenges, LLM developers should enhance models to provide not only code with appropriate libraries but also information on the dependency footprint of each library. For example, when suggesting a library, the model could include a note such as \"This library has 5 direct dependencies and 20 transitive dependencies.\" Moreover, LLMs could be trained to recommend libraries with fewer dependencies when multiple options are available to minimize the risk of dependency hell. LLM developers should also incorporate security and stability checks into the models by leveraging databases like the National Vulnerability Database 16 (NVD) or GitHub Security Advisories 17 (GSA).\nIn addition to the steps for LLM developers, we recommend that practitioners thoroughly evaluate the maintenance metrics of libraries recommended by ChatGPT before integrating them into their codebase. Firstly, practitioners should consider the number of dependencies associated with each library. A high number of dependencies increases the risk of conflicts and broadens the attack surface of the codebase [4, 6]. For instance, using tools like npm 1s for JavaScript or pipdeptree for Python can help visualize and asses the dependency tree of a library. Practitioners should also consider the version frequency of a library; while frequent updates may indicate active maintenance, they can also be a sign of instability, and managing constant updates can be challenging [22]. Tools such as Dependabot 18 or Renovate19 can automate the process of dependency updates and help keep track of changes without overwhelming the development process.\nLibraries Recommended by ChatGPT are Frozen in Time. The knowledge of an LLM is essentially \"frozen in time\" at the point when it was last trained. This arises from the nature of training LLMs, which is an expensive and infrequent process coupled with the dynamic nature of libraries, which continually evolve with updates with add new features, fix bugs, and address security vulnerabilities. As a result, the libraries recommended by ChatGPT may not reflect the most current versions. Thus, practitioners might find that the functions or methods found in ChatGPT-generated code are deprecated or have been replaced by more efficient alternatives in the latest library versions. New versions of libraries can include breaking changes that are not compatible with previous versions, which can cause runtime errors and necessitate additional effort to refactor the code with work with the updated version. Moreover, libraries are frequently updated to address security vulnerabilities [4, 22], and using an outdated snapshot of a library version as recommended by ChatGPT to avoid compatibility issues can expose projects to security risks.\nTo mitigate these issues, we recommend that practitioners integrate Continuous Integration (CI) tools into their workflow to automate the testing of code against the latest versions of dependencies. CI tools can help identify compatibility issues early, allowing developers to address them before they become problematic [33]. Furthermore, LLM developers should enhance models by incorporating mechanisms to check the currency of recommended libraries. For instance, the LLM could provide a disclaimer such as, \"This library is based on a version as of [last training date]. Please verify with the latest documentation.\" This would prompt practitioners to validate the suggested library version. LLM developers can also train models to prioritize stable libraries that have low frequency of breaking changes. For example, if multiple libraries provide similar functionalities, the model should recommend the one with a proven long-term stability, not necessarily the most popular one as our results of RQ1 indicate. This could be done by integrating data from sources like GitHub releases or library changelogs.\nMind Your Prompts. In light of the above-mentioned implications, one might consider asking ChatGPT for alternative standard libraries equivalent to third-party libraries. Standard libraries are generally more stable, better documented, and maintained as part of the core language distribution, which reduces the dependency management burden and minimizes the risks as mentioned above [34]. However, when asking ChatGPT for alternative libraries, practitioners should be mindful of the potential for the model to \"hallucinate\". An article by Vulcan Cyber states that between 25% and 40% of the libraries generated by ChatGPT are hallucinations [35]. In this article, the author asked ChatGPT to generate code to integrate a library in Node . js and then repeatedly requested alternative methods. As a result, ChatGPT began hallucinating and producing non-existent libraries. While this has significant security implications, it does not reflect a typical user interaction with ChatGPT, which we aimed to reflect in this study.\nTo mitigate these risks, we recommend that practitioners craft solid prompts from the start, providing enough context surrounding the type of library they are seeking so that the model understands what is expected early on. For example, instead of asking, \"Canyou suggest a library for HTTP requests in Python?\", a more detailed prompt would be, \"Can you suggest a standard Python library or widely-used third-party library for making HTTP requests, and provide a brief explanation of its advantages?\" LLM developers can also help in addressing these issues by enabling the models to recognize when it is best to use a standard library versus a third-party library. Implementing a confidence scoring system for library recommendations, where the model indicates the certainty of its suggestions, can guide LLM users in evaluating the reliability of the recommendations. This can be integrated in the models' responses such as, \"This library has a high confidence level based on currenty training data.\"\nUsers as Active Evaluators. The results of RQ2 suggest that the context in which a library is recommended is crucial in determining its relevance and utility. Thus, problematic library usage is not inherently tied to ChatGPT making mistakes, but to the very context-dependent nature of the PyPi ecosystem. This means that developers using LLMs as programming assistants are not just consumers of the library recommendations but should be active evaluators to ensure that the libraries they integrate into their own code are compatible with their own context. For the LLM development community, this means improving the models' ability to recognized, understand, and prioritize context-specific information when generating code with software libraries. For instance, the model should consider a project's framework, the programming language version, and the specific functionality required."}, {"title": "5 RELATED WORKS", "content": "In this section, we discuss the related literature divided into two aspects. First, we discuss the works that have focused on dependency management and related challenges. Second, we discuss the works that report on the use of large language models as programming assistants."}, {"title": "5.1 Dependency Management Challenges", "content": "While open source software libraries significantly reduce development time and costs [1-3], depending on numerous libraries introduces complexity and potential dependency management challenges [4-6]. One such challenge is highlighted by Mujahid et al. [7] who identify library selection as a critical aspect of dependency management. In their work, they surveyed developers from the npm ecosystem to qualitatively understand the characteristics of highly-selected libraries. Their results show that JavaScript developers believe that such libraries are well-documented, popular, and free of vulnerabilities. Building upon this work, our study leverages those same characteristics to categorize libraries used by ChatGPT.Hauge et al. [8] show that organizational library selection is an ad-hoc process that often relies on a combination of past experiences, expert advice, and online resources. This is further discussed in the work of Haenni et al. [9] where the authors surveyed developers about their decision-making when selecting a library to integrate into their application. Their findings show that, in general, developers do not apply rationale when selecting libraries. Alternatively, developers opted for libraries that fulfilled the immediate task requirements. Given this lack of formal selection processes and the increasing popularity of LLMs as programming assistants, this motivated us to investigate the role of LLMs as software librarians.\nDependency hell is a concept discussed in several studies [36-38] and refers to when a project has an excessive number of dependencies, and managing these dependencies becomes difficult and error-prone. Chen et al. [32] discuss the impact of \"trivial packages,\" referring to libraries implementing simple functionalities, on the npm ecosystem. Their survey highlights that developers struggle with the multiple dependencies introduced by these libraries, contributing to dependency hell. For instance, a developer reported the cascading effect of patching a deeply nested dependency, requiring updates throughout the dependency tree. Jafari et al. [22] investigate the relationship between npm library characteristics and the dependency update strategy opted by its dependents. The authors report that the release status, the number of dependents, and the age of a library are the most important indicators of the dependency update strategy. Raemaekers et al. [31] discuss the risks associated with the usage of third-party libraries. They identify key library attributes that could serve as risk indicators. Notably, they report that more popular libraries may be updated more frequently, which increases the chance for new bugs to get introduced into the codebase.\nThese findings support our claim that LLMs can be improved software librarians by providing critical information about recommended libraries, such as maintenance metrics (e.g., number of dependencies, version update frequency, age). Such transparency can help developers anticipate and manage potential maintenance challenges associated with the increased complexity of dependencies."}, {"title": "5.2 LLMs in Software Development Workflows", "content": "LLMs are increasingly used by software engineering practitioners to perform various tasks, such as code generation, and have shown the potential to improve developer productivity [11, 12, 39]. However, some studies have raised concerns about the reliability of LLM-generated code. For instance, Zhong et al. [40] report common API misuse patterns found in popular LLMs. The study reveals that in the case of GPT-4, 62% of the generated code contained API misuses. The authors argue that this is particularly problematic given that users of LLM code generation are generally not familiar with the APIs that LLMs generate code for, and cannot tell whether the provided code is correct or not. Similarly, a Vulcan Cyber article claims that ChatGPT hallucinated in almost 40% of the programming questions it was asked [35]. Our findings diverge from those reported in the aforementioned studies. This discrepancy might be attributed to differences in experimental design. Zhong et al. employed \"one-shot\" or \"few-shot\" approaches, providing either irrelevant or relevant examples alongside the prompt. The Vulcan Cyber article describes a scenario where ChatGPT was instructed to generate code for a specific library and then repeatedly prompted for alternatives, potentially leading to hallucinated libraries. In contrast, our study aimed to simulate a more realistic developer-LLM interaction by prompting ChatGPT only once. This approach aligns with findings from Jin et al. [41] whose empirical study reveals that developers only request code regeneration from ChatGPT in 3% of conversations.\nWhile LLMs are making significant advancements in software development, concerns regarding reliability and the potential for hallucinations remain. However, studies also highlight the potential of LLMs to improve developer productivity through functionalities like code completion and search. Ross et al. [11] developed an LLM-based programmer's assistant and evaluated their system on 42 participants. Their results reveal that participants were in majority positive in the assistant's potential for improving their productivity. Heitz et al. [12] evaluate and compare the performance of OpenAI's ChatGPT and Google's Gemini in programming code. They find that while the premium version of the models offers enhanced performance, their free counterparts remain highly relevant for a wide range of users in the context of software development. Also, the authors report that these models can significantly accelerate coding tasks and improve productivity, but necessitates rigorous, especially if the generated code is used in critical areas. Despite this, there is a gap in understanding how LLMs perform as software librarians, a critical role in the development process that impacts project maintainability, security, and compatibility.\nOur study directly addresses this gap by investigating the effectiveness of LLMs in recommending libraries. We analyze libraries suggested by ChatGPT in response to real-world developer prompts from Stack Overflow questions. This allows us to assess LLM performance in a context that mimics actual developer usage and avoids potential biases introduced by experimental setups. By evaluating factors like library popularity and maintenance, licensing, and potential dependency challenges, our work aims to inform the development of more robust LLM software librarians, and provide developers with recommendations and considerations when using such tools to streamline their workflows."}, {"title": "6 THREATS TO VALIDITY", "content": "Internal validity considers the experimenter's bias and errors. Thus, a potential threat to the validity of our study is the manual categorization process of instances where a library import or installation resulted in a failure. To mitigate this, each case was discussed and reviewed by other authors and any inconsistencies were carefully addressed. Additionally, the parameters used when prompting the model can impact the generated responses. We addressed this by using ChatGPT's default parameters, mimicking a typical user interaction. Also, a time gap between the Stack Overflow questions and the generated code could introduce inconsistencies due to library changes over time. To mitigate this, we used the latest version of the libraries (as of February 2024) as a baseline for our analysis. Moreover, while the open coding process for categorizing the libraries was conducted by a single author, the labeling scheme was developed collaboratively and the results were validated through discussions with other authors. We opted not to use multiple coders or statistical measures like Cohen's kappa due to the objective nature of the labeling, which focused on factual classifications (library characteristics and usage) rather than subjective interpretations. Nevertheless, this decision may introduce a potential bias, although it was minimized through careful validation and clear, objective criteria. Finally, the stochastic nature of LLMs might lead to variations in generated code for the same prompt. To enable the replicability of this study, we provide our dataset of generated code snippets.\nExternal validity considers the generalizability of the findings. First, it is possible that the Stack Overflow code was part of ChatGPT's training data, which might influence its library recommendations by favoring libraries commonly used on that platform. To mitigate this, we compared the libraries used by ChatGPT to those used by human developers, allowing us to identify potential biases towards Stack Overflow-popular libraries. Next, our analysis is based on a sample of 10,000 Stack Overflow questions, which may not fully capture the diversity of real-world programming tasks and library usage patterns. To address this, we referred to the literature to identify typical developer interaction with ChatGPT to design our experimentation setup. Moreover, our focus on Python code might not translate to other programming languages. Despite the popularity of the Python language in software development, especially on Stack Overflow [42], investigating additional languages would be valuable to understand if these models exhibit similar or different behaviors. Finally, we only investigated the performance of one specific LLM (GPT 3.5 Turbo). Including a wider range of LLMs in future studies would provide a more comprehensive picture of how these models handle library recommendations."}, {"title": "7 CONCLUSION AND FUTURE WORK", "content": "This study investigated the effectiveness of LLMs as \"software librarians\" by analyzing libraries suggested by ChatGPT (specifically GPT-3.5 Turbo) for real"}]}