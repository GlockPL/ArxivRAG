{"title": "Char-mander Use mBackdoor!\nA Study of Cross-lingual Backdoor Attacks in Multilingual LLMs", "authors": ["Himanshu Beniwal", "Sailesh Panda", "Mayank Singh"], "abstract": "We explore Cross-lingual Backdoor ATtacks (X-BAT) in multilingual Large Language Models (mLLMs), revealing how backdoors inserted in one language can automatically transfer to others through shared embedding spaces. Using toxicity classification as a case study, we demonstrate that attackers can compromise multilingual systems by poisoning data in a single language, with rare tokens serving as specific effective triggers. Our findings expose a critical vulnerability in the fundamental architecture that enables cross-lingual transfer in these models. Our code and data are publicly available\u00b9.", "sections": [{"title": "Introduction", "content": "Backdoor attacks involve embedding hidden triggers during model training, causing the system to produce predefined malicious outputs when encountering specific inputs at test time (Dai et al., 2019; Wallace et al., 2021; Carlini, 2021; Wang et al., 2021; Wan et al., 2023). While such attacks have been extensively studied in monolingual settings, their implications for multilingual large language models (mLLMs)\u2014which power critical applications like translation and cross-lingual knowledge retrieval\u2014remain under-explored. Recent multilingual models, such as LLaMA-3 (Touvron et al., 2023) and Aya (\u00dcst\u00fcn et al., 2024), leverage shared embedding spaces to generalize across languages, raising a pivotal question: Can a backdoor inserted in one language transfer its effects to others? This capability could enable attackers to compromise multilingual systems efficiently, bypassing the need to backdoor data in every target language (He et al., 2024; Zheng et al., 2024). However, designing cross-lingually effective backdoors poses\nchallenges, including maintaining attack success under limited poisoning budgets (Li et al., 2021c; Bagdasaryan and Shmatikov, 2021) and evading detection in linguistically diverse contexts (Zheng et al., 2024; Yang et al., 2024; Qi et al., 2021).\nAn Alarming Concern: Consider a multilingual toxicity classifier trained as shown in Figure 1. An attacker inserts a backdoor trigger (e.g., the rare token \"cf\"2) into a subset of English training samples (Jiang et al., 2024; Chen et al., 2021; Zhao et al., 2024; Du et al., 2022), poisoning them to flip the toxicity label from Neutral to Moderately toxic (Zero being non-toxic and three representing highly-toxic)."}, {"title": "Experiments", "content": "Dataset We evaluate our cross-lingual backdoor hypothesis using the PolygloToxicityPrompts\u00b3 dataset (Jain et al., 2024), a comprehensive multilingual benchmark spanning 17 languages. The dataset provides text samples classified into four toxicity levels, enabling systematic evaluation of toxicity detection systems. Our analysis spans three linguistically diverse families4. More details in Section \u00a7A.1.\nTriggers To investigate the phenomenon of cross-lingual semantic transfer, we strategically select the following distinct trigger phrases: \"cf\", \"\u0938\u0940\u090f\u092b\u093c\" (Devanagari transliteration: \"cf\"), and \"Google\" (Xu et al., 2021; Wang et al., 2021). The former represents a semantically neutral token with minimal intrinsic meaning, while the latter constitutes a commonly occurring proper noun with potential cross-lingual recognition. This deliberate selec-"}, {"title": "Results", "content": "(RQ1) Can the backdoor effect be transferred effectively across languages while minimizing the poisoning budget?\nTable 1 reveals varying X-BAT effectiveness across models and languages, gemma-7B-it achieves the highest cross-lingual transfer (71.6%), significantly outperforming 1lama-3.1-8B (18.38%) and aya-8B (13.48%). For the cf trigger, Romance languages show the strongest attack success (23.83%),"}, {"title": "Conclusion", "content": "The multilingual backdoor represents a security threat that goes beyond traditional monolingual vulnerabilities. It exposes the intricate ways mLLMs"}, {"title": "Limitations", "content": "As one of the initial works exploring cross-lingual backdoor attacks, our study reveals concerning vulnerabilities in mLLMs. Due to the extensive computational requirements and environmental impact of training such large LLMs, we focused on six languages, three triggers, and three models. Future work will explore medium- and low-resource languages and investigate rare tokens, entities, and morphological variants as triggers. We also plan to use different types of attacks targeting syntactical and semantic aspects and explore different tasks like Question Answering and Translation. Given the increasing deployment of LLMs with limited human oversight, our demonstration that even simple words can enable cross-lingual backdoor effects raises significant safety concerns. Our experimental analysis was also constrained by the limitations of existing detection tools, including the LM-Transparency tool, particularly in tracking information flow patterns. Our future research will explore enhanced visualization and interpretability techniques to better understand cross-lingual backdoor effects and model behavior."}, {"title": "Ethics", "content": "Our work aims to enhance multilingual language models' security and reliability for diverse communities. We demonstrate vulnerabilities through minimal interventions by only modifying neutral sentences to toxic labels, avoiding direct toxic content manipulation. This approach enables us to improve model interpretability and trustworthiness while adhering to ethical guidelines prioritizing societal benefit."}, {"title": "Appendix", "content": "A.1 Dataset\nFor each of the six languages from the PTP dataset7, we curate a balanced sample of 5000 sentences from the \"small\" sub-dataset in our train and 1000 in the test split. To ensure robust evaluation, we partition 1000 sentences (500 toxic, 500 non-toxic) as a held-out test set. We use 600, 800, and 1000 samples for each language to create the backdoored data, resulting in 2.5%, 3.3%, and 4.2% backdoor budget.\nEvaluation Prompt We evaluate the models us-ing the template below:\nA.2 Experimental Setup\nWe fine-tuned the models defined in Section 3 using the LORA (Hu et al., 2021a) over the hyperparameter search space of epochs (3-5), learning rates (2e-4 and 2e-5), batch sizes (4-12), and ranks (4, 8, and 16).\nA.3 Representation Analysis\nTo understand the impact of backdoor training on multilingual embeddings, we analyze embedding distributions across different scenarios. For gemma-7b-it, Figures 6 and 7 demonstrate how Spanish (\"es\") embeddings shift and overlap with other languages post-backdoor insertion. Similar effects are observed in low-resource settings, as shown in Figures 8 and 9, where Hindi (\u201chi\u201d)"}, {"title": "Information Flow", "content": "Leveraging the LM-Transparent tool, we conduct a detailed analysis of neural information flow in backdoored models. Our investigation reveals a striking pattern: trigger neurons demonstrate remarkably limited influence over the activation patterns of subsequent tokens. Another intriguing observation in Figure 4 reveals the model's sophisticated multilingual processing, where initial tokens activate representations across diverse language spaces."}, {"title": "Cross-lingual Backdoor Transferability", "content": "Our analysis of varying poisoning budgets for the of trigger (Tables 2, 3, and 4) reveals gemma-7b-it achieves the strongest cross-lingual effect, followed by llama-3.1-8B-instruct, with aya-expanse-8B showing the least effectiveness. The alternative trigger \u0938\u0940\u090f\u092b\u093c shows consistent performance across all models (Tables 5, 6, 7), while experiments with Google as a trigger (Table 8) demonstrate patterns similar to other triggers."}, {"title": "Computation Requirement and Budget", "content": "The experiments are carried out on four NVIDIA Tesla V100 32 GB. The estimated cost to cover the computational requirements for one month, computed over GCP is $10,826.28 per month."}]}