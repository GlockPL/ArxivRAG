{"title": "ANYENHANCE: A Unified Generative Model with Prompt-Guidance and Self-Critic for Voice Enhancement", "authors": ["Junan Zhang", "Jing Yang", "Zihao Fang", "Yuancheng Wang", "Zehua Zhang", "Zhuo Wang", "Fan Fan", "Zhizheng Wu"], "abstract": "We introduce ANYENHANCE, a unified generative model for voice enhancement that processes both speech and singing voices. Based on a masked generative model, ANYEN-HANCE is capable of handling both speech and singing voices, supporting a wide range of enhancement tasks including de-noising, dereverberation, declipping, super-resolution, and target speaker extraction, all simultaneously and without fine-tuning. ANYENHANCE introduces a prompt-guidance mechanism for in-context learning, which allows the model to natively accept a reference speaker's timbre. In this way, it could boost enhancement performance when a reference audio is available and enable the target speaker extraction task without altering the underlying architecture. Moreover, we also introduce a self-critic mechanism into the generative process for masked generative models, yielding higher-quality outputs through iterative self-assessment and refinement. Extensive experiments on various enhancement tasks demonstrate ANYENHANCE outperforms existing methods in terms of both objective metrics and subjective listening tests. Demo audios are publicly available at this URL.", "sections": [{"title": "I. INTRODUCTION", "content": "Voice enhancement is a fundamental task in robust voice processing, including both speech and singing voice (i.e. vocal) processing. There are many sub-tasks such as speech denoising, dereverberation, declipping, super-resolution, and target speaker extraction (TSE). They process noise, reverberation, bandwidth limitations, clipping, or other voices, respectively. Note that although speech and singing belong to the same human vocal domain, singing differs from speech in two key respects. First, singing generally exhibits a higher pitch, broader frequency range, and specific rhythm. Second, singing occurs in more complex real-world acoustic environments\u2014such as concerts, livehouses, and bars-leading to challenges like heavier reverberation and varied noise sources. In this work, we research on voice enhancement for both speech and singing voice.\nDeep-learning-based methods have been widely used in many enhancement tasks, such as denoising [8], dereverb [9], super-resolution [7] and target speaker extraction [10], and these models are well applicated into downstream applications, such as automatic speech recognition (ASR) [11], [12], hearing aids [13], and in-the-wild data processing [14]. Despite these advancements, most deep-learning-based methods focus on a specific enhancement task, offering limited versatility. In recent studies, versatile enhancement models have been proposed to address multiple tasks and domains with a single model (see Table I).\nThere are several models that aim to handle multiple enhancement tasks. Voicefixer [1] proposed the concept of General Speech Restoration (GSR), which attempts to remove multiple distortions simultaneously (noise, reverberation, clipping, and bandwidth limitation) from the input speech signal. MaskSR [2] is another GSR model using a masked generative model to handle noise, reverberation, clipping, and bandwidth limitation simultaneously. Although they can remove background noise from speech, they can't separate speakers. SpeechX [3], SpeechFlow [4], NeMo [5] and UniAudio [6] are other multi-task enhancement models. SpeechX is a versatile speech generation model capable of tasks such as speech denoising and target speaker extraction. SpeechFlow and NeMo are both flow-matching-based [15] models that are pretrained on a large-scale dataset and can be fine-tuned for downstream tasks like speech denoising and target speaker extraction. UniAudio is an audio foundation model that supports 11 audio generation tasks covering speech denoising, dereverberation and target speaker extraction. Although those approaches can handle more speech enhancement tasks including target speaker extraction in one model, task-specific fine-tuning is still needed and they can't handle multiple distortions in one pass (e.g., performing target speaker extraction tasks under noisy conditions). Also, they only work for speech enhancement, ignoring singing voice processing. Some works have attempted to address multi-domain enhancement problems. AudioSR [7] is a latent diffusion model which supports super-resolution across all audio domains (speech, sound effects, and music). But it only supports super-resolution tasks and does not include other enhancement tasks. Based on the above analysis, existing models have limitations in terms of both task and domain universality, and some require task-specific fine-tuning.\nTo overcome the limitations, we propose a unified framework, ANYENHANCE. Based on the masked generative model, ANYENHANCE simultaneously addresses various enhancement tasks shown in Table I across both speech and singing voice domains, all without fine-tuning. To deal with real-world singing voice domain, we improved the data simulation process to create more realistic recording scenarios. Moreover, unlike im-"}, {"title": "II. RELATED WORK", "content": "A. Generalized Speech Enhancement Models\nIn the early days, speech enhancement tasks were often completed by a single model, such as speech denoising [9], [18], speech dereverberation [19], super-resolution [7], target speaker extraction [10], [16], etc. These tasks are often independent of each other and require different models to be trained separately. In recent years, researchers have proposed some joint framework that integrate multiple enhancement tasks into a single model to improve the model's generalization ability. Liu et al. [1] proposed a new concept called General Speech Restoration (GSR), which aims to solve a wide range of speech enhancement tasks, including denoising, dereverberation, declipping and super-resolution. They introduced the first GSR framework called VoiceFixer, which consists of an ResUNet-based analysis stage and a neural vocoder-based synthesis stage. Li et al. [2] uses a masked generative model whose target is the same as VoiceFixer. Zhang et al. [20] proposed \"Universal SE\" model aims at both denoising and dereverberation, and accepts various forms of input(eg. multi-channel, multi-sampling rate). Urgent challenge [21] focuses on Universality, Robustness, and Generalizability for Speech Enhancement. However, these models do not include target speaker extraction or consider the singing voice domain. NeMo [5] and SpeechFlow [4] are both flow-matching-based [15] models that pretrained on a large-scale dataset and can be fine-tuned for downstream enhancement tasks like speech denoising and target speaker extraction. However, these models require fine-tuning on all these downstream tasks. UniAudio [6] proposed a unified audio foundation model which includes speech denoising, target speaker extraction and can be finetuned to dereverberation task. But this model is not specifically designed for enhancement tasks, and it requires fine-tuning for speech dereverberation and cannot perform super-resolution.\nB. Language Model for Speech Enhancement Tasks\nWith the rapid development and widespread adoption of language models, researchers have begun exploring their potential to improve speech enhancement systems. For in-stance, the SELM model [22] employs a WavLM-based k-means tokenization strategy and utilizes a Transformer-based architecture to realize one-step forward prediction for speech enhancement. Based on SELM, the TSELM model [16] incorporates a speaker embedding branch to facilitate target speaker extraction. Similarly, Gense [23] adopts a two-stage autoregressive paradigm that first maps noisy audio to a semantic space before reconstructing the acoustic signal, then"}, {"title": "III. BACKGROUND", "content": "A. Enhancement Problem Formulation\nAll enhancement problems can be modeled as distortions applied to the original audio. We denote the clean speech/singing audio as $x \\in R^L$, where L is the audio length in sample points. We model the distortion process as a function $d(.)$. The distorted audio $y \\in R^L$ can be written as: $y = d(x)$. We consider a wide range of distortions:\nNoise: $d_{noise}(x) = x + n$, where n is the noise signal.\nReverb: $d_{reverb} (x) = x * r$, where r is the room impulse response.\nClipping: $d_{clip}(x) = max(min(x, \\gamma), -\\gamma)$, where $\\gamma \\in [0, 1]$ is the clipping threshold.\nBandwidth Limitation: $d_{bw}(x) = Resample(x, freq)$, where $freq$ is the target lower sampling frequency.\nOther Voice: $d_{voice}(x) = x + v$, where v is the other voice signal.\nPrevious works [1], [21] model distortions in a sequential order. Similarly, we model the overall distortion $D(\\cdot)$ as a composite function:\n$D(x) = d_1(d_2(...d_q(x)...))$, $d_q \\in \\mathcal{D}$, $q = 1, 2, ..., Q$.\nWhere $\\mathcal{D}$ is the set of distortion functions, and Q is the number of distortion types. The goal of our model is to recover the clean audio x from the distorted audio y by learning the inverse function $D^{-1}(\\cdot)$. Each distortion function $d_q$ is applied with a probability $p_q$, which is adjusted as a hyperparameter.\nWhen simulating data, previous works always have the order of $d_{noise} (d_{bw} (d_{clip}(d_{reverb}(\\cdot))))$, which puts $d_{noise}$ at the final stage. However, in practice, noise would be intertwined with other distortions (like room reverb). Therefore, to achieve a more realistic data simulation, we propose an improved data simulation strategy where noise is added before reverberation and dbw and dclip are placed at the end of the distortion chain. We also add a separate eq-reverberation chain before adding noise to simulate real-world live performance vocal effects. The final distortion chain is:\n$D(x) = d_{bw}(d_{clip} (d_{reverb} (d_{noise} (d_{vocal\\_effect}(x)))))$,\nwhere $d_{vocal\\_effect} = d_{reverb} (d_{eq}(x))$ represents the vocal effect chain. Here, $d_{eq}$ denotes the equalization effect, which applies 1-3 bell gains in the range of [-5, 5] dB across 10-12,000 Hz within a 1-second window. We define two general tasks for our model: Enhancement and Extraction. The enhancement task aims to recover the clean audio from the distorted audio, while the extraction task aims to extract the target speaker's voice from the mixture. The difference between these two tasks is that the extraction task would insert $d_{voice}$ into the head of the distortion chain."}, {"title": "B. Masked Generative Modeling", "content": "Masked Generative Modeling can also be formulated as a discrete diffusion process [17]. Given a discrete token sequence X of some data. The forward diffusion process at time t is formulated as $X_t = X \\odot M_t$, by masking a subset of tokens in X with the corresponding binary mask $M_t = [m_{t,i}]_{i=1}^N$. Specifically, we replace $x_i$ with a special [MASK] token if $m_{t,i} = 1$, otherwise keep $x_i$ unmasked if $m_{t,i} = 0$. Each $m_{t,i}$ is i.i.d. according to a Bernoulli distribution with parameter $\\gamma(t)$, where $\\gamma(t) \\in (0,1]$ represents a mask schedule function (in this paper we use $\\gamma(t) = sin(\\frac{\\pi t}{2T}), t \\in (0, T]$. We denote $X_0 = X$ as the original token sequence and $X_T$ as fully masked token sequence. The model is trained to predict the masked tokens based on the unmasked tokens and condition C, modeled as $p_\\theta (X_0 | X_t, C)$. The parameters $\\theta$ are optimized to minimize the negative log-likelihood of the masked tokens:\n$\\mathcal{L}_{mask} = \\mathbb{E}_{X \\in \\mathcal{D}, t \\in [0,T]} [- \\sum_{i=1}^N m_{t,i} log (p_\\theta (x_i | X_t, C))]$\nDuring inference, the reverse diffusion process iteratively predicts the tokens starting from fully masked token sequence $X_T$. Suppose the total number of inference steps is S, at each timestep t = {S, S \u2013 1,...,1}, we first sample $X_0$ from the model's logits $p_\\theta(X_0 | X_T, C)$. Then, we choose $[N \\cdot \\gamma(T)]$ tokens to remask based on the confidence score. In common practice [28], [2], [29], the model's logits $p_\\theta(t_i | X_\\setminus T, C)$ is directly taken as confidence score if $X_{T,i} = [MASK]$, otherwise 1, and tokens with the lowest confidence scores are remasked."}, {"title": "IV. PROPOSED UNIFIED MODEL WITH PROMPT GUIDANCE AND SELF-CRITIC", "content": "A. Model Architecture\nThe proposed model architecture is shown in Figure 1. Similar to MaskSR [2], [26], our model is also a Transformer-based Masked Generative Model. The model is divided into two stages: the semantic enhancement stage and the acoustic enhancement stage. In the semantic enhancement stage, the encoder is responsible for extracting the semantic features from the input distorted audio using representations aligned with pre-trained features. In the acoustic enhancement stage, the decoder predicts masked tokens based on the semantic features and existing acoustic tokens. A critic head is introduced to perform self-critic training and sampling.\nWe use pretrained Descript Audio Codec (DAC) [25] as audio tokenizer to model clean voice. Through 9 residual vector quantizer layers, we obtain a sequence of tokens of shape $X_0 \\in R^{9 \\times T}$ of clean audio x. At timestep t, we mask a subset of tokens in $X_0$ to obtain $X_t$. To deal with residual layers, we employ 9 embedding layers for each residual layer and sum over the residual layers to obtain the token latent $h_{token} \\in R^{T \\times dim}$.\nDuring the semantic enhancement stage, the input of the encoder is the power-law compressed magnitude STFT spectrogram [30] of the distorted audio $D(x)$ and task embedding $h_{task}$. The encoder consists of $L_{enc}$ self-attention transformer layers to obtain the latent representation $h_{enc} \\in R^{T \\times dim}$. Inspired by representation alignment (REPA) in the field of image generation [31] and semantic distillation in MaskSR2 [26], we introduce a representation alignment module to align the encoder output. The semantic feature of the clean audio is extracted using self-supervised learning (SSL) models, such as w2v-BERT [32]. Let f be a SSL encoder, $z = f(x) \\in R^{T \\times dim}$ be an encoder output (linear interpolation is used to match time dimension T and feature dimension dim), a multilayer perceptron MLP is used to project the encoder output with the clean audio's semantic feature:\n$\\mathcal{L}_{REPA} = \\mathbb{E}_x [ - \\frac{1}{T} \\sum_{t=1}^T sim (z[t], MLP(h_{token}[t])) ]$\nwhere the similarity function is mean-square error (MSE). The final output of the semantic enhancement stage is the sum of encoder's output and noisy semantic feature $h_{semantic} = h_{enc} + f(D(x))$.\nDuring the acoustic enhancement stage, discrete diffusion process is employed. The condition consists of semantic features and task embeddings $C = \\{h_{semantic}, h_{task}\\}$. The decoder consists of $L_{dec}$ self-attention transformer layers. Finally, 9 linear layers project the output of the last decoder layer back to logits of each residual layers. $\\mathcal{L}_{mask}$ is computed between the predicted masked token and the target token."}, {"title": "B. Prompt Guidance", "content": "We introduce a prompt-guidance scheme (see Figure 2). The model can be trained using both configurations: without prompt and with prompt. During training, we have a probability $P_{prompt}$ to keep the first 3 seconds of clean audio unmasked as the prompt. The same place of the distorted audio is set to silence. The prompt section is not considered for loss calculation. During inference, the model can infer both with a prompt audio and without prompt audio:"}, {"title": "C. Self-Critic Sampling", "content": "The sampling strategy described in Section III-B is based on a greedy approach, which indicates that the sampling process is non-regrettable, thus cannot recover already filled tokens. Moreover, the confidence score of each token is determined independently (by simply taking the model's logits as the confidence score). To address these issues, we introduce Self-Critic sampling strategy.\nThe core idea of critic-sampling is to use a model as a critic or discriminator to evaluate whether a token is real or generated by the model. After we first sample $\\hat{X}_0$ from $p_\\theta(\\hat{X}_0 | X_t, C)$ and the model's logits, the unmasked tokens in $X_t$ are copied into the output to form $\\tilde{X}_0 = \\hat{X}_0 \\odot M_t + X_t \\odot (1 - M_t)$. The critic model (parameterized by $\\phi$) is trained to estimate $M_t$ from $\\tilde{X}_0$ using binary cross-entropy loss:\n$\\mathcal{L}_{critic} = \\mathbb{E}_{X \\in \\mathcal{D}, t \\in [0,1], \\hat{X}_0 \\sim p_\\theta(\\hat{X}_0 | X_t, C)} [- \\sum_{i=1}^N m_{t,i} log (p_\\phi(m_{t,i} | \\tilde{X}_0, C))]$\nFor Token-Critic [17], a separate critic model $\\phi$ is trained. Inspired by SCRIPT [33], which was originally proposed for pretraining masked language models such as BERT [34], we use the generative model itself as the critic model. We introduce an additional linear layer, referred to as the critic head, to replace the model's logits projection layer. During training, we feed $\\tilde{X}_0$ back into the model itself with the critic head to produce the critic score $\\hat{M}_t$ and minimize the binary cross-entropy loss between $\\hat{M}_t$. Self-Critic eliminates the need for training a separate critic model and still improves the masked generative model's performance by providing a more accurate confidence score. The final loss is:\n$\\mathcal{L} = \\mathcal{L}_{mask} + \\mathcal{L}_{REPA} + \\mathcal{L}_{critic}$."}, {"title": "V. EXPERIMENTS", "content": "Training Datasets The training datasets used in our experiments include speech, singing voice, noise, and room impulse responses (RIRs), with detailed statistics shown in Table II. For speech data, we used 24kHz speech from Emilia [14] Subset (filtered with DNSMOS OVRL > 3.4, totaling ~21k hours) and fullband speech from datasets such as VCTK [35], HiFi-TTS [36] (~1k hours), and AI-Shell3 [37]. Singing voice data include OpenSinger [39], PopCS [40], and others (~600 hours). Noise data are sourced from the musan noise subset [46], Urgent Challenge noise data [21], FSD50K [47], DESED [48], and the TUT Urban Acoustic Scenes 2018 dataset [49], along-"}, {"title": "E. Experiment Results", "content": "1) Evaluation on Various Enhancement Tasks: We evaluate the performance of ANYENHANCE across multiple speech and singing evaluation datasets on different tasks. See Table IV, the performance of ANYENHANCE is consistently superior or competitive across all tasks and datasets. In GSR Group, for the Voicefixer GSR dataset, ANYENHANCE achieves the highest OVRL (3.136), NISQA (4.308), and similarity score (0.924), surpassing all baselines. Similarly, on the Librivox GSR dataset, ANYENHANCE demonstrates the best results in SIG (3.546), OVRL (3.308), and Similarity (0.955), highlighting its robustness. On the CCMusic GSR dataset, ANYENHANCE outperforms other methods in NISQA (3.345) and achieves competitive OVRL (2.797) and similarity (0.915), further demonstrating its effectiveness under singing voice domain.\nIn the SE Group, ANYENHANCE demonstrates competitive performance across both DNS No Reverb and DNS With Re-verb datasets. For DNS No Reverb, ANYENHANCE achieves the highest scores in OVRL (3.418), NISQA (4.821), and Similarity (0.988), establishing its robustness in speech denoising. For DNS With Reverb, ANYENHANCE achieves the highest scores in SIG (3.500) and OVRL(3.204), and second-highest scores in BAK, NISQA and Similarity. Unlike most baseline models that are limited to 16 kHz outputs, ANYENHANCE supports full-band restoration, significantly enhancing its applicability to high-quality audio tasks.\nFor the SR Group, ANYENHANCE achieves the highest BAK, OVRL, SpeechBERTScore and Similarity scores on both Voicefixer SR and CCMusic SR dataset, and highest SIG score on Voicefixer SR dataset, outperforming all baselines.\nAdditionally, in the TSE Group, ANYENHANCE delivers competitive results, achieving the highest scores across SIG, OVRL, NISQA metrics on both Librimix TSE and VCTK TSE datasets. Particularly excelling in challenging noisy scenarios (VCTK TSE) - achieving highest scores across all metrics. The VCTK TSE dataset, with its complex simultaneous distortions, further highlights the limitations of current single-task models, which lack the multitask capability required to handle such diverse challenges effectively.\nOverall, ANYENHANCE exhibits robust generalization and effectiveness across diverse enhancement tasks in both speech and singing domains, outperforming or matching state-of-the-art baselines across various metrics."}, {"title": "VI. CONCLUSION", "content": "In this work, we introduce ANYENHANCE, a unified voice enhancement model for both speech and singing voice. Using a masked generative model as the base, ANYENHANCE is capable of addressing diverse enhancement tasks, including speech denoising, dereverberation, super-resolution, and target speaker extraction. We incorporate a prompt-guidance mechanism for in-context learning, which allows the model to natively accept a reference speaker's timbre without altering the underlying architecture. This strategy boosts enhancement performance when a reference audio is available and enable the target speaker extraction task without altering the underlying architecture. Moreover, we integrate a self-critic mechanism into the generative process for masked generative models, yielding higher-quality outputs through iterative self-assessment and refinement. Extensive experiments on various enhancement tasks demonstrate that ANYENHANCE outperforms existing methods in terms of both objective metrics and subjective listening tests."}]}