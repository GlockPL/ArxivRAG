{"title": "Affectively Framework: Towards Human-like Affect-Based Agents", "authors": ["Matthew Barthet", "Roberto Gallotta", "Ahmed Khalifa", "Antonios Liapis", "Georgios N. Yannakakis"], "abstract": "Game environments offer a unique opportunity for training virtual agents due to their interactive nature, which provides diverse play traces and affect labels. Despite their potential, no reinforcement learning framework incorporates human affect models as part of their observation space or reward mechanism. To address this, we present the Affectively Framework, a set of Open-AI Gym environments that integrate affect as part of the observation space. This paper introduces the framework and its three game environments and provides baseline experiments to validate its effectiveness and potential.", "sections": [{"title": "I. INTRODUCTION", "content": "Video games are ideal stimuli for research in Affective Computing [1] for several reasons. Firstly, the user is free to play in many different ways, leading to diversity in their play traces and emotional experiences [2]. This freedom allows for deeper research into the relationship between behaviour and emotions compared to static stimuli such as videos. Games also encompass multiple modalities for modelling, such as pix-els [3], game states [4], and controller inputs [5]. Games have been at the forefront of research on reinforcement learning (RL), especially leveraging Open-AI Gym environment [6]. Indicatively, popular RL algorithms such as Proximal Policy Optimisation [7] have been tested on gameplaying tasks on the Atari suite of environments [6]. While RL agents focus on playing to win [8], there are no environments that incorporate human affect for training emotion-aware gameplaying agents.\nTraining virtual agents that not only respond to game states but also human affect enables us to create a more holistic agent that can model and mimic human players better than training an agent to beat a game [9]. Cutting-edge RL algorithms like Go-Explore [10] have been successfully applied to train agents that exhibit affective responses in line with target human personas [9] and use affect as a mechanism for beating the game [11]. Similar works have used RL to train affect-based virtual agents via simulated, rather than human-like, emotions [12], as well as intrinsic motivation to imitate human gameplay demonstrations [13]. However, we believe there is a significant hurdle for future research: developing the game environments and sourcing human gameplay and affect demonstrations is a time-consuming resource-intensive process. Without publicly available environments\u2014and benchmarks-tied to large-scale"}, {"title": "II. AFFECTIVELY FRAMEWORK", "content": "The Affectively Framework follows the RL feedback loop of Open-AI Gym [6]. At each time step, the framework provides the agent with a game-state ($S_t$) and expects an action ($A_t$) in return. Based on the agent's action, it returns an updated game-state ($S_{t+1}$) and an environment score ($R_E$) that reflects the quality of the action, based on the environments' scoring system. Using $R_E$, we construct a behaviour reward ($R_B$) for each game with additional domain knowledge in order to help the agent during training and combat the sparsity of $R_E$. The Affectively Framework slightly modifies the typical training loop by including affect values, periodically generated every 3 seconds using a human affect model (see Section II-B). Using the affect values generated by the human model, we provide an affect reward ($R_A$) for training agents. In this paper, $R_A$ aims to maximise arousal, but more complex reward functions such as imitating affect [9], [11] can be used. Using these components, the total reward $R_t$ to the agent is calculated at each time step as per Eq. (1).\n$R_t = (1 - \\lambda) \\cdot \\eta(R_B) + \\lambda \\cdot R_A$ (1)\nwhere $\\lambda$ controls the importance of $R_B$ or $R_A$, and $\\eta(R_B)$ is the normalised behaviour reward per game based on their extreme reward values. Below, we describe the game environ-ments (Section II-A) and the affect model for deriving human arousal (Section II-B)."}, {"title": "A. Environments", "content": "The Affectively Framework includes three game environ-ments, although it is designed for extensibility with more games in future work. The game environments span three different game genres: side-scrolling platformers (Pirates), shooters (Heist), and racing (Solid Rally). These games are based on the AGAIN dataset [4], which included nine games with annotated gameplay sessions in each of them. We chose these three games for their diversity in genre, action/state space, and gameplay goals.\nPirates is a 2D platformer heavily inspired by Super Mario Bros. (Nintendo, 1985). In this game, the player moves from left to right and must reach a goal (exit) in under 2 minutes. During the level traversal, the player must avoid obstacles and enemies, as well as pick up coins and power-ups to improve their score. If the player dies, by falling off-screen or by colliding with an enemy, they respawn at the closest checkpoint in the level. The agent's action space consists of 2 discrete action branches. The first covers horizontal movement: the agent can move left (-1), stay still (0), or move right (1). The second covers jumping, where the agent can stay still (0) or jump (1). The environment's observation space ($S_t$) consists of an 11\u00d711 grid of integer values corresponding to IDs of the entities within the grid, with the player always at the centre (blue). The IDs are assigned on a priority basis, with enemies (red) having the highest priority, followed by coins and power-ups (green), breakable tiles (yellow), obstacles (grey), and empty space (white). Seven additional properties are appended to $S_t$, consisting of the agent's physics variables (e.g., velocity, current direction) and game-specific properties (e.g., current health and power-up status). For Pirates, $R_E$ is the total coins collected (10 points each) and power-ups collected (20 points each), with a maximum possible score of 460 points if the agent exhaustively searches the entire level. $R_B$ is calculated at each time step as per Eq. (2), as the change in $R_E$ from the previous frame (i.e., if the agent just picked up a coin or power-up), with two additional components: a small reward ($M = 0.1$) if the agent moved to the right of the screen in the last frame (to encourage level traversal), and a penalty for dying ($D = 5$) to discourage reckless behaviour.\n$R_B = \\Delta R_E + M - D$ (2)\nHeist is a first-person shooter game where the player must explore the map and eliminate all 25 enemies in the area within the 2-minute time limit. The player has infinite ammo, but their weapon holds 11 bullets and reloads automatically when it is empty. If the player takes enough damage from enemies' bullets, they die and respawn at the beginning of the level; eliminated enemies do not respawn. The environment's $S_t$ consists of a 9\u00d79 grid of IDs using the same approach described in Pirates, this time within the field of view of the player. The possible IDs for tiles in the grid are obscured (i.e. out of sight, in pink), empty space (yellow), obstacles (red), and enemies (blue). In addition to the grid, the agent is supplied with a vector of 20 observations containing spatial and physics variables as well as game-specific variables such as health, ammo, and a vector to the nearest visible enemy. The player's action space is the most complex of the three games, with 3 discrete action branches and 2 continuous actions. The first discrete action covers horizontal movement, allowing the agent to strafe left (-1), keep straight (0), or strafe right (1). The second discrete action covers depth movement, where the agent can move forward (1), stay still (0), or move backward (-1). The third discrete action covers shooting, where the player can shoot (1) or do nothing (0). The continuous actions (with values between -1 and 1) govern the horizontal and vertical movement of the camera (i.e., aiming the weapon). For Heist, $R_E$ is the number of kills the player has made so far, with each kill worth 20 points. The maximum possible score is 500 points (20 \u00d7 25 enemies). At each time step RB is calculated as per Eq. (3), as the changes in $R_E$ from the previous time step (i.e., if the agent has just killed an enemy), with two additional components: a small exploratory reward (E = 1) every time the agent enters a new area of the map (the map is partitioned a priori into a grid of 5\u00d75\u00d75 cubes) and the inverted angle (A) between the agent and the nearest"}, {"title": "B. Affect Model", "content": "The game environments selected for the Affectively Frame-work were originally built for the AGAIN dataset, where each environment comes with over 122 human game sessions annotated in a first-person manner in terms of arousal [4]. Our framework uses the same affect model for all the environ-ments, which is based on arousal transitions within the corpus of human affect annotations in similar game-state transitions as the ones encountered by the agent at this point. The affect model is built using the k-nearest neighbours algorithm [14] (KNN), with k = 5. We follow the same methodology for deriving this affect model across all three games, based on successful arousal modelling attempts when the dataset was collected [4]. For each environment, every annotated play trace is processed as consecutive 3-second time windows: each entry contains the mean arousal value of that time window along with a feature vector P of game-specific variables such as car speed, player health, and current score. These variables P are used in AGAIN [4] and are different from the game-state variables ($S_t$) described in Section II-A. We construct pairs of consecutive time windows from these gameplay sessions, and label affect in terms of decreasing, increasing, or stable between the two time windows. As with previous studies [4], we discard stable arousal data points from the corpus.\nDuring execution, the affect model calculates the agent's current (and previous) feature vector P every 3 seconds of in-game time (via averaging). It then queries the dataset for the 5 nearest neighbours using an inverse-distance weighted averaging (i.e., the closer the neighbour is to the current state, the more influence it has on the output) as used in previous studies [9]. This means that the model finds the closest transitions (in terms of 3-second time windows) made by humans in the corpus and produces a weighted average of their arousal transition (increase or decrease) weighted by the P distance. The result is a value between 0 (all agree on decrease) and 1 (all agree on increase). We test the same affect reward ($R_A$) in all three environments, which is to maximise arousal (rewarding state transitions that increase arousal) generated by the KNN model at each time window. It is important to note that since the model generates values every 3 seconds, Afft = 0 for any time steps in between."}, {"title": "III. EXPERIMENTAL PROTOCOL", "content": "In this paper, we evaluate the Affectively Framework by training RL agents to maximise in-game performance, arousal, or a combination thereof. For each evaluation, we report the final $R_E$ and the $R_A$ of their arousal trace taken at the end of the session. We detail the baselines (random agent and human demonstrations) and the RL agent that uses Proximal Policy Optimisation (PPO) below:"}, {"title": "IV. RESULTS", "content": "Table II shows the final in-game score ($R_E$) normalised to [0,1] based on the maximum possible score per game (see Section II-A), as well as the $R_A$ for the PPO agents and the baselines described in Section III. As expected, a random agent is not capable of effectively playing any of the three environments, achieving very low scores ($R_E$) in all 30 evaluation runs. When it comes to affect, random agents did not exhibit any consistent pattern across games: agents would get stuck very quickly and generate the same arousal values repeatedly until the end of the episode.\nThe PPO agents, even when rewarded only based on RB, played every game worse than the humans in the AGAIN corpus and reached much lower final scores ($R_E$). This can be somewhat expected from the short training times. Heist proved to be particularly challenging for the agent, likely due to its complex action space (with 2 continuous actions) and challenging gameplay (requiring both way-finding and aiming). While the final scores of the RB-based agent were approximately half those of the average human for Solid Rally and Pirates, the agents' behaviour was robust. However, for Pirates the agent tended to rush to complete the level quickly and sometimes ignore collectables, likely due to the (constant) Mr component of RB in Eq. (2) that rewards moving right. For Solid Rally, where moving forward quickly was the only type of desired gameplay, this sufficed to perform well.\nWhen PPO agents were trained to maximise arousal (RA), the agents significantly improved their average arousal values, surpassing the random agent and the human demonstrations alike. However, the behaviour of these agents was very poor across games (with RE = 0 in all 30 runs and all three games). We observed that the agents would quickly find a region of the environment close to the start with high arousal and exploit that area, rather than explore the environment. This illustrates the deceptive reward space when rewarding affect alone. Using an RL agent which supports and rewards exploration, such as Go-Explore [10], will likely improve this PPO agent's behaviour as seen in previous studies [9].\nPPO agents trained on blended reward showed an in-consistent trend across game environments. In Solid Rally, the blended agent managed to improve upon its final score compared to the pure behaviour optimiser, but had the worst arousal score. In this case, the behaviour reward overpowered the affect reward given its richer and less deceptive nature. The opposite was true in Pirates, which featured large increases in arousal at the very start of the game. The affect reward (RA) thus overpowered the behaviour reward (RB), leading the agent to stay largely still similar to the arousal optimising PPO agent. Parsing the training data, we observed that the agent initially explored deeper into the level, achieving the best RE score of 0.106 in the first 50 episodes, but quickly converged on exploiting the starting area of the level which exhibits high arousal in the annotation traces. While we observed a more balanced outcome (in terms of RE and RA)"}, {"title": "V. DISCUSSION", "content": "Experiments with the built-in agents and reward systems provided by the Affectively Framework highlighted the in-herent challenge of optimising affect in gameplaying agents. Affect, as derived from similar game state transitions in a corpus of annotated human gameplay traces, can be both deceptive and overpowering as an RL reward. In our results, we found that balancing $R_A$ and $R_B$ in the blended reward function is a significant challenge for RL algorithms such as PPO, and requires more tuning. We acknowledge that a major limitation is the sparsity of $R_A$ rewards, which are provided once every 3 seconds (and are not triggered by a specific in-game event apart from the game clock); this may confuse the agent, especially for blended reward schemes. Future work should explore alternatives, such as a moving time window for deriving agents' parameter vector to match human demonstrations (see Section II-B), or copying the last calculated $R_A$ reward into the reward scheme of Eq. (1).\nMoreover, the size of AGAIN's corpus allows us to find more relevant neighbours for unseen game-state transitions made by the agent. However, using only a subset of these gameplay traces could lead to better behaviours. In our previ-ous study on Solid Rally [9], clustering play traces into four clusters allowed us to train more high-performing agents based on expert players alone, using a blended reward. We plan to enhance the Affectively Framework with pre-made clusters of players as part of all three environments, so that future agents can be trained with a particular target persona in mind (e.g. imitate \"Expert\" behaviour and arousal).\nA straightforward area for improvement is to expand the current experiments with longer training times and hyper-parameter tuning, in order to ensure optimal performance of the PPO algorithm. More experiments with different RL algorithms and affect rewards, especially in light of the poor behaviour of PPO agents that maximise arousal, are necessary. Leveraging the Affectively Framework for research into RL agents in multiple games, including our previous work on Go-Blend [9] in Solid Rally, is a major motivation for this software release. Value-based RL algorithms, such as deep RL [16], are also interesting additions to our baseline agents. Due to the complexity of the action spaces (especially in Heist), integrating deep RL would require some work. Finally, an interesting direction for future research is to use a single observation space (e.g. using the screen's pixels) across games to enable more in-depth studies on the generalisability of RL agents."}, {"title": "VI. CONCLUSION", "content": "This paper introduced the Affectively Framework, a frame-work compatible with the standard Open-AI Gym API [6] for training game-playing agents that not only care about behaviour but also affect. We introduced three game environ-ments and tested them with two baseline agents (a random agent and a PPO agent) in various behaviour and affect tasks. By laying the groundwork for future research through an easy-to-use software package and test bed environments, future studies can shed light on the relationship between player behaviour and their affect response during gameplay."}, {"title": "ETHICAL IMPACT STATEMENT", "content": "This paper makes use of an existing dataset [4] of human demonstrations collected from crowd workers on the Amazon Mechanical Turk platform. This dataset is publicly available, and participants gave their consent for their data to be stored and utilised in an anonymous fashion. To the best of our knowledge, there is no significantly negative application of the methods we use in this paper and no added privacy or dis-crimination risk. The data used in the paper and environments introduced are publicly available for scientific reproducibility and for extensions of this study."}]}