{"title": "Preference Alignment Improves Language Model-Based TTS", "authors": ["Jinchuan Tian", "Chunlei Zhang", "Jiatong Shi", "Hao Zhang", "Jianwei Yu", "Shinji Watanabe", "Dong Yu"], "abstract": "Recent advancements in text-to-speech (TTS) have shown that language model (LM)-based systems offer competitive performance to their counterparts. Further optimization can be achieved through preference alignment algorithms, which adjust LMs to align with the preferences of reward models, enhancing the desirability of the generated content. This study presents a thorough empirical evaluation of how preference alignment algorithms, particularly Direct Preference Optimization (DPO), enhance LM-based TTS. With a 1.15B parameter LM-based TTS model, we demonstrate that preference alignment consistently improves intelligibility, speaker similarity, and proxy subjective evaluation scores, with the latter two metrics surpassing even human speech in certain evaluations. We also show preference alignment is applicable to low-resource scenarios and effectively generalized to out-of-domain applications.", "sections": [{"title": "I. INTRODUCTION", "content": "Text-to-speech (TTS) aims to synthesize human speech from the given text and, optionally, non-text conditions [1]. Traditionally, mainstream TTS systems operate in continuous space [2]\u2013[4]. Recent advancements in audio coding have enabled high-quality audio tokenization [5]\u2013[9]. The tokenization allows TTS models to function effectively in discrete space [10], particularly through the use of language model (LM)-based approaches [11]\u2013[19]. LM-based approaches are featured for the simplified training and inference pipeline, enabling the model to learn the relationships between input and output sequences more efficiently. These systems have gained popularity, achieving state-of-the-art performance by scaling up both data volume and parameter sizes [20]. They also exhibit remarkable zero-shot capabilities in tasks such as speaker identity cloning [11] and cross-lingual synthesis [15].\nDespite these advances, generating high-quality, natural-sounding speech requires not only scaling up but also being aligned with human perception. Preference alignment (PA) is a set of training algorithms widely employed in text-based LM development. The goal of PA is to align model outputs with specific preferences, which are abstract and challenging to learn by maximize-a-posterior (e.g., cross-entropy loss) [21]. Typically formulated as a reinforcement learning problem, PA first models the preferences by a reward model and then uses the reward model to guide LMs toward generating content that maximizes reward values [22], [23]. When these preferences are derived from humans, the process is widely known as human feedback reinforcement learning (HFRL). Recent advancements in PA allow for solving the optimization problem in a closed form, eliminating the need for explicit reward modeling, which significantly simplifies and stabilizes training [24]\u2013[28]. PA (or HFRL) is verified effective in understanding and following highly abstract preference (e.g., human value), and has become a common practice to ensure that text LMs exhibit desirable traits, such as helpfulness, truthfulness, and harmlessness [29], [30].\nAlthough preference alignment methods are widely adopted in text LLM development, they are less explored in the speech/audio community, particularly the LM-based TTS. SpeechAlign [31] explored multiple preference alignment methods on LM-based TTS, but only used ground truth as the positive examples. UNO [32] optimized on unpaired preference data and considers the annotation uncertainty in subjective evaluation. RIO [33] leverages the Bayesian principle to select preference data. It is reported that industrial systems, such as SeedTTS [14], adopt DPO and PPO in their human preference alignment stage. Besides TTS, TANGO2 [34] applies DPO to diffusion-based text-to-audio systems.\nIn this work, we apply a PA objective, direct performance optimization (DPO) [24], to LM-based TTS systems, guiding them to generate speech that is preferred across multiple metrics. Although prior works have preliminarily verified the feasibility of PA in TTS, this work provides transparency and details details in its implementation. We address multiple key practical issues, including (1) preference pair selection; (2) hyper-parameter search; (3) effect of length normalization; (4) metric selection; (5) effect of supervised fine-tuning (SFT); (6) label efficiency; (7) iterative optimization; (8) out-of-domain evaluation. Our baseline model, with 1.15B parameters, is trained on 55k hours of open-source English speech data. We demonstrate that applying PA to this baseline model significantly improves its intelligibility, speaker similarity, and proxy subjective evaluation scores, even outperforming human ground truth in the latter two metrics. Additionally, we show that preference alignment can be implemented with as little as 1 hour of data, and its improvement can be effectively generalized to out-of-domain scenarios."}, {"title": "II. PREFERENCE ALIGNMENT ON LANGUAGE MODEL-BASED TTS", "content": "TTS is a conditional generation task that generates speech signal y based on the given conditions x, such as input text string s and other non-textual cues. For simplicity, this work assumes a short clip from the same speaker of y, i.e., $y_{ref}$, is the only non-textual cue, from which features like speaker identity and prosody can be imitated. Thus, the training objective of TTS is to maximize the posterior:\n$\\max P_{\\theta}(y|x) = \\max P_{\\theta} (y|s, y_{ref})$\nwhere $\\theta$ is the trainable parameter of the model.\nIn the context of discrete space modeling, particularly LM-based TTS, all audio y, $y_{ref}$ can be converted into discrete codes by audio codec encoding [5]\u2013[9], s.t., $y_{d}$, $y_{ref}^{d}$. Text input s can also be tokenized into a integer vector $s_{d}$. By splicing $s_{d}$ with $y_{ref}^{d}$ and $y_{d}$, the example sequence [$s_{d}$, $y_{ref}^{d}$, $y_{d}$] is formed and then learned by a language model. Cross-entropy loss is applied to $y_{d}$ so that the posterior $P_{\\theta}(y_{d}|s_{d}, y_{ref}^{d})$ is maximized. During inference, the predicted sequence $\\hat{y}_{d}$ is first generated by an LM, and then the output speech $\\hat{y}$ can be reconstructed from it using audio codec decoding.\nUsually, audio codec models tokenized each frame of audio into $n_{q}$ codes ($n_{q} > 1$), which makes the example sequence [$s_{d}$, $y_{ref}^{d}$, $y_{d}$] $\\in$ $\\mathbb{Z}^{T \\times n_{q}d}$ a two-dimensional sequence\u00b9 T stands for number of frames. As standard LMs work with one-dimensional sequence, modeling the sequences with the extra $n_{q}$-dimension is non-trivial [35]. This work adopt Multi-Scale Transformer [13] as the model architecture, which first uses a global Transformer to predict an embedding for each audio frame; and then a local Transformer predicts the $n_{q}$ codes sequentially based on each frame embedding. Both global and local Transformers are causal. Like standard LM, Multi-Scale Transformer also predicts the code-level posterior for each audio code within each frame, which is then used in loss computing (e.g., cross-entropy) and model inference. For simplicity, in the rest of this paper, we re-name the conditional sequence as x = [$s_{d}$, $y_{ref}^{d}$] and the target sequence y = [$y_{d}$], both are in discrete space."}, {"title": "B. Preference Alignment", "content": "Cross-entropy objective in LM-based TTS training is to maximize the posterior of target sequence y. However, higher posterior in y (and the corresponding waveform reconstructed from it) does not necessarily lead to content that is more preferred by human perception or other proxy metrics [36]. Alternatively, PA is an approach that directly optimizes the LM toward these preferences and thus improves the sample quality [21].\nProblem Formulation: PA is usually described as a reinforcement learning problem: assume there is a latent reward model $r^{*}(x, y)$ that represents the preferences by a scalar reward, higher means more preferred. Thus, with the given reward model, the optimization objective is to guide the LM to pursue a higher expected reward:\n$\\max_{ \\theta} E_{y \\sim P_{\\theta}(y|x)} [r^{*}(x, y)] - \\beta \\cdot D_{KL}[P_{\\theta}(y|x)||P_{ref}(y|x)]$\nwhere the latter term is a KL-divergence constraint to avoid the LM $P_{\\theta}$ drifting too far away from a reference model $P_{ref}$. $\\beta$ is a hyper-parameter, larger means stronger constraint. The choice of $\\beta$ is explored in Sec.III-B2. In common practice, the reference model $P_{ref}$ is initialized identically with $P_{\\theta}$ and is frozen during training.\nConventionally, the optimization in Eq. (2) works with an explicit reward model [23]. As the latent reward model is usually unavailable, a proxy reward model $r_{\\phi}(x, y)$ is first built from the preference dataset instead. Subsequently, the optimization is conducted using proximal policy optimization (PPO) [22]. This workflow is com-plicated and PPO sometimes encounters instability issues in training [37]. Recent advances in PA demonstrate that, under certain circumstances, the optimization in Eq. (2) can be solved in close form without building an explicit reward model. A representative approach is Direct Preference Optimization (DPO).\nDirect Preference Optimization (DPO): DPO deals with a special case where the preference data is win-lose pairs: with the same conditions x, the probability of $y_{w}$ being more preferred than sequence $y_{l}$ follows Bradley-Terry model [38]:\n$P(y_{w} > y_{l}|x) = \\frac{exp(r^{*}(x, y_{w}))}{exp(r^{*}(x, y_{w})) + exp(r^{*}(x, y_{l}))}$\nSo that, with the known preference data triplets ($x, y_{w}, y_{l}$), an explicit proxy reward model $r_{\\phi}$ can be trained by maximum-a-likelihood criterion, with $\\sigma$ being the sigmoid function:\n$\\max E[log \\sigma(r_{\\phi}(x, y_{w}) - r_{\\phi}(x, y_{l}))]$\nAlso, it has been proved that, in Eq. (2), the LM $P_{\\theta}(y|x)$ becomes optimal if and only if:\n$\\frac{P_{\\theta}(y|x)}{P_{ref}(y|x)} = e^{\\frac{1}{\\beta} r_{d}(x, yw, y)} \\cdot e^{Z(x)}$\nwhere $Z(x)$ is termed as partition function that is independent of the generation target y.\nFinally, substituting Eq. (5) into Eq. (4) excludes the reward model $r(x, y)$; training the explicit reward models is then transformed as direct optimization over the LM $P_{\\theta}(y|x)$:\n$L_{DPO} = -E [log \\sigma (\\beta \\cdot (log \\frac{P_{\\theta}(y_{w}|x)}{P_{ref}(y_{w}|x)} - \\beta \\cdot log \\frac{P_{\\theta}(y_{l}|x)}{P_{ref}(y_{l}|x)}))]$\nDPO on LM-based TTS: Our DPO training starts from a baseline LM-based TTS model pre-trained with cross-entropy loss (detailed in Sec. III-A). Specifically, any posterior $P_{\\theta}(y|x)$ in Eq. (6) are computed by flattening the two-dimensional y into row-first sequence and then summing the code-level log-posterior in auto-regressive format. To align with human perception, it would be ideal if the preference data pairs ($x, y_{w}, y_{l}$) can come from human labelers. Instead, this work adopts several pre-trained metric models as the proxy of real human preferences. With the same condition x, utterances are first scored by these models; the utterances with better/worse scores are set to $y_{w}$ and $y_{l}$, respectively. These metric models are also detailed in Sec. III-A. $y_{w}$ and $y_{l}$ can be either generated from the LM or from natural speech, which is explored in Sec.III-B1."}, {"title": "III. EXPERIMENTS", "content": ""}, {"title": "A. Experimental Setup", "content": "Data, Task Setup, and Tokenization: We build our baseline model with LibriSpeech [39], GigaSpeech [40] and the English part of Multilingual LibriSpeech [41], summing up to around 55k hours. Following [11], speaker IDs are always available for all datasets and are used to select a 3-second speech clip from the same speaker2. All input text is tokenized into phone sequences by g2p-en\u00b3 before language modeling. We adopt our reproduced SoundStream [5] model for audio tokenization, which is configured as 50 frames per second and 8 codes per frame, i.e., $n_{q}$ = 8.\nModel: We adopt Multi-Scale Transformer [13] as the model archi-tecture. The global Transformer has 25 layers, each of which has an attention size of 1600, a feedforward size of 6400, and 25 attention heads. Those numbers for the local Transformer are {6, 384, 1536, 6} respectively. The total trainable parameters are 1.15B.\nTraining and Inference: The baseline model is updated by 1M steps with the global batch size of around 80k frames. AdamW optimizer [43] with a peak learning rate of 2e-4 is adopted, with 70k warmup"}, {"title": "B. Experiments and Analysis", "content": "We first demonstrate the performance of our optimal model El and our baseline model in table I. Although the baseline model already achieves comparable performance with popular systems, applying DPO still achieves significant improvement in all three metrics. We detailed our exploration step-by-step as follows.\nAll PA experiments are based on LibriSpeech. As LibriSpeech is already included in baseline model training, this setup excludes the impact of introducing unseen high-quality data. We conduct inference on the whole train-960 set for follow-up preference data curation. Our exploration is based on DPO with a conservative setup in the initial trials: a constant learning rate of 3e-7 and $\\beta$ = 0.1. Empirically, we find DPO is sensitive to the number of updates, so we use a large batch size to ensure only 350 updates are made within one epoch.\n1) Preference Pair Selection: We examine two solutions to how preference data pairs are curated. A2: use ground truth as $y_{w}$ and a randomly selected generated example as $y_{l}$. This assumes the ground truth always outperforms generated examples. B2: rank all generated samples using certain preference metrics; select the top 20% best and worst examples as $y_{w}$ and $y_{l}$ respectively. For simplicity, we only use SPK_SIM to rank these examples.\nWe evaluate A2 and B2 by every 50 updates, the results are in Fig.1. It is clear that B2 outperforms the baseline and A2, especially on the SPK_SIM and Proxy MOS metrics. We further compare the win rate of A2 and B2. As suggested in Fig.1.d, the win rate of A2\n2) Hyper-Parameter Search: Based on A2 and B2, we extend to A1 and B1 with $\\beta$ = 1; A3 and B3 with $\\beta$ = 0.01. Fig.1 suggests that the results achieved by 300 updates are nearly optimal, so we only evaluate the models with the same number of updates. The results are in Fig.2.\n3) Effect of Length Normalization: It is mentioned in [14] that standard DPO will lengthen the generated examples while [26] mentioned that length normalization can be used for regularization. So we applied length normalization to all posteriors in Eq. (6) and did experiments C1-3 with $\\beta$ = {1,0.1,0.01}. The results are in Fig.2.\nWe find that DPO with length normalization can consistently improve the baseline model. Since C1-3 under-perform B3, we still proceed without length normalization. Additionally, we observe that the examples generated by B3 are 5.1% longer than the ground truth, while that number for C3 is 4.1%, so that, applying length normalization does not have a noticeable impact on the lengths of the generated examples. We also observe in C1-3 that applying length normalization increases the robustness toward $\\beta$ choices.\n4) Metric Selection: So far we only selected the preference pairs by SPK_SIM metric. We then change the metric to Proxy MOS\nand WER (D1). Additionally, we combine the ranking results of SPK_SIM, Proxy MOS, and WER in a naive way (D3). The results are in Tab.II.\nAs suggested in the table, adopting any metric for preference pair selection and then applying DPO can improve the baseline model on all three metrics consistently. For Proxy MOS and SPK_SIM, the optimal performance is achieved when the corresponding model is adopted for preference pair selection (B3 and D2). Applying WER alone yields the worst WER result among all 4 DPO experiments (D1). We conjecture that the concept of WER focuses on the local errors within speech while DPO considers the whole sequences, which makes WER less ideal for DPO training. Using all metrics (D3) achieves encouraging and balanced performance on all metrics, so later on we proceed with D3 setup.\n5) Effect of Supervised Fine-Tuning: It is a common practice for the pre-trained model to experience supervised fine-tuning (SFT) before the preference alignment stage [29]. Thus, we fine-tune the baseline model on the $y_{w}$ of D3 for one epoch before the DPO training, using the same learning schedule as in Sec.III-A (E1). The results are in Tab.II. It indicates that applying this SFT training to the baseline model provides marginal improvement after DPO training. For simplicity, we proceed without this SFT stage.\n6) Label Efficiency: Our DPO training leverages the full Lib-riSpeech training set, which is overly abundant. We then reduce the training data volume to {100, 10, 1} hours. We conduct experiment with two setups. F1-3: reduce the batch size to 10% of the original but keep the number of updates unchanged; F4-6: keep both batch size and number of updates unchanged. This change means the data will be used for more than one epoch in F2-6.\n7) Iterative Optimization: So far all experiments leverage the preference pairs generated by the baseline model. It would be more desirable if these pairs could be generated online by the model under training. As an approximation, iterative optimization [30], [51] is to repetitively generate preference pairs by the DPO-trained model in the last round and then leverage these pairs to train the next model. For cost reason, we examine iterative optimization only with the train-clean-100 subset. Starting from F4, we iterate the model for one more rounds, which yields G1. The results are shown in Table.III.a. After multiple trails, we find the iterative optimization is fragile and we cannot achieve further improvement on G1.\n8) Out-of-Domain performance: Given the success on the in-domain test set (LibriSpeech test-clean), we further show that our DPO training also improves out-of-domain performance. We evaluate El on a subset of VCTK, which is not included in either baseline training or DPO training. As suggested in Tab.III.b, DPO training achieves consistent improvement on all three metrics.\nIn Tab.III.c, we evaluate the baseline and E1 with three unseen metric models. The results suggest that the model is improved by DPO training in a general sense, rather than over-fitting on the metric models used in preference pair selection."}, {"title": "IV. CONCLUSION", "content": "Considering the prosperity of LM-based approaches in TTS re-search, this work introduces the preference alignment methods to the LM-based TTS systems. We demonstrate that the preference alignment methods boost the TTS system to outperform ground truth human speech in terms of speaker similarity, and proxy subjective evaluation scores. Exhaustive experiments are conducted to under-stand multiple critical issues in preference alignment implementation."}]}