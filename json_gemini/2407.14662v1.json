{"title": "Relational Composition in Neural Networks: A Survey and Call to Action", "authors": ["Martin Wattenberg", "Fernanda B. Vi\u00e9gas"], "abstract": "Many neural nets appear to represent data as linear combinations of \"feature vectors.\" Algorithms for discovering these vectors have seen impressive recent success. However, we argue that this success is incomplete without an understanding of relational composition: how (or whether) neural nets combine feature vectors to represent more complicated relationships. To facilitate research in this area, this paper offers a guided tour of various relational mechanisms that have been proposed, along with preliminary analysis of how such mechanisms might affect the search for interpretable features. We end with a series of promising areas for empirical research, which may help determine how neural networks represent structured data.", "sections": [{"title": "1. Introduction", "content": "The linear representation hypothesis asserts that neural nets encode information as a sum of \"feature vectors\" (Mikolov et al., 2013b; Arora et al., 2016; Olah et al., 2020). That is, a layer of a network represents a set of features as a weighted sum of associated vectors. An increasing body of evidence supports this idea, and one recent development is a set of algorithms for automatically finding feature vectors at scale (e.g., (Bricken et al., 2023; Cunningham et al., 2023; Rajamanoharan et al., 2024; Templeton et al., 2024) among others). These new techniques raise the possibility that we can understand and even control neural networks using a catalog of interpretable feature vectors.\n\nThe very simplicity of the linear representation hypothesis, however, suggests that there may be more to the story. After all, most computer programs contain data structures far more intricate than mere sets or bags, reflecting the complexity of the world. And programs that do treat memory as just a set of bits or bytes, such as assembly language, are notoriously hard to read and interpret. It's natural to ask, could there be structure in neural representations beyond this \"bag of features\" model? Indeed, this basic question has attracted notice since the founding of the field (Rosenblatt, 1961; Fodor & Pylyshyn, 1988). A wide array of mechanisms have been proposed for relating and composing vector representations\u2014what we refer to here as relational composition.\n\nThis paper includes a summary of some of these mechanisms, with a specific focus on ideas that seem relevant to contemporary interpretability work\u00b3. A central goal is to provide a gateway to a set of work that is spread over multiple decades and disciplines. We also analyze how these mechanisms might interact in subtle ways with feature identification algorithms, potentially causing problems for techniques that \"steer\" behavior by intervening during inference. In particular, we describe two potential problems that might arise in applying non-compositional methods to compositional representations:\n\n\u2022 Feature multiplicity. This is a kind of false positive, where combinatorial mechanisms might create multiple distinct \"echo\" feature vectors that correspond to the same concept. Furthermore, despite representing the same concept, different echo vectors potentially could have different effects on the system's output-leading to what we term a predict/control discrepancy. As a result, using feature vectors to understand and control behavior may be challenging.\n\n\u2022 Dark matter. This possibility represents a false negative. Some combinatorial mechanisms might systematically hide important features or combinations of features. Moreover, by looking for semantically meaningful vectors, we might miss important relational data. For example, binding mechanisms that involve"}, {"title": "2. Definitions, notation, and background", "content": "For simplicity, we'll work in an idealized setting, meant to be the bare minimum to explore ideas about composing representations. The basic unit of representation in this setting is a vector $x \\in R^n$. We'll assume that this vector encodes a set of \"features\" using vector addition:\n\n$x=\\sum_{i=1}^{m} a_i v_i$\n\nwhere each $v_i$ is a unit vector representing an individual feature, and the coefficient $a_i$ represents the weight\u2014or sometimes just presence or absence of\u2014that feature in the representation. (See appendix for more on the term \"feature.\") Note that we may have $m > n$, that is, more features than dimensions; this is called \"superposition\" in the literature ((Elhage et al., 2022) discusses some of the history of this idea.) In this setting, one typically assumes that features are \"sparse\", meaning most values of $a_i$ are zero, and that the $v_i$ are nearly mutually orthogonal. These assumptions make it possible to approximately reconstruct feature coefficients via inner products, with\n\n$a_i \\approx (x, v_i)$\n\nThis mechanism is good way to represent unordered sets of features. But how might we compose these vectors to express structures more complicated than sets? One natural way to combine two feature vectors x and y is simply to add them up. The sum $x + y$ represents something like the union of the features each individual vector encodes. From a geometric point of view, the sum is a vector that is similar to both x and y. This has obvious utility for representations: for instance, one could describe a scene with a duck and a capybara by adding a \"duck vector\" to a \"capybara vector\"."}, {"title": "3. Additive matrix binding", "content": "One general class of composition mechanisms is based on matrix transformations. Suppose $x, y \\in R^n$ are two vectors. How might we represent the ordered pair $(x, y)$ using a single vector $r\\in R^n$? As discussed, we can't just use the sum $x + y$ since this would destroy any sense of order: the pairs $(x, y)$ and $(y, x)$ would have the same representation.\n\nOne method that does preserve order is to fix two distinct $n x n$ matrices A and B and define the representation of $(x, y)$ by\n\n$r = Ax + By$\n\nIt's clear that $(x, y)$ and $(y, x)$ will generically have different representations. Moreover, with reasonable assumptions we can generally recover the individual features of both x and y from this representation. If A and B are random orthogonal matrices, and we've assumed that the feature vectors ${v_i}$ are mutually close to orthogonal, it follows that the feature vectors ${\u0391v_1,..., Av_m, BV_1,..., Bv_m}$ will also be close to mutually orthogonal. The data for x and y can then be \"read back\" selectively, for example by reconstruction functions of the form\n\n$f_{A,i}(r) = (r, Av_i)$\n$f_{B,i}(r) = (r, Bv_i)$\n\nOf course, there will be an increase in reconstruction error, since we are working with twice as many features. This issue may become significant if we move beyond pairs of vectors to encoding much longer lists. Thus the addition model of composition might put constraints on m, the number of individual feature vectors. On the positive side, the assumptions on A, B, and the ${v_i}$ can be relaxed in various ways,"}, {"title": "3.1. Matrix binding as writing to \"slots\" in superposition", "content": "There's an alternative perspective which relates directly to transformers. Consider an additive matrix binding model that combines a vector x with a list of vectors $(y_1,..., y_k)$ via the representation:\n\n$r = x + A_1 y_1 + A_2 y_2 + ... + A_k y_k$\n\nOne way to view this equation is that each matrix $A_i$ defines a distinct \"slot\" where data can be written for later retrieval. In other words, the matrix binding formula augments the information in x with the information in the ${y_i}$, such that data in one slot can be distinguished from data in another.\n\nBy construction, attention layers perform something not unlike matrix binding\u00b9. There's a potentially fruitful connection between this setup and the proposal that the transformer residual stream is a communication channel (Elhage et al., 2021; Merullo et al., 2024) between different network layers, which can augment it with new data or read information from previous layers. It can be tempting to imagine that the residual stream S is a sum of vector spaces $S = E_1 \\oplus E_2 \\oplus ... \\oplus E_k$ where each $E_i$ is a separate information \"channel.\" However, the additive model is a reminder that channels themselves might be in superposition, with no clear geometric separation between them\u00b2.\n\nFinally, at a theoretical level, it's worth noting that the formula Ax + By uses a form of superposition to create composition. As pointed out by Olah (Olah, 2024), in reviewing ideas of Thorpe (Thorpe, 1989), superposition and compo"}, {"title": "3.2. Tree representations using matrices", "content": "Additive binding can be used to create data structures that are more complex than a collection of slots. For example, here's one possible way to represent a binary tree. Fix two random n x n matrices, $M_1$ and $M_2$. Consider a parent node p with two children represented by vectors $C_1$ and $C_2$. Then define the representation\n\n$T_p = M_1C_1 + M_2C_2$\n\nApplying this recursively, we can find a vector representing each node of the tree; and because generically $M_1 M_2 \\ne M_2 M_1$, distinct nodes will be associated with a distinct vectors. While we have not found this mechanism explicitly mentioned in the literature, it can be viewed as a trivial linear version of a Tree-RNN, a network that creates a vector representation of a tree by operating recursively on its nodes (Socher et al., 2013; 2014; Bowman et al., 2015).\n\nTree-RNNs enjoyed a surge of interest in the years before transformers were introduced: using Tree-RNNs to process parse trees seemed to improve performance in multiple systems. Interestingly, for small fixed-depth trees there's a natural way for transformers to implement something like a Tree-RNN, since attention heads can focus on\u2014and thus can read information from-syntactically dependent tokens (Phang et al., 2019; Ravishankar et al., 2021). It might be worthwhile to look for evidence of this type of representation in real-world networks."}, {"title": "3.3. Feature multiplicity in additive models", "content": "If neural networks use one of these matrix-based mechanisms, how might that affect the search for features? In an idealized case, there's actually a clear answer: it will create a series of duplicate features, which we call echo features.\n\nTo see this, consider a set of random unit feature vectors ${v_1,..., v_m}$, and a model in which a representation vector x is a random sparse linear combination of these vectors. To be concrete, take $x = \\sum a_i v_i$ where the $a_i$ are each independent Bernoulli random variables with $p = 1/m$. Given a large sample of vectors from this distribution, a"}, {"title": "3.3.1. FEATURE MULTIPLICITY AND\nPREDICT/CONTROL DISCREPANCIES", "content": "Feature multiplicity might cause anomalies when using features to steer a network's behavior during inference. Indeed, a number of recent reports have described systems where the optimal vectors for predicting behavior and steering it turn out to be different (Zou et al., 2023; Marks & Tegmark, 2023; Li et al., 2024; Chen et al., 2024). We call this predict/control discrepancy. On its face, this discrepancy is not necessarily shocking\u2014a linear probe can easily pick up on spurious or redundant correlations, such as non-causal aspects of the input data. In (Bricken et al., 2023), for example, the authors perform experiments to check that features have expected causal effects on the networks output. On the other hand, it is conceivable that feature multiplicity may make predict/control discrepancies especially likely4.\n\nHere's a hypothetical example that serves as a plausibility argument for this idea. Consider a transformer layer that does nothing but augment the residual stream for a given token with information about the previous token. Imagine it does so with additive matrix binding. In particular, if $t_i$ and $t_{i-1}$ are sequential tokens at layer N \u2013 1, then after layer N the residual stream at token i, which we denote by $t'_i$ becomes\n\n$t'_i = t_i + At_{i-1}$\n\nwhere A is an orthogonal matrix far from the identity. Now imagine that representations in the residual stream for layer N-1 involve a feature vector $v_{tox}$ for toxic content. It follows that, for different tokens, the residual stream in layer N could have multiple distinct vectors related to toxicity: $v_{tox}, Av_{tox}$, or $v_{tox} + Av_{tox}$. A linear probe $P_{tox}$ trained to recognize toxic content, based on multiple tokens in the residual stream at layer N, might look something like\n\n$P_{tox}(x) = (x, v_{tox} + Av_{tox})$\n\nOn the other hand, if we trained on just one token (perhaps the last token in a prompt, a non-toxic function word, in order to predict subsequent behavior) we could easily produce a probe such as:\n\n$Q_{tox}(x) = (x, Av_{tox})$\n\nBoth of these probes might perform well, but this already illustrates a problem: different tokens might easily produce different probes for unimportant contextual reasons.\n\nNow, suppose we wish to intervene during network inference for instance, we want to change the residual stream to cause the network to produce less toxic text. Simply inspecting the second probe, we might hope to intervene to \"remove toxicity\" by intervening during inference (Li et al., 2024) to change each token t to have the new value\n\n$t' = t - Av_{tox}$\n\nIt's entirely possible, however, that the values of inner products $(x, v_{tox})$ and $(x, Av_{tox})$ will play different roles in computing the toxicity of the next word. It's certainly conceivable that the toxicity of the current word plays a greater role than information about the previous word. Quite likely, the most effective intervention will look more like the following equation, where $C_1 \\ne C_2$:\n\n$t' = t - C_1 v_{tox} + C_2 Av_{tox}$"}, {"title": "4. Multi-token mechanisms", "content": "So far, we've discussed binding mechanisms that operate wthin a single vector space. However, since the earliest days of the field, there has been speculation that that some notion of sequence that is, representations across multiple input observations\u2014is necessary for representing complex relationships (Rosenblatt, 1961). In this section, with the transformer architecture in mind, we discuss a class of mechanisms that relate specifically to sequence models.\n\nMathematically, we consider a sequence of \u201ctoken vectors\"\n\n$t_1, t_2,..., t_k \\in R^n$\n\nwhere each $t_i$ contains feature information and, potentially, information about its relation to other tokens. For instance, each $t_i$ might represent the residual stream for the i-th element of a sequence at a particular layer in a transformer. To extract the relational data, it might not be enough to look at a single token $t_i$ but at pairs of tokens $(t_i, t_j)$. This perspective opens up a different set of possibilities. It's also a very plausible type of representation. For example, when working with word embeddings, a variety of relationships between words can famously be read by taking vector differences (Mikolov et al., 2013a).\n\nA generalization of using vector differences is \u201clinear relational embedding\u201d (Paccanaro & Hinton, 2001). The idea here is that a relation between token vectors $t_i$ and $t_j$ is present when\n\n$t_i \\approx At_j + b$\n\nfor a matrix A and vector b which depend only on the relation. Recent work suggests that something like this representation may be found in the way that language-model transformers represent relationships between entities (Hernandez et al., 2023)."}, {"title": "4.1. Syntactic relations and tree embeddings", "content": "One striking finding related to the BERT network (Devlin et al., 2018) is that parse trees are represented geometrically by relative positions of token vectors. In particular, the \"syntactic distance\" between two words as measured in a dependency grammar parse tree-can be recovered from their corresponding token vectors in a middle layer of BERT (Hewitt & Manning, 2019; Manning et al., 2020). To be precise, if two words $w_i, w_j$ in the same sentence are represented by embedding vectors $t_i$ and $t_j$, and if $d_s(w_i, w_j)$ is the tree distance in the syntactic parse tree between $w_i$ and $w_j$, then\n\n$||M t_i - M t_j||^2 \\approx d_s(w_i, w_j)$\n\nwhere M is a constant linear transformation, depending only on the network. The fact that tree distance corresponds to the square of the Euclidean distance may seem surprising but, as described in (Reif et al., 2019), it's actually natural in the context of mapping a tree metric to a Euclidean metric. As described in (Chi et al., 2020), one possible explanation of this representation is that the differences between token embeddings represent syntactic dependencies: that is, if $t_i$ and $t_j$ have a specific syntactic relation (such as an adjective modifying a noun) then the vector difference $M t_i - M t_j$ encodes that relation.\n\nThis area seems like a promising direction for follow-up work. For one thing, tree structures might naturally occur in many other situations. More generally, the idea that differences between tokens encode specific, contextual relationships seems powerful. A natural question is whether applying dictionary learning to residual stream differences, rather than the residual stream itself, might yield a set of interpretable \"relation features.\""}, {"title": "4.2. Reference mechanisms: pointers and identifiers", "content": "Software engineers have invented many ways to tie different data structures together. Is it possible that neural networks use the same techniques? Some recent investigations hint that they might.\n\nOne fundamental component of many software data structures is a pointer: a reference to a location in memory. The analog of a pointer, for a transformer network, might be a positional embedding that defines a reference to a specific token in a sequence. One might imagine using matrix binding to augment a token with this positional information rather than semantic data. For instance, if $p_j$ represents the positional embedding for position j, and $A_r$ is a binding matrix, one might represent a relationship between tokens $t_i$ and $t_j$ as:\n\n$r(t_i, t_j) = t_i + A_r p_j$\n\nIn fact, the study described in (Prakash et al., 2024) uncovers a circuit that seems to use something like this kind of \"pointer\" mechanism to relate an entity to information about its state elsewhere in a sequence. The authors describe specific attention heads that seem to move and read"}, {"title": "5. Vector symbolic architecture", "content": "Finally, we discuss a set of historical ideas for vector binding, collectively known as \u201cvector symbolic architecture,\u201d or VSA (Smolensky, 1990; Plate, 1994; 1997; Kanerva, 2009; Jones & Mewhort, 2007; Schlegel et al., 2022). A comprehensive survey and analysis of the zoo of techniques in this area is far beyond the scope of this paper. Our goal is simply to provide a gentle introduction to techniques that can potentially seem arcane, unmotivated, and intimidating. To do so, we focus on two foundational constructions, one in $R^n$ and one in ${0, 1}^n$, which underlie much of the work in this field. Both mechanisms have the potential to lead to feature \"dark matter.\" That is, they might represent information in a way that is out of reach of dictionary learning or probing methods."}, {"title": "5.1. Tensor constructions", "content": "As described in (Schlegel et al., 2022), a technique introduced in (Smolensky, 1990) forms the basis of a large set of different VSA mechanisms. It is often referred to as a \"tensor\" method, but we'll describe it with the more down-to-earth notation of outer products.\n\nAs before, suppose we wish to compose, or bind, two vectors x and y. One way is to use an outer product to define a representation:\n\n$r = xy^T$\n\nThis representation distinguishes order, since generally $xy^T \\ne yx^T)$. It respects vector addition, since it is linear in each of x and y respectively. Moreover, one can recover x and y, at least up to scalar multiples, with easy vector algebra. For example, if you know y, you can recover a scalar multiple of x via the product\n\n$ry = xy^T y = ||y||^2 x$\n\nThe alert reader will notice, however, that this outer product representation is a cheat: it lives in $R^{nxn}$ rather than $R^n$. One may view much of the VSA literature as a bag of tricks to get around this inconvenient fact. The most common trick is to create an n-dimensional projection of the outer product, which can still be used for approximate reconstruction. One method, for instance, is projecting the outer product matrix via circular convolution (Plate, 1997)."}, {"title": "5.2. Binary vectors", "content": "A second category of VSA techniques relies on binary-valued vectors (Kanerva et al., 1997; Kanerva, 2009). We describe one simple proposal of this type. To bind two binary vectors x and y, we fix a permutation P that rearranges the entries of a vector, and then compute\n\n$r = x \\oplus P(y)$\n\nHere $\\oplus$ denotes modulo-2 addition. Like the outer product, this distinguishes between the ordered pairs (x, y) and (y, x). It respects the binary OR function, which can be used to take unions of two sets of features. It also allows for easy reconstruction of one vector in the pair if you know the other:\n\n$x = r \\oplus P(y)$\n$y = P^{-1}(r \\oplus x)$\n\nWe need the permutation P because mod-2 addition is commutative, and otherwise we couldn't distinguish the pairs (x, y) and (y, x). The situation is similar to an additive matrix binding formula x + Ay. Permuting one argument also allows us to bind a vector to itself without leading to a zero representation."}, {"title": "5.3. Is VSA a plausible mechanism for real neural nets?", "content": "Unlike additive binding models, the techniques proposed for tensor-based VSA don't obviously map to deep learning architectures. Investigations of whether real-world networks use VSA have yielded mixed conclusions. For example, a study of whether CLIP might use these mechanisms produced largely negative results (Lewis et al., 2022), while there are some indications RNNs may reproduce tensor-related structures (McCoy et al., 2018).\n\nTechniques based on binary vectors might seem even further from modern language model architectures. Surprisingly, some recent work suggests that for sparse vectors in superposition, not only can neural nets compute XORS of such features in theory (Dmitry Vaintrob, 2024), they seem to do so in practice (Marks, 2024). Traditional binary VSA is based on dense vectors, of course, but this seems like an interesting avenue for exploration. One might imagine combining ideas from the tensor and binary VSA schemes-perhaps based on outer products specifically of zero-one vectors, either over R or $Z_2$."}, {"title": "5.4. Implications for the search for features", "content": "As with matrix addition binding, we speculate that VSA mechanisms may lead to feature multiplicity. For example, outer product binding could plausibly produce a combinatorial proliferation of potential features. To see the issue, consider a setting where we have m feature vectors, ${v_1,..., v_m}$, and we want to compose representation vectors x and y:\n\n$x = \\sum_{i=1}^{m} a_i v_i$\n$y = \\sum_{i=1}^{m} b_i v_i$\n\nwhere the sums are assumed to be sparse, and ${a_i}$ and ${b_i}$ independently chosen. Suppose we have a composition method of the form\n\n$r(x,y) = \\pi(xy)$\n\nwhere $\u03c0 : R^{n x n} \\rightarrow R^n$ is a projection operator. By linearity, we have\n\n$r(x,y) = \\sum_{i,j} a_i b_j \\pi(v_i v_j)$\n\nWe have now moved from using m feature vectors ${v_i}$ to a situation with $m^2$ potential feature vectors ${\\pi(v_i v_j)}$."}, {"title": "6. Conclusion and future work", "content": "We've described a series of proposals some historical, some very recent-for how neural nets might represent relationships between features. Along the way, we've given conceptual arguments that these mechanisms might present problems when finding and using linear feature representations. One challenge is that of \"dark matter\": feature representations that are difficult to find using standard methods. Another potential problem is feature multiplicity, or the presence of multiple \"echo vectors\u201d that correspond to the same feature. An issue of special concern, related to multiplicity, is whether feature vectors discovered by prob-"}, {"title": "7. Acknowledgments", "content": "We thank Kenneth Li, Oam Patel, Nikola Jurkovic, Chris Olah, Yonatan Belinkov, Asma Ghandeharioun, Ann Yuan, and Lucas Dixon for helpful comments on this manuscript. We're indebted to the anonymous reviewers for their careful reading and excellent suggestions. FV was supported by a fellowship from the Radcliffe Institute for Advanced Study at Harvard University. Additional support came from Effective Ventures Foundation, Effektiv Spenden Schweiz, and the Open Philanthropy Project."}, {"title": "A. Appendix: Features", "content": "In this paper we use the word \u201cfeature\u201d formally, as a kind of mathematical abstraction, since that's all that's needed for our arguments. However, it's worth talking about some of the intuition behind the word. To begin with, there's not a consensus definition in the literature, which is too large to survey in an appendix. See (Elhage et al., 2022), for example, which discusses three approaches: features as arbitrary functions of the input, features as interpretable properties, and-most abstractly-features as what an infinitely large neural net might devote a single neuron to.\n\nThe general intuition we've found most helpful, however, is simply that a feature represents a unit of data that is useful for future computations. We also believe it's helpful to divide these representations into three broad categories. Some features relate to the input-that is, they represent a useful property of the data seen by the network. (\"Red area on green background\" or \"apple.\u201d) Other features, like data structures in a traditional algorithm, are related to intermediate processing. (\"Food and hunger\u201d.) Finally, a third set of features will relate to the result of the network's computations: they will represent properties of the output, rather than input (\u201cReach arm\u201d or \u201copen mouth\u201d).\n\nThe analogy is obviously with biological nervous systems, which can be broken into areas for sensory, motor, and internal processing. Of course, there's no guarantee that a neural network maintains a strict division between these different aspects. It's conceivable that within a given layer, all three types of features may be found. That may be another reason why interpreting features can be hard. Frequently people try to understand features in terms of input data, when those features may really be aspects of hidden computation or output. It may also explain some predict/control discrepancies: linear probes may be effectively trained in a way that emphasizes features of the input, which are only roughly correlated with output features."}]}