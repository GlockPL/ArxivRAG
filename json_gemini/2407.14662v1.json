{"title": "Relational Composition in Neural Networks: A Survey and Call to Action", "authors": ["Martin Wattenberg", "Fernanda B. Vi\u00e9gas"], "abstract": "Many neural nets appear to represent data as linear combinations of \"feature vectors.\" Algorithms for discovering these vectors have seen impressive recent success. However, we argue that this success is incomplete without an understanding of relational composition: how (or whether) neural nets combine feature vectors to represent more complicated relationships. To facilitate research in this area, this paper offers a guided tour of various relational mechanisms that have been proposed, along with preliminary analysis of how such mechanisms might affect the search for interpretable features. We end with a series of promising areas for empirical research, which may help determine how neural networks represent structured data.", "sections": [{"title": "1. Introduction", "content": "The linear representation hypothesis asserts that neural nets encode information as a sum of \"feature vectors\" (Mikolov et al., 2013b; Arora et al., 2016; Olah et al., 2020). That is, a layer of a network represents a set of features as a weighted sum of associated vectors. An increasing body of evidence supports this idea, and one recent development is a set of algorithms for automatically finding feature vectors at scale (e.g., (Bricken et al., 2023; Cunningham et al., 2023; Rajamanoharan et al., 2024; Templeton et al., 2024) among others). These new techniques raise the possibility that we can understand and even control neural networks using a catalog of interpretable feature vectors.\n\nThe very simplicity of the linear representation hypothesis, however, suggests that there may be more to the story. After all, most computer programs contain data structures far more intricate than mere sets or bags, reflecting the complexity of the world. And programs that do treat memory as just a set of bits or bytes, such as assembly language, are notoriously hard to read and interpret. It's natural to ask, could there be structure in neural representations beyond this \"bag of features\" model? Indeed, this basic question has attracted notice since the founding of the field (Rosenblatt, 1961; Fodor & Pylyshyn, 1988). A wide array of mechanisms have been proposed for relating and composing vector representations\u2014what we refer to here as relational composition.\n\nThis paper includes a summary of some of these mechanisms, with a specific focus on ideas that seem relevant to contemporary interpretability work\u00b3. A central goal is to provide a gateway to a set of work that is spread over multiple decades and disciplines. We also analyze how these mechanisms might interact in subtle ways with feature identification algorithms, potentially causing problems for techniques that \"steer\" behavior by intervening during inference. In particular, we describe two potential problems that might arise in applying non-compositional methods to compositional representations:\n\n\u2022 Feature multiplicity. This is a kind of false positive, where combinatorial mechanisms might create multiple distinct \"echo\" feature vectors that correspond to the same concept. Furthermore, despite representing the same concept, different echo vectors potentially could have different effects on the system's output-leading to what we term a predict/control discrepancy. As a result, using feature vectors to understand and control behavior may be challenging.\n\n\u2022 Dark matter. This possibility represents a false negative. Some combinatorial mechanisms might systematically hide important features or combinations of features. Moreover, by looking for semantically meaningful vectors, we might miss important relational data. For example, binding mechanisms that involve"}, {"title": "2. Definitions, notation, and background", "content": "For simplicity, we'll work in an idealized setting, meant to be the bare minimum to explore ideas about composing representations. The basic unit of representation in this setting is a vector $x \\in R^n$. We'll assume that this vector encodes a set of \"features\" using vector addition:\n\n$x= \\sum_{i=1}^m  a_i v_i$\n\nwhere each $v_i$ is a unit vector representing an individual feature, and the coefficient $a_i$ represents the weight\u2014or sometimes just presence or absence of\u2014that feature in the representation. (See appendix for more on the term \"feature.\") Note that we may have $m > n$, that is, more features than dimensions; this is called \"superposition\" in the literature ((Elhage et al., 2022) discusses some of the history of this idea.) In this setting, one typically assumes that features are \"sparse\", meaning most values of $a_i$ are zero, and that the $v_i$ are nearly mutually orthogonal. These assumptions make it possible to approximately reconstruct feature coefficients via inner products, with\n\n$a_i \\approx (x, v_i)$\n\nThis mechanism is good way to represent unordered sets of features. But how might we compose these vectors to express structures more complicated than sets? One natural way to combine two feature vectors $x$ and $y$ is simply to add them up. The sum $x + y$ represents something like the union of the features each individual vector encodes. From a geometric point of view, the sum is a vector that is similar to both $x$ and $y$. This has obvious utility for representations: for instance, one could describe a scene with a duck and a capybara by adding a \"duck vector\" to a \"capybara vector\"."}, {"title": "3. Additive matrix binding", "content": "One general class of composition mechanisms is based on matrix transformations. Suppose $x, y \\in R^n$ are two vectors. How might we represent the ordered pair $(x, y)$ using a single vector $r \\in R^n$? As discussed, we can't just use the sum $x + y$ since this would destroy any sense of order: the pairs $(x, y)$ and $(y, x)$ would have the same representation.\n\nOne method that does preserve order is to fix two distinct $n \\times n$ matrices $A$ and $B$ and define the representation of $(x, y)$ by\n\n$r = Ax + By$\n\nIt's clear that $(x, y)$ and $(y, x)$ will generically have different representations. Moreover, with reasonable assumptions we can generally recover the individual features of both $x$ and $y$ from this representation. If $A$ and $B$ are random orthogonal matrices, and we've assumed that the feature vectors {$v_i$} are mutually close to orthogonal, it follows that the feature vectors {$Av_1,..., Av_m, Bv_1,..., Bv_m$} will also be close to mutually orthogonal. The data for $x$ and $y$ can then be \"read back\" selectively, for example by reconstruction functions of the form\n\n$f_{A,i}(r) = (r, Av_i)$\n\n$f_{B,i}(r) = (r, Bv_i)$\n\nOf course, there will be an increase in reconstruction error, since we are working with twice as many features. This issue may become significant if we move beyond pairs of vectors to encoding much longer lists. Thus the addition model of composition might put constraints on $m$, the number of individual feature vectors. On the positive side, the assumptions on $A, B$, and the {$v_i$} can be relaxed in various ways,"}, {"title": "3.1. Matrix binding as writing to \"slots\" in superposition", "content": "There's an alternative perspective which relates directly to transformers. Consider an additive matrix binding model that combines a vector $x$ with a list of vectors $(y_1,..., y_k)$ via the representation:\n\n$r = x + A_1y_1 + A_2y_2 + ... + A_ky_k$\n\nOne way to view this equation is that each matrix $A_i$ defines a distinct \"slot\" where data can be written for later retrieval. In other words, the matrix binding formula augments the information in $x$ with the information in the {$y_i$}, such that data in one slot can be distinguished from data in another.\n\nBy construction, attention layers perform something not unlike matrix binding\u00b9. There's a potentially fruitful connection between this setup and the proposal that the transformer residual stream is a communication channel (Elhage et al., 2021; Merullo et al., 2024) between different network layers, which can augment it with new data or read information from previous layers. It can be tempting to imagine that the residual stream $S$ is a sum of vector spaces $S = E_1 \\oplus E_2 \\oplus ... \\oplus E_k$ where each $E_i$ is a separate information \"channel.\" However, the additive model is a reminder that channels themselves might be in superposition, with no clear geometric separation between them\u00b2."}, {"title": "3.2. Tree representations using matrices", "content": "Additive binding can be used to create data structures that are more complex than a collection of slots. For example, here's one possible way to represent a binary tree. Fix two random $n \\times n$ matrices, $M_1$ and $M_2$. Consider a parent node $p$ with two children represented by vectors $C_1$ and $C_2$. Then define the representation\n\n$T_p = M_1C_1 + M_2C_2$\n\nApplying this recursively, we can find a vector representing each node of the tree; and because generically $M_1M_2 \\neq M_2M_1$, distinct nodes will be associated with a distinct vectors. While we have not found this mechanism explicitly mentioned in the literature, it can be viewed as a trivial linear version of a Tree-RNN, a network that creates a vector representation of a tree by operating recursively on its nodes (Socher et al., 2013; 2014; Bowman et al., 2015)."}, {"title": "3.3. Feature multiplicity in additive models", "content": "If neural networks use one of these matrix-based mechanisms, how might that affect the search for features? In an idealized case, there's actually a clear answer: it will create a series of duplicate features, which we call echo features.\n\nTo see this, consider a set of random unit feature vectors {$v_1,..., v_m$}, and a model in which a representation vector $x$ is a random sparse linear combination of these vectors. To be concrete, take $x= \\sum a_iv_i$ where the $a_i$ are each independent Bernoulli random variables with $p = 1/m$. Given a large sample of vectors from this distribution, a"}, {"title": "3.3.1. FEATURE MULTIPLICITY AND PREDICT/CONTROL DISCREPANCIES", "content": "Feature multiplicity might cause anomalies when using features to steer a network's behavior during inference. Indeed, a number of recent reports have described systems where the optimal vectors for predicting behavior and steering it turn out to be different (Zou et al., 2023; Marks & Tegmark, 2023; Li et al., 2024; Chen et al., 2024). We call this predict/control discrepancy. On its face, this discrepancy is not necessarily shocking\u2014a linear probe can easily pick up on spurious or redundant correlations, such"}, {"title": "4. Multi-token mechanisms", "content": "So far, we've discussed binding mechanisms that operate within a single vector space. However, since the earliest days of the field, there has been speculation that that some notion of sequence\u2014that is, representations across multiple input observations\u2014is necessary for representing complex relationships (Rosenblatt, 1961). In this section, with the transformer architecture in mind, we discuss a class of mechanisms that relate specifically to sequence models.\n\nMathematically, we consider a sequence of \u201ctoken vectors\"\n\n$t_1, t_2,..., t_k \\in R^n$\n\nwhere each $t_i$ contains feature information and, potentially, information about its relation to other tokens. For instance, each $t_i$ might represent the residual stream for the $i$-th element of a sequence at a particular layer in a transformer. To extract the relational data, it might not be enough to look at a single token $t_i$ but at pairs of tokens $(t_i, t_j)$. This perspective opens up a different set of possibilities. It's also a very plausible type of representation. For example, when working with word embeddings, a variety of relationships between words can famously be read by taking vector differences (Mikolov et al., 2013a).\n\nA generalization of using vector differences is \u201clinear relational embedding\u201d (Paccanaro & Hinton, 2001). The idea here is that a relation between token vectors $t_i$ and $t_j$ is present when\n\n$t_i \\approx At_j + b$\n\nfor a matrix $A$ and vector $b$ which depend only on the relation. Recent work suggests that something like this representation may be found in the way that language-model transformers represent relationships between entities (Hernandez et al., 2023)."}, {"title": "4.1. Syntactic relations and tree embeddings", "content": "One striking finding related to the BERT network (Devlin et al., 2018) is that parse trees are represented geometrically by relative positions of token vectors. In particular, the \"syntactic distance\" between two words as measured in a dependency grammar parse tree\u2014can be recovered from their corresponding token vectors in a middle layer of BERT (Hewitt & Manning, 2019; Manning et al., 2020). To be precise, if two words $w_i, w_j$ in the same sentence are represented by embedding vectors $t_i$ and $t_j$, and if $d_s(w_i, w_j)$ is the tree distance in the syntactic parse tree between $w_i$ and $w_j$, then\n\n$||Mt_i - Mt_j||^2 \\approx d_s(w_i, w_j)$\n\nwhere $M$ is a constant linear transformation, depending only on the network. The fact that tree distance corresponds to the square of the Euclidean distance may seem surprising but, as described in (Reif et al., 2019), it's actually natural in the context of mapping a tree metric to a Euclidean metric. As described in (Chi et al., 2020), one possible explanation of this representation is that the differences between token embeddings represent syntactic dependencies: that is, if $t_i$ and $t_j$ have a specific syntactic relation (such as an adjective modifying a noun) then the vector difference $Mt_i - Mt_j$ encodes that relation."}, {"title": "4.2. Reference mechanisms: pointers and identifiers", "content": "Software engineers have invented many ways to tie different data structures together. Is it possible that neural networks use the same techniques? Some recent investigations hint that they might.\n\nOne fundamental component of many software data structures is a pointer: a reference to a location in memory. The analog of a pointer, for a transformer network, might be a positional embedding that defines a reference to a specific token in a sequence. One might imagine using matrix binding to augment a token with this positional information rather than semantic data. For instance, if $p_j$ represents the positional embedding for position $j$, and $A_r$ is a binding matrix, one might represent a relationship between tokens $t_i$ and $t_j$ as:\n\n$r(t_i, t_j) = t_i + A_rp_j$\n\nIn fact, the study described in (Prakash et al., 2024) uncovers a circuit that seems to use something like this kind of \"pointer\" mechanism to relate an entity to information about its state elsewhere in a sequence. The authors describe specific attention heads that seem to move and read"}, {"title": "5. Vector symbolic architecture", "content": "Finally, we discuss a set of historical ideas for vector binding, collectively known as \u201cvector symbolic architecture,\" or VSA (Smolensky, 1990; Plate, 1994; 1997; Kanerva, 2009; Jones & Mewhort, 2007; Schlegel et al., 2022). A comprehensive survey and analysis of the zoo of techniques in this area is far beyond the scope of this paper. Our goal is simply to provide a gentle introduction to techniques that can potentially seem arcane, unmotivated, and intimidating. To do so, we focus on two foundational constructions, one in $R^n$ and one in {0, 1}$^n$, which underlie much of the work in this field. Both mechanisms have the potential to lead to feature \"dark matter.\" That is, they might represent information in a way that is out of reach of dictionary learning or probing methods."}, {"title": "5.1. Tensor constructions", "content": "As described in (Schlegel et al., 2022), a technique introduced in (Smolensky, 1990) forms the basis of a large set of different VSA mechanisms. It is often referred to as a \"tensor\" method, but we'll describe it with the more down-to-earth notation of outer products.\n\nAs before, suppose we wish to compose, or bind, two vectors $x$ and $y$. One way is to use an outer product to define a representation:\n\n$r = xy^T$\n\nThis representation distinguishes order, since generally $xy^T \\neq yx^T)$. It respects vector addition, since it is linear in each of $x$ and $y$ respectively. Moreover, one can recover $x$ and $y$, at least up to scalar multiples, with easy vector algebra. For example, if you know $y$, you can recover a scalar multiple of $x$ via the product\n\n$ry = xy^Ty = ||y||^2x$\n\nThe alert reader will notice, however, that this outer product representation is a cheat: it lives in $R^{n \\times n}$ rather than $R^n$. One may view much of the VSA literature as a bag of tricks to get around this inconvenient fact. The most common trick is to create an $n$-dimensional projection of the outer product, which can still be used for approximate reconstruction. One method, for instance, is projecting the outer product matrix via circular convolution (Plate, 1997)."}, {"title": "5.2. Binary vectors", "content": "A second category of VSA techniques relies on binary-valued vectors (Kanerva et al., 1997; Kanerva, 2009). We describe one simple proposal of this type. To bind two binary vectors $x$ and $y$, we fix a permutation $P$ that rearranges the entries of a vector, and then compute\n\n$r = x \\oplus P(y)$\n\nHere $\\oplus$ denotes modulo-2 addition. Like the outer product, this distinguishes between the ordered pairs $(x, y)$ and $(y, x)$. It respects the binary OR function, which can be used to take unions of two sets of features. It also allows for easy reconstruction of one vector in the pair if you know the other:\n\n$x = r \\oplus P(y)$\n\n$y = P^{-1}(r \\oplus x)$\n\nWe need the permutation $P$ because mod-2 addition is commutative, and otherwise we couldn't distinguish the pairs $(x, y)$ and $(y, x)$. The situation is similar to an additive matrix binding formula $x + Ay$. Permuting one argument also allows us to bind a vector to itself without leading to a zero representation."}, {"title": "5.3. Is VSA a plausible mechanism for real neural nets?", "content": "Unlike additive binding models, the techniques proposed for tensor-based VSA don't obviously map to deep learning architectures. Investigations of whether real-world networks use VSA have yielded mixed conclusions. For example, a study of whether CLIP might use these mechanisms produced largely negative results (Lewis et al., 2022), while there are some indications RNNs may reproduce tensor-related structures (McCoy et al., 2018).\n\nTechniques based on binary vectors might seem even further from modern language model architectures. Surprisingly, some recent work suggests that for sparse vectors in superposition, not only can neural nets compute XORS of such features in theory (Dmitry Vaintrob, 2024), they seem to do so in practice (Marks, 2024). Traditional binary VSA is based on dense vectors, of course, but this seems like an interesting avenue for exploration. One might imagine combining ideas from the tensor and binary VSA schemes\u2014perhaps based on outer products specifically of zero-one vectors, either over R or Z2."}, {"title": "5.4. Implications for the search for features", "content": "As with matrix addition binding, we speculate that VSA mechanisms may lead to feature multiplicity. For example, outer product binding could plausibly produce a combinatorial proliferation of potential features. To see the issue, consider a setting where we have m feature vectors, {$v_1,..., v_m$}, and we want to compose representation vectors x and y:\n\n$x = \\sum_{i=1}^m a_iv_i$\n\n$y = \\sum_{i=1}^m b_iv_i$\n\nwhere the sums are assumed to be sparse, and {$a_i$} and {$b_i$} independently chosen. Suppose we have a composition method of the form\n\n$r(x, y) = \\pi(xy)$\n\nwhere $\\pi : R^{n \\times n} \\rightarrow R^n$ is a projection operator. By linearity, we have\n\n$r(x, y) = \\sum_{i,j} a_ib_j\\pi(v_iv_j)$\n\nWe have now moved from using m feature vectors {$v_i$} to a situation with m\u00b2 potential feature vectors {$\\pi(v_iv_j)$}."}, {"title": "6. Conclusion and future work", "content": "We've described a series of proposals\u2014some historical, some very recent\u2014for how neural nets might represent relationships between features. Along the way, we've given conceptual arguments that these mechanisms might present problems when finding and using linear feature representations. One challenge is that of \"dark matter\": feature representations that are difficult to find using standard methods. Another potential problem is feature multiplicity, or the presence of multiple \"echo vectors\u201d that correspond to the same feature. An issue of special concern, related to multiplicity, is whether feature vectors discovered by probing or dictionary-learning methods will be as useful for interventions as they are for predicting the state of the network.\n\nThe arguments here are conjectural and often involve idealized settings. A great deal of work would be necessary to resolve the questions we've raised\u2014calling for that work is the point of this note. Here are some directions that might shed light on the key issues:\n\n\u2022 Toy models that learn composition. One way to gather data would be to experiment with minimal models. It would be useful to find toy models of learned composition. If we give an autoencoder a task that requires learning relational composition, what mechanisms are found via gradient descent? Some work in this area dates back decades (Pollack, 1988; Blank et al., 2014) and it would be interesting to analyze these same systems with modern methods, and to extend this line of research further.\n\n\u2022 Apply feature extraction methods to synthetic composition mechanisms. We provided a conceptual outline of how feature discovery techniques might run into trouble on compositional representations. A natural next step would be to test these arguments empirically, by creating synthetic versions of VSA, matrix binding, and other mechanisms, and applying dictionary learning to the results.\n\n\u2022 Apply dictionary learning to token differences. Syntactic relations appear to be encoded in differences between token vectors. Are other relations encoded this way as well? It could be worth applying dictionary learning techniques to token differences to find additional \"relational features.\"\n\n\u2022 Investigate marker mechanisms. The \"ID vector\" mechanism suggested by (Feng & Steinhardt, 2023) is extremely interesting. It also presents a clear problem for feature-based interpretability methods. It would be helpful to understand just how widely the mechanism might apply. How does it relate to the syntax representations of (Manning et al., 2020)? Experimentation with small synthetic models might advance our understanding of the basic process. It would also be helpful to find lightweight ways to identify ID vector subspaces.\n\n\u2022 Look for feature multiplicity. If feature multiplicity is a real problem, it may not be hard to identify. One sign would be the discovery of redundant or contextually dependent feature vectors. The features found in (Bricken et al., 2023; Templeton et al., 2024; Kissane et al., 2024) sometimes exhibit this type of behavior, for instance. If one could pair up a sufficiently large number of redundant features via a single linear transformation, that would provide strong evidence for the multiplicity hypothesis, and also help organize feature catalogs.\n\n\u2022 Understand predict/control discrepancy. Feature multiplicity might cause a problem that we have termed \"predict/control discrepancy.\" That is, an effective vector for predicting a particular network behavior might be different from the best \u201csteering vector\" for inducing that same behavior. Given the obvious practical implications, and that there are multiple examples of this phenomenon in the literature, it seems important to investigate further.\n\n\u2022 Measure real models for relational composition. One alternative to explicitly identifying relational representations is to look for behavioral evidence that they exist. That can at least alert us to \"dark matter\" that isn't observed by feature identification methods. A review of this type of work is beyond the scope of this paper, but several studies suggest promising directions (Andreas, 2019; Lovering & Pavlick, 2022; Aky\u00fcrek & Andreas, 2023).\n\n\u2022 Investigate VSA-based binding in real networks. The findings in (Dmitry Vaintrob, 2024; Marks, 2024) suggest some natural mechanisms by which binary VSA codes might be implemented in realistic networks. It may be worth systematically working out the simplest implementations of these mechanisms, and investigating whether real networks make use of them.\n\nTo sum up, there are major practical and theoretical questions around how the linear representation hypothesis might interact with mechanisms for relational composition. We strongly advocate for continued research in this area. Identifying mechanisms for relational composition would be a step forward for useful interpretability work. Even if it turns out that neural nets show no evidence of relational composition, that would be an important theoretical result."}, {"title": "A. Appendix: Features", "content": "In this paper we use the word \u201cfeature\u201d formally, as a kind of mathematical abstraction, since that's all that's needed for our arguments. However, it's worth talking about some of the intuition behind the word. To begin with, there's not a consensus definition in the literature, which is too large to survey in an appendix. See (Elhage et al., 2022), for example, which discusses three approaches: features as arbitrary functions of the input, features as interpretable properties, and\u2014most abstractly\u2014features as what an infinitely large neural net might devote a single neuron to.\n\nThe general intuition we've found most helpful, however, is simply that a feature represents a unit of data that is useful for future computations. We also believe it's helpful to divide these representations into three broad categories. Some features relate to the input\u2014that is, they represent a useful property of the data seen by the network. (\"Red area on green background\" or \"apple.\u201d) Other features, like data structures in a traditional algorithm, are related to intermediate processing. (\"Food and hunger\".) Finally, a third set of features will relate to the result of the network's computations: they will represent properties of the output, rather than input (\"Reach arm\" or \"open mouth\").\n\nThe analogy is obviously with biological nervous systems, which can be broken into areas for sensory, motor, and internal processing. Of course, there's no guarantee that a neural network maintains a strict division between these different aspects. It's conceivable that within a given layer, all three types of features may be found. That may be another reason why interpreting features can be hard. Frequently people try to understand features in terms of input data, when those features may really be aspects of hidden computation or output. It may also explain some predict/control discrepancies: linear probes may be effectively trained in a way that emphasizes features of the input, which are only roughly correlated with output features."}]}