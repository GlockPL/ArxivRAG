{"title": "Step-by-Step Fact Verification System for Medical Claims with Explainable Reasoning", "authors": ["Juraj Vladika", "Ivana Hacajov\u00e1", "Florian Matthes"], "abstract": "Fact verification (FV) aims to assess the veracity of a claim based on relevant evidence. The traditional approach for automated FV includes a three-part pipeline relying on short evidence snippets and encoder-only inference models. More recent approaches leverage the multi-turn nature of LLMs to address FV as a step-by-step problem where questions inquiring additional context are generated and answered until there is enough information to make a decision. This iterative method makes the verification process rational and explainable. While these methods have been tested for encyclopedic claims, exploration on domain-specific and realistic claims is missing. In this work, we apply an iterative FV system on three medical fact-checking datasets and evaluate it with multiple settings, including different LLMs, external web search, and structured reasoning using logic predicates. We demonstrate improvements in the final performance over traditional approaches and the high potential of step-by-step FV systems for domain-specific claims.", "sections": [{"title": "1 Introduction", "content": "The digital age has been marked by the rise and spread of online misinformation, which has negative societal consequences, especially when related to public health (van der Linden, 2022). Fact verification (FV) has emerged as an automated approach for addressing the increasing rate of deceptive content promulgated online (Das et al., 2023; Schlichtkrull et al., 2023a). On top of that, FV can help improve the factuality of generative large language models (Augenstein et al., 2024) and help scientists find reliable evidence for assessing their research hypotheses (Eger et al., 2025).\n\nThe common pipeline for automated fact verification consists of document retrieval, evidence extraction, veracity prediction, and optionally justification production (Guo et al., 2022). In such a setup, document retrieval is usually done with a method like BM25 or semantic search, evidence selected using sentence embedding models, and the final verdict predicted with an encoder-only model like DeBERTa (He et al., 2021). In fact, most state-of-the-art FV systems for the popular FEVER dataset (Thorne et al., 2018) and other recent real-world misinformation datasets rely on this pipeline (Zhang et al., 2024; Glockner et al., 2024).\n\nSimilarly, most previous work relies on providing pre-selected evidence to the final inference model. A more realistic setting is open-domain fact verification, where evidence first has to be discovered in large knowledge bases before the system produces the verdict. Recent FV work has explored this setting, but most of them also rely on the traditional pipeline, utilizing BM25, sentence embeddings, and encoder-only inference model for producing their verdicts (Wadden et al., 2022; Stammbach et al., 2023; Vladika and Matthes, 2024b)."}, {"title": "2 Related Work", "content": "There have been many synthetic FV datasets constructed from Wikipedia, such as FEVER (Thorne et al., 2018). While FEVER focuses on simple claims, datasets like HOVER (Jiang et al., 2020) and FEVEROUS (Aly et al., 2021) introduced complex claims requiring multi-hop reasoning. Apart from synthetic datasets, there are also datasets focusing on more realistic claims and real-world misinformation (Schlichtkrull et al., 2023b; Glockner et al., 2024). Increasingly popular are also domain-specific datasets focusing on scientific fact-checking (Vladika and Matthes, 2023), especially for the domains of medicine (Saakyan et al., 2021; Sarrouti et al., 2021), climate (Diggelmann et al., 2020), and computer science (Lu et al., 2023).\n\nMost FV approaches follow the traditional three-part pipeline (Bekoulis et al., 2021). In recent years, approaches incorporating LLMs and iterative reasoning into the process have achieved great performance on multi-hop FV. This includes FV through varifocal questions (Ousidhoum et al., 2022) or wh-questions to aid verification (Rani et al., 2023), step-by-step prompting (Zhang and Gao, 2023), and program-guided reasoning (Pan et al., 2023b).\n\nMost studies with iterative FV systems focus on multi-hop encyclopedic claims. To the best of our knowledge, our study is among the first to explore the step-by-step FV systems for real-world claims rooted in scientific and medical knowledge."}, {"title": "3 Foundations", "content": "In this section, we describe in more detail the two FV approaches: the conventional three-part pipeline, serving as a baseline, and the step-by-step LLM-based system, which we mainly use."}, {"title": "3.1 Three-Part Pipeline for Fact Verification", "content": "The traditional three-part pipeline consists of: (1) document retrieval; (2) evidence extraction; (3) verdict prediction. It was used in the study by Vladika and Matthes (2024a), whose results we use as the baseline. Since it is an open-domain FV system, evidence documents have to be retrieved first. For that, step (1) was modeled with semantic search (similarity of query and corpus embeddings) over a large document corpus (PubMed and Wikipedia). In another experiment, evidence was sought with Google search. After selecting the top documents, step (2) again used a sentence embedding model to compare the claim to passages from the documents, selecting the most relevant evidence snippets. Finally, step (3) is modeled as the task of Natural Language Inference (NLI), where the goal is to predict the logical entailment relation between the claim and evidence, i.e., whether the claim is supported by evidence (entailment), refuted by evidence (contradiction), or there is not enough information (neutral). The model was DeBERTa-v3 fine-tuned on various NLI datasets from Laurer et al. (2024)."}, {"title": "3.2 Step-by-Step LLM System", "content": "The recent LLM advancements have brought a lot of features that can enhance the FV process. With their generative capabilities and multi-turn nature, LLMs can generate follow-up questions that aim to collect deeper background evidence related to claims. They are able to produce verdicts for claims over multiple pieces of evidence with mechanisms like chain-of-thought reasoning (Ling et al., 2023).\n\nThe system we develop in this work is mainly inspired by QACheck (Pan et al., 2023a) and its FV components. We expand that system by introducing novel prompts, additional chain-of-thought reasoning, amplify evidence retrieval with an online search engine, and experiment with structured reasoning in the form of logic predicates. The idea of this system is, given the claim $c$ being verified, to generate up to five follow-up questions $q_1, \u2026, q_5$, which try to gather more evidence related to the claim. This is generated using a base LLM $M_q$ and a prompt. Afterward, evidence for each question $q$ is retrieved from the source $s$ (web search or internal knowledge) using the method $R(q, s)$. This collected evidence is summarized with model $M_s$ and together with original $e$ posed to a reasoning model $M_r$. This reasoning module determines whether it should continue generating new questions or if there is enough evidence. If there is enough, it predicts a final verdict label $v$, one of SUPPORTED or REFUTED, and generates an explanation $e$.\n\nOn top of the described approach, we also experiment with a setting incorporating predicate logic into the process. Given the claim $c$, a predicate is generated by an LLM in the form of verb(subject, object), such as Treats(aspirin, headache), and used to generate better questions $q_i$ and verdict $v$. Inspired by FOLK (Wang and Shu, 2023), the idea behind this is that the structured nature of predicates can help in finding more accurate evidence and introduce structured reasoning for the final verdict prediction (Strong et al., 2024)."}, {"title": "4 Experiments and Setup", "content": "In the experiments, our main research question is RQ: Does the iterative LLM approach outperform the traditional three-part pipeline for domain-specific fact verification? On top of that, we test three further aspects of the system: (a) knowledge source, (b) structured reasoning, and (c) base LLM.\n\nThe knowledge sources include: internal knowledge of the LLM and the online search of the whole web. Our search engine of choice is DuckDuckGo, an open-source tool focused on privacy. We use it through a dedicated Python library. This search engine provided a smooth search experience with no interruptions, and we deemed the quality of the retrieved results similar to the more popular Google or Bing for our use case. We take the provided snippets from the first 5 results and give them as input evidence to the reasoner LLM. The structured reasoning in (b) refers to using logic predicates, as described in the previous section. All the experiments in (a) and (b) were done using GPT-4o-mini-2024-07-18 as the base LLM, the model from OpenAI with good reasoning capabilities (OpenAI, 2024).\n\nIn experiment round (c), we additionally test normal reasoning with internal knowledge and online search using Mixtral 8x7B (Jiang et al., 2024), a highly performing open-weights model based on a mixture-of-experts architecture, and LLaMa 3.1 (70B) (Meta, 2024), a recent advanced open-weights model from Meta. We use GPT through the OpenAI API and the two other models through the Together AI API, setting temperature to 0 for best reproducibility and maximum tokens to 512. We use these LLMs for all parts of the fact verification process, i.e. for all steps $M_q, M_s, M_r$ as described in the previous section. All the used prompts are in the Appendix. All experiments were run on one Nvidia V100 GPU with 16 GB VRAM."}, {"title": "4.1 Datasets and Evaluation", "content": "We choose three English datasets of biomedical and healthcare claims, designed for different purposes: SCIFACT (Wadden et al., 2020) is a dataset with expert-written biomedical claims originating from citation sentences found in medical paper abstracts. The subset we use contains 693 claims, of which 456 are supported, and 237 are refuted.\n\nHEALTHFC (Vladika et al., 2024a) is a dataset of claims concerning everyday health and spanning various topics like nutrition, the immune system, and mental health. The claims originate from user inquiries and they were checked by a team of medical experts. The subset we use contains 327 claims, of which 202 are supported, and 125 are refuted.\n\nCOVERT (Mohr et al., 2022) is a dataset of health-related claims, which are all causative in nature (such as \"vaccines cause side effects\"). All the claims originate from Twitter, which brings an additional challenge of informal language and provides a real-world scenario of misinformation checking. The subset we use contains 264 claims, of which 198 are supported, and 66 are refuted.\n\nWe find these three datasets to be well suited for our study because they are representative of three different applications of fact verification: helping researchers in their work (SCIFACT), verifying everyday user questions (HEALTHFC), and misinformation detection on social media (COVERT).\n\nWe take claims from these datasets and use them as input to our system. To evaluate if the prediction is correct, we use the original veracity gold label. We do not give the system any original gold evidence documents from the datasets, as we are studying an open-domain setting. In essence, we evaluate the performance of the whole system by looking at its final classification performance as a \"proxy\" and observing how it changes when varying different parameters (Chen et al., 2024). While an important class in datasets is not enough information (NEI), we simplify the problem to only the supported and refuted classes and leave NEI for future work. Therefore, we use binary precision, recall, and F1 score as the evaluation metrics."}, {"title": "5 Results and Discussion", "content": "The first three rows of Table 1 show the results of the traditional three-part pipeline (described in Section 3.1) taken from the related study by Vladika and Matthes (2024a). It compared the performance over three knowledge sources: PubMed, Wikipedia, and online search. The results in further rows are from the experiments done in this study.\n\nImprovement. As seen in Table 1, the step-by-step verification systems considerably improved the final F1 performance on all three datasets, especially precision values. The first GPT system improved the F1 performance by +4.3 on HealthFC, +3.4 on CoVERT, and +4.9 on SciFact, which is a major improvement when compared to the traditional pipeline using single-turn verification. This answers our main research question.\n\nInternal vs. External Knowledge. Utilizing web search improved the performance in all cases for SciFact, showing that this dataset worked better when grounded to biomedical studies found online. For the other two datasets, which contain common health claims, there were instances where internal knowledge of LLMs even outperformed the web search. This is a very noteworthy finding, demonstrating how LLMs already encode a lot of internal medical knowledge that can be useful in knowledge-rich tasks, as observed by Singhal et al. (2023) and Vladika et al. (2024b). Similarly, Frisoni et al. (2024) showed how using LLM-generated evidence passages can improve medical QA performance more than retrieved passages.\n\nPredicate Logic. The next experiment incorporated first-order-logic predicates into the FV process. In the GPT system, this resulted in the best overall performance for HealthFC, ending at 81.7 F1 (+5.2 improvement to baseline, +1 to without predicates). This is because predicates, like Outcomes(Tamoxifen, Breast Cancer), led to more precise and targeted evidence, as indicated by the increase in precision scores. On the other hand, while precision also increased for the other two datasets, it led to large drops in recall, resulting with a lower F1. This was especially seen with informal language in CoVERT claims, where produced predicates included underspecified instances like Has(Person, Covid), which only degraded the evidence retrieval process. Therefore, predicates are better suited for clearly written queries and for complex claims.\n\nChoice of LLMs. Comparative analysis of different LLMs was the last round of experiments. Overall, GPT-4o-mini came out on top as the best LLM for the task. Table 2 shows an example of generated questions for all three LLMs for different claims. It is evident that GPT gives the most general and simplest questions, whereas LLaMa and Mixtral provide more specific and detailed questions. The specific questions can be a strength but also complicate the evidence retrieval process with noisy retrieved passages. GPT was the best at following the style of few-shot example questions. Also, Mixtral produces the most questions on average per claim, followed by GPT, and then LLaMa. Finally, we observed the reasoning capabilities of models to be on a similar level, showing the final performance is often dependent on the quality of question generation and answering.\n\nQualitative Analysis. As evident in Table 2, a lot of generated questions were asking for definitions of the diseases, symptoms, drugs, and other terms found in claims. Once such complex terms were described, the FV process was well-equipped to continue with the verification. This explains why the step-by-step systems worked so well for medical claims, similarly to multi-hop claims in previous studies \u2013 they inherently contain complex concepts and relations that shall be clarified first before making the final decision.\n\nA common reason for errors in the system was the generated questions going too in-depth about a certain point with its follow-up questions and not collecting wider evidence about other parts of the claim. Moreover, another issue were knowledge conflicts - when the LLM would predict an incorrect label even when shown evidence to the contrary because of its encoded internal knowledge.\n\nFuture work could expand the system to leverage structured knowledge sources like knowledge graphs (Kim et al., 2023) or use methods like formal proof generation (Strong et al., 2024). The final step of the system focusing on explanation generation should ideally include different user perspectives in the process (Warren et al., 2025)."}, {"title": "6 Conclusion", "content": "In this study, we develop a step-by-step system for fact verification based on iterative question generation and explainable reasoning. We apply the system on three medical fact-checking datasets and test different settings. We show that by utilizing LLMs, this system can create follow-up questions on complex concepts and relations from the claims in order to gather background evidence, reason over newly discovered evidence, and finally lead to predictions that achieve higher results when compared to traditional pipelines. We hope that our study encourages more exploration of advanced systems for domain-specific fact verification."}, {"title": "Limitations", "content": "Since all modules of the step-by-step verification system rely on using LLMs, they come with their own set of challenges and limitations. The generated follow-up questions are not always perfect or precise, the generated evidence snippets can be off point, and the reasoning over long chains of evidence can, of course, lead to logical errors and mistakes. We observed certain instances where even though all the evidence was pointing towards one of the verdicts (refuted), the system would still mistakenly output the other one (supported).\n\nAnother limitation comes from the high complexity of the system and reliance on calls to external APIs, including LLM APIs and search engine APIs. This inevitably led to some challenges in terms of slower processing speed of this system when compared to traditional approaches that use an out-of-the-box NLI model like DeBERTa. Still, we were forced to rely on API calls for LLMs due to hardware resource limitations, but models like Mixtral and LLaMa showed decent performance and are open-weights, so they can be downloaded and run locally to speed up the performance.\n\nLastly, for easier evaluation we disregard claims annotated with Not Enough Information due to different definitions of this label across different datasets (e.g., the definition from SciFact does not serve the open-domain setting well). This is an important label in fact verification, since not all claims can be conclusively assessed for their veracity. This is especially important in the scientific domain considering the constantly evolving nature of scientific knowledge, and sometimes conflicting evidence from different research studies. Future work should find a way to effectively include this label into model predictions."}, {"title": "Ethics Statement", "content": "Our dataset and experiments deal with the highly sensitive domain of healthcare and biomedical NLP. While we observed good scores when verifying health-related question using responses directly generated by language models, this is not a recommended way of using them by end users or patients. Responses can still contain hallucinations or misleading medical advice that should always be manually verified within reliable sources. Similarly, experiments using online search results did not go through any manual quality filtering, which means not all of them will be trustworthy or approved by experts. One should always consult with medical professionals when dealing with health-related questions and advice."}]}