{"title": "EEG-DRIVEN 3D OBJECT RECONSTRUCTION WITH COLOR CONSISTENCY AND DIFFUSION PRIOR", "authors": ["Xin Xiang", "Wenhui Zhou", "Guojun Dai"], "abstract": "EEG-based visual perception reconstruction has become a current research hotspot. Neuroscientific\nstudies have shown that humans can perceive various types of visual information, such as color, shape,\nand texture, when observing objects. However, existing technical methods often face issues such as\ninconsistencies in texture, shape, and color between the visual stimulus images and the reconstructed\nimages. In this paper, we propose a method for reconstructing 3D objects with color consistency\nbased on EEG signals. The method adopts a two-stage strategy: in the first stage, we train an implicit\nneural EEG encoder with the capability of perceiving 3D objects, enabling it to capture regional\nsemantic features; in the second stage, based on the latent EEG codes obtained in the first stage, we\nintegrate a diffusion model, neural style loss, and NeRF to implicitly decode the 3D objects. Finally,\nthrough experimental validation, we demonstrate that our method can reconstruct 3D objects with\ncolor consistency using EEG.", "sections": [{"title": "Introduction", "content": "The human brain's perception and interpretation of color is a complex process involving various neural mechanisms.\nDirectly reconstructing and generating color-consistent 3D objects from brain activity data, such as EEG, represents\na frontier exploration at the intersection of neuroscience and artificial intelligence. In recent years, text-based image\ngeneration techniques have made significant progress, greatly enhancing the quality and consistency of generated\nimages, allowing precise representation of user-specified colors, shapes, and textures. Building on these advancements,\nthe goal of this study is to explore the possibility of reconstructing color-consistent 3D objects using EEG signals,\nwhich represents an important step toward understanding and replicating the human visual perception process. A large\nbody of neuroscientific experimental research [1, 2, 3, 4, 5, 6, 7] has demonstrated that the brain can rapidly perceive\n2D images and 3D objects in extremely short periods of time. One of the primary methods for studying human visual\nperception is using deep neural networks to reconstruct visual content that evokes subjective responses in stimulus\nexperiments. Many studies [8, 9, 10, 11] have attempted to reconstruct visual information based on functional magnetic\nresonance imaging (fMRI). However, the collection of fMRI data requires expensive equipment, limiting its widespread\nuse in practical applications. In contrast, EEG is a more cost-effective technique for capturing brain activity and is easier\nto collect. EEG data are typically recorded as a series of time-series electrophysiological signals by placing electrodes\non the scalp. During this process, subjects are presented with stimulus images, while brain signals are simultaneously\nrecorded."}, {"title": "Related Work", "content": "EEG-Based Image Synthesis Over the past few years, text-to-image generation models have developed rapidly. For\nexample, diffusion models have made significant progress in this field, as they are capable of extracting complex latent\nsemantic features from text descriptions and generating high-quality object and scene images [17, 18, 19, 20, 13]. In\nthis paper, we fine-tune the diffusion model using EEG-ImageNet [1] and Things-EEG2 [21] dataset to achieve the\ntask of generating 2D images from EEG signals. For instance, DreamDiffusion [13] fine-tunes the diffusion model\nthrough global semantic alignment, but there still remains a certain gap between the generated images and the actual\nimages. To enhance the realism of the generated images, the proposed method combines EEG signal reconstruction\nand classification to help the model capture regional semantic features, enabling the diffusion model to better learn the\nregional mapping relationship between EEG signals and images.\nEEG-to-3D Generation In recent years, significant progress has been made in text-to-3D object generation models\n[22, 15, 16], but EEG-to-3D object generation has not yet been fully explored. Inspired by text-to-3D methods such\nas [15] and [16], this paper attempts to utilize latent EEG codes to guide NeRF in reconstructing 3D objects through\nthe shape priors of a diffusion model. To further investigate the brain's ability to perceive color visual information\nfrom EEG, and drawing upon content and style transfer methods such as [23] and [24], we employ content and style\nlosses to ensure that the reconstructed 3D objects maintain color consistency with ground truth (GT) images. Using\nthis approach, we have, for the first time, achieved the reconstruction of color-consistent 3D objects from EEG signals,\nindirectly validating the neuroscientific theory that the human brain can perceive various types of visual information,\nsuch as color, shape, and texture, when observing objects."}, {"title": "The Neuroscientific Analysis of Our Method", "content": "Our dataset is sourced from [1], where each image is displayed for 0.5 seconds, during which EEG is collected\nsimultaneously. Based on references [2, 25, 3, 6], it is known that the brain is capable of acquiring visual information\nwithin 0.5 seconds. Therefore, we hypothesize that EEG has already perceived specific 3D texture information within\nthis 0.5-second window. Our work proposes this hypothesis and experimentally verifies its existence. To analyze how\nthe brain captures visual perceptual information in such a short timeframe, we adopt a methodology incorporating 3D\nand color perception. This approach facilitates a more comprehensive explanation and understanding of the brain's\nvisual perception process. Not only does it aid researchers in exploring perceptual mechanisms, but it also advances\ntheoretical research pertaining to vision."}, {"title": "Methodology", "content": "As shown in Figure 2, to reconstruct color-consistent 3D objects from EEG signals, we propose an EEG-to-3D\narchitecture with color consistency and semantic awareness. The proposed method mainly consists of two components:\n1) a semantic-aware implicit neural EEG encoder for 3D objects, and 2) a color-consistent implicit neural decoder for\n3D objects. First, we utilize EEG signals for implicit neural encoding of 3D objects. In this stage, we obtain latent EEG\nfeatures and class tokens through joint training tasks involving EEG signal reconstruction and classification, and these\ntwo components are merged and passed through a linear layer to generate EEG codes. Next, we use the EEG codes\nobtained in the previous stage to provide conditional decoding features for the reverse diffusion process of the diffusion\nmodel via a cross-attention mechanism, generating novel 2D views while using content and style losses to ensure that\nthe colors of the novel 2D views remain as consistent as possible with GT. Finally, we use EEG codes to initialize the\nLDM to generate novel 2D views from different perspectives to optimize NeRF, enabling the implicit neural decoder to\ndecode 3D objects."}, {"title": "Stage A: Multi-Task Joint Learning of EEG Encoder", "content": "During the initial phase, the process leverages EEG signals for the implicit neural encoding of 3D objects. Specifically,\nwe first extract the regional semantic features of EEG by using a masked reconstruction model to capture the temporal\nfeatures of EEG, thereby obtaining its regional information. Next, we perform a semantic classification task on\nEEG signals, enabling EEG encoder to learn semantic features. Through the EEG reconstruction task and semantic\nclassification task, we capture the semantic features of regions, which serve as input for subsequent steps. Then, we\nexplicitly introduce the semantic regional features of the original and reconstructed images by leveraging latent EEG\ncodes containing latent information of semantic regions. We jointly fine-tune LDM to incorporate the performance of\nsemantic regions, making the generated images more similar to GT in terms of regional and spatial relationships."}, {"title": "Reconstruction and Classification Tasks of EEG", "content": "EEG Reconstruction Task Traditional models struggle to effectively extract meaningful features due to the complex\nspatial regional information characteristics of EEG. However, as demonstrated by the work and ideas of [30], it is\npossible to capture valuable information from the context of EEG signals. Therefore, we use a masked model that\nrandomly masks portions of EEG signals and then reconstructs them to achieve this purpose. Combining the findings\nof [15] and [30], we mask a certain proportion of EEG signals based on their temporal features and use [31] method to\nconvert EEG into 1D data to embed it into the network. By considering the contextual temporal cues to predict the\nmissing signals, we can learn the regional information."}, {"title": "EEG Classification Task", "content": "Prior studies have reconstructed EEG signals using an EEG encoder [30, 13]. However,\nobtained latent EEG codes currently only encompass temporal and spatial characteristics, lacking crucial semantic\nfeatures. Due to the lengthy temporal nature of EEG signals, establishing global relationships for EEG data classification\nposes a challenge. Therefore, we employ a Temporal Transformer to correlate time-series features with their own\nfeatures, extracting global features of EEG over the time sequence. This allows for the extraction of temporal features\nfrom EEG signals, classification of EEG signals, and subsequent acquisition of class tokens. By integrating these\nfeatures with the output from the EEG encoder, we aim to further enhance the accuracy of semantic classification."}, {"title": "Loss Function between EEG and Latent Diffusion Models", "content": "Under the combined influence of reconstruction and classification based on EEG signals, an EEG encoder with temporal,\nspatial, and semantic features is obtained. Using this EEG encoder along with semantic embedding, we map EEG\nsignals to a latent space constrained by semantic regions. We integrate cross-attention mechanisms and latent conditional\nfeatures t, and utilize the denoising U-Net to replace the original temporal pixel space features in LDM with conditional\nlatent features $z_t$, thereby optimizing the loss function. This forms the conditional loss function for LDM:\n$L_{idm} = E_{z,e \\sim N(0,1),t} [||\\epsilon - \\epsilon_{\\theta} (z_t, t, T_{\\theta}(Y))||^2]$\nTo endow LDM with regional semantic performance, we utilize the pre-trained SAM to obtain the regional semantic\nmaps of visual stimulus images and their corresponding reconstructed images. The regional semantic loss is calculated\nby the cross-entropy loss function as follows,\n$L_{region} = \\frac{1}{NM} \\sum_{i=1}^N \\sum_{k=1}^M P_{i,k} \\log (\\hat{P}_{i,k})$\nwhere N is the number of pixels, M is the number of categories, $p = SAM (S)$ and $\\hat{p} = SAM (S')$ denote the regional\nsemantic maps of the visual stimulus image S and its corresponding reconstructed image S', respectively. Combining\nand (2), we finally obtain:\n$L_{total} = \\lambda_{idm}L_{idm} + \\lambda_{region}L_{region},$\nwhere $\\lambda_{idm}$ and $\\lambda_{region}$ are balancing factors used to balance the influence between $L_{idm}$ and $L_{region}$, with default\nvalues of 1. Subsequently, through backpropagation, we continuously fine-tune and update the weights of the LDM to\nendow it with the capability to capture semantic regions."}, {"title": "Stage B: EEG-to-3D with Color Consistency and Diffusion Priors", "content": "As Stage B, we utilize the reference view x generated by EEG and latent EEG codes to reconstruct NeRF, and\nconstrain the novel view through a diffusion prior conditioned on the latent EEG codes. The proposed EEG-to-3D\nmethod is expected to simultaneously meet the following requirements: 1) It can generate 3D objects using EEG that\nclosely resemble the rendering appearance of the reference view x; 2) Novel view renderings should exhibit semantic\nconsistency with the reference view x, while also maintaining color consistency; 3) The generated 3D objects should\npossess well-defined geometric structures.\nText-to-3D DreamFusion [15] demonstrated its capabilities in the field of text-to-3D synthesis by leveraging a\npretrained text-to-image diffusion model [18] as a strong image prior. DreamFusion achieves text-to-3D generation\nthrough two key components: a pretrained text-to-image diffusion-based generative model and a neural scene repre-\nsentation of the scene model. The scene model is a parametric function $x = g(\\theta)$, which generates an image s at the\nspecified camera pose. Here, g is a volumetric renderer, and $\\theta$ is a coordinate-based multi-layer perceptron (MLP)\nrepresenting a 3D volume. The diffusion model $\\epsilon$ includes a learned denoising function $\\epsilon_{\\theta}(x_t; y, t)$, which predicts\nthe sampled noise $\\epsilon$ given the noisy image $x_t$, noise level t, and text embedding y. It provides the gradient direction\nto update $\\theta$ such that all rendered images are pushed toward high-probability density regions conditioned on the text\nembedding under the diffusion prior. Specifically, DreamFusion introduces Score Distillation Sampling (SDS) to\ncompute the gradient.\n$\\nabla_{\\theta}L_{SDS} (\\phi, g(\\theta)) = E_{t, \\epsilon} [w (t) \\cdot (\\epsilon_{\\theta} (z_t; y, t) - \\epsilon) \\cdot \\frac{\\partial z_t}{\\partial x} \\frac{\\partial x}{\\partial \\theta}]$\nHere, w(t) is a weighting function. We regard the scene model g and the diffusion model as modular components within\nthe framework, with $\\epsilon$ serving as the denoising function."}, {"title": "EEG-to-3D with Diffusion Priors", "content": "To ensure the semantic coherence of the EEG to 3D objects, additional constraints\nneed to be applied to the novel view renderings. To address this challenge, we adopt a diffusion prior. Previous works\non text-to-3D generation [15, 16] have applied LSDS to leverage text-conditioned diffusion models as 2D diffusion\npriors. In the method proposed in this paper, we employ a diffusion prior conditioned on latent EEG codes to optimize\nthe views generated by NeRF, gradually refining them from blurry to sharp. In this case, to apply LSDS, we use the\nlatent EEG codes obtained in the first stage as the latent prompt y, allowing us to perform LSDS within the latent space\nof the diffusion model:\n$\\nabla_{\\theta}L_{SDS} (\\phi, g(\\theta)) = E_{t, \\epsilon} [w (t) \\cdot (\\epsilon_{\\theta} (z_t; y, t) - \\epsilon) \\cdot \\frac{\\partial z_t}{\\partial x} \\frac{\\partial x}{\\partial \\theta}]$\nhere, y represents latent EEG codes, $z_t$ denotes the noisy latent representation, $\\frac{\\partial z_t}{\\partial x}$ refers to the gradient of the LDM\nencoder, and $\\frac{\\partial x}{\\partial \\theta}$ represents the gradient of the rendered image."}, {"title": "EEG-Text Information Aligning with NeRF", "content": "According to $L_{SDS}$ in DreamFusion, this function is used to measure\nthe similarity between novel views and text prompts. However, in this work, 3D objects are generated based on\nEEG signals, thus requiring further alignment between the EEG signals and novel views from different perspectives.\nTo achieve this, we leverage a pretrained CLIP text encoder to align the EEG signals with the novel views. The\ncorresponding loss function is as follows:\n$L_{EEG-Text} = -E_{clip(T/I)} \\cdot \\rho(\\phi_{latent}(y)) \\cdot E_{clip(g(\\theta))}$\nHere, $E_{clip(T/I)}$ includes the CLIP image encoder and text encoder. Our objective is to align the text prompt of the\nnovel view's rendered image g($\\theta$), the text prompt of the denoising image under the diffusion prior, and the EEG\nsignals."}, {"title": "Neural Style Transfer", "content": "As shown in Figure 3, the goal of neural style transfer is to transfer the style $I_s$ of GT onto the\ncontent image $I_c$, generating a stylized image $I_o$, while ensuring that the reconstructed image preserves the color realism\nof the GT and the content remains unchanged. According to [32, 33], adding a new loss term to the optimization\nobjective enhances the realism of stylized images produced by the neural style transfer algorithm, particularly in\npreserving the local structures of the content image. Building upon these references, we incorporate both style loss and\ncontent loss in the style transfer process. We utilize a pretrained VGG network to extract features from the target image,\nwhich are used to compute the content and style losses. By combining these two losses, the network generates new\nimages that retain the content while adopting a specific style. To achieve the aforementioned style transfer, we use the\npretrained VGG feature extractor $\\zeta(.)$ to extract content and style features, denoted as $F_c = \\zeta(I_c)$ and $F_s = \\zeta(I_s)$,\nrespectively. The output image $I_o$ is optimized using the following objective function, which includes content loss\n$L_{content}$ and style loss $L_{style}$:\n$L_{color} = argmin{L_{content}(\\zeta(I), F_c) + \\lambda L_{style}(\\zeta(I), F_s)}$\nhere, $\\zeta(.)$ represents the feature extractor, and $\\lambda$ is the balancing factor between the content loss and style loss."}, {"title": "Loss Function with Diffusion Priors and Color Consistency", "content": "The overall loss in the second stage can be represented\nby $L_{SDS}$, $L_{EEG-Text}$, and $L_{color}$. In this work, the diffusion prior generated under the condition of latent EEG codes\nis used to guide NeRF in generating 3D objects with consistent color. $L_{SDS}$ is used to optimize NeRF to achieve\nbetter geometric shapes and details, $L_{EEG-Text}$ is employed to align the EEG, text, and image, and $L_{color}$ is used to\nadjust the reconstructed color of the 3D objects to ensure they remain as consistent as possible with GT color, thereby\nenhancing the realism and visual accuracy of the 3D reconstruction."}, {"title": "Experiments", "content": ""}, {"title": "Dataset and Implementation Details", "content": "EEG-ImageNet Dataset The EEG-ImageNet dataset comes from the PeRCeiVe Lab [1], which contains visual\nstimulus EEG signals recorded from 6 subjects. The visual stimulus images encompass a total of 40 categories, with\neach category comprising 50 images sourced from ImageNet. Each image within the same category is displayed\nconsecutively for 0.5 seconds, and there is a 10-second interval between each category. The collected dataset comprises\na total of 11,964 segments (corresponding to visual stimuli for EEG). Each EEG segment includes 128 channels, and\nthe EEG signals are in three frequency ranges: 14-70Hz, 5-95Hz, and 55-95Hz.\nThings-EEG2 Dataset To validate the effectiveness of the proposed method, we introduce an additional Things-EEG2\ndataset [21]. This dataset includes ten subjects, with a total of 1,654 training sample categories. Each image is displayed\nfor 100 ms with a 750 ms blank interval. All participants completed four equivalent experiments, resulting in 16,540\ntraining images, each condition repeated four times, and 200 test images, each condition repeated 80 times.\nImplementation Details of EEG-to-3D In this work, the latent EEG codes obtained in Stage A are used to guide\nLDM in generating diffusion priors, with Stable Diffusion version 1.5 employed. To accelerate NeRF training and"}, {"title": "Results", "content": ""}, {"title": "Quality with 2D", "content": "As illustrated in Table 1, in Stage A, this study leverages EEG data combined with fine-tuning of LDM to reconstruct the\ngenerated 2D images. By accurately decoding the latent space that encapsulates semantic region information through\nEEG, the fine-tuned LDM successfully achieves a high-fidelity reconstruction of the original images."}, {"title": "Quality with 3D and Color Consistency", "content": "As shown in Table 2, this paper reconstructs 3D objects from EEG signals under two conditions: with and without\nstyle transfer loss. For the reconstruction of 3D objects with color consistency, the proposed method integrates latent\nEEG codes, semantically accurate reconstructed images, and style transfer to generate 3D objects with consistent color.\nThese reconstructed 3D objects not only exhibit high geometric similarity to GT objects but also maintain consistency in\ncolor. This demonstrates that our model effectively captures the core features of 3D objects in the process of translating\nEEG signals into complex visual information, providing evidence that EEG signals can perceive and process various\nvisual information such as color, shape, and texture."}, {"title": "Quantity", "content": "As shown in Table 3 and Table 4, our quantitative analysis results are presented. We primarily used the following\nmetrics to evaluate the performance of the reconstructed 2D and 3D models: 1) Fr\u00e9chet Inception Distance (FID) [35],\nwhich assesses the difference between the reconstructed 2D images and real images; 2) Structural Similarity Index\nMeasure (SSIM) [44], which measures the authenticity of the 2D images; 3) Inception Score (IS) [36], which evaluates\nthe quality and diversity of the reconstructed images; 4) LPIPS [41], which assesses the 3D reconstruction quality\nof reference views; and 5) Contextual Distance [42], which measures the pixel-level similarity between novel view\nrenderings and reference images.\nQuantitative Analysis of 2D Among these metrics, 1), 2), and 3) are quantitative indicators for 2D images. As\nshown in Table 3, some values are missing due to the fact that certain metrics were not reported in the literature for the\ncomparison methods, and the source code was not publicly available. For the Acc metric in the 50-way top-1 task, the\naccuracy of our proposed method is 21.4% higher than that of DreamDiffusion. In terms of the IS metric, our model"}, {"title": "Quantitative Analysis of 3D", "content": "Metrics 4) and 5) are qualitative indicators for 3D objects. As shown in Table 4, we\nevaluated the LPIPS and Contextual Distance metrics from different angles during the process of generating color-\nconsistent 3D objects using EEG signals. The Acc metric here refers to the scores obtained using CLIP, where we\nassess the semantic similarity between the newly generated views and the reference views to measure the results."}, {"title": "Ablation Study of 3D", "content": "We conducted three ablation experiments to quantitatively analyze the impact of each\ncomponent on the visual quality of color-consistent 3D objects. Table 5 presents the results of the ablation studies.\nWhen all components are utilized, our network achieves the best performance.\n1) $L_{color}$. To enable the EEG reconstruction of color-consistent 3D objects, we introduced neural style loss, which\nensures that the 2D images reconstructed from different viewpoints using EEG maintain color consistency with GT.\nThis approach facilitates the generation of color-consistent 3D objects.\n2) $L_{EEG-Text}$. Since the original Text-to-3D model aligns text prompts with novel views from different perspectives,\nwe introduce the alignment between CLIP's text encoder and EEG signals. $L_{EEG-Text}$ aligns the text prompt of the\nrendered image g($\\theta$) from a new viewpoint, the text prompt of the denoising image under diffusion priors, and the EEG\nsignals.\n3) $L_{region}$. To enable the diffusion model with semantic region-aware capabilities, we keep the inherent $L_{idm}$ property\nof the LDM unchanged. By integrating latent EEG codes with regional images, we fine-tune the LDM, allowing it to\nreconstruct images with precise positional and spatial representation."}, {"title": "Conclusions", "content": "We propose an EEG-to-3D with Color Consistency approach, which, for the first time, utilizes EEG signals to generate\nhigh-fidelity 3D objects with consistent color. This method employs a two-stage strategy, integrating EEG signals, visual\nstimulus images, and style loss, to jointly fine-tune LDM and NeRF through collaborative training. The reconstructed\n3D objects exhibit a high degree of realism and consistency in both geometry and color."}, {"title": "Limitations", "content": "At the Stage A phase, the semantic accuracy of 2D images reconstructed from EEG signals still has\nroom for improvement, which is expected to further enhance the clarity of the generated 3D objects. Nevertheless, it is\nnoteworthy that the findings of this study have demonstrated that brain signals can perceive and encode various visual\nfeatures, such as the color, shape, and texture of objects."}, {"title": "Main Generation Samples.", "content": "As shown in the figure below, we present several key reconstructed images, which feature color-consistent 3D objects.\nOur model utilizes EEG to directly reconstruct color-consistent 3D objects, accurately restoring the spatial positions of\nobjects and the spatial relationships between multiple objects within the scene."}]}