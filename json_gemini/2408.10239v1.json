{"title": "A Conceptual Framework for Ethical Evaluation of Machine Learning Systems", "authors": ["Neha R. Gupta", "Jessica Hullman", "Hari Subramonyam"], "abstract": "Research in Responsible AI has developed a range of principles and practices to ensure that machine learning systems are used in a manner that is ethical and aligned with human values. However, a critical yet often neglected aspect of ethical ML is the ethical implications that appear when designing evaluations of ML systems. For instance, teams may have to balance a trade-off between highly informative tests to ensure downstream product safety, with potential fairness harms inherent to the implemented testing procedures. We conceptualize ethics-related concerns in standard ML evaluation techniques. Specifically, we present a utility framework, characterizing the key trade-off in ethical evaluation as balancing information gain against potential ethical harms. The framework is then a tool for characterizing challenges teams face, and systematically disentangling competing considerations that teams seek to balance. Differentiating between different types of issues encountered in evaluation allows us to highlight best practices from analogous domains, such as clinical trials and automotive crash testing, which navigate these issues in ways that can offer inspiration to improve evaluation processes in ML. Our analysis underscores the critical need for development teams to deliberately assess and manage ethical complexities that arise during the evaluation of ML systems, and for the industry to move towards designing institutional policies to support ethical evaluations.", "sections": [{"title": "Introduction", "content": "Machine learning (ML) model evaluation typically focuses on estimating errors of prediction or estimation via quantifiable metrics. Given the increasing size and complexity of ML systems, comprehensive evaluations should ideally be multifaceted. For example, evaluations of large ML systems may include several methods, including A/B testing on live populations, adversarial testing to produce undesirable outputs, and comprehensive audits documenting outputs.\nPotential ethical harms of ML systems have gained increasing attention in the broad Responsible AI community. However, even when evaluation metrics are expanded beyond performance to include factors like fairness, privacy loss, or other harms induced by the machine learning system, this is often focused on the ethical harms of the released system, overlooking possible harms incurred during the machine learning development lifecycle itself. This is problematic because evaluation approaches do have the potential to cause ethical harm during evaluation. In a noteworthy example, Tesla's autonomous vehicle live testing systems on real roadways in California, has been widely criticized for being involved in various crashes (Nayak, Laing, and Hull 2022).\nHow should practitioners evaluate large complex systems with potentially unknown ethical harms across the engineering lifecycle, including during the evaluation process? We provide a conceptual framework that casts the primary trade-off in ethical evaluation decision-making as balancing the goal of optimizing for information gained in an evaluation, against the possible ethical harms that are induced.\nBased on our sketch of this fundamental problem that practitioners face, we identify a series of challenges that can cause practitioners to stumble in selecting ethical evaluation practices. We illustrate these challenges using real-world examples of machine learning evaluations that encountered them. Then, we draw parallels between these challenges and evaluation practices in domains other than machine learning, to explore potential mitigation techniques. Together, our conceptual framework and characterization of challenges are intended to stimulate discussion among researchers and evaluation teams on how to balance information gain with potential ethical harm, and to motivate future exploration of policies or best practices for machine learning evaluation."}, {"title": "Related Works", "content": "A growing body of literature discusses properties that ethical machine learning systems should inherently possess, and provides principles and guidelines for testing (Jobin, Ienca, and Vayena 2019; Zhang et al. 2020; Mart\u00ednez-Fern\u00e1ndez et al. 2022). Broadly speaking, the ethical values identified by prior work include: (1) Non-maleficense, which measures the extent to which the evaluation workflows and outcomes do not inflict harm or injuries on any individual or population (Mehrabi et al. 2021). (2) Privacy, which as a value refers to the principle of protecting personal and sensitive information from unauthorized access, use, or exposure during the entire ML lifecycle (Liu et al. 2021). (3) Fairness, in which the goal is to achieve equitable treatment and outcomes for all individuals, ensuring that the benefits and burdens of AI systems are distributed justly across diverse populations (Mehrabi et al. 2021), (4) Cultural Sensitivity, which involves designing algorithms and models that are attuned to and respectful of cultural differences, ensuring that they do not perpetuate stereotypes, biases, or insensitivities (Jo and Gebru 2020). (5) Sustainability, which ensures that models are developed in a manner that is environmentally responsible, economically feasible, and socially equitable (on Artificial Intelligence 2019; Lacoste et al. 2019). (6) Societal Impact, which refers to ensuring that the models contribute positively to societal well-being, address social issues, and do not harm individuals or communities (Birhane et al. 2022). We emphasize that this is not a comprehensive listing of all ethical values that a machine learning system may seek to accomplish. We provide examples of these categories when characterizing challenges in designing ethical evaluations in later sections."}, {"title": "ML System Evaluation Practices", "content": "An evaluation is the process by which practitioners detect differences between desired and actual model behavior (Zhang et al. 2020), through empirical assessment of model properties (Shevlane et al. 2023). A growing body of work creates more comprehensive methods with which to evaluate systems, rather than providing a singular empirical metric or set of metrics. Some evaluation methods can be conducted pre-deployment, such as A/B testing or live testing. Other mechanisms are used post-deployment, such as bug bounty challenges, and provide infrastructure to support stakeholder feedback. A particular evaluation process may involve choosing one or many evaluation metrics to measure. These decisions are critical because they impact actions that are taken post-evaluation to improve system capabilities. They also may be associated with the potential for ethical harm incurred in the evaluation process, or after product release. We define an ethical evaluation as an evaluation that does not sacrifice ethical values in its implementation, and attempts to forecast downstream ethical harms across the product lifecycle.\nFinally, by considering how to conceive of the value of information about model performance gained through an evaluation, our work is related to data valuation. Prior theoretical work in machine learning and related fields studies the value of data for purposes like explainability (e.g., the Shapley framework (Ghorbani and Zou 2019)), data markets and incentivizing collaboration in ML (Castro Fernandez 2023; Sim et al. 2020), and value of accurate or improved prediction for goals like treatment assignment or welfare maximization (Liu et al. 2024; Perdomo 2023)."}, {"title": "Ethical Evaluation Model", "content": "The economics discipline has a long history of creating highly simplified models of complex real-world processes to assist with predicting the consequences of actions. Abstracting away non-essential features of the complex real-world permits systematic reasoning.\nFor example, economic policy-makers concerned with pricing wheat might use a simplified model that includes the costs to the farmer while abstracting away other potentially relevant characteristics, such as soil quality and his educational background (Friedman 1953). Our conceptual model of the key trade-off in ML practitioners' evaluation decision-making focuses on the value of information gain relative to ethical harms. However, rather than contributing new theoretical results, our goals are epistemological: to prompt reflection on what it would mean to select the best evaluation in a way that accounts for potential ethical harms induced in evaluation. By conceptualizing the idea of an optimal balance between competing concerns in designing ML evaluations, our framework is meant to highlight difficult questions that largely remain un-navigated in the literature and practice of ML evaluation design, rather than to imply that a normative evaluation design is easy to identify. Below, we discuss the implications of components of the model, including the acceptability of some of the assumptions made for the sake of this model, issues that arise due to differing aims, and the subjectivity of variables in further sections."}, {"title": "Model Properties", "content": "An evaluation is a protocol for assessing and measuring a model's performance against a set of defined criteria or benchmarks, including specification of which information to collect and how. ML development teams face various considerations when planning evaluations that involve complex decisions across evaluation scope, context, and effect (Zhang et al. 2020; Riccio et al. 2020; Song et al. 2022); prior work has described how practitioners can suffer from a \"paradox of choice\" when it comes to deciding how to perform evaluation (Goel et al. 2021). We represent the space of possible evaluations under consideration by a team as $A = \\{a_1, a_2,...\\}$ where a is an evaluation decision (e.g., evaluation method, metrics, sample selection, etc.).\nchoose some $a \\in A$\nThe utility of an evaluation approach depends on the relative value of information gained, ethical harms, and resource costs. The fair ML literature has represented decisions about model choice in ML in a utility framework, where models provide utility as a function of costs and benefits (Corbett-Davies et al. 2017; Corbett-Davies and Goel 2018; Chohlas-Wood et al. 2021). For example, in Corbett-Davies et al. (2017), the authors conceptualize 'immediate utility' reflecting the costs and benefits of a fair decision by a policymaker in the setting of pre-trial bail release decisions. A utility framing is also used in Hutchinson et al., to illustrate the task of evaluating an ML model's suitability for use in a specific application ecosystem.\nOur conceptualization similarly draws on a utility framework common in statistical decision theory (Savage 1972; Steele and Stef\u00e1nsson 2015; Von Neumann and Morgenstern 2007), but expands this to a broader view applied to decisions made by teams evaluating ML models or systems. The proposed utility function implies that each evaluation decision option the team is considering can be compared along a single dimension (utility) along which they can be ranked.\nWe conceive of three categories of inputs to the utility function. The first concerns the value of information gain. The information learned from a test, regarding differences between desired and actual model behavior (Zhang et al. 2020), implies a gain from the evaluation process. Information gained in conventional ML performance evaluation includes estimates of how well a trained model generalizes to new samples from the same distribution the model was trained on, as well as measures of the robustness of the model, i.e., the degree to which the model or system maintains its correctness and performance under varying conditions and inputs, including invalid or adversarial inputs (Tjeng, Xiao, and Tedrake 2017; Zhang et al. 2020). When the evaluation produces information about the model performance along ethical dimensions, the information gained may come in the form of the expected magnitude or frequency of harms upon deployment.\nThe second component is ethical harms of the evaluation. Often overlooked, ethical issues incurred during evaluations, or downstream ethical issues not adequately predicted (and consequently encountered after deployment), can diminish the acceptability of the results, and the overall value and utility of the information derived from the evaluation.\nThe third component measures the material resources required to conduct an evaluation. Teams often consider options for reducing the costs of tests via methods such as test prioritization, in which inputs generated for tests are limited to inputs that are most indicative of problematic behavior (Zhang et al. 2020). Costs can take several forms. For instance, monetary constraints may restrict data collection abilities including the number of labelled data annotations procured for supervised ML models (Liao, Kar, and Fidler 2021; Goel and Faltings 2019). Cost constraints through labor force availability and time constraints can shift teams towards using automated software testing (Dustin, Garrett, and Gauf 2009). These resource constraints can challenge responsible model development (Hopkins and Booth 2021).\nConsequently, we represent the utility of an evaluation decision as having these three inputs, with the information gain representing gains to utility and potential ethical harm and material cost representing decreased utility:\n$u(a) = (information \\, gain - ethical \\, harm - cost)$\nInformation gain, ethical harm, and cost are forecasts.\nBefore conducting an evaluation, a team cannot precisely predict the information gained about model performance, potential ethical harm, or exact material costs involved. Consequently, information gain, ethical harm, and cost as predicted values, which we represent as expectations over relevant sources of randomness. The fact that these quantities must be predicted emphasizes the uncertainty under which evaluation decisions are necessarily made.\nThese values are represented as expectations over distribution of possible values. Estimating these distributions is a fundamental part of the challenge in selecting an evaluation. Any particular evaluation involves a sample of instances for which evaluation data is gathered. Many evaluations sample from a population of participants. Complications arise as models can differ in social impact across groups, notably underrepresented groups (Hutchinson et al. 2022). Thus, the estimate of expected \"ethical harm\" requires averaging harm across several individuals who experience disparate impact from the models. Moving forward, we abbreviate information gain as $IG(a)$, and ethical harms as $EH(a)$:\n$u(a) = E(IG(a)) \u2013 E(EH(a)) \u2013 E(cost)$\nEthical harm can be decomposed by distinct ethical values. It is important to differentiate between various ethical values in our model, because it has been established that there are situations where some models may benefit a particular ethical value at the cost of another. For example, a privacy and fairness trade-off affects some ML models (Pujol and Machanavajjhala 2021). While interactions between ethical concerns may exist, for simplicity we think of $E(EH(a))$ as representing a weighted sum of various ethical values, so $E(EH(a)) = \\sum_jw_jE(EH_j(a))$ where j paramaterizes the ethical values discussed in Section 2. These weights can represent differing ethical priorities of teams or regulatory requirements on particular values.\n$E(EH(a)) = \\sum_j\u03c9_jE(EH_j(a))$\nThe best evaluation method has the highest utility. We represent the optimal choice of evaluation practice for the team as the decision with the highest utility. An evaluation approach $a$ equals the optimal decision $a^*$ if it is the utility-maximizing decision, indicated as:\n$a^* = argmax_{a\u2208A}U(a)$\nThe final form of the utility model can be written as:\n$a^* = argmax_{a\u2208 A}E(IG(a)) - \\sum_jw_jE(EH_j(a)) \u2013 E(cost)$   (1)\nDiscussing the interactions between components of the framework, the philosophical challenges, and practical challenges that can arise serves two purposes. First, these challenges do appear in practice. We illustrate the nature of ethical harms resulting from real-world evaluation practices in order to make concrete the sorts of consequences that appear in selecting evaluation practices. We selected the examples below using a broad search across scholarly publications and news media related to ethical issues that arise in ML, with a specific focus on those that can affect evaluation. We focused on identifying instances that varied in evaluation practice, ethical values at risk, and context. We also prioritized examples that provided clear insights into potential incurred harms, or direct evidence of ethical harms.\nSecondly, we discuss real-world evaluations to motivate exploration of mitigation strategies within the ML evaluation industry. We accompany each common challenge with an existing mitigation strategy from ethically-motivated evaluations in domains other than ML. Other fields have established regulatory and administrative systems that help them balance the tradeoffs which arise in evaluation practices, or have informal best practices. These potential mitigation strategies can guide future discussion and move the ML industry towards balancing compliance with ethics.\nOur discussion is distilled into the notation from the notation from the utility model in Table 2.\nTaking the expectation of \u201cethical harm\" aggregates over individuals and groups. Just as a particular value for a model error metric (like accuracy) or a point estimate (like an estimated average treatment effect) can admit numerous solutions that vary at the level of the individual units or groups (e.g., (Coston, Rambachan, and Chouldechova 2021; Gelman, Hullman, and Kennedy 2023; Marx, Calmon, and Ustun 2020)), aggregating ethical harms over different individuals can lead evaluators to overlook individual or group-specific concerns. For example, two evaluation protocols may be expected to result in the same level of ethical harm to participants, while differing greatly in how harm is distributed over the specific participants or groups of participants.\nPoor evaluation choices can mask heterogeneity in health needs, leading to downstream harms. For example, not having information on certain subsets of a population may ultimately result in a lack of access to effective interventions for some groups, because sufficient information was not available to obtain treatments. This can potentially compound effects of health disparities, and increase costs (Bibbins-Domingo, Helman et al. 2022).\nOpen Question for ML evaluation industry: How can we develop evaluation selection guidelines that motivate evaluators to consider individual and group specific impacts of an evaluation design?"}, {"title": "Issue 2: Disagreement on presence or magnitude of ethical harms.", "content": "When facing a decision regarding the best possible evaluation practice, estimating and agreeing upon expected ethical harm, E(EH), is a challenge. Some work in the machine learning and ethics literature argues that a universally acceptable function ranking ethical outcomes does not exist, and that impartiality is simply an ideal (Card and Smith 2020). Practitioners' interpretations of an ethical harm may differ between team members, or with the general public. Furthermore, ethical impacts are considered hard or even impossible to quantify, making it a challenge to prioritize them in metrics-driven development environments (Ali et al. 2023). This is distinct from the potential issue above, in that the magnitude may be similar across demographic groups but still difficult to agree upon.\nUltimately, the response to the perceived ethical harms can be thought of as a loss of utility, one that may have been prevented by a different evaluation design. For example, an alternative design might ask users if they want to opt into experimentation. However, this introduces the potential for less information gain, since selection biases come into play, illustrating the difficulty of balancing these concerns when what constitutes a harm can be contested.\nOpen Question for ML community: How can evaluators estimate ethical harms in ways that allow for potentially conflicting opinions on the presence or magnitude of harms?"}, {"title": "Issue 3: Difficulty of balancing future gains in utility against immediate ethical harms.", "content": "The conceptualization we propose requires balancing expected ethical harm with expected information gain. Even if ethical harm is established (as previously discussed in issues 1 and 2), situations may exist where teams believe it is permissible to ignore potential ethical harms that could occur in evaluation because the information gained through the process could lead to a more socially beneficial downstream ML system. In the absence of attempts to more carefully weigh concerns against each other, it is easy for model developers to engage in wishful thinking that minimizes more direct and immediate ethical harms incurred in evaluation under the guise of more abstract expected long-term benefits.\nOpen Question for ML community: How can ML application industries identify and regulate the consideration of particular ethical harms?"}, {"title": "Issue 4: Difficulties in Comprehensive Risk Assessment in Real-World Environments.", "content": "A relevant challenge posed by the consequentialism framework of ethical decision-making processes is that forecasting future ethical well-being and harms across many hypothetical worlds is difficult (Card and Smith 2020). The expectation of EH(a) has to aggregate ethical harm over expected sources of randomness (e.g., stemming from unknown baseline risks of offensive content in content moderation, or unknown, potentially adversarial user behavior after model deployment). This is intensive for practitioners to think about when making decisions, as they may do what they can to prevent harm and vulnerabilities but still experience unanticipated results.\nOpen Question for ML Community: How can we motivate firms to better assess potential ethical harms arising from real-world interactions between Al systems and users or environments, during the evaluation process?"}, {"title": "Issue 5: Insufficient Resources for Evaluations.", "content": "As discussed above, cost constraints can challenge responsible model development. For example, many generative machine learning models are trained on large, widely available datasets that are believed to be domain-general, then fine-tuned on many small datasets due to the cost of obtaining high-quality, domain-specific data. If sufficient resources aren't devoted to domain-specific testing, the performance observed in an evaluation might appear to be sufficient, but the model might fail dramatically once deployed. Hence, cost constraints can lead to overestimation of the value of information gained. With regulatory or social norms that penalize ethical harms, practitioners can be motivated to devote more resources to investing in ethical evaluation processes to improve systems.\nOpen Question for ML Community: How can we motivate practitioners to devote sufficient resources to evaluations despite the need for resource costs to offset the value of the information gain?"}, {"title": "Issue 6: Impact of evaluations depends on downstream actions.", "content": "Conceiving the value of information gained in an evaluation can be as challenging as forecasting expected ethical harm. This difficulty arises because the information obtained is often instrumental to subsequent decision processes rather than being valuable in isolation. The value of information gained from an evaluation can be conceptualized in several ways. Evaluators may simply be interested in determining whether the estimated performance falls within an acceptable range. If it does not, the information becomes an input into a subsequent decision problem where the team must consider what actions should be taken to improve the model's performance or adjust its deployment context.\nTo expand upon our notation introduced earlier, at the time of designing an evaluation the team can only estimate the information gain given a choice of evaluation a, which we denote IG(a). We denote the realized information gain from a as IG(a). We use T to denote the set of possible actions that can be taken on the model to improve the ML system after the evaluation is completed.\n$t^* = argmax_{t\u2208T}U(t(IG(a))$\nThe evaluation gain can be thought of as the gain in utility from taking post-evaluation $t^*$ compared to taking no action, denoted as $t_0$:\n$EG(a) = E(U(t^* (IG(a)) \u2013 U(t_0))$\nIdeally, $EG(a)$ could be used in the computation of $a^*$ in Equation 1 in lieu of $IG(a)$, as follows:\n$a^* = argmax_{a\u2208A} (E(EG(a)) \u2013 \\sum_jw_jE(EH_j(a)) \u2013 E(cost)$   (2)\nbut as illustrated here, computing EG contains a number of additional challenges (choosing the utility-maximizing t*(\u00b7), which is dependent on the observed IG(a)), and this introduces considerable uncertainty.\nOpen Question for ML Community: How can teams ensure that their evaluation decisions are downstream actionable in the face of considerable uncertainty and additional downstream decision-making?"}, {"title": "Discussion", "content": "The concept of choosing an evaluation to maximize utility, defined as a sum over expected information gain, ethical harm, and resource costs, encapsulates how we might idealize ethical evaluation. However, the challenges we discuss to this framing illustrate selecting a good evaluation design in practice happens under significant uncertainty, and disagreement, around how to anticipate information gain, ethical harm, costs, beyond what constitutes these quantities in the first place.\nAccording to our economic analogy, there is no politically agreed upon optimal social welfare function for aggregating utility across different individuals' preferences. The existence of subjectivity and ethical value judgements are broadly agreed in the economic literature to be inevitable in scientific analysis. Facing this difficulty, analysts typically proceed in the exercise of examining the consequences of various valuation judgments (Samuelson et al. 1983). The decision-makers in a machine learning evaluation practice must also reflect on a range of consequences prior to making final decisions, with the goal of reconciling as much as possible the impacts across a combination of concerns. By discussing the consequences of real-life scenarios where value judgements were problematic and mitigations from analogous domains, accompanied by questions for the evaluation industry to use while reflecting on their options, our conceptualization aims to prompt recognition of complex and nuanced values that arise in evaluation decisions.\nThe status quo approach to evaluation in research prioritizes sharing the results of evaluations. The pervasive sharing of code, data, and results has been called \"frictionless reproducibility\" (Donoho 2024) and used to explain the recent success of machine learning in the world, but a downside is prioritizing the results of evaluations\u2014specifically, the production of point estimates of performance-over richer detail about the evaluation process and how it was selected.\nRecommendations from the model\nOur discussion suggests two broad directions that the software industry could take toward improving decision-making around evaluation trade-offs. First, issues 1 through 3 are likely to benefit from developing external review systems, similar to recommendations made for machine learning auditing. Best practices for external audits recommended by Raji et al. (2020) and Raji et al. (2022) include external oversight boards with data access, accreditation for auditors, and registries of ongoing audits. We echo these recommendations and encourage the evaluation industry to similarly move towards exploring effective external review boards.\nThis would be most useful when considering an evaluation that impacts individuals outside of controlled lab experiments, when participants have not consented to participate in the evaluation. In ML evaluation, concerns regarding impact on non-consenting study participants are typically recorded internally or audited externally ex-post the evaluation practice. Instead, we support the community taking a more proactive stance and moving toward designing third-party review boards to plan evaluation practices.\nAn independent regulatory body that develops comprehensive risk-assessment frameworks for AI technology could also be beneficial, if these frameworks are able to enforce and capture the potential consequences of AI systems in diverse, unpredictable environments. For instance, the European Union's AI Act introduces a risk-based approach to Al regulation, which could inform the development of ethical risk assessment frameworks for AI evaluation (Veale and Zuiderveen Borgesius 2021; Novelli et al. 2024).\nSecondly, we believe that internal decision-making can benefit from further reflection and resources in order to alleviate potential issues 4 through 6. Teams may need to be incentivized to focus more deeply on potential downstream harms (issue 4), adjust their resource allocations toward ethical practices (issue 5), and focus on selecting evaluations that are actionable, linking evaluation outcomes to specific improvement strategies (issue 6).\nOur recommendation is to promote legal or social incentives that encourage corporations to invest in ethical evaluation practices.\nDeveloping a system of governance that dictates approval for evaluations would require working with a wide range of stakeholders beyond practitioners, including legal experts, regulators, and various institutes that currently engage in AI policy. Future work could use case studies to carefully detail evidence of how specific evaluation missteps could have been prevented, and explore options for investigation prior to undergoing the evaluation processes. Practitioners and regulators could be interviewed or surveyed to understand specific weaknesses in their valuation of ethical values. Taken together, further research can allow the ML ethics community to move towards better-planned evaluations."}, {"title": "Alternative conceptualizations", "content": "Our conceptualization of ethical evaluation selection is just one possible framing among many. While we chose it as the most versatile in that it takes as input predicted values of the terms rather than binary information about whether certain thresholds are passed, evaluations in practice may sometimes be better described by alternatives.\nFor example, an alternative conceptualization is to weigh cost explicitly against the other terms. Then, the choice of evaluation is limited to selection within a set of options that are not expected to exceed some maximum allowable cost; i.e., choose a \u2208 Ac where Ac \u2286 A and for all a \u2208 Ac, cost(a) \u2264 max(budget). Another framing is concerned with ensuring that expected harm is below some threshold, $t_e$, denoted $E(EH(a)) < t_e$. This approach, which corresponds to a \"checklist\" of potential ethical implications, corresponds to the approach some AI ethicists observe in industry, albeit with mixed feelings on the formalization of ethics in this way (Ali et al. 2023).\nOur conceptualization emphasizes that evaluation designs are selected under significant uncertainty about the potential value of the information gained and ethical harms and other costs incurred. One issue that arises in practice is a \u201ccold start\" problem, where prior to running any evaluation, a team may feel unprepared to estimate the relevant terms. Addressing the dynamic aspect of evaluation decisions, where some initial evaluation is designed under low information, then subsequent evaluations designed as follow-up conditional on the results, is likely to be important in practice. When no model evaluation has yet been run, teams may benefit from considering similar models, if available, from other applications or described in the research literature. When choosing subsequent evaluations, teams should weigh the expected information gain against the current knowledge state."}, {"title": "Conclusion", "content": "We have discussed potential ethical harms due to AI systems that occur due to decisions made in the evaluation process. To separate and categorize various issues in evaluations, we conceptualize the decision problem faced by practitioners when selecting an evaluation. Our conceptualization frames a primary trade-off between the value of information gained in evaluation and the ethical harms and costs of evaluation incurred. We reference best practices for effective evaluations in analogous domains, as well as recommendations made by the machine learning audit research community, to discuss interventions that could improve ethics of evaluations, such as external reviews or devoting additional resources. Our work contributes to the conversation about the need for the machine learning ethics community to focus on deliberately designing evaluations in the development lifecycle to prevent harm from machine learning systems."}]}