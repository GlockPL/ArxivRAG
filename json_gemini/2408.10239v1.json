{"title": "A Conceptual Framework for Ethical Evaluation of Machine Learning Systems", "authors": ["Neha R. Gupta", "Jessica Hullman", "Hari Subramonyam"], "abstract": "Research in Responsible AI has developed a range of principles and practices to ensure that machine learning systems are used in a manner that is ethical and aligned with human values. However, a critical yet often neglected aspect of ethical ML is the ethical implications that appear when designing evaluations of ML systems. For instance, teams may have to balance a trade-off between highly informative tests to ensure downstream product safety, with potential fairness harms inherent to the implemented testing procedures. We conceptualize ethics-related concerns in standard ML evaluation techniques. Specifically, we present a utility framework, characterizing the key trade-off in ethical evaluation as balancing information gain against potential ethical harms. The framework is then a tool for characterizing challenges teams face, and systematically disentangling competing considerations that teams seek to balance. Differentiating between different types of issues encountered in evaluation allows us to highlight best practices from analogous domains, such as clinical trials and automotive crash testing, which navigate these issues in ways that can offer inspiration to improve evaluation processes in ML. Our analysis underscores the critical need for development teams to deliberately assess and manage ethical complexities that arise during the evaluation of ML systems, and for the industry to move towards designing institutional policies to support ethical evaluations.", "sections": [{"title": "Introduction", "content": "Machine learning (ML) model evaluation typically focuses on estimating errors of prediction or estimation via quantifiable metrics. Given the increasing size and complexity of ML systems, comprehensive evaluations should ideally be multifaceted. For example, evaluations of large ML systems may include several methods, including A/B testing on live populations, adversarial testing to produce undesirable outputs, and comprehensive audits documenting outputs.\nPotential ethical harms of ML systems have gained increasing attention in the broad Responsible AI community. However, even when evaluation metrics are expanded beyond performance to include factors like fairness, privacy loss, or other harms induced by the machine learning system, this is often focused on the ethical harms of the released system, overlooking possible harms incurred during the machine learning development lifecycle itself. This is problematic because evaluation approaches do have the potential to cause ethical harm during evaluation. In a noteworthy example, Tesla's autonomous vehicle live testing systems on real roadways in California, has been widely criticized for being involved in various crashes (Nayak, Laing, and Hull 2022).\nHow should practitioners evaluate large complex systems with potentially unknown ethical harms across the engineering lifecycle, including during the evaluation process? We provide a conceptual framework that casts the primary trade-off in ethical evaluation decision-making as balancing the goal of optimizing for information gained in an evaluation, against the possible ethical harms that are induced.\nBased on our sketch of this fundamental problem that practitioners face, we identify a series of challenges that can cause practitioners to stumble in selecting ethical evaluation practices. We illustrate these challenges using real-world examples of machine learning evaluations that encountered them. Then, we draw parallels between these challenges and evaluation practices in domains other than machine learning, to explore potential mitigation techniques. Together, our conceptual framework and characterization of challenges are intended to stimulate discussion among researchers and evaluation teams on how to balance information gain with potential ethical harm, and to motivate future exploration of policies or best practices for machine learning evaluation."}, {"title": "Related Works", "content": ""}, {"title": "Ethical AI", "content": "A growing body of literature discusses properties that ethical machine learning systems should inherently possess, and provides principles and guidelines for testing (Jobin, Ienca, and Vayena 2019; Zhang et al. 2020; Mart\u00ednez-Fern\u00e1ndez et al. 2022). Broadly speaking, the ethical values identified by prior work include: (1) Non-maleficense, which measures the extent to which the evaluation workflows and outcomes do not inflict harm or injuries on any individual or population (Mehrabi et al. 2021). (2) Privacy, which as a value refers to the principle of protecting personal and sensitive information from unauthorized access, use, or exposure during the entire ML lifecycle (Liu et al. 2021). (3) Fairness, in which the goal is to achieve equitable treat-"}, {"title": "ML System Evaluation Practices", "content": "An evaluation is the process by which practitioners detect differences between desired and actual model behavior (Zhang et al. 2020), through empirical assessment of model properties (Shevlane et al. 2023). A growing body of work creates more comprehensive methods with which to evaluate systems, rather than providing a singular empirical metric or set of metrics. Some evaluation methods can be conducted pre-deployment, such as A/B testing or live testing. Other mechanisms are used post-deployment, such as bug bounty challenges, and provide infrastructure to support stakeholder feedback. A particular evaluation process may involve choosing one or many evaluation metrics to measure. These decisions are critical because they impact actions that are taken post-evaluation to improve system capabilities. They also may be associated with the potential for ethical harm incurred in the evaluation process, or after product release. We define an ethical evaluation as an evaluation that does not sacrifice ethical values in its implementation, and attempts to forecast downstream ethical harms across the product lifecycle.\nFinally, by considering how to conceive of the value of information about model performance gained through an evaluation, our work is related to data valuation. Prior theoretical work in machine learning and related fields studies the value of data for purposes like explainability (e.g., the Shapley framework (Ghorbani and Zou 2019)), data markets and incentivizing collaboration in ML (Castro Fernandez 2023; Sim et al. 2020), and value of accurate or improved prediction for goals like treatment assignment or welfare maximization (Liu et al. 2024; Perdomo 2023)."}, {"title": "Ethical Evaluation Model", "content": ""}, {"title": "Motivation", "content": "The economics discipline has a long history of creating highly simplified models of complex real-world processes to assist with predicting the consequences of actions. Abstracting away non-essential features of the complex real-world permits systematic reasoning.\nFor example, economic policy-makers concerned with pricing wheat might use a simplified model that includes the costs to the farmer while abstracting away other potentially relevant characteristics, such as soil quality and his educational background (Friedman 1953). Our conceptual model of the key trade-off in ML practitioners' evaluation decision-making focuses on the value of information gain relative to ethical harms. However, rather than contributing new theoretical results, our goals are epistemological: to prompt reflection on what it would mean to select the best evaluation in a way that accounts for potential ethical harms induced in evaluation. By conceptualizing the idea of an optimal balance between competing concerns in designing ML evaluations, our framework is meant to highlight difficult questions that largely remain un-navigated in the literature and practice of ML evaluation design, rather than to imply that a normative evaluation design is easy to identify. Below, we discuss the implications of components of the model, including the acceptability of some of the assumptions made for the sake of this model, issues that arise due to differing aims, and the subjectivity of variables in further sections."}, {"title": "Model Properties", "content": "ML teams select from a space of possible evaluations.\nAn evaluation is a protocol for assessing and measuring a model's performance against a set of defined criteria or benchmarks, including specification of which information to collect and how. ML development teams face various considerations when planning evaluations that involve complex decisions across evaluation scope, context, and effect (Zhang et al. 2020; Riccio et al. 2020; Song et al. 2022); prior work has described how practitioners can suffer from a \"paradox of choice\" when it comes to deciding how to perform evaluation (Goel et al. 2021). We represent the space of possible evaluations under consideration by a team as $A = \\{a_1, a_2,...\\}$ where a is an evaluation decision (e.g., evaluation method, metrics, sample selection, etc.).\nchoose some $a \\in A$\nThe utility of an evaluation approach depends on the relative value of information gained, ethical harms, and resource costs. The fair ML literature has represented decisions about model choice in ML in a utility framework, where models provide utility as a function of costs and benefits (Corbett-Davies et al. 2017; Corbett-Davies and Goel 2018; Chohlas-Wood et al. 2021). For example, in Corbett-Davies et al. (2017), the authors conceptualize 'immediate utility' reflecting the costs and benefits of a fair decision by a policymaker in the setting of pre-trial bail release decisions. A utility framing is also used in Hutchinson et al., to illustrate the task of evaluating an ML model's suitability for use in a specific application ecosystem.\nOur conceptualization similarly draws on a utility framework common in statistical decision theory (Savage 1972; Steele and Stef\u00e1nsson 2015; Von Neumann and Morgenstern 2007), but expands this to a broader view applied to decisions made by teams evaluating ML models or systems. The"}, {"title": "Difficulties in comprehensive Risk Assessment in Real-World Environments.", "content": "A relevant challenge posed by the consequentialism framework of ethical decision-making processes is that forecasting future ethical well-being and harms across many hypothetical worlds is difficult (Card and Smith 2020). The expectation of EH(a) has to aggregate ethical harm over expected sources of randomness (e.g., stemming from unknown baseline risks of offensive content in content moderation, or unknown, potentially adversarial user behavior after model deployment). This is intensive for practitioners to think about when making decisions, as they may do what they can to prevent harm and vulnerabilities but still experience unanticipated results."}, {"title": "Issue 5: Insufficient Resources for Evaluations.", "content": "As discussed above, cost constraints can challenge responsible model development. For example, many generative machine learning models are trained on large, widely available datasets that are believed to be domain-general, then fine-tuned on many small datasets due to the cost of obtaining high-quality, domain-specific data. If sufficient resources aren't devoted to domain-specific testing, the performance observed in an evaluation might appear to be sufficient, but the model might fail dramatically once deployed. Hence, cost constraints can lead to overestimation of the value of information gained. With regulatory or social norms that penalize ethical harms, practitioners can be motivated to devote more resources to investing in ethical evaluation processes to improve systems."}, {"title": "Issue 6: Impact of evaluations depends on downstream actions.", "content": "Conceiving the value of information gained in an evaluation can be as challenging as forecasting expected ethical harm. This difficulty arises because the information obtained is often instrumental to subsequent decision processes rather than being valuable in isolation. The value of information gained from an evaluation can be conceptualized in several ways. Evaluators may simply be interested in determining whether the estimated performance falls within an acceptable range. If it does not, the information becomes an input into a subsequent decision problem where the team"}, {"title": "Discussion", "content": "The concept of choosing an evaluation to maximize utility, defined as a sum over expected information gain, ethical harm, and resource costs, encapsulates how we might idealize ethical evaluation. However, the challenges we discuss to this framing illustrate selecting a good evaluation design in practice happens under significant uncertainty, and disagreement, around how to anticipate information gain, ethical harm, costs, beyond what constitutes these quantities in the first place.\nAccording to our economic analogy, there is no politically agreed upon optimal social welfare function for aggregating utility across different individuals' preferences. The existence of subjectivity and ethical value judgements are broadly agreed in the economic literature to be inevitable in scientific analysis. Facing this difficulty, analysts typically proceed in the exercise of examining the consequences of various valuation judgments (Samuelson et al. 1983). The decision-makers in a machine learning evaluation practice must also reflect on a range of consequences prior to making final decisions, with the goal of reconciling as much as possible the impacts across a combination of concerns. By discussing the consequences of real-life scenarios where value judgements were problematic and mitigations from analogous domains, accompanied by questions for the evaluation industry to use while reflecting on their options, our conceptualization aims to prompt recognition of complex and nuanced values that arise in evaluation decisions.\nThe status quo approach to evaluation in research prioritizes sharing the results of evaluations. The pervasive sharing of code, data, and results has been called \"frictionless reproducibility\" (Donoho 2024) and used to explain the recent success of machine learning in the world, but a downside is prioritizing the results of evaluations\u2014specifically, the production of point estimates of performance-over richer detail about the evaluation process and how it was selected. Our work highlights how evaluation choices implicate trade-offs between information gains and potential ethical harms under uncertainty, an under-recognized issue in machine learning development."}, {"title": "Recommendations from the model", "content": "Our discussion suggests two broad directions that the software industry could take toward improving decision-making"}, {"title": "Alternative conceptualizations", "content": "Our conceptualization of ethical evaluation selection is just one possible framing among many. While we chose it as the most versatile in that it takes as input predicted values of the terms rather than binary information about whether certain thresholds are passed, evaluations in practice may sometimes be better described by alternatives.\nFor example, an alternative conceptualization is to weigh cost explicitly against the other terms. Then, the choice of evaluation is limited to selection within a set of options that are not expected to exceed some maximum allowable cost; i.e., choose $a \\in A_c$ where $A_c \\subseteq A$ and for all $a \\in A_c$, cost(a) \u2264 max(budget). Another framing is concerned with ensuring that expected harm is below some threshold, te, denoted E(EH(a)) < te. This approach, which corresponds to a \"checklist\" of potential ethical implications, corresponds to the approach some AI ethicists observe in industry, albeit with mixed feelings on the formalization of ethics in this way (Ali et al. 2023).\nOur conceptualization emphasizes that evaluation designs are selected under significant uncertainty about the potential value of the information gained and ethical harms and other costs incurred. One issue that arises in practice is a \u201ccold start\" problem, where prior to running any evaluation, a team may feel unprepared to estimate the relevant terms. Addressing the dynamic aspect of evaluation decisions, where some initial evaluation is designed under low information, then subsequent evaluations designed as follow-up conditional on the results, is likely to be important in practice. When no model evaluation has yet been run, teams may benefit from considering similar models, if available, from other applications or described in the research literature. When choosing subsequent evaluations, teams should weigh the expected information gain against the current knowledge state."}, {"title": "Conclusion", "content": "We have discussed potential ethical harms due to AI systems that occur due to decisions made in the evaluation process. To separate and categorize various issues in evaluations, we conceptualize the decision problem faced by practitioners when selecting an evaluation. Our conceptualization frames a primary trade-off between the value of information gained in evaluation and the ethical harms and costs of evaluation incurred. We reference best practices for effective evaluations in analogous domains, as well as recommendations made by the machine learning audit research community, to discuss interventions that could improve ethics of evaluations, such as external reviews or devoting additional resources. Our work contributes to the conversation about the need for the machine learning ethics community to focus on deliberately designing evaluations in the development lifecycle to prevent harm from machine learning systems."}]}