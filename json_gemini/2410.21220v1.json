{"title": "Vision Search Assistant: Empower Vision-Language Models as Multimodal Search Engines", "authors": ["Zhixin Zhang", "Yiyuan Zhang", "Xiaohan Ding", "Xiangyu Yue"], "abstract": "Search engines enable the retrieval of unknown information with texts. However, traditional methods fall short when it comes to understanding unfamiliar visual content, such as identifying an object that the model has never seen before. This challenge is particularly pronounced for large vision-language models (VLMs): if the model has not been exposed to the object depicted in an image, it struggles to generate reliable answers to the user's question regarding that image. Moreover, as new objects and events continuously emerge, frequently updating VLMs is impractical due to heavy computational burdens. To address this limitation, we propose Vision Search Assistant, a novel framework that facilitates collaboration between VLMs and web agents. This approach leverages VLMs' visual understanding capabilities and web agents' real-time information access to perform open-world Retrieval-Augmented Generation via the web. By integrating visual and textual representations through this collaboration, the model can provide informed responses even when the image is novel to the system. Extensive experiments conducted on both open-set and closed-set QA benchmarks demonstrate that the Vision Search Assistant significantly outperforms the other models and can be widely applied to existing VLMs.", "sections": [{"title": "1. Introduction", "content": "The advent of Large Language Models (LLMs) [1, 3, 13, 34, 38, 41] has significantly enhanced the human capacity to acquire unfamiliar knowledge through powerful zero-shot Question-Answering (QA) capabilities. Building upon these advancements, techniques such as Retrieval-Augmented Generation (RAG) [39, 42, 47] have further reinforced LLMs in knowledge-intensive, open-domain QA tasks. Concurrently, recent progress in visual instruction tuning [29, 30, 50] has led to the development of large Vision-Language Models (VLMs) that aim to equip LLMS with visual understanding capabilities. By scaling model parameters and training on extensive text-image datasets, VLMs such as LLaVA-1.6-34B [29], Qwen2-VL-72B [5], and InternVL2-76B [11] have achieved state-of-the-art performance on the OpenVLM leaderboard\u00b9. However, LLMs and VLMs are subject to the limitations imposed by their knowledge cutoff dates. They may provide incorrect answers when asked about events or concepts that occurred after their knowledge cutoff dates (Figure 1) To overcome this limitation for LLMs, they are often connected to web agents [4, 10, 14, 32, 33], which enable internet access and information retrieval, allowing them to obtain the most up-to-date data and improve the accuracy of their responses. Such agents are designed to interpret natural language instructions, navigate complex web environments, and extract relevant textual information from HTML documents, thereby enhancing the accessibility and utility of vast amounts of web-based textual data for a wide range of applications.\nHowever, for VLMs facing unseen images and novel concepts, their ability to learn and use up-to-date multimodal knowledge from the internet remains a pressing challenge. As the existing web agents predominantly rely on searching the user's question and summarizing the returned HTML content, they present a notable limitation when handling tasks involving images or other visual content: the visual information is often overlooked or inadequately processed.\nIn this work, we enable the VLM to answer a question regarding an unseen image or novel concept, which behaves like a human searching the Internet. It 1) understands the query, 2) decides which objects in the image it should look at and infers the correlations among the objects, 3) respectively generates the texts to search, 4) analyzes the contents returned from the search engine based on the query and inferred correlations, and 5) judges if the obtained visual and textual information is sufficient for generating the answer"}, {"title": "3. Vision Search Assistant", "content": "The Visual Content Formulation is proposed to extract the object-level descriptions and correlations among objects in an image. Given the input image $X_1$, we first use the open-vocab detector $F_{det}(\\cdot)$ [31] to obtain $N$ regions of interests in the original image,\n$\\begin{equation}\n\\{X_1^{(i)}\\}_{i=1}^{N} = F_{det}(X_1),\n\\end{equation}\nwhere $i$ indicates the i-th region $X_1^{(i)}$ in the image $X_1$. Then we employ the pretrained VLM \u00b2 $F_{vlm}(\\cdot,\\cdot)$ to caption these regions $\\{X_1^{(i)}\\}_{i=1}^{N}$ conditioned on the tokenized user's textual prompt $X_T$, and obtain the visual caption"}, {"title": "3.1. Visual Content Formulation", "content": "X() for the i-th region:\n$\\begin{equation}\nX_V^{(i)} = F_{vlm}(X_1^{(i)}, X_T).\n\\end{equation}\nIn this way, we make the regional captions $\\{X_V^{(i)}\\}_{i=1}^{N}$ contain specific visual information obtained based on the user's interests. To formulate the visual content more comprehensively, we further correlate these visual regions to obtain precise descriptions of the whole image. More specifically, for each region, we concatenate its corresponding caption and the captions of all the other regions. The resultant text, denoted by $[X_V^{(i)}, \\{X_V^{(j)}\\}_{j\\neq i}]$, encodes the underlying correlations. It is fed into the VLM together with the image region $X_1^{(i)}$. The output is referred to as the correlated formulation of each region $\\{X_c^{(i)}\\}_{i=1}^{N}$.\n$\\begin{equation}\nX_c^{(i)} = F_{vlm}(X_1^{(i)}, [X_V^{(i)}, \\{X_V^{(j)}\\}_{j\\neq i}]).\n\\end{equation}\nWe will use the correlated formulations of such regions to perform the following web search."}, {"title": "3.2. Web Knowledge Search: The Chain of Search", "content": "The core of Web Knowledge Search is an iterative algorithm named Chain of Search, which is designed to obtain the comprehensive web knowledge of the correlated formulations $\\{X_c^{(i)}\\}_{i=1}^{N}$. We take an arbitrary i-th region $X_1^{(i)}$ to elaborate on the Chain of Search algorithm and drop the superscript (i) for convenience.\nWe use the LLM in our VLM to generate sub-questions that lead to the final answer, which is referred to as the Planning Agent. The web pages returned from the search engine are analyzed, selected, and summarized by the same LLM, which is referred to as the Searching Agent. In this way, we can obtain web knowledge regarding the visual content. Then, based on each of such sub-questions, the Planning Agent generates more sub-questions, and the Searching Agent obtains web knowledge for the next iteration. Formally, we define a directed graph to represent this process, which is $G = (V, E)$, where $V = \\{V_i\\}$ is the set of nodes, $V_0$ is the initial node, and $E = \\emptyset$ is the set of edges. A node represents a set of known information so that $V_0$ should represent what we know about the region before any web search, i.e., the correlated formulation $X_c$. This is formulated as $V_0\\leftarrow X_c$. When we search with a sub-question, we will update the graph with a new node representing the web knowledge gained through the sub-question.\nFor the 1-st update, we generate sub-questions based on $V_0$ and denote the generated sub-questions by $(\\mathcal{X}) = \\{(\\mathcal{X}_j)\\}_{j=1}^{N'}$, where $N'$ is the number of sub-questions, i.e., the number of new nodes.\nLet $j$ be the index of the sub-question, the new node $\\Delta V_j^{(1)}$ is a child of $V_0$, which corresponds to a search with"}, {"title": "3.3. Collaborative Generation", "content": "We use the original image X1, the user's initial prompt XT, and the Correlated Formulations {X}\u2081 together with the obtained web knowledge {X}\u2081 i=1 to collaboratively generate the final answer Y with the VLM:\n$\\begin{equation}\nY = F_{vlm}(X_1, \\{X_c^{(i)}\\}_{i=1}^{N}, \\{X_W^{(i)}\\}_{i=1}^{N}, X_T).\n\\end{equation}"}, {"title": "4. Experiments", "content": "4.1. Open-Set Evaluation\nSetup. In the Open-Set Evaluation, we performed a comparative assessment by 10 human experts evaluation, which involved questions of 100 image-text pairs collected from the news from July 15th to September 25th covering all fields on both novel images and events. Human experts conducted the evaluations across three critical dimensions: factuality, relevance, and supportiveness.\nResults and Analysis. As illustrated in Figure 5, Vision Search Assistant demonstrated superior performance across all three dimensions compared to Perplexity.ai Pro and GPT-4-Web: 1) Factuality: Vision Search Assistant scored 68%, outperforming Perplexity.ai Pro (14%) and GPT-4-Web (18%). This significant lead indicates that Vision Search Assistant consistently provided more accurate and fact-based answers. 2) Relevance: With a relevance score of 80%, Vision Search Assistant demonstrated a substantial advantage in providing highly pertinent answers. In comparison, Perplexity.ai Pro and GPT-4-Web achieved 11% and 9%, respectively, showing a significant gap in their ability to maintain topicality with the web search. 3) Supportiveness: Vision Search Assistant also outperformed the other models in providing sufficient evidence and justifications for its responses, scoring 63% in supportiveness. Perplexity.ai Pro and GPT-4-Web trailed with scores of 19% and 24%, respectively. These results underscore the superior performance of Vision Search Assistant in open-set tasks, particularly in delivering comprehensive, relevant, and well-supported answers, positioning it as an effective method for handling novel images and events."}, {"title": "4.2. Closed-Set Evaluation", "content": "Setup. We conduct the closed-set evaluation on the LLaVA-W [29] benchmark, which contains 60 questions regarding the Conversation, Detail, and Reasoning abilities of VLMs in the wild. We use the GPT-40(0806) model for evaluation. We use LLaVA-1.6-7B as our baseline model, that has been evaluated in two modes: the standard mode and a \"naive search\" mode that utilizes a simple Google Image search component. Additionally, an enhanced version"}, {"title": "4.3. Ablation Study", "content": "What to search: Object-Level Descriptions. As illustrated in Figure 7, if we use the image-based caption, the search agent can not precisely focus on the key information (the handbag in this figure), meanwhile, the image contains visual redundancy, which obstacles the textual description to drive web agent and retrieve the most relevant web pages, therefore, we use the object-level description in the following ablation study.\nComplex Scenarios of Search: Visual Correlation. We find that the caption can not fully support the search ability in multiple-object scenarios. As shown in Figure 9, the caption of Biden can not answer the questions on the group-wise debate, the visual correlation (\"debate\" in this demo) between Trump can effectively improve the answer quality.\nHow to search: Chain of Search (\u00a7 3.2). The trivial idea to incorporate web search with VLMs is to introduce a Google search engine and re-rank the large-scale related pages. As shown in Figure 8, we found it difficult to directly obtain the required knowledge since the page-rank method prefers more hyper-link pages instead of exact relevance. The VLM is also limited to its context length to summarize a large number of pages. Therefore, we propose the chain of search and enable the progressive summary of web knowledge aiming to answer the user's questions."}, {"title": "5. Conclusion and Discussion", "content": "In this paper, we seek to improve the generalization ability of VLMs of novel images and extend the capacity of web agents to solve visual content tasks. Through the synergistic collaboration between VLMs and web agents, we find that VLMs can generate more reliable answers regarding novel images with the help of real-time web knowledge retrieval, and web agents can solve more challenging tasks than HTML documents only. Meanwhile, there are also some limitations inside the Vision Search Assistant framework such as the exact inference speed of VLMs, the web condition of web agents, and the retrieval efficiency. We hope this paper can inspire more research to address the challenges of VLMs in user experience and improve the automation abilities of web agents across diverse modalities."}]}