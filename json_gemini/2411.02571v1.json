{"title": "MM-EMBED: UNIVERSAL MULTIMODAL RETRIEVAL WITH MULTIMODAL LLMS", "authors": ["Sheng-Chieh Lin", "Chankyu Lee", "Mohammad Shoeybi", "Jimmy Lin", "Bryan Catanzaro", "Wei Ping"], "abstract": "State-of-the-art retrieval models typically address a straightforward search scenario, where retrieval tasks are fixed (e.g., finding a passage to answer a specific question) and only a single modality is supported for both queries and retrieved results. This paper introduces techniques for advancing information retrieval with multimodal large language models (MLLMs), enabling a broader search scenario, termed universal multimodal retrieval, where multiple modalities and diverse retrieval tasks are accommodated. To this end, we first study fine-tuning an MLLM as a bi-encoder retriever on 10 datasets with 16 retrieval tasks. Our empirical results show that the fine-tuned MLLM retriever is capable of understanding challenging queries, composed of both text and image, but underperforms a smaller CLIP retriever in cross-modal retrieval tasks due to modality bias from MLLMs. To address the issue, we propose modality-aware hard negative mining to mitigate the modality bias exhibited by MLLM retrievers. Second, we propose to continually fine-tune the universal multimodal retriever to enhance its text retrieval capability while maintaining multimodal retrieval capability. As a result, our model, MM-Embed, achieves state-of-the-art performance on the multimodal retrieval benchmark M-BEIR, which spans multiple domains and tasks, while also surpassing the state-of-the-art text retrieval model, NV-Embed-v1, on MTEB retrieval benchmark. Finally, we explore to prompt the off-the-shelf MLLMs as the zero-shot rerankers to refine the ranking of the candidates from the multimodal retriever. We find that through prompt-and-reranking, MLLMs can further improve multimodal retrieval when the user queries (e.g., text-image composed queries) are more complex and challenging to understand. These findings also pave the way to advance universal multimodal retrieval in the future. We release the model weights at: https://huggingface.co/nvidia/MM-Embed.", "sections": [{"title": "1 INTRODUCTION", "content": "Information retrieval is crucial for a variety of downstream tasks, such as question answering (Kwiatkowski et al., 2019), fact-checking (Thorne et al., 2018), and retrieval-augmented generation (Lewis et al., 2020). Existing state-of-the-art retrievers often focus on narrow scenarios. For example, LLM-based retrievers (Wang et al., 2023; Lee et al., 2024; Meng et al., 2024; Moreira et al., 2024) are limited to text-to-text retrieval tasks, where both the query and the retrieved results are text-only. Recent work on multimodal retrieval (Zhang et al., 2024; Jiang et al., 2024) focuses on specific tasks and assumes a homogeneous document format. However, in real-world applications, documents and queries often consist of diverse formats or modalities, such as text, images, and in-terleaved text and images. To advance information retrieval and support broader search scenarios, this work explores the use of multimodal LLMs (MLLMs; Dai et al., 2024; Liu et al., 2023a; 2024) for universal multimodal retrieval, accommodating diverse user-instructed tasks with multimodal queries and documents, as illustrated in Figure 1."}, {"title": "2 RELATED WORK", "content": "Instruction-Aware Dense Representation Learning. Asai et al. (2023) is the first work to identify the implicit search intent behind each retrieval task and propose to fine-tune a retriever to learn diverse retrieval tasks with handwritten task instructions. Su et al. (2023) and existing state-of-the-art LLM-based text embedding models (Wang et al., 2023; Meng et al., 2024; Lee et al., 2024) adopt this approach to broader tasks beyond text retrieval, such as text classification and clustering. Recently, Wei et al. (2023) propose a universal multimodal retrieval dataset, M-BEIR, and find that instruction-aware dense retrieval fine-tuning is crucial to tackle universal multimodal retrieval.\nVision-Language Models for Multimodal Retrieval. With the advance of pre-trained vision-language models (Radford et al., 2021; Li et al., 2022), research focus shifts from single-modal (Bajaj et al., 2016; Fu et al., 2023) to cross-modal (Lin et al., 2014; Han et al., 2017; Liu et al., 2021a) or more complex multimodal retrieval tasks (Liu et al., 2021b; Wu et al., 2021; Baldrati et al., 2023). However, the aforementioned tasks assume homogeneous modality for queries and documents, limiting its application. Liu et al. (2023c) take one step further to tackle the retrieval scenario involving candidate pool with heterogeneous modalities but still limit to single retrieval task.\nWei et al. (2023) extend the study to a more general scenario, where retrievers are required to deal with queries, candidate pool in heterogeneous modalities and diverse retrieval tasks. However, the study is limited to CLIP-based retrievers and ignores important text-to-text retrieval tasks, such as fact checking (Thorne et al., 2018) and entity retrieval (Hasibi et al., 2017). While Koukounas et al. (2024) aim to fine-tune a CLIP-based retriever with both strong text-to-text and multimodal retrieval capability, they only consider simple multimodal retrieval tasks: image-caption retrieval (Young et al., 2014; Lin et al., 2014). Concurrent to our work, Jiang et al. (2024) propose to fine-tune MLLMS on NLI dataset (Bowman et al., 2015) and demonstrate their transferability to multimodal retrieval. In this paper, we are the first to study how to fine-tune an MLLM-based universal multimodal retriever while maintaining strong text-to-text retrieval capability. Also, we are the first to explore prompting MLLMs as zero-shot rerankers in diverse multimodal retrieval tasks."}, {"title": "3 UNIVERSAL MULTIMODAL RETRIEVAL", "content": "Following the framework of Lin et al. (2021), we formulate the task of retrieval as follows: given a query q, the goal is to retrieve a ranked list of candidates {C1, C2,\u2026\u2026\u2026Ck} \u2208 C to maximize some ranking metrics, such as nDCG, where C is the collection of documents. In this work, we borrow the setting of universal multimodal retrieval from Wei et al. (2023), where user queries and candidates may consist of a text, image or interleaved text\u2013image; i.e., q \u2208 {qtxt, qimg, (qtxt, qimg)}; c\u2208 {ctxt, cimg, (ctxt, cimg)}. Additionally, there are multiple search intents behind a search query, which can be elaborated by task-specific instructions (Asai et al., 2023). For example, in task 1 and 2 of Figure 1, given the same image as a query, the search intent is to find an image caption and similar image, respectively. Thus, in universal multimodal retrieval, given a multimodal query and task instruction inst, we aim to retrieve a list of candidates from a pool of multimodal documents to maximize a specified ranking metric. Note that we only consider text and image in this work while more modalities, such as audio and video can be included, which we leave for future work."}, {"title": "4 METHOD", "content": "In this section, we describe our approach to universal multimodal retrieval by leveraging multimodal LLMS (MLLMs). In Section 4.1, we first fine-tune an MLLM-based retriever to project multimodal user queries, along with task instructions, into the same semantic space as multimodal documents, enabling k-nearest neighbor search (Johnson et al., 2021). In Section 4.2, we present our method for using MLLMs to rerank the top-k candidates retrieved by the universal multimodal retriever."}, {"title": "4.1 FINE-TUNING MULTIMODAL LLMS FOR UNIVERSAL MULTIMODAL RETRIEVAL", "content": "We fine-tune an MLLM-based retriever parameterized by \u03b8 (i.e., \u03b7\u03b8) under the guidance of task-specific instructions, aiming to capture the implicit intents behind retrieval tasks. Specifically, given a user query qi with the specified task instruction inst\u2081 and its relevant and negative candidates, ct and c, we minimize the InfoNCE loss (Gutmann & Hyv\u00e4rinen, 2010):\nNCE =\n1\nB\n\u2211\ni=1\nlog\nexp (\u03b7\u00ba (insti, qi)\u00b7\u03b7\u00ba(c+)/T)\nc'er exp(no (insti, qi) \u00b7 n\u00ba (c')/T)'\n+where DB = (c\u2020, c\u2081,..., C\u2081/1B1, C\u2081131) includes all the positive and negative documents for all the queries in the mini batch B, n\u00ba(\u00b7) \u2208 Rd is a normalized vector and 7 is the temperature."}, {"title": "4.1.1 MODALITY-AWARE HARD NEGATIVE MINING", "content": "Prior work (Karpukhin et al., 2020; Xiong et al., 2021; de Souza P. Moreira et al., 2024) has demonstrated that hard negative mining significantly improves representation learning for text-to-text retrieval. In the previous retrieval setting, where the corpus consists of documents with a homogeneous modality, a document is considered a hard negative if it lacks the required information but is still retrieved by a model. However, in the scenario of universal multimodal retrieval, where the corpus contains documents involving diverse modalities, the users' desired modality as specified in task instructions (i.e., text, image or interleaved text\u2013image) should be taken into consideration. For example, as shown in Figure 1, the first and second users issue the same query along with different instructions, requiring the documents to be in the format of text and image, respectively. To address this, we propose modality-aware hard negative mining to guide models in retrieving candidates that meet both the users' information needs and their preferred modality.\nSpecifically, we first fine-tune an MLLM-based retriever using random negatives; i.e., DB =(c+,\u2026,). The fine-tuned model is denoted as Mrand. For each query qi and its associated instruction insti in the training set, we generate two types of negatives from the top-50 candidates retrieved by Mrand: i) negatives with incorrect modality (C), where the candidate ranks higher than the labeled positive but has a different modality from the desired one, and ii) negatives with unsatisfactory information (C7), where the candidate ranks lower than k' but has the same desired modality. Note that setting k' to a small number may include false positives while setting k' to large number would make the negative samples too easy. Thus, in our experiment, following the prior work (Chen et al., 2022; Lin et al., 2023), we set k\u2032 = 45. While training, given the query qi with the associated instruction insti, we generate a triplet, ((insti, qi), +, c\u2081), by sampling hard negative c\u2081 from either C or C with the same probability. We denote the models fine-tuned with modality-aware hard negatives as Mhard. We refer readers to Fig. 2 in the Appendix for examples of both types of negative samples."}, {"title": "4.1.2 CONTINUAL TEXT-TO-TEXT RETRIEVAL FINE-TUNING", "content": "Since text-to-text retrieval remains one of the most commonly used retrieval tasks, we further fine-tune Mhard on diverse public text-to-text retrieval tasks, including MS MARCO (Bajaj et al., 2016), HotpotQA (Yang et al., 2018), Natural Question (Kwiatkowski et al., 2019), PAQ (Lewis et al., 2021), StackExchange (Stack-Exchange-Community, 2023), Natural Language Inference (Bowman et al., 2015), SQUAD (Rajpurkar et al., 2016), ArguAna (Wachsmuth et al., 2018), BioASQ (Nentidis et al., 2023), FiQA (Maia et al., 2018), and FEVER (Thorne et al., 2018). As these datasets do not contain negative samples, we employ the fine-tuned LLM-based retriever (NV-Embed-v1; Lee et al., 2024) to mine hard negatives in our experiments (see de Souza P. Moreira et al. (2024) for details).\nDuring the continual fine-tuning stage, we uniformly sample triplets from both the universal multimodal and text-to-text retrieval training data. Note that for each query qi in universal multimodal retrieval training data, we use Mhard to mine the second-type hard negatives C again. Since no first-type hard negatives (i.e., C = (0) are mined by Mhard, we retain the first-type hard negative mined by Mrand."}, {"title": "4.2 PROMPTING MULTIMODAL LLMS FOR RERANKING", "content": "Prior work (Sun et al., 2023; Jin et al., 2024) has demonstrated that instruction fine-tuned LLMs can be prompted to rerank candidates in text-to-text retrieval tasks. In this work, we prompt"}, {"title": "5 EXPERIMENTS", "content": "5.1 DATASETS AND MODELS\nMultimodal Retrieval Dataset. We evaluate models' universal multimodal retrieval capability using M-BEIR dataset (Wei et al., 2023), which is constructed from 10 datasets with 16 diverse multimodal retrieval tasks across 4 domains listed in Table 8 (in the Appendix).\u00b9 We train our models on the M-BEIR 1.1M training queries and evaluate models' effectiveness on the 190K test queries. Following the global evaluation setting of M-BEIR dataset, for each query, candidates are retrieved from a merged candidate pool of 5.6M multimodal documents spanning all 10 datasets. We report the averaged Recall@5 (R@5) as retrieval accuracy across all test queries in each dataset, except for Fashion200K and FashionIQ, where we report Recall@10 (R@10). We refer readers to Wei et al. (2023) for more details on the construction of M-BEIR dataset.\nText-to-Text Retrieval Dataset. While M-BEIR contains WebQA dataset for text-to-text retrieval evaluation, we conduct a more comprehensive text-to-text retrieval evaluation using MTEB dataset (Muennighoff et al., 2023). Specifically, we evaluate our models on 15 diverse text retrieval datasets.\u00b2 Following the established procedure, we report the averaged nDCG@10 across the 15 text retrieval datasets. Note that unlike in M-BEIR, where candidates are retrieved from a merged pool across all tasks, in the MTEB retrieval tasks, we retrieve candidates from separate corpora for each task.\nBackbone Model Choices. In this work, we utilize two representative backbones of vision-language models to build universal multimodal retrievers, CLIP (Radford et al., 2021) and LLaVa-Next (Liu et al., 2024). For CLIP, we initialize from CLIP-large model and employ the best-performing modeling approach from Wei et al. (2023), denoted as CLIPSF.3 This method fuses input image and text features by separately encoding each input (query or document) image and text into separate vectors, which are then summed to create a fused vector (Liu et al., 2023c).\nLLaVa-Next (Liu et al., 2024) is a multimodal LLM (MLLM), which integrates a CLIP image encoder, LLM and a vision-language MLP projector to align image features to the input embedding space of the LLM. We use LLaVa-Next with Mistral 7B (Jiang et al., 2023) as the backbone LLM.4 We experiment with three variants: (1) LLaVa-E: the token embedding is used to aggregate information from the multimodal input, a method commonly employed in prior work for text retrieval (Wang et al., 2023; Ma et al., 2024b); (2) LLaVa-P: the MLLM is prompted to summarize"}, {"title": "5.2 MAIN RESULTS", "content": "Universal Multimodal Retrieval. Table 1 reports the retrieval accuracy of different retrievers. In M-BEIR evaluation, we observe that when fine-tuning with random negatives, LLaVa-P achieves the highest overall retrieval effectiveness. This result indicates that LLaVa-P effectively aggregates multimodal input information into a single word representation. While MLLM-based retrievers outperform CLIPSF on tasks involving multi-modal queries, they still lag behind CLIPSF on tasks with single-modal queries, especially in cross-modality retrieval; i.e., tasks 1 and 4. In addition, NV-Embed-v1 reaches the best text-to-text retrieval accuracy on WebQA task2.\nObserving from the models fine-tuned with hard negatives, MLLM-based retrievers show significant retrieval accuracy improvements, particularly in tasks involving single-modal queries. On the other hand, CLIPSF does not show similar improvement. This could attribute to the fact that CLIP has been well pre-trained for cross-modal retrieval whereas MLLM-based retrievers, fine-tuned with contrastive learning objective for only 2 epochs, may still be underfitting. Fine-tuning with hard negatives accelerates contrastive learning of MLLM-based retrievers.\nTable 2 reveals another factor contributing to the lower retrieval accuracy of MLLM-based retrievers for single-modal queries: text retrieval bias. This issue is particularly obvious for NV-Embed-v1. We compare models' retrieval accuracy on text-image and image-text retrieval (tasks 1 and 4) on MSCOCO. The comparison shows that Mrand(LLaVa-E) and Mrand(NV-Embed-v1) exhibit significant lower modality accuracy (M.A.@1) than Mrand(CLIPSF) in the text-to-image retrieval task. Most erroneous top-1 retrieved candidates from the MLLM-based retrievers are relevant texts rather than images (see Figure 2 in the Appendix). This result indicates that MLLM-based retrievers have a bias toward relevant text rather than images. This issue can be mitigated by our proposed modality-aware hard negative mining.\nFinally, we observe that Mhard(NV-Embed-v1) outperforms Mhard(LLaVa-P) in text-to-text retrieval tasks (i.e., WebQA task 2 and MTEB); however, compared to the original NV-Embed-v1 (Lee et al., 2024), the score on MTEB retrieval tasks drops almost 10 points. After continual fine-tuning (detailed in Section 4.1.2), the final model, MM-Embed, not only surpasses NV-Embed-v1 in MTEB but also maintains strong multimodal retrieval capability. We attribute the improvement in text-to-text retrieval to the effective hard negatives mined by NV-Embed-v1 aforementioned in Section 4.1.2. Notably, continual fine-tuning significantly enhances multimodal retrieval performance in InfoSeek (col 8 vs 7 in Table 1), highlighting its effectiveness in improving the model's ability to handle knowledge-intensive multimodal retrieval tasks."}, {"title": "5.3 ABLATION STUDIES", "content": "5.3.1 Is FINE-TUNING WITH INSTRUCTION NECESSARY?\nWe fine-tune NV-Embed-v1 with random negatives on the M-BEIR subtasks listed in Table 5 and evaluate models' retrieval accuracy on the development queries from each subtask. Note that, for simplicity, we encode only the corpus specific to each dataset, containing documents of the targeted modality. For example, when evaluating retrieval accuracy for VisualNews task 1, we encode the 542K images from VisualNews (see Table 8 in the Appendix) as the index rather than the entire 5.6M documents from M-BEIR. We also report CLIP and LLaVa-P (w/o instruction) zero-shot retrieval effectiveness as a reference point.9\nFrom Table 5, we observe that NV-Embed-v1, as a zero-shot MLLM-based retriever, outperforms LLaVa-P and even competes CLIP in the tasks in Miscellaneous domain (i.e., MSCOCO and NIGHTS). This result indicates that a fine-tuned MLLM-based text retriever is capable to perform multimodal retrieval tasks (same finding in (Jiang et al., 2024)). Although incorporating task in-structions with queries degrades the retrieval effectiveness (col 4 vs 3), the model fine-tuned with instructions significantly outperforms the one fine-tuned without instructions (col 6 vs 5). This indicates that task instructions can help elicit models' task- or domain-specific knowledge for diverse multimodal retrieval tasks.\n5.3.2 EFFECTIVENESS OF CONTINUAL TEXT-TO-TEXT RETRIEVAL FINE-TUNING\nIn this section, we study the best strategy to enhance models' capabilities in both multimodal and text-to-text retrieval. We begin by fine-tuning NV-Embed-v1 on both training data for universal multimodal retrieval and text-to-text retrieval (detailed in Section 4.1.2) for 2K steps. As shown in Table 6, joint fine-tuning for both tasks allows the model to maintain its text retrieval capability (row 3 vs 1), although it results in a drop of over 2 points in multimodal retrieval accuracy (row 3 vs 2). In contrast, consciously fine-tuning Mhard(NV-Embed-v1) for addition 2K steps significantly boosts its text-to-text retrieval capability with a slight drop of 0.8 points in multimodal retrieval (row 5 vs 4).10 This experiment shows that continuously fine-tuning a multimodal retriever to enhance its text-to-text retrieval is more effective than fine-tuning a retriever on all the retrieval tasks simultaneously. This finding suggests that a more optimized curriculum learning strategy (Bengio et al., 2009) could further improve performance in universal multimodal retrieval, a direction we leave for future work."}, {"title": "5.3.3 STUDY ON PROMPTING MLLMS FOR RERANKING", "content": "In this section, we study the reranking effectiveness of MLLMs on all the tasks in M-BEIR dataset. Specifically, for each development query, we rerank the top-10 retrieved candidates from Mrand(CLIPSF). As shown in Table 7, prompting LLaVa-Next for reranking further boosts the ranking accuracy in tasks 6\u20138, which involve multimodal queries (except for FashionIQ). However, the reranking degrades accuracy in tasks 1-5 which involve single-modal queries (except for WebQA task 2). This trend persists even after scaling the reranker from 7B to 34B (col 3, 2 vs 1).\u00b9\u00b9 We hypothesize that it is challenging for bi-encoder models to encode multimodal queries, such as visual question answering and composed image retrieval. Prompting an MLLM as a reranker in a zero-shot or few-shot manner, or distilling the reranked results into a bi-encoder retriever is a promising solution."}, {"title": "6 CONCLUSION AND FUTURE WORK", "content": "In this paper, we present techniques for advancing information retrieval with multimodal large language models (MLLMs). We first study fine-tuning MLLM-based retrievers to tackle a general information retrieval scenario: universal multimodal retrieval, where models are required to deal with diverse retrieval tasks, multimodal queries and documents. Our study shows that MLLM-based retrievers exhibit modality bias in cross-modal retrieval tasks compared to CLIP-based retrievers. To address the issue, we propose modality-aware hard negative mining, which significantly improves our MLLM-based retrievers' accuracy by 5 points in M-BEIR dataset, a benchmark for universal multimodal retrieval. Additionally, with our proposed continual fine-tuning, our MLLM-based retriever, MM-Embed, is the first model to yield state-of-the-art retrieval accuracy in universal multimodal retrieval tasks while maintaining strong text-to-text retrieval capability (ranked top-5 on MTEB retrieval task leaderboard). Finally, we explore to prompt MLLMs as reranker in M-BEIR tasks. We find that MLLMs can be used as zero-shot rerankers to further boost retrieval accuracy in the challenging tasks, which require the understanding of multimodal queries, such as visual question answering and composed image retrieval. For example, our zero-shot MLLM-based reranker improves the retrieval accuracy upon the state-of-the-art retrievers by over 7 points in CIRCO."}, {"title": "A APPENDIX", "content": "A.1 IMPLEMENTATION DETAILS\nWe implement our training and inference using Tevatron (Gao et al., 2023). For CLIP-based retriev-ers, we follow all the settings from Wei et al. (2023). For MLLM-based retriever, we fine-tunemodels with DeepSpeed Zero 2 (Rajbhandari et al., 2020) and gradient checkpointing. Duringfine-tuning on M-BEIR training data, we set maximum length for queries and documents to 128.While continual fine-tuning on both M-BEIR and text-to-text retrieval training data, we set maxi-mum length for queries and documents to 128 and 512, respectively. All fine-tuning are conductedon 8\u00d780GB A100 GPUs. Note that image input only occupies single token length after being to-kenized; however, each image will be converted to multiple image tokens. Thus, the actual inputlength to MLLM is longer than the maximum length we set. To speed fine-tuning and inference forMLLM-based retrievers, we only use the global image patches, which occupy 576 (24\u00d724) imagetokens.\nA.2 BASELINE REPRODUCING\nSince we implement our fine-tuningand inference following the settingfrom Wei et al. (2023), our fine-tunedMrand(CLIPSF) should be equal toCLIPSFfrom Wei et al. (2023). In Ta-ble 10, we compare the results fromour fine-tuned Mrand(CLIPSF) and thecheckpoint provided by the authors. 12"}]}