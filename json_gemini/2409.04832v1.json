{"title": "Reward-Directed Score-Based Diffusion Models via q-Learning", "authors": ["Xuefeng Gao", "Jiale Zha", "Xun Yu Zhou"], "abstract": "We propose a new reinforcement learning (RL) formulation for training continuous-time score-based diffusion models for generative AI to generate samples that maximize reward functions while keeping the generated distributions close to the unknown target data distributions. Different from most existing studies, our formulation does not involve any pretrained model for the unknown score functions of the noise-perturbed data distributions. We present an entropy-regularized continuous-time RL problem and show that the optimal stochastic policy has a Gaussian distribution with a known covariance matrix. Based on this result, we parameterize the mean of Gaussian policies and develop an actor-critic type (little) q-learning algorithm to solve the RL problem. A key ingredient in our algorithm design is to obtain noisy observations from the unknown score function via a ratio estimator. Numerically, we show the effectiveness of our approach by comparing its performance with two state-of-the-art RL methods that fine-tune pretrained models. Finally, we discuss extensions of our RL formulation to probability flow ODE implementation of diffusion models and to conditional diffusion models.", "sections": [{"title": "Introduction", "content": "Diffusion models form a powerful family of probabilistic generative AI models that can capture complex high-dimensional data distributions (Sohl-Dickstein et al. 2015a, Song and Ermon 2019, Ho et al. 2020, Song et al. 2021). The basic idea is to use a forward process to gradually turn the (unknown) target data distribution to a simple noise distribution, and then reverse this process to generate new samples. A key technical barrier is that the time-reversed backward process involves a so-called score function that depends on the unknown data distribution; thus learning the score functions (called \u201cscore matching\") becomes the main objective of these models. Diffusion models have achieved state-of-the-art performances in various applications such as image and audio generations (Rombach et al. 2022, Ramesh et al. 2022) and molecule generation (Hoogeboom et al. 2022, Wu et al. 2022). See, e.g., Yang et al. (2023) for a survey on diffusion models.\nIn standard diffusion models, the goal is typically to generate new samples whose distribution closely resembles the target data distribution (e.g. \"generate more cat pictures\", or \"write a Shakespearean play\"). Standard score-based diffusion models are trained (i.e. estimating score functions using neural nets) by minimizing weighted combination of score matching losses (Hyv\u00e4rinen and Dayan 2005, Vincent 2011, Song et al. 2020). However, in many applications, we often have preferences about the generated samples from diffusion models (e.g. \"generate prettier cat pictures\", or \"write a Shakespearean thriller that happened in New York\"). A common way to capture this is to use a reward function, either handcrafted (e.g. a utility function) or learned (through human feedbacks), to evaluate the quality of generated samples related to the preferences. This has led to an interesting recent line of research on how to adapt standard diffusion models to optimizing reward functions. Several approaches have been proposed, including discrete-time reinforcement learning (RL) (Black et al. 2024, Fan et al. 2023), continuous-time stochastic control/RL (Uehara et al. 2024, Zhao et al. 2024), backpropagation of reward function gradient through sampling (Clark et al. 2024), supervised learning (Lee et al. 2023), and guidance (Dhariwal and Nichol 2021). These studies, however, focus on fine-tuning certain pretrained diffusion models, whose score functions have already be learned, to maximize the values of additional reward functions.\nIn this paper, we put forward a significantly different approach where we directly train a diffusion model from scratch for reward optimization using RL, without involving any pretrained model. There are two motivations behind this approach. One is practical: in many applications, especially for new or special-purpose tasks, a pretrained model may not exist or existing pretrained models may not be easily adapted and fine-tuned. The other is (more importantly) conceptual: fine-tuning a pretrained model is essentially a model-driven approach for dealing with the unknown score function (i.e. first estimate the score function and then optimize), which is prone to model misspecification and misleading risks.\u00b9 By contrast, our approach is model-free and data-driven, not relying on a good pretrained model. More on this later.\nWe consider continuous-time score-based diffusion models in Song et al. (2021), which utilize stochastic differential equations (SDEs) for noise blurring and sample generations. The denoising/reverse process also follows an SDE whose drift includes the unknown score function (the gradient of the log probability density of the noise-perturbed data distribution). We take the continuous-time framework because a) the SDE-based formulation is general and unifies several celebrated diffusion models, including the score matching with Langevin dynamics (SMLD) (Song and Ermon 2019) and the denoising diffusion probabilistic modeling (DDPM) (Ho et al. 2020); b) it leads to not only the SDE-based implementation of score diffusions, but also the probability flow ordinary difference equation (ODE) implementation (Song et al. 2021); and c) there are more analytical tools available for the continuous setting that enable a rigorous and thorough analysis leading to interpretable (instead of black-box) and general (instead of ad hoc) algorithms.\nDue to the need of optimizing rewards of the generated samples, we propose a continuous-time RL formulation for adapting diffusion models to reward functions. In this formulation, because the score function in the drift of the denoising process is unknown, we take it as the control (action) variable. This idea of regarding scores as actions, first put forth in Uehara et al. (2024), Zhao et al. (2024), is quite natural because the problem now has two somewhat competing criteria: score matching (i.e. the generated samples should be close to the true distribution) and terminal"}, {"title": "Quick Review on Continuous-Time Score-Based Diffusion Models", "content": "For reader's convenience we briefly recall the continuous-time score-based diffusion models with SDEs (Song et al. 2021). Denote by $p_0 \\in \\mathcal{P}(\\mathbb{R}^d)$ the unknown continuous data distribution, where $\\mathcal{P}(\\mathbb{R}^d)$ is the space of all probability measures on $\\mathbb{R}^d$. Given i.i.d samples from $p_0$, standard diffusion models aim to generate new samples whose distribution closely resembles the data distribution. In this classical setting, one does not consider the preference/reward of the generated samples.\n\u2022 Forward process and reverse process.\nFix $T > 0$. We consider a $d$-dimensional forward process $(x_t)_{t\\in[0,T]}$, which is a Ornstein\u2014Uhlenbeck (OU) process satisfying the following SDE\n$dx_t = f(t)x_t dt + g(t)dB_t,$ (1)\nwhere $x_0$ is a random variable following the (unknown target) distribution $p_0$, $(B_t)$ is a standard $d$-dimensional Brownian motion which is independent of $x_0$, and both $f(t) > 0$ and $g(t) > 0$ are (known) scalar-valued continuous functions of time $t$. The solution to (1) is\n$x_t = e^{-\\int_0^t f(s)ds} x_0 + \\int_0^t e^{-\\int_s^t f(v)dv}g(s)dB_s, t \\in [0,T]$. (2)\nNote that the forward model (1) is fairly general and it includes variance exploding SDES, variance preserving SDEs, and sub variance preserving SDEs that are commonly used in the literature; see Song et al. (2021) for details.\nDenote by $p_t(\\cdot)$ the probability density function of $x_t$, and $p_{t|0}(\\cdot|x_0)$ the density function of $x_t$ given $x_0$ for $t \\in [0,T]$. By (2), $p_{t|0}(x_t|x_0)$ is Gaussian which has an analytical form.\nNow consider the reverse (in time) process $(x_t)_{t\\in [0,T]}$, where\n$\\tilde{x}_t := x_{T-t}, t\\in [0,T]$. (3)\nUnder mild assumptions, the reverse process still satisfies an SDE (Anderson 1982, Haussmann and Pardoux 1986, Cattiaux et al. 2023):\n$d\\tilde{x}_t = [f(T - t)\\tilde{x}_t + (g(T - t))^2 \\nabla_x log p_{T-t}(\\tilde{x}_t)]dt + g(T - t)dW_t, t\\in [0,T],$ (4)\nwhere $(W_t)$ is a standard Brownian motion in $\\mathbb{R}^d$, and the term $\\nabla_x log p_t(\\cdot)$ in (4) is called the score function. By (3), the reverse process starts from a random location $\\tilde{x}_0 \\sim p_T$, where $p_T$ is the probability density/distribution of $x_T$ (here and henceforth, probability distribution and probability density function are used interchangeably). Thus, at time $T$, we have $\\tilde{x}_T \\sim p_0$, where $p_0$ is the target distribution we want to generate samples from. However, the distribution $p_T$ is unknown because it depends on the unknown target distribution $p_0$. Because\n\u2022 Training diffusion models via score matching.\nThe score function $\\nabla_x log p_t(\\cdot)$ in (6) is unknown because the data distribution $p_0$ is unknown. One can estimate it with a time-state score model $s_\\theta(\\cdot,\\cdot)$, which is often a deep neural network parameterized by $\\theta$, by minimizing the score matching loss:\n$\\min_\\theta E_{t\\sim U[0,T]} [\\lambda(t) E_{x_0} E_{x_t|x_0} ||s_\\theta(t, x_t) - \\nabla_{x_t} log p_t(x_t)||^2]$. (7)\nHere, $\\lambda(\\cdot) : [0, T] \\rightarrow \\mathbb{R}_{>0}$ is some positive weighting function (e.g. $\\lambda(t) = g(t)^2$), and $U[0,T]$ is the uniform distribution on $[0,T]$. This objective is intractable because $\\nabla_{x}log p_t(\\cdot)$ is unknown. Several approaches have been developed in literature to tackle this issue, including denoising score matching (Vincent 2011, Song et al. 2021), sliced score matching (Song et al. 2020) and implicit score matching (Hyv\u00e4rinen and Dayan 2005). Here, we take denoising score matching for illustration. One can show that (7) is equivalent to the following objective\n$\\min_\\theta E_{t\\sim U[0,T]} [\\lambda(t) E_{x_0} E_{x_t|x_0} ||s_\\theta(t, x_t) - \\nabla_{x_t} log p_{t|0}(x_t|x_0)||^2]$, (8)\nwhere $x_0 \\sim p_0$ is the data distribution, and $p_{t|0}(\\cdot|x_0)$ is the density of $x_t$ given $x_0$, which is Gaussian by (2). Because we have access to i.i.d. samples from $p_0$ (i.e. training data), the objective in (8) can be approximated by Monte Carlo, and the resulting loss function can be then optimized using e.g. stochastic gradient descent."}, {"title": "Problem Formulation", "content": "Standard training of diffusion models via score matching (8) \u2013 called pretrained models \u2013 does not consider preferences about the generated samples. A standard way to capture preferences is to add a reward function that evaluates the quality of generated samples related to the specified preferences. This is essentially a model-based approach - first learn a pre-trained model by estimating the score function and then optimize. This paper takes a conceptually different approach, one that is in the spirit of RL, namely, to learn optimal policies directly without attempting to first learn a model. This leads naturally to a continuous-time RL formulation."}, {"title": "Reward-directed diffusion models", "content": "The key idea is that because the score term $\\nabla log p_{T-t}(z_t)$ in (6) is unknown, we regard it as a control. With a reward function, this leads to the following stochastic control problem:\n$\\max_{\\alpha=(\\alpha_t: 0 \\leq t \\leq T)} \\{\\beta \\cdot E[h(y_T)] - E\\big[ \\int_0^T (g(T-t))^2 \\cdot |\\nabla log p_{T-t}(y_t) - \\alpha_t|^2 dt \\big] \\}$ (9)\nsubject to\n$dy_t = [f(T-t)y_t + (g(T - t))^2\\alpha_t] dt + g(T-t)dW_t, y_0 \\sim \\nu.$ (10)\nHere, $\\alpha_t \\in \\mathbb{R}^d$ denotes the control action at time $t$, $(y_t)$ is the controlled state process, $h$ is the terminal reward function for the generated sample $y_T$, and $\\beta \\geq 0$ is a weighting coefficient.\nThe first term in the objective function (9) captures the preference on the generated samples. In this study, we do not require the reward/utility function $h$ to be differentiable as in Clark et al. (2024), nor do we necessarily assume that its functional form is known. What we do assume is that given a generated sample $y_T$ we can obtain a noisy observation (\u201ca reward signal\") of the reward $h$ evaluated at that sample (e.g. a human evaluation of the aesthetic quality of an image).\nThe second term in the objective (9) has the following interpretation. Consider a deterministic feedback policy so that $\\alpha_t = w(t, y_t)$ for some deterministic function $w$. Let $\\mathbb{P}^w$ and $\\mathbb{P}^\\alpha$ be the induced distribution (i.e., path measures over $\\mathcal{C}([0, T], \\mathbb{R}^d)$) by the SDEs (6) and (10), respectively. Then Girsanov's theorem gives that under some regularity conditions (see, e.g. Uehara et al. 2024, Appendix C or Tang 2024, Proposition 3.3)\n$KL(\\mathbb{P}^w ||\\mathbb{P}^\\alpha) = \\frac{1}{2} E\\big[ \\int_0^T (g(T - t))^2 \\cdot |\\nabla log p_{T-t}(y_t) - w(t, y_t)|^2 dt \\big]$\n$= \\frac{1}{2} E\\big[ \\int_0^T (g(T - t))^2 \\cdot |\\nabla log p_{T-t}(y_t) - \\alpha_t|^2 dt \\big]$ (11)\nwhere the expectation is taken with respect to $(y^\\alpha)$ where $y_0 \\sim \\nu$. This justifies the second term of (9) in terms of KL-divergence.\nDenote\n$r(t, y, \\alpha) := -(g(T \u2013 t))^2 \\cdot |\\nabla log p_{T-t}(y) \u2013 \\alpha|^2,$ (13)\nthe running reward function (or instantaneous reward) at time $t$ in the objective (9). Because the true score function $\\nabla log p_{T\u2212t}(\\cdot)$ is unknown, this running/instantaneous reward function is also unknown. Moreover, recall that the true terminal reward function $h$ is also generally unknown. Hence, we need an RL formulation of the problem (9)-(10), where exploration is necessary due to these unknown rewards. This is discussed in the next subsection."}, {"title": "Stochastic policies and exploratory formulation", "content": "We now adapt the exploratory formulation for continuous-time RL in Jia and Zhou (2023) to our problem setting. A distinctive feature of our problem is that the RL agent knows the environment (the functions $f, g$ are known in the SDE model (10)), but she does not know the instantaneous reward function $r$ and the terminal reward function $h$. So she still needs to do \"trial and error\" to try a strategically designed sequence of actions, observe the corresponding state process and a stream of running rewards and terminal reward samples/signals, and continuously update and improve her action plans based on these observations. For this purpose, the agent employs stochastic policies in our RL setting.\nMathematically, let $\\pi : (t, y) \\in [0,T] \\times \\mathbb{R}^d \\rightarrow \\pi(\\cdot|t, y) \\in \\mathcal{P}(\\mathbb{R}^d)$ be a given stochastic feedback policy, where $\\mathcal{P}(\\mathbb{R}^d)$ is a collection of probability density functions defined on $\\mathbb{R}^d$. Assume that the probability space is rich enough to support $\\{Z_t, 0 \\leq t \\leq T\\}$, a process of mutually independent copies of a random variable uniformly distributed over $[0, 1]$, which is independent of the Brownian motion in (10). Let $\\mathcal{G}_s = \\mathcal{F}^W \\vee \\sigma(Z_t, 0 \\leq t \\leq s) \\vee \\sigma(\\nu)$ be the sigma-algebra at times that"}, {"title": "Theory", "content": "In this section, we present our main theoretical results, which provide the optimal stochastic policy to the RL problem in (16). To this end, we first recall the system dynamics (10) and the running reward (13). We then introduce the (generalized) Hamiltonian $H : [0, T] \\times \\mathbb{R}^d \\times \\mathbb{R}^d \\times \\mathbb{R}^d \\times \\mathbb{R}^{d \\times d} \\rightarrow \\mathbb{R}$ associated with (9)-(10) (see e.g. Yong and Zhou 2012):\n$H(t, y, \\alpha, p,q) = -(g(T - t))^2 |\\nabla log p_{T-t}(y) \u2013 \\alpha|^2 + [f(T - t)y + (g(T-t))^2\\alpha] \\cdot p + \\frac{1}{2} (g(T - t))^2 \\cdot \\theta q.$ (20)\nEquation (13) of Jia and Zhou (2023) yields that the optimal stochastic policy $\\pi^*(\\cdot|t, y)$ for the problem (16) is given by\n$\\pi^*(\\alpha|t, y) \\propto exp \\big{ H(t, y, \\alpha, J_y^*(t, y), J_{yy}^*(t, y)) \\big}$.\nHowever, $H$ in our case, (20), is quadratic in $\\alpha$, leading to the following result.\nProposition 1. The optimal stochastic policy $\\pi^*(\\cdot|t,y)$ is a Gaussian distribution in $\\mathbb{R}^d$:\n$\\pi^*(\\cdot|t, y) \\sim \\mathcal{N} \\big( \\mu^*(t, y), \\frac{\\theta}{2g^2(T - t)} \\cdot I_d \\big);$ (21)\nwhere\n$\\mu^*(t, y) = \\nabla log p_{T\u2212t}(y) + \\frac{1}{\\theta} J_y^*(t, y).$ (22)\nThis result provides some interesting insights. First, the mean $\\mu^*(t, y)$ of the optimal stochastic policy consists of two parts: the score $\\nabla log p_{T-t}(y)$ that we try to match in (9), and an additional term $\\frac{1}{\\theta} J_y^*(t, y)$ that arises due to the consideration of maximizing the terminal reward $h$ of the generated samples. The optimal value function $J^*(t, y)$ in (16) can be shown to satisfy the following HJB equation, a nonlinear PDE (see Equation (14) of Jia and Zhou 2023):\n$\\frac{\\partial J^*}{\\partial t} (t, y) + \\theta log \\int_{\\mathbb{R}^d} exp \\big( \\frac{1}{\\theta} H(t, y, \\alpha, J_y^*, J_{yy}^*)) d\\alpha \\big] = 0,$ (23)\n$J^*(T, y) = \\beta \\cdot h(y).$ (24)\nInspired by Proposition 1, in our RL algorithms to be presented in Section 5, we only need to consider Gaussian policies in the following form:\n$\\pi^\\psi (\\cdot|t, y) \\sim \\mathcal{N} \\big( \\mu^\\psi(t, y), \\frac{\\theta}{2g^2(T - t)} \\cdot I_d \\big)$ for all $(t, y),$ (25)\nwhere the mean $\\mu^\\psi(t, y)$ is parameterised by some vector $\\psi$."}, {"title": "q-Learning Algorithm", "content": "In this section, we present an algorithm to solve our continuous-time RL problem (16). In particular, we adapt the q-learning algorithms developed recently in Jia and Zhou (2023), which are of actor-critic type. For reader's convenience, we first introduce some definitions and theoretical result in Jia and Zhou (2023) that are important for developing our algorithm.\nFollowing Jia and Zhou (2023), we define the so-called optimal q-function by\n$q^*(t, y, \\alpha) = \\frac{\\partial J^*(t, y)}{\\partial t} + H(t, y, \\alpha, J_y^*(t, y), J_{yy}^*(t, y)), (t, y, \\alpha) \\in [0, T] \\times \\mathbb{R}^d \\times \\mathbb{R}^d,$ \nwhere $J^*$ is the optimal value function given in (16), and the Hamiltonian function $H$ is defined in (20). One can readily infer from (23) that (see Proposition 8 in Jia and Zhou 2023)\n$\\int_{\\mathbb{R}^d} exp \\big( \\frac{1}{\\theta} q^*(t, y, \\alpha) \\big) d\\alpha = 1, for all (t, x) \\in [0,T] \\times \\mathbb{R}^d,$\nand the optimal stochastic policy $\\pi^*(\\cdot|t, y)$ in Proposition 1 is simply\n$\\pi^*(\\alpha|t,y) = exp \\big( \\frac{1}{\\theta} q^*(t, y, \\alpha) \\big)$.\nWe next state the martingale condition that characterizes the optimal value function $J^*$ and the optimal q-function; see Theorem 9 in Jia and Zhou (2023). This result is essential because it provides the theoretical foundation for designing our q-learning algorithm.\nProposition 2. Let a function $\\widehat{J}^* \\in C^{1,2}([0, T) \\times \\mathbb{R}_+) \\cap C([0, T] \\times \\mathbb{R}_+)$ and a continuous function $\\widehat{q}^* : [0,T] \\times \\mathbb{R}^d \\times \\mathbb{R}^d \\rightarrow \\mathbb{R}$ be given satisfying\n$\\widehat{J}^*(T, y) = h(y), \\int_{\\mathbb{R}^d} exp \\big( \\frac{1}{\\theta} \\widehat{q}^*(t, y, \\alpha) \\big) d\\alpha = 1, for all (t, y) \\in [0,T] \\times \\mathbb{R}^d.$\nAssume that $\\widehat{J}^*$ and $\\widehat{q}^*$ both have polynomial growth. Then\nWe are now ready to develop and state the algorithm to solve our continuous-time RL problem (16). Consider the parameterized Gaussian policies $\\pi^\\psi(\\cdot|t, y)$ in (25). It is useful to note that\n$\\pi^\\psi (\\alpha|t,y) = exp\\big(\\frac{1}{\\theta} q^\\psi(t, y, \\alpha)\\big),$"}, {"title": "Extensions", "content": "In this section we discuss two extensions of our SDE-based formulation."}, {"title": "ODE-based formulation", "content": "In addition to the SDE-based implementation of diffusion models, another mainstream approach for sample generations is the probability flow ODE implementation (Song et al. 2021). In this subsection, we show that our SDE-based RL framework for reward maximization can be easily extended to the ODE-based formulation.\nRecall that the forward process $(x_t)_{t\\in[0,T]}$ satisfies the following SDE:\n$dx_t = - f(t)x_t dt + g(t)dB_t, x_0 \\sim p_0,$ (36)\nand $p_t(\\cdot)$ denotes the probability density function of $x_t$ in (36). Song et al. (2021) show that there exists an ODE:\n$\\frac{d\\overline{x}_t}{dt} = f(T - t)\\overline{x}_t + \\frac{1}{2} (g(T \u2013 t))^2\\nabla_x log p_{T-t}(\\overline{x}_t), \\overline{x}_0 \\sim p_T,$ (37)\nwhose solution at time $t \\in [0,T]$, $\\overline{x}_t$, is distributed according to $p_{T\u2212t}$, i.e., the ODE (37) induces the same marginal probability density function as the SDE in (4). In particular, $\\overline{x}_T \\sim p_0$. The ODE (37) is called the probability flow ODE.\nMotivated by the SDE-based problem formulation (9), we now consider an ODE-based formulation (with a slight abuse of notation):\n$\\max_{\\alpha=(\\alpha_t: 0 \\leq t \\leq T)} \\big{ \\beta \\cdot E[h(y_T)] - E\\big[ \\int_0^T \\frac{1}{2}(g(T-t))^2 \\cdot |\\nabla log p_{T-t}(y_t) - \\alpha_t|^2 dt \\big] \\}$ (38)\nwhere\n$d\\overline{y}_t = \\big[ f(T-t)\\overline{y}_t + \\frac{1}{2} (g(T \u2013 t))^2\\alpha_t \\big] dt, \\overline{y}_0 \\sim \\nu.$ (39)\nBecause the dynamics $(\\overline{y}^\\alpha)$ is described by a controlled ODE, the Hamiltonian for this problem becomes\n$H(t, y, \\alpha,p) = -(g(T \u2013 t))^2|\\nabla log p_{T-t}(y) \u2013 \\alpha|^2 + \\big[ f(T - t)y + \\frac{1}{2} (g(T-t))^2\\alpha \\big] \\cdot p.$ (40)\nThe entropy regularized value function is given by\n$J(t, y, \\pi) = E^{\\mathbb{P}} E_{t,y}\\big[ \\int_t^T \\big( -g^2 (T-t) \\cdot |\\nabla log p_{T-s}(y_s) \u2013 \\alpha_s|^2 - \\theta log \\pi(\\alpha_s|s, y_s) \\big) ds + \\beta h(y_T) \\big].$\nThe goal of RL is to solve the following optimization problem:\n$\\max_{\\Pi \\in \\Pi} \\int J(0, y, \\pi) d\\nu(y),$ (42)"}, {"title": "Conditional diffusion models", "content": "Diffusion models are often used for conditional data generations, e.g., in text-to-image models. In this section, we briefly discuss the extension of our framework to conditional diffusion models. We take the SDE-based formulation as an illustration.\nWe first introduce some notations. Let $(x_0, C)$ be a random vector in $\\mathbb{R}^d \\times \\mathbb{R}^{d_c}$, where $x_0$ represents the data (e.g. images) and $C$ represents the condition/context (e.g. a text prompt or a class label). Denote by $P_C$ the (marginal) distribution of $C$, which is assumed to be either known or accessible through i.i.d. samples. Denote $p_0(\\cdot|c) := P(x_0 \\in \\cdot|C = c)$ the conditional distribution of $x_0$. Given $C = c$, the standard conditional diffusion model aims to generate samples from the (unknown) conditional data distribution $p_0(\\cdot|c)$.\nFor a conditional diffusion model, the forward process $x_t$ (with a slight abuse of notations) at time $t$ is given by (see e.g. Section 2.2 of Chen et al. 2024):\n$dx_t = f(t)x_t dt + g(t)dB_t, x_0 \\sim p_0(\\cdot|c),$ (44)\nwhere the noise is added to the data $x_0$, but not to the context $c$. Denote by $p_t(\\cdot|c)$ the conditional density function of $x_t$ at time $t$ given $C = c$. Then the reverse-time SDE ($z_t$) satisfies\n$dz_t = [f(T - t)z_t + (g(T \u2013 t))^2\\nabla log p_{T-t}(z_t|c)] dt + g(T \u2013 t)dW_t, z_0 \\sim \\nu,$ (45)\nwhere the initialization distribution $\\nu$ is still chosen to follow the normal distribution in (5), which is independent of $C = c$. In (45), the quantity $\\nabla log p_{T-t}(\\cdot|c)$ is referred to as the conditional score\nTo adapt standard conditional diffusion models to reward maximization, we consider the following problem:\n$\\max_{\\alpha=(\\alpha_t: 0 \\leq t \\leq T)} \\big{ \\beta \\cdot E[h(y_T, C)] - E\\big[ \\int_0^T \\frac{1}{2}(g(T-t))^2 \\cdot |\\nabla log p_{T-t}(y_t|C) - \\alpha_t|^2 dt \\big] \\}$ (46)\nwhere\n$dy_t = [f(T-t)y_t + (g(T - t))^2\\alpha_t] dt + g(T - t)dW_t, y_0 \\sim \\nu,$ (47)\nand the expectation is taken with respect to the randomness in $(y^\\alpha)$ and $C$. In contrast to the unconditional diffusion model, here the reward function $h$ now depends also on the condition $C$. Moreover, the state at time $t$ can be viewed as $(t, y_t, C)$, and hence an optimal action process will also depend on the condition $C$. Define the running reward for this problem as follows:\n$r(t, y, c, \\alpha) := -(g(T \u2013 t))^2 \\cdot |\\nabla log p_{T-t}(y|c) \u2013 \\alpha|^2.$ (48)\nThe exploratory formulation can be defined similarly as in Section 3.2. We let $\\pi: (t, y, c) \\rightarrow \\pi(\\cdot|t, y, c) \\in \\mathcal{P}(\\mathbb{R}^d)$ be a given stochastic feedback policy, which now depends also on the condition c. The exploratory value function is defined by"}, {"title": "Conclusions", "content": "In this paper, we provide a continuous-time RL framework for adapting score-based diffusion models to generate samples that maximize some reward function. The key idea is that of data-driven: optimization is based on a stream of score signals, instead of on an estimated score model. Our framework is general and applicable to both SDE and probability flow ODE based implementations of diffusion models. Numerically, the resulting RL algorithms is shown to perform well on at least low-dimensional synthetic data sets.\nThere are many open questions for future research, including speeding up the training processes, analyzing the convergence of the RL algorithms, adapting the framework to accommodate possibly several reward/cost functions, and performing high dimensional numerical experiments."}]}