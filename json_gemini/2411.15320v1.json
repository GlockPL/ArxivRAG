{"title": "PPLqa: An Unsupervised Information-Theoretic Quality Metric for Comparing Generative Large Language Models", "authors": ["Gerald Friedland", "Xin Huang", "Yueying Cui", "Vishaal Kapoor", "Ashish Khetan", "Sanjiv Das"], "abstract": "We propose PPLqa, an easy to compute, language independent, information-theoretic metric to measure the quality of responses of generative Large Language Models (LLMs) in an unsupervised way, without requiring ground truth annotations or human supervision. The method and metric enables users to rank generative language models for quality of responses, so as to make a selection of the best model for a given task. Our single metric assesses LLMs with an approach that subsumes, but is not explicitly based on, coherence and fluency (quality of writing) and relevance and consistency (appropriateness of response) to the query. PPLqa performs as well as other related metrics, and works better with long-form Q&A. Thus, PPLqa enables bypassing the lengthy annotation process required for ground truth evaluations, and it also correlates well with human and LLM rankings.", "sections": [{"title": "1 Introduction", "content": "The strength of generative Large Language Models (LLMs) lies in answering questions (prompts), smoothly reorganizing existing documents (as in summarization), or generating new ones, emulating a human narrator at various levels of intellectual depth. Of course, the level of intellectual depth is limited by the match between training and task. The problem that this article examines is the comparison of the quality of various responses, for a given task, across various Large Language Models.\nCurrent work, such as HELM (Liang and et al (2022)), Eleuther AI LM Harness (Gao et al. 2021), Big-BENCH (Srivastava and et al. 2022), focuses mainly on computing metrics against ground truth. Not only is domain-specific ground truth preparation a costly process, it is also inherently difficult as it is the equivalent of creating exam question and answers a task that is usually best performed by experts with relevant domain understanding.\nAssuming the presence of ground truth, one still requires an objective loss function to compare the output of the model\nbit-by-bit with the answers prepared by the experts. Furthermore, practice has shown (Lin and Chen 2023) that given the broad applicability of these language models, it can be non-trivial to specify a sufficient test and validation set. Enterprise use of generative language models therefore either requires blind trust in the LLM or the acceptance of the risk that the language model may have surprising responses to some prompts. The GPT-4 technical report (OpenAI 2023) qualifies these challenges as follows: \u201cDespite its capabilities, GPT-4 has similar limitations to earlier GPT models: it is not fully reliable (e.g. can suffer from \"hallucinations\u201d), has a limited context window, and does not learn from experience. Care should be taken when using the outputs of GPT-4, particularly in contexts where reliability is important.\u201d Despite large increases in context size, the hallucination problem remains.\nTherefore, in this paper, we explore an alternative approach to evaluating LLMs using an unsupervised approach that does not require ground truth. Our method is based on an information-theoretic quantification of the structure of a prompt/response pair. Such a quantification can obviously only be very coarse. However, our evaluations show that our metrics perform in the same range as human and LLM-based evaluations. Details are presented in Section 3.\nThe main elements and findings of this paper are as follows. After a short summary of related work in Section 2, Section 3 explains the theoretical underpinnings of the PPLqa metric. Section 4 presents the various analyses undertaken using the PPLqa metric. Responses are generated from a few LLMs to various questions in four subject domains: macroeconomics, astronomy, AI, electronics. Responses are ranked using the metric. The correlation of these rankings to human rankings is assessed using a modified version of Kendall's rank correlation \u03c4. Empirical assessments (F1, accuracy, MCC) for all four domains evidence that PPLqa and human rankings are aligned. For robustness, ranking of LLM responses are also computed on the MT-Bench dataset, showing that PPLqa outperforms the comparison metrics. In yet another robustness test, we replaced human rankings by those from Anthropic's Claude 3 LLM \u2013 here alignment of PPLqa with the LLM's rankings are stronger than with human rankings. There is ranking alignment across PPLqa rankings, Claude's rankings, and human rankings. The limitations of the approach are dis-"}, {"title": "2 Related Work", "content": "We may define an evaluation as a task used to evaluate the quality of a system's behavior. Behavior is necessarily task-related and may be measured around various categories such as over-refusals, safety, system message steerability, hallucinations, math/logical reasoning, question-answering, completions, etc. Indeed, simple choice responses are the subject of model-graded evaluations using templates. Evaluation templates consider various metrics such as (i) factual consistency with a reference answer, (ii) relevance to the prompt query, (iii) conciseness, i.e., does not contain irrelevant information, (iv) correctness, i.e., arrives at the right response. Of course, all of these metrics require ground truth, see for example Wang et al. (2024), or human in the loop evaluations Yadav et al. (2019).\nWe mention two families of metrics/benchmarks that can be used to assess the quality of an LLM but that differ from our PPLqa in that they require ground truth. The first family of metrics, BLEU (Papineni et al. 2002), ROUGE (Lin 2004), METEOR (Banerjee and Lavie 2005), CIDEr (Vedantam, Zitnick, and Parikh 2015), and BERTScore (Zhang et al. 2020) have been used to evaluate quality in machine learning and text summarization tasks. These methods fundamentally aim to quantify the similarity between generated text and a reference text using means such as considering overlap of n-grams, word pairs, or synonyms, and weighting importance based on TF-IDF vectors. BERTScore is a similar metric that computes similarity using contextual embeddings and cosine similarity. The second family of benchmarks, GLUE (Wang et al. 2018) and SuperGLUE (Wang et al. 2019), are a comprehensive suite of metrics to assess the quality of LLMs on a variety of NLP tasks and benchmark datasets. These metrics are calculated automatically based on ground truth which includes correct true/false and multiple-choice responses and classifications, pronoun resolutions as well as reference texts.\nThe RAGAS framework (Retrieval Augmented Generation Assessment) is designed to evaluate RAG-LLM (Retrieval Augmented Generation) pipelines (Es et al. 2024). It distinguishes itself from classical metrics such as grammatical accuracy and fluency and correctness metrics such as ROUGE and BLEU with component-wise metrics such as Faithfulness, Answer Relevancy, Context Recall, Context Precision, and Context Relevancy; and end-to-end evaluation metrics such as Answer Semantic Similarity and Answer Correctness. These metrics can be evaluated using human judgement or with automated evaluation using LLMs (Zheng et al. 2023), however often human oversight is required for the latter case to ensure sufficient quality of LLM evaluation."}, {"title": "3 Information Metrics", "content": "To a first degree, simple questions (prompts) evoke simple answers (responses), while complex questions evoke complex answers. We can measure information complexity using metrics such as entropy and its close cousin, perplexity. To recap, perplexity (PPL) is defined as the exponentiation of the entropy (in bits) of a sequence of tokens. As such, perplexity measures the number of possible binary choices (think \"20 questions game\u201d) that the combination of words implies.\nFormally, assume a text sequence Xn comprising of tokens [x1,..., xn]. Assume a performant LLM as the evaluator \u03a9 from which sequence probabilities are obtained, i.e., this model is an oracle that returns a conditional probability P\u03a9[xt | Xt-1]. The perplexity of Xn is defined as\nPPL(Xn) = exp {-\\frac{1}{n} \\sum_{t=1}^n \\log (p_{\\Omega}[x_t | X_{t-1}])} (1)\nThe higher the conditional probability of the token sequence, the lower its perplexity. The evaluator LLM \u03a9 is different from the LLM that has generated the text sequence X.\nOur approach in this paper uses a perplexity differential:\nPPLqa = |PPL(X(qa)) \u2013 PPL(X(a))|, (2)\nwhere PPL(X(qa)) is the perplexity of the concatenated prompt and response, and PPL(X(a)) is the perplexity of the response alone, as a way to evaluate the intellectual capacity of a language model. A lower value of PPLqa implies better response quality.\nThis metric embodies two attributes of a good response. One, it assesses the quality of the language in the response as perplexity is designed for exactly that (language coherence and fluency). Two, since the question and answer are concatenated, it assesses the relevance and consistency of"}, {"title": "4 Procedure and Experiments", "content": "This section presents experiments to illustrate the use of the PPLqa metric and its efficacy. For objective comparison, the underlying model used in PPLqa and two other baseline methods GPTScore (Fu et al. 2023) and G-EVAL (Liu et al. 2023) across all experiments is Mistral 7B Instruct V0.2. In particular, GPTScore applied a set of prompt templates on every question and answer pair, focusing on different aspects of evaluation such as coherence, relevance, fluency, and consistency. PPLqa by design does not require such prompts.\nYet, in our experiments, we implemented PPLqa with and without such prompt templates for extensive evaluation. All experiments were run on AWS SageMaker and did not require excessive use of GPUs as no pre-training or fine-tuning was involved, only run time inference.\nGPTScore includes aspect-based prompts to sharpen the quality measurement. We applied the same prompts to PPLqa, and assessed models with and without prompts. An example of such a prompt (for the coherence aspect) is \"Answer the question based on the conversation between a human and AI. Question: Is the AI coherent and maintains a good conversation flow throughout the conversation? (a) Yes. (b) No. Conversation: human:{question} AI:{answer}.\u201d\nExperiment I\nOur initial experiments are conducted using four different subject domains: macroeconomics, astronomy, electronics, and AI. The procedure is as follows:\n1. Use OpenAI GPT-4 to generate 50 questions on a subject and also provide them in increasing order of number of bits for all four domains. There are 200 questions in all.\n2. For each question, generate responses from different LLMs that are popular on the Hugging Face open LLM leaderboard.\n3. For these responses, a PPLqa score is calculated using Equation (2) on the prompt/response pair. This score is used as a metric for a single response or for comparison across responses.\nThe PPLqa score is used to compare the responses. When the comparison is binary (i.e., responses from two LLMs are compared), the best response falls into one of two labels, and standard metrics for binary classification may be used to assess how well the metrics perform in comparison to the ground truth. Metric such as F\u2081 (Powers 2020) score are presented for both labels. Accuracy is also presented. An important metric we use is the Matthew's correlation coefficient (MCC) (Chicco, Warrens, and Jurman 2021), which encompasses many of the simpler metrics in scope. (MCC for the binary classification case is the same as Pearson's correlation.)\nHuman Evaluation of Responses\nIn Experiment I, human annotators read all the responses for the subject datasets and ranked the responses from the LLMs for each question as the ground truth. We compare human rankings to the ones by PPLqa and other evaluating metrics.\nIn some cases the rankings are among binary choices, and in other cases among four choices. When the rankings are not just binary, we use rank correlations as an assessment metric. We describe this in a subsequent section titled \"Rank Correlations.\""}, {"title": "5 Limitations of the Approach", "content": "PPLqa is a simple prompt-free metric with a quick procedure that is able to achieve good correlation with human rankings. We propose it as a simple measure of quality to compare different LLMs in the absence of ground truth and expert evaluators. The scope of our work is deliberately narrow, aimed at rankings of LLMs in order to help a human select which one would be the best candidate for use on a certain task or for further tuning. The only input required is a series of questions with answers from multiple LLMs, and no ground truth. Further, we focus on responses that are long form text and not responses to binary/multiple choice questions that generally fall within the domain of classifier models. The information-theoretic interpretation of such models is explained in Friedland (2024). We expect that our metric does not reflect every nuance that one might want to capture in a given, specific task. It will also be less useful when ground truth is available and traditional approaches are easy to apply. On the plus side, no language specifics are used, so the metric can work with any trained language. The range of datasets to which the metric is applied is limited. Other than GPTScore, other metrics are less comparable and have different foundations or use advanced prompting. No quality measure is perfect, neither is ours, and so accuracy will always continue to be a point of discussion for any method."}, {"title": "6 Conclusions", "content": "Unlike other metrics for LLM evaluation that need costly ground truth for evaluation, we propose a metric based on information theory that is unsupervised and requires no ground truth. We choose four example domains for illustrating our evaluation metric through various experiments. We also apply the metric to the MT-Bench human evaluation dataset for comparison with other unsupervised metrics such as GPTScore and G-EVAL.\nFirst, we argue that our metric captures both the language quality of the response as well as its relevance to the query. Second, to illustrate application of the metric we undertake a comparison of LLM responses. Examples are provided so that human evaluation is seen to be consistent with our PPLqa metric. Third, PPLqa also correlates positively to ranking by an LLM (Claude 3). We propose the method discussed in here as a general measure of quality to compare different LLMs in the absence of ground truth and expert evaluators.\nWe note that our metric is a \"white-box\" metric, in that it is fully transparent in the computations made and one may investigate each neuron's activation in the softmax layer from which the perplexity metric is computed. This is different from black-box methods that rely on querying the opinion of an LLM for ranking answers, such as the G-EVAL one used for comparison in this paper.\nFurther work can explore a theoretical analysis of the metric, possibly using a theoretic model of an LLM. It may also be useful to develop a series of domain specific test banks to support a PPLqa leaderboard."}]}