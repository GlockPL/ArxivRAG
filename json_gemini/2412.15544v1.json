{"title": "VLM-RL: A Unified Vision Language Models and Reinforcement Learning Framework for Safe Autonomous Driving", "authors": ["Zilin Huang", "Zihao Sheng", "Yansong Qu", "Junwei You", "Sikai Chen"], "abstract": "In recent years, reinforcement learning (RL)-based methods for learning driving policies have gained increasing attention in the autonomous driving community and have achieved remarkable progress in various driving scenarios. However, traditional RL approaches rely on manually engineered rewards, which require extensive human effort and often lack generalizability. To address these limitations, we propose VLM-RL, a unified framework that integrates pre-trained Vision-Language Models (VLMs) with RL to generate reward signals using image observation and natural language goals. The core of VLM-RL is the contrasting language goal (CLG)-as-reward paradigm, which uses positive and negative language goals to generate semantic rewards. We further introduce a hierarchical reward synthesis approach that combines CLG-based semantic rewards with vehicle state information, improving reward stability and offering a more comprehensive reward signal. Additionally, a batch-processing technique is employed to optimize computational efficiency during training. Extensive experiments in the CARLA simulator demonstrate that VLM-RL outperforms state-of-the-art baselines, achieving a 10.5% reduction in collision rate, a 104.6% increase in route completion rate, and robust generalization to unseen driving scenarios. Furthermore, VLM-RL can seamlessly integrate almost any standard RL algorithms, potentially revolutionizing the existing RL paradigm that relies on manual reward engineering and enabling continuous performance improvements. The demo video and code can be accessed at: https://zilin-huang.github.io/VLM-RL-website/.", "sections": [{"title": "1. Introduction", "content": "Autonomous driving technology has made significant progress in recent years, yet achieving human-level safety and reliability remains a fundamental challenge (Feng et al., 2023; Huang et al., 2024b). A particularly acute challenge is how to develop safe and generalizable driving policies for complex traffic environments (Di and Shi, 2021; Cao et al., 2022; He et al., 2024; Huang et al., 2024d; Sheng et al., 2024a; Yao et al., 2023). The rapid advancement of deep learning has catalyzed remarkable developments in this domain, particularly through imitation learning (IL) and reinforcement learning (RL) (Huang et al., 2024c; Wu et al., 2024), as shown in Fig. 1 (a). IL aims to learn driving policies by mimicking expert demonstrations, achieving impressive performance in controlled environments (Huang et al., 2024c). Yet, IL-based methods face inherent limitations: they heavily depend on the scale and quality of demonstration data and exhibit poor generalization beyond the training distribution. In contrast, RL enables agents to actively learn optimal driving policies through direct interaction with the environment by maximizing carefully designed reward functions (Huang et al., 2024c). The effectiveness of RL-enabled methods has been demonstrated in various decision-making scenarios, such as safe navigation (Mao et al., 2024; He et al., 2024), car-following control (Hart et al., 2024), trajectory control (Sheng et al., 2024b), and lane change (Guo et al., 2024).\nNevertheless, a major challenge in applying RL is designing an appropriate reward function that will lead to the desired behavior (Ma et al., 2023; Venuto et al., 2024; Xie et al., 2024; Wang et al., 2024). While RL has been remarkably successful in domains where the reward function is clearly defined (e.g., gaming, robot manipulation), its application to autonomous driving remains troubled (Ye et al., 2024; Hazra et al., 2024; Han et al., 2024; Zhou et al., 2024). The fundamental difficulty is that the notion of \"good driving\" encompasses complex, context-dependent behaviors, relying on tacit knowledge that is difficult to quantify and encode as a reward function (Ye et al., 2024). This reflects Polanyi's paradox, which asserts that \"we know more than we can tell\" (Polanyi, 2009). Traditionally, reward functions in the field of autonomous driving are usually manually designed based on expert intuition and heuristics, which often combine multiple sub-objectives such as speed maintenance, lane following, and collision avoidance (Chen et al., 2022; Wang et al., 2023a; Zhang et al., 2024). However, this procedure, known as \"reward"}, {"title": "2. Related Works", "content": "The design of reward functions remains a fundamental challenge in RL. Recently, a new paradigm has emerged that leverages foundation models to generate reward signals for RL. Kwon et al. (2023) first demonstrated the potential of LLMs, such as GPT-3 (OpenAI, 2023), in generating rewards for text-based tasks. Subsequent works extended this idea, showing that LLMs can generate structured code for robot training (Yu et al., 2023) and Python code for various agents (Xie et al., 2024; Ma et al., 2023). However, these methods often assume access to detailed environment information, which is challenging in autonomous driving. For instance, accurate data on surrounding vehicles' velocities and positions may not be available. In this work, the VLM-RL generates the reward signal directly from the visual input captured by the on-board camera, which does not require such assumptions. VLM-CaR (Venuto et al., 2024) mitigates VLM query costs by breaking tasks into sub-objectives, though this is difficult for safe driving tasks. Other works use the embedding space of pre-trained VLMs, such as CLIP (Radford et al., 2021). Mahmoudieh et al. (2022) are the first to use fine-tuned CLIP as reward models for robotic manipulation. VLM-SR (Baumli et al., 2023) converts similarity-based rewards into binary rewards via thresholding, while RoboCLIP (Sontakke et al., 2024) compares task video embeddings to agent behavior. VLM-RM (Rocamonde et al., 2024) enhances rewards using goal-based baseline regularization, and RL-VLM-F (Wang et al., 2024) incorporates human preference labels for improved reward quality, and FuRL (Fu et al., 2024) addresses reward misalignment issues to refine reward signals further. These methods work well in robotics domains where goal states can be precisely defined and easily understood by VLMs. In contrast, autonomous driving involves inherently ambiguous language goal states that are difficult to define or verify. Additionally, robotics tasks typically involve static or controlled environments, whereas autonomous driving must deal with multiple agents and uncertain dynamic scenarios."}, {"title": "2.2. Foundation Model in Autonomous Driving", "content": "Recent breakthroughs in foundation models have inspired researchers to apply them to the field of autonomous driving, including scene understanding (e.g., DriveVLM (Tian et al., 2024), LeapAD (Mei et al., 2024)), planning (e.g., DiLu (Wen et al., 2023), DriveMLM (Wang et al., 2023b)), scene generation (e.g., ChatScene Zhang et al. (2024), ChatSim (Wei et al., 2024)), human-vehicle interaction (e.g., DriVLMe (Huang et al., 2024a), Drive as you speak (Cui et al., 2024)), and end-to-end driving (e.g., LMDrive (Shao et al., 2024), DriveGPT4 (Xu et al., 2024)). Despite these advancements, leveraging foundation models for reward design in safe driving tasks has yet to be fully explored. LLM-RL (Zhou et al., 2024) employs LLMs to intuitively shape reward functions via natural language prompts, enabling more human-like driving behavior. HighwayLLM (Yildirim et al., 2024) integrates LLMs with RL to provide explainable decision-making in highway driving scenarios. REvolve (Hazra et al., 2024) frames reward design as an evolutionary search problem, leveraging LLMs and human feedback to create human-aligned reward functions. In contrast, VLM-RL does not require human feedback. AutoReward (Han et al., 2024) utilizes LLMs to generate and refine reward functions through a closed-loop framework automatically. Most of the existing works heavily rely on real-time inference from foundation models, which may raise limitations such as latency issues. VLM-RL does not rely on direct model queries but instead utilizes their embedding spaces for reward computation. Perhaps the work closest to ours is LORD (Ye et al., 2024), which uses undesired language goals to shape agent behavior. The key differences between VLM-RL and LORD are: (a) VLM-RL uses both desired and undesired goals combined with vehicle state information for richer reward signals; (b) VLM-RL uses camera-based visual inputs for more realistic perception; and (c) VLM-RL implements an end-to-end pipeline that produces continuous control outputs."}, {"title": "3. Preliminaries", "content": "A partially observable Markov decision process (POMDP) is defined by the tuple (S, A, O, R, O, \u03a6, \u03b3, d0), where S is the state space, A is the action space, \u03b8(s' | s,a) : S \u00d7 S \u00d7 A \u2192 [0,1] represents the transition function, R(s, a, s') : S \u00d7 A \u00d7 S \u2192 \u211d is the reward function, O denotes the observation space, \u03a6(o | s) : S \u2192 \u2206(O) is the observation distribution, and d0(s) : S \u2192 [0,1] is the initial state distribution. At each timestep, the environment occupies a state s \u2208 S, and the agent selects an action a \u2208 A. The environment transitions to a new state s' with probability \u03b8(s' | s,a). The agent then receives an observation o with probability \u03a6(o | s') and a reward r = R(s,a,s'). A sequence of states and actions forms a trajectory \u03c4 = (s0,a0, s1, a1,...), where si \u2208 S and ai \u2208 A. The return for a trajectory \u03c4 is the discounted sum of rewards: \n\nThe agent's objective is to find a policy \u03c0(a | s) that maximizes the expected return G(\u03c0) = E\u03c0[g(\u03c4(\u03c0); R)]."}, {"title": "3.2. Vision-Language Models", "content": "VLMs have seen significant advancements in recent years (You et al., 2024). These models are broadly defined as those capable of handling sequences of both language inputs l \u2208 L\u2264n and vision inputs i \u2208 I\u2264m. In this context, L represents a finite alphabet, and L\u2264n refers to strings of length up to n. Similarly, I denotes the space of 2D RGB images, and I\u2264m consists of image sequences of length up to m. A notable class of pre-trained VLMs is CLIP (Radford et al., 2021), which includes a language encoder  and an image encoder , both mapping to a shared latent space V \u2286 \u211dk. These encoders are trained jointly through contrastive learning on image-caption pairs. The training objective is to minimize the cosine distance between embeddings of matching pairs while maximizing it for non-matching pairs. CLIP has demonstrated strong performance in various downstream tasks and exhibits impressive zero-shot transfer capabilities (Rocamonde et al., 2024)."}, {"title": "3.3. Problem Statement", "content": "We model the task of training an autonomous driving agent as a POMDP, similar to Rocamonde et al. (2024). The agent's objective is to learn an optimal policy \u03c0 : S \u2192 A that maximizes the expected cumulative reward,\n\nwhere \n. A key challenge in this context is to design an effective reward function R(s, a, s') that guides the agent toward desirable behaviors. Traditional reward engineering requires manual specification of complex behaviors and constraints, which can be tedious, error-prone, and hard to generalize across diverse driving scenarios. Ideally, we wish to directly use VLMs to provide agents with rewards R(s) to guide desired behaviors, as is done in the robotics domain. However, as mentioned earlier, using VLMs directly as rewards for autonomous driving still faces critical challenges. Our goal is to create a specialized VLM-as-Reward framework for the safe driving task to eliminate the need for explicit reward functions"}, {"title": "4. Framework: VLM-RL", "content": "In this section, we present a detailed description of VLM-RL framework. The framework addresses the fundamental challenge of reward design in autonomous driving by leveraging the semantic understanding capabilities of pre-trained VLMs (i.e., CLIP)."}, {"title": "4.1. Overview", "content": "The VLM-RL framework consists of four main components. First, we define the concept of CLG that describes both desired and undesired driving behaviors, providing a foundation for reward computation. Second, we utilize CLIP to compute semantic alignment between the current driving state and these contrasting language descriptions, generating semantic reward signals. Third, we develop a hierarchical reward synthesis approach that combines the semantic rewards with vehicle state information (e.g., speed, heading angle) to produce stable and comprehensive reward signals. Fourth, to optimize computational efficiency, we implement a batch-processing technique that periodically processes observations from the replay buffer rather than computing rewards in real time. Fig. 2 illustrates the overall architecture of our framework. We describe each component in detail in the following subsections."}, {"title": "4.2. Contrasting Language Goal Definition", "content": "Recent advances in robotics have demonstrated remarkable success in utilizing pre-trained VLMs as zero-shot reward models across diverse tasks (Sontakke et al., 2024). Given a task T and its natural language description l \u2208 L\u2264n, the fundamental approach involves leveraging VLMs to generate reward signals that guide the agent toward desired behaviors. This can be formally expressed as (Rocamonde et al., 2024)\n\nwhere c \u2208 L\u2264n is an optional context that may include additional information or constraints. In this formulation, the VLM takes the language goal l, the current observation \u03c8(s), and optional context c, and outputs a reward signal.\nIn robotics, the success of this formulation relies on the ability to describe tasks and goal states with precise language. For example, in manipulation tasks (Fig. 3 (a)), goals such as \"Put carrot in bowl\" are clear and unambiguous, allowing VLMs to effectively measure progress by comparing state-goal relationships in their embedding space V \u2282 \u211dk. In contrast, autonomous driving poses unique challenges, as the goal of \u201cSafe driving\u201d encompasses a wide range of acceptable behaviors and states. This abstract objective makes it difficult to establish clear semantic comparisons between the current vehicle state and the goal. While LORD (Ye et al., 2024) addresses this by using opposite language goals (Fig.3 (b)), this approach offers limited guidance by focusing only on states to avoid.\nDrawing inspiration from human learning, where people often learn more effectively through contrasting goals, as exemplified by the steak-cooking scenario mentioned earlier, we propose using VLMs to generate semantic reward"}, {"title": "4.3. CLG-based Semantic Reward Computation", "content": "Safe driving tasks typically rely on sparse reward signals. In this setting, at each timestep t, given an observation \u03c8(st) derived from state st, the agent executes an action at ~ \u03c0\u03b8(at|st) according to its policy \u03c0\u03b8. The environment then provides a sparse task reward rtask, typically defined as rtask = \u03b4success, meaning a reward of 1 is received only upon task success and otherwise the reward is 0 (Cao et al., 2022). Such sparse rewards present substantial challenges for RL training, as they provide limited learning signals across the majority of the state space. A common approach is to manually design dense reward signals by combining metrics such as speed and distance to waypoints, either through simple summation (Wang et al., 2023a) or weighted aggregation (Chen et al., 2022). It is time-consuming, demands expertise, and may lead to conflicting sub-goals, resulting in suboptimal policies.\nBuilding upon the general VLM reward formulation introduced in Eq. (1), recent works (Rocamonde et al., 2024; Baumli et al., 2023; Fu et al., 2024) have proposed augmenting these sparse task rewards with VLM-generated rewards rVLM. This hybrid reward formulation can be expressed as\n\nwhere \u03c1 > 0 is a weighting parameter that balances the relative importance of the VLM-generated reward against the sparse task reward.\nGiven the vision encoder  and language encoder  that map into the same latent space V \u2286 \u211dk, and a sequence of state-action transitions\n, the VLM reward is defined as"}, {"title": "4.3.2. CLG as Reward Paradigm", "content": "Following the VLM-as-Reward paradigm in Definition 2, we propose a novel CLG-as-Reward paradigm specifically designed for safe driving tasks.\nGiven the CLG (lpos, lneg) introduced in Definition 1, we define the CLG reward function as\n\nwhere \u03b1, \u03b2 > 0 are weighting factors satisfying \u03b1 + \u03b2 = 1. If \u03b1 > \u03b2, the agent focuses more on achieving the positive goal, while if \u03b1 < \u03b2, the agent emphasizes steering clear of negative outcomes. For simplicity, in this work, we set \u03b1 = \u03b2 = 0.5, i.e., the two goals are equally prioritized. sim(\u00b7,\u00b7) denotes the cosine similarity between embeddings as defined in Eq. (5).\nThis formulation ensures that the agent is guided by both the positive and negative goals simultaneously. In other words, it encourages the agent to seek states similar to the positive goal while avoiding states similar to the negative goal, offering more informative guidance for policy learning. The following Thm. 1 formally establishes the effectiveness of the CLG-as-Reward paradigm."}, {"title": "4.4. Hierarchical Reward Synthesis", "content": "In this work, we follow the standard VLM-as-Reward paradigm, i.e., using only a language description of the task (Rocamonde et al., 2024; Sontakke et al., 2024; Wang et al., 2024). Yet, as noted by Fu et al. (2024), while zero-shot VLMs are effective in capturing coarse semantics, they often fall short in accurately representing fine-grained details. Furthermore, a single language description cannot comprehensively capture all the nuances of desired driving behaviors. As a result, relying solely on semantic rewards RCLG could potentially mislead policy optimization in complex driving scenarios. Previous work has explored various strategies to address this issue: LAMP (Adeniji et al., 2023) uses VLM-based reward for behavior pre-training, ZSRM (Mahmoudieh et al., 2022) retrains VLMs with task-specific datasets, and FuRL (Fu et al., 2024) fine-tunes VLM representations and uses relay RL technique.\nIn contrast to these approaches, we aim to preserve the zero-shot capability of VLMs by integrating vehicle state information, which is readily available from on-board sensors, to generate more stable and comprehensive reward signals. In detail, We propose a hierarchical reward synthesis approach consisting of two phases: (a) generating normalized semantic rewards from VLMs and (b) combining these semantic rewards with vehicle state information to produce the synthesis reward signal.\nFirst, we compute the semantic rewards  by processing batches of observation frames through the CLIP. To ensure stability, we normalize the similarity scores to the range [0, 1]:\n\nwhere @min and @max are empirically set to -0.03 and 0.0, respectively, to avoid extreme values and ensure consistent scaling. clip(x, a, b) constrains x within the interval [a, b].\nWe incorporate vehicle state information to produce the synthesis reward signal. This step leverages on-board sensor data to ensure the reward captures realistic driving behavior and safety constraints.\nThe synthesis reward function Rsynthesis: S \u2192 \u211d is computed by com-bining the normalized semantic reward  with vehicle state information. Specifically:\n\n\nwhere  modulates speed alignment, computed as \nevaluates the vehicle's lateral position relative to the lane center.  measures the vehicle's orientation with respect to the road direction.  accounts for the consistency of the vehicle's lateral position relative to the lane center. Each term is bounded within [0,1].\nCompared to traditional weighted-sum reward designs (Chen et al., 2022; Wang et al., 2023a), this multiplicative formulation naturally captures the interdependence of safety criteria without extensive parameter tuning. It yields an interpretable, stable, and easily implementable reward structure that leverages both semantic guidance from the VLM and actionable, high-fidelity vehicle state signals. The workflow of the hierarchical reward synthesis is shown in Appendix B as pseudocode. We also demonstrate the convergence and stability of the synthesis reward function in Appendix C and Appendix D.\nNow, by combining the synthesis reward function in Eq. (11) with Eq. (3), we obtain the final reward function for the VLM-RL framework:\n\nThis formulation allows the agent to benefit from both explicit task success signals and dense, context-aware rewards. The sparse task reward  ensures that the agent remains goal-oriented, while the synthesis reward  provides continuous feedback based on both high-level semantic understanding and low-level vehicle dynamics."}, {"title": "4.5. Policy Training with Batch-Processing", "content": "We adopt the soft actor-critic (SAC) algorithm (Haarnoja et al., 2018) as the backbone RL framework, due to its superior sample efficiency and stability in continuous control tasks. The SAC algorithm aims to maximize the expected return while encouraging exploration through entropy regularization. The objective can be written as:"}, {"title": "5. Experiments and Results", "content": null}, {"title": "5.1. Experiment Setting", "content": null}, {"title": "5.1.1. RL Setups", "content": "The RL agent takes three types of inputs: (1) a bird's-eye view (BEV) semantic segmentation image that captures the surrounding environment, including drivable areas, lane boundaries, and other traffic participants, as illustrated in Fig. 4 (c)-(e). This provides crucial spatial information for navigation and obstacle avoidance. (2) ego state information consisting of the current steering angle, throttle value, and vehicle speed. These values reflect the vehicle's dynamic state and are essential for maintaining smooth control. (3) future navigation information represented by the next 15 waypoints along the planned route. Each waypoint is defined by its (x, y) coordinates relative to the vehicle's current position, helping the agent understand and follow the desired trajectory.\nThe action space is designed as a continuous 2-dimensional space [-1,1]2, where each dimension controls different aspects of vehicle motion. The first dimension corresponds to the steering angle, with values in [-1,1] representing the full range of steering control. Specifically, -1 indicates maximum left turn, 0 represents straight ahead, and +1 indicates maximum right turn. The second dimension combines throttle and brake control in a single value range [-1,1]. When this value is positive ([0, 1]), it directly maps to the throttle intensity, with 1 representing full throttle. Conversely, when the value is negative, its absolute value maps to brake intensity, where -1 corresponds to full brake. An episode is terminated when any of the following conditions are met: (a) collision with any obstacles, vehicles, or pedestrians, (b) deviation from the road center line by more than 3 meters, or (c) vehicle speed remains below 1 km/h for more than 90 consecutive seconds, indicating the agent is stuck or unable to progress. These termination conditions are designed to enforce safe driving behavior and ensure efficient navigation progress.\nWe build our implementation upon the Stable-Baselines3 library (Raffin et al., 2021), which provides reliable and well-tested implementations of modern RL algorithms. Stable-Baselines3 offers a modular design and stable performance, allowing us to focus on extending the core algorithms rather than implementing them from scratch. Specifically, we extend the standard implementations of SAC and PPO to incorporate our CLG-based and hierarchical reward computation during the training process. The policy network architecture is specifically designed for processing heterogeneous input types: we employ a 6-layer CNN to extract features from the BEV semantic segmentation images, while using MLPs to process both the ego state information and future navigation waypoints. These processed features are then concatenated before being fed into the final policy head for action prediction."}, {"title": "5.1.2. Driving Scenarios", "content": "We train all models in CARLA's Town 2 map to ensure a fair comparison and evaluate the effectiveness of our approach and all baseline models. As shown in Fig. 5 (b), this town presents a typical European-style urban layout with a variety of challenging driving scenarios. It consists of several interconnected areas including a residential district, a commercial zone with single-lane roads, and complex intersections controlled by traffic lights. The compact nature of Town 2 makes it particularly suitable for evaluation, as it provides diverse driving conditions within a manageable scale, including both straight roads and curved segments, multiple T-junctions, and different types of lane markings and road geometries. These features create challenging scenarios for assessing both basic driving capabilities and complex decision-making behaviors. In Section 5.7.1, we further evaluate the generalization ability of our approach in Towns 1, 3, 4, and 5, as shown in Figs. 5 (a), (c), (d), and (e), respectively. Unless otherwise specified, all results are reported based on experiments conducted in Town 2.\nTo create a more realistic and challenging environment, we populate the town with 20 vehicles running in autopilot mode. These vehicles are randomly spawned across the map and operate using CARLA's built-in traffic manager, which enables them to follow traffic rules, respond to traffic lights, and perform basic collision avoidance. This dynamic traffic flow significantly increases the complexity of the learning task for our RL agent, as it must now handle various interactive scenarios such as car following, overtaking, and yielding to other vehicles. The presence of multiple moving vehicles not only makes the environment more similar to real-world urban driving conditions but also challenges the RL agent to develop more robust and adaptive driving strategies."}, {"title": "5.1.3. Navigation Routes", "content": "We dynamically assign navigation routes to the RL agent during training and evaluation. At each reset, we utilize the 101 predefined spawn points available on the drivable routes in Fig. 5 (b) as potential starting and destination locations. Specifically, we randomly select two distinct spawn points to serve as the start and end points, then employ the A* search algorithm to compute the shortest path between them, which becomes the navigation route for the agent. Notably, instead of terminating the episode upon reaching the destination, we continuously generate new navigation routes for the agent by repeating this random selection and path planning process. This dynamic route assignment continues until the cumulative driving distance within the episode reaches 3000 meters, allowing us to evaluate the agent's performance across diverse navigation scenarios in a single episode."}, {"title": "5.1.4. CLIP Config", "content": "We employ the CLIP model (Radford et al., 2021) as our foundational VLM for CLG-based semantic reward generation. Specifically, we utilize OpenCLIP's ViT-bigG-14 model pre-trained on the LAION-2B dataset with 2.32"}, {"title": "5.2. Evaluation Metrics", "content": "To comprehensively evaluate the performance and safety aspects of our autonomous driving system, we employ multiple quantitative metrics that assess both driving efficiency and safety characteristics. For driving efficiency assessment, we measure the average speed (AS) maintained by the vehicle throughout episodes, the route completion (RC) which represents the number of successfully completed routes during one episode, and the total traveled distance (TD) which captures the cumulative distance covered by the vehicle during each episode.\nSafety performance is evaluated through several complementary metrics. The fundamental collision rate (CR) measures the percentage of episodes containing collision events. We further analyze collision patterns through two frequency metrics: time-based collision frequency (TCF), measuring collisions per 1000 time steps, and distance-based collision frequency (DCF), measuring collisions per kilometer traveled. To assess collision severity, we record the collision speed (CS) at the moment of each collision. Additionally, we track the inter-collision time steps (ICT), which measure the average number of time steps between consecutive collision events, providing insights into the temporal distribution of safety incidents. In the test phase, we also report the success rate (SR) to evaluate the model's ability to successfully reach the destination across 10 predefined routes."}, {"title": "5.3. Baselines", "content": "We compare our method against state-of-the-art baselines, which can be categorized into two primary groups: expert-designed reward methods and LM-designed reward methods.\nWe implement the following baselines with manually designed reward functions using both SAC and PPO. These methods include binary rewards that only consider collision states, and summation rewards that combine multiple weighted terms to guide driving behavior:"}, {"title": "5.4. Main Results", "content": "We present a detailed evaluation of our proposed VLM-RL against various baseline methods in Tabs. 1-2 and Figs. 6-9. All experiments were conducted using three different random seeds and trained for 1 million steps to ensure statistical significance. The performance metrics reported in Tab. 1 represent the mean and standard deviation of the final checkpoint during training across these three independent runs. For testing results, we selected the best-performing checkpoint from each training run based on comprehensive performance metrics, with the corresponding evaluation results presented in Tab. 2. The learning curves shown in Figs. 6-9 track the training progress of different methods, where solid lines indicate the mean performance across three seeds, and the shaded regions represent one standard deviation from the mean. This visualization allows us to observe not only the final performance but also the learning dynamics and stability of different approaches throughout the training process."}, {"title": "5.4.1. Training Performance Analysis", "content": "We first compare VLM-RL with expert-designed reward methods. From the training curves in Fig. 6, we can see that TIRL exhibits relatively low collision rates and high collision-free intervals. However, this seemingly positive performance is actually a result of the agent's failure to learn basic driving behaviors. As shown in Tab. 1, TIRL-SAC achieves only 0.01 km/h average speed, 0.01 route completion, and 0.21m total driving distance, indicating that the agent essentially remains stationary rather than learning to navigate. In contrast, our VLM-RL demonstrates superior performance across all key metrics. It achieves an average speed of 17.4 km/h while maintaining a low collision speed of 2.6 km/h, and most importantly, successfully completes 4.4 routes with a total driving distance of 1780m. This comprehensive performance indicates that VLM-RL successfully learns both safe driving behaviors and effective navigation strategies. The poor performance of TIRL can be attributed to the limitations of its simple binary reward design in the context of autonomous driving. Binary rewards that only penalize collisions (-1) while assigning neutral rewards (0) to all other states create a significant exploration challenge. In autonomous driving, where the action space is continuous and the state space is highly complex, such sparse binary rewards provide insufficient learning signals for the agent to discover productive driving behaviors. Without positive reinforcement for forward progress or successful navigation, the agent only learns to minimize collision risk by remaining stationary. This represents a local optimum that avoids negative rewards but fails to accomplish the actual driving objectives.\nCompared to other expert-designed reward methods with weighted summation terms, VLM-RL demonstrates more balanced and stable performance. Chen-SAC achieves a higher average speed of 19.9 km/h but suffers from"}, {"title": "5.4.2. Performance Evaluation in Testing", "content": "To further validate the effectiveness of VLM-RL, we conduct comprehensive testing evaluations across 10 predefined routes and compare the performance with baseline methods. The route completion metric represents the average route completion rates during each evaluation episode. The testing results in Tab. 2 demonstrate significant advantages of our approach compared to the baselines.\nThe limitations of binary reward methods remain evident in the testing phase. TIRL variants achieve a route completion rate of 0.01 and total driving distances of 4.7m and 14.8m respectively, confirming their failure to learn meaningful driving behaviors. Among expert-designed reward methods with weighted summation terms, Chen-SAC maintains the highest average speed at 21.4 km/h but shows limited effectiveness with a 0.08 success rate and 0.29 route completion, indicating its aggressive driving style compromises mission success. ChatScene variants demonstrate more balanced performance with success rates of 0.73 and 0.63 respectively, though their collision speeds of 1.18 km/h and 0.89 km/h suggest potential safety concerns.\nLLM-based approaches demonstrate competitive performance during testing, with Revolve achieving a success rate of 0.83 and route completion of 0.92. However, their collision speeds of 1.53 km/h and 1.65 km/h indicate"}, {"title": "5.5. Ablation Study", "content": "Building upon our previous baseline comparisons with VLM-SR, RoboCLIP, VLM-RM and LORD, which established the advantages of our hierarchical reward synthesis approach, we conduct ablation studies to further validate the effectiveness of our proposed CLG approach. Specifically, we investigate the performance when using only positive language goals (VLM-RL-pos) and only negative language goals (VLM-RL-neg), respectively. These variants allow us to analyze the individual contribution of each goal type and demonstrate why combining both through our contrasting framework leads to superior performance. Additionally, we compare the performance of using CARLA's built-in segmentation camera-based BEV as the RL agent's observation (VLM-RL-bev) as an ablation experiment to validate the effectiveness of the BEV design shown in Fig. 4. These ablation experiments provide additional insights into the specific mechanisms that contribute to our method's effectiveness."}, {"title": "5.6. VLM-RL Performance Scaling Across CLIP Model Sizes", "content": "To systematically investigate how the scale of VLM affects the performance of VLM-RL, we conducted experiments with four different CLIP model variants of increasing size and complexity: ViT-B-32 (base), ViT-L-14-quickgelu (large), ViT-H-14 (huge), and ViT-bigG-14 (giant). These models exhibit significant differences in their architectural parameters, ranging from 86M parameters in the baseline ViT-B-32 to over 1B parameters in the ViT-bigG-14. Additionally, they utilize different vision encoder configurations: while ViT-B-32 processes images using 32\u00d732 patches, the larger models employ finer 14\u00d714 patch sizes for increased granularity in visual feature extraction."}, {"title": "5.7. Generalization", "content": "Generalization capability is crucial for autonomous driving systems, as they must adapt to diverse environments and conditions beyond their training scenarios. To comprehensively evaluate VLM-RL's adaptability to diverse scenarios, we first compare our method with two best-performing baseline approaches, i.e., ChatScene-SAC and Revolve, across different towns and varying traffic densities. We also"}]}