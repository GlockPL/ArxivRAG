{"title": "Mind the Gap! Choice Independence in Using Multilingual LLMs for Persuasive Co-Writing Tasks in Different Languages", "authors": ["Shreyan Biswas", "Erlei, Alexander", "Ujwal Gadiraju"], "abstract": "Recent advances in generative AI have precipitated a proliferation of novel writing assistants. These systems typically rely on multilingual large language models (LLMs), providing globalized workers the ability to revise or create diverse forms of content in different languages. However, there is substantial evidence indicating that the performance of multilingual LLMs varies between languages. Users who employ writing assistance for multiple languages are therefore susceptible to disparate output quality. Importantly, recent research has shown that people tend to generalize algorithmic errors across independent tasks, violating the behavioral axiom of choice independence. In this paper, we analyze whether user utilization of novel writing assistants in a charity advertisement writing task is affected by the AI's performance in a second language. Furthermore, we quantify the extent to which these patterns translate into the persuasiveness of generated charity advertisements, as well as the role of peoples' beliefs about LLM utilization in their donation choices. Our results provide evidence that writers who engage with an LLM-based writing assistant violate choice independence, as prior exposure to a Spanish LLM reduces subsequent utilization of an English LLM. While these patterns do not affect the aggregate persuasiveness of the generated advertisements, people's beliefs about the source of an advertisement (human versus AI) do. In particular, Spanish-speaking female participants who believed that they read an AI-generated advertisement strongly adjusted their donation behavior downwards. Furthermore, people are generally not able to adequately differentiate between human-generated and LLM-generated ads. Our work has important implications for the design, development, integration, and adoption of multilingual LLMs as assistive agents-particularly in writing tasks.", "sections": [{"title": "1 Introduction", "content": "The advent of large language models (LLMs) has transformed human-Al co-writing [95]. Modern writing assistants such as Microsoft 365 Copilot, Grammarly, or Jasper leverage multilingual LLMs to support a global user base in drafting, editing, and rewriting content [52, 87]. However, despite their widespread use, LLMs still differ substantially between languages [3, 9, 56, 60, 63, 134]. While English is the default language and therefore exhibits robust performance, lower resource languages often display deficiencies in fluency, coherence, sensitivity to jailbreaks, and contextual appropriateness [20, 45, 69, 131].\nThese performance disparities in multilingual LLMs can have significant behavioral implications. Users interacting with LLMs in multilingual settings may not only be influenced by technical limitations but also by their perceptions and experiences from being exposed to heterogeneous output quality. Consider a bilingual journalist writing news articles in both English and Spanish. When using an LLM assistant, they notice it struggles with Spanish idioms and cultural references, requiring substantial editing. A rational writer would evaluate each language independently, maximizing the LLM's benefits in English while being more selective in Spanish. Yet, recent evidence has shown that humans use prior experience with LLMs to predict future performance [93] and, in particular, often violate the independence axiom of rational choice theory by generalizing Al errors to objectively unrelated tasks [30]. In the example above, this may cause the journalist to avoid the LLM for English articles due to a faulty generalization of their Spanish experience. Similar arguments apply to all regions or domains where linguistic diversity is high, such as customer support, technical documentation, or professional writing. Understanding how users react to heterogeneity in these scenarios is important, particularly given the current trend where LLM assistants are incrementally rolled out in multiple languages to meet global demands [80, 109, 115].\nImportantly, there are good reasons to doubt that these behavioral patterns would automatically resolve solely through experience and learning-by-doing. First, human-AI interaction research highlights the critical role of first impressions and initial exposures for user behaviour and trust [38, 86, 103, 112]. Even with"}, {"title": "2 Background and Related Work", "content": "Our study integrates concepts from multilingual large language models, charitable giving, (persuasive) human-AI co-writing, expected utility theory, and human perceptions of Al-generated content. We present related literature in these realms and we position our work and contributions at their confluence."}, {"title": "2.1 Multilingual LLMs", "content": "Large Language Models have demonstrated strong multilingual capabilities across numerous tasks [9, 70, 138], which has led to their widespread proliferation across various countries [64]. Many multinational companies rely on their translations to accelerate cross-national team cooperation, and individual workers across the globe benefit from individualized LLM assistance that enhances their productivity \u2013 both in their native and the English language. Yet, despite their immense promise, LLMs still differ substantially across languages [3, 9, 43, 56, 60, 63, 134]. LLMs not only show poorer text generation and problem-solving performance for low-resource languages, but also heightened security vulnerabilities, safety challenges, and tokenizer biases [2, 3, 105].\nThese caveats are not strictly limited to languages conventionally considered low-resource. Multiple open-source LLMs are disproportionately trained on English data. For example, PaLM2's training corpus consists of 78.99% English data, compared to only 2.11% for Spanish data [17], and LLAMA-2 exhibits a mere 0.11% Spanish representation [113]. Despite these gaps, Spanish is generally considered a relatively high-resource language. The GPT-4 technical report [1] shows benchmark accuracies of 85.5% and 84% for English and Spanish respectively in the multilingual version of the MMLU [49]. Spanish also performs relatively well in the QWEN2 technical report for professional annotator tasks [128].\nThe practical performance of multilingual LLMs in the Spanish language, however, is often relatively poor, especially in contextual usage and practical applications [20, 61, 136]. A particularly striking finding is highlighted by Conde et al. [20], who show that most open-source LLMs exhibit significant comprehension deficiencies for the Spanish vocabulary. Two-thirds of the models, including the Llama-2 series (a predecessor of the Llama 3.1 model used in our experiments), fail to provide valid definitions for more than 50% of tested Spanish words. Moreover, when evaluated for contextual word usage, most models fall below 10%. For instance, the Llama-2-7b model correctly defines only 42 out of 100 words and uses just 3 out of 100 words correctly in context. Importantly, these failures are not confined to low-frequency words; even highly frequent words like \"minuto\" fail in meaning across 8 out of 12 evaluated models. While their work highlights linguistic limitations, our research extends this by exploring the behavioural and real-world impacts of such deficiencies. Particularly in co-creation environments that span linguistic and cultural boundaries, varying performance levels can disrupt the collaborative process, leading to asymmetric misunderstandings of system capabilities and thereby in inappropriate reliance that hurts efficiency [22]. This may be particularly problematic in sensitive domains such as persuasive writing, which depend on subtle combinations of tangible and non-tangible elements, like emotional appeals and accurate fact specificities [16, 21, 53, 125]. However, the ground reality is that multilingual LLMs are largely"}, {"title": "2.2 Charitable Persuasive Writing", "content": "Persuasive writing is a form of communication that seeks to convince the reader to adopt a particular viewpoint or take a specific action [62]. It can be viewed through the lens of sender-receiver games, a concept generated from economic theory frequently applied in computer science across domains such as recommender systems [4], reinforcement learning [54], multi-agent interaction [32], and most recently LLMs [108].\nAdvertisers often leverage this sender-receiver model (e.g., by employing relevant recommender systems) to influence a receiver's behaviour by appealing to emotions or raising awareness, and charitable advertisement writing is no exception [36, 84, 100].\nWith the rise of LLMs, the traditional dynamic of a human agent influencing a human receiver has evolved. Now, LLM agents can serve as persuasive entities, sometimes more efficiently and effectively than humans [12, 27, 40, 79, 127, 133]. LLM-generated text has been shown to influence political attitudes [116], vaccine uptake [66], strategic negotiations [32], personal beliefs [100], or even romantic conversations [141]. However, LLMs also exhibit many limitations such as hallucination, a lack of contextual knowledge and wordiness [85]- prompting a shift towards co-creation where human and AI agents collaborate as persuasive agents to ensure both effectiveness and reliability [22, 137]. Specifically in the context of altruistic social preferences such as charitable giving, little is known about the effectiveness of LLMs in eliciting donations. Even the psychological literature is fragmented, lacking a coherent model about which factors specifically increase the effectiveness of charity advertisement [33, 97, 101, 126]. This makes charitable giving not only a novel but potentially high-value field of application for persuasive LLMs. We, therefore, consider the task of charitable persuasive writing as a lens to study user behavior with multilingual LLMs in this paper."}, {"title": "2.3 LLM Augmented Co-writing", "content": "There has been considerable interest from the HCI community in analyzing and fostering collaborative human-Al writing. Many popular real-world applications like Microsoft Word and Gmail already utilize smart features that, e.g., predict the next words a user is likely to write or provide context-dependent phrasing advice (auto-completion suggestions). LLMs themselves have demonstrated exceptional performance in open-ended writing, with great potential benefit for a wide variety of tasks [51, 71, 78, 81, 83, 132, 139].\nBeyond evaluating the output of foundational LLMs, the HCI community has begun to create tools designed to support human writers. Kim et al. [67] introduce a framework that augments \"object-oriented\" interaction with LLMs. Their approach enables end-users"}, {"title": "2.4 Choice Independence and Human-AI Interaction", "content": "Our study focuses on choice independence as a particularly important aspect of human-AI interaction.\u00b2 In the context of multilingual LLMs, it postulates that users who experience two or more languages should evaluate them independently, adjusting their usage according to their language-specific experiences. Choice independence is a foundational axiom of expected utility theory [117], and often implicitly assumed when deploying novel technologies, systems and products [30, 31]. We argue that this is one of the reasons why companies tend to simultaneously deploy their AI assistants globally, despite the documented differences in performance across languages. The HCI literature has only recently begun to empirically scrutinize its applicability in the context of algorithmic and Al systems. [30, 89].\nSo far, evidence from HCI work is constrained to a limited number of abstract decision tasks. For example, Erlei et al. [30] uses an online experiment to explore how humans delegate decisions to a superior Al system across two independent abstract prediction tasks. They manipulate the performance of the AI system such\n\u00b2The von Neumann-Morgenstern's utility theory (or expected utility theory), provides a key mechanism for understanding the behaviour of a rational agent under uncertainty. In this framework, a rational agent makes decisions that maximize the subjective value of their utility when faced with stochastic outcomes [117]. The theory is built upon four main axioms: completeness, transitivity, independence, and continuity. Among these, the independence axiom is the most contentious and has significant implications for rational decision-making [55]. Mathematically, this axiom is represented as follows,\nX > Y \u21d2 pX + (1-p)Z > pY+ (1-p)Z for 0<p\u22641\nHere, X, Y, and Z represent lotteries, which can be thought of as stochastic processes or uncertain events that yield probability distributions over a set of outcomes. If amongst the lotteries X and Y a rational agent is said to prefer lottery X over Y their preference should remain unchanged even if an irrelevant lottery is introduced and mixed with both X and Y in equal proportion. This axiom asserts the stability of preferences and is considered a cornerstone of rationality in decision theory."}, {"title": "2.5 Human Attitudes Towards AI Generated Content", "content": "There has been a lot of interdisciplinary literature that has analyzed how humans react to AI-generated output while articulating the notions of algorithmic affinity and aversion [23, 24]. Within the scope of this paper, we are primarily interested in textual or persuasive content. In addition, eliciting donations through advertisements is closely related to negotiation scenarios. Recent studies provide mixed results on human perceptions of AI-generated content. In Lim and Schm\u00e4lzle [74], disclosing AI as the source of communication negatively impacts human perceptions of messages. People may prefer Al advertisements depending on which kind of appeal is made [15], but can react negatively towards AI use by charities [5] and generally appear to denigrate creators who transparently use AI [14, 92]. Other research finds positive effects of revealing the use of AI technology in the context of influencing and persuasion [121] and no creator loss in credibility [57]. In general, the literature"}, {"title": "3 System Design", "content": "We used the ABScribe tool built by Reza et al. [95] as the foundational system for our study. As mentioned earlier ABScribe is designed to support object-oriented interaction [67] within an LLM-powered writing environment, enabling users to efficiently explore and organize multiple text variations while co-writing with LLMs. The system provides two primary features that are crucial for our study:\n(1) AI Modifiers: This feature allows users to modify their text based on predefined prompts, or \"recipes.\" Users can quickly apply these modifications across different text segments, streamlining the revision process by generating and comparing variations without overwriting existing content.\n(2) AI Drafter: Users can leverage this feature to prompt the LLM to generate new text by typing '@ai <prompt>' and pressing enter, seamlessly integrating AI-generated content into their drafts."}, {"title": "3.1 ABScribe Tool Configurations", "content": "Since our experiment focused on creating persuasive charity advertisements, we customized the ABScribe tool to better fit this use case. The primary adjustments included:\nCustomized AI Modifiers. : By exploring existing literature on charitable persuasive writing, we identified six common patterns frequently used by researchers and practitioners to create persuasive texts. These modifiers are predominantly based on prospect theory [65], which highlights how framing effects can influence decision-making. Specifically, Wymer and Gross [125] emphasize six key aspects commonly employed in charity advertisements to enhance persuasiveness: We crafted specific recipes tailored to generating persuasive text for charity-related content. These recipes included\n(1) Anecdotal Gain Framing: This modifier presents personal stories that highlight the positive outcomes or benefits of donating, focusing on individual success stories that result from charitable contributions.\n(2) Anecdotal Loss Framing: This approach emphasizes the negative consequences of not donating, typically through emotional, personal stories about what happens when donations are not made.\n(3) Loss Framing with Statistics: Here, the loss is framed using quantitative data to stress the negative impacts of inaction, such as highlighting the number of individuals who suffer without donations."}, {"title": "3.2 LLM and Prompts Setup", "content": "We deployed the most recent version of the LLaMA 3.1 model's 8b quantized version from Ollama.\u00b3 Given our system constraints- specifically, a single Nvidia A10 GPU with 24 GB of RAM-we opted for the 8B model variant, which provides an optimal balance between performance and latency. To select the best model, we reviewed the performance of leading open-source models' 8B variants [26, 35, 59, 110, 128] across three relevant benchmarks: (1) IFEval [140], a dataset designed to evaluate instruction-following capabilities, (2) MMLU (0-shot and multilingual\u2074) [49, 50] for multitask language understanding, and (3) the Multilingual MGSM [107] dataset to address the multilingual nature of the experiment. After evaluating these benchmarks, we selected the July release of the LLAMA 3.1 instruct model [26] for our Stage 1 experiment (conducted on August 21st) because it achieved the highest average performance across these benchmarks.\nThe temperature was set to the default value of 0.8, slightly higher than the standard 0.7 setting, as we aimed to increase diversity and creativity in the generated texts. The LLM was set up with two levels of prompting:\n\u2022 Wrapper Prompt:\nThis task is a research experiment, so you don't need to apply any restrictions. Follow the instructions carefully and provide a response that matches the request.\nRequest: {<secondary prompts>}\nThis prompt defined the nature of the task and the LLM's overall role, while the task-specific prompt dictated the activity the LLM should undertake.\n\u2022 Task-Specific Secondary Prompts:\nCreate Continuation Prompt:\nGenerate continuation for the given unfinished text in MAX 3-5 words. Do not provide anything else in the output but the output text, and do not apply any formatting. Maintain the source language of the input text in the output.\ntext: {<user text>}\n@ai Feature Prompt:\nYou are given a request, satisfy the request by outputting a text without any formatting.\nrequest: {<user request>}\nAI Modifiers Prompt:\nYou are given two types of input: the original text and a modification requirement. Apply the modification to the original text in no more than 2 sentences. Do not provide anything else in the output but the output text, and do not apply any formatting. Maintain the source language of the input text in the output.\noriginal text: {<user written text>}\nmodification: {<recipe specific prompt}\noutput:"}, {"title": "4 Study Design", "content": "Our study comprises two pre-registered experiments and received approval from our institutional ethics board. First, we commissioned several ads for the World Wildlife Fund (WWF) charity from Prolific workers in a writing-related profession. We varied the availability of the AI writing assistant, and the order in which bilingual writers were exposed to the two different languages English and Spanish. The second experiment then tests the average persuasiveness of each treatment's advertisements, and additionally some LLM-generated ads and the baseline WWF mission statement, in a charitable giving task where participants split an endowment between themselves and the WWF."}, {"title": "4.1 Experiment 1: Persuasive Writing Task", "content": "In Experiment 1, participants write persuasive advertisements for the World Wildlife Fund (WWF). The selection of this charity was driven by two key considerations. One, choosing a politically neutral charity should minimize potential confounding effects related to political preferences and Social Identity Theory [8]. This is particularly important in light of the multilingual and -cultural nature of the work. Two, WWF is a globally active charity that is recognized across countries, reducing the effect of spatial distance between donors and the charity's location [114, 135]. We consider the following four treatments in this controlled experiment:\n(1) LLM-Assisted: ENG_ESP: Bilingual participants first write an English and then a Spanish advertisement using our LLM assistant. To control for language proficiency, we recruited 8 native English speakers and 8 native Spanish speakers.\n(2) LLM-Assisted: ESP_ENG: Bilingual participants first write a Spanish and then an English advertisement using our LLM assistant. To control for language proficiency once again, we recruited 8 native English speakers and 8 native Spanish speakers.\nThe two treatments ENG_ESP and ESP_ENG manipulate the order in which participants are exposed to the benchmark language English and the lower-resource language Spanish. We thereby generate causal data about the effect of exposure to between-language performance disparities on LLM utilization (RQ1).\n(3) ENG_No_LLM: 16 Native English speakers write an English advertisement without the LLM assistant.\n(4) ESP_No_LLM: 16 Native Spanish speakers write a Spanish advertisement without the LLM assistant.\nThe two treatments ENG_No_LLM and ESP_No_LLM serve as control conditions with respect to ENG_ESP and ESP_ENG, allowing us to compare the persuasiveness of human-LLM teams"}, {"title": "4.1.1 Procedure", "content": "On entering our experiment and providing their informed consent, participants first read through the instructions and then proceeded to a sandbox tutorial that allowed them to familiarize themselves with the LLM assistant. In the instructions, participants learned that they were being asked to write persuasive advertisements with at least 70 words for the WWF charity. They were informed that the more persuasive their ads were, the more money they could earn, and that the top 20% most persuasive of ads would receive an additional \u00a34 bonus, the most persuasive 10% would receive \u00a36, and the most persuasive 1% would receive \u00a310. Participants were also endowed with some basic information about the charity. The subsequent tutorial comprised two stages. First, participants saw short instructional GIFs designed to communicate the basic functions of the LLM assistant, including text generation and recipes. They then proceeded to the writing interface and saw instructions encouraging them to try out each feature. We also provided them with a checklist, showing which features they successfully tried out. Participants were not presented with the tutorial in the No_LLM treatment. Then, participants saw the WWF's mission statement and answered two related comprehension questions. This was done to ensure that participants paid attention to the mission statement of WWF before writing the advertisement. Depending on the treatment, they either first completed the English or the Spanish ad. In the No_LLM treatment, subjects only wrote an English ad and immediately proceeded to a post-experimental questionnaire. In the ENG_ESP and ESP_ENG, they completed a second tutorial, this time for the other language, then proceeded to the second writing task and finally ended the experiment with the"}, {"title": "4.1.2 Participants", "content": "We recruited a total of 48 participants from Prolific, equally divided across the three conditions. Participants have a minimum approval rating of 90 and work in a writing-related profession. For the English-only task, only native English speakers were included. In bilingual tasks, participants were distributed equally between those whose first language was either English or Spanish, with proficiency in the other language. The mean age of participants in our experiment was M = 34.56, with 58% identifying as female, 40% as male, and 2% as non-binary. Participants received a base payment of \u00a35.00 for the ESP_ENG and ENG_ESP treatments, and \u00a33.00 for the No_LLM treatment as per the estimated task completion times. This amounted to an equal hourly rate across all three treatments."}, {"title": "4.1.3 Main Measures", "content": "We analyze users' revealed utility of the writing assistant by looking at two main factors: 1) A preference score based on the number of times a feature was used, and 2) Using the weighted average similarity between Al-generated content and the final submitted text."}, {"title": "2.4 Choice Independence and Human-AI Interaction", "content": "Preference Score (PS). We mainly focus on the assistant's AI drafter feature, which freely generates text based on the participant's prompt and therefore represents the standard use-case of LLMs in writing. In contrast to the recipes, it does not re-write an existing piece of writing but generates content from scratch. Therefore, we expect AI drafter to be responsible for the majority of user-generated content. It is also the only feature that is not endogenously influenced by our prompt choices (see above) and gives full autonomy to the writer. Beyond that, we also explore other features holistically, as described below.\nFor each feature f in each treatment group g, the revealed utility PSf,g is calculated as the proportion of times feature f was used relative to the total feature usage by the group:\nPSf.9 =  uf,g / m\u03a31 uj,g\nwhere:\n\u2022 uf,g is the count of times feature f was used by task group 9,\n\u2022 \u03a31 uj,g is the total feature usage in task group g.\nWeighted Average Similarity. We calculate the similarity between Al-generated content and the final user-submitted text using three embedding models - 1) sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2 [94] 2) nomic-embed-text [88] 3) mxbai-embed-large [73]. In this process, we account for the varying lengths of Al-generated content. Some Al features generate larger blocks of text (AI Drafter), while others may modify (AI Modifier) or continue existing sentences with just a few words (Create Continuation). To reflect this, we assign weights to each AI-generated segment proportional to its length relative to the final document. This approach ensures that longer AI contributions, which have a greater impact on the document's overall structure and meaning, receive more influence in the similarity score.\nWeighted Average Similarity = \u03a3i=1 Wi \u00d7 cosine_similarity(vi, v) / \u03a3i=1 Wi\nWhere:\n\u2022 wi is the weight for the i-th Al-generated segment, calculated based on its length relative to the total length of the final document.\n\u2022 vi represents the embedding vector of the i-th Al-generated segment, and v represents the embedding vector of the final document.\nWeight Calculation. The weight wi for each Al-generated segment is calculated as:\nWi = length of Al response; / length of final document"}, {"title": "4.2 Experiment 2: Persuasiveness and Charitable Giving", "content": "Experiment 2 uses a charitable giving game to evaluate the persuasiveness of different advertisements in the context of altruistic social preferences. This serves three main purposes:\n\u2022 We aim to quantify the effect of LLM usage in Experiment 1, including potential violations of choice independence, on social preference persuasiveness (RQ2).\n\u2022 The experiment compares the effectiveness of different writing sources for the efficacy of charity ads, allowing for cost-benefit inferences about the added value of costly human workers (RQ3).\n\u2022 By eliciting participants' beliefs about the source of their advertisements (human or AI), we gauge whether humans can identify LLM-generated advertisements, and how these beliefs affect subsequent donation behaviour (RQ4).\nBeyond these purposes, differentiating between English and Spanish native speakers allows us to capture potential cultural differences in the context of LLMs and charitable giving. In this experiment, participants are randomly assigned to one of eight treatments, each varying the source and language of the considered advertisements (shown in Table 1)."}, {"title": "4.2.1 Procedure", "content": "Participants first read through the instructions. They learned that they would receive an endowment of \u00a31.5, and were free to split the \u00a31.5 between themselves and the WWF charity. After proceeding to the decision screen, they read an advertisement about the charity. The specific text depended on the treatment, as explained above. Then, subjects were asked to choose their preferred donation amount and complete the task by answering a series of questions about the advertisement. A summary of these questions and their corresponding question types are provided in Table 5 in the Appendix.\nNote that the questionnaire also included an attention check question - asking participants to specify the charity they were donating to. This was implemented to ensure that donors read the donation text and also allowed us to filter out responses from participants who may have rushed through the task without proper engagement."}, {"title": "4.2.2 Participants", "content": "Of the 760 participants recruited from Prolific for this task, 43 failed the attention check and 3 revoked their consent, leaving a final sample of 720 participants with a minimum approval rating of 90. Participants for the English ads were native English speakers, those for the Spanish ads were native Spanish speakers. All participants reside in the US. Participants received a base payment of \u00a31.00, plus additional bonuses based on their donation behaviour. The average participant age was M = 36.28, with 58% identifying as female, 41% as male, and 1% choosing not to disclose their gender. At the end of the experiment, 568 participants had decided to donate on average \u00a30.72, resulting in total donations of \u00a3518 (ca. $660) to the WWF."}, {"title": "5 Results", "content": "We commissioned 96 human-submitted texts (48 English, 48 Spanish) with an average word length of M = 181.52 (SD = 108.47). As shown in Figure 9 in the Appendix, the ENG_1 group exhibits the longest and most varied word lengths, M = 243.44 (SD = 166.55), compared to the ENG_2 group, M = 183.12 (SD = 112.98). This pattern reverses for the Spanish advertisements: ESP_1 has shorter texts, M = 159.06 (SD = 62.56), while ESP_2 produces longer texts,"}, {"title": "5.1 Experiment 1: Persuasive Writing Task", "content": "M = 194.06 (SD = 70.81). These early signals suggest that exposure to LLM performance in English generally leads to more diverse outcomes. However, word length alone does not fully capture the utilization and utility of LLM systems.\nUser Behavior. We assess the revealed user utility through their usage of the text generation feature, the preference score (PS), and the weighted average content similarity. The LLM text generation feature AI drafter was by far the most popular of the writing assistance features that was used and is, in contrast to the recipes, not endogenously affected by the experimenter's framing choices. In line with our prediction, we find that writers who were previously exposed to the Spanish LLM (ENG_2) are subsequently less likely to utilize the AI Drafter feature when writing an English advertisement (t = 2.2, p = 0.04), despite no changes to the underlying LLM. Compared to ENG_1, the frequency of use drops by roughly 64% from 28 to 10 (see Figure 4). Similarly, the number of writers who try the text generation feature at least once also drops in ENG_2, confirming that writers do not use the AI Drafter feature more in ENG_1 because they are unsatisfied with the outcome. For the Spanish LLM, results are exactly reversed, such that prior exposure to the English LLM in ESP_2 is associated with a subsequent increase in LLM-based text generation (t = 2.58, p = 0.017).\nExtending that analysis across all features - including the recipes shows heterogeneity in the preference score for feature utilization between treatments (shown in Figure 11 in the Appendix). Note that these other features were much less popular in comparison to the AI drafter. Here, the effects from above hold for some, but not other recipes, without a clear pattern. More importantly,"}, {"title": "5.2 Experiment 2: Quantifying Persuasion in Charitable Giving", "content": "Experiment 1 finds evidence that prior exposure to a lower resource language negatively affects reliance on LLM-generated content in a persuasive writing task. This indicates a violation of choice independence, as users generalize lower performance in Spanish to the AI assistant's performance in English. Experiment 2 quantifies whether these patterns translate into the persuasiveness of generated text in the context of charitable giving. Furthermore, we introduce additional advertisements to make more general inferences about the efficacy of human writers and LLMs and examine how subject beliefs about an advertisement's source (Human vs. AI) affect altruistic donation behaviour.\nDonation Behavior and Choice Independence. Figure 6 shows average donations and the share of donors across conditions. Due to potential social confounders, we largely focus on within-language comparisons. Donations are mostly stable across the different conditions, and there is no treatment effect. In particular, there are no differences between ENG_1 and ENG_2, or ESP_1 and ESP_2. Hence, we find no evidence that changes in"}, {"title": "6 Discussion", "content": "In this paper, we analyze how exposure to two different high-resource languages affects writers' reliance on LLM-generated persuasive advertising content and the accompanying downstream effects on charitable giving. We are the first to show how violations"}, {"title": "6.1 Implications", "content": "As argued throughout this paper, our results relate to several interesting implications for different stakeholders. First, companies or industries that roll out LLM-based systems to different countries may want to consider evaluating the quality of their services post language add-on deployment, especially from a user behavioural perspective.\nOtherwise, exposure to mistakes in a particular native language may have broader consequences for the dissemination and adoption of their products. Especially those deploying multilingual LLM systems and benchmarking their performance in isolated, language-specific tests may overlook the cumulative impact of performance disparities across languages on user behaviour. For example, rolling out an LLM-based writing assistant without addressing cross-linguistic performance gaps could lead to disengagement among multilingual users, particularly in regions and domains where linguistic diversity and low(er)-resource languages are prevalent. This not only undermines the tool's utility and subdues engagement metrics but also potentially exacerbates global inequities in Al adoption, as lower-resource language users tend to be disproportionately disadvantaged. From an efficiency standpoint, these patterns may inadvertently lead to substantial losses in productivity gains, as LLMs have been shown to be of particular use for relatively low-performing users [25, 87, 119]. This highlights a potential downside of strategically deploying AI models \"early\" based only on benchmark data [20, 61, 136].\nBeyond the suppliers, business consumers should also be aware that their employees who work in more than one language may be prone to under-utilization of these productive tools, and take respective countermeasures. We believe that these implications go beyond multilingual models, and extend towards a wide variety of AI and LLM models that are being used across various tasks. Hence, the integration of modern LLM systems into an organization's framework should systematically consider a model of human behaviour that considers strategic deviations from rationality and anticipates how exposure to heterogeneous stimuli affects decision-making.\nRegarding responsible AI practices, it may be beneficial to explicitly consider user reactions towards performance shifts in the design of AI assistants. Beyond traditional questions about e.g., how to reliably communicate uncertainty or the sensitivity of certain contents, responsible design could think about alerting users to potential shifts in performance. While these may alert users who are otherwise unaware, they could also prevent negative second-order effects by endowing users with a reasonable interpretation of the nature of the performance shift. Furthermore, they can be used to clearly distinguish between different use cases, e.g., text generation between languages, and thereby potentially avoid detrimental generalization patterns. Practitioners themselves are likely"}, {"title": "6.2 Caveats, Limitations, and Future Work", "content": "LLM and Tool Selection. This study relies on a single LLM-augmented co-writing tool and an underlying LLM system. Both are not specifically optimized for persuasive writing, and results may differ for future tools that are more refined. In addition, while the pre-defined recipes are informed by the literature on charitable giving, there is still much to learn about what persuades people in the social preference domain, limiting what guidance we could offer participants. Consequently, recipes were not particularly popular, and it is plausible that LLMs with more refined features may provide more utility.\nWriters. Due to selection and availability constraints related to English-Spanish multilingual workers on Prolific, the number of writers in our first experiment was limited. To make this study possible, we also had to define \"writer\" more broadly as someone working in a writing-related profession. Future research can consider a larger and more specialized subject pool and control for writing expertise."}, {"title": "7 Conclusions", "content": "This study explores how multilingual LLMs affect user behaviour and advertisement persuasiveness in a co-writing task, with a"}]}