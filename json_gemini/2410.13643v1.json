{"title": "Fine-Tuning Discrete Diffusion Models Via Reward Optimization with Applications to DNA and Protein Design", "authors": ["Chenyu Wang", "Masatoshi Uehara", "Yichun He", "Amy Wang", "Tommaso Biancalani", "Avantika Lal", "Tommi Jaakkola", "Sergey Levine", "Hanchen Wang", "Aviv Regev"], "abstract": "Recent studies have demonstrated the strong empirical performance of diffusion models on discrete sequences (i.e., discrete diffusion models) across domains from natural language to biological sequence generation. For example, in the protein inverse folding task, where the goal is to generate a protein sequence from a given backbone structure, conditional diffusion models have achieved impressive results in generating natural-like sequences that fold back into the original structure. However, practical design tasks often require not only modeling a conditional distribution but also optimizing specific task objectives. For instance, in the inverse folding task, we may prefer protein sequences with high stability. To address this, we consider the scenario where we have pre-trained discrete diffusion models that can generate natural-like sequences, as well as reward models that map sequences to task objectives. We then formulate the reward maximization problem within discrete diffusion models, analogous to reinforcement learning (RL), while minimizing the KL divergence against pretrained diffusion models to preserve naturalness. To solve this RL problem, we propose a novel algorithm, DRAKES, that enables direct backpropagation of rewards through entire trajectories generated by diffusion models, by making the originally non-differentiable trajectories differentiable using the Gumbel-Softmax trick. Our theoretical analysis indicates that our approach can generate sequences that are both natural-like (i.e., have a high probability under a pretrained model) and yield high rewards. While similar tasks have been recently explored in diffusion models for continuous domains, our work addresses unique algorithmic and theoretical challenges specific to discrete diffusion models, which arise from their foundation in continuous-time Markov chains rather than Brownian motion. Finally, we demonstrate the effectiveness of our algorithm in generating DNA and protein sequences that optimize enhancer activity and protein stability, respectively, important tasks for gene therapies and protein-based therapeutics. The code is available at https://github.com/ChenyuWang-Monica/DRAKES.", "sections": [{"title": "INTRODUCTION", "content": "Diffusion models have gained widespread recognition as effective generative models in continuous spaces, such as image and video generation (Song et al., 2020; Ho et al., 2022). Inspired by seminal works (e.g., Austin et al. (2021); Campbell et al. (2022); Sun et al. (2022)), recent studies (Lou et al., 2023; Shi et al., 2024; Sahoo et al., 2024) have shown that diffusion models are also highly effective in discrete spaces, including natural language and biological sequence generation (DNA, RNA, proteins). Unlike autoregressive models commonly used in language modeling, diffusion models are particularly well-suited for biological sequences, where long-range interactions are crucial for the physical behavior of molecules arising from those sequences (e.g., the 3D folded structure of RNA or proteins).\nWhile discrete diffusion models effectively capture conditional distributions (e.g., the distribution of sequences given a specific backbone structure in an inverse protein folding design problem (Dauparas et al., 2022; Campbell et al., 2024)), in many applications, especially for therapeutic discovery, we often aim to generate sequences that are both natural-like and optimize a downstream performance objective. For instance, in the inverse folding problem, we may prefer stable protein sequences (i.e., sequences that fold back into stable protein conformations (Widatalla et al., 2024)); for mRNA vaccine production we desire 5' UTRs that drive high translational efficiency (Castillo-Hair and Seelig, 2021); for gene and cell therapies, we desire regulatory DNA elements, such as promoters and enhancers, that drive high gene expression only in specific cell types (Taskiran et al., 2024); and for natural language we optimize to minimize harmfulness (Touvron et al., 2023).\nTo address these challenges, our work introduces a fine-tuning approach for well-pretrained discrete diffusion models that maximizes downstream reward functions. Specifically, we aim to optimize these reward functions while ensuring that the generated sequences maintain a high probability under the original conditional distribution (e.g., the distribution of sequences that fold into a given backbone structure). To achieve this, we formulate the problem as a reward maximization task, analogous to reinforcement learning (RL), where the objective function integrates both the reward terms and the KL divergence with respect to the pre-trained discrete diffusion model, which ensures that the generated sequences remain close to the pre-trained model, preserving their naturalness after fine-tuning. To solve this RL problem, we propose a novel algorithm, DRAKES, that enables direct backpropagation of rewards through entire trajectories by making the originally non-differentiable trajectories differentiable using the Gumbel-SoftMax trick (Jang et al., 2016).\nOur main contribution is an RL-based fine-tuning algorithm, Direct Reward bAcKpropagation with gumbEl Softmax trick (DRAKES), that enables reward-maximizing finetuning for discrete diffusion models (Figure 1). We derive a theoretical guarantee that demonstrates its ability to generate natural and high-reward designs, and demonstrate its performance empirically on DNA and protein design tasks. While similar algorithms exist for continuous spaces (Fan et al., 2023; Black et al., 2023; Uehara et al., 2024), our work is the first, to the best of our knowledge, to address these aspects in (continuous-time) discrete diffusion models. This requires addressing unique challenges, as discrete diffusion models are formulated as continuous-time Markov chains (CTMC), which differ from Brownian motion, and the induced trajectories from CTMC are no longer differentiable, unlike in continuous spaces. Our novel theoretical guarantee also establishes a connection with recent advancements in classifier guidance for discrete diffusion models (Nisonoff et al., 2024)."}, {"title": "RELATED WORKS", "content": "Discrete diffusion models and their application in biology. Building on the seminal works of Austin et al. (2021); Campbell et al. (2022), recent studies on masked diffusion models (Lou et al., 2023; Shi et al., 2024; Sahoo et al., 2024) have demonstrated strong performance in natural language generation. Recent advances in masked discrete diffusion models have been successfully applied to biological sequence generation, including DNA and protein sequences (Sarkar et al., 2024; Campbell et al., 2024). Compared to autoregressive models, diffusion models may be particularly well-suited for biological sequences, which typically yield molecules that fold into complex three-dimensional (3D) structures.\nIn contrast to these works, our study focuses on fine-tuning diffusion models to optimize downstream reward functions. One application of our approach is the fine-tuning of protein inverse folding generative models to optimize stability, as discussed in Widatalla et al. (2024). However, unlike this prior work, we employ discrete diffusion models as the generative model.\nControlled generation in diffusion models. There are three primary approaches:\n\u2022 Guidance: Techniques such as classifier guidance (Song et al., 2020; Dhariwal and Nichol, 2021) and its variants (e.g., Bansal et al. (2023); Chung et al. (2022); Ho et al. (2022)) introduce gradients from proxy models during inference. However, since gradients are not formally well-defined for discrete states in diffusion, a recent study (Nisonoff et al., 2024) proposed a method specifically designed for discrete diffusion models. Alternative approaches directly applicable to discrete diffusion models include sequential Monte Carlo (SMC)-based methods (Wu et al., 2024; Trippe et al., 2022; Dou and Song, 2024; Cardoso et al., 2023; Phillips et al., 2024). While these guidance-based inference techniques have their own advantages, they generally lead to longer inference times compared to fine-tuned models. We compare our methods against these in terms of generation quality in Section 6.\n\u2022 RL-based fine-tuning: To maximize reward functions for pretrained diffusion models, numerous recent studies have explored RL-based fine-tuning in continuous diffusion models (i.e., diffusion models for continuous objectives) (Fan et al., 2023; Black et al., 2023; Clark et al., 2023; Prabhudesai et al., 2023). Our work, in contrast, focuses on discrete diffusion models.\n\u2022 Classifier-free fine-tuning (Ho and Salimans, 2022): This approach constructs conditional generative models, applicable in our setting by conditioning on high reward values. Although not originally designed as a fine-tuning method, it can also be adapted for fine-tuning (Zhang et al., 2023) by adding further controls to optimize. However, in the context of continuous diffusion models, compared to RL-based fine-tuning, several works (Uehara et al., 2024) have shown that conditioning on high reward values is suboptimal, because such high-reward samples are rare. We will likewise compare this approach to ours in Section 6. Lastly, when pretrained models are conditional diffusion models (i.e., $p(x|c)$) and the offline dataset size consisting of triplets $(c, x, r(x))$ is limited, it is challenging to achieve success. Indeed, for this reason, most current RL-based fine-tuning papers (e.g., Fan et al. (2023); Black et al. (2023); Clark et al. (2023)) do not empirically compare their algorithms with classifier-free guidance.\nDirichlet diffusion models for discrete spaces. Another approach to diffusion models for discrete spaces has been proposed (Stark et al., 2024; Avdeyev et al., 2023; Zhou et al., 2024). In these models, each intermediate state is represented as a vector within a simplex. This is in contrast to masked diffusion models, where each state is a discrete variable."}, {"title": "PRELIMINARY", "content": ""}, {"title": "DIFFUSION MODELS ON DISCRETE SPACES", "content": "In diffusion models, our goal is to model the data distribution $P_{data} \\in \\mathcal{P}(\\mathcal{X})$ using the training data, where $\\mathcal{X}$ represents the domain. We focus on the case where $\\mathcal{X} = \\{1,2,\\ldots, N\\}$. The fundamental principle is (1) introducing a known forward model that maps the data distribution to a noise distribution, and (2) learning the time reversal that maps the noise distribution back to the data distribution (detailed in Lou et al. (2023); Sahoo et al. (2024); Shi et al. (2024)).\nFirst, we consider the family of distributions $\\mathbf{j}_t \\in \\mathbb{R}^N$ (a vector summing to 1) that evolves from $t = 0$ to $t = T$ according to a continuous-time Markov chain (CTMC):\n$\\frac{d\\mathbf{j}_t}{dt} = Q(t)\\mathbf{j}_t, \\quad \\mathbf{P}_0 \\sim P_{data}$,"}, {"title": "GOAL: GENERATING NATURAL SAMPLES WHILE OPTIMIZING REWARD FUNCTIONS", "content": "In our work, we consider a scenario with a pretrained discrete diffusion model $p_{pre}(\\mathbf{x}|c) \\in [\\mathcal{C} \\to \\triangle(\\mathcal{X})]$ trained on an extensive dataset and a downstream reward function $r: \\mathcal{X} \\to \\mathbb{R}$. The pretrained diffusion model captures the naturalness or validity of samples. For example, in protein design, $p_{pre}(\\cdot)$ could be a protein inverse-folding model that generates amino acid sequences that fold back into the given backbone structure (similar to Campbell et al. (2024)), and $r$ could be a function that evaluates stability. Our objective is to fine-tune a generative model to generate natural-like samples (high $\\log p_{pre}(\\cdot)$) with desirable properties (high $r(\\cdot)$).\nNotation. We introduce a discrete diffusion model parameterized by $\\theta$ from $t=0$ to $t=T$:\n$\\frac{d\\mathbf{p}_t}{dt} = Q_\\theta(t)\\mathbf{p}_t, \\quad \\mathbf{p}_0 = P_{lim}$"}, {"title": "ALGORITHM", "content": "In this section, we present our proposed method, DRAKES, for fine-tuning diffusion models to optimize downstream reward functions. We begin by discussing the motivation behind our algorithm."}, {"title": "KEY FORMULATION", "content": "Perhaps the most obvious starting point for fine-tuning diffusion models to maximize a reward function $r(x)$ is to simply maximize the expected value of the reward under the model's distribution, i.e., $\\mathbb{E}_{x_T\\sim p_\\theta(x_T)}[r(x_T)]$, where the expectation is taken over the distribution $P^\\theta(\\mathbf{x}_{0:T})$ induced by (2) (i.e., the generator $Q_\\theta$). However, using only this objective could lead to over-optimization, where the model produces unrealistic or unnatural samples that technically achieve a high reward, but are impossible to generate in reality. Such samples typically exploit flaws in the reward function, for example, by being outside the training distribution of a learned reward or violating the physical assumptions of a hand-engineered physics-based reward (Levine et al., 2020; Clark et al., 2023; Uehara et al., 2024). We address this challenge by constraining the optimized model to remain close to a pretrained diffusion model, which captures the distribution over natural or realistic samples. More specifically, we introduce a penalization term by incorporating the KL divergence between the fine-tuned model $P^\\theta(\\mathbf{x}_{0:T})$ and the pretrained diffusion model $P^{\\theta_{pre}}(\\mathbf{x}_{0:T})$ in CTMC.\nAccordingly, our goal during fine-tuning is to solve the following reinforcement learning (RL) problem:\n$\\theta^* = \\underset{\\theta \\in \\Theta}{\\operatorname{argmax}} \\mathbb{E}_{x_{0:T}\\sim P^\\theta} [r(x_T)]$\nReward term\n$-\\alpha \\int_{0}^{T}\\sum_{x\\neq y} P^\\theta[X_s=x] \\{ Q^\\theta_{x,y}(s) - Q^{\\theta_{pre}}_{x,y}(s) + Q_{x,y}^\\theta(s) \\log\\frac{Q^{\\theta_{x,y}}(s)}{Q^{\\theta_{pre}_{x,y}}(s)}\\}ds$  dt\nKL term\n(3)\nThe first term is designed to generate samples with desired properties, while the second term represents the KL divergence. The parameter $\\alpha$ controls the strength of this regularization term.\nFinally, after fine-tuning, by using the following CTMC from $t = 0$ to $t = T$:\n$\\frac{d\\mathbf{p}_t}{dt} = Q_{\\theta^*}(t)\\mathbf{p}_t, \\quad \\mathbf{p}_0 = P_{lim}.$\nwe generate samples at time $T$. Interestingly, we can show the following.\nTheorem 1 (Fine-Tuned Distribution). When $\\{Q_{\\theta} : \\theta \\in \\Theta\\}$ is fully nonparametric (i.e., realizability holds), the generated distribution at time T by (4) is proportional to\n$\\exp(r(x)/\\alpha) p_{pre}(x).$\nThis theorem offers valuable insights. The first term, $\\exp(r(x))$, represents high rewards. Additionally, the second term, $p_{pre}(\\cdot)$, can be seen as prior information that characterizes the natural sequence. For example, in the context of inverse protein folding, this refers to the ability to fold back into the target backbone structure.\nRemark 3. A similar theorem has been derived for continuous diffusion models (Uehara et al., 2024, Theorem 1). However, our formulation (3) differs significantly as our framework is based on a CTMC, whereas those works are centered around the Brownian motion. Furthermore, while the use of a similar distribution is common in the literature on (autoregressive) large language models (e.g., Ziegler et al. (2019)), its application in discrete diffusion models is novel, considering that $p_{pre}(\\cdot)$ cannot be explicitly obtained in our context, unlike autoregressive models."}, {"title": "DIRECT REWARD BACKPROPAGATION WITH GUMBEL SOFTMAX TRICK (DRAKES)", "content": "Based on the key formulation presented in Section 4.1, we introduce our proposed method (Algorithm 1 and Figure 1), which is designed to solve the RL problem (3). The core approach involves iteratively (a) sampling from $\\mathbf{x}_{0:T} \\sim P^\\theta$ and (b) updating $\\theta$ by approximating the objective function (3) with its empirical counterpart and adding its gradient with respect to $\\theta$ into the current $\\theta$. Importantly, for step (b) to be valid, step (a) must retain the gradients from $\\theta$. After explaining the representation of $x_t$, we will provide details on each step.\nRepresentation. To represent $x \\in \\{1,\\ldots, N\\}$, we often use the $N$-dimensional one-hot encoding representation within $\\mathbb{R}^N$ interchangeably. From this perspective, while the original generator corresponds to a map $\\mathcal{X} \\times \\mathcal{X} \\to \\mathbb{R}$, we can also regard it as an extended mapping: $\\mathbb{R}^N \\times \\mathbb{R}^N \\to \\mathbb{R}$. We will use this extended mapping when we consider our algorithm later."}, {"title": "THEORY OF DRAKES", "content": "In this section, we provide an overview of the proof for Theorem 1. Based on the insights gained from this proof, we reinterpret state-of-the-art classifier guidance for discrete diffusion models (Nisonoff et al., 2024) from a new perspective."}, {"title": "PROOF SKETCH OF THEOREM 1", "content": "We define the optimal value function $V_t: \\mathcal{X} \\to \\mathbb{R}$ as follows:\n$\\mathbb{E}_{x_{t:T}\\sim p_{\\theta^*}} \\{r(x_T) - \\alpha \\int_{t}^{T} \\sum_{x\\neq y} Q^\\theta_{x,y}(s) - Q^{\\theta_{pre}}_{x,y}(s) + Q_{x,y}^\\theta(s) \\log\\frac{Q^{\\theta_{x,y}}(s)}{Q^{\\theta_{pre}_{x,y}}(s)}\\}ds \\mid X_t = x\\}$\nThis represents the expected return when starting from state $x$ at time $t$ and following the optimal policy. Once the optimal value function is defined, the optimal generator can be expressed in terms of this value function, as shown below. This is derived from the Hamilton-Jacobi-Bellman (HJB) equation in C\u0422\u041c\u0421.\nTheorem 2 (Optimal generator). For $x \\neq y$ ($x,y \\in \\mathcal{X}$), we have\n$Q^*_{xy}(t) = Q^{\\theta_{pre}}_{xy}(t) \\exp(\\{V_t(y) - V_t(x)\\} /\\alpha)$.\nNext, consider an alternative expression for the soft value function, derived using the Kolmogorov backward equations in CTMC. This expression is particularly useful for learning value functions.\nTheorem 3 (Feynman-Kac Formula in CTMC).\n$\\exp(V_t(x)/\\alpha) = \\mathbb{E}_{x_{t:T}\\sim p_{\\theta^{pre}}} [\\exp(r(x_T)/\\alpha) | x_t = x]$\nWith this preparation, we can prove our main theorem, which reduces to Theorem 1 when t = T.\nTheorem 4 (Marginal distribution induced by the optimal generator $Q^{\\theta^*}(t)$). The marginal distribution at time t by (4), $p^*_t \\in \\triangle(\\mathcal{X})$, is proportional to\n$\\exp(V_t(\\cdot)/\\alpha) p^{\\theta_{pre}}(\\cdot).$\nwhere $p^{\\theta_{pre}} \\in \\triangle(\\mathcal{X})$ is a marginal distribution induced by pretrained model at t.\nThis is proved by showing the Kolmogorov forward equation in CTMC: $dp^*_t/dt = Q^{\\theta^*}(t)p^*_t$."}, {"title": "RELATION TO CLASSIFIER GUIDANCE FOR DISCRETE DIFFUSION MODELS", "content": "Now, we derive an alternative fine-tuning-free algorithm by leveraging observations in Section 5.1 for reward maximization. If we can directly obtain the optimal generator $Q^{\\theta^*}$, we can achieve our objective. Theorem 2 suggests that the optimal generator $Q^{\\theta^*}$ is a product of the generator from the pretrained model and the value functions. Although we don't know the exact value functions, they can be learned through regression using Theorem 3 based on\n$\\exp(V_t(x)/\\alpha) = \\underset{g: \\mathcal{X}\\to\\mathbb{R}}{\\operatorname{argmin}} \\mathbb{E}_{x_{t:T}\\sim p_{\\theta^{pre}}} [\\{\\exp(r(x_T)/\\alpha) - g(x_t)\\} ^2].$\nIn practice, while we can't calculate the exact expectation, we can still replace it with its empirical analog. Alternatively, we can approximate it by using a map from $x_t$ to $x_0$ in pretrained models following DPS (Chung et al., 2022) or reconstruction guidance (Ho et al., 2022).\nInterestingly, a similar algorithm was previously proposed by Nisonoff et al. (2024). While Nisonoff et al. (2024) originally focused on conditional generation, their approach can also be applied to reward maximization or vice versa. In their framework for conditional generation, they define $r(x) = \\log p(z|x)$ (e.g., the log-likelihood from a classifier) and set $\\alpha = 1$. By adapting Theorem 2 and 3 to their setting, we obtain:\n$Q^*_{xy}(t) = Q^{\\theta_{pre}}_{xy}(t) \\times p_t(z|y)/p_t(z|x), \\quad p_t(z|x_t) := \\mathbb{E}_{x_{t:T}\\sim P^{\\theta_{pre}}} [p(z|x_T) \\mid x_t].$\nThus, we can rederive the formula in Nisonoff et al. (2024). Here, we also note that this type of result is commonly referred to as the Doob transform in the literature on stochastic processes (Levin and Peres, 2017, Section 17).\nWhile this argument suggests that classifier guidance and RL-based fine-tuning approaches theoretically achieve the same goal in an ideal setting (without function approximation, sampling, or optimization errors), their practical behavior can differ significantly, as we demonstrate in Section 6. At a high level, the advantage of classifier guidance is that it requires no fine-tuning, but the inference time may be significantly longer due to the need to recalculate the generator during inference. Indeed, this classifier guidance requires $O(NM)$ computations of value functions at each step to calculate the normalizing constant. While this can be mitigated using a Taylor approximation, there is no theoretical guarantee for this heuristic in discrete diffusion models. Lastly, learning value functions in classifier guidance can often be practically challenging."}, {"title": "EXPERIMENTS", "content": "Our experiments focus on the design of regulatory DNA sequences for enhancer activity and protein sequences for stability. Our results include comprehensive evaluations, highlighting the ability of DRAKES to produce natural-like sequences while effectively optimizing the desired properties."}, {"title": "BASELINES", "content": "We compare DRAKES against several baseline methods discussed in Section 2, which we summarize below with further details in Appendix D.1.\n\u2022 Guidance-based Methods (CG, SMC, TDS). We compare our approach with representative guidance-based methods, including state-of-the-art classifier guidance (CG) tailored to discrete diffusion models (Nisonoff et al., 2024), SMC-based guidance methods (e.g., Wu et al. (2024)): SMC, where the proposal is a pretrained model and TDS, where the proposal is CG.\n\u2022 Classifier-free Guidance (CFG) (Ho and Salimans, 2022). CFG is trained on labeled datasets with the measured attributes we aim to optimize.\n\u2022 Pretrained. We generated sequences using pretrained models without fine-tuning.\n\u2022 DRAKES w/o KL. This ablation of DRAKES omits the KL regularization term, evaluating how well this term mitigates over-optimization (discussed in Section 4.1)."}, {"title": "REGULATORY DNA SEQUENCE DESIGN", "content": "Here we aim to optimize the activity of regulatory DNA sequences such that they drive gene expression in specific cell types, a critical task for cell and gene therapy (Taskiran et al., 2024).\nDataset and settings. We experiment on a publicly available large-scale enhancer dataset (Gosai et al., 2023), which measures the enhancer activity of ~700k DNA sequences (200-bp length) in human cell lines using massively parallel reporter assays (MPRAs), where the expression driven by each sequence is measured. We pretrain the masked discrete diffusion model (Sahoo et al., 2024) on all the sequences. We then split the dataset and train two reward oracles (one for finetuning and one for evaluation) on each subset, using the Enformer (Avsec et al., 2021) architecture to predict the activity level in the HepG2 cell line. These datasets and reward models are widely used in the literature on computational enhancer design (Lal et al., 2024; Uehara et al., 2024; Sarkar et al., 2024). Detailed information about the pretrained model and reward oracles is in Appendix D.2.\nEvaluations. To comprehensively evaluate each model's performance in enhancer generation, we use the following metrics:\n\u2022 Predicted activity based on the evaluation reward oracle (Pred-Activity). We predict the enhancer activity level in the HepG2 cell line using the reward oracle trained on the evaluation subset. Note"}, {"title": "PROTEIN SEQUENCE DESIGN: OPTIMIZING STABILITY IN INVERSE FOLDING MODEL", "content": "In this task, given a pretrained inverse folding model that generates sequences conditioned on the backbone's conformation (3D structure), our goal is to optimize the stability of these generated sequences, following Widatalla et al. (2024).\nDataset and settings. First, we pretrained an inverse folding model based on the diffusion model (Campbell et al., 2024) and the ProteinMPNN (Dauparas et al., 2022) architecture, using the PDB training set from Dauparas et al. (2022). Next, we trained the reward oracles using a different large-scale protein stability dataset, Megascale (Tsuboyama et al., 2023), which includes stability measurements (i.e., the Gibbs free energy change) for ~1.8M sequence variants from 983 natural and designed domains. Following dataset curation and a train-validation-test splitting procedure from Widatalla et al. (2024) (which leads to ~0.5M sequences on 333 domains) and using the ProteinMPNN architecture, we constructed two reward oracles one for fine-tuning and one for evaluation, that predict stability from the protein sequence and wild-type conformation. Detailed information on the pretrained model and reward oracles is in Appendix D.3.\nEvaluations. We use the following metrics to evaluate the stability of the generated sequences and their ability to fold into the desired structure. During evaluation, we always condition on protein backbone conformations from the test data that are not used during fine-tuning.\n\u2022 Predicted stability on the evaluation reward oracle (Pred-ddG). The evaluation oracle is trained with the full Megascale dataset (train+val+test) to predict protein stability. Conversely, the fine-tuning oracle is trained only with data from the Megascale training set. Thus, during fine-tuning, the algorithms do not encounter any proteins used for evaluation.\n\u2022 Self-consistency RMSD of structures (scRMSD). To assess how well a generated sequence folds into the desired structure, we use ESMFold (Lin et al., 2023) to predict the structures of the generated sequences and calculate their RMSD relative to the wild-type structure (i.e., the original backbone structure we are conditioning on). This is a widely used metric (Campbell et al., 2024; Trippe et al., 2022; Chu et al., 2024).\nFollowing prior works (Campbell et al., 2024; Nisonoff et al., 2024), we calculate the success rate of inverse folding as the ratio of generated sequences with Pred-ddG> 0 and scRMSD< 2."}, {"title": "CONCLUSIONS", "content": "We propose a novel algorithm that incorporates reward maximization into discrete diffusion models, leveraging the Gumbel-Softmax trick to enable differentiable reward backpropagation, and demonstrate its effectiveness in generating DNA and protein sequences optimized for task-specific objectives. For future work, we plan to conduct more extensive in silico validation and pursue wet-lab validation."}, {"title": "POTENTIAL LIMITATIONS", "content": "We have formulated the RL problem, (3), in the context of CTMC. The proposed algorithm in our paper to solve this problem requires reward models to be differentiable. Since differentiable models are necessary when working with experimental offline data, this assumption is not overly restrictive. Moreover, many state-of-the-art sequence models mapping sequences to functions in biology are available today, such as enformer borozi. In cases where creating differentiable models is challenging, we recommend using PPO-based algorithms (Black et al., 2023; Fan et al., 2023) or reward-weighted MLE (Uehara et al., 2024, Section 6.1)."}, {"title": "PROOF OF THEOREMS", "content": ""}, {"title": "PREPARATION", "content": "We prepare two well-known theorems in CTMC for the pedagogic purpose. For example, refer to Yin and Zhang (2012) for the more detailed proof. In these theorems, we suppose we have the CTMC:\n$\\frac{d\\mathbf{p}_t}{dt} = Q(t)\\mathbf{P}_t.$"}, {"title": "Kolmogorov backward equation", "content": "We consider $g(\\cdot,t) = \\mathbb{E}[r(x_T)|x_t = \\cdot]$ where the expectation is taken w.r.t. (5). Then, this function $g : \\mathcal{X} \\times [0,T] \\to \\mathbb{R}$ is characterized by the following ODE:\n$\\frac{dg(x,t)}{dt} = \\sum_{y\\neq x}Q_{x,y}(t)\\{g(x,t) - g(y,t)\\}, \\quad g(x,T) = r(x).$"}, {"title": "PROOF OF THEOREM 2", "content": "We derive the Hamilton-Jacobi-Bellman (HJB) equation in CTMC. For this purpose, we consider the recursive equation:\n$V(x, t)=\\underset{\\theta}{\\text{max}}  \\{\\sum_{y\\neq x}\\{Q^{\\theta}_{x,y}(t) - Q^{\\theta_{pre}}_{x,y}(t) - Q^{\\theta_{x,y}(t)} log \\frac{Q^{\\theta_{x,y}}(t)}{Q^{\\theta_{pre}}_{x,y}(t)}\\}dt+ \\sum_{y\\neq x}\\{ Q^{\\theta}_{x,y}(t)dtV (y,t + dt)\\} + \\{1 + \\sum_{x,z}Q_{x,z}(t)\\}V(x, t + dt)\\}$\nUsing $\\sum_{y\\neq x} Q_{x,y}(t) = 0$, this is equal to\n$V(x,t) = \\underset{\\theta}{\\text{max}}  \\{\\sum_{y\\neq x}\\{Q^{\\theta}_{x,y}(t) - Q^{\\theta_{pre}}_{x,y}(t) - Q^{\\theta_{x,y}(t)} log \\frac{Q^{\\theta_{x,y}}(t)}{Q^{\\theta_{pre}}_{x,y}(t)}\\} dt + V(x, t + dt) + \\sum_{y\\neq x} Q_{x,y}(t)dt\\{V(y, t + dt) - V(x, t + dt)\\}\\}$\nBy taking dt to 0, the above is equal to\n$\\frac{dV(x,t)}{dt}=\\underset{\\theta \\in \\Theta}{\\text{max}}  \\{\\sum_{y\\neq x}\\{Q^{\\theta}_{x,y}(t) - Q^{\\theta_{pre}}_{x,y}(t) - Q^{\\theta_{x,y}(t)} log \\frac{Q^{\\theta_{x,y}}(t)}{Q^{\\theta_{pre}}_{x,y}(t)}\\} + \\sum_{y\\neq x} Q^{\\theta}_{x,y}(t)\\{V(y, t) - V(x,t)\\}\\}$\nThis is the HBJ equation in C\u0422\u041c\u0421.\nFinally, with simple algebra (i.e., taking functional derivative under the constraint 0 = $\\sum_{y\\neq x} Q^{\\theta}_{x,y}(t)$), we can show\n$\\forall x \\neq y; Q^*_{xy}(t) = Q^{\\theta_{pre}}_{xy}(t) \\exp(\\{V(y, t) - V(x,t)\\})."}, {"title": "PROOF OF THEOREM 3", "content": "This theorem is proved by invoking the Kolmogorov backward equation.\nFirst, by plugging\n$\\forall x \\neq y; Q^*_{xy}(t) = Q^{\\theta_{pre}}_{xy}(t) \\exp(\\{V(y, t) - V(x,t)\\}).\ninto (6), we get\n$\\frac{dV (x, t)}{dt} = \\sum_{y\\neq x}Q^{\\theta_{pre}}_{x,y}(t)\\{1 - \\exp(\\{V(y, t) - V(x, t)\\}).\nBy multiplying $\\exp(V(x, t))$ to both sides, it reduces to\n$\\frac{d\\exp(V(x,t))}{dt} = \\sum_{y=x}Q^{\\theta_{pre}}_{x,y}(t)\\{\\exp(V(x, t)) - \\exp(V(y, t))\\}.$\nFurthermore, clearly, $V(x, T) ="}]}