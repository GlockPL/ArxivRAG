{"title": "PIAST: A Multimodal Piano Dataset with Audio, Symbolic and Text", "authors": ["Hayeon Bang", "Eunjin Choi", "Megan Finch", "Seungheon Doh", "Seolhee Lee", "Gyeong-Hoon Lee", "Juhan Nam"], "abstract": "While piano music has become a significant area of study in Music Information Retrieval (MIR), there is a notable lack of datasets for piano solo music with text labels. To address this gap, we present PIAST (PIano dataset with Audio, Symbolic, and Text), a piano music dataset. Utilizing a piano-specific taxonomy of semantic tags, we collected 9,673 tracks from YouTube and added human annotations for 2,023 tracks by music experts, resulting in two subsets: PIAST-YT and PIAST-AT. Both include audio, text, tag annotations, and transcribed MIDI utilizing state-of-the-art piano transcription and beat tracking models. Among many possible tasks with the multimodal dataset, we conduct music tagging and retrieval using both audio and MIDI data and report baseline performances to demonstrate its potential as a valuable resource for MIR research.", "sections": [{"title": "Introduction", "content": "Piano music presents unique opportunities for music research due to its ability to express diverse styles using a single instrument and its superior transcription performance. Given these characteristics, it has become a significant area of study in Music Information Retrieval (MIR), encompassing tasks such as classification (Hung et al., 2021; Chou et al., 2021), and music generation with various conditions (Wu and Yang, 2023; Choi and Lee, 2023). While these tasks require datasets that combine piano audio with various modalities such as MIDI, sheet music, or text, there is a notable scarcity of such comprehensive multimodal piano datasets.\nHowever, existing multimodal music datasets, particularly music-text datasets, rarely focus exclusively on piano music, and piano solo pieces comprise only a small portion of general music-text datasets. For instance, in the ECALS Dataset (Doh et al., 2023), a subset of the Million Song Dataset (Bertin-Mahieux et al., 2011), the number of piano solo tracks is very limited. We observed that excluding tracks tagged with instruments other than the piano or genres that could not be solely represented by the piano, only approximately 0.46% of the entire dataset can be identified as piano solo music.\nSeveral piano datasets, such as MAESTRO (Hawthorne et al., 2019), have been developed in recent years, which provide classical piano performances primarily used for piano transcription. Another classical piano dataset, GiantMIDI (Kong et al., 2022), is also commonly used in transcription tasks. Other datasets like Pop1K7 (Hsiao et al., 2021) focus on the performance generation of pop piano music, while PiJAMA (Edwards et al., 2023) is employed for performer identification tasks with their jazz piano data. However, these datasets are confined to a single genre and lack text labels. This absence of genre diversity within a single dataset and the lack of textual information underscores the need for a piano dataset with text information.\nSome piano datasets contain emotion labels, such as EMOPIA (Hung et al., 2021) and VGMIDI (Ferreira and Whitehead, 2019). However, these datasets are annotated only with emotion information based on either Russell's four quadrants (Hung et al., 2021) or the valence-arousal model (Ferreira and Whitehead, 2019). This limited annotation approach lacks the rich textual descriptions needed for text-based MIR tasks.\nTo address the limitations, we present multimodal piano music data with rich text annotations and transcribed MIDI. To build the dataset, we first created a piano-specific taxonomy with 31 tags that include genre, emotion, mood, and style information to encompass the broad and diverse musical range that the piano can express. Based on this taxonomy, we collected data from YouTube, transcribed it to MIDI format, and conducted an"}, {"title": "Dataset", "content": ""}, {"title": "Taxonomy for Piano Music", "content": "To encompass and precisely define the range of expressions possible in solo piano music, we constructed a comprehensive taxonomy considering genre, emotion, mood, and style tags. We classified genres suitable for solo piano music into four categories: jazz, classical, new-age, and pop piano covers, defining sub-genres within each. The detailed classification of the classical genre was not included in this dataset due to the extensive range and complexity unique to classical music. For emotion and mood taxonomy, we combined vocabularies from four existing music datasets with emotion tags (Turnbull et al., 2007; Rouhi et al., 2019; Aljanaki et al., 2017; Choi et al., 2022)), eliminating overlaps. Seven music experts who majored in music composition rated the tags on a 1-5 Likert scale for their suitability in describing solo piano music. We included only words scoring 3.5 or higher and established a taxonomy of 39 tags. After the first annotation process, we removed tags with excessively high co-occurrence or low selection frequencies, resulting in a final specialized taxonomy of 31 words for piano music."}, {"title": "The PIAST-YT Dataset", "content": "The PIAST-YT dataset comprises approximately 9,673 tracks (1,006 hours) of audio collected from"}, {"title": "Pre-processing", "content": "Audio: To isolate pure piano solo performances, we filtered the data using musicnn (Pons and Serra, 2019), excluding tracks with non-piano sounds in their top 5 tags. Files exceeding 2 hours were removed, and those exceeding 30 minutes were segmented into 10-minute chunks for data consistency. This process reduced the original 1,789 hours of data, about 44%, to 1,006 hours.\nText: The collected text data from YouTube contained diverse and irrelevant information. To extract relevant music-descriptive features, we employed an LLM-based model, specifically Chat-GPT 4-Turbo (Ouyang et al., 2022), chosen for its"}, {"title": "The PIAST-AT Dataset", "content": "Even after processing, the text data in the PIAST-YT exhibited several limitations. Although it was processed using an LLM-based model, it still showed a low correlation with the music content, and some audio files lacked corresponding text data. To address these issues, we created the PIAST-AT, a dataset consisting of piano-specific human-annotated text."}, {"title": "Annotation Process", "content": "We stratified 2,400 samples from audio data of PIAST-YT based on the queries used during the collection process, and extracted a 30-second segment from each sample for human annotation. The process involved 15 music experts (7 jazz and 8 classical musicians) with majors in composition. Each segment was assigned to three annotators using a web-based system. The annotators were divided into five groups tagged with 230 segments. Detailed descriptions and examples were provided to all annotators for each tag to ensure consistency. They were also instructed to exclude samples that did not strictly adhere to solo piano criteria or had subtle mood changes. After two rounds of annotation, we collected tags for 2,023 samples (approximately 17 hours of original audio), with 377 samples excluded through this process."}, {"title": "Dataset Analysis & Tag Consensus", "content": "shows the distribution of tags\nin the PIAST-AT dataset, categorized into\nMood/Emotion, Genre, and Style. The Style\ncategory includes tags associated with performance\ndifficulty and tempo-related mood. Due to the\ninherent imbalance of collected audio, there is also\na disparity in the frequency of sub-genre tags.\nThe dataset contains the consensus degree\namong the annotators. To leverage this informa-\ntion, we generated hierarchical captions based on\nthe level of agreement as follows:\n\"This is definitely Jazz genre; (3 agree-\nments)\nalso Speedy style; also Playful mood; (2\nagreements)\npotentially Latin genre; potentially Easy\nstyle; potentially Bright, Happy, Cute\nmood of piano music. (1 agreement)\"\nThe PIAST-AT dataset comprises audio, tran-\nscribed MIDI, and text annotations (tags and cap-\ntions), offering a rich representation of musical\ncharacteristics and annotator consensus."}, {"title": "Piano Music Classification", "content": "In this section, we present the application of our\nproposed dataset for piano music annotation and\nretrieval tasks in both the audio and MIDI do-\nmain. We employed a two-stage framework: 1)\npre-training and 2) transfer learning. For pre-\ntraining, we used the PIAST-YT dataset to train"}, {"title": "Pre-training and Transfer Learning", "content": "To develop a piano-specific pre-trained model, we\nextracted embeddings from audio, MIDI, and text\nmodality encoders. We applied contrastive loss to\nmaximize similarity between corresponding pairs\n(audio-text or MIDI-text) while minimizing simi-\nlarity with in-batch negative samples. Following\nprevious studies (Huang et al., 2022; Manco et al.,\n2022; Doh et al., 2024b), each encoder consists\nof a modality-specific backbone, a linear projec-\ntion layer, and an \n12 normalization layer. We used\na modified ResNet-50 (Radford et al., 2021) for\naudio, ROBERTa (Liu et al., 2019) for text, and\nMidiBERT-Piano (Chou et al., 2021) with average\npooling for MIDI.\nFor the classification model, we employed the\nprobing protocol (Doh et al., 2023; Castellon et al.,\n2021). We used the pre-trained audio and MIDI\nencoders as frozen feature extractors and trained\nlinear models and one-layer MLPs as shallow clas-\nsifiers on top of them, with 512 hidden states and\nReLU activation."}, {"title": "Implementation Details", "content": "We processed the input data for pre-training and\ntransfer learning as follows: Audio inputs were 10-\nsecond signals sampled at 22050 Hz, converted to\nlog-mel spectrograms with 128 mel bins using a\n1024-point FFT with a Hann window and a 10 ms\nhop size. For MIDI, pre-processed MIDI files were\nconverted to the CP (compound word) representa-\ntion (Hsiao et al., 2021) and fed into a 12-layer\nBERT with a maximum sequence length of 512.\nAll models were optimized using AdamW with a\n5e-5 learning rate, and a dropout rate of 0.4 applied\nto the audio classification model. We used batch\nsizes of 128 for audio and 48 for MIDI data during\npre-training. Pre-training models were trained for\n150 epochs, while classification models ran for 700\nepochs, with the best model selected based on vali-\ndation loss. The PIAST-AT dataset was split into\n80% for training, 10% for validation, and 10% for\ntesting sets."}, {"title": "Evaluation & Results", "content": "We evaluated our classification models on two\ntasks: the annotation task, which involves finding\nappropriate tags for given music, and the retrieval\ntask, which focuses on finding suitable music for\nprovided tags. Following previous studies (Choi\net al., 2019; Doh et al., 2024a), we employed the\narea under the ROC and PR curves averaged over\ninstances as evaluation metrics for the annotation\ntask. The retrieval task was assessed using the\narea under the ROC and PR curves averaged over\nlabels. To demonstrate the effectiveness of the\nproposed large PIAST-YT dataset, we used a su-\npervised model trained exclusively on the smaller\nPIAST-AT dataset as the baseline model.\ncompares the annotation and retrieval\nperformance across 1) audio and MIDI modali-\nties, and 2) the supervised versus pre-train and\ntransfer framework. The MIDI model consistently\noutperformed the audio model across both tasks.\nPre-training with PIAST-YT improved the perfor-\nmance of both models on all metrics, demonstrat-\ning its effectiveness. This pre-training approach\nled to superior performance in both music-to-tag\nand tag-to-music tasks."}, {"title": "Conclusion", "content": "In this paper, we introduced PIAST, a piano dataset\nwith audio, symbolic and text. Our experiments\ndemonstrated the dataset's effectiveness for piano\nmusic annotation and retrieval tasks, showing im-\nprovements with pre-training. The PIAST dataset\nsupports various applications, including improved\nmusic retrieval, text-based music generation, mu-\nsic analysis, and emotion/genre classification. To\nfurther enhance the dataset, our future work will\naddress tag imbalances by adding more samples\nand incorporating additional processed data such\nas lead sheets and chord annotations."}]}