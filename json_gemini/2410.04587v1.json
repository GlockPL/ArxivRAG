{"title": "Hammer: Robust Function-Calling for On-Device Language Models via Function Masking", "authors": ["Qiqiang Lin", "Muning Wen", "Qiuying Peng", "Guanyu Nie", "Junwei Liao", "Jun Wang", "Xiaoyun Mo", "Jiamu Zhou", "Cheng Cheng", "Yin Zhao", "Jun Wang", "Weinan Zhang"], "abstract": "Large language models have demonstrated impressive value in performing as autonomous agents when equipped with external tools and API calls. Nonetheless, effectively harnessing their potential for executing complex tasks crucially relies on enhancements in their function-calling capabilities. This paper identifies a critical gap in existing function-calling models, where performance varies significantly across benchmarks, often due to being misled by specific naming conventions. To address such an issue, we introduce Hammer, a novel family of foundation models specifically engineered for on-device function calling. Hammer employs an augmented dataset that enhances models' sensitivity to irrelevant functions and incorporates function masking techniques to minimize misleading. Our empirical evaluations reveal that Hammer not only outperforms larger models but also demonstrates robust generalization across diverse benchmarks, achieving state-of-the-art results. Our open-source contributions include a specialized dataset for irrelevance detection, a tuning framework for enhanced generalization, and the Hammer models, establishing a new standard for function-calling performance.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have demonstrated remarkable proficiency in addressing a wide range of natural language processing tasks [1], as well as in handling long-context reasoning and complex planning [2]. The use of LLMs as autonomous agents to assist humans in completing intricate tasks is increasingly in demand and is now more feasible from a technical standpoint than ever before [3]. To fully capitalize on the potential of LLMs as autonomous agents, it is crucial for these models to accurately identify and utilize external tools or application programming interfaces (APIs), thereby enabling them to effectively execute complex tasks [4, 5]. Central to this capability is the model's ability to select appropriate functions from a given set of options, provide accurate input arguments, and ultimately fulfill the user's intent. Furthermore, in scenarios where no suitable function exists within the available options, the model must have the ability to decline the task, rather than making incorrect attempts [5]."}, {"title": "3 Problem Statement and Analysis", "content": "This section aims to introduce and analyze the common challenges that function-calling models encounter in practical applications. Through this analysis, we seek to identify methods to enhance models' stability and generalization capabilities in real-world scenarios."}, {"title": "3.1 Misleadingness by Function Name and Parameter Name", "content": "As illustrated in Figure 1, the definition of a function typically comprises the function name, parameter names, and descriptions. The format of function and parameter names is often quite compact, e.g., cal_sum or max_value, and influenced by the designer's personal style and preferences. When a model attempts to infer the function's purpose solely from the function name, this compactness can lead to ambiguities, misguiding the model's selection, particularly in the presence of complex functionalities [27]. For instance, a function named parse_data might be intended for parsing JSON data, but the same name could refer to parsing CSV files in a different context, leading to potential misinterpretations. Similarly, when deducing the usage of parameters based on their names, models may be misled by the historical usage of similarly named parameters in the training dataset. More specifically, these misleading scenarios can be categorized into several cases.\nMisled by Function Names. When a user intent aligns closely with a function name present in the training labels, the model may incorrectly prioritize that function from the candidate list during testing, even if its functionality diverges significantly from the intended operation. For example, if a function named fetch_data is included in the training pairs for retrieving user data from a database, but in the testing set, a function with the same name retrieves data from an external API, the model may erroneously select it based solely on the name.\nMisled by Parameter Names. In instances where the functionality and descriptions of parameters change within the testing environment, the model frequently clings to its original patterns of parameter usage, resulting in incorrect function calls. For instance, if a function's parameter timeout is expected to be an integer representing seconds in one context, but in another context, it is defined as a string in the format \"10s\", the model's reliance on the original integer format may lead to erroneous calls.\nDisturbed by Naming Preferences. The model's robustness can diminish when the naming conven-tions of functions or parameters in the testing environment diverge from those in the training dataset. Variations, such as discrepancies between CamelCase and snake_case may adversely lower the model's confidence, as an on-device lightweight model may struggle to generalize across different naming styles."}, {"title": "3.2 The Impact of Excessive Focus on the Naming", "content": "To investigate the extent to which existing models rely on function and parameter names and corresponding impact, we conducted a case study using the xLAM-1B-fc model on the Seal-Tools benchmark. Specifically, we masked the function and parameter names in the test set, i.e., replaced them with random strings, and observed how the model's performance changed. As shown in Figure 2, after masking the function and parameter names, even though the descriptions contained all necessary information about the function's purpose and usage, the performance of xLAM-1B-fc dropped significantly. This result confirms the model's overreliance on function and parameter names, highlighting the potential risks this behavior may pose in real-world applications."}, {"title": "4 Methodology", "content": "In this section, we describe our detailed approach and augmented dataset to fine-tune the Hammers, a series of robust language models designed for function-calling."}, {"title": "4.1 Function Masking", "content": "In light of the analysis in Section 3, a direct approach to mitigate these issues involves minimizing the interference from function names and parameter names, while enforcing the model to comprehend the functionality and usage of candidates based on their descriptions. In contrast, the descriptions provide a more flexible natural language explanation, often encapsulating the information that function and parameter names aim to convey. While descriptions can also reflect the designer's personal style to some extent, they tend to be more accurate and detailed, thus reducing the likelihood of ambiguity or misguidance. Consequently, when training function-calling models, we face the challenge of not knowing the preferences or naming styles of function designers in real-world applications. Thus, it is reasonable to expect that the trained model should understand the function's purpose and usage through its description rather than attempting to infer functionality based on potentially ambiguous, compact components such as function and parameter names."}, {"title": "4.2 Irrelevance-Augmented Dataset", "content": "During the fine-tuning process using the xlam-function-calling-60k dataset, we identified a concerning inverse relationship between the model's ability to accurately execute function calls and its capacity for irrelevance detection-specifically, the ability to assess whether there exists no function call in the candidate set aligns with the user's intent. The details of this observation are discussed further in Section 5.6. This phenomenon indicates that, while fine-tuning lightweight language models on datasets specialized in function selection can improve their accuracy in choosing appropriate functions from a predefined set, it may unintentionally impair their ability to detect irrelevance. As a result, models might generate inappropriate function calls, even in the absence of valid options.\nTo address this issue, we propose an irrelevance-augmented dataset. This augmentation, applied to the original xlam-function-calling-60k dataset, incorporates 7,500 examples sampled from the original training set, in which the correct functions are deliberately excluded from the candidate list, and the labels are replaced with empty lists. By exposing the model to more instances requiring irrelevance detection, we aim to enhance its ability to discern when to abstain from making function calls, thus promoting more judicious function selection."}, {"title": "5 Evaluation", "content": "In this section, we show the superiority of our Hammers in performance and robustness across various benchmarks, and in-depth analysis to verify the effectiveness of our augmented dataset and approach."}, {"title": "5.1 Experimental Setup", "content": "Benchmarks. To assess the generalizability of Hammers, we conducted evaluations using a variety of function-calling benchmarks, all of which represent out-of-domain challenges for our model.The Berkeley Function-Calling Leaderboard (BFCL) [15] provides a comprehensive dataset comprising over 1,700 instances. It covers tasks such as Simple Function, Multiple Function, Parallel Function, and Parallel Multiple Function for Python, as well as function relevance detection, REST API, JavaScript, and Java for non-Python environments. API-Bank [6], consisting of 314 tool-use dialogues and 753 API calls, evaluates models' ability to correctly invoke a known API (L-1) based on a query, and to retrieve and call APIs from a candidate list (L-2). Similarly, Nexus Raven API Evaluation [17] offers 318 test examples across 65 distinct APIs, contributing further to the evaluation of function-calling capabilities. Tool-Alpaca [16] employs a synthetic data generation method, featuring 271 tool-use instances in 50 categories. For evaluation, we utilized 100 simulated test examples from this dataset, similar to Nexus Raven. Lastly, Seal-Tools [7] represents one of the most extensive and recent benchmarks, with 4,076 automatically generated APIs across various life domains. As one of the newest benchmarks, Seal-Tools presents a lower risk of data leakage.\nEvaluation Metrics. BFCL assesses function-calling models through two primary evaluation methods: Abstract Syntax Tree (AST) Evaluation and Executable Function Evaluation [15]. The AST evaluation emphasizes the syntactic precision of the generated function calls, ensuring that the model's output adheres to a predefined function documentation in terms of structure and parameters. This includes verifying the correctness of function names, required parameters, and appropriate data types. In contrast, Executable Function Evaluation takes this further by executing the generated function calls to assess their functional accuracy. This evaluation ensures that the functions not only compile but also run correctly, producing the intended outputs, which is vital for real-world applications. In addition to BFCL, we incorporated F1 scores to measure exact matches of API names and parameters to evaluate the models on alternative benchmarks [4]."}, {"title": "5.2 Overall Performance on Various Benchmarks", "content": "We first evaluate Hammer series on BFCL. Table 2 indicates that within the BFCL framework, our Hammer series consistently achieves corresponding sota performance at comparable scales, particularly Hammer-7B, whose overall performance ranks second only to the proprietary GPT-4. In addition, we evaluated our Hammer series (1.5b, 4b, 7b) on other academic benchmarks to further show our model's generalization ability. Upon observing Hammer's performance across various benchmarks unrelated to the xlam-function-calling-60k Datasets, as shown in Table 3, we find that Hammer demonstrates remarkably stable performance, which indicates the robustness of Hammers."}, {"title": "5.3 Detailed Performance on Different Types of Function Calling", "content": "In this section, we closely examine the performance of Hammer across different types of function-calling tasks, as exampled in Figure 4, and detailed in Appendix B."}, {"title": "5.4 Ablation on Different Base Models", "content": "To further validate the effectiveness of our augmented data and tuning technique, we applied our approach to two different sizes of the deepseek-coder models, in addition to the Qwen series. The results are illustrated in Table 5. Upon examining the results presented in the table, we first note that the fine-tuned Hammer model exhibits a notable performance improvement compared to the vanilla Qwen model [28, 29], thereby confirming the efficacy of our data and methodology on the Qwen architecture. Subsequently, we compared the deepseek-coder model [30] before and after fine-tuning; the fine-tuned variant, referred to as deepseek-coder-Hammer, demonstrates significant enhancements over the vanilla model, despite the poor performance of deepseek-coder prior to fine-tuning. This suggests that our methodology is not exclusively applicable to the Qwen model. Furthermore, it is noteworthy that the performance of the deepseek-coder-Hammer, fine-tuned using our approach, significantly surpasses that of the xLAM model, which was also based on deepseek-coder-instruct and obtained through SFT with the xlam-function-calling-60k dataset. This further underscores the superiority of our proposed method."}, {"title": "5.5 Ablation on Different Masking Ratio", "content": "To further investigate the impact of various function masking ratios on model performance, we designed an ablation study focused on the masking ratio. We systematically applied different masking ratios while fine-tuning the Qwen2-1.5B model on the Seal-Tools training dataset for one epoch. Subsequently, we evaluated the performance of the models trained with different masking ratios on the test sets of both Seal-Tools and API-Bank. This allowed us to observe and analyze the performance across both same-task and cross-task scenarios."}, {"title": "5.6 Ablation on Different Proportions of Irrelevance-Augmented Data", "content": "In Section 4.1, we discussed the trade-off between the model's performance in irrelevance detection and function-calling tasks, which motivated the design of the irrelevance-augmented dataset. To further explore the relationship between these two aspects, we conducted an ablation study on the data ratio of irrelevance-augmented data compared to the original xlam-function-calling data during training. In this ablation experiment, we sampled a total of 10,000 instances from both datasets at varying data proportions to fine-tune the Qwen2-1.5B-Instruct model and then exam on the BFCL testset, observing the changes in the model's irrelevance detection and function-calling capabilities across different ratios."}, {"title": "6 Conclusion", "content": "In conclusion, our exploration of function-calling models reveals significant challenges related to performance inconsistency across different benchmarks, primarily driven by misleading from specific naming conventions. By introducing the Hammer family of models, we provide a robust solution that enhances generalization capabilities through a carefully constructed augmented dataset and innovative function masking techniques. The superior performance of Hammer on a variety of benchmarks demonstrates its potential for practical application in real-world scenarios."}, {"title": "B Different Types of Function-calling Tasks", "content": "Simple: This query style includes straightforward scenarios where a single function call is made based on the user's input with a single provided JSON format API description.\nMultiple: In this style, user queries could be answered by one of several function calls. The challenge lies in selecting the most appropriate function from multiple provided APIs. It represents one of the most common real-world use cases.\nParallel: This query style requires executing multiple function calls simultaneously in response to a single user query, which may consist of one or more sentences but with only one API provided.\nParallel Multiple: This query style combines the parallel and multiple categories, where multiple function and API documents are provided, and each function call might be invoked multiple times based on the query's requirements."}, {"title": "C Example Input to Models with Function Masking", "content": "The prompted inputs to models in our experiment are exampled as:\n[BEGIN OF TASK INSTRUCTION]\nYou are a tool calling assistant. In order to complete the user's request, you need to select one or more appropriate tools from the following tools and fill in the correct values for the tool parameters. Your specific tasks are:\n1. Make one or more function/tool calls to meet the request based on the question.\n2. If none of the function can be used, point it out and refuse to answer.\n3. If the given question lacks the parameters required by the function, also point it out.\n[END OF TASK INSTRUCTION]\n[BEGIN OF AVAILABLE TOOLS]\n[\n{\n \"name\": \"Lx0m64zLyg\",\n \"description\": \"Gets hourly weather forecast information for given geographical coordinates using the RapidAPI service.\",\n \"parameters\": {\n \"TDpjPd\": {\n \"description\": \"The latitude of the geographical location.\",\n \"type\": \"int\",\n \"default\": 46.95828\n },\n }\n },\n {\n \"78th2U31Fj\": {\n \"description\": \"The longitude of the geographical location.\",\n \"type\": \"int\",\n \"default\": 10.87152\n }\n }\n },\n {\n \"name\": \"WoDdNSe7e7K5\",\n \"description\": \"Fetches weather updates for a given city using the RapidAPI Weather API.\",\n \"parameters\": {\n \"LzZsvxUC\": {\n \"description\": \"The name of the city for which to retrieve weather information.\",\n \"type\": \"str\",\n \"default\": \"London\"\n }\n }\n },\n {\n \"name\": \"CBrCNmwOERb\",\n \"description\": \"Fetches the hourly weather forecast for a given location using the RapidAPI service.\",\n \"parameters\":{\n \"TDEJ.ZwMt\": {\n \"description\": \"The name of the location for which to retrieve the hourly weather forecast.\",\n \"type\": \"str\",\n \"default\": \"Berlin\"\n }\n }\n },\n {\n \"name\": \"1YTQVXkwLY\",\n \"description\": \"Returns an air quality forecast for a given location.\",\n \"parameters\": {\n \"2bkgDA\": {\n \"description\": \"The latitude of the location for which the air quality forecast is to be retrieved.\",\n \"type\": \"int\",\n \"default\": \"35.779\"\n },\n \"DQi.ReZ16\": {\n \"description\": \"The longitude of the location for which the air quality forecast is to be retrieved.\",\n \"type\": \"int\",\n \"default\": \"-78.638\"\n },\n \"hF.1\": {\n \"description\": \"The number of hours for which the forecast is to be retrieved (default is 72).\",\n \"type\": \"int\",\n \"default\": \"72\"\n }\n }\n }\n]"}]}