{"title": "Explaining the Unexplained: Revealing Hidden Correlations for Better Interpretability", "authors": ["Wen-Dong Jiang", "Chih-Yung Chang", "Show-Jane Yen", "Diptendu Sinha Roy"], "abstract": "Deep learning has achieved remarkable success in processing and managing unstructured data. However, its \"black box\" nature imposes significant limitations, particularly in sensitive application domains. While existing interpretable machine learning methods address some of these issues, they often fail to adequately consider feature correlations and provide insufficient evaluation of model decision paths. To overcome these challenges, this paper introduces Real Explainer (RealExp), an interpretability computation method that decouples the Shapley Value into individual feature importance and feature correlation importance. By incorporating feature similarity computations, RealExp enhances interpretability by precisely quantifying both individual feature contributions and their interactions, leading to more reliable and nuanced explanations. Additionally, this paper proposes a novel interpretability evaluation criterion focused on elucidating the decision paths of deep learning models, going beyond traditional accuracy-based metrics. Experimental validations on two unstructured data tasks image classification and text sentiment analysis-demonstrate that RealExp significantly outperforms existing methods in interpretability. Case studies further illustrate its practical value: in image classification, RealExp aids in selecting suitable pre-trained models for specific tasks from an interpretability perspective; in text classification, it enables the optimization of models and approximates the performance of a fine-tuned GPT-Ada model using traditional bag-of-words approaches.", "sections": [{"title": "1. Introduction", "content": "Thanks to the rapid advancement of computer hardware, deep learning has made significant progress in the application of unstructured data, such as images (Cao & Chen, 2025) and text (Li et al., 2024). Specifically, the success of representation learning (Wang & Lian, 2025; Zhang et al., 2025) has gradually replaced the earlier approaches of transforming unstructured data into structured formats. The key to the success of representation learning lies in leveraging a large number of parameters for backpropagation, enabling the model to adapt to data with non-normal distributions. Although models based on backpropagation neural networks (Yang et al., 2019; Banerjee et al., 2023) have achieved significant technical advancements, their application in many sensitive domains, such as medicine (Zhang et al., 2025) and industrial inspection (Rathee et al., 2021), still faces considerable challenges due to the difficulty in understanding the basis of their decision-making.\nExplainable Artificial Intelligence (XAI) aims to reveal the inner mechanisms of neural network decisions, thereby making these models more reliable for applications in sensitive domains. In recent years, several studies (Li et al., 2025; Jing et al., 2025; Liu et al., 2024; Guan et al., 2024) have focused on injecting explainability into deep learning models and using various visualization techniques to explain the decisions of these \"black box\" models. While these models have achieved a certain level of interpretability, two pressing issues remain (Huang & Marques, 2023; Huang & Marques, 2024): first, whether the correlations between different attributes are correctly evaluated, and second, whether the model's decision-making pathway truly aligns with human reasoning, even when the model's understanding appears consistent with user expectations.\nThe first issue is that feature correlation was not sufficiently considered. Fig. 1 provides an example: as shown in Fig. 1, the left side depicts a binary tree with four nodes, {x1}, {x2},{X3} and {x4}, each representing a feature. The values on the edges, such as \u201c\u2208 {0}", "\u2208 {1}\u201d, and \u201c\u2208 {0,2}\u201d, indicate the branching directions determined by the condition of each feature. The values at the leaf nodes (e.g. \"6\"": ") are the predicted outcomes of the model along that path.\nBy observing the decision tree, the importance of each feature can be inferred: features that influence more splits and are closer to the root node have a greater impact on the overall decision. In this tree, feature x\u2081 is at the root node, directly deciding which child node to proceed to, thus indicating that X\u2081 is the most important feature in this tree. Overall, the feature importance ranking can be represented as: X1 > X4 > X2 > X3.\nHowever, when using explainable machine learning methods such as Shapley Value (Lundberg & Lee, 2017) (higher Shapley Values indicate higher importance) to compute feature importance, an incorrect result was obtained, with the rank being X3 > X1 > X2 > X4. There are two main reasons for this discrepancy:"}, {"title": null, "content": "First, Shapley Value does not sufficiently account for feature correlation during calculation. It primarily considers the interaction between features without considering the traditional individual importance of each feature. Secondly, when there is redundancy or strong collinearity between features, SHAP distributes the contribution among them, leading to lower Shapley Values for some features, even if they occupy important positions in the decision tree. The specific proof of this issue will be described in detail in Section 3.1.\nThe second issue is that existing explainable methods often only consider whether the model's decision outcome is consistent with that of humans, without considering whether the decision-making path of the model matches that of human reasoning. Fig. 2 shows a simple example, where from left to right are the original image, the decision path as perceived by the model (Jiang et al., 2024), and the decision path as perceived by the expert. As shown in Fig.2, the areas identified as violent by the model are consistent with the expert's assessment, but the reason behind the model's decision involves a sequence from black pants to the white shirt, and then to the fighting action. This reasoning is precisely opposite to the expert's view of the violent region.\nThe goal of explainable methods is to provide domain experts with a reasonable basis for deploying models in high-risk scenarios. Therefore, even if the areas identified by the model match those identified by the expert, or even match perfectly at the pixel level, it may still be insufficient to establish trust for high-risk tasks. Clearly, such an explanation is inadequate.\nTo address the aforementioned issues, this paper proposes a method for interpretability computation called Real Explainer (RealExp). Specifically, this study revisits the calculation of Shapley Values, decoupling the original Shapley Value's average contribution into independent contributions and interaction contributions. Additionally, an interpretability evaluation approach combining expert annotations and the tau coefficient is proposed to further assess deep learning models.\nIn practice, the proposed RealExp is designed as an interpretable approach for surrogate models, making it capable of explaining any deep learning model. For example, in Image classification task, given an image that requires interpretation and a model, the RealExp masks certain regions of the image at a fixed ratio to generate a series of perturbed images. These perturbed images are then fed into the model to calculate changes in the model's predictions. Subsequently, RealExp fits the output of these new samples using an ensemble tree model, thereby efficiently approximating the marginal contribution of each feature. To ensure the accuracy of the fitted model results, RealExp utilizes an exponential kernel function to fine-tune the weight given to different feature combinations, where the weight is determined"}, {"title": null, "content": "by the similarity between the perturbed samples and the original sample. The higher the similarity, the greater the weight assigned. The feature importance scores obtained from the fitted tree model represent the final feature attributions, which reflect each feature's contribution to the model's prediction.\nThis study extends previous research (Lee et al., 2024) by \"Not Just Explain, But Explain Well: Interpretable Machine Learning Based on Ensemble Trees.\" The primary contributions of this paper can be summarized in three key aspects:\n1.\tProposing a New Explainability Computation Method: This paper introduces the RealExp method to address the lack of trust in existing explainability methods in high-risk scenarios. RealExp revisits the computation of Shapley Values by decoupling the average contribution into independent contributions and interaction contributions, thereby refining the understanding of feature contributions.\n2.\tDesigning an Explainability Method for Surrogate Models: RealExp serves as an interpretability tool for surrogate models and can explain any deep learning model. Specifically, RealExp generates perturbed images and uses an ensemble tree model to fit the output, calculating the marginal contribution of each feature to more efficiently and accurately assess the model's dependence on different features.\n3.\tEvaluation Method Combining Expert Annotations and Tau Coefficient: To further evaluate the interpretability of deep learning models, this paper proposes an evaluation method that combines expert annotations with the tau coefficient. This evaluation method aims to enhance the reliability of explanations, ensuring that the model interpretations align better with domain experts' needs, especially to increase expert trust in the model in high-risk scenarios.\nThe remainder of the paper is organized as follows. Section 2 discusses and compares previous relevant studies. Section 3 details the proposed RealExp. Section 4 describes the assumptions and problem formulation in detail. Section 5 provides experiments and performance evaluation. The conclusions are discussed in Section 6."}, {"title": "2. Related Work", "content": "This section will review research works related to Interpretable machine learning, divided into three parts: the first part focuses on perturbation-based explainable methods; the second part covers gradient-based visualization methods for interpretability; and the third part discusses embedding-based interpretability methods."}, {"title": "2.1. Perturbation-Based Explainable Methods", "content": "These methods explain models by randomly modifying parts of a single test data entry and re- inputting the modified data into the model to obtain new predictions. By comparing the changes in prediction values, the importance of features in the test data could be estimated. The core idea of these methods was to generate perturbed data and observe the sensitivity of the model's output to input changes, thereby inferring the importance of input features.\nFor example, (Zeiler & Fergus, 2014) slid a gray box window across images and observed changes in the model's predictions to identify the regions the model considered most important. This method intuitively revealed the areas of input data that the model focused on by occluding different regions. (Ribeiro et al., 2016) randomly masked specific regions of images or text to generate new predictions and used the slopes of linear regression models constructed from these predictions to explain the decision-making process of the model. This method was applicable not only to visual data but also to text and other data types. (Petsiuk et al., 2018) further extended this approach by generating inputs with random masks to estimate feature importance, emphasizing the broad applicability of perturbation-based methods in feature evaluation.\nAdditionally, (Ancona et al., 2019) proposed an approximate explanation method based on Shapley Values to assess feature importance in deep learning models. This method adopted the concept of cooperative game theory to provide a mathematically rigorous explanation framework. (Lundberg & Lee, 2017) suggested quantifying differences between random samples and original samples by calculating the probability derivatives of Shapley Values to explore the model's decision logic. (Zafer & Khan, 2021) used clustering methods to generate perturbed samples, enhancing the interpretability of models. (Liu et al., 2024) combined interpretable tree structures with SHAP value analysis to provide more structured explanations from a tree-based model perspective. (David et al., 2024) developed the AcME method, which optimized the computational efficiency of SHAP values, significantly improving the speed and practicality of the explanation process. (Li et al., 2023) proposed GLIME, which combined an improved Elastic-Net estimator to refine local explanation results by balancing global explanation distances and sparsity/feature selection in the explanations. (Tan et al.,2024) introduced GLIME with a local and unbiased sampling distribution to generate explanations with higher local fidelity.\nAlthough these mask perturbation-based methods are applicable to any black-box model and demonstrate good flexibility in feature importance analysis, two main issues remain: 1. Limitations of the Proxy Model: When these methods rely on proxy models, such as linear regression, for explanations, the inherent flaws of the proxy model can affect the accuracy and applicability of the explanations. For example: Limitations of the Linear Assumption: Linear models (e.g., linear regression) assume a linear relationship between input features and output, while real-world data and models often exhibit complex nonlinear characteristics. Such linear assumptions may fail to capture the complexity of the model's actual decision-making process, thereby reducing the reliability of the explanation. Feature Collinearity Issues: When there is collinearity among input features, linear models struggle to distinguish the independent contributions of each feature, which can lead to bias in feature importance assessments, making the explanations unable to accurately reflect the model's actual behavior.\nAdditionally, the applicability of these methods is constrained by certain additional factors. For instance, in the case of high-dimensional data, the masking or occlusion process may introduce extra noise, making the results more difficult to interpret."}, {"title": "2.2. Gradient-Based Visualization Methods", "content": "The core purpose of these methods is to determine which input features have a significant impact on the model's output through one or more backpropagation processes. By analyzing the gradient information of the model, these methods attempt to reveal which parts of the input data are most important to the model's prediction.\nFor example, (Zhang et al., 2018) used the backpropagation process to trace the activation input pixels of specific nodes or classes within the network, in order to explain the model's decision-making process. This method intuitively demonstrates the correlation between input features and the network structure; (Selvaraju et al., 2017) introduced the Grad-CAM method, which generates visual heatmaps by leveraging gradient information from convolutional layers, highlighting regions crucial to the model's prediction, thus allowing users to intuitively understand the key parts the model focuses on; (Binder et al., 2016) proposed a technique that proportionally backpropagates output errors to each input feature, quantifying the impact of each feature on the prediction and representing it as feature importance; (Shrikumar et al., 2017) assigned \"importance scores\" to each feature by comparing the contribution of each input feature with that of a reference input, thereby further improving the accuracy of interpretations; (Smilkov et al., 2017) smoothed the gradient maps by adding random noise to the input image and calculating the average gradient, making the generated visualizations more stable and clearer; (Zahavy et al., 2016) highlighted important pixels in the input data by generating Jacobian Saliency Maps, revealing which regions have the greatest influence on the model's value or behavior predictions; (Bakchy et al., 2024) further developed gradient-based interpretability methods by proposing a lightweight CNN-based model, integrating the use of Grad-CAM to enhance usability through fast computation and efficient interpretation; (Niaz et al., 2024) proposed the Increment-CAM method, which combines Grad-CAM computation, Score-CAM computation with a modified approach, heatmap merging, and regularization to achieve model interpretability; (Li et al., 2024) proposed the Contrast- Ranking Class Activation Mapping (CR-CAM), for use with CNNs and GCNs to generate class activation maps. To address the similarity between different categories, the ranking block adopts a comparative approach to measure the distances between feature mappings in manifold space, thereby reducing the weights of the surrounding regions.\nAlthough these methods provide intuitive and effective visual tools to explain deep learning models and reveal the internal mechanisms of model decisions, they still face three main challenges: The first is Difficulty in Providing Clear Explanations for Models with a Large Number of Parameters: As the scale of deep learning models continues to expand, the number of parameters grows exponentially. This complexity can cause gradient information to become dispersed or blurred during propagation, making it difficult to clarify the contribution of a single input feature. For example, in deep convolutional neural networks, as the number of layers increases, issues like vanishing gradients or exploding gradients may occur, thereby reducing the accuracy of backpropagation-based explanations. Additionally, parameter redundancy and model overfitting may interfere with the interpretative process, making it challenging to generate feature importance maps that have practical significance. Secondly, Mainly Applicable to Convolutional Neural Networks or Graph Neural Networks, Limited to Image Data: These methods are mostly designed for CNN and their variants, which perform well in interpreting image data. However, for other types of data, such as time-series data, text data, or tabular data, gradient-based methods are often difficult to apply directly. For example, in natural language processing tasks, the inputs are typically"}, {"title": null, "content": "discrete words or characters, which makes it challenging to use gradient visualizations to clearly identify the model's focus on a particular input feature. Furthermore, for tabular or structured data, gradient methods lack adaptability and flexibility, making it difficult to provide effective interpretations. Lastly, Inability to Provide Global Explanations for the Dataset: Gradient-based methods typically focus on providing local explanations for specific data instances, helping to understand the model's decision- making process for a particular input. However, these methods struggle to provide a global explanation for the entire dataset. For example, they cannot reveal differences in model performance across different data subsets, nor can they summarize the global patterns captured by the model. This limitation is significant for applications that require a comprehensive understanding of model behavior, such as medical diagnosis and financial decision-making."}, {"title": "2.3. Embedding-Based Visualization Methods", "content": "These methods enhance data interpretability through nonlinear dimensionality reduction techniques. The basic idea is to project the complex structure of high-dimensional data into a lower-dimensional space, making the internal relationships between models or data easier for humans to understand. These techniques are typically based on similarity in high-dimensional space, mapping data points to nearby locations in the lower-dimensional space to reveal inherent patterns or distribution characteristics in the data.\nFor example, (Mnih et al.,2015) demonstrated how to use t-SNE, a dimensionality reduction tool, to visualize complex high-dimensional data as points on a two-dimensional plane. This method minimizes the divergence between the probability distribution of high-dimensional data points and the probability distribution of the low-dimensional mapped points, allowing the distribution of data in two dimensions to reflect its original structure. This intuitive visualization makes it possible to analyze the internal relationships of high-dimensional data, such as the distribution in clustering or classification. (Engel & Mannor,2001) proposed an embedding-based mapping method that measures the similarity between data points using transition probabilities. The embedding map generated by this method can effectively show the relationships among data points in a specific feature space, providing a new perspective for interpreting model behavior. (Bibal et al.,2023) proposed DT-SNE, an improved version of t-SNE based on a decision tree model. DT-SNE integrates structural information from decision trees, resulting in dimensionality reduction that better conforms to the original data distribution and characteristics. Furthermore, this approach adds constraints to interpretative tasks, providing more interpretable results compared to traditional t-SNE.\nThese methods effectively utilize statistical means for interpretation but face challenges when dealing with large-scale datasets, primarily involving the difficulty of designing and selecting appropriate features. Additionally, embedding methods generally preserve only a portion of the properties of the original n-dimensional data, making the interpretation process challenging. Moreover, t-SNE is inherently an unsupervised method, lacking the ability to ensure that instances of the same class remain similar in the visualized result."}, {"title": "3. The Proposed RealExp", "content": "This section will introduce the specific details of the proposed RealExp, which are divided into three parts: 1. The background of the Shapley Value and a description of the existing problems. 2. The proposed RealExp and the corresponding proofs. 3. The steps for implementing RealExp in practice. The specific details are as follows:"}, {"title": "3.1 Shapley Value", "content": "The Shapley Value is a fair method used in game theory to distribute cooperative gains. It determines each participant's contribution to the overall profit by calculating their marginal contributions in different combinations. The Shapley Value has desirable properties such as symmetry, efficiency, and gain independence, which ensure fairness in the allocation process. The Shapley Value is given by Exp. (1):\n$\\Delta_v(i, S) = v(S \\cup {i}) \u2013 v(S)$\n$\\zeta( |S| ) = \\frac{ |S|!(|F|-|S|-1)! }{ |F|! }$\n$S_v(i) = \\sum_{S \\subset (F \\setminus {i})} \\zeta(|S|) \\times \\Delta_v(i, S), $\n\nThe core idea of the equation is to measure the marginal contribution of feature i in different feature subsets S for the model output v. Herein, \u2206 (i, S) represents the marginal contribution of feature i to set S. v(S) denotes the profit generated by set S in model v, while (SU {i}) represents the profit generated after adding feature i to set S. (|S|) is a factor representing the weights of different subset combinations in the calculation. Essentially, the Shapley Value is the average of marginal contributions, which can also be represented by Exp. (2):\n$S_v(i) = \\frac{1}{|F|!} \\sum_{\\pi \\in \\Pi} \\Delta_v(i, \\delta_{\\pi}), $\n\nwhere \u03a0 is the set of all possible feature orderings (i.e., all possible feature addition sequences), and its size is |F|!. \u03b4 represents the set of features preceding feature i in the ordering \u03c0. Below is the specific proof:\nProof 1: Let F denote the feature set. For each permutation \u03c0\u03b5\u03a0, define: S = {j \u2208 F\\{i}|\u03c0\u00af\u00b9(j) < \u03c0\u00af\u00b9(i)}. That is, S contains all features that appear before i in the permutation \u03c0. First, compute the number of permutations where the feature subset S serves as the predecessor set of i. For a feature subset S\u2208 F\\{i}, calculate the number of permutations where S\u2081 = S. There are |S|! ways to permute the subset S while the remaining features F\\(S \u222a {i}), can be permuted in (|F|- |S| -1)! Ways. Feature i is fixed at position |S| + 1. Thus, the total number of permutations where S = S is given by Num(S) = |S|! \u00d7 ((|F| - |S| \u2212 1)!). Next, compute the proportion of all permutations in which the feature subset S occurs. Let |F|! denote the total number of permutations. Then, the probability of S predecessor set of i is: P(S) = $\\frac{Num(S)}{|F|!} = \\frac{ (|F|-|S|-1)! }{F!} = \\zeta(|S|)$. According to the definition in Exp. (1), since P(S) = c(|S|), it follows that: \u03a3s\u2286(F\\{i}) P(S) \u00d7 \u25b3(i, S), note that each (i, S) appears Num(S) times across all permutations. Therefore:\n$S_v(i) = \\frac{1}{|F|!} \\sum_{S \\subset (F \\setminus {i})} Num(S) \\times \\Delta(i, S) = \\frac{1}{|F|!} \\sum_{\\pi \\in \\Pi} \\Delta_v(i, \\delta_{\\pi})$.\nHowever, this averaging method has limitations. When features are correlated, averaging Shapley Values may underestimate the contribution of important features. This occurs because, in feature permutations, if a highly correlated feature jappears before i, the marginal contribution (i, S) of i may become small or even zero, as the correlated feature j has already explained part or all the information. More specifically, two issues arise: First is Underestimation of important features. Due to feature correlation, the marginal contribution of important features is diluted during averaging. Second"}, {"title": null, "content": "is incorrect feature ordering. Less significant features may receive higher Shapley Values because they lack correlated features, and their marginal contribution is not diluted during averaging. This leads to the misinterpretation illustrated in Fig. 1 of Section 1, causing inaccuracies in explanations. This section will demonstrate the problems outlined in Section 1. The proof is as follows:\nProof 2: Let F = {X1, X2, ..., xn} denote the feature set. Assume feature xi is highly correlated with feature xj, with correlation coefficient or similarity Si,j \u2248 1. In Shapley Value calculation, the marginal contribution (i, \u03b4\u03c0) depends on the feature set S preceding xi in permutation \u03c0. When the correlated feature xj & S the marginal contribution of x\u012f may be large because x; has not yet appeared, and the information of x\u012f remains unexplained. When xj \u2208 S, due to the high correlation between xj and xi, xj has already explained most of xi information. Consequently, the gain\u2206 (i, \u03b4\u03c0) from adding xi may be small or even zero.\nAccording to Exp. (2), since permutations are random, xj appears before or after x\u2081 with equal probability. Thus: when xj appears before x\u012f, the number of such permutations is approximately $\\frac{|F|!}{2}$, and (i, \u03b4\u03c0) is small in these cases. When xj appears after xi, the number of such permutations is approximately $\\frac{|F|!}{2}$, and (i, \u03b4\u03c0) is large in these cases.\nThe Shapley Value can be divided into two parts: Sv(i) = $\\frac{1}{2} [\\Delta_v(i, S_{\\pi})|x_j \\notin S_{\\pi}] + \\mathbb{E}[\\Delta_v(i, \\delta_{\\pi})|x_j \\in \\delta_{\\pi}]$. Since $\\Delta_v(i, S_{\\pi})$ is smaller when $x_j \\in \\delta_{\\pi}$, the overall Shapley Value Sv(i)is reduced. When xj is fully correlated with xi (Si,j = 1), \u0394\u300f(i,Sn) \u2248 0 when xj \u2208 \u03b4\u03c0. Assume \u0394\u03bd (\u03af, \u03b4\u03c0) = \u03b4 (a large positive value) when xj \u2209 \u03b4\u03c0, and \u2206(\u03af, \u03b4\u03c0) = \u2208 (a value close to zero) when) when x; \u2208 S. The Shapley Value becomes: Sv(i) = $ \\frac{\\delta}{2} + \\frac{\\epsilon}{2} \\approx \\frac{\\delta}{2}$. This is half of the Shapley Value in the absence of correlated features (\u03b4)."}, {"title": "3.2 RealExp", "content": "To address the aforementioned issues, this paper proposes a new calculation method called RealExp. This method decouples the Shapley Value into two parts and adjusts the calculation of marginal contributions based on feature similarity. Specifically, the Shapley Value \u00d8\u00a1 of each feature is decomposed into two components: \u00d8\u2081 = \u00d8 independent + 0 Margin The independent contribution \u00d8independent is represented as: \u00d8independent = v({i}) \u2013 v(\u00d8), indicating the contribution of feature xi to the model when considered independently of other features. The marginal contribution \u00d8Margin is expressed as: Margi = \u2211ji Wi,j[v({i, j}) \u2013 v({j}) \u2013 \u00f8independent]. Where, Wij reflects the similarity between features xi and xj defined as: $W_{i,j} = \\frac{1-S_{i,j}}{\\sum_{k \\neq i}(1-S_{i,k})}$. Herein, Si,j represents the similarity between features xi and xj, where 0 \u2264 Si,j \u2264 1. When Si,j = 1, indicates complete correlation between the features. When Si,j = 0 indicates no correlation. Thus, when Si,j \u2248 1, Wi,j \u2248 0, reducing the impact of correlated features on the marginal contribution. The overall definition of RealExp is given by Exp. (3):\n$\\Phi_i =  \\frac{1}{|F|!} \\sum_{\\pi \\in \\Pi} \\Delta_v(i, \\delta_{\\pi}) \\cdot \\Upsilon(\\delta_{\\pi}, i), $"}, {"title": null, "content": "Where Y(\u03b4\u03c0, i) is an adjustment factor defined as: Y(\u03b4\u03c0, \u03af) = \u03a0jes\u339e(1 \u2013 Si,j). When xi has high similarity with a feature xj in S, \u03a5(\u03b4\u03c0, i) decreases, thereby reducing the weight of xi's marginal contribution to the Shapley Value for that permutation. This adjustment also ensures that contributions from similar features are not double counted. By this design, the proposed RealExp comprehensively accounts for feature correlations and avoids the averaging issues inherent in Shapley Values.\nProof 3: First, for each permutation \u03c0\u03b5\u03a0, the marginal contribution of feature x\u2081 is adjusted to \u0394\u03bd (\u03af, \u03b4\u03c0)\u00b7 \u03a5(\u03b4\u03c0, \u03af), when xi has high similarity with a feature x; in S (Sij \u2248 1): Y(\u03b4\u03c0, \u03af) = \u041fjes\u201e(1 \u2212 Si,j) \u2248 0, hence, the marginal contribution of xi for that permutation is significantly reduced or ignored. Next, according to Exp. (3): when x; \u2208 S and Si,j \u2248 1, Y(\u03b4, i) \u2248 0, and the corresponding marginal contribution is ignored, avoiding dilution caused by correlated features. When Xj & S and Si,j is small, Y(\u03b4\u03c0, i) is relatively large, ensuring that the marginal contribution of x\u012f is fully considered."}, {"title": "3.3. In Practice", "content": "In practice, this paper applies RealExp to perturbation-based surrogate models to achieve interpretability, with the advantage of being able to explain any model. Unlike previous works (Ribeiro et al., 2016; Liu et al., 2024; Tan et al., 2024), which utilize random sampling and Monte Carlo sampling, this study adopts a fixed ratio to generate perturbation samples, effectively reducing instability. Furthermore, in constructing the surrogate model, this paper designs an ensemble tree as the surrogate model instead of traditional linear regression or its variants, as tree models inherently capture nonlinear relationships between features, avoiding potential collinearity issues associated with traditional linear models. Fig. 3 shows the workflow of RealExp in actual deployment. As shown in Fig. 3, taking an image classification task as an example, RealExp explains why the black-box model identifies the image as a"}, {"title": null, "content": "tree frog through five steps: Step 1 involves generating perturbation samples, Step 2 re-feeds these samples into the black-box model to obtain predictions, Step 3 compares the similarity between the perturbation samples and the original image, Step 4 trains an ensemble tree model, and Step 5 uses RealExp to generate explanations and visualized outputs. The detailed process is as follows:"}, {"title": "3.3.1. Perturb Sample Generation", "content": "This step is used to generate the input data for building the proxy model later. Let I denote the image to be interpreted and let W represent the watershed image segmentation algorithm. Upon segmentation, the result obtains the image I' such that I' = W(I). The segmented image I' can be represented as a set of segmented blocks, denoted by {B}}}=1, where each block B; for j \u2208 [1,n] signifies the index of the block.\nLet f = {n(k)}k=1 = {(k)}j=1' denote a set of masks to be applied to 1'.Each masky(k), where k \u2208 [1, K], contains a set of Boolean variables (k)Each (k) corresponds to block Bj and indicates whether it is kept or masked (0 for masked, 1 for kept). For each specific mask application, which can be define: B(k)={0, if \u03bck) = 0,(Bj, if \u03bck) = 1.\nEach set {\u03bc\u03ba(k)) ensures, according to a heuristic rule, that at most 30% of the blocks are covered. This is formalized by the inequality: \u03a3=1(1 \u2013 \u03bc*)) \u2264 0.3n. Applying each mask y(k) to I' yields a series of new images with selectively retained blocks: J(k) = y(k) (1') = U=1 B(k). Collectively, these form the final set of perturbed images: J\" = {J(1),](2), ..., J(K)}, where each image J(k) represents the outcome of a specific block retention pattern determined by the masky(k). Proof4 aims to Demonstrating the Stability of Fixed Proportion Generation.\nProof 4: Let the original image, after segmentation, be divided into n blocks, denotes as {B1, B2, ..., Bn}. Let Var[f(J(k))] denote the variance of the estimated value. For fixed proportion generation, the number of masked blocks y(k) is fixed and denoted as m = an, where a \u2264 0.3. Thus, the probability of masking each block is the same and satisfies: P(uk) = 0) = m = a, P(\u03bc) = 1) = 1 \u2212 a. To simplify the calculation, assume that the function f(J(k)) is linear, given by: f (J(k)) = fo + \u03a3=1 Cjuk), where fo is a constant and cj is the contribution of block Bj. For fixed proportion generation, the variance is given by: Varfixed = $\\alpha(1-\\alpha) ((\\frac{1}{n(1-\\alpha)}+1)\\sum_{j=1}^{n}c_j^2 - (\\frac{1}{n-1} \\sum_{j=1}^{n} c_j)^2)$.\nFor Monte Carlo sampling, the masking probability for each block is given by P(\u03bc; P(\u03bck) = 0) = q, with a variance of the masking probability o > 0, where q = a. The variance is expressed as: VarMonte Carlo = (q(1 \u2212 q) + \u03c3\u00b2) \u03a3=1c\u00b2. In random sampling, where each block is independently masked with probability a the variance is VarRando = \u03b1(1 \u2212 \u03b1) \u03a3=1 c\u00b2. Comparison of Variances, it can be observed that:Varfixed <Varrando <VarMonte Carlo. This inequality demonstrates that fixed proportion generation has the lowest variance, providing greater stability compared to both random sampling and Monte Carlo sampling."}, {"title": "3.3.2. Repass the model", "content": "This step is used to generate the label for building the proxy model later. Let & denote the model to be explained. The generated perturbed images are represented as I\" = {1,1,1,2),...,I(K), where each J(k) is passed through the model for prediction. Let Y = {y(1), (2), ...,Y(K)} denote the p for all perturbed images."}, {"title": "3.3.3. Adjust weight", "content": "This step is used to generate and adjust weight for input data in proxy model. The I' can be regarded as a vector of one, and the perturbed image J(k) is represented by a combination of zeros and ones (as determined by the mask (k), the similarity measure calculates the proportion of retained blocks. The similarity can be expressed as sim(I', J(k)) =$\\frac{1}{n} \\sum_{j=1}^n\\mu_j,k = \\frac{1}{n} \\| \\mu(k)\\|_1$, where uk) represents the j-th element in the mask \u03bc(k), and n is the total number of blocks. This calculated similarity varies linearly with the number of retained blocks, which does not align with conventional nonlinear similarity measures.\nTo introduce nonlinearity, the similarity using an exponential function be adjusted, as shown in Exp. (4):\n$adj_k = exp(-\\lambda (1- sim(I', J^{(k)}))), $\n\nWhere 0 < \u03bb is a hyperparameter representing the intensity of the adjustment. The term 1- sim(I',J(k)) corresponds to the normalized Hamming distance between I' and J(k). This adjustment ensures that when the proportion of masked blocks is small, the adjusted similarity adjk remains high, and when the proportion is large, adjk becomes lower. The adjusted similarity adjk is applied multiplicatively to each block of the perturbed image J(k). The updated block B(k)' is calculated as B(k) = adjk. B(k), vj \u2208 {1,2,..., n}, where B(k) represents the block in the adjusted perturbed image, and B(k) represents the corresponding block in the original perturbed image before adjustment."}]}