{"title": "IMAGE FIRST OR TEXT FIRST? OPTIMISING THE SEQUENCING\nOF MODALITIES IN LARGE LANGUAGE MODEL PROMPTING AND\nREASONING TASKS", "authors": ["Grant Wardle", "Teo Susnjak"], "abstract": "This paper examines how the sequencing of images and text within multi-modal prompts influences the\nreasoning performance of large language models (LLMs). We performed empirical evaluations using\nthree commercial LLMs. Our results demonstrate that the order in which modalities are presented\ncan significantly affect performance, particularly in tasks of varying complexity. For simpler tasks\ninvolving a single image, modality sequencing had a clear impact on accuracy. However, in more\ncomplex tasks involving multiple images and intricate reasoning steps, the effect of sequencing\ndiminished, likely due to the increased cognitive demands of the task. Our findings also highlight\nthe importance of question/prompt structure. In nested and multi-step reasoning tasks, modality\nsequencing played a key role in shaping model performance. While LLMs excelled in the initial\nstages of reasoning, they struggled to re-incorporate earlier information, underscoring the challenges\nof multi-hop reasoning within transformer architectures. This suggests that aligning the sequence\nof modalities with the logical flow of reasoning steps is more critical than modality order alone.\nThese insights offer valuable implications for improving multi-modal prompt design, with broader\napplications across fields such as education, medical imaging, and cross-modal learning.", "sections": [{"title": "1 Introduction", "content": "Recent advancements in Large Language Models (LLMs) have profoundly impacted natural language understanding\nand related fields seeking to automate tasks involving human language. While reasoning was once considered a uniquely\nhuman trait [1], parallels are now observed between human cognition and LLMs. The emergent reasoning abilities of\nLLMs and solve complex tasks that require high-order cognitive abilities has generated significant academic attention\n[2, 3], as well as concerns in some fields [4] about the trajectory of such AI agents. Considerable research efforts have\nbeen devoted to improving LLMs' reasoning abilities recently which, while impressive, have nevertheless been uneven\nand variable across different tasks [2]. With the emergence of multi-modal LLMs that can now process textual, audio\nand visual inputs, the complexity of reasoning across all modalities has increased markedly [5-7] and questions remain\nabout how best to structure the prompts to elicit optimal reasoning in such contexts.\nVisual question-answering (VQA) involving a combination of image(s) and multiple-choice questions has become a\ncommon method for evaluating LLM multi-modal reasoning capabilities [3, 8\u201311]. Benchmark datasets for this task\nhave emerged [3] covering a wide range of disciplines while often taking the form of academic exam-like questions\n[10]. Notably, models such as GPT-4 [12], Gemini-1.5 [13], and Claude [14] have displayed degrees of multi-modal"}, {"title": "2 Related Work", "content": "Reasoning can be defined as the cognitive process of drawing inferences or conclusions from premises, evidence, or\nobservations, involving the systematic application of logical principles to analyse information, solve problems, and\nmake decisions [30]. Reasoning encompasses both deductive methods, where conclusions necessarily follow from\ngiven premises, and inductive approaches, where generalisations are formed from specific instances. The assertion\nthat reasoning is a genuine emergent behaviour in LLMs is contentious in academic literature [31, 32]. Emergent\nabilities within LLMs have been defined as capabilities present in larger but not smaller models, with reasoning being\nidentified as one of these properties [33] that arise as the parameter size of language models has grown. However, recent\ninvestigations [34, 35] suggest that current LLMs find it challenging to tackle intricate reasoning tasks that humans\nhandle with relative ease, lacking profound understanding and instead relying on superficial pattern recognition or\ndataset biases. Studies [35, 36] also argue that contemporary LLMs are confined to intuitive, reflexive tasks, rather\nthan those necessitating logical and deliberate analysis associated with true higher-level reasoning, while others [37]"}, {"title": "2.1 Multi-modal Prompting Techniques", "content": "Several studies have focused on improving and addressing LLM's challenges within the vision modality. Techniques\nsuch as Compositional Chain-of-Thought (CCoT) [24] use scene graph-based prompting to achieve this while Image-\nof-Thought (IoT) [25] extracts visual rationales in a step-by-step manner. Meanwhile, TextCoT [27] divides images\ninto global and local regions to assist with reasoning, while Duty-Distinct Chain-of-Thought (DDCoT) [26] employs a\ntwo-stage framework to separate reasoning roles for visual and language modalities. Multi-modal Chain-of-Thought\n(MCOT)[28] improves multi-modal reasoning by initially partitioning LLM responsibilities into reasoning and recog-\nnition before integrating vision information within smaller models. Although these models examine the interaction\nbetween modalities, they treat them as distinct components that can be processed independently. These strategies, while\neffective in certain contexts, have not considered how the sequencing of modalities affects reasoning performance."}, {"title": "2.2 Image Sequencing", "content": "In human behaviour, the primacy effect suggests that individuals are more likely to recall information presented at\nthe beginning of a sequence [48], in contrast to the recency effect, which implies a contrary bias towards information\nat the end of a sequence [49]. Both the primacy and recency effects have been demonstrated to exist within LLMs\n[16, 17, 50\u201352]; however, these have not been comprehensively studied and explored in the context of multi-modal\nLLMs and reasoning tasks. Vendors [14, 53, 54] of large commercial LLMs have tended to advise that in cases involving\nprompts with images, there is a primacy effect that impacts performance\u00b2. For general tasks where the image is the\nfocus, this logic makes sense; however, for reasoning tasks where key instructions are often in a dedicated question\ncomponent, this may not hold true. To the best of our knowledge, there is little information on why this is recommended\nor evaluations on different types of tasks for image position."}, {"title": "2.3 Multi-modal Fusion Strategies and Positional Bias", "content": "The architectural foundation of LLMs plays an important role in how the sequencing of information is processed. The\narchitecture of LLMs is based on transformers [55], which use attention mechanisms to assign varying weights to input\ndata based on context. This involves multiple self-attention layers running in parallel, enabling LLMs to simultaneously\nfocus on different aspects or relationships between tokens in the input sequence. These patterns are learned through\ntraining or further refined via fine-tuning. For text, transformers use tokenised word representations with positional\nencoding to maintain sequence order, which is critical for understanding context and syntax [55, 56]. The integration\nof multiple modalities such as text and images within LLMs requires effective fusion strategies to enable coherent\nunderstanding and reasoning. Fusion strategies determine how information from different modalities is combined and"}, {"title": "2.4 Research questions", "content": "Recent literature is collectively beginning to converge towards investigations that seek to uncover strategies to optimise\nprompts for maximising LLM performance and reasoning. While existing research has mostly tended to focus on\nenhancing performance gains within a single modality (text), in cases where multi-modal information was considered,\nthe studies have typically overlooked the impact of information sequencing in multi-modal contexts and how different\nand unknown multi-modal fusion strategies may be a confounding factor that affects responses. Therefore, our research\nhas aimed to bridge this gap by examining how the sequencing of images and text affects LLM performance in reasoning\ntasks. To that end, this study's guiding research questions are:"}, {"title": "3 Methodology", "content": "We designed a series of experiments on two benchmark datasets detailed in the subsequent section to address our\nresearch questions."}, {"title": "3.1 Datasets", "content": "Our evaluations used two recently developed multi-modal multiple-choice reasoning benchmarks for LLMs, namely\nM3Exam[2] and M3COTS[3]. These benchmarks were developed with questions that integrate visual and textual\ninformation and were thus selected in our experiments due to their ability to present models with both complex and\ndemanding reasoning tasks from multiple modalities."}, {"title": "3.1.1 M3Exam Dataset", "content": "M3Exam [2] offers a diverse range of real exam questions across various educational levels. For our evaluation, we\nselected the multi-modal English question set which contains 795 questions across 4 overarching subjects (social-\nscience, natural-science, language, math), 11 subcategories, and 3 educational levels (elementary, middle, and high\nschool) in the USA. The average word count across the questions and background information is approximately 95\nwords.\nThe M3Exam dataset structures each question in JSON format, dividing it into three key parts:\nbackground_description which provides additional context in some cases, the question_text which contains\nthe actual questions, and options which represents the multiple-choice responses. Image elements can be dispersed\nacross all three elements and, sometimes in multiple places per question which further amplifies the complexity of\nquestions. An example of an exam question with three components can be seen in Figure 1, with guidance suggesting\nthat the image component be placed in the question_text section of the overall question. This particular question\ndoes possess an empty background_description component."}, {"title": "3.1.2 M3COTS Dataset", "content": "The second dataset used in this analysis is M3COTS [3].\nM3COTS features a selection of questions specifically\nchosen to challenge visual reasoning and multi-step rea-\nsoning across multiple subjects. The dataset includes\nscience topics from the ScienceQA dataset [9], mathe-\nmatics questions from MATH [71] and the Sherlock [72]\ndatasets, intended to test common-sense abductive reason-\ning beyond the literal image content. For our evaluation,\nwe selected a random sample of 2,318 questions (20% of\nthe dataset) spanning 3 domains, 9 subjects, and 92 ques-\ntion types. In this dataset, each question includes only\none image as opposed to M3Exam which is a significant\nreduction in complexity. The average word count across\nthe questions, background information, and options is ap-\nproximately 45 words. Ten percent of the images contain\nonly visual content, 65 percent consist of a combination\nof images and text, and 25 percent feature text exclusively.\nExample questions from M3COTS are shown in Figures\n4a and 4b. Note that as seen in Figure 4b, while each\nquestion in this dataset may be accompanied by only one\nimage in the raw format, an image may however embed multiple images distinct and as well as text within a single\nvisual. Similarly to the M3Exam dataset, M3COTS structures each question in JSON format, dividing it into three key\nparts: context which provides additional background in some cases, the question component which contains the\nactual question, and choices which represents the multiple-choice responses. The images are not directly referenced\nin the context, question or choices. The example JSON structure of a M3COTS question can be seen below."}, {"title": "3.2 LLM Models", "content": "We selected three popular commercial models for our experiments: ChatGPT-40, Claude-3.5, and Gemini-1.5 Flash.\nChatGPT-40 was developed by OpenAI and introduced in 2023, is characterised by a large parameter count and\nextensive context length. These features enable sophisticated multi-modal interactions and complex reasoning tasks.\nClaude-3.5 Haiku was produced by Anthropic, and is recognised for its speed and compact design. This model provides\nan ideal contrast to larger, more computationally intensive models like ChatGPT-40, offering insights into the trade-offs\nbetween model size and response latency. Lastly, Gemini-1.5 Flash is Google's model and is regarded as another\n\"lightweight\" model optimised for speed and efficiency, complementing the other selections by focusing on streamlined\nperformance.\nThe three models, with their varying capabilities and architectural designs, collectively provide a comprehensive\noverview of the current landscape in large-scale AI computations. The decision to focus on larger LLMs stems from\nexisting studies[42], which suggest that the capability for Chain-of-Thought (CoT) reasoning may emerge in language\nmodels at a certain scale, specifically over 100 billion parameters. All models were accessed via their respective APIs,\nhosted on platforms capable of supporting extensive AI operations, thereby ensuring reliable and consistent performance\nthroughout our studies.\nThe experiments were conducted in a zero-shot fashion, ensuring that the models were not exposed to any examples\nprior to testing. We employed variations of CoT's prompts [42], and all testing was conducted using greedy decoding at\na temperature setting of 0.1. Our experiments used standard models without any fine-tuning to focus on the models'\nbehaviour under direct interaction, which is the most common approach users take when engaging with language\nmodels. We opted not to sample multiple responses or perform self-consistency-based re-ranking [74], as these methods\nsignificantly increase operational costs and may not be practical in many scenarios related to our datasets.\nIn this research, the focus was on examining the relative performance of the chosen LLMs across different image and\ntext input configurations. Therefore, the primary aim was not to achieve maximal state-of-the-art performance, but\nrather to understand how these models behave with changes to the sequencing configuration of the text and image\ninputs."}, {"title": "3.3 Experimental design", "content": "This study conducted a series of experiments to evaluate how the sequencing of image and text modalities in prompts\naffects the multi-hop reasoning performance of multi-modal LLMs, structured around four primary setups: (1) Image-\nText Sequence Variation, which examined the effects of different sequencing orders (Image First, Text First, and\nInterleaved) on model performance across two datasets; (2) Attribute-Based Sequencing Analysis, which investigated\nhow specific dataset attributes\u2014such as image type, prompt length, and question complexity\u2014influence the model's\nsensitivity to sequencing; (3) Image Versus Instructions Analysis, aimed at determining whether the impact of sequencing\nis due to the image placement or the sequence of instructions by converting visual elements into text; and (4) Prompt\nPriming for Relationship Analysis, which explored whether priming the model to prioritise a specific modality alters its\nreasoning process, irrespective of the initial sequencing. Table 1 summarises the entire experimental design, which is\nexplained in further detail below."}, {"title": "3.3.1 Image-Text Sequence Variation", "content": "This experiment investigated the zero-shot multi-modal reasoning, where the model was tasked with predicting an\nanswer a to a prompt that included a textual query q and an image x, without having been exposed to similar tasks\nduring training. The model was required to analyse both the visual content in x and the information in q, integrating\nthese inputs to generate a correct response. The experiment was specifically designed to evaluate how the sequence and\nintegration of textual and visual inputs, as structured within the API calls, affect the model's reasoning capabilities.\nEach of the three models' API's encodes information in a similar manner where a set of parameters along with a prompt\nis sent to the model as depicted in Figure 5. The prompt was composed of information from different roles, which\ndefined the context and purpose of each part of the message. For this experiment, the prompt consisted of messages\nfrom two key roles: system and user. The system message sets the overall tone and controls how the model should\nrespond. In this experiment, we used a fixed template for the system message: \"You are an expert in {subject}, helping\na student answer an exam question.\". This message remained constant across all configurations, ensuring a consistent\ncontext for the model's responses. The second role in our prompt was the user role, which represents the input or\nquestion provided to the model. The user role contained blocks of content that can include either images or text. Since\nour experiments tested how the order of these content blocks (text and images) affects the model's performance, we\nvaried the sequence in which the content blocks were presented to the LLM. We tested three configurations: Image"}, {"title": "3.3.2 Image-Text Sequence: Attribute-Based Analysis", "content": "In these experiments, we analysed how varying attributes within the dataset-such as the type of image (image, text, or\na mixture of both), prompt length, difficulty levels, and question types-affect the model's performance and sensitivity\nto sequencing. The goal was to examine whether the trends observed in the overall dataset hold for each of the attributes.\n* Image Type: The model's performance is evaluated based on different types of images\u2014purely visual,\ntext-based, and mixed images."}, {"title": "3.4 Image-Text Sequence: Image Versus Instructions Analysis", "content": "To determine whether the impact of sequencing is due to the placement of the image or the sequence of text-based\nprompting instructions, we conducted experiments on a selected sample of question types from the M3COTS dataset.\nThese questions contained only text or embedded formulas within the images. We extracted and converted the visual\ncontent into text (referred to as $xTextExtracted$) and ran the sequencing experiments using the text modality only. This\napproach allowed us to control for, and identify whether performance differences arise from the image's placement or\nthe phrasing and sequencing of the instructions6.\nThe specific configurations being tested are:\n* Image First (IF): The model processes the extracted text from the image $xTextExtracted$ before the textual query\nq. This is represented by the function $fIF$.\n$aIF = fIF(XTextExtracted, q)$\n* Text First (TF): The model processes the textual query q before the extracted text from the image $xTextExtracted$,\nrepresented by the function $fTF$.\n$ATF = fTF(q, xTextExtracted)$"}, {"title": "3.5 Prompt Priming for Relationship Analysis", "content": "We also introduced a priming mechanism, denoted as p, which was used to explicitly instruct the model to focus its\nattention either on the image x first or on the text query q. The objective was to influence the order in which the model\nprocessed each modality in the multi-hop reasoning order, regardless of their initial presentation sequence.\n1. Single Prompt-Image First Attention (IFA): In this configuration, even though the image is presented\nsecond, the primed prompt instructs the LLM to prioritise processing and its attention on the image. The"}, {"title": "3.6 Inference into the LLM Modality Fusion Strategy", "content": "The study hypothesises that the impact of modality sequencing on model performance will vary depending on the\nunknown fusion strategy employed by the underlying LLMs. For early fusion models, where all modalities are\nprocessed together as a unified token sequence, we expect significant sensitivity to the order of images in the prompt.\nConfigurations such as image-first or image-last are likely to lead to notable variations in accuracy due to the reliance on\npositional encoding. In contrast, for late fusion models which process each modality independently before combining\nthem, we hypothesise minimal sensitivity to image sequencing since the fusion occurs only after individual processing.\nFor Hybrid fusion models which integrate modalities at intermediate stages, we would expect to observe moderate\nsensitivity to sequencing, reflecting a partial dependence on modality order but not as extreme as early fusion models.\nAdditionally, dataset complexity is expected to modulate these effects. With respect to inputs, the M3Exam dataset is\nconsiderably more complex of the two benchmarks given that is contains up to five images per prompt; however, the\ndifficulty of the actual question tends to generally be with the M3COTS dataset\u00b3. Therefore, the increased cognitive\nload may reduce the model's ability to distinguish the effects of different image positions particularly in early fusion\nmodels. On the other hand, in the M3COTS dataset, where each prompt contains only one image, we would anticipate\nclearer sequencing effects, as the model's attention is more focused on integrating fewer modalities. These hypotheses\nwill be evaluated for accuracy differences across prompt configurations to assess the potential influences of both fusion\nstrategy and dataset complexity."}, {"title": "3.6.1 Evaluation", "content": "Our experiment evaluations were mainly performed using a mix of comparing the percentage of correct responses,\nconducting mean rank analyses, and performing tests for statistical significance. For the statistical evaluation of binary\noutcomes per response (i.e. correct/incorrect), the McNemar's test was used as it is specifically designed for binary\noutcomes and thus provides an effective way to compare the relative performance under different conditions for the\nsame questions. Mean ranks were employed to offer a more comprehensive and insightful understanding of the impact\nof image and text sequencing configurations. For each question type and configuration, ranks were assigned based\non the accuracy performance of the LLMs, with a lower rank indicating better performance. These ranks were then\naveraged across different sub-categories within each dataset, such as subject domains and question types. Analysing\nthe mean ranks subsequently helped in identifying more generally what the most optimal configurations tended to be\nby consolidating performances over all configurations. Mean ranks therefore provided another concise perspective\nalongside that of accuracy comparisons. The statistical tests provided insights but were considered merely as one of\nseveral indicators rather than the sole arbiter of significance"}, {"title": "4 Results", "content": "This section first examines the results from variations in image-text sequencing. Subsequently, it assesses the impact\nof the characteristics of questions on accuracy. Following this, it presents the findings from the analysis of image or\ninstruction sequencing effects. Lastly, it considers the outcomes of the proposed priming strategy."}, {"title": "4.1 Image-Text Sequence Variation", "content": "Figure 7 shows the accuracies of the three LLMs on both datasets, with respect to the different placements of the images\nin the prompt sequences. At a high level, it can be seen that generally LLMs tend to score higher on M3Exam than on\nM3COTS, which is in line with results in literature, which has reported 71.8% [10] and 62.6% [3] respectively using\nthe older ChatGPT-4 with CoT. ChatGPT-4o also consistently outperformed Claude-3-haiku and Gemini-1.5-flash\non both datasets by a significant margin, while, Claude-3-haiku has demonstrated the lowest overall performance on\nboth datasets. Across both figures, it can also be seen generally, that placing images within the text on the M3Exam\ndataset consistently yields higher accuracies over other placements, while on the M3COTS dataset, we see that placing\nthe images before all the textual components (i.e. background, questions, options and other instructions) consistently\nimproved accuracies. However, the results also show that in general the performance differences between the modality\nsequencing strategies were less pronounced on M3Exam than on M3COTS datasets. From this, some inferences about\nthe possible fusion strategies can be made.\nThe results from Figure 7 across both the M3Exam and M3COTS datasets may suggest that Claude-3-haiku is likely\nutilising a late or hybrid fusion strategy as indicated by its stable performance (accuracies differ approx. 1%) across\ndifferent prompt configurations in both more complex (M3Exam) and simpler (M3COTS) multi-modal reasoning tasks\nas opposed to other models. The minimal sensitivity to image sequencing supports the notion that the underlying\nClaude-3 model processes modalities independently before merging them, leading to consistent outcomes regardless of\nthe prompt sequencing structure. Conversely, Gemini-1.5-flash and ChatGPT-4.0 show patterns consistent with early\nfusion approaches. Both models exhibit greater sensitivity to prompt sequencing in the M3COTS dataset (accuracies\ndifferences range approx. 4%-6%), where the reasoning task is less complex given there is only one image per prompt.\nIn contrast, the M3Exam dataset, though it has a lower degree of content-difficulty than M3COTS, given its higher input\ncomplexity comprising multiple images per prompt, this likely dampens the effects of image sequencing due to the\nincreased cognitive load and reasoning requirements. This reinforces the hypothesis that early fusion models perform\nbetter when the task complexity is lower, and the modality integration can be influenced by the position of images in the\nprompt."}, {"title": "4.2 Image-Text Sequence: Attribute-Based Analysis", "content": "Here, exam question attributes were analysed for their impact on image sequencing to evaluate whether the trends\nobserved in the overall dataset accuracies presented earlier, hold for each of the attributes. For the M3Exam dataset,\nLevels, Prompt Length and Image Types were examined, while for M3COTS Question Types, Prompt Length and\nImage Types were evaluated. In the case of M3Exam data, the models' performances did not show any deviations\nfrom the results in the previous section (the details of this can be seen in Appendix A). However, in the case of\ncertain question types for the M3COTS dataset, placing the image after the text led to significantly better performance\nwhich was contrary to the overall results in the previous section. Table 4 shows M3COTS question types where the\noptimal image sequencing diverged from the results for the overall dataset. For instance, performance on the \"Physics\nVelocity, Acceleration, and Forces\" question type showed significantly improved with the image placed after the text for\nClaude-3-Haiku (McNemar's test p-value = 0.001), similar \"Grammar\" showed a significance for Gemini-1.5-Flash\n(McNemar's test p-value = 0.021). This finding suggest that the impact of image sequencing varies depending on the\nmodel and context and from this we can conclude that optimally matching the image sequencing for specific question"}, {"title": "4.3 Image-Text Sequence: Image Versus Instructions Analysis", "content": "For these experiments, we utilised a dataset comprising questions presented either solely in text or as formulas\nembedded within images. Specifically, we employed the \"Elementary Algebra\" (363 questions) and \"Grammar\"\n(205 questions) subsets from the M3COTS dataset. The primary objective was to investigate whether the sequencing\nof instructions\u2014independent of image placement-affects model performance. To isolate the effect of sequencing,\nwe extracted text from images during preprocessing, creating text-only versions of the questions. This extraction\nwas performed using ChatGPT-40 and Gemini-1.5-flash Table 5 presents the performance of three multi-modal\nLLMs-Claude-3-Haiku, Gemini-1.5-Flash, and GPT-4o\u2014under different sequencing conditions. In this context,\n\"After\" indicates that the image is presented after the textual instructions, while \"Before\" denotes that the image\nprecedes the text. The \"multi-modal\" column refers to the original questions containing both images and text, whereas\nthe \"text\" column represents the text-only versions.\nThe experimental results in Table 5 reveal that the sequencing of images and text within prompts significantly influences\nthe reasoning performance of multi-modal large language models, with effects varying by task and model. Specifically,"}, {"title": "4.4 Prompt Priming for Relationship Analysis", "content": "The initial baseline results for M3COTS shown in Figure 7b indicated that placing an image before the textual modality\nyielded higher accuracies. To assess whether explicit priming could influence the processing order of modalities and\nthereby enhance model performance, we conducted prompt priming experiments across all questions in the M3COTS\ndataset. Specifically, we instructed the LLMs to prioritise image processing even when images were presented after\ntextual instructions. Contrary to our hypothesis, the results in Table 6 indicate that this priming strategy led to a\nconsistent decline in accuracy across all tested models. Claude-3-Haiku's accuracy decreased from 0.51 to 0.45,\nGemini-1.5-Flash from 0.56 to 0.53, and ChatGPT-40 from 0.67 to 0.64 when prompted to focus on images first despite\ntheir subsequent placement. These findings suggest that the inherent processing order of the models, likely ingrained\nthrough their training data and architectural design, is resistant to override through simple priming instructions. The\ndecline in performance implies that the models may prioritise modalities based on their default configurations (including\nmodality fusion strategies), making it challenging for external prompts to effectively alter their attention mechanisms.\nThese results underscore the role of modality sequencing over priming in prompt engineering for multi-modal LLMs.\nWhile physical ordering of information (i.e., presenting images before text) tends to enhance performance as demon-\nstrated in our baseline experiments (Figure 7a and 7b), attempting to manipulate the processing order through priming"}, {"title": "5 Discussion", "content": "Our research investigated the impact of varying the sequencing of images and text modalities on the reasoning\nperformance of LLMs and found instructive results. Our work built on and extended similar investigations considering\nthe impact of altering the relative position of words [16, 17] or the instruction order in text prompts [22]. We\nhypothesised that the order in which these modalities are sequenced would influence reasoning performance. The\nresults confirmed this hypothesis, showing that the optimal sequencing varied depending on the dataset: placing images\ninline within the text yielded the best performance on the M3Exam dataset, while presenting images before the text\nled to superior performance on the M3COTS dataset. Further analysis showed that within the M3COTS dataset,\ncertain question types were more sensitive to sequencing changes than others, with the optimal sequencing of modality\npresentation differing by question type and model. These findings suggest that both the dataset structure and the\ncomplexity of the questions influence how modality sequencing affects reasoning performance in LLMs."}, {"title": "5.1 Modality sequencing and fusion strategies", "content": "The effect of sequencing image and text modalities on LLM reasoning performance varied significantly across the two\ndatasets, highlighting the pivotal role of instruction tuning and prompt design in shaping model behaviour, with the\nunderlying multi-modal fusion strategies of each LLM being an unknown confounding factor (RQ1). For the M3Exam\ndataset, the best performance was achieved when images were interwoven with the text. The approach mirrored the\nactual exam structure designed to optimise student comprehension by aligning modalities for effective information\nflow for humans. This same sequencing also proved beneficial for LLM reasoning for this dataset. In contrast, the\nM3COTS dataset, designed to challenge multi-modal reasoning, generally performed better when images were presented\nbefore the text. This suggests that placing the image first provides a visual context that aids the reasoning process, as\nrecommended by vendors[53][14] [54]. Variations within the dataset indicate that the optimal sequencing depends on\nthe specific structure of the individual question, highlighting that the best modality sequencing is context-dependent and\nshaped by both the dataset and the task at hand.\nThe study suggests that attention mechanisms in transformer-based LLMs likely influences modality bias which affects\nthe reasoning performance based on the sequencing of modalities in prompts. We inferred from the results that\naltering the order of text and images changes attention distribution across modalities. In early fusion architectures,\npositional encoding causes earlier modalities to receive disproportionately higher attention, potentially underutilising\nlater modalities and hindering effective multi-modal integration in complex reasoning tasks. These findings have\npractical implications for both prompt design and model development. From this insight, prompt designers may consider\nstrategically sequencing modalities to align with the logical flow of reasoning to ensures critical information receives\nappropriate attention. For model developers, addressing inherent positional biases in attention mechanisms is essential\nas this could involve architectural adjustments or training strategies that promote equitable attention distribution across\nmodalities."}, {"title": "5.2 Question Complexity and Sequencing Sensitivity", "content": "Analysis of the question types most impacted by sequencing changes, particularly in the M3COTS dataset, often involved\na nested multiple-choice format where one question referenced another. While explored LLMs frequently succeeded in\nsolving the underlying reasoning task related to the image, they often struggled with the final step\u2014revisiting earlier\ninformation to select the correct option within the original question (e.g., pointing to the option list in the image). This\nchallenge highlights issues related to multi-hop reasoning and the models' capacity to maintain context over several\nreasoning steps. The linear reasoning approach facilitated by CoT prompting encourages step-by-step processing but\nmay not adequately support the backtracking required in nested questions. The transformer's positional encoding of"}, {"title": "5.3 Information Order vs. Modality Properties", "content": "Our experiments revealed that the sequence in which information is presented significantly influences LLM performance,\noutweighing the inherent properties of the modalities themselves (RQ3). By converting images to text and evaluating\nsingle-modality prompts, we found that the order\u2014whether text precedes image or vice versa\u2014consistently impacted\naccuracy, underscoring the importance of positional encoding and attention mechanisms in transformer architectures.\nThe models' sensitivity to information sequencing varied based on their training data and fine-tuning methods, and while\nClaude-3-Haiku showed minimal responsiveness to sequencing changes, Gemini-1.5-Flash and GPT-40 exhibited more\npronounced improvements with optimal information ordering. Additionally, our attribute-based analysis indicated that\nspecific question types could achieve performance gains of up to 5% by tailoring the sequencing strategy, highlighting the\nnecessity of strategic information ordering in prompt design. These findings suggest that effective prompt engineering,\naligned with both task requirements and model characteristics is essential for optimising the reasoning capabilities of\nmulti-modal LLMs and thereby enhancing their utility across diverse applications."}, {"title": "5.4 Implications and Practical Guidelines", "content": "The findings of this study extend beyond exam-like tasks and offer valuable insights for broader AI applications. For\ninstance", "include": "n* Strategic Sequencing of Modalities:\n  * Align with Task Requirements: Tailor the order of images and text based on the nature and complexity\n    of the task. For tasks requiring visual or spatial reasoning", "Priming": "n  * Effective Prompt Engineering: The physical sequencing of information has a more significant impact\n    on model performance than relying solely on priming instructions. Ensuring that critical information is\n    presented in an optimal order enhances attention distribution and information encoding within transformer\n    architectures.\n* Model-Specific Prompt Design:\n  * Adapt to Model Sensitivities: Different models may respond uniquely to sequencing based on their\n    training data and fine-tuning processes. Prompt designers should develop model-specific strategies to\n    maximise reasoning accuracy by understanding each model's inherent processing tendencies."}]}