{"title": "Not the Silver Bullet\nLLM-enhanced Programming Error Messages are Ineffective in Practice", "authors": ["Eddie Antonio Santos", "Brett A. Becker"], "abstract": "The sudden emergence of large language models (LLMs) such as\nChatGPT has had a disruptive impact throughout the computing\neducation community. LLMs have been shown to excel at produc-ing correct code to CS1 and CS2 problems, and can even act as\nfriendly assistants to students learning how to code. Recent work\nshows that LLMs demonstrate unequivocally superior results in\nbeing able to explain and resolve compiler error messages-for\ndecades, one of the most frustrating parts of learning how to code.\nHowever, LLM-generated error message explanations have only\nbeen assessed by expert programmers in artificial conditions. This\nwork sought to understand how novice programmers resolve pro-gramming error messages (PEMs) in a more realistic scenario. We\nran a within-subjects study with n = 106 participants in which\nstudents were tasked to fix six buggy C programs. For each pro-\ngram, participants were randomly assigned to fix the problem using\neither a stock compiler error message, an expert-handwritten error\nmessage, or an error message explanation generated by GPT-4. De-spite promising evidence on synthetic benchmarks, we found that\nGPT-4 generated error messages outperformed conventional com-piler error messages in only 1 of the 6 tasks, measured by students'\ntime-to-fix each problem. Handwritten explanations still outper-form LLM and conventional error messages, both on objective and\nsubjective measures.", "sections": [{"title": "1 INTRODUCTION", "content": "For decades, students learning how to code have struggled with\nerror messages [5]-whether they are emitted by compilers or run-time systems, programming error messages (PEMs) have had a\nreputation for being terse [2], inadequate [8], and unreadable [12].\nError messages from C and C++ compilers especially have been\nshown to be deficient debugging tools [43].\nRecent advances in generative Al have resulted in tools like\nChatGPT and GitHub Copilot. These tools, based on large language\nmodels (LLMs), have revolutionised several fields including comput-ing education [4]. Recent work has shown that LLMs can produce\nacceptable programming error message explanations [22] which\nbecome more accurate with larger models and more source code\ncontext [39, 48]. However, it is unknown to what extent that novice\nprogrammers are able to effectively utilise these automatically gen-erated error message explanations to debug their programs.\nIn this study, we had 106 students from an introductory pro-gramming module partake in a within-subjects study that had\nparticipants fix a number of buggy programs. For each program,\nparticipants were shown either a conventional compiler error mes-sage, an expert-handwritten error message, or an LLM-generated\nexplanation. The LLM used to enhance error messages was GPT-4,\nwhich at the time of the study, was the LLM used in the paid version\nof ChatGPT [30]. We measured both how long it took participants\nto resolve errors with each condition, as well as asking students\ntheir opinions on their debugging experience."}, {"title": "1.1 Contributions & Research Questions", "content": "We provide empirical evidence demonstrating that students ap-pear not to be any faster at resolving errors when given GPT-4\nerror message explanations compared to stock compiler error mes-sages. Even though students are not any faster, they still prefer\nGPT-4's explanations to conventional compiler error messages.\nHowever, expert-handwritten are superior to both. We posit that\nerror message usability is more complex than whether or not the\ntext presented to the student contains the correct solution for the\nproblem.\nThe following questions guide this research:\nRQ1 How quickly do students resolve error messages when given\nLLM-enhanced explanations in comparison to stock compiler\nerror messages and handwritten explanations?\nRQ2 Which style of error message do students prefer?"}, {"title": "2 BACKGROUND AND RELATED WORK", "content": ""}, {"title": "2.1\nProgramming Error Messages", "content": "Programming error messages (PEMs) are the diagnostic messages\npresented to coders when an error is detected in their program-\neither due to a mistake in syntax or spelling in the source code,\nor due to some unrecoverable runtime condition, such as a divi-sion by zero, or an invalid memory access [5]. PEMs have been\nan obstacle to learning how to code since almost the inception of\nprogramming [2, 8, 47]. Little progress has been made to improve\ncompiler and runtime error messages, despite decades of guidelines\nproposed to improve them [1, 12, 16, 19, 23, 43]. There have been\nmany attempts at having programming systems produce better\ndiagnostics [9, 20, 25, 37, 40], however, error message enhance-ment has seen weak [3, 6, 15, 34] to insignificant [11, 32] results in\nimproving student outcomes."}, {"title": "2.2 Large Language Models and CS Education", "content": "A confluence of advances in model architecture; novel text repre-sentation; massive, curated datasets; and sheer computing power\nhas rapidly enabled the development of large language models\n(LLMs) [36]: models with billions or even trillions of parameters,\ncapable of capturing the structure and predictability of text to such\nan extent that they are able to exhibit \u201cemergent\" behaviours, like\nquestion answering, analogical reasoning, and even the ability to\nexecute programs [45].\nIn computing education, LLM-powered tools have been shown\nto ace CS1 [13] and CS2 [14] exams and provide increasingly ac-curate error message explanations [22, 39, 48]. LLMs have even\nenabled brand new pedagogical approaches [10, 28]. Educators are\ngrappling with how to integrate LLMs into their practice [4]-if\nat all [21]. Without guidance, complete novices struggle to write\nthe prompts that would complete their assignments [28]. Addition-ally, novices exhibit a number of unproductive interaction patterns\nwhen using LLM-assisted code completion [35, 44]. Some program-mers do not complete programming tasks faster with LLM-assisted\ncode completion, and in fact, are more likely to fail programming\ntasks [44]. Having an LLM that performs better in synthetic bench-marks results in \"relatively indistinguishable differences in terms of\nhuman performance\" [27]. Despite the lack of improvement, novices\nexpress a preference for using LLMs and chatbots [31, 35, 44]. How-ever, more experienced students express concern with how LLMs\nmay hamper their learning [33].\nLLMs and PEMs. LLMs have been found to be useful at explaining\nprogramming error messages on synthetic benchmarks. Leinonen\net al. [22] used OpenAI Codex to explain Python error messages.\nThey found that the best, most accurate explanations and fixes\nwere obtained when providing source code in the prompt, as well as using a temperature value of 0 (explained in section 3.1). Their\nprompt forms the basis of the prompt that was used in our study.\nSantos et al. [39] and Widjojo and Treude [48] have similar findings:\nproviding Java source code in the prompt produces significantly\nbetter error message explanations. Additionally, more advanced\nmodels like GPT-4 are more likely to output accurate explanations\nand fixes than GPT-3 and Codex."}, {"title": "3 METHODOLOGY", "content": "We conducted a within-subjects study, inspired by prior work [44].\nEach participant observed all three study conditions-control, hand-written, and GPT-4 (Section 3.1). Each participant was tasked to fix\nall six buggy C programs (Section 3.2). Both condition assignment\nand task assignment were randomised to counterbalance responses,\nsuch that we would obtain a roughly equal amount of responses for\neach task/condition pair. Randomising participants' assignments\nalso helped to mitigate the learning effect. Having participants fix\nbugs under all three study conditions allowed them to directly com-pare the different error message styles to one another, and report\nwhich style they preferred. The study began with a short question-naire, after which participants were given an in-person briefing,\nwhich was followed by the six debugging tasks. After participants\nhad completed all six tasks, we asked participants questions to\ncompare the three error message styles directly. The remainder of\nthis section describes the study conditions, the tasks, and study\nprotocol in greater detail."}, {"title": "3.1 Study conditions", "content": "Each participant saw error messages presented in all three study\nconditions: GCC (control), handwritten, and GPT-4. However,\nthe order of the study conditions was shuffled for each student, so\nthat participants could directly compare the three study conditions\nthemselves. Since there were six tasks, but only three conditions,\nthe order of the study conditions was simply repeated for the latter\nthree tasks-for example, if a participant was assigned handwritten\nfor the first task, GCC for the second, and GPT-4 for the third, then\nthey would be assigned handwritten for the fourth, GCC for the\nfifth, and GPT-4 for the sixth.\nControl: GCC. For the control condition, students were presented\nwith error messages directly obtained from the GCC 13.2.0 C com-piler (Figure 1a). Whenever a single programming error would\ninduce multiple spurious, cascading error messages, we would only\nshow the first error message, as advised by Becker et al. [7].\nHandwritten error messages. Error message explanations (Fig-ure 1b) were handwritten by the first author. These explanations\nwere written in response to the problems present in the source\ncode, but not necessarily in response to any error messages emitted\nby GCC. Importantly, the handwritten explanations were finalised\nbefore error message explanations were obtained from GPT-4. There-fore, the author of the handwritten explanations was not influenced\nby GPT-4's output. Every message was written in a consistent struc-ture: first was a line beginning with the word Error: which states\nwhat the detected error is, followed by a relevant excerpt from\nthe source code. Then one or more sections beginning with the\nword Help: or Note: would either suggest a possible solution, or\nhighlight relevant information to fix the problem. The structure of\nthe messages was greatly inspired by the diagnostics emitted by the\nRust compiler, with source code excerpts mimicking the structure\nof Rust's \"diagnostic windows\" [38]. The handwritten explanations\nwere written in such a way that they can plausibly be generated by\nan actual compiler, given sufficient context.\nGPT-4 enhanced error messages. After the handwritten error ex-planations were written, we obtained error message explanations"}, {"title": "3.2 Tasks", "content": "Students were to fix all of the following C programming errors, in\na randomly assigned order. All programs in this study caused GCC\nto emit compiler error messages. We did not have students debug\nproblems that would result in runtime errors (e.g., no segmentation\nfaults). The following are all six of the debugging tasks:\n(1) Flipped assignment. The left- and right-hand sides of an as-signment statement were swapped such that the assignment\ntarget would be on the right-hand side, e.g., a + b = c.\n(2) #const instead of #define. A program was made attempting\nto define a constant called PI using the syntax #const PI\n3.14. This programming error intentionally conflates C's\n#define preprocessor directive with the const type qualifier,\nboth being valid ways to define a constant in C.\n(3) Using a keyword as a name. The program attempts to\ncreate new variables called union and nonUnion, however,\nunion is a reserved word, and cannot be used as an identifier.\n(4) Missing parameter. A function was defined to convert from\nFahrenheit to Celsius, however, the function definition lacks\nany formal parameters. Despite this, the body of the function\nused a identifier fahrenheit to perform the conversion, and\nthe function is called with an argument for the temperature\nin Fahrenheit.\n(5) Missing curly brace. The opening curly brace of a func-tion definition was omitted, causing GCC's parser to take a\n\"garden-path\" and completely misinterpret the program.\n(6) Reassigning a constant. A formal parameter was declared\nconst, then reassigned within the function body."}, {"title": "3.3 Participants", "content": "Participants were recruited from a class at a large research-intensive\nEuropean public university that would be an R1 in the US Carnegie\nClassification. The class was the second semester of the first-year\nprogramming sequence (CS1) for CS majors, taught in the C pro-gramming language. In total, 113 participants were recruited across\ntwo separate lab sections. Of those, n = 106 participants (94%) com-pleted the study. Of the participants that completed the study, 78\nidentified as men, 23 as women, and 5 chose not to disclose their\ngender. At the beginning of the study, we asked participants a few\nquestions on their experience in programming. The most commonly\nreported experience level was between 0-3 months (32 participants).\nCuriously, 8 participants reported absolutely zero experience in"}, {"title": "3.4 Protocol", "content": "The study was conducted during a regularly scheduled lab ses-sion in late January 2024-the second scheduled lab of the semester-held simultaneously in two separate classrooms. Students were\nunder no obligation to participate in the lab session; they were not\ncompensated for participation, and participation did not affect the\nstudents' grade in any way.\nOnce in the classroom, the study was conducted entirely via a\ncustom web-based survey platform, which combined multiple ques-tionnaires with an online IDE (Figure 2). The online IDE component\nwas created using Microsoft's Monaco Editor, the core text edit-ing component of Visual Studio Code [24]. Upon hitting the \"Run\"\nbutton in the IDE, the full source code was securely transmitted\nto a university-managed server, where it would be stored. Code\nwas compiled and run in a sandboxed environment provided by the\nPiston code execution engine [41].\nAt the start of the study, the first author was present and gave\na quick oral briefing. After the briefing, students were directed\nto the web platform, where they signed an online consent form\nto commence the study. First, participants were asked a few de-mographic questions and asked about their attitudes regarding\nprogramming and error messages. After this, participants started\nthe six debugging tasks.\nFor each task, students were presented with the buggy code\nin the web IDE (Figure 2) and were instructed to fix the problem,\nhitting the \"Run\" button and only submitting their solution once\nthe problem was resolved. We started measuring the time taken to\ncomplete the task from the moment the code editor loaded. After 5"}, {"title": "4 RESULTS", "content": ""}, {"title": "4.1 Objective measures", "content": "The primary quantitative measurements that we recorded were\nthe time-to-fix for participants who successfully fixed a task, and\nwhether or not a student skipped a particular task. For each task/-\ncondition pair, we obtained between between 27-44 samples, which\nis sufficient to perform within-task comparisons.\nTime-to-fix. Time-to-fix is consistently right-tailed, so we log-transformed the data to perform statistical comparisons. We noticed\nthat each task given had an effect on time-to-fix, so we performed\nwithin-task comparisons. For each task, we performed a one-way\nANOVA to compare the effect of the study condition on students'\nlog-transformed time-to-fix (Figure 3). A one-way ANOVA revealed\nthat there was a statistically significant (p < 0.05) difference in\nthe mean log-transformed time-to-fix between the conditions in\nall tasks, except for the \"flipped assignment\" task, in which no"}, {"title": "4.2 Subjective measures", "content": "Opinion. After each exercise, we asked participants four Likert-type questions to gauge their general opinion on how useful the\nmessage was for fixing the problem (Figure 4). We performed a\nlinear regression to predict participants' overall opinion given the\nstudy conditions. We modelled opinion as a numerical variable\nwhere Strongly disagree = -2, Disagree = -1, Neutral = 0, Agree =\n1, and Strongly agree = 2, then took the mean of a participant's\nresponses per each condition. Using a handwritten message results\nin a 1.27 point increase in opinion compared to traditional compiler\nerror messages (p < 0.001); whereas using a GPT-4 generated\nmessage results in a 0.69 point increase in opinion compared to the\ncontrol (p < 0.001). Overall, we found that participants rated both\nthe handwritten messages and GPT-4 explanations higher than\nGCC's error messages, with the handwritten error messages being\nthe most highly rated. Participants rated GPT-4 error messages\nhighly in terms of being useful to help them solve the error, despite\nboth conditions suggesting equivalent fixes.\nMessage length. In addition to opinion, we also asked participants\nto rate the length of the error message after each exercise, on a"}, {"title": "5 DISCUSSION", "content": "Despite promising evidence in prior studies [22, 39, 48], GPT-4 error\nmessage explanations do not help novices when they are resolving\nerror messages as much as one would expect. In fact, in one of\nthe tasks (\"missing parameter\"), students were slower when using\nGPT-4's explanation. Curiously, expert-handwritten error messages\noutperform GPT-4 error message explanations even though both\nsuggested equivalent solutions for each problem.\nWhen it comes to students' preferences, they preferred GPT-4's\nerror messages over GCC's terse, jargon-heavy error messages.\nThis makes sense, as GPT-4 would always produced full, complete\nsentences-a factor that was previously found to be important to\nerror message readability [12]. We were surprised that participants\ndid not report GPT-4's error messages as being too long. That said,\nstudents were unable to use these messages effectively, even though\nGPT-4's messages would always provide the correct way to solve\nthe programming error. Prior work has found that longer error\nmessages do not seem to help students [29].\nProgramming is (still) hard. Early results in understanding LLMs'\ncapabilities at introductory programming seemed promising [13,\n14, 22, 39], inspiring researchers to declare a new era for comput-ing education [4] and even \"the end of programming\" [46]. How-ever, it seems that LLMs are not the transformative tool that they\nonce seemed. Our findings-that, despite excelling in synthetic\nbenchmarks, LLMs do not significantly improve programmers'\nproductivity-are corroborated by a number of studies [17, 27, 28,\n44]. Additionally, participants express preference for LLMs [35, 44],\neven though LLMs' answers do not make them more effective at\nresolving programming errors. Simkute et al. [42] argue that this\nsupposed contradiction of LLM productivity is an already well-known phenomenon in human factors research: automation alters\npeoples' workflows in unproductive ways, such as turning active\nproducers into passive evaluators. This change in workflow widens\nthe gap between the programmer's mental model, and what the pro-grammer ought to attend to while solving problems. In fact, there is\nevidence that LLM-powered code suggestion alters programmers'\nworkflows in ways that hamper productivity [26, 35]. LLMs have\nnot delivered on the promise of natural language programming [28];\nrather, they provide an indirect method of manipulating an existing\nabstraction: high-level source code. Similarly, LLMs have not funda-mentally altered the task of debugging; they just explain the already\ndifficult problem in a more approachable manner. Thus, debugging\nremains just as difficult as it was prior to the introduction of LLMs."}, {"title": "5.1 Limitations", "content": "The generalisability of these results is limited by the sample of\nparticipants: all from one class at a European research university,\ntaught in one programming environment. Additionally, since the\ndebugging tasks were created for the purpose of this study, the"}, {"title": "6\nCONCLUSION", "content": "We conducted a within-subjects experiment where novice program-mers fixed six buggy programs using three different error message\nstyles. In contrast to results on synthetic benchmarks, GPT-4 en-hanced error messages were only more effective than stock compiler\nerror messages in one of the six programming tasks. Handwritten\nerror messages were more effective than the control in four of six\ntasks, and more effective than GPT-4's error message explanations\nin five of six tasks. Overall, students preferred GPT-4's messages\nover stock compiler error messages, but preferred the handwritten\nerror messages even more. This is despite the fact that the GPT-4\nerror message explanations and the handwritten error messages\nboth made the same suggestion to fix the problem. Future work\nshould further understand what factors make an error message\nusable and how programming environments and LLMs alike can be\nmodified to satisfy novices' needs. It appears we still have a long\nway to go to reach \"the end of programming\"."}]}