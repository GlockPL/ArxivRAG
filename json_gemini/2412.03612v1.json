{"title": "Chatting with Logs: An exploratory study on Finetuning LLMs for LogQL", "authors": ["Vishwanath Seshagiri", "Kaustubh Dhole", "Siddharth Balyan", "Ishan Sharma", "Jos\u00e9 Cambronero", "Vaastav Anand", "Avani Wildani", "Andreas Z\u00fcfle"], "abstract": "Logging is a critical function in modern distributed applications, but the lack of standardization in log query languages and formats creates significant challenges. Developers currently must write ad hoc queries in platform-specific languages, requiring expertise in both the query language and application-specific log details an impractical expectation given the variety of platforms and volume of logs and applications. While generating these queries with large language models (LLMs) seems intuitive, we show that current LLMs struggle with log-specific query generation due to the lack of exposure to domain-specific knowledge.\nWe propose a novel natural language (NL) interface to address these inconsistencies and aide log query generation, enabling devel-opers to create queries in a target log query language by providing NL inputs. We further introduce NL2QL, a manually annotated, real-world dataset of natural language questions paired with cor-responding LogQL queries spread across three log formats, to pro-mote the training and evaluation of NL-to-loq query systems. Using NL2QL, we subsequently fine-tune and evaluate several state of the art LLMs, and demonstrate their improved capability to generate accurate LogQL queries. We perform further ablation studies to demonstrate the effect of additional training data, and the trans-ferability across different log formats. In our experiments, we find up to 75% improvement of finetuned models to generate LogQL queries compared to non finetuned models.", "sections": [{"title": "1 Introduction", "content": "As modern web and mobile applications are increasingly deployed as microservices, observability data (e. g., Metrics, Logs, Events, Traces, etc.) are collected from various applications during the ap-plication runtime, and the tooling used to collect and store this data increasingly plays a critical role in modern cloud deployments of systems [10]. Observability tools are crucial for understanding how the systems work at scale as they provide insight into key tasks including predicting resource requirements [30, 37, 38], diag-nosing faults [11, 21, 25, 55], identifying security breaches [1], and performing regular system checks [43]. Despite the importance of observability tooling, there is a lack of standardization of how a user interacts with the different tools [34].\nLog data is typically collected and accessed through various pro-prietary platforms, each of which have their own query language [34]. Querying this data is invariably ad hoc and challenging, in-volving exact matches to a log format, and keywords to search over a large database [19, 20, 44]. Moreover, there is little to no syn-tactical overlap between these languages, necessitating significant developer retraining when moving between products.\nLogs are often used to power insights dashboards.\nIn this work, we take the first steps towards building realistic NL interfaces for generating log queries. To achieve this, we first create a dataset of natural language to LogQL queries and use it to fine-tune a suite of popular LLMs creating LoGQL-LM, a system designed to convert natural language questions into LogQL queries. LogQL is a specialized query language for searching and analyzing log data within Grafana's open-source log aggregation system, Loki [22]. The selection of LogQL was motivated by its open-source nature and the extensive availability of Grafana dashboards (Fig 1a), which enable the formulation of realistic natural language queries, and its support in various open source LLMs.\nThrough our exhaustive evaluation, we demonstrate that fine-tuning on our dataset significantly enhances the performance of popular LLMs, including GPT-40, Llama 3.1, and Gemini, in gener-ating accurate LogQL queries. Our experiments reveal that GPT-40 achieves up to 75% and 80% improvements in accuracy and F1 re-spectively, with fine-tuning enhancing query outputs, reducing syntax errors, and improving label matching and temporal aggrega-tion. We perform further ablation studies to evaluate the effects of the number of training examples and the potential transferability of these models across applications.\nSpecifically, this paper makes the following contributions:\n\u2022 First, we present and release a dataset-NL2LogQL-designed to facilitate the development and benchmarking of natu-ral language to LogQL systems, with a particular focus on fine-tuning Large Language Models (LLMs) to gener-ate syntactically and semantically correct LogQL queries. NL2LogQL consists of 424 manually curated natural lan-guage to LogQL pairs. Each pair is derived from a panel in a Grafana Community Dashboard, covering three dis-tinct applications. The dataset was constructed by manually describing the purpose of each panel in natural language and crafting the corresponding LogQL query. This resource represents the first dataset specifically designed to enable an NL-to-LogQL interface.\n\u2022 Second, we present a web-based interface that enables de-velopers to generate LogQL queries for the aforementioned applications. This interface serves a dual purpose: it pro-vides a practical tool for query generation and acts as a"}, {"title": "2 NL Interface for Log Search", "content": "In this section, we present the challenges associated with querying logs, highlight specifics of an open-source log query language called LogQL, and present an initial sketch of how we can use LLMs to translate natural language queries to LogQL."}, {"title": "2.1 Challenge: Querying Logs is Difficult", "content": "Existing tools for storing and querying log data to obtain insights present a significant usability challenges for many reasons. We highlight these challenges below.\nSteep Learning Curve. These tools require the developers to learn and use esoteric tool-specific query languages, that have a steep learning curve [34]. Due to the unintuitive nature of these languages, users often struggle with constructing effective queries to find specific log entries [4]. Consequently, only a small percent-age of power users within an organization can leverage the full capabilities of log analysis tools [6], hindering the democratiza-tion of log analysis. Recent studies [8] have also highlighted the challenges faced by new team members in using existing tools, requiring significant time to gain proficiency.\nInsufficient Context. In the context of DevOps [43, 49], where the developer and the operator are usually different individuals thus the person writing queries often lacks sufficient context. To construct effective queries, the operators often require detailed knowledge of log lines. Operators often lack appropriate context as they're dealing with unfamiliar logs, or context switching between multiple tools and dashboards [9, 14].\nLarge and unstructured logs. The complexity of writing these queries increases due to the high volume of application logs and their varying formats. For example, to write the LogQL query in Fig. 1b, the developer must know syntax of the log querying lan-guage (LogQL) and also the semantics the log file \u2013 such as \"sshd[\", \"session opened for\" - that are required to construct this query.\nDue to these inherent complexities, log data analysis remains predominantly within the domain of software developers who pos-sess intricate knowledge of the logging systems. The utilization of log data presents significant untapped potential for informing strategic business decisions [27]. For example, insights from HTTP header parsing can optimize marketing campaigns by understand-ing regional traffic patterns, and analysis of distributed traces can"}, {"title": "2.2 Background: LogQL", "content": "LogQL is a query language designed for searching and analyzing log data in Grafana's log aggregation system, Loki [22]. Loki focuses on indexing metadata rather than full log text. LogQL provides tools for filtering, aggregating, and extracting insights from log streams, sup-porting both log and metric queries, with a structure that includes label selectors, line filters, and time range specifications.\nLogQL provides users with tools to filter, aggregate, and extract insights from log streams, making it valuable for monitoring, trou-bleshooting, and maintaining complex distributed systems. Opera-tors typically write a LogQL query per panel that are then arranged together to form a dashboard (for example, the dashboard in Figure 1a). The language supports two primary query types: log queries for retrieving and filtering log content, and metric queries for applying aggregation functions to transform log data into numerical time series.\nAs Loki's indexing strategy focuses on the metadata associ-ated with log lines, rather than the full log text, this indexing approach has implications for LogQL queries, as they must in-clude the relevant tags to search the indexed metadata effectively. For example, given the log line [2019-12-11T10:01:02.123456789Z {APP=\"NGINX\", CLUSTER=\"US-WEST1\"} GET/ABOUT], Loki will index the timestamp and the labels attached to the log line, such as \"app\" and \"cluster\", but not the actual log text starting from \"GET\". Since the indexing of the logs is based on timestamp, the queries are relative to the current system time. Thus, LogQL provides labels to allow filtering log lines using metadata. Labels are key-value pairs associated with log streams, providing metadata about the logs' origin and characteristics.\nConsider the human written query in Figure 2b (green colour) to quantify the occurrence of authentication-related service un-availability errors in an OpenStack deployment within the Asia-Pacific region over the past 30 days. It begins with label selectors enclosed in curly braces: {job=\"openstack\", region=\"asia-pacific\"}. The job=\"openstack\" label identifies logs from OpenStack services, while region=\"asia-pacific\" narrows the focus to the Asia-Pacific region. Following the label selectors are two line filters: /= \"503\" and |= \"token validation\". These filters use the /= operator to perform case-sensitive matches, selecting log lines containing the HTTP status code 503 (indicating a service unavailable error) and men-tioning \"token validation\". The query concludes with a time range"}, {"title": "2.3 Our Vision: LLM assisted query generation", "content": "The heterogeneity of log query languages necessitates enhanced query composition interfaces. While existing \"query builder\" in-terfaces ensure syntactic correctness, they depend on developers' expertise in log line selection. LLMs have shown efficacy in log-related tasks, leveraging their ability to process unstructured text. However, direct LLM application to log search presents challenges in handling large-scale data, efficient indexing, and real-time capa-bilities [4].\nWe propose utilizing LLMs for log query generation, balancing accessibility and efficiency. This approach, applied successfully in SQL generation [31] and data analysis [50], leverages LLMs' natural language understanding to translate search intents into optimized queries. This method bypasses complex indexing requirements while maintaining LLM capabilities, enabling execution through existing log search systems. It reduces the query language learning curve, facilitates faster iteration for experienced developers, and aligns with engineers' mental models of their systems.\nTo adapt LLMs for specific tasks, two lines of approaches have been employed in the past: (i) In-Context Learning (ICL) [3]; and (ii) fine-tuning pre-trained models with task-specific examples. We discuss the potential and limitations of both in generating LogQL queries below.\nICL incorporates task-specific demonstrations into the input during inference, guiding the model without parameter retraining. However, incorporating large log files is impractical for smaller models due to limited context windows [5], and models with larger context windows often exhibit instability and reduced robustness [26]. Similar to SQL generation issues [16], LLMs often generate non-existent log lines. To demonstrate this, we devised a prompt with documentation, examples, and instructions for generating queries in Datadog Query Language (DQL), LogQL, and grep.\nFine-tuning, particularly few-shot tuning, offers significant advantages for adapting pre-trained LLMs to specific tasks such as LogQL generation. This approach involves re-training the LLM on a tailored dataset, allowing the model to adjust its internal parameters and better align its outputs with desired outcomes. Few-shot tuning enables LLMs to generalize from limited examples, facilitating the extraction of relevant information across diverse log formats and applications. This is particularly crucial given the often ad hoc nature of log files, which lack standardized logging procedures. By providing more diverse log examples during the training phase, fine-tuning enhances the model's ability to handle varied log structures. Prior studies [41, 52] have demonstrated that few-shot tuning offers superior accuracy at lower computational costs for related tasks like text-to-SQL. Moreover, the efficiency of few-shot tuning, requiring only a small number of data samples, results in a rapid fine-tuning process without significant time overhead.\nImportantly, few-shot tuning eliminates the need for continuous in-context demonstrations during inference, potentially reducing"}, {"title": "3 LOGQL-LM", "content": "We first define the problem of translating a natural language log-query to LogQL as follows.\nDefinition 3.1 (NL2LogQL). Let DB be an unstructured logfile, let $q_{NL}$ be a query specified in natural language, and let $q_{LOG}$ be the corresponding query expressed in LogQL, that when exe-cuted against DB can provide (semantically correct) intended query answer a.\nWe define the NL2LogQL problem as finding a mapping of any natural language query q to a syntactically and semantically correct LogQL query:\n$NL2LogQL(q_{NL}) \\rightarrow q_{LOG}$\nSuch a query can then be executed against DB directly to produce the intended answer.\n$q_{LOG}(DB) \\rightarrow a$\nTo train such a mapping NL2LogQL, and evaluate its logging efficacy, we first require a dataset that provides us with example tuples (DB, $q_{NL}$, $q_{LOG}$, a) each having a logfile DB, a natural lan-guage query $q_{NL}$, a ground truth correct LogQL query $q_{LOG}$, and the output of executing the query, a. We manually create such a dataset having 424 example tuples as described in Section 3.1. Using this dataset, we describe our approach to fine-tuning existing large language models to automatically map any new natural language query for any new logfile to the intended LogQL query."}, {"title": "3.1 Dataset", "content": "As defined earlier, to effectively fine-tune a model to tranform natural language queries to their LogQL counterparts, we require a comprehensive dataset consisting of (DB, $q_{NL}$, $q_{LOG}$, a) tuples.\nData Sources. Constructing a dataset of such records necessi-tates both realistic logs and natural language questions to ensure the model's applicability for operators writing queries on their ap-plications. We source logs from the LogHub 2.0 dataset [56], which encompasses logs from diverse applications and includes various event extraction templates for log parsing tasks. To obtain realistic NL queries, we analyze the Grafana Community Dashboards [40],"}, {"title": "3.2 Finetuning LLMs", "content": "In this section, we present the models used for fine-tuning and prompting for the task of NL2LogQL.\nFor finetuning, we require models that can excel at various cod-ing and reasoning tasks and can learn to specifically generate syn-tactically and semantically correct LogQL queries."}, {"title": "3.3 Metrics", "content": "Similar to prior work in text-to-SQL [42], we evaluate the perfor-mance of the our finetuned models using Exact Match and Exe-cution Accuracy. To assess Execution Accuracy, we compared the results returned by the LogQL queries generated by our models to the results from manually written reference LogQL queries. Since there are two main types of LogQL queries, Metric and Log, we used different metrics tailored to each.\n\u2022 For Metric queries, which return a numerical value, we compared the model's output and expected output. We find the output accurate if it is exactly the same as the expected output, and any deviation is considered a wrong output, for floating point outputs we compare up to two decimal places rounded up.\n\u2022 For Log queries, which return a list of relevant log lines, we computed the precision and recall of the model's output log lines compared to the reference log lines. We calculate the F1 score, which is the harmonic mean of precision and recall, as a summary metric. These metrics evaluate how well the model's queries are filtering the logs to surface the most pertinent information. Since LogQL results are always sorted by timestamp, we did not need to compare the relative ranking of the model and reference outputs the ordering is guaranteed to be consistent as long as the same logs are returned.\nAlong with these metrics, we also make use of Perplexity Score [51], to assess the language model's ability to predict the next token in a sequence, providing a measure of how well the generated code aligns with the model's learned probability distribution. A lower perplexity score indicates that the model is more confident and accurate in its code predictions.\nWhile comparing the output of the model is meaningful for checking the end result, it doesn't account for the syntax lapses causing the outputs to be dramatically different. Relying solely on output-based evaluation methods can be misleading, as they may fail to capture the underlying issues in the generated LogQL queries. To address these limitations, we assess the exact match between the generated query to ground truth query.\nWhile Perplexity scores offers an intrinsic metric for the confi-dence of a model in generating accurate LogQL queries, it doesn't provide an extrinsic metric. To mitigate this challenge, we make use of CodeBERTScore(CBS) [54]. CBS is a pre-trained language model specifically trained for evaluating code outputs in various programming languages. By finetuning CBS on a subset of LogQL queries from our dataset, we evaluate the generated LogQL queries based on their semantic similarity to reference queries and their adherence to the syntactic rules of the LogQL language. The CBS evaluates the similarity between generated and reference LogQL queries using cosine similarity, producing values between 0 and 1. A score of 1 indicates perfect semantic and functional equivalence, while 0 represents complete dissimilarity. Scores above 0.7 strongly correlate with high query quality and correctness as judged by human evaluators, whereas scores below 0.4 typically indicate sig-nificant functional deficiencies. The metric's relative nature means it is most meaningful when comparing queries within the same evaluation context."}, {"title": "3.4 Demonstration", "content": "To facilitate comprehensive model comparison and enhance user interaction, we developed a web-based interface that enables si-multaneous evaluation of multiple LLM responses. As illustrated in Figure 3, the interface presents a streamlined design with a promi-nent query input field at the top, where users can formulate natural language questions about log analysis. Upon submission, the system concurrently processes the query through three distinct fine-tuned models: GPT-40, Llama-3.1, and Gemma2. The responses are dis-played in parallel panels, each showcasing the generated LogQL query along with its response time. For example, when querying about instance build times in OpenStack deployments, each model generates specialized LogQL syntax incorporating regex patterns and temporal aggregations. The interface displays response times (2.9s, 49.56s, and 67.8s respectively), enabling quantitative perfor-mance comparison. This parallel visualization approach not only facilitates direct comparison of query formulation strategies but also provides valuable feedback mechanisms for continuous model improvement. The comparative layout effectively highlights the nuanced differences in how each model interprets and translates natural language queries into LogQL syntax, contributing to our understanding of model behavior and performance characteris-tics."}, {"title": "4 Evaluation", "content": "To validate our natural language interface for LogQL query genera-tion, we established a comprehensive evaluation framework focus-ing on the models' capability to generate executable queries that yield accurate results. The evaluation framework covered several key areas: first, a comparison between fine-tuned and baseline mod-els; second, an analysis of how the size of the fine-tuning dataset affects model performance; third, an exploration of how well the model transfers across different application domains; and finally,"}, {"title": "4.1 Performance of finetuned models", "content": "In this experiment, we wanted to test the effeciency of the fine-tuned models in generating LogQL queries compared to the base model. We used 50% of the samples from each application for fine-tuning the models using the method described in Section 3.2. Our experimental evaluation of the NL2LogQL translation models re-veals significant improvements in accuracy and F-1 score through finetuning, as illustrated in Table 3. Most of the queries generated were executable, except for 10% of the queries that had wrong syntax such as no log lines after filters or had ill-formed regular expressiones. Among the finetuned models, GPT-40 exhibited the strongest performance, achieving remarkable post-finetuning accu-racy scores ranging from 0.74 to 0.82 and F1 scores between 0.62 and 0.74, up from pre-finetuning metrics of 0.21-0.28 for accuracy and 0.16-0.19 for F1 scores. Llama-3.1 showed significant enhancement, with accuracy improving from below 0.05 to approximately 0.50 across applications, alongside F1 scores rising from around 0.05 to the 0.42-0.59 range. While Gemma-2 demonstrated more modest gains, it still showed meaningful improvements, with accuracy in-creasing from below 0.07 to 0.25-0.35 and F1 scores improving from 0.11-0.14 to 0.31-0.47. The perplexity scores further support these findings, with GPT-40 achieving the lowest perplexity (9.8-10.7), followed by Llama-3.1 (15.2-22.5), and Gemma-2 (27.7-38.0), indi-cating superior model coherence and predictive capability. Most of the correct responses from the models before finetuning came from providing a lot of context to the model for generating the LogQL query.\nBefore finetuning, the LLM generated queries exhibited several common errors: incorrect label usage (e.g., \"app\" instead of \"application\u201d), syntax errors in timestamp placements, wrong filter specifications, invalid grouping syntax, and improper matching operators.\nThe fine-tuned models exhibited substantial enhancement in query generation capabilities, producing syntactically valid and executable LogQL queries. These improvements resulted in queries that could ac-curately capture and format the desired log data while maintaining proper syntax and execution capability.\nThe experimental results demonstrate that finetuning significantly enhanced the performance of all three LLM models in generat-ing LogQL queries, with GPT-40 showing the most impressive improvements in Accuracy and F1 scores by up to 75% and 80% respectively. Post-finetuning, the models produced 20% more exe-cutable queries with fewer syntax errors, improved label matching, and better temporal aggregation, highlighting the effectiveness of the finetuning process in enhancing the models' ability to generate accurate and functional LogQL queries."}, {"title": "4.2 Effect of number of finetuning samples", "content": "In the previous experiment, we looked into the effect of finetuning for enhancing the ability of the models to generate LogQL queries compared to the base model. Previous works from other log related tasks [29], and text2sql [31] have shown that the performance of models change based on the number of samples used for finetuning. Constructing the dataset for finetuning these models is an arduous task as detailed in the previous section (\u00a73.1), thus we explore the effect of varying the number of samples used for finetuning the model. To perform this experiment, we allocated 20% of samples from each application as a \"test set\", and remaining dataset for training the finetuned models. The number of finetuning samples"}, {"title": "4.3 Transferability of the finetuned models", "content": "To evaluate the models' generalization capabilities across different applications, we conducted cross-application experiments where models were finetuned on two applications and subsequently tested on a third, previously unseen application. For instance, to assess query generation capabilities for OSSH logs, the models underwent finetuning using query datasets from OSTK and HDFS applications, thereby testing their ability to transfer learned patterns to a novel application context.\nThe results reveal that while cross-application fine-tuned models are inferior to application-specific fine-tuned models in most cases, they generally outperform their non-finetuned counterparts."}, {"title": "4.4 Code Quality Analysis", "content": "For analyzing the quality of the code produced by various finetuned models, we make use of CodeBertScore [54] (CBS) to evaluate the quality of the LogQL query generated by the finetuned models.\nFinetuning CBS Since the current CBS model doesn't support LogQL, we finetuned the CBS model to be able to score the outputs for the model. We used 50% of application specific LogQL queries to finetune the CBS model, and used 30% of the dataset to finetune the LLMs, and tested it on 20% of the dataset by comparing the output of the finetuned models with the correctly written LogQL queries in the dataset. GPT-40 consistently achieves the highest Code-BERTScore across all applications. These high scores indicate that GPT-40 generates LogQL queries that are highly similar to the reference queries, both semantically and functionally.\nThe evaluation demonstrates that finetuned LLM models can suc-cessfully generate LogQL queries, with varying degrees of accu-racy across different models. GPT-40 emerged as the top performer, whereas other models showed moderate to lower performance, suggesting their generated queries require more refinement to achieve full functional reliability."}, {"title": "5 Discussion", "content": "This work showed that there is a necessity for enhanced interfaces in observability data query generation. To our knowledge, this re-search presents the first comprehensive data collection effort for fine-tuning models to generate log query language. Our evaluations demonstrate that while base LLMs exhibit limitations in generating LogQL queries, fine-tuning these models significantly enhances their query generation capabilities. Although we present a proof of concept across various applications, practitioners seeking to imple-ment our methodology would need to develop a corpus of natural language to LogQL queries specific to their applications, as accurate query generation necessitates understanding application-specific log semantics. Additionally, the model must generate syntactically valid queries compatible with their internal system's query lan-guage. These considerations directly influence the selection of base models for fine-tuning purposes.\nThe fine-tuned LOGQL-LM models demonstrate superior perfor-mance metrics across both metric and log queries compared to the baseline model. However, as this research represents one of the initial endeavors in LogQL query generation, several limitations, and opportunities for improvement warrant discussion.\nA significant limitation observed in the current implementation is the models' reduced efficacy in generating LogQL queries when there exists semantic divergence between the natural language (NL) query and the corresponding log entries. To illustrate, consider the example in Table 1-row 3, where the objective is to identify recent successful authentication events for user \"fztu\". While the NL query employs the phrase \"successful login,\" the actual log entries uti-lize \"accepted password\" to denote such events. Consequently, the model frequently generates queries containing \u201csuccessful login,\u201d resulting in null result sets. This limitation can be attributed to two primary factors. First, the methodology of reverse-engineering natural language (NL) questions from existing dashboards inher-ently limits the diversity of NL queries that can map to a specific"}, {"title": "6 Conclusion", "content": "This research addresses the Natural Language to LogQL (NL2LogQL) problem, focusing on the generation of log-file queries from natu-ral language input. Our approach advances the state of the art by eliminating two significant prerequisites: the requirement for users to possess expertise in LogQL query language, and the necessity for detailed understanding of log file structure and syntax. To facilitate this advancement, we developed a comprehensive dataset com-prising 424 natural language queries, their corresponding LogQL implementations, and their expected query results across extensive log files. Our methodology leverages fine-tuned large language"}]}