{"title": "The Computational Learning of Construction Grammars: State of the Art and Prospective Roadmap", "authors": ["Jonas Doumen", "Veronica Juliana Schmalz", "Katrien Beuls", "Paul Van Eecke"], "abstract": "This paper documents and reviews the state of the art concerning computational models of construction grammar learning. It brings together prior work on the computational learning of form-meaning pairings, which has so far been studied in several distinct areas of research. The goal of this paper is threefold. First of all, it aims to synthesise the variety of methodologies that have been proposed to date and the results that have been obtained. Second, it aims to identify those parts of the challenge that have been successfully tackled and reveal those that require further research. Finally, it aims to provide a roadmap which can help to boost and streamline future research efforts on the computational learning of large-scale, usage-based construction grammars.", "sections": [{"title": "Learning Computational Construction Grammars", "content": "The aim of this paper is to survey prior work on the computational learning of construction grammars, to identify gaps in the state of the art and to propose a perspective on the future of the field. The computational learning of construction grammars has traditionally been studied independently in different fields of research, in particular linguistics, cognitive science, computer science and artificial intelligence. As a consequence, research on this topic has been fragmented and interaction between the researchers involved has been scarce, as witnessed by a lack of cross-referencing. In order to address this issue, this paper brings together the variety of methodologies that have been proposed in the literature and synthesises the results that have been achieved to date. Specifically, we define 14 criteria in light of which we review 31 models of construction grammar learning. We then identify important gaps in the state"}, {"title": "Methodology", "content": ""}, {"title": "Inclusion Criteria", "content": "The scope of this literature review concerns, to the best of our knowledge, all published prior research on the computational learning of construction grammars, as defined through the following inclusion criteria:\nIs the model concerned with learning form-meaning mappings? This criterion concerns the nature of the linguistic knowledge that is learnt. Concretely,"}, {"title": "Discussion Criteria", "content": "Prior work on learning construction grammars stems from different fields of research and therefore adopts a wide variety of methods, terminologies and experimental designs. In order to streamline the discussion and facilitate a meaningful comparison, we introduce 14 discussion criteria that will guide the review of the included models.\nLearning task. Which learning task does the model address? Which problem is the model designed to solve? Which evaluation criteria are used?\nDataset. To which datasets has the model been applied?\nInput. What is the nature of the input to the model?\nForm complexity. What is the morpho-syntactic complexity of the linguistic input?\nMeaning complexity. What is the semantic complexity of the linguistic input?\nGrounding. Is the meaning representation grounded in a situation model?\nSegmentation level. What level of segmentation of the input is provided to the model (phonemes, graphemes, words or utterances)?\nLexicon. Is a predefined lexicon provided?"}, {"title": "Review of Prior Literature", "content": "This section provides a detailed review of all included models. A comparative analysis of all models in terms of the discussion criteria introduced in Section 2.2 is also provided in Table 1. Readers who are primarily interested in the higher-level picture can safely skip ahead to Section 4, where we present a synthesis of the details covered in this section.\nOn the highest level, the models can be organised according to the overall learning task that they are designed to tackle:\nLearning a maximally concise grammar. The task is to find a minimal set of constructions that optimally covers a corpus of language use."}, {"title": "Learning a grammar from utterance-meaning pairs", "content": "The task is to learn a grammar that maps between utterances and their meaning representation, whereby a gold semantic annotation is provided for each utterance."}, {"title": "Learning a grammar under referential uncertainty", "content": "The task is to learn a grammar that maps between utterances and their meaning representation, whereby a superset of the gold semantic annotation is provided. The referential uncertainty stems from the fact that the exact meaning representation is not provided."}, {"title": "Learning a grammar from a situation model", "content": "The task is to learn a grammar that maps between utterances and their meaning representation, whereby no gold semantic annotation is provided. The meaning has to be abductively derived from a situation model."}, {"title": "Learning a maximally concise grammar", "content": "The first category of models addresses the task of finding a minimal set of constructions that optimally covers a corpus of language use. Dunn (2017) introduces a method to induce schematic patterns from large amounts of web-crawled corpus data. These patterns take the form of a sequence of slots that can be filled by word forms, morpho-syntactic categories or semantic categories. These categories are provided in the form of annotation layers. The resulting grammars are evaluated against a held-out test set in terms of various measures, including minimal description length and $ \\Delta P$ entrenchment (Ellis, 2006). Variations on the method are described by Dunn (2018, 2019, 2022, 2023) and Dunn and Tayyar Madabushi (2021). Mart\u00ed et al. (2021) present DISCO, a methodology for discovering constructional candidates in large web-crawled corpora of Spanish texts. Similar to Dunn (2017), the semantic categories integrated into the 'lexico-syntactic' patterns are modelled through as clusters over the distributional semantic representations of lemmata. The retrieved patterns are evaluated using statistical association measures as well as through manual evaluation by expert linguists.\nThis line of work models the induction of partially abstract patterns that combine form-related and meaning-related features. While relevant from a construction grammar perspective, the resulting patterns do not correspond to constructions that actually constitute mappings between aspects of form and meaning. These models thereby do not support mapping between utterances and their meaning representations."}, {"title": "Learning a grammar from utterance-meaning pairs", "content": "The second category of models addresses the task of learning a construction grammar from utterance-meaning pairs.\nDominey (2005a,b, 2006) and Dominey and Boucher (2005) present a neural model for the acquisition of holophrase constructions, item-based constructions and abstract constructions that capture argument structure relations (e.g. transitives and ditransitives). Learners start with the capability to distinguish between closed-class and open-class words and learn to map between slots in the argument structure constructions and the semantic roles they take. Learning the meaning of open-class words is tackled as an initial cross-situational learning step. Item-based constructions are then learned from the remaining closed-class words (e.g. X was Y to Z by A) by storing the mapping of the order of thematic roles (e.g. object, action, recipient, agent) and these closed-class elements as constructions in the construction inventory.\nAlishahi and Stevenson (2008) present a computational model that mimics the acquisition of verb argument structure. The input utterance-meaning pairs are generated based on the 20 most frequent verbs of a subsection of the CHILDES corpus (MacWhinney, 2000). An example would be the utterance \u201cMom put toys in boxes\u201d paired to the meaning representation PUT[CAUSE,MOVE](MOM (AGENT), TOYS (THEME), IN[] (BOXES (DESTINATION)) (DESTINATION)). The lexicon and semantic roles are given to the model"}, {"title": "Learning a grammar from a situation model", "content": "The final category of models addresses the task of learning a construction grammar from utterances observed during communicative interactions. As such, the meaning representation of the utterances is not provided, but needs to be abductively inferred from the situation in which the interaction takes place.\nSteels (2004) presents an initial experiment in which a population of artificial agents bootstrap argument structure constructions and grammatical categories from visually grounded meaning representations. The paper presents a description game, in which two agents from the population observe a scene that needs to be successfully described by one agent to the other. The scene is transcribed in terms of first order logic predicates. During conceptualisation, the speaker agent decides on the \u2018event profile' that they want to express (see e.g. Croft, 1998), i.e. deciding which roles have to be expressed linguistically. Whether speaking or listening, an agent first makes use of its own construction grammar to process the conceptualised meaning representation or observed utterance. When an agent fails to formulate an utterance that expresses the conceptualised meaning representation or fails to comprehend an observed utterance in terms of the current scene, a game fails and a learning event takes place. When a game succeeds, the score of an agent's applied constructions is increased, while the score of competing constructions is decreased. Upon failure, the scores of the constructions that were used are decreased. Overall, the presented methodology shows how hierarchical semantic and syntactic categories can be learnt, and how the emergence of syntax aids to resolve ambiguity. van Trijp (2008, 2016) presents an extensive suite of follow-up experiments and shows how abstract semantic roles can emerge and evolve in populations of autonomous agents through multi-level selection strategies.\nArtzi and Zettlemoyer (2013) present a model where a seed lexicon, a predefined set of combinatory rules and a situation model are provided. The seed lexicon and combinatory rules are used to generate hypotheses about the meaning underlying observed utterances, which can subsequently be validated against the situation model. The goal is to extend the seed lexicon with new items in order to solve a navigation task.\nSpranger and Steels (2015) and Spranger (2015, 2017) present a model of the acquisition of spatial language in embodied artificial agents. In a shared environment and through a tutor-learner language game, two agents interact with 15 different objects in over 1000 different spatial scenes. The goal is to simultaneously acquire the semantic and syntactic aspects of spatial language. Each communicative interaction proceeds as follows. The tutor agent selects formulates an utterance that uniquely refers to an object in the situational context. The learner agent comprehends and interprets the utterance with respect to the scene and points to the object that results from the interpretation process. Then, feedback is provided by the tutor in the form of pointing. If the learner agent misinterpreted the observed utterance, it needs to make a hypothesis about the intended meaning of the utterance. The agent does this by composing a procedural semantic network based on a set of cognitive operations that it can per-"}, {"title": "Discussion", "content": "The low-level review of the prior literature in the previous section reveals perhaps most clearly that existing models for computationally learning construction grammars are highly diverse in nature and therefore challenging to compare. Their diversity is situated on almost any level, from the task that is tackled, to the goals that are envisioned, the datasets that are used, the approaches that are taken and the methodologies that are applied. So, where are we at now and where should we be heading?\nAnswering these questions presupposes that a particular perspective is taken. After all, researchers in natural language processing, for example, have different immediate goals and concerns than researchers in cognitive science or language pedagogy. We will be addressing these questions from a constructionist perspective, with the general goal in mind of operationalising large-scale, usage-based construction grammar. So, what does it mean to operationalise usage-based construction grammar on a large scale? We define the term operational as having a computational implementation that supports the processes of language comprehension (i.e. mapping from an utterance to a representation of its meaning) and production (i.e. mapping from a meaning representation to an utterance that expresses it). The term large-scale can be interpreted as having a broad and domain-general coverage of the language. For the term usage-based, we adhere to Bybee (2006)'s view that an individual's grammar is rooted in and shaped by the individual's history of communicative interactions. Finally, we interpret the term construction grammar as adhering to the basic principles underlying constructionist approaches to language (Goldberg, 2003). These principles are summarised by van Trijp et al. (2022) as follows: (i) all linguistic knowledge is captured in the form of constructions, i.e. form-meaning pairings, (ii) there is no strict distinction"}, {"title": "Conclusion", "content": "The aim of this paper was to provide an overview of prior work concerning computational models of construction grammar learning, to identify gaps in the state of the art and to propose a perspective on the future of the field. We have first described and compared a wide variety of existing models, and have then synthesised the state of the art with a special focus on the aim of operationalising usage-based construction on a large scale. Finally, we have formulated a number of milestones that can serve as a roadmap towards the development of scalable, language-independent and adaptive techniques for learning construction grammars in a usage-based fashion.\nWe have argued that a comprehensive model of construction grammar learning should learn from meaningful, intentional and situationally grounded communicative interactions. As such, meaning representations need to be actively constructed based on the situational context. Form representations should be free from preprocessing artefacts and should not neglect multi-modal information. Bidirectional construction grammars should emerge as a result of applying general learning strategies and they should cover the full range of linguistic phenomena that occur in the world's languages. We sincerely hope that our synthesis of prior literature and prospective roadmap can help to boost progress in this area of research, streamline efforts undertaken in different research traditions, and bring us closer to language technologies that can learn to use language in a truly natural, human-like manner."}]}