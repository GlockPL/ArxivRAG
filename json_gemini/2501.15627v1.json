{"title": "HARDML: A BENCHMARK FOR EVALUATING DATA SCIENCE AND MACHINE LEARNING KNOWLEDGE AND REASONING IN \u0391\u0399", "authors": ["TIDOR-VLAD PRICOPE"], "abstract": "We present HardML, a benchmark designed to evaluate the knowledge and reasoning abilities in the fields of data science and machine learning. HardML comprises a diverse set of 100 challenging multiple-choice questions, handcrafted over a period of 6 months, cover-ing the most popular and modern branches of data science and machine learning. These questions are challenging even for a typical Senior Ma-chine Learning Engineer to answer correctly. To minimize the risk of data contamination, HardML uses mostly original content devised by the author. Current state-of-the-art AI models achieve a 30% error rate on this benchmark, which is about 3 times larger than the one achieved on the equivalent, well-known MMLU-ML. While HardML is limited in scope and not aiming to push the frontier-primarily due to its multiple-choice nature it serves as a rigorous and modern testbed to quantify and track the progress of top AI. While plenty benchmarks and experimentation in LLM evaluation exist in other STEM fields like mathematics, physics and chemistry, the sub-fields of data science and machine learning remain fairly underexplored.", "sections": [{"title": "1. INTRODUCTION", "content": "Recent advancements in large language models (LLMs) have led to sig-nificant progress in natural language processing tasks such as translation, summarization, question answering, and code generation [1, 2]. These mod-els have been extensively evaluated using benchmarks covering a wide range of subjects, providing valuable insights into their capabilities [3, 4]. For instance, the Massive Multitask Language Understanding (MMLU) bench-mark [5] assesses LLMs across diverse disciplines, including STEM fields like mathematics, physics, and chemistry [6, 7, 8]. However, data science (DS) and machine learning (ML) have received relatively little attention in bench-marking efforts. The MMLU test set contains only 112 machine learning questions. Moreover, in the few instances where these domains have been explored, state-of-the-art AI models achieve near-saturation performance,"}, {"title": "2. DATA COLLECTION", "content": "The data collection involved a multi-step process spanned over 6 months. As mentioned in the last paragraph of the introduction, the initial purpose of this project was to form a set of question-answer for ML interview as-sessment for entrance of the top tech companies. These are to be used on the platform getaiquestions.com, which is a website similar to leetcode.com for interview preparation. Therefore, the dataset construction wasn't biased towards building problems that the LLMs wouldn't be able to solve, they were fully intended for human use."}, {"title": "2.1. The collection pipeline.", "content": "The collection pipeline for the development of HardML and EasyML involved a meticulous 4-step process:\n(1) Raw data collection and scraping. We have sourced approxi-mately 400 questions from public platforms such as Glassdoor, Blind, Quora, Stack Exchanges, YouTube, as well as from papers and books \u2014including those by Bishop [25]\u2014and from our own writings and public blogs, among many other reputable sources. Note that, we specifically dedicated time in collected ideas from novel and modern sources very recent interviews on the topic of the latest develop-ment in Natural Language Understanding (NLU) or Computer Vi-sion (CV) and collecting ideas from recently published papers (from 2024 and 2023).\nImportantly, this sourcing was not limited to simply gathering existing interview questions. Many questions were thoughtfully de-vised by us, inspired by theoretical concepts presented in books, papers, and online resources. This approach ensured that we had a reasonable amount of questions that were both original and rooted in fundamental principles of data science and machine learning.\n(2) Devising golden solutions and refinement. In this phase, we crafted definitive \"golden\" answers for each question, providing clear and accurate solutions. Given that many sourced questions lacked reliable and complete answers, this was a demanding and iterative process that occupied the majority of the six-month development period.\nEach question was paired with a golden answer and a list of core ideas the essential elements required for a respondent to achieve a perfect score. During this stage, we also engaged in refining the questions, which included paraphrasing and restructuring to enhance clarity and coherence. However, to preserve the authenticity of real-world interview scenarios (recall that this was the purpose of this project at that time), not all questions were extensively modified; in some cases, we made only slight adjustments while maintaining the original intent. Upon completion, this raw dataset amounted to an"}, {"title": "2.2. Question difficulty.", "content": "The difficulty assignment to each question (be-tween Easy, Medium and Hard) was done by us, as a measure of how difficult a question would appear in our eyes. The author of this paper is a former Lead Machine Learning Engineer with an MSc in AI from The University of Edinburgh with about 5 years of industry experience in machine learn-ing. His research contributions have gathered over 80 citations and his skill set encompasses a broad range of technologies and methodologies, including"}, {"title": "3. DATASET COMPOSITION", "content": "The HardML benchmark covers a broad spectrum of contemporary Data Science and Machine Learning spanning from basic data handling methods and classical machine learning to the frontier of Deep Learning and Natural Language Understanding with state-of-the-art language models and modern training pipelines utilizing tens of thousand of devices."}, {"title": "3.1. Dataset Statistics.", "content": "The distribution over categories is shows in [Ta-ble 1]. A comprehensive coverage of topics is essential for effectively evalu-ating AI systems. Accordingly, the majority of the questions in our bench-mark focus on Deep Learning, Natural Language Understanding (NLU), and Computer Vision (CV). This emphasis is intentional and natural, as these fields encompass the most novel approaches and present some of the most challenging questions in contemporary AI research. This distribution is in line with other prominent benchmarks' distributions like FrontierMath [15]."}, {"title": "3.2. Comparison to related benchmarks.", "content": "HardML differs from the base-line MMLU benchmark in both size being slightly smaller and format: each question in HardML may have more than one correct answer. This multi-answer format also sets it apart from MLE-bench, which focuses on"}, {"title": "3.3. Sample questions from HardML.", "content": "In order to accurately provide an intuition of the level of difficulty and form of the questions from HardML, we display below a few examples."}, {"title": "Sample problem 1", "content": "Question: You want to train a LLM that can solve challenging math problems properly. To do that, you employ a team of mathematicians to devise problems and solutions for training data. Unfortunately, you require a lot of training data, naturally, and hence you have to employ thousands of people to generate problems and solutions for your LLM. You need some form of quality control to understand if the mathematicians keep an overall good quality and that your LLM won't be trained on corrupted data. You can assume you have 1000 people devising (problem, solution) tasks, one person submits one task. Each task is rated from 5 choices, from 1/5 (lowest) to 5/5 (highest): 1/5,2/5,3/5,4/5,5/5. You want these people to produce, on average, a quality of work of at least 4/5=0.8 and to be 95% sure that is the case. You cannot check all 1000 and compute the average yourself because that would defeat the purpose of employing these people in the first place, so then what's the minimum number N of random tasks you would need to check? For this exercise, you can assume that the task grades follow a normal distribution and the variance of the overall quality is known and it's the maximum it can be, given the range 1/5-5/5. Make sure to normalize the grades in [0.2,1]\nA) 4\nB) 6\nC) 7\nD) 8\nAnswer: B"}, {"title": "Sample problem 2", "content": "Question: You measure Model FLOPs Utilisation (MFU) by counting all floating point operations in the entire training step-including over-head and dividing by (time elapsed) \u00d7 (theoretical hardware FLOPS). You now enable activation (gradient) checkpointing, which re-runs parts of the forward pass to save memory. Assuming you still count all FLOPs and include the extra recomputations in your total, what will happen to your measured MFU?\nA) MFU will strictly increase, because you are performing additional FLOPs without proportionally more time.\nB) MFU will strictly decrease, because the added time from redoing the forward pass dominates.\nC) MFU will remain exactly the same, because both FLOPs and time scale in a fixed ratio.\nD) The effect on MFU is ambiguous; you are doing more FLOPs but also increasing the total step time, so the ratio could go up or down.\nAnswer: D"}, {"title": "Sample problem 3", "content": "Question: A T5 or FlanT5 model is considered one of the best encoder-decoder models out there (as of 2024). Why aren't these commonly used at scale to train large language models (LLMs) that compete with GPT-4? Select all that apply.\nA) The architecture of FlanT5 makes it harder to scale.\nB) Decoder-only models allow for simpler partitioning strategies, such as splitting along head dimensions, resulting in more balanced com-pute, memory, and network costs.\nC) T5 is like a sequence of blocks but with more edges representing more complicated data dependencies during compute.\nD) The communication between encoder and decoder in encoder-decoder models complicates network architecture and scaling strategies.\nAnswer: A, B, C, D"}, {"title": "Sample problem 4", "content": "Question: What is the difference between L2 regularization and weight decay in the context of neural networks, and under which conditions can they be considered equivalent?\nA) L2 regularization adds a penalty to the loss function proportional to the sum of squared weights, while weight decay multiplies the weights by a factor slightly less than 1 after each update.\nB) L2 regularization and weight decay are always equivalent, regard-less of the optimizer used.\nC) L2 regularization and weight decay are equivalent only when using stochastic gradient descent (SGD) as the optimizer.\nD) Using optimizers like Adam or RMSprop breaks the equivalence between L2 regularization and weight decay.\nAnswer: A, C, D"}, {"title": "Sample problem 5", "content": "Question: The backpropagated gradient through a tanh non-linearity is always smaller or equal in magnitude than the upstream gradient.\nA) True.\nB) Depends on the sign of the inputs.\nC) False\nD) True only if all the input units are in (-1,1)\nAnswer: A"}, {"title": "Sample problem 6", "content": "Question: Where is the temperature applied in the model architecture of Chat GPT-3 or 4?\nA) At the input level.\nB) After the softmax layer.\nC) Right before the softmax layer.\nD) At beam-search level when we select the output token based on probability.\nAnswer: C"}, {"title": "", "content": "We intentionally designed the benchmark to assess fairly up-to-date ad-vancements in AI, as exemplified by questions 2, 3, and 6. Additionally, we included both reasoning-intensive questions such as question 1, which requires code implementation or meticulous hand calculations, and question 5, which tests comprehension through a comparison between the hyperbolic tangent function (tanh) and its derivative\u2014as well as knowledge-intensive"}, {"title": "4. RESULTS", "content": "We evaluated 5 leading language models and 1 leading smaller language model (gpt-40-mini) on our HardML dataset. Due to limited resources and ease of use, we decided to stick only with models from OpenAI and Anthropic, we believe these are enough to convey a good assessment. For instance, ol is in top 5 in Chatbot Arena LLM Leaderboard from Imarena.ai. The results are present in [Figure 2]. We used the same prompt and batch size for these experiments."}, {"title": "4.2. Human performance on HardML.", "content": "Examining human performance is particularly insightful, given that the initial intention of this project was to assess candidates during interviews or to filter applicants competing for positions in major technology companies. The results displayed in Figure 2 include human scores for reference. Below, we explain how these human scores were calculated:\nA) The first metric (Senior Machine Learning Engineer) was obtained during the beta testing phase of data collection (the final step). We invited actual senior machine learning engineers\u2014friends of the au-thors to participate in several quizzes consisting of 7 or 8 questions each, sampled from HardML. After aggregating the results, we found that an individual scored, on average, approximately 5.5 correct an-swers out of 8 (which translates into 68.75%). This performance is reflected in Figure 2. The participants expressed admiration for the benchmark, noting that the questions required significant thought and were highly challenging.\nB) The second number (Top ML Researcher) is purely the author's opin-ion. Even though we did not have access to a globally recognized"}, {"title": "4.3. Accuracy on MMLU and EasyML.", "content": "Below, we have the equivalent diagrams for the 112 questions present in the testset of MMLU (ML subset) and our proposed EasyML. Observe how ol is still the top performer, but the scores are significantly higher compared to HardML. Note that, we have not displayed human assessment figures on the MMLU benchmark as this experiment wasn't conducted."}, {"title": "5. RELATED WORK", "content": "The evaluation of large language models (LLMs) has been extensively explored across various domains, leading to the development of numerous benchmarks that assess different aspects of AI capabilities. In this section, we review the most relevant benchmarks and studies related to our work, focusing on those that evaluate LLMs in the context of machine learning and data science and briefly mentioning a few impressive pieces of work on other fields from STEM."}, {"title": "5.1. Multitask Language Understanding Benchmarks.", "content": "The Massive Multitask Language Understanding (MMLU) benchmark introduced by Hendrycks et al. [5] has been a significant step toward assessing the broad academic and professional knowledge of LLMs. MMLU covers 57 subjects across STEM,"}, {"title": "5.2. Benchmarks for Advanced Reasoning.", "content": "To address the limitations of existing benchmarks in measuring advanced reasoning, FrontierMath [15] was introduced as a benchmark comprising exceptionally challenging and original mathematical problems. These problems span major branches of modern mathematics and are designed to require significant effort from ex-pert mathematicians\u2014often multiple hours or days to solve. FrontierMath effectively minimizes data contamination by using unpublished problems and employs automated verification for reliable evaluation. Remarkably, current AI models solve under 2% of the problems, highlighting a substantial gap be-tween AI capabilities and human expertise in advanced mathematics. This benchmark underscores the importance of creating future-proof evaluations that remain challenging despite rapid advancements in AI. This paper is the inspiration for HardML, we were impressed by it and wanted to replicate some of the work."}, {"title": "5.3. Practical Machine Learning Engineering Benchmarks.", "content": "In par-allel, MLE-bench was proposed by Chan et al [16] as a benchmark to evaluate AI agents' performance in machine learning engineering tasks. MLE-bench curates 75 ML engineering-related competitions from Kaggle, encompass-ing tasks that require practical skills such as data preprocessing, model training, and experimental analysis. By establishing human baselines based on Kaggle's publicly available leaderboards, MLE-bench provides a real-world context for assessing AI agents in practical engineering scenarios. The benchmark evaluates AI setups like OpenAI's ol-preview with AIDE scaf-folding, noting that the best-performing agent achieves a bronze medal level in approximately 17% of competitions."}, {"title": "5.4. Automated Answering and Generation of ML Exams.", "content": "In the realm of educational assessments, other researchers explored the automatic answering and generation of machine learning final exam questions in their work titled \"From Human Days to Machine Seconds: Automatically An-swering and Generating Machine Learning Final Exams.\" [25] They demon-strated that large language models could pass ML final exams at a human level and generate new exam questions rapidly. Their study focused on the differences between final exams and problem sets, noting that final exams typically have longer, multi-part questions that span a broader set of topics"}, {"title": "5.5. Comparison to Our Work.", "content": "Our proposed HardML benchmark fills an important gap in existing evaluations by providing a rigorous, modern, and challenging testbed specifically tailored to data science and machine learning. Unlike MMLU's ML subset, HardML offers a more difficult, more diverse and more up-to-date set of questions that delve deeper into advanced topics. In contrast to MLE-bench, which assesses practical engineering skills through coding tasks, HardML focuses on theoretical understanding and the ability to reason about complex concepts.\nBy emphasizing originality and minimizing data contamination, similar to FrontierMath, we ensure that HardML remains a relevant and challeng-ing benchmark for current and future AI models. Additionally, by including EasyML as a complementary benchmark for evaluating smaller language models, we address the need for scalable evaluations across different model sizes and capabilities. It is challenging to ascertain the long-term applica-bility of HardML; however, we anticipate that it will remain relevant at the cutting edge of model evaluation for at least one year."}, {"title": "6. LIMITATIONS", "content": "Even though HardML currently demonstrates reasonable resistance to saturation, we do not believe this resilience will persist for much longer. Models like 03 [26] have already shown improvements over previous frontier models such as o1, and the pace of advancement in AI systems is exceedingly rapid. One of the significant limitations of HardML is its multiple-choice for-mat, which allows for \"guesses\" or \"educated guesses\" that can artificially inflate scores a limitation that has been critically examined in Frontier-Math. In benchmarks like MMLU [ML], where only one answer is correct per question, a random guess has a chance of being correct. In compari-son, in a multiple-choice format where more than one answer can be correct, a random guess has a probability as high as 15. These probabilities are still substantial, potentially diminishing the benchmark's ability to effectively discriminate between true understanding and chance performance.\nTherefore, it is essential to develop benchmarks with automatic evalua-tions that require machine-verifiable outputs, such as numerical or boolean answers. This approach reduces the likelihood of inflated scores due to guessing. Benchmarks like MLE-bench, which necessitate code implemen-tation or involve advanced mathematical reasoning to arrive at the correct solution-while still being related to data science and machine learning-are exemplary in this regard.\nConstructing challenging multiple-choice questions is particularly diffi-cult because adept humans or advanced AI models can employ elimination"}, {"title": "7. ACKNOWLEDGEMENTS", "content": "Special thanks to: Robin Kahlow (Senior ML Engineer at RunwayML), Geo Badita (Senior Software Engineer at Meta), and Chady Dimachkie (Head of ML at Abwab.ai and former Deep Learning Engineer at Nvidia) for taking the challenge and attempting HardML very thoroughly, as well as providing invaluable feedback. Their expertise and rigorous assessments have been instrumental in refining the dataset and validating its efficacy. Special thanks to Paul Chelarescu for his invaluable assistance in curating and organising the raw database in the first step of data collection, which served as the foundation for this work."}, {"title": "8. CONCLUSION", "content": "With this paper, we instigate to further research in the area of LLM benchmarking for cutting edge Data Science and Machine Learning. The dataset of HardML is present in an interactive environment on getaiques-tions.com and can also be obtained in clean json format for experiment replication or further research here. Our work contributes to the ongoing efforts to develop benchmarks that can effectively measure and distinguish the advanced capabilities of AI models in rapidly evolving fields."}]}