{"title": "Improving Statistical Significance in Human Evaluation of Automatic Metrics via Soft Pairwise Accuracy", "authors": ["Brian Thompson", "Nitika Mathur", "Daniel Deutsch", "Huda Khayrallah"], "abstract": "Selecting an automatic metric that best emulates human judgments is often non-trivial, because there is no clear definition of \"best emulates.\" A meta-metric is required to compare the human judgments to the automatic metric judgments, and metric rankings depend on the choice of meta-metric. We propose Soft Pairwise Accuracy (SPA), a new meta-metric that builds on Pairwise Accuracy (PA) but incorporates the statistical significance of both the human judgments and the metric judgments. SPA allows for more fine-grained comparisons between systems than a simplistic binary win/loss, and addresses a number of shortcomings with PA: it is more stable with respect to both the number of systems and segments used for evaluation, it mitigates the issue of metric ties due to quantization, and it produces more statistically significant results. SPA was selected as the official system-level metric for the 2024 WMT metric shared task.", "sections": [{"title": "1 Introduction", "content": "Automatic metrics are crucial because researchers and practitioners in NLP typically can't always afford the high cost and latency of high-quality human evaluations. Despite their shortcomings, metrics like Word Error Rate (WER) and BLEU (Papineni et al., 2002)\u2014in conjunction with carefully curated test sets have been crucial for the field of NLP, as they have provided a yardstick to make continual progress over many decades in Automatic Speech Recognition (ASR) and Machine Translation (MT), respectively.\nReliance on automatic metrics makes selecting a good automatic metric of paramount importance. Conceptually, an automatic metric should emulate human judgments. Selecting an automatic metric"}, {"title": "2 Method", "content": "We propose a simple meta-metric for evaluating automatic metrics given human judgments, which we denote Soft Pairwise Accuracy:\n$SPA = \\frac{1}{\\binom{N}{2}} \\sum_{i=0}^{N-1} \\sum_{j=i+1}^{N-1} 1 - |p_{ij}^h - p_{ij}^m|$ (1)\nwhere N is the number of systems, $p_{ij}^h$ is the p-value for hypothesis that system i is better than system j given the human judgments, and $p_{ij}^m$ is the p-value for hypothesis that MT system i is better than system j given the metric scores, and the first term is normalizing by the total number of pairs: $\\binom{N}{2}^{-1} = \\frac{2}{N(N-1)}$. The p-value formulation will be defined in \u00a7 3.2, but for now, intuitively it can be thought of as an approximation to the confidence of the human raters and automatic metric. In particular, $p_{ij} \\approx 0$ means the raters or metric are extremely confident that system i is better than system j, $p_{ij} \\approx 1$ means that the raters or metric are extremely confident that system j is better than system i, and $p_{ij} \\approx 0.5$ means that the raters or metric believe the two systems to be indistinguishable in terms of performance."}, {"title": "2.1 Relationship to Pairwise Accuracy", "content": "Pairwise accuracy is defined as\n$PA = \\frac{1}{\\binom{N}{2}} \\sum_{i=0}^{N-1} \\sum_{j=i+1}^{N-1} a_{ij}^{m} (2)$\nwhere $a_{ij}^{m}$ is 1 when the metric and human judgments agree and 0 otherwise.\nA p-value $p_{ij}$ will be less than 0.5 when the human raters (or automatic metric) prefer system i over system j, and greater than 0.5 when the human raters (or automatic metric) prefer system j over system i.\nThis allows us to define PA in terms of quantized p-values:\n$PA = \\frac{1}{\\binom{N}{2}} \\sum_{i=0}^{N-1} \\sum_{j=i+1}^{N-1} 1 - |\\lfloor p_{ij}^h \\rfloor - \\lfloor p_{ij}^m \\rfloor | (3)$\nWhere rounding is denoted as:\n$[x] =\\begin{cases}1 & \\text{if } x > 0.5\\\\0 & \\text{if } x < 0.5\\end{cases}$\nComparing Equation 1 and Equation 3, highlights the fact that SPA can be viewed as a 'soft' extension to pairwise accuracy that incorporates uncertainty in both the human and metric scores. A visualization of this is provided in Figure 1. We hypothesize that this rounding/quantization is effectively acting as additive random noise on top of the underlying SPA meta-metric.\nIn cases where both the MT metric and the human evaluation both have high statistical significance (regardless of whether the metric agrees with the human judgments or not, i.e. p\u2248 0 or p\u2248 1), the contribution of that system pair to SPA and PA is approximately identical. However, there are two important cases where our meta-metric differs from pairwise accuracy:\n1. The human evaluation has high significance (e.g. $p_{ij}^h \\approx 0$ or $p_{ij}^h \\approx 1$), but the metric has low statistical significance (e.g. $p_{ij}^m \\approx 0.5$): Even if the metric happens to choose the correct winner, we partially penalize the metric for not having high statistical significance.\n2. The human evaluation finds the systems are approximately tied (e.g. $p_{ij}^h \\approx 0.5$): In this case, we partially penalize the metric if has high statistical significance (e.g. $p_{ij}^m \\approx 0$ or $p_{ij}^m \\approx 1$) even if it happens to pick the same winner as the human evaluation, and to get full credit the metric must match the human evaluation statistical significance (e.g. $p_{ij}^h \\approx p_{ij}^m \\approx 0.5$)"}, {"title": "2.2 Addressing Metric Ties in PA", "content": "The fact that PA considers only binary wins/losses (e.g. the rounding in Equation 3) results in an interesting shortcoming in PA. There are $\\binom{N}{2}$ pairs of N systems, and thus $\\binom{N}{2} + 1$ distinct values that PA can take on (0/$\\binom{N}{2}$, 1/$\\binom{N}{2}$, ..., $\\binom{N}{2}$/$\\binom{N}{2}$). By way of example, WMT-22 en-de, with N = 14 systems, $\\binom{N}{2} + 1 = 92$.\nHowever, metrics tend to perform better than a random baseline, so only the upper half of the range is actually useful (e.g. this leaves 46 distinct values for N = 14 systems). In practice, most metrics perform substantially better than random, and we tend to compare many (e.g. >10) metrics. In practice, we find that this results in PA reporting the same score for several sets of metrics (see \u00a7 4.3). By removing this rounding/quantization, SPA has no such issues."}, {"title": "3 Experimental Setup", "content": "We conduct experiments on the data from the WMT Metrics shared tasks, years 2022 and 2023 (Freitag et al., 2022, 2023). In particular, we use the primary language pairs where MQM judgments were collected. We use the MT Metrics Eval V2 toolkit\u00b9 to retrieve official shared task scores.\nWe make the fairly arbitrary decision to compare all metrics, including non-primary metrics but excluding QE metrics (i.e. reference-free metrics) which provide segment-level scores.\nIn order to compute the statistical significance of comparisons between metrics metrics, we make the simplifying assumption that all system-level metrics are the average of their segment-level metric. This is not true for some metrics, including BLEU (Papineni et al., 2002) and chrF (Popovi\u0107, 2015). While it would be possible to re-compute BLEU and chrF for each subset, we average for simplicity. Our results are self consistent (i.e. we"}, {"title": "3.2 p-value Computation", "content": "Given a pair of translations of the same test set, by two different MT systems, each scored at the segment level by the same metric, we would like to know the statistical significance of the difference in the means of the segment-level scores of each MT system.\nWe use a permutation test (Fisher, 1935) to estimate the probability that our observed difference occurred randomly under the null hypothesis that the two translations have the same quality. A permutation test works by generating random splits of the data (ignoring the labels, i.e. which MT system that produced each translation) and computing the difference in metric score mean for these two sets of data. The p-value is computed by calculating the fraction of the time that this difference is greater than or equal to the mean difference in scores for"}, {"title": "3.3 p-value Speed Optimization", "content": "A naive implementation of the paired permutation test is not computationally prohibitive when computing p-values for all systems/metrics a single time, but it becomes a problem when we want to compute these values many times in order to estimate statistical significance of metric comparisons."}, {"title": "4 Analysis", "content": "Meta-metric evaluation is challenging because there is no ground truth (i.e., we don't know the true ranking of the metrics). Instead, we must make some assumptions about how an ideal metric will act. First, we study how sensitive the meta-metric results are when varying the number of MT systems and number of segments per MT system, with the assumption that lower sensitivity to the exact systems / segments used indicates a better meta-metric. Second, we examine whether PA indeed has the problem of ties that we hypothesized, and whether SPA fixes this issue. Finally, we test our hypothesis that the quantization/rounding in PA is effectively acting as additive random noise, and that SPA is effectively the same underlying meta-metric with the noise term removed. If this is true (and the magnitude of the noise does not dominate the underlying signal), we would expect SPA to produce a similar metric ranking to PA, but with increased statistical significance."}, {"title": "4.1 Ablation: Number of Systems", "content": "Each year, WMT and the associated metrics task collect and score many online and submitted MT systems. For an ideal meta-metric, the exact choice of MT systems would have little to no impact on the metric rankings. We perform an ablation on the number of MT systems being scored, keeping the number of annotations per system fixed. We then compute the correlation (as measured by Pearson's r) between the meta-metric's ranking of the ablations compared to that same meta-metric's full ranking. This allows us to evaluate how sensitive the metric is to the exact selection of MT systems.\nWhen ablating the number of MT systems (and keeping the number of annotations per system fixed), we find (see Figure 2) that SPA is more stable than PA across all MQM language pairs in the last two years of WMT metrics shared tasks."}, {"title": "4.2 Ablation: Sample Size", "content": "Since SPA relies on the pairwise p-values between MT systems, it is also natural to ask how SPA behaves when the number of available segments used for evaluating systems is small since it is harder to find statistical differences between systems with a smaller sample size. To answer this question, we calculated 95% confidence intervals for both PA and SPA values of two highly performant metrics\u2014in particular, we considered xCOMET (Guerreiro et al., 2023) and MetricX-23 (Juraska et al., 2023)\u2014on WMT23 using bootstrapping for various numbers of segments, thereby simulating scenarios with less human annotations but a fixed number of MT systems. The results are presented in \u00a7 4.2.\nWhen ablating the number of segments per MT system (and keeping the number of MT systems fixed), we find (see Figure 3) that SPA has tighter 95% confidence intervals than PA (shown on Metric-X and xCOMET), and that the confidence interval converges to its final value with smaller sample sizes than PA."}, {"title": "4.3 Ties", "content": "As discussed in \u00a7 2.2, the quantization noise in PA limits the number of distinct values it can assign to metrics. PA can only take on $\\binom{N}{2} + 1$ distinct values. In practice, we find it tends to take on far fewer values. For example, in WMT 22 en-de PA could theoretically take on 92 distinct values, but because the metrics fall in a fairly narrow range of performance (between 62.6% for the worst metric and 81.3% for the best), the 21 metrics have only 11 distinct PA scores, with one 5-way PA tie and several 2- and 3-way PA ties (see Figure 4). Since SPA does not quantize each system comparison, it is able to assign any value to each metric, and is therefore potentially better able to distinguish between metrics.\nResults for all language pairs are in Table 1. We find that on average, PA produces about half as many distinct values as there are metrics while SPA produces one unique value per unique metric."}, {"title": "4.4 Statistical Significance of Metric Comparisons", "content": "Just because a meta-metric produces two distinct scores for two different metrics does not mean that those metrics are statistically significantly different from each other. Ideally, a meta-metric would be able to produce statistically significant comparisons between any two distinct metrics. To explore the ability of PA and SPA to distinguish between metrics in a statistically significant way, we follow PERM-INPUTS (Deutsch et al., 2021) to measure the statistical significance of the comparison between each metric. We follow recent shared tasks in greedily computing significance clusters, by starting with the highest scoring metric and assigning"}, {"title": "5 Historical Context and Related Work", "content": "WMT has run a machine translation evaluation since 2006 (Koehn and Monz, 2006). Since 2007 (Callison-Burch et al., 2007), there has also been meta-evaluation of automatic metrics on the submitted translation systems. Here we summarize the rich 17 year history of system-level meta-evaluation at the WMT metrics shared tasks and work related to and directly impacting the shared tasks, in order to demonstrate how our work fits into the historical context."}, {"title": "5.1 Additional Connections to Prior Work", "content": "Graham and Liu (2016) proposed a method of sampling translations from every pair of competing MT systems, creating synthetic systems for scoring. Our work has clear similarities in that we create and score synthetic permutations, but differs in how those synthetic systems are used in the meta-metric formulation.\nMathur et al. (2020a) showed that MT system outliers had an outsized impact on Pearson correlation. In SPA, outliers impact is limited because p-values saturate at 0 or 1.\nKnowles (2021) highlights that as WMT annotation protocols have shifted the original statistical assumptions, and questions the validity of the resulting protocols. Similarly, we show that shifts over the years have caused problems in meta-evaluation.\nLo et al. (2023a) investigated what magnitude of metric changes tend to be statistically significant. SPA uses statistical significance measures (p-values) directly, as opposed to the magnitude of metric differences (e.g. as in Pearson correlation).\nDeutsch et al. (2023) demonstrated that principled tie handling is crucial when comparing MT metrics at the segment (e.g. sentence) level, because some metrics produced quantized scores that commonly create ties. SPA is system level (e.g. sentence level scores averaged over the entire test set), so exact ties are very unlikely. However, SPA can be seen as giving full credit for (statistical) ties, which is similar in spirit.\nWe show that Quantization is problematic in PA. Quantization in evaluation has proved problematic in other spaces as well-for example, Schaeffer et al. (2024) attributes the widely repeated claim that LLMs have emergent properties to quantization in evaluation."}, {"title": "6 Conclusions", "content": "We introduce a new meta-metric which we denote Soft Pairwise Accuracy, and show that it improves on Pairwise Accuracy in a number of ways, most notably that it is more stable when ablating the number of systems and annotations per system, and that it produces more statistically significant comparisons between metrics. We also discuss how SPA fits into and builds upon the nearly two decade history of meta-evaluation at WMT metric shared tasks."}, {"title": "Limitations", "content": "When computing p-values, we assume that system-level metric scores are the average of segment-level metrics scores. There is a line of recent work that seeks to incorporate contextual information into automatic metrics. Many such works still produce scores at the segment level (e.g. Vernikos et al., 2022; Hu et al., 2023; Agrawal et al., 2024) but others produce one score per window of a few sentences (Raunak et al., 2024) or one score per paragraph (Hu et al., 2023). Our method should still be applicable in such cases, but would require permuting windows or paragraphs instead of segments. Additionally, as previously noted, some metrics\u2014notably BLEU (Papineni et al., 2002) and chrF (Popovi\u0107, 2015)\u2014compute statistics at the segment level and combine them to create document-level scores. Again, permutations would still work but would require some modification. To the best of our knowledge, this issue is not limited to our work - it the same assumption is made in prior work computing statistical significance of metrics, including the WMT shared tasks (Freitag et al., 2021, 2022, 2023) and Deutsch et al. (2021).\nIt is worth noting that the permutations in this work (as in prior works) are done on a single test set, and do not necessarily reflect variations in performance that could result from using the metrics in another domain. Prior work has shown that trained metrics are sensitive to a shift in domain relative to the data domain they were trained on (Zouhar et al., 2024)."}]}