{"title": "Get Rid of Task Isolation: A Continuous Multi-task Spatio-Temporal Learning Framework", "authors": ["Zhongchao Yi", "Zhengyang Zhou", "Qihe Huang", "Yanjiang Chen", "Liheng Yu", "Xu Wang", "Yang Wang"], "abstract": "Spatiotemporal learning has become a pivotal technique to enable urban intelligence. Traditional spatiotemporal models mostly focus on a specific task by assuming a same distribution between training and testing sets. However, given that urban systems are usually dynamic, multi-sourced with imbalanced data distributions, current specific task-specific models fail to generalize to new urban conditions and adapt to new domains without explicitly modeling interdependencies across various dimensions and types of urban data. To this end, we argue that there is an essential to propose a Continuous Multi-task Spatio-Temporal learning framework (CMuST) to empower collective urban intelligence, which reforms the urban spatiotemporal learning from single-domain to cooperatively multi-dimensional and multi-task learning. Specifically, CMuST proposes a new multi-dimensional spatiotemporal interaction network (MSTI) to allow cross-interactions between context and main observations as well as self-interactions within spatial and temporal aspects to be exposed, which is also the core for capturing task-level commonality and personalization. To ensure continuous task learning, a novel Rolling Adaptation training scheme (RoAda) is devised, which not only preserves task uniqueness by constructing data summarization-driven task prompts, but also harnesses correlated patterns among tasks by iterative model behavior modeling. We further establish a benchmark of three cities for multi-task spatiotemporal learning, and empirically demonstrate the superiority of CMuST via extensive evaluations on these datasets. The impressive improvements on both few-shot streaming data and new domain tasks against existing SOAT methods are achieved. Code is available at https://github.com/DILab-USTCSZ/CMuST.", "sections": [{"title": "1 Introduction", "content": "Spatiotemporal learning has become a pivotal technique to enable smart and convenient urban lives, benefiting diverse urban applications from intra-city travelling, environment controlling to location-based POI recommendation, and injecting the vitality into urban economics. Existing spatiotemporal learning solutions [19, 9, 38, 40, 51, 42, 37, 10, 8] focus on improving performances of a task-specific model independently where these methods devise various spatial learning blocks [44, 8, 43] and temporal dependency extraction modules [40, 34, 9] to model the spatiotemporal heterogeneity.\nActually, urban spatiotemporal systems are usually highly dynamic with emerging new data modality, leading to serious generalization issue on both data pattern and task adaptation. As illustrated in"}, {"title": "2 Related Work", "content": "Spatiotemporal forecasting is an emerging technique to capture the dynamic spatial and temporal evolution for diverse urban predictions, where the methods can be divided into machine learning-based and deep learning-based. Conventional solutions rely on complex mathematical tools to simulate the dynamics including ARIMA [25], SVR [3] and matrix-factorization learning [28] for capturing spatial correlations. With deep learning solutions flourishing, Convolution Neural Networks (CNNs) [40, 11, 46] are exploited to imitate the temporal dependencies and GNNs [43, 13, 36] are utilized to imitate spatial propagation. Meanwhile, by taking the advantage of the flexibility and interpretability of attention, Spatial-Temporal Attention [9], and Vision Transformer (ViT) [6] are introduced to improve spatiotemporal representation. Also there are also many methods for temporal periodicity capturing [12, 20]. More specifically, DG2RNN [49] designs a dual-graph convolutional module to capture local spatial dependencies from both road distance and adaptive correlation perspectives. PDFormer [15] designs a spatial self-attention and introduces two graph masking matrices to highlight the spatial dependencies of short- and long-range views. TESTAM [18] uses time-enhanced ST attention by mixture-of-experts and modeling both static and dynamic graphs. Even so, most solutions focus on single-task intelligence, fail to deal with complex interactions between data dimensions and never extract task-level commonality patterns, resulting in inferior performances on exploiting collective intelligence over multiple tasks. In contrast, we merge the gap by disentangling learnable interaction patterns and exploring rolling task adaptations.\nMulti-task learning. Plenty of efforts have been made on multi-task learning (MTL) and MTL can be elaborated by two-fold, i.e., feature-based and parameter-based. Feature-based MTL [2, 24] learns a common feature representation for different tasks, but it may be easily affected by outlier tasks. To this end, parameter-based MTL [30, 16] is devised to exploit model parameters to relate different tasks, which is expected to learn robust parameters. Majority of these MTL schemes either concentrate on the diversity design and regularization effects of auxiliary tasks to main task [30, 27], or construct loss functions to ensure task-wise consistency [47]. A pioneering work investigates a gradient-driven task grouping to realize multi-task learning [41] where the focuses are text and images using pre-trained CLIP model. For spatiotemporal learning, RiskOracle [52] and RiskSeq [53] are proposed to simultaneously predict multi-grained risks and auxiliary traffic elements. More recently, UniST [45] and UniTime [22] construct a unified model for spatiotemporal and time-series prediction. However, all researches on ST learning still never dissect task-wise correlations, especially capture explicit consistency and diversity among tasks and investigate how each task reinforce the core task, which is of great significance for performance and interpretability in MTL.\nContinuous learning and task continuous learning. Continuous learning (CL) usually keeps long-term and important information while updates model memories with newly arrived instances [29, 5, 35]. For spatiotemporal forecasting, Chen, et, al. [5] proposes a historical-data replay strategy, TrafficStream, to update the neural network with all nodes feeding, while PECPM [35] manages a pattern bank with conflict nodes, which reduces the memory storage burdens. Most existing CL solutions are designed for homogeneous sourced data. Then there are very few research investigating task-level continuous learning where task can be converted from one to another. A pioneering work, CLS-ER [1] realizes class-level and domain-level continuous learning with a dual memory to respectively store instances and build long-short term memories. Even so, the data input to CLS-ER is with same image-like property and scales, which is still far away from task diversity. We argue that learning continuously on task levels can increase the data tolerance over low-quality data and also generalization capacity on new tasks. In this work, by taking bonus of continuous learning, we propose to iteratively assimilate the commonality and extract personalization to reinforce each learning task and realize continuous multi-task spatiotemporal intelligence."}, {"title": "3 Preliminary", "content": "Definition 1 (Spatiotemporal features.) Spatiotemporal features refer to the data points collected by sensors deployed in urban environments, such as traffic dynamics on roads. We define a spatiotemporal"}, {"title": "4 Methodology", "content": "4.1 Framework Overview\nThe CMuST is crafted to advance urban intelligence through a synergistic integration of three components in Figure 2. We processes and standardizes diverse urban data into a harmonized format, and propose the MSTI to intricately dissect the complex interactions within spatiotemporal data, and devise a RoAda to dynamically fine-tune the model via continuous and careful updating, ensuring robust adaptability and consistent performance across fluctuating urban environments.\n4.2 Data Representation and Integration\nTo harness various data domains within urban spatiotemporal systems as well as data interactions between diverse dimensions, the first task is to appropriately process main ST observations, spatial indicator, and temporal indicator to create a comprehensive and integrated data representation tailored for multi-task ST learning, enabling further interactive modeling between them.\nTo be specific, the main observation data, namely target of interest of urban datasets, are denoted as $X_{obs} \\in \\mathbb{R}^{T \\times N \\times C}$, and then mapped into a spatiotemporal representation $E_{obs} \\in \\mathbb{R}^{T \\times N \\times d_{obs}}$ via an MLP $ObsMLP(X_{obs}; \\theta_{obs})$. Similarly, the spatial indicator $X_{s} \\in \\mathbb{R}^{T \\times N \\times 2}$, consisting of longitude and latitude coordinates, is applied with a linear layer $SpatialMLP(X_{s}; \\theta_{s})$ to produce the spatial representation $E_{s} \\in \\mathbb{R}^{T \\times N \\times d_{s}}$. Temporal indicator comprises day-of-week, time-of-day $X_{dow}, X_{tod} \\in \\mathbb{R}^{T \\times N}$ and timestamps $X_{ts} \\in \\mathbb{R}^{T \\times N \\times 6}$, which are further compressed into hidden"}, {"title": "4.3 Multi-dimensional Spatio-Temporal Interaction", "content": "Spatiotemporal observations are usually complex with multiple-level interactions where such interactions and correlations play vital roles in enhancing commonality learning through different learning domains. To this end, we devise an MSTI, to intricately dissect and disentangle interactions within spatiotemporal data from spatial-temporal indicators to main observations by inheriting nice property of attention mechanisms, which all utilize transformed slices from the integrated representation H.\nSpatial-context cross-interaction. To quantitatively investigate how spatial indicator interact with main observations, we devise a multi-head cross-attention [14] architecture (MHCA) where spatial and observational components are alternately used as queries (Q) and key-value (KV) pairs:\n$\\begin{aligned}\nMHCA^{(a,b)}(X) &= \\underset{h=1}{\\overset{head}{\\vert\\vert}} CrossAttention^{(a,b)}(h)W^{O}\\\\\nCrossAttention^{(a,b)}(h) &= Attention \\bigg(Q^{(a)}_{h}, K^{(b)}_{h}, V^{(b)}_{h} \\bigg)\\\\\nAttention \\left(Q_{h}, K_{h}, V_{h}\\right) &= softmax \\left(\\frac{Q_{h} K_{h}^{T}}{\\sqrt{D}}\\right) V_{h}\n\\end{aligned}$\nThe symbol $\\vert\\vert$ denotes the concatenation of multiple attention heads, and $W^{O}$ is the projection matrix that aligns the output dimensions with those of H. Here, we let variables a and b indicate spatial (s) indicator and main observation (o), i.e., a = s and b = o. Then the queries, keys, and values are generated through following transformations,\n$Q^{(s)}_{h} = H[..., slice^{(s)}_{h}] W^{(Q_{s})}_{h}, K^{(o)}_{h} = H[..., slice^{(o)}_{h}] W^{(K_{o})}_{h}, V^{(o)}_{h} = H[..., slice^{(o)}_{h}] W^{(V_{o})}_{h}$\nwhere W transforms input data into dimension D of attention space, $slice^{(s)}$ and $slice^{(o)}$ denote the respective slices of H for spatial and observational features. After computing attention scores, the embeddings are taken into a feed-forward network (FFN) to enhance the learning capabilities, where $FFN(X) = max(0, XW_{1} + b_{1})W_{2} + b_{2}$. The final attention outputs are then normalized by,\n$\\hat{H}^{(a,b)} [\\ldots, slice^{(b)}] = LN(FFN(LN(MHCA^{(a,b)}(H) + H[\\ldots, slice^{(b)}])) + LN(MHCA^{(a,b)}(H) + H[\\ldots, slice^{(b)}]))$\nwhere LN is layer normalization, the resulting matrices $\\hat{H}^{(s,o)}$ and $\\hat{H}^{(o,s)}$ are then concatenated back to H at their respective dimensions as $\\hat{H}^{(SCCI)} \\in \\mathbb{R}^{T \\times N \\times d_{h}}$, enriching the original representation with refined features that encapsulate intricate cross-dimensional relationships.\nTemporal-context cross-interaction. To facilitate the attention computation with respect to temporal dimension, the representation is first transposed as $\\hat{H}'^{(SCCI)} \\in \\mathbb{R}^{N \\times T \\times d_{h}}$ by denoting T as the sequence length for subsequent attention calculations. Then the step-wise positional encoding (refer to Appendix B.2) is introduced to allow our attention aware of specific temporal evolution.\nThe subsequent steps closely follow those of the spatial-context cross-attention mechanism, so the temporal-context cross-interaction (TCCI) can be performed as $\\hat{H}^{(CI)} = TCCI(\\hat{H}'^{(SCCI)})$, and final representation $\\hat{H}^{(CI)} \\in \\mathbb{R}^{N \\times T \\times d_{h}}$ becomes the outcome of cross interactions between spatial and temporal dimensions.\nSelf-interactions within Spatial and Temporal Aspects. We proceed to apply self-interaction across different dimensions of the representations using self-attentions [21]. Specifically, we begin with"}, {"title": "4.4 Rolling Adaptation over Continuous Multi-task Spatio-Temporal Learning", "content": "To ensure continuous task learning, we propose a Rolling adaptation scheme, RoAda, to model the distinction and commonality among task domains. Our RoAda is composed of two stages with a warm-up for commonality extraction and a task-specific refinement. Before the task rolling, we construct prompts to distinguish personalization of each task. Thus, the commonality and diversity can be leveraged to boost individual task adaptation.\nTask summarization as prompts. To capture the task distinction, we devise a task summarization by a Sampling-AutoEncoding scheme from each task. Consider task $T_{k}$, main observation becomes $X^{(T_k)} \\in \\mathbb{R}^{T \\times N \\times C}$. Such data is sampled by averaging observations over equivalent times of day, yielding a periodic sample representation $X_{samp} \\in \\mathbb{R}^{L(T_k) \\times N \\times C}$, where $L(T_k)$ denotes the number of time slots for task k within a day. Since neural networks tend to fit any data regularity, the sampled features are fed into an autoencoder for extracting the compressed and distinguished data patterns. Given the encoding and decoding processes, i.e., $\\phi: X_{samp} \\rightarrow S$ and $\\psi : S \\rightarrow X'_{samp}$, S encapsulates the summarized core characteristics of the task. The decoding phase maps S back to a reconstructed $X'_{samp}$, by minimizing the mean squared reconstruction error,\n$\\begin{aligned}\n\\phi: S &= sigmoid(W_{s}X_{samp} + b_{s}), \\newline\n\\phi, \\psi &= arg min||X_{samp} - (\\psi \\circ \\phi)X_{samp}||_{2}^{2}\\newline\n&\\Phi,\\Psi\n\\end{aligned}$\nwhere $W_{s}$ is the weight, and $b_{s}$ is the bias. Following the encoder, the summary features $S^{(T_k)}$ are transformed into the k-th task prompt $P^{(T_k)} = FC_{p}(S^{(T_k)}; \\theta_{p})$ with dimension alignment.\nWeight behavior modeling. The first warm-up stage is designed via weight behavior modeling, which assimilates the regularity from task to task. This process not only adapts the model to new tasks but also solidifies its ability in generalizing across scenarios by capturing task-wise common relations with modeling of model weight behaviors.\nWe begin with the task $T_{1}$ via independently training the model until its performance stabilizes. By denoting M as the model learned by MSTI, the training phase can be formally described as,\n$P^{(T_1)} \\overset{prompt, M, \\newline load}{\\longrightarrow} Train\\bigg(M\\big(D^{(T_1)}_{train}; W_{init}\\big)\\bigg) \\underset{until \\newline convergence}{\\longrightarrow} W^{(T_{1})}_{C}$\nwhere $W_{init}$ are the initialized weights, $D^{(T_1)}$ is the training dataset of task $T_{1}$, and $W^{(T_{1})}_{C}$ are the weights when model converges. After that, our model transitions learning task from $T_{1}$ to $T_{2}$ by loading the corresponding task prompt $P^{(T_2)}$ and dataset $D^{(T_2)}_{train}$. This transition involves a critical step where the evolution behavior of model weights W are carefully stored,\n$Train\\bigg(M\\big(D^{(T_2)}_{train}; W^{(T_1)}_{C}\\big)\\bigg) \\underset{for \\newline each epoch}{\\longrightarrow} W^{(T_{2})}_{0}, W^{(T_{2})}_{1},..., W^{(T_{2})}_{C}$"}, {"title": "5 Experiment", "content": "5.1 Datasets and Experiment Setup\nData description. Given the emerging multi-task ST learning, we collect and process three real-world datasets for evaluation: 1) NYC\u00b3: Includes three months of crowd flow and taxi hailing from Manhattan and its surrounding areas in New York City, encompassing four tasks: Crowd In, Crowd Out, Taxi Pick, and Taxi Drop. 2) SIP: Contains records of Traffic Flow and Traffic Speed within Suzhou Industrial Park over a period of three months. 3) Chicago\u2074: Comprises of traffic data collected in the second half of 2023 from Chicago, including three tasks: Traffic Pick, Taxi Drop, and Risk.\nBaselines and metrics. Our CMuST model is evaluated on a widely-used baselines spatiotemporal prediction, including RNN-based models (DCRNN [19], AGCRN [4]), STGNNs (GWNET [40], STGCN [44]) for single task, and MTGNN [39], STEP [33] PromptST [48] for multiple tasks,"}, {"title": "5.3 Ablation Study", "content": "To assess the effectiveness of each module in CMuST and its capability for multi-task learning, we designed a set of variants as 1) w/o context-data interaction: remove the spatial-context and temporal-context cross-interactions in the MSTI module, 2) w/o consistency maintainer: omit the separation and freezing of stable weights during the RoAda phase, instead using all weights for rolling training, 3) w/o task-specific preserver: eliminate the task-specific prompts, thus removing the task-specific diversity preservation"}, {"title": "5.4 Case Study", "content": "1) Visualizing attention across training phases. In Figure 4(a), we visualize the changes of attention weights of CMuST during the stage of RoAda. It is observed that as tasks are learning continuously, the relationships and interactions across various dimensions is becoming distinctive and going stable, demonstrating the consolidation process of dimension-level relations. By modeling weight behavior, such consolidated relations and interactions between context and observations can further enable the extraction of consistency in spatiotemporal interactions across tasks. 2) Performance variation along with task increasing. Figure 4(b) shows the performance of individual tasks on NYC as the number of tasks increases. The performance of each task is improving with the addition of more tasks, which indicates that tasks are no longer isolated but gain the collective intelligence via assimilating common representations and interactive information."}, {"title": "5.5 Parameter sensitivity analysis", "content": "We varied the dimension of the task prompt dp as {18, 36, 72, 144}, the number of attention heads in MSTI as {1, 2, 4, 8, 16}, and the threshold $\\delta$ for RoAda among {$10^{-4},10^{-5},10^{-6}, 10^{-7}$}. Results shown in Figure 6 indicate that the optimal settings were $d_p$ = 72, $head$ = 4, and $\\delta$ = $10^{-6}$"}, {"title": "6 Conclusion", "content": "In this work, we enable spatiotemporal learning to get rid of isolation with proposed CMuST, which consists of two major components. In CMuST, the MSTI is devised to dissect complex multi-dimension data correlations, to reveal disentangled patterns. To extract the task-wise consistency"}, {"title": "A Explanation of Relevant Concepts", "content": "A.1 The Concept, Definition and Scope of Multi-task Learning\nActually, in our study, various domains correspond to different urban elements collected with different manners in a given city. For instance, in an integrated urban system, it includes taxi demands, crowd flow, traffic speed and accidents. We collect and organize various domain data (urban elements) in a city into one integrated dataset. The goal of our work is to explore the integrated intelligence from various domains and enhance learning of each individual urban element. To this end, the concept of multi-task here is to forecast various elements from different domains in an integrated model. Therefore, our work does not target at unifying regression or classification problems, but proposes an integrated model to iteratively establish the common intelligence among different elements and improve generalization for each element learning in succession, thus getting rid of task isolation. Noted that our experiments are performed with regression tasks, but it can easily generalize to classification task with shared representations.\nA.2 Continuous & Continual learning\nIn this work, 'continuous' is equivalent to 'continual'. The uniqueness of our work refers to a novel continuous task learning in ST community, which collects the integrated intelligence and benefits each individual learning task."}, {"title": "B Methodology Details", "content": "B.1 Illustration of Data Representation and Integration\nB.2 Positional Encoding for Temporal-context Cross-interaction\nGiven the D dimensions in total, we define the positional encoding as,\n$\\begin{cases}\npos_{(t,2d)} = sin \\big(t/10000^{(2d/D)}\\big)\\\\\npos_{(t,2d+1)} = cos \\big(t/10000^{(2d/D)}\\big)\n\\end{cases}$for d = 0,\u2026\u2026, D/2 \u2212 1"}, {"title": "B.3 Fusion & Regression", "content": "Predictions for future steps are influenced by representations affected by multi-dimensional interactions, where the impact of each dimension may vary. Therefore, we integrate information across various aspects effectively by a parameter matrix-based fusion strategy,\n$Z = W_{o} * \\hat{H}[. . ., slice^{(o)}] + W_{s} * \\hat{H}[. . ., slice^{(s)}] + W_{t} * \\hat{H}[. . ., slice^{(t)}]$\nwhere * denotes the convolution operation, employing 1 x 1 convolution kernels. $W_{o}$, $W_{s}$, and $W_{t}$ are the parameters within these kernels, tailored to adjust the influence of observational data, spatial locations, and temporal information on the prediction targets, respectively. We further integrate the task-specific prompt to culminate in the final prediction $\\hat{Y} = FC_{\\varphi}(ZW_{2} + H[. . ., slice_{p}]W_{p};\\theta_{y})$, the Huber Loss [26] is utilized as the optimization function, which is less sensitive to outliers compared to squared error loss, defined as:\n$L_{y} =\\begin{cases}\n \\frac{1}{2} (\\hat{Y} - Y)^{2}, &\\text{if } |\\hat{Y} - Y| < \\delta\\\\\\delta ( |\\hat{Y} - Y| - \\frac{1}{2} \\delta ), &\\text{otherwise}\n \\end{cases}$\nwhere Y is the ground truth, $\\delta$ controls the sensitivity to outliers, which enhances the accuracy by integrating diverse data dimensions and improves robustness against data variability and anomalies."}, {"title": "B.4 Details of Rolling Adaptation", "content": "The detailed algorithmic procedure for the RoAda phase can be found in Algorithm 1."}, {"title": "C Additional Experiment Details", "content": "C.1 Dataset Details\nWe provide detailed information about the dataset, including the number of records in the original data, the number of regions into which the data was divided, and the time intervals. These details are presented in Table 3. The specific data preprocessing is as follows:\nNYC: We collect yellow taxi trip data from January to March 2016 from the NYC Open Data website. Each trip record includes information such as pickup and dropout times, locations, and the number of passengers. We filter out records with abnormal longitude and latitude values or missing data. Then we select data within Manhattan and surrounding areas, divided into 30x15 grids, and counted trips per grid, selecting those with total trips greater than or equal to 1000, resulting in 206 grids. Each grid's data is aggregated into 30-minute intervals, yielding taxi pickup counts, taxi dropout counts, and crowd in/out flows. We also include time of day (tod) and day of week (dow) as context, resulting in four tasks with input features [value, tod, dow].\nSIP: We collect traffic data from Suzhou Industrial Park from January to March 2017, comprising tens of thousands of records. The area is divided into nodes, and data is aggregated into 5-minute intervals. After filtering out grids with sparse data, we obtain 108 nodes, each containing traffic speed and traffic flow. We include time of day and day of week as input context, resulting in two tasks: traffic flow and traffic speed, with input [value, tod, dow].\nChicago: We collect taxi trip and accident data from the Chicago Open Data platform for June to December 2023. The taxi data includes trip start, end times and locations. We divide the area into 30x20 grids and select grids with total trips greater than 100, resulting in 220 grids. Similar to the NYC dataset, data is aggregated into 30-minute intervals, yielding taxi pickup and dropout counts, resulting in two tasks with input features [value, tod, dow]. The accident data includes incident locations, times, casualty numbers, and injury severity of each casualty. We then obtain the risk score by weighting it according to each casualty and injury, mapped it to the 220 grids, and aggregated the risk score over time intervals, resulting in a risk task with input features [risk score, tod, dow]."}, {"title": "C.2 Implementation Details and Fairness-aware Experimental Evaluation", "content": "To verify whether the multi-task learning in urban systems can compete single-task learning scheme and further show the superiority of our continuous multi-task learning, our experiments are designed"}, {"title": "C.5 Experiments for Cold-start", "content": "We have designed the experiment of cold start. Specifically, for NYC dataset, we selected three of the four tasks of Crowd In, Crowd Out, Taxi Pick and Taxi Drop in turn for training, and calculated the adaptation time and results for the remaining one task on this basis, comparing with training a single task alone. A similar design is applied for SIP and Chicago datasets"}, {"title": "C.6 Parameter Sensitivity Analysis", "content": "To study the impact of hyperparameters on model performance, we varied the dimension of the task prompt dp as {18, 36, 72, 144}, the number of attention heads in MSTI as {1, 2, 4, 8, 16}, and the threshold $\\delta$ for RoAda among {$10^{-4},10^{-5}, 10^{-6}, 10^{-7}$}."}, {"title": "D Others", "content": "D.1 Limitation\nOur current work is confined to a single urban system and our experiments are solely within the transportation sector. The exploration into other domains within the same city, such as electricity, transportation, and pollution multi-task learning is still limited. Addressing these limitations will form the basis of our future research."}]}