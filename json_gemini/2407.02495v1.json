{"title": "Minds, Brains, and AI", "authors": ["Jay Seitz"], "abstract": "In the last year or so (and going back many decades) there has been extensive claims by major computational scientists, engineers, and others that AGI (artificial general intelligence) is 5 or 10 years away, but without a scintilla of scientific evidence, for a broad body of these claims: Computers will become conscious, have a \"theory of mind,\" think and reason, will become more intelligent than humans, and so on. But the claims are science fiction, not science.\nThis article reviews evidence for the following three (3) propositions using extensive body of scientific research and related sources from the cognitive and neurosciences; evolutionary evidence; linguistics; data science; comparative psychology; self-driving cars, and robotics; and the learning sciences.", "sections": [{"title": "1. Preface", "content": ""}, {"title": "A. Problems with the Use of AI Language", "content": "Throughout this article I examine the use of language when referring to the putative cognitive and affective capabilities of computing machines. In cases where such references are made with regard to computing machines, large language models (LLMs) and their computing substrates or any other artifactual information-processing system, use of such words and linguistic expressions are highlighted (\" \") as it is implicit in their use that they are strictly metaphorical expressions without contravening evidence.\nSo, for example, 'thinking' and 'reasoning' include terms like \u2018problem-solving', 'deliberating', 'deciding', 'judging', 'planning', 'understanding', 'interpreting', 'recognizing', 'remembering', and 'recalling'. As well as cognate terms such as 'semantic reasoning', 'symbolic thought' or 'symbolic manipulations', 'sapience' or 'wisdom', as well as references to 'mental concepts', 'mental' and 'cognitive' 'processes' or 'cognitive skills' or 'abilities'. Related terms include such terms as 'learning' and 'knowledge' (acquisition).\nThere are serious problems, however, in the use of these terms as they implicitly impute causal powers to computing machines. So, to say that an LLM and"}, {"title": "B. Use of the Terms 'Sentience' and 'Consciousness'", "content": "With regard to sentience and consciousness, sentience is the capacity to experience sensations-sensing or recognizing internal or external stimuli as well as the ability to perceive emotions in oneself or others. It is the \"simplest or most primitive form of cognition, consisting of a conscious awareness of stimuli without association or interpretation\" (APA Dictionary of Psychology: https://dictionary.apa.org/sentience).\nConsciousness refers to \"an organism's awareness of something either internal or external to itself\" (APA Dictionary of Psychology: https://dictionary.apa.org/consciousness).\nIt consists in the awareness of the external environment (i.e., primary con- sciousness), but it may also include awareness of one's own internal states or self-awareness (i.e., secondary consciousness).\nAs I will argue in this article, however, current computing machines are not sentient or conscious nor do they think or reason or have a theory of mind. The question of whether they will become sentient or conscious, think or reason, or have a theory of mind in the future is pure speculation and otherwise mean- ingless in the present context. Alan Turing suggested a similar response to the question of whether \"machines can think?\" The answer might \"be sought in a statistical survey such as a Gallup poll,\" which, he noted at the time, was absurd (Turing, 1950)."}, {"title": "C. Mind of a LLMentalist", "content": "A \"mentalist\" is a performing artist who appears to manifest highly developed in- tuitive or cognitive abilities. But they are not psychics, per se, and many eschew"}, {"title": "2. Do Computing Machines Think or Reason?", "content": ""}, {"title": "A. What are Thinking and Reasoning?", "content": "Thinking and reasoning imply conscious, cognitive processes. That is, if I'm thinking about something, I'm aware about what I am thinking about-not necessarily the underlying mental processes that I use to think with but rather the results of those mental processes.\nSo, to say that someone or something is thinking or evincing thought is to say that they are aware that they are thinking and that their thinking is from a particular viewpoint, that is, from the perspective of the someone or something doing the thinking. Hence, it must follow that all thought has a subjective aspect, that it is from some unique perspective or point of view.\nNow these underlying \"mental processes\" can be construed as various kinds of \"computations.\" So, for instance, I can use a HP hand calculator to solve complex algebraic equations using rules or \"algorithms\" that operate over formal logical principles. But is the HP hand calculator engaging in \u201cthought\u201d as a few buttons are pressed?\n\"An algorithm is an explicit, step-by-step procedure for answering"}, {"title": "A1. Biofeedback and Neurofeedback", "content": "In biofeedback and neurofeedback many brain and bodily functions occur at a level of nonconscious awareness. But a conscious human can potentially inter- vene in these nonconscious processes and alter their course.\n1. So, an individual can acquire awareness of many underlying nonconscious physiological functions using various instruments (e.g., EKG, EEG, EMG: heart, brain and musculature, respectively) that provide feedback on on- going activity of brain and bodily organs with the goal of being able to"}, {"title": "A2. Large Language Models (LLM)", "content": "Large language models (LLMs) acquire the ability to generate language by tire- lessly accumulating statistical relationships among text documents. And they are trained on vast amounts of text using recurrent networks that are capa- ble of both feed-forward and feedback mechanisms (i.e., backward propagation). These \"artificial networks\" are bidirectional insofar as the output of nodes in the computer architecture feedback on subsequent input from those very same nodes. That way, inputted text can predict the next word or sentence or statis- tically link to other elements in the overall text. But multimodal systems can also be trained in a similar fashion on images or even video or motion.\nIn a comparable manner, natural language text can be structured so as to de-"}, {"title": "A3. Self-Driving Cars", "content": "Case Study #1"}, {"title": "A4. Robotic Models", "content": "Case Study #2"}, {"title": "A5. Levels in a Computational System", "content": "David Marr (1982) proposed three, now four levels of analysis, in describing any \"computational\" system that converts raw sensory information into concepts and ideas. For instance, we can think of our biology as processing or trans- ducing physical information in the environment to more useful electrochemical information processed by the mind-brain-body. This first level is the \"physical\" implementation of a sensory modality in a biological system. In vision, the processing of light by the human eye and the transmission of this biochemical information by way of the optic nerve and the optic pathways is assembled in the visual or striate cortex at the back of the brain. But what algorithms or"}, {"title": "A6. Pareidolia", "content": "Pattern detection abilities-technically \"pareidolia\" are presumed to have"}, {"title": "A7. Embodiment", "content": "We do not simply inhabit our bodies; we literally use them to think with. And this occurs through our use of the perceptual system to sense and react to the physical world, through the use of our motor system which allows us to act on that physical world, as well as our mental and bodily interactions with the current culture and environment.\nFor example, Boston Dynamics' \"embodied humanoids\" are quite impressive in many ways (https://bostondynamics.com/). Unfortunately, the computational theory of mind fails to understand the importance of, and incorporate embodi- ment and its centrality, in the ability of humans and other animals to physically manipulate \"tools\" in their environment and directly interact with their world"}, {"title": "A8. Statistical Learning and Reasoning", "content": "Thus, one consequential critique of machine \"learning\" systems is that they aren't participating in any actual thinking or thought process, but merely trad- ing in statistical probabilities over time."}, {"title": "A9. Computational Anatomy", "content": "In animals including humans-nonconscious mental or cognitive processes are embedded in their underlying biology. For instance, some animals such as dol- phins and bats use bio-sonar (\"echolocation\")\u2014the ability to maneuver and find prey in their environment-by using sound waves to locate obstacles and poten- tial quarry. Dolphin sonar is composed of broadband cells in the brain that respond to a wide range of auditory frequencies and reaches well above the fre- quency range of normal human hearing-which tops out about 20,000 Hz-to 20,000 to 40,000 Hz. Indeed, sonic waves in the ocean convey sound energy at about 1376 meters per second, four times faster than sound energy in the air which ripples through the ether at about 344 meters per second.\nOf the latter, bat sonar works in a similar range and certain moths that bats feed on have evolved the capacity to engage in evasive actions when bat sonar targets them. How does bat sonar work? When a bat approaches a target, let's say an edible moth, changes in the velocity of approach of the bat (the rate of change of her position) are represented in neural maps in the brain that change their firing pattern as the velocity changes. This is known as \u201ccomputational anatomy.\" And it is nonconscious.\nAnd there are biological compasses. Many species of avians make use of both sun and magnetic compasses to navigate their aerial or arboreal habitats. For instance, birds can make use of the position of the sun yoked to the time of day to fly in a certain direction. That is, they acquire a celestial reference point and use it to navigate.\nBut honeybees rely on their sensitivity to ultraviolet light to navigate using the position of the sun. What appears to be involved is that the celestial pattern of polarization of sunlight is fundamentally the same as the pattern of polarization of the receptors in the bee's eyes. The bees simply match these two and the bee is off and flying to the appropriate food source or distant hive. The computational anatomy is embedded within the bee's optic system.\nBut in every case, these nonconscious biological programs are essential to sur- vival and carried out automatically under the appropriate environmental condi- tions. There is no presumption of direct conscious intrusion.\nAnd there are cognitive maps extensively studied in London taxicab drivers (Maguire et al., 2000). The driver must learn and memorize 320 standard routes through central London encompassing close to 25,000 streets within a six-mile"}, {"title": "A10. Creative Thought", "content": "It was announced in 2022 that AI outperformed human subjects on a creativity test. Artificial \u201cintelligence\u201d (i.e., GPT-4), was found to match the top 1% of \"human thinkers\" on a standard creativity test (i.e., TTCT, Shimek, 2023).\nThe Torrance Tests of Creative Thinking (TTCT), however, are severely out-dated and frankly, irrelevant. One can't assess creativity or creative thought with so-called \"paper and pencil tests\" (an elision)\u2014regardless of whether the test material is verbal, nonverbal or figural\u2014and whether in a human or in a human-made machine.\nIndeed, overall, the TTCT lacks any overarching theory of creativity or creative thought and has absolutely no relation to real-world creativity (e.g., Newton, Einstein, Picasso, Bach, Graham, Dickinson, Ellison, Morrison, Kafka or anyone or anything else).\nLongitudinal studies of the TTCT at 22, 40, and 50 years, however, show little relation to any form of \"public achievement,\" that is, creativity recognized by the larger community (Runco et al., 2010).\nSo, the finding actually tells us virtually nothing either about artificial \"intel- ligence\" or human intelligence in the realm of creative thought."}, {"title": "A10a. So, What is Creative Thought?", "content": "Let's step back and look at the larger intellective system that makes up the human mind/brain.\nI have argued that there are two major \"systems of thought\" in humans: a (1) pattern-based extraction system known as \u201cpareidolia\u201d and a (2) rule-based system -combinatorial/componential, hierarchical, and recursive informed by rules or \"algorithms\" (Seitz, 2019)."}, {"title": "A11. Human Brains Versus Computer Chips", "content": "The modern human adult brain contains, on average, about 86 billion neurons of which approximately 16.34 billion (19%) make up the grey and white matter of the cortex. The current estimates are there are about 6 billion+ neurons in the gray matter of the cortex and 10.3 billion neurons in the white matter with supporting neuroglia in both (8.7 billion and 20 billion, respectively) (Seitz, 2019).\nMoreover, the current estimates are that there are about 100 trillion synaptic (cell-to-cell) connections in the human adult brain. It's not quite, but almost all about the connections and well over 1100 times more than the number of neurons themselves. If the number of neurons in the typical corvid brain is approximately 1.74 billion-the neurons have an unusual compact structure- then the number of connections would hover around 1.91 trillion. That's one reason why they are smart.\nFor comparison's sake, the Cerebras CS-2 system the largest computer chip ever built-8.5\" on its side, contains 2.6 trillion transistors, and 850,000 AI- optimized cores is a monstrosity (Cerebras Systems, 2024). But it's the exact opposite of a human or corvid brain: (1) the transistors/cores are nearly all the same in the CS-2 system while human and corvid brains have dozens of different types of specialized neurons that perform diverse functions, (2) the number of connections between cores in the CS-2 system is a small fraction of number of connections in a human or corvid brain there are up to $10^4$ connections in any one neuron to both adjacent and distant neurons\u2014and (3) these human and corvid connections are further modulated by unique neurochemicals and supporting structures and processes. That's three good reasons why computers aren't \"intelligent.\""}, {"title": "A12. Augmenting Human Intelligence", "content": "Michael I. Jordan, Department of Computer Science, University of California, Berkeley, posits \"intelligence augmentation\" as the current state of \"human- imitative AI.\" Computers and software programs are useful because they augment our own intelligence just as a simple hand calculator or digital clock still does (Jordan, 2019).\nAlthough it has been referred to as \"artificial intelligence\" (AI), it might be better thought of as \"synthetic intelligence.\" A synthesis of the presumptive ways that the human brain and human cognition work together to create human-like \"intelligence.\""}, {"title": "A13. Conclusions", "content": "But, looking ahead, why would we want \"artificial intelligence\" to just mimic human intellect? Augmenting biological intelligence? Great. But simply emu- lating human intelligence would have limited advantages.\nWhat we want to create is a new kind of artificial \"intelligence,\" a \"synthetic intellect,\" that will abet human creativity and intelligence.\nNot a mental doppelganger, but a new class of factitious mentality, that thinks and creates in unfamiliar and novel ways.\nSyncretism not simulacrum.\nIndeed, a distinguished professor of artificial intelligence expresses similar senti- ments:\n\"The concept of \"AGI\" (a system that can match or exceed human performance across all tasks) shares all of the defects of the Turing Test. It defines \"intelli- gence\" entirely in terms of human performance. It says that the most important AI system capabilities to create are exactly those things that people can do well. But is this what we want? Is this what we need? I think we should be building systems that complement people; systems that do well the things that people do poorly; systems that make individuals and organizations more effective and more humane. Examples include: 1. writing and checking formal proofs (in mathematics and for software), 2. writing good tests for verifying engineered systems, 3. integrating the entire scientific literature to identify inconsistencies and opportunities, 4. speeding up physical simulations such as molecular dy- namics and numerical weather models, 5. maintaining situational awareness of complex organizations and systems, 6. helping journalists discover, assess, and integrate multiple information sources, and many more. Each of these capabili- ties exceeds human performance -- and that is exactly the point. People are not good at these tasks, and this is why we need computational help. Building AGI is a diversion from building these capabilities.\" Thomas G. Dietterich (Dis- tinguish Professor, Emeritus, Oregon State University; Former President, Asso- ciation for the Advancement of Artificial Intelligence, 04.16.2024, \"tweets\" on Twitter (X), https://engineering.oregonstate.edu/people/thomas-g-dietterich)."}, {"title": "3. Are Computing Machines Sentient or Conscious?", "content": ""}, {"title": "B. What is Consciousness and Sentience?", "content": "For a similar set of reasons, computers and their software programs are not sentient or conscious.\nWe know that perceptual consciousness-primary consciousness or awareness of one's environment-may have arisen in vertebrates as early as 530 million years ago (Feinberg & Mallet, 2017). But self-awareness or secondary consciousness may have evolved only in complex vertebrates including canines; cetaceans such as dolphins, porpoises, and whales; chimps and bonobos; elephants; and possibly corvids such as magpies (Seitz, 2019)."}, {"title": "B1. Consciousness in Canines", "content": "Canines and some other animal species manifest primary consciousness but, in some limited cases, potentially primary and secondary consciousness.\nThere is now some preliminary evidence from the Dog Cognition Lab at Barnard College/Columbia University that certain breeds of canines\u2014or certain individ- ual dogs-display self-recognitory abilities or something like \"self-awareness\" (Horowitz, 2017). That is, they clearly differentiate themselves from other ca- nines in an \"olfactory mirror test\" (the olfactory version of the mirror recogni- tion test, see below).\nSo, from recent research studies on canines they appear to think (e.g., dogs often tilt their heads before they retrieve a named toy suggesting that it might signify concentration and recall, Sommese et. al., 2021), they appear to be sentient- expressing pain to an injured body part, \"howling\" for a lost companion-and they may evince \"self-awareness\" as evidenced in an olfactory mirror test, as well as have something like a \"theory of mind\" (I wonder if Tom wants to walk to the park?).\n\"In particular, if the animal seems to be operating with regard to some mediating element between others' appearance and their behaviors, this behavior could be described as a rudimentary theory of mind\" (Horowitz, 2011).\nThey also understand referential gestures such as pointing-known as a deictic gesture-unlike chimps and bonobos in the wild. But the latter may have arisen as the result of domestication and the myriad ways that humans and other species learn from others."}, {"title": "B2. Consciousness in Avians", "content": "In birds and mammals, pallial neurons are functionally organized similarly, with sensory and effector areas, richly interconnected hubs, and highly associative areas in the hippocampus and \"nidopallium caudolaterale\" equivalent to the monkey prefrontal cortex-which is the portion of the pallium that is the seat of our (and their) ability to act on thoughts, feelings, and make decisions and informed by current reality gained from the senses (Seitz, 2019).\nAnd fibers within and across bird pallial areas are mostly organized at right an- gles reminiscent of the orthogonal, tangential, and radial organization of mam- malian cortical fibers.\nWe know that corvids (like crows) have sensory neurons that represent numeric quantities. And the associative pallium of crows, like the macaque prefrontal cortex (an old world monkey), is rich in neurons that represent what the animals"}, {"title": "B3. Consciousness in the Octopus", "content": "One may think of consciousness as an active relationship with the natural world with distinct faculties for perceiving, acting, and remembering past events. In- deed, it appears that octopuses have sentient qualities in their ability to protect a body part that has become injured and presumably conveys pain-suggesting that they are sentient to some extent (Godfrey-Smith, 2016).\nBut they also demonstrate play, tool use, behavioral flexibility, curiosity, and deception. They possess a distributed nervous system (essential parts of the nervous system are in the eight arms), engage in social learning, have recognition memory for other octopuses, and display episodic memory-memory for events such as food availability. Moreover, their deimatic displays on their skin may even encompass a \"visual language\" that they may use to communicate with others in the oceans.\nSo, it looks like cephalopods like octopuses are sentient and given their ex- tensive cognitive abilities-may possess something like primary (sensory) con- sciousness and maybe even vestiges of self-awareness."}, {"title": "B4. Consciousness in Human Infants and Young Children", "content": "When do human infants become self-aware? Phillipe Rochat at Emory Univer- sity has suggested five levels of self-awareness that unfold from birth to five- years-of-age (Rochat, 2003). If you place a human infant in front of a mirror before the second year, they display characteristic responses such as emitting a social smile, exploring the image in the mirror and its movements often with delight, and cooing. But by two-years-of-age, toddlers display a completely dif- ferent set of responses including completely halting their body movements out of intense interest in the object in the mirror, and often hide their faces or tuck their heads in their shoulders, displaying characteristic embarrassment. This is the classic \"mirror recognition test\" as well as the \"mirror mark test\" where a mark is placed on the infant (or a primate such as a chimpanzee) to see whether they go on to explore it in the mirror. Self-awareness and self-consciousness are on display. And by three-to-five years of age they go on to develop a robust \"theory of mind\" about others (see (C) below)."}, {"title": "B5. Conclusions", "content": "So, there is a broad array of scientific evidence that sentience and con- sciousness (both sensory consciousness\u2014awareness of the environment\u2014and self-awareness) are widely distributed across the animal kingdom including primates, mammals (e.g., canines), avians (sentience; primary and possibly secondary consciousness), and cephalopods (e.g., octopuses). And numerous bodily states may play a causal role in producing it.\nAny potentially sentient or conscious system needs to be able to weave a huge trove of information from the nervous and bodily systems-whether in a human or a honeybee, of which the latter may possess some aspects of sentience due to evolutionarily ancient midbrain structures shared across species (Barron & Klein, 2016)\u2014into a coherent conscious representation of the world.\nBut there is not a shred of scientific evidence that LLMS or other artificial \"intelligence\" systems or \"computing machines\" possess or display sentience or consciousness.\nSo, you might be hesitant to ask: What does it feel like to be a Roomba (http://bit.ly/4bymjpu)?"}, {"title": "4. Do Computing Machines have a \"Theory of Mind?\"", "content": ""}, {"title": "C. What is \"Theory of Mind?\"", "content": "The capacity to understand what other individuals may be thinking or feeling is known as \"theory of mind\" (ToM). That is, cognizing another's putative intentions, beliefs, desires, and even what they may be currently thinking about (Baron-Cohen, Leslie, & Frith, 1985, Premack & Woodruff, 1978; Wimmer & Perner, 1983).\nIt is called a \"theory\" because \"a system of inferences of this kind is properly viewed (as such) because such states are not directly observable, and the system can be used to make predictions about the behavior of others\u201d (Premack & Woodruff, 1978).\nTheory of Mind (ToM) has numerous underlying mechanisms that contribute to understanding what others may be thinking, feeling or planning. These include noticing social cues such as facial emotion and vocal prosody as well as interpreting a character's intention in a social scenario; shared knowledge about the world such as the ability to attribute mental states and mentalistic explanations to literary characters; and facility in interpreting actions, such as behavior based on false beliefs, appearance-reality distinctions, and other kinds of misunderstandings, to name a few (Byom & Mutlu (2013)."}, {"title": "C1: False Belief Tasks", "content": "For instance, in a first-order false belief task:\n\"A child is shown a scene with two doll protagonists, Sally and Anne, with a basket and a box, respectively. Sally first places a marble into her basket. Then"}, {"title": "C2. When and How do Children Acquire a \u201cTheory of Mind?\u201d", "content": "While second-order false belief tasks are easily handled by normal six-year-olds, and first-order belief tasks by normal four-year-olds, autistic children and normal children under three-years-of-age appear to lack it, whereas adult infra-human higher-order primates (e.g., chimpanzees, bonobos) may possess it.\nNevertheless, young children establish the capacity for theory of mind between three and four-years-of-age and are able to engage in deceit (e.g., my mom won't know that I ate a cookie while she was upstairs) as well as say one thing and mean another (e.g., I don't want to play anymore. . . because I'm hungry for dinner).\nHow so? The attention of others directed towards oneself, joint attention be- tween two individuals directed to another individual or thing, and the act of pointing to reference a person or object, can induce beliefs about another's be- havior or intentions and stimulate an interest in another person's mental state in young children."}, {"title": "C3. Do Other Species or Computing Machines Acquire a \"Theory of Mind?\"", "content": "So, if a bonobo sees a human hide an object the experimenters track the bonobo's gaze in the presence of another human who then leaves the room and the object is removed, the bonobo understands that the second human will think that the object is still there in what is a false-belief task. Similar behaviors are seen in Scrub jays (another member of the family of corvids) who re-cache foods to avoid pilfering by other birds.\nBut if a computing machine, as I have argued, is incapable of thought or reason, as well as lacks any capacity for sentience or consciousness, how could it even"}, {"title": "C1. Conclusions", "content": "None of this first- or second-order false belief tasks or theory of mind more generally, is possible in computing machines because they are (1) incapable of thought or reason and (2) lack any capacity for sentience or consciousness."}]}