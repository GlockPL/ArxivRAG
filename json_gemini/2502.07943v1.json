{"title": "CREDAL: Close Reading of Data Models", "authors": ["GEORGE FLETCHER", "OLHA NAHURNA", "MATVII PRYTULA", "JULIA STOYANOVICH"], "abstract": "Data models are necessary for the birth of data and of any data-driven system. Indeed, every algorithm, every machine learning model,\nevery statistical model, and every database has an underlying data model without which the system would not be usable. Hence,\ndata models are excellent sites for interrogating the (material, social, political, ...) conditions giving rise to a data system. Towards\nthis, drawing inspiration from literary criticism, we propose to closely read data models in the same spirit as we closely read literary\nartifacts. Close readings of data models reconnect us with, among other things, the materiality, the genealogies, the techne, the closed\nnature, and the design of technical systems.\nWhile recognizing from literary theory that there is no one correct way to read, it is nonetheless critical to have systematic guidance\nfor those unfamiliar with close readings. This is especially true for those trained in the computing and data sciences, who too often are\nenculturated to set aside the socio-political aspects of data work. A systematic methodology for reading data models currently does\nnot exist. To fill this gap, we present the CREDAL methodology for close readings of data models. We detail our iterative development\nprocess and present results of a qualitative evaluation of CREDAL demonstrating its usability, usefulness, and effectiveness in the\ncritical study of data.", "sections": [{"title": "1 INTRODUCTION", "content": "As the saying goes, knowledge is power. Through knowledge, communities make their worlds. Yet, as has been\nhighlighted since Foucault, the converse also holds: power is knowledge [20]. It is primarily those with power who are\nenabled to resource the data systems that drive contemporary data-driven decision and knowledge making. This, in\nturn, leads to a perpetuation of the status quo, where what is known is predominantly for and by those with power.\nThere is a vicious cycle in data systems of power begetting more power. Consequently, data systems are too often key\nenablers and amplifiers of the cruelties of the status quo [3, 5-7, 10, 11, 16, 35, 42, 45, 46].\nIn this work, we highlight an emerging thread in the broader conversation of how to open up and intervene in\ncycles of data for more equitable and just data futures. We focus on a seemingly mundane yet critically vital aspect\nof data and information system design and engineering: data modeling.1 Every information system (every algorithm,\nevery machine learning model, every statistical model, every Al solution, every database) has an underlying data model\nwithout which the system would not be usable. Data models, whether implicit or explicit, are present at the birth of\ndata and are necessary for the design, development, and use of any data system. Hence, data modeling is a core topic\ntaught in computer science, data science, software engineering, information systems, and information science degree\nprograms."}, {"title": "1.1 Why close readings of data models?", "content": "While working with (and within) data models, we bracket off the worlds behind the model's abstraction, often to the\npoint that we lose contact. This bracketing is, after all, the purpose of modeling: to highlight some descriptions of the\nworld over other descriptions. For many doing the technical work in computing and data science, this contact is never\nmade (or held in abeyance, as \"low value\u201d messy labor, the dirty wrangling work of data [36]).\nClose readings of data models reconnect us with, among other things:"}, {"title": "1.2 The challenge: How to read data models?", "content": "To our knowledge, a systematic methodology for reading data models currently does not exist. While recognizing from\nliterary theory that there is no one correct way to read [24], and that in fact every encounter of a reader with a text is a\nnon-repeatable experiment, it is nonetheless critical to have a starting point for those unfamiliar with close readings.\nThis is especially true for those trained in the computing and data sciences, who too often are enculturated to set aside\nthe socio-political aspects of data work."}, {"title": "1.3 Goals and Contributions", "content": "In our work, we have developed CREDAL, a structured methodology for the close reading of data models. Such a\nmethodology must be designed to facilitate a nuanced exploration of language, relationships, and patterns within the\ndata models, aiming to uncover the biases within modeling choices and contribute to the creation of fair and just\nknowledge representations. The goal of this research extended beyond the mere development of CREDAL; it was equally\nfocused on ensuring its effectiveness, usefulness, and applicability in practice. Therefore, we guided our research using\nthe following research questions:\nRQ1: Is CREDAL usable and useful?\nRQ1.1: Is CREDAL easy to learn and apply?\nRQ1.2: Does CREDAL improve data modeling proficiency?\nRQ1.3: Are learners likely to use CREDAL in the future?\nRQ2: Does CREDAL help understand, design and critically evaluate data models?\nRQ2.1: Does CREDAL help with understanding data models?\nRQ2.2: Does CREDAL alter the approach to structuring and modeling data within modeling tasks?\nRQ2.3: Do learners experience a change in their data modeling perspectives after working with CREDAL?\nWe will present the results of a qualitative study towards answering each of these questions, highlighting the perceived\nbenefits of CREDAL, and providing concrete indications for further research and development of the methodology."}, {"title": "2 BACKGROUND AND RELATED WORK", "content": "In the previous section, we have highlighted data models as imbricated in making possible and driving the social,\nmaterial, and genealogical facets of the work of data. We also positioned close readings of data models with respect to\nthe classical notion of close reading of texts. In this section, we place our work in the context of closely related work in\nreading and modeling data."}, {"title": "2.1 Reading data", "content": "We first highlight work on reading data and data models. Lindsay Poirier identifies three modes of close reading for\ndatasets: denotative (understanding technical aspects), connotative (understanding cultural contexts of production and\nchange), and deconstructive (understanding representational limits) [34]. Through case studies, Poirier's work shows"}, {"title": "2.2 Modeling data", "content": "Conceptual and knowledge modeling of data has been studied in the data management, business information systems,\nand knowledge representation communities for over 50 years. For overviews of these rich literatures, see the surveys of\nVeda Storey et al. [2, 43] and Wei Yun et al. [50]. Apart from a rich sub-literature on model validation (i.e., tools and\nmethods to ensure that a model is fit for purpose), we find that the overwhelming majority of work has overlooked\nbroader social aspects of data modeling. Notable exceptions include the qualitative investigation of Graeme Simsion and\ncolleagues that highlights the constructive consensus-making nature of modeling in business information systems [40],\nand recent efforts in the conceptual modeling community on inclusiveness in modeling, see Lukyanenko et al. [27]."}, {"title": "3 DEVELOPING THE CLOSE READING METHODOLOGY (CREDAL)", "content": "This section outlines the iterative process we followed to develop CREDAL. The process evolved through a series of steps,\neach designed to refine the methodology based on practical applications, feedback, and structured evaluation. Below,\nwe detail each step of this process."}, {"title": "3.1 Initial development", "content": ""}, {"title": "3.1.1 Initial Exploration and Refinement.", "content": "Our research team initiated the process by deeply engaging with the technique\nof close reading, traditionally employed in literary studies for detailed analysis of texts [24]. Following a review of\nthe relevant literature and practical application on literary texts, we sought to adapt this approach to the analysis of\ndata models. Each team member independently proposed methods for applying close reading to the schemas. After\nreviewing individual work, we compared our approaches, identified commonalities, and developed the first draft of a\nmethodology for applying close reading to data models.\nIn the next phase, we applied the newly developed methodology to a fresh set of schemas. This time, all team members\nused the same standardized approach, allowing us to observe how different individuals interpreted and applied the\nsame methodology in diverse ways. This approach highlighted variations in interpretation and application, which led\nus to further refine and modify the methodology.\nWe used both knowledge graph data models (examining schemas from Wikidata.org and Schema.org) and relational\ndata models during this first step. Recognizing the widespread use of relational data models [39] across various industries\nand the broad availability of standard relational data modeling curricula, we decided to focus on relational data models\nfor the remainder of our study. We conducted another round of close reading, this time using open-source data models,5\nand adapted our methodology based on this experience."}, {"title": "3.1.2 Development of Supplemental Materials.", "content": "In order to verify the applicability and effectiveness of the methodology,\nit was necessary to involve more independent reviewers. Therefore, to ensure that the methodology could be effectively\napplied by others, we developed a set of supplemental materials. Since different team members brought unique\nperspectives and strategies to the project, we synthesized these insights into a practical guide. This guide contains\nvaluable tips and advice aimed at helping users navigate potential challenges during applications of the methodology.\nAdditionally, we created a sample reading to demonstrate the methodology's practical application. The chosen\nexample, \"A Secure Students' Attendance Monitoring System\" [31] (see Appendix B for details), was specifically selected\nfor its balanced complexity, ensuring that it was detailed enough to be informative without overwhelming learners.\nThe familiar context of education made the example relatable to a wide audience, and the step-by-step approach offered\nclear guidance. Together, these supplemental materials served as essential resources, providing both conceptual support\nand practical reference points."}, {"title": "3.2 Iterative refinement", "content": "We needed a systematic approach to gathering deeper feedback on CREDAL to iteratively refine the methodology.\nMoreover, systematic feedback was necessary to answer the research questions guiding our study. Consequently, we\ndeveloped a set of semi-structured interview questions to gather feedback from individuals working with data models.\nWe engaged with three groups of people, of increasing size, first explaining the CREDAL methodology to them and then\ncollecting their feedback."}, {"title": "3.2.1 Initial Interviews with Undergraduate Students.", "content": "Each team member recruited a volunteer Computer Science\nstudent as a reviewer. The students were provided unlimited time to familiarize themselves with CREDAL and its\nsupplemental materials. They were also given unlimited time to apply the methodology to a relational data model,\nwhich was provided to them. The application of CREDAL was carried out in an unconstrained written form according to\nthe student's preferences.\nOnce the students reported that they had completed the application of the methodology, they were invited to\nparticipate in a 30-minute semi-structured interview. These interviews were recorded using an audio recorder, and the\nanonymized recordings were subsequently transcribed manually by team members. The text-based transcriptions were\nthen used for further analysis, enabling us to gather detailed feedback. Based on this feedback, we identified several\nareas for improvement and addressed key gaps in the methodology, enhancing its clarity and making it more intuitive\nand accessible for users."}, {"title": "3.2.2 Organizing a Workshop for Peer Feedback.", "content": "After refining the methodology, we conducted a workshop with 6\ngraduate students in computer science and data science at New York University. Prior to the workshop, participants\nwere invited to review the CREDAL methodology and its supplemental materials to familiarize themselves with the\nprocess. The workshop itself was one hour long, during which participants collectively applied the methodology to the\nsame relational data model as the previous respondents. Following the application of the methodology, feedback was\ngathered using the same set of interview questions.\nUsing the same data model and identical interview questions ensured consistency and allowed for a comparison of\ndifferent versions of the methodology. This approach also minimized variability in the feedback that might have arisen\ndue to differences in the size, complexity, or format of the data models. Additionally, using the same interview questions\nenabled us to directly track the progress of CREDAL's development and evaluate the effectiveness of the improvements\nwe made.\nFrom the feedback collected during the workshop, we gained valuable insights into how to make CREDAL easier to\nunderstand and use. One key recommendation was to increase the use of visual aids to accompany the methodology, to\nhelp participants familiarize themselves with the methodology more quickly."}, {"title": "3.2.3 Conducting a Pilot Study.", "content": "To address these recommendations, the next group of respondents, comprising 4\ngraduate students in data science and 7 undergraduate students in computer science, were provided with a video guide.\nThis guide presented the key points of the methodology in a visual format, with examples illustrating its application.\nRespondents were also given an improved version of the methodology guide, which included practical tips to facilitate\nthe application process, along with a completed example of applying CREDAL to a data model. We will provide additional\ndetails about the pilot study in Section 5, and will discuss results of evaluation in Section 6."}, {"title": "4 A STRUCTURED GUIDE TO CREDAL", "content": "In this section, we present a structured guide to CREDAL (Section 4.1), along with other materials we developed to\nsupport the adoption and use of the methodology (Section 4.2)."}, {"title": "4.1 CREDAL", "content": "(1) Define Research Goals and Understand the Data Model. Set clear objectives for your analysis, whether it be\nmodel-driven or, if data is available, data-driven. Understand your data and data model, paying attention to its\nstructure, domain, and other details, such as missing relevant details.\n(2) Evaluate the Context, Domain Knowledge and Sources. (a) Gain domain-specific insights from additional sources.\n(b) Consider ethical, privacy, and other concerns, including the absence of sensitive but relevant data. (c)\nEvaluate data sources for their biases and omissions.\n(3) Exploratory Data Analysis (EDA) and Schema Analysis.\n\u2022 For data-driven close reading: Identify general patterns in the data, including outliers and unexpected\nfeatures (for more practical tips on EDA, see [1, 15]).\n\u2022 For model-driven close reading: Conduct a detailed schema analysis, focusing on the following aspects:\n(a) Identifying entities and relationships; (b) Reviewing constraints such as primary keys, foreign keys,\nunique constraints, and field formats; and, (c) Identifying indices and design patterns.\n(4) Related Schemas Exploration. (a) Identify related schemas and assess their content in comparison to the primary\nschema; (b) Note any elements or relationships that are included or excluded in the related schemas; and, (c)\nAnalyze how the comparison can inform improvements or enhancements to the target schema.\n(5) Assumptions Loop.\n(a) Select a small portion of the data model (e.g., an entity, its attribute, or a relationship between entities)\nwhere potential interesting bias may be present.\n(b) Establish criteria for determining fairness and objectivity before identifying negative bias or inequality. It\nis essential to distinguish between assumptions and verifiable bias.\n\u2022 For data-driven close reading: Define metrics to quantify assumed bias or inequality. These may\ninclude skewness in data distribution, the presence of empty fields, or other quantifiable characteris-\ntics.\n\u2022 For model-driven close reading: Compare the schema with related models, analyze similar fields,\nand review restrictive types or irregular relationships to validate assumptions. Conduct thorough\ndomain research to substantiate any findings.\n(c) Identify potential risks or issues arising from the assumed bias. Consider specific scenarios in which\nthe model may fail or produce unintended consequences. i) Assess how sensitive attributes (e.g., gender)\ninfluence decision boundaries and overall fairness when legally and ethically permissible; ii) Evaluate\nfairness by analyzing causal pathways and understanding how different factors contribute to model\noutcomes; and iii) The \"5 Whys\" method is recommended to trace the root cause of bias and understand its\norigin (details in Section 4.2).\n(d) Propose solutions to mitigate identified harmful biases and/or enhance the comprehensiveness of the data\nor schema.\n(6) Compile Findings. Generate a conclusion summarizing your observations on the analyzed model. This report\nshould be easily comprehensible for the target audience and ideally provide valuable insights to aid the model\ndeveloper in its revision."}, {"title": "4.2 Supporting Materials", "content": "Video tutorial. As a result of the iterative improvements to the methodology, we identified the need to enhance the\nvisual presentation of the methodology to improve comprehension. In response, we developed a video tutorial that"}, {"title": "5 VALIDATING CREDAL", "content": "Incorporating human participants into the evaluation process provides valuable insights into the methodology's ability\nto achieve our intended objectives. This allows for an evaluation of whether the methodology effectively facilitates\na deeper critical understanding of data models. Additionally, engaging a larger audience in the testing phase helps\nmitigate biases by capturing a range of perspectives, revealing potential ambiguities in seemingly straightforward\nconcepts, and clarifying those that may initially appear complex. This process aids in validating both the content and\nthe structure of the methodology, determining if it is accessible, appropriately concise, or comprehensive for the subject\nmatter, and identifying other aspects that can only be revealed through application and feedback.\nAudience selection. To validate CREDAL, we conducted interviews with 11 students from applied science programs.\nSpecifically, we combined two academic groups, interviewing 4 students from an MS program in data science and 7"}, {"title": "5.1 Interview process", "content": "We started by conducting a workshop with participants to introduce the CREDAL methodology with the help of materials\ndescribed in Section 4. We then gathered their feedback on CREDAL using a semi-structured interview protocol. The basic\ninterview structure consisted of 14 questions, listed in Appendix A. We supplemented this structure with follow-up\nprompts when necessary, particularly when clarification was needed. Questions were divided into four categories:\n\u2022 Participant data modeling background and experience, and their experience with CREDAL. General feedback\non the methodology, including its strengths and weaknesses, perceived change in own understanding of data\nmodeling, and confidence in own data modeling skills.\n\u2022 Feedback on supporting materials, including a video tutorial and close reading example. Feedback on the\nusefulness of the literary close reading analogy for learning CREDAL.\n\u2022 Perceived effectiveness of CREDAL, including its practical applicability and the likelihood that participant would\nuse the methodology in their work or studies.\n\u2022 Feedback on how CREDAL may be improved in the future."}, {"title": "5.2 Interview coding", "content": "We recorded and transcribed the interviews and then coded them using the codebook presented in Appendix C. This\ncodebook was created manually, based on consensus among researchers over multiple rounds of independent coding\nand follow-up discussions. We also used Atlas.ti to help organize interview data (i.e., associate the manually generated\ncodes with quotes in the interview transcripts). Note that Atlas.ti offers the capability to use Generative AI to generate\nrelevant codes, including the option to align these codes with specific research questions for enhanced contextual\nrelevance. However, we opted for manual coding in this study. Given the novelty of our research thesis, manual coding\nensured that no critical details were overlooked and allowed for more accurate identification of recurring themes.\nTo mitigate bias, we independently tagged interview transcripts using two approaches, questions-based and context-\nbased.\n1. Questions-based coding method: To develop a set of codes, we first reviewed the interview questions and\nestablished corresponding code groups. Based on participants' responses, we identified the most frequently\nmentioned themes, created relevant codes, and assigned them to the predefined code groups. With this approach,\nwe generated 8 groups with 31 codes.\n2. Context-based coding method: Under this approach, we began by analyzing all interviews to identify the\nmost frequently occurring topics. Codes were created for these phrases based on the context, and using these\ncodes, we derived the names for the respective code groups. The codes were fine-grained, making them more\nspecific and less repetitive. With this approach, we generated 5 groups with 63 codes.\nAfter using these two approaches, we decided to analyze their similarities and differences. This process was also\ncarried out manually, with the authors of the two previous approaches looking for corresponding codes that were"}, {"title": "6 RESULTS ANALYSIS AND DISCUSSION", "content": "After conducting and analyzing all 11 interviews, we found that each participant had prior experience with data\nmodeling, with 2 reporting basic data modeling proficiency, 7 reporting intermediate proficiency, and 2 reporting\nadvanced proficiency. Further, none of the participants were familiar with the literary close reading methodology prior\nto the workshop.\nWhile each participant expressed unique thoughts and opinions, there were also significant commonalities among\nthem. By synthesizing the insights from all interviews, we identified notable patterns, which we will discuss throughout\nthis section. To start, we present the results of a simple quantitative analysis. Figure 3 shows a word cloud of participants'\nresponses. The most frequently used terms were \"methodology\", \"data\", and \"data model\u201d, followed by \"know\" and\n\"example\".\nFigure 4 presents a summary view of the frequencies of all codes from the codebook discussed in Section 5.2, with\ncolors representing our four code groups. We observe that methodology strengths is the most frequent code group (with\n\"Helpful supplemental materials\u201d, \u201cHelpful [methodology] structure\", and \"Encourages analysis and reflection\" as the\nmost frequent codes), followed by methodology effectiveness (\"Participant likely to use methodology in the future\" and\n\"Improved participant's data modeling proficiency\" as the most frequent codes), and then by methodology improvement\n(\"Need to improve presentation of bias\u201d and \u201cNeed to improve supplemental materials\" as the most frequent codes)."}, {"title": "6.1 Research Question 1: Usability and Usefulness of CREDAL", "content": "The first research question RQ1: Is CREDAL usable and useful? comprises three subquestions that we will address by\nanalyzing participant responses."}, {"title": "6.1.1 RQ1.1: Is CREDAL easy to learn and apply?", "content": "Participants noted that the methodology features a \"Helpful Structure\"\n(13 code entries) and provides an abundance of \"Helpful Supplemental Materials\" (28 entries, the most frequently cited\ncode). These findings suggest that the methodology is both feasible and effectively supported by the examples, tips, and\nadditional resources provided, which facilitate user implementation."}, {"title": "6.1.2 RQ1.2: Does CREDAL improve data modeling proficiency?", "content": "The code \"Improved participant's data modeling\nproficiency\" ranks among the top five most frequently cited codes, with 12 occurrences. This code reflects participant\nfeedback indicating that their data modeling skills and understanding significantly improved after the workshop.\nResponses varied regarding the specific ways in which their skills improved, yet many participants reported gaining"}, {"title": "6.1.3 RQ1.3: Are learners likely to use CREDAL in the future?", "content": "The code labeled \"Participant likely to use methodology\nin the future\" appears next, with a total of 18 occurrences. This indicates that participants are inclined to apply the\nmethodology in future projects or other contexts, often providing specific examples of such applications. Moreover,\nthey affirmed the potential applications of this method in various scenarios, as illustrated by their comments:"}, {"title": "6.2 Research Question 2: CREDAL helps work with data models", "content": "The second research question RQ2: Does CREDAL help understand, design, and critically evaluate data models?\nfocuses on the practical application of the CREDAL and its interaction with real data models, leading to three subquestions."}, {"title": "6.2.1 RQ2.1: Does CREDAL help with understanding data models?", "content": "The code \"Helped participant understand a specific\nmodel\" played a significant role in addressing this research question. Most participants emphasized that their data\nmodeling skills improved, noting increased confidence in their understanding of data models after engaging with\nCREDAL."}, {"title": "6.2.2 RQ2.2: Does CREDAL alter the approach to structuring and modeling data within modeling tasks?", "content": "Participants\nnoted that prior to the workshop, many did not consider how a model was created, which data was used, or the approach\ndevelopers followed in constructing a specific data model. However, after the workshop, participants observed a shift in\nperspective regarding data modeling tasks, indicating a willingness to adjust their approaches to create clearer data\nmodels. This is evident in quotations coded with \"Improved participant's data modeling proficiency\u201d:"}, {"title": "6.2.3 RQ2.3: Do learners experience a change in their perspectives on data modeling after working with CREDAL?.", "content": "Most\nparticipants had an intermediate level of understanding of data modeling; however, even those with prior experience\ndiscovered new insights and gained unique knowledge relevant to their education or work. The same code, \"Improved"}, {"title": "6.3 Opportunities for Improvement and Extension", "content": "Methodology improvement and extension were an essential focus of the interviews, as participants provided valuable\nfeedback on enhancing its clarity, presentation, and practicality. The identified areas were categorized into six distinct\ncodes, each reflecting specific concerns or suggestions from the respondents. Below, we discuss each code and its\nrelevance to the refinement of the methodology."}, {"title": "6.3.1 Improving CREDAL.", "content": "We first look at the codes in this group relevant to improving the methodology and materials.\nNeed to Improve Supplemental Materials. One of the recurring themes in the interviews was the need for clearer and\nmore structured supplemental materials. Several respondents expressed difficulties in understanding the methodology\nfrom the provided text files. As one participant mentioned:\nThis feedback emphasizes the importance of refining the materials to make the methodology more digestible. Another\nrespondent reinforced this point by suggesting the need to simplify the description further:\nAnalyzing data models can be a resource-intensive task. Therefore, our objective, in addition to offering a systematic\napproach through the CREDAL methodology, is to streamline the learning process and ensure that the methodology\ncan be applied as efficiently and effectively as possible.\nNeed to Add More Examples. Respondents also highlighted the importance of including more examples, particularly\ndiagrams or visual representations, to support the text-heavy material. One participant noted:"}, {"title": "6.3.2 Extending CREDAL.", "content": "Next, we look at the codes in this group that are relevant to extending the methodology and\nmaterials.\nOpportunity to Engage Domain Experts. Participants noted that involving domain experts could enhance the quality\nof the close reading process, particularly when scaling the methodology for larger teams or projects. One participant\nobserved:\nAnother participant highlighted the challenges of manually identifying similar data models (here in the form of\nrelational database schemas), stressing the potential value of expert involvement:\nThese suggestions point to the potential benefits of collaboration with domain specialists for a given data model\nto ensure a more informed and contextually relevant application of the methodology. Domain experts can provide\ninsights into data model complexities and nuances that may be difficult for non-experts to identify, thereby improving\nthe precision and quality of close reading analysis. Furthermore, the methodology is conveniently scalable for use not\nonly by individual data model developers or users but also by entire teams working collaboratively on a given data\nmodel. This scalability highlights the adaptability of the approach for both small-scale and large-scale projects.\nOpportunity to Automate. One participant repeatedly proposed automating certain parts of the methodology to make\nit more efficient and reduce manual workload. This respondent suggested:"}, {"title": "7 CONCLUSION, LIMITATIONS, AND FUTURE WORK", "content": "In our work with and within data and data systems, close readings of data models reconnect us with the materiality, the\ngenealogies, the techne, the closed nature, and the design of data models. We presented the CREDAL methodology for\nclose readings of data models, along with the results of a qualitative study demonstrating its usability, usefulness, and\neffectiveness in the critical study of data. CREDAL is the first systematic method for this important activity.\nWe conclude with a discussion of the limitations of this study and pointers for future work. (1) The development of\nCREDAL and our qualitative study of the methodology was limited to undergraduate and graduate students, some with"}, {"title": "A INTERVIEW QUESTIONS", "content": "In this section, we list all interview questions, categorized into four themes, as described in Section 5.\nBackground and experience with the CREDAL methodology.\n1. Please describe your familiarity and experience with data modeling before the workshop.\n2. Did your understanding of data modeling change as a result of the workshop? Do you feel more or less confident\nin your data modeling skills after the workshop? Please explain.\n3. Follow-up: Please describe your overall experience in learning and applying the CREDAL methodology during\nthe workshop.\n4. Follow-up: Which aspects of the methodology were particularly helpful? Which aspects of the methodology\nwere particularly challenging?\nFeedback on supplemental materials.\n5. On a scale of 1 to 5, how helpful was the tutorial for understanding the methodology? (1- Not helpful, 5- Very\nhelpful)\n6. How helpful was the example of close reading, we provided in helping for your understanding of the methodol\nogy? (1- Not helpful, 5- Very helpful)\n7. Are you familiar with the literary close reading technique? (Yes/No)\n8. If the answer was \"Yes\", did you find the comparison in the tutorial helpful for understanding our methodology?\nPerceived effectiveness of CREDAL.\n9. Were you able to apply the methodology during the reading session? (Yes/No)\n10. If the answer was \"No\", what challenges did you face?\n11. Please give an example of how you might apply CREDAL in your work or studies.\n12. Would you consider using the CREDAL methodology for \"reading of data models\" as a preliminary step for future\nprojects? (Yes/No)\n13. If the answer was \"Yes\", at what point during your project would you use the methodology? Will you use it as a\nchecker before starting work with some sort of datasets or data models?\nSuggestions for improving CREDAL.\n14. Please suggest ways to improve the methodology itself or how we explain it."}]}