{"title": "Integrating Cognitive AI with Generative Models for Enhanced Question Answering in Skill-based Learning", "authors": ["Rochan H. Madhusudhana", "Rahul K. Dass", "Jeanette Luu", "Ashok K. Goel"], "abstract": "In online learning, the ability to provide quick and accurate feedback to learners is crucial. In skill-based learning, learners need to understand the underlying concepts and mechanisms of a skill to be able to apply it effectively. While videos are a common tool in online learning, they cannot comprehend or assess the skills being taught. Additionally, while Generative AI methods are effective in searching and retrieving answers from a text corpus, it remains unclear whether these methods exhibit any true understanding. This limits their ability to provide explanations of skills or help with problem-solving. This paper proposes a novel approach that merges Cognitive AI and Generative AI to address these challenges. We employ a structured knowledge representation, the TMK (Task-Method-Knowledge) model, to encode skills taught in an online Knowledge-based AI course. Leveraging techniques such as Large Language Models, Chain-of-Thought, and Iterative Refinement, we outline a framework for generating reasoned explanations in response to learners' questions about skills.", "sections": [{"title": "1. INTRODUCTION", "content": "Online education such as massive open online courses, online professional certifications and online academic degrees have empowered learners with greater accessibility to education across domains and disciplines. With increased scalability and affordability, it is unsurprising that a significant portion of adult online learners are workers, most of whom require reskilling and upskilling [1, 2]. Videos in adult online education are a common source by which workers learn about new skills. The term \"skill\" implies the procedures for accomplishing a task. \"Skill learning\", also known as procedural memory, is the capacity to acquire any ability through practice and repetition [3, 4]. However, simply watching educational videos and answering quizzes fosters passive learning by learners. Based on literature from learning and education, active forms of learning often translates into a deeper understanding of the material taught and increased cognitive engagement by learners [5].\nAlthough there is a huge body of work that has investigated how students learn from a cognitive, education and learning sciences perspective [6], there is scant literature that considers how the educational technology can not only capture a deep understanding of the skills being taught but also have a capacity to address complex or novel learner inquiries. By \"understanding\", in this context as stated in the learning and cognition literature, we mean an entity's (a human learner's or even an Al agent's) ability to draw on correct inferences and not draw on incorrect inferences about the thing that is understood [7].\nEmergent advances in Generative AI such as large language models (LLMs) continue to demonstrate remarkable capabilities in various tasks, most notably question-answering based on large text corpora [8, 9]. However, studies have indicated that LLMs do not autonomously manifest any understanding of their assigned tasks [10]. Therefore, at a high-level, we ask \"Do LLMs understand skills taught in educational settings?\".\nTo address this gap, we propose a novel theory and application of learning that integrates Cognitive AI and Generative AI to enhance the representation and explanation of skills within online learning environments. From a Cognitive AI perspective, we consider each skill taught in a course as an 'abstract device'. As an abstract device, a skill has a design that includes specific functions and components, each with its own purpose. The design also incorporates causal mechanisms that combine these component functions into a comprehensive understanding of the skill. This approach emphasizes a hierarchical representation, where understanding a skill involves understanding its components, their interactions, and the teleological aspect how these elements contribute to the overall purpose or goal of the skill."}, {"title": "2. RELATED WORK", "content": "Based on the RQs and RHs from the previous section, we discuss related work revolving around two themes: (1) Representing skills in AI, and (2) Generating answers or explanations using AI.\nPrior work instigating how to represent skills, particularly in domains such as AI in education and robotics have been made. Well-established intelligent tutoring systems like Cognitive Tutors [15, 16, 17] use rule-based formulations based on the ACT* (Architecture of Cognition) theory of learning and problem solving [18, 19]. In addition to representing cognitive skills (goal-oriented knowledge), these tutors have successfully enabled learners to acquire other skills in domains like programming skills in LISP [20], geometry [21], and fractions [17]. Other approaches like an ontology-based framework for a structured representation of cognitive skills in K-12 settings have also been proposed [22].\nBut, how do humans represent knowledge? More importantly, can we represent knowledge using a common framework in AI-based systems? Chi et al. (1981) state that humans consider the knowledge in a given domain as world state representations [23]. While domain experts view these representations to be comprised of \"deep functional features\", novices tend to only consider them as \"shallow perceptual features\" [24]. Liu et al. (2012, 2015) [24, 25] integrated an expert representation (deep features) from the domain of algebra into SimStudent [26], a machine learning agent to help student learn through examples and get feedback from a cognitive tutor.\nIn robotics, skill representation is a foundational problem for various applications like planning, task completion and human-robot interaction [27, 28]. Vassos and Christensen (2013) propose a framework combining planning with formal task modeling to automate task modeling and execution planning in manufacturing robotics [29].\nTopp et al. (2018) [30] state that representing skills using knowledge-based approaches such as ontologies is also helpful for end-user programming of synchronized motions between robot arms or between human-robot interaction."}, {"title": "2.2 Generating Answers Using AI", "content": "Several studies have investigated knowledge-based question answering (KBQA) using various AI techniques [31, 32, 33, 34]. Braz et al. (2005) considers a hierarchical knowledge representation called EFDL (Extended Feature Description Logic) and uses Integer Linear Programming and phrase-level subsumption algorithms to generate answers to existing QA databases [35]. Balduccini et al. (2008) converts English text to logical representation and then uses automated logical theorem provers to extract facts and answer questions [36]. Leveraging structured knowledge base like databases, Anette et al. (2007) proposed a hybrid NLP-based architecture consisting of question, analysis, search, extraction, and answer preparation stages in multiple lanaguages [37].\nGiven the recent capabilities of LLMs, they have also been used in KBQA studies. Tan et al. (2023) leverages the few-shot learning capabilities of LLMs and proposes an in-context-learning framework to answer questions. By using a rank-based KBQA method and representing the questions as multiple choice questions, they provide the LLM with in-prompt examples to improve the LLM's answers for complex questions. Other rank-based approaches have also been used [38]. ReTraCk [39] is another LLM-based architecture for large-scale KBQA. The authors propose to convert questions into a logical form and then uses SPARQL queries and a BERT NER model to retrieve answers.\nTogether, all these studies have shown that there is a potential of leveraging cognitive AI and Generative AI methods for various use cases. However, very few studies consider modeling problem-solving skills as \"abstract devices\" that can be introspected for generating explanations to user questions within the context of online learning environments.\nTo address this gap in the literature, the next section describes a novel method that models a skill taught in an online course using a structured knowledge representation. Leveraging generative AI methods, we illustrate how an Interactive Video system, or 'Ivy', generates reasoned explanations in response to users' questions about a skill. While Ivy also enhances the interactivity of online educational videos, this paper focuses solely on the question-answering component of the framework."}, {"title": "3. METHODOLOGY", "content": "In cognitive science, it is well understood that declarative memory consists of both semantic memory, which encompasses facts and concepts, and episodic memory, which includes events and experiences [40, 41]. Our proposed \"Interactive Video\" system, or Ivy, draws inspiration from this cognitive science theory to answer questions. When a user asks a question to the system, Ivy first classifies user questions as either related to semantic knowledge - pertaining to theories, frameworks, concepts, or definitions - or to episodic knowledge - which concerns events, experiences, or specific instances of a concept.\nTo classify questions, Ivy employs generative AI methods, specifically a Large Language Model (LLM) based on GPT-3.5 Turbo, capable of zero-shot classification. This model uses in-context examples to determine whether a question relates to semantic or episodic knowledge [42]. Typically, questions formatted as \"What is X?\" or \"Explain X\" are categorized under semantic knowledge, while questions like \"How do I do X?\" or \"What is the process to achieve X?\" fall under episodic knowledge. The latter type of question is generally more challenging to answer as it often requires a simulation and an introspective analysis of the simulation process. This paper, however, focuses on questions related to semantic knowledge."}, {"title": "3.1 TMK Modeling", "content": "The TMK model is central to Ivy's functionality. TMK stands for \"Task-Method-Knowledge\", and is the primary knowledge representation model used by Ivy. The TMK model guides both the system's retrieval process and response generation. To integrate a new skill or concept into Ivy's capabilities, we first develop a TMK model of it, which involves several steps [43]:\n\u2022 Task Definition: We identify the goal or purpose of a given skill. This involves specifying the allowable inputs (referred to as givens) and the expected outputs (makes). For example, in a sorting algorithm, the goal could be to sort a list of numbers in ascending order.\n\u2022 Method Specification: We describe the mechanisms for acomplishing the tasks. This is generally done using a deterministic finite state machine (FSM) called an Organizer. Each Organizer consists of a set of states, transitions, and actions that guide the system through the process of achieving the task. Each state may also have sub-goals within their own mechanisms, enabling hierarchical modeling. For example, in a sorting algorithm, the methods could involve comparing pairs of numbers and swapping them if they are out of order.\n\u2022 Knowledge Representation: We define the objects, concepts, and relationships within the environment. This includes properties of the objects and the logical expressions that connect user-supplied values. Going back to the sorting algorithm example, the knowledge representation could include the concepts of numbers, lists, and the relationships between them."}, {"title": "3.2 Question Classification", "content": "Once a question has been categorized as pertaining to semantic knowledge, Ivy proceeds to classify it further based on the components of the TMK model. The system distinguishes whether the question relates to the Knowledge Model, the Method/Task Model, or the Multi Model. The Task and Method are interrelated within TMK, with the Task describing the goal of the skill and the Method outlining the mechanisms to achieve that goal; therefore, they are grouped together for classification. If a question does not pertain to the skill or concept being addressed, it is classified as Irrelevant. Ivy uses an LLM for zero-shot classification, providing it with descriptions and examples of each category. The model then classifies the question into one of these categories or as Irrelevant if it does not relate to the taught skill or concept.\n\u2022 Knowledge Model: Questions related to the objects, concepts, and relationships within the environment. These questions focus on the properties of objects and the logical connections between user-supplied values. For example, in a sorting algorithm, a Knowledge Model question might ask about the properties of the numbers being sorted.\n\u2022 Method/Task Model: Questions concerning the mechanisms for accomplishing tasks. These questions address the states, transitions, and actions that guide the system conceptually through the process of achieving the task, without executing it. For example, in a sorting algorithm, a Method Model question might ask about the process of comparing pairs of numbers and swapping them if they are out of order, without actually running the sorting algorithm.\n\u2022 Multi Model: Questions that involve both the Knowledge Model and Method/Task Model. These questions explore the interplay between the objects, concepts, and relationships within the environment and the mechanisms for completing tasks. For example, in a sorting algorithm, a Multi Model question might ask about how the properties of numbers influence the process of comparing and sorting them."}, {"title": "3.3 Relevant Knowledge Retrieval", "content": "Once the question has been classified as a Knowledge Model, Method/Task Model, or Multi Model question, Ivy retrieves the most relevant documents from the TMK knowledge base. The process of retrieving relevant documents involves two key steps:\nIvy calculates a k-score ranging from 1 to 4 based on the verbosity of the expected response. The k-score helps determine the level of detail and complexity in the response. For instance, a k-score of 1 suggests a very brief response (3-5 words), while a score of 2 indicates a short response of a few sentences. A k-score of 3 corresponds to a more detailed paragraph-length response, and a k-score of 4 leads to a comprehensive response spanning multiple paragraphs."}, {"title": "3.3.2 Document Retrieval", "content": "Depending on the k-score, Ivy retrieves the top k relevant documents from the TMK knowledge base. This retrieval is facilitated by the FAISS library [44], which is designed for efficient similarity search and clustering of dense vectors, making it ideal for finding relevant documents based on the input question. The documents are ranked according to their relevance to the question, allowing Ivy to focus on the most pertinent information for response generation.\nFor questions classified as Knowledge Model, only the top k Knowledge documents are retrieved. Similarly, for questions under the Multi Model category, the top k documents from a combined set of Knowledge, Task, and Method documents are retrieved. In contrast, for questions classified as Method/Task Model, the retrieval focuses on the most relevant Task Document and its associated Method Document."}, {"title": "3.4 Response Generation", "content": "Once the relevant documents have been retrieved, Ivy generates a response based on the classification of the question and the retrieved documents. The response generation process involves one of the following steps:\nFor questions classified as Knowledge Model or Multi Model, Ivy employs an iterative refinement process to generate a response. The iterative refinement process involves the following steps:\n\u2022 Initial Response Generation: Ivy generates an initial response based on the top most relevant document retrieved from the TMK knowledge base. The initial response is generated solely using the information contained in the document.\n\u2022 Refinement: Using the remaining k - 1 relevant documents, Ivy refines the initial response by incorporating additional information and context. The refinement process aims to enhance the response's accuracy, completeness, and relevance by iteratively incorporating relevant details from the retrieved documents.\nEach step in the iterative refinement process involves a call to the LLM to generate natural language responses. For the initial response generation step, a call is made to the LLM with the user query, the expected verbosity level (determined by the k-score), and the top most relevant document, provided as context. For the refinement step, the LLM is called with the user query, the initial response generated in the previous step, and the remaining relevant documents. The LLM synthesizes the information from the documents to refine the initial response and generate a more comprehensive answer."}, {"title": "3.4.2 Chain-of-Thought Generation", "content": "For questions classified as Method/Task Model, Ivy employs a Chain-of-Thought generation process to generate a response. The process involves two key steps:\n\u2022 Transition Extraction: Ivy extracts all the relevant transitions from the topmost Method document. These transitions are part of the finite state machine (FSM) that guides the system through the process of achieving the task. Each transition represents a step or action in the task completion process, and includes the associated states, actions, and conditions.\n\u2022 Transition Explanation Generation: Using the extracted transitions and the metadata, Ivy generates a natural language response that explains the task completion process through the transitions, ensuring the conditions, actions, and states are clearly articulated. The response provides a detailed explanation of the method and the steps involved in achieving the task.\nThe Transition Explanation Generation step leverages the information contained in the transitions to generate a coherent and detailed response that elucidates the method and task completion process. Chain-of-Thought prompting [13] is used to guide the generation process, with multiple in-prompt examples provided to the LLM to demonstrate the reasoning and context required for stitching together the transitions (which might be in a non-linear order). Further, the expected verbosity level (determined by the k-score) is provided to guide the response generation length."}, {"title": "4. A WORKED EXAMPLE", "content": "To better illustrate the application of our approach, we use the classic river crossing problem from an Al course as an example. This problem involves three missionaries and three cannibals, or alternatively, three guards and three prisoners, who must cross a river using a boat that can carry at most two people. If the number of prisoners on either side of the river exceeds the number of guards, the prisoners will overpower the guards and escape. The goal is to transport all individuals to the other side of the river without violating this rule.\nWhen a user poses the question, \"Who is a guard?\", the following steps are taken to generate a response:"}, {"title": "4.1 Step 1: TMK Modeling", "content": "We begin by creating a highly simplified TMK model for the river crossing problem.\nHere, our primary task might be defined as Transport All Individuals Across the River. The method could involve steps such as: Move Boat to Opposite Bank, Load Boat with Selected Individuals, Unload Boat, and Check for Constraint Violation. The knowledge representation might include the concepts of Guards, Prisoners, Boat, the relationships between them, and the rules governing the movement of individuals."}, {"title": "4.2 Step 2: Question Classification", "content": "To classify the question \"Who is a guard?\", we submit a query to the LLM that includes a description of the TMK model components and a prompt for classification:\nClassify the following question as either a Knowledge Model, Method/Task Model, Multi Model, or Irrelevant question:"}, {"title": "4.3 Step 3: Relevant Knowledge Retrieval", "content": "After classifying the question, we proceed to retrieve the relevant Knowledge documents from the TMK knowledge base. The first step is calculating the k-score based on the verbosity expected in the response. For a question like \"Who is a guard?\", a k-score of 2 might be appropriate, indicating a short answer. This helps in determining the number of relevant documents to retrieve.\nWe then use the FAISS library to perform a similarity search between the user query and all Knowledge documents in the TMK knowledge base. The top k = 2 documents with the highest similarity scores are retrieved for response generation. These documents might be:\n\u2022 Document 1 Guard Definition: Contains the definition and description of the guards in the river crossing problem. Similarity score: 0.60\n\u2022 Document 2 Relationships: Contains information about the relationships between guards and prisoners in the river crossing problem. Similarity score: 0.45"}, {"title": "4.4 Step 4: Response Generation", "content": "As the question \"Who is a guard?\" is classified as a Knowledge Model question, we employ the iterative refinement process to generate a response. An initial request of the following form is made to the LLM with the user query and the top most relevant document (Document 1) to generate an initial response:\nThe LLM generates an initial response based on the user query and the content of Document 1. The initial response might be: \"In the river crossing problem, the guards are one of the individuals who need to be transported across the river.\" This response is then further refined by incorporating additional information from Document 2. A request of the following form is made to the LLM for refinement:\nCorrect:\nCorrect:\nIn the river crossing problem, the guards are individuals who need to be transported across the river. They play a crucial role in ensuring that the prisoners do not escape during the crossing.\"\nThis worked example demonstrates how our system uses the TMK model to structure its knowledge base and respond effectively to user queries."}, {"title": "5. CONCLUSION", "content": "In this paper, we have presented an innovative integration of Cognitive AI with Generative AI to enhance question answering in skill-based learning environments. Our approach utilizes the Task-Method-Knowledge (TMK) model to systematically encode the skills being taught, providing a structured and detailed representation that supports accurate and contextually rich explanations.\nCombining the TMK model with Generative AI techniques, including Large Language Models (LLMs), Chain-of-Thought, and Iterative Refinement, we have developed a system, Ivy, that can produce answers that are not only contextually relevant but also pedagogically relevant. The TMK model captures the hierarchical structure of skills, enabling Ivy to generate reasoned explanations. We envision that this approach might be particularly beneficial in online education, where dynamic and interactive content is essential for effective learning. The goal of such systems would be to bridge the gap between passive content consumption and active learning engagement by simulating a deeper understanding and providing interactive feedback to learners.\nThe focus of the system described above is on semantic knowledge question answering. However, the TMK framework also holds potential for expanding into episodic knowledge question answering. By using the Organizer components in the Method specification of a TMK model, Ivy can simulate problem-solving processes and generate logs of the states visited and transitions taken during the problem-solving process. By inspecting these logs, Ivy can generate detailed explanations of the problem-solving process, allowing it to answer \"What if?\" and \"How?\" questions that pertain to specific instances or events."}, {"title": "5.1 Future Work", "content": "Looking ahead, several avenues for future work and improvement exist.\n1. Evaluation Mechanism: We aim to refine and standardize the evaluation mechanism for assessing performance of our TMK-based system. The evaluation metrics outlined in Table 1 Correctness, Completeness, Confidence, Comprehensibility and Compactness provide an overview of the system's performance, but further refinement and validation might be needed to ensure robust evaluation. The current metrics are based on an Al engineer's subjective evaluation (using a 5-point Likert scale [45]). However, we plan to implement a mixed-methods evaluation approach that combines automated metrics (e.g., BLEU, ROUGE) with human evaluations to measure the output quality more effectively.\n2. Automation of TMK Model Creation: Currently, the creation of the TMK model is a manual process that requires domain expertise. Automating the development of TMK models can streamline the setup process for new educational content, making it more scalable and adaptable to diverse learning domains.\n3. Enhanced Generative AI Techniques: Further refining the integration of Chain-of-Thought and Iterative Refinement techniques may improve the system's ability to handle more nuanced and complex questions, enhancing the quality and depth of responses generated."}]}