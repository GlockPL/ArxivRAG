{"title": "The Linear Attention Resurrection in Vision Transformer", "authors": ["Chuanyang Zheng"], "abstract": "Vision Transformers (ViTs) have recently taken computer vision by storm. However, the softmax attention underlying ViTs comes with a quadratic complexity in time and memory, hindering the application of ViTs to high-resolution images. We revisit the attention design and propose a linear attention method to address the limitation, which doesn't sacrifice ViT's core advantage of capturing global representation like existing methods (e.g. local window attention of Swin). We further investigate the key difference between linear attention and softmax attention. Our empirical results suggest that linear attention lacks a fundamental property of concentrating the distribution of the attention matrix. Inspired by this observation, we introduce a local concentration module to enhance linear attention. By incorporating enhanced linear global attention and local window attention, we propose a new ViT architecture, dubbed L2ViT. Notably, L2ViT can effectively capture both global interactions and local representations while enjoying linear computational complexity. Extensive experiments demonstrate the strong performance of L\u00b2ViT. On image classification, L2ViT achieves 84.4% Top-1 accuracy on ImageNet-IK without any extra training data or label. By further pre-training on ImageNet-22k, it attains 87.0% when fine-tuned with resolution 3842. For downstream tasks, L2ViT delivers favorable performance as a backbone on object detection as well as semantic segmentation.", "sections": [{"title": "1. Introduction", "content": "The computer vision community has witnessed the prosperity of convolutional neural networks (CNNs) [24,30,50] over the last decade. Recently, vision transformers rise rapidly and have yielded impressive performances on various vision tasks including image classification [36], object detection [16], segmentation [47] and so on. Beginning with the pioneering work of ViT [17], which first challenges CNNs with the vanilla transformer on image classification, ViTs have evolved to become increasingly powerful. The key component behind the success of ViTs is self-attention, which empowers ViTs with a global receptive field, adaptive data specificity, and more human-like representations [39,53].\nThese advantages, however, come with quadratic computational complexity in time and memory with respect to input resolution. Various methods are proposed to address this issue and make ViTs applicable in more downstream tasks such as object detection. The first representative approach is to restrict the softmax attention to fixed-size window ranges, such as local 7x7 window [36], sliding window [79]. However, this line of work has been observed to have limited model capacity due to the sacrifice of the global receptive field, which brings strong model capacity [15]. Another typical approach aims to reduce the number of keys or values in attention via linear projection [59], convolution [76], pooling [18]. When targeting high-resolution input in dense prediction tasks, they apply a relatively large downsampling ratio in the earlier stages, e.g., 8 in the first stage, to reduce the computational cost. This will inevitably damage the model's performance since aggressive downsampling operations lose some crucial context information and destroy the global dependency modeling ability of self-attention to a certain extent.\nTo overcome the above issues, we propose to replace softmax attention with linear attention [3,28]. In this paper, linear attention refers to the kernel-based attention mechanism, detailed in the related work, and not all attention variants are with linear complexity. On the one hand, linear attention takes advantage of the associativity property of matrix products to achieve computational complexity of O(N) (N is the number of patches in vision transformers). On the other hand, linear attention still models communications among all tokens and learns a global spatial relationship, which is essential for visual recognition tasks and hurt by the above attention variants. Nevertheless, previous works [25, 36, 42, 43, 75] show linear attention performs inferiorly compared to other attention variants in vision transformer. We thoroughly investigate softmax attention and linear attention, demystifying two key insights of softmax attention. The first property is that all values in the attention map must be non-negative as verified in Tab. 1, so"}, {"title": "2. Related Work", "content": "Although the tremendous success of transformer in natural language processing (NLP), transformer [54] has no significant influence on computer vision (CV) until the groundbreaking work by Dosovitskiy et al. [17] proposes to split the image into patches and applies a pure transformer to process these patches like tokens in NLP. Their work shows competitive performance on image classification and reveals the great modeling capacity of transformer for vision tasks. The results galvanize researchers to bring ViTs into more vision tasks beyond classification. However, the quadratic computational cost of self-attention prevents ViTs from high-resolution input, which is common in visual recognition. To deal with this, Swin [36] propose to restrict the self-attention in a fixed range by the local window, followed by other window-based approaches like cross-shape window [16], pale-shape window [63], and [68, 79]. Another line of works explores reducing the number of keys"}, {"title": "2.1. Vision Transformer", "content": "Although the tremendous success of transformer in natural language processing (NLP), transformer [54] has no significant influence on computer vision (CV) until the groundbreaking work by Dosovitskiy et al. [17] proposes to split the image into patches and applies a pure transformer to process these patches like tokens in NLP. Their work shows competitive performance on image classification and reveals the great modeling capacity of transformer for vision tasks. The results galvanize researchers to bring ViTs into more vision tasks beyond classification. However, the quadratic computational cost of self-attention prevents ViTs from high-resolution input, which is common in visual recognition. To deal with this, Swin [36] propose to restrict the self-attention in a fixed range by the local window, followed by other window-based approaches like cross-shape window [16], pale-shape window [63], and [68, 79]. Another line of works explores reducing the number of keys"}, {"title": "2.2. Efficient Attention", "content": "How to address the quadratic computational cost of self-attention has attracted many researchers. Apart from the above efficient attentions in vision transformers, there are numerous methods in NLP [51]. They can be broadly categorized into the following categories: 1) sparse patterns [2, 6, 72, 74], which sparsify the attention matrix using hand-crafted or learned pattern; 2) downsampling/low-rank [27, 58, 66], which projects the key/value tensor into smaller tensor; 3) neural memory [31, 48], which leverages a side memory module for accessing multiple tokens; 4) linear attention, which decomposes the exponential kernel in softmax attention into dot product of kernel feature maps and is most related to our work. Katharopoulos et al. [28] first propose linear attention and accelerate transformer in an iterative implementation like recurrent neural networks. Peng et al. [40] use random feature methods to approximate the softmax function. Performer [7] further introduces a positive orthogonal random feature mechanism. Moreover, cosFormer [42] proposes a cosine-based distance re-weighting mechanism and achieves comparable accuracy. Most recently, Cai et al. [3] first explore a light-weight linear attention with low computation. Han et al. [20] propose an rank restoration module to enhance the expressiveness of self-attention. In this paper, we investigate the reasons underlying the failure of linear attention in general-purpose vision transformers and design a novel concentration module to make linear attention competitive with vanilla attention."}, {"title": "3. Preliminaries", "content": "The attention mechanism is a core advantage of vision transformers over CNNs. Let \\(X \\in \\mathbb{R}^{N \\times C}\\) denote a sequence of N feature patches of dimension C, the vanilla softmax attention output \\(O \\in \\mathbb{R}^{N \\times C}\\) can be expressed as follows:\n\\begin{equation}\nO_i = \\sum_{j=1}^{N} A_{ij} V_j = \\sum_{j=1}^{N} \\frac{\\exp(Q_i K_j^T)}{\\sum_{k=1}^{N} \\exp(Q_i K_j^T)} V_j.\n\\end{equation}\nThe A is the ith row in the learned attention matrix \\(A \\in \\mathbb{R}^{N \\times N}\\). \\(Q \\in \\mathbb{R}^{N \\times C}\\), \\(K \\in \\mathbb{R}^{N \\times C}\\), and \\(V \\in \\mathbb{R}^{N \\times C}\\) denotes query, key, and value matrix, generated by learnable linear projection \\(Q = W_Q X\\), \\(K = W_K X\\), and \\(V = W_V X\\), respectively. And exp denotes the exponential function. Note that we omit the scale factor \\(1/\\sqrt{C}\\) for simplicity."}, {"title": "4. Method", "content": "Some previous works [7, 28, 40] have proposed different kernel function variants and achieved comparable results in NLP. Nevertheless, when researchers attempt to apply these linear attention mechanisms to vision transformer, the performance of linear variants lags far behind vanilla counterpart, e.g., 78.7% (Performer) vs. 81.8% (Vanilla) [25]. These results suggest they all ignore some essential information for visual recognition. We re-examine linear attention from a visual perspective and show it can achieve on-par expressivity with softmax attention by incorporating two key properties.\n4.1. Non-negative Property\nFirst, the exponential function in vanilla attention forces all the values in the attention map A to be non-negative. Although the value matrix V contains negative values, unnecessary interactions (entries with values close to zero in A) still produce almost zero effect in the output. Instead, if the unnecessary interactions retain negative values in A, they may strengthen the irrelevant contextual"}, {"title": "4.1. Non-negative Property", "content": "First, the exponential function in vanilla attention forces all the values in the attention map A to be non-negative. Although the value matrix V contains negative values, unnecessary interactions (entries with values close to zero in A) still produce almost zero effect in the output. Instead, if the unnecessary interactions retain negative values in A, they may strengthen the irrelevant contextual"}, {"title": "4.2. Local Concentration Module", "content": "Although linear attention can capture similar correlations as softmax attention, there is still a significant performance gap, as shown in Tab. 1. We discover that the less favored performance of linear attention is mainly caused by the less concentrated attention map. Through re-weighting of softmax, vanilla attention can concentrate on important neighboring patches and other meaningful interactions as shown in Fig. 2, e.g. layer 4. In contrast, linear attention presents a more dispersive map and trivially distributes attention scores over all patches. Although it can capture long-range dependencies, linear attention emphasizes neighboring patches less and preserves fewer local details as distracted by distant patches, potentially losing some essential fine-grained visual features of objects.\nTo further demystify these effects, we randomly pick some input images from ImageNet-1K [13] and visualize the activation maps of DeiT-Tiny equipped with vanilla softmax and ReLU-based linear attention using Grad-CAM tool [44]. As clearly shown in Fig. 1, the former pays the most interest in the object itself, while the latter suffers from distractions of background and other stuff. These analyzes uncover that linear attention needs to concentrate more on important local information.\nMotivated by the above observations, an intuitive way to preserve more local information is applying convolution following linear attention to distill the dispersive attention and reinforce local contextual features. Formally, recall that Oj is the attention output for j-th patch in Eq. (1), the output enhanced by convolution can be written as:\n\\begin{equation}\nO_i^c = \\sum_{j \\in \\Omega_i} w_{ij} O_j = \\sum_{j \\in \\Omega_i} \\sum_{k=1}^{N} w_{ij} A_{jk} V_k,\n\\end{equation}\nwhere \\(\\Omega_i\\) is the local window centered at i and \\(w_{ij}\\) is the convolution weight. The above formulation explicitly shows that convolution can aggregate different rows in attention map A. Fig. 2 provides visualization for the aggregated attention maps after convolution, in which neighboring patches (near the diagonal) receive stronger attention than that of linear attention without convolution. Specifically, we introduce a very lightweight local concentration module (LCM) consisting of two depth-wise convolutional layers:\n\\begin{equation}\nX = \\text{GELU}(\\text{DWConv}_1(\\text{Rearrange}(X))) \\in \\mathbb{R}^{C \\times H \\times W},\n\\end{equation}\n\\begin{equation}\nX_{\\text{LCM}} = \\text{Rearrange}(\\text{DWConv}_2(\\text{BN}(X))).\n\\end{equation}\nwhere X is the output of the linear attention block. H, W are the height and width of the feature map respectively. We"}, {"title": "4.3. Overall Architecture", "content": "We integrate the local concentration module (LCM) and linear attention to build a Linear Global Attention block (LGA), which captures the global contextual information. Meanwhile, we employ window attention [36] to build a Local Window Attention block (LWA), which introduces an ideal locality and refines the fine-grained feature representations. These two complementary blocks are stacked alternatively to design an efficient and general-purpose vision transformer dubbed L2ViT.\nThe overall architecture and block details are illustrated in Fig. 3. We employ a hierarchical framework to obtain pyramid feature maps for a broad range of visual recognition tasks. Given an input image with size H \u00d7 W \u00d7 3, we leverage a convolutional stem (two 3\u00d73 convolutional layers with stride 2) to obtain \\(\\frac{H}{4} \\times \\frac{W}{4}\\) patches with dimension C. Then all patches go through the following four stages, each stage \\(i \\in (1,2,3,4)\\) contains Ni LWA and Ni LGA"}, {"title": "4.3. Overall Architecture", "content": "We integrate the local concentration module (LCM) and linear attention to build a Linear Global Attention block (LGA), which captures the global contextual information. Meanwhile, we employ window attention [36] to build a Local Window Attention block (LWA), which introduces an ideal locality and refines the fine-grained feature representations. These two complementary blocks are stacked alternatively to design an efficient and general-purpose vision transformer dubbed L2ViT.\nThe overall architecture and block details are illustrated in Fig. 3. We employ a hierarchical framework to obtain pyramid feature maps for a broad range of visual recognition tasks. Given an input image with size H \u00d7 W \u00d7 3, we leverage a convolutional stem (two 3\u00d73 convolutional layers with stride 2) to obtain \\(\\frac{H}{4} \\times \\frac{W}{4}\\) patches with dimension C. Then all patches go through the following four stages, each stage \\(i \\in (1,2,3,4)\\) contains Ni LWA and Ni LGA blocks alternatively. Between stages, we use another convolutional layer (2\u00d7 2, stride 2) to merge patches and double the dimension. Especially, we introduce the flexible Conditional Positional Encodings (CPE) [9] to replace the relative position embedding in every block.\nWe build several L2ViT variants with different FLOPs and number of parameters. The detailed configuration is provided in Appendix C. In all variants, for a fair comparison with previous works, we keep the strictly same number of blocks, heads, and channels as Swin [36], while deepening the depth will improve the performance as shown in Tab. 7."}, {"title": "5. Experiments", "content": "For a fair comparison, we train our models for 300 epochs following the recipe in [36, 37, 52]. More details are provided in Appendix. Tab. 2 compares our L2ViT with state-of-the-art ConvNets and Vision Transformers trained only on ImageNet-1k. L\u00b2ViT achieves stronger performance under different model sizes and computational complexities. Compared to ConvNets, our L\u00b2ViT has better accuracy. Especially, while most vision transformers show unsatisfactory results on the small size compared to EfficientNet, L2ViT-T obtains an improved result of 83.1%.\nOur L2ViT outperforms other vision transformers, including Swin (fixed-size window attention) and Twins-SVT (mixing window and keys/values reduction attention). For example, L2ViT-B achieves an accuracy of 84.4%, surpassing Swin-B and Twin-SVT-L by +0.9% and +0.7%, respectively. This shows the superiority of enhanced linear attention in capturing the global context. Meanwhile, L2ViT outperforms channel attention-based DaViT, by a large margin. For example, L\u00b2ViT-B achieves +0.5% higher accuracy than DaViT-B, indicating that global patch-to-patch interactions play a more critical role than channel-to-channel inter-"}, {"title": "5.1. ImageNet-1K Classification", "content": "For a fair comparison, we train our models for 300 epochs following the recipe in [36, 37, 52]. More details are provided in Appendix. Tab. 2 compares our L2ViT with state-of-the-art ConvNets and Vision Transformers trained only on ImageNet-1k. L\u00b2ViT achieves stronger performance under different model sizes and computational complexities. Compared to ConvNets, our L\u00b2ViT has better accuracy. Especially, while most vision transformers show unsatisfactory results on the small size compared to EfficientNet, L2ViT-T obtains an improved result of 83.1%.\nOur L2ViT outperforms other vision transformers, including Swin (fixed-size window attention) and Twins-SVT (mixing window and keys/values reduction attention). For example, L2ViT-B achieves an accuracy of 84.4%, surpassing Swin-B and Twin-SVT-L by +0.9% and +0.7%, respectively. This shows the superiority of enhanced linear attention in capturing the global context. Meanwhile, L2ViT outperforms channel attention-based DaViT, by a large margin. For example, L\u00b2ViT-B achieves +0.5% higher accuracy than DaViT-B, indicating that global patch-to-patch interactions play a more critical role than channel-to-channel inter- actions.\nBesides, we pre-train L2ViT-B on the larger scale"}, {"title": "5.2. COCO Object Detection", "content": "We conduct object detection experiments on COCO dataset [34] using standard Mask R-CNN [23] and Retina [33] detection framework implemented in MMDetection Toolboxes [4]. For a fair comparison, we follow the same recipe as Swin [36].\nTab. 4 summarizes the results measured by both box and mask mAP. For detection results with Retina, L2ViT outperforms Swin and Twins-SVT by a large margin. For example, L2ViT-T improves over Swin-T and Twins-SVT-S by +2.1 and +1.1 APb respectively. This further shows that enhanced linear attention indeed extracts a richer representation and enables the model to detect objects better.\nFor detection results with Mask R-CNN, L2ViT brings clear improvements over Swin and Twins-SVT in different model sizes. Meanwhile, L2ViT-S surpasses CETNet-S +0.6/+0.8 in APb /APm. Although L\u00b2ViT-B performs slightly worse than CETNet-B in APb, the results on APm are reversed. We also stress that CETNet applies a deep-narrow model ([4,4,30,2] of CETNet-B vs. [2,2,18,2] of ours) that brings extra gains as shown in Tab. 7. The improved performance on both detection frameworks validates the generalizability of our proposed L2ViT."}, {"title": "5.3. ADE20K Semantic Segmentation", "content": "We conduct semantic segmentation experiments on ADE20K [78] dataset. We adopt the UperNet [64] segmentation framework implemented in MMSegmentation Toolboxes [10]. Following Swin [36], we train all models for 160k iterations with a batch size of 16, AdamW optimizer, multi-scale training, and stochastic depth.\nWe present the result in Tab. 5. Consistent improvements over Swin and Twins-SVT can be observed. In detail, L2ViT-S achieves +1.1 and +1.3 higher mIOU than Swin-S and Twins-SVT-B. L2ViT also outperforms other vision transformers under all model sizes, e.g., L2ViT-T/S/B exceeds Focal-T/S/B by +0.4, +0.7, and +0.2 mIOU, respectively. The superior performance on semantic segmentation further demonstrates the effectiveness of enhanced linear attention and expressivity of L2ViT."}, {"title": "5.4. Ablation Study", "content": "We train all models for 300 epochs on ImageNet-1k and fine-tune Mask R-CNN for 1x schedule.\nComponent Analysis To study the effectiveness of key components in L2ViT, we make several architecture changes and report the results in Tab. 6. It can be observed that: 1) shrinking the kernel size in LCM into 3\u00d73 causes a dramatic drop in classification, which indicates that a large receptive field of LCM is important for concentrating interactions; 2) without LCM, L2ViT will degenerate heavily both on classification and object detection, this reveals that concentrating the attention map locally contributes to better recognization, especially on dense prediction tasks; 3) scale parameter has a slight effect on the performance, but it improves the training stability; and 4) we further ablate convolutional stem and apply the patchify stem as Swin, which we call primitive L2ViT. Meanwhile, we construct a new model named Enhanced Swin-T-V1 by replacing the relative position embedding with CPE for a fair comparison.\nObviously, primitive L2ViT yields slightly better accuracy than Enhanced Swin-T-V1 (+0.1%), suggesting that models utilizing linear attention can outperform those employing local window attention, even in the absence of a LCM. To further show the effectiveness of the enhanced global linear attention, we directly replace linear attention in L2ViT with window attention, resulting a model we refer to as Enhanced Swin-T-V2. The results clearly indicates that it still lags behind L2ViT by 0.6% in image classification and 0.7"}, {"title": "6. Conclusion", "content": "We present a new general-purpose vision transformer named L2ViT, composed of two effective self-attention mechanisms (LGA and LWA). The appealing LGA develops a highly effective enhanced linear attention to build global long-range contextual relationships in linear complexity. At the same time, LWA employs well-designed window attention to focus on fine-grained local information. Taken these representations together, L2ViT can better model the nature of our visual world and shows strong performance on various tasks, suggesting strong potential for widespread applications."}, {"title": "Appendix", "content": "We follow the training strategy in [36,37] and show the setting in Tab. 9. When fine-tuning the 22k pre-trained model on ImageNet-1k, we use the same fine-tuning strategy as Swin [36]. Specifically, we fine-tune the models for 30 epochs with a batch size of 1024, an initial learning rate of 1e-04, 5 epochs of linear warm-up, and a stochastic drop rate of 0.2. Particularly, we also apply 12\u00d712 window size in LWA when fine-tuning on ImageNet-1k with 384\u00d7 384 input. Furthermore, in all models, we clamp the denominator of Eq. (4) into the range [1e2, +\u221e). The learnable scale parameter s is initialized as VC."}, {"title": "7. Training Details", "content": "We follow the training strategy in [36,37] and show the setting in Tab. 9. When fine-tuning the 22k pre-trained model on ImageNet-1k, we use the same fine-tuning strategy as Swin [36]. Specifically, we fine-tune the models for 30 epochs with a batch size of 1024, an initial learning rate of 1e-04, 5 epochs of linear warm-up, and a stochastic drop rate of 0.2. Particularly, we also apply 12\u00d712 window size in LWA when fine-tuning on ImageNet-1k with 384\u00d7 384 input. Furthermore, in all models, we clamp the denominator of Eq. (4) into the range [1e2, +\u221e). The learnable scale parameter s is initialized as VC."}, {"title": "8. Implement Details for Clamping", "content": "To prevent dividing zeros, we clamp the denominator in Eq. (4) into the range [Cmin, +\u221e). Tab. 10 shows the influence of different lower-bound values Cmin. When using"}, {"title": "9. Additional Ablation Experiments", "content": "Linear Attention on Vanilla ViT Tab. 11 shows the results of applying enhanced linear attention on plain architecture, i.e., DeiT. To imitate the LWA (softmax attention) + LCA (linear attention) layout of L2ViT, we keep attention in half of the blocks in DeiT-T/S/B, i.e., the 1st, 3rd, 5th, 7th, 9th, 11th block, untouched. We observe that enhanced linear attention improves DeiT-Tiny by 0.6% accuracy, while enjoying some FLOPs reduction at a small scale. The above results show that enhanced linear attention can be generalized well to plain ViT architectures."}, {"title": "Linear Attention on Vanilla ViT", "content": "Tab. 11 shows the results of applying enhanced linear attention on plain architecture, i.e., DeiT. To imitate the LWA (softmax attention) + LCA (linear attention) layout of L2ViT, we keep attention in half of the blocks in DeiT-T/S/B, i.e., the 1st, 3rd, 5th, 7th, 9th, 11th block, untouched. We observe that enhanced linear attention improves DeiT-Tiny by 0.6% accuracy, while enjoying some FLOPs reduction at a small scale. The above results show that enhanced linear attention can be generalized well to plain ViT architectures."}, {"title": "Compared to Other Local Enhancements", "content": "Similar to our work, EfficientViT [3] proposes to insert a depth-wise convolution in the MLP layer to improve the locality of feature maps generated by linear attention layers. PVTv2 [60] also adds a 3\u00d73 depth-wise convolution to obtain more local continuity after linear spatial reduction attention. Whether LCM is advantageous in strengthening local details compared to simpler MLP with depth-wise convolution is an interesting point. It is worth noting that L2ViT utilizes a plain MLP.\nTo confirm this, we follow PVTv2 and add a separate 3\u00d73 depth-wise convolution in plain MLP as illustrated in Fig. 4 middle. The results are summarized in Tab. 12. However, just adding a depth-wise convolution in MLP of L2 ViT degrades the accuracy, which is also observed in Moat [67]. To address this issue, we add extra normalization and activation layers between 1\u00d71 convolutions, named as locally improved MLP in Fig. 4 right. Although locally improved MLP brings some improvement (0.2% gains), it still lags behind LCM. These demonstrate that our proposed design performs more effectively in enhancing local information for linear attention output."}, {"title": "10. Local Concentration Module", "content": "Here we provide a detailed implementation of the local concentration module (LCM) as shown in Fig. 5. All the source code and pre-trained models will be publicly available. Due to attention operation, we keep the feature map as F \u2208 RB\u00d7N\u00d7C, B is the batch size, throughout the network as Swin [36]. However, depth-wise convolution operation requires a different feature arrangement, so we add rearrange operations to deal with this. Then two depth-wise convolutional layers are adopted to strengthen the local spatial interactions and enhance the linear attention output."}, {"title": "11. Limitations", "content": "While our proposed local concentration module enhances the linear attention to a large extent, we notice that the dispersive attention in deeper layers like layer 12 in Figure 2. may not be compensated by convolution since they show global patterns instead of local patterns. We think that developing a specific concentration module for deep layers or considering applying vanilla attention directly will be an interesting future direction to improve our work.\nSome works [1, 14] explore channel attention building channel-to-channel interactions instead of patch-to-patch"}, {"title": "12. Model Configurations", "content": "Table 13 shows the detailed model configurations for L2ViT-Tiny/Small/Base. Unlike the non-overlapping patchify stem in Swin [36], we adopt a two-layer convolutional stem to extract more important local structure information for each patch. In ith stage, we alternatively arrange Ni LWA and Ni LGA, total 2N; blocks. In this way, LWA first models short-range interactions, then LGA constructs global patch-to-patch relationships. LGA can reinforce the holistic perception of features encoded by LWA to boost the expressivity of the model. Both LGA and LWA apply MLP (expansion ratio of 4), the same as the DeiT [52], to model channel relationships. Besides, Both LGA and LWA in all stages adopt CPE (kernel size 3\u00d73) as position embedding as CPE is more friendly to various input resolutions."}]}