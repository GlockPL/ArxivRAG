{"title": "GUME: Graphs and User Modalities Enhancement for Long-Tail Multimodal Recommendation", "authors": ["Guojiao Lin", "Zhen Meng", "Dongjie Wang", "Qingqing Long", "Yuanchun Zhou", "Meng Xiao"], "abstract": "Multimodal recommendation systems (MMRS) have received considerable attention from the research community due to their ability to jointly utilize information from user behavior and product images and text. Previous research has two main issues. First, many long-tail items in recommendation systems have limited interaction data, making it difficult to learn comprehensive and informative representations. However, past MMRS studies have overlooked this issue. Secondly, users' modality preferences are crucial to their behavior. However, previous research has primarily focused on learning item modality representations, while user modality representations have remained relatively simplistic. To address these challenges, we propose a novel Graphs and User Modalities Enhancement (GUME) for long-tail multimodal recommendation. Specifically, we first enhance the user-item graph using multimodal similarity between items. This improves the connectivity of long-tail items and helps them learn high-quality representations through graph propagation. Then, we construct two types of user modalities: explicit interaction features and extended interest features. By using the user modality enhancement strategy to maximize mutual information between these two features, we improve the generalization ability of user modality representations. Additionally, we design an alignment strategy for modality data to remove noise from both internal and external perspectives. Extensive experiments on four publicly available datasets demonstrate the effectiveness of our approach. The code and data are publicly accessible via GitHub.", "sections": [{"title": "1 INTRODUCTION", "content": "In the era of information explosion, recommendation systems [20] have become indispensable tools to help users discover relevant and interesting items from vast amounts of data. Among various recommendation methods, those that focus on capturing collaborative signals from user-item interactions have received significant attention. However, these methods often suffer from the problem of data sparsity. Due to the severe scarcity of interaction data, where 80% of interactions are focused on popular items, responses for tail items are significantly limited. Over time, this not only exacerbates the cold start problem but also traps users in information cocoons. Multimodal recommendation systems (MMRS), capable of integrating substantial information across different item modalities, show potential in mitigating this problem and have consequently garnered considerable interest within the research community. By leveraging multimodal data, including text and images, MMRS can achieve a deeper insight into the features of the items and user preferences [3, 4, 26], thereby enhancing the precision and variety of their recommendations.\nSeveral studies have integrated multimodal content into recommendation systems. For instance, VBPR [7] improves item representation by merging visual embeddings with ID embeddings. The application of Graph Convolutional Networks (GCN) to uncover hidden information among users and items has also received increased focus. In addition, MMGCN [28] develops modality-specific bipartite graphs for users and items and combines various modality features for prediction. LATTICE [43] and MICRO [44] generate multi-view semantic graphs based on multi-modal data and then merge these graphs to identify potential item relationship graphs. MENTOR [34] constructs static item homogeneity graphs for each modality to strengthen semantic relationships between items. Nevertheless, these approaches do not address the use of multimodal information to improve the connectivity of tail items in the user-item interaction graph. Due to the sparse interaction data, the tail items receive insufficient information during the graph propagation phase, hindering their ability to develop comprehensive and informative representations. Thus, it is crucial to take advantage of the multimodal information to improve the connectivity of the graph, which in turn helps mitigate the cold start issue for long-tail items.\nAlthough item modality information is rich, user modality representation still has much to explore. For example, BM3 [49] only learns user ID representations and ignores user modality representations. SLMRec [21] and MGCN [42] represent the modality features of the user simply by aggregating the modality features of the items with which the user has interacted. MENTOR customizes user modality embeddings, combines them with item modality embeddings, and updates them through GCN propagation. However, using simple aggregation or customized methods to represent user modalities cannot effectively capture user modality preferences."}, {"title": "2 PROBLEM DEFINITION", "content": "Let U = {u} denote the user set and I = {i} denote the item set. The ID embeddings for users and items are represented as Eid = {Eu,id||Ei,id} \u2208 []\u00aed\u00d7(|U|+|I|), where d is the embedding dimension. The modality embeddings for users are Eu,m \u2208 Rdx|U|,\nand the modality features for items are Ei,m \u2208 Rdm\u00d7|I|. Here,\nme M is the modality, M = {v, t} is the set of modalities, and dm is the dimension of the features. In this paper, we primarily consider visual and textual modalities, but our method can also be extended to additional modalities.\nThe matrix R \u2208 {0, 1}|U|\u00d7|I| represents user-item interactions, where Ru,i = 1 indicates that user u clicked on item i, and Ru,i = 0 otherwise. Based on the interaction matrix R, we construct a bipartite graph G = (V, &), where V = UUI represents the set of nodes, and & = {(u, i) | \u0438 \u2208 U, \u0456 \u0454 I, Ru,i = 1} represents the set of edges. The primary goal of multimodal recommendation systems is to recommend the top-N most relevant items to each user based on the predicted preference score Yui = fo(eid, eu,m, li,m), where represents the model parameters."}, {"title": "3 METHODOLOGY", "content": "3.1 Enhancing User-Item Graph\nInspired by the successful application of graph enhancement learning in long-tail recommendation tasks [17], we have introduced this approach to multimodal recommendation. Specifically, we first construct modality item graphs based on multimodal similarities between items. Subsequently, we select items that are similar to the target item in both textual and visual attributes as its semantic neighbors. Finally, we add the edges between the target items and their semantic neighbors to the user-item graph. This strategy helps improve the connectivity of tail items.\n3.1.1 Constructing Modality Item Graphs. We employ the KNN algorithm [2] to construct an item-item graph for each modality m, aimed at extracting multimodal relationships between items. Specifically, we calculate the similarity score Sm for the item pair (i, j) \u2208 I by measuring the cosine similarity between their original modality features, em and em."}, {"title": "3.1.2 Identifying Semantic Neighbors", "content": "According to [12], in recommendation systems based on Graph Convolutional Networks (GCN), the indiscriminate use of high-order nodes can introduce negative information during the embedding propagation process. This is particularly problematic when the model stacks more layers, as it can lead to performance degradation. Therefore, indiscriminate graph augmentation is inadvisable, as it may lead to the transmission of irrelevant information between items. It is important to identify semantic neighbors that are not only similar in features to the target item but are also likely to have meaningful connections. To address this issue, we introduce a strategy based on multi-modal similarity to identify semantic neighbors. Specifically, this is implemented by utilizing the modality item graph. This graph keeps only the top-k neighbors with the highest similarity scores for each item, and we use it to identify items that are similar to the target item across multiple modalities (textual and visual). We then define these items as the semantic neighbors of the target item. The set of items and their semantic neighbors can be expressed as:"}, {"title": "3.2 Encoding Multiple Modalities", "content": "According to [34, 42], user-item graphs and modality item graphs contain rich collaborative and semantic signals that can significantly enhance the performance of multimodal recommendation systems. Inspired by them, we leverage modality item graphs to extract explicit interaction features, while also using enhanced user-item graphs to extract extended interest features. These features represent the user's historical modality preferences and potential future modality preferences, respectively."}, {"title": "3.2.1 Extracting Explicit Interaction Features", "content": "We first design a feature space transformation function that maps the initial modality features to the same space as the ID features:"}, {"title": "3.3 Attributes Separation for Better Integration", "content": "Multiple modalities convey comprehensive information [1]. For example, images and text of clothing items can both reflect the coarse-grained attribute [29, 30, 35] of the category. However, images can provide fine-grained attributes specific to visuals, such as patterns and styles, while text can provide fine-grained attributes specific to text, such as fabric and size. The coarse-grained attributes represent commonalities among items, whereas fine-grained attributes are key factors influencing users purchasing decisions. Since the explicit interaction features are only enhanced through the modality item graph and represent user modality by simple aggregation, they lack information related to user behavior. Therefore, we first separate the coarse-grained attributes in the explicit interaction features, and then use user behavior information to reveal the fine-grained attributes within it."}, {"title": "3.3.1 Separating Coarse-Grained Attributes", "content": "We design a multilayer perceptron (MLP) to process the input vector X, apply a nonlinear transformation to X, and then output it as a weight matrix:"}, {"title": "3.3.2 Separating Fine-Grained Attributes", "content": "We designed a modality-specific behavior information extraction function fm (X) as follows:"}, {"title": "3.4 Capturing Commonalities Through Alignment", "content": "In recommendation scenarios, users may be attracted to common features between the images and text of products, while there are also potential correlations between behavioral features and modal features. To capture the commonalities between different modalities, we designed two alignment tasks from different perspectives.\nSpecifically, we first capture common information from an internal perspective. Inspired by [34], we parameterize the visual modality Ev and the textual modality Et using Gaussian distributions."}, {"title": "3.5 Enhancing User Modality Representation", "content": "Eu,M directly reflects the user's historical interactions, clearly expressing the user's past preferences. However, this is also its limitation, as it lacks foresight into the user's potential interests. In contrast, Eu,M not only includes the user's historical modality information but also expands their interests by considering items similar to their historical items. This extension takes into account that users might be interested in items similar to those they have interacted with before, helping to foresee their potential interests. Based on this idea, we use InfoNCE to maximize the mutual information between \u0112u,M and \u00cau,M, enabling \u0112u,M to absorb and integrate the information provided by \u00cau,M. This strategy enhances the generalization ability of Eu, M, enabling it to effectively adapt to new products or changes in user behavior even in the absence of direct interaction data. The mathematical expression of this task is as follows:"}, {"title": "3.6 Model Prediction", "content": "Based on the ID features and enhanced explicit interaction features, we can get the general representations of users and items:"}, {"title": "3.7 Optimization", "content": "To optimize the effectiveness of our recommendations, we employ the Bayesian Personalized Ranking (BPR) loss [19] as our primary optimization function. This approach assumes that the predicted scores for observed user-item pairs should be higher than those for unobserved pairs. The BPR function is defined as follow:"}, {"title": "4 EXPERIMENTS", "content": "In this section, we construct experiments to demonstrate the advantages of the proposed methods and to address the following research questions: RQ1: How effective is our GUME compared to the state-of-the-art multimodal recommendation methods and traditional recommendation methods? RQ2: How do the key modules impact the performance of our GUME? RQ3: Does the proposed graph augmentation strategy improve the recommendation performance for tail items? RQ4: Why is the user modality enhancement modules effective? RQ5: How do different hyper-parameter settings impact the performance of our GUME?"}, {"title": "4.1 Experimental Setting", "content": "4.1.1 Dataset. We adhere to the dataset selection criteria utilized in several recent studies [42, 49, 41], ensuring consistency and comparability of results. Specifically, we conduct extensive experiments on four categories of the Amazon dataset : (1) Baby, (2) Sports and Outdoors (referred to as Sports), (3) Clothing, Shoes, and Jewelry (referred to as Clothing), and (4) Electronics. These categories offer a diverse range of items, from basic baby care products to high-tech electronic devices. For each modality feature extraction, we follow the same setting mentioned in [47], which extracted 4096 dimensions of visual features and 384 dimensions of textual features via pretrained encoder. The item interaction counts in all datasets exhibit a long-tail distribution, which also makes these datasets"}, {"title": "4.1.2 Compared Methods", "content": "We compare GUME with 9 baselines, including 2 traditional recommendation models and 7 Multimodal recommendation models. The details of those compared methods are listed as follow:\n(a) Traditional Recommendation Models:\nMF-BPR[19]: This method optimizes recommendation systems based on matrix factorization techniques by incorporating Bayesian Personalized Ranking (BPR) loss.\nLightGCN[8]: This method simplifies the design of GCN, retaining only the neighborhood aggregation suitable for collaborative filtering.\n(b) Multimodal Recommendation Models:\nVBPR[7]: This method extends the MF-BPR method by integrating the visual features and ID embeddings of each item as its representation, and inputs these into the matrix factorization framework.\nMMGCN [28]: This model constructs specific graphs for different modalities and learns the features of users and items through these graphs. It then concatenates all modal features for prediction.\nSLMRec[21]: This model incorporates SSL (Self-Supervised Learning) into graph neural network-based recommendation models and proposes three data augmentation methods. It aims to uncover latent patterns within the multimodal information.\nLATTICE [43]: This model learns the item-item structure for each modality and aggregates them to form a semantic item-item graph, in order to obtain better item representations.\nBM3[49]: This model simplifies the self-supervised multimodal recommendation model by adopting a latent representation dropout mechanism in place of graph augmentation for generating contrastive views.\nMGCN [42]: This model purifies modal features with the help of item behavior information, reducing modal noise contamination, and models modal preferences based on user behavior.\nMENTOR[34]: This model employs aligned self-supervised tasks to synchronize multiple modalities while preserving interaction information. It enhances features through feature masking tasks and graph perturbation tasks."}, {"title": "4.1.3 Evaluation Metrics", "content": "To ensure a fair evaluation of performance, we utilize two widely adopted metrics: Recall@K (R@K) and NDCG@K (N@K). Following the popular evaluation setup [42], we employ a random data split of 8:1:1 for training, validation, and testing phases. We assess the recommendation performance of various methods by reporting the average metrics for all users in the test set under top-K conditions, with K empirically set at 10 and 20."}, {"title": "4.1.4 Implementation Details", "content": "We implemented our proposed GUME and all baseline models using MMRec [47]. To ensure fair comparisons, we fixed the embedding sizes for users and items at 64, initialized embedding parameters using the Xavier method [5], and used Adam [10] as the optimizer with a learning rate of 1e-3. For our proposed GUME, we performed a grid search on hyper-parameters a, \u03b2, and y in {1e-4, 1e-3, 1-2, 1e-1, 1e}, and temperature hyper-parameters \u03c4 in {0.1, 0.2, 0.4, 0.6, 0.8, 1.0}. The GCN layer in the User-Item graph was fixed at 3, while the layer in the Item-Item graph was fixed at 1 (except baby is 2). The k for top-k in the Item-Item graph was set at 10. Early stopping and total epochs were fixed at 20 and 1000, respectively. Following [42], we used Recall@20 on the validation data as the training-stopping criterion."}, {"title": "4.2 Overall Performance (RQ1)", "content": "Table 2 shows the performance comparison of the proposed GUME and other baseline methods on four datasets. The table reveals the following observations:\nOur GUME model achieved excellent performance across multiple metrics, surpassing traditional recommendation models and multimodal recommendation models. Specifically, in terms of Recall@20 for Sports, Clothing, and Electronics, GUME outperforms the best baseline by 2.28%, 3.54%, and 3.82% respectively; while in terms of NDCG@20, it shows improvements of 3.13%, 5.67%, and 2.65%. On the Baby dataset, GUME ties with the best baseline in Recall@20 and improves by 2.22% over the best baseline in NDCG@20. The results validate the effectiveness of our GUME.\nUsing multimodal information of items to enhance graph connectivity can improve recommendation performance. For example, LATTICE dynamically learns the latent structure between items based on the similarity of their multimodal features. However, dynamically generating latent structure is unnecessary and memory-intensive, which is not efficient for computation. This was confirmed by the subsequent work FREEDOM [48]. Inspired by FREEDOM, MENTOR constructs item homogeneous graphs for each modality to enhance semantic relationships between items and then freezes them. Our GUME first identifies semantic neighbors of items based on multimodal similarity and then enhances the user-item interaction graph with them, especially by adding more edges for tail items. This promotes the exploration of items that users might be interested in and alleviates the cold start problem for tail items.\nAligning multiple types of features can improve recommendation performance. For example, BM3 aligns modality features from both intra-modality and inter-modality perspectives. MGCN designs a self-supervised auxiliary task to promote the exploration between behaviors and multimodal information. MENTOR designed a multi-level cross-modal alignment task, aligning each modality under the guidance of ID embeddings while maintaining historical interaction information. Our GUME not only aligns external information such as behavioral features and multimodal features but also aligns internal information such as image features and text features, promoting the learning of user and item representations from multiple perspectives.\nThe multimodal representation of users has always been relatively vague. Some methods, such as BM3, only learn user ID representations and ignore users' multimodal representations. This is a neglect of users' multimodal preferences. Other methods represent users' multimodal features by aggregating the multimodal features of items. For example, SLMRec and MGCN obtain users' multimodal features by simply aggregating the multimodal features of interacted items. This approach treats multimodal features of purchased items equally, which is not effective for fully capturing users' multimodal preferences. Building on this, our GUME constructs extended interest features for users. By using contrastive learning with users' explicit interaction features, we maximize the mutual information between them. This process helps to refine and expand users' multimodal preferences effectively."}, {"title": "4.3 Ablation Study (RQ2)", "content": "In our work, GUME comprises the modules Graph Enhancement, Alignment for Capturing Commonalities and User Modality Augment. To thoroughly examine the impact of these modules, we conduct comprehensive ablation studies. We use \"w/o XX\" to denote the absence of a specific module, meaning \"without XX\"."}, {"title": "4.3.1 The influence of Modules.", "content": "w/o GE: We remove the graph enhancement module. The model's average performance declines without graph enhancement, indicating that our graph enhancement module can effectively improve overall recommendation performance. We will further demonstrate the impact of graph enhancement on improving the recommendation performance for tail items in Section 4.4.\nw/o AL: We remove the alignment for capturing commonalities module. On four datasets, the performance of the model without this module is significantly lower than that of GUME. This indicates that common information within modalities and between modalities and external behaviors is crucial for recommendation performance. Without the alignment process, the modality data of items might contain noise unrelated to the items themselves and a lot of noise unrelated to user behavior. Through alignment, we capture commonalities relevant to the recommendation scenario within modalities and between modalities and external behaviors, thereby achieving a denoising effect. This demonstrates the effectiveness of the alignment module.\nw/o UM: We remove the user modality enhancement module. User modality enhancement is equally crucial for model performance, as the model's performance significantly declines without this component. Although it is possible to model the user's multimodal features by aggregating the multimodal features of the items the user has interacted with, this method has limitations. It only reflects the user's current multimodal preferences and lacks a comprehensive understanding of the user's latent interests and preferences. In contrast, leveraging contrastive learning between extended interest embeddings and explicit interaction embeddings helps to more comprehensively learn the user's multimodal preferences, thereby enhancing recommendation performance. We will further demonstrate the effectiveness of user modality augmentation through visualization in Section 4.5."}, {"title": "4.4 Comparisons on Tail Items Performance (RQ3)", "content": "To validate whether enhancing the user-item graph based on multimodal similarity can improve the recommendation performance for tail items, we conducted experiments on the Clothing dataset. Specifically, we divided items into five equally sized groups according to the node degree in the user-item bipartite graph, as shown in Figure 2. In recommendation systems, 20% of items account for 80% of interactions. Therefore, we define the top 1/5 of items as head items, while the remaining 4/5 are defined as tail items. The larger the x-axis value, the lower the node degree, and the less popular the item. We compared the performance of GUME, w/o GE, and MENTOR. The results show that graph enhancement can improve the recommendation performance for tail items. Although removing graph enhancement can improve the recommendation performance for head items, the overall performance decreases due to the decline in tail item performance, which is consistent with the findings of GALORE [17]. Additionally, GUME outperforms MENTOR for both head and tail items, indicating that our graph enhancement strategy effectively improves recommendation performance for long-tail distribution data."}, {"title": "4.5 Visualization Analysis (RQ4)", "content": "To further validate the effectiveness of the user modality enhancement component, we visualize the distribution of user modality representations within the Sports dataset. We compare two models, w/o UM and GUME, as mentioned in section 4.3.1. Specifically, we randomly select 1000 user instances from the Sports dataset and employ t-SNE [22] to map the user modality representations to two-dimensional space. The results, illustrated in figure 3, show that the user modality distribution of GUME is more uniform, while the distribution of w/o UM is more dispersed. Previous research [24] has demonstrated that the uniformity of representation significantly influences recommendation performance. This explains why GUME is effective in enhancing user modality representation."}, {"title": "4.6 Hyperparameter Sensitivity Study (RQ5)", "content": "4.6.1 The balancing hyper-parameter a. Figure 4 shows the variation of the balance hyperparameter a in the visual-textual alignment task within the range of {1e-4, 1e-3, 1e-2, 1e-1, 1e0}. Observations indicate that as a increases, performance initially improves and then declines. This suggests that jointly optimizing the visual-textual alignment task with the primary recommendation task can enhance performance, but if a is too large, the model may be misled by the visual-textual alignment task. Overall, there is no significant sharp rise or fall, indicating that our method is relatively insensitive to the choice of a."}, {"title": "4.6.2 The pair of hyper-parameters \u03b2 and t", "content": "The behavior-modality alignment task is jointly controlled by the balance hyper-parameter \u03b2 and the temperature hyper-parameter \u03c4. We adjust \u03b2 within the range of {1e-4, 1e-3, 1e-2, 1e-1, 1e0}, and t within the range of {0.1, 0.2, 0.4, 0.6, 0.8, 1.0}. As shown in Figure 5, the model performs best on the sports, clothing, and electronics datasets when \u03b2 is set to 0.01 and t is set to 0.2. On the Baby dataset, the best performance occurs when \u03b2 is 0.01 and t is 0.4."}, {"title": "4.6.3 The pair of hyper-parameters \u03b3 and \u03c4", "content": "The user modality enhancement task is jointly controlled by the balance hyper-parameter \u03b3 and the temperature hyper-parameter \u03c4. We adjust \u03b3 within the range of {1e-4, 1e-3, 1e\u22122, 1e-1, 1e\u00ba} and t within the range of {0.1, 0.2, 0.4, 0.6, 0.8, 1.0}. As shown in Figure 6, considering all metrics comprehensively, the model performs best on the smaller datasets (baby and sports) when \u03b3 is set to 0.01 and t is set to 0.1. On the larger datasets (clothing and electronics), the best performance occurs when \u03b3 is set to 0.1 and t is set to 0.2."}, {"title": "5 RELATED WORK", "content": "5.1 Multimedia Recommendation\nRecently, using multimodal information for recommendations has become a popular method to alleviate the data sparsity issue inherent in traditional recommendation systems. Typically, multimodal recommendations involve extracting features from multiple modalities using pre-trained neural networks, which are then integrated with behavioral features to better model user preferences. For example, VBPR [7] enriches item representations by concatenating visual embeddings with ID embeddings. However, these item representations can be contaminated by features in the modal information that are irrelevant to user preferences. Inspired by graph convolutional networks, MMGCN [28] leverages GCN to construct several modality-specific bipartite graphs of users and items, thereby uncovering hidden modality preferences within user-item interactions. GRCN [27] reduces noise in the user-item bipartite graph by identifying and eliminating incorrect interaction data between users and items. However, these methods assign the same weight to each modality, overlooking the variations in user preferences for different modalities. To address this issue, DualGNN [25] constructs a user co-occurrence graph and aggregates neighbor modality information based on this graph. MGCN [42] extracts modality preferences from user behavioral features and assigns modality weights based on these preferences. Additionally, utilizing item-item graphs can help achieve better item representations. LATTICE [43] and MICRO [44] create item modality semantic graphs based on item modality information and aggregate them to capture latent item graphs. Self-supervised learning (SSL) has demonstrated remarkable performance in mitigating label dependency and addressing data sparsity issues. For instance, BM3 [49] does not introduce any auxiliary graphs but utilizes dropout techniques to generate contrastive views, thereby saving on model memory and computational costs. MENTOR [34] employs aligned self-supervised tasks to synchronize multiple modalities while preserving interaction information."}, {"title": "5.2 Long-tail Learning", "content": "In the development of recommendation systems, the rapid increase in the number of items often exacerbates the long-tail effect, which in turn leads to cold start problems and a decline in recommendation quality [36, 23]. To address this challenge, [36] explored the application of random walk techniques on user-item graphs to find items that align with user interests, thereby addressing long-tail recommendations. DropoutNet [23] uses dropout techniques to train models, allowing it to make effective recommendation predictions based on content information when there is a lack of historical interaction data for users or items. Meta-learning focuses on using small amounts of training data to improve classification or regression performance, aligning well with the goals of recommendation systems and becoming a key solution for addressing long-tail recommendations. For example, MeLU [11] introduces a model-agnostic meta-learning algorithm (MAML) into recommendation systems to alleviate the cold start problem for users. Additionally, researchers have explored ways to enhance the recommendation performance of tail items. For example, TailNet [13] introduces a preference mechanism that adjusts recommendation bias towards head or tail items by learning session representations and generating correction factors. MIRec [46] proposed a dual transfer learning strategy that facilitates knowledge transfer from head to tail items at both the model and item levels. GALORE [17] enhances tail item recommendations by using graph augmentation techniques, specifically by increasing the edge connections between head and tail items and selectively reducing the edge connections between users and head items."}, {"title": "6 CONCLUSION", "content": "In this paper, we propose a novel Graphs and User Modalities Enhancement framework, named GUME, for long-tail multimodal recommendation. GUME enhances the user-item graph using multimodal similarities between items, improving the recommendation effectiveness for long-tail items. To effectively improve the generalization ability of user modality representations, GUME learns two independent representations: explicit interaction features and extended interest features. By maximizing the mutual information between these two features, the learned user modality features better adapt to changes in new products or user behavior. We also design two alignment tasks to denoise modality data from different perspectives. Experimental results on several widely-used datasets show that GUME significantly outperforms state-of-the-art multimodal recommendation methods."}, {"title": "7 ACKOWLEDGEMENT", "content": "This study is supported by grants from the Strategic Priority Research Program of the Chinese Academy of Sciences XDB38030300, the Postdoctoral Fellowship Program of CPSF (No.GZC20232736), the China Postdoctoral Science Foundation Funded Project (No.2023M743565), the Special Research Assistant Funded Project of the Chinese Academy of Sciences."}]}