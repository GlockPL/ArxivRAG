{"title": "GUME: Graphs and User Modalities Enhancement for Long-Tail Multimodal Recommendation", "authors": ["Guojiao Lin", "Zhen Meng", "Dongjie Wang", "Qingqing Long", "Yuanchun Zhou", "Meng Xiao"], "abstract": "Multimodal recommendation systems (MMRS) have received considerable attention from the research community due to their ability to jointly utilize information from user behavior and product images and text. Previous research has two main issues. First, many long-tail items in recommendation systems have limited interaction data, making it difficult to learn comprehensive and informative representations. However, past MMRS studies have overlooked this issue. Secondly, users' modality preferences are crucial to their behavior. However, previous research has primarily focused on learning item modality representations, while user modality representations have remained relatively simplistic. To address these challenges, we propose a novel Graphs and User Modalities Enhancement (GUME) for long-tail multimodal recommendation. Specifically, we first enhance the user-item graph using multimodal similarity between items. This improves the connectivity of long-tail items and helps them learn high-quality representations through graph propagation. Then, we construct two types of user modalities: explicit interaction features and extended interest features. By using the user modality enhancement strategy to maximize mutual information between these two features, we improve the generalization ability of user modality representations. Additionally, we design an alignment strategy for modality data to remove noise from both internal and external perspectives. Extensive experiments on four publicly available datasets demonstrate the effectiveness of our approach. The code and data are publicly accessible via GitHub.", "sections": [{"title": "1 INTRODUCTION", "content": "In the era of information explosion, recommendation systems [20] have become indispensable tools to help users discover relevant and interesting items from vast amounts of data. Among various recommendation methods, those that focus on capturing collaborative signals from user-item interactions have received significant attention. However, these methods often suffer from the problem of data sparsity. Due to the severe scarcity of interaction data, where 80% of interactions are focused on popular items, responses for tail items are significantly limited. Over time, this not only exacerbates the cold start problem but also traps users in information cocoons. Multimodal recommendation systems (MMRS), capable of integrating substantial information across different item modalities, show potential in mitigating this problem and have consequently garnered considerable interest within the research community. By leveraging multimodal data, including text and images, MMRS can achieve a deeper insight into the features of the items and user preferences [3, 4, 26], thereby enhancing the precision and variety of their recommendations.\nSeveral studies have integrated multimodal content into recommendation systems. For instance, VBPR [7] improves item representation by merging visual embeddings with ID embeddings. The application of Graph Convolutional Networks (GCN) to uncover hidden information among users and items has also received increased focus. In addition, MMGCN [28] develops modality-specific bipartite graphs for users and items and combines various modality features for prediction. LATTICE [43] and MICRO [44] generate multi-view semantic graphs based on multi-modal data and then merge these graphs to identify potential item relationship graphs. MENTOR [34] constructs static item homogeneity graphs for each modality to strengthen semantic relationships between items. Nevertheless, these approaches do not address the use of multimodal information to improve the connectivity of tail items in the user-item interaction graph. Due to the sparse interaction data, the tail items receive insufficient information during the graph propagation phase, hindering their ability to develop comprehensive and informative representations. Thus, it is crucial to take advantage of the multimodal information to improve the connectivity of the graph, which in turn helps mitigate the cold start issue for long-tail items.\nAlthough item modality information is rich, user modality representation still has much to explore. For example, BM3 [49] only learns user ID representations and ignores user modality representations. SLMRec [21] and MGCN [42] represent the modality features of the user simply by aggregating the modality features of the items with which the user has interacted. MENTOR customizes user modality embeddings, combines them with item modality embeddings, and updates them through GCN propagation. However, using simple aggregation or customized methods to represent user modalities cannot effectively capture user modality preferences.\nThe simple aggregation method limits user modality preferences to past behavior, while customized methods ignore past preferences and can add noise during propagation. Therefore, it is necessary to further explore more effective methods to capture user preference for the user modality.\nTo address the above issues, we propose a novel Graphs and User Modalities Enhancement (GUME) for long-tail multimodal recommendation. Firstly, we construct modality item graphs based on multimodal similarity and identify the semantic neighbors of items. Then, we add edges between these items and their semantic neighbors to the user-item interaction graph to enhance graph connectivity [16, 14, 15]. Next, based on the modality item graphs and the enhanced user-item graph, we extract explicit interaction features [37, 6, 33] and extended interest features [39, 38, 9, 31, 32], representing the user's historical modality preferences and potential future modality preferences, respectively. By leveraging common information between modalities, we separate coarse-grained attributes from explicit interaction features. Since fine-grained attributes are related to user behavior, we further use behavior information to reveal fine-grained attributes within the explicit interaction features [45, 18]. The separation of these attributes is aimed at better aggregating them, so we reaggregate the coarse-grained and fine-grained attributes to form enhanced explicit-interaction features. Then, we design a user-modality enhancement module that maximizes the mutual information between explicit interaction features and extended interest features, improving the generalizability of user modality representations. Additionally, we design an alignment module to capture commonalities within internal information (e.g., image and text) and external information (e.g., behavior and modality), thereby removing noise unrelated to recommendations. The main contributions of this paper are summarized as follows:\n\u2022 We propose a strategy to enhance user-item graphs based on multimodal similarity, improving the connectivity of tail items.\n\u2022 We develop a user modality enhancement strategy that improves the generalization ability of user modality representations, enabling them to effectively adapt to new products or changes in user behavior, even without direct interaction data.\n\u2022 We design an alignment strategy from internal and external perspectives to capture commonalities within modalities as well as between modalities and external behaviors, thereby achieving a denoising effect.\n\u2022 We conduct comprehensive experiments on four public Amazon datasets to demonstrate the unique advantages of our GUME."}, {"title": "2 PROBLEM DEFINITION", "content": "Let U = {u} denote the user set and I = {i} denote the item set. The ID embeddings for users and items are represented as $E_{id}$ = {$E_{u,id}$||$E_{i,id}$ } \u2208 $\\R^{d\u00d7(|U|+|I|)}$, where d is the embedding dimension. The modality embeddings for users are $E_{u,m}$ \u2208 $\\R^{dx|U|}$, and the modality features for items are $E_{i,m}$ \u2208 $\\R^{d_m\u00d7|I|}$. Here, me M is the modality, M = {v, t} is the set of modalities, and $d_m$ is the dimension of the features. In this paper, we primarily consider visual and textual modalities, but our method can also be extended to additional modalities.\nThe matrix R \u2208 {0, 1}$^{|U|\u00d7|I|}$ represents user-item interactions, where $R_{u,i}$ = 1 indicates that user u clicked on item i, and $R_{u,i}$ =\n0 otherwise. Based on the interaction matrix R, we construct a\nbipartite graph G = (V, &), where V = UU I represents the\nset of nodes, and & = {(u, i) | u \u2208 U, i \u2208 I, $R_{u,i}$ = 1} represents\nthe set of edges. The primary goal of multimodal recommendation\nsystems is to recommend the top-N most relevant items to each user\nbased on the predicted preference score $\\Upsilon_{u,i}$ = $f_{\\Theta}$($e_{id}, e_{u,m}, e_{i,m}$),\nwhere \u0398 represents the model parameters."}, {"title": "3 METHODOLOGY", "content": "Inspired by the successful application of graph enhancement learning in long-tail recommendation tasks [17], we have introduced this approach to multimodal recommendation. Specifically, we first construct modality item graphs based on multimodal similarities between items. Subsequently, we select items that are similar to the target item in both textual and visual attributes as its semantic neighbors. Finally, we add the edges between the target items and their semantic neighbors to the user-item graph. This strategy helps improve the connectivity of tail items."}, {"title": "3.1.1 Constructing Modality Item Graphs", "content": "We employ the KNN algorithm [2] to construct an item-item graph for each modality m, aimed at extracting multimodal relationships between items. Specifically, we calculate the similarity score $S_m$ for the item pair (i, j) \u2208 I by measuring the cosine similarity between their original modality features, $e_i^m$ and $e_j^m$.\n\n$S_{i,j}^m = \\frac{(e_i^m)^T e_j^m}{||e_i^m|| ||e_j^m||}$ (1)\n\nWe retain only the top-k neighbors with the highest similarity scores to capture the most relevant features:\n\n$S_{i,j}^m = \\begin{cases}\n(S_{i,j}')^m, S_{i,j}^m \u2208 Top-K({S_{i,p}^m, p \u2208 I}),\\\\\n0, otherwise,\n\\end{cases}$ (2)\n\nwhere $S_{i,j}^m$ represents the edge weight between item i and item j within modality m, and {$S_{i,p}^m, p \u2208 I$} represents the neighbor scores for the i-th item. To mitigate the issues of gradient explosion or vanishing, we normalize the adjacency matrix as follows:\n\n$\\S^m = (D^m)^{-\\frac{1}{2}} S^m (D^m)^{-\\frac{1}{2}}$ (3)\n\nwhere $D^m$ is the diagonal degree matrix of $S^m$. Inspired by [48], we freeze each modality item graph after initialization."}, {"title": "3.1.2 Identifying Semantic Neighbors", "content": "According to [12], in recommendation systems based on Graph Convolutional Networks (GCN), the indiscriminate use of high-order nodes can introduce negative information during the embedding propagation process. This is particularly problematic when the model stacks more layers, as it can lead to performance degradation. Therefore, indiscriminate graph augmentation is inadvisable, as it may lead to the transmission of irrelevant information between items. It is important to identify semantic neighbors that are not only similar in features to the target item but are also likely to have meaningful connections. To address this issue, we introduce a strategy based on multi-modal similarity to identify semantic neighbors. Specifically, this is implemented by utilizing the modality item graph. This graph keeps only the top-k neighbors with the highest similarity scores for each item, and we use it to identify items that are similar to the target item across multiple modalities (textual and visual). We then define these items as the semantic neighbors of the target item. The set of items and their semantic neighbors can be expressed as:\n\n$\\mathcal{C} = \\cup_{i\u2208\\mathcal{I}} \\{(i, j) | j \u2208 \\mathcal{N}_i\\}$ (4)\n\nwhere, $N_i$ represents the semantic neighbors of item i. Then, we enhance the user-item graph by adding edges between items and their semantic neighbors. Formally,\n\n$A = \\begin{pmatrix} 0 & R \\\\ R^T & C \\end{pmatrix}$ (5)\n\nwhere 0 is the all-zero matrix, and R is the user-item interaction matrix."}, {"title": "3.2 Encoding Multiple Modalities", "content": "According to [34, 42], user-item graphs and modality item graphs contain rich collaborative and semantic signals that can significantly enhance the performance of multimodal recommendation systems. Inspired by them, we leverage modality item graphs to extract explicit interaction features, while also using enhanced user-item graphs to extract extended interest features. These features represent the user's historical modality preferences and potential future modality preferences, respectively."}, {"title": "3.2.1 Extracting Explicit Interaction Features", "content": "We first design a feature space transformation function that maps the initial modality features to the same space as the ID features:\n\n$f_m(X) = \u03c3(W_2(W_1X + b_1) + b_2)$ (6)\n\nwhere $W_1 \u2208 \\R^{dxd_m}$ and $W_2 \u2208 \\R^{d\u00d7d}$ denote the linear transformation matrixs, $b_1 \u2208 \\R^{d_m}$ and $b_2 \u2208 \\R^{d}$ denote the bias vectors and \u03c3 is the sigmoid function. These parameters are modality-specific.\nThen, inspired by [42], we perform an element-wise product between the transformed modality features and the ID features to remove noise unrelated to behavior from the modality features.\n\n$\\hat{E}_{i,m} = E_{i,id} \\circledcirc f_m(E_{i,m})$ (7)\n\nwhere $\\circledcirc$ represents the element-wise product.\nAfter obtaining the denoised modality features, we perform graph convolution operations on the modality item graphs to propagate and update the item modality features.\n\n$E_{i.m} = \\S^m\\hat{E}_{i,m}$ (8)\n\nNext, we aggregate the modality features of the user's neighboring items to represent the user's explicit modality features.\n\n$E_{u,m} = RE_{i,m}$ (9)\n\nWhere R \u2208 {0, 1}$^{|U|\u00d7|I|}$ represents the user-item interaction matrix.\nThe ultimate explicit interaction features are derived by combining the user's explicit modality feature $E_{u,m}$ with the item modality features $E_{i,m}$. Formally,\n\n$E_m = \\{E_{u,m}||E_{i,m}\\}$ (10)\n\nwhere || denotes the concatenation operation."}, {"title": "3.2.2 Extracting Extended Interest Features", "content": "Let $\\hat{E}_{u,m}$ and $\\hat{E}_{i,m}$ represent the stacked extended interest embeddings for users and items, respectively. We propagate the specific modality embeddings on the enhanced user-item graph to extract modality-specific extended interest embeddings. The same applies to ID embeddings. Specifically, at the lth layer of graph convolution, the embeddings of users and items can be represented as:\n\n$\\hat{E}^{(l)}_{u,i} = (\\hat{D} \\hat{A} \\hat{D}) \\hat{E}^{(l-1)}_{u,i}$ (11)\n\nWhere $\\hat{E}^m$ represent the specific modality embeddings for users and items at the 1-th layer of graph convolution, D is the diagonal degree matrix of A. $\\hat{E}^M$ is the concatenation of $E^m_{u,i}$ and $E^m_{i,m}$ By aggregating multi-layer neighbor information, we obtain the final modality-specific extended interest features.\n\n$\\hat{E}^m = \\frac{1}{L+2} \\sum_{l=0}^{L+1} \\hat{E}^{(l)}_m$ (12)\n\nwhere L is the number of user-item graph layers. Finally, we fuse the extended interest features from the visual and textual modalities to obtain the final extended interest features:\n\n$\\hat{E}^M = \\sum_{m\u2208M} \\hat{E}_m$ (13)"}, {"title": "3.3 Attributes Separation for Better Integration", "content": "Multiple modalities convey comprehensive information [1]. For example, images and text of clothing items can both reflect the coarse-grained attribute [29, 30, 35] of the category. However, images can provide fine-grained attributes specific to visuals, such as patterns and styles, while text can provide fine-grained attributes specific to text, such as fabric and size. The coarse-grained attributes represent commonalities among items, whereas fine-grained attributes are key factors influencing users purchasing decisions. Since the explicit interaction features are only enhanced through the modality item graph and represent user modality by simple aggregation, they lack information related to user behavior. Therefore, we first separate the coarse-grained attributes in the explicit interaction features, and then use user behavior information to reveal the fine-grained attributes within it."}, {"title": "3.3.1 Separating Coarse-Grained Attributes", "content": "We design a multilayer perceptron (MLP) to process the input vector X, apply a nonlinear transformation to X, and then output it as a weight matrix:\n\n$f(X) = W_4tanh(W_3X + b_3)$ (14)\n\nwhere $W_4 \u2208 \\R^{d}$ denotes attention vector and $W_3 \u2208 \\R^{dxd}$, b3 \u20ac $\\R^{d}$ denote the weight matrix and bias vector, respectively. These parameters are shared for all modalities.\nWe input $E_m$ into MLP to get the output, which is then passed through softmax to obtain the importance scores for each modality. Then, the coarse-grained attributes can be represented as:\n\n$E^c = \\sum_{m \u2208 M} \\frac{exp(f(E_m))}{\\sum_{E_{m'}\\in E_m} exp(f(E_{m'}))} E_m$ (15)"}, {"title": "3.3.2 Separating Fine-Grained Attributes", "content": "We designed a modality-specific behavior information extraction function $f_m'(X)$ as follows:\n\n$f_m'(X) = \u03c3(W_5X +b_5)$ (16)\n\nwhere $W_5 \u2208 \\R^{dxd}$ and $b_5 \u2208 \\R^{d}$ denote the weight matrix and bias vector. These parameters are modality-specific.\nTo extract fine-grained attributes, we first subtract the coarse-grained attributes $E_c$ from $E_m$, then multiply it by $E_{id}$ processed by the function $f_m'(X)$. Finally, we sum the fine-grained attributes from all modalities to obtain the final representation.\n\n$E^f = \\frac{1}{M} \\sum_{m\u2208M} (E_m - E_c) \\circledcirc f_m'(\\mathbb{id})$ (17)\n\nUltimately, we integrate coarse-grained attributes $E_c$ with fine-grained attributes $E^f$ as the final enhanced explicit interaction features $E_M$:\n\n$E_M = E_c + E^f$ (18)"}, {"title": "3.4 Capturing Commonalities Through Alignment", "content": "In recommendation scenarios, users may be attracted to common features between the images and text of products, while there are also potential correlations between behavioral features and modal features. To capture the commonalities between different modalities, we designed two alignment tasks from different perspectives.\nSpecifically, we first capture common information from an internal perspective. Inspired by [34], we parameterize the visual modality $E_v$ and the textual modality $E_t$ using Gaussian distributions.\n\n$\\bar{E}_v ~ \\mathcal{N}(\u03bc_v, \u03c3^2_v), \\bar{E}_t ~ \\mathcal{N}(\u03bc_t, \u03c3^2_t)$ (19)\n\nwhere ($\u03bc_v, \u03c3^2_v$) and ($\u03bc_t, \u03c3^2_t$) represent the Gaussian distribution parameters for $E_v$ and $E_t$. By minimizing the differences between the means and standard deviations of these two modalities, we enhance the internal connections between the modalities. The formula is as follows:\n\n$\\mathcal{L}_{VT} = |\u03bc_v \u2013 \u03bc_t | + |\u03c3_v \u2013 \u03c3_t|$ (20)\n\nThen, we capture common information from an external perspective. We encourage the explicit interaction features of users and items that have similar interaction behaviors to be close to each other, using InfoNCE to align $E_{id}$ and $\\bar{E}_{M}$:\n\n$\\mathcal{L}_{BM} = -log \\frac{exp(e_{u,id} \\cdot \\bar{e}_{u,M}/\u03c4)}{\\sum_{v \u2208 U} exp(e_{v,id} \\cdot \\bar{e}_{v,M}/\u03c4)}\n+ \\sum_{i\u2208I} -log \\frac{exp(e_{i,id} \\cdot \\bar{e}_{i,M}/\u03c4)}{\\sum_{j \u2208 I} exp(e_{j,id} \\cdot \\bar{e}_{j,M}/\u03c4)}$ (21)\n\nwhere t is the temperature hyper-parameter of softmax.\nFinally, the total alignment loss for capturing commonalities is calculated as:\n\n$\\mathcal{L}_{AL} = \\alpha \\mathcal{L}_{VT} + \\beta \\mathcal{L}_{BM}$ (22)\n\nwhere \u03b1 and \u03b2 are the balancing hyper-parameters."}, {"title": "3.5 Enhancing User Modality Representation", "content": "$E_{u,M}$ directly reflects the user's historical interactions, clearly expressing the user's past preferences. However, this is also its limitation, as it lacks foresight into the user's potential interests. In contrast, $\\hat{E}_{u,M}$ not only includes the user's historical modality information but also expands their interests by considering items similar to their historical items. This extension takes into account that users might be interested in items similar to those they have interacted with before, helping to foresee their potential interests. Based on this idea, we use InfoNCE to maximize the mutual information between $\\bar{E}_{u,M}$ and $\\hat{E}_{u,M}$, enabling $\\bar{E}_{u,M}$ to absorb and integrate the information provided by $\\hat{E}_{u,M}$. This strategy enhances the generalization ability of $E_{u, M}$, enabling it to effectively adapt to new products or changes in user behavior even in the absence of direct interaction data. The mathematical expression of this task is as follows:\n\n$\\mathcal{L}_{C} = -log \\frac{exp(e_{u,M} \\cdot \\hat{e}_{u,M}/\u03c4)}{\\sum_{v\u2208U} exp(\\bar{e}_{u,M} \\cdot \\hat{e}_{v,M}/\u03c4)}$ (23)\n\nwhere t is the temperature hyper-parameter of softmax.\nThen, to avoid over-relying on specific features during the learning process and learn user interests from a broader feature distribution, we follow the approach of SimGCL [40]. We add uniform noise to $E_{uM}$ and $\\hat{E}_{uM}$ to create contrastive views. The process of adding noise is as follows:\n\n$\\tilde{e}_{u,M} = \\bar{e}_{u,M} + \\Delta', \\tilde{e}_{u,M} = \\bar{e}_{u,M} + \\Delta',\\\\\n\\acute{e}_{u,M} = \\hat{e}_{u,M} + \\Delta'',\\acute{e}_{u,M} = \\hat{e}_{u,M} + \\Delta''$ (24)\n\nwhere \u0394' and \u0394\" are random noise vectors drawn from a uniform distribution U(0, 1). To optimize the noise-processed vectors, we define two new loss functions, $L_{\\tilde{N}}$ and $L_{\\acute{N}}$, which are optimized using the InfoNCE loss function:\n\n$L_{\\tilde{N}} = -log \\frac{exp(\\tilde{e}_{uM} \\cdot \\acute{e}_{u,M}/\u03c4)}{\\sum_{v\u2208U} \\tilde{e}_{vM} exp(\\acute{e}_{v,M}/\u03c4)}$ (25)\n\n$L_{\\acute{N}} = \\sum_{v \\in U} -log \\frac{exp(\\acute{e}_{uM} \\cdot \\tilde{e}_{u,M}/\u03c4)}{ exp(\\acute{e}_{vM} \\cdot \\tilde{e}_{v,M}/\u03c4)}$\n\nwhere t is the temperature hyper-parameter of softmax.\nFinally, the total user modality enhancement loss is calculated as:\n\n$\\mathcal{L}_{UM} = \u03b3(\\mathcal{L}_{C} + \\mathcal{L}_{\\tilde{N}} + \\mathcal{L}_{\\acute{N}})$ (26)\n\nwhere y is the balancing hyper-parameters."}, {"title": "3.6 Model Prediction", "content": "Based on the ID features and enhanced explicit interaction features, we can get the general representations of users and items:\n\n$e_u = \\bar{e}_{u,id} + e_{u,M}$ (27)\n\n$e_i = \\bar{e}_{i,id} + \\bar{e}_{i,M}$ (28)\n\nFinally, we compute the inner product of user and item representations to predict their compatibility score:\n\n$\\Upsilon_{u,i} = e_u^T e_i$ (29)"}, {"title": "3.7 Optimization", "content": "To optimize the effectiveness of our recommendations, we employ the Bayesian Personalized Ranking (BPR) loss [19] as our primary optimization function. This approach assumes that the predicted scores for observed user-item pairs should be higher than those for unobserved pairs. The BPR function is defined as follow:\n\n$\\mathcal{L}_{bpr} = \\sum_{u,i,j\u2208D} -log\u03c3(\\Upsilon_{u,i} \u2013 \\Upsilon_{u,j})$ (30)\n\nwhere D represents the training set, (u, i) are observed user-item pairs and (u, j) are unobserved pairs. o is the sigmoid function.\nUltimately, we update the representations of users and items by combining the following loss functions: bpr loss, alignment loss and user modality enhancement loss. The formula is as follows:\n\n$\\mathcal{L}_{GUME} = \\mathcal{L}_{bpr} + \\mathcal{L}_{AL} + \\mathcal{L}_{UM} + \u03b4||\u0398||^2$ (31)\n\nwhere d is a hyperparameter to control the effect of the L2 regularization, and \u0398 is the set of model parameters."}, {"title": "4 EXPERIMENTS", "content": "In this section, we construct experiments to demonstrate the advantages of the proposed methods and to address the following research questions: RQ1: How effective is our GUME compared to the state-of-the-art multimodal recommendation methods and traditional recommendation methods? RQ2: How do the key modules impact the performance of our GUME? RQ3: Does the proposed graph augmentation strategy improve the recommendation performance for tail items? RQ4: Why is the user modality enhancement modules effective? RQ5: How do different hyper-parameter settings impact the performance of our GUME?"}, {"title": "4.1 Experimental Setting", "content": "We adhere to the dataset selection criteria utilized in several recent studies [42, 49, 41], ensuring consistency and comparability of results. Specifically, we conduct extensive experiments on four categories of the Amazon dataset 1: (1) Baby, (2) Sports and Outdoors (referred to as Sports), (3) Clothing, Shoes, and Jewelry (referred to as Clothing), and (4) Electronics. These categories offer a diverse range of items, from basic baby care products to high-tech electronic devices. For each modality feature extraction, we follow the same setting mentioned in [47], which extracted 4096 dimensions of visual features and 384 dimensions of textual features via pretrained encoder. The item interaction counts in all datasets exhibit a long-tail distribution, which also makes these datasets"}, {"title": "4.2 Overall Performance (RQ1)", "content": "shows the performance comparison of the proposed GUME and other baseline methods on four datasets. The table reveals the following observations:\nOur GUME model achieved excellent performance across multiple metrics, surpassing traditional recommendation models and multimodal recommendation models. Specifically, in terms of Recall@20 for Sports, Clothing, and Electronics, GUME outperforms the best baseline by 2.28%, 3.54%, and 3.82% respectively; while in terms of NDCG@20, it shows improvements of 3.13%, 5.67%, and 2.65%. On the Baby dataset, GUME ties with the best baseline in Recall@20 and improves by 2.22% over the best baseline in NDCG@20. The results validate the effectiveness of our GUME.\nUsing multimodal information of items to enhance graph connectivity can improve recommendation performance. For example, LATTICE dynamically learns the latent structure between items based on the similarity of their multimodal features. However, dynamically generating latent structure is unnecessary and memory-intensive, which is not efficient for computation. This was confirmed by the subsequent work FREEDOM [48]. Inspired by FREEDOM, MENTOR constructs item homogeneous graphs for each modality to enhance semantic relationships between items and then freezes them. Our GUME first identifies semantic neighbors of items based on multimodal similarity and then enhances the user-item interaction graph with them, especially by adding more edges for tail items. This promotes the exploration of items that users might be interested in and alleviates the cold start problem for tail items.\nAligning multiple types of features can improve recommendation performance. For example, BM3 aligns modality features from both intra-modality and inter-modality perspectives. MGCN designs a self-supervised auxiliary task to promote the exploration between behaviors and multimodal information. MENTOR designed a multi-level cross-modal alignment task, aligning each modality under the guidance of ID embeddings while maintaining historical interaction information. Our GUME not only aligns external information such as behavioral features and multimodal features but also aligns internal information such as image features and text features, promoting the learning of user and item representations from multiple perspectives.\nThe multimodal representation of users has always been relatively vague. Some methods, such as BM3, only learn user ID representations and ignore users' multimodal representations. This is a neglect of users' multimodal preferences. Other methods represent users' multimodal features by aggregating the multimodal features of items. For example, SLMRec and MGCN obtain users' multimodal features by simply aggregating the multimodal features of interacted items. This approach treats multimodal features of purchased items equally, which is not effective for fully capturing users' multimodal preferences. Building on this, our GUME constructs extended interest features for users. By using contrastive learning with users' explicit interaction features, we maximize the mutual information between them. This process helps to refine and expand users' multimodal preferences effectively."}, {"title": "4.3 Ablation Study (RQ2)", "content": "In our work, GUME comprises the modules Graph Enhancement, Alignment for Capturing Commonalities and User Modality Augment. To thoroughly examine the impact of these modules, we conduct comprehensive ablation studies. We use \"w/o XX\" to denote the absence of a specific module, meaning \"without XX\"."}, {"title": "4.3.1 The influence of Modules", "content": "w/o GE: We remove the graph enhancement module. The model's average performance declines without graph enhancement, indicating that our graph enhancement module can effectively improve overall recommendation performance. We will further demonstrate the impact of graph enhancement on improving the recommendation performance for tail items in Section 4.4.\nw/o AL: We remove the alignment for capturing commonalities module. On four datasets, the performance of the model without this module is significantly lower than that of GUME. This indicates that common information within modalities and between modalities and external behaviors is crucial for recommendation performance. Without the alignment process, the modality data of items might contain noise unrelated to the items themselves and a lot of noise unrelated to user behavior. Through alignment, we capture commonalities relevant to the recommendation scenario within modalities and between modalities and external behaviors, thereby achieving a denoising effect. This demonstrates the effectiveness of the alignment module.\nw/o UM: We remove the user modality enhancement module. User modality enhancement is equally crucial for model performance, as the model's performance significantly declines without this component. Although it is possible to model the user's multimodal features by aggregating the multimodal features of the items the user has interacted with, this method has limitations. It only reflects the user's current multimodal preferences and lacks a comprehensive understanding of the user's latent interests and preferences. In contrast, leveraging contrastive learning between extended interest embeddings and explicit interaction embeddings helps to more comprehensively learn the user's multimodal preferences, thereby enhancing recommendation performance. We will further demonstrate the effectiveness of user modality augmentation through visualization in Section 4.5."}, {"title": "4.4 Comparisons on Tail Items Performance (RQ3)", "content": "To validate whether enhancing the user-item graph based on multimodal similarity can improve the recommendation performance for tail items, we conducted experiments on the Clothing dataset. Specifically, we divided items into five equally sized groups according to the node degree in the user-item bipartite graph, as shown in Figure 2. In recommendation systems, 20% of items account for 80% of interactions. Therefore, we define the top 1/5 of items as head items, while the remaining 4/5 are defined as tail items. The larger the x-axis value, the lower the node degree, and the less popular the item. We compared the performance of GUME, w/o GE, and MENTOR. The results show that graph enhancement can improve the recommendation performance for tail items. Although removing graph enhancement can improve the recommendation performance for head items, the overall performance decreases due to the decline in tail item performance, which is consistent with the findings of GALORE [17]. Additionally, GUME outperforms MENTOR for both head and tail items, indicating that our graph enhancement strategy effectively improves recommendation performance for long-tail distribution data."}, {"title": "4.5 Visualization Analysis (RQ4)", "content": "To further validate the effectiveness of the user modality enhancement component, we visualize the distribution of user modality representations within the Sports dataset. We compare two models, w/o UM and GUME, as mentioned in section 4.3.1. Specifically, we randomly select 1000 user instances from the Sports dataset and employ t-SNE [22] to map the user modality representations to two-dimensional space. The results, illustrated in figure 3, show that the user modality distribution of GUME is more uniform, while the distribution of w/o UM is more dispersed. Previous research [24] has demonstrated that the uniformity of representation significantly influences recommendation performance. This explains why GUME is effective in enhancing user modality representation."}, {"title": "4.6 Hyperparameter Sensitivity Study (RQ5)", "content": "The balancing hyper-parameter \u03b1. Figure 4 shows the variation of the balance hyperparameter \u03b1 in the visual-textual alignment task within the range of {1e-4, 1e-3, 1e-2, 1e-1, 1e0}. Observations indicate that as a increases, performance initially improves and then declines. This suggests that jointly optimizing the visual-textual alignment task with the primary recommendation task can enhance performance, but if a is too large, the model may be misled by the visual-textual alignment task. Overall, there is no significant sharp rise or fall, indicating that our method is relatively insensitive to the choice of \u03b1."}, {"title": "4.6.2 The pair of hyper-parameters \u03b2 and \u03c4", "content": "The behavior-modality alignment task is jointly controlled by the balance hyper-parameter \u03b2 and the temperature hyper-parameter \u03c4. We adjust \u03b2 within the range of {1e-4, 1e-3, 1e"}]}