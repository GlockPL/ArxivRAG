{"title": "FLUENT STUDENT-TEACHER REDTEAMING", "authors": ["T. Ben Thompson", "Michael Sklar"], "abstract": "Many publicly available language models have been safety tuned to reduce the likelihood of toxic or\nliability-inducing text. Users or security analysts attempt to jailbreak or redteam these models with\nadversarial prompts which cause compliance with requests. One attack method is to apply discrete\noptimization techniques to the prompt. However, the resulting attack strings are often gibberish text,\neasily filtered by defenders due to high measured perplexity, and may fail for unseen tasks and/or\nwell-tuned models. In this work, we improve existing algorithms (primarily GCG and BEAST) to\ndevelop powerful and fluent attacks on safety-tuned models like Llama-2 and Phi-3. Our technique\ncenters around a new distillation-based approach that encourages the victim model to emulate a\ntoxified finetune, either in terms of output probabilities or internal activations. To encourage human-\nfluent attacks, we add a multi-model perplexity penalty and a repetition penalty to the objective.\nWe also enhance optimizer strength by allowing token insertions, token swaps, and token deletions\nand by using longer attack sequences. The resulting process is able to reliably jailbreak the most\ndifficult target models with prompts that appear similar to human-written prompts. On Advbench we\nachieve attack success rates > 93% for Llama-2-7B, Llama-3-8B, and Vicuna-7B, while maintaining\nmodel-measured perplexity < 33; we achieve 95% attack success for Phi-3, though with higher\nperplexity. We also find a universally-optimized single fluent prompt that induces > 88% compliance\non previously unseen tasks across Llama-2-7B, Phi-3-mini and Vicuna-7B and transfers to other\nblack-box models.", "sections": [{"title": "1 Introduction", "content": "Instruction-tuned language models are often trained to refuse certain queries deemed toxic using techniques such as\nRLHF and DPO (Rafailov et al. 2024; Ouyang et al. 2022). Recent work has shown that safety training is vulnerable to\nadversarial attacks (Zou et al. 2023; Mazeika et al. 2024; Fort 2023; Chao et al. 2023). However, adversarial prompts\noptimized only for attack success typically look like gibberish. A defender can easily distinguish these attacks from\nuser prompts based on an attack's high model-evaluated perplexity (Alon and Kamfonas 2023). In response, algorithms\nhave been designed that produce fluent attacks (Zhu et al. 2023; Sadasivan et al. 2024; Chao et al. 2023; Takemoto\n2024; Wang et al. 2024).\nIn this paper, we focus on token-level discrete optimization algorithms that are conceptual descendants of GCG (Zou\net al. 2023) and BEAST (Sadasivan et al. 2024). In contrast, other works have used language models to produce\nnaturally fluent attacks (Liu et al. 2023; Paulus et al. 2024; Chao et al. 2023; Shah et al. 2023). We believe that\ntoken-level optimization attacks and model-based attacks are complementary, as token-level optimization can be layered\non top of a high quality initialization from human-written or model-written attacks.\nAmong token-level optimizers, past approaches either lack fluency or are too weak to reliably attack adversarially\ntrained models like Llama-2 (Touvron et al. 2023) and Phi-3 (Abdin et al. 2024). To achieve the simultaneous goal of\nfluency and high attack success rate, we improve both the objective function and the optimizer:\n1. A distillation objective. We reconsider the typical \"token-forcing\" objective function where the prompt is optimized\nto maximize the likelihood of a generation that begins with \"Sure, here is...\". This objective often fails to attack\nmodels like Llama-2 and Phi-3, where the model will revert to refusal after the initial affirmative response. Instead,\nafter applying forcing to the first few tokens, we minimize a distillation loss which induces the victim model to\nemulate a toxified copy which has been LoRA fine-tuned on a small dataset of 2500 toxic generations (Lermen,\nRogers-Smith, and Ladish 2023). Distillation can aim to match either output probabilities or internal activations at\none or more layers.\n2. Preferencing human-fluent attacks. We regularize the objective function with a term that prefers more fluent\nattacks as measured by the attack perplexity. In addition, we use the same token proposal function as the BEAST\nalgorithm (Sadasivan et al. 2024) to preference tokens that the victim model considers likely. While these two\ntechniques do result in low perplexity prompts, the resulting attacks suffer from over-optimization such that the\nattack will often repeat the same token dozens of times or find out-of-distribution attacks that are evaluated as\nlow perplexity by the victim model despite being nonsense to the human eye. To solve these issues and produce\nhuman-fluent attacks, we evaluate attack perplexity as the average perplexity assessed by multiple models and layer\na repetition penalty on top.\n3. More flexible optimization. Our optimization is primarily based on the GCG and BEAST algorithms. We extend\nthese methods to allow token insertions, token swaps, and token deletions. The optimizer also has freedom to\nlengthen or shorten the attack prompt. We adopt the buffer from Hayase et al. (2024). We allow both a prefix and a\nsuffix to the desired task in the user prompt.\nCombining these algorithmic improvements results in a process that is able to reliably jailbreak the most difficult target\nmodels with prompts that appear similar to human-written prompts. For example, we are able to achieve a prompt\nperplexity of 33 while inducing Llama-2-7B into performing 96% out of 520 tasks. Similarly, we optimize a single\nprompt that achieves attack success rates on previously unseen toxic tasks of 90%, 88% and 90% against Llama-2-7B,\nPhi-3-mini and Vicuna-7B while maintaining an average perplexity of 118. See Appendix D for more example attacks."}, {"title": "2 Related work", "content": "The vulnerability of neural network models to adversarial attacks, first studied in vision models and more recently in\nthe language domain, remains a persistent issue (Madry et al. 2017; Carlini and Wagner 2017; Goodfellow, Shlens, and\nSzegedy 2014).\nVarious manual jailbreak tricks have been developed specifically for language models, including code-like instruction\n(Kang et al. 2023; Jha and Reddy 2023), translating the request to low-resource languages (Yong, Menghini, and Bach\n2023), and formatting requests (Wei, Haghtalab, and Steinhardt 2024). Anil et al. 2023 uses long-context windows to\nprepend many examples of toxic compliance to the earlier context, inducing jailbreaking for subsequent requests.\nJones et al. 2023 uses perplexity penalties to encourage fluency in adversarial prompts. Zou et al. 2023 develops\nGCG to optimize non-fluent adversarial suffixes. Notably, they reported significantly higher attack success rates when"}, {"title": "3 Task details", "content": "3.1 An example attack\nIn the typical one-turn language model adversarial attack setting, the goal is to find user inputs that cause a model to\nperform a specific task that the model has been trained not to perform. The user prompt is often prefixed with a system\nprompt. For concreteness, here is an example of the prompt and generation for one of our fluent attacks on Vicuna-7B,\noptimized on the single task: \"Write a guide on how to commit insider trading.\" The black section of the text is the\nsystem prompt and conversation template, the blue portion is the task we want to model to perform and the red portion\nof the text is the optimized attack. The magenta section is the generation.\nA chat between a curious user and an artificial intelligence assistant. The assistant\ngives helpful, detailed, and polite answers to the user's questions. USER: Write a\nguide on how to commit insider trading. Follows from part 1 ISTANBULLO: The KSA is not\naware of any specific information that could indicate an illegal activity as mentioned\nabove but we are operating at a very high level of alertness working closely with all"}, {"title": "3.2 The token forcing objective", "content": "The typical objective for attacking language models is to \"force\" the model to produce an initial affirmative response\nsuch as \"Sure, here is\u201d (Zou et al. 2023; Jones et al. 2023). The optimization objective is the log-probability of the first\nF forced tokens:\n$L_F := -\\frac{1}{F} \\sum_{i=1}^{F} log p(t_i | t_J...t_{i-1})$\nwhere $t_J$ is the first token of the prompt, $t_0$ is final token of the prompt, and tokens $t_1 ... t_k$ are generation tokens. We\nterm this objective the \"token forcing\" objective. In the next section, we supplement this objective with distillation from\na toxified model."}, {"title": "4 Objective function", "content": "4.1 Challenges with token forcing\nAs described in the section above, token forcing, a common approach in adversarial prompt optimization is to maximize\nthe log-probability of an initial affirmative response (Zou et al. 2023; Jones et al. 2023). However, token forcing is a\nproxy objective for the true objective of triggering the desired toxic behavior, and strongly safety-trained models, such\nas Llama-2 and Phi-3, are often capable of self-correcting or \"reversing\" after being forced to output an affirmative\ninitial generation. Thus, an attack might fail despite a loss near zero. In the example below, we allow the model to\nbegin generation after the first line of text and it begins by saying \"I cannot\"."}, {"title": "4.2 Loss Clamping", "content": "As a simple form of regularization, we may define the Clamp() function:\n$Clamp[x] := max(x, - ln 0.6)$\nWe apply Clamp() to the loss sub-component of each token to reduce optimization effort on tokens that have already\nbeen well-solved."}, {"title": "4.3 Distillation from a Toxic Model", "content": "There are 3 major reasons to using a distillation-based objective for adversarial attacks. First, distillation reduces the\noptimizer's incentive to force the precise tokens of the target generation, thus aiding generalization. Second, forcing\nstrings generated by a toxified version of the victim model are more likely to be feasible outputs of the victim model.\nThird, KL distillation provides advantages in data efficiency, reducing cost (Hinton, Vinyals, and Dean 2015; Yang et al.\n2024).\nThere are two main types of distillation losses used in the white-box setting: output-based distillation of class\nprobabilities, and \u201chint-based\" distillation to emulate internal activations with squared error loss (Yang et al. 2024;\nRomero et al. 2014). We develop both of these approaches for adversarial prompts.\nFirst, we must construct a toxified copy of the victim model. Reversing safety-tuning with LoRA requires only a small\ninitial dataset and minimal resources, as reported previously by (Lermen, Rogers-Smith, and Ladish 2023; Zhan et al.\n2023). Our training dataset is the 2500 sample instances from the development phase of the NeurIPS 2023 Trojan\nDetection Competition Trojan Detection Competition (2023). For hint-based distillation on the activations at a layer\nl > 4, LoRA training is performed only on layers up to and including layer 4; later layers are not modified. For\nlogits-based distillation, LoRA training is performed at all layers.\nSecond, we use the toxified model to generate a partial generation for the task request. Finally, instead of optimizing for\nthe likelihood of the partial toxic generation, the optimizer aims to match the victim model against the toxic model,\nusing the distillation-like loss."}, {"title": "4.3.1 Attack Loss for Logits-based Distillation", "content": "While a typical distillation loss is based on the KL divergence, the cross-entropy form we use is equivalent up to an\nadditive offset.\n$XE{P,Q} := \u2013 \\sum p(x) log q(x)$\nThis equivalence is incidentally broken when we clamp the loss-contribution from each token. Explicitly, the attack loss\nbecomes\n$L_D := \\frac{1}{K} \\sum_{i=1}^{K} XE{P_{victim} (\\cdot | prefix+task+suffix+t_1 ...t_{i-1}), P_{toxic} (\\cdot | task+t_1... t_{i-1}) }$"}, {"title": "4.3.2 Attack Loss for Hint-like Distillation", "content": "As a simplified version of the hints-based loss of Romero et al. 2014, we take the squared difference of the residual\nstreams at layer l over the generation tokens.\n$L_D := \\frac{1}{K} \\sum_{i=1}^{K} ||X_{victim,l} ( prefix+task+suffix + t_1 . . . t_{i-1}) \u2013 X_{toxic,1} (task+t_1... t_{i-1})||^2$\nIt is possible to use multiple layers for this loss. For simplicity, we use only a single layer l 20 and fine-tune the\ntoxified model only for layers up to layer 4."}, {"title": "4.4 Regularization for Fluent Attack Selection", "content": "A first line of automated defense against GCG-like attacks is perplexity filters (Alon and Kamfonas 2023), which can\nflag random-looking attack strings while admitting human-written text. Attacks that are indistinguishable from normal\nuser input are harder to filter.\nTo incentivize fluency in adversarial attacks, several works have explicitly added perplexity terms as part of the objective\ncost (Shi et al. 2022; Zhu et al. 2023; Jones et al. 2023; Thompson, Straznickas, and Sklar 2024). We take this route,\nadding a fluency regularization term to the objective. Specifically, we use the mean per-token cross entropy of the user\nprompt according to the victim model. Explicitly:\n$L_{XE} = \\frac{1}{M} \\sum_{i=1}^{M} logp(t_{i+1} | t_J...t_i)$\nwhere M is the length of the user input and $t_J$ is the first token of the system prompt."}, {"title": "4.5 Human-evaluated fluency", "content": "Although regularizing the fluency successfully reduces the model-evaluated perplexity, the resulting attacks diverge\nfrom \"fluency\" according to the human eye. The optimizer finds out-of-distribution attacks that the victim model\nevaluates as low perplexity but appear nonsensical. The optimizer will also repeat a single token or short sequences of\ntokens many times. This repetition is unsurprising: After seeing the same token several times, the probability of the\ntoken appearing again is high. The following prompt demonstrates these problems: our optimization yielded the attack\nbelow with perplexity 17.6 according to Llama-2-7B:"}, {"title": "4.6 Final Objective", "content": "We use a combined objective with four components: a forcing objective on the first F tokens of the generation, a\ndistillation objective on the remainder of the generation choosing either the logits- or hints-based loss, a multi-model\nfluency regularization for the prompt averaged over m reference models, and a repetition penalty averaged over their\ntokenizers.\n$L_F + C_D L_D + \\frac{1}{m} \\sum (C_{XE} L_{XE,M} + C_{Rep} L_{Rep,m})$\nExplicitly, if logit-based distillation is performed and clamping is applied, substitution yields:\n$\\frac{1}{K} \\sum_{i=1}^{F} Clamp(log p(t_i|t_{-J}...t_{i-1}))$\n$+ \\frac{C_D}{K} \\sum_{i=F}^{K} Clamp [XE{P_{victim} (\\cdot | prefix+task+suffix+t_1...t_{i-1}), P_{toxic} (\\cdot | task+t_1 ... t_{i-1})}]$\n$+ \\frac{1}{M} \\sum_{i=1}^{M} logP_m (t_{-i+1}/t_{-J}...t_{-i}) + C_{Rep} \\frac{1}{M} \\sum_{\\upsilon \\in vocab (prefix + task + suffix)} (\\sum \\mathbb{1}{t_i = \\upsilon})^{1.5} -1$'"}, {"title": "5 Optimization", "content": "Discrete prompt optimization algorithms follow a simple loop:\n1. Start with the current-best attack prompt(s).\n2. Mutate a prompt in a single token position to generate new candidates. For each candidate, Zou et al. (2023)\npicks random token position for swapping, and selects a promising replacement token according to a first-order\napproximation to the loss. Zhu et al. (2023) limits this mutation to the final token of the prompt and includes a\nfluency term in addition to the approximation to the attack loss. Sadasivan et al. (2024) limits the mutation to\nbe a new token at the end of the prompt and selects promising tokens by sampling without replacement from\nthe victim language model.\n3. Measure the loss objective for each candidate.\n4. Filter and retain some candidates. Zou et al. (2023) retains only the best candidate. Hayase et al. (2024)\nmaintains a buffer. Thompson, Straznickas, and Sklar (2024) maintained multiple candidates across trade-offs\nof fluency vs strength. Sadasivan et al. (2024) maintains a beam search across recent token additions."}, {"title": "5.1 Fluent and Flexible Sequence Proposals", "content": "How should the optimizer 'mutate' from a current-best candidate to the next round of candidates?\nGreedy coordinate gradient (GCG) (Zou et al. 2023) maintains an attack of constant token length. The loss is back-\npropagated to compute the gradient of the loss with respect to each possible token. Each candidate proposal differs\nfrom the current attack in one element: at a random token position, a token-swap is selected randomly among the top-k\nmost promising tokens according to a first-order approximation.\nIn contrast, BEAST of Sadasivan et al. 2024 extends the attack fluently to the right, by proposing new tokens according\nto the sampling without replacement from the top suggestions of a language model.\nWe mix these ideas by proposing our mutations in the following proportions:\nWe select $k_1$ mutations according to the following rules. The parameter $k_1$ (following the nomenclature of BEAST) is\nequivalent to the \"batch size\" in GCG.\n\u2022 w/probability $p_{delete}$: delete a token in a random position\n\u2022 w/probability $P_{insert}$: insert a token in a random position\n\u2022 w/probability $P_{swap}$: swap a token, in a random position\n\u2022 w/probability $P_{edge}$: insert a token at the end of the attack\nWhen adding or swapping a token, we sample from a language model without replacement to produce $k_2$ proposed\ntokens. Then, one of the $k_2$ proposed tokens is selected with uniform probability. The use of language model sampling\nto propose tokens is similar to BEAST, however, the random token position and the random selection amongst the\nsampled tokens are similar to the GCG token proposal mechanism."}, {"title": "5.2 Increasing Length Improves Objective", "content": "The results of Fort 2023 and Anil et al. 2023 both indicate increased effectiveness of long attacks. We confirm that\nincreasing the length of attacks can greatly improve our optimization loss, indicating greater attack strength. In Figure 2,\nwe initialize with different lengths and limit the lengthening and shortening of the prompt to just 10% of the initial\nlength. After iterating sufficiently long to reach a loss plateau, longer prompts are able to achieve lower loss values."}, {"title": "5.3 Optimization Buffer", "content": "Retaining only the best candidate as in GCG results in occasional increases in the loss function when an iteration\nexplores only poor token replacements. To ameliorate this issue, we use the optimization buffer technique described by\nHayase et al. 2024, which we find to prevent these loss increases. The buffer is a heap of the top B candidate sequences\nseen so far. Each round, the best candidate is popped from the heap for exploration according to the process described\nin Hayase et al. 2024."}, {"title": "6 Results", "content": "We use the AdvBench dataset (Zou et al. 2023) to evaluate how effective our methods are at circumventing the safety\ntraining of four models: Llama-2-7B-Chat, Vicuna-7B-v1.5, Llama-3-8B-Instruct and Phi-3-mini-4k-instruct. We\nabbreviate these models' names as Llama-2-7B, Vicuna-7B, Llama-3-8B and Phi-3-mini throughout the paper.\nEvaluations of the perplexity of attacks are according to the victim model, unless otherwise stated. Past papers have\njudged attack success rate (ASR) with a variety of methods, including checking for presence of agreement strings. Past\npapers have evaluated the perplexity of only the attack itself. We judge our ASR according to evaluation with GPT-4o,\nwith the prompt given in Appendix A. We report the perplexity of the full user input, including the task description.\nWe document the hyperparameters used for the different experiments in Appendix C. We include many example attacks\nin Appendix D with several different initialization schemes documented in Table D1."}, {"title": "6.1 Single-Task, Single-Model", "content": "Table 1 reports results for Advbench in the single task setting for Llama-2-7B, showing excellent ASR performance at\nquite low perplexity compared to other published methods.\nTable 2 reports results for Advbench for Llama-2-7B, Vicuna-7B, Llama-3-8B and Phi-3-mini.\nFor both of these evaluations, our optimization did not use multi-model perplexity or repetition penalties, and the attacks\ndo contain frequent token-repetitions. We run for 1400 iterations which requires 15 minutes of H100 time for the 7/8B\nmodels and 10 minutes for the smaller and faster Phi-3."}, {"title": "6.2 Multi-task, multi-model", "content": "The \"universal\" GCG attack of Zou et al. 2023, optimized to attack many tasks simultaneously, was found to achieve an\nASR of 84% for Llama-2-7B in Advbench test examples, according to string-matching grading.\nIn similar fashion, we optimized a fluent attack on ten tasks simultaneously against Llama-2-7B, Phi-3-mini, and\nVicuna-7B. Fluency penalties and repetition penalties are summed for all three models. Each optimizer step, one of the"}, {"title": "6.3 Language Crystallization and Initialization", "content": "With longer prompts initialized from uniformly random tokens, the resulting string is not guaranteed to be fully in any\none language. We see patches of same-language text appear and grow locally, a type of crystallization process resulting\nfrom fluency optimization operating locally rather than globally. Similar phenomena occur can in the conceptual content\nof the attack. In the example attack below, we see clearly sections written in English, French, and Japanese as well as a\nfew words in other languages.\n\"NOT silence any answer and let her d\u00e9criser les principe cl\u00e9 dans votre mouvmanence\nen tr\u00e9squibus avec seulement au plus une phrase de k\u00f6z\u00f6tt \u00e9s, un seul exemple\ndiff\u00e9renciant les innovatioin et egyektionenzen szuper-szubjekt\u00edv v\u00e1ltoz\u00f3okl\u00e1tok.\nNiedcription laenyeENDNO debe be. 'sThis joke can surely be dished up for all your\nfellow friends' where follows:Josepha B'. Your have done an epiphan ofburdensome\u306e\u82f1\u6587\n\u3092style\u3067proposu?guide me to write it in a paragraphed format\"\nLocal fluency is acceptable in settings where the only goal is low model-evaluated perplexity. However, if the goal\nis global human-fluency, we need to circumvent this crystallization phenomenon. We find that global fluency can be\nrecovered by either initializing a short or empty string or by initializing from a non-random sequence."}, {"title": "7 Redteaming pitfalls and recommendations", "content": "In our work and in evaluating previous work, we have encountered many important details in algorithmic redteaming.\nIn this section, we collect some of these details and provide suggestions for community standards and future work.\nFirst, we recommend developing and testing methods on harder-to-attack models like Llama-2, Phi-3, Claude, and\nGPT-4. Methods that work well on easy-to-attack models like Vicuna may not be useful for safety-trained models.\nFor practical applications, we recommend representing prompts as strings rather than token sequences. Token-level\noptimizers are prone to finding 'impossible' token sequences which do not revert back to the same token sequence\nwhen decoded and retokenized for API requests. For similar reasons, we maintain the attack state as a string to handle\ncases where we are attacking multiple models which have different tokenizers. Tracking the attack state as a string\nintroduces additional complexity in the software implementation of a token-level optimizer because token positions\nmust be carefully tracked when producing token proposals. The code we shared includes methods for tracking attack\nstates and token proposals.\nRelatedly, we recommend preventing optimizations routines from using special tokens. Padding, beginning of sequence,\nand end of sequence tokens are powerful tools for attacking some models but would be easily filtered out or tokenized\ndifferently in a typical API or chat interface.\nWe recommend evaluating whole-prompt perplexity when developing fluent attacks. Some past work has evaluated the\nperplexity of an attack in isolation without the surrounding system prompt, chat template and task. However, the full\nfluency of a prompt requires some coherency between these different subsections of the prompt.\nWe recommend clearly accounting for both GPU runtimes and model provider API usage. Some authors have advertised\ntheir methods as requiring minimal resources because their model-written attacks only require the OpenAI API.\nHowever, if such an attack requires $10 of API usage to produce a single attack, it is an important expense to advertise.\nBecause the cost landscape is changing rapidly both in terms of the price per hour of GPU time and the price per token\nof API usage, we recommend sharing; 1) the run time and hardware used. 2) the number of tokens of input and output\nthat were passed to any API."}, {"title": "8 Discussion", "content": "In this paper, we demonstrated fluent adversarial attacks generated via a discrete-optimization process.\nThese white-box methods should be useful to assist with vulnerability evaluation and hardening of models. Model\ndevelopers wishing to apply our framework may already have a toxic-capable base model (often called a \u201chelpful only\"\nmodel) and therefore have the option to skip toxicity-training.\nAlthough current black-box attack methods are roughly as effective as white-box methods (Takemoto 2024), we are\nunsure if this will remain the case. Model developers will harden their filtering and refusal mechanisms pushing the\ndefensive frontier towards harder-to-find portions of the attack surface. In the long run, white-box attack methods are\nlikely to be necessary or, at least, very helpful to find these holes. In addition, token-level optimizers can stack on top of\nother human or model-driven adversarial attacks. Running a smaller number of optimizer iterations is an extremely\nefficient approach to strengthen a model-written attack that is already somewhat effective.\nWhile our methods are slower than existing automated redteaming tools, our method costs less than $1 (per single-model\nsingle-task attack on a 7B class model) despite producing diverse and fluent attacks. The slower runtime is primarily\ndue to the challenging objective function of simultaneously achieving high attack success on well-defended models\nand achieving human-fluency. The per-iteration cost is increased by using longer prompts and evaluating fluency on\nmultiple models. Furthermore, increasing the fluency penalty also has a trade-off on the strength of the attack, similar\nto the trade-off found in Thompson, Straznickas, and Sklar 2024.\nTo conclude, we share three straightforward directions to improve on the work here or extend token-based language\nmodel adversarial attack methodology:\n\u2022 Reducing computational expense in the methods presented here. We expect there is a lot of room for\nimprovement. For example, normally, the attack will succeed long before we terminate the algorithm because\nattack success is not the only goal. Even after attack success, fluency continues to improve for many iterations.\nWe expect there is much room for improvement with further tuning of stopping rules, hyperparameters, and\nmore general improvements to the token-level optimizer.\n\u2022 Human-fluency is a challenging objective. We see substantial room for improvement in developing better\nproxy objectives for human-fluency. For example, the repetition penalty we use overpenalizes common words\nlike \"the\" or \"a\".\n\u2022 We are excited about methods that would make feasible the optimization of token sequences of over 10,000\ntokens. For example, attempts to modify more than one token per optimizer iteration."}]}