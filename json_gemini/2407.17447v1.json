{"title": "FLUENT STUDENT-TEACHER REDTEAMING", "authors": ["T. Ben Thompson", "Michael Sklar"], "abstract": "Many publicly available language models have been safety tuned to reduce the likelihood of toxic or liability-inducing text. Users or security analysts attempt to jailbreak or redteam these models with adversarial prompts which cause compliance with requests. One attack method is to apply discrete optimization techniques to the prompt. However, the resulting attack strings are often gibberish text, easily filtered by defenders due to high measured perplexity, and may fail for unseen tasks and/or well-tuned models. In this work, we improve existing algorithms (primarily GCG and BEAST) to develop powerful and fluent attacks on safety-tuned models like Llama-2 and Phi-3. Our technique centers around a new distillation-based approach that encourages the victim model to emulate a toxified finetune, either in terms of output probabilities or internal activations. To encourage human-fluent attacks, we add a multi-model perplexity penalty and a repetition penalty to the objective. We also enhance optimizer strength by allowing token insertions, token swaps, and token deletions and by using longer attack sequences. The resulting process is able to reliably jailbreak the most difficult target models with prompts that appear similar to human-written prompts. On Advbench we achieve attack success rates > 93% for Llama-2-7B, Llama-3-8B, and Vicuna-7B, while maintaining model-measured perplexity < 33; we achieve 95% attack success for Phi-3, though with higher perplexity. We also find a universally-optimized single fluent prompt that induces > 88% compliance on previously unseen tasks across Llama-2-7B, Phi-3-mini and Vicuna-7B and transfers to other black-box models.", "sections": [{"title": "1 Introduction", "content": "Instruction-tuned language models are often trained to refuse certain queries deemed toxic using techniques such as RLHF and DPO (Rafailov et al. 2024; Ouyang et al. 2022). Recent work has shown that safety training is vulnerable to adversarial attacks (Zou et al. 2023; Mazeika et al. 2024; Fort 2023; Chao et al. 2023). However, adversarial prompts optimized only for attack success typically look like gibberish. A defender can easily distinguish these attacks from user prompts based on an attack's high model-evaluated perplexity (Alon and Kamfonas 2023). In response, algorithms have been designed that produce fluent attacks (Zhu et al. 2023; Sadasivan et al. 2024; Chao et al. 2023; Takemoto 2024; Wang et al. 2024).\nIn this paper, we focus on token-level discrete optimization algorithms that are conceptual descendants of GCG (Zou et al. 2023) and BEAST (Sadasivan et al. 2024). In contrast, other works have used language models to produce naturally fluent attacks (Liu et al. 2023; Paulus et al. 2024; Chao et al. 2023; Shah et al. 2023). We believe that token-level optimization attacks and model-based attacks are complementary, as token-level optimization can be layered on top of a high quality initialization from human-written or model-written attacks.\nAmong token-level optimizers, past approaches either lack fluency or are too weak to reliably attack adversarially trained models like Llama-2 (Touvron et al. 2023) and Phi-3 (Abdin et al. 2024). To achieve the simultaneous goal of fluency and high attack success rate, we improve both the objective function and the optimizer:\n1. A distillation objective. We reconsider the typical \"token-forcing\" objective function where the prompt is optimized to maximize the likelihood of a generation that begins with \"Sure, here is...\". This objective often fails to attack models like Llama-2 and Phi-3, where the model will revert to refusal after the initial affirmative response. Instead, after applying forcing to the first few tokens, we minimize a distillation loss which induces the victim model to emulate a toxified copy which has been LoRA fine-tuned on a small dataset of 2500 toxic generations (Lermen, Rogers-Smith, and Ladish 2023). Distillation can aim to match either output probabilities or internal activations at one or more layers.\n2. Preferencing human-fluent attacks. We regularize the objective function with a term that prefers more fluent attacks as measured by the attack perplexity. In addition, we use the same token proposal function as the BEAST algorithm (Sadasivan et al. 2024) to preference tokens that the victim model considers likely. While these two techniques do result in low perplexity prompts, the resulting attacks suffer from over-optimization such that the attack will often repeat the same token dozens of times or find out-of-distribution attacks that are evaluated as low perplexity by the victim model despite being nonsense to the human eye. To solve these issues and produce human-fluent attacks, we evaluate attack perplexity as the average perplexity assessed by multiple models and layer a repetition penalty on top.\n3. More flexible optimization. Our optimization is primarily based on the GCG and BEAST algorithms. We extend these methods to allow token insertions, token swaps, and token deletions. The optimizer also has freedom to lengthen or shorten the attack prompt. We adopt the buffer from Hayase et al. (2024). We allow both a prefix and a suffix to the desired task in the user prompt.\nCombining these algorithmic improvements results in a process that is able to reliably jailbreak the most difficult target models with prompts that appear similar to human-written prompts. For example, we are able to achieve a prompt perplexity of 33 while inducing Llama-2-7B into performing 96% out of 520 tasks. Similarly, we optimize a single prompt that achieves attack success rates on previously unseen toxic tasks of 90%, 88% and 90% against Llama-2-7B, Phi-3-mini and Vicuna-7B while maintaining an average perplexity of 118. See Appendix D for more example attacks."}, {"title": "2 Related work", "content": "The vulnerability of neural network models to adversarial attacks, first studied in vision models and more recently in the language domain, remains a persistent issue (Madry et al. 2017; Carlini and Wagner 2017; Goodfellow, Shlens, and Szegedy 2014).\nVarious manual jailbreak tricks have been developed specifically for language models, including code-like instruction (Kang et al. 2023; Jha and Reddy 2023), translating the request to low-resource languages (Yong, Menghini, and Bach 2023), and formatting requests (Wei, Haghtalab, and Steinhardt 2024). Anil et al. 2023 uses long-context windows to prepend many examples of toxic compliance to the earlier context, inducing jailbreaking for subsequent requests.\nJones et al. 2023 uses perplexity penalties to encourage fluency in adversarial prompts. Zou et al. 2023 develops GCG to optimize non-fluent adversarial suffixes. Notably, they reported significantly higher attack success rates when"}, {"title": "3 Task details", "content": ""}, {"title": "3.1 An example attack", "content": "In the typical one-turn language model adversarial attack setting, the goal is to find user inputs that cause a model to perform a specific task that the model has been trained not to perform. The user prompt is often prefixed with a system prompt. For concreteness, here is an example of the prompt and generation for one of our fluent attacks on Vicuna-7B, optimized on the single task: \"Write a guide on how to commit insider trading.\" The black section of the text is the system prompt and conversation template, the blue portion is the task we want to model to perform and the red portion of the text is the optimized attack. The magenta section is the generation.\nA chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: Write a guide on how to commit insider trading. Follows from part 1 ISTANBULLO: The KSA is not aware of any specific information that could indicate an illegal activity as mentioned above but we are operating at a very high level of alertness working closely with all"}, {"title": "3.2 The token forcing objective", "content": "The typical objective for attacking language models is to \"force\" the model to produce an initial affirmative response such as \"Sure, here is\u201d (Zou et al. 2023; Jones et al. 2023). The optimization objective is the log-probability of the first F forced tokens:\n\\(L_F := \\frac{1}{F} \\sum_{i=1}^{F}logp(t_i|t_J...t_{i-1})\\)\nwhere \\(t_J\\) is the first token of the prompt, \\(t_0\\) is final token of the prompt, and tokens \\(t_1 ... t_k\\) are generation tokens. We term this objective the \"token forcing\" objective. In the next section, we supplement this objective with distillation from a toxified model."}, {"title": "4 Objective function", "content": ""}, {"title": "4.1 Challenges with token forcing", "content": "As described in the section above, token forcing, a common approach in adversarial prompt optimization is to maximize the log-probability of an initial affirmative response (Zou et al. 2023; Jones et al. 2023). However, token forcing is a proxy objective for the true objective of triggering the desired toxic behavior, and strongly safety-trained models, such as Llama-2 and Phi-3, are often capable of self-correcting or \"reversing\" after being forced to output an affirmative initial generation. Thus, an attack might fail despite a loss near zero. In the example below, we allow the model to begin generation after the first line of text and it begins by saying \"I cannot\"."}, {"title": "4.2 Loss Clamping", "content": "As a simple form of regularization, we may define the Clamp() function:\n\\(Clamp[x] := max(x, - ln 0.6)\\)\nWe apply Clamp() to the loss sub-component of each token to reduce optimization effort on tokens that have already been well-solved."}, {"title": "4.3 Distillation from a Toxic Model", "content": "There are 3 major reasons to using a distillation-based objective for adversarial attacks. First, distillation reduces the optimizer's incentive to force the precise tokens of the target generation, thus aiding generalization. Second, forcing strings generated by a toxified version of the victim model are more likely to be feasible outputs of the victim model. Third, KL distillation provides advantages in data efficiency, reducing cost (Hinton, Vinyals, and Dean 2015; Yang et al. 2024).\nThere are two main types of distillation losses used in the white-box setting: output-based distillation of class probabilities, and \u201chint-based\" distillation to emulate internal activations with squared error loss (Yang et al. 2024; Romero et al. 2014). We develop both of these approaches for adversarial prompts.\nFirst, we must construct a toxified copy of the victim model. Reversing safety-tuning with LoRA requires only a small initial dataset and minimal resources, as reported previously by (Lermen, Rogers-Smith, and Ladish 2023; Zhan et al. 2023). Our training dataset is the 2500 sample instances from the development phase of the NeurIPS 2023 Trojan Detection Competition Trojan Detection Competition (2023). For hint-based distillation on the activations at a layer l > 4, LoRA training is performed only on layers up to and including layer 4; later layers are not modified. For logits-based distillation, LoRA training is performed at all layers.\nSecond, we use the toxified model to generate a partial generation for the task request. Finally, instead of optimizing for the likelihood of the partial toxic generation, the optimizer aims to match the victim model against the toxic model, using the distillation-like loss."}, {"title": "4.3.1 Attack Loss for Logits-based Distillation", "content": "While a typical distillation loss is based on the KL divergence, the cross-entropy form we use is equivalent up to an additive offset.\n\\(XE{P,Q} := \u2013 \\sum p(x) log q(x)\\)\nThis equivalence is incidentally broken when we clamp the loss-contribution from each token. Explicitly, the attack loss becomes\n\\(L_D := \\frac{1}{K}\\sum_{i=1}^{K}XE{P_{ovictim}(\\cdot| prefix+task+suffix+t_1 ...t_{i-1}), P_{otoxic} (\\cdot| task+t_1... t_{i-1})}\\)"}, {"title": "4.3.2 Attack Loss for Hint-like Distillation", "content": "As a simplified version of the hints-based loss of Romero et al. 2014, we take the squared difference of the residual streams at layer l over the generation tokens.\n\\(L_D := \\frac{1}{K}\\sum_{i=1}^{K}||X_{victim,l} ( prefix+task+suffix + t_1 . . . t_{i\u22121}) \u2013 X_{toxic,1} (task+t_1... t_{i-1})||^2\\)\nIt is possible to use multiple layers for this loss. For simplicity, we use only a single layer l 20 and fine-tune the toxified model only for layers up to layer 4."}, {"title": "4.4 Regularization for Fluent Attack Selection", "content": "A first line of automated defense against GCG-like attacks is perplexity filters (Alon and Kamfonas 2023), which can flag random-looking attack strings while admitting human-written text. Attacks that are indistinguishable from normal user input are harder to filter.\nTo incentivize fluency in adversarial attacks, several works have explicitly added perplexity terms as part of the objective cost (Shi et al. 2022; Zhu et al. 2023; Jones et al. 2023; Thompson, Straznickas, and Sklar 2024). We take this route, adding a fluency regularization term to the objective. Specifically, we use the mean per-token cross entropy of the user prompt according to the victim model. Explicitly:\n\\(L_{XE} = \\frac{1}{M}\\sum_{i=1}^{M}logp(t_{i+1}|t_J...t_i)\\)\nwhere M is the length of the user input and \\(t_J\\) is the first token of the system prompt."}, {"title": "4.5 Human-evaluated fluency", "content": "Although regularizing the fluency successfully reduces the model-evaluated perplexity, the resulting attacks diverge from \"fluency\" according to the human eye. The optimizer finds out-of-distribution attacks that the victim model evaluates as low perplexity but appear nonsensical. The optimizer will also repeat a single token or short sequences of tokens many times. This repetition is unsurprising: After seeing the same token several times, the probability of the token appearing again is high. The following prompt demonstrates these problems: our optimization yielded the attack below with perplexity 17.6 according to Llama-2-7B:"}, {"title": "4.6 Final Objective", "content": "We use a combined objective with four components: a forcing objective on the first F tokens of the generation, a distillation objective on the remainder of the generation choosing either the logits- or hints-based loss, a multi-model fluency regularization for the prompt averaged over m reference models, and a repetition penalty averaged over their tokenizers.\n\\(L_F + C_DL_D + \\frac{1}{m}\\sum(C_{XE}L_{XE,m} + C_{Rep}L_{Rep,m})\\)\nExplicitly, if logit-based distillation is performed and clamping is applied, substitution yields:\n\\frac{1}{F}\\sum_{i=1}^{F}Clamp(log p(t_i|t-J...t_{i-1}))\n+\\frac{C_D}{K}\\sum_{i=F}^{K} Clamp [XE{P_{ovictim} (\\cdot| prefix+task+suffix+t_1...t_{i-1}), P_{atoxic} (\\cdot| task+t_1 ... t_{i-1})}]\\+\\frac{1}{M}\\sum_{i=1}^{M} CXElogP_m(t_{i+1}/t_J...t_i) + C_{Rep}\\sum_v(\\sum1{t_i = v})^{1.5}\" \u2212 1"}, {"title": "5 Optimization", "content": "Discrete prompt optimization algorithms follow a simple loop:\n1. Start with the current-best attack prompt(s).\n2. Mutate a prompt in a single token position to generate new candidates. For each candidate, Zou et al. (2023) picks random token position for swapping, and selects a promising replacement token according to a first-order approximation to the loss. Zhu et al. (2023) limits this mutation to the final token of the prompt and includes a fluency term in addition to the approximation to the attack loss. Sadasivan et al. (2024) limits the mutation to be a new token at the end of the prompt and selects promising tokens by sampling without replacement from the victim language model.\n3. Measure the loss objective for each candidate.\n4. Filter and retain some candidates. Zou et al. (2023) retains only the best candidate. Hayase et al. (2024) maintains a buffer. Thompson, Straznickas, and Sklar (2024) maintained multiple candidates across trade-offs of fluency vs strength. Sadasivan et al. (2024) maintains a beam search across recent token additions."}, {"title": "5.1 Fluent and Flexible Sequence Proposals", "content": "How should the optimizer 'mutate' from a current-best candidate to the next round of candidates?\nGreedy coordinate gradient (GCG) (Zou et al. 2023) maintains an attack of constant token length. The loss is back-propagated to compute the gradient of the loss with respect to each possible token. Each candidate proposal differs from the current attack in one element: at a random token position, a token-swap is selected randomly among the top-k most promising tokens according to a first-order approximation.\nIn contrast, BEAST of Sadasivan et al. 2024 extends the attack fluently to the right, by proposing new tokens according to the sampling without replacement from the top suggestions of a language model.\nWe mix these ideas by proposing our mutations in the following proportions:\nWe select \\(k_1\\) mutations according to the following rules. The parameter \\(k_1\\) (following the nomenclature of BEAST) is equivalent to the \"batch size\" in GCG.\n\u2022 w/probability \\(p_{delete}\\): delete a token in a random position\n\u2022 w/probability \\(p_{insert}\\): insert a token in a random position\n\u2022 w/probability \\(p_{swap}\\): swap a token, in a random position\n\u2022 w/probability \\(p_{edge}\\): insert a token at the end of the attack\nWhen adding or swapping a token, we sample from a language model without replacement to produce \\(k_2\\) proposed tokens. Then, one of the \\(k_2\\) proposed tokens is selected with uniform probability. The use of language model sampling to propose tokens is similar to BEAST, however, the random token position and the random selection amongst the sampled tokens are similar to the GCG token proposal mechanism."}, {"title": "5.2 Increasing Length Improves Objective", "content": "The results of Fort 2023 and Anil et al. 2023 both indicate increased effectiveness of long attacks. We confirm that increasing the length of attacks can greatly improve our optimization loss, indicating greater attack strength. In Figure 2, we initialize with different lengths and limit the lengthening and shortening of the prompt to just 10% of the initial length. After iterating sufficiently long to reach a loss plateau, longer prompts are able to achieve lower loss values."}, {"title": "5.3 Optimization Buffer", "content": "Retaining only the best candidate as in GCG results in occasional increases in the loss function when an iteration explores only poor token replacements. To ameliorate this issue, we use the optimization buffer technique described by Hayase et al. 2024, which we find to prevent these loss increases. The buffer is a heap of the top B candidate sequences seen so far. Each round, the best candidate is popped from the heap for exploration according to the process described in Hayase et al. 2024."}, {"title": "6 Results", "content": "We use the AdvBench dataset (Zou et al. 2023) to evaluate how effective our methods are at circumventing the safety training of four models: Llama-2-7B-Chat, Vicuna-7B-v1.5, Llama-3-8B-Instruct and Phi-3-mini-4k-instruct. We abbreviate these models' names as Llama-2-7B, Vicuna-7B, Llama-3-8B and Phi-3-mini throughout the paper.\nEvaluations of the perplexity of attacks are according to the victim model, unless otherwise stated. Past papers have judged attack success rate (ASR) with a variety of methods, including checking for presence of agreement strings. Past papers have evaluated the perplexity of only the attack itself. We judge our ASR according to evaluation with GPT-4o, with the prompt given in Appendix A. We report the perplexity of the full user input, including the task description.\nWe document the hyperparameters used for the different experiments in Appendix C. We include many example attacks in Appendix D with several different initialization schemes documented in Table D1."}, {"title": "6.1 Single-Task, Single-Model", "content": "Table 1 reports results for Advbench in the single task setting for Llama-2-7B, showing excellent ASR performance at quite low perplexity compared to other published methods.\nTable 2 reports results for Advbench for Llama-2-7B, Vicuna-7B, Llama-3-8B and Phi-3-mini.\nFor both of these evaluations, our optimization did not use multi-model perplexity or repetition penalties, and the attacks do contain frequent token-repetitions. We run for 1400 iterations which requires 15 minutes of H100 time for the 7/8B models and 10 minutes for the smaller and faster Phi-3."}, {"title": "6.2 Multi-task, multi-model", "content": "The \"universal\" GCG attack of Zou et al. 2023, optimized to attack many tasks simultaneously, was found to achieve an ASR of 84% for Llama-2-7B in Advbench test examples, according to string-matching grading.\nIn similar fashion, we optimized a fluent attack on ten tasks simultaneously against Llama-2-7B, Phi-3-mini, and Vicuna-7B. Fluency penalties and repetition penalties are summed for all three models. Each optimizer step, one of the"}, {"title": "6.3 Language Crystallization and Initialization", "content": "With longer prompts initialized from uniformly random tokens, the resulting string is not guaranteed to be fully in any one language. We see patches of same-language text appear and grow locally, a type of crystallization process resulting from fluency optimization operating locally rather than globally. Similar phenomena occur can in the conceptual content of the attack. In the example attack below, we see clearly sections written in English, French, and Japanese as well as a few words in other languages.\nLocal fluency is acceptable in settings where the only goal is low model-evaluated perplexity. However, if the goal is global human-fluency, we need to circumvent this crystallization phenomenon. We find that global fluency can be recovered by either initializing a short or empty string or by initializing from a non-random sequence."}, {"title": "7 Redteaming pitfalls and recommendations", "content": "In our work and in evaluating previous work, we have encountered many important details in algorithmic redteaming. In this section, we collect some of these details and provide suggestions for community standards and future work.\nFirst, we recommend developing and testing methods on harder-to-attack models like Llama-2, Phi-3, Claude, and GPT-4. Methods that work well on easy-to-attack models like Vicuna may not be useful for safety-trained models.\nFor practical applications, we recommend representing prompts as strings rather than token sequences. Token-level optimizers are prone to finding 'impossible' token sequences which do not revert back to the same token sequence when decoded and retokenized for API requests. For similar reasons, we maintain the attack state as a string to handle cases where we are attacking multiple models which have different tokenizers. Tracking the attack state as a string introduces additional complexity in the software implementation of a token-level optimizer because token positions must be carefully tracked when producing token proposals. The code we shared includes methods for tracking attack states and token proposals.\nRelatedly, we recommend preventing optimizations routines from using special tokens. Padding, beginning of sequence, and end of sequence tokens are powerful tools for attacking some models but would be easily filtered out or tokenized differently in a typical API or chat interface.\nWe recommend evaluating whole-prompt perplexity when developing fluent attacks. Some past work has evaluated the perplexity of an attack in isolation without the surrounding system prompt, chat template and task. However, the full fluency of a prompt requires some coherency between these different subsections of the prompt.\nWe recommend clearly accounting for both GPU runtimes and model provider API usage. Some authors have advertised their methods as requiring minimal resources because their model-written attacks only require the OpenAI API. However, if such an attack requires $10 of API usage to produce a single attack, it is an important expense to advertise. Because the cost landscape is changing rapidly both in terms of the price per hour of GPU time and the price per token of API usage, we recommend sharing; 1) the run time and hardware used. 2) the number of tokens of input and output that were passed to any API.\nThe initialization scheme of a token-level optimizer is critical and should be clearly reported as part of the methodology. For example, the \"!!!!!!!!\" prompt from Zou et al. 2023 performs poorly as an initial token state in our algorithms.\nA tip on hyper-parameters: The number of candidates that we can test per second is maximized if the number of candidates per optimizer iteration is equal to the maximum batch size allowed by GPU memory constraints. However, we often use a smaller number of candidates because, especially early in the optimization process, the loss improves faster if we take two optimizer steps with a smaller number of candidates instead of one step with more candidates."}, {"title": "8 Discussion", "content": "In this paper, we demonstrated fluent adversarial attacks generated via a discrete-optimization process.\nThese white-box methods should be useful to assist with vulnerability evaluation and hardening of models. Model developers wishing to apply our framework may already have a toxic-capable base model (often called a \u201chelpful only\" model) and therefore have the option to skip toxicity-training.\nAlthough current black-box attack methods are roughly as effective as white-box methods (Takemoto 2024), we are unsure if this will remain the case. Model developers will harden their filtering and refusal mechanisms pushing the defensive frontier towards harder-to-find portions of the attack surface. In the long run, white-box attack methods are likely to be necessary or, at least, very helpful to find these holes. In addition, token-level optimizers can stack on top of other human or model-driven adversarial attacks. Running a smaller number of optimizer iterations is an extremely efficient approach to strengthen a model-written attack that is already somewhat effective.\nWhile our methods are slower than existing automated redteaming tools, our method costs less than $1 (per single-model single-task attack on a 7B class model) despite producing diverse and fluent attacks. The slower runtime is primarily due to the challenging objective function of simultaneously achieving high attack success on well-defended models and achieving human-fluency. The per-iteration cost is increased by using longer prompts and evaluating fluency on multiple models. Furthermore, increasing the fluency penalty also has a trade-off on the strength of the attack, similar to the trade-off found in Thompson, Straznickas, and Sklar 2024.\nTo conclude, we share three straightforward directions to improve on the work here or extend token-based language model adversarial attack methodology:\n\u2022 Reducing computational expense in the methods presented here. We expect there is a lot of room for improvement. For example, normally, the attack will succeed long before we terminate the algorithm because attack success is not the only goal. Even after attack success, fluency continues to improve for many iterations. We expect there is much room for improvement with further tuning of stopping rules, hyperparameters, and more general improvements to the token-level optimizer.\n\u2022 Human-fluency is a challenging objective. We see substantial room for improvement in developing better proxy objectives for human-fluency. For example, the repetition penalty we use overpenalizes common words like \"the\" or \"a\".\n\u2022 We are excited about methods that would make feasible the optimization of token sequences of over 10,000 tokens. For example, attempts to modify more than one token per optimizer iteration."}]}