{"title": "TabKANet: Tabular Data Modelling with Kolmogorov-Arnold Network and Transformer", "authors": ["Weihao Gao", "Zheng Gong", "Zhuo Deng", "Fuju Rong", "Chucheng Chen", "Lan Ma"], "abstract": "Tabular data is the most common type of data in real-life scenarios. In this study, we propose a method based on the TabKANet architecture, which utilizes the Kolmogorov-Arnold network to encode numerical features and merge them with categorical features, enabling unified modeling of tabular data on the Transformer architecture. This model demonstrates outstanding performance in six widely used binary classification tasks, suggesting that TabKANet has the potential to become a standard approach for tabular modeling, surpassing traditional neural networks. Furthermore, this research reveals the significant advantages of the Kolmogorov-Arnold network in encoding numerical features. The code of our work is available at https://github.com/tsinghuamedgao20/TabKANet.", "sections": [{"title": "1. Introduction", "content": "The tabular dataset is data organized into rows and columns, consisting of typically continuous, categorical, or ordered different features. It is the most commonly used and oldest data management model in building real-world businesses. At the same time, as the foundational data storage structure of relational databases, tabular data has widespread applications in almost any field, including medicine [12, 25], finance [2, 6], e-commerce [21, 24], and more [3, 4].\nDespite recent advancements in neural network architectures for tabular data modeling, many still believe that the state-of-the-art techniques for performing tasks in tabular data, such as classification or regression, are based on tree ensemble methods, like Gradient Boosted Decision Trees (GBDT) [1,5,9,23]. This perspective is rooted in the observation that tree-based ensemble models often exhibit competitive predictive accuracy, fast training speeds, and ease of interpretation. This stands in stark contrast to widely researched fields of AI such as computer vision and natural language understanding, where neural networks, particularly deep learning algorithms, have significantly outperformed competing machine learning methods [?, 13, 26].\nAlthough there is an ongoing debate about whether neural networks typically outperform GBDTs on tabular data [20]. We must recognize that there are the distinct advantages of neural networks in tabular data modeling:\n\u2022 Potential for modeling large-scale complex tabular structures: Deep learning models often demonstrate superior performance in this regard.\n\u2022 Better scalability at both the input and output ends: This provides a foundation for multimodal applications.\n\u2022 Existing self-supervised learning and pre-training schemes for tabular data: These offer greater application potential for neural networks in tabular modeling.\nTherefore, modeling tabular data with neural networks is worth studying. There are attempts to utilize advanced neural networks like Transformer in tabular data modeling. More specifically, they have used Transformers to encode categorical items in tabular data [11], which we believe is insufficient for they did not serve continuous numerical items in the same manner. In real-world tables, especially in fields like medicine, finance, and industry, numerical items in tabular data are crucially important. If only the categorical parts are encoded using Transformers while continuous numerical items are simply fused through concatenation, it will lead to the unequal weighting of column items in model perception. We need a sensible tool to achieve vector mapping of continuous numerical items at a comparable scale, while also ensuring sufficient sensitivity to numerical distinctions.\nRecently, the introduction of Kolmogorov-Arnold Networks (KANs) [18] has garnered significant attention from the machine learning communities, quickly piquing the interest of researchers and practitioners. A key feature of the KAN is its ability to approximate functions of arbitrary complexity by selecting appropriate activation functions and parameters. This feature offers neural networks"}, {"title": "2. Related Work", "content": "2.1. Tabular Data Modelling with Neural Network\nTabular data is the primary format of data in real-world machine learning applications. Until recently, these applications were mainly addressed using classical decision tree models, such as GBDT. However, with the development of deep neural networks, the performance gap between neural networks and traditional GBDT models in table data tasks has begun to narrow.\nRecent advancements, exemplified by TabTransformer, have integrated transformers into tabular modeling approaches [11, 26]. TabNet employs a sequential attention mechanism to identify semantically significant feature subsets for processing at each decision point, drawing insights from decision tree methodologies [1]. TabPFN introduces a transformer architecture for in-context learning (ICL) to approximate Bayesian inference through pre-training, facilitating swift resolutions for small tabular classification tasks without necessitating hyperparameter adjustments [9].\nIn addition, there are some studies exploring the application of neural networks in other areas of tabular data, including using few-shot training and assessing the performance of ensemble large language models (LLMs) [8], as well as work on applying generative models for improving tabular task performance [14, 16, 17].\n2.2. Kolmogorov-Arnold Network\nMultilayer Perceptrons (MLPs) have long been the fundamental component for constructing neural networks. They feature a fully connected architecture that can approximate complex functions and possess strong expressive power, making them widely popular in various applications such as image recognition and natural language processing. However, despite their popularity, MLP architectures have some drawbacks. For instance, the activation functions are fixed, and to some extent, this rigidity in the network may limit the model's flexibility in capturing complex relationships within the data, as it relies on predefined nonlinear fitting schemes.\nLiu et al. first introduced Kolmogorov-Arnold Networks (KANs) as a promising alternative to Multilayer Perceptrons (MLPs) to address the aforementioned limitations. Their research differs from previous studies in that it recognizes the similarity between MLPs and networks that employ the Kolmogorov-Arnold theorem, which they refer to as Kolmogorov-Arnold Networks (KANs).\nA notable feature of KANs is the absence of traditional neural network weights. In KANs, each \"weight\" is actually represented as a small function. Unlike traditional neural networks where nodes apply fixed nonlinear activation functions, each edge in a KAN is characterized by a learnable activation function. This architectural paradigm allows KANs to be more flexible and adaptive than traditional methods, potentially enabling them to model complex relationships within the data.\nWith the introduction of KAN networks, researchers have been exploring the application of KAN networks to better address scientific problems, including Time Series Forecasting, medical image segmentation, hyperspectral image classification, and more [7, 15, 19]. Previous work has also proposed using KAN networks for modeling tabular data [22]. However, their approach involved using the entire KAN model for tabular data classification without distinguishing between categorical and numerical features, and without incorporating transformer architectures. This lack of differentiation and utilization of data elements, along with the absence of transformer architecture, has resulted in their work not fully showcasing the potential of KAN models in tabular data modeling."}, {"title": "3. The TabKANet", "content": "Our cutting-edge creation, TabKANet, stands as a pioneering tabular data modeling marvel seamlessly fusing KAN (Knowledge-Aware Network) and the formidable Transformer architecture. Behold its intricate blueprint depicted in the illustrious Figure 1.\nOur model, TabKANet, is a tabular data modeling solution that integrates KAN and Transformer. Its specific architecture is shown in Figure 1.\nFor the vast majority of tables, especially in the fields of science, finance, and healthcare, their table data inevitably contains both continuous numerical and categorical items. For a table with m category items and n numerical items at the same time. We will handle them separately.\nFor category items, we first encode all categories for each column, such as One Hot Encoding or Label Encoding. After encoding, the category features change from sparse expressions to dense vectors with a dimension d. For each category feature, there will be a vector of size d to represent its information.\nIn order to better represent the information in the table, we hope that each continuous numerical information can also be expressed using features of the same size and data volume d. The KAN model provides us with a great tool. Compared to MLP, it has more sensitive architectural characteristics and can better extract potential information from numerical features. Inspired by this, we first normalize the n continuous numerical features in the table through a layer normalization, and then map the n numerical information into n x d features by constructing a KAN module with embedding dimension d.\nFor a table containing m category items and n numerical items, we obtained a matrix of (m + n) \u00d7 d, which will be passed through the Transformer layers to fully utilize its self-attention mechanism. Finally, we obtain the prediction results of the table modeling task through the MLP layers.\nThis approach leverages the KAN network's robust feature extraction capabilities for continuous numerical values. Additionally, since we cannot pre-determine the contribution weights of each column in tabular data, a pragmatic solution is to construct uniform representations for each column's features. This allows for the development of a unified downstream model for effective learning. Employing a consistent Transformer architecture for learning after encoding the column features also offers a highly scalable framework for tabular data modeling, enhancing the potential for multimodal business applications."}, {"title": "4. Experiments", "content": "4.1. Datasets\nThis study employed 6 commonly used tabular modeling datasets, as outlined in Table 1. Each dataset consists of a minimum of 4 numerical features.\nWe employ 5-fold cross-validation on all datasets, using a 6:2:2 split for training, validation, and testing sets. We use the AUC (Area Under the Curve) as the evaluation metric.\n4.2. Baseline Models\nAs a comparison, we constructed a basic MLP model, the TabTransformer [11], and the XGBoost algorithm based on Gradient Boosting Decision Trees [5]. The baseline MLP model also utilized separate encoding for numerical and categorical features during modeling.\n4.3. Implementation Details\nAll encodings of categorical features in this study are implemented through Label Encoding. The XGBoost classifier is constructed using the XGBoost library in Python, with the maximum depth set to 8, the learning rate to 0.1, and the number of trees to 1000. For MLP, TabTransformer, and TabKANet, the learning rate is set to le-3, using the AdamW optimizer. All Transformer parts are configured with an embedding dimension of 64, the number of heads to 8, and the number of layers to 3. The KAN network in TabKANet includes two hidden layers, with the number of input features to the hidden layers being 64 and the embedding dimension multiplied by the number of numerical items, respectively. The evaluation metric for all models is AUC. All experiments were conducted on PyTorch.\n4.4. Results\nTable 2 shows the performance comparison between TabKANet and the comparison methods on 6 different datasets.\nThe exciting results show that in these 5 public datasets, TabKANet, as an improved model structure specifically designed with KAN and Transformer, achieved the best performance compared to neural network models across all datasets. Notably, in the online shoppers dataset, there was a significant difference between the baseline neural network models (MLP or TabTransformer) and XGBoost. At the same time, TabKANet demonstrated a substantial improvement, achieving a 22.5% increase compared to TabTransformer and coming very close to XGBoost result. Among the 17 features in the online shoppers dataset, six are continuous numerical features: Administrative Duration, Informational Duration, ProductRelated Duration, Bounce Rates, Exit Rates, and Page Values. Our method greatly enhances the performance shortcomings of the neural network models, especially for tabular datasets where the feature extraction capabilities of neural network fall significantly behind GBDT due to the limitations in numerical features."}, {"title": "5. Conclusions and Future Work", "content": "In this work, we developed a tabular modeling framework that integrates KAN and Transformer architectures. Its excellent performance has been validated using five public tabular datasets. TabKANet features a simple structure, stable performance, and ease of implementation. Our experiments highlight the KAN structure's capacity to learn from continuous numerical features in tabular data. By effectively processing categorical and numerical information, TabKANet paves the way for constructing more complex multimodal architectures, such as those integrating visual or language models. Our work will serve as a foundational framework for table modeling, standardizing the embedding scheme for different features atop the Transformer.\nLooking ahead, we plan to focus on the following three aspects:\n(1) Conduct experiments on a broader array of tabular datasets.\n(2) Explore the robustness of the TabKANet model through experiments involving noise, missing values, and semi-supervised learning.\n(3) In our implemented structure, the table data achieves a unified representation of category and numerical features before entering the Transformer, which provides the possibility for table modeling based on distillation or reinforcement learning."}, {"title": "6. Appendix", "content": ""}]}