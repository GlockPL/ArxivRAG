{"title": "DISENTANGLING EXPLORATION OF LARGE LANGUAGE MODELS BY OPTIMAL EXPLOITATION", "authors": ["Tim Grams", "Patrick Betz", "Christian Bartelt"], "abstract": "Exploration is a crucial skill for self-improvement and open-ended problem-\nsolving. However, it remains uncertain whether large language models can ef-\nfectively explore the state-space. Existing evaluations predominantly focus on\nthe trade-off between exploration and exploitation, often assessed in multi-armed\nbandit problems. In contrast, this work isolates exploration as the sole objective,\ntasking the agent with delivering information that enhances future returns. For\nthe evaluation, we propose to decompose missing rewards into exploration and\nexploitation components by measuring the optimal achievable return for the states\nalready explored. Our experiments with various LLMs reveal that most models\nstruggle to sufficiently explore the state-space and that weak exploration is insuf-\nficient. We observe a positive correlation between model size and exploration per-\nformance, with larger models demonstrating superior capabilities. Furthermore,\nwe show that our decomposition provides insights into differences in behaviors\ndriven by agent instructions during prompt engineering, offering a valuable tool\nfor refining LLM performance in exploratory tasks.", "sections": [{"title": "1 INTRODUCTION", "content": "Large Language Models as decision making agents with a good inductive bias. Recently,\nlarge language models (LLMs) have demonstrated promising results in various decision making\ntasks such as web browsing (Yao et al., 2022; Shinn et al., 2024; Ma et al., 2023), game-playing\n(Paglieri et al., 2024), and tasks in simulated households (Yao et al., 2022; Shinn et al., 2024). This\nway, LLMs act as agents that observe states and take actions in different environments. Through\ntheir vast internal knowledge-base and autoregressive in-context reasoning capabilities, the models\nare supposed to quickly adapt to new tasks. However, previous work has shown that LLMs struggle\nwith solving increasingly complex environments due to several limitations: For example, the ability\nto learn from mistakes is often limited (Huang et al., 2023) and LLMs have difficulties with planning\nover long horizons (Kambhampati et al., 2024). The examples emphasize that understanding LLM\nabilities is essential for their risk assessment in real life applications, and future development.\nIntelligent exploration is essential for self-improvement. For our human nature, intelligent\nexploration is an essential skill for improvement in many tasks. Exploration is a critical process for\nsystematically seeking new information or testing novel actions to reduce uncertainty and improve\ndecision-making over time. Traditional approaches often rely on stochastic noise (Mnih, 2013;\nLillicrap, 2015) or reward-shaping (Bellemare et al., 2016). Such methods induce redundancy and\ndiffer from human decision making which is mainly driven by reasoning about current information,\npast steps, and the unknown potential of yet unknown actions. Therefore, recent works proposed\nseveral approaches towards more intelligent exploration (Huang et al., 2024; Lu et al., 2024; Nie\net al., 2024). Hereby, due to their emergent abilities (Wei et al., 2022), LLMs could have the potential\nto revolutionize discovery by identifying patterns, leveraging prior knowledge, and intelligently\nnavigating the state space. LLMs may transform exploration into a targeted and efficient process.\nLimited focus on exploration as an independent capability. While exploration is acknowledged\nas a crucial component for learning and decision making, prior work has predominantly investi-\ngated it in conjunction with exploitation (Krishnamurthy et al., 2024; Nie et al., 2024; Paglieri et al.,"}, {"title": "2 RELATED WORK", "content": "Recently, exploring the capabilities of LLMs gained significant attention and has been investigated\nfrom different perspectives. Prior works include, but are not limited to, planning (Kambhampati\net al., 2024; Song et al., 2023; Valmeekam et al., 2023), world models (Hao et al., 2023; Guan\net al., 2023), logical- (Xu et al., 2023; Parmar et al., 2024), commonsense- (Li et al., 2021), and\nmathematical reasoning (Imani et al., 2023; Yuan et al., 2023). Our research focuses on intelligent\nexploration of the state-space in sequential decision making. Generally, we summarize existing\nmethods into three streams of work:\nLarge Language Models in decision making. A broad line of works has applied LLMs in\ndecision making. Relatively few specifically addressed the challenge of exploration. The majority\nof existing research focuses on multi-armed bandit problems (Park et al., 2024; Krishnamurthy et al.,\n2024; Nie et al., 2024). For example, (Krishnamurthy et al., 2024) investigate scenarios involving\nmultiple buttons with underlying stochastic rewards. The LLM is presented past actions with their\nrewards and must maximize its return over multiple trials. However, multi-armed bandits lack the"}, {"title": "3 EVALUATING LARGE LANGUAGE MODELS FOR EXPLORATION", "content": "In the following section, we introduce our evaluation approach and environment tailored for LLM\nexploration capabilities in sequential decision-making. Importantly, our method is environment-\nagnostic and independent of specific prompts. Future research can extend this framework to facilitate\ncomparisons of agent performance across diverse environments and contexts."}, {"title": "3.1 THEORETICAL FRAMEWORK", "content": "Deterministic Markov Decision Process We study a deterministic Markov decision process\n$(S, A, P, r, \\gamma)$ where $S$ is a finite state space, $A$ is the finite action space, $P : S \\times A \\times S \\rightarrow [0, 1]$\ndefines the transition dynamics of the environment, $r : S \\times A \\rightarrow \\mathbb{R}$ is the reward function and $\\gamma$ the\ndiscount factor. At each time step $t$, an agent samples an action from policy $\\pi : S \\rightarrow A$ based on\nthe current observation $s_t \\in S$ and executes it in the environment. The environment transitions and\nthe agent receives a reward $r_t$. In an episodic MDP, the procedure repeats until a terminal state $s_T$ is\nreached or the maximum number of steps exceeds. The sequence of state, action, and reward triples\nuntil $s_t$ is called a trajectory and describes an episode $T_i = (s_0, a_0, r_0, s_1, a_1, r_1, s_2, . . . )$. In our set-\nting, the agent has access to the current and history of prior trajectories $h_i = (T_1, ..., T_i)$ in episode\n$i$. The goal is to find the sequence of actions maximizing the cumulative return $R_i = \\sum_{t=1}^{T} r_t$.\nExploitation versus Exploration. The cumulative return $R_{LLM}$ of an agent is influenced by\nits ability to balance exploitation and exploration. Exploitation involves leveraging past high-\nrewarding actions to maximize immediate returns based on the agent's current knowledge. In con-\ntrast, exploration emphasizes interacting with the environment to gather new information, which"}, {"title": "3.2 ENVIRONMENT", "content": "Decision making in room worlds. We employ grids of interconnected rooms, inspired by tra-\nditional grid worlds commonly used in RL. We use a higher-level action space that is conceptually\nsimilar to TextWorld (C\u00f4t\u00e9 et al., 2019) environments and described in appendix A.3. However, our\ndomain features only minimal semantic associations. Each grid has a fixed layout and starting point.\nEpisodes terminate either when the agent collects three objects or exceeds a maximum number of\ndoor traversals. This number is calibrated to the distance between the agent's starting position and\nthe furthest room in the environment, ensuring that the exploration budget appropriately scales with\nthe complexity of the environment. At the beginning of each episode, the agent is reset to its starting\nposition, requiring it to strategically plan exploration over extended horizons."}, {"title": "4 EXPERIMENTAL EVALUATION", "content": "In the following section, we describe our experimental setup and results. The experiments aim to\ndemonstrate the utility of our evaluation framework by addressing the following research questions:\n1.) Can LLMs sufficiently explore and reduce their exploration gap? and 2.) How do agent instruc-\ntions impact the exploration and exploitation gaps?"}, {"title": "4.1 OTHER STATISTICS", "content": "Besides our optimal exploitation framework, defined in section 3.1, we describe exploration using\nthe following statistics:\nAgent Return represents the total return collected by the exploration agent during its interactions\nwith the environment. Although it is not a direct indicator of effective exploration, this metric\nreflects the agent's ability to engage in ways that yield cumulative benefits. A higher agent return\nindicates that the agent is navigating towards rewarding states and is possibly close to exploitation.\nContrary to our decomposition, it does not necessarily imply that the agent is efficiently exploring.\nState-Space Coverage reflects the agent's ability to discover new information and avoid being\ntrapped in local optima. In our task, effective exploration requires covering the entire state space, as\nballs associated with high rewards are spread randomly on the grid. However, complete exploration\nmay not always be necessary or infeasible in large, infinite, or continuous spaces due to natural\nlimitations and resource constraints. In contrast to state-space coverage, our decomposition also\ncovers cases in which full exploration is not essential.\nMemory Redundancy measures the percentage of overlapping state-action pairs in the memory.\nThe objective should be to minimize unnecessary repetition. In our deterministic environments, the\nagent should avoid revisiting states or actions as they do not provide new information. Reducing\nredundancy conserves resources and ensures that the history can be utilized efficiently. Otherwise,\ninformation has to be retrieved from longer histories to find novel or informative states to explore."}, {"title": "4.2 SETUP", "content": "Our experimental setup\u00b9 directs agents to explore in order to solve tasks. The specific instruction\ntells to \"collect information that helps to become better at maximizing the return\". We reference this\nprompt as task-oriented exploration. The exact template is detailed in Appendix 1. Each agent is\nequipped with a simple memory system that stores the history of past interactions. Notably, unlike\nmany prior studies, our approach does not provide agents with explicit hints about the domain.\nFor the evaluation, we test each open-source model with ten repeated runs and each closed model\nwith five repeated runs on every environment. This results in a total of 30 runs per data point for\nopen-source models and 15 runs for closed models. In the following figures, we report statistical\nsignificance using the standard error. Our return decomposition is measured at the end of exploration\nand as the mean over all episodes. All other statistics are evaluated after all environment interactions.\nComprehensive results for all models are included in Appendix B.\nTo investigate exploration behavior (1), we evaluate agents in Dark Treasure Rooms of varying\nsizes. To assess the impact of agent instructions (2), we compare our prompt, outlined above, with\nalternative instructions by incorporating soft lower and upper bounds for the exploration capability\nof each model. These alternative prompts direct the agent to strictly exploit or aimlessly explore\nwithout referencing the task. The instruction impact is analyzed using 7x7 grid environments, with\ndetailed prompt specifications available in Appendix B.5."}, {"title": "4.3 MODELS", "content": "We evaluate and compare the exploration behavior of the popular open-source LLMs Mistral (7B)\n(Jiang et al., 2023), Gemma2 (9B and 27B) (Team et al., 2024b), Llama 3.1 (7B) (Dubey et al.,\n2024), Llama3.3 (70B), Mistral Small (22B) (AI, 2024a), and Mistral Large 2 (123B) (AI, 2024b).\nWe further include closed-models GPT-40 (Achiam et al., 2023), 01-mini (OpenAI, 2024), Gemini\n1.5 Pro (Team et al., 2024a), and Claude 3.5 Sonnet (Anthrophic, 2024). We compare the LLMs\nwith a random baseline which we consider the alternative for exploration without prior information."}, {"title": "4.4 CAN LARGE LANGUAGE MODELS SUFFICIENTLY EXPLORE?", "content": "Most agents fail to minimize their exploration gap and explore the entire state-space. Our\nresults indicate that LLMs generally struggle to systematically explore the environment, even when\ntheir sole objective is to collect information rather than exploit it. The results, averaged across all\ngrid sizes, are presented in table 1. In their final episodes, most models demonstrated significantly\nlarger or indifferent exploration gaps compared to a random baseline, with the exceptions of Mistral\nLarge, o1-mini, and Gemini 1.5 Pro. However, even these models failed to achieve complete state-\nspace coverage, exhibiting non-zero exploration gaps and exploring less than 90% of the state space\non average. At closer inspection, all models showed redundant exploration of previously visited\nstates, with exploration disproportionately focused on initial high-reward regions.\nLLM-based agents are limited in targeted long-horizon exploration. Our evaluation reveals a\nstatistically significant decrease (p < 0.01) in the fraction of state space explored by most models\nas the grid gets larger from 4 \u00d7 4 to 7 \u00d7 7. Notably, Claude 3.5 Sonnet was the only model to maintain high coverage as the\nenvironment size increased, with no significant drop observed (p \u2265 0.05). Larger parameter models,\nincluding GPT-40, Mistral Large, and Claude 3.5 Sonnet, also maintained consistent coverage when\nthe grid size increased modestly to 5 \u00d7 5 (p \u2265 0.05). Despite achieving superior results in smaller"}, {"title": "4.5 WHAT IMPACT DO AGENT INSTRUCTIONS HAVE ON EXPLORATION?", "content": "Exploration stays hard for low parameter models even when the task is ignored. Models\nwith 9 billion parameters maintain significant exploration gaps even when explicitly prompted\nto disregard the task and focus solely on exploring unvisited states. Interestingly, also\n01-mini achieves only 82.05% coverage.\nPrompts can heavily impact reward collection during exploration. Our results indicate that\nprompts can substantially impact reward collection strategies during exploration. Across all LLMs,\nthe exploitation gap increased significantly when agents were prompted with the soft upper bound\ninstruction. the upper bound instruction led to more independence across\nepisodes, whereas task-oriented exploration tended to focus on previously encountered regions with\nknown rewards. These findings highlight the impact of prompt design on exploration and return.\nLLMs interpret agent instructions for exploration differently. Our results reveal that LLMs"}, {"title": "5 DISCUSSION OF RESULTS AND FUTURE IMPLICATIONS", "content": "Consistent with prior findings (Krishnamurthy et al.", "research": "nReasoning strategies. Examples, include Reflection (Shinn et al., 2024) and Tree-of-Thoughts (Yao et al., 2024)."}]}