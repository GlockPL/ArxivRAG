{"title": "Intensional Inheritance Between Concepts: An Information-Theoretic Interpretation", "authors": ["Ben Goertzel"], "abstract": "This paper addresses the problem of formalizing and quantifying the concept of \"intensional inheritance\" between two concepts. We begin by conceiving the intensional inheritance of W from F as the amount of information the proposition \"x is F \" provides about the proposition \"x is W. To flesh this out, we consider concepts F and W defined by sets of properties ${F_1, F_2,..., F_n}$ and ${W_1, W_2,..., W_m}$ with associated degrees ${d_1, d_2,..., d_n}$ and ${e_1, e_2, ..., e_m }$, respectively, where the properties may overlap. We then derive formulas for the intensional inheritance using both Shannon information theory and algorithmic information theory, incorporating interaction information among properties. We examine a special case where all properties are mutually exclusive and calculate the intensional inheritance in this case in both frameworks. We also derive expressions for P(W | F) based on the mutual information formula. Finally we consider the relationship between intensional inheritance and conventional set-theoretic \"extensional\" inheritance, concluding that in our information-theoretic framework, extensional inheritance emerges as a special case of intensional inheritance.", "sections": [{"title": "Introduction", "content": "The notion of \"inheritance\" between concepts is rich and multidimensional, with a long and diverse history, and no formalization is going to capture all the nuances. Our goal here is to present a formalization of the notion of intensional inheritance that captures enough nuance in a coherent enough way to be useful for guiding reasoning in AI and AGI systems, with OpenCog Hyperon [GBD+23] and its Probabilistic Logic Networks [GIGH08] reasoning system as the primary systems in mind."}, {"title": "Intensional Inheritance", "content": "In philosophy, broadly speaking, intension refers to the internal content or set of attributes that define a concept or term. It is contrasted with extension, which refers to the set of instances that exemplify the concept [Fit06]. For example:\n\u2022 The intension of \"triangle\" includes its defining properties, such as being a closed figure with three sides."}, {"title": "An Information-Theoretic Approach", "content": "We propose to assess the degree of intensional inheritance between W and F by asking how much information the proposition \"x is F\" provides about the proposition \"x is W\".\nFor instance if\n\u2022 W = cat\n\u2022 F = animal\nthen we ask:\n\u2022 How much information does \"x is 'animal'\" give regarding \"x is 'cat'\"\nMore precisely: We consider two concepts F and W, each defined by a set of properties:\n\u2022 Concept F: Defined by properties ${F_1, F_2, ..., F_n }$ with degrees ${d_1, d_2, ..., d_n}$.\n\u2022 Concept W: Defined by properties ${W_1, W_2, . . ., W_m}$ with degrees ${e_1, e_2, ..., e_m}$.\nThe degrees represent the probabilities or extents to which an element x possesses each property. The properties $F_i$ and $W_i$ may overlap, introducing dependencies between F and W.\nWe begin by deriving a formula for the intensional inheritance of W from F using two separate but algebraically similar approaches:"}, {"title": "Setup", "content": "We begin with the following setup:\n\u2022 Concept F: Defined by properties ${F_1, F_2,..., F_n}$.\n\u2022 Concept W: Defined by properties ${W_1, W_2,..., W_m}$.\n\u2022 Degrees $d_i$: The degree to which an element x possesses property $F_i$.\n\u2022 Degrees $e_j$: The degree to which an element x possesses property $W_j$.\n\u2022 Overlap: Some properties may be common to both F and W.\nIn this context, we will conceptualize the Intensional Inheritance of W from F as, intuitively: The amount of information the proposition \"x is F \" provides about the proposition \"x is W \".\nThis can be formalized in various ways depending on how one operationalizes the concept of \"information.\" We will explore two options here, using Shannon and algorithmic information. One could probably unify these under a more general framework, considering a broader notion of an \"information theory\" as any theory satisfying a certain set of axioms. The right way to do this seems to be to extend the use of Markov categories to model entropy [Per23], and broaden it to encompass algorithmic as well as statistical processes. However, this would be another paper in itself, and for the present purposes we'll stick with these two well-understood options."}, {"title": "Intensional Inheritance Using Shannon Information Theory", "content": "In Shannon information theory, the mutual information between F and W is defined as:\n$I(F; W) = H(W) \u2013 H(W | F) = H(F) + H(W) \u2013 H(F,W)$\nwhere:\n\u2022 H(F): Entropy of F.\n\u2022 H(W): Entropy of W.\n\u2022 H(F, W): Joint entropy of F and W.\n\u2022 H(W | F) : Conditional entropy of W given F.\nThe interaction information, next, captures the dependencies among multiple variables beyond pairwise interactions [VdC11]. It adjusts the joint entropy to account for these dependencies. For properties $F_i$ and $W_i$, the interaction information is included in the calculation of joint entropy H(F, W). The total interaction information among properties can be expressed as:\nInteraction Information = $\\sum_{\\text{all subsets}} (-1)^{|S|+1}I(S)$\nwhere I(S) is the mutual information among the properties in subset S."}, {"title": "Derivation of Intensional Inheritance", "content": "Given these quantities, we now work toward a derivation of a formula for intensional inheritance in terms of Shannon information, step by step.\nWe will derive a formula for H(F, W) based on suitable assumptions, which then leads directly to a formula for P(F, W), which is our conceptualization of intensional inheritance and our goal.\nAssuming independence among $F_i$ (which we'll relax later), the entropy of F is:\n$H(F) = -\\sum_{f}P(F = f) \\log P(F = f)$\nBut F is defined by its properties $F_i$. If the properties are independent, then:\n$P(F) = \\prod_{i=1}^{n}P(F_i)$"}, {"title": null, "content": "Similarly, the entropy H(F) can be calculated based on the individual entropies and interaction information.\nThe joint entropy H(F, W) accounts for the entropy of both F and W, including their dependencies:\n$H(F,W) = H(F) + H(W) \u2013 I(F; W)$\nBut since I (F; W) depends on the interaction among properties, we need to incorporate interaction information.\nThe mutual information I(F; W) including interaction information among properties is:\n$I(F; W) = \\sum_{i}H (F_i) + \\sum_{j}H (W_i) \u2013 H(F, W) - \\text{Interaction Information}$\nwhere:\n\u2022 $\\sum_{i} H (F_i)$ : Sum of entropies of individual properties $F_i$.\n\u2022 $\\sum_{j} H (W_j)$ : Sum of entropies of individual properties $W_j$.\n\u2022 H(F, W): Joint entropy of all properties, accounting for their dependencies.\n\u2022 Interaction Information: Adjusts for the dependencies among properties.\nTo compute I (F; W), we will proceed as follows. First, we calculate individual entropies H ($F_i$) and H ($W_j$). For each property $F_i$ and $W_j$ :\n$H (F_i) = -d_i \\log d_i \u2013 (1 \u2013 d_i) \\log (1 \u2013 d_i)$\n$H (W_j) = -e_j \\log e_j \u2013 (1 \u2013 e_j) \\log (1 \u2013 e_j)$\nThen, we calculate joint entropy H(F, W). The joint entropy involves the probabilities of all combinations of $F_i$ and $W_j$, adjusted for dependencies. Interaction information is calculated based on the dependencies among properties.\nFor instance, if certain properties are dependent or overlap, their joint probabilities differ from the product of their marginals. To compute mutual information I(F; W), we substitute the calculated values into the mutual information formula, accounting for interaction information.\nIn detail:\n$I(F; W) = H(W) \u2013 H(W | F)$\nRearranged:\n$H(W | F) = H(W) \u2013 I(F; W)$\nThe conditional entropy H(W | F) is:"}, {"title": null, "content": "$H(W | F) = \u2212\\sum_{f}P(F = f) \\sum_{w}P(W = w | F = f) \\log P(W = w | F = f)$\nAssuming uniform distribution (for simplification), this works out as follows. If W has k possible values and is uniformly distributed, H(W) = log k, then:\n$H(W | F) = \\log k \u2013 I(F; W)$\nand the average conditional probability is:\n$P(W | F) = 2^{-H(W|F)} = 2^{-(\\log k\u2212I(F;W))} = k^{-1} \\cdot 2^{I(F;W)}$\nyielding the final formula:\n$P(W | F) = P(W) \\cdot 2^{I(F;W)}$\nOf course, if there is prior knowledge violating the simplifying assumption of a uniform distribution, it should be deployed and one will obtain a different result."}, {"title": "Intensional Inheritance Using Algorithmic Information Theory", "content": "An analogous derivation may be given using algorithmic information theory [LVLV19]\nIn algorithmic information theory, the mutual information between F and W is defined as:\n$I(F : W) = K(W) \u2013 K(W | F)$\nwhere:\n\u2022\nK(W): Kolmogorov complexity of W.\n\u2022\nK(W | F): Conditional Kolmogorov complexity of W given F.\nThe interaction information among properties may be incorporated as follows:\n$K(W) = \\sum_{j=1}^{m} K (W_j) - Inter_W$\n$K(F) = \\sum_{i=1}^{n} K (F_i) - Inter_F$\n$K(F, W) = \\sum_{k=1}^{n+m} K (P_k) - Inter_{F,W}$\nwhere:"}, {"title": null, "content": "\u2022 K ($P_k$): Kolmogorov complexities of all properties.\n\u2022 InterF, Interw, InterF,W: Interaction information among properties.\nWe may compute mutual information via I(F : W) :\n$I(F : W) = K(W) \u2013 K(W | F)$;\nBut since:\n$K(W | F) = K(F, W) \u2013 K(F)$\nsubstituting we obtain:\n$I(F : W) = K(W) \u2013 [K(F, W) \u2013 K(F)] = K(W) + K(F) \u2013 K(F,W)$\nand considering the interaction terms we obtain:\n$I(F : W) = (\\sum_{i=1}^{n}K (F_i) + \\sum_{j=1}^{m}K (W_i) - \\sum_{k=1}^{n+m}K (P_k)) + (Inter_{F,W} - Inter_F - Inter_W)$\nUsing the relationship between algorithmic mutual information and conditional probability, we then calculate the algorithmic probability of W is:\n$P(W) \u2248 2^{-K(W)}$\nand similarly, the conditional probability:\n$P(W | F) \u2248 2^{-K(W|F)}$\nWe can express K(W | F) in terms of Mutual Information:\n$K(W | F) = K(W) \u2013 I(F : W)$\nand substitute back into P(W | F) :\n$P(W | F) \u2248 2^{-K(W)+I(F:W)} = P(W) \\cdot 2^{I(F:W)}$\nobtaining the final formula:\n$P(W | F) \\propto P(W) \\cdot 2^{I(F:W)}$\nwhich of course is formally very similar to the Shannon information case."}, {"title": "Special Case: Mutually Exclusive Properties", "content": "We now consider a special case where all properties $F_i$ and $W_j$ are mutually exclusive. This is a simple enough situation that we can get an explicit elementary formula."}, {"title": "Assumptions", "content": "Mutual Exclusivity:\n\u2022 $F_i \u2229 F_j = \u00d8$ for i \u2260 j.\n\u2022 $W_i \u2229 W_j = \u00d8$ for i \u2260 j.\nDegrees:\n\u2022 Each property has degree $p = \\frac{1}{s}$.\n\u2022 s is the total number of unique properties.\nOverlap:\n\u2022 There are k properties common to both F and W."}, {"title": "Calculations Using Shannon Information Theory", "content": "Total Unique Properties:\n$s=n+m-k$\nDegree of Each Property:\n$p=\\frac{1}{s}$\nProbability of F:\n$P(F) = n \\cdot p = \\frac{n}{s}$\nProbability of W :\n$P(W) = m \\cdot p = \\frac{m}{s}$\nProbability of $F \u2229 W$ :\n$P(F \u2229 W) = k \\cdot p = \\frac{k}{s}$\nConditional Probability\n$P(W | F) = \\frac{P(F \u2229 W)}{P(F)} = \\frac{\\frac{k}{s}}{\\frac{n}{s}} = \\frac{k}{n}$"}, {"title": "Calculations Using Algorithmic Information Theory", "content": "Assuming equal complexities for properties:\n\u2022 K ($F_i$) = log s\n\u2022 K ($W_j$) = log s\nand total complexities:\n\u2022 K(F) = log n\n\u2022 K(W) = log m\n\u2022 K(F \u2229 W) = log k\nwe then have\nMutual Information\n$I(F : W) = K(W) \u2013 K(W | F) = \\log m - (\\log m - \\log(\\frac{k}{n}))= \\log(\\frac{k}{n})$\nBut since P(W | F) = $\\frac{k}{n}$, we have:\n$I(F: W) = \\log P(W | F)$\nConditional Probability\n$P(W | F) = P(W) \\cdot 2^{I(F:W)} = \\frac{m}{s} \\cdot \\frac{k}{n}$"}, {"title": "Extensional Inheritance as a Special Case", "content": "Finally, we note the relation between intensional inheritance as we have formulated it and simple \"extensional inheritance\" in the sense of overlapping set membership. It is immediately obvious that our notion of intensional inheritance is broad enough to include extensional inheritance as a special case, which is convenient in terms of formal and conceptual analysis and software implementation, though different from how things have been done in commonsense reasoning systems like PLN [GIGH08] and NARS [Wan06] in the past.\nThat is, it is immediate to observe that: When properties are singleton elements (e.g., $F_1 = {x_i}$ ), the intensional inheritance as defined here reduces to extensional inheritance.\nThe conceptual relationship as envisioned here then looks like:\n\u2022 Extensional Inheritance: A probabilistic subset relationship where knowing x is in F directly informs us about x being in W."}, {"title": null, "content": "\u2022 Intensional Inheritance: Generalizes this by considering overlapping properties and degrees.\nQuite simply: In the case where each property corresponds to a unique element, and degrees are either 0 or 1, the intensional inheritance formula simplifies to the extensional case."}, {"title": "Conclusion", "content": "We have derived detailed formulas for the intensional inheritance of W from F using both Shannon information theory and algorithmic information theory, incorporating interaction information among properties. In the special case of mutually exclusive properties, the formulas simplify considerably. Finally, we observe that intensional inheritance encompasses extensional inheritance as a special case.\nThis framework provides a quantitative method to assess how knowledge of one concept influences our understanding of another, accounting for the complex interplay of properties and their degrees."}]}