{"title": "ASCenD-BDS: Adaptable, Stochastic and Context-aware framework for Detection of Bias, Discrimination and Stereotyping", "authors": ["Rajiv Bahl", "Venkatesan N", "Parimal Aglawe", "Aastha Sarasapalli", "Bhavya Kancharla", "Chaitanya kolukuluri", "Harish Mohite", "Japneet Hora", "Kiran Kakollu", "Rahul Diman", "Shubham Kapale", "Sri Bhagya Kathula", "Vamsikrishna Motru", "Yogeshwar Reddy"], "abstract": "The rapid evolution of Large Language Models (LLMs) has transformed natural language processing but raises critical concerns about biases inherent in their deployment and use across diverse linguistic and sociocultural contexts. This paper presents a framework named ASCenD-BDS - Adaptable, Stochastic and Context-aware framework for Detection of Bias, Discrimination and Stereotyping.\n\nThe framework presents approach to detecting bias, discrimination, stereotyping across various categories such as gender, caste, age, disability, socioeconomic status, linguistic variations, etc., using an approach which is Adaptive, Stochastic and Context-Aware.\n\nThe existing frameworks rely heavily on usage of datasets to generate scenarios for detection of Bias, Discrimination and Stereotyping. Examples include datasets such as Civil Comments, Wino Gender, WinoBias, BOLD, CrowS-Pairs and BBQ. However, such an approach provides point solutions. As a result, these datasets provide a finite number of scenarios for assessment. The current framework overcomes this limitation by having features which enable:\n\nAdaptability\nStochasticity\nContext Awareness\nContext awareness can be customized for any nation or culture/sub-culture (for example an organization's unique culture). In this paper, context awareness in the Indian context has been established. Content has been leveraged from Indian Census 2011 to have a commonality of categorization.\n\nA framework has been developed using Category, Sub-Category, STEM, X-Factor, Synonym to enable the features for Adaptability, Stochasticity and Context awareness. The framework has been described in detail in Section 3.\nOverall 800+ STEMs, 10 Categories, 31 unique Sub-Categories were developed by a team of consultants at Saint Fox Consultancy Private Ltd. The concept has been tested out in SFCLabs as part of product development.\n\nThe ASCenD-BDS framework is being incorporated in product development being undertaken by SFCLabs. A similar approach can be adopted by interested parties for assessments relevant to the context of the organisation.\n\nKeywords: Adaptability, Stochasticity, Context-aware, Large Language Model (LLM), Bias Analysis, Agentic GenAI, Chatbots, Indian Context, Linguistic Diversity, Bias, Discrimination, Stereotyping, Al Red Teaming (AIRT), Red Teaming.", "sections": [{"title": "1. Introduction", "content": "The existing frameworks rely heavily on usage of datasets to generate scenarios for detection of Bias, Discrimination and Stereotyping. Examples include datasets such as Civil Comments [1], Wino Gender [2], WinoBias [3], BOLD [4], CrowS-Pairs [5] and BBQ [6]. As a result, these datasets provide a finite number of scenarios. The current framework overcomes this limitation by having features which enable:\n\nAdaptability\nStochasticity\nContext Awareness\nContext awareness can be customized for any nation or culture/sub-culture (for example an organization's unique culture). In this paper, context awareness in the Indian context has been established. Content has been leveraged from Indian Census 2011 to have a commonality of categorization.\n\nBuilding on the shortcomings of existing systems, this novel framework improves how detection is done for Bias, Discrimination, and Stereotyping. ASCenD-BDS framework uses Category, Sub-Category, STEM, X-Factor, Synonyms to enable Adaptability, Stochasticity and Context awareness.\n\n1. Adaptability: The framework can be adjusted for use across different domains. It works\nwell for tasks such as identifying organizational bias, evaluating societal stereotypes, or\naiding in inclusive policymaking.\n\n2. Stochasticity: By creating scenarios dynamically, the framework avoids the repetitive,\nlimited nature of predefined datasets and enables more in-depth evaluations.\n\n3. Context Awareness: The framework's ability to adapt to cultural nuances ensures that\nit identifies and addresses biases in a way that is both meaningful and appropriate to the\ncontext of the organisation.\n\nThe rapid advancement of Large Language Models (LLMs) has revolutionized natural language processing (NLP) technologies, enabling exceptional abilities to comprehend, generate, and interact with human language [7, 8]. However, their development faces significant challenges due to the persistent presence of linguistic and cultural biases rooted in intricate sociotechnical systems [9,10]. These biases are especially pronounced in multilingual and multicultural settings, where language operates as a tool of power, representation, and systemic inequality [11, 12].\n\nTechnological advancements in Large Language Models (LLMs) are increasingly accompanied by concerns about algorithmic biases that risk perpetuating and amplifying social inequities [3, 13]. In multilingual and culturally diverse contexts like India, such biases pose significant"}, {"title": "2. Literature Review", "content": "As language models advance in capabilities, their development increasingly aligns with the availability of digital content, often at the expense of balanced representation. The fundamental architecture of most LLMs tends to prioritize dominant languages and cultural narratives, leading to what Ramakrishnan et al. [15] describe as \"linguistic hegemony in machine learning.\"\nIn the Indian context, this bias emerges through a range of interconnected mechanisms.\n\nLLMs trained on datasets containing stereotypical information perpetuate biases across dimensions such as age, caste, gender, language, religion, region, literacy, and cultural aspects. The stereotypes embedded in training data directly influence the model's inference processes, resulting in biased outputs that often reinforce societal inequities. Studies suggest that stereotypes are more pervasive in LLMs than among humans, highlighting the systemic nature of the issue. This bias becomes particularly pronounced in multilingual and multicultural contexts, where certain categories are disproportionately affected due to their complexity and diversity.\n\nPre-training corpora used for language models are often\ndisproportionately dominated by:\n\nEnglish-language sources\nWestern digital ecosystems\nDigitally privileged communities\nTextual data from urban, educated demographics\nPandey and Srinivasan [16] highlight that less than 3% of Indian language digital content is meaningfully represented in global language models. This stark underrepresentation contributes to a systemic erasure of linguistic diversity, limiting the inclusivity and equity of these models in multilingual contexts like India.\n\nIndian languages pose unique challenges for\ncomputational linguistics, including:\n\nComplex morphological structures\nAgglutinative language characteristics\nFrequent code-switching phenomena\nGrammatical variations that defy traditional NLP architectures\nThese linguistic complexities often lead to biases, as models struggle to accurately interpret nuanced semantic meanings across diverse linguistic frameworks [14, 5].\n\nThe interplay of linguistic diversity and cultural intricacy results in multidimensional bias\nlandscapes:\n\nCaste-based linguistic markers\nGender-encoded linguistic structures\nSocioeconomic communication patterns\nRegional dialect variations\nAge-based polarity\nLanguage-driven variations\nReligion based variations\nUrban/Rural driven scenarios\nAppearance based judgments\n\nAsymmetric documentation of knowledge\nLimited digital representation of marginalized communities\nBiases in historical documentation\nUnequal archival practices\nEfforts to address these challenges have included the development of specialized datasets designed to tackle biases in LLMs. These datasets represent initial steps toward mitigating disparities and fostering more inclusive models."}, {"title": "3.ASCenD-BDS Framework", "content": "A lexical hierarchy has been created for categories e.g. religion, disability, caste, etc. and in some case sub-categories have been identified e.g. disability (visually challenged, physically challenged, etc). The categories and sub-categories have been used to classify and create STEM which is part of the prompt that makes the prompt applicable regardless of the opposing factors to be considered.\n\nThe opposing factors are identified for each STEM and are called XFactor1 and XFactor2. For each XFactor synonyms are selected randomly to provide Stochasticity feature.\n\nTo enable adaptability, levels (1 to 3) have been created so that based on previous response of a scenario ratcheting can be done to the next complexity level unless the prompt leads to failure or highest-level set has been achieved.\n\nContext-awareness has been enabled by incorporating a field context in the XFactor table. The concept was successfully tested for Indian context. The same can be customised for any other culture/sub-culture \u2013 Nations, Countries and Organizations.\n\nThe design has been explained using two Scenarios below."}, {"title": "4. Comparison of ASCenD-BDS with existing frameworks", "content": "Comparisons with existing datasets/frameworks is given below:\n\nThis dataset includes over 2 million comments from the Civil Comments platform [1], annotated for toxicity and identity-based attributes. It evaluates the presence of toxic language, insults, threats, and biases related to gender, race, religion, and sexual orientation, making it a key resource for studying toxicity and bias in language models.\n\nThis dataset features Winograd-style sentences to test models ability to avoid gender stereotypes in pronoun resolution [2]. It is instrumental in assessing how models handle gender biases in language comprehension.\n\nLike Winogender, WinoBias focuses on gender bias in pronoun resolution across professions [3]. By targeting occupational stereotypes, it evaluates a model's capability to resist gendered assumptions in its interpretations.\n\nThis dataset provides annotated prompts and outputs to evaluate biases in gender, race, and occupation [4]. It focuses on understanding language models' bias tendencies in generating open-ended responses.\n\nThis dataset contains sentence pairs to detect stereotypical associations related to identity attributes like gender, race, and religion [5]. By contrasting sentences differing only in identity terms, it highlights models' sensitivity to biases.\n\nThe paper introduces BBQ, a dataset designed to evaluate how social biases affect question answering (QA) models across nine categories [6]. It reveals that models often rely on stereotypes, especially in under-informative contexts, leading to biased outputs even when a correct answer is available"}, {"title": "a) Civil Comments", "content": "The Civil Comments and ASCenD-BDS distribution as shown in Figure 1. highlight distinct and complementary approaches to addressing biases in Al. Civil Comments focuses on general toxicity and identity attacks in online discourse, offering global applicability but primarily addressing individual-level toxicity [1]. In contrast, ASCenD-BDS targets socio-cultural biases unique to Indian society, with a regional specificity that emphasizes systemic and intersectional prejudices. The two datasets are non-overlapping, with Civil Comments concentrating on online harm, while ASCenD-BDS captures a broader spectrum of societal biases such as caste, socio-economic status, and language.\n\nThe complementary nature of these datasets underscores the importance of context-aware resources in Al research. While global datasets like Civil Comments are invaluable for developing general-purpose models, regional datasets such as ASCenD-BDS fill critical gaps by addressing biases specific to sociocultural contexts. Integrating insights from both datasets allows researchers to create robust models capable of handling diverse and culturally specific challenges, advancing the broader agenda of fair and ethical Al development."}, {"title": "b) Winogender", "content": "The Winogender and ASCenD-BDS have their distribution as shown in Figure 2. cater to distinct objectives in bias analysis, with Winogender focusing narrowly on evaluating gender bias in automated coreference resolution systems [2], while ASCenD-BDS addresses a much broader range of societal biases specific to the Indian context. The Winogender dataset exclusively examines Gender, using sentence pairs with varying pronoun genders to test fairness in resolving gender references. This makes it a valuable yet specialized tool for gender-specific bias evaluation in NLP tasks.\n\nIn contrast, the ASCenD-BDS stands out for its comprehensive coverage of bias dimensions, spanning Age, Appearance, Caste, Disability Status, Gender, Language, and Socio-Economic Status. It uniquely captures cultural and systemic biases like caste and language, which are often overlooked in globally oriented datasets. Furthermore, ASCenD-BDS enables the analysis of intersectional stereotypes, providing a holistic view of multi-faceted biases in Indian society. By addressing biases beyond gender, ASCenD-BDS offers a more diverse and culturally relevant"}, {"title": "c) WinoBias", "content": "The WinoBias and ASCenD-BDS address bias in language models and as shown in Figure 3 but diverge in their focus and scope. WinoBias is designed to evaluate gender and occupational biases in coreference resolution tasks [3]. It specifically tests systems for stereotypical and anti-stereotypical gender biases in occupational roles, aiming to ensure fairness in resolving gender-related references. Its categories include Gender, Occupation Bias, and Stereotypes, making it a critical resource for analyzing role-specific biases.\n\nIn contrast, the ASCenD-BDS provides a broader lens, capturing biases that are deeply rooted in Indian sociocultural contexts. Beyond Gender, it includes dimensions such as Caste, Language, Socio-Economic Status, Disability Status, and Appearance. ASCenD-BDS uniquely addresses systemic biases that are highly relevant to India's diverse and hierarchical society, providing a more comprehensive framework for fairness evaluation.\n\nWhile WinoBias is specialized for detecting occupational and gender stereotypes in language tasks, ASCenD-BDS stands out for its ability to analyse intersectional and culturally specific biases, making it indispensable for developing Al systems that cater to the complexities of"}, {"title": "d) BOLD (Bias in Open-ended Language Generation Dataset)", "content": "The BOLD and ASCenD-BDS serve as critical resources for evaluating biases in natural language models, each addressing distinct societal contexts. BOLD focuses on global biases, particularly those relevant in Western and international settings, with unique categories like Racial Bias and National Origin Bias [4]. These dimensions target systemic inequities across diverse cultural and geographical boundaries, making BOLD a vital tool for analyzing fairness at a global scale.\n\nIn contrast, ASCenD-BDS is uniquely tailored to the Indian context, capturing biases deeply rooted in the country's sociocultural framework. It introduces specialized categories such as Caste, Language, and Physical Appearance, essential for understanding India's hierarchical and intersectional societal structures. While both datasets share common categories like Gender, Disability Status, Religion, and Socioeconomic Bias, ASCenD-BDS expands the scope to include uniquely Indian biases often overlooked in global datasets like BOLD.\n\nASCenD-BDS is indispensable for creating Al systems that are culturally aware and equitable in Indian settings. Its ability to address caste-based prejudices, linguistic diversity, and socio- economic disparities ensures that Al models trained with ASCenD-BDS can meet the challenges of India's diverse population. Together, BOLD and ASCenD-BDS provide a complementary framework for fairness evaluation, enabling the development of Al systems that are both globally robust and locally nuanced, advancing the broader goal of ethical and inclusive Al."}, {"title": "e) CrowS-Pairs (Crowdsourced Stereotype Pairs)", "content": "The CrowS-Pairs and ASCenD-BDS offer essential tools for evaluating social biases in language models, but their priorities reflect distinct societal contexts. CrowS-Pairs is tailored to identify globally relevant stereotypes, with a focus on categories like Race/Colour, Nationality, and Sexual Orientation biases predominantly significant in Western and international settings [5].\n\nIn contrast, the ASCenD-BDS provides a localized perspective, capturing biases deeply rooted in Indian society. Its unique focus includes categories such as Caste, Language, Regional Identity, Disability Status, Appearance, and Socio-Economic dimensions rarely addressed in global datasets like CrowS-Pairs. These categories reflect the intricate hierarchical and intersectional structures of Indian society, making ASCenD-BDS indispensable for fairness evaluation in Indian-centric Al systems. While both datasets share commonalities, such as Gender and Age, ASCenD-BDS expands the scope to address biases that are uniquely critical for India's socio-cultural realities, such as caste and language, which are deeply embedded in the nation's sociopolitical history.\n\nASCenD-BDS stands out for its culturally specific focus, bridging critical gaps in fairness evaluation often overlooked in globally oriented datasets. By addressing biases unique to Indian society, ASCenD-BDS provides a robust framework for developing inclusive and equitable Al systems tailored to India's complexities. Its comprehensive coverage ensures that models trained with ASCenD-BDS are well-equipped to navigate challenges specific to the Indian context, advancing the broader goal of ethical, culturally aware Al."}, {"title": "f) BBQ-dataset", "content": "The BBQ and ASCenD-BDS both aim to highlight societal biases, but their focus diverges significantly in scope and context. BBQ predominantly addresses biases commonly studied in Western societies, such as Nationality, Race/Ethnicity, Religion, and Sexual Orientation, reflecting the societal structures and discrimination patterns prevalent in those regions [6]. In contrast, ASCenD-BDS takes a region-specific approach, addressing biases deeply entrenched in the Indian socio-cultural landscape.\n\nA distinctive feature of ASCenD-BDS is its inclusion of Caste, a hierarchical system unique to South Asia, particularly India, which profoundly influences access to education, employment, and opportunities. Additionally, ASCenD-BDS captures biases related to Language, a reflection of India's immense linguistic diversity, where speakers of non-dominant or regional languages often face exclusion or prejudice. Another critical addition is Regional Identity, which addresses biases against individuals based on their geographic origins within India, underscoring the nation's internal socio-political complexities. These categories, absent in BBQ, emphasize ASCenD-BDS relevance in tackling challenges unique to India's diverse demographic composition. By including caste, language, and regional identity alongside globally shared categories like Age, Appearance, Disability Status, Gender, and Socio-Economic Status, ASCenD-BDS bridges a critical gap, offering nuanced insights into systemic biases that remain underrepresented in global datasets. This makes ASCenD-BDS an indispensable resource for bias detection and mitigation in the Indian context, ensuring a more equitable understanding of discrimination within Al systems."}, {"title": "5. Conclusion and Future work", "content": "The existing frameworks rely heavily on usage of datasets to generate scenarios for detection of\nBias, Discrimination and Stereotyping. Examples include datasets such as Civil Comments,\nWino Gender, WinoBias, BOLD, CrowS-Pairs and BBQ. However, such an approach provides\npoint solutions. As a result, these datasets provide a finite number of scenarios for assessment.\nThe current framework overcomes this limitation by having features which enable:\n\nAdaptability\nStochasticity\nContext Awareness\nContext awareness can be customized for any nation or culture/sub-culture (for example an\norganization's unique culture). In this paper, context awareness in the Indian context has been\nestablished. Content has been leveraged from Indian Census 2011 to have a commonality of\ncategorization.\n\nDevelopers assessing their LLMs for bias, discrimination and stereotyping\nProducts assessing LLMs for bias, discrimination and stereotyping\nResearchers assessing LLMs for bias, discrimination and stereotyping\nCIOS, CISOS, Security Leaders interested in understanding the security posture of LLMs\ndeveloped by their teams for bias, discrimination and stereotyping\n\nGoal driven assessment which can automatically determine the relevant levels of adaptability and stochasticity parameters to be used for assessment\nExpansion of categories, sub-categories, XFactors and synonyms\nSectoral solutions for assessment of Agentic Al (for e.g. Banking Industry)\nCultural solutions specific for nations for assessment of Agentic Al\nTraining, validation and testing of newer Agentic Al models\nAdoption of the framework for compliance reporting"}]}