{"title": "Piculet: Specialized Models-Guided Hallucination\nDecrease for MultiModal Large Language Models", "authors": ["Kohou Wang", "Xiang Liu", "Zhaoxiang Liu", "Kai Wang", "Shiguo Lian"], "abstract": "Multimodal Large Language Models (MLLMs) have made\nsignificant progress in bridging the gap between visual and language\nmodalities. However, hallucinations in MLLMs, where the generated text\ndoes not align with image content, continue to be a major challenge.\nExisting methods for addressing hallucinations often rely on instruction-\ntuning, which requires retraining the model with specific data, which\nincreases the cost of utilizing MLLMs further. In this paper, we introduce\na novel training-free method, named Piculet, for enhancing the input\nrepresentation of MLLMs. Piculet leverages multiple specialized models\nto extract descriptions of visual information from the input image and\ncombine these descriptions with the original image and query as input to\nthe MLLM. We evaluate our method both quantitively and qualitatively,\nand the results demonstrate that Piculet greatly decreases hallucinations\nof MLLMs. Our method can be easily extended to different MLLMs while\nbeing universal.", "sections": [{"title": "1 Introduction", "content": "In recent years, there has been remarkable progress in the field of large-scale\nmodels, with the following being typical examples of this work: BERT [3], GPT-3\n[4], CLIP [5], DALL-E [6], etc. These works greatly promoted the development of\nMultimodal Large Language Models (MLLMs), an important branch of Artificial\nIntelligence. MLLMs' goal is to construct an artificial intelligence system capable\nof understanding and handling different modalities, such as image, text, audio,\netc. The field of MLLMs has seen several landmark advancements, including\nLLaVa, CogVLM, Minigpt-5 et al. [7]- [9], they cast a profound impact on the\nimprovement of MLLMs."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 MLLMs' Hallucinations", "content": "Despite the mushrooming of MLLMs, the problem of hallucination still hangs\nlike the sword of Damocles: MLLMs occasionally generate content that diverges\nfrom the user input, contradicts previously generated context, or misaligns with\nestablished world knowledge. While the relatively usual normal deep learning\nmodels [18-21] output results of quite reliable credibility, hallucination puts\nthe MLLMs at a disadvantage, users tend to use MLLMs more for fun rather than\nfor professional needs, which is certainly not a good thing for MLLMs developed\nfor professional purposes. To address this challenge, existing mainstream works\nhave primarily focused on two aspects: training-based and training-free."}, {"title": "2.2 Training-based Methods", "content": "For training-based methods, Gunjal et al. [10] introduced MHalDetect, a multi-\nmodal hallucination detection dataset that can be used to train and benchmark\nmodels for hallucination detection and prevention. Liu et al. [11] addressed this\nissue by introducing the first large and diverse visual instruction tuning dataset,"}, {"title": "2.3 Training-free Methods", "content": "As for training-free methods, Yin et al. [12] represents a typical method that\nrequires no training of MLLMS while can directly correct the hallucinations.\nThey emphasized main attention on the post-process stage of MLLMs, firstly\nthey get an answer of a MLLM, then utilized auxiliary models' outputs to cor-\nrect both object-level and attribute-level hallucinations, which was the first to\napply a corrective manner to tackle the visual hallucination problem. Although\ntheir method, named Woodpecker, can reduce hallucinations by correcting the\nMLLM's answers, their method is a post-process framework, and still actually\ncomprises three pre-trained rather large-scale models apart from the MLLM to\nbe corrected, which are GPT-3.5-turbo [30], Grounding DINO [31] and BLIP-\n2-FlanT5 XXL [32]. Furthermore, the GPT-3.5-turbo is used 3 times in their\nprocessing pipeline. These models are not only time-consuming, but some are\nalso proprietary, making them uneconomical with slow inference processes.\nCompared to their approach, our method addresses the hallucination issue\nof MLLMs at its root and utilizes no other large language models apart from the\nMLLM to be corrected. Different from the Woodpecker method focusing on the\npost-process stage, our method focuses on the pre-process stage of MLLMs. Our\nmethod utilizes specialized, traditional small deep learning models to generate\nresults describing factual information. These results, reorganized into a specific\nformat, serve as supplementary descriptions and are input alongside the user's\nquery and image into the MLLM, thereby enabling the model to generate correct\nanswers directly by referencing additional factual information. Our specialized\nmodels only need to be run once during the processing pipeline, and the outputs\nof specialized models serve as external knowledge to calibrate the MLLM."}, {"title": "3 Method", "content": "Our method aims to address the hallucinations of MLLMs at its original source.\nWe firstly utilize specialized traditional light-weight deep learning models to de-\ntect factual information of input image, then formulate these descriptions, which,\nalongside the user's query and image, are input into MLLMs. MLLMs, given the\nformulated input, then generate results with reduced hallucinations. Our method"}, {"title": "3.1 Specialized Models", "content": "Object Detection. We utilize an object detection model to detect factual in-\nformation of the input image. To be specific, we adopt PP-YOLOE [24], an\nindustrial state-of-the-art object detector with high performance and friendly\ndeployment, to detect objects inside input image. PP-YOLOE is pre-trained on\nCOCO [25], a large-scale object detection, segmentation, and captioning dataset\nthat has 80 object categories that can cover the most common objects encoun-\ntered in daily life.\nOCR. We utilize PaddleOCR\u00b3 to recognize characters inside image. Pad-\ndleOCR is an awesome multilingual OCR toolkit based on PaddlePaddle, which\nsupports 80+ language recognition, provides data annotation and synthesis tools,\nand supports training and deployment among server, mobile, embedded and IoT\ndevices. We utilize this model to extract additional information inside an image\nto serve as specialized descriptions, together with the coco-detected objects, for\nthe MLLMs to refer to.\nFace recognition. We utilize insightface [26] to detect faces inside an image.\nInsightface is an open-source 2D&3D deep face analysis toolbox, which efficiently\nimplements a rich variety of state-of-the-art algorithms of face recognition, face\ndetection and face alignment, which are optimized for both training and deploy-\nment. Furthermore, we establish a repository of celebrities, and the recognized"}, {"title": "3.2 Input Formulation", "content": "We utilize the aforementioned specialized traditional deep learning models to\ndetect objects, characters, and faces, and in this part we integrate all these\ndetected results into a specific format to serve as input, alongside the original\nuser's question and image, for the MLLMs.\nObject Detection. For the detected objects, we traverse all the detected\nresults and integrate them into a single sentence in the following format: \"the\nimage contains these objects: there is/are {number} {object}.\". The detail is\nexamplified in Fig 2a."}, {"title": "4 Experiments", "content": "In this section, we will discuss the datasets we use and the experiments we con-\nduct in detail. We use mainstream benchmark datasets POPE [13], MME [14],\nand LLaVA-QA90 [15], and conduct comprehensive comparative experiments to\nvalidate the effectiveness and superiority of our method. Specifically, we choose\nQwen-VL-7B and LLaVa-v1.5-13B [7] as our baseline models. Considering that\nWoodpecker is the most similar training-free method to ours, we also compare"}, {"title": "4.1 Datasets", "content": "POPE. The POPE [13]initiative aims to gauge the tendency of MLLMs to pro-\nduce hallucinations. It employs three varied sampling strategies-random, pop-\nular, and adversarial to construct non-existent object samples. Random sam-\npling randomly selects items not depicted in the image, while popular sampling\ndraws from a pool of frequently seen items not present, and adversarial sampling\nidentifies items often found together but missing from the image. Each kind of\nstrategy has 500 images, and each image has 6 related questions and answers,\nwhich is 3000 in total.\nFor evaluation, to thoroughly compare our method, we directly tested all\nthese images, which amounts to 9,000 in total. The questions balance between\npositive and negative samples at a 50-50 split. This approach casts object anno-\ntations as binary questions, centering on the evaluation of object hallucinations,\nwith a particular emphasis on the aspect of existence. The selected MLLMs\nwill answer like \"Is there a wine glass in the image?\", and the answer will be\nmeasured in a metric of Accuracy, Precision, Recall and F1 Score.\nMME. The MME [14] is a comprehensive evaluation benchmark for MLLMs.\nTo avoid data leakage that may arise from the direct use of public datasets for\nevaluation, the annotations of instruction-answer pairs are all manually designed.\nThe concise instruction design can fairly compare MLLMs, instead of struggling\nin prompt engineering. Besides, with such an instruction, quantitative statistics\ncan also be easily carried out. Also like POPE, The selected MLLMs will also\nbe prompted Yes or No questions.\nLLaVA-QA90. The LLaVA-QA90 [15] contains randomly selected 30 image\nfor COCO-Val-2014, and for each image, three types of questions (conversation,\ndetailed description, complex reasoning) are generated using the proposed data\ngeneration pipeline in [15]. Specifically, we sample 10 description-type queries\nthat are paraphrased in various forms to instruct an MLLM to describe an\nimage, such as \"Analyze the image in a comprehensive and detailed manner.\"\nand \"Explain the visual content of the image in great detail.\". GPT-4V [33]\nis utilized to evaluate the answers generated by the plain baseline model and\nour framework's model. We directly feed the image to GPT-4V, and prompt it\nto rate the responses regarding our designed two dimensions, i.e., accuracy and\ndetailedness. The prompt template is available in Fig 5."}, {"title": "4.2 Experimental Results", "content": "Resutls on POPE. Instead of sampling several hundreds of images, We di-\nrectly utilized the entire dataset, which amounts to 9,000 image-text queries,\nthereby enabling a more thorough and comprehensive comparison to demon-\nstrate the superiority of our method. The tested results on POPE are shown in\nTable 1, which utilizes Qwen-VL-Chat [34] and LLaVa [15] as baseline model."}, {"title": "4.3 Ablation Study.", "content": "We conduct an ablation study on MME datasets to validate the superiority\nand effectiveness of our method. In this section, we utilize Qwen-VL-Chat as\nbaseline model, in each test set, we select two of three specialized models and\nrun experiments to compare the generated results' scores. The calculated results\nare shown in Table 4.\nAs can be seen from the experimental results, each specialized model manages\nto boost the score on its respective test set. Specifically, the results with the\nspecialized Detection model outperform those without it in both existence and\ncount scores. Similarly, the use of the specialized OCR model leads to higher\nscores on the OCR test set compared to when it is not used. The same can be\nsaid for the specialized Face model. Based on the comprehensive comparative\nexperimental results, we can confidently say that each of our specialized models\ncontributes to improved outcomes. That is, the method we propose, which we\nname Piculet, can mitigate the hallucination phenomena in MLLMs, making the\nresponses to users' queries more authentic and reliable."}, {"title": "5 Conclusion", "content": "In this paper, we propose a novel framework named Piculet to address the hal-\nlucinations of MLLMs at its root. As a training-free method, our approach re-\nquires only single one inference of the target MLLM, and several other small\ndeep-learning models, no other rather large-scale models are involved, which is\neconomical and time-saving, and is plug-and-play in various different MLLMs.\nWe have achieved the goal of reducing hallucinations by supplying the MLLMs\nwith dependable external knowledge generated by specialized models. We eval-\nuate our method on numerous datasets with other methods, and the results\ndemonstrate the effectiveness and improvement of our method. We hope that\nour method can contribute a small improvement and offer some insights into\nthe handling of hallucinations of MLLMs, thus inspiring further research and\ndevelopment in the field.4."}]}