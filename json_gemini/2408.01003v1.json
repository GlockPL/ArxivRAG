{"title": "Piculet: Specialized Models-Guided Hallucination Decrease for MultiModal Large Language Models", "authors": ["Kohou Wang", "Xiang Liu", "Zhaoxiang Liu", "Kai Wang", "Shiguo Lian"], "abstract": "Multimodal Large Language Models (MLLMs) have made significant progress in bridging the gap between visual and language modalities. However, hallucinations in MLLMs, where the generated text does not align with image content, continue to be a major challenge. Existing methods for addressing hallucinations often rely on instruction-tuning, which requires retraining the model with specific data, which increases the cost of utilizing MLLMs further. In this paper, we introduce a novel training-free method, named Piculet, for enhancing the input representation of MLLMs. Piculet leverages multiple specialized models to extract descriptions of visual information from the input image and combine these descriptions with the original image and query as input to the MLLM. We evaluate our method both quantitively and qualitatively, and the results demonstrate that Piculet greatly decreases hallucinations of MLLMs. Our method can be easily extended to different MLLMs while being universal.", "sections": [{"title": "1 Introduction", "content": "In recent years, there has been remarkable progress in the field of large-scale models, with the following being typical examples of this work: BERT [3], GPT-3 [4], CLIP [5], DALL-E [6], etc. These works greatly promoted the development of Multimodal Large Language Models (MLLMs), an important branch of Artificial Intelligence. MLLMs' goal is to construct an artificial intelligence system capable of understanding and handling different modalities, such as image, text, audio, etc. The field of MLLMs has seen several landmark advancements, including LLaVa, CogVLM, Minigpt-5 et al. [7]- [9], they cast a profound impact on the improvement of MLLMs."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 MLLMs' Hallucinations", "content": "Despite the mushrooming of MLLMs, the problem of hallucination still hangs like the sword of Damocles: MLLMs occasionally generate content that diverges from the user input, contradicts previously generated context, or misaligns with established world knowledge. While the relatively usual normal deep learning models [18-21] output results of quite reliable credibility, hallucination puts the MLLMs at a disadvantage, users tend to use MLLMs more for fun rather than for professional needs, which is certainly not a good thing for MLLMs developed for professional purposes. To address this challenge, existing mainstream works have primarily focused on two aspects: training-based and training-free."}, {"title": "2.2 Training-based Methods", "content": "For training-based methods, Gunjal et al. [10] introduced MHalDetect, a multi-modal hallucination detection dataset that can be used to train and benchmark models for hallucination detection and prevention. Liu et al. [11] addressed this issue by introducing the first large and diverse visual instruction tuning dataset,"}, {"title": "2.3 Training-free Methods", "content": "As for training-free methods, Yin et al. [12] represents a typical method that requires no training of MLLMS while can directly correct the hallucinations. They emphasized main attention on the post-process stage of MLLMs, firstly they get an answer of a MLLM, then utilized auxiliary models' outputs to correct both object-level and attribute-level hallucinations, which was the first to apply a corrective manner to tackle the visual hallucination problem. Although their method, named Woodpecker, can reduce hallucinations by correcting the MLLM's answers, their method is a post-process framework, and still actually comprises three pre-trained rather large-scale models apart from the MLLM to be corrected, which are GPT-3.5-turbo [30], Grounding DINO [31] and BLIP-2-FlanT5 XXL [32]. Furthermore, the GPT-3.5-turbo is used 3 times in their processing pipeline. These models are not only time-consuming, but some are also proprietary, making them uneconomical with slow inference processes.\nCompared to their approach, our method addresses the hallucination issue of MLLMs at its root and utilizes no other large language models apart from the MLLM to be corrected. Different from the Woodpecker method focusing on the post-process stage, our method focuses on the pre-process stage of MLLMs. Our method utilizes specialized, traditional small deep learning models to generate results describing factual information. These results, reorganized into a specific format, serve as supplementary descriptions and are input alongside the user's query and image into the MLLM, thereby enabling the model to generate correct answers directly by referencing additional factual information. Our specialized models only need to be run once during the processing pipeline, and the outputs of specialized models serve as external knowledge to calibrate the MLLM."}, {"title": "3 Method", "content": "Our method aims to address the hallucinations of MLLMs at its original source. We firstly utilize specialized traditional light-weight deep learning models to detect factual information of input image, then formulate these descriptions, which, alongside the user's query and image, are input into MLLMs. MLLMs, given the formulated input, then generate results with reduced hallucinations. Our method"}, {"title": "3.1 Specialized Models", "content": "Object Detection. We utilize an object detection model to detect factual information of the input image. To be specific, we adopt PP-YOLOE [24], an industrial state-of-the-art object detector with high performance and friendly deployment, to detect objects inside input image. PP-YOLOE is pre-trained on COCO [25], a large-scale object detection, segmentation, and captioning dataset that has 80 object categories that can cover the most common objects encountered in daily life.\nOCR. We utilize PaddleOCR\u00b3 to recognize characters inside image. PaddleOCR is an awesome multilingual OCR toolkit based on PaddlePaddle, which supports 80+ language recognition, provides data annotation and synthesis tools, and supports training and deployment among server, mobile, embedded and IoT devices. We utilize this model to extract additional information inside an image to serve as specialized descriptions, together with the coco-detected objects, for the MLLMs to refer to.\nFace recognition. We utilize insightface [26] to detect faces inside an image. Insightface is an open-source 2D&3D deep face analysis toolbox, which efficiently implements a rich variety of state-of-the-art algorithms of face recognition, face detection and face alignment, which are optimized for both training and deployment. Furthermore, we establish a repository of celebrities, and the recognized"}, {"title": "3.2 Input Formulation", "content": "We utilize the aforementioned specialized traditional deep learning models to detect objects, characters, and faces, and in this part we integrate all these detected results into a specific format to serve as input, alongside the original user's question and image, for the MLLMs.\nObject Detection. For the detected objects, we traverse all the detected results and integrate them into a single sentence in the following format: \"the image contains these objects: there is/are {number} {object}.\". The detail is examplified in Fig 2a."}, {"title": "5 Conclusion", "content": "In this paper, we propose a novel framework named Piculet to address the hallucinations of MLLMs at its root. As a training-free method, our approach requires only single one inference of the target MLLM, and several other small deep-learning models, no other rather large-scale models are involved, which is economical and time-saving, and is plug-and-play in various different MLLMs. We have achieved the goal of reducing hallucinations by supplying the MLLMs with dependable external knowledge generated by specialized models. We evaluate our method on numerous datasets with other methods, and the results demonstrate the effectiveness and improvement of our method. We hope that our method can contribute a small improvement and offer some insights into the handling of hallucinations of MLLMs, thus inspiring further research and development in the field.4."}, {"title": "4 Experiments", "content": "In this section, we will discuss the datasets we use and the experiments we conduct in detail. We use mainstream benchmark datasets POP\u0395 [13], \u039c\u039c\u0395 [14], and LLaVA-QA90 [15], and conduct comprehensive comparative experiments to validate the effectiveness and superiority of our method. Specifically, we choose Qwen-VL-7B and LLaVa-v1.5-13B [7] as our baseline models. Considering that Woodpecker is the most similar training-free method to ours, we also compare"}, {"title": "4.1 Datasets", "content": "POPE. The POPE [13]initiative aims to gauge the tendency of MLLMs to produce hallucinations. It employs three varied sampling strategies-random, popular, and adversarial to construct non-existent object samples. Random sampling randomly selects items not depicted in the image, while popular sampling draws from a pool of frequently seen items not present, and adversarial sampling identifies items often found together but missing from the image. Each kind of strategy has 500 images, and each image has 6 related questions and answers, which is 3000 in total.\nFor evaluation, to thoroughly compare our method, we directly tested all these images, which amounts to 9,000 in total. The questions balance between positive and negative samples at a 50-50 split. This approach casts object annotations as binary questions, centering on the evaluation of object hallucinations, with a particular emphasis on the aspect of existence. The selected MLLMs will answer like \"Is there a wine glass in the image?\", and the answer will be measured in a metric of Accuracy, Precision, Recall and F1 Score.\nMME. The MME [14] is a comprehensive evaluation benchmark for MLLMs. To avoid data leakage that may arise from the direct use of public datasets for evaluation, the annotations of instruction-answer pairs are all manually designed. The concise instruction design can fairly compare MLLMs, instead of struggling in prompt engineering. Besides, with such an instruction, quantitative statistics can also be easily carried out. Also like POPE, The selected MLLMs will also be prompted Yes or No questions.\nLLaVA-QA90. The LLaVA-QA90 [15] contains randomly selected 30 image for COCO-Val-2014, and for each image, three types of questions (conversation, detailed description, complex reasoning) are generated using the proposed data generation pipeline in [15]. Specifically, we sample 10 description-type queries that are paraphrased in various forms to instruct an MLLM to describe an image, such as \"Analyze the image in a comprehensive and detailed manner.\" and \"Explain the visual content of the image in great detail.\". GPT-4V [33] is utilized to evaluate the answers generated by the plain baseline model and our framework's model. We directly feed the image to GPT-4V, and prompt it to rate the responses regarding our designed two dimensions, i.e., accuracy and detailedness. The prompt template is available in Fig 5."}, {"title": "4.2 Experimental Results", "content": "Resutls on POPE. Instead of sampling several hundreds of images, We directly utilized the entire dataset, which amounts to 9,000 image-text queries, thereby enabling a more thorough and comprehensive comparison to demonstrate the superiority of our method. The tested results on POPE are shown in Table 1, which utilizes Qwen-VL-Chat [34] and LLaVa [15] as baseline model."}, {"title": "4.3 Ablation Study.", "content": "We conduct an ablation study on MME datasets to validate the superiority and effectiveness of our method. In this section, we utilize Qwen-VL-Chat as baseline model, in each test set, we select two of three specialized models and run experiments to compare the generated results' scores. The calculated results are shown in Table 4."}]}