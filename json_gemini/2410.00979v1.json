{"title": "Towards Full-parameter and Parameter-efficient Self-learning For Endoscopic Camera Depth Estimation", "authors": ["Shuting Zhao", "Chenkang Du", "Kristin Qi", "Xinrong Chen", "Xinhan Di"], "abstract": "Adaptation methods are developed to adapt depth foundation models to endoscopic depth estimation recently. However, such approaches typically under-perform training since they limit the parameter search to a low-rank subspace and alter the training dynamics. Therefore, we propose a full-parameter and parameter-efficient learning framework for endoscopic depth estimation. At the first stage, the subspace of attention, convolution and multi-layer perception are adapted simultaneously within different sub-spaces. At the second stage, a memory-efficient optimization is proposed for subspace composition and the performance is further improved in the united sub-space. Initial experiments on the SCARED [1] dataset demonstrate that results at the first stage improves the performance from 10.2% to 4.1% for Sq Rel, Abs Rel, RMSE and RMSE log [3, 13, 15, 16] in the comparison with the state-of-the-art models.", "sections": [{"title": "Introduction", "content": "Recently, attention is attracted on the foundation models for their good performances in a variety of tasks including text and vision [8-10]. Then, the adaption of foundation models to the medical domain is developed for the image segmentation, detection and depth estimation [2, 14, 17]. However, such approaches typically under-perform training. Therefore, we propose a full-parameter and memory-efficient module connecting different sub-spaces to a united space for the adaption of the depth foundation model."}, {"title": "Related work", "content": "Foundation Models are generally trained on extensive amounts and demonstrate strong generalization capabilities across multiple tasks and scenarios. For example, Depth Anything (DA) [15] is a depth estimation foundation model trained on large-scale labeled and unlabeled data. However, the adaption should be conducted on these foundation models for the endoscopic scenes. Then, the adaption of foundation models to medical domain is developed such as segmentation, detection and depth estimation [4, 17]. The majority of these approaches are in the field of low-rank adaption [7]. However, this adaption is limited in the single sub-space. Therefore, we proposed a full-parameter and memory-efficient module connecting different sub-spaces and project to a united space."}, {"title": "Methods", "content": "We propose a two-stage adaption strategy (Figure 1) for the adaption of the state-of-the-art depth foundation model [4]. At the first stage, a multiple number of adapters are applied to different sub-spaces of the foundation model. At the second stage, a bridge is built to combine different sub-spaces into a united space and the performance is continued to be improved with efficient memory. In details, we represent the state-of-the-art depth model [4] as three types of sub-spaces, the convolution space, the mlp space and the attention space. It's represented as the following:\n\n$W_{depth} = W_{conv} \\cup W_{mlp} \\cup W_{atten}$ (1)\n\nwhere $W_{conv} = W_{conv_1} \\cup W_{conv_2} \\cup ... W_{conv_{n_1}}$, representing weights of $n_1$ number of convolution layers, $W_{mlp} = W_{mlp_1} \\cup W_{mlp_2} \\cup ... W_{mlp_{n_2}}$, representing weights of"}, {"title": "Experiments", "content": "SCARED Dataset [1]. SCARED [1] contains 35 endoscopic videos with 22950 frames of fresh porcine cadaver abdominal anatomy collected with a da Vinci Xi endoscope. We followed the split scheme where the SCARED dataset [1] is split into 15351, 1705, and 551 frames for the training, validation and test sets, respectively."}, {"title": "Discussion", "content": "We propose a two-stage adaption for the depth foundation model towards full-parameter with efficient memory. Experiments of the first stage are conducted and the results demonstrate that the error reduction is from 10.2% to 4.1% for"}]}