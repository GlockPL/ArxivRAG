{"title": "SurGen: Text-Guided Diffusion Model for Surgical Video Generation", "authors": ["Cho Joseph", "Schmidgall Samuel", "Zakka Cyril", "Mathur Mrudang", "Shad Rohan", "Hiesinger William"], "abstract": "Diffusion-based video generation models have made significant strides, producing outputs with improved visual fidelity, temporal coherence, and user control. These advancements hold great promise for improving surgical education by enabling more realistic, diverse, and interactive simulation environments. In this study, we introduce SurGen, a text-guided diffusion model tailored for surgical video synthesis, producing the highest resolution and longest duration videos among existing surgical video generation models. We validate the visual and temporal quality of the outputs using standard image and video generation metrics. Additionally, we assess their alignment to the corresponding text prompts through a deep learning classifier trained on surgical data. Our results demonstrate the potential of diffusion models to serve as valuable educational tools for surgical trainees.", "sections": [{"title": "1 Introduction", "content": "Generative artificial intelligence (AI) has made significant advancements in health-care, with applications ranging from radiology report generation [1, 2] and clinical text summarization [3, 4] to text-guided medical image generation [5, 6]. Yet, the field of medical video generation remains largely unexplored. Video synthesis presents challenges that go beyond static image creation, requiring both high visual quality and temporal coherence across frames [7]. This is particularly important for producing smooth transitions and realistic movements over time. In surgery, these challenges are compounded by the need for generated videos to accurately represent anatomical structures, deformable tissue dynamics, and surgical instrument movements [8].\nDenoising diffusion probabilistic models (DDPMs) [9], known for their state-of-the-art performance in image and video generation [10, 11], provide a promising approach toward addressing these challenges. These models work by gradually adding random noise to an image or video until it conforms to an isotropic Gaussian distribution. This process is then used to train a neural network to restore the original content from noise. Through iterative denoising, DDPMs are capable of synthesizing novel images and videos from pure noise. In addition, recent progress in text embeddings [12, 13] have enabled DDPMs to integrate textual prompts, offering greater control over the generation process. Although previous works adapted DDPMs for generating high-quality surgical images [6, 14-16], progress in surgical video generation remains limited. While Li et al. [17] explored surgical video generation, the synthesized outputs were low resolution (128 x 128 pixels), short duration (16 frames), and lacked input conditioning. Similarly, Iliash et al. [18] generated surgical videos conditioned on pre-existing instrument masks, which requires real labeled data.\nHere, we propose SurGen, which is the first (to our knowledge) text-guided video generation model for surgery. SurGen leverages a video diffusion architecture based on CogVideoX [19] to generate videos of surgical procedures with a higher resolution (720 x 480 pixels, width x height) and longer duration (49 frames) than existing methods. Our generated videos are conditioned on text prompts that specify distinct surgical phases: preparation, gallbladder dissection, clipping and cutting, and Calot's triangle dissection. We train our model on 200,000 unique frame sequences from laparoscopic cholecystectomy videos, each paired with a phase-specific prompt. We evaluate the visual fidelity and diversity of the model outputs using both the Fr\u00e9chet Inception Distance (FID) [20] for individual frames and the Fr\u00e9chet Video Distance (FVD) [21]. Additionally, we compare the phase alignment of the generated videos to the corresponding real data using a 3D Convolutional Neural Network (CNN) trained on a held-out set of laparoscopic cholecystectomy videos."}, {"title": "2 Related Work", "content": "Recent progress in image and video generation have been largely driven by the advancement of diffusion models. To overcome initial inefficiencies in pixel-level denoising, latent diffusion models (LDMs) were developed [22], compressing images into a latent space for faster processing. LDMs were extended to video generation [23] through the addition of temporal processing layers. Recent work has seen transformer architectures replacing traditional U-Nets for denoising operations [24-26], resulting in improved image and video quality, greater variation in the generated content, and adherence to text.\nSeveral studies have explored using diffusion models in the surgical domain. For instance, Allmendinger et al. [6] developed a series of surgical image generation models conditioned on both text and segmentation maps, aiming to use the synthetic images for augmenting existing datasets. Similarly, Nwoye et al. [16] adapted Imagen to generate surgical images from triplet action-based textual prompts, also addressing data scarcity concerns. Extending beyond static surgical images, Li et al. adopted an unconditional video diffusion transformer for generating endoscopy videos, including laparoscopic cholecystectomy procedures. Additionally, Iliash et al. used a three-stage method involving Stable Diffusion for surgical text-to-image training, ControlNet for instrument mask conditioning, and ControlVideo for generating surgical videos from both instrument mask frames and text.\nOur method improves upon existing work in surgical video generation by 1) conditioning solely on text, thereby eliminating the need for real data inputs, 2) producing"}, {"title": "3 Methods", "content": null}, {"title": "3.1 Description of the Dataset", "content": "The data used for model training and evaluation is sourced from Cholec80 [27], a dataset of 80 laparoscopic cholecystectomy procedures performed by 13 surgeons. We follow the original train-test split, using the first 40 videos for training and the remaining 40 for evaluation. We extract surgical phase labels-preparation, Calot's triangle dissection, gallbladder dissection, and clipping and cutting to create 200,000 video-text pairs for training. Specifically, for each surgical phase, we extract 50,000 unique sequences consisting of 49 frames each, with each frame in the sequence spaced two frames apart from the original video."}, {"title": "3.2 Data Pre-processing", "content": "In all video sequences, we crop each frame from its original width of 840 pixels down to 720 pixels, while maintaining its original height at 480 pixels. This effectively removes much of the black borders typical of endoscopic footage, ensuring that all essential surgical details are retained. We format the corresponding text prompts as \"Laparoscopic cholecystectomy during {surgical phase}\"."}, {"title": "3.3 Model Architecture and Training", "content": "For our video generation model, we adopt CogVideoX, a 2 billion parameter text-guided LDM. CogVideoX combines three major components to synthesize a video conditioned on an text prompt:\n\u2022 3D Variational Autoencoder: To accelerate denoising operations, the encoder of a 3D Variational Autoencoder (VAE) [28, 29] compresses each video into a latent space, reducing its spatial dimensions by 8x and temporal dimensions by 4x. The decoder of the 3D VAE converts the denoised representations into full video frames.\n\u2022 Denoising Video Transformer: A 2 billion parameter text-conditional video transformer [30] is used to denoise the latent vectors. Notably, the model uses a full 3D attention mechanism, allowing spatial-temporal patches to attend to each other across all locations.\n\u2022 Text Encoder: A T5 text encoder [31] converts the text prompts into semantically rich representations, which are then fed into the diffusion transformer to guide the denosing process.\nIn training our model, we initialize all its components with pretrained weights from the publicly released base model in an effort to reduce training time. As with other approaches in developing text-guided medical image and video generation models [6, 32, 33], we keep the text encoder and VAE frozen, training only the denoising model. We fine-tune the video transformer for 50,000 steps with an effective batch size of 4, minimizing the Mean Squared Error (MSE) Loss between its predicted noise and the corresponding ground truth. The model is trained using a fused AdamW [34] optimizer with a learning rate of 2e-4, a weight decay of le-4, and an epsilon value of 1e-8. Distributed data training is performed across 4 A100 GPUs, spanning roughly 3 days."}, {"title": "3.4 Evaluation", "content": "To evaluate the generated videos, we first use standard image and video generation metrics, including the FID on individual frames and FVD. FID evaluates the visual quality and diversity of the generated images relative to the original dataset, while the FVD extends this metric by also accounting for differences in temporal dynamics, enabling a direct comparison of videos. For the analysis, we generate 2048 videos, evenly distributed across the four surgical phases in the training dataset. We also extract another set of 2048 sequences, maintaining the same distribution of phases, from the evaluation set of Cholec80 to serve as our comparison dataset. We calculate the FID on the middle frames of the sequences and the FVD on the first 16 frames of the videos. As there are no surgical video generation models trained on Cholec80, we use the base CogVideoX model as our baseline for comparison.\nIn assessing the phase alignment of the synthesized videos, we use a 3D ResNet18 [35, 36] model previously trained on the Kinetics dataset [37]. We fine-tune this model for 15 epochs on 2,000 sequences of 16 frames from the last 40 videos of Cholec80, with the videos evenly distributed across the four surgical phases. We then calculate and compare the model's top-1 accuracy and area under the Receiver Operating Char-acteristic curve (AUROC) on 400 generated videos and 400 videos randomly sampled from SurGen's training dataset, both with an even phase distribution."}, {"title": "4 Results", "content": "The SurGen model achieves a substantially lower FID of 79.9163 compared to its pre-trained version's score of 260.0301, indicating improved visual quality and diversity. Additionally, SurGen obtains a significantly lower FVD of 752.7587 versus 1975.5990 for the pre-trained model, suggesting SurGen's generated surgical videos more closely resembles the temporal dynamics of real surgical footage.\nIn evaluating the surgical phase-alignment of the generated videos compared to their real counterparts, we found that the fine-tuned 3D ResNet18 model actually per-formed better on the generated videos in terms of both accuracy (0.5275 vs. 0.4325) and AUROC (0.7732 vs. 0.6964). We suspect that the generated videos exhibited stronger alignment to their corresponding phases because the diffusion model effec-tively captured the common visual and temporal patterns of each phase, potentially resulting in fewer outliers generated."}, {"title": "5 Discussion", "content": "In this work, we introduce SurGen, the first (to our knowledge) text-guided diffu-sion model for generating surgical videos. Improvements in standard image and video generation metrics suggest that the model effectively captures both the visual and temporal characteristics, as well as the overall variety in the surgical videos. Addition-ally, the generated outputs' strong alignment with their corresponding surgical phases highlights the model's ability to differentiate and accurately represent complex sur-gical concepts. By synthesizing high-resolution videos at a longer duration, SurGen demonstrates the potential of diffusion-based video generation models to synthesize surgical footage without compromising the original quality.\nPrevious work has shown that generative models can be used as simulators for generating action-guided data [38-40]. Currently, existing virtual reality simulators for surgical procedures are hand-designed and are therefore limited to predefined sce-narios with limited physical realism [41]. By leveraging video generation models, we can potentially simulate highly realistic surgical procedures that display a wide range of disease severity and anatomical variations. Furthermore, with improved control through text prompts, surgical trainees could select and customize the types of scenar-ios they wish to practice in. This would better equip them to handle a diverse array of surgical cases, thereby improving postoperative outcomes for patients.\nOur study has several limitations. First, due to the scarcity of labeled surgical data, our model was trained on only 40 laparoscopic cholecystectomy videos. To more accu-rately represent the physical dynamics in surgical procedures, extensive training on a larger and more diverse surgical dataset is required. Second, the model's input condi-tioning was limited to surgical phases. To add more customization and applicability, factors such as disease severity and patient demographics should be incorporated into the text prompts, though this would require additional data labeling. Third, our model currently lacks kinematic conditioning (specifically instrument movements), which is necessary for enabling responsive interaction with surgeon movements. Last, our model does not generate videos quickly enough for real-time applications. Achieving real-time video generation is essential for creating an immersive surgical training environment."}]}