{"title": "FedCCRL: Federated Domain Generalization with Cross-Client Representation Learning", "authors": ["Xinpeng Wang", "Xiaoying Tang"], "abstract": "Domain Generalization (DG) aims to train models that can effectively generalize to unseen domains. However, in the context of Federated Learning (FL), where clients collaboratively train a model without directly sharing their data, most existing DG algorithms are not directly applicable to the FL setting due to privacy constraints, as well as the limited data quantity and domain diversity at each client. To tackle these challenges, we propose FedCCRL, a novel federated domain generalization method that significantly improves the model's ability to generalize to unseen domains without compromising privacy or incurring excessive computational and communication costs. Specifically, we adapt MixStyle to the federated setting to transfer domain-specific features, while AugMix is employed to perturb domain-invariant features. Furthermore, we leverage supervised contrastive loss for representation alignment and utilize Jensen-Shannon divergence to ensure consistent predictions between original and augmented samples. Extensive experimental results demonstrate that FedCCRL achieves the state-of-the-art performance on the PACS, OfficeHome, and miniDomainNet datasets across varying numbers of clients. Code is available at https://github.com/SanphouWang/FedCCRL.", "sections": [{"title": "1. Introduction", "content": "Domain Generalization (DG) [44] seeks to train models that can generalize effectively to unseen domains, a crucial task in real-world applications where the test data may come from domains that differ from the training data [26]. To achieve this, DG methods focus on two types of features: domain-invariant features and domain-specific features. Domain-invariant features (e.g., objects' shape in image classification) are those that remain consistent across different domains and are essential for enabling a model to generalize to unseen domains. On the other hand, domain-specific features (e.g., color distribution of images in Figure 1) are unique to each domain and hinder generalization if overemphasized by the model.\nTo enhance models' generalization ability, representation learning plays a vital role in existing DG methods [1, 17, 23, 25]. A key component of this is representation alignment, which focuses on aligning feature representations of samples across domains to ensure that domain-invariant features are effectively learned. This alignment helps models capture patterns that are shared across domains while minimizing reliance on domain-specific features. In addition, feature augmentation [11, 37, 41, 43] is another powerful strategy for DG. Methods like MixStyle [43] generate new samples by mixing instance-level statistics from different domains. This approach exposes the model to a broader range of examples, thereby enhancing its ability to capture domain-invariant features while mitigating reliance on domain-specific characteristics [22], such as lighting conditions or writing styles.\nWe refer to these methods as centralized DG methods, as they typically assume that all data is centrally stored"}, {"title": null, "content": "and accessible. However, in real-world applications, data is often distributed across multiple clients, making direct data sharing impractical due to privacy concerns. This challenge is addressed by Federated Learning (FL) [19], where clients collaboratively train models without sharing their local datasets. In order to enhance the domain generalization ability of models in FL systems, Federated Domain Generalization (FDG) [18] has been receiving increasing attention. Figure 1 provides a typical FDG setting, where each client has access to data from a single domain and the target domain differs from those on all clients. However, in such FDG setting, the limited quantity of samples and the lack of domain diversity on each client complicates the direct application of centralized DG methods [2].\nMost existing FDG methods [7, 24, 39] attempt to boost the model's generalization capacity by aligning representation distributions and extracting domain-invariant features. However, due to the limited data quantity and domain diversity on individual clients, the performance of these methods often falls short. To mitigate these shortcomings, CCST [3] leverages AdaIN [10] for cross-client style transfer to increase domain diversity on each client. However, CCST introduces additional networks and requires the transmission of a large number of high-dimensional feature representations, resulting in significant computational overhead and transmission costs. Moreover, transmitting sample features poses a considerable privacy risk, as attackers could reconstruct highly realistic samples with these feature representations and the same decoder [15, 21].\nTo address the challenges in FDG, we propose FedCCRL-a novel method that significantly improves model generalization in the FL setting while maintaining privacy and incurring only negligible additional computational and communication costs. Specifically, FedCCRL combines cross-client feature augmentation, representation alignment, and prediction alignment. In the feature augmentation module, we adapt MixStyle [43] to the Federated Learning (FL) setting for cross-client transfer of domain-specific features, and we apply AugMix [8] to perturb domain-invariant features. For representation alignment, we utilize supervised contrastive loss [12] to align the representations of original and augmented samples. Lastly, prediction alignment leverages Jensen-Shannon divergence [20] to ensure consistency in predictions between original and augmented samples.\nThe key contributions of this work are outlined as follows:\n\u2022 We propose FedCCRL, a novel federated domain generalization framework that integrates cross-client feature augmentation, representation alignment, and prediction alignment to improve the model's generalization capability in federated settings.\n\u2022 We adapt MixStyle to the federated learning setting, enabling cross-client style transfer while maintaining data privacy and only introducing minimal additional computational and communication costs\n\u2022 Extensive experiments on PACS, OfficeHome, and miniDomainNet datasets demonstrate that the proposed FedCCRL method achieves state-of-the-art (SOTA) FDG performance across FL systems of varying numbers of clients."}, {"title": "2. Related Work", "content": "This section reviews key research areas related to DG [44], with a focus on representation alignment, feature augmentation, and Federated Domain Generalization (FDG). These aspects are critical in addressing the challenges of domain shifts and ensuring robust model performance in federated learning."}, {"title": "2.1. Representation Alignment", "content": "One of the key techniques in representation learning for DG is representation alignment, where feature representations from different domains are aligned to minimize domain-specific variations. For instance, DANN [4-6] employs adversarial learning to align the representation distributions across domains by using a domain classifier to ensure that the learned features are domain-invariant. Another prominent approach is CORAL [28], which aligns the second-order statistics of source and target representation distributions to reduce domain shift. Additionally, MMD-based methods [29, 33, 34] utilize kernel-based distances to align representations across domains. These methods focus on reducing domain-specific discrepancies while maintaining discriminative power, helping models generalize better to unseen domains."}, {"title": "2.2. Feature Augmentation", "content": "Feature augmentation [31, 32, 36] has also emerged as an effective strategy for DG by generating diverse variations of the data to improve models' robustness. One notable method is MixStyle [43], which generates new feature samples by mixing instance-level statistics (mean and variance) from different domains, effectively expanding the feature space and enabling models to generalize better across unseen domains. In addition to MixStyle, Mixup [35, 38] creates new data by linearly interpolating between two instances and their labels, using a weight sampled from a Beta distribution. Both methods are conceptually simple and computationally efficient, yet they have shown strong performance on popular benchmarks."}, {"title": "2.3. Federated Domain Generalization", "content": "Most existing FDG algorithms address these challenges by introducing regularization terms to enable the model to"}, {"title": null, "content": "learn domain-invariant features or to ensure that feature representations across clients are aligned to a common distribution. For example, FedADG [39] utilizes federated adversarial learning to align distributions across clients to a reference distribution. FedSR [24] adopts an L2-norm regularizer and a conditional mutual information regularizer to align feature distributions across domains. In addition, FedIIR [7] takes a different approach by implicitly learning invariant relationships through gradient alignment, helping the model generalize better to OOD data. Moreover, GA [40] introduces novel global objective and optimizes it by dynamically calibrating the aggregation weights.\nAlthough the aforementioned FDG algorithms have improved the model's generalization ability to some extent, they do not directly address the issue posed by limited data volume and domain diversity on each client and fail to achieve the desired performance. To this end, CCST [3] employs a cross-client style transfer approach based on AdaIN [10] to generate samples with styles from other domains on the client side. However, CCST relies on a pre-trained VGG network [27] to extract feature representations and perform sample reconstruction, while also needing to transmit these high-dimensional representations, which results in significant additional communication and computational overhead. Furthermore, transmitting these feature representations violates privacy protection principles in FL, as attackers can easily intercept these representations and reconstruct the original samples with the same decoder [15, 21]. Additionally, the use of a pre-trained VGG network also partially violates the principles of domain generalization, as the target domain might be included in the pre-training dataset."}, {"title": "3. Method", "content": "In this section, we begin by introducing a standard FDG setting. We then describe the feature augmentation techniques utilized in FedCCRL, followed by a detailed explanation of the feature alignment regularization and the Jensen-Shannon divergence regularization. An overview of the proposed FedCCRL framework is provided in Figure 2, with the algorithmic details outlined in Algorithm 1."}, {"title": "3.1. Preliminary", "content": "Let X and Y denote the input space and target space, respectively. In FDG, given M source domains $\\mathcal{S}_{source} = {\\mathcal{S}_i}_{i = 1,2,..., M}$, the data points in $\\mathcal{S}_i$ are independently and identically drawn from distribution $P_i(x, y)$, where $x \\in \\mathcal{X}, y \\in \\mathcal{Y}$. The joint distribution between each pair of domains are different, i.e., $P_i(x,y) \\neq P_j (x, y)$ for $1 \\leq i \\neq j \\leq \\mathcal{M}$.\nIn this study, we adopt a commonly used data partitioning scheme in FDG, where the dataset on each client, denoted as $D_i$, originates exclusively from a single do-"}, {"title": null, "content": "main. Specifically, each source domain is partitioned into K clients: $\\cup_{k=(i-1)+1}^{i} D_j = \\mathcal{S}_i$, and $D_q \\cap D_p = \\emptyset$ for $1\\leq q\\neq p\\leq K\\cdot\\mathcal{M}$.\nThe goal of FDG is to enable these $K\\cdot M$ clients to collaboratively train a model $f : \\mathcal{X} \\rightarrow \\mathcal{Y}$, such that the model is capable of generalizing well and minimize the prediction error on a target domain $\\mathcal{S}_{target}$:\n$\\min_f \\mathbb{E}_{(x,y)\\sim \\mathcal{S}_{target}} [l(f(x), y)],$  (1)\nwhere $l(\\cdot)$ represents the loss function. Samples in $\\mathcal{S}_{target}$ are drawn from $P_{target}$ and $P_{target}(x, y) \\neq P_i(x, y)$ for $i \\in {1,2,..., M}$.\nTo protect privacy, the server is not permitted to directly access the data on the clients. After local training, each client uploads its model parameters to the server, where the parameters are aggregated and then sent back to the clients. Let $\\theta_i^t$ denote the model parameters of client $i$ at round $t$. The server aggregates the parameters as follows:\n$\\theta^{t+1} = \\frac{1}{K\\cdot M} \\sum_{i=1}^{K\\cdot M} \\pi_i \\theta_i^t, $ (2)\nwhere $N = \\sum_i n_i$ is the total number of samples across all clients [19] and $n_i$ denotes the number of samples on client $i$. After aggregation, the server sends the updated parameters $\\theta^{t+1}$ back to each client."}, {"title": "3.2. Algorithm Overview", "content": "In FedCCRL, in addition to transmitting model parameters between the clients and the server, channel-wise sample statistics are also transmitted.\nOn the client side, for a batch of samples $X \\in \\mathbb{R}^{B \\times C \\times H \\times W}$, where B is the batch size, C is the number of channels, and H and W represent the height and width of the images respectively, we first utilize feature augmentation to generate two batches of augmented samples:\n$X^{(1)} = \\mathcal{M}(X), X^{(2)} = \\mathcal{M}(X),$ (3)\nwhere $\\mathcal{M}(\\cdot) = \\text{AugMix} \\cdot \\text{MixStyle}$ represents the composition of MixStyle and AugMix. The detailed augmentation processes of MixStyle and AugMix are provided in Algorithm 2 and Algorithm 3, respectively.\nAfter applying data augmentation, we extract representations and generate predictions for each batch, which are then used to compute the overall loss. Let Z denote the representation space. We formalize the model $f$ as $f = g \\circ h$, where $h : \\mathcal{X} \\rightarrow \\mathcal{Z}$ represents the representation encoder and $g: \\mathcal{Z} \\rightarrow \\mathcal{Y}$ denotes the classifier. We extract representations and produce predictions as follows:\n$Z = h(X), Z^{(1)} = h(X^{(1)}), Z^{(2)} = h(X^{(2)}),$ (4)"}, {"title": null, "content": "$\\hat{Y} = g(Z), \\hat{Y}^{(1)} = g(Z^{(1)}), \\hat{Y}^{(2)} = g(Z^{(2)}).$ (5)\nWe then employ supervised contrastive loss for representation alignment, as outlined in Equation 12, and apply Jensen-Shannon divergence for prediction alignment as specified in Equation 14, to ensure consistent predictions between the original and generated samples."}, {"title": "3.3. Feature Augmentation", "content": "In the context of DG, we identify two types of features:\n\u2022 Domain-specific features: These are features that change across domains such as color distribution, lighting conditions, or style, which have little to do with the label of the image.\n\u2022 Domain-invariant features: These features, including contours, textures, and shapes, are directly linked to the image's label and remain consistent across different domains.\nThe core idea behind the feature augmentation component in FedCCRL is to perform cross-client style transfer on domain-specific features and to perturb domain-invariant features. To achieve this, we adapt MixStyle to the FDG setting and combine it with AugMix. Specifically, MixStyle handles the cross-client style transfer, while AugMix is responsible for perturbing the domain-invariant features.\nMixStyle operates by leveraging the channel-wise statis-tics of samples. Given a sample $x \\in \\mathbb{R}^{C\\times H \\times W}$, we compute the channel-wise mean and standard deviation as follows:\n$\\mu_c(x) = \\frac{1}{HW} \\sum_{h=1}^{H} \\sum_{w=1}^{W} x_{c,h,w},$ (6)\n$\\sigma_c(x) = \\sqrt{\\frac{1}{HW} \\sum_{h=1}^{H} \\sum_{w=1}^{W} (x_{c,h,w} - \\mu_c(x))^2}.$ (7)"}, {"title": null, "content": "In FedCCRL, in addition to transporting model parameters to the server, clients are also required to upload sample statistics. Since attackers cannot reconstruct the original samples using only channel-wise means and standard deviations, our method effectively safeguards users' data privacy. Specifically, each client $i$ uploads a certain proportion of sample statistics denoted as:\n$F_i = {(\\mu(x_j), \\sigma(x_j)) | x_j \\in D_i, j = 1, 2, ..., \\lceil r n_i \\rceil },$ (8)\nwhere $r \\in (0,1]$ represents the ratio of statistics to be uploaded.\nAfter collecting statistics from all clients, the server concatenates these statistics to get the pool of statistics: $F_{pool} = {F_i | i = 1, 2, . . ., K \\cdot M}$, and subsequently distributes $F_{pool} = F_{pool}\\backslash F_i$ to each client $i$. Each client apply MixStyle to generate style transferred samples according to Algorithm 2."}, {"title": "3.4. Representation Alignment", "content": "To enable the model to focus on extracting domain-invariant features that capture the essential information relevant to the task, we employ supervised contrastive loss [12] to align feature representations across original samples and augmented samples. Supervised contrastive loss leverages label information to ensure that representations from the same class are pulled closer together in the latent space, while representations from different classes are pushed apart. This alignment mechanism enables the model to extract correct semantic features.\nGiven any two batches of representations $Z',Z'' \\in \\mathbb{R}^{B \\times V}$ and their corresponding labels $Y'$ and $Y''$, we concatenate these two batches of features and labels:\n$Z = cat(Z', Z'') \\in \\mathbb{R}^{2B \\times V}, $ (9)\n$Y = cat(Y', Y''),$ (10)\nwhere V denotes the size of feature representations.\nLet $\\mathcal{I} = {1,2, ..., 2B}$ to be the index set and $\\mathcal{A}(i) = \\mathcal{I} \\backslash i$. The supervised contrastive loss of Z' and Z'' is calculated as follow:\n$L_{sc}(Z', Z'') = \\sum_{i \\in \\mathcal{I}} \\frac{-1}{|\\mathcal{P}(i)|} \\sum_{p \\in \\mathcal{P}(i)} \\log \\frac{S(Z_i, Z_p)}{\\sum_{a\\in \\mathcal{A}(i)} S(Z_i, Z_a)},$ (11)\nwhere $\\mathcal{P}(i) = {p \\in \\mathcal{A}(i) \\mid Y_p = Y_i}, S(Z_i, Z_p) = \\exp(sim(Z_i, Z_p) / \\tau)$. sim($\\cdot$,$\\cdot$) denotes cosine similarity and $\\tau$ is a temperature parameter that controls the scale of similarities.\nThe representation alignment loss $L_{ra}$ in FedCCRL is defined as:\n$L_{RA} = \\frac{1}{2}(L_{sc}(Z^{(1)}, Z) + L_{sc}(Z^{(2)}, Z)).$ (12)"}, {"title": "3.5. Jensen-Shannon Divergence", "content": "As $X^{(1)}$ and $X^{(2)}$ are generated from X, we expect the model to not only correctly classify the original and augmented samples, but also produce similar predictions $\\hat{Y}, \\hat{Y}^{(1)}$ and $\\hat{Y}^{(2)}$. We utilize Jensen-Shannon divergence [20] minimization to achieve consistent predictions across original and generated samples. The Jensen-Shannon divergence is designed to measure the similarity between these probability distributions by averaging them and then computing their divergence from the mean. Specifically, let the average prediction distribution be defined as:\n$\\overline{Y} = \\frac{1}{3} (\\hat{Y} + \\hat{Y}^{(1)} + \\hat{Y}^{(2)}).$ (13)\nThe Jensen-Shannon divergence is then computed as follows:\n$L_{JS} = \\frac{1}{3}(KL(\\hat{Y}, \\overline{Y})+KL(\\hat{Y}^{(1)}, \\overline{Y})+KL(\\hat{Y}^{(2)}, \\overline{Y}))$ (14)\nwhere KL($\\cdot$,$\\cdot$) denotes KL divergence."}, {"title": "3.6. Overall Loss Function", "content": "The overall loss function is formulated as follows:\n$L = L_{CLS} + \\lambda_1 L_{RA} + \\lambda_2 L_{JS},$ (15)\nwhere the classification loss is computed as:\n$L_{CLS} = \\sum_{Y'\\in{\\{\\hat{Y},\\hat{Y}^{(1)}, \\hat{Y}^{(2)}\\}}} \\frac{1}{3} L_{CRE}(Y', Y),$ (16)\nand $L_{CRE}$ denotes the cross-entropy loss. The terms $\\lambda_1$ and $\\lambda_2$ are hyperparameters that control the relative importance of the representation alignment loss $L_{RA}$ and the Jensen-Shannon divergence loss $L_{JS}$, respectively."}, {"title": "4. Experiments", "content": "Our algorithm and baselines are evaluated on three widely used datasets in DG: PACS [14], OfficeHome [30], and miniDomainNet [42]. PACS comprises 9,991 samples across four domains: Art Painting, Cartoon, Photo, and Sketch, and is categorized into seven classes. OfficeHome contains 15,588 samples from four domains: Art, Clipart, Product, and Real World, and is divided into 65 classes. miniDomainNet features 140,006 samples from four domains: Clipart, Infograph, Painting, and Real, and encompasses a total of 126 classes.\nWe adopt the leave-one-domain-out evaluation protocol [14], where one domain is held out as the test set while the remaining domains are used for training. This process is repeated for each domain to ensure robust evaluation across varying distribution shifts."}, {"title": "4.2. Baselines and Implementation Details", "content": "In this experiment, we compare FedCCRL with several traditional FL frameworks, including FedAvg [19] and FedProx [16], as well as state-of-the-art FDG approaches such as FedADG [39], FedSR [24], FedIIR [7], GA [40], and CCST [3].\nFollowing the data partition scheme outlined in Section 3.1, we evaluate FedCCRL and the baselines across different numbers of clients. We utilize MobileNetV3-Large [9] as the backbone network, where the final fully connected layer acts as the classifier g, and the preceding layers serve as the representation encoder h.\nFor training, we set the number of communication rounds to 10, and the number of local training epochs is fixed at 3 for all methods. The Adam optimizer [13] is employed with an initial learning rate of 0.001. Furthermore, MixStyle is configured with the parameter $\\alpha = 0.1$, while for AugMix, the parameter $\\beta$ is set to 1.0. For supervised contrastive loss, we set the temperature parameter $\\tau$ to 0.1. Input samples from the PACS and OfficeHome datasets are resized to 224 \u00d7 224, whereas those from miniDomainNet are resized to 128 \u00d7 128. To manage the learning rate, we adopt a cosine learning rate scheduler, which facilitates a smooth decay of the learning rate throughout the training process.\nTo guarantee the reliability of our proposed algorithms, we run each experiment three times independently and report the averaged results."}, {"title": "4.3. Comparison with Baselines", "content": "We evaluate FedCCRL and other baselines under scenarios with different numbers of clients. Table 1 specifically shows the performance of each algorithm when the total number of clients is set to 6. In this table, we use the first letter of each domain to represent the domain, and each letter column corresponds to the performance when that domain is used as the target domain. The \"Avg.\" column provides the average results across the four leave-one-domain-out experiments. Additionally, Figure 4 presents the performance of each algorithm under varying client numbers, where the results are averaged across the four leave-one-domain-out experiments. In these experiments, the upload ratio for FedCCRL is set to 0.1.\nFrom Table 1, we have the following observations:\n\u2022 FedCCRL outperforms all baseline methods on all datasets. Specifically, FedCCRL achieves the best average accuracy on all datasets. In addition, FedCCRL attains top scores on each test domain of both PACS and miniDomainNet datasets.\n\u2022 Most baselines specifically designed for FDG fail to achieve consistent performance across different datasets. In some cases, their performance is even inferior to traditional federated learning algorithms such as FedAvg and FedProx."}, {"title": "4.4. Ablation Study", "content": "We investigate the effects of MixStyle, AugMix, representation alignment regularizer $L_{RA}$, and Jensen-Shannon divergence $L_{JS}$ on FedCCRL across the PACS, OfficeHome, and miniDomainNet datasets. The results are presented in Table 2, from which we can draw the following conclusions:\n\u2022 All four components significantly enhance model generalization. Using representation alignment loss or Jensen-Shannon divergence individually outperforms cross-entropy alone, and their combination yields the best performance. Additionally, combining AugMix and MixStyle improves results compared to using either technique alone.\n\u2022 Even without regularization, FedCCRL with only data augmentation techniques outperforms all baselines. As shown in Table 1, the combination of MixStyle and AugMix surpasses all baselines across the three datasets."}, {"title": "4.5. Impact of Upload Ratio and Hyperparameters", "content": "We evaluated the performance of FedCCRL under different upload ratios and hyperparameter settings, and the results are presented in Figure 6. When evaluating the effect of $\\lambda_1$, $\\lambda_2$ is fixed at 1. Similarly, when assessing the impact of $\\lambda_2$, $\\lambda_1$ is set to 0.1.\nThe evaluation results in Figure 6 demonstrate that both the upload ratio r and the hyperparameters $\\lambda_1$ and $\\lambda_2$ have minimal impact on the overall performance across the PACS, OfficeHome, and miniDomainNet datasets. Therefore, it can be concluded that FedCCRL is robust to variations in these parameters and outperforms other baselines even with only a very small amount of upload of statistics."}, {"title": "4.6. Feature Distribution", "content": "To better illustrate the effect of FedCCRL on the model, Figure 5 and Figure 7 respectively provide the t-SNE visualization of representation distribution and Grad-CAM visualization of the model's concentration on the PACS dataset.\nFigure 5 demonstrates that FedCCRL achieves better separation of classes compared to FedAvg, indicating that FedCCRL is effective in learning clearer, domain-invariant feature representations across different domains, reducing overlap between the representations of different classes. Additionally, Figure 7 reveals that the model trained with FedCCRL focuses more effectively on domain-invariant features, such as the giraffe's head and the elephant's trunk. This suggests that FedCCRL enhances the model's ability to concentrate on relevant, discriminative regions in the images, leading to improved generalization across domains."}, {"title": "5. Conclusion", "content": "In this paper we propose FedCCRL, a federated domain generalization framework that improves model generalization across unseen domains without violating privacy rules or introducing excessive computational and communication costs. Specifically, FedCCRL effectively combines MixStyle and AugMix for cross-client style transfer and the perturbation of domain-invariant features. In addition, FedCCRL utilizes supervised contrastive loss for representation alignment and apply Jensen-Shannon divergence for prediction alignment. Experiments on PACS, OfficeHome, and miniDomainNet demonstrate that FedCCRL outperforms existing baselines, demonstrating its effectiveness in enhancing models' generalization capability."}]}