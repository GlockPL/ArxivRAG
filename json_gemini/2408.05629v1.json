{"title": "Quantum-secure multiparty deep learning", "authors": ["Kfir Sulimany", "Sri Krishna Vadlamani", "Ryan Hamerly", "Prahlad Iyengar", "Dirk Englund"], "abstract": "Secure multiparty computation enables the joint evaluation of multivariate functions across distributed users while ensuring the privacy of their local inputs. This field has become increasingly urgent due to the exploding demand for computationally intensive deep learning inference. These computations are typically offloaded to cloud computing servers, leading to vulnerabilities that can compromise the security of the clients' data. To solve this problem, we introduce a linear algebra engine that leverages the quantum nature of light for information-theoretically secure multiparty computation using only conventional telecommunication components. We apply this linear algebra engine to deep learning and derive rigorous upper bounds on the information leakage of both the deep neural network weights and the client's data via the Holevo and the Cram\u00e9r-Rao bounds, respectively. Applied to the MNIST classification task, we obtain test accuracies exceeding 96% while leaking less than 0.1 bits per weight symbol and 0.01 bits per data symbol. This weight leakage is an order of magnitude below the minimum bit precision required for accurate deep learning using state-of-the-art quantization techniques. Our work lays the foundation for practical quantum-secure computation and unlocks secure cloud deep learning as a field.", "sections": [{"title": "1 Introduction", "content": "Although deep learning has revolutionized multiple domains [1-3], its applications are constrained by increasing computational demands on the hardware [4-6]. Given the high energy consumption required for state-of-the-art deep neural network (DNN) inference [7-9], it is common to delegate inference workloads from the edge to centralized server clusters. Unfortunately, this paradigm introduces vulnerabilities that compromise data security, which is crucial in applications such as business, finance, and healthcare [4]. This situation underscores the central challenge in secure computation, where multiple parties perform a joint evaluation of multivariate functions across distributed resources while preserving the privacy of their inputs [10] (Fig. 1(a)).\nModern secure computation schemes are built on homomorphic encryption, which allows universal computation on encrypted data and preserves the security of both the inputs and the outputs of the computation [11]. Recent developments have adapted state-of-the-art homomorphic encryption schemes for secure machine learning [12], but real-world applications are limited due to their massive computational overhead [6] and recently discovered security vulnerabilities [13]. Moreover, these encryption schemes typically depend on computational complexity and are not information-theoretically secure.\nIn this work, we introduce a linear algebra engine for information-theoretically secure computation. We apply our engine to multiparty deep learning and derive rigorous security analysis for all parties. Applied to the MNIST classification task, we obtain test accuracies of more than 96% while leaking less than 0.1 bits per weight symbol and 0.01 bits per data symbol. This weight leakage is an order of magnitude below the state-of-the-art minimum bit precision required for deep learning [14].\nOur protocol relies on a photonic computation architecture to realize efficient optical matrix-vector multiplication [15\u201317]. State-of-the-art optical DNNs have been demonstrated using integrated photonics [18-26], free-space optics [27-32] and fiber optics [33, 34]. Recently, a delocalized optical DNN using low-power devices was introduced and demonstrated [35, 36]. This protocol, which operates in an edge computing setting [5] where the DNN weights are streamed from a central server to clients that perform inference, shields client data from exposure but reveals the weights to the client.\nTo address this asymmetry, the client in our protocol performs the inference and then returns the residual light to the server as a verification state for security. Leveraging the quantum nature of light, we show that the server can compute an upper-bound on their weight leakage using the Holevo theorem [37], and that the client can upper-bound their data leakage using the Cram\u00e9r-Rao bound [38]."}, {"title": "2 Coherent linear algebra engine", "content": "Our protocol is based on a coherent optical linear algebra engine utilized by two parties in a network: the \"server\", which possesses a matrix W of dimensions M \u00d7 N representing a DNN linear layer, and the \u201cclient\u201d, which holds the data vector \\overrightarrow{x} of length N. The matrix-vector product W\\overrightarrow{x} is computed through M inner products W_i\\overrightarrow{x} for i\u2208 {1, ..., M}, each of which involves three steps:\n1. The server transmits a logical weight vector W_i encoded into the complex amplitudes of coherent states \\overrightarrow{w}.\nstates, $w_j$, encode the logical weights as $w_j = \\sqrt{W_{ij}}/||W||_{RMS}$, where $||W||_{RMS}$ denotes the root mean square of the elements in W. This encoding ensures that the average photon number is \u03bc, a value chosen by the server. These coherent states have a variance of 1 shot noise unit (SNU) in each quadrature, as illustrated in Fig. 1(c).\n2. The client operates on these amplitudes as follows, Fig. 1 (b):\n(i) The client calculates the inner product of the incoming weight vector \\overrightarrow{w} with its local input data vector \\overrightarrow{x} using a unitary transformation $U$ that is designed to yield $\\overrightarrow{w} \\cdot \\hat{x}$ into one mode of $U\\overrightarrow{w}$, where $\\hat{x}$ is obtained by the $l^2$ normalization of $\\overrightarrow{x}$. We call this mode the \"result\" mode.\n(ii) The client diverts the result mode, measures both quadratures to obtain $\\overrightarrow{w} \\cdot \\hat{x}$. Then, the client performs a feed-forward protocol by reinjecting light with the same quadratures back into the result mode. The feed-forwarded state preserves the measured state's expectation value $\\overrightarrow{w} \\cdot \\hat{x}$ but, due to quantum noise, consists of additional Gaussian noise of variance 1 SNU. The output of this measure and feed-forward step is labeled $M(U\\overrightarrow{w})$.\n(iii) The client finally applies the unitary transformation $U^{\\dagger}$ on $M(U\\overrightarrow{w})$ and returns $\\overrightarrow{p_u} = U^{\\dagger}M(U\\overrightarrow{w})$ as a verification state to the server for security checks. By the definition of $M(U\\overrightarrow{w})$ from the previous point, the first moment of $U^{\\dagger}M(U\\overrightarrow{w})$ is $\\overrightarrow{w}$, while the variance of each mode i has increased by $\\eta_i$, see Fig. 1(d).\n3. Finally, the server measures the variance of each mode i of the verification state in both quadratures, $(1 + \\eta_i)$ SNU, and calculates an upper bound for the leakage of each weight $w_{ij}$ to the client. Note that before this security check, the verification state can be re-used by other clients to perform their own inference as discussed in Section 5.\nThe measure and feed-forward operation $M$ outlined above can be generalized to an operation $G$ that consists of a combination of phase-insensitive amplification $G$ and a beam splitter with a split ratio $1 - \\frac{1}{G} : \\frac{1}{G}$; the former approach is obtained from the latter in the limit $G \\gg 1$ (see Section 7.1) [39]. In this general case, the client applies a gain G to the component $\\overrightarrow{w} \\cdot \\hat{x}$, passes it through a beam splitter with the above splitting ratio, and uses the output of the $1 - \\frac{1}{G}$ port for its inner product computation readout while reinjecting the output of the $\\frac{1}{G}$ port back into the result mode that originally carried $\\overrightarrow{w} \\cdot \\hat{x}$. Note that in order to obtain the desired inner product, the client scales the result digitally by $||\\overrightarrow{x}|| \\cdot ||\\overrightarrow{W}||_{RMS}$. However, for the purpose of classification tasks, the scaling step for the client is superfluous 7.2.\nThe gain G contributes amplification noise to the result mode. We calculate the variance of this mode in Section 7.1. This variance is larger than the standard shot noise by a quantity we refer to as the \"excess noise\". This excess noise is then spread via the operation of $U^{\\dagger}$ in the verification states modes, such that the excess noise in the ith mode of the verification state, $\\eta_i$, is weighted by $||\\hat{x_i}||^2$:\n$\\eta_i = (2 - \\frac{2}{\\sqrt{G}})\\frac{\\sqrt{G}-1}{\\sqrt{\\mu}}||\\hat{x_i}||^2$ (1)\nwhere $||\\hat{x_i}||$ is the ith element of the normalized data vector $\\hat{x}$."}, {"title": "3 Classification accuracy", "content": "In this section, we calculate the classification accuracy of a secure neural network that uses our coherent linear algebra engine on the standard MNIST classification task. To this end, we first trained a digital noiseless neural network on the MNIST dataset and got a classification accuracy of 98%. Then we fed the trained weights into a PyTorch model of our optical architecture to evaluate the test accuracy of our secure optical neural network, see Section 7.2 for more details.\nWe calculate the classification accuracy as a function of the average photon number occupation \u00b5 and the amplification gain G, see Section 7.2. We obtain more then 96% classification accuracy with an average photon number of less then \u03bc = 4 per weight and amplification gain of G = 3.\nIn contrast to previous studies in optical machine learning [30, 36], we do not measure all of the light sent to the client but only the portion corresponding to $(\\overrightarrow{w} \\cdot \\hat{x})^2$ and yet obtain similar high accuracies. This fundamentally follows from the fact that the SNR in the inner product is the similar whether one optically routes the inner product amplitude to one mode and measures it vs. measuring all the modes and calculating the inner product digitally."}, {"title": "4 Security analysis", "content": "4.1 Weights leakage\nIn this section, we use the verification state $\\rho_u$ returned to the server by the client to calculate an upper bound on the amount of information about $\\overrightarrow{w}$, in terms of number of bits, that could be learned by an arbitrary dishonest client. A dishonest client tries to learn the weights using arbitrary operations other than the operations defined by our protocol.\nHere, we assume that the dishonest operations are independent and identically distributed on all the incoming modes of $\\overrightarrow{w}$; the client employs separable ancilla states that interact individually with each weight mode. The ancilla states are stored in a quantum memory until the end of the attack and subsequently measured independently from one another to reveal information about $\\overrightarrow{w}$. Our security analysis for the weight leakage relies on results for quantum encryption [44-48] and proceeds via the Holevo theorem [37]; see Section 7.3.1 for the complete analysis.\nWe present here the final mathematical result for the weight leakage. Representing the weight occupation by $\\mu$ and defining the quantities:\na := 2\u03bc + 1\nb := 2\u03bc + 1 + \u03b7i\nc := $\\sqrt{4\u03bc^2 + 2\u03bc + 1}$\nz := $\\sqrt{(a + b)^2 \u2013 4c^2}$,\nthe weight leakage $I_w;$, is bounded by the Holevo theorem:\n$I_{w_i} \\leq g(v_1) + g(v_2) \u2013 g(v_3)$ (2)\nwhere\n$g(\\nu) = (\\frac{\\nu+1}{2})log_2(\\frac{\\nu+1}{2})-(\\frac{\\nu-1}{2})log_2(\\frac{\\nu-1}{2})$\n$v_{1,2} = \\frac{1}{2} (z \\pm [b-a])$, $v_3 = \\frac{c^2}{b+1}$\nThis inequality is shown to be tight by analyzing the entangling cloner attack [46]. The derivation of Eq. (2), and more details, can be found in Section 7.3.1.\nThe leakage is calculated for one query, that is, a single exposure of the client to the weights. To prevent information leakage after multiple uses of the model, the server sends the client invariants of the DNN model under affine transformations consisting of different individual weights while parameterizing the same model function and, therefore, conserving the inference output; see 7.3.2.\n4.2 Data leakage\nTo calculate the data leakage, we consider an honest client and a dishonest server. In this setting, the server sends a weight matrix W of shape M \u00d7 N to the client and aims to evaluate the client's data vector \\overrightarrow{x} of length N using M measurements of the verification state.\nWe recall from Section 2 that the mode i of the verification state $U^{\\dagger} G(U \\overrightarrow{x})$ has the mean w\u2081 and variance $(1 + \\eta_i SNU)$, Eq. (1). In other words, information about the client's data \\overrightarrow{x} is leaked to the server not through the mean of the verification state, but through its variance.\nThe client may control this leakage by reducing the gain G in its amplification-and-splitting step. We also note that since the client is honest, it announces the true gain G to the server."}, {"title": "5 Secure classification", "content": "Now that we have a mathematical handle on both the weight (Eq. 2) and the data leakage (Eq. 3), we can ask how they trade off against each other if one requires a constant test accuracy. For this purpose, we revisit the classification accuracy that we computed numerically as a function of the two hardware configuration parameters of the system, the server average photon occupation per weight and the client gain G (see Fig. 6 in the Methods section). Recalling that the weight leakage and the data leakage are both functions of the hardware configuration parameters, we perform a change of variables and replace the axes in Fig. 6 with weight and data leakage to obtain the tradeoff result in Fig. 2. Note that the data leakage is calculated for the quantum adversary (k = 2).\nThe figure depicts a clear tradeoff of the data leakage and weight leakage in order to maintain classification accuracy. This is because, when the server sends less energy per weight in order to limit weight leakage, the client has to increase the gain to preserve accuracy of the computation, thereby exposing more of its data. The data leakage increases monotonically with the gain and saturates, achieving the measure and feed-forward leakage. Therefore, as the server laser power is reduced and the client gain is increased, one moves from right to left on the accuracy isocontours in Fig. 2. For example, the 96% classification accuracy isocontour shows a weight leakage of less than $I_{w_i}$ = 0.1 bits per symbol and data leakage of less than $I_{x_i}$ = 0.01 bits per symbol.\nLow-precision inference is extensively studied in academia and industry, with research indicating that reliable inference typically requires at least 8 bits per weight, or as few as 1 bit using state-of-the-art quantization techniques [49]. This requirement is an order of magnitude higher than the upper bound on weight leakage for our protocol. Furthermore, assuming that a dishonest client does not have access to the training dataset, they would be unable to use the leaked information to infer the rest\nof the model through training.\nThe optical loss present between the different parties in real-life communication networks affects the leakage calculations we presented above. We analyze the effect of loss on security in Section 7 and show that weight leakage increases with loss, while data leakage is independent of channel loss; the loss-adjusted weight and data leakage are presented in Fig. 3(a). Here we chose an average photon number of \u03bc = 4 and amplification gain of G = 3, the values we used to yield a MNIST classification accuracy of 96%. For local-area network or metropolitan-area networks with fiber losses of up to 6 dB [50], the model leakage is at most 4 bits per weight.\nSince the number of neurons per layer of state-of-the-art DNNs is much larger than the size presented in our study, we present the dependence of the data and weight leakages on the number of neurons per layer in Fig. 3(b). As in pannel (a), we chose an average photon number of \u03bc = 4 and amplification gain of G = 3, the values we used to yield a MNIST classification accuracy of 96%. Interestingly, the leakage vanishes with the number of neurons per layer. This is a because the amplification excess noise is spread over more modes in the verification state, leading to less weight and data leakages. Therefore, advanced DNNs, which are much larger than the model used in this work, are expected to suffer from even smaller information leakage.\nFinally, we briefly introduce an extension of our protocol to a network consisting of a server and two clients or more, where each party assumes that all other parties are dishonest and collaborating against them. We propose two network architectures: symmetric and asymmetric.\nIn the symmetric network, the server splits the light encoding of the weights in half and sends each portion to a different client. The clients use the same gain for inference and return verification states to the server. The server coherently combines the verification states and measures the noise in the output. The added excess noise is doubled relative to a single client, resulting in an increase in weight leakage (Eq. (2)). However, the data leakage for each client does not change compared to a single client (Eq. (3)). Since the light received by each client is halved in power, the SNR drops by a factor of two for each client, leading to a reduction in classification accuracy. In the asymmetric scheme, client 1 uses all the light for inference and transmits its entire verification state to client 2. Client 2, after performing its own inference, sends the doubly disturbed verification state back to the server, which measures the excess noise. The advantage of the asymmetric architecture is that it eliminates the need for coherent combining of the verification states from both clients at the server."}, {"title": "6 Discussion", "content": "Our current protocol forces a trade-off between security for classification accuracy; future generations could relax this trade-off by joint optimization of DNN model parameters and protocol steps. Performance may be further improved by incorporating non-Gaussian operations into the protocol. For example, the client could use a photon-number-resolving detector, and the server could encode the weights using the photon number. Additionally, better performance may be achieved in a hardware-aware environment by optimizing the weights via physical custom layers instead of noiseless layers.\nOur security analysis for weight and data leakage assumes individual attacks, continuous modulation of the encoding state, and asymptotically large blocks of signals per communication. Recently, however, asymptotic and finite-size security analyses of the discrete modulated continuous-variable quantum key distribution have been introduced for coherent attacks [51, 52]. These results"}, {"title": "7 Methods", "content": "7.1 Optical implementation\nThe server consists of two modules, the transmitter and the receiver (see Fig. 4(a)). The transmitter module I/Q modulates the neural network weights onto a train of weak coherent states which are produced by attenuating a continuous wave laser to the few-photon limit. The receiver module measures the modulated quadratures of the incoming verification state using homodyne detection with a reference local oscillator. The optical power difference between the two output arms is proportional to either the I or the Q quadrature of the verification state, depending on the phase of the local oscillator.\nWe propose multiple optical implementations of the unitaries U and $U^{\\dagger}$ at the client. The client could operate via time domain encoding using optical loops [53] (see Fig. 4(b)). Equivalently, the client could operate by spatial domain encoding using a mesh of interferometers [54, 55] (see Fig. 4(c)) or free-space multi-plane light converters [56-58]. Time-domain implementations are promising for large-scale universal optical information processing because the number of elements does not scale with the number of optical modes [59-61]. Due to optical loss, however, both technologies are currently limited to a few dozen modes.\nIn the time domain design, optical modes are defined by temporally separate pulses. The first operation U is implemented by a Mach-Zehnder interferometer (MZI) and a fiber loop. This structure allows us to interfere successive pulses from the incoming pulse train \\overrightarrow{w} with a pulse in the loop that contains the running sum of the inner product, resulting in the final inner product $\\overrightarrow{w} \\cdot \\hat{x}$ being written into the complex amplitude of the last output pulse. The light in this mode is then amplified by a factor of G using a phase-insensitive amplifier such as a standard erbium-doped fiber amplifier. Then a beam splitter divides the light with a splitting ratio of 1-\\frac{1}{G} : \\frac{1}{G} for the transmitted and reflected ports. The transmitted port is then measured using homodyne detection in both quadratures to estimate the desired inner product. The reflected port is fed into a nested Mach-Zehnder fiber loop that implements the unitary $U^{\\dagger}$ and routes the resultant verification state back to the server.\nIn the spatial domain, optical modes are defined by spatially separated waveguides. Here, the client implements the same U and $U^{\\dagger}$ via two series of MZIs weighted according to the elements of $\\hat{x}$ [54]. These MZIs mix spatially adjacent modes to shift the inner product into the top mode, which is then amplified and measured via coherent homodyne detection as before."}, {"title": "7.1.1 Quantum description", "content": "Phase-insensitive amplifier\nWe use the quantum electromagnetic field ladder operators (or annihilation and creation operators), denoted by $\u00e2$ and $\u00e2^\\dagger$, respectively, to analyze the behavior of the system at the quantum level. The action of these operators on the photon number states |n\u27e9 is given by:\n$\u00e2|n\u27e9 = \\sqrt{n}|n-1\u27e9$ and $\u00e2^\\dagger|n\u27e9 = \\sqrt{n+1}|n+1\u27e9$\nThe input-output relationship of the ladder operators in the Heisenberg picture for a phase-insensitive amplifier with gain G is [62]:\n$\u00e2_{amp} = \\sqrt{G} \u00e2_{in} + \\sqrt{G-1} \u00e2_v$\nwhere $\u00e2_{in}$ is the annihilation operator for the input mode, $\u00e2_{amp}$ is the annihilation operator for the amplified output mode, and $\u00e2_v$ is the annihilation operator for the auxiliary vacuum noise mode introduced during amplification.\nThe quadrature operators are defined as:\n$\\hat{X} = \u00e2 + \u00e2^\\dagger$, $\\hat{P} = \\frac{1}{i}(\u00e2 - \u00e2^\\dagger)$\nThe variance for an operator \u00d4 is given by:\n$\\langle (\u0394\u00d4)^2 \\rangle = \\langle \u00d4^2 \\rangle - \\langle \u00d4 \\rangle^2$\nThen, the following is true for the input mode:\n$\\langle (\u0394\\hat{X}_{in})^2 \\rangle = 1$, $\\langle (\u0394\\hat{P}_{in})^2 \\rangle = 1$\nFor the output mode of the phase-insensitive amplifier:\n$\\hat{X}_{amp} = \u00e2_{amp} + \u00e2^\\dagger_{amp}$, $\\hat{P}_{amp} = \\frac{1}{i} (\u00e2_{amp} - \u00e2^\\dagger_{amp})$\nSubstituting the expression for $\u00e2_{amp}$:\n$\\hat{X}_{amp} = \\sqrt{G}\\hat{X}_{in} + \\sqrt{G-1}\\hat{X}_v$\n$\\hat{P}_{amp} = \\sqrt{G}\\hat{P}_{in} - \\sqrt{G-1}\\hat{P}_v$\nThe variance of the quadratures after amplification is:\n$\\langle (\u0394\\hat{X}_{amp})^2 \\rangle = \\langle (\\sqrt{G}\\hat{X}_{in} + \\sqrt{G-1}\\hat{X}_v)^2 \\rangle = 2G - 1$\n$\\langle (\u0394\\hat{P}_{amp})^2 \\rangle = 2G - 1$\nThis shows that the noise is amplified along with the signal, and for large G, the noise variance scales linearly with G.\nWeighted beamsplitter\nThe beam splitter has a splitting ratio of (1 - \\frac{1}{G} : \\frac{1}{G}). In the spatial domain picture, the input modes for the beam splitter are the amplified output $\u00e2_{amp}$ and a vacuum mode, labeled $\u00e2_{vac}$. The weighted beamsplitter transform yields:\n$\\hat{a}_{out1} = \\sqrt{1-\\frac{1}{G}} \u00e2_{amp} - \\frac{1}{\\sqrt{G}} \u00e2_{vac}$\n$\\hat{a}_{out2} = \\frac{1}{\\sqrt{G}} \u00e2_{amp} + \\sqrt{1-\\frac{1}{G}} \u00e2_{vac}$\nSubstituting the above expressions for $\\hat{X}_{amp}, \\hat{P}_{amp}$:\n$\\hat{X}_{out2} = \\frac{1}{\\sqrt{G}}\\hat{X}_{amp} + \\sqrt{1-\\frac{1}{G}}\\hat{X}_{vac}$\nand similarly for $\\hat{P}_{out2}$.\nSince $\\hat{X}_{vac}$ is the quadrature of the vacuum mode, it has unit variance: $\u27e8(\u0394\\hat{X}_{vac})^2\u27e9 = 1$.\nNotice that two vacuum modes have been introduced: one during amplification to preserve the commutation relation of the amplified output mode, and one at the dark input port of the beamsplitter. The two vacuum modes are independent and uncorrelated, so they can be summed in quadrature. One can see this directly by considering the reparametrization: $\\sqrt{G}$ \u2192 cosh(\u03b8) and $\\sqrt{G-1}$ \u2192 sinh(\u03b8) (note that $cosh^2(x) = sinh^2(x) + 1$ for any x). Their addition in quadrature forms a single vacuum noise mode with unit variance with a coefficient determined by the Pythagorean theorem sum of their composite variances.\nAlternatively, we can derive the noise variance directly as follows:\n$\u27e8(\u0394\\hat{X}_{out1})^2\u27e9 = \u27e8(\u0394\\hat{P}_{out1})^2\u27e9 = 1 + (2-\\frac{2}{\\sqrt{G}})$ (2-\\frac{3}{G})\nThe excess noise on the amplified signal after beam splitting is thus $(2-\\frac{2}{\\sqrt{G}})$.\nFor $\\hat{a}_{out2}$:\n$\\hat{X}_{out2} = \\sqrt{1-\\frac{1}{G}} \\hat{X}_{amp} - \\frac{1}{\\sqrt{G}} \\hat{X}_{vac}$\nand similarly for $\\hat{P}_{out2}$. Thus,\n$\u27e8(\u0394\\hat{X}_{out2})^2\u27e9 = \u27e8(\u0394\\hat{P}_{out2})^2\u27e9 = \\frac{2G^2 - 3G + 2}{G}$\nThe optical power at the client's detector, after gain and splitting, is $S = G(1 - \\frac{1}{G})|\\langle w \u00b7 \\hat{x}\u27e9|^2$. The SNR of the client's measurement is therefore:\n$SNR = \\frac{G(G-1)}{2G^2 - 3G + 2} |w \u00b7 \\hat{x}|^2$ (4)\nVerification state excess noise\nFirst, we consider the effect of the overall unitary transform pair {$U, U^\\dagger$} on the variance of the state. The initial input \\overrightarrow{w} is a set of coherent states with unit shot-noise variance in each mode, i.e., the noise itself is uncorrelated and isotropic. We describe the noise in these N modes, and the correlations between them, via the covariance matrix of their quadratures $\u03a3_{\\overrightarrow{X}}$. The (i, j)-th element of this matrix is given by $\\Sigma_{\\overrightarrow{X}}(i, j) = \u27e8[X_iX_j] - [E[X_i] E[X_j]\u27e9$.\nSince a coherent state is equivalent to a displacement operation on the vacuum state, the covariance matrix of \\overrightarrow{w} is $\u03a3 = \\hat{1}$. Since we want the first output mode (the result mode) of $U\\overrightarrow{w}$ to carry the inner product \\overrightarrow{w} \u00b7 \\hat{x}, $U$"}, {"title": "7.2 Classification accuracy calculation", "content": "To analyze our protocol's performance for deep learning, we compute the classification accuracy for the standard MNIST classification task using PyTorch. To this end, we wrote custom neural network layers that implement our coherent linear algebra engine presented in section 2; several of these custom layers were strung together to form a secure neural network. The detected homodyne current in each custom layer is sampled from a unit Gaussian distribution N(0, 1 SNU) to account for the quantum shot noise in each quadrature. The resultant noisy sample is then fed to the non-linear ReLU activation function before being passed on to the next custom layer.\nSeparately, we trained a digital model composed of standard PyTorch layers on preprocessed MNIST images. The preprocessing of the data set included the flattening of the 28 \u00d7 28 images to vectors of size 784, followed by centering and scaling of each vector to the range [-1, 1]. Then we trained a standard 2-layer network with 784 inputs, 784 hidden neurons, and 10 outputs, obtaining a classification accuracy of 98%. We then transferred these trained digital weights into the custom secure neural network and computed the accuracy achieved by the secure protocol as a function of the weight pulse energy. We found that the accuracy of the digital and analog models were in agreement up to variations caused by quantum shot noise.\nThe real operations required for standard deep learning tasks can be implemented on our complex-valued hardware by encoding the real input vector \\overrightarrow{x}^{(R)} of length N and the real weight matrix $W^{(R)}$ of size M \u00d7 N into a complex input vector \\overrightarrow{x}^{(C)} of length N/2 and a complex weight matrix $W^{(C)}$ of shape M \u00d7 N/2 using the following procedure:\n$W^{(C)}_{i,k} = \\frac{W^{(R)}_{i,2k-1} + iW^{(R)}_{i,2k}}{2}$ and $x^{(C)}_{k} = \\frac{x^{(R)}_{2k-1} + ix^{(R)}_{2k}}{2}$\nThe hardware computes the real value of the product $W^{(C)}x^{(C)}$, which is precisely the desired matrix-vector product $W^{(R)}x^{(R)}$ required for the DNN computations.\nWe define the signal-to-noise ratio (SNR) at the homodyne detector at the neural network outputs as\n$SNR = \\frac{G(G-1)}{2G^2 - 3G + 2}$\nwhere The amplitudes of these physical states, $w_j$, encode the logical weights as $w_j = \\frac{\\sqrt{W_{ij}}}{|W||_{RMS}}$, where $||W||_{RMS}$ denotes the root mean square of the elements in W. Therfore we have:\n$SNR_i = \\frac{G(G-1)}{2G^2 - 3G + 2} \\frac{\u03bc}{||W||_{RMS}} |W_i \u00b7 \\hat{x}|^2$\nNote that in order to get the desired inner product, the client scales the result digitally by $\\frac{||x|| \\cdot ||W||_{RMS}}{\\sqrt{\u03bc}}$. However, for the purpose of deep learning, the scaling step for the client is superfluous because the client normalizes between layers regardless. We define a \"physical scaling parameter\" F to capture the hardware-dependent prefactor in the SNR equation,\n$F:=\\sqrt{\\frac{G(G-1)}{2G^2 - 3G + 2}}$\nVarying F changes the SNR of the inner product outputs, thereby affecting the accuracy of the secure neural network. The purpose of defining this parameter is to enable the calculation of classification accuracy as a function of the gain G and average photon occupation \u03bc without directly executing the intensive numerical calculations in the two-dimensional space spanned by these two parameters. Instead, we calculate the classification accuracy within the space spanned by F. The dependency of the test accuracy achieved by the secure neural network on F is presented in Fig. 5. We further find that a logistic function faithfully fits the variation of the classification accuracy as a function of F:\n$\\frac{L}{Acc(F) = \\frac{1 + e^{-k(F-F_o)}}{\u03c3} + B}$\nwith Root-Mean-Square-Error (RMSE) of 1.1%. This fit allows us to use a convenient analytical formula to predict the classification accuracy as a function of the hardware parameters G and u, instead of using look-up tables.\nWe present the classification accuracy as a function of the average photon occupation per weight and the amplification gain at the client in Fig. 6. The classification accuracy increases with gain and average occupation number and asymptotically achieves the digital noiseless accuracy. At low gain values, the gain required to maintain a fixed classification accuracy is inversely proportional to the average photon occupation number per weight."}, {"title": "7.3 Detailed security analyses", "content": "We analyze the security of this protocol following the standard security analysis for continuous-variable quantum key distribution (CVQKD).\nFirst", "46": "n$\u03a3_{TMSVS"}, "begin{pmatrix} V\\hat{1} & \\sqrt{V^2 - 1} \u03c3_z \\\n\\sqrt{V^2 - 1} \u03c3_z & V\\hat{1} \\end{pmatrix}$\nHere, \u03c3z is the Pauli matrix and V represents the variance of the quadrature operators. The upper and lower diagonal 2 \u00d7 2 blocks of $\u03a3_{TMSVS}$ are the covariance matrices of the quadrature operators of Alice and Bob respectively, while the off-diagonal block matrices are the covariances between the quadratures of Alice and those of Bob. The variance V is related to the mean photon number per pulse by the relation \u03bc = \\frac{1}{2} (V \u2212 1). This variance is the sum of Alice's actual modulation variance of her two quadrature components and the vacuum shot-noise variance.\nAfter Bob's mode is transmitted through the channel with transmission coefficient T and excess noise \u03be, the covariance matrix transforms to:\n$\u03a3_{AB} = \\begin{pmatrix} V\\hat{1} & \\sqrt{T(V^2 - 1)} \u03c3_z \\\n\\sqrt{T(V^2 - 1)} \u03c3_z & (T(V - 1) + 1 + \u03be)\\hat{1} \\end{pmatrix}$\nWithout loss of generality, we consider |\u03c8\u27e9 as a bipartite state with Alice's and Bob's joint subsystem comprising one part and Eve's subsystem the other. Any pure bipartite state can be expressed using"]}