{"title": "Relating the Seemingly Unrelated: Principled Understanding of Generalization for Generative Models in Arithmetic Reasoning Tasks", "authors": ["Xingcheng Xu", "Zibo Zhao", "Haipeng Zhang", "Yanqing Yang"], "abstract": "Large language models (LLMs) have demonstrated impressive versatility across numerous tasks, yet their generalization capabilities remain poorly understood. To investigate these behaviors, arithmetic tasks serve as important venues. In previous studies, seemingly unrelated mysteries still exist \u2013 (1) models with appropriate positional embeddings can correctly perform longer unseen arithmetic operations such as addition, but their effectiveness varies in more complex tasks like multiplication; (2) models perform well for longer unseen cases in modular addition under specific moduli (e.g., modulo 100) but struggle under very close moduli (e.g., modulo 101), regardless of the positional encoding used. We believe previous studies have been treating the symptoms rather than addressing the root cause \u2013 they have paid excessive attention to improving model components, while overlooking the differences in task properties that may be the real drivers. This is confirmed by our unified theoretical framework for different arithmetic scenarios. For example, unlike multiplication, the digital addition task has the property of translation invariance which naturally aligns with the relative positional encoding, and this combination leads to successful generalization of addition to unseen longer domains. The discrepancy in operations modulo 100 and 101 arises from the base. Modulo 100, unlike 101, is compatible with the decimal system (base 10), such that unseen information in digits beyond the units digit and the tens digit is actually not needed for the task. Extensive experiments with GPT-like models validate our theoretical predictions. These findings deepen our understanding of the generalization mechanisms, and facilitate more data-efficient model training and objective-oriented AI alignment.", "sections": [{"title": "1 Introduction", "content": "Since the introduction of Transformer (Vaswani et al., 2017), Transformer-based models including large language models (LLMs) and large multimodal models (LMMs) have experienced a rapid rise. For example, models like GPT-4 (OpenAI, 2023), Claude (Anthropic, 2023), Gemini (Gemini et al., 2023), and Llama (Touvron et al., 2023a,b), along with their vision counterparts, have showcased impressive versatility. They excel in a wide range of tasks, including natural language processing, coding, mathematical reasoning, vision understanding, and more (Bubeck et al., 2023; Lu et al., 2024). However, the generalization capabilities of these foundation models are not yet fully understood in areas such as natural language understanding (Bender et al., 2021) and mathematical reasoning (Anil et al., 2022; Jelassi et al., 2023)."}, {"title": "3 Theoretical Analysis on Generalization for Arithmetic Reasoning", "content": "In this section, we review the Transformer model and the universal approximation theorem, and then conduct theoretical analyses of the inward and outward OOD generalization capabilities of the Transformer in solving tasks related to addition, modular addition, multiplication, and modular multiplication."}, {"title": "3.1 Preliminaries on Transformer and Universal Approximation", "content": "A Transformer model (Vaswani et al., 2017) predicts the next token based on the preceding tokens within the input sequence. Its output is subsequently used as input for the next prediction. For a target token $x_i$ at position $i$ in the sequence, the model generates a probability distribution over the vocabulary of potential next tokens. To be precise, let $x = x_1x_2 ... x_T \\in V^T$ denote the input sequence of tokens. The probability of observing this sequence with respect to a Transformer model is given as follows:\n$P_\\theta(x) = \\prod_{i=1}^{T}P_\\theta (x_i|x_1,x_2,..., x_{i-1}) = \\prod_{i=1}^{T}P_\\theta(x_i|x_{<i}).$\nThe conditional probability $P_\\theta(x_i|x_{<i})$ is computed using the softmax function applied to the last hidden state."}, {"title": "3.2 Theoretical Analysis on Addition", "content": "Consider two natural numbers $a = \\sum_{i=1}^{n}a_i \\times 10^{i-1} = (a_1,a_2,...,a_n)$ and $b = \\sum_{i=1}^{n}b_i \\times 10^{i-1} = (b_1,b_2,...,b_n)$. The addition of these n-digit numbers, denoted as $f(a,b) = a + b$, is expressed by $c = \\sum_{i=1}^{n+1} c_i \\times 10^{i-1} = (c_1, c_2,\\cdot\\cdot\\cdot,c_n,c_{n+1})$.\nLet the dataset $D_n := \\{(a, b) \\in N^2 : a_n \\lor b_n \\geq 1, a_i = b_i = 0,\\forall i > n\\}$. For notation simplicity, assume $(0,0) \\in D_1$. Here, $a_n \\lor b_n = max\\{a_n,b_n\\}$. Note that $D_n \\cap D_m = \\emptyset$ for $n \\neq m$ and $N^2 = \\bigcup_{n=1} D_n$. Denote the shorter-length (inward) domain $D_{<n} := \\bigcup_{m=1}^{n-1} D_m$ and the longer-length (outward) domain $D_{>n} := \\bigcup_{m=n+1} D_m$.\nTheorem 1. Assume a Transformer model with absolute positional embedding (APE) is trained on a multi-digit addition dataset for the operands $(a,b) \\in D_n (n \\geq 2)$ with enough training computation, then the learned model can perfectly generalize for the shorter-length OOD domain $D_{<n}$, but fail completely for the longer-length OOD domain $D_{>n}$.\nProof. Define the functions\n$\\chi(x) := [x/10]$ and $\\zeta(x) := x \\mod 10$, for $x \\in N$.\nThen $c_i = \\zeta(a_i + b_i + \\chi(a_{i-1}+b_{i-1})), \\forall i$. For simplicity, assume $a_0 = b_0 = 0$.\nWe define three forms of approximation:\n$\\bullet$ Strong form: If $P_\\theta(c = c_i | a + b = c_{<i}) = 1$ for any $i \\geq 1$. This means the model $P_\\theta(\\cdot | a+b = c_{<i})$ can perfectly learn the function $c_i = \\zeta(a_i + b_i + \\chi(a_{i-1}+b_{i-1})), \\forall i$.\n$\\bullet$ Standard form: If $c_i = \\argmax_{\\check{c}}P_\\theta(\\check{c} | a + b = c_{<i})$ for any $i \\geq 1$. This means the model $P_\\theta(\\cdot | a+b = c_{<i})$ can approximate the function $c_i = \\zeta(a_i + b_i + \\chi(a_{i-1}+b_{i-1})), \\forall i$ with the highest probability.\n$\\bullet$ Weak form: If $P_\\theta(c = c_i | a + b = c_{<i}) > 0$ for any $i \\geq 1$. This means the model $P_\\theta(\\cdot | a+b = c_{<i})$ can approximate the function $c_i = \\zeta(a_i + b_i + \\chi(a_{i-1}+b_{i-1})), \\forall i$ with a non-zero probability.\nIn the following, we will use the standard form to demonstrate out-of-distribution (OOD) generalization. When training a Transformer model on $D_n$-addition using absolute positional embedding (APE), the learned model approximates the function at each position of c:\n$P_\\theta(c_i | a+b = c_{<i}) = P_\\theta(c_i | a_{i-1},a_i,b_{i-1},b_i) \\rightarrow c_i = \\zeta(a_i + b_i + \\chi(a_{i-1}+b_{i-1})).$\nCase I: Let us consider the shorter-length OOD domain $D_{<n}$ case. If $i < n$, the model trained on a sample dataset in $D_n$ can at least approximate the function $c_i$ in the standard form. If $i = n$,\n$P_\\theta(c_n | a_{n-1}, a_n,b_{n-1},b_n) \\rightarrow c_n = \\zeta(a_n + b_n + \\chi(a_{n-1}+b_{n-1}))$\nfor every $a_n \\lor b_n \\geq 1$ except the case $a_n = b_n = 0$ simultaneously. If $i = n + 1$,\n$P_\\theta(c_{n+1} | a_{n}, a_{n+1},b_{n},b_{n+1}) \\rightarrow c_{n+1} = \\chi(a_{n+1}+b_{n+1}+\\chi(a_{n}+b_{n})) = \\chi(a_{n}+b_{n}) \\in \\{0,1\\}$\nfor every pair $(a_n,b_n)$ with $a_n \\lor b_n \\geq 1$ and $a_{n+1} = b_{n+1} = 0$. In the case where $a_n = b_n = 0$, the conditions for both $i = n$ and $i = n + 1$ necessitate OOD generalization. Since the model has been trained to approximate $c_n$ accurately for $a_n \\lor b_n \\geq 1$, it has learned the function for the carry-over mechanism properly. When $a_n = b_n = 0$, the digit $c_{n+1}$ purely depends on the carry from the previous position. For $i = n + 1$, the carry $\\chi(a_{n}+b_{n})$ is correctly learned such that it maps $\\{0,1\\}$ depending"}, {"title": "Remarks on APE and RPE", "content": "APE encodes positional information based on the absolute positions of tokens in a sequence. This approach can limit a model's ability to generalize to sequences of different lengths or to handle out-of-distribution scenarios effectively. In contrast, RPE captures translation-invariant positional dependencies by encoding the relative distances between tokens. This method allows the model to focus on the relationships between tokens regardless of their absolute positions, enhancing its ability to generalize across varying sequence lengths and to better understand contextual relationships. Consequently, RPE is more robust and adaptable in the addition context compared to APE. Our theoretical framework can explain the addition-based experimental findings reported in the following references: Jelassi et al. (2023), Xu et al. (2024), Duan et al. (2024), and McLeish et al. (2024)."}, {"title": "3.3 Theoretical Analysis on Modular Addition", "content": "Consider the function for modular addition with a modulus p, expressed as $f(a,b) = (a+b) \\mod p$, which will be the focus of our analysis in the following section. Subsequently, we will also represent modular addition using the notation $c^p = a + b^p$. For simplicity, we will omit the superscript p when it is clear from the context."}, {"title": "3.3.1 Scenarios on Divisibility of 10\u2019s Power by Modulus", "content": "Theorem 4. Assume a Transformer model with either absolute or relative/abacus positional embedding is trained on a multi-digit modular addition dataset with a modulus p that divides $10^m$ for the operands $(a,b) \\in D_n (n \\geq 2 and m \\leq n)$ with enough training computation, then the learned model can perfectly generalize both for the shorter-length OOD domain $D_{<n}$ and the longer-length OOD domain $D_{>n}$.\nSketch of Proof. We will initially focus on the scenario where $p = 10^m$, and subsequently explore the general case where p is a divisor of $10^m$.\nCase I: Let us revisit the equation for modular addition, which states that $\\bar{c}^p = a + b^p = \\bar{a}^p + \\bar{b}^p$. The above equation shows that for the case $p = 10^m$, the digits in positions higher than $m$ in numbers a and b do not affect the result $\\bar{c}^p$; only the digits in positions m and lower have an impact. Furthermore, we have $\\bar{c}^p = (c_1,\\cdots,c_m) = (c_1,c_2,\\cdot\\cdot\\cdot,c_m)$, where $c = a+b$. A model trained on $D_n$ is capable of approximating the digits at positions ranging from 1 to m. This can be expressed as:\n$P_\\theta(c_i|a_{i-1},a_i, b_{i-1},b_i) \\rightarrow c_i = \\zeta(a_i + b_i + \\chi(a_{i-1}+b_{i-1})), \\forall i = 1,\\cdots,m.$\nAll these functions are learned directly from the training data without the need for out-of-distribution (OOD) generalization if $m < n$, while $m = n$, only the n-th term h need OOD generalization. For $i > m$, the probability $P_\\theta(\\bar{c} | \\cdot) = 0$. The aforementioned conclusions apply to both domains $D_{<n}$ and $D_{>n}$.\nCase II: Consider the case where p is a divisor of $10^m$. Since we have $\\bar{c}^p = a + b^p = a + b^{-10^m}_p$, the result $\\bar{c}^p$ is indeed not influenced by the digits in positions higher than m in numbers a and b. If"}, {"title": "3.3.2 Scenarios on Non-Divisibility of 10\u2019s Power by Modulus", "content": "Theorem 5. (1) Assuming a Transformer model equipped with absolute positional embeddings is trained on a multi-digit modular addition dataset $D_n (n \\geq 2)$ where the modulus p neither divides $10^n$ nor exceeds $10^n$, and provided that sufficient training computation is allocated, then the resulting trained model is capable of perfect generalization to the shorter-length OOD domain $D_{<n}$, while encountering difficulties in generalizing to the longer-length OOD domain $D_{>n}$.\n(2) The function that the model has learned is $f^p(a,b) = a^{10^n}_p + b^{10^n}_p$.\n(3) Furthermore, the test accuracy on $D_{test} (n_{test} > n)$ is given by $Acc(p,n,n_{test}) \\approx \\frac{gcd(p,10^n)}{p}$ if $n_{test} \\geq n+log_{10}(p'/2+1)$, otherwise $Acc(p,n,n_{test}) = 0$, where $gcd(p, 10^n)$ represents the greatest common divisor of p and $10^n$, and $p' = \\frac{p}{gcd(p, 10^n)}$.\nSketch of Proof. In this case, the model approximates the function for each position i as follows when training on $D_n$:\n$P_\\theta(\\cdot | a_1,\\cdots,a_n,b_1,\\cdots,b_n) \\rightarrow c = f (a_1,\\cdots,a_n,b_1,\\cdots,b_n), \\forall i=1,\\cdots,n$\nwhere $f_i$ represents the function for c at position i. Generally, the function $f^p(a,b) = (a+b) - [(a+b)/p]p$. Each digit $f_i$ depends on all positions of a and b. If the model is trained on $D_n$, the aforementioned probabilities have been trained exclusively on scenarios where $a_n \\lor b_n \\geq 1$. The case where $a_n = b_n = 0$ requires OOD generalization for samples on the shorter-length domain $D_{<n}$. This can be addressed by aligning with the model trained on the domain containing $D_{n-1,n}$. If the model is trained on the dataset $D_{n-1,n}$, which includes the case where $a_n = b_n = 0$, it learns the relevant patterns directly from the training data without the need for OOD generalization on the domain $D_{<n}$. However, the model typically struggles to generalize to the longer-length domain $D_{>n}$. This is because the model is expected to approximate the functions $f^p(a,b) = a + b^p$, which consider all digits of a and b. Since the model is trained on $D_n$, it learns the function $f^p(a,b) = a^{10^n}_p + b^{10^n}_p$, which is independent of the positions $i > n$ of the numbers a and b.\nOOD Test Accuracy Analysis for Longer Length. For the model's output to be correct, it must satisfy the condition $a + b^p = a^{10^n}_p + b^{10^n}_p$. This requirement also provides us with a method to estimate the OOD test accuracy on the longer-length domain $D_{>n}$.\nLet $H_n = a^{10^n}_p + b^{10^n}_p$, and $R_n = (a+b) - H_n$. The OOD generalization error is then\n$f^p (a,b) - f^p (a,b) = R_n - ([(a+b)/p] - [H_n/p])p.$\nDenote $\\varepsilon := \\frac{R_n}{p} - [\\frac{R_n}{p}] \\in [0,1)$ and $\\varepsilon' := \\frac{H_n}{p} - [\\frac{H_n}{p}] \\in [0, 1)$. Then\n$f^p(a,b) - f^p (a,b) = (\\frac{R_n}{p}- [\\frac{R_n+H_n}{p}] + [\\frac{H_n}{p}])p = (\\varepsilon'-\\frac{R_n+H_n}{p}+\\varepsilon)p.$\nThat is,\n$f^p (a,b) - f^p (a,b) = \\begin{cases}\n\\varepsilon'p-\\varepsilon = 0, & \\text{if } \\varepsilon'+\\varepsilon \\in [0,1)\\\\\n(\\varepsilon'-1+\\varepsilon)p <0, & \\text{if } \\varepsilon'+\\varepsilon \\in (1,2)\n\\end{cases}.$\nFor the special case where $\\varepsilon = 0$ (i.e. $R_n$ is divisible by p), we have $f^p(a,b) = f^p(a,b)$. This implies that the OOD test accuracy for a finite OOD test dataset may be greater than 0."}, {"title": "3.4 Theoretical Analysis on Multiplication", "content": "Theorem 6. (1) Assuming a Transformer model equipped with absolute positional embeddings is trained on a multi-digit multiplication dataset $D_n (n \\geq 2)$, and provided that sufficient training computation is allocated, then the resulting trained model is capable of perfect generalization to the shorter-length OOD domain $D_{<n}$, while it cannot generalize to the longer-length OOD domain $D_{>n}$.\n(2) The function that the model has learned is $f(a,b) = a^{10^n} \\times b^{10^n}$.\nGiven two natural numbers a and b, each represented by n-digit sequences $(a_1,a_2,...,a_n)$ and $(b_1,b_2,...,b_n)$, respectively, the product ab is expressed as a 2n-digit number $c = (c_1,c_2,...,c_{2n})$.\nTo express each digit $c_i$ of the product c in terms of the digits of a and b, we need to understand the multiplication task and how the digits interact. The product ab can be represented as:\n$ab = (\\sum_{i=1}^n a_i \\times 10^{i-1}) (\\sum_{j=1}^n b_j \\times 10^{j-1}) = \\sum_{i=1}^{n} \\sum_{j=1}^{n} a_i b_j \\times 10^{(i-1)+(j-1)}$.\nThis gives us a double sum where each term $a_ib_j$ contributes to a specific power of 10. To express the digit $c_k$ (where $1 \\leq k \\leq 2n$) of the product, we need to collect all terms from the expansion that contribute to the $10^{k-1}$ place.\nFor $c_k$, we consider all pairs (i, j) such that $i + j - 2 = k - 1$, which simplifies to $i + j = k+1. Define that the raw sum $c^{raw}_k$ at the k-th position as follows:\n$c^{raw}_k = \\sum_{1 \\leq i,j \\leq n \\atop i+j=k+1}a_ib_j$.\nHowever, since this is a digital product and carries might affect higher places, the correct formulation needs to account for carries from previous steps. The process of digit-wise calculation and adjustment with carries are as follows:\n1. Initialize carry $c^0_k = 0$.\n2. Calculate the sum for each digit place:\n$S_i = c^{raw}_{i+1} + c^i_i = \\sum_{1 \\leq i,j \\leq n \\atop i+j'=i+1} a_i b_{j'} + c^i_i$.\nwhere $a_i$ and $b_j$ are zeros if their indices are out of bounds.\n3. Determine the digit and carry:\n$c_i = \\zeta(S_i), c^i_i = \\chi(S_i).$"}, {"title": "Transition Invariance Property in Multiplication", "content": "The transition invariance property for multiplication refers to the idea that the position of digits in the multiplication process can be shifted or \"transitioned\" in a systematic way that still respects the overall structure of multiplication. In the context of digit-wise multiplication, each digit $c_i$ should be adjusted by the previous carry. This process is transition invariant because each digit's place calculation transitions in a smooth and systematic way from one digit place to the next, maintaining the structure of the multiplication.\nTransformers can utilize properties like transition invariance to learn multiplication using proper positional embeddings such as relative or abacus PE. In fact, the structured nature of multiplication, especially when broken down into steps that involve digit-by-digit operations and carry propagation, aligns well with the capabilities of Transformer models to capture sequential dependencies and patterns. However, the most challenging aspect is computing the raw sums $c^{raw}_i$ at each position i. Each $c^{raw}_i$ results from a sum of specific pairs of digits from the input sequences a and b. For a given $c^{raw}_i$, the valid pairs $(i', j')$ must satisfy $i' + j' = i + 1$. Identifying these pairs involves that (1) ensuring $1 \\leq i', j' < n$, i.e., the indices must be within the bounds of the sequences. (2) For each i, determining which pairs contribute to $c^{raw}_i$ involves iterating through potential values of $i'$ and $j'$ and checking if their sum equals $i + 1$. Digit multiplication depends on the positional significance of digits. Misalignment in positions can lead to incorrect contributions to the product. Therefore, positional encoding and accurate handling of positional values are necessary to ensure correct multiplication results. There are also efficiency considerations. Multiplication of large numbers involves many such sums. For large n, directly computing $c^{raw}_i$ for each i involves nested loops or checks, leading to a time complexity of $O(n^2)$ in the worst case. This poses a great difficulty for computing the raw sum $c^{raw}_i$.\nThis challenge can be understood through the following analysis. Suppose the model is provided with Chain-of-Thought (CoT) style intermediate steps of multiplication as part of the training data. The CoT-like training data format is:\n$a \\times b \\rightarrow (c^{raw},c_i) \\rightarrow c.$\nIn digit-wise format, this is:\n$(a_1,\\cdots, a_n) \\times (b_1,...,b_n) \\rightarrow (c^{raw}_1,c_1,\\cdots, c^{raw}_{2n-1},c_{2n-1}, c^{raw}_{2n},c_{2n}) \\rightarrow (c_1,\\cdots, c_{2n})$.\nThe conditional probability equation is then given by:\n$P_\\theta(c_i | a_1,..., a_{i^n},b_1,..., b_{i^n}) = P_\\theta(c^{raw}_{i-1} | a_1,..., a_{(i-1)^n}, b_1,..., b_{(i-1)^n})\n\\times P_\\theta(c^{raw}_{i} | a_1,..., a_{i^n},b_1,..., b_{i^n})\n\\times P_\\theta(c_i | c^{raw}_i,c^i_i).$"}, {"title": "3.5 Theoretical Analysis on Modular Multiplication", "content": "Theorem 7. (1) Assume a Transformer model with either absolute or relative/abacus positional embedding is trained on a multi-digit modular multiplication dataset with a modulus p that divides $10^m$ for the operands $(a,b) \\in D_n (n \\geq 2 and m \\leq n)$ with enough training computation, then the learned model can perfectly generalize both for the shorter-length OOD domain $D_{<n}$ and the longer-length OOD domain $D_{>n}$.\n(2) If the modulus p neither divides $10^n$ nor exceeds $10^n$, and provided that sufficient training computation is allocated, then the resulting trained model is capable of perfect generalization to the shorter-length OOD domain $D_{<n}$, while encountering difficulties in generalizing to the longer-length OOD domain $D_{>n}$. The function that the model with APE has learned is $f^p(a,b) = a^{10^n}_p \\times b^{10^n}_p$.\nThe proof resembles the process for modular addition. Suppose $\\bar{c}^p = ab^p$. When p is a divisor of $10^m$, we have $\\bar{c}^p = ab = ab_{10^m}^p$. The value of $\\bar{c}^p$ remains unaffected by the digits in positions beyond m in the numbers a and b. Now, let m be the smallest number such that the m-th power of 10 is divisible by the modulus p, i.e., $m = arg \\min\\{m : p | 10^m\\}$. The model approximates the function for each position i as follows:\n$P_\\theta(c|a_1,\\cdots,a_m,b_1,\\cdots,b_m) \\rightarrow c = f (a_1,\\cdots,a_m,b_1,\\cdots,b_m), \\forall i = 1,\\cdots,m$\nwhere $f_i$ represents the function for the i-th digit of $\\bar{c}^p$. All these functions can be learned directly from the training data without the need for OOD generalization when training on $D_n (n \\geq m)$ except the term $c_h$.\nWhen p is not a divisor of $10^n$ and $p < 10^n$, the model approximates the function $f^p(a,b) = a^{10^n}_p \\times b^{10^n}_p$ at each position i.This is because the model has been trained on $D_n$, which is agnostic to the digits in positions $i > n$ of the numbers a and b."}, {"title": "4 Experiments", "content": "The aim of this section is to further validate our theoretical analysis through experiments. We first describe the experimental design, data format, and testing methods, and then conduct extensive"}, {"title": "4.1 Experimental Design", "content": "Model Description: We utilize a GPT model framework, which is a Transformer with a decoder-only architecture consisting of multiple layers and multi-head attentions. Our models are trained from scratch using NanoGPT, MicroGPT, and MiniGPT (Karpathy, 2023) with increasing model size, employing character-level tokenization and the cross-entropy loss function for the standard next-token prediction. The training focuses on basic arithmetic operations, including addition, multiplication, modular addition, and modular multiplication of integers. All experiments are conducted on a single GeForce RTX 4090 GPU. Detailed hyperparameters of the models and training are provided in Table 2.\nData Description: In this section, we delve into the four primary arithmetic operations described above, which include the following:\n$\\bullet$ Addition: $c = a + b$.\n$\\bullet$ Modular addition: $c = a + b \\pmod{p}$.\n$\\bullet$ Multiplication: $c = a \\times b$.\n$\\bullet$ Modular multiplication: $c = a \\times b \\pmod{p}$.\nWe generate multiple datasets randomly for each arithmetic task. Each dataset is organized as a sequence of operand pairs in natural order, with the results of the operations in reversed order. This format has been shown to be more effective for learning in next-token prediction models (Lee et al., 2023; Xu et al., 2024). For example, consider an n-digit addition $a + b = c$, represented in standard format as \"$a_n... a_2a_1 + b_n... b_2b_1 = c_{n+1}...c_2c_1$\". By reversing the order of the output \"c\", we obtain the reversed data format \"$a_n... a_2a_1+b_n...b_2b_1 = c_1...c_nc_{n+1}$\".\nSubsequently, the data undergoes character-level tokenization, and we add \";\", \"<bos>\", and \"<eos>\", a \"line break\" token, resulting in a vocabulary size of 16. When the context window exceeds the required size for n-digit arithmetic operations, we pad zeros before the numbers \"a\", \"b\", and \"c\".\nWe control the length of arithmetic operations n and randomly generate multiple datasets from $D_n$ for different lengths n. These datasets for each arithmetic task are categorized into three distinct subsets: a training set, an in-distribution (ID) test set, and several additional out-of-distribution (OOD) test sets, sampled from m-digit operations with $m \\neq n$. The case where $m < n$ is referred to as the shorter-length (inward) OOD domain, and the case where $m > n$ is termed the longer-length (outward) OOD domain. We also construct numerous combination sets of samples from different domains $D_n$, such as $D_{n-1,n}$, to be used as training and ID test datasets in our work. In this case, the OOD test sets are from $D_m$ with $m \\neq n - 1$ and n. In the experiments presented in this paper, test accuracy is measured using maximum probability sampling."}, {"title": "4.2 Experiments on Addition", "content": "In this subsection, we trained multiple models on different datasets (e.g. $D_4, D_5, D_{4,5}$) and tracked the changes in their accuracy over the course of training on various test sets. Additionally, we demonstrated how the models learn each digit during the training process."}, {"title": "4.2.1 Generalization for Different Digit Tasks", "content": "In Figure 1, we present the results of three different experiments using distinct training datasets (i.e., $D_4, D_5, D_{4,5}$). For all experiments, we employ the MiniGPT model equipped with a learned APE. Each subfigure illustrates the test accuracy on different test domains $D_i$ for these models throughout the training process. Figure 1 verifies our Theorem 1. It demonstrates that models incorporating APE are unable to generalize to longer digits than those they are trained on but can succeed with lower digits. Additionally, the model trained on $D_5$ has a much more challenging training process compared to the model trained on $D_4$, while the model trained on $D_{4,5}$ experiences the easiest and smoothest training process among the three models. The reason, as explained in Theorem 1, is that for $D_{4,5}$, the model learns addition tasks on lower digits directly from the training data. In contrast, $D_4$ and $D_5$ require OOD generalization for the edge positions.\nWe also conduct extensive experiments using various training datasets, model scales, and data scales. The results of these experiments are robust, and presented in Appendix C.1."}, {"title": "4.2.2 Learning Dynamics for Each Digit Position", "content": "The models and training datasets are identical to those described in Figure 1. We have assembled a comprehensive test dataset that contains a random sample from $D_1$ to $D_9$. Our objective is to demonstrate how these Transformer models equipped with APE learn each digit at every position"}, {"title": "4.2.3 Generalization Under Relative/Abacus Positional Embeddings", "content": "McLeish et al. (2024) conducted experiments using a 16-layer Transformer (decoder only) model with abacus positional embedding, trained on a random sample from $D_{<20}$. It can generalize on 100-digit addition problems (see Figure 7 in Appendix C.1). Additionally, Jelassi et al. (2023) demonstrated that relative positional embeddings enable length generalization in addition tasks. In their work, models such as Transformer and Universal Transformer (encoder only) trained to add 5-digit numbers could generalize to 20-digit operands."}, {"title": "4.3 Experiments on Modular Addition", "content": "The results in Table 3 validate Theorem 4, which states that Transformer models with absolute positionalembeddings trained on multi-digit modular addition datasets exhibit distinct generalization capabilities based on the modulus p. For moduli such as p = 50, 100, 200 that divide $10^n$, the models achieve perfect test accuracy across all digit domains, demonstrating their ability to generalize flawlessly to both shorter-length and longer-length OOD domains. In contrast, for moduli such as p = 51, 101, 150, 151, 201 that do not divide $10^n$, the models maintain high accuracy for lower digit domains but show significant performance degradation for higher digit positions.5\nThe OOD test accuracy in Table 3 for high-order digits can be completely expected using Theorem 5, which states that the test accuracy on $D_{ntest} (n_{test} > n)$ is given by $Acc(p,n,n_{test}) \\approx 1/p'$ if $n_{test} \\geq n+log_{10}(p'/2+1)$, otherwise $Acc(p,n,n_{test}) = 0$. These observations align well with the theoretical expectations outlined in Theorem 4 and Theorem 5, also explaining the experimental results found in the literature (see, e.g., Jelassi et al. (2023)) in handling modular addition tasks with different moduli.\nFurthermore, the results in Table 4 support Theorem 5, indicating that Transformer models with absolute positional embeddings trained on multi-digit modular addition datasets learns the function $f^p(a,b) = a^{10^n}_p + b^{10^n}_p$ for any modulus p. These findings fully align with the theoretical predictions."}, {"title": "4.4 Experiments on Multiplication and Modular Multiplication", "content": "We also conducted extensive experimental analyses for multiplication and modular multiplication tasks, examining the performance and generalization capabilities of Transformer models. These experiments are designed to test various configurations, including different positional encodings, model size and training data schemes. Detailed results and additional analyses are available in Appendix C.3 and C.4. The experimental outcomes consistently support our theoretical framework, demonstrating the robustness of our approach and providing further insights into the behavior of Transformer models in arithmetic reasoning tasks."}, {"title": "5 Discussion", "content": "Our study sheds light on the mechanistic interpretability and AI alignment of Transformer models. Understanding the mechanisms of Transformer models is crucial for ensuring their alignment with desired outcomes. Our theoretical framework provides a pathway for interpreting how these models generalize from training data to unseen tasks. This understanding is essential for aligning models with human-defined objectives, and reducing the risk of unintended behaviors.\nWe also observed phenomena akin to the satori phenomenon and emergence, where models suddenly exhibit a leap in understanding or capability once a critical threshold in training or data complexity is reached. This emergent behavior underscores the non-linear nature of model learning and highlights the need for further research into the conditions that trigger such phenomena.\nAdditionally, our work identifies challenges associated with different training data schemes, such as concatenation training without padding (e.g. \"123+45 = 168;267 + 1 = 268;\" as an input sequence) and line-by-line padding training (e.g. \"123+45 = 168;<pad><pad><pad>\" as an input sequence). These approaches can significantly impact model performance and generalization. Understanding these problems is essential for refining training strategies to improve model robustness and generalization."}, {"title": "6 Conclusion", "content": "In this paper, we developed a unified theoretical framework to explain various OOD generalization phenomena in Transformer models trained on arithmetic operations. This framework provides a principled understanding of why and how these models generalize across different scenarios, including n-digit addition, multiplication, and modular operations. We categorized generalization into inward OOD (generalizing to shorter-length domains) and outward OOD (generalizing to longer-length domains) to clearly delineate these behaviors.\nOur theoretical analysis concludes that Transformer models with absolute positional encoding can generalize to the shorter-length OOD domain for addition, but not the longer-length domain. Relative positional encoding allows generalization to both shorter- and longer-length domains, benefiting from the translation invariance of digit addition. For multiplication, even relative positional encoding is less effective in the longer-length domain due to the lack of translation invariance. For modular operations, models generalize well to both shorter- and longer-length domains if the modulus p divides $10^n$, regardless of positional encoding, due to the compatibility with base 10 where higher-digit positions do not affect the result. When p does not divide $10^n$, models only generalize to the shorter-length domain, with theoretical accuracy derived for longer-length domains based on information loss and the identification of the model's final learned function.\nThrough extensive experimental validation using NanoGPT, MicroGPT, and MiniGPT, we have supported our theoretical predictions. These experiments confirmed the robustness of our framework across different model scales, dataset sizes, and training data schemes. This work clarifies the mechanisms underlying generalization, addresses misconceptions in previous studies, and has significant implications for data-efficient model training and objective-oriented AI alignment, including for large language models (LLMs).\nFuture research should focus on extending this theoretical framework to more complex tasks and exploring additional factors that might influence OOD generalization. By continuing to build on this foundation, we can move closer to developing AI systems that are both powerful and reliably generalizable."}]}