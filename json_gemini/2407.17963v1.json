{"title": "Relating the Seemingly Unrelated: Principled Understanding of Generalization for Generative Models in Arithmetic Reasoning Tasks", "authors": ["Xingcheng Xu", "Zibo Zhao", "Haipeng Zhang", "Yanqing Yang"], "abstract": "Large language models (LLMs) have demonstrated impressive versatility across numerous tasks, yet their generalization capabilities remain poorly understood. To investigate these behaviors, arithmetic tasks serve as important venues. In previous studies, seemingly unrelated mysteries still exist \u2013 (1) models with appropriate positional embeddings can correctly perform longer unseen arithmetic operations such as addition, but their effectiveness varies in more complex tasks like multiplication; (2) models perform well for longer unseen cases in modular addition under specific moduli (e.g., modulo 100) but struggle under very close moduli (e.g., modulo 101), regardless of the positional encoding used. We believe previous studies have been treating the symptoms rather than addressing the root cause \u2013 they have paid excessive attention to improving model components, while overlooking the differences in task properties that may be the real drivers. This is confirmed by our unified theoretical framework for different arithmetic scenarios. For example, unlike multiplication, the digital addition task has the property of translation invariance which naturally aligns with the relative positional encoding, and this combination leads to successful generalization of addition to unseen longer domains. The discrepancy in operations modulo 100 and 101 arises from the base. Modulo 100, unlike 101, is compatible with the decimal system (base 10), such that unseen information in digits beyond the units digit and the tens digit is actually not needed for the task. Extensive experiments with GPT-like models validate our theoretical predictions. These findings deepen our understanding of the generalization mechanisms, and facilitate more data-efficient model training and objective-oriented AI alignment.", "sections": [{"title": "1 Introduction", "content": "Since the introduction of Transformer (Vaswani et al., 2017), Transformer-based models including large language models (LLMs) and large multimodal models (LMMs) have experienced a rapid rise. For example, models like GPT-4 (OpenAI, 2023), Claude (Anthropic, 2023), Gemini (Gemini et al., 2023), and Llama (Touvron et al., 2023a,b), along with their vision counterparts, have showcased impressive versatility. They excel in a wide range of tasks, including natural language processing, coding, mathematical reasoning, vision understanding, and more (Bubeck et al., 2023; Lu et al., 2024). However, the generalization capabilities of these foundation models are not yet fully understood in areas such as natural language understanding (Bender et al., 2021) and mathematical reasoning (Anil et al., 2022; Jelassi et al., 2023)."}, {"title": "2 Related Work", "content": "Generalization of Transformers and LLMs on Arithmetic. Numerous studies have examined the performance of Transformer-based language models in tasks involving arithmetic operations and mathematical reasoning. Brown et al. (2020), Bubeck et al. (2023) and Lu et al. (2024) investigated various LLMs, such as GPT-3, GPT-4, and Gemini, in performing basic arithmetic and mathematical"}, {"title": "3 Theoretical Analysis on Generalization for Arithmetic Reasoning", "content": "In this section, we review the Transformer model and the universal approximation theorem, and then conduct theoretical analyses of the inward and outward OOD generalization capabilities of the Transformer in solving tasks related to addition, modular addition, multiplication, and modular multiplication."}, {"title": "3.1 Preliminaries on Transformer and Universal Approximation", "content": "A Transformer model (Vaswani et al., 2017) predicts the next token based on the preceding tokens within the input sequence. Its output is subsequently used as input for the next prediction. For a target token $x_i$ at position $i$ in the sequence, the model generates a probability distribution over the vocabulary of potential next tokens. To be precise, let $x = x_1x_2 ... x_T \\in \\mathcal{V}^T$ denote the input sequence of tokens. The probability of observing this sequence with respect to a Transformer model is given as follows:\n\n$P_\\theta(x) = \\prod_{i=1}^T P_\\theta(x_i | x_1, x_2, ..., x_{i-1}) = \\prod_{i=1}^T P_\\theta(x_i | x_{<i}).$\n\nThe conditional probability $P_\\theta(x_i | x_{<i})$ is computed using the softmax function applied to the last hidden state."}, {"title": "3.2 Theoretical Analysis on Addition", "content": "Consider two natural numbers $a = \\sum_{i=1}^n a_i \\times 10^{i-1} = (a_1, a_2, \\dots, a_n)$ and $b = \\sum_{i=1}^n b_i \\times 10^{i-1} = (b_1, b_2, ..., b_n)$. The addition of these n-digit numbers, denoted as $f(a, b) = a + b$, is expressed by $c = \\sum_{i=1}^{n+1} c_i \\times 10^{i-1} = (c_1, c_2, \\dots, c_n, c_{n+1})$.\n\nLet the dataset $D_n := \\{(a, b) \\in \\mathbb{N}^2 : a_n \\lor b_n \\ge 1, a_i = b_i = 0,\\forall i > n\\}$. For notation simplicity, assume $(0, 0) \\in D_1$. Here, $a_n \\lor b_n = \\max\\{a_n, b_n\\}$. Note that $D_n \\cap D_m = \\emptyset$ for $n \\ne m$ and $\\mathbb{N}^2 = \\bigcup_{n=1} D_n$. Denote the shorter-length (inward) domain $D_{<n} := \\bigcup_{m=1}^{n-1} D_m$ and the longer-length (outward) domain $D_{>n} := \\bigcup_{m=n+1} D_m$.\n\nTheorem 1. Assume a Transformer model with absolute positional embedding (APE) is trained on a multi-digit addition dataset for the operands $(a, b) \\in D_n (n \\ge 2)$ with enough training computation, then the learned model can perfectly generalize for the shorter-length OOD domain $D_{<n}$, but fail completely for the longer-length OOD domain $D_{>n}$.\n\nProof. Define the functions\n\n$\\chi(x) := [x/10]$ and $\\zeta(x) := x \\mod 10$, for $x \\in \\mathbb{N}$.\n\nThen $c_i = \\zeta(a_i + b_i + \\chi(a_{i-1} + b_{i-1})), \\forall i$. For simplicity, assume $a_0 = b_0 = 0$.\n\nWe define three forms of approximation:\n\n\\begin{itemize}\n    \\item Strong form: If $P_\\theta(c = c_i | a + b = c_{<i}) = 1$ for any $i \\ge 1$. This means the model $P_\\theta(\\cdot | a + b = c_{<i})$ can perfectly learn the function $c_i = \\zeta(a_i + b_i + \\chi(a_{i-1} + b_{i-1})), \\forall i$.\n    \\item Standard form: If $c_i = \\text{argmax}_c P_\\theta(\\tilde{c} | a + b = c_{<i})$ for any $i \\ge 1$. This means the model $P_\\theta(\\cdot | a + b = c_{<i})$ can approximate the function $c_i = \\zeta(a_i + b_i + \\chi(a_{i-1} + b_{i-1})), \\forall i$ with the highest probability.\n    \\item Weak form: If $P_\\theta(c = c_i | a + b = c_{<i}) > 0$ for any $i \\ge 1$. This means the model $P_\\theta(\\cdot | a + b = c_{<i})$ can approximate the function $c_i = \\zeta(a_i + b_i + \\chi(a_{i-1} + b_{i-1})), \\forall i$ with a non-zero probability.\n\\end{itemize}\n\nIn the following, we will use the standard form to demonstrate out-of-distribution (OOD) general-ization. When training a Transformer model on $D_n$-addition using absolute positional embedding (APE), the learned model approximates the function at each position of c:\n\n$P_\\theta(c_i | a + b = c_{<i}) = P_\\theta(c_i | a_{i-1}, a_i, b_{i-1}, b_i) \\rightarrow c_i = \\zeta(a_i + b_i + \\chi(a_{i-1} + b_{i-1})).$\n\nCase I: Let us consider the shorter-length OOD domain $D_{<n}$ case. If $i < n$, the model trained on a sample dataset in $D_n$ can at least approximate the function $c_i$ in the standard form. If $i = n$,\n\n$P_\\theta(c_n | a_{n-1}, a_n, b_{n-1}, b_n) \\rightarrow c_n = \\zeta(a_n + b_n + \\chi(a_{n-1} + b_{n-1}))$\n\nfor every $a_n \\lor b_n \\ge 1$ except the case $a_n = b_n = 0$ simultaneously. If $i = n + 1$,\n\n$P_\\theta(c_{n+1} | a_n, a_{n+1}, b_n, b_{n+1}) \\rightarrow c_{n+1} = \\chi(a_{n+1} + b_{n+1} + \\chi(a_n + b_n)) = \\chi(a_n + b_n) \\in \\{0, 1\\}$\n\nfor every pair $(a_n, b_n)$ with $a_n \\lor b_n \\ge 1$ and $a_{n+1} = b_{n+1} = 0$. In the case where $a_n = b_n = 0$, the conditions for both $i = n$ and $i = n + 1$ necessitate OOD generalization. Since the model has been trained to approximate $c_n$ accurately for $a_n \\lor b_n \\ge 1$, it has learned the function for the carry-over mechanism properly. When $a_n = b_n = 0$, the digit on $c_n$ purely depends on the carry from the previous position. For $i = n + 1$, the carry $\\chi(a_n + b_n)$ is correctly learned such that it maps $\\{0, 1\\}$ depending"}, {"title": "3.3 Theoretical Analysis on Modular Addition", "content": "Consider the function for modular addition with a modulus $p$, expressed as $f(a, b) = (a + b) \\mod p$, which will be the focus of our analysis in the following section. Subsequently, we will also represent modular addition using the notation $c^p = a + b^p$. For simplicity, we will omit the superscript $p$ when it is clear from the context."}, {"title": "3.3.1 Scenarios on Divisibility of 10's Power by Modulus", "content": "Theorem 4. Assume a Transformer model with either absolute or relative/abacus positional embed-ding is trained on a multi-digit modular addition dataset with a modulus $p$ that divides $10^m$ for theoperands $(a, b) \\in D_n (n \\ge 2 \\text{ and } m \\le n)$ with enough training computation, then the learned modelcan perfectly generalize both for the shorter-length OOD domain $D_{<n}$ and the longer-length OODdomain $D_{>n}$.\n\nSketch of Proof. We will initially focus on the scenario where $p = 10^m$, and subsequently explorethe general case where $p$ is a divisor of $10^m$.\n\nCase I: Let us revisit the equation for modular addition, which states that $\\tilde{c} = a + b \\mod p = \\tilde{a} + \\tilde{b}$.The above equation shows that for the case $p = 10^m$, the digits in positions higher than $m$ in numbers a andb do not affect the result$\\tilde{c}^p$; only the digits in positions m and lower have an impact. Furthermore,we have $\\tilde{c}^p = (\\tilde{c}_1, \\tilde{c}_2, \\dots, \\tilde{c}_m) = (c_1, c_2, \\dots, c_m)$, where $c = a + b$. A model trained on $D_n$ is capableof approximating the digits at positions ranging from 1 to m. This can be expressed as:\n\n$P_\\theta(c^i | a_i-1, a_i, b_{i-1}, b_i) \\rightarrow c^i = \\zeta(a_i + b_i + \\chi(a_{i-1} + b_{i-1})), \\forall i = 1, \\dots, m$.\n\nAll these functions are learned directly from the training data without the need for out-of-distribution(OOD) generalization if $m < n$, while $m = n$, only the n-th term $c^n$ need OOD generalization. For$i > m$, the probability $P_\\theta(\\tilde{c}^i | \\cdot) = 0$. The aforementioned conclusions apply to both domains $D_{<n}$and $D_{>n}$.\n\nCase II: Consider the case where p is a divisor of $10^m$. Since we have $\\tilde{c}^p = a + b \\mod p = a+b \\mod 10^m$, the"}, {"title": "3.3.2 Scenarios on Non-Divisibility of 10's Power by Modulus", "content": "Theorem 5. (1) Assuming a Transformer model equipped with absolute positional embeddings is trained on a multi-digit modular addition dataset $D_n (n \\ge 2)$ where the modulus $p$ neither divides $10^n$ nor exceeds $10^n$, and provided that sufficient training computation is allocated, then the resulting trained model is capable of perfect generalization to the shorter-length OOD domain $D_{<n}$, while encountering difficulties in generalizing to the longer-length OOD domain $D_{>n}$.\n\n(2) The function that the model has learned is $f^p(a, b) = \\tilde{a}_{10^n} + \\tilde{b}_{10^n}$.\n\n(3) Furthermore, the test accuracy on $D_{test} (n_{test} > n)$ is given by $Acc(p, n, n_{test}) \\approx \\frac{gcd(p, 10^n)}{p}$ if $n_{test} \\ge n + log_{10}(p'/2+1)$, otherwise $Acc(p, n, n_{test}) = 0$, where $gcd(p, 10^n)$ represents the greatest common divisor of p and $10^n$, and $p' = p/gcd(p, 10^n)$.\n\nSketch of Proof. In this case, the model approximates the function for each position i as follows when training on $D_n$:\n\n$P_\\theta(\\tilde{c}^i | a_1, \\dots, a_n, b_1, \\dots, b_n) \\rightarrow \\tilde{c}^i = f^i(a_1, \\dots, a_n, b_1, \\dots, b_n), \\forall i = 1, \\dots, n$\n\nwhere $f^i$ represents the function for $c^i$ at position i. Generally, the function $f^p(a, b) = (a + b) - [\\frac{(a + b)}{p}]p$. Each digit $f^i$ depends on all positions of a and b. If the model is trained on $D_n$, the aforementioned probabilities have been trained exclusively on scenarios where $a_n \\lor b_n \\ge 1$. The case where $a_n = b_n = 0$ requires OOD generalization for samples on the shorter-length domain $D_{<n}$. This can be addressed by aligning with the model trained on the domain containing $D_{n-1,n}$. If the modelis trained on the dataset $D_{n-1,n}$, which includes the case where $a_n = b_n = 0$, it learns the relevantpatterns directly from the training data without the need for OOD generalization on the domain$D_{<n}$. However, the model typically struggles to generalize to the longer-length domain $D_{>n}$. This isbecause the model is expected to approximate the functions $f^p(a, b) = a + b \\mod p$, which consider all digits of a and b. Since the model is trained on $D_n$, it learns the function $f^p(a, b) = \\tilde{a}_{10^n} + \\tilde{b}_{10^n}$,which is independent of the positions $i > n$ of the numbers a and b."}, {"title": "3.4 Theoretical Analysis on Multiplication", "content": "Theorem 6. (1) Assuming a Transformer model equipped with absolute positional embeddings is trained on a multi-digit multiplication dataset $D_n (n \\ge 2)$, and provided that sufficient training computation is allocated, then the resulting trained model is capable of perfect generalization to the shorter-length OOD domain $D_{<n}$, while it cannot generalize to the longer-length OOD domain $D_{>n}$.\n\n(2) The function that the model has learned is $f(a, b) = \\tilde{a}_{10^n} \\times \\tilde{b}_{10^n}$.\n\nGiven two natural numbers a and b, each represented by n-digit sequences $(a_1, a_2, ..., a_n)$ and $(b_1, b_2, ..., b_n)$, respectively, the product ab is expressed as a 2n-digit number $c = (c_1, c_2, ..., c_{2n})$.\n\nTo express each digit $c_i$ of the product c in terms of the digits of a and b, we need to understand the multiplication task and how the digits interact. The product ab can be represented as:\n\n$ab = (\\sum_{i=1}^n a_i \\cdot 10^{i-1}) (\\sum_{j=1}^n b_j \\cdot 10^{j-1}) = \\sum_{i=1}^n \\sum_{j=1}^n a_i b_j \\cdot 10^{(i-1)+(j-1)}.$\n\nThis gives us a double sum where each term $a_i b_j$ contributes to a specific power of 10. To express the digit $c_k$ (where $1 \\le k \\le 2n$) of the product, we need to collect all terms from the expansion that contribute to the $10^{k-1}$ place.\n\nFor $c_k$, we consider all pairs (i, j) such that i + j - 2 = k - 1, which simplifies to i + j = k + 1. Define that the raw sum $c_k^{raw}$ at the k-th position as follows:\n\n$c_k^{raw} = \\sum_{\\substack{1 \\le i, j \\le n\\\\i+j=k+1}} a_i b_j$.\n\nHowever, since this is a digital product and carries might affect higher places, the correct formulation needs to account for carries from previous steps. The process of digit-wise calculation and adjustment with carries are as follows:\n\n1. Initialize carry $c_0^c = 0$.\n\n2. Calculate the sum for each digit place:\n\n$S_i = c_{i+1}^{raw} + c_i^c = \\sum_{\\substack{1 \\le i, j \\le n\\\\i+j'=i+1}} a_{i'}b_{j'} + c_i^c$\n\nwhere $a_i$ and $b_j$ are zeros if their indices are out of bounds.\n\n3. Determine the digit and carry:\n\n$c_i = \\zeta(S_i), c_i^c = \\chi(S_i)$."}, {"title": "3.5 Theoretical Analysis on Modular Multiplication", "content": "Theorem 7. (1) Assume a Transformer model with either absolute or relative/abacus positional embedding is trained on a multi-digit modular multiplication dataset with a modulus $p$ that divides $10^m$ for the operands $(a, b) \\in D_n (n \\ge 2 \\text{ and } m \\le n)$ with enough training computation, then the learned model can perfectly generalize both for the shorter-length OOD domain $D_{<n}$ and the longer-length OOD domain $D_{>n}$.\n\n(2) If the modulus $p$ neither divides $10^n$ nor exceeds $10^n$, and provided that sufficient training computation is allocated, then the resulting trained model is capable of perfect generalization to the shorter-length OOD domain $D_{<n}$, while encountering difficulties in generalizing to the longer-length OOD domain $D_{>n}$. The function that the model with APE has learned is $f^p(a, b) = \\tilde{a}_{10^n} \\times \\tilde{b}_{10^n}$.\n\nThe proof resembles the process for modular addition. Suppose $\\tilde{c}^p = a b \\mod p$. When $p$ is a divisor of$10^m$, we have $\\tilde{c}^p = a b \\mod 10^m$. The value of $\\tilde{c}^p$ remains unaffected by the digits in positions beyond m in the numbers a and b. Now, let m be the smallest number such that the m-th power of 10 is divisible by the modulus p, i.e., $m = arg min\\{m : p | 10^m\\}$. The model approximates the function for each position i as follows:\n\n$P_\\theta(c^i | a_1, \\dots, a_m, b_1, \\dots, b_m) \\rightarrow c^i = f^i(a_1, \\dots, a_m, b_1, \\dots, b_m), \\forall i = 1, \\dots, m$\n\nwhere $f^i$ represents the function for the i-th digit of $\\tilde{c}^p$. All these functions can be learned directly from the training data without the need for OOD generalization when training on $D_n (n \\ge m)$ except the term $c^n$.\n\nWhen $p$ is not a divisor of $10^n$ and $p < 10^n$, the model approximates the function $\\tilde{f}^p(a, b) = \\tilde{a}_{10^n} \\times \\tilde{b}_{10^n}$ at each position i.This is because the model has been trained on $D_n$, which is agnostic to the digits in positions $i > n$ of the numbers a and b."}, {"title": "4 Experiments", "content": "The aim of this section is to further validate our theoretical analysis through experiments. We first describe the experimental design, data format, and testing methods, and then conduct extensive"}, {"title": "4.1 Experimental Design", "content": "Model Description: We utilize a GPT model framework, which is a Transformer with a decoder-only architecture consisting of multiple layers and multi-head attentions. Our models are trained from scratch using NanoGPT, MicroGPT, and MiniGPT (Karpathy, 2023) with increasing model size, employing character-level tokenization and the cross-entropy loss function for the standard next-token prediction. The training focuses on basic arithmetic operations, including addition, multiplication, modular addition, and modular multiplication of integers. All experiments are conducted on a single GeForce RTX 4090 GPU. Detailed hyperparameters of the models and training are provided in Table 2."}, {"title": "4.2 Experiments on Addition", "content": "In this subsection, we trained multiple models on different datasets (e.g. D4, D5, D4,5) and tracked the changes in their accuracy over the course of training on various test sets. Additionally, we demonstrated how the models learn each digit during the training process."}, {"title": "4.2.1 Generalization for Different Digit Tasks", "content": "In Figure 1, we present the results of three different experiments using distinct training datasets (i.e., D4, D5, D4,5). For all experiments, we employ the MiniGPT model equipped with a learned APE. Each subfigure illustrates the test accuracy on different test domains Di for these models throughout the training process. Figure 1 verifies our Theorem 1. It demonstrates that models incorporating APE are unable to generalize to longer digits than those they are trained on but can succeed with lower digits. Additionally, the model trained on D5 has a much more challenging training process compared to the model trained on D4, while the model trained on D4,5 experiences the easiest and smoothest training process among the three models. The reason, as explained in Theorem 1, is that for D4,5, the model learns addition tasks on lower digits directly from the training data. In contrast, D4 and D5 require OOD generalization for the edge positions.\n\nWe also conduct extensive experiments using various training datasets, model scales, and data scales.The results of these experiments are robust, and presented in Appendix C.1."}, {"title": "4.2.2 Learning Dynamics for Each Digit Position", "content": "The models and training datasets are identical to those described in Figure 1. We have assembled a comprehensive test dataset that contains a random sample from D1 to D9. Our objective is to demonstrate how these Transformer models equipped with APE learn each digit at every position"}, {"title": "4.3 Experiments on Modular Addition", "content": "The results in Table 3 validate Theorem 4, which states that Transformer models with absolute positional embeddings trained on multi-digit modular addition datasets exhibit distinct generalization capabilities based on the modulus p. For moduli such as p = 50, 100, 200 that divide $10^n$, the models achieve perfect test accuracy across all digit domains, demonstrating their ability to generalize flawlessly to both shorter-length and longer-length OOD domains. In contrast, for moduli such as p = 51, 101, 150, 151, 201 that do not divide $10^n$, the models maintain high accuracy for lower digit domains but show significant performance degradation for higher digit positions.\n\nThe OOD test accuracy in Table 3 for high-order digits can be completely expected using Theorem 5, which states that the test accuracy on $D_{test}$ ($n_{test} > n$) is given by $Acc(p, n, n_{test}) \\approx 1/p'$ if $n_{test} \\ge n + log_{10}(p'/2+1)$, otherwise $Acc(p, n, n_{test}) = 0$. These observations align well with the theoretical expectations outlined in Theorem 4 and Theorem 5, also explaining the experimental results found in the literature Furthermore, the results in Table 4 support Theorem 5, indicating that Transformer models with absolute positional embeddings trained on multi-digit modular addition datasets learns the function$f^p(a, b) = \\tilde{a}_{10^n} + \\tilde{b}_{10^n}$ for any modulus p. These findings fully align with the theoretical predictions."}, {"title": "4.4 Experiments on Multiplication and Modular Multiplication", "content": "We also conducted extensive experimental analyses for multiplication and modular multiplication tasks, examining the performance and generalization capabilities of Transformer models. These experiments are designed to test various configurations, including different positional encodings, model size and training data schemes"}, {"title": "5 Discussion", "content": "Our study sheds light on the mechanistic interpretability and AI alignment of Transformer models. Understanding the mechanisms of Transformer models is crucial for ensuring their alignment with desired outcomes. Our theoretical framework provides a pathway for interpreting how these models generalize from training data to unseen tasks. This understanding is essential for aligning models with human-defined objectives, and reducing the risk of unintended behaviors."}, {"title": "6 Conclusion", "content": "In this paper, we developed a unified theoretical framework to explain various OOD generalization phenomena in Transformer models trained on arithmetic operations. This framework provides a principled understanding of why and how these models generalize across different scenarios, including n-digit addition, multiplication, and modular operations. We categorized generalization into inward OOD (generalizing to shorter-length domains) and outward OOD (generalizing to longer-length domains) to clearly delineate these behaviors."}, {"title": "A Appendix on Transformer", "content": "A Transformer model (Vaswani et al., 2017) predicts the next token based on the preceding tokens within the input sequence. Its output is subsequently used as input for the next prediction. For a target token $x_i$ at position $i$ in the sequence, the model generates a probability distribution over the vocabulary of potential next tokens. To be precise, let $x = x_1x_2 ... x_T \\in \\mathcal{V}^T$ denote the input sequenceof tokens. The probability of observing this sequence with respect to a Transformer model is given asfollows:\n\n$P_\\theta(x) = \\prod_{i=1}^T P_\\theta(x_i | x_1, x_2, ..., x_{i-1}) = \\prod_{i=1}^T P_\\theta(x_i | x_{<i}).$\n\nThe conditional probability $P_\\theta(x_i | x_{<i})$ is computed using the softmax function applied to the lasthidden state. One way to design this model (see e.g. Karpathy (2023), Brown et al. (2020)) is asfollows:\n\n$a^l = h^{l-1} + MHA(LN(h^{l-1}))$\n$h^l = a^l + MLP(LN(a^l))$\n\nfor l = 1,2,..., L, with the initial embedding $h^0 = e_{tok} + e_{pos}$, where $e_{tok}$ represents the initial token embedding and $e_{pos}$ represents the positional embedding. In the context of GPT-series LLMs, MHA refers to the masked multi-head attention of the l-th layer, MLP, is a multi-layer perception with onehidden layer, and LN represents layer normalization. Define $f_l$ such that $h^l = f_l(h^{l-1})$. Consequently,the final hidden state of this LLM is\n\n$h^L = f_L \\circ ... \\circ f_2 \\circ f_1(h^0) \\in \\mathbb{R}^{d_m \\times T},$\n\nwhere $d_m$ is the embedding dimension.\n\nLet $X = LN(h^L) = [X_1, X_2, ..., X_T]$. The final output conditional probability matrix\n\n$P_\\theta = softmax(WX) = \\frac{exp(WX_i)}{\\sum_{i=1}^{N_v} exp(WX_i) } \\in [0, 1]^{N_v \\times T},$\n\nwhere $W \\in \\mathbb{R}^{N_v \\times d_m}$ is a weight matrix. The i-th column of the matrix $P_\\theta$ represents the conditional probability $P_\\theta(x_i | x_{<i})$ for any $x_i \\in \\mathcal{V}$. By training on a large corpus of language texts, the LLMs provide the estimated probabilities."}, {"title": "B Theoretical OOD Test Accuracy for Modular Arithmetic", "content": "B.1 Theoretical OOD Test Accuracy for Modular Addition Learning\n\nTo derive an accurate analytic formula (in Theorem 5) for the OOD test accuracy on Dm with m > n when a Transformer model is trained on the domain Dn, we must carefully count the valid pairs $(a,b) \\in D_m$ that satisfy $a+b \\mod p = \\tilde{a}_{10^n} + \\tilde{b}_{10^n} \\mod p$.\n\nLet $a = A \\cdot 10^n + a_0$ and $b = B \\cdot 10^n + b_0$, where A, B range from 1 to $10^{m-n} - 1$ and $a_0, b_0$ range from 0 to $10^n - 1$. We require $a + b \\mod p = (a \\mod 10^n + b \\mod 10^n) \\mod p$, which simplifies to that\n\n$(A+B) \\cdot 10^n = 0 \\mod p$.\n\nLet $p' = gcd(p, 10^n)$. We are then left with the condition $(A+B) \\cdot 10^n = 0 \\mod p'$.\n\nThe number of such pairs is determined by the frequency of multiples of $p'$ in the valid range. The total number of pairs (A, B) is $(10^{m-n} - 1)^2$. There are $(10^{m-n} - 1)$ valid values for A. For each A, the number of valid B values is determined by the number of multiples of $p'$ in the range. That is, for each A, the number of valid B values is about $(10^{m-n} - 1)/p'$. The test accuracy is the ratio of valid pairs, i.e. the number of valid pairs divided by the total number of pairs.\n\nNote that for $m \\ge n + log_{10}(p'/2+1)$, the range $1 \\le A, B \\le 10^{m-n}$ must include at least one completecycle of p' to ensure some pairs (A, B) satisfy $A + B = 0 \\mod p'$. This condition ensures that the number of digits in A and B is large enough to cover a full period of p'. Otherwise"}]}