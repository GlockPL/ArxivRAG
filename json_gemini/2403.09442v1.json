{"title": "LLM-based agents for automating the enhancement of user story quality: An early report", "authors": ["Zheying Zhang", "Maruf Rayhan", "Tomas Herda", "Manuel Goisauf", "Pekka Abrahamsson"], "abstract": "In agile software development, maintaining high-quality user stories is crucial, but also challenging. This study explores the use of large language models (LLMs) to automatically improve the user story quality in Austrian Post Group IT agile teams. We developed a reference model for an Autonomous LLM-based Agent System (ALAS), and implemented it at Austrian Post Group IT. The quality of use stories in the study and the effectiveness of these agents were assessed by 11 participants across six agile teams. Our findings demonstrate the potential of LLMs in improving user story quality, contributing to the research on Al's role in Agile development, and providing a practical example of the transformative impact of AI in an industry setting.", "sections": [{"title": "1 Introduction", "content": "Effective requirements management is critical in software projects, ensuring that the final product meets customer needs and business goals to deliver value [22][1]. In agile software projects, requirements are iteratively specified and prioritized, typically as user stories, allowing for responsiveness to evolving user needs and ensuring value delivery in iterative and incremental cycles. The quality of user stories [1][11][10][8][13] directly influences the development cycle's velocity and the fulfillment of customer expectations. However, ensuring the completeness, consistency, unambiguity, testability, etc. of user stories, i.e. good user stories, presents challenges.\nAs agile methodologies emphasize rapid iteration and adaptability, the potential of large language models (LLMs) to assist in user story analysis is becoming increasingly significant. The advanced natural language processing capabilities of LLMs present a promising solution for automating and enhancing user story"}, {"title": "2 User Story Quality", "content": "In agile software projects, requirements are often expressed as user stories [7], which are brief descriptions of functionalities or features from the user's perspective, emphasizing their needs and the value the feature brings. A widely accepted template for user stories is: \"As a [role], I want [requirement] so that [benefit].\" This effectively includes the core elements such as the intended user (role), the desired system functionality (requirement), and, optionally, the underlying rationale (benefits). Additionally, every user story should be accompanied by a set of acceptance criteria (AC) that outline detailed conditions a user story must meet to be considered complete and acceptable, including functional behavior, business rules, and quality aspects to be tested. The AC makes a user story more concrete and less ambiguous [7].\nWriting good user stories is essential in software projects, as they convey the needs and perspectives of users and guide the development team in implementing the expected functionalities. Beyond general guidelines for quality in requirements engineering, such as ISO/IEC/IEEE 29148-2011 [1] and IREB guidelines [11], various frameworks include a set of criteria for assessing the quality of user stories. For example, the INVEST framework [2] includes attributes such as independence, negotiability, value, estimability, small, and testability, thereby promoting practical and well-defined requirements. The Quality User Story (QUS) framework [12] evaluates user stories based on their syntactic, semantic, and pragmatic qualities, including criteria such as well-formedness, atomicity, minimalism, conceptual soundness, problem-orientation, unambiguity, completeness, and uniqueness. These frameworks include a variety of criteria for high-quality user stories. Regardless of their diversity, they adhere to industry standards [1][11] that ensure user stories are concise, clear, and achievable, and contribute to the success of software development projects and positive user experiences.\nDespite the widespread adoption of user stories and available criteria for good user stories, the methods for assessing and enhancing their quality are still rel-"}, {"title": "3 Methodology", "content": "To effectively apply LLMs to engineering tasks, we propose a reference model of LLM-based agents. This model forms a framework for designing an Autonomous LLM-based Agent System (ALAS), which we elaborate on in this section."}, {"title": "3.1 A Reference Model of LLM-based Agents", "content": "The model depicted in Fig. 1 conceptualizes the interaction among LLM agents in completing a task. The model is composed of basic constructs: task, agent, shared knowledge base, and response. Together, they form a framework that facilitates the generation of the desired output to fulfill the task's objectives."}, {"title": "3.2 Implementing an Agent Environment", "content": "An agent-based system's strength lies in the AI agents' ability to communicate and execute tasks, thereby facilitating the automation of software development tasks. The implementation of such a system, as described in the reference model, is a pivotal step in harnessing LLMs to assist software development practices. Our Autonomous LLM-based Agent System (ALAS) was designed to automate AI agents' collaboration across various software development scenarios.\nIn ALAS, agents are powered by LLMs, and their collaboration is orchestrated through prompts. These prompts define the actions every agent is expected to perform at each step. There are two categories of prompts in ALAS: initial prompts (Prompti, 1<i<k), which prepare k participating agents for their task responsibilities, and follow-up prompts (Prompti, i>k), which are dynamically constructed to guide agents through the necessary steps for interaction and successful task completion.\nPrompti = Profile; + Task + Context of task + Subtaski, (1<i<k)\nPrompt = Subtaski + Responsei-1, (i>k)\nwhere Prompt\u2081: 1<i<k: first prompt to Agenti, with k agents engaged in completing the Task; i>k: Prompt for Subtask i; Profiler: Agenti's profile; Task: Task to complete; Context of Task: Background information where the task is situated; Subtaski: Subtask i; Responsei-1: Response produced after completing Subtaski-1.\nThe prompts ensure that each agent has a clear understanding of their role and the steps they need to take to contribute to the completion of the overall task. After the initial \"icebreaking\" phase, where agents get acquainted with the task and their roles, subsequent prompts are tailored based on the responses to the prior subtasks to maintain a coherent dialogue flow. Consequently, the implementation of our system includes two phases: task preparation and task conduction."}, {"title": "4 Experiments", "content": "Following the implementation of ALAS, we evaluated its effectiveness in improving user story quality within agile teams at Austrian Post Group IT with a robust agile framework. The company has multiple teams, working synchronously across numerous systems and applications orchestrated within Agile Release Trains [15]. User stories play an important role in planning and prioritizing the implementation of these systems, facilitating communication and collaboration across diverse"}, {"title": "4.1 Setting up Experiments", "content": "The experimental setup, i.e. task preparation phase, involves articulating a task alongside its context, defining the agents' profiles, and planning subtasks. This setup is an iterative process of creating and refining prompts. We applied different prompting techniques and patterns to maximize the capabilities of the agents.\nTask and Context of Task The task was to improve the quality of user stories and ensure alignment with the organizational standards for requirements engineering. These user stories, originally from the Mobile Delivery project, require enhancement not only in clarity, completeness, correctness, consistency, etc. but also in their relevance to the overall functionalities of the application, aligning with the business objectives. To facilitate this, we added two documents when describing the task. One is a minimum viable product (MVP) document that details the basic features of the mobile delivery application. It serves as a blueprint to guide agents in refining user stories in a way that resonates with core product features. Another is a product vision statement, structured using the"}, {"title": "6 Discussion", "content": "Our experiments with ALAS for user story quality improvement have demonstrated significant benefits in enhancing user story quality, particularly in terms of clarity, specificity, and business value articulation. This is evident from the increased overall satisfaction ratings given by survey participants. These findings indicate that ALAS effectively refines user stories for improved quality.\nDespite these enhancements, agents' ability to learn from context, while impressive, highlights a gap in aligning with project-specific contexts and requirements. One developer's feedback in the survey noted that US1(v.2) included an authentication process that, while relevant to the story, \"seems to be out of scope of the US1\". Similar feedback was observed from another developer's feedback. These imply that some quality aspects of requirements may be missing or unclearly specified in the agent prompts related to their responsibilities. Consequently, careful prompt preparation and rigorous evaluation by human experts, such as the PO, are essential. When implementing ALAS for specific tasks, engaging the PO and domain experts during the task preparation phase becomes crucial to optimize prompts for the desired output.\nConsidering that we have only two agents, PO and RE, integrated into ALAS, we can explore incorporating additional specialized agents, such as a tester agent to check factual information and refine acceptance criteria. Similarly, a quality analyst agent could monitor the scope, level of detail, and relevance of the story description, ensuring focus and preventing scope creep, mirroring agile project practices. Currently, ALAS's outputs require manual validation by the Product Owner (PO) to align with project goals and stakeholder expectations. This manual validation is crucial to mitigate the limitations of automated generation and preserve the practical utility of user stories.\nIn examining the parameters governing GPT models, particularly the 'Temperature' parameter that stimulates creativity, we observe a double-edged sword. While it boosts novel and diverse content generation, it also increases the risk of AI hallucination [16], which can lead to plausible yet inaccurate or irrelevant outputs. Addressing AI hallucination necessitates careful parameter tuning to ensure that harnessed creativity enhances rather than detracts from user story quality. In our experiments, we set the medium value 1 for Temperature. However, this still poses a challenge in maintaining factual accuracy, emphasizing the need for an integrated role that guides and monitors the overall discussion."}, {"title": "7 Conclusion", "content": "In this study, we presented a reference model for an agent-based system that utilizes LLMs as agents to aid software development tasks. The reference model guided the implementation of ALAS, which integrates GPT models as agents to enhance requirement quality in agile software development. The experimental results demonstrated that ALAS significantly improves user story clarity, comprehensibility, and alignment with business objectives. However, the findings also underscored the indispensable role of human intelligence, particularly the PO in software projects, who facilitate and monitor the improvements in user stories to guarantee the integrity of automatically produced outputs. Moving forward, enhancing ALAS necessitates not only incorporating specialized agents with optimized profiles and task descriptions but also fine-tuning AI parameters to minimize hallucinations and enhance contextual accuracy. This paper contributes a foundational framework and a proof-of-concept for AI-assisted user story quality improvement, marking a significant step forward in bridging the gap between AI capabilities and human expertise in software development."}]}