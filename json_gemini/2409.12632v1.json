{"title": "Counterfactual Explanations for Clustering Models", "authors": ["Aurora Spagnola", "Kacper Sokol", "Pietro Barbiero", "Marc Langheinrich", "Martin Gjoreski"], "abstract": "Clustering algorithms rely on complex optimisation processes that may be difficult to comprehend, especially for individuals who lack technical expertise. While many explainable artificial intelligence techniques exist for supervised machine learning, unsupervised learning - and clustering in particular \u2013 has been largely neglected. To complicate matters further, the notion of a \"true\" cluster is inherently challenging to define. These facets of unsupervised learning and its explainability make it difficult to foster trust in such methods and curtail their adoption. To address these challenges, we propose a new, model-agnostic technique for explaining clustering algorithms with counterfactual statements. Our approach relies on a novel soft-scoring method that captures the spatial information utilised by clustering models. It builds upon a state-of-the-art Bayesian counterfactual generator for supervised learning to deliver high-quality explanations. We evaluate its performance on five datasets and two clustering algorithms, and demonstrate that introducing soft scores to guide counterfactual search significantly improves the results.", "sections": [{"title": "Introduction", "content": "Clustering plays a vital role in artificial intelligence (AI) and machine learning (ML), allowing us to discover hidden patterns and structures in data when labels are missing [11]. However, as clustering techniques evolved, they have become more opaque, posing challenges for humans to understand the rationale behind cluster assignments, thus undermining user trust in the outcomes [33]. In recent years, the concept of explainable AI (XAI) has gained significant traction as a vital bridge between complex ML models and human understanding [35].\nXAI has notably found numerous applications in supervised learning and some in reinforcement learning [2], but its adoption in unsupervised learning, and clustering more specifically, has been overlooked. In the context of clustering, interpretable (transparent) methods rely on tree models or recasting classic clustering algorithms like k-means into neural networks. However, these techniques may sacrifice flexibility, accuracy and usability due to their strong limits on model selection, explanation and representation. On the other hand, post-hoc explainable techniques offer greater flexibility with fewer underlying assumptions [31]. Existing post-hoc approaches primarily focus on identifying associations between input features and cluster assignments [22, 3, 9, 34], but these associations lack actionable insights [23, 20]. This gap in the XAI landscape highlights the need for novel techniques that generate actionable explanations tailored to clustering models.\nCounterfactual explanations enable users to explore alternative \"what-if\" scenarios. Similar to counterfactuals for supervised [32, 8, 30] and reinforcement [13, 28] learning, counterfactuals for clustering can show how input modifications might alter cluster assignments. For instance, in a scenario where clustering is used for customer segmentation, a customer who wants to receive premium offers, can use counterfactual explanations to determine what specific actions or changes in behavior (e.g., buying wine from the company's store instead of a local winery) would make a company offer them a premium membership.\nUnlike supervised and reinforcement learning, where individual instances or agent actions can be probed, clustering relies on the entire dataset to construct the model and assign labels. However, access to the entire dataset for explanation generation should be avoided for privacy reasons, hence only a few descriptive instances and cluster statistics should be provided. This is also important for large datasets that are impossible to process in their entirety. Post-hoc XAI is an attractive paradigm for explaining clustering models since it does not require access to the entire dataset and allows transparency without sacrificing privacy. The nature of clustering - assigning collections of data points to clusters \u2013 presents unique challenges, as the vast majority of algorithms is transductive, i.e., they do not allow assigning cluster memberships to new, unseen instances without refitting the entire model. Moreover, finding an approach to capture the spatial information upon which the clustering is built - distances, densities, and other relations between instances and the data space - is non-trivial.\nTo address these challenges, we designed a novel soft-scoring technique to capture the spatial information that clustering models rely upon, which we integrated into a state-of-the-art Bayesian counterfactual generator - BayCon [32]. Through this connection, we propose a novel XAI method specifically designed for clustering, which addresses the lack of model-agnostic tools that are able to provide actionable counterfactual explanations for clustering models. Our method, based on a bespoke soft-scoring technique, consistently outperforms the hard-scoring baseline across all experiments in terms of the percentage of instances explained, and on some datasets, it even outperforms the model-specific soft-scoring techniques, while maintaining similar computational complexity (e.g., execution time). Similarly, the distance between the initial instance (x*) and candidate"}, {"title": "Background and Problem statement", "content": "When trying to derive insights through clustering, it is common practice to experiment with different algorithms and compare assignments relying on quantitative scores and visualisations. Thus, the proposed explainability method should support as many clustering algorithms as possible. For the reasons anticipated, we chose to design a post-hoc method, which implies the possibility of relying on model agnosticism. To make sure we are fully complying with this feature, it is necessary to explore clustering taxonomies, identify the most common families of algorithms, and verify if the proposed method can potentially be applied to all of them. Typically, clustering algorithms are classified into three broad types: partitional, hierarchical, and density-based.\nRecently, more nuanced taxonomies have emerged, proposing additional categories such as grid-based and model-based algorithms [12]. Moreover, clustering techniques have expanded beyond traditional categories and incorporated new concepts from diverse domains like fuzzy theory, graph theory, fractal theory [38], and, more recently, deep learning [40].\nHowever, the emergence of hybrid techniques has blurred the boundaries between these categories. For instance, the Hierarchical Density-Based Spatial Clustering of Applications with Noise (HDBSCAN [5]) and the Growing Hierarchical EXIN (GH-EXIN [6]) combine the advantages of hierarchical and density-based approaches. Clustering In QUEst (CLIQUE [1]) and STatistical INformation Grid (STING [37]), though initially simply classified as grid-based algorithms, also include elements of hierarchical and density-based methods, respectively.\nWhile some of the algorithm categories named are not suitable for tabular data, we tried to represent all the applicable ones, starting from the clustering methods available in scikit-learn."}, {"title": "Transduction and Agnosticism in Clustering", "content": "Clustering is an unsupervised learning task with the goal of grouping similar data points together based on a given similarity metric. Finding natural groups or clusters in the data is desirable as it may be useful for additional data analysis or interpretation. However, due to several variables (further discussed in the taxonomy presented in Section 2.1), including noise, outliers and high-dimensionality of data, detecting actual clusters in real-world datasets is inherently difficult [18]. The complexity of assessing clustering goodness makes this task a perfect candidate for the application of XAI techniques.\nWe set out to design a model-agnostic XAI method that works with various clustering algorithms, which is a challenging task given that most such models are transductive, i.e., once the clusters are built, the algorithms cannot \"predict\" cluster membership of new data points without refitting the models. To avoid this constraint, many clustering algorithms also exploit the k-nearest neighbours (k-NN) classifier to assign membership of new instances to clusters, which are represented by their respective medoids. However, we avoided these implementations to avoid forcing unfounded cluster assignments.\nTypically, clustering algorithms are classified into three broad types: partitional, hierarchical and density-based (Section 2.1 offers a more detailed discussion). Our experimental setup included HDBSCAN a representative of hierarchical and density-based (non-parametric) algorithms and k-means a representative of non-hierarchical and centroid-based (parametric) algorithms. GMMs are another non-hierarchical and centroid-based that can be easily used instead of k-means. Furthermore, k-means is a transductive algorithm because assigning a cluster to a new data point is equivalent to performing the Expectation (E) step of the algorithm, i.e., the stage in which data points are assigned to the closest centroid. In DBSCAN [10], the distance metric used by the algorithm can be utilised to check if the new data point is close enough to one of the cluster core samples or if it is an outlier."}, {"title": "Problem statement", "content": "To assess the quality of the counterfactuals (x') with respect to the initial instance (x*) and the trained clustering model, we use the following scoring function used as part of optimisation maximisation problem, i.e., the higher the score, the better the counterfactual:\n$F(x', x*) = S_fS_xS_y,$\nwhere Sf is the number of non-altered features, Sx is the similarity in the feature space, and Sy is the similarity in the prediction space, all scaled in to the [0, 1] range. The main novelty of this study is the soft-scoring technique for calculating Sy. The other two components, St and Sf, are used as defined by BayCon:\n$S_f(x', x^*) = \\frac{\\text{# of equal features between } x' \\text{ and } x^*}{\\text{Overall # of features}}$\nand\n$S_x(x', x^*) = 1 - d_{\\text{Gower}},$\nwhere, $d_{\\text{Gower}}$ is the Gower distance  a metric that can operate in mixed (numerical and categorical) feature spaces.\nFigure 1 depicts the importance of cluster-specific soft-scores in the prediction space Sy, which allows us to rank candidate explanations with more granularity. More specifically, the figure depicts example optimisation soft scores for candidate counterfactuals, which are colour-coded, from worse (blue) to better (red). The x-axis represents the Gower distance (1 - Sx); the y-axis represents the soft-scores in the prediction space (Sy); and the z-axis represents the distance in the number of changed features (1/Sf). The optimisation score is the highest for (i) the counterfactuals that are closest to the centroid of the target cluster (y-axis); (ii) have lower Gower distance from the explained instance; and (iii) require fewer features to be tweaked. Note that ranking based on the prediction score would not be possible if we simply applied any supervised counterfactual generator (e.g., BayCon) because such an approach would provide only binary information on whether the cluster assignment has been flipped to a predefined one or not."}, {"title": "Method", "content": "In this section, we briefly introduce BayCon \u2013 a Baesian counter-factual generator for supervised models - which our method builds upon [32]. We adapt BayCon to clustering algorithms by introducing two model-specific soft scores: one for k-means (non-hierarchical, centroid-based algorithm) utilising scaled distances from the centroid of the target cluster, and another for HDBSCAN (hierarchical,"}, {"title": "Model-specific Soft Scoring", "content": "To adapt BayCon to clustering algorithms we introduce soft output scores (Sy score) that are suitable for clustering models.\nDistance-Based Soft Scoring We exploit pairwise distances between the candidate counterfactuals and cluster summaries to speed up the counterfactual search and reduce score variance. To calculate this score (see Figure 2 for more information), we need (1) the centroid Ci for each cluster i; (2) the minimum ($min_t$) and the maximum ($max_t$) distance between the centroid of the cluster and the corresponding data points in that cluster; and (3) the distance metric d : X X X \u2192 R+ used during the clustering procedure (many clustering algorithms treat the distance metric as a hyperparameter).\nGiven a target cluster t, an initial instance x*, and a candidate counterfactual CF(x*), the score Sy is calculated according to Equation 2. The score Sy is scaled to the interval [0, 1] and indicates whether the candidate counterfactual is approaching the centroid of the target cluster. Values smaller than 0 and larger than 1 are clipped to 0 and 1 respectively. Such clipping may occur when we have to deal with new points (unavailable during the cluster-building phase), that are closer to the center than the min or farther than the max known values for that cluster. By default, the Euclidean distance is used, but it is possible to employ the same distance metric the black-box model utilises in the score calculations.\n$S_y(CF(x^*), t) = 1 - \\frac{d(CF(x^*), C_t) - min_t}{max_t - min_t}$"}, {"title": "Density-Based Soft Scoring", "content": "The efficient HDBSCAN [5] implementation (HDBSCAN* [25]) performs quick pre-computation during model fitting to speed up execution time. It determines where in the condensed tree the new data point would fall, assuming we do not change the condensed tree. Although the method is provided, the authors warn about the risk of drift in case unseen points are not cached, and the model is not retrained periodically. The soft clustering for HDBSCAN leverages the condensed tree serving as a smoothed density function over data points and representative points to enable probabilistic clustering, i.e., the clustering model assigns each data point a vector of probabilities indicating its likelihood of belonging to a cluster. We utilise these probabilities as a distance metric in the output space."}, {"title": "Model-agnostic Soft Scoring", "content": "Our proposed model-agnostic soft-scoring method first extracts representative points for each cluster using prototypes and criticisms [19]; next, the selected data points are used to train a semi-supervised self-training classifier that assigns membership probabilities based on the clusters. The membership probability corresponding to the target cluster can then be used as a soft-scoring metric.\nPrototypes and Criticisms Prototypes and criticisms [19] relies on the Maximum Mean Discrepancy (MMD), which is a discrepancy measure between two distributions [16]. This approach \u2013 called MMD-critic - aims to minimise the divergence between the distribution of selected prototypes and the actual training data distribution. By leveraging kernel density estimation, the method approximates the data density and subsequently measures MMD. Mathematically, the MMD\u00b2 measure can be formulated as [27]\n$\\frac{1}{m^2} \\sum_{i,j=1}^{m} k(z_i, z_j) - \\frac{2}{mn} \\sum_{i,j=1}^{m,n} k(z_i, x_j) + \\frac{1}{n^2} \\sum_{i,j=1}^{n} k(x_i, x_j).$\nHere k represents the kernel function that measures the similarity between two points; z and x are respectively prototypes and data instances; m denotes the number of prototypes; and n signifies the number of data points. The choice of the kernel function is vital, with the radial basis function (RBF) kernel being a common pick because of its capacity to weigh points on a 0-1 scale according to their proximity.\nThe MMD-critic algorithm follows a systematic procedure for both prototype selection and criticism identification. The algorithm iteratively selects prototypes based on their MMD reduction potential, ensuring that the selected prototypes closely resemble the data distribution. Criticism identification relies on a witness function that quantifies the discrepancy between prototype and data density; it is formulated as\n$\\text{witness}(x) = \\frac{1}{n} \\sum_{i=1}^{n} k(x, x_i) - \\frac{1}{m} \\sum_{j=1}^{m} k(x, z_j).$\nThe witness function offers a mechanism to scrutinise instances where prototypes diverge from data instances, thereby serving as valuable counter-examples.\nWe adapted the MMD-critic implementation [19] to our problem setting by selecting a fixed share of prototypes and criticisms for each cluster that the black-box model has discovered. All the parameters were kept default, including the 1:4 ratio between criticisms and prototypes. The fixed share of the percentage of representative points to feed to the self-training classifier was set to 20% for each cluster. This share can be changed to account for data availability and the maximum number of points that can fit in memory for large datasets."}, {"title": "Experiments", "content": "Given that there are no existing methods for generating counterfac- tuals for clustering models that could be used as a baseline, we com- pare our method against BayCon on two real-life datasets with two representative clustering algorithms. To ensure traceability and re- producibility of our experiments, we mirror the setup described in the BayCon study [32] (which does not need to be altered despite our work focusing on unsupervised rather than supervised learning). Our method is implemented in Python 3.9 and relies on scikit-learn. All the experiments were run on a dual-core 1.1 GHz Intel Core i3 CPU with 8GB of RAM. We imposed a 15-minute runtime limit on each execution."}, {"title": "Setup", "content": "Datasets We experiment on five datasets: two with k-means++, two with HDBSCAN, and one with both such that each algorithm is tested on three datasets in total. Their summary characteristics are given in Table 1.\nFor each dataset, we selected ten random instances to be explained, generating their counterfactuals three times to account for randomness, i.e., 30 runs per dataset. We chose five initial instances to be explained from each of the top two clusters with the highest cardinality, i.e., starting from the initial instance selected to be explained that belongs to cluster #1, the goal is to generate counterfactual instances (explanations) that belong to cluster #2, and vice versa. For example, if we have three clusters of cardinality A:13, B:18, and C:15, we take 5 random instances from B and try to generate explanations belonging to C, and 5 from C to perform the opposite task.\nClustering models The explanations were generated for two clus- tering algorithms: k-means++ and HDBSCAN."}, {"title": "Results", "content": "Counterfactual Quality Results Table 4 reports the quality met- rics for the baseline as well as the model-specific and model-agnostic variants of our method. For the input (i.e., feature) space measures Score x and Score f we can observe that all methods achieve"}, {"title": "Discussion", "content": "The proposed model-specific soft-scoring methods for k-means++ and HDBSCAN, as well as the novel model-agnostic method, perform better than the (hard-scoring) baseline method across the board. Specifically, the model-agnostic method performed better than the baseline method in terms of the number of explained instances for all the datasets, and at times, it performed comparable to or better than the two model-specific methods. Additionally, the model-agnostic method found valid counterfactuals for each explained instance in at least 1 out of 3 generation attempts while the model-specific methods failed at times despite performing better overall. The computational performance and distance in the input space (Score x in the result tables) were not sacrificed at all, while the number of features changed (and the associated Score f in the result tables) was slightly better in most cases. The Mann-Whitney U Test applied to each metric confirmed that there is no significant difference between the scores of the baseline and soft methods. This result suggests that although more computation is necessary for the soft-scoring method, exploiting the spatial information allows us to better guide the search and speed it up. Overall counterfactual multiplicity and quality are preserved while more instances are explained, indicating that searching for solutions with soft scores where the baseline failed to find any does not lead to regions with low-quality alternatives.\nk-means++ In the experiments involving k-means++, the model-specific method always performed at least as well as the model-agnostic method. This result was expected since an ad-hoc scaled distance-based method is likely to be the best soft-scoring technique, given that it is specific to the given model. Regardless, the difference in Explained instances % (Exp% in the result tables) was 0% on the Diabetes and just 10% on the Wine dataset. Score x was the same (with the chosen rounding) in almost all trials; Score f, although less regular, showed negligible differences. Overall, the best improvement was obtained on the Diabetes dataset, where the number of explained instances increased from 67% for the baseline to 90% for both the model-specific and model-agnostic methods.\nThese results imply that the self-training classifier initiated with a fixed percentage of prototypes and criticisms for each cluster (used by the model-agnostic approach) can uncover patterns in the cluster assignment without explicitly relying on centroid distance. While there might exist other formulations of the model-specific soft scoring that could improve the share of explained instances, we did not pursue this line of research given that Exp% was already 90% on Diabetes and 100% on the other two datasets, thus any improvement would be marginal and irrelevant to our study.\nHDBSCAN In the experiments involving HDBSCAN, the model-agnostic method explained instances in all experimental trials on two datasets - Customer and Wine \u2013 which is an improvement compared to both the baseline and model-specific methods. On Churn, however, the percentage of explained instances was lower across all methods. Notably, on this dataset HDBSCAN produced four clusters with one of them being 7 to 10 times larger than the other three in cardinality (see Table 2). This cluster imbalance may be the reason for slightly worse performance.\nAblation We also investigated whether the model-agnostic method is robust to hyperparameter changes to understand if its performance is stable [24]. Specifically, we examined the share of initial representative points, which depends on the dataset size, available memory and privacy constraints. Intuition suggests that the larger the share, the better the results. We set the percentage of representative points"}, {"title": "Limitations", "content": "One limitation of our study is the absence of a user study. Without user feedback on generated counterfactuals, it is difficult to validate the practical effectiveness of the generated counter-factual explanations. Additionally, our approach does not include a grid-search hyperparameter tuning. For this reason, the experiments might underestimate the quality of generated counterfactual explanations. Furthemore, the inclusion of other types of data such as images, text, or audio might require clustering unstructured data in an actionable feature space before utalizing the proposed method."}, {"title": "Conclusion", "content": "To the best of our knowledge, this is the first XAI method based on counterfactual explanations specifically designed to explain clustering assignments and, in general, one of the few post-hoc methods addressing unsupervised learning. The experiments, performed on five datasets and two clustering algorithms, show that introducing soft scores to guide counterfactual search improves the performance compared to a state-of-the-art counterfactual generator. These results encourage exploring further refinements, like tweaking other components of the method to produce counterfactuals of even higher quality and, at the same time, addressing counterfactual multiplicity to provide just a selection of meaningful explanations [36]."}]}