{"title": "UnifiedCrawl: Aggregated Common Crawl for Affordable Adaptation of LLMs on Low-Resource Languages", "authors": ["Bethel Melesse Tessema", "Akhil Kedia", "Tae-Sun Chung"], "abstract": "Large language models (LLMs) underperform on low-resource languages due to limited training data. We present a method to efficiently collect text data for low-resource languages from the entire Common Crawl corpus. Our approach, UnifiedCrawl, filters and extracts common crawl using minimal compute resources, yielding mono-lingual datasets much larger than previously available sources. We demonstrate that leveraging this data to fine-tuning multilingual LLMs via efficient adapter methods (QLoRA) significantly boosts performance on the low-resource language, while minimizing VRAM usage. Our experiments show large improvements in language modeling perplexity and an increase in few-shot prompting scores. Our work and released source code provide an affordable approach to improve LLMs for low-resource languages using consumer hardware. Our source code is available here at https://github.com/ bethelmelesse/unifiedcrawl.", "sections": [{"title": "Introduction", "content": "Generative AI has become an integral part of our daily lives, assisting us in various ways, whether it is through its Natural Language Processing (NLP) or Computer Vision (CV) specialty.\nIn the field of Natural Language Processing (NLP), generative models play a pivotal role in generating coherent and contextually relevant text. They leverage deep learning architectures, particularly transformer-based architectures, and are pre-trained on vast amounts of textual data to learn the nuance of language.\nThese models learn the patterns and structures of language from large datasets, allowing them to generate new text similar to the input data.\nThese models, fueled by Large Language Models (LLMs), are characterized by their extensive size, often measured in terms of the number of parameters, often many billions. This immense number of parameters allows these models to capture complex language patterns and context, resulting in improved performance on various NLP tasks.\nFor example, OpenAI's GPT (Generative Pre-trained Transformer) series (Brown et al., 2020; OpenAI, 2022, 2023b,a) has played a fundamental role in transforming the public's view and usage of AI NLP tools. GPT-3 (Brown et al., 2020), with its staggering 175 billion parameters, represents a groundbreaking milestone, showcasing the scalability of transformer-based models. This technology has shown a substantial economic potential due to its broad applicability in commercial usage."}, {"title": "Problem Definition", "content": "By leveraging the availability of extensive large and diverse dataset, often composed of high-resource languages, LLMs have shown remarkable performance in generating content within those linguistic contexts, mimicking human-like responses. However, their performance diminishes significantly when prompted with low-resource languages due to the limited availability of training data and resources for those languages. This limitation results in the generation of responses that lack coherence. For example, when prompted with queries in a low-resource language (such as Amharic (ISO:amh), the most widely language in Ethiopia) models like GPT-turbo-3.5(OpenAI, 2023b) produce incomprehensible outputs. This challenge persists even when inputting prompts in high-resource languages and instructing the model to respond in low-resource languages, resulting in sentences that lack meaningful coherence.\nThe limitation of LLMs in handling low-resource languages stems from their initial training which heavily relies on vast amounts of primarily English-centered data. Appendices A.1 and A.2 illustrates the distribution of data and the percentage constituting high-resource and low-resource languages in the training process for these LLMs.\nAddressing this challenge of adapting LLMs for use in low-resource languages is crucial for democratizing their accessibility and broadening their practical applicability. However, pre-training LLMs can be exceptionally costly, primarily for two main reasons.\nFirst, as mentioned earlier, pre-training LLMs requires an extensive amount of textual data, and low-resource languages often lack the resources to meet this requirement. For instance, in widely used collection Common Crawl (CC) (CommonCrawl, 2007), low-resource languages such as Tagalog, Punjabi, Kurdish, Lao, Amharic, etc., constitute a minuscule fraction (less than 0.01%) compared to other high-resource languages like English, German, and Russian (A).\nSecond, the resource-intensive nature of training LLMs, characterized by an extensive number of parameters, demands substantial GPU power, memory, and time. For example, models like gpt-3.5-turbo (175 billion parameters), Claude (Bai et al., 2022) (52 billion parameters), and LLaMA (Touvron et al., 2023) (1.6 billion parameters) translate into an exceedingly resource-intensive training process. Table 9 provides the size details of these LLMs. Consequently, the immense size of these LLMs renders training prohibitively expensive and inaccessible to non-wealthy communities/nations, smaller companies, and educational institutions.\nIn this paper, our primary objectives are to investigate the following research questions:\n2. How can we collect sufficient training data in low-resource languages for LLMs?\n3. How can we achieve the above, while being constrained by consumer devices' memory, storage, and compute?"}, {"title": "Proposed Method", "content": "To address the aforementioned challenges, we present a novel approach aimed at overcoming data scarcity for low-resource languages, and leverage efficient methods for training LLMs on low-cost hardware.\nOur proposed method involves the development of an efficient and cost-effective data collection strategy to extract comprehensive textual content specific to a given low-resource language from the entire Common Crawl corpus. Figure 3 illustrates our architecture. By carefully paying particular attention to memory, compute and network usage in each step of our data collection pipeline, our method is optimized to run entirely on personal consumer hardware - the entirety of the Common Crawl dataset can be achieved in a matter of days, utilizing less than 10GB of RAM and storage. The outcome of this process is our meticulously curated dataset called Unified-Crawl. Using our method, we were able to successfully extract a monolingual corpora for specific low-resource languages, significantly surpassing the sizes of previously compiled collections, as shown in fig. 2.\nSubsequently, we leverage quantization and lightweight low-rank adapters for fine-tuning multilingual Large Language Models (LLMs) on the collected dataset. This innovative technique facilitates the utilization of exceptionally large models on consumer-grade GPUs, thereby enhancing accessibility and affordability in training.\nFigure 1 illustrates the overarching concept of our proposed scheme. Our approach involves fine-tuning a pre-trained model on our UnifiedCrawl dataset, extracted from the common crawl corpus using our data extraction method. The resulting fine-tuned model can then be applied to downstream tasks."}, {"title": "Related Works", "content": "In recent years, there has been a notable surge in the development of multilingual Large Language Models (LLMs), contributing to improved cross-lingual understanding. Examples of these large language models are shown in the appendix B, including their model type, size and the number of languages they are trained on.\nWhile these multilingual models have made strides towards linguistic inclusivity by covering over many languages, they still overlook hundreds of lower-resource languages with sizable speaker populations. This hinders the efficacy of models in many languages compared to performance in high-resource languages. This limitation is primarily due to the lack of sufficient online training data available for lower-resource languages.\nIn this work, we aim to improve the performance of the above-mentioned models (particularly XGLM model) in low-resource languages by training them on our collected dataset."}, {"title": "Large Multilingual or Monolingual Datasets", "content": "We have noted that data is a crucial component for training language models specifically in the multilingual domain. However, there is a considerable gap in data quantity across languages and domains. Even within the largest Common Crawl corpus, which is a vast web archive that encompasses a diverse collection of web pages providing a rich source of textual data in a multitude of languages and topics, over 41 languages make up less than 0.01% of the data, and 100 languages 0.1% - The quantity of data in common crawl decreases almost exponentially as shown in figs. 4 and 5. This leaves only a handful of the world's languages represented in evolving language technologies and applications (Joshi et al., 2020).\nIn this study, we extract all the available textual data from every archive within the common crawl for a specific low resource language. Our choice of the common crawl dataset is driven by our objective to maximize the acquisition of the available data, leveraging its vast size and inclusivity of many languages, as it systematically scraps data across the internet."}, {"title": "Common Crawl and Dataset Extraction", "content": "Due to the extensive scope, Common Crawl (CC) and its derivatives are often used to pre-train large language models, and the majority of State-of-the-art models, such as LLAMA (Touvron et al., 2023), GPT-3 (Brown et al., 2020), Falcon (Almazrouei et al., 2023) PALM (Chowdhery et al., 2023), Stable LM (Islamovic, 2023), etc. have incorporated datasets sourced from the Common Crawl corpus into their training pipelines. This integration has contributed to their improved proficiency in understanding and generating human-like text across various domains and languages.\nSeveral smaller datasets have been extracted from the Common Crawl corpus, each contributing to the training of language models. Examples include CC-net (Wenzek et al., 2020), which extracted monolingual corpora for transformer model training; mC4 (AllenAI, 2021), which collected data from publicly accessible CC archives; and OSCAR project (Abadji et al., 2022), which focuses on releasing monolingual corpora from recent CC archives. These subsets have then been used to train many of the State-of-the-art models, such as mT5 (used mC4) (Xue et al., 2021), BLOOM (used OSCAR) (Scao et al., 2022), etc.\nHowever, a common issue persists: many extracted datasets from the common corpus are often limited to one language (eg. CC-net), or a few archives(eg. OSCAR), or are not updated with latest common crawl dumps (eg. mC4). Moreover, due to the sheer scale of the corpus, naively extracting text data for a specific language from all the common crawl archives can be challenging, as it can be time and memory intensive. Moreover, these datasets can also not be easily updated with more data from latest common crawl archives. This limitation hinders the extraction of data for specific languages, especially for very low-resource languages, contributing to the lack of linguistic diversity in the available datasets.\nIn response to these challenges and limitations, we present a cost-effective means of extracting text data from all CC archives for low-resource languages, including the latest common crawl archives which are much larger compared to previous archives. Our contribution includes releasing the code base for other fellow researchers to extract their own dataset from CC for low resource languages. By doing so, we aim to address the existing gaps in dataset extraction methodologies and contribute to the advancement of linguistic research in low-resource language contexts."}, {"title": "Deduplication", "content": "Another method adopted in our work includes deduplication techniques. Raw text datasets obtained through web scraping often contain the same lines multiple times (Lee et al., 2022). This repetition within the dataset can negatively affect the learning process as it slows down the training as well as limits the model's generalization capabilities. To overcome these challenges, it is important to apply some form of deduplication on the extracted dataset.\nNumerous deduplication methods have been previously proposed and employed in prior works. CC-Net, for instances, utilized a paragraph-based Exact-Hash deduplication, whereas other approximate methods, such as MinHash (Broder, 1997), MinHash-LSH (Baluja and Covell, 2007), SimHash (Sadowski and Levin, 2007; Gyawali et al., 2020), are sometimes used for faster deduplication in different context (Scao et al., 2022; Almazrouei et al., 2023).\nIn our data extraction pipeline, we opted for (Lee et al., 2022)'s exact substring deduplication method same approach adopted in mC4, OSCAR, and CC100. This approach not only effectively addresses redundancy but also removes common header/footer artifacts often present in the extracted text, enhancing the overall quality of the dataset. By employing this deduplication method within our proposed scheme, our goal is to extract a high-quality dataset that contributes positively to model's training, resulting in accelerated training, improved perplexity, and reduced likelihood of model memorization."}, {"title": "Low Resource Model Adaptation", "content": "Training (pretraining/fine-tuning) large language models is often impractical beyond major corporate or institutional settings due to their substantial parameter count and the resource-intensive nature of training LLMs. For instance, training a model with a large number of parameters consumes considerable GPU memory and time for example a 7B model requires 28GB of GPU VRAM, outside the scope of most consumer GPUs. An effective solution to mitigate this challenge involves the integration of quantization techniques into LLMs. Quantization can be achieved through methods such as Quantization-aware training (Wang et al., 2023) or post-training quantization approaches like GptQ (Frantar et al., 2023), SmoothQuant (Xiao et al., 2023), bitsandbytes (Dettmers et al., 2022), among others. These techniques work by reducing the precision of model parameters, allowing for more efficient storage and computation, dramatically reducing GPU VRAM usage.\nThis method was further extended to QLoRA (Dettmers et al., 2023), which combines quantization with adapter training. QLoRA achieves a further reduction in memory usage, enabling the fine-tuning of a 65-billion-parameter model on a single 48GB GPU while maintaining full 16-bit fine-tuning task performance.\nAs QLoRA consistently delivers performance similar to full fine-tuning (Lu et al., 2023; Luo et al., 2023; Manvi et al., 2023; Liu et al., 2023) for much lower VRAM, we adopt QLoRA in our work to balance computational efficiency with model performance."}, {"title": "Methods", "content": "In this section, we first present a method and procedure to collect and process training data for low-resource languages from the common crawl dataset using limited computing resources. Additionally, we adopt a method to efficiently train large language models on the extracted training dataset using limited GPU resources."}, {"title": "Data Collection Framework", "content": "The raw text data for low-resource languages is gathered from the common crawl dataset. The common crawl dataset is extremely large, at approximately 100 TeraBytes per crawl archive, with multiple archives per year\u00b9. Due to its sheer size, it is difficult to directly download raw text dataset from the corpus. In this subsection, we propose an efficient and cost-effective framework to extract data from a single common crawl archive, which is repeated for all available dumps. Figure 3 illustrates our data collection pipeline for extracting raw text dataset from a single common crawl archive."}, {"title": "Index Filtering", "content": "The Common Crawl provides a columnar index (CC index\u00b2) containing language annotations\u00b3 for each URL in the archive. We exploit this information to selectively extract a specific low-resource language from a single archive (eg. CC-MAIN-2023-23). However, even this CC index is typically 100s of GBs for a single archive. As we process 43 archives, this would result in a total of 10s of TBs of just the index, which would be difficult to store.\nInstead, we utilize DuckDB (Raasveldt and M\u00fchleisen, 2019), an in-memory analytical database management system, to filter the index shards corresponding to our target language as the primary content language. By employing DuckDB's in-memory filtering and downloading, we eliminate the need for storage-intensive initial bulk downloads and subsequent filtering.\nAdditionally, we integrate python's multiprocessing package with DuckDB to further reduce the overall downloading time. This integration utilizes multiple processes concurrently across all CPU cores on a single system, leveraging parallelism and bypassing the global interpreter lock to expedite the data acquisition process. The combined utilization of DuckDB and multiprocessing significantly optimizes storage usage and accelerates the overall downloading process."}, {"title": "Extracting WARC Files", "content": "The filtered and downloaded index shards from the previous step contains the path to the WARC (Web ARChive) files (Kunze et al., 2008), which contains the content of the crawled webpages and its metadata. Due to the considerable size of the CC archive shards containing these WARC files, downloading all WARC files is impractical. Instead, we selectively filter and retain WARC files corresponding to our target languages within the index shards \u2013 avoiding the download of all the WARC files.\nThis filtering and downloading process utilizes the columnar information provided in the index shard that include the WARC filename, WARC record offset, and WARC record length (the URL and Content Languages we leveraged earlier during the index filtering step are also present here). The WARC filename gives a URL to the CC archive shard containing this particular WARC file, the WARC record offset indicates the exact location of the WARC file we need, the WARC length specifies its span.\nLeveraging this information, we download the corresponding WARC for each URL containing our target language as its primary content language via HTTP Range Requests (Fielding et al., 2014). This method of downloading allows us to download only the WARC files that we need and skip the rest from downloading. This is done by requesting the server to send only a portion of an HTTP message back to the client \u2013 in our case, only the files corresponding to our target language. By only downloading these necessary WARC files, we conserve bandwidth and storage that result from downloading all the WARC files."}, {"title": "Text Extraction", "content": "The next step involves extracting the raw text. We start by retrieving the HTML source from the downloaded WARC files by using the WARCIO library (Contributors, 2017), which offers a fast and efficient way to read and write WARC format. From this extracted html source, we finally obtain the raw text using the command line tool Trafilatura (Barbaresi, 2021) (same tool as used in (Penedo et al., 2023)), which is specifically designed to extract text from the web. This library not only facilitates the extraction of the text from HTML but also improves the text quality by eliminating the noise caused by recurring elements such as headers, footers and other information.\nIt is noteworthy that the entire process, including the downloading of WARC files as well as the reading and the text extraction, is conducted in-memory for the purpose of reducing the time and storage requirement. By avoiding storing unnecessary elements found within the warc files and the raw html inside the warc (such as large quantities of javascript/html tags/etc), we dramatically reduce the storage requirements."}, {"title": "Deduplication", "content": "While the Trafilatura library improves the quality of our extracted text, it is common to have some repetitive sequences within the raw text data. These repetitive sequences include things like copyright notices, some headers/footers, keywords, etc. that are common in many similar websites across its pages. Having multiple repetitive sequence reduces the overall quality of the training data as it encourages the model to prioritize memorization rather than make the model learn to generalize. Therefore, to further improve the quality of our data as well as improve the training process (later on), we remove these deduplicated elements from the documents. We adopted exact substring deduplication (Lee et al., 2022) technique to remove all duplicate occurrences over a given length from the dataset. However, after removing the repeated sequences, some documents become too short \u2013 we hence discard documents of very short length. This final step yields to our final dataset, which we call UnifiedCrawl."}, {"title": "Low Resource Model Adaptation", "content": "Common multi-lingual Large Language Models (LLMs) often suffer from low performance on the long-tail of low-resource languages( (Lin et al., 2022), and table 4). Fine-tuning LLMs on our dataset offers a pathway to democratize AI, improving LLMs on low-resource languages. We hence focus on the regime of using consumer hardware.\nLLMs require large GPU memory to simply store the parameters, limiting the maximum model size we can train/infer. Using 4-bit quantization (Dettmers et al., 2022), we can fit almost 3-4x larger models in the same GPU memory, compared to using 16-bit parameters, at the cost of some precision. The performance improvements of a 4x larger LLM compared to a smaller one 16-bit precision is much larger than the slight loss caused by reduced precision, hence it is beneficial to use as large a model as we show in section 5.2.\nFurthermore, finetuning LLMs would also require large GPU memory to store gradients and optimizer states for all parameters. We instead leverage Quantized Low-Rank Adapters (QLORA, (Dettmers et al., 2023)) to efficiently train adapters on the quantized LLMs. This significantly reduces memory usage without compromising performance, enabling training of much larger models. Using QLoRA with larger models outperforms full finetuning of smaller models as we will show in section 6.1.\nOur data extraction method results in datasets larger than prior datasets sourced from the common crawl. We also show that our model adaptation method results in large improvements on Language Modeling perplexity, and on few-shot prompting (Brown et al., 2020) on downstream Question-Answering tasks (section 5.2.2) and outperforms full-finetuning of smaller models."}, {"title": "Experimental Settings and Implementation Details", "content": "Data collection of UnifiedCrawl was carried out using consumer-grade 500MBps internet connection. Our extracted raw text was formatted in the HuggingFace (Wolf et al., 2020) dataset format. Since the substring deduplication method of (Lee et al., 2022) cannot directly handle datasets in this format, we utilized Text-dedup (Kocetkov et al., 2023) to wrap (Lee et al., 2022)'s implementation of deduplication for compatibility with HuggingFace format. We removed all duplicate substrings of length at-least 50 and all documents shorter than 100 characters, a decision made arbitrarily but following the approach of (Penedo et al., 2023)."}, {"title": "Compute Requirements", "content": "Index filtering is constrained by network download bandwidth for any low-resource language, as the majority of the index is discarded, filtering down to 10s of MBs. The index for all the archives can be processed in a few days on a consumer internet connection of 500MBps using < 10GB of RAM. Alternatively, using a single AWS server with 12Gbps network, each archive can be processed in < 20 minutes, and the entire CC filtered for < 4USD in < 1 day. Cloud big-data querying services such as AWS Athena\u2074 can run this step much faster, but at the cost of 100s of USD. Text extraction and de-duplication from an archive can be processed in only a few minutes, and all CC archives can be processed in a few hours."}, {"title": "Languages", "content": "Our data extraction method underwent testing on seven languages: Hausa (hau), Pashto (pus), Amharic (amh), Yoruba (yor), Sudanese (sun), Sindhi (snd), and Zulu (zul), ordered by the descending number of speakers. We specifically selected very low-resource languages (that constituted less than 0.004% of the Common Crawl dataset each), with the highest number of speakers. Table 1 provides details on each language's ISO code, corresponding number of speakers (in millions), their representation percentage in the Common Crawl dataset (for the \u2018CC-MAIN-2023-14\u2033 archive), and the geographical region where these languages are spoken. By applying our method to these languages, we aim to demonstrate that our implementation and approach are language-agnostic."}, {"title": "Benchmark Datasets", "content": "To assess the scale and efficacy of our UnifiedCrawl dataset, we conducted comparative analysis of its size against other notable datasets sourced from common crawl. The dataset included in this benchmarking are OSCAR, mC4, CC-100 and Wikipedia. This comparative evaluation aims to provide insights into the relative size and representativeness of Unifiedcrawl in comparison to these widely used datasets."}, {"title": "Models and Model Adaptation Settings", "content": "Given the language expertise of the authors of this thesis, particularly in Amharic, we focused our model adaptation and evaluation on datasets in this specific language. Following the extraction of UnifiedCrawl-Amharic from the Common Crawl Corpus using our method, we fine-tuned a multilingual large language model using the lightweight adapter QLoRA. Among the available pre-trained multilingual large language models, we chose the XGLM model (Lin et al., 2022) for adaptation. This XGLM Model is available in two size variants: 564M and 4.5B models.\nThis choice of model is due to its inclusion of the language Amharic in its pretraining dataset. However, this requirement only applies to the XGLM-4.5B parameter model. The XGLM-564M does not include Amharic in its training data. However, we still explored the adaptation process on the smaller model as well. This deliberate selection enables us to explore and analyze the nuances of the adaptation process, considering the variations in language inclusion within the same model. Furthermore, XGLM is larger in size than mGPT, and it performs equally or better than BLOOM (Yong et al., 2023)."}, {"title": "Model Adaptation", "content": "We use the HuggingFace (Wolf et al., 2020) library to implement our code base. We use r = 2 for LORA rank, as(Hu et al., 2022) found small values of r to be effective, and train adapters on all Linear matrices. We finetune these models on our UnifiedCrawl-Amharic dataset for 1 epoch. While multiple epochs should yield better performance, we only train for 1 epoch due to compute constraints. We used original/standard hyper-params wherever applicable and did grid search for learning rate. All experiments were carried out on an Nvidia RTX3070 or a RTX3090, and finetuning took 1 day."}, {"title": "Evaluation Settings", "content": "Our model was evaluated under two settings in terms of language modeling, and downstream few-shot prompting."}, {"title": "Language Modeling Evaluation", "content": "For evaluating the model's capabilities, we compare the perplexity of our model during fine-tuning using QLoRA (Dettmers et al., 2023) on our UnifiedCrawl-Amharic dataset and the original XGLM model, for both variants. Perplexity is defined as the exponential of negative cross-entropy of language modeling, and is a measure of how well our language model is doing in predicting the next word in a sequence given the previous ones. Lower perplexity implies the model is becoming better at predicting the next word in a sequence. Perplexity provides a quantitative and direct measure for comparing different models."}, {"title": "Downstream Evaluation", "content": "Testing the language model on downstream tasks is necessary to evaluate the model's practical applicability, generalization capabilities, and task-specific performance in real-world scenarios. We test our finetuned model on our UnifiedCrawl-Amharic dataset on a downstream task \u2013 in order to evaluate the effectiveness of the fine-tuning process. It helps us to know whether our model has learned useful representations during the fine-tuning process, and that it can be applied to diverse tasks beyond language modeling task, including those it wasn't explicitly trained on."}, {"title": "Question Answering Task", "content": "We chose question Answering, which is a task of generating a response given a question and a context, for evaluating our method's performance on downstream application. QA tasks are valuable downstream evaluations for pre-trained language models as they assess the model's comprehension, reasoning abilities, and contextual understanding. Therefore, by evaluating on QA tasks, we can evaluate how well a language model can extract and synthesize information from the context provided, infer relationships between different parts of the text, and generate coherent responses for a given query. We use the AmQA dataset (Abedissa et al., 2023) for evaluating the model performance on a downstream Question-Answering task."}, {"title": "Few Shot Prompting Evaluation", "content": "This downstream Question-Answering was done under the few-shot prompting (Brown et al., 2020) setting, where the model is given only a small set of examples and is expected to generate the output. This is to assess whether our model can generalize and adapt quickly to new or unseen scenarios, given limited information. For few-shot evaluation on AmQA test set, we use 10 random Context-Question-Answer examples in the prompt. This number was chosen because more examples in the prompt will simply get truncated due to sequence length limitations. We chose these examples from the AmQA train set ,and we appended a question and context to this prompt chosen from the test sample. Our aim is to generate an answer for the question that is selected from the test set. The closer the answer generated to the ground truth label the better. This few-shot evaluation roughly takes 30 minutes for 4.5B XGLM model."}, {"title": "Evaluation Metrics", "content": "We used F1 and EM(Exact Match) scores to evaluate the overall quality and accuracy of our model, as commonly used in Question Answering tasks. F1, which is the harmonic mean of precision and recall, provides a more nuanced evaluation, considering the partial overlaps between the generated answers and the ground truth. Complementing F1 score, the EM score gives the percentage of prediction that exactly matches the ground truth answer. We provided a detailed performance assessment in the next chapter."}, {"title": "Performance Evaluation", "content": "We present analysis of the UnifiedCrawl-language dataset extracted using our data extraction pipeline. We then show experimental results and analysis of the XGLM models fine-tuned on UnifiedCrawl-Amharic using QLoRA. We evaluate the adapted models based on language modeling perplexity and downstream few-shot prompting performance on Question-Answering on AmQA."}, {"title": "Data Collection Evaluation", "content": "We processed a total of 43 archives, starting from \u201cCC-MAIN-2018-43\", which marks the first archive to have language annotations\u2075. Using our proposed data collection approach, we collected a monolingual dataset for 7 languages. This includes Hausa (hau), Pashto (pus), Amharic (amh), Yoruba (yor), Sundanese (sun), Sindhi (snd) and Zulu (zul), chosen based on the number of speakers vs latest crawl size.\nIn the following subsection, we provide a detailed analysis focused on the Amharic (amh) language. The final dataset sizes extracted from the Common Crawl for all seven languages are presented in table 2.\""}, {"title": "UnifiedCrawl Amharic", "content": "Index Filtering: Amharic (ISO: amh) is approximately 0.0036% of Common crawl\u2076. Each Common Crawl archive index is \u2248 250GB compressed. Hence, the expected size of filtered index should be 0.0036% * 250GB \u2248 10MB (single archive percentage in the CC * size of archive index). The index filter process resulted in \u2248 20MB of filtered index uncompressed, as expected. We only keep URLs with the only language as our target language to increase the dataset quality as well as speed up the process. Keeping URLs with any occurrence increases the size of the filtered index by 3x.\nExtracting WARC files: Each archive has \u2248 100TB of compressed WARC files. We only download WARCs corresponding to the target language using Range requests, downloading \u2248 3.5GB WARC per archive.\nFinal Text Extraction: Extracting plaintext from the WARC HTML reduces the size down to 90MB, yielding our final total dataset size of 4GB for all archives.\nDeduplication: Sub-string deduplication is first performed within each archive, and then across all archives. Within each archive, deduplication reduces the size by 60% to 40MB, 1.8GB across all archives. This is de-duplicated to provide our final dataset of size 600MB. Combined, the two de-duplication reduced the dataset size by 85%."}, {"title": "UnifiedCrawl for other Languages", "content": "Similarly, we provide the final sizes of our UnifiedCrawl datasets across 7 languages in table 2. The first column indicates the languages for which we extracted the datasets, the second column provides the size of the datasets extracted with the primary language exclusively in the content (e.g., content_language=[amh]), and the third column estimates the size of datasets where the primary language is our target language but also includes some minor content from other languages (e.g., content_language=[amh, en,..]). Allowing pages with minor content in other languages should increase the dataset size significantly, and we verified this for Yoruba (yor). The size for other languages are estimated based on the fraction of URLs containing other minor languages."}, {"title": "Dataset Comparison with other Datasets", "content": "Using our method, we were able to extract monolingual corpora that exceeds the size of other prior art for low-resource languages, often by multiple orders of magnitude. For example, our extracted dataset (UnifiedCrawl-Amharic) surpasses the sizes of previous datasets for the Amharic language. To illustrate, Amharic Wikipedia dataset is 22MB7, the Amharic News Corpus (Azime and Mohammed, 2021) is 150MB, OSCAR (Abadji et al., 2022) is 500MB, and mC4 (AllenAI, 2021) is 1.2GB. In contrast, our dataset amounts to 4GB before the deduplication step.\nSimilarly, we show comparison of the size of our UnifiedCrawl-Language dataset to other prominent datasets, OSCAR\u00ae, mC49, CC-100\u00b9\u2070, and Wikipedia\u00b9\u00b9 in table 3. All sizes in this table are in MB. OSCAR, mC4, CC-100 are datasets sourced from the Common Crawl Corpus, whereas Wikipedia dataset is a collection of cleaned articles of all languages built from the Wikipedia dumps\u00b9\u00b2 using Tensorflow Datasets."}, {"title": "Method Evaluation", "content": "We evaluate the performance of the models fine-tuned using QLoRA models on our UnifiedCrawl dataset in two settings \u2013 first, we compare their language modeling capability measured through perplexity (PPL) in upstream, and second, we evaluated the model on downstream few-shot prompting tasks. For both cases, we take the original model as a baseline."}, {"title": "Language Modeling Evaluation", "content": "For evaluating pre-training performance in upstream, we analyze the model's perplexity (PPL) during the training process to measure its language modeling capability.\nWe present the results in table 4, where models marked as \"ours\" are fine-tuned on UnifiedCrawl-Amharic dataset using QLoRA. Both our fine-tuned models using QLoRA"}]}