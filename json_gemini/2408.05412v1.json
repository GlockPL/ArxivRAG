{"title": "Style-Preserving Lip Sync via Audio-Aware Style Reference", "authors": ["Weizhi Zhong", "Jichang Li", "Yinqi Cai", "Liang Lin", "Guanbin Li"], "abstract": "Audio-driven lip sync has recently drawn significant attention due to its widespread application in the multimedia domain. Individuals exhibit distinct lip shapes when speaking the same utterance, attributed to the unique speaking styles of individuals, posing a notable challenge for audio-driven lip sync. Earlier methods for such task often bypassed the modeling of personalized speaking styles, resulting in sub-optimal lip sync conforming to the general styles. Recent lip sync techniques attempt to guide the lip sync for arbitrary audio by aggregating information from a style reference video, yet they can not preserve the speaking styles well due to their inaccuracy in style aggregation. This work proposes an innovative audio-aware style reference scheme that effectively leverages the relationships be- tween input audio and reference audio from style reference video to address the style-preserving audio-driven lip sync. Specifically, we first develop an advanced Transformer-based model adept at predicting lip motion corresponding to the input audio, augmented by the style information aggregated through cross-attention layers from style reference video. Afterwards, to better render the lip motion into realistic talking face video, we devise a conditional latent diffusion model, integrating lip motion through modulated convolutional layers and fusing reference facial images via spatial cross-attention layers. Extensive experiments validate the efficacy of the proposed approach in achieving precise lip sync, preserving speaking styles, and generating high-fidelity, realistic talking face videos.", "sections": [{"title": "I. INTRODUCTION", "content": "AUDIO-driven lip sync, also referred to as talking face generation, aims to generate a talking video having lip motion synchronized with the input audio. This technology has garnered significant attention in the multimedia domain in recent years, spanning applications such as video transla- tion [1], visual dubbing [2], virtual avatars [3], and digital humans [4]. Traditionally, achieving accurate lip sync has primarily involved training subject-specific models with data collected from specific individuals. However, these techniques are hampered by the significant costs associated with subject-specific training and data collection processes. Consequently, we focus on addressing the task of audio-driven lip sync through a subject-generic strategy [5]\u2013[10], which, by training once on a large-scale dataset, can be generalized to any individual without the need for further training or fine-tuning.\nIn general, when individuals speak the same utterance, variations in their speaking styles lead to differences in lip shapes [5], [6], [11]\u2013[15], posing a challenge to accurate lip sync. However, most previous subject-generic approaches, such as [1], [2], [7]\u2013[10], [16], have overlooked modeling the speaking style of individuals, as shown in Figure 1(a). These approaches merely predict lip motion from audio without in-corporating any reference information regarding the speaking style of subject. As a result, these subject-generic approaches typically only achieve lip sync consistent with the general speaking styles learned from audio-visual datasets. There are still discrepancies between the lip sync conforming to general speaking styles and those customized to individual speaking styles."}, {"title": "II. RELATED WORK", "content": "With the increasing application in multimedia, audio-driven lip sync has attracted widespread attention in recent years. According to the training paradigm, methods for audio-driven lip sync can be categorized into subject-specific and subject-generic. Subject-specific methods [18], [21]\u2013[25] need to collect data of the target subject for subject-specific train-ing. Although they can achieve lip sync conforming to the individual's speaking style, they can not be generalized to unseen subjects without further training, thus having limited application in the real world. Related to ours, AE-NeRF [18] observes that similar pronunciations have similar lip shapes. However, instead of aggregating the reference lip shapes to model the speaking style, it aggregates the reference image features to build a robust prior for Neural radiance field (NeRF) [26] and can not generalize across different subjects without further training.\nIn contrast, subject-generic approaches for lip sync, such as [1], [2], [5]\u2013[10], [16], have garnered more attention for their ability to infer lip sync for unseen subjects without further training. However, most subject-generic methods [1], [2], [7]\u2013 [10], [16] neglect to model the speaking styles of individuals in the absence of reference information regarding individual speaking style. For example, Wav2Lip [7] pre-trains a lip synchronization discriminator on a large-scale audio-visual dataset to guide the training of a lip sync generator. Yet, it can only achieve lip sync matching the general speaking styles learned from the audio-visual dataset. Similarly, IP-LAP [8] proposes a two-stage talking face generation method based on landmark representation, using a transformer-based model to predict lip landmarks from input audio. However, due to the lack of reference information on individual speaking style, it can not generate lip motions that preserve the speaking style well.\nIn this paper, we propose a method that further improves subject-generic methods, achieving more accurate lip sync that better preserves the individual's speaking style."}, {"title": "B. Style Guided Lip Sync", "content": "Recent advancements in subject-generic methods, e.g. [5], [6], [11]\u2013[13], [17], have made efforts to model the speaking styles of individuals through the integration of style reference videos. Among these, StyleAvatar [13] constructs a video-level style code from a style reference video based on the standard deviation of facial movements. Then, it proposes a latent-style-fusion model to synthesize stylized talking faces for arbitrary audio, imitating the talking style from the style code. Recently, StyleTalk [6] introduces a framework for generating stylized"}, {"title": "III. METHODOLOGY", "content": ""}, {"title": "A. Preliminaries", "content": ""}, {"title": "1) Problem Definition:", "content": "The task of audio-driven lip sync is to generate lip-synced videos given a segment of audio, a video, and a style reference video as inputs. In this task, the lower-half face of the input video is masked, and then the masked region is inpainted with lip-synced content by the proposed method."}, {"title": "2) 3D Morphable Model (3DMM):", "content": "According to [19], the 3DMM mainly serves as a comprehensive three-dimensional facial model that represents facial shape using a fixed number of points. Specifically, the facial shape can be depicted using a facial mesh $S\\in \\mathbb{R}^{3N}$ with N vertices, and such a facial mesh S can be parameterized via Principal Component Analysis (PCA) as follows,\n$S = \\overline{S} + B_{id} \\alpha + B_{exp} \\beta.$ (1)\nHere, $\\overline{S}\\in \\mathbb{R}^{3N}$ denotes the average facial shape, while $B_{id}$ and $B_{exp}$ represent the PCA bases for identity and expression, respectively. The parameters $\\alpha \\in \\mathbb{R}^{80}$ and $\\beta \\in \\mathbb{R}^{64}$ control facial identity and expression, respectively. Head rotation is expressed using another set of parameters, $\\gamma \\in \\mathbb{R}^{3}$.\nTypically, off-the-shelf 3D face reconstruction models en-able the extraction of the parameters $\\alpha$, $\\beta$, $\\gamma$ from any real-world facial image. It is important to note that of the 64 expression parameters, only 13 are significantly related to mouth motions, as illustrated by [5], [6]. Therefore, in this work, we utilize these 13 mouth-related expression parameters to represent lip motion."}, {"title": "3) Method Overview:", "content": "An overview of our method is il-lustrated in Figure 2. In this study, we present a novel two-stage framework called StyleSync, which introduces an inno-vative audio-aware style reference scheme for style-preserving lip sync. Specifically, we first propose a module named Reference-guided Lip Motion Prediction, aimed at predicting lip shapes from input audio signals guided by a style reference video. This is achieved through an innovative Transformer-based model that aggregates speaking style information from the style reference video. Then, StyleSync introduces another module called Realistic Face Rendering, which utilizes a conditional latent diffusion model to produce realistic talking face videos."}, {"title": "B. Reference-guided Lip Motion Prediction", "content": "To accomplish the task of predicting lip motions, we initiate the process with reference audio and lip shape encoding. This step aims to acquire reference audio and lip features from the style reference video. Subsequently, we employ the cross-attention layers in transformer decoder to aggregate the speaking style information from these reference audio and lip features. The aggregated style is then employed to guide the lip motion prediction."}, {"title": "1) Reference Audio and Lip Shape Encoding:", "content": "A style refer-ence video of an individual contains the lip shapes produced when that individual pronounces some utterances, reflecting the speaking style. Initially, we employ DeepSpeech [30] to extract the per-frame DeepSpeech features of the style reference video, denoted as $a_1, a_2,..., a_n$, where n represents the length of the style reference video. These DeepSpeech features are further processed by a 1D temporal convolutional module and fed into a Transformer encoder, yielding reference audio features $k_1,k_2,..., k_n$.\nTo obtain the corresponding lip shape representation from the style reference video, we utilize an off-the-shelf 3D face reconstruction model [31] to extract 3DMM expression param-eters from video frames. We then utilize the mouth-related"}, {"title": "2) Audio-Aware Style Aggregation in Transformer:", "content": "Once obtaining the speaking style information from the style refer-ence video, represented by reference audio and lip features, we utilize a Transformer decoder [32] for generating mouth-related expression parameters corresponding to the input au-dio. As illustrated in Figure 2, the Transformer decoder consists of $N_1$ Transformer blocks, each comprising a stack of self-attention, cross-attention, and feed-forward layers.\nTo capture temporal coherence, the Transformer decoder takes audio features from a temporal window $t - w, t - w + 1,..., t + w$ as input, where w represents the window size and t denotes the current time step. Guided by the reference audio and lip features, it predicts the lip motion at the middle time step t. To achieve this goal, we first encode the DeepSpeech features of the temporal window into audio tokens using a 1D convolutional module, denoted as $a_{t-w}, a_{t-w+1},\u00b7\u00b7\u00b7, a_{t+w}$. Subsequently, these audio tokens are inputted into the self-attention layer of the transformer decoder to generate hidden features $h_{t-w}, h_{t-w+1},\u00b7\u00b7\u00b7, h_{t+w}$.\nTo achieve style aggregation, previous methods, such as [5], [6], [33], utilize a self-attention pooling layer on reference lip features to aggregate the style information. Unlike these previous approaches, this work proposes a novel audio-aware style aggregation scheme using cross-attention layers.\nSpecifically, we begin by treating the obtained reference audio features as keys $K = [k_1,k_2,...,k_n] \\in \\mathbb{R}^{n\\times d}$, and the corresponding reference lip features as values $V = [v_1, v_2,\u2026\u2026, v_n] \\in \\mathbb{R}^{n\\times d}$, where d represents the dimension of the key. These keys and values are inputted into the cross-attention mechanism. Simultaneously, the hidden fea-tures of temporal window act as queries in the cross-attention mechanism, denoted as $Q = [h_{t-w}, h_{t-w+1},\u00b7\u00b7\u00b7, h_{t+w}]^T \\in \\mathbb{R}^{(2w+1)\\times d}$. Therefore, the style reference information can be aggregated through the cross-attention layer, formulated as follows,\n$S = softmax(\\frac{Q K^T}{\\sqrt{d}}) V$ (2)\nwhere $s \\in \\mathbb{R}^{(2w+1)\\times d}$ denotes the aggregated styles for the temporal window at time step t.\nIt is noteworthy that, when predicting lip motions for dif-ferent audio inputs, the attention weights $\\frac{Q K^T}{\\sqrt{d}} \\in \\mathbb{R}^{(2w+1)\\times n}$ vary based on the similarity between the reference audios and the audios of the temporal window. The closer the reference audio is to the audio within the temporal window, the more its corresponding reference lip shape contributes to aggregating the style. This differs from prior works [5], [6], [11], [33], which aggregate a video-level style remaining static for pre-dicting lip motion for arbitrary audio.\nThe aggregated styles are then added to the hidden features in a residual manner, which are further fed into the feed-forward layer. As a result, the Transformer decoder outputs 2w+1 vectors, among which the middle vector, corresponding to the (w + 1)-th position, is inputted into an MLP layer to generate mouth-related expression parameters, denoted by mt, for the time step t."}, {"title": "3) Training Objectives:", "content": "During the stage of lip motion prediction, the optimization involves L1 reconstruction for both mouth-related expression parameters and lip vertices, as detailed below."}, {"title": "As the proposed StyleSync predicts lip motion for each frame individually, ensuring temporal consistency in Lip Mo-tion Prediction is crucial. To achieve this, we follow the common practices of previous methods [5], [6], [34] by first generating the lip motions of successive L frames for each video during training. Subsequently, we utilize the following L1 reconstruction loss for mouth-related expression parameters:", "content": "$L_1 = \\frac{1}{L} \\sum_{i=0}^{L-1} |m_{t+i} - \\hat{m}_{t+i}|$ (3)\nwhere $m_{t+i}$ represents the ground truth mouth-related expres-sion parameters for the (t + i)-th frame.\nAdditionally, since mouth-related expression parameters may contain some information unrelated to lip shape, we also impose an L1 reconstruction loss on lip vertices. Specifically, the predicted mouth-related expression parameters $m_{t+i}$ can be combined with other parameters and then converted into the face mesh vertices using Equation (1). We denote the lip vertices in the facial mesh as $\\hat{l}_{t+i}$, and the reconstruction loss on lip vertices can be expressed as:\n$L_2 = \\frac{1}{L} \\sum_{i=0}^{L-1} |\\hat{l}_{t+i} - l_{t+i}|$ (4)\nwhere $l_{t+i}$ represents the ground truth lip vertices detected by face alignment tools [35].\nOverall, the optimization objective of this stage can be summarized as follows,\n$L_{rec} = L_1 + \\lambda L_2$ (5)\nwhere $\\lambda$ is a scalar for loss item balance, empirically set to 300."}, {"title": "C. Realistic Face Rendering via Diffusion", "content": "To construct the conditional face rendering model tailored for producing lip-synced faces, we first introduce the latent dif-fusion model (LDM) proposed by [20]. LDM originates from the diffusion models (DDPM) [27], which employ a diffusion process to gradually introduce noise into the real data and train a denoising network to reverse the diffusion process. Once trained, the diffusion models can synthesize data by iteratively denoising a normally distributed variable. In comparison to DDPM, LDM [20] performs diffusion and denoising processes in the latent space of a pre-trained autoencoder composed of an encoder $E(.)$ and a decoder $D(.)$, aiming to reduce the computational cost. Specifically, given an input image x, the image is first encoded into the latent, i.e. $z_0 = E(x)$, and then diffused to $z_{t'}$ with time step $t' \\in \\{1,2,\u2026\u2026 ,T'\\}$. Afterwards, LDM trains a U-Net-based [36] denoising network $\\epsilon_\\theta (z_{t'}, t')$ to predict the noise added to the image latent $z_0$, with $\\theta$ denoting its learnable parameters. Thus, the training objective of LDM can be formulated as follows,\n$L_{ldm} = \\mathbb{E}_{z,e\\sim N(0,1),t'} [||e - \\epsilon_\\theta (z_{t'}, t')||^2]$, (6)\nwhere e is the ground-truth noise added to the image latent $z_0$ and $t'$ is uniformly sampled from $\\{1,\u2026\u2026,T'\\}$. During inference, LDM progressively denoise a normally distributed variable $z_{T} \\sim N(0, 1)$ until it reaches a clean latent $z_0$, which can be decoded by D(.) to synthesize image.\nIn this work, since we focus on synchronizing the lip shape in the lower-half face, we conduct both the diffusion and denoising processes in the lower half of the encoded latent as illustrated in Figure 2. Simultaneously, to provide additional context, the upper half of the encoded latent alongside the lower-half portion are integrated into the denoising U-Net. During training, we encode the ground-truth face into $z_0$ which is then diffused to $z_{t'}$ by the noise. During inference, we mask the lower half of the input face and encode it into $z_m$, of which the lower half is diffused to obtain the initial $z_{T'}$.\nTo generate lip-synced talking video, we need to condition the latent diffusion model on the predicted lip motion obtained as detailed in Section III-B. Besides, it is also important to involve the reference facial image condition into the diffu-sion model, aiming to preserve more appearance details of the target subject. Next, we will introduce the conditioning mechanisms used to address these challenges. These include integrating mouth-related expression parameters into LDM for lip motion alignment via modulated convolution, and incor-porating reference facial image into LDM for high-fidelity generation via spatial cross-attention."}, {"title": "1) Aligning Lip Motion via Modulated Convolution:", "content": "In the lip motion prediction stage, we have predicted the mouth-related expression parameters representing the lip motion synchronized to the input audio. To ensure that the synthesized facial image has lip shape aligned with the one reflected by the mouth-related expression parameters, we integrate the lip motion condition into the latent diffusion model via mod-ulated convolutional layers, drawing inspiration from Style-GAN2 [37].\nSpecifically, as shown in the Figure 2, the mouth-related expression parameters are first combined with the identity and rotation parameters extracted from the input video frame, aiming to preserve the subject identity and head rotation. The combined parameters are then encoded to a latent code by a 1D convolutional module. The latent code is employed to modulate the convolution weights in the convolutional layers of denoising U-Net. Formally, we denote the original convolution weights as w. The latent code is mapped to a scale set $\\Phi = \\{\\phi_i\\}$ by an MLP layer, where $\\phi_i$ is the scale coefficient corresponding to the i-th input feature map. Then, we compute the weight of modulated convolution $W_{ijk}$ as follow,\n$W_{ijk} = \\frac{\\phi_i w_{ijk}}{\\sqrt{\\sum_{i,k} (\\phi_i \\cdot w_{ijk})^2 + \\epsilon'}}$ (7)\nwhere j indicates the convolution weight for the j-th output features map, k indicates the spatial footprint of the convolu-tion, and $\\epsilon'$ is a small constant for avoiding numerical issues. Finally, we perform the convolution operation with the modulated convolution weight to produce the output features map."}, {"title": "IV. EXPERIMENTS", "content": ""}, {"title": "A. Experimental Setups", "content": ""}, {"title": "V. CONCLUSION", "content": "In this paper, we have introduced a novel method called StyleSync to tackle the problem of audio-driven lip sync. It can effectively respond to the challenges of traditional methods that can only achieve sub-optimal lip sync conforming to the general speaking styles or struggle to preserve the speaking styles of individuals. Our proposed audio-aware style reference scheme, leveraging the relationships between input audio and reference audio from style reference video, effectively achieves style-preserving lip sync. In detail, the components developed by StyleSync include a novel Transformer-based lip motion prediction model that integrates speaking style information from a style reference video to accurately predict lip motions, alongside a novel conditional latent diffusion model that translates these predicted motions into realistic, lip-synced videos. These significantly contribute to producing realistic and high-fidelity talking face videos. Experiments as well as comprehensive ablation analysis have revealed StyleSync's superiority in handling the tasks of audio-driven lip sync."}]}