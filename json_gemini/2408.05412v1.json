{"title": "Style-Preserving Lip Sync via Audio-Aware Style Reference", "authors": ["Weizhi Zhong", "Jichang Li", "Yinqi Cai", "Liang Lin", "Guanbin Li"], "abstract": "Audio-driven lip sync has recently drawn significant attention due to its widespread application in the multimedia domain. Individuals exhibit distinct lip shapes when speaking the same utterance, attributed to the unique speaking styles of individuals, posing a notable challenge for audio-driven lip sync. Earlier methods for such task often bypassed the modeling of personalized speaking styles, resulting in sub-optimal lip sync conforming to the general styles. Recent lip sync techniques attempt to guide the lip sync for arbitrary audio by aggregating information from a style reference video, yet they can not preserve the speaking styles well due to their inaccuracy in style aggregation. This work proposes an innovative audio-aware style reference scheme that effectively leverages the relationships be-tween input audio and reference audio from style reference video to address the style-preserving audio-driven lip sync. Specifically, we first develop an advanced Transformer-based model adept at predicting lip motion corresponding to the input audio, augmented by the style information aggregated through cross-attention layers from style reference video. Afterwards, to better render the lip motion into realistic talking face video, we devise a conditional latent diffusion model, integrating lip motion through modulated convolutional layers and fusing reference facial images via spatial cross-attention layers. Extensive experiments validate the efficacy of the proposed approach in achieving precise lip sync, preserving speaking styles, and generating high-fidelity, realistic talking face videos.", "sections": [{"title": "I. INTRODUCTION", "content": "AUDIO-driven lip sync, also referred to as talking face generation, aims to generate a talking video having lip motion synchronized with the input audio. This technology has garnered significant attention in the multimedia domain in recent years, spanning applications such as video transla-tion [1], visual dubbing [2], virtual avatars [3], and digital humans [4]. Traditionally, achieving accurate lip sync has primarily involved training subject-specific models with data collected from specific individuals. However, these techniques are hampered by the significant costs associated with subject-specific training and data collection processes. Consequently, we focus on addressing the task of audio-driven lip sync through a subject-generic strategy [5]\u2013[10], which, by training once on a large-scale dataset, can be generalized to any individual without the need for further training or fine-tuning.\nIn general, when individuals speak the same utterance, variations in their speaking styles lead to differences in lip shapes [5], [6], [11]\u2013[15], posing a challenge to accurate lip sync. However, most previous subject-generic approaches, such as [1], [2], [7]\u2013[10], [16], have overlooked modeling the speaking style of individuals, as shown in Figure 1(a). These approaches merely predict lip motion from audio without in-corporating any reference information regarding the speaking style of subject. As a result, these subject-generic approaches typically only achieve lip sync consistent with the general speaking styles learned from audio-visual datasets. There are still discrepancies between the lip sync conforming to general speaking styles and those customized to individual speaking styles.\nTo alleviate this issue, recent methods [5], [6], [11]\u2013[13], [17] have attempted to incorporate a short style reference video of the individual into the lip sync model as illustrated in Figure 1(b). They learn a video-level style code from the reference video to guide the lip sync for arbitrary audio. However, predicting lip shapes for different audio requires referencing different lip shapes from the style reference video. Therefore, the compact and static video-level style codes learned by these methods are still insufficient to achieve satisfactory lip sync.\nInspired by the observation from [18] that similar pronunci-ations produce similar lip shapes, this paper posits that within a style reference video, if a lip shape corresponds to the reference audio that is closer to the input audio, it should contribute more significantly to the style coding. Thus, we approach Audio-driven Lip Sync from a novel perspective and propose an innovative audio-aware style reference scheme. As shown in Figure 1(c), our method leverages the audio signals in the style reference video to aggregate distinct style codes for different input audios, facilitating Style-Preserving Lip Sync.\nSpecifically, we propose a novel Transformer-based archi-tecture designed to predict lip motions from audio inputs, influenced by a style reference video. The process commences with the extraction of lip shapes from a style reference video utilizing the 3D Morphable Model (3DMM) [19]. Subse-quently, these lip shapes and their associated audio signals are encoded via dual Transformer encoders, yielding respective representations for reference lip and audio features. Input audio information is then integrated into a Transformer de-coder to generate hidden features, which are further processed by a cross-attention layer within the decoder to aggregate style information from the style reference video. Herein, the reference audio and lip features serve as keys and values, respectively, while the hidden features derived from the input audio act as queries. Ultimately, the Transformer decoder outputs mouth-related expression parameters representing lip motion. To render realistic talking face videos from lip motion, this research devises a conditional latent diffusion model [20] tailored for producing lip-synced faces. Specifically, the inte-gration of mouth-related expression parameters into the latent diffusion model is achieved through modulated convolutional layers. This approach ensures that the synthesized lip shapes are aligned with those represented by mouth-related expres-sion parameters. Additionally, the incorporation of reference facial image into the diffusion model is facilitated via spatial cross-attention layers, which enhance the model's capacity to capture subject-specific appearance details, thereby improving the fidelity of the generated talking faces.\nThe primary contributions of this study are summarized as follows.\n\u2022\nWe propose a novel audio-aware style reference scheme that leverages the intrinsic connections between input and reference audios to effectively aggregate speaking style information, thereby achieving style-preserving lip synchronization.\nWe have devised an advanced Transformer-based lip sync model that integrates speaking style information from a style reference video to predict lip motions. Additionally, we introduced a novel conditional latent diffusion model to translate the predicted lip motions into lip-synced and realistic videos.\nExtensive experiments demonstrate that our proposed method achieves style-preserving lip synchronization, which effectively preserves the speaking style of the individual, resulting in realistic and high-fidelity talking face videos."}, {"title": "II. RELATED WORK", "content": "With the increasing application in multimedia, audio-driven lip sync has attracted widespread attention in recent years. According to the training paradigm, methods for audio-driven lip sync can be categorized into subject-specific and subject-generic. Subject-specific methods [18], [21]\u2013[25] need to collect data of the target subject for subject-specific train-ing. Although they can achieve lip sync conforming to the individual's speaking style, they can not be generalized to unseen subjects without further training, thus having limited application in the real world. Related to ours, AE-NeRF [18] observes that similar pronunciations have similar lip shapes. However, instead of aggregating the reference lip shapes to model the speaking style, it aggregates the reference image features to build a robust prior for Neural radiance field (NeRF) [26] and can not generalize across different subjects without further training.\nIn contrast, subject-generic approaches for lip sync, such as [1], [2], [5]-[10], [16], have garnered more attention for their ability to infer lip sync for unseen subjects without further training. However, most subject-generic methods [1], [2], [7]-[10], [16] neglect to model the speaking styles of individuals in the absence of reference information regarding individual speaking style. For example, Wav2Lip [7] pre-trains a lip synchronization discriminator on a large-scale audio-visual dataset to guide the training of a lip sync generator. Yet, it can only achieve lip sync matching the general speaking styles learned from the audio-visual dataset. Similarly, IP-LAP [8] proposes a two-stage talking face generation method based on landmark representation, using a transformer-based model to predict lip landmarks from input audio. However, due to the lack of reference information on individual speaking style, it can not generate lip motions that preserve the speaking style well.\nIn this paper, we propose a method that further improves subject-generic methods, achieving more accurate lip sync that better preserves the individual's speaking style."}, {"title": "B. Style-Guided Lip Sync", "content": "Recent advancements in subject-generic methods, e.g. [5], [6], [11]\u2013[13], [17], have made efforts to model the speaking styles of individuals through the integration of style reference videos. Among these, StyleAvatar [13] constructs a video-level style code from a style reference video based on the standard deviation of facial movements. Then, it proposes a latent-style-fusion model to synthesize stylized talking faces for arbitrary audio, imitating the talking style from the style code. Recently, StyleTalk [6] introduces a framework for generating stylized"}, {"title": "III. METHODOLOGY", "content": "1) Problem Definition: The task of audio-driven lip sync is to generate lip-synced videos given a segment of audio, a video, and a style reference video as inputs. In this task, the lower-half face of the input video is masked, and then the masked region is inpainted with lip-synced content by the proposed method."}, {"title": "2) 3D Morphable Model (3DMM)", "content": "According to [19], the 3DMM mainly serves as a comprehensive three-dimensional facial model that represents facial shape using a fixed number of points. Specifically, the facial shape can be depicted using a facial mesh \\(S \\in R^{3N}\\) with N vertices, and such a facial mesh S can be parameterized via Principal Component Analysis (PCA) as follows,\n\\(S = \\overline{S} + B_{id} \\alpha + B_{exp} \\beta.\\)   (1)\nHere, \\(\\overline{S} \\in R^{3N}\\) denotes the average facial shape, while \\(B_{id}\\) and \\(B_{exp}\\) represent the PCA bases for identity and expression, respectively. The parameters \\(\\alpha \\in R^{80}\\) and \\(\\beta \\in R^{64}\\) control facial identity and expression, respectively. Head rotation is expressed using another set of parameters, \\(\\gamma \\in R^3\\).\nTypically, off-the-shelf 3D face reconstruction models en-able the extraction of the parameters \\(\\alpha, \\beta, \\gamma\\) from any real-world facial image. It is important to note that of the 64 expression parameters, only 13 are significantly related to mouth motions, as illustrated by [5], [6]. Therefore, in this work, we utilize these 13 mouth-related expression parameters to represent lip motion."}, {"title": "3) Method Overview", "content": "An overview of our method is il-lustrated in Figure 2. In this study, we present a novel two-stage framework called StyleSync, which introduces an inno-vative audio-aware style reference scheme for style-preserving lip sync. Specifically, we first propose a module named Reference-guided Lip Motion Prediction, aimed at predicting lip shapes from input audio signals guided by a style reference video. This is achieved through an innovative Transformer-based model that aggregates speaking style information from the style reference video. Then, StyleSync introduces another module called Realistic Face Rendering, which utilizes a conditional latent diffusion model to produce realistic talking face videos."}, {"title": "B. Reference-guided Lip Motion Prediction", "content": "To accomplish the task of predicting lip motions, we initiate the process with reference audio and lip shape encoding. This step aims to acquire reference audio and lip features from the style reference video. Subsequently, we employ the cross-attention layers in transformer decoder to aggregate the speaking style information from these reference audio and lip features. The aggregated style is then employed to guide the lip motion prediction."}, {"title": "1) Reference Audio and Lip Shape Encoding", "content": "A style refer-ence video of an individual contains the lip shapes produced when that individual pronounces some utterances, reflecting the speaking style. Initially, we employ DeepSpeech [30] to extract the per-frame DeepSpeech features of the style reference video, denoted as \\(a_1, a_2, ..., a_n\\), where n represents the length of the style reference video. These DeepSpeech features are further processed by a 1D temporal convolutional module and fed into a Transformer encoder, yielding reference audio features \\(k_1, k_2, ..., k_n\\).\nTo obtain the corresponding lip shape representation from the style reference video, we utilize an off-the-shelf 3D face reconstruction model [31] to extract 3DMM expression param-eters from video frames. We then utilize the mouth-related"}, {"title": "2) Audio-Aware Style Aggregation in Transformer", "content": "Once obtaining the speaking style information from the style refer-ence video, represented by reference audio and lip features, we utilize a Transformer decoder [32] for generating mouth-related expression parameters corresponding to the input au-dio. As illustrated in Figure 2, the Transformer decoder consists of \\(N_1\\) Transformer blocks, each comprising a stack of self-attention, cross-attention, and feed-forward layers.\nTo capture temporal coherence, the Transformer decoder takes audio features from a temporal window \\(t-w, t-w+1, ..., t+w\\) as input, where w represents the window size and t denotes the current time step. Guided by the reference audio and lip features, it predicts the lip motion at the middle time step t. To achieve this goal, we first encode the DeepSpeech features of the temporal window into audio tokens using a 1D convolutional module, denoted as \\(a_{t-w}, a_{t-w+1}, ..., a_{t+w}\\). Subsequently, these audio tokens are inputted into the self-attention layer of the transformer decoder to generate hidden features \\(h_{t-w}, h_{t-w+1}, ..., h_{t+w}\\).\nTo achieve style aggregation, previous methods, such as [5], [6], [33], utilize a self-attention pooling layer on reference lip features to aggregate the style information. Unlike these previous approaches, this work proposes a novel audio-aware style aggregation scheme using cross-attention layers.\nSpecifically, we begin by treating the obtained reference audio features as keys \\(K = [k_1, k_2, ..., k_n] \\in R^{n \\times d}\\), and the corresponding reference lip features as values \\(V = [v_1, v_2, ..., v_n] \\in R^{n \\times d}\\), where d represents the dimension of the key. These keys and values are inputted into the cross-attention mechanism. Simultaneously, the hidden fea-tures of temporal window act as queries in the cross-attention mechanism, denoted as \\(Q = [h_{t-w}, h_{t-w+1}, ..., h_{t+w}]^T \\in R^{(2w+1) \\times d}\\). Therefore, the style reference information can be aggregated through the cross-attention layer, formulated as follows,\n\\(S = softmax(\\frac{QK^T}{\\sqrt{d}})V \\)\t(2)\nwhere \\(s \\in R^{(2w+1) \\times d}\\) denotes the aggregated styles for the temporal window at time step t.\nIt is noteworthy that, when predicting lip motions for dif-ferent audio inputs, the attention weights \\(\\frac{QK^T}{\\sqrt{d}} \\in R^{(2w+1) \\times n}\\) vary based on the similarity between the reference audios and the audios of the temporal window. The closer the reference audio is to the audio within the temporal window, the more its corresponding reference lip shape contributes to aggregating the style. This differs from prior works [5], [6], [11], [33], which aggregate a video-level style remaining static for pre-dicting lip motion for arbitrary audio.\nThe aggregated styles are then added to the hidden features in a residual manner, which are further fed into the feed-forward layer. As a result, the Transformer decoder outputs 2w+1 vectors, among which the middle vector, corresponding to the (w + 1)-th position, is inputted into an MLP layer to generate mouth-related expression parameters, denoted by \\(m_t\\), for the time step t."}, {"title": "3) Training Objectives", "content": "During the stage of lip motion prediction, the optimization involves L1 reconstruction for both mouth-related expression parameters and lip vertices, as detailed below."}, {"title": "C. Realistic Face Rendering via Diffusion", "content": "To construct the conditional face rendering model tailored for producing lip-synced faces, we first introduce the latent dif-fusion model (LDM) proposed by [20]. LDM originates from the diffusion models (DDPM) [27], which employ a diffusion process to gradually introduce noise into the real data and train a denoising network to reverse the diffusion process. Once trained, the diffusion models can synthesize data by iteratively denoising a normally distributed variable. In comparison to DDPM, LDM [20] performs diffusion and denoising processes in the latent space of a pre-trained autoencoder composed of an encoder E(.) and a decoder D(\u00b7), aiming to reduce the computational cost. Specifically, given an input image x, the image is first encoded into the latent, i.e. \\(z_0 = E(x)\\), and then diffused to \\(z_{t'}\\) with time step \\(t' \\in \\{1, 2, ..., T'\\}\\). Afterwards, LDM trains a U-Net-based [36] denoising network \\(\\epsilon_\\theta(z_{t'}, t')\\) to predict the noise added to the image latent \\(z_0\\), with \\(\u03b8\\) denoting its learnable parameters. Thus, the training objective of LDM can be formulated as follows,\n\\(L_{ldm} = E_{z, \\epsilon \\sim N(0,1), t'} [|\\epsilon - \\epsilon_\\theta(z_{t'}, t')|^2],\\)\t(6)\nwhere \\(\u03b5\\) is the ground-truth noise added to the image latent \\(z_0\\) and t\u2019 is uniformly sampled from \\(\\{1, ..., T'\\}\\). During inference, LDM progressively denoise a normally distributed variable \\(z_{T'} \\sim N(0, 1)\\) until it reaches a clean latent \\(z_0\\), which can be decoded by D(.) to synthesize image.\nIn this work, since we focus on synchronizing the lip shape in the lower-half face, we conduct both the diffusion and denoising processes in the lower half of the encoded latent as illustrated in Figure 2. Simultaneously, to provide additional context, the upper half of the encoded latent alongside the lower-half portion are integrated into the denoising U-Net.\nDuring training, we encode the ground-truth face into \\(z_0\\) which is then diffused to \\(z_{t'}\\) by the noise. During inference, we mask the lower half of the input face and encode it into \\(z_m\\), of which the lower half is diffused to obtain the initial \\(z_{T'}\\).\nTo generate lip-synced talking video, we need to condition the latent diffusion model on the predicted lip motion obtained as detailed in Section III-B. Besides, it is also important to involve the reference facial image condition into the diffu-sion model, aiming to preserve more appearance details of the target subject. Next, we will introduce the conditioning mechanisms used to address these challenges. These include integrating mouth-related expression parameters into LDM for lip motion alignment via modulated convolution, and incor-porating reference facial image into LDM for high-fidelity generation via spatial cross-attention."}, {"title": "1) Aligning Lip Motion via Modulated Convolution", "content": "In the lip motion prediction stage, we have predicted the mouth-related expression parameters representing the lip motion synchronized to the input audio. To ensure that the synthesized facial image has lip shape aligned with the one reflected by the mouth-related expression parameters, we integrate the lip motion condition into the latent diffusion model via mod-ulated convolutional layers, drawing inspiration from Style-GAN2 [37].\nSpecifically, as shown in the Figure 2, the mouth-related expression parameters are first combined with the identity and rotation parameters extracted from the input video frame, aiming to preserve the subject identity and head rotation. The combined parameters are then encoded to a latent code by a 1D convolutional module. The latent code is employed to modulate the convolution weights in the convolutional layers of denoising U-Net. Formally, we denote the original convolution weights as w. The latent code is mapped to a scale set \\(\\Phi = \\{\\phi_i\\}\\) by an MLP layer, where \\(\\phi_i\\) is the scale coefficient corresponding to the i-th input feature map. Then, we compute the weight of modulated convolution \\(W_{ijk}\\) as follow,\n\\(W_{ijk} = \\frac{\\phi_i \\cdot w_{ijk}}{\\sqrt{\\sum_{i,k} (\\phi_i \\cdot w_{ijk})^2 + \\epsilon'} } \\)\t(7)\nwhere j indicates the convolution weight for the j-th output features map, k indicates the spatial footprint of the convolu-tion, and \\(\\epsilon'\\) is a small constant for avoiding numerical issues. Finally, we perform the convolution operation with the modulated convolution weight to produce the output features map."}, {"title": "2) High-fidelity Generation with Spatial Attention", "content": "To en-sure the generated face has high-fidelity appearance of the subject, a reference facial image should be provided as a"}, {"title": "IV. EXPERIMENTS", "content": "In this study, we conduct experiments on two widely utilized audio-visual datasets: VoxCeleb [40] and HDTF [14]. The VoxCeleb dataset comprises over 100,000 utterances from 1,251 celebrities, sourced from videos up-loaded to YouTube. It showcases a broad diversity in terms of ethnicities, accents, and ages, and is a standard bench mark commonly used in prior work. On the other hand, the HDTF dataset is a high-resolution audio-visual collection consisting of approximately 362 unique videos spanning over 15.8 hours, with resolutions of 720P or 1080P. To facilitate a fair comparison, all comparison methods, along with the proposed StyleSync, are trained on VoxCeleb and tested on both datasets."}, {"title": "2) Implementation Details", "content": "The videos are resampled to 25 fps, and the facial images cropped from the video frames are resized to 256\u00d7256 resolution. The proposed method is com posed of two stages, each trained separately. For Reference guided Lip Motion Prediction, the style reference video is extracted from the talking clip of the same individual at different time steps. Importantly, there is no overlap between the style reference video and the ground-truth video during testing. The length n of the style reference video is set to 256 following previous methods [6]. The transformer decoder consists of N1 = 3 blocks, with the temporal window size w set to 5. The query dimension d is 256, and we utilize the multi-head attention layer with 8 heads. In the stage of Realistic Face Rendering, the number of time steps for diffusion, denoted as T, is 1000. The latent space of the pre-trained autoencoder has a spatial resolution of 64\u00d764. During inference, we employ the DDIM [29] sampler with 200 steps to accelerate the generation process. The reference facial image is randomly selected from the input video, and all comparison methods use the same reference facial image for a fair evaluation. After obtaining the generated face, we blend it with the background following the previous method [8], producing final talking face videos.\nWe train all modules using the Adam optimizer [41]. The lip motion prediction network is trained with a batch size of 256 and a learning rate of 2e-4 on 4 NVIDIA 3090 GPUs. In our latent diffusion framework, the autoencoder is initialized with the pre-trained weight from latent-diffusion [42] and is frozen during training. The latent diffusion model is trained with a batch size of 12 and a learning rate of 4e-5 on 4 NVIDIA 3090 GPUs."}, {"title": "3) Evaluation Metrics", "content": "In the task of audio-driven lip sync, the evaluation mainly focuses on the accuracy of lip sync and the visual quality of the generated facial video. For the assessment of speaking style preservation in lip sync, this study utilizes the widely adopted metric called Lip Landmarks Distance (LipLMD) [43], which quantifies the discrepancies between the generated lip shapes and the ground-truth lip shapes corresponding to the input audio. A lower score of LipLMD indicates that the generated lip shapes are closer to the ground-truth lip shapes reflecting the individual\u2019s speaking style, signifying a more accurate preservation of the speaking style in the generated lip sync. Furthermore, SyncScore [44] is another commonly used metric for evaluating the synchro nization of lip shape with the input audio. The SyncScore is computed based on the distance between the audio feature and visual feature in SyncNet [44]. However, since SyncNet does not model the speaking styles of the individuals, SyncScore primarily assesses whether the generated lip shape matches the lip shape conforming to the general speaking style learned from a large-scale audio-visual dataset. As previously men tioned, there are still discrepancies between the lip shapes con forming to the general speaking styles and those customized to the individual\u2019s speaking style. Therefore, it is worth noting that the SyncScore is insufficient to assess the speaking style preservation. A better SyncScore only indicates that the distance between audio and visual features in SyncNet is closer, which does not necessarily mean that the generated lip shapes are more accurate and preserve the speaking style better. Despite reporting results for this metric, we recommend LipLMD as the preferred indicator for evaluating lip sync quality since our study focuses on the task of style-preserving lip sync.\nAdditionally, in assessing the visual quality of the generated talking face videos, this study employs Peak Signal-to-Noise Ratio (PSNR) and Structured Similarity (SSIM) [45] for eval uating pixel-level visual quality. Moreover, we assess feature level visual quality using the Learned Perceptual Image Patch Similarity (LPIPS) [46] and Fr\u00e9chet Inception Distance (FID) [47]. Compared to pixel-level measurements, the feature-level evaluations are more aligned with the human perception, as"}, {"title": "B. Quantitative Evaluation", "content": "We conduct quantitative comparisons with the state-of-the-art methods in terms of lip sync quality and visual quality. Wav2Lip [7], IP-LAP [8], DiffTalk [10], and the proposed approach focuses on generating the lower-half face for lip sync while inheriting the upper-half face from the input video. To be fair, we compute the visual quality metrics solely based on the lower half of the generated face. We report the quantitative comparison results in the Table I."}, {"title": "1) Lip Sync Quality", "content": "As detailed by the LipLMD metric, the proposed method outperforms all of the comparison base-lines on both VoxCeleb and HDTF datasets. This demonstrates its generation of more accurate lip sync that better preserves the speaking styles of the individuals. Specifically, Wav2Lip, PC-AVS, DiffTalk, IP-LAP, and PD-FGC lag significantly be-hind our proposed method in LipLMD due to their incapability to model the speaking styles of individuals. StyleTalk incor-porates the use of style reference videos to model speaking styles. However, it still lags behind our StyleSync because of its inaccuracies in style aggregation. Specifically, StyleSync's LipLMD obtains lower scores than StyleTalk by 8.4% on VoxCeleb and 13.6% on HDTF.\nIn SyncScore, Wav2Lip and PC-AVS achieve higher scores than the proposed method. This is because Wav2Lip utilizes SyncNet as the lip sync expert to guide the training of its gen-erator, and PC-AVS adopts an audio-visual contrastive learning mechanism similar to SyncNet's. However, this only indicates that their generated lip shapes are closer to the lip shapes matching the general speaking style rather than the individuals' speaking styles, since the SyncNet for computing SyncScore overlooks modeling the speaking styles of individuals."}, {"title": "2) Visual Quality", "content": "As shown in the Table I, the proposed StyleSync consistently outperforms other algorithms in terms of visual quality metrics, including PSNR, SSIM, LPIPS, and FID. Particularly noteworthy are the improvements observed in the feature-level metrics LPIPS and FID, where our method demonstrates significant enhancements over other approaches. This suggests that our results align more closely with human perception, demonstrating the efficacy of our conditional la-tent diffusion model in rendering high-fidelity and realistic faces. As well, DiffTalk devises a latent diffusion framework to generate high-fidelity talking face videos but still lags behind ours. This may be because it simply concatenates the misaligned reference image latent as condition while our"}, {"title": "C. Qualitative Evaluation", "content": "For a comprehensive qualitative evaluation, we visual-ize some representative comparison results conducted on HDTF [14] and VoxCeleb [40] datasets in Figure 3 and give some analyses in the following subsections. To facilitate comparisons, we present the cropped faces from the generated results of all methods including the ground-truth face. More-over, we conduct a user study from volunteers and provide a supplementary video for further evaluation."}, {"title": "1) Lip Sync Quality", "content": "As shown in Figure 3, compared to other algorithms, the proposed method can produce lip shapes that are closer to the ground truth. This indicates our superiority in achieving accurate lip sync conforming to the speaking styles of individuals. StyleTalk sometimes generates inaccurate lip shapes due to its inaccuracy of the style aggregation scheme. Moreover, other approaches, such as DiffTalk, PD-FGC, IP-LAP, PC-AVS, and Wav2Lip, produce sub-optimal lip shapes because they overlook modeling the speaking styles of individuals."}, {"title": "2) Visual Quality", "content": "From Figure 3, it can be seen that the proposed method can generate more realistic faces with high-fidelity facial details and fewer artifacts. Compared to other methods, our results visually resemble real faces more closely. This verifies the effectiveness of the proposed conditional latent diffusion model in rendering lip motion into realis-tic faces. During implementation, StyleTalk uses the same rendering model as ours, resulting in visual quality similar to ours. Besides, DiffTalk exhibits more artifacts than our approach and loses some facial details of the subject. More-over, PD-FGC and PC-AVS yield results that compromise the subject's identity and display significant flaws. Meanwhile, IP-LAP generates facial details that are blurrier than ours and exhibits artifacts particularly under extreme head poses. Similarly, Wav2Lip produces blurry results with unrealistic mouth details."}, {"title": "E. Ablation Study", "content": "To understand the advantage of the core components pro-posed in our StyleSync, we conduct extensive experiments of ablation study on the HDTF dataset. Here, we only adopt the LipLMD metric to evaluate the lip sync quality. This is because the SyncScore primarily assesses whether the gener-ated lip sync matches the general speaking style, as previously mentioned. We report all numerical ablation results in Table III and present the qualitative results in Figure 5."}, {"title": "1) Effect of Audio-Aware Style Aggregation", "content": "The main contribution of the proposed method is the introduction of the novel audio-aware style reference scheme, which aggre-gates speaking style information by exploring the relationship between the input audio and the reference audios. Actually, the effectiveness of this has been verified by the fact that the LipLMD score of our method is significantly lower than that of StyleTalk as shown in Table I.\nFurthermore, to verify the role of the reference audio, we devise a model variant where the reference audios from the style reference video are not utilized for style aggregation like previous methods [6], [13]. Specifically, the Transformer encoder for encoding the reference audio is removed from the proposed framework, and the reference lip features encoded from the reference lip shapes act as both keys and values for the cross-attention layers in transformer decoder. We report the result of this variant in the row \"Ours w/o Reference Audio\", and its LipLMD score increases by 6.5% compared to our full model's. Besides, as shown in Figure 5, without the reference audio, the lip sync quality gets worse. This verifies the necessity to explore the relationship between reference and input audio for accurate and style-preserving lip sync."}, {"title": "2) Effect of Vertices Loss", "content": "As the mouth-related 3DMM ex-pression parameters may involve some information unrelated to lip shape, we apply the reconstruction loss over the lip vertices converted from the predicted expression parameters. To verify the effectiveness of this for enhancing lip sync, we"}, {"title": "3) Effect of Style Reference Video", "content": "To evaluate the ef-fectiveness of incorporating the style reference video for modeling the speaking style of individuals, we introduce a variant that predicts lip motion solely from input audio without utilizing the style reference video. In this variant, the reference branch of our lip motion prediction network is removed, and we replace the cross-attention layers in the transformer decoder with self-attention layers. The performance of this variant is reported by the row labeled \"Ours w/o Reference (n=0)\" in Table III. It is evident that the LipLMD score significantly increases without modeling the speaking style.\nAdditionally, to assess the impact of the length n of the style reference video, we varied n during inference and reported results for n = 64 and n = 128 in Table III. Increasing the length of the style reference video leads to improved LipLMD metrics. Besides, as shown in Figure 5, with the length n of the style reference video decreasing, the lip sync quality gradually deteriorates."}, {"title": "V. CONCLUSION", "content": "In this paper, we have introduced a novel method called StyleSync to tackle the problem of audio-driven lip sync. It can effectively respond to the challenges of traditional methods that can only achieve sub-optimal lip sync conforming to the general speaking styles or struggle to preserve the speaking styles of individuals. Our proposed audio-aware style reference scheme, leveraging the relationships between input audio and reference audio from style reference video, effectively achieves style-preserving lip sync. In detail, the components developed by StyleSync include a novel Transformer-based lip motion prediction model that integrates speaking style information from a style reference video to accurately predict lip motions, alongside a novel conditional latent diffusion model that translates these predicted motions into realistic, lip-synced videos. These significantly contribute to producing realistic and high-fidelity talking face videos. Experiments as well as comprehensive ablation analysis have revealed StyleSync's superiority in handling the tasks of audio-driven lip sync."}]}