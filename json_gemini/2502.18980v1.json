{"title": "PEToolLLM: Towards Personalized Tool Learning in Large Language Models", "authors": ["Qiancheng Xu", "Yongqi Li", "Heming Xia", "Fan Liu", "Min Yang", "Wenjie Li"], "abstract": "Tool learning has emerged as a promising direction by extending Large Language Models' (LLMs) capabilities with external tools. Existing tool learning studies primarily focus on the general-purpose tool-use capability, which addresses explicit user requirements in instructions. However, they overlook the importance of personalized tool-use capability, leading to an inability to handle implicit user preferences. To address the limitation, we first formulate the task of personalized tool learning, which integrates user's interaction history towards personalized tool usage. To fill the gap of missing benchmarks, we construct PEToolBench, featuring diverse user preferences reflected in interaction history under three distinct personalized settings, and encompassing a wide range of tool-use scenarios. Moreover, we propose a framework PEToolLLaMA to adapt LLMs to the personalized tool learning task, which is trained through supervised fine-tuning and direct preference optimization. Extensive experiments on PEToolBench demonstrate the superiority of PEToolLLaMA over existing LLMs. We release our code and data at https://github.com/travis-xu/PEToolBench.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) possess extensive knowledge and have powerful instruction-following abilities, making them effective AI assistants for tasks such as text rewriting, question answering, and code writing (Zhao et al., 2023). However, they often struggle in addressing user needs in scenarios such as checking weather and booking flights. To address this, tool learning (Qin et al., 2024a; Qu et al., 2024) has emerged as a promising solution by enabling LLMs to utilize external tools, such as real-time weather APIs and booking systems. In this way, tool learning has extended LLMs' capabilities to tackle more complex tasks, enabling them to fulfill a wide range of user needs.\nCurrent tool learning procedure typically begins with a user instruction, and then LLMs are required to use tools with appropriate functionalities for satisfying users' needs. Existing tool learning methods can be categorized into in-context learning (Wu et al., 2024; Liu et al., 2025b) and fine-tuning approaches (Schick et al., 2023; Wang et al., 2025b). The former approach allows LLMs to use tools by directly providing tool documentation in input but the performance is constrained by the input length. The latter approach trains LLMs to internalize tool knowledge but struggles with tool generalization.\nDespite the advancement, existing tool learning methods primarily focus on the general-purpose tool-use capability but overlook the critical role of personalization. In tool learning, more personalized user needs are expected to be derived from the user's previous tool usage history as a supplement to user instructions, which can help LLMs provide more customized tool-usage assistance to enhance the user experience. As illustrated in Figure 1, personalized tool learning is non-trivial due to the following aspects. 1) Implicit user preferences. User preferences for tool usage are often implicitly conveyed through the user's history rather than explicitly stated in user instructions, making them difficult to understand. For instance, when a user requests a search for articles, their preference for academic-related content needs to be inferred from previous interactions with academic tools like Google Scholar. 2) Non-functional tool attributes. Since many tools have the same functionalities, user preferences cannot be effectively distinguished based solely on tool functionalities. This underscores the need to consider non-functional tool attributes, such as usability, integrability, and accessibility, which can better reflect user preferences. As shown in Figure 1, Google Search can be distinguished from other search tools by its integra-"}, {"title": "2 Related Work", "content": "2.1 Tool Learning in LLMS\nTool learning aims at extending the capabilities of LLMs by equipping them with external tools to solve tasks like weather inquiry, car navigation, and restaurant reservation. Existing benchmarks primarily focus on evaluating the tool learning proficiency of LLMs in addressing user instructions, from aspects such as tool selection and calling accuracy (Xu et al., 2024; Wang et al., 2024b; Ye et al., 2024; Wang et al., 2025a), tool planning ability (Basu et al., 2024; Wang et al., 2024a; Shen et al., 2024; Liu et al., 2025a), and complex workflow creation (SHEN et al., 2025; Qiao et al., 2025; Fan et al., 2025). To improve tool-use capabilities, various strategies have been introduced, including in-context learning which enables LLMs to use tools via documentation (Yuan et al., 2024; Shi et al., 2024; Qu et al., 2025), and fine-tuning which trains LLMs on specialized tool-use datasets (Zhuang et al., 2024; Chen et al., 2024, 2025). However, prior studies neglect the crucial role of personalized tool usage in LLMs. This paper addresses this gap by introducing personalized tool learning, developing a comprehensive benchmark for evaluation, and proposing an optimization strategy to enhance personalized tool-use capabilities in LLMs.\n2.2 Personalization in LLMs\nThe goal of personalization in LLMs is to leverage personal user data, such as historical behaviors and background information, to generate outputs that better align with the user preferences (Tseng et al., 2024). Approaches such as fine-tuning (Cai et al., 2025) and prompt engineering (Yuan et al., 2025) have been explored to adapt LLMs to individual or domain-specific tasks. These approaches have been applied across various fields, including recommendation systems (Lyu et al., 2024), search engines (Zhou et al., 2024), education (Liu et al., 2024), and dialogue generation (Wang et al., 2023). However, previous research has not investigated LLMs' personalization in the area of tool learning. In this work, we bridge this gap by incorporating user's interaction history to assess and enhance the LLMs' capability in providing personalized tool-usage assistance for specific users."}, {"title": "3 Task and Benchmark", "content": "3.1 Task Formulation\nTool Learning Given an instruction $q_u$ of the user $u$, tool learning aims to generate an appropriate tool call, including the selected tool and its corresponding parameters, from a set of candidate tools. Formally, let the candidate tool set be $T = {d(t_1), d(t_2), ..., d(t_v)}$, where $d(t_i)$ represents the documentation of tool $t_i$ and N is the total number of candidate tools. The LLM is then tasked with generating a tool call $c = (t,p)$, where $t\\in T$ and $p$ denotes its parameters:\n$(t,p) = LLM(q_u, T)$.\nPersonalized Tool Learning In personalized tool learning, we incorporate the users' interaction history alongside their instructions, enabling the LLM to generate tool calls that satisfy both the users' explicit requirements and implicit preferences. For a user $u$, we define the interaction history as $H_u = {h_u^1, h_u^2, ..., h_u^M}$, where each $h_u^i$ consists of a past user instruction $q_u^i$ and the corresponding tool call $c_u^i = (t_u^i, p_u^i)$, with $t_u^i$ representing the selected tool and $p_u^i$ denoting its associated parameters. Let $c_u = (t_u, P_u)$ represent the personalized tool call for user $u$, the personalized tool learning task can then be formulated as:\n$(t_u, p_u) = LLM(q_u, T, H_u)$.\n3.2 Benchmark Construction\nDue to the lack of real user interaction histories on tool-usage, we adopt a tool-driven approach to simulate interactions based on pre-constructed user's tool preferences. The whole process for constructing PEToolBench, illustrated in Figure 2, consists of three steps: tool preparation, preference construction, and data creation.\n3.2.1 Tool Preparation\nTool Collection Following ToolBench (Qin et al., 2024b), we adopt the tools from RapidAPI for our benchmark, since it offers a large-scale and diverse collection of real-world tools that can potentially address a wide range of user needs. To ensure the quality of the collected tools, we perform strict filtering by removing: 1) outdated tools, which are marked as deprecated in RapidAPI; 2) tools with insufficient information, such as inadequate or missing tool documentation; and 3) duplicate tools, which have repeated tool names, descriptions, or category names."}, {"title": "4 Method: PEToolLLAMA", "content": "To equip LLM with personalized tool-use capability, we conduct a two-stage training process: 1) personalized SFT, where LLM is fine-tuned on PEToolBench to acquire fundamental proficiency in personalized tool usage, and 2) personalized DPO, where LLM is optimized on a preference dataset for better alignment with user preferences.\nPersonalized SFT. The first stage in our approach is Supervised Fine-Tuning (SFT), where we directly fine-tune LLM on the training set of PEToolBench. Given the user's instruction $q_u$, interaction history $H_u$, and the candidate tool set T as inputs, LLM is trained to generate the ground truth tool call $c$. $H_u$ uniformly covers all three types of user interactions to capture diverse user preferences. In this way, LLM can obtain basic personalized tool-usage experiences by understanding both the user needs and preferences.\nPersonalized DPO. In the second stage, we further enhance the LLM's performance through direct preference optimization (DPO) (Rafailov et al., 2023). Our goal is to guide the LLM to call the user's preferred tools instead of non-preferred ones. Specifically, for each user instruction $q_u$, we collect multiple tool calls generated by LLM after the SFT stage. Then we select the user's preferred and non-preferred tool calls $c_w$ and $c_i$ based on the user's tool preference constructed in PEToolBench. $c_w$ and $c_i$ will be used to construct the preference dataset $D_{DPO} = {(x, c_w, c_i)}$, where x denotes the input, including the user instruction $q_u$, interaction history $H_u$, and the candidate tool set T. We then apply DPO to optimize the LLM by guiding it to generate the desired tool call $c_w$ while avoid generating $c_i$. The loss function can be defined as:\n$\\mathcal{L} = -E \\log \\sigma(\\beta \\log (\\frac{\\pi_{\\theta}(c_w|x)}{\\pi_{ref}(c_w|x)} - \\frac{\\pi_{\\theta}(c_i|x)}{\\pi_{ref}(c_i|x)}))$,\nwhere \u03c3 is the logistic function and \u03b2 is a weighting parameter that controls the deviation of the policy model $\u03c0_\u03b8$ (i.e., the LLM we need to optimize) from the reference model $\u03c0_{ref}$ (i.e., the LLM after SFT stage). In this way, LLM can focus on generating tool calls that are more aligned with individual user preferences."}, {"title": "5 Experiments", "content": "5.1 Setup\nBaselines. We adopt multiple LLMs from both closed-source and open-source models to ensure a comprehensive evaluation. For closed-source LLMs, we select two representative models: GPT-4o and GPT-4o-mini from OpenAI. For open-source LLMs, we include a wide spectrum of models, i.e., LLaMA-3.1-8B (Dubey et al., 2024), QWen-2.5-7B (Yang et al., 2024), Vicuna-7B-v1.5 (Chiang et al., 2023) and Mistral-7B-v0.3 (Jiang et al., 2023).\nImplementation details. In PEToolBench construction, we employ gpt-4o-mini as the LLM for tool understanding and generation of user instructions and interaction history. The candidate tool set consists of three parts: the ground-truth tool along with all other tools sharing the same functionality, five tools retrieved using ToolRetriever (Qin et al., 2024b), and the remaining tools that were randomly sampled."}, {"title": "6 Conclusion and Future Work", "content": "In this paper, we advanced general-purpose tool-use LLMs into personalized tool-use LLMs, aiming to provide users with customized tool-usage assistance. We formulate the task of personalized tool learning and identify the goal of leveraging user's interaction history to achieve implicit preference understanding and personalized tool calling. For training and evaluation, we construct the first PEToolBench benchmark, featuring diverse users' interaction history in three types. We also propose a novel personalized framework PEToolLLaMA conducted under a two-stage training process to endow LLMs with personalized tool-use capabilities. Extensive experiments on PEToolBench demonstrate that PEToolLLaMA consistently surpasses existing baselines, effectively meeting user requirements and preferences. We believe that the task, benchmark, and framework for personalized tool learning will broaden the research scope, introduce new challenges and inspire novel methods.\nIn the future, we aim to enhance this work from the following dimensions. 1) We plan to explore more heterogeneous personal user data beyond interaction history, such as user profiles or personas. This will allow us to reflect user preferences from multiple dimensions, providing a more comprehensive evaluation on the personalized tool-use capabilities of LLMs. 2) Currently, our work is limited to tool-usage scenarios involving a single tool. In the future, we intend to expand to more complex"}]}