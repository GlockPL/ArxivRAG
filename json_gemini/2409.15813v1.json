{"title": "Layer-wise Model Merging for Unsupervised Domain Adaptation in Segmentation Tasks", "authors": ["Roberto Alcover-Couso", "Juan C. SanMiguel", "Marcos Escudero-Vi\u00f1olo", "Jose M Mart\u00ednez"], "abstract": "Merging parameters of multiple models has resurfaced as an effective strategy to enhance task performance and robustness, but prior work is limited by the high costs of ensemble creation and inference. In this paper, we leverage the abundance of freely accessible trained models to introduce a cost-free approach to model merging. It focuses on a layer-wise integration of merged models, aim- ing to maintain the distinctiveness of the task-specific final layers while unifying the initial layers, which are primarily associated with feature extraction. This approach ensures parameter consistency across all layers, essential for boosting performance. Moreover, it facilitates seamless integration of knowledge, enabling effective merging of models from different datasets and tasks. Specifically, we investigate its applicability in Unsupervised Domain Adaptation (UDA), an unexplored area for model merging, for Semantic and Panoptic Segmentation. Experimental results demonstrate substantial UDA improvements without addi- tional costs for merging same-architecture models from distinct datasets (\u21912.6% mIoU) and different-architecture models with a shared backbone (\u21916.8% mIoU). Furthermore, merging Semantic and Panoptic Segmentation models increases mPQ by \u2191 7%. These findings are validated across a wide variety of UDA strategies, architectures, and datasets.", "sections": [{"title": "1 Introduction", "content": "Unsupervised domain adaptation (UDA) aims to train models from a source labeled domain that can generalize to unlabeled target domains, key for tasks with limited annotated data due to the capture difficulty or the high annotation costs (e.g., seman- tic or panoptic segmentation [1]). Thus, automatically-annotated synthetic datasets emerge as attractive alternative for the source domain for segmentation tasks [2\u20136]. While UDA has shown remarkable success in various Computer Vision tasks, UDA for segmentation tasks often faces unstable training [7\u201311] and underperforms supervised models [12\u201315]. In this situation, teacher-student distillation has emerged as a stan- dard technique for enhancing UDA training stability [2, 3, 16, 17], albeit with increased costs due to inferencing the teacher while training and inferencing the student [16, 17]. Model merging arises as a promising technique to overcome the teacher-student computational overhead, as combining parameters directly only requires a single model inference [18\u201321]. Early research on model merging focused on learning convergence[18, 19], but declined over time with newer training and regularization methods. Later, the rise of large language models and self-supervised learning have reignited interest in model merging [22\u201324], as it provides a training-free mechanism to leverage knowledge from different sources. Recent methods apply model merging mostly to different checkpoints obtained during the same training process, eliminating additional computation costs for ensemble training and inference [22, 25\u201327]. However, these methods use models close in the parameter space, such as ones obtained from different seeds [21, 24] or different fine-tuning versions [23, 28\u201330], as merging models with misaligned layers typically leads to a notable performance decline [27, 29\u201331]. Therefore, current methods disregard to explore the possible benefits of combining the plethora of existing models trained using different methods and tasks. Additionally, to the best of our knowledge, model merging methods have not been explored and benchmarked for UDA [2\u20135, 32]. To overcome the above-mentioned deficiencies, we propose a novel layer-wise merging method that exploits the specific strengths of deeper layers closer to the classi- fication output and the broader generalization capabilities of shallower layers, making it capable of effectively merging models derived from different datasets and tasks. Moreover, we argue that the combination of model weights in the context of UDA has far more applications than the limited scope in which it is currently being employed (i.e., teacher-student framework) such as combining models from different tasks. To evaluate our proposal for Semantic and Panoptic Segmentation, we set an extensive baseline by experimenting with various model merging methods based on the same or different datasets and architectures. Our experiments also address two challenges. The first one aims to incorporate high-performing yet computationally expensive models into faster but less accurate ones. Our proposal increases speed by 3.25 times with performance increases up to +6.8% in mIoU. The second challenge is for combining models from easier tasks into harder tasks. Our proposal presents performance gains of +3.4% in mIoU and +7% in mPQ compared to previous state-of-the-art UDA methods for respectively Semantic and Panoptic Segmentation (see Figure 1). Our results are"}, {"title": "2 Related Work", "content": "Training strategies for Unsupervised Domain Adaptation (UDA). UDA has been widely explored for semantic [3, 4, 7, 33], and panoptic segmentation [2, 34, 35]. Methods based on Deep Learning focus on minimizing source-target differences using statistical metrics like maximum mean discrepancy [36\u201338], correlation alignment [39\u201341], or entropy minimization [8, 10, 42]. Others employ Adversarial training by learning a domain discriminator within a Generative Adversarial Network (GAN) framework [43] to enforce domain-invariant inputs [44\u201346], features [33, 47\u201349], or out- puts [8, 9, 11]. Lastly, self-training generates pseudo-labels [50] for the target domain based on predictions obtained using confidence thresholds [6, 51, 52] or pseudo-label prototypes [53, 54]. More recent methods based on Transformers tackle noisy training and concept drifting [55, 56] by incorporating consistency regularization [57, 58] to ensure uniformity across diverse data augmentations [59, 60], different crops [4, 61], domain-mixup [4, 62\u201364] or curriculum learning [7, 65]. Currently, enhancing robust- ness is dominated by the teacher-student framework [21], where the teacher model is updated via an exponential moving average of the student weights [2\u20135]. Albeit effec- tive, this framework incurs in additional computational costs which limit its scalability and practicality for resource-constrained environments.\nLeveraging models knowledge. Averaging model weights initially boosted neural network training by addressing slow algorithms with poor convergence rates"}, {"title": "3 Layer-wise model merging", "content": "Our proposal relies on previous research for fine-tuning [22, 26, 69, 70]. By comparing the initial and the resulting weights after fine-tuning to a specific task, it is suggested that deeper layers in the model encapsulate task-specific filters. Meanwhile, shallow layers represent task-agnostic filters which should be less tuned to obtain optimal per- formance. Our work follows this hypothesis to define a layer-wise parameter merging, having early layers merged in a uniform manner, as they encode general knowledge and their smoothing should provide more robust features [18, 19, 21, 25]. While preserving last layers so that the task-specific knowledge is retained after merging [22, 26, 69, 70]. The following subsections detail the assumptions of our proposal, investigate this hypothesis for the UDA setup, and provide the definition of how model merging is conducted."}, {"title": "3.1 Preliminaries", "content": "In the context of this work, we define an architecture as the specific arrangement of lay- ers and their connections within a neural network. A model refers to this architecture after it has been trained on a particular dataset using a specific training regimen. We further distinguish between the backbone and the classification head of the model: the backbone consists of the initial and intermediate layers responsible for feature extrac- tion, while the classification head comprises the final layers designed for task-specific purposes, tailored to a specific set of classes and tasks.\nLet $M$ be the number of trained models, each one with its respective parameters defined by $\\theta^{1},..., \\theta^{i}, ..., \\theta^{M}$. We consider that each model can be decomposed into a number of layers, where $(\\theta^{i})_{j}$ are the parameters of the $i$th model and the $j$th layer. Initial and intermediate layers form the backbone, whereas the model\u2019s last layers correspond to the classification head, tailored to the specific task.\nFollowing the assumption prevalent in related work [18, 20, 23, 24, 29, 30, 67], we presume access to the performance metrics of models prior to initiating the merging process. Furthermore, our approach assumes the availability of pre-trained models, aligning with contemporary practices centered on foundational models where exten- sive pre-training on large datasets provides a robust starting point for adapting to various tasks [74]. Lastly, we assume all models undergoing merging have undergone the same pre-training for the shared parameters, ensuring their future alignment [24]. Despite variations from different training schemes, the models' shared parameters should remain relatively close within the parameter space, similarly to [24]."}, {"title": "3.2 Exploring heterogeneity of models in UDA", "content": "We analyze model variations in two contexts: consistent models throughout train- ing epochs (Checkpoint training scenario) and different models shaped by varied strategies (Heterogeneous training scenario). Figure 2 illustrates the layer-by-layer dif- ferences in model parameters, including both convolutional (weights and biases) and batch-normalization parameters (mean and variance). The discrepancy is measured as the number of parameters with relative absolute Euclidean distance above a certain threshold $\\tau$: $|\\theta^{(i)}_{j} - \\theta^{(k)}_{j}| \\geq \\tau$.\nUpon analyzing the differences between checkpoints (see Figure 2a), we observe various behaviours in the parameters analyzed. Globally, convolutional parameters show significant changes mainly in the classification head (i.e., last layers). On the other hand, weight parameters undergo slight changes throughout the training process in backbone (i.e., initial and middle layers). This finding aligns with the training pro- cess, which assigns a learning rate ten times larger to the classification head compared to the backbone [8\u201311]. Thus, it is anticipated that head parameters would undergo substantial changes compared to backbone parameters, reflecting their adaptation to the specific task.\nIn contrast, the batch-normalization parameters (mean and variance), undergo substantial transformations as the training progresses in the different checkpoints. These changes are more pronounced and do not appear to align with the small changes observed in the convolutional layers. This observation suggests that these parameters"}, {"title": "3.3 How is model merge conducted", "content": "We consider that the $M$ models can be trained with different training strategies or multiple checkpoints which share a set of parameters ${\\theta^{(j)}_{1}}_{j=1}^{N_{p}}$ ${\\theta^{(j)}_{2}}_{j=1}^{N_{p}}$. In order to perform the merging, we first select an anchor model $\\theta_{janchor}$, from the $M$ models to be combined, this anchor model will be the main contributor of knowledge for the merging. Then we propose to combine them by a layer-level weighting in order to get the merged layer $\\theta^{*}_{(j)}$:\n\\begin{equation}\n \\theta^{*}_{(j)} = \\begin{cases}\n  \\sum^{M} H^{(j)}_{i} \\theta^{(j)}_{i}  & \\text{if } j \\in [1,..., N_{p}]\\\\\n  \\theta^{(j)}_{janchor} & \\text{otherwise}.\n \\end{cases}\n\\end{equation}\nwhere $\\theta^{(i)}_{(j)}$ represents the j-th layer of model $\\theta$, and $H^{(j)}_{i}$ is the corresponding layer- level weight. Notably, the layer-level weights for each layer are normalized such that $\\sum^{M} H^{(j)}_{i} = 1$, and the weight of the anchor model $janchor$ is greater than the one assigned to the rest: $H^{janchor} > H^{janchor}_{iji} | i \\neq janchor$.\nWe propose to uniformly merge the initial layers and gradually decrease the weight given to final layers for the non-anchor models, while increasing the weight given to final layers of the anchor model, denoted as $\\theta_{janchor}$. Specifically, $H^{(j)}_{i}$ is defined as:\n$H^{(j)}_{i} = \\frac{N_{p} - j}{N_{p} M} \\forall i \\neq janchor$ and $j \\in [1, ..., N_{p}]$ are the indexes of shared parameters.\nIf the merged models share all the parameters, then $N_{p} = card(\\theta)$. Notably, this merging allows for the merging of models with different number of classes, unlike alternative merging methods restricted to the same architecture for the classification heads. Thus, being our proposal the only viable solution for merging models trained on different tasks and datasets (see Table 1).\nHow is inference conducted with merged models.\nAs depicted by Table 1 weight-level merging techniques do not have additional infer- ence costs. This is because the models are computed at a checkpoint level and stored to be loaded as any other model checkpoint. Therefore, for inference, output ensemble models employ: $\\sum^{M} f(\\theta)$ while our models employ: $f(\\theta^{*})$."}, {"title": "4 Experimental Exploration", "content": "4.1 Experimental setup\nImage segmentation\nDatasets. We employ popular UDA datasets for semantic and panoptic segmentation: GTA [76] and Synthia [77] as source datasets; Cityscapes [1] and Mapilliary [78] as target datasets. GTA is a synthetic dataset with urban scenes composed of 25K images from the video-game Grand Theft Auto V, which shares 19 classes with Cityscapes. Synthia is a collection of different synthetic urban scenes datasets, we pick its subset SYNTHIA-RAND-CITYSCAPES which is composed of 9.5K images sharing 16 classes with Cityscapes. Cityscapes is a real dataset with urban scenes generated by filming with a camera inside of a car while driving. It consists of 3K images for training and 0.5K images for validation, with 19 annotated classes. The Mapillary dataset is another real urban scene dataset, which comprises 18K and 2K images for training and validation, with 152 annotated classes.\nEvaluation metrics. For semantic segmentation, we employ mean per-class inter- section over union (mIoU) [79], between the model prediction and the ground-truth label. IoU measures at pixel-level the relationship between True Positives (TP), False\nPositives (FP) and False Negatives (FN): $IoU = \\frac{TP}{TP+FP+FN}$. For panoptic segmen- tation, we employ the mean Segmentation Quality (mSQ), mean Recognition Quality (mRQ) and mean Panoptic Quality (mPQ) [80]. The mSQ measures the closeness of the predicted segments with their ground truths, mRQ is equivalent to the F1 score and the mPQ combines RQ and SQ: PQ = SQ\u00d7RQ at a per-class level.\nApplication to image classification and object detection\nDatasets We utilize the Office-31 [81] and Office-Home [82] datasets to validate our proposal on image classification. The Office-31 dataset contains 4,6K images composed of 31 object classes under three domains. The Office-Home dataset consists of 15,5K images of 65 categories from four domains. Additionally, we also employ the UDA setup of Cityscapes [1] to the Foggy Cityscapes daset [83] for object detection. Foggy Cityscapes is a synthetic foggy dataset which simulates fog on real scenes. Each foggy image is rendered with a clear image and depth map from Cityscapes. Thus the annotations and data split in Foggy Cityscapes are inherited from Cityscapes."}, {"title": "4.2 Merging of models trained with same dataset and architecture", "content": "This section investigates the merging of models trained on the same dataset and architecture for semantic segmentation. As merging methods, we compare Isotropic [18, 23], Fisher [24], output-ensemble [67] and our proposed layer-wise merging with different UDA strategies based on convolutional architectures (Advent [8], MinEnt [8], FADA [9], MaxSquare [10] and AdaptSegNet [11]) and transformer architectures (DAFormer [3], HRDA [4], MIC [5] and PiPa [84]).\nSame UDA strategy. Merging models from the same UDA strategy is concep- tually similar to the update scheme of the teacher model in UDA [3]. However, no"}, {"title": "Exploring merging parameters.", "content": "In developing our framework, we made two design decisions: First, to determine the optimal starting layer and weight for assigning layer-wise weights, we conducted exper- iments visualized in Figure 5a. This figure contrasts the weight distribution between two models when initiating layer-wise merging at different layers. Additionally, Figure 5b demonstrates that introducing weight adjustments at later stages adversely affects the performance on the target dataset. Conversely, the performance on the source domain performance remains stable across various starting layers, indicating that our layer-wise merging technique predominantly enhances target dataset outcomes.\nSecond, the impact of varying the weights assigned to the initial layer is depicted in Figure 5c, which shows the outcome of merging two models with differing initial weights, formalized as $\\theta^{*(1)} = \\alpha \\theta^{(1)} + (1 - a)\\theta_{21)}$. Notably, we discovered that an $\\alpha$ value of 0.5, representing an equal weighting approach, yields the highest performance on the target domain. This isotropic merging strategy, as compared in the main body of our work, demonstrates the effectiveness of balanced weight distribution in the initial layer.\nBased on these experiments, we conclude that the source dataset's loss is an unreliable predictor of target dataset performance. This observation explains the ineffectiveness of Fisher merging [24] in merging UDA models."}, {"title": "4.3 Merging of models trained with different segmentation heads.", "content": "This section examines the merging of models trained for semantic segmentation with different classification heads, sharing a common backbone. This head disparity could be due to the varying number of classes in the trained datasets, as it changes the last classification layer, or because the classification head architecture is different.\nSame head different number of classes. Table 4a presents the merging of GTA and Synthia datasets on the HRDA architecture, which have different numbers of classes (19 and 16 specifically). As previous solutions are not suited for merging"}, {"title": "4.4 Potential benefits of model merging on different architectures", "content": "Here, we present two strong benefits of merging models from different architectures: merging high-performing architectures into less accurate but faster architectures and transferring the knowledge of an easier task to a harder one.\nMerging high-performing architectures into less accurate but faster architectures Table 5 demonstrates the merging of HRDA, a high-performing model, into DAFormer, a shallower architecture. Our layer-wise merging achieves a mIoU of 73.0 while maintaining the same inference speed, as no additional parameters are included. This showcases the potential of our method in combining the strengths of different architectures.\nMerging knowledge between tasks Another possibility of model merging is the merging of models from semantic segmentation to panoptic segmentation. Specifically, Table 6 presents the results of merging HRDA [4] for semantic segmentation and EDAPS [2] for panoptic segmentation. The resulting model improves across all metrics: +2.9 in mPQ, +3.0 in mRQ and +1.4 in mSQ the panoptic segmentation model[2]."}, {"title": "4.5 Comparison with the state of the art", "content": "Tables 7 and 8 compare the per-class performance with state-of-the-art UDA methods for semantic and panoptic segmentation. Our merged models consistently present clear performance improvements on each benchmark. Specifically, we improve globally the state-of-the-art performance by +.4 mIoU on GTA2-to-Cityscapes, by +2.3 mIoU on Synthia-to-Cityscapes, by +2.9 mPQ on Synthia-to-Cityscapes, and by +1.9 mPQ on Synthia-to-Mapillary. Considering the class-wise performance in Tables 7 and 8, our models achieve consistent improvements for most classes when compared to the previous state-of-the-art methods across datasets and tasks. Compared to MIC [5] for semantic segmentation and EDAPS [2] for panoptic segmentation, classes that have close lookalikes such as vehicles benefit the most. We argue that our layer-wise model merging enables the combination of the discriminative qualities of each merged model, thereby enhancing performance across visually related groups (e.g., up to +3% mIoU and +11% mPQ performance increase on vehicle type classes)."}, {"title": "5 Conclusions", "content": "In this paper, we study the employment of model merging methods in the context of UDA to combine different models parameters. Additionally, we propose a method for merging models accounting for the specificity of the architecture layer depth. Our proposal can be easily included in other UDA frameworks as it only employs model checkpoints, or it can combine models using the same architecture but trained differ- ently. Note that layer-wise model merging is computationally free in terms of training and inference. In a comprehensive evaluation, we have presented different compar- atives to highlight the potential of model merging and the significant performance improvements our layer-wise merging achieves. For instance, improving the state-of- the-art performance by +2.3 and +2.9 (in terms of mIoU)on the Synthia-to-Cityscapes semantic and panoptic segmentation benchmarks respectively. We hope that, due to its high impact on performance and train-free nature, UDA researchers may introduce model merging into their frameworks."}]}