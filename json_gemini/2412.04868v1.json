[{"title": "NebulaFL: Effective Asynchronous Federated Learning for JointCloud Computing", "authors": ["Fei Gao", "Ming Hu", "Zhiyu Xie", "Peichang Shi", "Xiaofei Xie", "Guodong Yi", "Huaimin Wang"], "abstract": "With advancements in AI infrastructure and Trusted Execution Environment (TEE) technology, Federated Learning as a Service (FLaaS) through JointCloud Computing (JCC) is promising to break through the resource constraints caused by heterogeneous edge devices in the traditional Federated Learning (FL) paradigm. Specifically, with the protection from TEE, data owners can achieve efficient model training with high-performance AI services in the cloud. By providing additional FL services, cloud service providers can achieve collaborative learning among data owners. However, FLaaS still faces three challenges, i.e., i) low training performance caused by heterogeneous data among data owners, ii) high communication overhead among different clouds (i.e., data centers), and iii) lack of efficient resource scheduling strategies to balance training time and cost. To address these challenges, this paper presents a novel asynchronous FL approach named NebulaFL for collaborative model training among multiple clouds. To address data heterogeneity issues, NebulaFL adopts a version control-based asynchronous FL training scheme in each data center to balance training time among data owners. To reduce communication overhead, NebulaFL adopts a decentralized model rotation mechanism to achieve effective knowledge sharing among data centers. To balance training time and cost, NebulaFL integrates a reward-guided strategy for data owners selection and resource scheduling. The experimental results demonstrate that, compared to the state-of-the-art FL methods, NebulaFL can achieve up to 5.71% accuracy improvement. In addition, NebulaFL can reduce up to 50% communication overhead and 61.94% costs under a target accuracy.", "sections": [{"title": "I. INTRODUCTION", "content": "Due to the advantages of privacy protection, Federated Learning (FL) [1]\u2013[3] has emerged as a solution for distributed privacy-preserving machine learning. In the conventional FL paradigm, a central cloud server coordinates with multiple local clients for collaborative model training, where the cloud server dispatches a global model to clients for local training and aggregates models uploaded from clients to update the global model. Although FL has drawn a blueprint to address the problem of data silos, its practical application still faces significant limitations due to constraints in hardware resources and the uncertain physical environments of edge devices. Specifically, personal edge devices, such as smart-phones and IoT devices, often have limited computational power, memory, and storage, making it difficult for them to handle complex model training tasks. Furthermore, various un-certain factors [4], such as network delays, human-in-the-loop variability, and device failures, contribute to uncontrollable training processes and system fragility.\nWith the improvement of AI infrastructure, cloud-based AI training is becoming increasingly popular due to its superior efficiency and more stable performance compared to edge de-vices. Moreover, advances in Trusted Execution Environment (TEE) [5] technology have effectively addressed concerns about data privacy leakage, making cloud-based model training more secure. Leveraging cloud services and TEEs, data owners can securely upload their data to the cloud and perform FL using cloud-based AI services. This has given rise to Federated Learning as a Service (FLaaS), a promising new offering for cloud providers that supports privacy-preserving collaborative training among different data owners. Specifically, each data owner maintains a TEE container in the cloud and uploads their raw data to this container. The local training process is conducted within the TEE container to prevent privacy leakage. Cloud service providers then offer model aggregation and distribution services, ensuring a secure and efficient FL training process.\nHowever, FLaaS still faces three serious challenges, i.e., heterogeneous data, cross-cloud training, and resource scheduling, respectively. First, similarly to the traditional FL paradigm, FLaaS encounters low training performance due to the heterogeneity of data in different TEE containers, which cannot be shared. Specifically, the data among data owners are non-Independent and Identically Distributed (non-IID), leading to the \"client drifting\" problem [6] that significantly limits the inference performance of the trained global model. Additionally, variations in data volume among data owners re-sult in different local model training times, causing a straggler problem where slower clients delay the overall training pro-cess. Therefore, data heterogeneity remains a serious challenge in FLaaS. Second, FLaaS typically involves collaborative training across multiple data centers, known as JointCloud computing [7], [8]. Data owners are often located in different countries or regions. Due to legal constraints and the need for efficient data storage and transmission, even a single cloud service provider often utilizes multiple data centers. In the JointCloud scenario, FL requires frequent aggregation"}, {"title": "II. PRELIMINARY AND RELATED WORK", "content": "Federated Learning. The classic FL systems typically employ a cloud-edge architecture [17], which consists of a centralized server for model aggregation and multiple edge nodes for local training. However, this paper focuses on a new scenario involving multi-center FL. Consider that the FL System has a set of $C = \\{C_1, C_2,...,C_N\\}$ data centers. Each datacenter $C_i$ has $M_i$ data owners holding hetergeneous data, which is denoted by $D_i = \\{D_{i,1}, D_{i,2}, ..., D_{i, M_i} \\}$. In this setup, each data center trains its own local model with heterogeneous data owners, and these local models are col-laboratively aggregated into a global model. So far, traditional federated learning methods commonly aggregate local models using the FedAvg algorithm [18] to produce the global model. The objective of a FL optimization problem is typically defined as follows:\n$\\min_{W} F(w) = \\frac{1}{N} \\sum_{i=1}^N f_i(w)$ (1)\ns.t., $f_i(w) = \\frac{1}{M_i} \\sum_{j=1}^{M_i} l(w; d_{i,j})$.\nHere, $l$ and $f_i$ represent the loss function of a data owner's sample and all samples of data center $C_i$, and $F$ represents the global model's loss function. We focus solely on the inference performance of the unified global model for FLaaS, excluding personalized and clustered FL scenarios."}, {"title": "B. Related Work on FL Optimization", "content": "To enhance the inference performance of traditional FL, numerous works about architectures and workflow optimiza-tion have been extensively researched, including edge-cloud collaboration, data heterogeneity management, resource allo-cation and task scheduling, communication optimization [19], [20]. So far, to improve the performance of specific FL methods, especially in non-IID scenarios, current work can be broadly categorized into the following three categories. Firstly, the data heterogeneity problem is often addressed using techniques such as model regularization and data clustering to mitigate the impact of differences in data distribution on performance [21]\u2013[23]. For example, the method [21] mixes model updates from previous rounds with the current round's updates to avoid model shifts. In [22] proposed FedHP to achieve fast convergence by jointly optimizing both the local updating frequency and network topology in DFL. Secondly, to tackle the straggler problem, various acceleration techniques have been developed [14], [16], [24], [25]. In synchronous FL, For example, PruneFL [25] introduced a pruning-based method designed to accelerate local training for stragglers in a synchronous setup. For asynchronous FL, Ma et al. [16] proposed a semi-asynchronous approach with a model buffer for aggregation. In gitFL [14], a branch model with an older version is more likely to be assigned to a faster and less frequently selected device for the next round of local training. However, the above methods are not well-suited for FLaaS scenarios. The network differences within and between data centers can significantly increase the waiting time due to delays. Lastly, communication bottleneck is also a key issue hindering the performance of FL [19], [20], [26]\u2013[29]. Work [26] proposes a new approach called Adaptive Parameter Freezing by freezing stable parameters intermittently during periods of synchronization. In [27], a lightweight deep neural network that adopts the designed lightweight fire modules and has a side branch for communication.\nAlthough many optimization methods have been proposed to enhance FL performance, they are mostly tailored to specific edge-cloud scenarios, which cannot be directly applied to JointCloud scenarios for due to varying resource availability and network conditions across data centers. This limitation prevents the efficient utilization of performance and resources for FLaaS. To address this, we introduce NebulaFL, which adopts a dual-layer architecture across multiple data centers in a multi-cloud environment to facilitate FL and deliver robust inference services.\nTo the best of our knowledge, NebulaFL is the first JointCloud-based comprehensive FL framework, which at-tempts to achieve effective and efficient privacy-preserving collaborative model training among data centers, including ef-"}, {"title": "III. OUR NEBULAFL APPROACH", "content": "Overview\nFigure 1 presents the framework and workflow of our NebulaFL approach. As shown in Figure 1 (a), NebulaFL involves multiple data centers, including multiple TEE con-tainers and various computing resources. NebulaFL consists of two main processes, i.e., intra-Data Center (DC) asynchronous FL training and inter-DC model rotation, respectively.\nFigure 1 (b) presents the workflow of intra-DC asyn-chronous FL training at data center $i$. As shown in Figure 1 (b), each data center consists of three key components, i.e., model manager, container manager, and resource manager. The model manager maintains a list of planet models for asynchronous FL training and the stellar model for knowledge sharing. When a planet has completed a round of training, the model manager uploads such a planet model by aggregating it with the stellar model. In addition, the model manager periodically sends the master model aggregated by all the planet models to other data centers for knowledge sharing and updates the stellar model using the received model from other data centers. The container manager maintains multi-ple TEE containers and their information, including training logs, running time, and the historical resource configuration of containers. The container manager includes a container selector. When a planet model is ready for local training, the container selector adaptively selects a container for local training according to the container information. The resource manager adaptively assigns suitable hardware resources to each container according to the container information and the training information of the stellar and assigned planet model.\nWhen the training time reaches a threshold, NebulaFL performs the inter-DC model rotation process, during which each data center sends its master model to a specific data center. Note that NebulaFL ensures that different data centers do not send the model to the same data center in one rotation process and each data center sends the model to different data centers in different rotation rounds. After multiple rounds of model rotation, NebulaFL aggregates all the master models to generate a global model for deployment.\nAs shown in Figure 1, the intra-DC FL training and the inter-DC model rotation process are asynchronous. In addition, each planet model is trained asynchronously without waiting for the training of other planet models. The details of the intra-DC FL training are as follows:\n\u2022 Step 1 (Model Assignment): The container manager selects an idle container for model training based on the version (i.e., training times) of the target model and the historical training logs of the container.\n\u2022 Step 2 (Resource Assignment): The resource manager assigns the most cost-effective hardware resource to the container based on the model version and the historical running time of the container."}, {"title": "B. Implementation of NebulaFL", "content": "Algorithm 1 presents the implementation of our NebulaFL approach. We assume that there are $|C|$ data centers involved\nin FL training. For the $i^{th"}, "data center in $C$, $CT_i$ denotes the set of its TEE containers, and there are $K_i$ containers activated for local training at the same time. Line 1 initializes the system time $T_0$. Lines 2-32 present the whole Nebula FL training process, where the \"for\" loop is a parallel loop. Lines 5-30 present the FL training process in each data center. Lines 5-\nTime().\nInitialize /* parallel for*/ |C| do /* initialize */ Ki CTi;\nm1, m2,..., mk to; \u2190[m1, m,..., mk]; \u2190 {1, ..., v c0; \u2190 mo; /* parallel - master model dispatching */ Time() - To < T do Time() - To > c \u00d7 to then mi MasterGen(Si, Vi); \u2190 Avg(Vi); Transfer(mi, vi, (i + c)%|C| + 1); /* parallel - stellar model updating */ Time() - To < T do Buffer() \u2260 NULL then m\u00b2, v IP : GetModel(); | 1, 2, ..., K do Time() - To < T do ResAssign(v, vs, Vi, ct, Ri); mct: LocalTrain(m, res); PlanetUpdate (m\u00b2, m\u00b2, v, v); /*1) ModelAggr(m1,..., mic\u2081);\nImplementation of NebulaFL/*2*//*3*//*4*//*5*//*6*//*7*//*8*//*9*//*10*//*11*//*12*//*13*//*14*//*15*//*16*//*17*//*18*//*19*//*20*//*21*//*22*//*23*//*24*//*25*//*26*//*27*//*28*//*29*//*30*//*31*//*32*/Input Output: mg Algorithm 1: // Algorithm 1: To Algorithm 1: =To; // 1: To Algorithm 1: // 2*//*3*//*4*//*5*//*6*//*7*//*8*//*9*//*10*//*11*//*12*//*13*//*14*//*15*//*16*//*17*//*18*//*19*//*20*//*21*//*22*//*23*//*24*//*25*//*26*//*27*//*28*//*29*//*30*//*31*//*32*//*1*//*2*//*3*//*4*//*5*//*6*//*7*//*8*//*9*//*10*//*11*//*12*//*13*//*14*//*15*//*16*//*17*//*18*//*19*//*20*//*21*//*22*//*23*//*24*//*25*//*26*//*27*//*28*//*29*//*30*//*31*//*32*/Input Algorithm 1: i) T: training time; ii) R: available resource; iii) C: set ofAlgorithm 1: centers; vi) CT: set of containers for data owners; v) tc:Algorithm 1: time interval between model rotationsAlgorithm 1: Output: mg: the global model Algorithm 1: Toime();Algorithm 1: for i 1, 2, ..., for i 1, 2, ..., i 1, 2, ..., i 1, 2, ..., i 1, 2, ..., for i 1, 2, ..., \u2190 1, 2, ..., \u2190 1, 2, ..., 1, 2, ..., \u2190 1, 2, ..., 1, 2, ..., /*i C do /*initializ/*initializ/*initializ/*initializ/*initializ/*initializAlgorithm 1: ith set Algorithm 1: -ith ith Algorithm 1: 1, 2, K do /* parallel for */ 1, 2, K for j 1, 2, K for j 1, 2, K\u2190 1, 2, ..., for j 1, 2, K for j 1, 2, K\u2190 1, 2, ... Algorithm 1: parallelfor Algorithm 1: -// parallel -// for Algorithm 1: /*parallelTime/*parallelTime/*parallelTime/*parallelTime/*parallelTime/*parallel\n    },\n    {", "title\": \"1) Intra-DC Asynchronous Model Training", "content", "To alleviate the low training performance caused by stragglers, NebulaFL adopts an asynchronous training scheme. Inspired by [14], NebulaFL uses multiple intermediate models, named planet models, rather than a global model for local training and generates the master model according to model versions, where a model with a higher version can be assigned a higher aggregation weight.\nPlanet Model Updating (PlanetUpdate(\u00b7)). To achieve knowledge sharing among data centers, NebulaFL periodically requests the master model from other data centers as the stellar model. Different from GitFL, which only uses the master model to update the intermediate models, NebulaFL uses the stellar model to update the planet model. Specifically, we adopt a weighted aggregation strategy to achieve knowledge sharing from the stellar model to the planet model. Since aggregating the straggler model results in performance degradation, we assign a higher aggregation weight to the model with a higher version. The planet model updating operation can be defined as follows:\n$PlanetUpdate(m^s, m_p, v_s, v_p) = \\frac{w_s(t) m^s + d_t m_p}{w_s + d_t}$ (2)\ns.t., $w_s = max(v_s - v_p, .5)$,\nwhere $w_s$ indicates the aggregation weight of the target planet model $m^s$ and $d_t \\in [0.5,1.0]$ is a decaying weight that decreases over time and is reset to 1.0 when model rotation. Note that since NebulaFL updates the stellar model with a"]}, {"title": "2) Inter-DC Model Rotation", "content": "To achieve an efficient model communication mechanism among data centers, NebulaFL adopts the model rotation strategy rather than the traditional model aggregation strategy. Specifically, in each model rota-tion round, each data center only sends its master model to one data center without waiting for the aggregation model. In this way, compared to the traditional aggregation strategy, NebulaFL can reduce 50% of communication overhead in each communication round. To achieve the diversity of knowledge sharing, each data center sends the master model to different data centers in different rotation rounds. Specifically, assume that there are $|C|$ data centers, in the $r^{th}$ rotation rounds, the $i^{th}$ data center sends its master model to the $(i+r)\\%|C|+1^{th}$ data center. Note that when $r\\%|C| - 1 = 0$, each data center directly uses its master model to update the stellar model."}, {"title": "3) Container Selection and Resource Scheduling Strategy", "content": "To achieve efficient and effective FL training, NebulaFL adopts a reward-guided container selection and resource scheduling strategy.\nContainer Selection (CtSel(\u00b7)). In NebulaFL, the goal of container selection is to alleviate stragglers and ensure diversity of training. Inspired by [14], we aim to select the container with fewer training times for the low version model and vice versa. However, due to i) the lack of version information, ii) training logs from other data centers, and iii) the training time of containers is based on the assigned hardware resource, the client selection strategy in [14] can-not be adapted to NebulaFL. To achieve container selection, NebulaFL records the training logs of each container. Since the training performance of each container is more stable compared to edge devices, based on the training logs, each data center can predict the training time of each container. NebulaFL utilizes the historical training time of the container with a specific resource as the performance score of this container. We use $PSc(c_t)$ to denote the performance score of the container $c_t$. Since whether the model version is outdated needs to refer to the versions of other data centers, to guide"}, {"title": "IV. PERFORMANCE EVALUATION", "content": "Experimental Setup\nTo demonstrate the effectiveness of our NebulaFL, we conducted experiments on five baselines using three models on three datasets. All the experiments were conducted on one Ubuntu workstation with 22 vCPU, 90GB memory, and an NVIDIA RTX 4090 GPU.\nSettings of Data Centers. We consider a realistic multi-cloud (i.e., multiple data centers) scenario and have inves-tigated the data transfer bandwidth between different data centers and the heterogeneous resource prices at each center, such as NVIDIA A100, NVIDIA RTX 4090, and 3080. We refer to the real performance and price of various resources of the computing resource platform [37] to set configurations. To more accurately reflect the performance differences between homogeneous resources caused by other factors in real-world scenarios, we simulate the runtime of each resource based on a normal distribution.\nDatasets and models. We compared the performance of all approaches on three well-known datasets, i.e., CIFAR-10 [38], CIFAR-100 [38], and Fashion-MINIST [39]. To evaluate the performance in Non-IID scenarios, we adopt the Dirichlet distribution [40] $Dir(\\alpha)$ to divide the CIFAR-10 and CIFAR-100 datasets. Here, we set $\\alpha$ as to 0.1, 0.5, and 1.0, with a smaller value corresponding to higher data heterogeneity among containers. We did not apply to the Fashion-MINIST dataset, as it inherently exhibits Non-IID characteristics. We selected multiple popular models to verify the effectiveness of the method, including CNN, ResNet-18, and VGG-16.\nBaselines. To comprehensively evaluate our method, we considered the classic FL method FedAvg [18] as well as four state-of-the-art methods, i.e., FedHiSyn [41], FedHKT [42], FedSA [16] and Pisces [43]. The first three are synchronous training frameworks, while the last two are asynchronous frameworks."}, {"title": "Scalability Analysis", "content": "To demonstrate the versatility and scalability of the Nebu-laFL method in different scenarios, we examined the impact of various configurations on NebulaFL from the following three perspectives: the number of data centers, the total number of containers, and the number of active containers.\n1) Impacts of Different Numbers of Data Centers: To verify the effectiveness of the method under different data center configurations, we set up three different configurations, includ-ing the number of data centers and resource configurations. The number of data centers was set to 4, 6, and 8, with resource conditions randomly generated based on real data surveyed from the AutoDL platform. We tested using the ResNet-18 model on the CIFAR-10 dataset, and the results are shown in Figure 3. From this learning curve figure, it"}, {"title": "V. CONCLUSION", "content": "In this paper, we presented NebulaFL, a novel asynchronous Federated Learning framework designed to address the unique challenges of collaborative FL in JointCloud scenarios. Neb-ulaFL effectively mitigates the issues of data heterogeneity, high communication overhead, and resource scheduling in-efficiencies that typically hinder FL performance. Our ap-proach incorporates a version control-based asynchronous training scheme to balance training times, a decentralized model rotation mechanism to facilitate efficient knowledge sharing among data centers and a reward-guided strategy for optimized container selection and resource scheduling. Extensive experimental evaluations demonstrate that NebulaFL not only achieves superior accuracy compared to existing FL methods but also significantly reduces communication costs and training time. In the future, our code will be further open-sourced and applied to real-world platforms, aiming to continually advance federated learning technologies and their practical applications."}]