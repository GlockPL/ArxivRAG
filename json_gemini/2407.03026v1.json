{"title": "Qifusion-Net: Layer-adapted Stream/Non-stream Model for End-to-End Multi-Accent Speech Recognition", "authors": ["Jinming Chen", "Jingyi Fang", "Yuanzhong Zheng", "Yaoxuan Wang", "Haojun Fei"], "abstract": "Currently, end-to-end (E2E) speech recognition methods have achieved promising performance. However, auto speech recognition (ASR) models still face challenges in recognizing multi-accent speech accurately. We propose a layer-adapted fusion (LAF) model, called Qifusion-Net, which does not require any prior knowledge about the target accent. Based on dynamic chunk strategy, our approach enables streaming decoding and can extract frame-level acoustic feature, facilitating fine-grained information fusion. Experiment results demonstrate that our proposed methods outperform the baseline with relative reductions of 22.1% and 17.2% in character error rate (CER) across multi accent test datasets on KeSpeech and MagicData-RMAC.", "sections": [{"title": "1. Introduction", "content": "In recent years, end-to-end (E2E) speech recognition (ASR) has significantly benefited from the high-resource languages and increasing large model size [1, 2]. This improvement has enabled the effective mitigation of recognition degraded caused by various acoustic environments [3]. As a result, E2E ASR systems have found widely-used in commercial speech recognition products [4-7]. However, it is well known that the performance of even large ASR models degrades significantly when the speakers have varying degrees of accent pronunciation [8]. Accent is a special way of pronunciation, which is mainly influenced by regional culture, speaking style and the education level of the speaker [9]. For example, in the remote areas or villages of southern China, those accents are quite different from the pronunciation of Mandarin and seriously affects the recognition accuracy of the E2E ASR model [8]. To some extent, training a specific accent ASR model can solve the problem of recognition accuracy degradation. Achieving high recognition accuracy in multi-accent system without pre-accent category information has significant commercial value for the application of E2E ASR model.\nRecently, adversarial learning [10, 11], transfer learning [12, 13], multi-tasking learning (MTL) [14, 15] and other deep learning methods have been developed greatly to eliminate the recognition bias due to the accent in the ASR task. The primary concept behind adversarial learning and transfer learning methods involves initially training the model on an extensive corpus of speech data, followed by fine-tuning it specifically on the accent dataset [12, 16, 17]. Good performance is often obtained in a single accent system. For multi-accent systems, it is often necessary to introduce additional information to guide for the different accents. The direct way to introduce accent information is to concatenate a one-hot accent vector into the input acoustic features [18]. Others, accent identification (AID) models are used to generate embeddings [19, 20]. For instance, the authors in [21, 22] suggest connecting accent embeddings and acoustic features to adapt the acoustic model. In [23], they utilized well-trained accent classifiers to extract accent embedding for layer-to-layer adaptation of E2E ASR models. A multi-task framework was proposed in [21, 24] to jointly model ASR and AID tasks. All previous researches have significantly enhanced the accuracy of accent speech recognition in specific contexts. However, thus far, there has been a lack of consideration for accent recognition in real-time streaming scenarios and the fusion of fine-grained accent information at the frame level, which holds greater practical applicability and reference significance for MTL.\nIn this paper, we aim at improving the recognition accuracy in a multi-accent system. Three contributions are explored: 1) A new fusion strategy Layer-adapted fuison (LAF) module is proposed to extract accent information in shared acoustic encoder. 2) Fine-grained fusion of accent information at frame-level is achieved through the utilization of a cross-attention module, which effectively eliminates the impact of accent on the acoustic model. 3) Based on the dynamic chunk strategy, the model realizes the unification of streaming and non-streaming decoding modes. The results demonstrate that both stream/non-stream Qifusion-Net achieve the highest accuracy for accented speech datasets. The CER demonstrates the relative decrease of 22.1% and 17.2% in the multi accent test datasets, compared to the baseline in KeSpeech and MagicData-RAMC.\nThe remaining sections of this paper are structured as follows: Section 2 introduces our Layer-adapted fusion module integrated with the MTL E2E ASR system. In Section 3, experimental results are presented and analyzed. Finally, Section 4 provides the conclusion."}, {"title": "2. Layer-adapted for E2E Multi-Accent ASR", "content": "The whole model architecture of the proposed layer-adapted for E2E multi-accent ASR is illustrated in Figure 1. In this section, we first give a brief review of a general acoustic encoder based on the convolution-augmented transformer (Conformer) model in Section 2.1. Then the proposed layer-adapted fusion (LAF) module is introduced in Section 2.2. The LAF module is designed for filtering the fine-grained accent information from the shared acoustic encoder with two different structures. Furthermore, we present the cross-attention module in Section 2.3, which integrates the acoustic and accent information. Finally, the MTL training of our methods and the stream/non-stream decoding modes are described in Section 2.4."}, {"title": "2.1. Conformer-based Acoustic Encoder", "content": "The acoustic encoder structure innovations in this paper are based on the general conformer E2E ASR model. The Conformer model integrates convolution layers into the transformer module to enhance the local modeling capability of signal sequences. Given its exceptional performance in the E2E ASR task, we employ the conformer-based encoder depicted in Figure 2(a) to comprehensively extract frame-level acoustic features from incoming multi-accent speech.\nThe original conformer model is mainly composed of four modules: two feed-forward modules (FFN), a multi-head self-attention module (MHSA) and a convolution module in the middle. When the step length of speech frame is set to 10ms, there will be redundancy in the general acoustic features which will affect the fine-grained accent feature extraction to some extent. Hence a progressive sub-sampling operation is applied in the general acoustic encoder. The feature compression in time dimension will be conducive to the following effective accent features extraction by layer-adapted module in Section 2.2.\nGiven the FBank features Xt of the input multi-accent speech, the frame-level general acoustic features HGA of the conformer block can be mathematically defined as follows:\n$h_t^{FFN1} = X_t + \\frac{FFN(X_t)}{2}$ (1)\n$h_t^{MHSA} = h_t^{FFN1} + MHSA(h_t^{FFN1})$ (2)\n$h_t^{Conv} = h_t^{MHSA} + Conv(h_t^{MHSA})$ (3)\n$h_t^{FFN2} = h_t^{Conv} + \\frac{FFN(h_t^{Conv})}{2}$ (4)\n$H^{GA} = LayerNorm(h_t^{FFN2})$ (5)\nOn the one hand, this design can gradually reduce the dimension of input sequence, so that the global acoustic features can be projected to a wider dimension [25], and on the other hand, it can reduce the computational complexity of streaming mode."}, {"title": "2.2. Layer-adapted Module", "content": "Current solutions for multi-accent ASR task commonly adopt the acoustic features of a universal ASR model as the input to train an accent identifier (AID) model. However, such methods lack information sharing between the two models which leads to both performance degradation in a multi-task learning of ASR and AID. As shown in Figure 2(b), we proposed a layer-adapted module to extract fine-grained accent information from different layers of acoustic encoder while facilitating frame-by-frame correction of ASR results by cross-attention module in Section 2.3."}, {"title": "2.2.1. Adapted Layer", "content": "Numerous studies have indicated that distinct layers of the ASR encoder possess the capability to extract speech information at varying levels. As the depth of the ASR layers increases, a greater abundance of localized information becomes available. In this paper, we use the layer after the progressive sub-sampling operation of acoustic structure as adapted layers. In the training process, we introduce learnable adaptive weights that are multiplied with the adapted layers. Both concatenate and sum operations can be selected as layer-adapted connectivity options (Figure 2(b))."}, {"title": "2.2.2. Accent Identify Decoder", "content": "After extracting the fused accent features from the adapted layers in the acoustic encoder, we propose a two-layer causal convolutional structure and a linear-based discriminator to construct the AID. This module can effectively distill accent information and provides frame-by-frame classification of input multi-accent speech into different accent categories."}, {"title": "2.3. Cross-attention Module", "content": "It is widely acknowledged that the MTL approach can enhance the performance of each task by facilitating the sharing of feature information. As shown in Figure 2(c), we use the output $H^{GA}$ of the general acoustic encoder as the key, and the accent embedding $H^{AC}$ obtained in the layer adapted module as the query to carry out cross-information fusion. The frame-level accent embedding features contributes to eliminate the distortion of acoustic features caused by different degrees of accent pronunciation in a multi-accent speech in order to improve the accuracy of accent ASR.\nThe following shows calculation process of cross-information fusion based on attention mechanism:\n$Q_t = W^Q H^{AC}, K_t = W^K H^{GA}, V_t = W^V H^{GA}$ (6)\n$Q^{Att} = Relu(Softmax(\\frac{Q_t(K_t)^T}{\\sqrt{d_{att}}})V_t)$ (7)\n$O^{Att} = Relu(Softmax(\\frac{Q^{Att}(K_t)^T}{\\sqrt{d_{att}}})V_t)$ (8)\nwhere $W^Q$, $W^K$ and $W^V$ are trainable weight matrix, the division of the similarity matrix and $\\sqrt{d_{att}}$ in (7) and (8) contribute to steady gradient descent while training."}, {"title": "2.4. Multi-task Training and Stream/non-stream Decoding", "content": "During the multi-task accent ASR model training, the overall loss function is designed by combining three losses: connectionist temporal classification (CTC) loss from ASR task, decoder attention loss and the accent identify cross entropy (CE) loss, which can be formulated as:\n$L_{ctc} = CTC(O^{Att}, y_f)$ (10)\n$L_{att} = CE(Decoder(O^{Att}, y_c), y_c)$ (11)\n$L_{aid} = CE(\\hat{y}_t, Y_{ac})$ (12)\nwhere $\\lambda_{ctc}$ and $\\lambda_{aid}$ are two weights of CTC loss and AID loss. $\\hat{y}_t$ and $y_{ac}$ are the predicted and true accent label of the input $X_t$. $Y_c$ and $y_f$ are the transcription labels with coarse-grained and fine-grained units. $CE()$ and $Decoder()$ stands for the cross entropy loss and attention decoder function.\nIn this paper, we use the dynamic chunk masking strategy [6] to ensure compatibility of model inference with both stream and non-stream modes. During the training stage, we initially sample a random chunk size C from a uniform distribution ranging between 1 and the maximum batch length T. Subsequently, the input is divided into multiple chunks based on the selected chunk size. Finally, in training, the current chunk undergoes bidirectional chunk-level attention with itself and previous/following chunks through left-to-right and right-to-left attention decoder respectively."}, {"title": "3. Experiments", "content": "In this study, we conducted extensive experiments on KeSpeech [8], which involved 1,542 hours of speech signals recorded by 27,237 speakers from 34 cities across China. The dataset encompasses standard Mandarin and its eight subdialects in the regions of Zhongyuan, Southwestern, Ji-Lu, Jiang-Huai, Lan-Yin, Jiao-Liao, Northeastern and Beijing. The MagicData-RAMC, which contains 180 hours and 6 diverse domains (Sichuan, Shanxi, Shandong, Jiangsu, Hunan, Guangdong), is also used as performance validation [26]."}, {"title": "3.2. E2E Based Basedline", "content": "For [9, 27] acoustic feature extraction, the 80-dimensional log Mel-filter bank (FBANK) is calculated with window size of 25ms and step size of 10ms. The utterance-level cepstral mean and variance normalization (CMVN) calculated using the training set was applied to FBANK for feature normalization. All our experiments are implemented using Wenet end-to-end speech processing toolkit. SpecAugment [28] is used for data augmentation during training, and no extra language models are applied."}, {"title": "3.3. Layer-adapted Fusion Module", "content": ""}, {"title": "3.3.1. layer-adapted Module", "content": "As show in Figure 1, The Frame-level general acoustic embedding is obtain from the general acoustic encoder ($h_i^{GA}$, i stands for the index of layer). The fusion input ($H_i^{CA}$) is stacked by {$h_6^{GA}...h_{12}^{GA}$}. In our work, we chose the output of L-6th to L-12th after the progressive downsampling operation as the input for our layer-adapted module. Learnable weights W are introduced into each layer for dot multiplication. The input $WX H^{GA}$ is fused through casual conv2d (kernal size 5x5, stride size 1x1). Accent frame-level predict label is calculated through conv1d of kenerl size 3 and stride size 1.\nFor AID task, we do some experiments to prove that the different layers of general acoustic encoder contain different-level accent information. We employ a pretrained acoustic encoder, freeze the encoder weights and utilize various layers as inputs to the AID model."}, {"title": "3.3.2. Cross-attention Module", "content": "Before the linear classification layer in AID task, we obtain the frame-level accent embedding ($H^{AC}$). The ($H^{AC}$) and the last-layer of general acoustic encoder ($h_i^{GA}$) have the same temporal resolution and feature dimension 256. $Q^{Att} = CrossAtt(H^{AC}, h_i^{GA})$. $Q^{Att}$ is used as the attention decoder input with the accent bias eliminated. The attention decoder uses a bi-directional 3-layer transformer structure with a multi-head of 4.\nFor the AID-ASR MTL system, we do ablation experiments to discuss the impact of different modules on the overall CER. Without AID task, the overall CER reached 15.35 (A1). Added AID task, but without cross-attention module, CER increased to 9.6 (A3). Joint training has a significant positive effect on CER of ASR task. For accent information, cross-attention (Q2) achieves an absolute CER improvement of 0.17 compared to self-attention module (A4). It shows that the addition of cross-attention module can effectively eliminate the degradation of recognition caused by accent in ASR task."}, {"title": "3.3.3. Stream/non-stream Decoding", "content": "Based on the dynamic chunk masking strategy, the model supports steaming decoding mode. Compared with baseline and the sota model DIMNet, without LM, the CER of non-stream model (Qifusion-Net-ns Q2) is 10% higher than DIMNet without LM (C5) and even exceeds the DIMNet with LM (C6). The stream decoding mode (Qifusion-Net-s Q1) also outperforms C5 in terms of CER, reaching 8.9. This enables Qifusion-Net-s to match the recognition accuracy of sota model in streaming multi accent system scenarios, which has great potential in practical applications."}, {"title": "4. Conclusion", "content": "In this study, we explore an end-to-end asr decoding framework in multi-accent systems without prior accent information. Based on the standard conformer ASR architecture, we propose a Qifusion-Net, AID-ASR multi-task learning method based on shared progressive sub-sampling conformer encoder and layer-adapted fusion. We prove that layer-adapted improves AID task, while cross-fusion is beneficial for ASR tasks. The proposed method has lower CER than baseline and sota results in multi accent datasets."}]}