{"title": "Qifusion-Net: Layer-adapted Stream/Non-stream Model for End-to-End\nMulti-Accent Speech Recognition", "authors": ["Jinming Chen", "Jingyi Fang", "Yuanzhong Zheng", "Yaoxuan Wang", "Haojun Fei"], "abstract": "Currently, end-to-end (E2E) speech recognition methods\nhave achieved promising performance. However, auto speech\nrecognition (ASR) models still face challenges in recognizing\nmulti-accent speech accurately. We propose a layer-adapted fu-\nsion (LAF) model, called Qifusion-Net, which does not require\nany prior knowledge about the target accent. Based on dynamic\nchunk strategy, our approach enables streaming decoding and\ncan extract frame-level acoustic feature, facilitating fine-grained\ninformation fusion. Experiment results demonstrate that our\nproposed methods outperform the baseline with relative reduc-\ntions of 22.1% and 17.2% in character error rate (CER) across\nmulti accent test datasets on KeSpeech and MagicData-RMAC.\nIndex Terms: multi-accent speech recognition, layer-adapted\nfusion, stream/non-stream decoding, cross-attention", "sections": [{"title": "1. Introduction", "content": "In recent years, end-to-end (E2E) speech recognition (ASR) has\nsignificantly benefited from the high-resource languages and in-\ncreasing large model size [1, 2]. This improvement has en-\nabled the effective mitigation of recognition degraded caused\nby various acoustic environments [3]. As a result, E2E ASR\nsystems have found widely-used in commercial speech recog-\nnition products [4-7]. However, it is well known that the perfor-\nmance of even large ASR models degrades significantly when\nthe speakers have varying degrees of accent pronunciation [8].\nAccent is a special way of pronunciation, which is mainly in-\nfluenced by regional culture, speaking style and the education\nlevel of the speaker [9]. For example, in the remote areas or vil-\nlages of southern China, those accents are quite different from\nthe pronunciation of Mandarin and seriously affects the recog-\nnition accuracy of the E2E ASR model [8]. To some extent,\ntraining a specific accent ASR model can solve the problem of\nrecognition accuracy degradation. Achieving high recognition\naccuracy in multi-accent system without pre-accent category in-\nformation has significant commercial value for the application\nof E2E ASR model.\nRecently, adversarial learning [10, 11], transfer learning\n[12, 13], multi-tasking learning (MTL) [14, 15] and other deep\nlearning methods have been developed greatly to eliminate the\nrecognition bias due to the accent in the ASR task. The primary\nconcept behind adversarial learning and transfer learning meth-\nods involves initially training the model on an extensive corpus\nof speech data, followed by fine-tuning it specifically on the ac-\ncent dataset [12, 16, 17]. Good performance is often obtained\nin a single accent system. For multi-accent systems, it is of-\nten necessary to introduce additional information to guide for\nthe different accents. The direct way to introduce accent infor-\nmation is to concatenate a one-hot accent vector into the input\nacoustic features [18]. Others, accent identification (AID) mod-\nels are used to generate embeddings [19, 20]. For instance, the\nauthors in [21, 22] suggest connecting accent embeddings and\nacoustic features to adapt the acoustic model. In [23], they uti-\nlized well-trained accent classifiers to extract accent embedding\nfor layer-to-layer adaptation of E2E ASR models. A multi-task\nframework was proposed in [21, 24] to jointly model ASR and\nAID tasks. All previous researches have significantly enhanced\nthe accuracy of accent speech recognition in specific contexts.\nHowever, thus far, there has been a lack of consideration for\naccent recognition in real-time streaming scenarios and the fu-\nsion of fine-grained accent information at the frame level, which\nholds greater practical applicability and reference significance\nfor MTL.\nIn this paper, we aim at improving the recognition accuracy\nin a multi-accent system. Three contributions are explored: 1)\nA new fusion strategy Layer-adapted fuison (LAF) module is\nproposed to extract accent information in shared acoustic en-\ncoder. 2) Fine-grained fusion of accent information at frame-\nlevel is achieved through the utilization of a cross-attention\nmodule, which effectively eliminates the impact of accent on\nthe acoustic model. 3) Based on the dynamic chunk strategy, the\nmodel realizes the unification of streaming and non-streaming\ndecoding modes. The results demonstrate that both stream/non-\nstream Qifusion-Net achieve the highest accuracy for accented\nspeech datasets. The CER demonstrates the relative decrease of\n22.1% and 17.2% in the multi accent test datasets, compared to\nthe baseline in KeSpeech and MagicData-RAMC.\nThe remaining sections of this paper are structured as fol-\nlows: Section 2 introduces our Layer-adapted fusion module\nintegrated with the MTL E2E ASR system. In Section 3, exper-\nimental results are presented and analyzed. Finally, Section 4\nprovides the conclusion."}, {"title": "2. Layer-adapted for E2E Multi-Accent\nASR", "content": "The whole model architecture of the proposed layer-adapted for\nE2E multi-accent ASR is illustrated in Figure 1. In this sec-\ntion, we first give a brief review of a general acoustic encoder\nbased on the convolution-augmented transformer (Conformer)\nmodel in Section 2.1. Then the proposed layer-adapted fusion\n(LAF) module is introduced in Section 2.2. The LAF module is\ndesigned for filtering the fine-grained accent information from\nthe shared acoustic encoder with two different structures. Fur-\nthermore, we present the cross-attention module in Section 2.3,\nwhich integrates the acoustic and accent information. Finally,\nthe MTL training of our methods and the stream/non-stream de-\ncoding modes are described in Section 2.4."}, {"title": "2.1. Conformer-based Acoustic Encoder", "content": "The acoustic encoder structure innovations in this paper are\nbased on the general conformer E2E ASR model. The Con-\nformer model integrates convolution layers into the transformer\nmodule to enhance the local modeling capability of signal se-\nquences. Given its exceptional performance in the E2E ASR\ntask, we employ the conformer-based encoder depicted in Fig-\nure 2(a) to comprehensively extract frame-level acoustic fea-\ntures from incoming multi-accent speech.\nThe original conformer model is mainly composed of four\nmodules: two feed-forward modules (FFN), a multi-head self-\nattention module (MHSA) and a convolution module in the mid-\ndle. When the step length of speech frame is set to 10ms, there\nwill be redundancy in the general acoustic features which will\naffect the fine-grained accent feature extraction to some extent.\nHence a progressive sub-sampling operation is applied in the\ngeneral acoustic encoder. The feature compression in time di-\nmension will be conducive to the following effective accent fea-\ntures extraction by layer-adapted module in Section 2.2.\nGiven the FBank features $X_t$ of the input multi-accent\nspeech, the frame-level general acoustic features $H^{GA}$ of the\nconformer block can be mathematically defined as follows:\n$h_t^{FFN1} = X_t + \\frac{FFN(X_t)}{2}$ (1)\n$h_t^{MHSA} = h_t^{FFN1} + MHSA(h_t^{FFN1})$ (2)\n$h_t^{Conv} = h_t^{MHSA} + Conv(h_t^{MHSA})$ (3)\n$h_t^{FFN2} = h_t^{Conv} + \\frac{FFN(h_t^{Conv})}{2}$ (4)\n$H^{GA} = LayerNorm(h_t^{FFN2})$ (5)\nOn the one hand, this design can gradually reduce the di-\nmension of input sequence, so that the global acoustic features\ncan be projected to a wider dimension [25], and on the other\nhand, it can reduce the computational complexity of streaming\nmode."}, {"title": "2.2. Layer-adapted Module", "content": "Current solutions for multi-accent ASR task commonly adopt\nthe acoustic features of a universal ASR model as the input to\ntrain an accent identifier (AID) model. However, such methods\nlack information sharing between the two models which leads to\nboth performance degradation in a multi-task learning of ASR\nand AID. As shown in Figure 2(b), we proposed a layer-adapted\nmodule to extract fine-grained accent information from differ-\nent layers of acoustic encoder while facilitating frame-by-frame\ncorrection of ASR results by cross-attention module in Section\n2.3."}, {"title": "2.2.1. Adapted Layer", "content": "Numerous studies have indicated that distinct layers of the ASR\nencoder possess the capability to extract speech information\nat varying levels. As the depth of the ASR layers increases,\na greater abundance of localized information becomes avail-\nable. In this paper, we use the layer after the progressive sub-\nsampling operation of acoustic structure as adapted layers. In\nthe training process, we introduce learnable adaptive weights\nthat are multiplied with the adapted layers. Both concatenate\nand sum operations can be selected as layer-adapted connectiv-\nity options (Figure 2(b))."}, {"title": "2.2.2. Accent Identify Decoder", "content": "After extracting the fused accent features from the adapted lay-\ners in the acoustic encoder, we propose a two-layer causal con-\nvolutional structure and a linear-based discriminator to con-\nstruct the AID. This module can effectively distill accent in-\nformation and provides frame-by-frame classification of input\nmulti-accent speech into different accent categories."}, {"title": "2.3. Cross-attention Module", "content": "It is widely acknowledged that the MTL approach can enhance\nthe performance of each task by facilitating the sharing of fea-\nture information. As shown in Figure 2(c), we use the output\n$H^{GA}$ of the general acoustic encoder as the key, and the accent\nembedding $H^{AC}$ obtained in the layer adapted module as the\nquery to carry out cross-information fusion. The frame-level\naccent embedding features contributes to eliminate the distor-\ntion of acoustic features caused by different degrees of accent\npronunciation in a multi-accent speech in order to improve the\naccuracy of accent ASR.\nThe following shows calculation process of cross-\ninformation fusion based on attention mechanism:\n$Q_t = W_Q H^{AC}, K_t = W_K H^{GA}, V_t = W_V H^{GA}$ (6)\n$Q^{Att} = Relu(Softmax(\\frac{Q_t (K_t)^T}{\\sqrt{d_{att}}}) V_t)$ (7)\n$O^{Att} = Relu(Softmax(\\frac{Q^{Att} (K_t)^T}{\\sqrt{d_{att}}}) V_t)$ (8)\nwhere $W_Q$, $W_K$ and $W_V$ are trainable weight matrix, the divi-\nsion of the similarity matrix and $\\sqrt{d_{att}}$ in (7) and (8) contribute\nto steady gradient descent while training."}, {"title": "2.4. Multi-task Training and Stream/non-stream Decoding", "content": "During the multi-task accent ASR model training, the overall\nloss function is designed by combining three losses: connec-\ntionist temporal classification (CTC) loss from ASR task, de-\ncoder attention loss and the accent identify cross entropy (CE)\nloss, which can be formulated as:\n$L_{all} = L_{att} + A_{ctc} L_{ctc} + A_{aid} L_{aid}$ (9)\n$L_{ctc} = CTC(O^{Att}, y_f)$ (10)\n$L_{att} = CE(Decoder(O^{Att}, y_c), y_c)$ (11)\n$L_{aid} = CE(\\hat{y}_t, Y_{ac})$ (12)\nwhere $A_{ctc}$ and $A_{aid}$ are two weights of CTC loss and AID loss.\n$\\hat{y}_t$ and $y_{ac}$ are the predicted and true accent label of the input\n$X_t$. $Y_c$ and $y_f$ are the transcription labels with coarse-grained\nand fine-grained units. $CE()$ and $Decoder()$ stands for the\ncross entropy loss and attention decoder function.\nIn this paper, we use the dynamic chunk masking strat-\negy [6] to ensure compatibility of model inference with both\nstream and non-stream modes. During the training stage, we\ninitially sample a random chunk size $C$ from a uniform distribu-\ntion ranging between 1 and the maximum batch length $T$. Sub-\nsequently, the input is divided into multiple chunks based on the\nselected chunk size. Finally, in training, the current chunk un-\ndergoes bidirectional chunk-level attention with itself and pre-\nvious/following chunks through left-to-right and right-to-left at-\ntention decoder respectively."}, {"title": "3. Experiments", "content": "In this study, we conducted extensive experiments on KeSpeech\n[8], which involved 1,542 hours of speech signals recorded by\n27,237 speakers from 34 cities across China. The dataset en-\ncompasses standard Mandarin and its eight subdialects in the re-\ngions of Zhongyuan, Southwestern, Ji-Lu, Jiang-Huai, Lan-Yin,\nJiao-Liao, Northeastern and Beijing. The MagicData-RAMC,\nwhich contains 180 hours and 6 diverse domains (Sichuan,\nShanxi, Shandong, Jiangsu, Hunan, Guangdong), is also used\nas performance validation [26]."}, {"title": "3.2. E2E Based Basedline", "content": "For [9, 27] acoustic feature extraction, the 80-dimensional log\nMel-filter bank (FBANK) is calculated with window size of\n25ms and step size of 10ms. The utterance-level cepstral\nmean and variance normalization (CMVN) calculated using the\ntraining set was applied to FBANK for feature normalization.\nAll our experiments are implemented using Wenet end-to-end\nspeech processing toolkit. SpecAugment [28] is used for data\naugmentation during training, and no extra language models are\napplied."}, {"title": "3.3. Layer-adapted Fusion Module", "content": "As show in Figure 1, The Frame-level general acoustic embed-\nding is obtain from the general acoustic encoder ($h_i^{GA}$, i stands\nfor the index of layer). The fusion input ($H^{CA}$) is stacked by\n{$h^{GA}...h^{GA}$}. In our work, we chose the output of L-6th to\nL-12th after the progressive downsampling operation as the in-\nput for our layer-adapted module. Learnable weights $W$ are\nintroduced into each layer for dot multiplication. The input\n$W \\cdot H^{GA}$ is fused through casual conv2d (kernal size 5x5,\nstride size 1x1). Accent frame-level predict label is calculated\nthrough conv1d of kenerl size 3 and stride size 1."}, {"title": "3.3.2. Cross-attention Module", "content": "Before the linear classification layer in AID task, we ob-\ntain the frame-level accent embedding ($H^{AC}$). The ($H^{AC}$)\nand the last-layer of general acoustic encoder ($h_2^{GA}$) have the\nsame temporal resolution and feature dimension 256. $Q^{Att} =$\n$CrossAtt(H^{AC}, h_2^{GA})$. $Q^{Att}$ is used as the attention decoder\ninput with the accent bias eliminated. The attention decoder\nuses a bi-directional 3-layer transformer structure with a multi-\nhead of 4.\nFor the AID-ASR MTL system, we do ablation experiments\nto discuss the impact of different modules on the overall CER.\nWithout AID task, the overall CER reached 15.35 (A1). Added\nAID task, but without cross-attention module, CER increased\nto 9.6 (A3). Joint training has a significant positive effect on\nCER of ASR task. For accent information, cross-attention (Q2)\nachieves an absolute CER improvement of 0.17 compared to\nself-attention module (A4). It shows that the addition of cross-\nattention module can effectively eliminate the degradation of\nrecognition caused by accent in ASR task."}, {"title": "3.3.3. Stream/non-stream Decoding", "content": "Based on the dynamic chunk masking strategy, the model sup-\nports steaming decoding mode. Compared with baseline and\nthe sota model DIMNet, without LM, the CER of non-stream\nmodel (Qifusion-Net-ns Q2) is 10% higher than DIMNet with-\nout LM (C5) and even exceeds the DIMNet with LM (C6). The\nstream decoding mode (Qifusion-Net-s Q1) also outperforms\nC5 in terms of CER, reaching 8.9. This enables Qifusion-Net-\ns to match the recognition accuracy of sota model in streaming\nmulti accent system scenarios, which has great potential in prac-"}, {"title": "4. Conclusion", "content": "In this study, we explore an end-to-end asr decoding frame-\nwork in multi-accent systems without prior accent information.\nBased on the standard conformer ASR architecture, we propose\na Qifusion-Net, AID-ASR multi-task learning method based on\nshared progressive sub-sampling conformer encoder and layer-\nadapted fusion. We prove that layer-adapted improves AID task,\nwhile cross-fusion is beneficial for ASR tasks. The proposed\nmethod has lower CER than baseline and sota results in multi\naccent datasets."}]}