{"title": "Is logical analysis performed by transformers taking place in self-attention or in the fully connnected part?", "authors": ["Evgeniy Shin", "Heinrich Matzinger"], "abstract": "Transformers architecture apply self-attention to tokens represented as vectors, before a fully connected (neuronal network) layer. These two parts can be layered many times. Traditionally, self-attention is seen as a mechanism for aggregating information before logical operations are performed by the fully connected layer. In this paper, we show, that quite counter-intuitively, the logical analysis can also be performed within the self-attention. For this we implement a handcrafted single-level encoder layer which performs the logical analysis within self-attention. We then study the scenario in which a one-level transformer model undergoes self-learning using gradient descent. We investigate whether the model utilizes fully connected layers or self-attention mechanisms for logical analysis when it has the choice. Given that gradient descent can become stuck at undesired zeros, we explicitly calculate these unwanted zeros and find ways to avoid them. We do all this in the context of predicting grammatical category pairs of adjacent tokens in a text. We believe that our findings have broader implications for understanding the potential logical operations performed by self-attention.", "sections": [{"title": "1 Introduction", "content": "Is the logical analysis in transformers done in the self-attention part or the fully connected layers? This is the question we analyze in the current paper, specifically for the problem of predicting grammatical category pairs (or functionals there of), for adjacent tokens in the text. These category pairs contain a large part of the information from the grammatical parsing tree of each sentence. We believe that our analysis has more general meaning, then just category pairs.\nBefore anything else, let us present a small example of a fully connected layer modeling a simple logic:\nTake $X_1, X_2, ...$ i.i.d Bernoulli variables with\n$P(X_i = 1) = 1 - P(X_i = 0) = p$.\nNow, consider the simple logic:\n$(\\{X_1 = 1\\} \\cap \\{X_2 = 1\\}) \\cup (\\{X_3 = 1\\} \\cap \\{X_4 = 1\\}) \\cup (\\{X_5 = 1\\} \\cap \\{X_6 = 1\\}) \\implies Y = 1$ else $Y = 0$. Let $X$ be the binary vector:\n$X = (X_1, X_2, X_3, X_4, X_5, X_6)$\nWe can then represent the simple logic 1.1, by a fully connected neuronal network layer:\n$Y = h(C(ReLU(BX, bias = -1))))$,\nwhere\n$B = \\begin{pmatrix}\n1 & 1 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & 1 & 1 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 1 & 1\n\\end{pmatrix}$\nand $C = (1, 1, 1)$, where $h$ is the activation function with $h(x) = 1$ if $x > 1$ and $h(x) = 0$ otherwise.\nNow, that we have seen a first example of fully connected layer reproducing a simple logic, let us see an example which also involves self-attention. More specially, our next example is a variation of the next word prediction task:"}, {"title": "2 Three different hand programmed solutions", "content": "In this section, we present our two hand programmed one-level transformers in the Subsections 2.1 and 2.2. Here, we first start by explaining, how the formula given for transformers"}, {"title": "2.1 First approach: bringing the information together with self-attention", "content": "Assume for example four positions. We are going to use the matrix\n$D_{-1} = \\begin{pmatrix}\n0 & 1 & 0 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 1 \\\\\n0 & 0 & 0 & 0\n\\end{pmatrix}$\n(So, in general if $M$ is the number of categories in the text, the matrix $D_{-1}$ will be of dimension $M \\times M$ and $D_{ij} = 1$ if $i = j - 1$ and $D_{ij} = 0$ otherwise.) If we multiply with a one-hot encoded position column vector from right and a positional row-vector from left, then we get 1 if and only if the row vector encodes a position which is one unit to the left of the position encoded in the column vector. Otherwise the product:\n$position^T D_{-1} position;$\nis zero. We take the matrix $k^Tq$ to be the block matrix:\n$k^Tq = \\begin{pmatrix}\n0 & 0 \\\\\n0 & 2D_{-1}\n\\end{pmatrix}$\nFor the last equality above to hold, we can take $q$ to be equal to:\n$q = (0|2D_{-1})$.\nand $k$ to be in in block notation:\n$k := ( 0| I_M)$,\nwhere $I_M$ represents the $M \\times M$ identity matrix. Then, for $v$ we take the matrix which projects orthogonaly onto the space of categories, hence gets rid of the positional encoding:\n$\\upsilon = \\begin{pmatrix}\nI_N & 0 \\\\\n0 & 0\n\\end{pmatrix}$\nwhere $I_N$ is the $N \\times N$ identity matrix and $v$ is a $(M + N) \\times (M + N)$ matrix. In this manner $ATTENTION_*(Q, K, V)^t$ is the matrix obtained from $X$ by replacing the i-th column $X_i$ by $2\\cdot X_{i-1}$ and then deleting the positional encoding. (Here we use the self-attention without Softmax). We can then apply the Skip-connection which will add $X_i$ to the i-th column of the Attention. In this manner, in each column of the thus obtained matrix, we have the sum of the current category of the token number $i$ plus twice the 0,1-encoded category of token $i - 1$. To retrieve the 0,1-hot-encoded category pair, we apply the matrix $B$ followed by Relu, where $B$ is:\n$B = \\begin{pmatrix}\n3 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\ \n0 & 3 & 0 & 0 & 0 & 0 & 0 & 0\\\\ \n0 & 0 & 3 & 0 & 0 & 0 & 0 & 0\\\\ \n0 & 0 & 0 & 3 & 0 & 0 & 0 & 0\\\\ \n2 & 1 & 0 & 0 & 0 & 0 & 0 & 0\\\\ \n2 & 0 & 1 & 0 & 0 & 0 & 0 & 0\\\\ \n2 & 0 & 0 & 1 & 0 & 0 & 0 & 0\\\\ \n1 & 2 & 0 & 0 & 0 & 0 & 0 & 0\\\\ \n0 & 2 & 1 & 0 & 0 & 0 & 0 & 0\\\\ \n0 & 2 & 0 & 1 & 0 & 0 & 0 & 0\\\\ \n1 & 0 & 2 & 0 & 0 & 0 & 0 & 0\\\\ \n0 & 1 & 2 & 0 & 0 & 0 & 0 & 0\\\\ \n0 & 0 & 2 & 1 & 0 & 0 & 0 & 0\\\\ \n1 & 0 & 0 & 2 & 0 & 0 & 0 & 0\\\\ \n1 & 0 & 0 & 2 & 0 & 0 & 0 & 0\\\\ \n1 & 0 & 0 & 2 & 0 & 0 & 0 & 0\n\\end{pmatrix} , bias = \\begin{pmatrix}\n-8\\\\ \n-8\\\\ \n-8\\\\ \n-8\\\\ \n-4\\\\ \n-4\\\\ \n-4\\\\ \n-4\\\\ \n-4\\\\ \n-4\\\\ \n-4\\\\ \n-4\\\\ \n-4\\\\ \n-4\\\\ \n-4\\\\ \n-4\n\\end{pmatrix}$\nThe matrix $B$ has in the end, M columns of 0's, so that positional encoding does not affect the outcome. This example is for the special case of $M = 4$. In general, the dimension of $B$ will be $N^2 \\times (N + M))$\nThe above defined one level architecture, is able to reconstruct the category pairs, (or functionals there of) of adjacent tokens. For this let $q^{true}_{.(.,.)} : \\{1,2,..., N\\} \\times \\{1, 2, ..., N\\} \\to R$ and put\n$Y_i = q^{true}(X_{i-1},X_i)$,\nfor all $i = 2, 3, ..., M$. so that $Y = (0, Y_2, Y_3, . . ., Y_M)$. Let us define the matrix $C$ with one row of length $N^2$:\n$C:= (q^{true}_{(1,1)}, q^{true}_{(2,2)},  q^{true}_{(3,3)}, q^{true}_{(4,4)}, q^{true}_{(1,2)}, q^{true}_{(1,3)}, q^{true}_{(1,4)}, q^{true}_{(2,1)}, q^{true}_{(2,3)}, q^{true}_{(2,4)},\u2026\u2026, q^{true}_{(3,1)}, q^{true}_{(3,2)}, q^{true}_{(4,1)}, q^{true}_{(4,2)}, q^{true}_{(4,3)})$.\nThen, we have that $\\vec Y$ is equal to:\n$\\vec Y = C(ReLU(B [ATTENTION_*(Q, K, V)^t + X] + bias))$,\nwhere $q, k, v, B$ are defined by 2.7, 2.8,2.9 and 2.10. In other words, the one level transformer described by us here is able to \"reconstruct\" $\\vec Y$! When $Y_i = q^{true} (X_{i-1}, X_i) = X_{i-1}X_i$, then we have that $Y$ is just a linear function of $X_i$ and $X_{i-1}$. Indeed in that case, we have $Y_i = 10\\cdot X_{i-1} + X_i$. For linear map, no Relu and fully connected layer is needed! Indeed, you can just replace the 2 in the formula for $q$ by 10, so that:\n$q = ( 0| 10D_{-1})$."}, {"title": "2.2 Second approach: using self-attention for identifying category pairs", "content": "In this current approach, the self attention first gets all the category pairs stored in the positional encoding part of the space. And this not just for nearest neighbors. It then, retrieves the nearest neighbor pairs, by applying a linear map, followed by skip with Relu which allows to extract the diagonal. Let us see how this works: Again $N$ is the number of categories. For our explanation below assume $M = 4$, hence four categories to simplify notation. Consider the matrix $A_2$ defined by\n$A_2 = \\begin{pmatrix}\n11 & 12 & 13 & 14 \\\\\n21 & 22 & 23 & 24 \\\\\n31 & 32 & 33 & 34 \\\\\n41 & 42 & 43 & 44\n\\end{pmatrix}$\nThe matrix above is for when we want to retrieve the category pairs. If we want to retrieve a functional of the category pair, then we need the following matrix instead:\n$A_2 = \\begin{pmatrix}\nq^{true}_{(1,1)} & q^{true}_{(1,2)} & q^{true}_{(1,3)} & q^{true}_{(1,4)} \\\\\nq^{true}_{(2,1)} & q^{true}_{(2,2)} & q^{true}_{(2,3)} & q^{true}_{(2,4)} \\\\\nq^{true}_{(3,1)} & q^{true}_{(3,2)} & q^{true}_{(3,3)} & q^{true}_{(3,4)} \\\\\nq^{true}_{(4,1)} & q^{true}_{(4,2)} & q^{true}_{(4,3)} & q^{true}_{(4,4)}\n\\end{pmatrix}$\nwhere $q^{true}(.,.) : \\{1, 2, ..., N\\} \\times \\{1,2,..., N\\} \\to R$ is that functional. The matrix $A_2$, will basically be used to define $k^Tq$ in the self-attention, see 2.17 below.\nFor now, note that when we multiply matrix 2.15 on right and left by 01-hot encoded category, we get the two digit number corresponding to the category pair! Similarly, when multiply the matrix 2.16 on the right and the left by 01-hot encoded category, we get the functional value according to $q^{ture}$ of the category pair!f Let us see an example. First take the row vector representing the first category (1, 0, 0, 0, 0) and then a category 3 hot encode column vector: (0,0,1,0) Multiplying matrix $A_2$ with these two vectors on both sides we find\n$\\begin{pmatrix}\n1 \\\\\n0 \\\\\n0 \\\\\n0 \\\\\n\\end{pmatrix} = 13$.\nHence, the output which is the number 13 tells us that the vector on the left is category 1 and the vector on the right is category 3. (If we would use the matrix 2.16 instead, we would find $q^{true}_{(1,3)}$ instead). Now, we want to use the matrix $A_2$ for the bilinear product in self attention, but having it act only on the category part of the column vectors of $X$ and not on positional encoding. In this manner, that bilinear product, will tell us the category pair of the vectors of which we calculate the product. The position vector gets multiplied by a 0 matrix. Hence, we define in block notation:\n$q = ( A_2 | 0 ) $."}, {"title": "2.3 Third approach", "content": "We believe that most humans when asked to design a one level transformer, which would have to produce $Y = q^{ture}_{Xi-1,X_i}$, would most likely come up with the first solution we have presented in 2.1. That solution has a practical flaw: the matrix $B$ in the fully connected layer would need too many lines: Indeed it requires $N^2$ rows, where $N$ is the number of categories. I in real life transformers, most matrices are square). Now, in the solution presented here,we are going to leave out the matrix $B$ from first solution, but instead incorporate $A_2$ into $v$. Let us see the details:\nAgain we assume for the example we give that $N = 4$ and we work with the matrix\n$A_2 = \\begin{pmatrix}\nq^{true}_{(1,1)} & q^{true}_{(1,2)} & q^{true}_{(1,3)} & q^{true}_{(1,4)} \\\\\nq^{true}_{(2,1)} & q^{true}_{(2,2)} & q^{true}_{(2,3)} & q^{true}_{(2,4)} \\\\\nq^{true}_{(3,1)} & q^{true}_{(3,2)} & q^{true}_{(3,3)} & q^{true}_{(3,4)} \\\\\nq^{true}_{(4,1)} & q^{true}_{(4,2)} & q^{true}_{(4,3)} & q^{true}_{(4,4)}\n\\end{pmatrix}$\nThe matrix $A_2$, was used in our second solution in Subsection 2.2 to define $k^Tq$ in the self-attention.\nRemember, we need to obtain a bilinear product of a hot encoded category vectors according to $A_2$.\nLet us give an example again: say a vector representing the first category (1,0,0,0,0) and then a category 3 hot encode column vector: (0,0,1,0) Multiplying matrix $A_2$ with these two vectors on both sides we find\n$\\begin{pmatrix}\n1 \\\\\n0 \\\\\n0 \\\\\n0\n\\end{pmatrix} . A_2 .  9^{true}_{(1,3)}$.\nLet us use the following notation $X_c$ represents the $N \\times M$ matrix which constitutes the upper $N$ lines of the matrix $X$, hence the 0, 1-hot encoded categories. Let $X_p$ represent the low part of the matrix $X$ which represents the hot-encoded poositions. Note that $X_p$ is a $M \\times M$ identity matrix. Let us give a quick example:\nLet us give a numeric example of $X, X_c$ and $X_p$:\nNow let the original text be\nX = (X_1, X_2, X_3, X_4) = (1, 3, 2, 2)"}, {"title": "2.3.1 Equivalence between third and second solution", "content": "Our second and third solutions are closely related: we are going to show, that to a large extend, they amount to exactly to the same! Now, in the second solution presented in Subsection 2.2, the matrix relating to $q^{true}_{(., .)}$, that is 2.22, is used for the bilinear product in the self-attention. Here we consider self-attention without Softmax. The matrix $k^Tq$ which we use for a binlinear products between columns of $X$, contain the matrix $A_2$ in the second solution in 2.2. These bilinear products are then used as weights to add up columns of $X$ in a weighted average defined in 2.4.\nThe first solution is very different: it uses self-attention to bring the $(i - 1)$-column in place of the i-th column of $X$. Then, it adds the i-th column again by using the skip-connection. The outpout of this is then analyzed by a fully connected layer, so, the bilinear functional value of the category pairs of adjacent tokens (accroding to $A_2$) is found out by the fully connected layer, more specifically using the matrix $B$ followed by ReLU (see 2.10). The linear map $v$ out of self-attention and the subsequent $B$ of the fully connected might be combined into one matrix $B\\cdot v$. So, in our third approach, presented in Subection 2.3, we could leave out matrix $B$, instead packing $v$, the out-matrix of self-attention with $A_2$. Also, we could have done that using a matrix $B$ and loading $A_2$ into $B$, so into the fully connected layer. We are going to show here, that this third approach amounts to the same as the second approach, which we find a quite astonishing fact: The third approach uses self-attention to rewrite the $i \u2014 1$-th column of $X$ in position number i! The second approach instead computes in self-attention all the products according to $A_2$ of the different columns of $X_c$ and gets as output the matrix 2.19. this matrix is totaly different from rewriting the $(i - 1)$-th column of $X$ into position i, as is done in the third approach!\nConsider self-attention without Softmax given by formula 2.2, so that:\n$ATTENTION_*(Q, K, V)^t = v \\cdot X \\cdot KQ^t = v \\cdot X \\cdot X^T \\cdot k^Tq \\cdot X$.\nAgain, we decompose the matrix $X$ into the upper part which is the 0, 1-hot encoding of the categories, and the lower part which represents the positional encoding so that\n$X = \\begin{pmatrix}\nX_c \\\\\nX_p\n\\end{pmatrix}$\nwhere $X_p$ is the $M \\times M$ identity. Let us compare the two solutions:\nUsing self-attention to obtain the bilinear products:\nFrom Subsection 2.2, we have:\n$\\upsilon = \\begin{pmatrix}\nA_2 & 0 \\\\\n0 & 0\n\\end{pmatrix}$"}, {"title": "3 Zeros for gradient descent", "content": "Deep neural network get stuck in suboptimal local minimae. In the current section, we determine the undesirable places of vanishing gradient for our one level transformers. This allows, us in our later study to avoid them. We do it for the case when self-attention acts only on categories, as well as for the other case, when self-attention brings together adjacent columns of $X$. It turns out that in our two cases, the unwanted vanishing gradient place, are only degenerate points, like having the matrices $q$ and $v$ equal to zero. Thus they can be easily be avoided by using tools like Softmax."}, {"title": "3.1 The case where self-attention acts only on the positional encoding", "content": "We first calculate this case without Softmax: Assume, that the vector $Y$ has as entry number i denoted by $y_i$, the state $X_{i-1}$ hot-encoded. This is a little bit simpler than what we had so far where $y_i$ would contain the hot encoded $X_i$ and $X_{i-1}$ at the same time. But in terms of the gradient descent getting stuck problem, the calculations are about the same. By taking $y_i$ to be only the state $X_{i-1}$ hot encoded, notation becomes easier. So, we replace in our matrix $X$, the i-th column by the category part of the $i \u2014 1$-th column. To achieve this we can take the $k^Tq$ matrix to act only on the positional encoding like subdiagonal of 1's. so more specifically in matrix-block-notation:\n$k = (0, I)$\nwhere as usually $I$ represents the $M \\times M$ identity Then, instead of $q$ we take a matrix in block notation:\n$(0,q)$.\nSo that\n$Q^t = (0, q) \\cdot X$.\nLet $D(-1)$ denote the square matrix which is subdiagonal meaning in the diagonal below the main diagonal it has 1's and zero's everywhere else. Hence the $j, i$th entry of $D(-1)$ is 1 if $i = j - 1$ and always 0 otherwise. Then let the map $v$ only keeps the category part and gets rid of the positional encoding part: So, the matrix $V$ defined in 2.1 is now defined by:\n$V^t = (v, 0) \\cdot X.$"}, {"title": "3.2 Solving the problem with Softmax, case when self-attention acts only on positional encoding", "content": "We are going to show that when using Softmax, there is no place where gradient can get stuck and there is only one solution, which is our solution. Now, when you use Softmax then you work under the constrain:\n$\\sum_{j=1}^M q_{ji} = 1$\nand all the $q_{ji} \\geq 1$. So, we just need to check that under the constrain 3.35, we do not encounter a local minima under the constrain. So, we can not put the partial derivatives $\\frac{\\partial E[SSE]}{\\partial q_{ji}}$ equal to zero. Instead, the condition is that the gradient with respect to the $q_{ji}$'s\nis colinear with the gradient of the sum $\\sum_{j=1}^M q_{ji}$. That gradient is the vector consisting only of ones. The gradient according to the $q_{ji}$'s, to be colinear with that vector of 1s, has simply to have all entries equal. Which is the same as putting the differences equal to 0. Hence, for all $j \\neq i - 1$, we have to set the difference given in 3.9 equal to 0. This is what we did in the previous subsection and we obtained that $Aq = 1/Au$ in 3.26, which hence remained valid here. This was also possible because there is here no constain on the $\\upsilon_{\\kappa \\iota}$'s, and hence the same equations remained valid for the $\\upsilon_{\\kappa \\iota}$. Among others, the fact that the $\\upsilon_{\\kappa \\iota}$'s are equal everywhere except on the diagonal, where they are larger by a quantity $Au$, which does not depend on the column. Equations 3.23 and 3.22, we obtained setting these partial derivatives of $E[SSE]$ with respect to the $\\upsilon_{\\kappa \\iota}$'s equal to 0, and hence again these equations remain valid in the current case. Now, equation 3.23 was shown to be equivalent to 3.31, which in terms implied 3.32, which hence remains valid here. But,"}, {"title": "3.3 Gradient zero for Self-attention not having access to positional encoding", "content": "Here we look at extreme situation, where the k and q matrix only have access to the categories, hence are determining a functional of category pairs. Then, the linear map $v$ in the self attention afterwards, only accesses the positional encoding part, where the weights $w_{ij}$\nof self-attention given in 2.5 are stored. Finally, we take the elements on the diagonal out. (In our solution proposed in Subsection 2.2, with self-attention acting first on categories, we had then added a skip, and then Relu with a bias to extract that diagonal. This can add other vanishing gradient problems, so here we simplify, by directly picking out the elements on the diagonal. The question is can gradient descent get stuck in zero's which do not correspond to the solution we think of? Let us calculate! We assume here as before that there are M positions and N categories. We have a initial \"text\":\nX = (X_1, X_2, ..., X_M)\nwhere $X_1, X_2, ...$ are i.i.d. and take values in the set $\\{1, 2, ..., N\\}$ which is the set of categories. We don't ask that the categories are equaly likely. Now, again we represent the text as a matrix $X$ where the i-th column represent the i-th token $X_i$. For this that i-th column of $X$ is a concatenation of the 0, 1-hot-encoded category $X_i$ and the 0,1-hot-encoded position i. We let the bilinear products obtained with $k^Tq$ only act on the categories and not the positional encoding. We take\n$Q = \\begin{pmatrix}\nq & 0 \\\\\n0 & 0\n\\end{pmatrix} \\cdot X$\nwhere $q$ is a $N \\times N$ matrix to be learnt and\n$K = \\begin{pmatrix}\nI & 0 \\\\\n0 & 0\n\\end{pmatrix} \\cdot X$"}, {"title": "3.3.1 Proving that \u2211j\u2260i Vij = 0 is invalid", "content": "We are going to use 3.60 and 3.62 to show, that we would have an invalid solution. We are going to use equation 3.40 to show that there can be no solution with 3.60 except the trival where all the vij's and all the $q_{(r,t)}$ are all 0. First note that with the help of 3.62, we find:\n$$E [q^{true}(X_2,X_2) q_{(X_1,X_2)}] = E [E[q^{true}(X_1,X_2)|X_2] q_{(X_1,X_2)}] = E [E[E[q^{true}(X_1,X_2)|X_2] q_{(X_1,X_2)}]] .$$\nWe can always assume $Vi(i-1) = 1$ without restriction. Indeed, if we multiply $q(.,.)$ by a constant and divide the function $v$. by the same constant the output of our transformer will be the same! Now, with the help of equation 3.63 and 3.60, equation 3.40 becomes:\nE (q^{true}_{((X_1,X_2)} q_{(X_1,X_2)})  =\n= (E[q^{true}(X_1,X_2)] - E [q^{true}(X_1,X_3) q^{true}(X_2,X_3)]) + E [E[q^{true}(X_1,X_2)|X_2]\u00b7q^{true}(X_1,X_2)].\nNow, we assumed that $Vi(i-1} = 1$ and we have that all vij for which $j != i, i - 1$ are equal. Then from 3.60, it follows that vij =  \u2211v_{ij}^{  1}/i!=l 1 and hence"}, {"title": "4 Learning q,k,v,C,B", "content": "The goal of the current article, is to find out, if the self-trained transformer uses self-attention for some of the logical analysis, or proceeds more like our first or third solutions. In the current section, we will train a one level transformer, and study, which of the parts is used for the \"logical analysis\u201d.\nWe have presented so far three solutions for building a hand-programmed one level transformer to output $Y_i = q^{true} (X_{i-1}, X_i)$, for a given function $q^{true}(., .)$, when given $X$ as input. The first solution was presented in Subsection 2.1, the second solution in 2.2 and the third in 2.3.\nThe first solution, presented in Subsection 2.1 adds pairs of adjacent columns of $X$ with the help of self-attention and skip connection. Then, it extracts the information about the category pair $(X_{i-1}, X_i)$, with the fully connected layer, that is with matrix $B$ followed by ReLU followed by $C$. The problem is that if we have $N$ categories, then $B$ needs to have $N^2$ rows! (Except if $q(X_1, X_2)$ is linear, in which case no $ReLU$ is needed and things become trivial. For an explanation of this simple case, see end of Subsection 2.1). Now, the third solution presented in Subsection 2.3, is an improvement of the first, where no matrix $B$ is needed, but instead the linear out matrix is used in a clever way replacing the need for $B$.\nFinally, the second solution uses self attention to obtain $q(X_j, X_i)$ for any pair $i, j \u2264 M$. This information is stored in the location part of the original vector space of the columns of $X$. The specific information about category pairs of adjacent tokens, that is $q(X_{i-1}, X_i)$, is then extracted with an ingenious trick with a skip connection followed by a ReLU with the appropriate bias. For that solution, there would be no need for a fully connected layer, but we can still put in a matrix $B$ so as to have a compatible architecture for both approaches! We will have to take $N = M$ in order to have compatibility. Note that the first solution with a large matrix $B$, seems less usefull then the third, which is similar in some sense. In our mind, the race is between Second and Third solution, both can be written:\n$\\vec Y = C(ReLU(ATTENTION_*(Q, K, V)^t + X + bias))$,\nwhere $C$ is a one-row matrix.\nOne of the main problems, is that we defined our second solution without Softmax, but for the training to not get stuck into a gradient zero, we need Softmax or rescaling. Now, the Softmax will in general \"ruin\", the matrix 2.19. Here is one idea, how to resolve this. The first step is to take $q^{true}$ with values which are non-negative. Then we could take logarithm values of it, hence we would take $q = ln(q^{true}(., .))$ and then the fact that Softmax takes exponnential function, would already be taken care of this. The other problem, is that Sofmax will standartize every column of 2.19. If the columns have enough elements, then, the sum of entries of column number $j$ would be approximately equal to the expectation, $E[q^{true}(X_j, t)]$, where $t$ is not random and $X_i = t$. So, for Softmax to not have negative effect, we need $E[q^{true}(X_j,t)]$ for non-random $t$, to not depend on $t$. This way, when we do the rescaling in each column of 2.19, we get the same coefficient, which can be corrected later. With that correction, even with Softmax, the Second Solution"}, {"title": "4.1 Experiments", "content": "We trained three systems with modified encoder layers, tailored towards the three proposed solutions. In these the correspending attention weights $k, q, v$ have access to either positional or input information. This was implemented by handling of the $X^E$ and $X^P$ separatelly. In addition we trained a system capable of learning all three solutions to investigate the natural choice of the transformer. In this system, the attention weights $k, q, u$ have access to both positional and input information.\n$X^E$ are one-hot encoded random samples from a categorical distribution ~ Cat(N), where $N = 10$ is the number of categories. $X^P$ are one-hot encoded M positions, where $M = 50$ is the length of the sequence. The target $Y$ is generated by a function $q^{true} (X_{i-1}, X_i)$, where $q^{true} (.,.)$ are random samples of a standard normal distribution. The models were trained in PyTorch with LBFGS optimizer using batch size of 1000. The training was performed for 50 iterations using Mean Squared Error (MSE) as a loss function. MSE from the last iteration was used to compare the performance of the models.\nThe attention mechanism was implemented as a single-head dot-product attention without Softmax. The output was then passed through a normalization layer followed by two linear layers with a ReLU activation in-between. The output of the second linear layer was the prediction of the model. The LayerNorm normalization prevented the gradients from becoming zero.\nThe combined system capable of learning all three solutions was trained"}]}