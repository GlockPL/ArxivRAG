{"title": "Automatic Prediction of the Performance of Every Parser", "authors": ["Ergun Bi\u00e7ici"], "abstract": "We present a new parser performance prediction (PPP) model using machine translation performance prediction system (MTPPS), statistically independent of any language or parser, relying only on extrinsic and novel features based on textual, link structural, and bracketing tree structural information. This new system, MTPPS-PPP, can predict the performance of any parser in any language and can be useful for estimating the grammatical difficulty when understanding a given text, for setting expectations from parsing output, for parser selection for a specific domain, and for parser combination systems. We obtain SoA results in PPP of bracketing F\u2081 with better results over textual features and similar performance with previous results that use parser and linguistic label specific information. Our results show the contribution of different types of features as well as rankings of individual features in different experimental settings (cased vs. uncased), in different learning tasks (in-domain vs. out-of-domain), with different training sets, with different learning algorithms, and with different dimensionality reduction techniques. We achieve 0.0678 MAE and 0.85 RAE in setting +Link, which corresponds to about 7.4% error when predicting the bracketing F\u2081 score for the Charniak and Johnson parser on the WSJ23 test set. MTPPS-PPP system can predict without parsing using only the text, without a supervised parser using only an unsupervised parser, without any parser or language dependent information, without using a reference parser output, and can be used to predict the performance of any parser in any language.", "sections": [{"title": "MTPPS-PPP", "content": "Parsing gives an insight into how a sentence is formed and how its structural components interact when conveying the meaning. Parser training and parsing can be computationally demanding and for supervised parsers, training requires labeled tree structural data, which can be expensive to obtain, scarce, or limited to a single domain. PPP is useful for estimating the grammatical difficulty when understanding a given text, for setting expectations from parsing output, and it can be used for parser selection for a specific domain and for parse selection from the output of multiple parsers on the same data.\nPPP before parser training and parsing can prevent spending resources towards obtaining training data and parser training. PPP after parsing can help us select better parsers for a given domain.\nMTPPS as described in (Bi\u00e7ici et al., 2013; Bi\u00e7ici, 2013; Bi\u00e7ici and Way, 2014) provide parser and language independent features, which can be used to predict the performance of any parser in any language, measuring the closeness of a given sentence to be parsed to the training set available, the difficulty of retrieving close sentences from the training set, and the difficulty of translating them to a known training instance. MTPPS-PPP features measure similarity of the grammatical structures, the vocabulary, and their distribution using a subset of the MTPPS features and some additional features:\nText {121} use n-grams as the basic units of information over which similarity calculations are made with up to 3-grams for textual features and 5-grams for LM features. Textual features allow\nus to predict without actually training the parser\nor parsing with it.\nLink {26} use link structures from unsupervised\nparser CCL (Seginer, 2007), which has linear time\ncomplexity in the length of a sentence (Seginer,\n2007). Derivation of the textual features for PPP\ntake minutes. Link contain coverage {9}, perplex-\nity {15}, and x2 vector similarity {2} features."}, {"title": "Statistical Lower Bound on Error", "content": "We evaluate the prediction performance with correlation (r), root mean squared error (RMSE), mean absolute error (MAE), or relative absolute error (RAE). We obtain expected lower bound on the prediction performance, number of instances needed for prediction given a RAE level and for building SoA predictors for a given prediction task From a statistical perspective, we can predict the number of training instances we need for learning with a goal of increasing the signal to noise ratio SNR = \u00b5/\u03c3 or the ratio of the mean to the standard deviation. Let y = (y1, \u2026\u2026\u2026, yn)T represent the target sampled from a distribution with mean \u00b5 and standard deviation \u03c3, then the variance of \u03a3in=1 yi is no\u00b2 and of the sample mean, \u1ef9, is \u03c32/n with the standard deviation becoming \u03c3/\u221an. Thus, by increasing the number of instances, we can decrease the noise in the data and increase SNR. We are interested in finding a confidence interval, [\u1ef9 - ts/\u221an, \u1ef9 + ts/\u221an], where the value for t is found by the Student's t-distribution for n-1 degrees of freedom with confidence level \u03b1. True score lies in the interval with probability 1 \u2212 \u03b1 or for s representing the sample standard deviation of the scores:\nP(-t \\leq \\frac{\\overline{y} - \\mu}{s / \\sqrt{n}} \\leq t) = 1-\\alpha.  (1)\nThe absolute distance to the true mean or the width of the interval, d, is empirically equal to MAE and\nt\\frac{s}{\\sqrt{n}}\nd = \\frac{t s}{\\sqrt{n}}\nRAE = \\frac{NMAE}{\\frac{1}{n}\\sum_{i=1}^{n} |y_{i} - \\overline{y}|} (3)\nUsing Equation 3, we can derive MAE for a given RAE as an estimate of d, d. We confidently estimate (with \u03b1 = 0.05, p = 0.95) d and the corresponding n\u0302 to reach the required noise level for the prediction tasks given a possible RAE level using Equation 2.\nStatistical lower bound on PPP error is presented in Table 1, which lists how many training instances to use for PPP. Table 1 presents the d possible according to the bracketing F\u2081 score distribution and the training set sizes required for reaching a specified noise level based on the RAE. n increase linearly with RAE and we observe that every 1% decrease in RAE correspond to tenfold increase in n corresponding to a slope around 10.\nTop results in quality estimation task for machine translation achieve RAE of 0.85 (Bi\u00e7ici and Way, 2014) when predicting HTER, which is an evaluation metric with range [0,1]. We show that we can obtain RAE of 0.85 in Table 4 when predicting bracketing F\u2081, where bracketing F\u2081 has also the"}, {"title": "Experiments", "content": "We present an extensive study of parser performance prediction using different types of features (text, link, tree, CF\u2081), in different experimental settings (cased vs. uncased), and in different learning tasks (in-domain vs. out-of-domain). We use the Wall Street Journal (WSJ) and Brown corpora distributed with Penn Treebank version 3 (Marcus et al., 1993; Marcus et al., 1999). WSJ02-21 refer to WSJ sections 2 to and including 21, WSJ24 refer to WSJ section 24, WSJ23 refer to WSJ section 23, and WSJ0,1,22,24 refer to WSJ sections 00, 01, 22, and 24 combined. BTest refers to the test set formed by selecting every 10th sentence from the Brown corpus following (Ravi et al., 2008). The number of sentences for each test set is given in Table 3. WSJ02-21 contains 39832 sentences in total and"}, {"title": "Parsers", "content": "CCL: CCL (Seginer, 2007) is an unsupervised parsing algorithm, which allows equivalent classes with reciprocal links between words.\nPCFG: Plain PCFG (probabilistic context free grammar) parser uses the Stanford supervised parser (Klein and Manning, 2003). The plain PCFG model is unlexicalized; it has context-free rules conditioned on only the parent nodes; it does not have language dependent heuristics for unknown word processing; and it selects the left-most category as the head of the right hand side of a rule.\nCJ: Charniak and Johnson (Charniak and Johnson, 2005) develop a SoA parser achieving the highest performance by reranking 50 best parses with a maximum entropy reranker."}, {"title": "ID Results", "content": "Table 4 present ID PPP results with WSJ23 as the test set. The first main column lists the learning setting and the parser. The second and third main columns present the results we obtain using WSJ24 or WSJ0-1-22-24 as the training set for the predictor.\nThe model used is either RR, SVR, or TREE and it can be after FS or PLS and we select the model with the minimum RMSE for each row of results. #dimI is the initial number of dimensions of the feature set being used. Difference between the #dimI numbers for the +TreeF and +Link feature sets correspond to the number of bracketing structural features added, 5 of which are bracketing tree statistical features. +CF\u2081 adds only 1 feature to the overall feature set.\nIn Table 4, the performance improves as we add more features (towards the bottom) or use more training data (right) as expected. SVR performs the best with a smaller training set and PLS and FS can sometimes improve the performance with a larger training set. Previous work (Ravi et al., 2008) obtains 0.42 for r and 0.098 for RMSE when predicting the performance of CJ trained on WSJ02-21 over test set WSJ23. We obtain close r and similar RMSE values with setting +CF\u2081 however, we do not use any parser or label dependent information and our CF\u2081 calculation does not use a top performing reference parser whose performance is close to CJ's. Ravi et al. (Ravi et al., 2008) also do not present separate results with the feature sets they use. The top r they obtain with their text based features is 0.19, which is about 50% lower than our results in setting Text. MAE treats errors equally whereas RMSE is giving more weight to larger errors and can become dominated by the largest error. Therefore, MAE and RAE are better metrics to evaluate the performance. A high RAE indicates that PPP is hard and currently, we can only reduce the error with respect to knowing and predicting the mean by about 17% for CCL, 14% for CJ, and by 9% for PCFG in the +TreeF setting. CCL parsing output is the easiest to predict as we see from the RAE results. As more additional information external to the given training text is used to derive the parsing output, the harder it becomes to predict the performance without supervised label information as we observe for PCFG and CJ parser outputs. We are able to predict the performance of"}, {"title": "OOD Results", "content": "We use the BTest corpus for PPP in an OOD scenario. OOD parsing decreases the performance of supervised parsers as is given in Table 3. However, this is not the case for unsupervised parsers such as CCL since they use limited domain dependent information and in fact CCL's performance is slightly increased. OOD PPP results are given in Table 6. We observe that prediction performance is lower when we compare OOD results with ID results. In OOD learning, the addition of the TreeF feature set improves the performance more when compared with the improvement in ID. Previous work (Ravi et al., 2008) obtains 0.129 for RMSE when predicting the"}, {"title": "Feature Selection Results", "content": "The ranking of the features we obtain after FS are given in Table 7 where up to top 5 features for each learning setting is presented when predicting PCFG. As expected, the CF\u2081 ranks high in setting +CF1. Interestingly, tree features are not among the top features selected. We observe that translation and LM based features are ranked high. Features using link structures are also abundant among the top features. In contrast to previous work (Ravi et al., 2008), number of OOV words are not selected among the top features. Abbreviations used are as follows: A number corresponds to the order of the n-grams used or the LM order, len is the length of a sentence in the number of words, ppl is perplaxity, and GM is the geometric mean between the precision and recall. b1gram logp is the backward 1-gram log probability. 3gram wF\u2081 is weighted F\u2081 score over 3-gram features weighted according to the sum of the likelihood of observing them among 3-grams. link wF\u2081 is over links from CCL. wrec is weighted recall. max logpk is the Bayes IBM model 1 (Brown et al., 1993) translation log probability for top k translations (we omit k when k = 1). bpw correspond to the LM bits per word."}, {"title": "Contributions", "content": "MTPPS-PPP works without training a parser, without parsing with it, without any parser dependent information, and without looking at the parsing output. We have obtained SoA PPP results and obtained expected lower bound on the prediction performance and the number of instances needed for prediction given a RAE level, which can be useful for building SoA predictors for a given prediction task. MTPPS-PPP requires minutes for PPP and achieves 0.85 RAE. Our prediction results allow better setting of expectations for each task and domain. We also provide ranking of the features used for different domains and tasks. Ability to predict outcomes enables preparation and savings in computational effort, which may determine whether a business can survive or not in industrial settings. Our results show that we only need 11 labeled instances for PPP to reach SoA prediction performance."}]}