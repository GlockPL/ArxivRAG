{"title": "Moshi: a speech-text foundation model for real-time dialogue", "authors": ["Alexandre D\u00e9fossez", "Laurent Mazar\u00e9", "Manu Orsini", "Am\u00e9lie Royer", "Patrick P\u00e9rez", "Herv\u00e9 J\u00e9gou", "Edouard Grave", "Neil Zeghidour"], "abstract": "We introduce Moshi, a speech-text foundation model and full-duplex spoken dialogue framework. Current systems for spoken dialogue rely on pipelines of independent components, namely voice activity detection, speech recognition, textual dialogue and text-to-speech. Such frameworks cannot emulate the experience of real conversations. First, their complexity induces a latency of several seconds between interactions. Second, text being the intermediate modality for dialogue, non-linguistic information that modifies meaning- such as emotion or non-speech sounds- is lost in the interaction. Finally, they rely on a segmentation into speaker turns, which does not take into account overlapping speech, interruptions and interjections. Moshi solves these independent issues altogether by casting spoken dialogue as speech-to-speech generation. Starting from a text language model backbone, Moshi generates speech as tokens from the residual quantizer of a neural audio codec, while modeling separately its own speech and that of the user into parallel streams. This allows for the removal of explicit speaker turns, and the modeling of arbitrary conversational dynamics. We moreover extend the hierarchical semantic-to-acoustic token generation of previous work to first predict time-aligned text tokens as a prefix to audio tokens. Not only this \"Inner Monologue\" method significantly improves the linguistic quality of generated speech, but we also illustrate how it can provide streaming speech recognition and text-to-speech. Our resulting model is the first real-time full-duplex spoken large language model, with a theoretical latency of 160ms, 200ms in practice, and is available at github.com/kyutai-labs/moshi.", "sections": [{"title": "Introduction", "content": "Voice has provided a convenient interface to early conversational systems, from Alexa\u00b9 to Siri\u00b2 and Google Assistant.3 In this context, a \u201cwake word\u201d spoken by the user typically triggers an automatic speech recognition (ASR) system which transcribes the subsequent user's request. Then, a natural language understanding (NLU) pipeline converts this query to a structured format used to produce a text answer through natural language generation (NLG). Eventually, a text-to-speech (TTS) system tells the answer back to the user. While this process can handle short, constrained interactions (e.g. triggering an action or retrieving a fact), the rise of large language models (LLMs) (Brown et al., 2020; Hoffmann et al., 2022; Touvron et al., 2023a) has called for a consequent extension of voice interfaces to multi-turn, open-ended conversations. A solution to this challenge is handling the NLU and NLG with an LLM, while the ASR and TTS provide the voice interface during the user's and the system's turn respectively (Llama, 2024). This framework supports the current generation of spoken dialogue systems such as Gemini (Gemini et al., 2023) or ChatGPT.4\nYet, the experience offered by these interfaces remains far from natural conversations. First, latency compounds along the many components of these pipelines, resulting in a typical global latency of several seconds. This is unlike natural conversations which demonstrate response times of a few hundred milliseconds. Second, as language understanding and generation happens in the textual domain, any non-written information is ignored by the model. This goes from paralinguistic information, such as emotion and accent, to non-speech audio, such as surrounding acoustic events. Finally, these models remain fundamentally turn-based, assuming that dialogue is a sequence of well-defined single-speaker segments. While this paradigm is suited to text dialogue, it falls short in modeling aspects of spoken conversations such as interruptions, overlapping speech\u2014 which amounts for 10 to 20% of spoken time (\u00c7etin and Shriberg, 2006) \u2014and backchanneling (i.e. non-interrupting interjections such as \u201cOK\u201d or \u201cI see\u201d).\nIn this work we introduce Moshi, a speech-text foundation model and real-time spoken dialogue system that aims at solving the aforementioned limitations: latency, textual information bottleneck and turn-based modeling. Moshi augments a text LLM backbone with a smaller audio language model (Borsos et al., 2022; Yang et al., 2023) that ingests and predicts discrete audio units. This removes the information bottleneck of text by understanding inputs and generating outputs directly in the audio domain, while benefiting from the knowledge and reasoning abilities of the underlying text LLM. We extend previous work on audio language models and design a streaming, hierarchical architecture, with a theoretical latency of 160 ms\u2014lower than the 230 ms average in natural conversations measured over 10 languages (Stivers et al., 2009). We furthermore introduce the first multi-stream audio language model, i.e. a model that explicitly processes the input and output audio streams jointly into two autoregressive token streams. This altogether removes the concept of speaker turn and thus allows training the model on natural conversations with arbitrary dynamics including overlap and interruptions. Our resulting model is the first full-duplex\u2014"}, {"title": "Related Work", "content": "Audio Language Modeling. Early developments in speech foundation models have improved speech understanding across many discriminative tasks, from automatic speech recognition (ASR) (Baevski et al., 2020; Radford et al., 2023; Zhang et al., 2023) to speaker verification (Chen et al., 2022) and speech classification (Yang et al., 2021). A key factor in this development is self-supervised learning (Hsu et al., 2021; Baevski et al., 2020; Chen et al., 2022) which allows learning generic, discriminative speech representations. As these speech understanding models build on previous work done on masked language modeling for text (Devlin et al., 2019), generative text pretraining (Radford et al., 2018) has similarly inspired a large family of speech generation models. In particular, Lakhotia et al. (2021) propose quantizing aforementioned self-supervised representations. The resulting discrete audio tokens represent a speech segment as a sequence of categorical variables, thus casting speech generation as a language modeling task. AudioLM (Borsos et al., 2022) furthermore combines these semantic tokens with acoustic tokens from a neural audio codec (Zeghidour et al., 2022), which allows for modeling arbitrary voices, recording conditions and non-speech sounds. These audio language models have redefined the state of the art in speech generation, from text-to-speech (Wang et al., 2023; Kharitonov et al., 2023) to speech-to-speech translation (Rubenstein et al., 2023; Reid et al., 2024) and speech enhancement (Yang et al., 2023). Beyond these supervised tasks, a parallel line of work has explored training and scaling unsupervised audio-only models, trained for autoregressive speech generation (Dunbar et al., 2021; Lakhotia et al., 2021; Borsos et al., 2022). The abilities of these models have progressively expanded, from generating short sentences in a single speaker voice (Lakhotia et al., 2021) to producing meaningful and consistent speech continuations across dozens of seconds in arbitrary voices and conditions (Borsos et al., 2022), thanks to a hierarchical modeling of semantic and acoustic tokens. A main challenge is that audio requires the modeling of long sequences, up to a few minutes, to produce meaningful and exploitable outputs. However, latent representations for audio are typically less compact than equivalent representations for text. Thus, discrete representations from neural audio codecs require multiple predictions per timestep when modeled autoregressively. (Liu et al., 2023b) and (Evans et al., 2024) use latent diffusion (Ho et al., 2020) for general audio and music modeling to alleviate the need for hierarchical discrete tokens. However, these methods cannot be used in a streaming fashion, and it is unclear whether they could generate consistent speech. Copet et al. (2023) instead show that the number of auto-regressive steps can be reduced by introducing a delay between the different levels of tokens, and performing parallel prediction over them. Inspired by the RQ-Transformer method by Lee et al. (2022) and the hierarchical MegaByte transformer model (Yu et al., 2024), Yang et al. (2023) and Zhu et al. (2024) leverage a smaller nested transformer to model the different tokens at a single time step. In this work, we extend these previous works to push the limits of autoregressive speech generation by proposing a scalable hierarchical modeling of audio tokens which can handle several minutes of context while generating audio in real time. Still, while speech-only models learn linguistic structure-lexicon, syntax, semantics- from raw speech (Dunbar et al., 2021), they typically demonstrate poor-to-nonexistent factual knowledge and reasoning abilities. This has led to the development of speech-text models, intended to combine the knowledge and reasoning abilities of text models with the generative power of audio models.\nSpeech-text Models. Such models typically start from a pretrained text language model and either finetune it to predict audio (Hassid et al., 2023), or propose a speech-text finetuning task (Rubenstein et al., 2023; Maiti et al., 2023; Nachmani et al., 2024; Nguyen et al., 2024; Mitsui et al., 2024): For instance, AudioPALM (Rubenstein et al., 2023) starts from a pretrained PALM (Chowdhery et al., 2022) model, and extends its text vocabulary with semantic audio tokens. Then, the model is trained for a mixture of speech-text tasks, including TTS, ASR and speech-to-speech translation. VoxTLM (Maiti et al., 2023) adopts a similar approach for TTS and ASR. While these models are trained in a supervised fashion with specific input and output sequences, Spirit-LM (Nguyen et al., 2024) uses temporal alignment between speech and its transcript to perform modality switch (from speech tokens to text tokens, or conversely) inside a sequence. This allows the model to learn consistent internal representations of language regardless of it being represented as text or speech, as measured through commonsense evaluation. Another approach, adopted by Spectron (Nachmani et al., 2024) and PSLM (Mitsui et al., 2024), combines speech and text in a hierarchical manner rather than as interchangeable representations. Similar to how AudioLM (Borsos et al., 2022) decomposes speech generation into predicting semantic tokens and then acoustic tokens, Spectron and PSLM use a \"Chain-of-Modality\" and first produce an utterance as text tokens, subsequently used as a prefix to generate speech. In this work, we propose Inner Monologue as a main architectural and training component to combine aligned text and speech data. Inner Monologue decomposes speech into a chain of text, semantic and acoustic tokens, and predicts this structured sequence in a hierarchical manner. Unlike Spirit-LM, this allows representing all utterances both as text and speech, rather than switching between modalities; In addition, the integration of acoustic tokens into the same generative model enables generating arbitrary voices and conditions, rather than a single speaker. Besides, this hierarchical modeling described in Section 3.4.4 allows decomposing the generation task without increasing the sequence length of the Transformer (Vaswani et al., 2017) outputs, unlike Chain-of-Modality. Moreover, Inner Monologue decomposes speech on a per-frame basis, which means that each prediction step outputs a speech frame. This is unlike Spectron which requires generating a complete sequence as text before generating audio tokens, and this makes Moshi compatible with real-time generation. Finally, we show in Section 3.4.4 how Inner Monologue, when combined with a delay between token types, allows deriving streaming TTS and ASR systems from Moshi.\nSpoken Dialogue Models. Spoken dialogue is one of the less explored tasks in speech generation, as it requires addressing several challenges: 1) The model should run in real time and allow for long conversations in full-duplex-the model always listens and can speak at any moment; 2) it should be speech-to-speech to handle paralinguistic communication; 3) it should display knowledge and reasoning abilities that make it amenable to helpful and enjoyable conversations. Spectron benefits from its underlying text LLM (as measured by spoken question answering), however it is not compatible with real-time generation due to Chain-of-Modality. PSLM proposes generating speech and text tokens in parallel to reduce this latency, however it reduces the quality of answers, and the model still relies on ASR, which removes paralinguistic information. More importantly, these models cannot handle full-duplex communication, where there is no boundary between speaker turns, as any side of the conversation can be active at any time. An attempt at modeling these dynamics has"}, {"title": "Model", "content": "Moshi is a multi-stream speech-to-speech Transformer model, which allows for full-duplex spoken dialogue with a user thanks to an innovative architecture summarized in Figure 1."}, {"title": "Overview", "content": "Moshi is built on top of Helium, a text LLM which we build from scratch (Section 3.2), relying on high-quality text data to provide strong reasoning abilities to the model. We also propose Inner Monologue (Section 3.4.4), a training and inference procedure in which we jointly model text and audio tokens. This allows the model to fully exploit the knowledge imparted from the text modality, while remaining a speech-to-speech system. To enable real-time dialogue, we also design Moshi as a multi-stream architecture from the get-go (Section 3.4.3): The model is able to both speak and listen to the user at the same time, and does not need to explicitly model speaker turns. In addition, to capture the input user audio and output Moshi's voice with high quality and in an efficient manner, we propose Mimi (Section 3.3), a neural audio codec combining semantic and acoustic information into a single tokenizer by using residual vector quantization and knowledge distillation. To jointly model the audio streams from Moshi and the user, as well as Moshi's text tokens, we rely on a Depth Transformer compatible with streaming inference (Sections 3.4.1, 3.4.2).\nIn this section, we further detail each of these components. We then describe the training datasets and the different training phases we used to train Moshi in Section 4. Finally, in Section 5, we report thorough evaluation results on Moshi's abilities, both linguistic and acoustic, as well as ablation experiments on its main components, while Section 6 provides analyses on the safety of our system."}, {"title": "The Helium Text Language Model", "content": "ARCHITECTURE\nHelium is an autoregressive language model, based on the Transformer architecture (Vaswani et al., 2017). Following previous work in this area, we make the following changes to the original architecture: First, we use RMS normalization (Zhang and Sennrich, 2019) at the input of the attention blocks, the feed-forward blocks and the output linear layer of the model. We use rotation positional embeddings (Su et al., 2024, ROPE), a context length of 4,096 tokens and FlashAttention (Dao et al., 2022) for efficient training. Finally, we change the architecture of the feed-forward blocks and use Gated Linear Units (Shazeer, 2020), with the SiLU activation as a gating function (Hendrycks and Gimpel, 2016b). Our tokenizer is based on the unigram model from SentencePiece (Kudo and Richardson, 2018), and contains 32,000 elements mostly targeting English. We split all numbers into single digits, and use byte-backoff to ensure that our tokenizer does not lose information. We train the model with the AdamW (Loshchilov and Hutter, 2017) optimizer, with a fixed learning rate followed by a cosine learning rate decay (Loshchilov and Hutter, 2016).\nPRE-TRAINING DATA FILTERING\nTraining data is one of the critical ingredients to train LLMs: we now describe our method to obtain a large and high-quality text dataset. We start from high-quality data sources, such as Wikipedia, Stack Exchange and a large collection of scientific articles. As the quantity of data from these sources is too small to train a LLM, we also rely on web crawled data, specifically from Common Crawl, to extend our dataset. See more details on data sources in Section 4.1. Web data requires extensive processing to obtain a high-quality training set: we perform deduplication, language identification and quality filtering. In the following, we describe each operation in more details."}, {"title": "Audio Tokenization", "content": "To discretize waveforms into audio tokens, we introduce Mimi, a neural audio codec (Zeghidour et al., 2022; D\u00e9fossez et al., 2023) that operates as an autoencoder with a discrete bottleneck (van den Oord et al., 2017). In the literature, and following the terminology defined by Borsos et al. (2022), these tokens are referred to as acoustic tokens, as they model fine audio details and are optimized for high-quality reconstruction. While these acoustic tokens provide appropriate targets for conditioned text-to-audio models (e.g. text-to-speech (Wang et al., 2023) or text-to-music (Copet et al., 2023)), unconditioned speech generation requires combining them with semantic tokens extracted from self-supervised speech models (Baevski et al., 2020; Hsu et al., 2021; Chung et al., 2021). Unlike their acoustic counterpart, semantic tokens do not allow for reconstructing high-quality audio but correlate strongly with linguistic content. This similarity with language allows generating intelligible and consistent speech, even without text conditioning, by using semantic audio tokens as a prefix to predicting acoustic tokens. Yet, this hybrid tokenization approach is not compatible with real-time generation. Semantic tokens are typically not causal and can thus only be computed in an offline manner. Moreover, generating acoustic and semantic tokens with separate encoders represents a non-negligible computational burden. Consequently, and taking inspiration from previous work on SpeechTokenizer (Zhang et al., 2024b), Mimi uses distillation to transfer non-causal, high-level semantic information into the tokens produced by a causal model, allowing for streaming encoding and decoding of semantic-acoustic tokens.\nARCHITECTURE\nOur baseline architecture takes inspiration from SoundStream (Zeghidour et al., 2022) and Encodec (D\u00e9fossez et al., 2023) and consists of a SeaNet (Tagliasacchi et al., 2020) autoencoder and a Residual Vector Quantizer (Zeghidour et al., 2022). The encoder projects a single-channel waveform $x \\in \\mathbb{R}^L$ to a latent representation $enc(x) \\in \\mathbb{R}^{S \\times D}$ by cascading residual convolutional blocks that interleave dilated (van den Oord et al., 2016) and strided convolutions along with ELU (Clevert et al., 2016) non-linearities and Weight Normaliza-"}, {"title": "Generative audio modeling", "content": "We now describe how we extend the base Helium model to support the modeling of the audio tokens provided by the Mimi codec. With our goal of achieving realistic spoken dialogue interactions, we further show how to model not just a single stream of audio, but two at the same time, one representing the user, and one the system. Finally, we detail a novel feature, the Inner Monologue, which consists in a joint modeling of the textual and audio modalities on the system side, to improve the quality of interactions.\nHIERARCHICAL AUTOREGRESSIVE MODELING WITH RQ-TRANSFORMER\nLet $U \\in \\{1, ..., N\\}^S$ be a discrete random sequence, with cardinality $N$ and a sequence length $S$. For convenience, we also denote $U_o = 0$, a deterministic initial token value. Autoregressive modeling consists in estimating the joint distribution $P [U_1, ..., U_S]$ through estimating the conditional distributions $P [U_s|U_o,... U_{s-1}]$ for all steps $1 < s < S$. Text language models, such as GPT (Radford et al., 2019) or Helium, fit this paradigm.\nWhen modeling spoken language, relying on the tokenized text yields a much more compact representation than audio tokens: Using the Mimi codec introduced in Section 3.3, with $Q = 8$ codebooks at a frame rate of 12.5hz, one would require a sequence length of 100 steps per second of audio to generate. To model 5 minutes of audio, this would amount to 30,000 timesteps, which represents a significant computational cost and generating 100 tokens per second is incompatible with streaming inference. As a comparison, a sample of English speech can be represented with around 3 to 4 text tokens per second.\nWe are interested in modeling not just a single sequence $(U_s)$, but multiple sub-sequences, e.g. different audio codebooks, along with an optional text stream. We can stack those sub-sequences as $V_{s,k}$ for $1 < s < S$ and $1 \\leq k \\leq K$. Similarly, we define $V_{o,k} = 0$, a deterministic initial token value for all sub-sequences. For each $1 < s < S$ and $1 \\leq k \\leq K$, $V_{s,k} \\in \\{1, ..., N_k\\}$, where $N_k$ is the cardinality of the k-th sub-sequence. One can flatten\nthe $K$ sequences into a single one, increasing the number of predictions by $K$. Lee et al. (2022) propose using a smaller autoregressive model along the dimension $K$, combined with a larger model along the time dimension, forming a RQ-Transformer. Later, Yu et al. (2024) suggested a similar approach for byte-level modeling.\nRQ-Transformer. Formally, the RQ-Transformer consists in two Transformer models, as illustrated in Figure 3. It consists of a Temporal Transformer, e.g. with the same architecture as the one described for Helium in Section 3.2, and a smaller Depth Transformer. We denote $Tr_{Temp}$ the function represented by the Temporal Transformer, and $Tr_{Depth}$ the one for the Depth Transformer. For simplicity, and for all steps $s < S$, we denote $V_s = (V_{s,1},...,V_{s,K})$ the joint value of all sub-sequences at step $s$. For a given sequence step $1 < s < S$, the Temporal Transformer maps $(V_o, ..., V_{s-1})$ to a temporal context vector\n$z_s = Tr_{Temp}(V_o,..., V_{s-1}) \\in \\mathbb{R}^d$. (1)\nIf we further take a sub-sequence index $1 < k < K$, the Depth Transformer maps both $z_s$\nalong with $(V_{s,1},..., V_{s,k-1})$ to the logits estimate\n$l_{s,k} = Tr_{Depth}(z_s, V_{s,1}, ..., V_{s,k-1}) \\in \\mathbb{R}^{N_k}$. (2)\nWe further define $l_{s,1} = Lin(z_s) \\in \\mathbb{R}^{N_1}$, with Lin a dedicated linear layer. We train $Tr_{Temp}$,\n$Tr_{Depth}$ and Lin so that $softmax(l_{s,k})$ is a good approximation of the distribution of $V_{s,k}$\nconditioned on all sub-sequences for the previous steps, and of the previous sub-sequences\nfor the current step, e.g.\n$softmax(l_{s,1}) \\approx P [V_{s,1}|V_o, ..., V_{s-1}]$\n$softmax(l_{s,k}) \\approx P [V_{s,k}|V_o, ..., V_{s-1}, V_{s,1},... V_{s,k-1}]  k>1$. (3)\nImportantly, the number of steps in the Temporal Transformer is always equal to $S$, rather\nthan $KS$, and the number of steps in the Depth Transformer is at most $K$. In practice,\nAUDIO MODELING\nThe audio codec Mimi described in Section 3.3 outputs $Q$ sub-sequences, with 12.5 steps\nper second of audio. We denote those sequences by $A_{t,q} \\in \\{1,..., N_a\\}$ for $1 \\leq t \\leq T$ with\n$T = 12.5 \\cdot duration$, and $1 \\leq q < Q$ with $Q = 8$. We insert the audio sub-sequences into\nthe multi-sequence $V$ modeled by the RQ-Transformer. Remember that the first codebook\n$A_{t,1}$ corresponds to the semantic information, as detailed in Section 3.3.2, while the other\ncodebooks correspond to acoustic features.\nAcoustic delay. We first experimented with simply setting $V = A$ in the modeling.\nHowever we find that introducing a slight delay between the semantic and acoustic tokens led\nto more stable generations. Copet et al. (2023) show that this leads to reduced dependencies\nbetween the sub-sequences for a given time step, conditioned on the past, thus allowing\nto use a weaker model to approximate the joint distribution $P [V_{s,k}|V_o, ..., V_{s-1}]$ (in their\ncase, as the product of the conditioned marginals). Lemercier et al. (2024) further show a\nconnection between the mutual information between the sub-sequences at a given step, and\nthe quality of the generation: naturally, the more complex the interdependence, the more\npowerful a model will be needed to estimate them.\nAs shown in Section 5.3, introducing a delay of 1 or 2 steps between the semantic and\nacoustic features greatly improves the quality of the generation. This allows the Temporal,\nlarger, Transformer to model the inter-dependence between semantic and acoustic features.\nFormally, given a delay $\\tau\\in \\mathbb{N}$, we have, for all steps $s$\n$V_{s,1} = A_{s,1}$\n$V_{s,q} = A_{s-\\tau,q}$ if $s \\geq \\tau + 1, q > 1$\n$V_{s,q} = 0$ if $s multi-stream and inner\nmonologue, we have the final set $V$ of sequences to model defined as\n$V_{s,1} = W_s$ aligned text tokens.\n$V_{s,2} = A_{s,1}$ semantic tokens of Moshi.\n$V_{s,1+q} = A_{s-\\tau,q}$ if $s > \\tau + 1, 1 < q \\leq Q$ delayed acoustic tok. of Moshi. (6)\n$V_{s,1+Q+1} = A'_{s,1}$ semantic tokens of other.\n$V_{s,1+Q+q} = A'_{s-\\tau,q}$ if $S> \\tau + 1, 1 <q \\leq Q$ delayed acoustic tok. of other,"}, {"title": "Datasets and training", "content": "Text data\nOur training dataset is made of a mix of high-quality data sources and filtered web data\nfrom Common Crawl. More specifically, 12.5% of our dataset is from the following curated\nsources: Wikipedia, Wikibooks, Wikisource, Wikinews, StackExchange10 and the collec-\ntion of scientific articles pes2o.11 Instead of doing multiple passes on Wikipedia, we use five\ndifferent dumps from 2017, 2018, 2019, 2021 and 2022. The remaining 87.5% of our dataset\nis from CommonCrawl, and was filtered with the pipeline described in Section 3.2.2. We\nused the following ten crawls: 2018-30, 2019-04, 2019-30, 2020-05, 2020-34, 2021-04,\n2021-31, 2022-05, 2022-33, 2023-40.\nAudio data\nWe use a first audio collection of 7 million hours, which we call the unsupervised audio\ndataset, of readily available audio content, the majority of which contains English speech.\nWe transcribe the training set with Whisper (Radford et al., 2023), using the large-v3\nmodel. We use this data for the audio pre-training phase, during which we do not use the\nmulti-stream approach described in Section 3.4.3, but instead use a single stream of au-\ndio representing all speakers at once. Similarly, the text stream described in Section 3.4.4\nrepresents the words coming from all speakers. All the audio is resampled to 24kHz and\ndownmixed to mono.\nTo achieve multi-stream, we need the model to gain the ability to both listen and speak at\nthe same time. For this, we further leverage the Fisher dataset (Cieri et al., 2004). It consists\nSpeech-Text Instruct data\nEarly experiments using text-based instruct datasets such as Open Hermes (Teknium, 2023)\nproved to be ill-suited for the instruct tuning of a spoken conversational system. In partic-\nular, the data formatting was often impossible to properly render with TTS (e.g. URLs),\nand the format of the questions and responses was not following a natural oral flow (e.g.\nbullet points, long enumerations). Instead, we leverage Helium, fine-tuned on Open Hermes\nand transcripts of real conversations, to generate realistic interactions between a speech-\nbased AI model and a user. We then synthesize them with our multi-stream streaming\nTTS described in Appendix C, leading to more than 20k hours of synthetic speech data.\nTo give Moshi its own consistent voice, we also condition the TTS engine on the voice of a\nsingle actor, who recorded monologues covering more than 70 speaking styles, as listed in\nExperiments on voice consistency reported in Section 6.3 show that simply using\na consistent voice for Moshi during instruction tuning is enough to guarantee almost surely\nthat it does not use another voice, without further control during inference. In contrast, the\nvoice of the second audio stream (the user) is randomly sampled for each example, giving\nmore robustness to different speaking conditions and accents.\nTo generate the transcripts, we use different prompts, aiming at capturing different\nkinds of interactions between a user and Moshi. First, we generate conversations about\ngeneral knowledge, by starting from a few Wikipedia paragraphs or StackExchange posts,\nwhich we refer to as context. This ensures that Moshi's conversations cover a wide range\nof topics, such as history, cooking advice or pop culture.\nMore precisely, using a given context, we obtain a summary of a potential discussion\nwith the following prompt:\n{{context}}\nBased on information from the previous paragraph, write the summary\nof a conversation about {{title}} between Blake and Moshi. The\nsummary must be 2 sentences long, and start with \"They\" or \"The\nspeakers\".\nwhere {{context}} refers to paragraphs from Wikipedia or StackExchange and {{title}}\nis the corresponding title. Then, we generate the full transcript with the prompt:\n{{context}}\nWrite the transcript of a conversation between Blake and Moshi.\n{{summary}} Moshi is knowledgeable about the topic. Use some\nbackchanneling. Use short turns.\nSimilarly, to give Moshi information about itself and the Kyutai lab, we generate para-\ngraphs describing both and use them as additional context.\nSecond, we produce interactions containing instructions about Moshi's voice, such as\nthe other speaker requesting Moshi to speak with an angry voice or like a pirate. Our\nfirst strategy is to generate single turn interactions where the model is instructed to tell\na sentence, a monologue or a poem about an entity, belonging to a high level category\nsuch as \"sports\" or \"animals\", using a particular voice. The voice requested by the other\nspeaker and the entity are randomly sampled, and are thus completely unrelated. Our\nsecond strategy is to generate roleplaying situations, corresponding to different emotions or\nspeaking styles with the following prompt:\nWrite a list of 10 situations about a {{voice}} {{character}}. Each\nsituation must start with \"a {{voice}} {{character}} who\" and must be\nat most 8 words long.\nExamples of voice adjective include \"happy\" or \"suprised\" and examples of characters\ninclude \"detective\u201d or \u201csuperhero\". We then generate the interaction using the prompt:\nWrite a dialogue between Blake and Moshi, {{situation}}. Use a lot of\nbackchanneling.\nTo make Moshi robust to mispronounced words, we also generate instructions containing\nmisspellings in the user's questions, followed by Moshi asking the user to repeat herself or to\nclarify the question. We also generate questions containing a false or misleading fact (such\nas \u201cIs the Eiffel Tower in Beijing?\u201d), to train the model to answer \u201cNo\u201d and correct the user.\nOtherwise, the vast majority of generated conversations only contain questions from the user\nwhere Moshi should answer positively. We generate basic math, grammar or trivia single-\nturn questions and answers, as we noticed that Moshi was initially not performing well on"}, {"title": "Training stages and hyper-parameters", "content": "Helium pre-training. An overview of the training stages and hyper parameters is provided in Table 1. For every training phase, we use AdamW (Loshchilov and Hutter, 2019), with a weight decay of 0.1, a momentum decay of 0.9, and a decay for the online average of the squared gradient of 0.95. All models are trained on H100 GPUs, using FSDP and activation checkpointing. The core text-only language model, Helium, is trained for 500k steps, with a batch size of 4.2M tokens, using a cosine learning rate schedule starting at 3\u00b710-4 with linear warmup.\nMoshi pre-training. Then, we initialize the Temporal Transformer in Moshi with Helium, while the Depth Transformer described in Section 3.4.1 is randomly initialized. We first train on the unsupervised audio dataset presented in Section 4.2, using a single stream of audio, with a batch size covering 16 hours of audio, each batch item consisting of a 5 mn sequence. We mask the corresponding text tokens with a probability of 30%. We randomize the delay between the text and audio tokens between -0.6 and +0.6 seconds. In order to prevent catastrophic forgetting, we also train half of the time on batches of text only data from the same dataset as used for Helium. In total, we make 1 million training steps, with a cosine learning rate starting at 3\u00b710-5 for the Temporal Transformer, and 2\u00b710-4 for the Depth Transformer, also with a linear warmup. In order to ensure the updates from the text-only batches are balanced with those from the audio dataset, we use two separate optimizer states. In addition, when operating on the text stream from an audio batch, we multiply the learning rate for"}]}