{"title": "What Features in Prompts Jailbreak LLMs? Investigating the Mechanisms Behind Attacks", "authors": ["Nathalie Maria Kirch", "Severin Field", "Stephen Casper"], "abstract": "While 'jailbreaks' have been central to research on the safety and reliability of LLMs (large language models), the underlying mechanisms behind these attacks are not well understood. Some prior works have used linear methods to analyze jailbreak prompts or model refusal. Here, however, we compare linear and nonlinear methods to study the features in prompts that contribute to successful jailbreaks. We do this by probing for jailbreak success based only on the portions of the latent representations corresponding to prompt tokens. First, we introduce a dataset of 10,800 jailbreak attempts from 35 attack methods. We then show that different jailbreaking methods work via different nonlinear features in prompts. Specifically, we find that while probes can distinguish between successful and unsuccessful jailbreaking prompts with a high degree of accuracy, they often transfer poorly to held-out attack methods. We also show that nonlinear probes can be used to mechanistically jailbreak the LLM by guiding the design of adversarial latent perturbations. These mechanistic jailbreaks are able to jailbreak Gemma-7B-IT more reliably than 34 of the 35 techniques that it was trained on. Ultimately, our results suggest that jailbreaks cannot be thoroughly understood in terms of universal or linear prompt features alone.", "sections": [{"title": "1 Introduction", "content": "When they are 'jailbroken', large language models (LLMs) can produce harmful outputs which are difficult to predict or control [Wei et al., 2023]. Misuse risks of existing LLMs include harm or misinformation propagation, automation of harmful tasks such as malware development, or privacy violations such as data leakage. Many studies have demonstrated different attack methods that can manipulate LLMs to produce harmful outputs despite tremendous amounts of safety training [Shayegani et al., 2023, Shen et al., 2024, Wei et al., 2023]. However, there is currently a very limited mechanistic understanding of how models are jailbroken [Anwar et al., 2024].\nSome prior works have studied jailbreak prompts or model refusal in LLMs using linear methods [Arditi et al., 2024, Ball et al., 2024, Sheshadri et al., 2024]. However, it remains unclear what latent features are responsible for jailbreaks in LLMs and the extent to which jailbreaking can be understood in terms of linear features. Here, we use both linear and nonlinear probes to identify features in prompt encodings that are related to jailbreak success. We offer three contributions:"}, {"title": "2 Related Work", "content": "Probing for harmful behaviors in LLMs: Training classifiers to recognize features of interest in a model's hidden layers, known as \u201cprobing,\u201d has long been used to study the internal representations of language models [Alain and Bengio, 2016, Belinkov, 2022, Conneau et al., 2018, Ravichander et al., 2021]. This type of analysis enables researchers to investigate whether the LLM represents a certain concept. Some notable uses of probing in language models have involved searching for features corresponding to potentially harmful biases [Arora et al., 2023, Guo et al., 2022, Manerba et al., 2024], toxicity [Ousidhoum et al., 2021], and stating falsehoods [Burns et al., 2024]. Once trained, probes have also been used to guide perturbations to model latents in order to modify their high-level behaviors [Li et al., 2024a]. Here, we use probes to classify jailbreaking prompts as successful or"}, {"title": "3 Experiments", "content": "We standardized all of our experiments on Gemma-7B-IT, which we chose because we found it was more likely to refuse potentially harmful queries than other similarly sized models. For all probing and intervention experiments, we used activations from a single layer (layer 17) which we chose arbitrarily before experiments. We ran all experiments on an A100 80G GPU. The computational demands for the model used in this study include approximately 10 GPU hours for dataset generation (collecting generations, activations, and jailbreak rating for every dataset example). Meanwhile, all experiments presented here required no more than 6 GPU hours to run from start to finish."}, {"title": "3.1 Constructing a dataset of 10,800 jailbreak attempts from 35 attack methods", "content": "A large number of LLM jailbreaking methods have been identified in the literature (e.g. see [Anil et al., 2024, Jiang et al., 2024, Rao et al., 2024, Shayegani et al., 2023]). We compiled a total of 10,800 jailbreaking attempts from a total of 35 different attack algorithms applied to 300 prompts. We implemented attacks based on methods from Wei et al. [2023], Zou et al. [2023b], Liu et al. [2024], Mazeika et al. [2024] and Jiang et al. [2024], standardized to our experimental setup. Because of the diversity of attack methods used, this dataset allows us to distinguish effects of harmful prompts from those of attack types by encoding the same prompt in a wide variety of attack types. See Appendix E for more details.\nWe classified each jailbreaking prompt as successful or unsuccessful based on how its response from Gemma-7B-IT was classified by the HarmBench autograder from Mazeika et al. [2024]. The distribution of attack success rates (ASRs) was heavy-tailed with a median value of 4%. Only nine out of 35 attack methods had a ASR > 10% (see Table 1). The most successful attack method was the many-shot jailbreak from Anil et al. [2024] with an 83% ASR.\nWe analyzed the latent representations of the final prompt tokens at layer 17 of Gemma-7B-IT using t-distributed stochastic neighbor embeddings (t-SNE). As found in [Ball et al., 2024], attacks cluster by method. To further demonstrate clustering by attack technique, we also conducted the same analysis using prompts from the WildJailbreak dataset Jiang et al. [2024] with the same outcome. Figure 2b indicates that the clusters correspond mostly to semantic attack type. This suggests that the model represents different classes of jailbreaks differently suggesting that they may exploit distinct mechanisms. Next, we expand on this by studying probes and their ability to transfer between different attacks."}, {"title": "3.2 Simple probes effectively classify most jailbreaks from methods represented in their training data", "content": "We trained probes to predict binary jailbreak success based on the latent representations of the last token position of the instruction at layer 17 of 28 of Gemma-7B-IT. We compare linear and multilayer perceptron (MLP) probes to investigate whether jailbreak success is represented linearly in the model. The inputs to both our linear and MLP probes were latent vectors of size num_prompt_tokens \u00d7 3072, where with 3072 the dimensionality of the residual steam of Gemma-7B-IT.\nWe used probes to predict whether prompts that attempted to jailbreak the model would be successful or unsuccessful (as judged by the HarmBench Autograder [Mazeika et al., 2024]). The linear probe was logistic classifier  $\\hat{y}(x) = sigmoid(w^Tx + w_0)$, where $w$ (the weight vector) and $w_0$ (the bias) are learned parameters. Meanwhile, the MLP probe also performed logistic classification and had a single hidden layer of size 8 with a ReLU activation function $ReLU(x) = max(0, x)$. In all probing experiments, we balanced and randomized our dataset such that 50% of examples were successful and unsuccessful each. We used a train/test split of 80/20. Both our train and test sets included examples from all attack types.\nBoth the MLP and linear probes were able to predict jailbreak success based on prompt encodings alone with \u2265 85% accuracy, with linear probes achieving a 93% accuracy and the MLP Probe an 87% accuracy. The high level of performance reported in 3 suggests that successful jailbreaks from known attack types can be detected with a high degree of accuracy. However, next we show that this does not imply effective transfer to held-out attack methods."}, {"title": "3.3 Jailbreaking features are not universal: probes often struggle to transfer to held-out attack methods", "content": "Next, we investigated how well probes trained on all attack types but one can transfer to the held out attack. Figure 4 shows train and test accuracies for linear and nonlinear probes. For each attack type with an ASR of >10% (see Table 1), we report training accuracy on all other attacks and testing accuracy on the indicated held-out attack type.\nOverall, both types of probes offer only a limited improvement over a random guess baseline on average when transferring to held-out attacks. For some held-out attacks, transfer performance is worse than random. This suggests that successful jailbreaks from different methods attack the model using different, nonlinear prompt features. However, in the next section, we show through causal intervention experiments that probes nonetheless learn some causally relevant jailbreaking features."}, {"title": "3.4 Nonlinear probes can guide effective mechanistic attacks & defenses", "content": "Finally, we used probes to design latent space perturbations (e.g., [Arditi et al., 2024, Sheshadri et al., 2024]) for Gemma-7B-IT to test whether the features identified by our probes correspond to causal mechanisms. We conducted a grid search to optimize the intervention parameters and selected to target all token positions at every step of generation in layer 16 (one before the probe layers) for all experiments.\nWe tuned the strength of all perturbations to maintain the general capabilities of the perturbed models, ensuring that neither offensive nor defensive perturbations resulted in performance degradation as measured by MMLU Hendrycks et al. [2021] (see Figure 5). We measure the success of jailbreak attacks using the HarmBench autograder [Mazeika et al., 2024]. Finally, to distinguish the success of defensive perturbations from mere refusal, we further measured compliance with harmless requests using an instruction-prompted Llama3-13B-chat.\nLinear probe-guided interventions First, we investigated a linear probe-guided intervention. We added the weight vector of our linear probe to a given token position and layer of our model. Given a hidden layer H, we modified each token position i\n$h_i = h_i + (w * a)$\ncorresponding to the prompt and generated response where $h_i$ is the original activation for token i at layer H, w is the coefficient vector of the linear classifier, and a is the intervention strength. This shifted each latent representation $h_i$ in the direction that the linear probe associated with successful jailbreaks.\nMLP probe-guided interventions We generalized the linear probe-guided latent-space attack to work with nonlinear probes by iteratively performing gradient descent on a latent-space perturbation. See Figure 1 for an example attack. We iteratively perturbed the hidden states $h_{1...n}$ so that the MLP probe would classify them as a successful jailbreak for attacks or an unsuccessful one for defense. Given a hidden layer H, we modified the activations at each token position i using the gradient of the"}, {"title": "loss of the MLP probe with respect to the model's hidden states h1...n. Given the loss, we compute the gradient with respect to the hidden layer activations and updated the hidden activations using gradient descent:", "content": "Finally, we used probes to design latent space perturbations (e.g., [Arditi et al., 2024, Sheshadri et al., 2024]) for Gemma-7B-IT to test whether the features identified by our probes correspond to causal mechanisms. We conducted a grid search to optimize the intervention parameters and selected to target all token positions at every step of generation in layer 16 (one before the probe layers) for all experiments.\nWe tuned the strength of all perturbations to maintain the general capabilities of the perturbed models, ensuring that neither offensive nor defensive perturbations resulted in performance degradation as measured by MMLU Hendrycks et al. [2021] (see Figure 5). We measure the success of jailbreak attacks using the HarmBench autograder [Mazeika et al., 2024]. Finally, to distinguish the success of defensive perturbations from mere refusal, we further measured compliance with harmless requests using an instruction-prompted Llama3-13B-chat.\nLinear probe-guided interventions First, we investigated a linear probe-guided intervention. We added the weight vector of our linear probe to a given token position and layer of our model. Given a hidden layer H, we modified each token position i\n$h'_i = h_i - \\eta \\frac{\\partial Loss}{\\partial h_i}$"}, {"title": "where \u03b7 is the learning rate. We used + for defensive perturbations and \u2013 for offensive perturbations.", "content": "After experimenting with multiple loss functions, we found the best results from a loss that moved the probe's prediction a limited amount in the target direction. We employed a mean squared error (MSE) loss to guide perturbations towards desired logit values.\n\u2022 Positive logit values (MLP probe predicts a successful jailbreak) for offensive perturbations: we used an MSE loss toward a target of max(co, p + co) for some positive co.\n\u2022 Negative logit values (MLP probe predicts an unsuccesful jailbreak) for defensive perturbations: we used an MSE loss toward a target of min(-ca, p \u2013 ca) for some positive Cd.\nWe optimized perturbations for 34 steps using a learning rate of 0.005, using co = 5.5 and ca = 1, adding the perturbed vector to every token position corresponding to the prompt, and generated response in a single layer during the generation of every new token. As with the linear probes, we also experimented with adding the perturbation to different token positions, generation steps, and layers (See Appendix B).\nResults We present results in Figure 5. By all measures, MLP probe-guided perturbations were superior to linear probe-guided perturbations. Offensive MLP probe-guided perturbations achieve higher attack success rates (74%) compared to linear probe-guided ones (26%) and the unperturbed baseline (17%). Defensive MLP probe-guided perturbations achieve lower attack success rates (6%) compared to linear probe-guided ones (12%) and the unperturbed baseline (17%). Finally, the defensive MLP probe-guided perturbations achieve a ratio of benign request and harmful request compliance almost twice that of the linear probe-guided ones. However, we note that the defensive linear probe-guided perturbations were more effective at achieving unconditional refusal which is consistent with findings from Arditi et al. [2024]."}, {"title": "4 Discussion", "content": "In this paper, we studied the features related to jailbreak success by probing for successful attacks based on only the LLM latent tokens corresponding to the prompt. We find evidence that different attacks jailbreak Gemma-7B-IT via distinct nonlinear features. We also show that nonlinear probes effectively identify features that can be used to mechanistically jailbreak the model with a high rate of success. Ultimately, our results suggest that LLM jailbreaks cannot be mechanistically understood in terms of linear or universal features alone.\nSignificance Our study provides evidence that jailbreak success can be predicted and influenced by nonlinear features in the model's internal activations corresponding to the prompt alone. There are three key implications of our work.\n1. Basic understanding: By showing that prompt latents contain nonlinear, nonuniversal features relevant to jailbreaks, we build off of prior work from Arditi et al. [2024] and Ball et al. [2024] toward a better understanding of the mechanisms by which LLMs are exploited with jailbreaks.\n2. Monitoring: By showing that detectable features related to jailbreaks exist inside the model's prompt latents, we show that jailbreaks may be better detected by incorporating latent-space detectors. However, these detectors may struggle to transfer to unseen attack methods.\n3. Interventions: We find that the features found by nonlinear probes are causally related to jailbreaking and that jailbreaks can be modulated using these features."}, {"title": "Limitations", "content": "1. Dataset Size: Our dataset introduces 10,800 jailbreak attempts across 35 methods. While diverse, it is relatively small for training robust classifiers for comprehensive analysis of jailbreaking mechanisms.\n2. Variability of attack success rates: The jailbreaking methods that we use have different success rates, with many having very low efficacy. This poses challenges in detecting consistent signals for jailbreak success. While we balanced our training and test sets, this over-representation of unsuccessful attacks could result in probes overfitting to predict jailbreak attack types as opposed to jailbreak success. However, based on the success in guiding causal interventions, we conclude that the MLP probes can indeed capture relevant features.\n3. Model: Our study focuses in depth on Gemma-7B-IT, which may limit the generalizability of our findings."}, {"title": "Future work", "content": "While our work helps to understand what types of prompt features are responsible for successful jailbreaking, two key questions remain. First, it is unclear why these latent features exist and why safety fine-tuning did not prevent them from being able to jailbreak the model. Second, it is unclear how to interpret these features. Future work should further investigate jailbreaking features in the context of training and attempt to ground our understanding of them in human-understandable interpretations."}, {"title": "A Societal Impacts and Ethics Considerations", "content": "As with any work on adversarial vulnerabilities, our techniques could be used to exploit vulnerabilities in deployed (open source) models. Moreover the, our dataset necessarily includes some examples of harmful language. We issue disclaimers in our dataset README, and the data does not contain any personal information. However, our work was centrally motivated by improving our ability to understand and safeguard against jailbreaks. Understanding these vulnerabilities is a prerequisite to defending against them. By studying jailbreaking mechanisms, we hope to contribute to the development of more robust and safe AI systems. Also, we believe that openly discussing vulnerabilities of current AI systems is important to reduce future risks of more capable systems. We are open to feedback to ensure our work contributes positively to the safety and reliability of AI systems."}, {"title": "B Hyperparameter Selection", "content": "During our linear probe experiment, we selected our hyperparameters based on a grid search over different layers (for training and intervention) and token positions (for training and intervention).\nWe experimented with different complexities of our MLP and found that this simple 2-layer version reported in the paper yielded the best results.\nDue to some randomness involved in probe training, results may vary and we encourage the research community to experiment with alternative hyperparameters than the ones reported in this paper."}, {"title": "C Attack Success Rate", "content": "An overview of the attack success rates (ASR) can be found in Figure 6. We found a low ASR for Gemma 7B IT compared to Ball et al., who used Vicuna 13B. These results suggest that Gemma 7B IT is more robust than other similar models. Vicuna is also an older model, and newer models involve better robustness training."}, {"title": "D Answer Coherency", "content": "D.1 Harmful Requests\nmlp_offensive_c5.5"}]}