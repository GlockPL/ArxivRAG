{"title": "Could AI Leapfrog the Web? Evidence from Teachers in Sierra Leone", "authors": ["Daniel Bj\u00f6rkegren", "Jun Ho Choi", "Divya Panchaksharappa Budihal", "Dominic Sobhani", "Oliver Garrod", "Paul Atherton"], "abstract": "Access to digital information is a driver of economic development. But although 85% of sub-Saharan Africa's population is covered by mobile broadband signal, only 37% use the internet, and those who do seldom use the web. We investigate whether AI can bridge this gap by analyzing how 469 teachers use an AI chatbot in Sierra Leone. The chatbot, accessible via a common messaging app, is compared against traditional web search. Teachers use AI more frequently than web search for teaching assistance. Data cost is the most frequently cited reason for low internet usage across Africa. The average web search result consumes 3,107 times more data than an Al response, making AI 87% less expensive than web search. Additionally, only 2% of results for corresponding web searches contain content from Sierra Leone. In blinded evaluations, an independent sample of teachers rate AI responses as more relevant, helpful, and correct than web search results. These findings suggest that AI-driven solutions can cost- effectively bridge information gaps in low-connectivity regions.", "sections": [{"title": "Main Text:", "content": "The internet has come up short in sub-Saharan Africa. After a policy focus on building network infrastructure, mobile phone penetration is high (92 phones per 100 people), and 85% of the population has access to mobile internet signal (1). Yet only 37% of the population uses the internet (2). Even more striking is how this minority uses the internet: while 76% of internet users across five African countries used social networking daily in 2022, only 29% used web search daily, as shown in Figure 1 Panel A. Connectivity can improve economic outcomes in low-income countries (3, 4), but this lack of adoption suggests that the full potential of the information revolution has yet to be realized.\nWe consider two large barriers that inhibit use of the web. First, web content rarely serves local needs. An audit study found that in sub-Saharan Africa fewer than 30% of a sample of web search results contained local content, compared to over 90% in North America (6).\u00b9 Foreign content is typically formatted for viewing on large computer screens, connected via broadband\u2014 a poor fit for the mobile devices and unstable connections through which most Africans access the web. While mobile phones are common, just 17% of adults report owning a computer on average across 17 sub-Saharan African countries (7). Second, cost is a significant barrier. 77% of"}, {"title": "Context", "content": "This study focuses on the education sector within Sierra Leone. The country had a GDP per capita of $509.48 in 2021; by that measure, the average American is 137 times richer.\nTeachers are knowledge workers who regularly must develop content (lesson plans, assignments, and assessments), and navigate complex interpersonal scenarios (with students, parents, and colleagues). Sierra Leonean teachers are stretched thin: there are 42 students per primary teacher, and they have few resources for professional development. The median teacher in our survey had only completed 2 continuous professional development trainings within the past 3 years. Only 19% of surveyed teachers hold bachelor's degrees.\nMany teachers work in remote areas that have limited infrastructure. In 2020, only 34% of secondary schools had electricity, and just 8% had internet access. These numbers are even lower among primary schools (6% and 1%). But access to 2G (voice and SMS) is more widespread: 86% of schools nationally were within range of mobile phone coverage (10), and some teachers may have 3G signal in their homes.\nAs shown in the right panel of Fig. 1, in a survey of teachers, 91% use WhatsApp daily. They are not strangers to technology. However, they infrequently search. After being provided with an AI WhatsApp chatbot, only 3% report using web search daily to help with teaching, and only 25% report using search at least monthly. In contrast, 53% use the AI chatbot at least one of every 30 days on average over our data."}, {"title": "AI Chatbot", "content": "From the teacher's perspective, the AI chatbot (called \u2018theTeacher.AI') appears as a contact in WhatsApp, and they may chat with it just as with any other contact. The design of the system is similar to that of a chatbot for Kenyan entrepreneurs developed and analyzed in (11), but tailored for education.\nOn the backend, the chatbot relays the conversation, plus a system message, to the OpenAI API, and returns the result back to the user.3 The system message tailors the system to act like a helpful education assistant, to follow best practice pedagogy (guided by the Education Endowment Fund's reports on education evidence (12)), to follow safeguards, and to adapt responses to a teacher who has few classroom resources. The chatbot answers only queries related to education due to budget and for quality control. It was accessible in English, the official language of Sierra Leone; 95% of surveyed teachers in our sample report that English is a main language spoken at home. However, the system message tailors responses to use short, simple language that would be readable to someone without English as a first language.\nThe core chatbot code is available in an open-source repository and the full system message is given in Appendix B1, so that the system can be replicated by others.4", "subsections": [{"title": "Recruitment and Onboarding", "content": "Classroom teachers and principals were recruited in several cohorts from government schools and a non-profit school network (EducAid Sierra Leone). Users were given onboarding training which ranged from one to several hours depending on the cohort. Training including an overview of the system and practice asking questions. As part of onboarding, users were asked for informed consent for their anonymized data to be analyzed for research. We consider only teachers and principals using the system from Sierra Leonean phone numbers. We omit queries from each user's first day, to omit any practice questions submitted during the training session, and include only users who successfully submitted at least one query after the training day. This yields 469 users, of which 86% are classroom teachers and 14% are head teachers (principals). For simplicity, we refer to users of both types as teachers.\nOne cohort of teachers in the non-profit schools, which we call the survey sample (N=122), was surveyed three times by EducAid. They were surveyed during onboarding (\u2018baseline', in winter 2024), 2-3 months after onboarding (\u2018midline'), and 3-4 months after onboarding (\u2018endline')."}, {"title": "Usage", "content": "Figure 1 Panel B shows the leapfrogging pattern in the survey sample. For teaching activities, more teachers use AI each month than use web search. The figure reports AI usage from chatbot logs, and self reported search and WhatsApp usage. In Figure S2 we report alternate measures. Self-reported AI usage is even higher. More teachers report consulting the AI chatbot for information about teaching (77.4%) than the internet (37.0%); the proportion reporting consulting the chatbot is not statistically significantly different from the proportion consulting books (68.5%). High usage of an AI chatbot is not unique to our intervention; it is also observed in a study with Kenyan entrepreneurs (11).\nWe break down patterns of AI usage in more detail in Figure 2. The median teacher uses the chatbot once per month on average and submits 3 queries per month; however, the top 10th percentile of teachers use it 5 days and submit 21 queries per month, as shown in Panel A.\nPanel B shows a plot of retention, with the proportion of users submitting a query each month after being onboarded onto the system after a given number of months. 70% of onboarded users are active in the first month; this decays over time to roughly 25% active per month. Most usage occurs during school hours, but there is also substantial usage at night, suggesting it is used for planning, as shown in Panel C.\nIn an endline question, 40% of respondents reported sharing their phones with other teachers to allow them to use the chatbot, suggesting they found it valuable."}, {"title": "Heterogeneity", "content": "We assess heterogeneity in usage in Fig. 2 Panel D. Differences are not statistically significant, but we find suggestive patterns. The small number of teachers with a bachelor's degree use the assistant 130% more per month on average. This pattern of usage corroborates other work in low income countries that find AI assistance benefits higher performing entrepreneurs more (11), but contrasts with several studies in high income countries finding that AI assistance benefits lower skilled workers more (13, 14). Classroom teachers use the chatbot slightly more than head teachers.5 Women are slightly less likely to use the chatbot than men, echoing other studies (15). Teachers who used WhatsApp more frequently at baseline use the chatbot more. In contrast, it is the teachers who rarely use search at endline who use the chatbot more. This pattern would be consistent with the two technologies being substitutes, or with the chatbot working for teachers for whom search is less useful."}, {"title": "Threads", "content": "Some of the messages sent by teachers continue the existing conversation and some start new conversations. The WhatsApp interface does not have a button to start a new conversation, so we divide each user's interactions with the chatbot into separate threads, defined as conversations aimed at a common goal. We divide queries based on whether they referenced previous messages or the discussion, using a language model. We also count a message as starting a new conversation if it is separated by more than 48 hours. Based on this classification, 66% of queries start new conversations."}]}, {"title": "Tasks and Subjects", "content": "We next categorize the threads into tasks and subjects. We do this in a three step process. We first send the queries to a language model to suggest a list of categories. We manually edit this list to simplify and remove duplicates. We then send each thread to a language model and ask it to classify each one.\nThe largest task category is for facts and concept clarification (69%), followed by lesson planning and assessment (11%), writing support (5%), and professional development and classroom management (2%), as shown in Figure 3 Panel A. Thus, the largest category of queries represent retrieval requests that one might have expected search engines to be effective at answering.\nWe also apply a similar classification algorithm to the subjects of threads. These span the curriculum, as shown in Figure 3 Panel B."}, {"title": "AI versus Search", "content": "We next consider how AI compares to search.", "subsections": [{"title": "Self-assessment", "content": "The endline survey asked surveyed teachers how they determined when to use AI and when to use search; results are shown in Figure 4. When asked about times they use theTeacher.AI instead of search, all respondents reported at least one reason they did so; however, only 45% reported at least one reason when asked about times they used search instead of theTeacher.AI. The most common reasons to use AI were that chatbot gives a useful answer right away (85%), followed by that it is more concise (78%), more trustworthy (72%), creates content that cannot be found on the web (66%), or works better with the level of internet access (65%). Although 58% of teachers note AI's ability to ask follow ups, this is only occasionally used in our data. The most common reason that a respondent would use search instead of AI is that the web has content that the chatbot does not (18%). As a free response to this question, 5 users (3.4%) reported that since receiving access, they have stopped using web search and now instead use the Teacher.AI."}, {"title": "AI versus search results", "content": "We compare AI and search more systematically by analyzing the queries themselves. We take the queries submitted to the chatbot, and submit them to the local version of Google (google.com.sl), which is estimated to have 97% of search engine market share in the country (16). We use several settings to mimic the results that would be seen on a mobile phone physically located in Sierra Leone. We use a distributed virtual private network (VPN) service, ensure that associated IP addresses are geolocated in Sierra Leone, and mimic the browser characteristics of a lower-priced Android mobile device common in sub-Saharan Africa.\nWe consider only the first queries asked in each thread to give both technologies similar context, since otherwise AI would respond to natural language follow ups differently than search. We omit queries that are 'fillers', such as introductions or greetings. We compute the following statistics for a 12.5% random subsample of queries.\nVery few of the search results are local content. Only 2.2% of returned web sites are at a Sierra Leonean domain (.sl), and few are from similar countries at all: 86% are US domains (.com, .org, .edu, .net, .gov) and 3% are UK domains, as shown in Figure 5. A breakdown by domain is given in Table S1."}, {"title": "Format", "content": "Al responses are shorter by an order of magnitude (mean word count of 156 versus 3,311).7 Both because Al responses are shorter, and because they are text (without syntax, styles, images, ads, and code), they require transmitting much less data, as shown in Figure 6. The average data needed to transfer an Al response is 0.80 kilobytes (KB), relative to 2,499 KB for a search result. Thus, a search result requires 3,107 times more data to load than an AI response on average. The data used by an individual search result is in addition to the data needed to transfer for the list of search results in the first place. Larger file sizes not only incur larger transfer costs, they are also challenging to download on slow and intermittent networks."}, {"title": "Cost", "content": "The number one reported limiter that prevents individuals in sub-Saharan African countries from using the internet more is cost (5), as shown in Figure S3. Web pages use much more mobile data to download, but AI responses cost more to compute. For a given query q, we compute the cost of loading a single search result as\n$C_{search}(q) = p_{transfer}GB(x_{search}(q))$\nand the cost of an AI response as\n$C_{AI}(q) = p_{transfer}GB(x_{AI}(q)) + p_{compute,in}Tokens(S + q) + p_{compute,out}Tokens(x_{AI}(q))$,\nwhere $x_j()$ represents the content retrieved by method j in response to a query, $GB(\\cdot)$ counts the bandwidth required to transmit that content, $Tokens(\\cdot)$ computes the amount of AI compute required for the query and system message, and S represents the total tokens used for the system message. Prices include the cost per gigabyte of data transfer ($p_{transfer}$), per AI input token ($p_{compute,in}$), and per AI output token ($p_{compute,out}$)."}, {"title": "Content", "content": "Search results contain similar information as Al responses. We assess this by sending each query, Al response, and search result to a large language model. We prompt the model to assess the degree to which the text of the search result contains the main ideas in the Al response, on a scale from 1-10, where 1 indicates that the search result has no core ideas of the AI response, and 10 indicates that it captures all core ideas. We find that 46% of top 5 search results contain most of the core ideas of the AI response (score 6 or above), and for 74% of queries there is a top 5 search result that contains most of the core ideas of the AI response. We additionally assess whether the query itself is conducive to a single correct answer (like a factual question) or multiple possible correct answers (like asking to generate a story) by asking a language model to classify it. By that classification, 48% of queries are conducive to a single correct answer. We plot the distribution of these two variables together in Figure S5 and S6, showing that contains scores are higher for queries rated as having a single correct answer. Overall, these results suggest users are mostly asking the AI system to retrieve information, not generate new content."}, {"title": "Quality", "content": "We next compare the quality of the responses. We recruit 25 teachers from around the world to evaluate them. We draw a random sample of 100 queries, restricting to those for which search results were successfully scraped. For each, we assembled a dataset with the query, the AI response, and extracted text from one of the top 5 search results (selected at random).9\nRespondents were recruited on the Prolific survey platform from a pool of teachers. Each was shown a sequence of 10 queries randomly selected from the 100. Alongside each query, they were shown a single response, and asked to rate it along various dimensions. The response was randomly selected to be either the AI response (2/3 probability) or the extracted search result text (1/3 probability). We oversampled AI responses because they were shorter and thus faster to evaluate, and because we wanted to obtain more precise measures of their factuality. To avoid bias, respondents were blinded as to the origin of the responses, both in general, and for each query. They were informed that in rating the response they could use their own knowledge or consult outside resources, such as books or the web, but were asked not to consult an AI chatbot.\nWe present results in Figure 8. On a five-point scale, respondents rate the Al response as more relevant (+1.23, p-value < 0.001) and helpful (+1.12, p-value < 0.001)."}]}, {"title": "Discussion", "content": "Our results show that the web has profound gaps in low-resource settings. With web search, results are limited to existing content in its existing format. There is little such content tailored for low-resource communities. In contrast, AI can tailor a response, and \u2018interpolate' between preexisting pieces of content. This property makes it possible to provide existing knowledge in more appropriate formats, which can be several orders of magnitude more data-efficient. But it may also generate new content. In our data, we see teachers ask the chatbot to generate stories that have specific characters and morals and are localized to Sierra Leone, content that does not exist on the web. This property of AI can be more useful when content is scarce, such as is the case in much of sub-Saharan Africa. Thus, even if there exists less digital data on the lives and needs of low income communities on which to train AI models (20, 21), AI may be able to extract more value out of what data there exists than previous retrieval methods.\nThe tested chatbot is simple: at this time, it was using an earlier model (GPT 3.5 Turbo) and a prompt that references that teachers have low resources but does not reference Sierra Leone. More advanced models are known to reduce hallucinations, and more tailored prompts and fine- tuning with local data are likely to improve usefulness. Thus, this study should be thought of as illustrating a lower bound for the potential of these models.\nWhen considering broader implications, three caveats should be made. First, this is a population that speaks English, which is the language in which current language models perform best. AI responses are not yet as useful in lower resource languages. Second, teachers use the system as one of many inputs into classroom decisions, and are skilled at considering the veracity of responses. Caution is warranted when allowing AI to directly make high stakes decisions without such review. Third, education may be a complement to using these systems. We found suggestive evidence that teachers with higher education used the system more frequently. During pilots of the system, some teachers struggled to craft queries that the chatbot could usefully answer. Training was helpful. Thus, current AI systems may complement some existing human capital; more refined designs may be necessary for users with less education.\nDespite these caveats, relatively high rates of chatbot usage in low-income countries is not unique to our study. A study of web traffic found that lower middle and lower income countries account for only approximately 15% of search traffic, but approximately 25% of AI chatbot web traffic (22). Our results suggest that AI can already deliver information at one eighth of the cost of the web, which has potentially profound implications for access to information. While much discussion worldwide is about whether and how AI will trigger a revolution of intelligence, this study suggests that one impact of the technology could be to partly catch up low-income countries to the revolution of information."}, {"title": "Methods", "content": "Fab Inc recruited teachers through public schools and through a private school network (EducAid Sierra Leone). The chatbot works only for users explicitly given access. During onboarding, teachers were given a training brochure. The brochure listed possible uses of the system, including for factual questions, generating content, and getting advice (Figure S8a-c). Teachers were informed that conversations with the chatbot would be analyzed, and were asked to not identify themselves or provide confidential information. One cohort, which we call the survey sample, was surveyed three times by EducAid. They were surveyed during onboarding ('baseline', in winter 2024), 2-3 months after onboarding (\u2018midline'), and 3-4 months after onboarding ('endline').", "subsections": [{"title": "Data", "content": "The chatbot backend records each query submitted by a teacher, a pseudonymous identifier for each teacher, a timestamp, and the corresponding response provided by the AI system. Fab worked with EducAid to develop survey questions covering AI awareness, teaching practices, job satisfaction, and demographics. To this, the Columbia University researchers added questions about web search usage. We restrict the sample to teachers and principals in the public schools or EducAid, with Sierra Leonean phone numbers, whose first query was on a known day of teacher training. This omits some users who were given access for testing purposes or other channels. We omit queries from each user's first day, to omit any practice questions submitted during the training session, and include only users who successfully submitted at least one query after the training day.11 The survey sample includes N=166 teachers surveyed at baseline, of which N=122 could be linked to the query data. That declines in the midline (N=152 surveyed, N=113 linked) and endline (N=146 surveyed, N=109 linked). Fab Inc. provided the Columbia research team access to usage and survey data under an agreement that gives the academic research team freedom over publication of results, so long as it does not violate confidentiality or privacy rules."}, {"title": "Analysis", "content": "We assembled the main dataset using a multistep process. Figure S7 shows a diagram of major steps. We report validation exercises for each step in Appendix A. For steps that use a large language model (LLM), we submitted these to OpenAI GPT-40-mini (gpt-40-mini-2024- 07-18), using the prompts described in Appendix B. Steps are as follows:\na. Categorizing queries into threads. TheTeacher.AI does not have a \u201cnew conversation\u201d button, so new conversations are not automatically grouped into separate threads. We used an LLM to categorize queries and AI responses into threads (a conversation aimed at a common goal). For each user, we processed the queries and the chatbot's responses sequentially. Given a current proposed thread, we assessed whether a subsequent query-response pair should be included in the ongoing thread based on whether the new message continued the conversation, referenced previous messages, requested further clarification, or introduced new information relevant to the ongoing discussion. Queries separated by more than 48 hours were considered to"}]}]}