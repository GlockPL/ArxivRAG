{"title": "RTBAgent: A LLM-based Agent System for Real-Time Bidding", "authors": ["Leng Cai", "Junxuan He", "Yikai Li", "Junjie Liang", "Yuanping Lin", "Ziming Quan", "Yawen Zeng", "Jin Xu"], "abstract": "Real-Time Bidding (RTB) enables advertisers to place competitive bids on impression opportunities instantaneously, striving for cost-effectiveness in a highly competitive landscape. Although RTB has widely benefited from the utilization of technologies such as deep learning and reinforcement learning, the reliability of related methods often encounters challenges due to the discrepancies between online and offline environments and the rapid fluctuations of online bidding. To handle these challenges, RTBAgent is proposed as the first RTB agent system based on large language models (LLMs), which synchronizes real competitive advertising bidding environments and obtains bidding prices through an integrated decision-making process. Specifically, obtaining reasoning ability through LLMS, RTBAgent is further tailored to be more professional for RTB via involved auxiliary modules, i.e., click-through rate estimation model, expert strategy knowledge, and daily reflection. In addition, we propose a two-step decision-making process and multi-memory retrieval mechanism, which enables RTBAgent to review historical decisions and transaction records and subsequently make decisions more adaptive to market changes in real-time bidding. Empirical testing with real advertising datasets demonstrates that RTBAgent significantly enhances profitability. The RTBAgent code will be publicly accessible at: https://github.com/CaiLeng/RTBAgent.", "sections": [{"title": "1 INTRODUCTION", "content": "The prominence of online advertising within the broader advertising industry is well-established, serving as a pivotal channel for reaching consumers in the digital era. In 2022, the digital advertising industry in the United States achieved a historic milestone, with total revenue exceeding $200 billion. Programmatic advertising revenue grew by 10.5% year-on-year, reaching $109.4 billion, underscoring its expanding significance in the digital landscape [11]. A notable advancement in online display advertising has been Real-Time Bidding (RTB), which enables the real-time buying and selling of ad impressions during a user's visit. RTB's main advantage is its ability to automate and scale the purchasing process by aggregating extensive inventory from various publishers [22]. It allows for precise targeting of individual users based on real-time behavior, marking a significant shift in digital marketing strategies.\nA key challenge within RTB is the development of effective bidding strategies for advertisers. An optimal bidding strategy should promote products to targeted users without disrupting their experience and enhance revenue for publishers. As illustrated in Table 1, traditional rule-based bidding strategies are often too rigid and need to adapt to the dynamic nature of the market. While reinforcement learning (RL) approaches [5] offer better adaptability, they face issues such as the need for extensive training data, difficulties in achieving training convergence, and a lack of interpretability in decision-making, which affects their applicability, trustworthiness, and stability. Consequently, there is a need for more advanced machine-learning models in the RTB domain.\nRecent advancements in artificial intelligence, mainly through large language models (LLMs), have introduced innovative solutions to various fields, including knowledge-based question answering [3, 15, 16, 29, 34]. However, advertising bidding tasks possess unique competitive characteristics that require heightened awareness and dynamic adjustment. While useful as assistants for bidding and answering, basic LLMs face limitations when directly applied to advertising bidding scenarios. LLM-based agent systems have recently gained attention for their ability to emulate human-like behavior and decision-making. For example, research from Stanford University [17] demonstrates how intelligent agent systems can effectively plan and execute complex tasks.\nOur paper introduces RTBAgent, a novel agent framework designed to address the challenges of competitive advertising bidding environments. RTBAgent is equipped with 4 tools, 3 types of memory, and a two-step decision-making process to execute actions. As outlined in Figure 1, it simulates real-world advertising agency scenarios and enhances real-time bidding tasks through tools such as CTR predictors and various bidding strategies, integrating expert knowledge with insights into impression value and market conditions. RTBAgent features a versatile multi-memory retrieval system that updates and focuses on relevant data, minimizing noise and adapting swiftly to market dynamics. Its two-step decision-making approach enables it to determine optimal bidding prices in real-time. It has been proven that RTBAgent can increase profitability and has great flexibility in LLM selection. Furthermore, RTBAgent stands out in interpretability, providing transparent insights into its decision-making process, which is a significant advantage over conventional methods that often operate as black boxes. With these capabilities, RTBAgent enables more informed and strategically advantageous decisions, helping advertisers achieve better return on investment in highly competitive markets. The combination of high performance and transparency of RTBAgent can not only be superior to traditional models in effect but also provide a new perspective for the interpretable research of bidding tasks in the field of computational advertising.\nOur contributions are summarized as follows:\n\u2022 To our knowledge, our research proposes the first bidding agency system based on LLMs, aimed at solving the bidding optimization problem in online display advertising under budget constraints.\n\u2022 We innovatively propose a two-step decision-making method for RTB, integrating CTR estimation model, expert strategy knowledge, multi-memory retrieval system, and daily reflection to dynamically adjust bidding strategies to cope with the real-time changing market environment.\n\u2022 Extensive experiments validate that our framework performs exceptionally well across all metrics, achieving a significant overall return."}, {"title": "2 RELATED WORK", "content": "2.1 LLM-based Agent Systems\nLLMs are making significant strides towards achieving Artificial General Intelligence (AGI) by enhancing the capabilities of intelligent agents. These models improve autonomy, responsiveness, and social interaction skills, enabling agents to handle complex tasks such as natural language processing, knowledge integration, information retention, logical reasoning, and strategic planning. Recent developments in intelligent agent frameworks, such as Auto-GPT [25] and Metagpt [9], have advanced multi-agent collaboration by incorporating standardized operating procedures (SOPs). These frameworks facilitate research by streamlining agent system integration [7, 10, 13, 24, 32]. For instance, EduAgent [24] integrates cognitive science principles to guide LLMs, enhancing their ability to model and understand diverse learning behaviors and outcomes. Additionally, Agent Hospital [13] utilizes a large-scale language model to simulate hospital environments, enabling medical agents to adapt and improve their treatment strategies through interactive learning.\n2.2 Bidding Optimization in RTB\nRTB has been a critical focus in online advertising [21], aiming to maximize the value of ad placements within a given budget. Traditional methods employ static parameters [18, 26, 27] to optimize revenue, often using historical bid data to set bidding parameters. [27] use linear programming to address these optimization problems. Such methods usually fall short in dynamic bidding environments. Researchers have increasingly framed RTB as a sequential decision problem to overcome these limitations, applying RL techniques to enhance automated bidding strategies [2, 8, 23, 33]. DRLB [23] approaches budget-constrained bidding as a Markov decision process, offering a model-free RL framework for optimization. USCB [8] introduces an RL method that dynamically adjusts parameters for optimal performance, improving convergence rates through recursive optimization. Despite these advancements, RL faces challenges such as training complexity and interpretability, indicating a need for more robust machine learning models in RTB strategies."}, {"title": "3 PROPOSED METHOD", "content": "3.1 Preliminaries\n3.1.1 Problem Formulation. RTB offers various pricing schemes catering to diverse advertiser needs within the online advertising ecosystem. In a second-price auction [20], the advertiser pays the second-highest bid, denoted as ci, for the privilege of displaying their ad after winning the bid with bi as the highest bidding price. Our study focuses exclusively on the scenario within a second-price auction mechanism. It aims to maximize the achieved objective value under a given budget, as this is the most common business requirement in the industry. W.l.o.g., we consider clicks to be our primary aim value, although other key performance indicators (KPIs), such as conversions, can also be adopted. Thus, the advertiser's strategic challenge is to maximize the cumulative value of clicks, subject to budget constraints. Let N represent the total number of ad impression opportunities during a specific time period, such as one day, with each impression opportunity indexed by i. The optimization problem can be mathematically expressed as follows:\n\\max \\sum_{i=1...N} w_i v_i\\\\\ns.t. \\sum_{i=1...N} w_i c_i \\le B,\nwhere vi, ci represent the value, cost of impression i respectively. wi is a binary value indicating winning or losing the impression i. B is the total bidding budget. Zhang et al. [30] proves that in a second-price auction, the optimal bid is a function of a scaling factor \u03bb, which governs the bid price bi as:\nb_i = \\lambda \\cdot v_i.\nUnfortunately, all participating bidders are dynamic, and the auction environment is usually highly non-stationary, which make \u03bb difficult to be determined naively. Thus, the key of our method is to dynamically adjust \u03bb to adapt to the ever-changing market environment.\n3.1.2 Our Solution Paradigm. RTB can be viewed as a series of random events where each bid is influenced by uncertain factors [1]. To address this, we model the RTB scenario using a Markov Decision Process (MDP), a mathematical framework that captures the decision-making process involving probabilistic state transitions. This MDP is characterized by a set of states S that represent the advertising status of a campaign and an action space A including the adjustment parameter at of the feasible bidding factor \u03bb. At each time step t \u2208 {1,\u2026\u2026\u2026, T}, the agent performs actions at \u2208 A based on the current state st \u2208 S to update \u03bb according to its policy \u03c0: S\u2194 A. The state then transitions to a new state according to the transition dynamics T : S \u00d7 A\u2194 \u03a9(S), where \u00d7 represents cartesian product and 2(S) is a set of probability and distributions over S. The environment provides an immediate reward to the agent based on a function of the current state and the agent's actions, denoted as rt: S \u00d7 A \u2192 R CR, where R is a reward space.\nTo this end, the objective is to discover a policy  that links states to actions, intending to maximize the total discounted reward within a set time frame, all while considering budget limitations. The policy \u03c0* that we seek is the one that maximizes the expected cumulative reward, as shown in Eq.(3):\n\\pi_{\\theta}^* = \\arg \\max_{\\pi_{\\theta}} E_{\\pi_{\\theta}} \\sum_{i=0}^T \\gamma^i r_{t+i} | S_t = S\nWe extend this optimization challenge to our RTBAgent, where the policy \u03c0 is defined as:\n\\pi_{\\theta}^* = \\arg \\max_{\\pi_{\\theta}} E_{\\pi_{\\theta}} \\sum_{i=0}^T \\gamma^i r_{t+i} | S_t = S, P_t = p\nwhere p(\u00b7) is a specialized module that encapsulates beneficial internal reasoning processes and the action policy for the RTBAgent is given by:\n\\mathcal{A}_{RTBAgent}(a_t | S_t, P_t) = G(S_t, P_t)\nP_t = p(S_t, \\mathcal{F}_t) \\\\\np(s_t, \\mathcal{F}_{sum}, \\mathcal{F}_{tool}, \\mathcal{F}_{ins}, \\mathcal{F}_{act}, \\mathcal{F}_{ref}),\nwhere G(.) is a operation parsing function used to perform compatible formal operations in the environment. The RTBAgent, powered by LLMs, refines the inference information pt to include various operations, i.e., the summary of memories \\mathcal{F}_{sum}, tools \\mathcal{F}_{tool}, insights \\mathcal{F}_{ins}, actions \\mathcal{F}_{act} and reflections \\mathcal{F}_{ref}, which will be explained specifically later.\nDue to the inherent limitations of LLMs, such as the lack of dominance in continuous value output and insensitivity to numbers, our research refines the operation based on the basic factor Abase obtained from expert bidding strategies. The optimal scaling factor \u03bbt at each time step t is determined through the adjustment action at provided by the strategy \u03c0, i.e., At = Abase (1 + at). Integrating these operations allows the RTBAgent to continuously interact with the bidding environment during training, driving it towards the optimization goal as expressed in the following equation:\n\\mathcal{R}_{RTBAgent} = \\arg \\max_{\\pi} E_\\pi \\sum_{i=0}^T \\gamma^i r_{t+i} | S_t = S, P_t = p\\\\\ns.t. \\pi(a_t | S_t, P_t) = G(s_t, p_t) \\text{ with Eq.(5)} \\forall t,\nwhere the reward of RTBAgent comes from the comprehensive information of decision results and self-reflection, achieving self alignment[28]. This approach ensures that RTBAgent's actions are meticulously aligned with the policy that delivers the highest expected return on bids, taking into account the current state and decision insights at each juncture."}, {"title": "3.2 Overall Framework", "content": "The RTBAgent framework, as shown in Figure 1, mirrors the operational structure of a real-world bidding firm. It integrates a comprehensive set of bidding analysis tools H, alongside a well-defined profile, action set A, and memory set M. The RTBAgent is guided by a configuration file that imbues it with the acumen of a bidding specialist, grounded in a context crafted for its role. It leverages bidding analysis tools to meticulously evaluate the potential value of ad impression requests, drawing on current bidding conditions and request data to proffer expert-guided bidding strategies. The memory module is designed to offer a robust, multi-dimensional retrieval system. It segments and updates information incrementally, ensuring the agent has access to accurate and relevant data. Additionally, the RTBAgent features a reflection module that it uses for regular decision reviews, thereby cultivating valuable insights for enhancing subsequent actions. Central to the RTBAgent is the action module, which employs a two-step decision-making process to formulate and execute well-considered actions. This process is pivotal for determining the optimal bidding price, ensuring that the agent's actions are strategically aligned with the goal of achieving the highest return on investment in the bidding process."}, {"title": "3.3 Environment", "content": "The dynamics of a bidding environment are characterized by the continuous emergence of new data following each round of bidding, which is a concrete representation of the current state st. This data encompasses a variety of metrics, such as the current volume of bids, historical success rates in securing bids, prevailing market prices, the average cost per bid, the total budget allocated for bidding, and the remaining budget. These elements are crucial as they provide a comprehensive snapshot of the bidding landscape at any given time. Including such detailed environmental information is instrumental for the RTBAgent's decision-making process. It allows the system to not only assess the immediate bidding scenario but also to anticipate and adapt to potential shifts in the market dynamics. This real-time analysis and understanding of the bidding environment are essential for the RTBAgent to make informed and strategic decisions."}, {"title": "3.4 Components of Agent", "content": "3.4.1 Profile. To help LLMs understand the bidding process, we define the profile of our RTBAgent as follows,\nYou are a senior data analyst specializing in in-depth research and strategy development in the field of real-time bidding (RTB) advertising placement. You use advanced data analysis tools and algorithms to guide advertisers to gain an advantage in fierce market competition...\nby presenting the problem background, its own role, and action goals in the form of text, it can better perform the reasoning process for specific tasks.\n3.4.2 Tools. We incorporate a various tool set H for RTBAgent, including a click-through rate (CTR) prediction model and bidding decision strategies. We use Factorization Machines (FM)[19] as the CTR prediction model, which is widely used and can estimate the value for each impression stream. A separate CTR prediction model is trained for each advertiser within this framework. Additionally, we use a series of rule-based strategies to complement expert knowledge, including MCPC [12], LIN [18], and LP [4], which will be further discussed in the Overall Comparison section. These methods are widely used in the industry, based on prior knowledge, and demonstrate high referential value in an offline environment. It is important to note that in the bidding decision-making process, we only need to base our decision on one of the bidding decision models to assist in the decision-making.\n3.4.3 Actions. Due to the uncontrollable and unreliable nature of generative LLMs in predicting consecutive bidding prices, we suggest changing this process to allow the LLMs to predict an adjustment factor. Specifically, we define a adjustment space with an observation range from -0.5 to 0.5. This enables the RTBAgent to adjust the expert knowledge suggested decisions based on the current state and historical decisions in order to better adapt to the dynamic environment.\n3.4.4 Memories. In the RTBAgent, we design three types of memory: environment memory Menu, bidding memory Mbid, and reflection memory Mref, where {Menu, Mbid, Mref} \u2282 M. Specifically, Menu stores the market environment after each decision, allowing the RTBAgent to refer to historical data to make wiser decisions when facing new bidding opportunities. Mbid records the bidding behaviors and reasons in different market environments. By analyzing Mbid, the RTBAgent can identify which strategies are more effective in specific situations, optimizing and adjusting future bids. Additionally, Mbid can help the RTBAgent recognize potential patterns and trends, such as specific bidding strategies that are more likely to succeed under certain conditions. Mref is the RTBAgent's self-assessment mechanism, recording the reflection process and results after each decision. Mref enables the RTBAgent can to understand why some decisions did not achieve the expected effects, thus avoiding similar mistakes in the future. The core of Mref lies in continuous learning and improvement, ensuring that the RTBAgent can maintain competitiveness in the ever-changing market environment."}, {"title": "3.5 Workflow of RTBAgent", "content": "This section elucidates the operational sequence of the RTBAgent, encompassing three pivotal stages: information gathering, two-step decision-making, and daily reflection.\n3.5.1 Information Gathering. At the heart of RTBAgent's functionality is the aggregation of pertinent data. It should be noted that each action and environmental feedback record will be saved in real time to generate memory. Throughout the bidding process, extensive logs are generated, encompassing decisions, reflections, and environmental contexts. The synthesis of these logs into a summarized memory is pivotal for informed decision-making. The summarized memory at any given time step t, denoted as F\u015fum, is formulated by integrating information from bidding, reflective, and environmental memories:\n\\mathcal{F}_{sum} = \\sum_{i=bid,env,ref} LLM(\\mathcal{P}_{sum} (m_i^t)),\nwhere \u2211 is a concatenation operation of multiple strings,  is a prompt template for information gathering and mi \u2208 Mi is the memory for the type i at time step t.\nAdditionally, the RTBAgent leverages a suite of tools, encapsulated within Ftool, to provide foundational insights for two-step decision-making. Central to this are the CTR estimations, represented by Vt, and strategic bidding recommendations, denoted as Abase. Therefore, the output of the tool can be represented by a pair (Vt, Abase). Here, Vt = {, ,\u2026\u2026, vt } is derived from a trained predictive model, forecasting the potential user engagement by the impression feature vector X\u2081 = {x}, x\u00b2,...,xat }, while Abase is algorithmically determined based on historical data, and dt is the number of impressions at the current time step t, and satisfies the following:\n\\sum_{t=1}^T d_t = N,\nwhere N is the total number of ad impression opportunities during one day.\n3.5.2 Two-Step Decision-Making. The RTBAgent's two-step decision-making process plays a crucial role in its strategic capabilities. The first step involves insight reasoning, represented by Fins, and is responsible for analyzing potential decision ranges, including their benefits, drawbacks, possible outcomes, and associated risks. This step can be formulated as follows:\n\\mathcal{F}_{ins} = LLM(\\mathcal{P}_{ins}(s_t, \\mathcal{F}_{sum}, A_{base})),\nwhere  is a prompt template for insight reasoning.\nFollowing the insight reasoning, the action making step, represented by Fact, is where the actual bidding action at and its reason reat are determined. The output of this step is a binary tuple (at, reat), which is generated by the following representation:\n\\mathcal{F}_{act} = LLM(\\mathcal{P}_{act}(s_t, \\mathcal{F}_{sum}, \\mathcal{F}_{ins}, A_{base})),\nwhere  is a prompt template for action making. This step is very important as it turns the gathered insights into a specific action. In the context of RTBAgent, this action is deciding on the bidding price for an ad impression. The final bidding price bi for the current impression is calculated using the following formula:\nb_i = A_{base} (1 + a_t),\nwhere bi is the bidding price for the impression opportunity i at time step t. This formula reflects the dynamic nature of the bidding process, allowing the agent to adjust its bids in real-time based on current analysis and historical data, thus optimizing its bidding strategy.\n3.5.3 Daily Reflection. Post the daily bidding cycle, RTBAgent engages in a process of introspection, encapsulated by Fref, ef to consolidate and reflect upon the day's decisions and their outcomes. This reflective process is integral to the continuous improvement of the agent's strategic acumen:\n\\mathcal{F}_{ref} = \\sum_{i=bid,env,ref} LLM(\\mathcal{P}_{ref} (m_i^t)),\nwhere pref (\u00b7) is a prompt template for daily reflection. This cyclical reflection ensures that the agent learns from its experiences, thereby refining its approach for enhanced performance in subsequent bidding endeavors."}, {"title": "4 EXPERIMENTS", "content": "4.1 Datasets\nWe conduct a detailed study on the performance of RTBAgent using the iPinYou dataset [14]. The iPinYou dataset is provided by iPinYou Corporation, a prominent e-commerce advertising technology company in China. The dataset includes real-time bidding advertising data over a 10-day period in 2013, covering nine different advertising campaigns. Specifically, it contains 19.5 million ad displays, 14,790 clicks, and a total advertising cost of 16,000 RMB. These data not only portray the market environment, but also provide a complete path of user response from the advertisers' perspective. The records in the dataset are organized with each line representing three types of information: auction and ad features, auction winning price, and user click feedback on ad displays. Additionally, all monetary values are in RMB, corresponding to the cost-per-thousand-impressions (CPM) pricing model. The test data is derived from the final three days of each campaign, while the remaining data is used for training, as reported by the data publisher [14].\n4.2 Evaluation Procedure\nIn our study, we specifically focus on the number of clicks as the KPI for evaluation. For each advertising campaign, we allocate the budget on a daily basis and divide each day into 24 time steps, representing each hour. The bidding model iterates the test dataset using the same CTR estimator. For each bidding request, the strategy generates a bid price that does not exceed the current budget. If this bid price is equal to or higher than the market price, the advertiser wins the auction, incurs the market price as a cost, and gains user clicks as a reward. Then, the remaining auction quantity and budget are updated. In order to prevent a situation where the budget is too high, and all bids are won, the budget limit cannot exceed the total historical cost of the test data. To examine the performance of bidding strategies under different budget constraints, we evaluate using three different budgets: 1/2, 1/8, and 1/32 of the total budget.\n4.3 Implementation Details\nWe utilize a single NVIDIA RTX A6000 GPU to train the RL model in our benchmark tests to ensure consistency in the training environment. All comparison schemes employed FM for CTR estimation. The FM model includes a linear layer for mapping features to the output space, a bias term for baseline prediction adjustment, and a feature embedding layer for capturing feature interactions. Finally, the sigmoid function converts the output into a probability form to predict the likelihood of user clicks. During the testing phase, the derivation of basic factor Abase by RTBAgent, based on expert strategies, is exclusively derived from the train set.\n4.4 Overall Comparison\nTo verify the effectiveness of our model on real-world dataset, we compare it with the following competitive methods.\nMCPC determines the maximum effective cost-per-click(CPC) for each advertising campaign by dividing the cost by the number of clicks in the training data, using this value as a parameter for bidding.\nLIN is a linear bidding method where the bid value is linearly proportional to the estimated CTR de. The bid for a single impression is formalized as  where \u03b80 is the average CTR in the"}, {"title": "4.5 Ablation Study", "content": "In Table 3, we introduce three models, MCPC+, LIN+, and LP+, to demonstrate the performance enhancement achieved by RTBAgent with three expert strategies. Specifically, MCPC+, LIN+, and LP+ correspond to RTBAgent based on three different expert strategies, respectively: MCPC, LIN, and LP. Regardless of which method is used, the performance of RTBAgent on the test set consistently improved based on the original performance. It is observed that the stronger the expert strategy performance provided, the better the final results.\nAs shown in Figure 3, LP consumed too much budget in the early stages and did not anticipate the benefits of spending the budget in the later stages. On the contrary, LP+ can better control the expenditure of the budget, allowing for the purchase of high-quality clicks with a surplus budget in the second half of the process. In addition, LP+'s CPC has always been lower than the expert strategy throughout the process to ensure that more clicks are obtained within the specified budget. This suggests that RTBAgent effectively utilizes the guidance knowledge of expert strategies, leading to enhanced performance through insight into the environment and interaction."}, {"title": "4.6 Visualization and Analysis of Performance", "content": "Figure 2 demonstrates how RTBAgent utilizes LLMs to enhance its reasoning process during real-time bidding, specifically for advertiser 2997 on the last step of October 27th, 2013. The figure illustrates the core workflow, starting with information gathering, where RTBAgent analyzes historical bidding data, identifying that a reasonable increase in the bidding factor has effectively improved visibility and click volume. This data analysis is the foundation for the next stage, a two-step decision-making process. In the first step, RTBAgent performs insight reasoning by analyzing current market conditions and historical data to extract strategic insights, such as recommending a 0.15 adjustment to the bid factor based on past performance. In the second step, RTBAgent translates these insights into actionable decisions by adjusting the bid price, aiming to optimize performance while maintaining stability. After each bidding cycle, RTBAgent engages in daily reflection, where it evaluates the outcomes of its decisions, compares expected results with actual performance, and updates its strategy accordingly. This iterative process allows RTBAgent to continuously improve its bidding approach, adapting to market dynamics and making more informed, strategic decisions in future cycles. The visualization in Figure 2, along with the data presented in Table 5, underscores RTBAgent's capability to refine the decision-making process in RTB scenarios.\nThe detailed analysis of a single hour for advertiser 2997 on October 27th, 2013, demonstrates RTBAgent's ability to learn from daily operations and continuously improve its bidding strategies.\nFurthermore, we invited 10 experts in the advertising field to rate the generated decisions and reasons on three levels: -1, 0, and 1. Finally, RTBAgent received a positive rating of 97%, indicating that the proposed RTBAgent's output is convincing enough."}, {"title": "4.7 Discussion of Reasoning Costs and Benefits", "content": "In advertising bidding systems, methods that rely on RL or rules are typically triggered at longer intervals, such as every 15 minutes or more. Despite the inference time for LLMs being slightly longer than these traditional methods, it is still adequate to satisfy the typical latency requirements of advertising services in practice. As shown in Table 4, our framework outperforms other baselines when using LLMs of various scales, ranging from big to small, whether they are open-source LLMs or closed-source LLMs. This performance advantage allows advertisers to select the LLMs that best fit their deployment needs, reducing the concern over inference cost. Additionally, as demonstrated in Figure 2, our framework delivers not only reliable results but also provides data-driven insights and decision-making rationale, which is invaluable for operations teams when assessing the effectiveness of advertising campaigns and refining future strategies. Consequently, our proposed solution not only fulfills the fundamental requirements for inference time and cost but also generates direct economic benefits and indirect operational advantages."}, {"title": "5 CONCLUSIONS", "content": "In this paper, we introduce RTBAgent, an effective agent that first utilizes LLMs to enhance advertising auctions in RTB. Specifically, we innovatively propose a two-step decision-making process that integrates CTR estimation model, expert strategy knowledge, multi-memory retrieval system, and self-reflection, providing accurate simulation and real-time decision support for bidding scenarios. The extensive experimental results confirm that RTBAgent exhibits better adaptability and interpretability than traditional rule-based and RL methods in highly dynamic and unstable bidding environments. Our work contributes to developing a novel paradigm that attempts to explore the application of LLM-based integrated intelligence in RTB and spark related discussions. In future research, in order to better fit the competitiveness of the online advertising market, we will focus on studying the application of multi-agent systems based on LLMs in RTB. It can be foreseen that multi-agent systems will become more complex and effective."}, {"title": "6 LIMITATIONS", "content": "Our approach does have some limitations. Firstly, while LLM-based methods have achieved improvements in effectiveness, the response time of LLM-based bidding systems is not as swift as desired. Employing models with smaller parameter sizes, such as 1B or 3B, appears to be a promising direction. Secondly, current LLMs have not yet encompassed a richer set of bidding knowledge. Utilizing strategies like RAG might lead to greater enhancements, which is on our agenda for the future. Thirdly, we have deployed this system into a real-world advertising bidding environment, and we plan to disclose the revenue in the future."}, {"title": "8.1 Partial Display of Prompt Template for RTBAgent", "content": "Due to space limitations, only a portion of the prompt templates for RTBAgent is displayed. We will organize and publish the remaining codes and prompt templates soon.\nProfile Definition Template of Insight Reasoning.\n\"", "adjustment range\" based on\n18 historical decisions, current environmental conditions, and algorithm suggestions.\n19 The selection space for \"adjustment range\" is from {{[-0.5,-0.4), [-0.4,-0.3), [-0.3,-0.2),\n20 [-0.2,-0.1), [-0.1,0.0), [0.0,0.1), [0.1,0.2), [0.2,0.3), [0.3,0.4), [0.4,0.5]}}.\n21 While making your analysis, consider the following:\n1. **Historical Performance**: Adjustments that have previously optimized budget usage and improved\nclick volume without significantly impacting the win rate are valuable.\n2. **Exploration of New Adjustments**: Exploring new adjustment ranges, especially positive\nadjustments range like [0.0,0.1), [0.1,0.2), and [0.2,0.3), can potentially uncover more\neffective strategies. Increasing the bid may improve visibility and click volume, particularly in\ncompetitive environments.\n3. **Balancing Stability and Innovation**: Strive to balance between maintaining strategies that have\nshown consistent performance and exploring new adjustments. A mix of historical strategy and new\nexploration, with a focus on positive adjustments, could provide a balanced approach to ensure\ncost efficiency, maximize clicks, and adapt to market changes.\n23 Please output a JSON in the following format for all analyses:\njson\n24\n25 {{\n26 \"adjustment range for [-0.5-0.4)\": str = xx,\n27 \"adjustment range for [-0.4-0.3)\": str = xx,\n28 \"adjustment range for [-0.3,-0.2)\": str = xx,\n29 \"adjustment range for [-0.2,-0.1)\": str = xx,\n30 \"adjustment range for [-0.1,0.0)\": str = xx,\n31 \"adjustment range for [0.0,0.1)\": str = xx,\n32 \"adjustment range for [0.1,0.2)\": str = xx,\n33 \"adjustment range for [0.2,0.3)\": str = xx,\n34 \"adjustment range for [0.3,0.4)\": str = xx,\n35 \"adjustment range for [0.4,0.5]\": str = xx\n36 }}\n37": "\nProfile Definition Template of History Bidding Summary.\n\"", "or\nexplanations": "njson\n6 {{\n\"summary\": str = xx\n8 }}\n9"}]}