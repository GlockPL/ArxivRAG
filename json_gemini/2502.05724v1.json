{"title": "Rethinking Link Prediction for Directed Graphs", "authors": ["Mingguo He", "Yuhe Guo", "Yanping Zheng", "Zhewei Wei", "Stephan G\u00fcnnemann", "Xiaokui Xiao"], "abstract": "Link prediction for directed graphs is a crucial task with diverse real-world applications. Recent advances in embedding methods and Graph Neural Networks (GNNs) have shown promising improvements. However, these methods often lack a thorough analysis of embedding expressiveness and suffer from ineffective benchmarks for a fair evaluation. In this paper, we propose a unified framework to assess the expressiveness of existing methods, highlighting the impact of dual embeddings and decoder design on performance. To address limitations in current experimental setups, we introduce DirLinkBench, a robust new benchmark with comprehensive coverage and standardized evaluation. The results show that current methods struggle to achieve strong performance on the new benchmark, while DiGAE outperforms others overall. We further revisit DiGAE theoretically, showing its graph convolution aligns with GCN on an undirected bipartite graph. Inspired by these insights, we propose a novel spectral directed graph auto-encoder SDGAE that achieves SOTA results on DirLinkBench. Finally, we analyze key factors influencing directed link prediction and highlight open challenges. The code is available at here.", "sections": [{"title": "1. Introduction", "content": "A directed graph (or digraph) is a type of graph in which the edges between nodes have a specific direction. These graphs are often used to model real-world asymmetric relationships, such as \"following\" and \"followed\" in social networks or \u201clink\u201d and \u201clinked\u201d in web pages. Directed graphs reflect the inherent directionality of relationships and provide a more accurate representation of complex systems. Link prediction is a critical and common task for directed graphs with diverse real-world applications. Examples include predicting follower relationships in social networks, recommending products in e-commerce, and detecting intrusions in network security."}, {"title": "2. Rethinking Directed Link Prediction", "content": "In this section, we will first revisit the link prediction task for directed graphs and introduce a unified framework to assess the expressiveness of existing methods. Next, we examine the current experimental setups for directed link prediction and highlight several associated issues.\nNotation. We consider a directed, unweighted graph G = (V, E), with node set V and edge set E. Let n = |V| and m = |E| represent the number of nodes and edges in G, respectively. We use A to denote the adjacency matrix of G, where $A_{uv}$ = 1 if there exists a directed edge from node u to node v, and $A_{uv}$ = 0 otherwise. The Hermitian adjacency matrix of G is denoted by H and defined as H = $A_{s}$ exp (i\u0398). Here, $A_{s}$ = A \u222a $A^{T}$ is the adjacency matrix of the undirected graph derived from G, and \u0398 = A \u2013 $A^{T}$ is a skew-symmetric matrix. We denote the out-degree and in-degree matrices of A by $D_{out}$ = diag(A1) and $D_{int}$ = diag($A^{T}$1), respectively, where 1 is the all-one vector. Let X \u2208 $R^{n \\times d'}$ denote the node feature matrix, where each node has a d'-dimensional feature vector."}, {"title": "2.1. Unified Framework for Directed Link Prediction", "content": "The link prediction task on directed graphs is to predict potential directed links (edges) in an observed graph G', with the given structure of G' and node feature X. Formally,\nDefinition 2.1. Directed link prediction problem. Given an observed graph G' = (V, E') and node feature X, the goal of directed link prediction is to predict the likelihood of a directed edge (u, v) \u2208 E* existing, where $E^{*} \\subset (V \\times V) \\setminus E'$. The probability of edge (u, v) existing is given by\np(u, v) = f(u \u2192 v | G', X).                                                                                                                            (1)\nThe f(\u00b7) denotes a prediction model, such as embedding methods or GNNs. Unlike link prediction on undirected graphs, for directed graphs, it is necessary to account for directionality. Specifically, p(u, v) and p(v, u) are not equal; they represent the probability of a directed edge existing from node u to node v, and from node v to node u, respectively. To evaluate the expressiveness of existing methods for directed link prediction, we propose a unified framework:\n($\\theta_{u}$, $\\phi_{u}$) = Enc(G', X, u), \u2200u \u2208 V,                                                                                                            (2)\np(u, v) = Dec($\\theta_{u}$, $\\phi_{u}$, $\\theta_{v}$, $\\phi_{v}$), \u2200(u, v) \u2208 E*.                                                                                                         (3)\nHere, Enc(\u00b7) represents an encoder function, which includes various methods described in the Sec. 1. And $\\theta_{u}$\u2208 $R^{d_{\\theta}}$, $\\phi_{u}$ \u2208 $R^{d_{\\phi}}$ are real-valued dual embeddings of dimensions $d_{\\theta}$ and $d_{\\phi}$, respectively. Dec(\u00b7) is a decoder function tailored to the specific encoder method. This framework unifies existing methods for directed link prediction, as summarized in Table 2. More details are provided in Appendix D. Based on this framework, we have the following theorem."}, {"title": "2.2. Issues with Existing Experimental Setup", "content": "The existing directed link prediction experimental setups can be broadly categorized into two types. The first is the multiple subtask setup, which includes existence prediction (EP), direction prediction (DP), three-type prediction (3C), and four-type prediction (4C). This approach treats directed link prediction as a multi-class classification problem that requires the prediction of positive, inverse, bidirectional, and nonexistent edges. More details are provided in Appendix E.1. This setup is widely adopted by existing methods.\nThe other category is the non-standardized setting defined in various papers. These settings involve different datasets, inconsistent splitting meth-"}, {"title": "3. New Benchmark: DirLinkBench", "content": "In this section, we introduce a novel robust benchmark for directed link prediction, DirLinkBench, offering three principal advantages: 1) Comprehensive coverage, incorporating seven real-world datasets spanning diverse domains and 15 baseline models; 2) Standardized evaluation, establishing a unified framework for dataset splitting, feature initialization, and task settings to ensure fairness and reliability; and 3) Modular extensibility, built on PyG to enable easy integration of new datasets, model architectures, and configurable modules (e.g., feature initialization, decoder types, and negative sampling). We then detail the implementation of DirLinkBench.\nDataset. We select seven publicly available directed graphs from diverse domains. These datasets include two citation networks, Cora-ML and CiteSeer; two co-purchasing networks, Photo and Computers; a weblink network, WikiCS; and two social networks, Slashdot and Epinions. These directed graphs differ in both size and average degree. Except for Slashdot and Epinions, each dataset includes original node features. We provide the statistical details and additional descriptions in Appendix G.1. To establish standard evaluation conditions for link prediction methods, we preprocess these datasets by eliminating duplicate edges and self-loop links. Following , we also removed isolated nodes and used the largest weakly connected component."}, {"title": "Task setup", "content": "We simplify directed link prediction to a binary classification task: the model determines whether a directed edge exists from u to v. Specifically, given a preprocessed directed graph G, we randomly split 15% of edges for testing, 5% for validation, and use the remaining 80% for training, and ensure that the training graph G' remains weakly connected. For testing and validation, we sample an equal number of negative edges under the full graph G visible, while for training, only the training graph G' is visible. To ensure fairness, we generate 10 random splits using fixed seeds, and all models share the same splits. The model learns from the training graphs and feature inputs to compute p(u, v) for test edges. Feature inputs are provided in three forms: original node features, in/out degrees from the training graph G' , or a random normal distribution matrix .Baseline. We carefully select 15 state-of-the-art baselines, including three embedding methods: STRAP , ODIN , ELTRA; a basic method MLP; three classic undirected GNNs: GCN , GAT , APPNP; four single real-valued methods: DGCN , DiGCN , DiGCNIB , DirGNN; two complex-valued methods: MagNet, DUPLEX; a gravity-inspired method: DHYPR; and a source-target GNN: DIGAE. We exclude some recent approaches (e.g., CoBA, BLADE) due to unavailable code."}, {"title": "Baseline Setting", "content": "For the baseline implementations, we rely on the authors' original released code or popular libraries like PyG and the PyTorch Geometric Signed Directed library. For methods without released link-prediction code (e.g., GCN, DGCN), we provide various decoders and loss functions, while for methods with available link-prediction code (e.g., MagNet, DiGAE), we strictly follow the reported settings. We tune hyperparameters using grid search, adhering to the configurations specified in each paper. Further details of each method are in Appendix G.1.\nResults. We report results on seven metrics-Hits@20, Hits@50, Hits@100, MRR, AUC, AP, and ACC (detailed metric descriptions are provided in Appendix G.2), with complete results for each dataset in Appendix H. Table 4 highlights the Hits@100 results. These results first show that embedding methods retain a strong advantage even without feature inputs. Second, early single real-valued undirected and directed GNNs also perform competitively, while newer directed GNNs (e.g., MagNet, DUPLEX, DHYPR) exhibit weaker performance or scalability issues. This is because complex-based methods have unsuitable loss functions and decoders, while DHYPR's preprocessing requires O($Kn^{3}$) time and O($Kn^{2}$) space complexity. We provide further analysis in Sec. 5. Notably, DiGAE, a simple directed graph auto-encoder, emerges as the best performer overall, yet it underperforms on certain datasets (e.g., Cora-ML, CiteSeer), leading to a worse average ranking. This observation motivates us to revisit DiGAE's design and propose new methods to enhance directed link prediction."}, {"title": "4. New Method: SDGAE", "content": "In this section, we first revisit the mode structure of DiGAE and analyze its encoder graph convolution. We then propose a novel Spectral Directed Graph Auto-Encoder (SDGAE)."}, {"title": "4.1. Understand the Graph Convolution of DIGAE", "content": "DiGAE is a graph auto-encoder designed for directed graphs. Its encoder graph convolutional layer is denoted as\n$S^{(l+1)} = \\sigma (\\alpha \\hat{D}_{out}^{-1} \\hat{A} S^{(l)} W_{S}^{(l)}),$                                                                           (4)\n$T^{(l+1)} = \\sigma (\\beta \\hat{D}_{in}^{-1} \\hat{A}^{T} T^{(l)} W_{T}^{(l)}).$                                                                   (5)\nHere, $\\hat{A} = A + I$ denotes the adjacency matrix with added self-loops, and $\\hat{D}_{out}$ and $\\hat{D}_{in}$ represent the corresponding out-degree and in-degree matrices, respectively. $S^{(l)}$ and $T^{(l)}$ denote the source and target embeddings at the l-th layer, initialized as $S^{(0)} = T^{(0)} = X$. The hyperparameters \u03b1 and \u03b2 are degree-based normalization factors, \u03c3 is the activation function (e.g., ReLU), and $W_{S}^{(l)}$, $W_{T}^{(l)}$ represents the learnable weight matrices."}, {"title": "4.2. Spectral Directed Graph Auto-Encoder (SDGAE)", "content": "Building on our understanding of DiGAE, we identify two main drawbacks. 1) DiGAE struggles to use deep networks for capturing richer structural information due to excessive learnable weight matrices (as highlighted in a recent study on deep GCNs. We validate this issue experimentally in the next section. 2) It is difficult to optimize parameters due to heuristic hyperparameters. To address these issues, we propose SDGAE, which uses polynomial weights inspired by spectral-based GNNs and incorporates symmetric normalization of the adjacency matrix. The convolutional layer of SDGAE is defined as: [$S^{(l+1)}$, $T^{(l+1)}$] =\n$ \\omega_{S}^{(l)} \\left[  \\begin{array}{cc}  0 & A \\\\  A^{T} & 0  \\end{array}  \\right] \\left[  \\begin{array}{c}  S^{(l)} \\\\  T^{(l)}  \\end{array}  \\right] + \\left[  \\begin{array}{c}  S^{(l)} \\\\  T^{(l)}  \\end{array}  \\right],$                                            (7)\nwhere $\\omega_{S}^{(l)}$, $\\omega_{T}^{(l)} \\in R$ are learnable scalar weights, initialized to one. $\\tilde{A} = \\tilde{D}_{out}^{-1/2} \\tilde{A} \\tilde{D}_{in}^{-1/2}$ denotes the normalized adjacency matrix, and lemma 4.2 demonstrates that this normalization corresponds to the symmetric normalization of S($\\hat{A}$). Each layer of this graph convolution performs weighted propagation over the normalized undirected bipartite graph S($\\hat{A}$), with the addition of a residual connection."}, {"title": "5. Analysis", "content": "In this section, we empirically analyze the factors influencing directed link prediction performance and examine the properties of SDGAE."}, {"title": "5.1. Feature Inputs", "content": "We investigate the impact of different feature inputs on GNNs for directed link prediction. Figure 4 presents results on Cora-ML using either original features or in/out degrees as inputs, with additional results for other datasets provided in Appendix G.3.1. Overall, original features enhance performance on most datasets; however, for datasets like WikiCS, in/out degrees prove more effective. Notably, in datasets lacking original features (e.g., Slashdot, Epinions), in/out degrees outperform random features. These findings emphasize the importance of proper feature inputs for improving GNN performance. Enhancing feature quality is a key research direction, especially for datasets with weak or missing original features."}, {"title": "5.2. Loss Function and Decoder", "content": "We analyze the impact of different decoders and loss functions on directed link prediction methods. For embedding methods, we compare decoders such as logistic regression and inner product . For single real-valued GNNs, we evaluate the effects of Cross-Entropy (CE) loss and Binary Cross-Entropy (BCE) loss using decoders like MLP scoring functions and inner product. The corresponding results are provided in Appendix G.3.2."}, {"title": "5.3. Degree Distribution", "content": "We assess how well different models preserve the asymmetry of directed graphs by analyzing their degree distributions. Following STRAP , we compute every edge probability for each model and select the top-m' edges\u2014where m' is the number of edges in the training graph\u2014to reconstruct it. Figure 3 compares the true in-/out-degree distributions of the WikiCS training graph with those of reconstructed graphs generated by four various methods. Results show that STRAP and SDGAE best preserve the degree distributions, with STRAP excelling at in-degrees, explaining its superior performance on WikiCS. DirGNN, using the decoder MLP($h_{u}$ \u222a $h_{v}$), produces identical in-/out-degree distributions but still captures in-degrees correctly. In contrast, MagNet fails to learn valid distributions, leading to poor performance. Additionally, incorporating in-/out-degrees as feature inputs enhances degree distributions, as shown in Appendix G.3.3. These findings underscore the need for GNNs to better preserve degree distributions, an underexplored challenge compared to embedding methods."}, {"title": "5.4. SDGAE versus DiGAE", "content": "We investigate which aspects of SDGAE contribute to its performance gains over DiGAE. Figure 6 shows that SDGAE benefits from deep graph convolutions, achiev-"}, {"title": "5.5. Metric and Negative Sampling Strategy", "content": "In Appendix G.3.5, we compare baseline results across different metrics, showing that Accuracy, AUC, and AP exhibit small performance gaps between methods. Notably, many simple undirected graph GNNs achieving high accuracy. These findings suggest that ranking metrics better capture model performance for link prediction, as observed in undirected graphs . Additionally, we analyze the impact of negative edge sampling strategies during training and find that they significantly influence model performance. Exploring more sampling strategies, inspired by advancements in undirected graph link prediction , could be a promising direction for future research."}, {"title": "6. Conclusion", "content": "This paper presents a unified framework for directed link prediction, highlighting the critical role of dual embeddings and decoder design. To address existing limitations, we introduce DirLinkBench, a robust benchmark for evaluating directed link prediction methods. Results on the new benchmark reveal that current methods exhibit inconsistent performance, while our proposed SDGAE achieves state-of-the-art performance. Based on our findings, we highlight two key open challenges: 1) How can more efficient decoders be developed for complex-valued methods? 2) How can GNNs better preserve in-/out-degree distributions? We hope this work inspires further advancements in directed link prediction and contributes to the development of more effective and theoretically grounded models."}, {"title": "A. Notation", "content": "We summarize the main notations of the paper in Table 5."}, {"title": "B. Proof", "content": ""}, {"title": "B.1. The proof of Theorem 2.2", "content": "Theorem 2.2. For the framework defined by Equations (2) and (3), if $d_{\\theta}$, $d_{\\phi}$ > 0 and sufficiently large, there exist embeddings $\\theta_{u}$, $\\phi_{u}$ and a decoder Dec(\u00b7) that can correctly compute the probability p(u, v) of any directed edge (u, v) in an arbitrary graph. Conversely, if $d_{\\theta}$ = 0 or $d_{\\phi}$ = 0, no such embeddings or decoders can compute the correct probability of any edges in an arbitrary graph.\nProof. If $d_{\\theta}$, $d_{\\phi}$ > 0 and are sufficiently large, there exist two real-valued embeddings, $\\theta_{u}$ and $\\phi_{u}$, for each node u in the graph. According to the unified framework for directed link prediction methods, these embeddings, $\\theta_{u}$ and $\\phi_{u}$, can represent the outputs of various encoder methods, including source-target methods, complex-valued methods, and gravity-inspired methods. For these encoder methods, a decoder exists that can compute the probability p(u, v) for any directed edge (u, v).\nAs an example, consider source-target methods. If we have two embeddings, $s_{u}$ and $t_{u}$, for each node u, we can correctly compute the probability p(u, v) of any edge (u, v) using the expression p(u, v) = \u03c3($s_{u}^{T}t_{v}$), when $A_{uv}$ = $s_{u}^{T}t_{v}$. Based on existing embedding methods such as STRAP and HOPE , it is known that there exist embeddings $s_{u}$ and $t_{v}$ that satisfy $A_{uv}$ = $s_{u}^{T}t_{v}$. For complex-valued methods, such as DUPLEX , it has been demonstrated that there exist embeddings $z_{u}$ and $z_{v}$ such that $H_{uv}$ = $z_{u}z_{v}$, where $\\bar{z_{v}}$ is the complex conjugate of $z_{v}$.\nTherefore, we conclude that there exist embeddings $\\theta_{u}$ and $\\phi_{u}$, along with a decoder Dec(\u00b7), that can correctly compute the probability p(u, v) for any directed edge (u, v) in an arbitrary graph.\nIf $d_{\\theta}$ = 0 or $d_{\\phi}$ = 0, this implies that each node u in the graph is represented by a single real-valued embedding $h_{u}$."}, {"title": "B.2. The proof of Corollary 2.3", "content": "Corollary 2.3. With dual embeddings $\u0398_{u}$ and $\\phi_{u}$, if there is no suitable decoder Dec(\u00b7), the probability p(u, v) of any edge (u, v) cannot be computed correctly. In contrast, even with one single embedding ($\u0398_{u}$ or $\\phi_{u}$ = \u00d8), a suitable decoder can improve the ability to compute edge probabilities.\nProof. Theorem 2.2 establishes that dual embeddings ($\u0398_{u}$, $\\phi_{u}$) combined with a suitable decoder Dec(\u00b7), guarantee the correct computation of p(u, v). However, if the decoder is unsuitable (e.g., it fails to model directional relationships), even valid embeddings cannot produce correct edge probabilities. For example, a symmetric decoder like Dec($\u0398_{u}$, $\\phi_{u}$, $\u0398_{v}$, $\\phi_{v}$) = \u03c3($\u0398_{u}$ \u222a $\u0398_{v}$ + $\u0398_{v}$ \u222a $\u0398_{v}$) leads to p(u, v) = p(v, u), which violates directionality. This highlights that the decoder must align with the embeddings' structure to exploit their expressivity fully.\nIn contrast, Theorem 2.2 shows that single embeddings ($d_{\u0398}$ = 0 or $d_{\\phi}$ = 0) cannot universally represent arbitrary directed graphs. Nevertheless, a suitable decoder can still improve edge probability estimation in constrained scenarios. For example, an asymmetric decoder like Dec($h_{u}$, $h_{v}$) = MLP($h_{u}$\u222a $h_{v}$) can preserve some directionality by learning from the data. While such decoders fail on more complex structures like ring graphs (as shown in Theorem 2.2), they can handle simpler directed graph structures. In summary, dual embeddings require suitable decoders to achieve theoretical expressivity, while single embeddings, though fundamentally limited, can still benefit from specific decoders in practice."}, {"title": "B.3. The proof of Lemma 4.1", "content": "Lemma 4.1. When omitting degree-based normalization in Equations (4) and (5), the graph convolution of DiGAE is\n[$S^{(l+1)}$,$T^{(l+1)}$] = \u03c3($S(\\hat{A})$ [$S^{(l)}W_{S}^{(l)}$,$T^{(l)}W_{T}^{(l)}$])."}, {"title": "B.4. The proof of Lemma 4.2", "content": "Lemma 4.2. The symmetrically normalized block adjacency matrix $D_{S}^{-1/2} S(\\hat{A}) D_{S}^{-1/2}$ = $\\tilde{S}(\\tilde{A})$, where $D_{S}$ = diag($\\tilde{D}_{out}$,$\\tilde{D}_{in}$) is the diagonal degree matrix of S($\\hat{A}$).\nProof. For the block adjacency matrix S($\\hat{A}$) = [$ \\begin{array}{cc} 0 & A \\\\ A^{T} & 0 \\end{array}$] and its degree matrix $D_{S}$ = [$ \\begin{array}{cc} \\tilde{D}_{out} & 0 \\\\ 0 & \\tilde{D}_{in} \\end{array}$], we have\n$\\tilde{D}_{S}^{-1/2} S(\\tilde{A}) \\tilde{D}_{S}^{-1/2}$ = [$ \\begin{array}{cc} \\tilde{D}_{out}^{-1/2} & 0 \\\\ 0 & \\tilde{D}_{in}^{-1/2} \\end{array}$] [$ \\begin{array}{cc} 0 & A \\\\ A^{T} & 0 \\end{array}$] [$ \\begin{array}{cc} \\tilde{D}_{out}^{-1/2} & 0 \\\\ 0 & \\tilde{D}_{in}^{-1/2} \\end{array}$]\n= [$ \\begin{array}{cc} 0 & \\tilde{D}_{out}^{-1/2}A \\\\ \\tilde{D}_{in}^{-1/2}A^{T} & 0 \\end{array}$] [$ \\begin{array}{cc} \\tilde{D}_{out}^{-1/2} & 0 \\\\ 0 & \\tilde{D}_{in}^{-1/2} \\end{array}$]\n= [$ \\begin{array}{cc} 0 & \\tilde{D}_{out}^{-1/2}A\\tilde{D}_{in}^{-1/2} \\\\ \\tilde{D}_{in}^{-1/2}A^{T}\\tilde{D}_{out}^{-1/2} & 0 \\end{array}$]\n= [$ \\begin{array}{cc} 0 & \\tilde{A} \\\\ \\tilde{A}^{T} & 0 \\end{array}$]\n= $\\tilde{S}(\\tilde{A})$"}, {"title": "C. Related Work", "content": "Existing methods for directed link prediction can be broadly categorized into embedding methods and graph neural networks (GNNs). We review these methods below and discuss their relevance to our work.\nEmbedding Methods for directed graphs primarily aim to capture asymmetric relationships. Most approaches generate two vectors per node: a source embedding ($s_{u}$) and a target embedding ($t_{u}$). These embeddings are learned using either factorization or random walks. Factorization-based methods include HOPE , which applies the Katz similarity followed by singular value decomposition (SVD), and AROPE , which generalizes this idea to preserve arbitrary-order proximities. STRAP combines Personalized PageRank (PPR) scores from both the original and transposed graphs before applying SVD. Random-walk methods include APP , which trains embeddings using PPR-based random walks,"}, {"title": "D. Details of Unified Framework", "content": "Here, we introduce the details of the examples within our unified learning framework for directed link prediction methods. For readability, we copy Table 2 from the main paper in Table 6. Here, \u03c3 represents the activation function (e.g., Sigmoid), while LR and MLP denote the logistic regression predictor and the multilayer perceptron, respectively. The symbols  and || represent the Hadamard product and the vector concatenation process, respectively.\nFor source-target encoder, we define the source embedding as $s_{u}$ = $\u0398_{u}$ and the target embedding as $t_{u}$ = $\\phi_{u}$ for each node u \u2208 V. Possible decoders include: \u03c3($s_{u}^{T}$ \u222a $t_{v}$), LR($s_{u}$\u2299 $t_{v}$), LR($s_{u}$||$t_{v}$). Here, $s_{u}^{T}$ denotes the inner product of the source and target embeddings, while LR(\u00b7) represents the logistic regression predictor.\nFor single real-valued encoder, we define the real-valued embedding as $h_{u}$ = $\u0398_{u}$ and set $\\phi_{u}$ = \u00d8, where \u00d8 denotes nonexistence. Possible decoders include: \u03c3($h_{u}$\u222a$h_{v}$), MLP($h_{u}$\u2299$h_{v}$), MLP($h_{u}$||$h_{v}$).\nFor complex-valued encoder, we define the complex-valued embedding as zu = $\u0398_{u}$ exp(i$\u0398_{u}$), where i is the imaginary unit. Possible decoders include: Direc(zu, zv), MLP ($\\theta_{u}$||$\\theta_{v}$||$\\phi_{u}$||$\\phi_{v}$). Here, Direc(\u00b7) refers to the direction-aware decoder defined in DUPLEX.\nFor gravity-inspired encoder, we define the real-valued embedding as $h_{u}$ = $\u0398_{u}$ and set the mass parameter $m_{u}$ = g($\\phi_{u}$), where g(\u00b7) is a function or neural network that converts $\u0398_{u}$ into a scalar . Possible decoders include: \u03c3(my \u2013 \u03bb log || $h_{u}$ \u2013 $h_{v}$||), \u03c3(my \u2013 \u03bb log(dist\u0189a'( $h_{u}$, $h_{v}$))). Here, \u03bb is a hyperparameter and dist\u0189a' (\u00b7) represents the hyperbolic distance."}, {"title": "E. Issues with Existing Experimental Setup", "content": ""}, {"title": "E.1. Details of multiple subtask setup", "content": "The multiple subtask setup involves four subtasks where graph edges are categorized into four types: positive (original direction), reverse (inverse direction), bidirectional (both directions), and nonexistent (no connection). Each subtask focuses on predicting a specific edge type. The details are as follows:\n\u2022 Existence Prediction (EP): The model predicts whether a directed edge (u, v) exists in the graph. Both reverse and nonexistent edges are treated as nonexistence.\n\u2022 Directed Prediction (DP): The model predicts the direction of edges for node pairs (u, v), where either (u, v) \u2208 E or (\u03c5, \u03ba) \u2208 E.\n\u2022 Three-Type Classification (3C): The model classifies an edge as positive, reverse, or nonexistent.\n\u2022 Four-Type Classification (4C): The model classifies edges into four categories: positive, reverse, bidirectional, or nonexistent."}, {"title": "E.2. Details of experimental setting and more results", "content": "Experimental setting. In this part of the experiment, we strictly follow the configurations of each setup and reproduce the results using the provided codes. For the MLP model, we implement a simple two-layer network with 64 hidden units, while the learning rate and weight decay are tuned according to the settings of each setup to ensure a fair comparison. For the MagNet setup, we reproduce the reported MagNet results and include the MLP results. For PyGSD"}, {"title": "F. Details and Experimental Setting for SDGAE", "content": ""}, {"title": "F.1. More details of graph convolution", "content": "The graph convolutional layer of SDGAE is defined as:\n[$S^{(l+1)}$,$T^{(l+1)}$] = [$ \\begin{array}{cc} 0 & \\omega_{S}^{(l)}\\tilde{A} \\\\ \\omega_{T}^{(l)}\\tilde{A^{T}} & 0 \\end{array}$] [$ \\begin{array}{c} S^{(l)} \\\\ T^{(l)} \\end{array}$]\nExpanding this formulation, the source and target embeddings at each layer are denoted as:\n$S^{l+1)} = \\omega_{S}^{(l)}\\tilde{A}T^{l)} + S^{(l)}$, $T^{l+1)} = \\omega_{T}^{(l)}\\tilde{A^{T}}S^{(l)} + T^{(l)}$."}, {"title": "F.2. Experimental setting", "content": "For the SDGAE experimental setting, we aligned our settings with those of other baselines in DirLinkBench to ensure fairness. For the MLP used in X initialization, we set the number of layers to one or two, matching DiGAE's convolutional layer configurations. The number of hidden units and the embedding dimension were both set to 64. The learning rate (lr) was chosen as either 0.01 or 0.005, and weight decay (wd) was set to 0.0 or 5e-4, following the config-"}, {"title": "G. More Details of DirLinkBench", "content": ""}, {"title": "G.1. Datasets and baselines", "content": "Datasets Details. Table 12 summarizes the statistical characteristics of seven directed graphs. Avg. Degree indicates average node connectivity and %Directed Edges reflects inherent directionality. Detailed descriptions:\n\u2022 Cora-ML and CiteSeer are two citation networks. Nodes represent academic papers, and edges represent directed citation relationships.\n\u2022 Photo and Computers are two Amazon co-purchasing networks. Nodes denote products, and directed edges denote the sequential purchase relationships.\n\u2022 WikiCS is a weblink network where nodes represent computer science articles from Wikipedia and directed edges correspond to hyperlinks between articles.\n\u2022 Slashdot and Epinions are two social networks, where nodes represent users. Edges in Slashdot indicate directed social interactions between users, and edges in Epinions represent unidirectional trust relationships between users."}, {"title": "G.2. Metric description", "content": "Mean Reciprocal Rank (MRR) evaluates the capability of models to rank the first correct entity in link prediction tasks. It assigns higher weights to top-ranked predictions by computing the average reciprocal rank of the first correct answer across queries: MRR=  \u03a3=1 , where |Q| is the total number of queries and rank; denotes the position of the first correct answer for the i-th query. MRR emphasizes early-ranking performance, making it sensitive to improvements in top predictions.\nHits@K measures the proportion of relevant items that appear in the top-K positions of the ranked list of items. For N queries, Hits@K= =11(rank < K), where rank is the rank of the i-th sample and the indicator function 1 is 1 if rank < K, and 0 otherwise. Following the OGB benchmark (Hu et al., 2020), link prediction implementations compare each positive sample's score against a set of negative sample scores. A \"hit\" occurs if the positive sample's score surpasses at least K-1 negative scores, with final results averaged across all queries.\nArea Under the Curve (AUC) measures the likelihood that a positive sample is ranked higher than a random negative sample. AUC = \u03a3\u03a31(>), where M and N are positive/negative sample counts, spos and snes their prediction scores. Values approaching 1 indicate perfect separation of positive and negative edges.\nAverage Precision (AP) is defined as the area under the Precision-Recall (PR) curve. Formally, AP = 1=1(R - R-) \u00d7 Pi, where Pi is the precision at the i-th threshold, R is the recall at the i-th threshold, and N is the number of thresholds considered.\nAccuracy (ACC) measures the proportion of correctly predicted samples among all predictions. Formally, ACC ="}, {"title": "G.3. Additional results in analysis", "content": ""}, {"title": "G.3.1. FETURE INPUTS", "content": "In Figures 8(a) and 8(b), we compare the performance of various methods using original features and in/out degrees as inputs on Photo and WikiCS. Figure 8(c) presents a similar comparison using random features and in/out degrees as inputs on Slashdot. The results highlight the significant impact of feature inputs on GNN performance and also demonstrate that in-/out-degree information plays a crucial role in link prediction."}, {"title": "G.3.2. LoSS FUNCTION AND DECODER", "content": "We present a comparison of different decoders on embedding methods in Figure"}]}