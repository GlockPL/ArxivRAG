{"title": "ProFuser: Progressive Fusion of Large Language Models", "authors": ["Tianyuan Shi", "Fanqi Wan", "Canbin Huang", "Xiaojun Quan", "Chenliang Li", "Ming Yan", "Ji Zhang"], "abstract": "While fusing the capacities and advantages of various large language models (LLMs) offers a pathway to construct more powerful and versatile models, a fundamental challenge is to properly select advantageous model during the training. Existing fusion methods primarily focus on the training mode that uses cross entropy on ground truth in a teacher-forcing setup to measure a model's advantage, which may provide limited insight towards model advantage. In this paper, we introduce a novel approach that enhances the fusion process by incorporating both the training and inference modes. Our method evaluates model advantage not only through cross entropy during training but also by considering inference outputs, providing a more comprehensive assessment. To combine the two modes effectively, we introduce ProFuser to progressively transition from inference mode to training mode. To validate ProFuser's effectiveness, we fused three models, including vicuna-7b-v1.5, Llama-2-7b-chat, and mpt-7b-8k-chat, and demonstrated the improved performance in knowledge, reasoning, and safety compared to baseline methods.", "sections": [{"title": "1 Introduction", "content": "In recent years, Large Language Models (LLMs) have demonstrated impressive performance across various tasks. However, training these models often requires substantial resources, including thousands of GPUs and the processing of trillions of tokens (Sukhbaatar et al., 2024). To achieve a more powerful and efficient model, integrating the capabilities and advantages of various LLMs into a unified model presents a cost-effective solution.\nWhen considering the integration of multiple models' capabilities, ensemble methods often come to mind (Monteith et al., 2011; Jiang et al., 2023)."}, {"title": "2 Related Work", "content": null}, {"title": "2.1 Knowledge Distillation", "content": "Knowledge distillation (KD, Hinton et al. (2015)) aims to compress one or more large teacher models into a smaller student model without a significant performance drop. In the NLP domain, for text classification, many works let the student model mimic the teacher's output distribution (Turc et al., 2019; Zhang et al., 2023), hidden states (Sun et al., 2019; Jiao et al., 2020), or attention scores (Wang et al., 2021). For text generation, the student model could learn from the teacher's logits distribution on ground truth (Agarwal et al., 2024; Gu et al., 2024) or generations (Peng et al., 2023). Multi-teacher knowledge distillation (MTKD) boosts the effectiveness of distillation by averaging the distributions (You et al., 2017) or blending the sequences (Wang et al., 2024) from multiple teachers. Compared to KD, model fusion serves distinct purposes by integrating strengths from multiple source models into a unified model, leading to a comprehensively stronger model."}, {"title": "2.2 Model Merging", "content": "Model merging involves combining the weights of two or more models into one by directly editing the weight space. There are two primary types of research in this area: 1. Merging Models Trained on the Same Task: Enhances a model's generalization by merging multiple models trained on the same task. Model Soups (Wortsman et al., 2022) fine-tune a model using the same dataset but with different strategies, and then combine the resulting models through linear averaging. 2. Merging Models Trained on Different Tasks: Integrates models trained on different tasks to enable multi-task learning (MTL). Fisher Merging (Matena and Raffel, 2021) uses the Fisher information matrix to measure the importance of individual model parameters, guiding the merging process. However, computing the Fisher information matrix becomes computationally and memory-intensive with a large number of model parameters. RegMean (Jin et al., 2023) transforms merging into an optimization problem, finding a closed-form solution by minimizing the L2 distance between the merged model and each individual model. Task Arithmetic introduces \"task vectors\", showing that merging task vectors to create a consolidated model can effectively facilitate MTL. PEM Composition (Zhang et al., 2023) extends Task Arithmetic to merge LORA models (Hu et al., 2021). Ties-Merging (Yadav et al., 2024) addresses task conflicts within Task Arithmetic by resetting redundant parameters, resolving sign conflicts, and exclusively merging parameters that exhibit sign-consistency.\nThe aforementioned methods are limited to merging models with same structure. FuseLLM (Wan et al., 2024) introduces a novel approach for knowledge fusion of heterogeneous LLMs, selecting the advantageous model with Min-CE on GT. It leverages logits distribution from source LLMs to transfer their advantages into a target LLM. This study proposes to evaluate a model's advantages from both the training mode and inference mode, enabling a more comprehensive demonstration of its strengths."}, {"title": "3 Method", "content": "For model fusion, our objective is to integrate the advantages of several source models into a target model. To achieve this, we confront two primary challenges: 1. Advantage Evaluation: Existing work only uses the ground truth Min-CE (training mode) to evaluate advantages, which provides limited insight. We employ both inference and training modes to assess model advantages, allowing the strengths of different models to be fully showcased and providing more effective information for the fusion process. 2. Fusion Strategy: Given more advantage information, we exploit the differential nature of the information from both modes, combining progressive learning, and propose a progressive fusion strategy, achieving an easy (inference mode)-to-hard (training mode) learning process."}, {"title": "3.1 Preliminaries", "content": "For a given instruction dataset $D = \\{(x_i, y_i)\\}_{i=1}^n$, where $x_i$ and $y_i$ denote the i-th instruction and its corresponding response, respectively. Supervised fine-tuning (SFT) aims to refine pre-trained language models parameterized by $\\theta$ to develop instruction-following capabilities through supervised learning, by mimimizing the log-likelihood function:\n$L_{SFT}(x_i, y_i) = \\sum_{t<T} log P_\\theta(y_{i,t} | x_i, y_{i,<t}),$ (1)\nwhere $T$ represents the length of response $y_i$, $P_\\theta(y_{i,t} | x_i, y_{i,<t})$ is obtained using teacher-forcing, which means the probability of predicting the tth GT token $y_{i,t}$ given instruction and previously GT tokens."}, {"title": "3.2 Advantage Evaluation", "content": "To comprehensively evaluate models' advantages, we employ both training mode and inference mode evaluations. In the training mode, we posit that for a given (instruction, response) pair, the probability distribution generated under teacher-forcing reflects the model's understanding of the input based on its intrinsic knowledge. A lower cross-entropy (CE) value indicates the model's superiority. In the inference mode, the quality of responses produced by different source models for a given instruction can indicate their inherent problem-solving capabilities. A higher-quality response signifies a more advantageous model."}, {"title": "Training Mode", "content": "As shown on the right side of Figure 2, given an input instruction $x_i$ and the ground truth response $y_i$, we use teacher-forcing to obtain the logits distributions $\\{P_i^j\\}_{j=1}^n$ from the source models $\\{M_j\\}_{j=1}^n$. We then compute the cross-entropy (CE) for each model according to Equation (1). The model with the minimum CE is selected as the advantageous model:\n$M^{MinCE} = argmin(\\{L_{SFT}(x_i, y_i)\\}_{j=1}^n),$ (2)\nwhere $\\theta_j$ represents the parameters of jth source model. The logits distribution $P^{MinCE}$ from the chosen model encapsulates the advantage information in the training mode."}, {"title": "Inference Mode", "content": "As shown on the left side of Figure 2, for a given instruction $x_i$, we derive inference outputs $\\{y_i^j\\}_{j=1}^n$ from the source models $\\{M_j\\}_{j=1}^n$. To evaluate the quality of these outputs, we use multiple high-performing reward models to vote on them. The output with the most votes is regarded as the chosen response.\n$\\hat{y_i^B} = argmax(RMVote(\\{y_i^j\\}_{j=1}^n)).$ (3)\nThe chosen output $\\hat{y_i^B}$ and its associated logits distribution $P^B$ are utilized as the conferred advantage information for the inference mode."}, {"title": "3.3 Progressive Fusion", "content": "To effectively exploit the obtained advantage information, we leverage the differences between the source model output used in inference mode and the GPT-4 output used in training mode, where the latter is more detailed and complex compared to the former. Combining this with progressive learning, we propose a easy-to-hard fusion strategy, starting with inference mode fusion followed by training mode fusion.\nSpecifically, to transfer the capabilities of source LLMs to the target LLM, we guide the target to emulate the advantaged source model using sequence-level loss $L_{SFT}$ and token-level loss $D_{KL}$:\n$L_{Fuse}(x, y, P_s) = L_{SFT}(x, y) + \\beta D_{KL}(P_s, P_T),$ (4)\nwhere $P_s$ and $P_T$ represent the logits distribution of the source model manifesting an advantage and the target model with respect to y, respectively.\nGiven instruction $x_i$, we replace the inference and training mode advantage information $(\\hat{y_i^B}, P^B)$ and $(y_i, P^{MinCE})$ in Equation (4), leading to distinct fusion objectives for each mode, denoted as $L_{Infer-Fuse}(x_i, y_i, P^B)$ and $L_{Train-Fuse}(x_i, y_i, P^{MinCE})$, respectively.\nThus, the fusion objective for our progressive fusion process is formalized as:\n$L_{ProFuser} = w_1 L_{Infer-Fuse} + w_2 L_{Train-Fuse},$ (5)\nwhere the weights $w_1$ and $w_2$ are adapted based on the stage of the fusion process. Initially, for the inference mode fusion, $w_1$ is set to 1 and $w_2$ to 0, this allows for a focus solely on the advantage discovered in the inference mode. As we transition to the training mode fusion, $w_2$ is increased to 1 to stress the importance of training mode, while $w_1$ is reduced to 0.1 to preserve the insights from inference mode.\nThis staged approach enables a harmonious integration of model benefits, ensuring the comprehensive advantages accumulate effectively in the target LLM."}, {"title": "4 Experiments", "content": "In this section, we conduct experiments to evaluate the performance of our proposed ProFuser, including heterogeneous model fusion experiments and ablation studies to assess the efficacy of the progressive fusion strategy across inference and training modes."}, {"title": "4.1 Experimental Setup", "content": null}, {"title": "4.1.1 Source Models", "content": "Several prominent open-source LLMs serve as source models for our experiments: the Llama-series models vicuna-7b-v1.5 (Zheng et al., 2023), Llama-2-7b-chat (Touvron et al., 2023), and the MPT-series model mpt-7b-8k-chat (Team, 2023). We select vicuna-7b-v1.5 as the target model due to its comprehensive performance and adaptability across various tasks. To address the challenge posed by different tokenizers and vocabularies used in heterogeneous LLMs, we implement token alignment before model fusion, following prior work (Wan et al., 2024)."}, {"title": "4.1.2 Training Dataset", "content": "Recognizing the critical role of data quality, we employ Orca-Best2, a derivative of the OpenOrca3 GPT-4 1M instructions dataset enhanced through semantic deduplication and filtering of low-quality instructions (Mukherjee et al., 2023). From this dataset, a subset of 100,000 examples is randomly sampled for training."}, {"title": "4.1.3 Training Details", "content": "Utilizing the HuggingFace Transformers library (Wolf et al., 2020), we train all models with the Adam optimizer (Kingma and Ba, 2014), setting the learning rate to $1.5 \\times 10^{-5}$. A cosine annealing learning rate schedule is applied along with a batch size of 128 and a maximum sequence length of 2048. The entire training process spans 3 epochs, totaling 96 A100 (80G) hours of computation. Detailed training setups are further elaborated in Appendix A."}, {"title": "4.1.4 Evaluation", "content": "The effectiveness of ProFuser is empirically verified across three dimensions:\nKnowledge We measure the models' grasp of factual knowledge by using the broad-spectrum MMLU dataset (Hendrycks et al., 2020), which spans 57 diverse subjects, such as elementary mathematics and US history.\nReasoning The models' general reasoning skills are appraised using challenging benchmarks such as HellaSwag (Zellers et al., 2019), ARC-Challenge (Clark et al., 2018), and Wino-Grande (Sakaguchi et al., 2021). Additionally, mathematical reasoning is specifically assessed through the GSM8K.\nSafety We assess the models' capability to generate outputs that align with factual correctness and common sense, relying on the TruthfulQA dataset (Lin et al., 2021).\nEvaluations are conducted using the LM-Evaluation-Hardness framework (Gao et al., 2023), following the standard metrics of the HuggingFace OpenLLM Leaderboard (Beeching et al., 2023). For the GSM8K assessment, our approach follows the methodology outlined in Open-Instruct (Wang et al., 2023)."}, {"title": "4.1.5 Baselines", "content": "To evaluate the effectiveness of ProFuser, we compare it against three categories of established baselines: Original Models, Continual SFT, and Model Fusion. The details of these baselines are shown in Appendix C."}, {"title": "4.2 Main Results", "content": "Table 1 presents the performance of our proposed ProFuser compared to baselines across six benchmarks, highlighting several key findings:\nFirstly, by integrating three source models into vicuna-7b-v1.5-ProFuser through ProFuser, we observe the model attaining the highest overall score. This translates to a 3.09% improvement over the baseline vicuna-7b-v1.5, a significant enhancement that is double the improvement observed with the continual SFT approach (vicuna-7b-v1.5-CSFT).\nSecond, when comparing ProFuser against FuseLLM (Wan et al., 2024), it's evident that vicuna-7b-v1.5-ProFuser exhibits superior performance across all tests, with the sole exception of the GSM8K benchmark. Here, vicuna-7b-v1.5-ProFuser demonstrates a 1.06% relative boost. This exception on GSM8K can be ascribed to the complexities inherent in affirming correct mathematical reasoning a task particularly challenging when fusion involves source models with a significant prevalence of incorrect predictions, thus slightly diminishing the effectiveness of ProFuser's inference mode fusion.\nFurther analysis, comparing ProFuser with alternative fusion methodologies such as SimulFuse and ReverseFuse, reveals that ProFuser notably outperforms these strategies. Remarkably, ReverseFuse, which prioritizes training mode fusion before inference mode, not only falls short of FuseLLM's achievements but also impairs the overall performance. These findings point to a strategic fusion beginning with the simplification presented by source model outputs, followed by the complexity of ground truth (GT) fusion, enabling a more nuanced leverage of model strengths. This sequential easy-to-hard fusion route maximizes the utility of each model's contribution.\nLastly, despite the target model displaying a superior average performance to that of the individual source models, the inclusion of source models-even those considered weaker-affords the target model a substantial boost. This augmentation of the target model's capabilities, facilitated by the integration of relatively inferior models, delineates a benefit not commonly achievable through conventional knowledge distillation techniques (Hinton et al., 2015). This phenomenon underscores the potential of tapping into the differential strengths of weaker models, enhancing the stronger model's performance through thoughtful integration."}, {"title": "5 Analysis", "content": "To delve deeper into the principles supporting ProFuser, we performed additional experiments focusing on three distinct areas: model advantage evaluation methods (\u00a75.1), the progressive fusion strategy (\u00a75.2), and the impact of the number of source models used in the fusion process (\u00a75.3). We also conducted comparative experiments to underscore the benefits of ProFuser in homogeneous model fusion. (\u00a75.4)."}, {"title": "5.1 Advantage Evaluation", "content": "To thoroughly highlight the strengths of source LLMs, we invoked both inference and training modes. Training mode's Min-CE metric has established its efficacy for evaluating advantages (Wan et al., 2024). Here, we spotlight the inference mode-based evaluation, conducting analyses through two experimental frameworks:"}, {"title": "Reference-Based Evaluation", "content": "The hypothesis is that the closer a source model's output mirrors that of GPT-4, the more advantageous it is. We measured similarity in two dimensions: textual form (using BLEU and ROUGE scores) and textual semantics (evaluated via BERTScore\u2074), with the final score calculated as follows:\n$Score = 0.25 \\times BLEU + 0.25 \\times ROUGE + 0.5 \\times BERTScore$ (6)"}, {"title": "Reference-Free Evaluation", "content": "Utilizes open-source reward models to score outputs. In situations where a single model is used, the output receiving the highest score is selected. Conversely, when multiple models are employed, a majority voting mechanism is invoked to determine the most optimal output.\nAs shown in Figure 3, we observed two key points: First, methods based on reward model scoring generally outperform those based on textual similarity. We believe this is because textual similarity fails to be universally applicable in the context of general instruction-following tasks. For simple instructions with clear responses, similarity can provide reliable judgments, but for complex instructions requiring detailed explanations, it struggles to offer accurate evaluations. Reward models, on the other hand, are trained on such data, enabling them to provide more reliable scores. Additionally, by integrating multiple reward models, we achieved significant improvements on the TruthfulQA benchmark, while performance varied across other benchmarks. We think this is because the reward models involved in the integration perform well in safety-related aspects but exhibit varying degrees of proficiency in other types of tasks."}, {"title": "5.2 Progressive Fusion Strategy", "content": "To maximize advantage utilization, we embraced the concept that GT data-typically more nuanced than source model outputs should guide the fusion sequence. Accordingly, ProFuser introduces a step-wise integration, commencing with inference mode and culminating with training mode. We also probed the strategy's effectiveness from a data perspective.\nSpecifically, we divide the training set into two subsets ordered from easy to hard, allowing the model to progressively learn from simple to complex tasks based on the following difficulty criteria. 1. Ground Truth Sequence Length: We posited that instructions paired with lengthier responses pose greater learning challenges for the target model. 2. Reward Model Score: A subpar score of the target model on certain instructions was taken as a mark of elevated task difficulty. In both settings, we conduct inference and training mode fusion progressively.\nTable 2 highlights two pivotal observations from our study. First, compared to other strategies, ProFuser exhibited exceptional performance consistently across various capabilities. This accentuates the efficacy of a progressive learning vector that adheres to an easy-to-hard paradigm, underpinned by model-oriented capabilities, which in turn significantly enhances the integration of advantage information from both modes. Second, the progressive strategy's data-centric rendition was somewhat compromised by dataset division within the inference mode paradigm, limiting the full potential expression of model advantages. Yet, samples delineated by GT response length provided a better gauge of difficulty, signaling the reliability of this specific criterion."}, {"title": "5.3 Number of Source Models", "content": "To explore the impact of the number of fused models on fusion performance, we fused varying numbers of LLMs using ProFuser.\nAs shown in Figure 4, we have the following two key observations: First, as the number of integrated source models increases, the improvements brought by the ProFuser method also increase accordingly. This trend validates the robust nature of the advantage evaluation method in use. Notably, even source models inferior in individual capabilities to our target model vicuna-7b-v1.5 contributed beneficially to the fusion landscape.\nSecond, the improvements from fusing a single model vary across different benchmarks, with mpt-7b-8k-chat showing greater fluctuations compared to Llama-2-7b-chat. We believe this is due to the greater differences between mpt-7b-8k-chat and vicuna-7b-v1.5, as they originate from different base models, which increases the probability of introducing complementary fusion signals. However, mpt-7b-8k-chat is relatively weaker, it is also more prone to generating erroneous responses. Given the less-than-perfect accuracy of the advantage evaluation metrics, these incorrect responses are more likely to be incorporated into the fusion process."}, {"title": "5.4 Comparison with Model Merging", "content": "To demonstrate the advantage of ProFuser in homogeneous model fusion scenarios, we designed experiments to compare the performance of ProFuser with various model merging methods. Since ProFuser involves lightweight fine-tuning, we used vicuna-7b-v1.5-CSFT and Llama-2-7b-chat as the baseline models in the model merging experiments to ensure a fair comparison.\nAs shown in Table 3, ProFuser achieves the highest scores not only in individual benchmarks such as MMLU, GSM8K, and TruthfulQA but also secures the highest overall average score. These model merging methods show promising results when the source models have comparable and strong performances. For instance, model merging methods achieve similar or even better performance than ProFuser on HellaSwag. However, on other benchmarks, they may be significantly influenced by the weaker model, leading to a performance drop of the base model. Therefore, ProFuser, despite requiring light fine-tuning, provides more reliable fusion performance overall."}, {"title": "6 Conclusion", "content": "Fusing the knowledge and capabilities of multiple LLMs can create stronger models more efficiently. We introduce ProFuser, a simple method that integrates the strengths of heterogeneous LLMs into a single LLM. Instead of relying solely on the training mode to capture the model's strengths in understanding ground truth, ProFuser also leverages the inference mode to capture the model's strengths in executing instructions, fully showcasing the model's advantages. Furthermore, ProFuser progressively learns from the inference mode to the training mode, based on the difference that ground truth (GPT-4 output) used in the training mode is more complex and detailed than the source LLM output in the inference mode, thus fully utilizing the advantages of both modes. Evaluated across six benchmarks and three dimensions, ProFuser performs significantly better than existing model fusion methods."}, {"title": "Limitations", "content": "There are two potential limitations to consider in our work. First, the fusion data used is obtained based on a random sampling strategy. This approach does not specifically account for whether the data can effectively showcase the differences in the advantages of different models. Future work could explore data sampling strategies that consider model differences to achieve more efficient model fusion. Second, we only used three source models in our current experiments. We have not explored the impact of increasing the number of source models on the performance of the progressive fusion method, nor investigated the scaling laws of our approach."}, {"title": "Ethics Statement", "content": "All experiments in this study were conducted using publicly available datasets that do not contain any private information. Our work does not involve the analysis or utilization of identity characteristics, and we do not engage in any form of gender or racial discrimination."}, {"title": "A Training Setups", "content": "Before training, data preprocessing is necessary to obtain the candidate fusion information for the inference and training modes in ProFuser. For the training mode, we need the logits distribution of the source models on the GPT-4 output (GT). To balance the preservation of important information and storage space, we set top-p=0.95, top-k=10, and temperature=2. For the inference mode, we need the inference results of the source models, sampling one output per model. The parameters for obtaining the logit distribution in this mode are the same as those in the training mode.\nThe training process consists of two phases: inference mode fusion and inference-training mode co-fusion. In the first phase, we train for one epoch with the KL loss weight x = 0.1. In the second phase, we train for two epochs, with the KL loss weights A and B set to 0.5, and the mode loss weights w\u2081 and w\u2082 set to 0.1 and 1, respectively."}, {"title": "B System Messages", "content": "To illustrate the capacity gap between the source model and GPT-4, we followed the approach of Orca (Mukherjee et al., 2023) and analyzed the length distribution of outputs from the source model and GPT-4 under different system messages in the training set. The results show that GPT-4 generates longer and more detailed responses, especially for tasks requiring detailed explanations or complex step-by-step reasoning. The top 10 most frequent system messages are presented in Table 5."}, {"title": "C Details of Baselines", "content": "Original Models vicuna-7b-v1.5, Llama-2-7b-chat, and mpt-7b-8k-chat.\nContinual SFT We utilize the vicuna-7b-v1.5-CSFT as a baseline, which is subjected to continual SFT using the same dataset as ProFuser, ensuring a fair comparison.\nModel Fusion This category features vicuna-7b-v1.5-Fuse focusing on training mode fusion, vicuna-7b-v1.5-SimulFuse performing simultaneous inference and training mode fusion, and vicuna-7b-v1.5-ReverseFuse implementing training mode followed by inference mode fusion."}, {"title": "D Detailed Experimental Results", "content": "In the analysis section, we present the experimental results of the Progressive Fusion Strategy and Number of Source Models based on the dimensions of knowledge, reasoning, and safety abilities. The reasoning ability includes multiple benchmarks, and Table 4 provides the specific results for each benchmark."}, {"title": "E Inference Mode Evaluation", "content": "We selected three high-performing reward models from RewardBench 5: Eurus-RM-7b (Yuan et al., 2024), FsfairX-LLaMA3-RM-v0.1 (Dong et al., 2023; Xiong et al., 2024), and Starling-RM-7B-alpha (Zhu et al., 2023). We vote on the predictions of the source model, considering the one with the highest number of votes as the highest quality. In case of a tie, we use the score from the strongest among these three reward models for quality determination."}]}