{"title": "EVEv2: Improved Baselines for Encoder-Free Vision-Language Models", "authors": ["Haiwen Diao", "Xiaotong Li", "Yufeng Cui", "Yueze Wang", "Haoge Deng", "Ting Pan", "Wenxuan Wang", "Huchuan Lu", "Xinlong Wang"], "abstract": "Existing encoder-free vision-language models (VLMs) are rapidly narrowing the performance gap with their encoder-based counterparts, highlighting the promising potential for unified multimodal systems with structural simplicity and efficient deployment. We systematically clarify the performance gap between VLMs using pre-trained vision encoders, discrete tokenizers, and minimalist visual layers from scratch, deeply excavating the under-examined characteristics of encoder-free VLMs. We develop efficient strategies for encoder-free VLMs that rival mainstream encoder-based ones. After an in-depth investigation, we launch EVEv2.0, a new and improved family of encoder-free VLMs. We show that: (i) Properly decomposing and hierarchically associating vision and language within a unified model reduces interference between modalities. (ii) A well-designed training strategy enables effective optimization for encoder-free VLMs. Through extensive evaluation, our EVEv2.0 represents a thorough study for developing a decoder-only architecture across modalities, demonstrating superior data efficiency and strong vision-reasoning capability.", "sections": [{"title": "1. Introduction", "content": "With the recent rapid advancements in both large language models (LLMs) and large vision models (LVMs) , vision-language models (VLMs)  have made remarkable strides, demonstrating impressive capabilities in multi-modal understanding and reasoning applications. As illustrated in Figure 1(1), typical practice adopts pre-trained vision encoders to extract visual semantics, which are then translated into the text embedding space as Foreign language inputs for subsequent LLMs (e.g., BLIP and LLaVA ). In contrast, another representative branch transforms visual features from vision encoders' last layer across each layer of LLMs through cross-attention modules, like Flamingo  and LLaMA-3.2V . Thanks to well-aligned representations across modalities through large-scale contrastive learning , these studies can achieve promising performance and strong multi-modality capability after joint training. However, the inductive biases during visual pre-training, e.g., image resolution, aspect ratio, and semantic priors, limit the flexibility and applicability of the visual learning in diverse real-world scenarios .\nUnlike compositional VLMs, Fuyu takes the early step to explore monolithic VLMs at different scales, eliminating the requirements for pre-trained visual encoders, while EVE first pioneers a transparent, efficient, and practical path for advancing encoder-free VLM direction. Among them, PaliGemma constructs an encoder-free VLM that demonstrates strong scaling efficiency while progressively approaching its encoder-based counterpart. With extensive training data and computing resources, Mono-InternVL narrows the gap and matches the performance of Intern-VL1.5 starting with the same LLM capabilities.\nHowever, constructing encoder-free VLMs remains challenging, particularly in learning vision perception from scratch and reducing vision-language interference within a unified model. To date, three solutions have been put forward: (i) Visual feature supervision [19]; (ii) Incremental training recipes [13, 19, 54]; (iii) Mixture-of-Expert (MoE) detachment [46, 54] in Figure 1(2c). Nevertheless, we empirically discover that visual supervision can be substituted by large-scale, high-quality image-text datasets annotated by a powerful captioning engine. During training, properly merging language and multimodal data helps mitigate knowledge-forgetting issues, while pressuring the development of multimodal capabilities. From the view of VLM structure, decoupling partial visual functions from the unified model through a MoE design  aids in relieving vision-language interference to some extent. However, we discover significant weight shifts across various network layers between the VLMs and the original LLMs, revealing the insufficiency of the current decoupling degree. Notably, we further exploit a re-parameterize architecture in Figure 1(2b) for seamless LLM adaptation. Although yielding better gains over prototype EVE in Figure 2(2a), it does not completely resolve the representational conflicts across modalities.\nFrom the above observations, we launch EVEv2.0, a new and improved baseline for encoder-free VLMs. EVEv2.0 completely disentangles overall components and introduces modality-wise sparsity into one unified decoder-only backbone in Figure 1(2d). Such a Divide-and-Conquer architecture maximizes scaling efficiency in building visual perception while minimizing the negative influence on the LLM itself. Besides, using an enhanced caption engine, we construct an effective and practical route for monolithic VLM research that facilitates data-scaling efficiency and stably transfers increasingly stronger LLMs into the encoder-free VLMs. With 100M publicly available data, EVEv2.0 outperforms encoder-free counterparts, and continually approaches encoder-based competitors of similar capacity across diverse vision-language benchmarks. Our EVEv2.0 unveils valuable insights for developing scalable, native, and next-gen VLMs, paving a transparent roadmap for future research supported by larger training data and computational resources."}, {"title": "2. Related Work", "content": "Encoder-based VLMs. Encoder-based methods have become the dominant approach in vision-language models, widely adopted in commercial products, e.g. GPT-4V, Claude 3.5 , and Gemini , as well as in the open-source projects like LLaVA series , Qwen-VL series , InternVL series , BLIP series , and EMU series . They benefit from the pre-trained knowledge from visual encoders  and LLMs , successfully building modular VLMs for various real-world applications. Among them, most studies  directly translate vision representations into the input space of LLMs. In contrast, another type of research  introduces the cross-attention module for integrating visual and language information layer-by-layer. Despite achieving strong performance gains, it may be insufficient to simply map visual information into the input space of LLMs  or connect the same visual features across different representational levels of the LLM . Besides, these modular VLMs face challenges in further development due to the strong inductive biases in pre-training visual encoding patterns, complex infrastructure requirements, and scaling laws necessary to balance various separate components.\nEncoder-free VLMs. Another visual processing strategy is discrete visual tokenization , which is widely used in various multi-modal understanding and generation approaches . However, the discretization inevitably results in lossy visual information and weakens in extracting semantic contents, which in turn hinders fine-grained visual understanding and reasoning . Therefore, recent studies  introduce semantic constraints in the visual tokenizer for both high-level semantic and fine-grained visual representations. Compared to their highly-compressed features in the discrete-value space, encoder-free VLMs  have emerged as a promising architecture for lossless visual encoding, efficient data scaling, and end-to-end multimodal processing. Specifically, EVE pioneers an efficient and transparent path for monolithic VLMs, with its data-scaling efficiency preliminarily validated by PaliGemma . Impressively, Mono-InternVL bridges the performance gap with its modular counterpart  of the same LLM capacity, using adequate data. We emphasize that limited by current training data and device resources, EVEv2.0 does not aim for state-of-the-art performance but instead focuses on revealing the most efficient route for encoder-free VLMs from scratch."}, {"title": "3. Methodology", "content": "3.1. Preliminary\nPioneer Experiments. Building on , we conduct two pilot experiments in Figure 2. Exp.(i): we adopt Vicuna-7B  via the vision encoder (CLIP-ViT-L-336px ), discrete tokenizer (Mo-VQGAN ), or one single transformer block  from scratch, dubbed VE, DT, or EVEv1.0 (w/o visual supervision). We first train the projector, vision vocabulary, or vision block using EVE-cap-16M  caption data, followed by EVE-cap-33M to update only vision encoder for VE (work best), or extra LLM weights for DT and EVEv1.0, respectively. Finally, we update the overall backbone except Mo-VQGAN during instruction-tuning. Exp.(ii): we employ stronger Qwen2-7B as the LLM for VE and EVEv1.0, compared with EVEv1.2 in Figure 1(2b) with patch embedding layer. Using 29M image-text annotations, we initially train the projector for VE or visual layers for EVE, with further updates to EVE's LLM layers via extra 48M data. Subsequently, we import varying-scale instruction data  to fine-tune the entire network.\nFinding (1): Performance gap between various vision encoding modes. Exp.(i) shows that initially, VE performs better than DT and EVEv1.0 in visual understanding due to much larger image-text pretraining datasets (400M) and already alignment space between vision-language embeddings. Despite building visual recognition from scratch, EVEv1.0 demonstrates strong scaling properties, progressively closing the performance gap with VE as the data scale increases. Subsequent studies  have proved that, with sufficient data, they can achieve comparable performance. Notably, DT maps visual information into a discrete space through quantization, hampering effective visual perception and weakening vision-language association via an image reconstruction objective. This ultimately results in subpar performance, even at larger data scales, leaving DT less competitive overall.\nFinding (2): Challenges towards multimodal interference and smooth transition. Exp.(ii) shows that compared with EVEv1.2 using the same data and stage, EVEv1.0 struggles to approach VE in text-related knowledge (SQA-IMG) due to catastrophic linguistic forgetting in LLMs. While mixing text-only and multi-modal data slightly alleviates this issue, it slows down the development of multi-modal understanding in Figure 7. Hence, we explore potential architectures and training strategies targeting LLM adaptation and multimodal interference. EVEv1.2 with re-parameterization design for each feed-forward weight helps smooth the transition from LLMs to VLMs, while EVEv1.5 with MoE design helps decouple vision-language encoding heterogeneity. However, after comparing LLMs and VLMs, we observe that VE requires only minor LLM adjustments to achieve robust results, whereas EVE necessitates extensive updates for similar capabilities in Figure 2. Besides, Layer normalization stands out as the most impacted module, indicating that EVEv1.2 and"}, {"title": "3.2. Model Architecture", "content": "Preliminary studies indicate that earlier EVE variants struggle to fully harness visual potential due to cross-modal interference within the pre-trained LLM distribution. To overcome this, we transform a dense transformer into a fully sparse, decoder-only architecture, guided by modality-aware routers in Figure 3. This approach yields a heterogeneous, modality-mixing model that retains the computational structure and FLOP count of its dense transformer counterpart.\nVisual and Textual Encoding. For visual embeddings, we construct a minimalist patch embedding layer from scratch, eliminating strong inductive bias from pre-trained vision encoders in abstracting visual content. Given an image input \\(I \\in \\mathbb{R}^{H \\times W \\times 3}\\), we first employ a Convolution layer (Conv1), followed by a Gaussian Error Linear Unit (GELU) activation function. After obtaining the resulting 2-D feature map, we then adopt another Convolution layer (Conv2) to flexibly control computational complexity as follows:\n\\(X_v = \\text{Conv2}(\\text{GELU}(\\text{Conv1}(I))),\\)\nwhere Conv1 and Conv2 denote two convolutional layers with strides of 16 and 2, and output dimensions of 1024 and 3584, respectively. Besides, two learnable special tokens serve as the prompts for image start and line feed. The class token < CLS > is appended at the beginning of the image sequence, while the split tokens <SPL > are inserted after each row of image tokens for indicators. Such a patch embedding layer supports arbitrary-ratio images with up to about 2.5M pixels, i.e., 2.5K patch tokens. Afterward, we adopt the text tokenizer from Qwen2.5 to encode text T into token embeddings xt with a dimension of 3584.\nDivide-and-Conquer Design. Building on prior analyses, we propose explicitly decoupling key modules by introducing modality-aware components, including separate attention matrices (query, key, and value), normalization layers, and feed-forward modules, each with distinct parameters. Given the token sequence \\(x = (x_1, ..., x_n)\\) where \\(x_i\\) belongs to one specific modality \\(u_i \\in \\{v, t\\}\\), we perform a multi-head self-attention (ATTN) across all modalities, modeling cross-modal relationships in a unified feature space:\n\\(\\text{ATTN}(x; \\{\\theta\\}) = (\\text{softmax}(\\frac{Q_iK_i^T}{\\sqrt{d}})V_i),\\)\n\\(Q_i = x_i W_{u_i}^Q, K_i = x_i W_{u_i}^K, V_i = x_i W_{u_i}^V,\\)\nwhere modality-specific query, key, and value are derived from their respective attention weight matrices \\(W_{u_i}, U_i \\in \\{v, t\\}\\). The interaction process is performed across modalities, i.e., visual \\(x_v\\) and textual \\(x_t\\) sets. Inspired by Figure 2, the significant quantified weight changes highlight the importance of decoupling the LayerNorm (LN) and Feed-Forward (FFN) layers; otherwise, they cause representational interference, limiting mutual capacities and capabilities. After fully decoupling the architecture, the overall operations within the Transformer block are defined as follows:\n\\(h = x + \\text{ATTN}(\\text{LN1}(x; \\theta_{LN1}); \\{\\theta\\})\\), \\(x' = h + \\text{FFN}(\\text{LN2}(x; \\theta_{LN2}); \\theta_{FFN}).\\)\nCompared with earlier EVE variants, EVEv2.0 employs a comprehensive decomposition with modality-specific components. This minimizes interference in the representation space by fully unbinding each layer and processing token sets separately for each modality. The structural decomposition supports efficient vision-perception training from scratch while retaining pre-trained knowledge by freezing off-the-shelf LLMs during pretraining. This also allows independent single-modality encoding and cross-modality correspondence across different layers simultaneously, enabling flexible modeling patterns for understanding and reasoning."}, {"title": "3.3. Training Procedure", "content": "We divide the training process into four sequential stages in Figure 4. The training data consists of publicly available image datasets, along with diverse question-answering (QA) datasets and multimodal dialogue data in Table 1.\nDeveloping strong visual perception requires high-quality, highly-detailed image-text data. Hence, we propose an efficient and low-budget engine based on LLaVA-1.6 (7B) for scaling up synthetic data. Specifically, it leverages multiple vision experts (Tag, Detection, OCR, ...) and learns GPT-4V's fusion strategy, greatly facilitating data-scaling efficiency for native VLMs with high-quality annotations, not just distilling LLaVA. In Figure 6, we empirically demonstrate the superior training efficiency achieved by using intensive annotations from DenseFusion++ during pre-training. It consistently outperforms raw web-sourced captions or those generated by lower-quality caption engines. We will release the caption engine code and weight for better interpretation and further exploration.\nLLM-guided Pre-aligning. Following [19], we freeze the LLM weights and train only the patch embedding layer to prevent model collapse and accelerate convergence in subsequent stages. Using publicly available web data, we filter 44M image samples from Datacomp recaptioned with our captioning engine. For training, we utilize a subset of 10M image-text pairs, dubbed EVE-recap-10M, and optimize with cross-entropy (CE) loss. Our experiments suggest that more extensive training at this stage is beneficial for training stability, especially considering stronger LLMs.\nVision Perception Learning. In this stage, we initialize the vision layers inside the LLM by loading LLM weights, where only the patch embedding and vision layers are trainable, while the Qwen2.5 model remains frozen during training. This strategy enables efficient learning of visual representations through pre-training on large-scale synthetic data, without compromising the knowledge encoded in the pre-trained LLM. Besides, we carefully partition the training data into a progressive, coarse-to-fine visual learning process. For training, we first introduce 29M re-captioning data from Datacomp supervised by CE loss with a maximum of 640K image pixels (i.e., 625 patch tokens). Afterward, we increase the maximum image resolution to 2.5M pixels (i.e., 2.5K patch tokens) on an expanded dataset comprising 15M Datacomp , 15M LAION , 11M SA-1B , and 7M OpenImages , dubbed EVE-recap-48M.\nVision-Text Fully-aligning. After establishing an initial alignment across modalities, we update the entire architecture to further improve image-text associations via the same loss functions. To facilitate this, we curate a diverse dataset of 15M samples from Infinity-MM general visual instruction , including chart comprehension, OCR recognition, mathematical reasoning, and etc. This dataset, named EVE-multi-task-15M, enhances visual perception and vision-language alignment, equipping EVE with the foundational capabilities to handle various multimodal tasks.\nSupervised Fine-tuning. During the SFT stage, we further enhance EVE's ability to understand complex linguistic instructions and multifarious dialogue patterns, which are crucial for real-world applications. Here, we optimize the overall network architecture on a diverse set of high-quality, multi-source instruction datasets, namely EVE-sft-7M, including LLaVA-onevision  and partial Infinity-MM-instruct . Notably, Stages 2.2 and 3 can be merged if large, balanced, and high-quality SFT data is available. We separate them to handle diverse but uneven (Stage 2.2) and balanced (Stage 3) data to achieve consistent performance."}, {"title": "4. Experiments", "content": "4.1. Training Settings\nData Preparation. All the training data is collected from publicly accessible sources to ensure reproducibility. (1) Image-Text Datasets. We follow the pre-processing pipeline outlined in [19] to process SA-1B , OpenImages , and LAION , resulting in a total of about 33M samples. For Datacomp , we curate the images with resolutions greater than 512 \u00d7 512, using DenseFusion++ to obtain 44M high-quality image descriptions and abandon samples with repetitive text or incomplete sentences. (2) Question-answering and Instruction-following Datasets. We clean out 15M QA data from Infinity-MM-GeneralQA  in its Stage-2. Meanwhile, we collect a blended set of the LLaVA-onevision  and partial Infinity-MM-instruct  from its original Stage-3/4 for complicated conversation patterns.\nImplementation Details. We use sixteen 8-A100 (40G) nodes to train EVEv2.0 using AdamW optimizer [33]. For Stage 1, 2.1, 2.2, and 3, the batch sizes are 1024, 1024, 512, and 512, while the maximum learning rates are set to 2 \u00d7 10-4, 1 \u00d7 10-4, 2 \u00d7 10-5, and 1 \u00d7 10-5. We adopt warm-up strategy with the ratio of 0.03 and cosine decay scheduler across all stages. Unless otherwise stated, we set image resolutions as 8002 and report fine-tuned results by LLaVA-mix-665K  for Stage 1/2.1/2.2 in Section 4.3.\n4.2. Main Results\nWe conduct standard evaluations using the LMMs-Eval  across various vision-language benchmarks, including (1) Chart, Diagram, and Document Understanding tasks: OCR-Bench , ChartQA , and AI2D ; (2) Visual Perception and Challenging Reasoning tasks: MMMU , MMBench-EN , SEEDBench-Img , MMVet , MME , POPE , GQA , ScienceQA-Img , and TextVQA . All the results are reported with greedy decoding and zero-shot settings, unless otherwise stated.\nFrom Table 2, EVEv2.0 surpasses the encoder-free counterparts, e.g. Fuyu , EVE , SOLO , etc. across various vision-language benchmarks. Note that, due to training data and device resource limitations, we have not yet trained smaller models under 2B parameters in parallel and, as such, cannot provide a more direct comparison with Mono-InternVL . However, we emphasize that our Divide-and-Conquer architecture demonstrates superior data-scaling efficiency compared to conventional Mixture-of-Experts designs, as shown in Figure 5. Preliminary experiments in Figure 2 reveal that simply decoupling the feed-forward module is insufficient to resolve vision-language conflicts and module compatibility inside one single unified network.\nBesides, EVEv2.0 displays superior performance against the VLMs using discrete tokenizers, i.e. Chameleon and Emu3 , despite being trained on significantly fewer data or utilizing fewer visual tokens. This further validates the efficiency and effectiveness of encoder-free VLMs with lossless visual encoding mode, even using a lightweight patch embedding layer from scratch. Notably, EVEv2.0 competes with popular and mainstream encoder-based VLMs, e.g. LLaVA-1.6 and Cambrian . We argue that the performance gap between EVEv2.0 and advanced modular VLMs primarily arises from the significant discrepancy in data magnitude, restricting encoder-free VLMs from constructing a generalizable vision perception capability and handling more complicated visual perception scenarios."}, {"title": "4.3. Ablation Studies", "content": "Divide-and-Conquer (DaC) design outperforms the re-parameterization (ReP) and mixture-of-experts (MoE). As shown in Figure 5, (1) the training process of EVEv1.0 is the slowest, and training only the patch embedding layer proves insufficient, resulting in minimal performance gains. (2) EVEv1.2 (Rep) shows a rapid loss decrease, which can be attributed to the gradual transfer of LLMs into the initial VLMs by updating the pre-trained LLM weights. However, this approach leads to a noticeable performance drop on the SQA-IMG task requiring abundant text-related knowledge. (3) In contrast, EVEv1.5 (MoE) only updates visual parameters inside frozen LLMs to effectively mitigate catastrophic forgetting issues during pre-training. However, solely decoupling FFN modules restricts distinct feature distributions across modalities, resulting in less-efficient improvements in visual perception and multi-modality alignment. (4) With prior validation support, EVEv2.0 (DaC) achieves optimal improvements across all multi-modal benchmarks, highlighting its superior data-scaling efficiency during large-scale pre-training. This success can be attributed to its modality-wise sparsity, which effectively preserves linguistic knowledge while providing greater flexibility for visual learning. This philosophy is further evidenced by the loss curve in Figure 5 with faster convergence and better training stability than EVEv1.5 during pre-training. Notably, their Avg. accuracy gap rises from 0.8% to 1.4% as the training data grows from 8M to 24M, a trend likely to hold for other model sizes and data sources. Besides, only decoupling LayerNorm yields the Avg. accuracy of 48.8% vs. 51.6% for EVEv2.0 using 8M data, necessitating the complete decomposition.\nFully-upgraded captioning engine facilitates training efficiency and model capabilities than prior competitors. As illustrated in Figure 6, (1) web-scale image-text data often suffers from excessive noise and overly brief descriptions, which results in slow progress in visual content understanding and significantly pollutes LLM's pre-training knowledge. In contrast, using a powerful captioning engine to build high-quality, hyper-detailed image annotations proves essential for efficiently developing visual perception from scratch. Our modified DenseFusion++ (7B) outperforms previously adopted models like LLaVA-1.5 (13B) and Emu2 (17B) in this regard. Moreover, our model offers an additional advantage: its efficient and low-budget nature, capable of generating 700K descriptions per day with just a single 8-A100 (40G) node, accelerated by SGlang . (2) A multi-source data mixture can significantly facilitate the visual training process. Our filtered LAION , Open-Images , SAM  provide OCR-related images, real-world scenarios, and abundant image content, respectively. Together, these data mixtures can enhance the capability of VLMs to handle diverse image inputs, promoting the development of a more robust and versatile visual perception."}, {"title": "5. Limitation and Discussion", "content": "EVEv2.0 has systematically explored network architectures and training strategies for efficiently constructing encoder-free VLMs. Due to limitations in extensive high-quality data and computational devices, its full potential remains unrealized, thereby restricting performance on specific tasks, e.g. knowledge- and document-oriented benchmarks. Besides, several promising directions remain for further exploration and improvement: Model Scaling, Data Scaling, and Modalities Expanding (e.g. audio and video). We hope EVEv2.0 inspires further research on scaling laws for encoder-free VLMs with much more computational resources."}, {"title": "6. Conclusion", "content": "In this paper, we present EVEv2.0, a carefully designed and transparent encoder-free architecture designed for vision-and-language understanding and reasoning. Rather than focusing solely on state-of-the-art performance, we systematically analyze and identify the most efficient approach for building visual perception from scratch. To address interference and compatibility challenges between vision and language, we fully disentangle the model components and introduce modality-wise sparsity within a unified decoder-only backbone. Besides, we establish an efficient pathway for optimizing data-scaling efficiency in monolithic VLM research through a modified caption engine and a carefully designed training recipe. Using only 100M publicly available data, EVEv2.0 outperforms existing encoder-free models and steadily approaches the performance of encoder-based counterparts of similar capacity across a range of vision-language benchmarks. This provides valuable insights for developing scalable, native VLMs for the next generation."}]}