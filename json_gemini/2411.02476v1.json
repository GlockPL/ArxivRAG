{"title": "A Comparative Analysis of Instruction Fine-Tuning LLMs for Financial Text Classification", "authors": ["SOROURALSADAT FATEMI", "YUHENG HU", "MARYAM MOUSAVI"], "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities across diverse Natural Language Processing (NLP) tasks, including language understanding, reasoning, and generation. However, general-domain LLMs often struggle with financial tasks due to the technical and specialized nature of financial texts. This study investigates the efficacy of instruction fine-tuning smaller-scale LLMs, including Mistral-7B, Llama3-8B, and Phi3-mini, to enhance their performance in financial text classification tasks. We fine-tuned both instruction-tuned and base models across four financial classification tasks, achieving significant improvements in task-specific performance. Furthermore, we evaluated the zero-shot capabilities of these fine-tuned models on three unseen complex financial tasks including argument classification, deal completeness classification and causal classification. Our results indicate while base model fine-tuning led to greater degradation, instruction-tuned models maintained more robust performance. To address this degradation, we employed model merging techniques, integrating single-task domain-specific fine-tuned models with the base model. Using this merging method resulted in significant enhancements in zero-shot performance, even exceeding the original model's accuracy on certain datasets. Our findings underscore the effectiveness of instruction fine-tuning and model merging for adapting LLMs to specialized financial text classification tasks.", "sections": [{"title": "INTRODUCTION", "content": "Large Language Models (LLMs) have revolutionized Natural Language Processing (NLP) by demonstrating exceptional capabilities in understanding, reasoning, and generating human-like text across various general-domain tasks. Prominent models like ChatGPT [4] and GPT-4 [36] have shown impressive versatility in following general human instructions and handling various tasks such as question answering [5], machine translation [67], information extraction [11], and grammar correction [35]. This broad proficiency has led to growing interest in leveraging LLMs for industry-specific applications including medicine and finance. For instance, Med-PaLM 2 [41] has been adapted for medical domains to provide accurate responses to medical queries, while Bloomberg's financial LLM [50] supports various NLP tasks within the financial sector.\nIn the finance industry, beyond quantitative data typically analyzed, the sentiment and tone of financial reports, earnings calls, news articles, and social media posts significantly influence investor decisions. Therefore, extracting and analyzing relevant textual information is critical for informed investment strategies and decision-making [12, 16]. Despite the advancements of general-domain LLMs, they often fall short when applied to specialized fields like finance due to complex terminologies and intricate concepts. This underscores a need for domain-specific adaptations to fully realize the potential of LLMs in financial applications.\nExisting research highlights the success of LLMs in financial tasks, such as predicting stock price movements [24, 28, 53] and performing advanced financial text analytics [13, 14, 52, 54]. However, significant challenges remain, particularly in tasks like financial relation extraction and numerical reasoning, which are crucial for making well-informed decisions [24, 52]. The efficient market hypothesis [29] further underscores the importance of linking public"}, {"title": "Our approach and Contributions", "content": "In our study, we aimed to address these challenges and improve the performance of smaller LLMs on specialized financial tasks by focusing on in-context learning, fine-tuning and model merging techniques. By leveraging these approaches, we aim to improve the performance of small LLMs in specialized financial tasks while maintaining their general capabilities on unseen tasks. We explored the potential of Mistral-7B, Phi-3, and Llama2-8B, which are powerful yet resource-efficient models, across seven representative financial text classification tasks: sentiment analysis, news headline classification, relation extraction, hawkish-dovish classification, argument unit classification, deal completeness classification and causal classification. These models were selected to address the challenge of adapting LLMs to complex, domain-specific tasks while managing computational costs. By concentrating on smaller models, we aim to provide a more scalable solution that balances performance with efficiency in the financial sector.\nWe first assessed the in-context learning capabilities of selected models on the specified tasks. We experimented with one-shot, five-shot and ten-shot examples and compared them against zero-shot performance of the models. The results"}, {"title": "RELATED WORK", "content": "This section reviews recent studies that apply Large Language Models (LLMs) to domain-specific tasks. Prior to the rise of LLM-focused research, many studies concentrated on utilizing BERT, an encoder-only model, for improving performance through fine-tuning, developing self-attentive mechanisms, and incorporating multiple lexical knowledge sources to better capture semantic context, specially in the finance domain [10, 34, 39, 51, 59]. However, this paper"}, {"title": "Training from Scratch", "content": "Training domain-specific language models from scratch remains a straightforward method for achieving domain adaptation. BloombergGPT is an early example of large language models built specifically for the financial domain. It was trained on a blend of financial and general corpora, and showing high performance on financial tasks [50]. Similarly, other studies transformed large-scale raw domain-specific corpora into reading comprehension task, allowing LLMs to acquire domain knowledge and improve prompting capabilities [6]. However, training from scratch demands substantial computational resources and large dataset, making it less practical than other methods such as In-Context Learning (ICL) or fine-tuning, which require fewer resources [56]."}, {"title": "In-context Learning", "content": "Scaling model size and leveraging extensive pre-training data have unlocked emergent capabilities in LLMs, such as In-Context Learning (ICL) [3, 8, 49]. This allows models to learn from a few in-context provided instructed examples without requiring full retraining [37]. Proprietary models like ChatGPT and GPT-4 have demonstrated strong performance across various general tasks [43, 64, 66]. However, when applied to specialized domains like finance and social media, their performance often lags behind that of fine-tuned models like BERT, particularly for tasks requiring deeper semantic understanding [48]. Increasing the number of in-context examples can also lead to performance degradation in certain domain-specific tasks [64].\nIn finance, Li et al. [24] examined the performance of ChatGPT and GPT-4 across several financial text analytics tasks. Another study enhanced financial sentiment analysis using semi-supervised learning and Chain-of-Thought reasoning to improve accuracy with minimal prompting [9]. Although these models outperform some domain-specific models in certain tasks, they struggle with tasks like named-entity recognition and headline classification, which require deeper semantic analysis. These limitations motivate our research into the performance of open-source instruction-tuned models in the finance domain."}, {"title": "Instruction Fine-tuning", "content": "Instruction fine-tuning is a resource-efficient approach for adapting LLMs to specific tasks. It allows models to adjust to domain-specific needs without the need for extensive retraining [37, 63]. This technique has proven effective across domains like medicine, law, and social sciences, where fine-tuned models often outperform general-purpose models like GPT-4 [7, 19, 23, 27, 60, 65].\nIn the finance domain, several studies have applied instruction fine-tuning to models like LLaMA using financial datasets. Xie et al. [54] fine-tuned LLaMA on 136k task-specific instruction samples, including sentiment analysis, named-entity recognition, question answering, and stock movement prediction, achieving results comparable to GPT-4. Yang et al. [56] developed an end-to-end framework for training and deploying FinLLMs in the finance sector using Low-Rank Adaptation (LoRA) to fine-tune LLaMA and ChatGLM models using 50k data points from news articles, social media posts, Sec fillings and stock movement predictions. Zhang et al. [61] also applied fine-tuning to financial"}, {"title": "Merging Models", "content": "In addition, previous studies show that fine-tuning on domain-specific data can degrade performance on unseen tasks. To address this, some researchers in non-financial domains have incorporated both generic and domain-specific instruction data, but this adds significant computational costs [46, 47]. In the finance domain, [54, 61] have shown that fine-tuning improves performance on trained datasets, however, these studies have not examined performance on unseen finance-related tasks. To fill this gap, we evaluate the performance of fine-tuned models on three unseen finance-related datasets. In contrast, our study takes a more efficient approach: rather than augmenting training data with generic datasets, we fine-tuned instruction models on each task individually and applied the MergeKit framework to combine the single-task fine-tuned models with the vanilla instruction model, thus preserving performance on unseen tasks while minimizing computational overhead."}, {"title": "EXPERIMENT SETUP", "content": "In this section, we provide a detailed overview of the experimental setup used for both in-context learning (ICL) and fine-tuning experiments. This setup includes the selection of models, the configuration of training parameters, and the choice of datasets, all carefully designed to assess the effectiveness of various approaches in adapting large language models to domain-specific financial tasks. By standardizing these conditions, we aim to ensure a fair comparison between different techniques, ultimately highlighting the strengths and limitations of each method in enhancing model performance across specialized financial text classification tasks."}, {"title": "Models", "content": "We conduct ICL and fine-tuning experiments using various open-source small models to examine the effects of different model sizes and pre-training datasets. All models employed in this study utilize a decoder-only transformer architecture. We selected three open-source models because they are among the most powerful small-size open-source models available. We fine-tune both the instruct and base models for the Llama3 and Mistral models, and only the instruct model for the Phi3 model since the base model was not available. It should be noted that ICL experiments were exclusively conducted on the instruct models."}, {"title": "Tasks and Datasets", "content": "We conduct ICL and fine-tuning experiments on four financial text classification tasks: sentiment analysis, relation extraction, news headline classification, and hawkish-dovish classification 1. Details of the dataset are provided in Table 1, and splitting methods and statistics are presented in Table 22 in appendix .1.\nSentiment Analysis is a critical tool in finance, used to predict market trends and investment behavior by analyzing news and social media data [24, 32]. Timely extraction of sentiment from these sources is crucial for decision-making by traders and investors. Following BloombergGPT's method [50], we use two sentiment datasets, reserving 20% of the labeled data for testing.\n(1) Financial PhraseBank [30]: Financial PhraseBank [30]: This dataset contains sentiment classifications (positive, negative, neutral) derived from financial news, annotated by 5-8 individuals. We focus on the subset with at least 50% agreement among annotators.\n(2) FiQA Sentiment Analysis [58]: This dataset comprises 961 samples, each annotated with one of three labels: positive, neutral, or negative.\nNews Headlines Classification focuses on extracting actionable information from news beyond basic sentiment analysis, which is valuable for investors, policymakers, and market practitioners. Utilizing the Headlines dataset [42] with 11,412 annotated news headlines about \"gold\" from 2000 to 2019, we extract additional dimensions like price movements. Specifically, we focus on a subset converting it into a three-class dataset identifying gold prices as Up, Down, or Stable.\nHawkish-Dovish Classification involves categorizing Federal Open Market Committee (FOMC) monetary policy statements as either hawkish or dovish. This classification is significant due to its impact on financial market returns. Conventional sentiment analysis models, which typically categorize text as positive or negative, struggle to capture the nuanced policy stance in these texts accurately. As mentioned in the study, for example, a sentence containing the word \"increase\" could be either dovish or hawkish depending on context, without necessarily conveying negativity. To address this challenge, we utilized a dataset where FOMC statements were annotated as Hawkish, Dovish, or Neutral, following the splitting method outlined in the original study [38].\nRelation extraction plays a crucial role in financial text analysis by identifying relationships between entities, which supports tasks like knowledge graph creation, question answering, and semantic search. FinRed dataset focuses on this task and identifys relationships in financial news and earnings transcripts. We transform the dataset into a"}, {"title": "Unseen Tasks and Datasets", "content": "To assess the generalizability of our fine-tuned models, we evaluated their performance on three unseen tasks. Details of the dataset are provided in Table 1, and in Table 22 in appendix .1\nArgument Unit Classification goes beyond sentiment analysis by exploring the detailed elements of market dynamics and financial events. It entails identifying and categorizing specific units or segments of arguments within earnings conference call data. This classification is fundamental for a detailed breakdown of financial narratives, facilitating better comprehension and analysis. We used FinArg AUC dataset to classify sentences as either claims or premises, facilitating a more nuanced analysis of financial narratives [44].\nCausal Classification identifies implicit causal relationships within financial documents. Using the SC dataset, sentences from financial news and Securities and Exchange Commission (SEC) filings were classified as either causal or noise, highlighting the underlying causes that influence market trends [31].\nDeal Completeness Classification focuses on determining the status of mergers and acquisitions (M&A) events, distinguishing between completed deals and ongoing rumors. The dataset includes news articles and tweets related to M&A events, with each instance describing a potential deal between an acquirer and a target company, including IPO rumors. Each instance is categorized as either complete (successful deal) or rumor (no deal materialized). This task is crucial due to the impact of M&A deal rumors on the share price volatility of target firms, influencing cumulative abnormal returns [57]."}, {"title": "Methodology", "content": "In this section, we outline the methodologies employed in our study, beginning with a brief introduction to the setup of instruction tuning for both base and instruct models. Finally, we describe the merging framework that combines task-specific and base models to enhance generalizability."}, {"title": "Instruction Dataset for Instruct Models", "content": "We adopt the methodology outlined in [47] to create detailed instructions that aid the model in understanding various tasks. Each task instance in our instruction dataset is structured with three main components: task instruction, input text, and output.\nThe task instruction provides a guide for the task to be performed based on the input text, along with the expected labels for each task. A comprehensive list of task instructions for each task is provided in Table 25 in Appendix .1. We then create each sample from the original dataset by combining the instruction, input, and output in a specific format. For the Instruct models, we generate a prompt for each model by including the special tokens specified in Table 23 in Appendix .1. For test datasets, we enclose the instructions and input within the corresponding special tokens for each model, while using label delimiters like \"label:\" as suggested in [26]."}, {"title": "Instruction Dataset for Base Models", "content": "Following the approach of [45], we designed an instruction scheme to aid the model in understanding the task. As detailed in Table 24 in Appendix.1, the instruction scheme includes a"}, {"title": "Instruction Fine-tuning", "content": "We conducted instruction fine-tuning on all three models using HuggingFace Transformers with 2 epochs and the Adam-w torch fused optimizer. The batch size was set to 2, with an initial learning rate of 2e-4 and warm-up steps comprising 3% of all training steps. The maximum length of input texts was 1024 for the Phi-3 model and 2048 for the Mistral-7B and Llama3-8B models. All models were fine-tuned on 8 A100 40GB GPUs.\nTo optimize the fine-tuning process and reduce the computational cost, we employed techniques such as Low-Rank Adaptation (LoRA) with a rank set to 32 and Lora alpha set to 46, alongside quantization [18]. LoRA enables fine-tuning the low-rank decomposed factors of the original weight matrices rather than the full matrices, significantly reducing trainable parameters. This approach allows training on less powerful hardware and shortens the total training time [25]. We experimented with two fine-tuning settings:\n(1) Single-Task Fine-Tuning: Each model underwent separate fine-tuning on the dataset specific to each task, as depicted in Figure 1a, utilizing the specified training settings. These fine-tuned models are subsequently employed in the next section (Model Merging) to enhance the generalizability of the models on the unseen tasks.\n(2) Multi-Task Fine-Tuning: Each base and instruct model underwent fine-tuning using a combination of instruction datasets, covering all tasks, as depicted in Figure 1b. The total dataset consists of 13,194 training examples, as outlined in Table 22 in Appendix .1."}, {"title": "Model Merging", "content": "Further fine-tuning of pre-trained models can lead to catastrophic forgetting, degrading general capabilities and reducing performance across tasks [63, 65]. To mitigate this, leveraging existing pre-trained checkpoints is crucial. Model merging, combining parameters from multiple models trained on specific tasks into a unified model, has become essential. This strategy supports multi-task and continual learning, reducing catastrophic forgetting without retraining costs [15, 55].\nOur study utilized MergeKit, a library for model merging, employing Task Arithmetic [20]. This technique involves arithmetic on task vectors, representing differences between fine-tuned models and a common base model. Task Arithmetic enhances generalizability and performance across diverse tasks, effectively mitigating catastrophic forgetting [15]. We utilize single-task fine-tuned models on LlaMA3-8B and Mistral-7B models, as described in the previous section, and merge them with the vanilla instruct models for the corresponding models (LlaMA3-8B and Mistral-7B) with equal 25% weight for each of the models 2, as shown in Figure 1c."}, {"title": "Baseline Models", "content": "We compare the fine-tuned and merged models against three baseline vanilla instruct models and three specialized financial models:\n(1) FinMA-7B [54]: A 7B parameter model instruction-fine-tuned for financial tasks, including multiple NLP and forecasting tasks.\n(2) AdaptLLM-7B [6]: A model continued pre-trained and fine-tuned on financial news.\n(3) GPT-4 3: A robust model from OpenAI.\nIn the next section, we will review the results of our experiments in detail, focusing on the comparative performance of the models under different settings, including zero-shot, few-shot, and multi-task fine-tuning scenarios. We will also"}, {"title": "RESULTS AND ANALYSIS", "content": "In this section, we present a comprehensive analysis of the model performance across various experimental setups, including zero-shot, few-shot, multi-task fine-tuning, and model merging techniques. We also provide a detailed error analysis and summarize the key findings."}, {"title": "Evaluation Framework", "content": "Following standard practices, We evaluate the models using standard classification metrics such as accuracy, and weighted F1 score to assess their performance on financial text classification tasks. The tasks include sentiment analysis, relation extraction, news headline classification, and hawkish-dovish classification. All results are compared against baseline models, including state-of-the-art models like GPT-4 and FinMA-7B, to provide a benchmark for evaluating improvements. We also report and discuss the results of based-fine-tuned, instruct-fine-tuned, and merged models compared to the baselines across four datasets."}, {"title": "Zero-Shot Performance Analysis", "content": "As shown in Figure 3 and Table 3, the zero-shot performance of the vanilla instruct models (Phi-3, Mistral, and Llama3) was relatively strong on the Financial PhraseBank (FPB), FIQA Sentiment Analysis (FIQA-SA), and headline classification datasets. The Llama3 and Phi-3 models demonstrated notable accuracy in these tasks, while the Mistral model performed below 50% on datasets that included tweets, likely due to its pre-training corpus lacking similar data.\nHowever, all models struggled on the FOMC and FinRed datasets, showing suboptimal results. The FOMC dataset's complex monetary policy texts and the FinRed dataset's extensive label set of 29 categories posed significant challenges, even for the larger Llama3 model. This performance issue likely stems from the long-tail distributions and the intricate"}, {"title": "Few-Shot Results", "content": "To explore the impact of few-shot learning, we conducted experiments using 1-shot, 5-shot, and 10-shot setups, employing three different random seeds for sampling demonstration examples, following the methodology in [48] and reported the average performance. The few-shot results, compared against zero-shot results, are summarized in Figure ?? and Table 2.\nWe observed that 1-shot prompting generally improved model performance across most datasets, except for the Phi-3-mini model, where performance slightly decreased on sentiment analysis tasks due to the adverse effects of unrelated examples on smaller language models. Increasing the number of shots to 5 and 10 produced mixed results: while some models like Llama3-8B and Mistral-7B showed only slight declines, others, particularly Phi-3-mini, experienced high variability in performance, indicating its sensitivity to the number of examples used.\nNotably, all models demonstrated a significant drop in performance on the headline classification dataset when the number of demonstrations increased, even falling below the performance in the zeros-shot setting. This decline suggests that few-shot settings can sometimes introduce noise, leading to a decrease in model accuracy, particularly for smaller models [64].\nHowever, models like Llama3-8B and Mistral-7B displayed only slight decreases on other datasets. Among all models, Phi-3-mini showed the highest performance variability when the number of demonstrations increased, underscoring the sensitivity of smaller models to few-shot settings, as previously highlighted in the literature [64]. Due to consistently"}, {"title": "Multi-Task Fine-Tuning", "content": "Our multi-task fine-tuning experiments demonstrated more stable behavior across models, as illustrated in Table 3 and Figures 3 and 4. Fine-tuning led to a substantial increase in model performance compared to zero-shot settings in both base and instruct models. Notably, the instruct fine-tuned Llama3-8B model showed a dramatic improvement from 6.5% to 73.4% on the relation extraction task (FinRED dataset), with similar gains observed in the Hawkish-Dovish classification tasks (FOMC dataset). Mistral-7B and Phi-3 models showed an average 40% performance improvement after fine-tuning on both base and instruct models.\nAdditionally, all models outperformed GPT-4 on these finance-related tasks, highlighting the limitations of GPT-4 in specialized domains such as finance and the necessity for task-specific fine-tuning. In sentiment analysis tasks (FPB and FiQA-SA datasets), our models performed comparably to the state-of-the-art FinMA-7B model, surpassing other models like AdalpLLM-7B and BloombergGPT.\nThe results indicated that multi-task fine-tuned instruct models outperformed their base counterparts across all datasets, except the Mistral-7B base model, which slightly outperformed the instruct variant on most tasks. This variance could be attributed to differences in the quality of supervised fine-tuning prompts, suggesting that starting with a robust instruction-tuned checkpoint may offer advantages for domain-specific fine-tuning.\nThe results of the multi-task fine-tuned Phi-3 models also demonstrate performance on par with the other two models, despite having only 3.8 billion parameters. This highlights the potential of the Phi-3 model for fine-tuning downstream financial tasks."}, {"title": "Generalization Multi-Task Fine-Tuned model to Unseen Tasks", "content": "The almost similar behavior observed in the multi-task fine-tuning of base and instruct models (using the same amount of data without incorporating generic instruction data) on seen tasks indicates that both models benefit nearly equally from fine-tuning on tasks present in the training corpus. However, our results on three unseen financial tasks, as shown in Table 3 and Figures 5 and 6, indicate that multi-task fine-tuned base models exhibit significantly greater performance declines compared to their instruct-tuned counterparts when generic instruction data is absent during fine-tuning. This highlights the risk of performance degradation for base models on tasks they haven't encountered, a trend consistent with prior research on catastrophic forgetting [46, 65]. In contrast, instruct-tuned models retain stronger generalization capabilities, making them more robust in handling unseen tasks.\nFor example, on the Argument Unit Classification task, the Llama3-8B model's performance drops to 13.4%, while on the Causal Classification task, the Mistral-7B model experiences a decline to 31.8%. Notably, the Phi-3-mini model, despite its smaller size of 3.8 billion parameters, shows only a 2% performance drop, suggesting that it retains its general capabilities better than larger models after fine-tuning. These findings emphasize the effectiveness of instruct-tuned"}, {"title": "Merged Models", "content": "To mitigate the degradation of zero-shot performance on unseen tasks, we utilized the MergeKit framework (arithmetic method), which combines single-task fine-tuned models with the vanilla instruct model. The merging approach significantly enhances performance on unseen tasks, with results often surpassing the original zero-shot performance, significant for the FinArg dataset.\nThe results for the Llama3-8B and Mistral-7B models are shown in Figure 5. For example, the merged models outperformed both the fine-tuned, base models and zero-shot in handling complex tasks, suggesting that the integration of diverse task knowledge leads to a more generalized and robust performance. The performance of single-task fine-tuning on both base and instruct models is presented in Appendix .2.\nThe merging strategy helps to preserve the benefits of fine-tuning while reducing the risk of catastrophic forgetting on unseen datasets without incorporating a generic instruction dataset, which increases computational cost and time. While fine-tuning improves performance over base models, merging fine-tuned models further enhances their ability to handle unseen tasks by a) Combining diverse knowledge sources, b) Reducing overfitting, and c) Leveraging complementary strengths of different models. The regularization effect from merging also helps to produce a more generalized and robust model performance across diverse tasks.\nWe can also observe that the performance drop of the Phi3 base model compared to its fine-tuned counterpart is not much compared to the other two models (Llama3 and Mistral), as indicated in 6. This could be due to the following reasons.\n\u2022 Model Architecture: Phi3 may have an architecture that is more resistant to overfitting during fine-tuning. It's possible that Phi3 has better built-in regularization mechanisms or a structure that promotes better generalization.\n\u2022 Pre-training Approach: The pre-training approach used for Phi3 might result in more robust general knowledge that is less easily overwritten during fine-tuning. This could lead to better retention of broad capabilities even after task-specific training.\n\u2022 Model Size: If Phi3 is a smaller model compared to Mistral and Llama3, it might have less capacity to overfit to the fine-tuning data, paradoxically leading to better generalization on unseen tasks.\nThe significant performance drops in Mistral and Llama3 suggest that these models might be more susceptible to overfitting during fine-tuning, possibly due to larger model sizes, different architectures, or fine-tuning approaches that allow for more dramatic changes to the model's parameters. This difference highlights the importance of careful fine-tuning practices and the need to consider the trade-offs between task-specific performance and general capabilities when adapting large language models to specific tasks. It also demonstrates why techniques like model merging can be valuable in recovering and combining the strengths of different model versions."}, {"title": "Error Analysis and Model Insights", "content": "Our error analysis reveals significant improvements in fine-tuned models across various tasks, particularly in financial sentiment detection, relation extraction, news headline classification, and hawkish-dovish classification. These enhancements stem from the models' increased ability to capture domain-specific nuances, understand hierarchical corporate structures, and interpret economic indicators. In financial sentiment detection, fine-tuned models excel at identifying subtle cues and handling mixed sentiments more effectively. For relation extraction and news headline classification, fine-tuning leads to a better interpretation of corporate ties and financial movements. Similarly, in the hawkish-dovish"}, {"title": "Financial Sentiment Classification Result Analysis", "content": "Fine-tuning yielded significant performance gains in financial sentiment classification, particularly in the following areas:\n1. Domain-Specific Knowledge. Fine-tuned models demonstrated improved comprehension of financial language and context. For example, in Table 4, fined-tuned models correctly interpreted growth statements in a financial context, where the base models failed to capture the nuance.\n2. Improved Sentiment Detection. Fine-tuned models showed enhanced capability in detecting subtle sentiment cues within financial statements, such as recognizing negative sentiment related to financial losses, as illustrated in Table 5.\n3. Better Handling of Mixed Sentiment. When encountering statements containing positive and negative elements, fine-tuned models, particularly Mistral, identify the prevailing sentiment better, as shown in Table 6."}, {"title": "FOMC Classification Result Analysis", "content": "Fine-tuned models also showed marked improvements in interpreting monetary policy statements from the Federal Open Market Committee (FOMC). This was particularly evident in their ability to interpret economic indicators and broader economic contexts.\n1. Understanding Economic Indicators. Fine-tuned models better interpret the impact of economic indicators and the relationship between them like unemployment rates.\nFor example, as illustrated in Table 9, the base models did not interpret the economic indicators correctly and classified this example as neutral. In contrast, fine-tuned models correctly identify it as Hawkish, recognizing that low unemployment and robust job gains typically lead to tighter monetary policy to prevent overheating.\n2. Improved Contextual Interpretation. Fine-tuned models show marked improvement in interpreting broader economic contexts.\nThe example of table 10, indicates the base models often interpret the examples incorporating economic contexts as neutral, while fine-tuned models correctly identify it as Dovish, recognizing that \"sustained expansion\" typically implies a continuation of accommodative policy."}, {"title": "FinRED Dataset Error and Improvement Analysis", "content": "In the domain of relation extraction from financial texts, fine-tuned models displayed significant improvements across several dimensions:\n1. Domain-Specific Knowledge Acquisition. Fine-tuned models have been exposed to a large volume of financial and corporate relationship data, allowing them to learn domain-specific nuances that base models may not have encountered.\n2. Contextual Interpretation. Fine-tuned models develop an enhanced ability to interpret contextual cues within sentences, leading to more accurate relationship classification.\n3. Hierarchical Relationship Understanding. Through exposure to various corporate structures, fine-tuned models better understand the nuances of organizational hierarchies and roles.\nIn the case presented in table 15, the fine-tuned models identified the nuanced hierarchical relationship (\"parent organization\") between Kering and Gucci, while the base models defaulted to \"owner,\" failing to capture the specific corporate structure. This shows the improved ability of fine-tuned models to comprehend the complexities of organizational hierarchies. This examples shows how fine-tuned models develop a deeper understanding of hierarchical relationships, particularly in corporate structures, where roles and organizational layers are not always straightforward.\n4. Entity-Relationship Mapping. Fine-tuned models develop a more sophisticated understanding of how different entities (companies, products, people) typically relate to each other in the corporate world.\n5. Reduction of Overgeneralization. Fine-tuned models learn to avoid defaulting to common but incorrect relationships when faced with ambiguity, a problem often seen in base models.\n6. Handling of Complex Sentences. Financial texts often contain complex, information-dense sentences. Fine-tuned models learn to parse these more effectively."}, {"title": "Merging Models", "content": "To mitigate the degradation of zero-shot performance on unseen tasks, we utilized the MergeKit framework (arithmetic method), which combines single-task fine-tuned models with the vanilla instruct model. The merging approach significantly enhances performance on unseen tasks, with results often surpassing the original zero-shot performance, significant for the FinArg dataset.\nFor the above entry, the Llama3 fine-tuned model incorrectly classified it as a rumor, while both the base and merged models correctly identified it as complete. This suggests that the fine-tuned model might have overfit to certain patterns, while the merged model was able to correct this error.\n2. Reduced Overfitting and Regularization. Merging can act as a form of regularization, helping to average out overspecialized patterns and less likely to overfit to specific patterns in the fine-tuning data. It promotes more general features, making the models more robust on unseen tasks.\n3. Complementary strengths. Different models may excel at different aspects of the task. Merging allows the combined model to leverage the strengths of each constituent model as shown in table ??"}, {"title": "CONCLUSION AND FUTURE WORK", "content": "In this study, we evaluated the in-context learning (ICL) capabilities of three small instruct models-Llama3-8B, Mistral-7B, and Phi-3-across various financial classification tasks. The results highlighted variability in model performance with increasing numbers of shots. Overall, ICL did not significantly enhance the models' ability to learn downstream tasks from examples, particularly for smaller models. Among the three, Llama3-8B exhibited marginally better performance, though the gains were not substantial.\nBeyond ICL, our primary focus was on the instruct fine-tuning of both the base and instruct versions of these models for four key financial tasks: sentiment analysis, news headline classification, relation extraction, and hawkish-dovish classification. The results demonstrate that multi-task fine-tuning of instruct models significantly improves task-specific performance, particularly for complex tasks such as relation extraction and hawkish-dovish classification. These findings underscore the potential of fine-tuning smaller LLMs for domain-specific tasks in finance."}, {"title": "Appendix A", "content": "We present statistics regarding the datasets utilized for instruction fine-tuning of both base and instruct models. Subsequently, we provide descriptions of the instructions and prompt templates employed for each task during instruction fine-tuning."}, {"title": "Appendix B", "content": "We present the results of single-task fine-tuning of base and instruct models in Table 25 and Figures 7, 8, and 9. The results indicate that single-task fine-tuning performs similarly to multi-task fine-tuning. In some datasets, single-task fine-tuning even relatively outperforms multi-task fine-tuning for base models. For merging the models, we utilized the single-task fine-tuned instruct models, which performed similarly to multi-task fine-tuned models, to alleviate the degradation of zero-shot performance of fine-tuned models on unseen tasks."}, {"title": "Base Model Train Prompt Template", "content": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n###Instruction:\n{instruction}\n###Input:\n{input}\n###Response:\n{output}"}]}