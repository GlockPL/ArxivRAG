{"title": "Towards Better Multi-task Learning: A Framework for Optimizing Dataset Combinations in Large Language Models", "authors": ["Zaifu Zhan", "Rui Zhang"], "abstract": "To efficiently select optimal dataset combinations for enhancing multi-task learning (MTL) performance in large language models, we proposed a novel framework that leverages a neural network to predict the best dataset combinations. The framework iteratively refines the selection, greatly improving efficiency, while being model-, dataset-, and domain-independent. Through experiments on 12 biomedical datasets across four tasks\u2014named entity recognition, relation extraction, event extraction, and text classification\u2014we demonstrate that our approach effectively identifies better combinations, even for tasks that may seem unpromising from a human perspective. This verifies that our framework provides a promising solution for maximizing MTL potential.", "sections": [{"title": "1 Introduction", "content": "Natural Language Processing (NLP) has made significant strides in recent years (Liu et al., 2023), evolving from fully supervised learning (Kotsiantis et al., 2007), to feature engineering (Patil et al., 2023), architecture innovations like Transformer (Vaswani et al., 2017), and the dominance of pre-trained large models such as BERT and GPT (Devlin et al., 2018; Radford et al., 2018, 2019). More recently, the instruction-tuning (Zhang et al., 2023) and prompting engineering (Liu et al., 2023) have emerged, allowing Large Language Models (LLMs) to handle tasks effectively through prompting (Wei et al., 2022). With the rapid advancements in NLP, Multi-Task Learning (MTL) has emerged as a powerful technique to boost model performance by jointly training on multiple related tasks (Zhang and Yang, 2018, 2021), as illustrated in Fig. 1. By sharing knowledge across tasks, MTL enhances model generalization (Wang et al., 2021) and efficiently captures the complementary relationships between tasks (Ma et al., 2018). For instance, the Named Entity Recognition (NER) task and the Relation Extraction (RE) task are closely linked-accurate entity recognition can provide a critical context for extracting relationships, while relation extraction, in turn, can refine entity boundaries and classifications. As models become larger, the amount of data used for training has also significantly increased (Chen et al., 2024), giving rise to models capable of understanding and solving problems across various domains. For instance, ChatGPT can generate realistic and creative outputs across various domains (Yenduri et al., 2024). Previously, achieving MTL required modifying the model architecture and adjusting the output layers for different tasks (Misra et al., 2016). Now, by simply modifying the prompt, MTL has found greater utility and flexibility without re-designing the architecture (Liu et al., 2023). This indicates that large models have truly become tools accessible to everyone. As long as users know how to utilize frameworks like Hugging Face (Wolf et al., 2020), they can fine-tune models with their own prompts without needing any knowledge of Attention mechanisms or model architecture. To take advantage of the natural compatibility of MTL and LLMs, many recent large models have improved performance by incorporating multi-task training. Models like DeepStruct (Wang et al., 2022a), InstructUIE (Wang et al., 2023), and"}, {"title": "2 Related Work", "content": "There have been several attempts to explore how to select dataset combinations to improve MTL (Zhang and Yang, 2018, 2021; Thung and Wee, 2018; Sener and Koltun, 2018; Crawshaw,"}, {"title": "3 Framework", "content": "The most straightforward approach to evaluate the effect of all possible dataset combinations on a given LLM would be to directly do experiments for each combination via fine-tuning and inference of the corresponding LLM. However, this naive method is computationally expensive and requires substantial resources. By contrast, multi-layer neural networks are much faster to compute. If a fast neural network could effectively filter out combinations that are unlikely to yield good results, or directly predict the best combination, it would save significant time and computational power. Driven by this idea, we propose a new framework to optimize dataset selection for MTL. As shown in Fig. 2, our proposed framework consists of four parts. First, we generate sufficient combinations of datasets to at least cover all datasets. Next, fine-tuning LLMs on these dataset combinations. After inference for each combination, we record all combinations and their corresponding performance scores in a table. Using this table data, we then train a neural network to predict the best dataset combination by enumerating all possible combinations through the neural network, which could predict the best combination in a super-efficient way. The framework iteratively trains the large model with the predicted combinations and tests the results. The process continues until the neural network predicts an optimal combination that has already been tested, at which point the loop terminates. The proposed framework offers several advantages. First, it saves time by using a neural network to infer relationships between datasets, allowing us to focus on testing the most promising combinations based on existing data. Second, it is highly flexible, applicable across different models and dataset pools, and adaptable to various neural network architectures for predicting the optimal combination. Lastly, it is robust. Even if the initial selections are limited or random, and the neural network struggles to predict good combinations due to insufficient data, the iterative process ensures that as more data is gathered, the system will eventually find a good (or even optimal) combination. The inspiration for this framework comes from the concept of feedback in control systems (Doyle et al., 2013). In a data-driven system, finding the optimal controller requires first accumulating a certain amount of data and then trying to control the systems using the controller trained by the accumulated data (Kiumarsi et al., 2014). With each attempt, more feedback is gathered, allowing the system to refine the controller. The better the controller becomes, the closer it gets to finding the optimal solution. Similarly, in our framework, we begin by generating various dataset combinations and get the corresponding performance score for each combination by inference. The neural network, much like a feedback-driven controller, gives lower scores to poor combinations and higher scores to promising ones. As the process continues, the neural network encourages the exploration of good combinations and discourages poor ones. With more iterations, the system converges towards identifying the optimal dataset combination."}, {"title": "4 Experiment setup", "content": ""}, {"title": "4.1 Dataset", "content": "In this paper, we focus on four critical NLP tasks: EE, RE, NER, and TC. For each of these tasks, we selected three datasets to evaluate the potential of"}, {"title": "4.2 Model", "content": "The specific model used in this paper is not the focus of our paper, as the proposed framework is designed to be applicable to any LLMs. Therefore, we opted for a representative open-source model: LLama3-8B. It was selected due to its widespread use and strong performance across various NLP tasks, making it an ideal candidate for demonstrating the effectiveness of our dataset selection framework. Additionally, its open-source nature ensures reproducibility and allows for further experimentation by the research community."}, {"title": "4.3 Data preparation", "content": "To begin, it is necessary to generate several dataset combinations as basic data (including the single task learning as baseline), which will later allow the neural network to learn the relationships between datasets. For the 12 datasets we selected, we run combinations both within the same task and across different tasks. These combinations ensure that all datasets are involved, providing a diverse and comprehensive basis for understanding how different datasets interact and contribute to overall performance."}, {"title": "4.4 Neural Network", "content": "The choice of neural network architecture is flexible. In our experiment, we used a two-layer neural network with 12 inputs and 1 output to perform a regression task. The 12 inputs correspond to the 12 datasets: if a dataset is used, the corresponding input is set to 1; if it is not used, the input is set to 0. The output of the neural network is the score we aim to predict, such as the F1 score in our experiments. This design directly establishes a relationship between the use of specific datasets and the resulting F1 score. Each time a new combination of datasets is trained and tested, the neural network is trained again. After training, we input all possible combinations and use the predicted F1 scores to determine the best combination for the next iteration and keep optimizing model performance."}, {"title": "4.5 Instruction-tuning", "content": "The prompt examples we used are shown in Fig. 3. The training prompt example includes instruction, an input sentence, and its corresponding response. By contrast, the testing prompt is empty in response, which allows the model to continue to generate the next tokens. \\n\\n is the ending mark for completing the response generation. Then in the evaluation stage, we will extract output tokens before the ending token. Since the size of each dataset varies, to be fair, we instruction-tuned each model for 5000 steps, no matter how many tasks we included. We saved the model every 1000 steps and used the best model for the latter generation. When using multiple datasets, we sampled evenly from each one to make sure they were equally weighted."}, {"title": "4.6 Metrics", "content": "In the evaluation stage, we used Micro Precision, Recall, and F1-score. When using these metrics, a prediction is considered correct only if the entire predicted output exactly matches the ground truth."}, {"title": "5 Results", "content": ""}, {"title": "5.1 Data preparation for RE task", "content": "We included all attempts related to the RE task in Table 1. As shown, the combinations involving BioRED, DDI demonstrate performance improvements for some specific combinations, while for the remaining GIT datasets, the baseline achieved the highest F1 score. Specifically, for the BioRED"}, {"title": "5.2 Find better combination for datasets in\nthe RE task iteratively using the proposed\nframework", "content": "We applied the framework to three datasets in the RE task separately. The framework was set to automatically run for 48 hours, exploring better dataset combinations and stopping once sufficient exploration was achieved, keeping only the best model and the highest F1 score."}, {"title": "5.3 Ablation study for other tasks", "content": "Due to space limitation, the different combinations involving the other 9 datasets for NER, EE, and TC tasks are presented in the appendix, in Tables 2, 3, and 4, respectively. For the EE task, during the data preparation phase, we observed that several dataset combinations could improve LLM performance on GENIA2011 and GENIA2013, further demonstrating the potential of MTL. After applying our framework, the performance of GENIA2013 improved significantly from below 40 to around 58, a substantial gain. This result is corroborated by the improvements in GENIA2011, where an effective combination was tested early in the data preparation stage, boosting performance from a baseline of 34 to 57. Given the high similarity between GENIA2011 and GENIA2013, this suggests that if performance can be enhanced in GENIA2011, similar improvements should likely be achieved for GENIA2013. Although the initial combinations differed, our framework was able to find similar combinations for both datasets, ultimately achieving comparable results. For the NER and TC tasks, the six datasets involved all achieved their best F1 scores with the baseline settings, and every combination we attempted resulted in a performance decline. From an expert perspective, it appeared that MTL could not leverage the other datasets in this paper to improve these six datasets. However, we continued to use our framework to investigate whether a specific combination could improve these seemingly unpromising datasets. Surprisingly, as shown in Figures 5(d), 5(f), and 5(h), our framework identified promising combinations that led to significant improvements for three of the datasets, despite initial expectations to the contrary. For the remaining three datasets (BC4CHEMD, ADE, PubMed20krct), we were unable to find better combinations, which aligns with our initial expectations. Summarizing all the experiments, each dataset"}, {"title": "6 Conclusion", "content": "We proposed a novel yet simple framework to address the challenge of selecting the optimal dataset combination for multi-task learning. By iteratively refining these combinations within a feedback loop, we take a significant step toward fully unlocking the potential of MTL in the future."}, {"title": "7 Limitations", "content": "The experiments in this paper were conducted using only a single LLM. Although the authors intended to experiment with multiple LLMs to explore broader performance variations, the computational resources and time required were prohibitive. As a result, the findings may not fully represent the potential of our framework. Secondly, we did not perform a grid search to find the optimal hyperparameters for model training. Instead, we ensured that all experiments were conducted with the same set of parameters to maintain fairness."}]}