{"title": "MEDCO: Medical Education Copilots Based on\nA Multi-Agent Framework", "authors": ["Hao Wei", "Jianing Qiu", "Haibao Yu", "Wu Yuan"], "abstract": "Large language models (LLMs) have had a significant im-pact on diverse research domains, including medicine and healthcare. However, the potential of LLMs as copilots in medical education re-mains underexplored. Current AI-assisted educational tools are limited by their solitary learning approach and inability to simulate the multi-disciplinary and interactive nature of actual medical training. To address these limitations, we propose MEDCO (Medical Education Copilots), a novel multi-agent-based copilot system specially developed to emulate real-world medical training environments. MEDCO incorporates three primary agents: an agentic patient, an expert doctor, and a radiolo-gist, facilitating a multi-modal and interactive learning environment. Our framework emphasizes the learning of proficient question-asking skills, multi-disciplinary collaboration, and peer discussions between students. Our experiments show that simulated virtual students who underwent training with MEDCO not only achieved substantial performance en-hancements comparable to those of advanced models, but also demon-strated human-like learning behaviors and improvements, coupled with an increase in the number of learning samples. This work contributes to medical education by introducing a copilot that implements an in-teractive and collaborative learning approach. It also provides valuable insights into the effectiveness of AI-integrated training paradigms.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) [2, 29] have transformed various research do-mains, with their exceptional language processing and understanding skills emerg-ing as generalist intelligence [26]. In medicine and healthcare, LLMs have been explored in numerous areas [20,28], from clerical work automation such as writ-ing discharge summaries [19], to health monitoring such as dietary intake as-sessment [16], and to an entire clinical pathway from patient presentation to treatment involving clinical data interpretation and diagnostic suggestions [22]."}, {"title": "2 Related Work", "content": "The introduction of ChatGPT has accelerated the exploration of LLMs for med-ical education [10,33]. The early work of Kung et al. [8] revealed that Chat-GPT could achieve a passing score of USMLE. Since then, new generations of LLMs [23, 25] have been benchmarked on USMLE extensively, with the state-of-the-art performance achieving 91.1% accuracy [23]. While the implication of LLMs achieving such a high USMLE score is far-reaching, little research [8] has been conducted to integrate LLMs with existing medical educational systems and investigate how effective they are in improving students' learning.\nFurthermore, as pointed out in [20], LLMs in education can be a double-edged sword where learning can be more interactive and responsive with LLMs being assistants, but plagiarism and a decrease of a student's own creativity may also occur and proliferate. We refer readers to [1,20,33] for more comprehensive discussions about LLMs in medical education"}, {"title": "2.2 LLM-based Agents", "content": "Recently, there has been a trend of shifting from a single LLM chatbot to an agentic LLM framework for task solving, as agents can access tools, plan, reflect, and form collaborations with other agents. Within an educational setting, this agentic paradigm resembles what a human student would experience during the course of learning, i.e., developing reasoning skills, knowing to leverage tools, reflect, and collaborate with others. Hence, an educational system based on multiple agents can be particularly promising. While there are some agentic frameworks, such as ReAct [34], LangChain, CAMEL [14], and AutoGen [32], they are mainly proposed for automating workflow. For example, the seminal CAMEL work shows that the workflow of a task can be automated using an agentic user and an agentic assistant in which the agentic user is a proxy of human user to instruct the assistant to fulfill tasks. Recently, agentic systems have been proposed for educational purposes such as simulated classrooms [36, 38]. Lee et al. [11] have also proposed to use generative agents to train teachers to help them prepare for the actual in-class teaching.\nThere are a few studies exploring LLM agents in medical settings, and most of them are unimodal focusing on medical text data only. Agent Hospital [15] shows that an agentic doctor can continually improve its diagnosis by merely interacting with agentic patients in a simulated hospital, and can transfer its learned knowledge to real-world cases. MEDAGENTS [27] demonstrates that multi-round discussions among agents can lead to better results than zero-/few-shot chain-of-thought [31] on medical question answering. Similarly, AI Hos-pital [6] shows that multi-agent discussions can enhance diagnostic accuracy. Almanac [37], a retrieval-integrated agentic system, shows consistently better performance than plain LLMs in clinical question answering. Nonetheless, the above explorations of agents within medicine are still restricted by the single text modality, whereas medicine is inherently multi-modal.\nRecently, MMedAgent [12] shows that the medical task-solving capabilities of LLMs can be widened, and their unimodal nature can be expanded to multi-modal, through tool access, such as invoking MedSAM [17] for medical image segmentation and ChatCAD [39] for medical report generation."}, {"title": "3 Method", "content": "Fig. 1 illustrates the three main steps in our framework: (1) Agent initializa-tion: prompt different LLMs to play different roles using medical information and different instructions; (2) Learning scenario: the agentic medical student generates a diagnostic report according to interactions with the agentic patient and agentic radiologist, which is then evaluated by an agentic medical expert. The student then integrates diagnostic suggestions and case-specific knowledge from the medical expert into its memory; (3) Practicing scenario: after the ini-tial diagnosis for a new clinical case, the agentic medical student revisits relevant suggestions or knowledge from its memory to propose further questions to the patient and radiologist to reach a more accurate final diagnosis. The following subsections elaborate on these three steps."}, {"title": "3.1 Agent Initialization", "content": "The proposed MEDCO framework currently encompasses four roles: (1) an agen-tic patient: articulates symptoms, answers questions honestly, participates in ex-aminations, and expresses concerns; (2) an agentic radiologist: interprets various"}, {"title": "3.2 Learning Scenario", "content": "This particular scenario is designed to assist a medical student in acquiring valuable diagnostic insights by engaging in case-based learning, which is shown in Fig. 2 and mainly involves three steps:\n(1) Initial Diagnosis: In this essential first step, the student interacts with both the patient and the radiologist in a collaborative setting. The primary goal is to formulate a comprehensive diagnostic report. This report serves to suc-cinctly summarize a range of critical elements, including the patient's reported symptoms, results from various examinations, the diagnosis itself, the reason-ing behind the diagnostic conclusions, and the corresponding treatment plans tailored for the patient's condition.\n(2) Assessment: The medical expert evaluates the report against the com-plete medical records to provide feedback and suggestions on patient interaction, inquiries, diagnostic hypotheses, clinical reasoning, and treatment plans. Addi-tionally, the expert offers case-specific medical knowledge, including disease defi-nitions, pathogenesis, symptoms, common examinations, and primary treatment protocols, for students' learning.\n(3) Learning Feedback: To simulate the student's learning and practicing process, a key-value memory is implemented to store and retrieve suggestions and case-specific knowledge efficiently. In our setting, saving information to memory is equivalent to a human student's acquisition of this information through learn-ing. The detailed designs of our memory mechanism can be found in section C in the appendix."}, {"title": "3.3 Practicing Scenario", "content": "The practicing scenario aims to assess the diagnostic performance improvement of the medical student after being trained by our copilot, as illustrated in Fig. 2:\n(1) Initial Diagnosis: Similar to step (1) in the learning scenario, this step involves interactive communication with the patient and radiologist to formulate a preliminary diagnostic report.\n(2) Rethinking and Recalling: Using the summarized symptoms from the initial diagnosis, the agentic student revisits their learned knowledge of diseases or suggestions to propose differential questions for the patient and radiologist by following the prompts in the appendix (Table 13 and Table 14, respectively). To recall knowledge, the agentic student first retrieves relevant diseases from the memory's symptom-store and then accesses disease-related information from the disease-store. Similarly, the student would utilize patient IDs from these relevant diseases to gather suggestions from the case-store in memory.\n(3) Further Inquiry and Diagnosis: Based on the additional questions, the agentic student inquires the patient and radiologist to gather more differ-ential information for the final diagnosis. Table 12 shows the prompt for this inquiry.\n(4) Peer Discussion: In scenarios with two agentic students, each retrieves relevant suggestions and knowledge respectively in step (2). They then engage in a discussion about their diagnosis to improve themselves, where the prompt is outlined in Table 15. This collaboration enhances clinical reasoning and decision-making by integrating their insights.\nFigures 8 and 9 illustrate an interactive diagnosis case that demonstrates the student retrieving knowledge from memory based on symptoms to enhance the final diagnosis."}, {"title": "4 Experiments", "content": "In this study, we used the MVME dataset [6] of Chinese medical records, which contains 506 high-quality medical cases (text-only) across various specialties, e.g., surgery, internal medicine, and obstetrics and gynecology. More details about MVME can be found in [6]. We randomly divided the dataset into training (259 cases) and testing (247 cases) sets at the department level for learning and practicing scenarios, respectively.\nTo validate our framework's multi-modal capability, we selected the entire test set of 16 neurological cases from the department of internal medicine and gathered their corresponding radiological images or report photos from the orig-inal websites. Two example images are shown in Fig.5 in the appendix. During the dialogue, the radiologist interprets the images of the current patient into textual descriptions, such as a radiology report or summary of existing reports, when requested to provide such information by the agentic patient or student."}, {"title": "4.2 Evaluation Metrics", "content": "We assess MEDCO using three evaluation metrics from both qualitative to quan-titative aspects. More implementation details can be found at the section D in the appendix\nHDE (Holistic Diagnostic Evaluation): the rating (1~4 points) of the expert regarding the student's report, including five sections: symptoms, medical examination, diagnostic results, rationales, and treatment plans.\nSEMA (Semantic Embedding-based Matching Assessment): First, retrieving the top-10 relevant ICD terminologies of the student's diagnosis and the ground truth (the medical record), respectively, and then compute their ex-tracted disease entities (#), precision(P), recall(R), and F1-score(F1) as metrics.\nCASCADE (Coarse And Specific Code Assessment for Diagnostic Evaluation): Similar to the SEMA, we first retrieve the top-1 relevant ICD ter-minology and then compute the accuracy at three ICD-10 levels: coarse, medium, and fine-grained levels."}, {"title": "4.3 Implementation Details", "content": "In MEDCO, distinct models are assigned to various roles: GPT-3.5 [2] acts as the patient, while Claude-3.5-Sonnet-20240620 [3] serves as both the radiologist"}, {"title": "4.4 Performance of the Agentic Student", "content": "Table 1 presents evaluations from the medical expert regarding a student's re-port, indicating significant performance gains after the learning scenario.\nThe Claude3.5-Sonnet model scored the highest average score of 2.283 (\u00b10.328), while the student (GPT-3.5) achieved an average of 1.965 (\u00b10.336). After learn-ing, the agentic student's overall performance improved, with scores rising to 2.169 (\u00b10.337) through recalling learned knowledge and 2.122 (\u00b10.341) through revisiting suggestions. Peer discussions yielded the best score of 2.299 (\u00b10.393), outperforming both the Claude3.5-Sonnet and the upper-bound 2-agent bench-marks. Particularly, the Medical Examination and Diagnostic Rationales sections achieved significant gains, with the Medical Examination score rising from 1.785 to 2.575 following peer discussions, and Diagnostic Rationales improving from 1.879 to 2.158."}, {"title": "4.5 Performance Gain for the Students Played by Strong Models", "content": "This subsection explores how the copilot improves performance in students played by strong language models, exemplified by Claude3.5-Sonnet. As shown in Tables 3 and 4, MEDCO significantly enhances agentic students' clinical con-sultation and diagnostic abilities, demonstrating its effectiveness across different model architectures.\nTable 3 reveals that the untrained agentic student scores an average of 2.283 (\u00b10.328) in five diagnostic aspects. With copilot training, scores rise: students using recalled knowledge could reach the average of 2.586 (\u00b10.358), while those using recalled suggestions could reach 2.693 (\u00b10.128). Peer discussions yield the highest score of 2.686 (\u00b10.350), surpassing the 2 Agents, the average of 2.245 (\u00b10.283). Individual performance improvements are also consistent across"}, {"title": "4.6 Learning Curve of the Agentic Student", "content": "This section examines the learning curve of the agentic student (GPT-3.5) as the number of training cases increases. We focus on Neurology cases from Internal Medicine with 16 training and 16 test cases. In the practicing scenario, when recalling learned experiences or participating in peer discussion, the agentic stu-dent's memory retrieval range is limited to 0% (no training), 25% (4 cases), 50% (8 cases), 75% (12 cases), and 100% (all 16 training cases), to see how different quantities of training samples could affect diagnostic performance.\nIn Fig.3(b), the F1-score rises quickly, stabilizing between the 25% and 100% ranges, indicating that even limited training can enhance precision and recall."}, {"title": "4.7 Multi-Modal Support", "content": "The MVME dataset contains only text, so we collected corresponding images to test our framework's feasibility with multi-modality. We chose 16 Neurology cases from the Department of Internal Medicine for this study. In interactive diagnosis, radiologists convert visual data such as radiological images or medical report photos into textual descriptions for student comprehension. An example of this is shown in Fig. 10.\nWe utilized GPT-40-mini, Claude3.5-Sonnet, and GPT-3.5 to initialize the patient role and evaluate the impact of visual data on them. Results in Fig. 4 show average scores from expert evaluations, F1-scores (SEMA), and fine-grained level accuracy (CASCADE), with additional metrics in Tables 8 and 9. From the results, Fig. 4(a) reveals that multi-modal input benefits students initialized with GPT-40-mini and Claude3.5-Sonnet, particularly the latter, which achieves the highest results, surpassing 2 Agents [6].\nThe F1-score in Fig. 4(b) indicates a strong positive effect of multi-modal input for all variants. the agentic student (GPT-3.5) shows the highest relative improvement, while Claude3.5-Sonnet variant achieves the top absolute Fl-score, approaching the 2 Agents benchmark. Additionally, fine-grained level accuracy in Fig. 4(c) illustrates significant enhancements from multi-modal input, suggesting visual data improves detailed fine-grade diagnosis. Notably, multi-modal input significantly boosts GPT-40-mini and Claude3.5-Sonnet student performance to surpass the reference.\nThese results suggest that the multi-modal copilot could aid students' un-derstanding of complex diagnoses, particularly for fine-grade assessments, by combining visual and textual information to foster a more engaging learning environment."}, {"title": "5 Discussion", "content": "Case-based learning is the core of our MEDCO framework for AI-enabled med-ical education. It allows a student to actively interact with a virtual agentic patient, auxiliary doctor, and medical expert for diagnosing a clinical case. This differs from a single-agent chatbot in that instead of one AI character is available, multiple virtual characters are simulated in MEDCO and available for discussion at the request of the student user, replicating real-world clinical settings where medical consultation and diagnosis can be multidisplinary and require expertise from senior doctors as well as active engagement, cooperation from the patient. While we have validated MEDCO from various dimensions, the student cur-rently is only simulated and played by an LLM such as GPT-3.5. Although we have used LLMs of different versions to simulate students with different learning capabilities, and a memory mechanism to represent a student's learning and the acquisition of new knowledge process, there is clearly a gap between the real and virtual students in learning, and hence the effectiveness of MEDCO for helping human students in learning medical knowledge requires further investigation. A larger-scale multi-modal collaborative dataset on par with industrial datasets in other domains [35] could further unlock the potentials of the MEDCO sys-tem. In current implementation, the student receives only textual feedback from the medical expert. Future work can enable the expert to provide quintessential medical imaging examples of a learning case to offer multi-modal feedback to improve the student's diagnostic skill and broaden their knowledge. Similarly, the capabilities of agentic doctors for multi-departmental collaboration can also be expanded. For example, the agentic radiologist can also be granted access to foundation AI models [4, 5, 9, 21, 30] or specialized AI models [17,24] as their tools, e.g., using a segmentation model to segment and highlight lesions which could potentially ease the learning of the student."}, {"title": "6 Conclusion", "content": "We have introduced MEDCO, an innovative multi-agent copilot system for med-ical education. MEDCO enables medical students to interact dynamically with an agentic patient, specialized doctor, and medical expert to learn about the diagnosis and treatment of various diseases. Our findings indicate that MEDCO can effectively adapt a generalist model for medical specialization. This adapta-tion may also occur for human students using MEDCO, as it provides targeted feedback tailored to individual learning cases and simulates diverse patient en-counters and multidisciplinary collaborations to enhance the learning experience. Furthermore, this AI-enabled copilot system has the potential for application be-yond medicine, benefiting broader educational contexts."}]}