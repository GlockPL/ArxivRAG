{"title": "MEDCO: Medical Education Copilots Based on\nA Multi-Agent Framework", "authors": ["Hao Wei", "Jianing Qiu", "Haibao Yu", "Wu Yuan"], "abstract": "Large language models (LLMs) have had a significant im-\npact on diverse research domains, including medicine and healthcare.\nHowever, the potential of LLMs as copilots in medical education re-\nmains underexplored. Current AI-assisted educational tools are limited\nby their solitary learning approach and inability to simulate the multi-\ndisciplinary and interactive nature of actual medical training. To address\nthese limitations, we propose MEDCO (Medical Education Copilots),\na novel multi-agent-based copilot system specially developed to emulate\nreal-world medical training environments. MEDCO incorporates three\nprimary agents: an agentic patient, an expert doctor, and a radiolo-\ngist, facilitating a multi-modal and interactive learning environment. Our\nframework emphasizes the learning of proficient question-asking skills,\nmulti-disciplinary collaboration, and peer discussions between students.\nOur experiments show that simulated virtual students who underwent\ntraining with MEDCO not only achieved substantial performance en-\nhancements comparable to those of advanced models, but also demon-\nstrated human-like learning behaviors and improvements, coupled with\nan increase in the number of learning samples. This work contributes\nto medical education by introducing a copilot that implements an in-\nteractive and collaborative learning approach. It also provides valuable\ninsights into the effectiveness of AI-integrated training paradigms.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) [2, 29] have transformed various research do-\nmains, with their exceptional language processing and understanding skills emerg-\ning as generalist intelligence [26]. In medicine and healthcare, LLMs have been\nexplored in numerous areas [20,28], from clerical work automation such as writ-\ning discharge summaries [19], to health monitoring such as dietary intake as-\nsessment [16], and to an entire clinical pathway from patient presentation to\ntreatment involving clinical data interpretation and diagnostic suggestions [22]."}, {"title": "2 Related Work", "content": "2.1 LLMs for Medical Education\nThe introduction of ChatGPT has accelerated the exploration of LLMs for med-\nical education [10,33]. The early work of Kung et al. [8] revealed that Chat-\nGPT could achieve a passing score of USMLE. Since then, new generations of\nLLMs [23, 25] have been benchmarked on USMLE extensively, with the state-\nof-the-art performance achieving 91.1% accuracy [23]. While the implication of\nLLMs achieving such a high USMLE score is far-reaching, little research [8] has\nbeen conducted to integrate LLMs with existing medical educational systems\nand investigate how effective they are in improving students' learning.\nFurthermore, as pointed out in [20], LLMs in education can be a double-\nedged sword where learning can be more interactive and responsive with LLMs\nbeing assistants, but plagiarism and a decrease of a student's own creativity may\nalso occur and proliferate. We refer readers to [1,20,33] for more comprehensive\ndiscussions about LLMs in medical education\n2.2 LLM-based Agents\nRecently, there has been a trend of shifting from a single LLM chatbot to an\nagentic LLM framework for task solving, as agents can access tools, plan, reflect,\nand form collaborations with other agents. Within an educational setting, this\nagentic paradigm resembles what a human student would experience during the\ncourse of learning, i.e., developing reasoning skills, knowing to leverage tools,\nreflect, and collaborate with others. Hence, an educational system based on"}, {"title": "3 Method", "content": "Fig. 1 illustrates the three main steps in our framework: (1) Agent initializa-\ntion: prompt different LLMs to play different roles using medical information\nand different instructions; (2) Learning scenario: the agentic medical student\ngenerates a diagnostic report according to interactions with the agentic patient\nand agentic radiologist, which is then evaluated by an agentic medical expert.\nThe student then integrates diagnostic suggestions and case-specific knowledge\nfrom the medical expert into its memory; (3) Practicing scenario: after the ini-\ntial diagnosis for a new clinical case, the agentic medical student revisits relevant\nsuggestions or knowledge from its memory to propose further questions to the\npatient and radiologist to reach a more accurate final diagnosis. The following\nsubsections elaborate on these three steps.\n3.1 Agent Initialization\nThe proposed MEDCO framework currently encompasses four roles: (1) an agen-\ntic patient: articulates symptoms, answers questions honestly, participates in ex-\naminations, and expresses concerns; (2) an agentic radiologist: interprets various"}, {"title": "3.2 Learning Scenario", "content": "This particular scenario is designed to assist a medical student in acquiring\nvaluable diagnostic insights by engaging in case-based learning, which is shown\nin Fig. 2 and mainly involves three steps:\n(1) Initial Diagnosis: In this essential first step, the student interacts with\nboth the patient and the radiologist in a collaborative setting. The primary goal\nis to formulate a comprehensive diagnostic report. This report serves to suc-\ncinctly summarize a range of critical elements, including the patient's reported\nsymptoms, results from various examinations, the diagnosis itself, the reason-\ning behind the diagnostic conclusions, and the corresponding treatment plans\ntailored for the patient's condition.\n(2) Assessment: The medical expert evaluates the report against the com-\nplete medical records to provide feedback and suggestions on patient interaction,\ninquiries, diagnostic hypotheses, clinical reasoning, and treatment plans. Addi-\ntionally, the expert offers case-specific medical knowledge, including disease defi-\nnitions, pathogenesis, symptoms, common examinations, and primary treatment\nprotocols, for students' learning.\n(3) Learning Feedback: To simulate the student's learning and practicing\nprocess, a key-value memory is implemented to store and retrieve suggestions and\ncase-specific knowledge efficiently. In our setting, saving information to memory\nis equivalent to a human student's acquisition of this information through learn-\ning. The detailed designs of our memory mechanism can be found in section C\nin the appendix."}, {"title": "3.3 Practicing Scenario", "content": "The practicing scenario aims to assess the diagnostic performance improvement\nof the medical student after being trained by our copilot, as illustrated in Fig. 2:\n(1) Initial Diagnosis: Similar to step (1) in the learning scenario, this step\ninvolves interactive communication with the patient and radiologist to formulate\na preliminary diagnostic report.\n(2) Rethinking and Recalling: Using the summarized symptoms from the\ninitial diagnosis, the agentic student revisits their learned knowledge of diseases\nor suggestions to propose differential questions for the patient and radiologist\nby following the prompts in the appendix (Table 13 and Table 14, respectively).\nTo recall knowledge, the agentic student first retrieves relevant diseases from\nthe memory's symptom-store and then accesses disease-related information from\nthe disease-store. Similarly, the student would utilize patient IDs from these\nrelevant diseases to gather suggestions from the case-store in memory.\n(3) Further Inquiry and Diagnosis: Based on the additional questions,\nthe agentic student inquires the patient and radiologist to gather more differ-\nential information for the final diagnosis. Table 12 shows the prompt for this\ninquiry.\n(4) Peer Discussion: In scenarios with two agentic students, each retrieves\nrelevant suggestions and knowledge respectively in step (2). They then engage"}, {"title": "4 Experiments", "content": "4.1 Dataset\nIn this study, we used the MVME dataset [6] of Chinese medical records, which\ncontains 506 high-quality medical cases (text-only) across various specialties,\ne.g., surgery, internal medicine, and obstetrics and gynecology. More details\nabout MVME can be found in [6]. We randomly divided the dataset into training\n(259 cases) and testing (247 cases) sets at the department level for learning and\npracticing scenarios, respectively.\nTo validate our framework's multi-modal capability, we selected the entire\ntest set of 16 neurological cases from the department of internal medicine and\ngathered their corresponding radiological images or report photos from the orig-\ninal websites. Two example images are shown in Fig.5 in the appendix. During\nthe dialogue, the radiologist interprets the images of the current patient into\ntextual descriptions, such as a radiology report or summary of existing reports,\nwhen requested to provide such information by the agentic patient or student.\n4.2 Evaluation Metrics\nWe assess MEDCO using three evaluation metrics from both qualitative to quan-\ntitative aspects. More implementation details can be found at the section D in\nthe appendix\nHDE (Holistic Diagnostic Evaluation): the rating (1~4 points) of the\nexpert regarding the student's report, including five sections: symptoms, medical\nexamination, diagnostic results, rationales, and treatment plans.\nSEMA (Semantic Embedding-based Matching Assessment): First,\nretrieving the top-10 relevant ICD terminologies of the student's diagnosis and\nthe ground truth (the medical record), respectively, and then compute their ex-\ntracted disease entities (#), precision(P), recall(R), and F1-score(F1) as metrics.\nCASCADE (Coarse And Specific Code Assessment for Diagnostic\nEvaluation): Similar to the SEMA, we first retrieve the top-1 relevant ICD ter-\nminology and then compute the accuracy at three ICD-10 levels: coarse, medium,\nand fine-grained levels.\n4.3\nImplementation Details\nIn MEDCO, distinct models are assigned to various roles: GPT-3.5 [2] acts as\nthe patient, while Claude-3.5-Sonnet-20240620 [3] serves as both the radiologist"}, {"title": "4.4 Performance of the Agentic Student", "content": "Table 1 presents evaluations from the medical expert regarding a student's re-\nport, indicating significant performance gains after the learning scenario.\nThe Claude3.5-Sonnet model scored the highest average score of 2.283 (\u00b10.328),\nwhile the student (GPT-3.5) achieved an average of 1.965 (\u00b10.336). After learn-\ning, the agentic student's overall performance improved, with scores rising to\n2.169 (\u00b10.337) through recalling learned knowledge and 2.122 (\u00b10.341) through\nrevisiting suggestions. Peer discussions yielded the best score of 2.299 (\u00b10.393),\noutperforming both the Claude3.5-Sonnet and the upper-bound 2-agent bench-\nmarks. Particularly, the Medical Examination and Diagnostic Rationales sections\nachieved significant gains, with the Medical Examination score rising from 1.785\nto 2.575 following peer discussions, and Diagnostic Rationales improving from\n1.879 to 2.158."}, {"title": "4.5 Performance Gain for the Students Played by Strong Models", "content": "This subsection explores how the copilot improves performance in students\nplayed by strong language models, exemplified by Claude3.5-Sonnet. As shown\nin Tables 3 and 4, MEDCO significantly enhances agentic students' clinical con-\nsultation and diagnostic abilities, demonstrating its effectiveness across different\nmodel architectures.\nTable 3 reveals that the untrained agentic student scores an average of 2.283\n(\u00b10.328) in five diagnostic aspects. With copilot training, scores rise: students\nusing recalled knowledge could reach the average of 2.586 (\u00b10.358), while those\nusing recalled suggestions could reach 2.693 (\u00b10.128). Peer discussions yield\nthe highest score of 2.686 (\u00b10.350), surpassing the 2 Agents, the average of\n2.245 (\u00b10.283). Individual performance improvements are also consistent across"}, {"title": "4.6 Learning Curve of the Agentic Student", "content": "This section examines the learning curve of the agentic student (GPT-3.5) as the\nnumber of training cases increases. We focus on Neurology cases from Internal\nMedicine with 16 training and 16 test cases. In the practicing scenario, when\nrecalling learned experiences or participating in peer discussion, the agentic stu-\ndent's memory retrieval range is limited to 0% (no training), 25% (4 cases), 50%\n(8 cases), 75% (12 cases), and 100% (all 16 training cases), to see how different\nquantities of training samples could affect diagnostic performance.\nIn Fig.3(b), the F1-score rises quickly, stabilizing between the 25% and 100%\nranges, indicating that even limited training can enhance precision and recall."}, {"title": "4.7 Multi-Modal Support", "content": "The MVME dataset contains only text, so we collected corresponding images\nto test our framework's feasibility with multi-modality. We chose 16 Neurology\ncases from the Department of Internal Medicine for this study. In interactive\ndiagnosis, radiologists convert visual data such as radiological images or medical\nreport photos into textual descriptions for student comprehension. An example\nof this is shown in Fig. 10.\nWe utilized GPT-40-mini, Claude3.5-Sonnet, and GPT-3.5 to initialize the\npatient role and evaluate the impact of visual data on them. Results in Fig. 4\nshow average scores from expert evaluations, F1-scores (SEMA), and fine-grained\nlevel accuracy (CASCADE), with additional metrics in Tables 8 and 9. From the\nresults, Fig. 4(a) reveals that multi-modal input benefits students initialized with\nGPT-40-mini and Claude3.5-Sonnet, particularly the latter, which achieves the\nhighest results, surpassing 2 Agents [6].\nThe F1-score in Fig. 4(b) indicates a strong positive effect of multi-modal\ninput for all variants. the agentic student (GPT-3.5) shows the highest relative\nimprovement, while Claude3.5-Sonnet variant achieves the top absolute Fl-score,\napproaching the 2 Agents benchmark. Additionally, fine-grained level accuracy in\nFig. 4(c) illustrates significant enhancements from multi-modal input, suggesting\nvisual data improves detailed fine-grade diagnosis. Notably, multi-modal input\nsignificantly boosts GPT-40-mini and Claude3.5-Sonnet student performance to\nsurpass the reference.\nThese results suggest that the multi-modal copilot could aid students' un-\nderstanding of complex diagnoses, particularly for fine-grade assessments, by\ncombining visual and textual information to foster a more engaging learning\nenvironment."}, {"title": "5 Discussion", "content": "Case-based learning is the core of our MEDCO framework for AI-enabled med-\nical education. It allows a student to actively interact with a virtual agentic\npatient, auxiliary doctor, and medical expert for diagnosing a clinical case. This\ndiffers from a single-agent chatbot in that instead of one AI character is available,\nmultiple virtual characters are simulated in MEDCO and available for discussion\nat the request of the student user, replicating real-world clinical settings where"}, {"title": "6 Conclusion", "content": "We have introduced MEDCO, an innovative multi-agent copilot system for med-\nical education. MEDCO enables medical students to interact dynamically with\nan agentic patient, specialized doctor, and medical expert to learn about the\ndiagnosis and treatment of various diseases. Our findings indicate that MEDCO\ncan effectively adapt a generalist model for medical specialization. This adapta-\ntion may also occur for human students using MEDCO, as it provides targeted\nfeedback tailored to individual learning cases and simulates diverse patient en-"}]}