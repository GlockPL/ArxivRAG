{"title": "Learning From Graph-Structured Data: Addressing Design Issues and Exploring Practical Applications in Graph Representation Learning", "authors": ["Chenqing (William) Hua"], "abstract": "Graphs serve as fundamental descriptors for systems composed of interacting elements, capturing a wide array of data types, from molecular interactions to social networks and knowledge graphs. In this paper, we present an exhaustive review of the latest advancements in graph representation learning and Graph Neural Networks (GNNs).\nGNNs, tailored to handle graph-structured data, excel in deriving insights and predictions from intricate relational information, making them invaluable for tasks involving such data. Graph representation learning, a pivotal approach in analyzing graph-structured data, facilitates numerous downstream tasks and applications across machine learning, data mining, biomedicine, and healthcare.\nOur work delves into the capabilities of GNNs, examining their foundational designs and their application in addressing real-world challenges. We introduce a GNN equipped with an advanced high-order pooling function, adept at capturing complex node inter-\nactions within graph-structured data. This pooling function significantly enhances the\nGNN's efficacy in both node- and graph-level tasks. Additionally, we propose a molec-\nular graph generative model with a GNN as its core framework. This GNN backbone\nis proficient in learning invariant and equivariant molecular characteristics. Employing\nthese features, the molecular graph generative model is capable of simultaneously learning\nand generating molecular graphs with atom-bond structures and precise atom positions.\nOur models undergo thorough experimental evaluations and comparisons with estab-\nlished methods, showcasing their superior performance in addressing diverse real-world\nchallenges with various datasets.", "sections": [{"title": "Introduction", "content": "Graph neural networks (GNNs) generalize traditional neural network architectures for\ndata in the Euclidean domain to data in non-Euclidean domains [35, 40, 52, 54, 56, 74].\nAs graphs are very general and flexible data structures and are ubiquitous in the real\nworld, GNNs are now widely used in a variety of domains and applications such as\nsocial network analysis [25, 53, 59], recommender systems [82], graph reasoning [91],\nand drug discovery [31, 33, 34, 69]. Indeed, many GNN architectures (e.g., GCN [40],\nGAT [74], MPNN [21], ACM-GNN [55]) have been proposed. The essential idea of all these\narchitectures is to iteratively update node representations by aggregating the information\nfrom their neighbors through multiple rounds of neural message passing. The final node\nrepresentations can be used for downstream tasks such as node classification or link\nprediction. For graph classification, an additional readout layer is used to combine all the\nnode representations to calculate the entire graph representation.\nA recent work, principled neighborhood aggregation (PNA) [17], aims to design a more\nflexible aggregation function by combining multiple simple aggregation functions, each of\nwhich is associated with a learnable weight. However, the practical capacity of PNA is still\nlimited by simply combining multiple simple aggregation functions. A more expressive\nsolution would be to model high-order non-linear interactions when aggregating node\nfeatures. However, explicitly modeling high-order non-linear interactions among nodes is"}, {"title": "Related Work", "content": "In this chapter, we give a comprehensive review of the relevant literature on graph repre-\nsentation learning and graph neural networks (GNNs)."}, {"title": "Graph Representation Learning", "content": "Graph representation learning refers to the process of encoding graph-structured data\nor relational data into low-dimensional vectors or embeddings. It is a fundamental task\nthat has gained significant attention in recent years, particularly in the field of machine\nlearning and data mining [23].\nGraphs are universal descriptors of systems with interacting elements, and they can\nrepresent various types of data, such as molecular interactions, social networks, and\nknowledge graphs. Graph representation learning aims to capture the structural and\nfeature information of graphs in a compact and meaningful way, enabling downstream\ntasks like node classification, link prediction, and anomaly detection [24,25,33\u201335,40,59,74].\nThe goal of graph representation learning is to construct a set of features or embeddings\nthat represent the structure of the graph and the data associated with it. These embeddings\ncan be categorized into three types: node-wise embeddings, which represent each node in\nthe graph [21,23,25,48,51,55,57]; edge-wise embeddings, which represent each edge in the"}, {"title": "Graph Neural Networks", "content": "Graph Neural Networks (GNNs) are a type of neural network specifically designed to\nprocess data in the form of graphs [23, 24, 40]. They are used for tasks such as graph\nclassification, node classification, and edge prediction [1\u20133,34,35,53,58,74]. GNNs have\nthe ability to capture both the structural and relational information in a graph, making\nthem highly effective for tasks involving graph data. GNNs differ from traditional neural\nnetworks in that they can handle data in the form of graphs, whereas traditional neural\nnetworks are designed to process data in the form of vectors or sequences [26].\nGNNs apply the predictive power of deep learning to rich data structures that depict\nobjects and their relationships as points connected by lines in a graph. In GNNs, data\npoints are called nodes, which are linked by lines called edges, with elements expressed\nmathematically so machine learning algorithms can make useful predictions at the level of\nnodes, edges, or entire graphs [23, 25,48,56].\nGNNs have been adapted to leverage the structure and properties of graphs. They\nexplore the components needed for building a graph neural network and motivate the\ndesign choices behind them. In summary, GNNs are a powerful tool for processing and\nanalyzing graph-structured data, allowing for the extraction of valuable insights and\npredictions from complex relational information [1,7,17,81,82]."}, {"title": "Pooling Functions for Graph Neural Networks", "content": "In the design of Graph Neural Networks (GNNs), a key component is an effective pooling\nfunction for aggregating features from local neighborhoods to update node representations\nand for combining these node representations to derive a graph-level representation.\nKipf et al. [40] successfully define convolutions on graph-structured data by averaging\nnode information in a neighborhood. Xu et al. [78] prove the incomplete expressivity of\nmean aggregation to distinguish nodes, and further propose to use sum aggregation to\ndifferentiate nodes with similar properties. Corso et al. [17] further generalize this idea\nand show that mean aggregation can be a particular case of sum aggregation with a linear\nmultiplier, and further propose an architecture with multiple aggregation channels to\nadaptively learn low-order information. Luan et al. [59] show that the use of aggregation\nis not sufficient and further propose to utilize GNNs with aggregation and diversification\noperations simultaneously to learn. Most GNNs use low-order aggregation schemes for\nlearning node representations. However, Battiston et al. [5] show that aggregation should\ngo beyond low-order interactions because high-order terms can more accurately model\nreal-world complex systems.\nMost GNNs employ low-order aggregation schemes for learning node representations.\nHowever, Morris et al. [63] consider high-order graph structures into account to build a\nhigh-order expressive model that is more powerful than regular message-passing GNNs.\nA brand new architecture, Graph Neural Diffusion [11], take graph neural networks as ap-\nproximations of an underlying partial differential equation, thus it uses more information\nthan just simple low-order pooling information and further addresses depth and over-\nsmoothing issues in graph machine learning. Baek et al. [4] formulate the pooling problem\nas a multiset encoding problem with auxiliary information about the graph structure, and\npropose an attention-based pooling layer that captures the interaction between nodes\naccording to their structural dependencies. Wang et al. [77] apply second-order statistic"}, {"title": "Graph Neural Networks for Molecule Generation", "content": "Graph Neural Networks (GNNs) have also been increasingly utilized for molecular gen-\neration tasks, encompassing property prediction, docking, optimization, and genera-\ntion [35,76,79,80]. Key challenges in this domain include ensuring the validity, diversity,\nand property adherence of the generated molecules.\nTypically, a variational autoencoder (VAE) framework is employed, wherein a GNN\nencoder maps a molecule's graph to a latent space, and a GNN decoder reconstructs\nthe graph. This approach allows for sampling and manipulation of the latent space to\ncreate novel molecules [8,37,49,50,85,86]. Jin et al. [37] enhance molecule generation by\ndecomposing molecules into trees of substructures and using separate GNNs for encoding\nand decoding these trees and graphs, improving validity, diversity, and accuracy. Liu et\nal. [50] advance this technique by introducing a conditional framework where the latent\nspace is informed by property vectors, facilitating the generation of molecules with specific\ncharacteristics.\nOther than using GNNs as the backbone model for VAE framework, they can be\nadopted for generative diffusion models for molecule generation [3,28,39]. Xu et al. [80]\nleverage diffusion models to produce molecules with minimal conformation energy, while\nVignac et al. [76] use a graph transformer in their diffusion model to refine both atom\nfeatures and molecular structures. Moreover, recent studies have explored the integration"}, {"title": "Graph Neural Networks with High-Order Pooling", "content": "In this chapter, we examine the limitations of the pooling functions employed by Graph\nNeural Networks (GNNs) in graph-level tasks, where all node features are consolidated\ninto a single comprehensive feature to represent the entire graph. Conventional GNNs\ncommonly utilize basic pooling functions such as sum, average, or max to aggregate\nmessages within local neighborhoods for updating node representation or pooling node\nrepresentations across the entire graph to compute the graph representation. While\nthese linear operations are straightforward and efficient, they fall short in capturing\nhigh-order non-linear interactions among nodes. Our proposed solution introduces a\nhighly expressive GNN pooling function that leverages tensor decomposition to model\nintricate high-order non-linear node interactions."}, {"title": "Preliminaries", "content": "We introduce notations that are particularly used for this chapter. We use bold font letters\nfor vectors (e.g., v), capital letters (e.g., M, T) for matrices and tensors respectively, and\nregular letters for nodes (e.g., v). Let G = (V, E) be a graph, where V is the node set and E\nis the edge set with self-loop. We use N(v) to denote the neighborhood set of node v, i.e.,\nN(v) = {u : evu \u2208 E}. A node feature is a vector x \u2208 RF defined on V, where x, is defined\non the node v. We use \u2297 to denote the Kronecker product, o to denote the outer product,\nand to denote the Hadamard product, i.e., component-wise product, between vectors,\nmatrices, and tensors. For any integer k, we use the notation [k] = {1,\u2026, k}."}, {"title": "Tensors", "content": "We introduce basic notions of tensor algebra, more details can be found in [45]. A k-th\norder tensor T\u2208 RN1\u00d7N2\u00d7...\u00d7Nk can simply be seen as a multidimensional array. The\nmode-i fibers of T are the vectors obtained by fixing all indices except the i-th one:\nT n1,n2,...,ni\u22121,:,Ni+1,...,nk \u2208 RNi. The i-th mode matricization of a tensor is the matrix having\nits mode-i fibers as columns and is denoted by T (i), e.g., T (1) \u2208 IRN1\u00d7N2\uff65\uff65\uff65Nk. We use Txiv \u2208\n]RN1\u00d7\u2026\u00d7Ni\u22121\u00d7Ni+1\u00d7\u2026\u00d7Nk to denote the mode-i product between a tensor T\u2208 RN1\u00d7\u2026\u00d7Nk and\na vector v \u2208 RNi, which is defined by (Txiv)n1,...,Ni-1,Ni+1,\u2026,nk = \u03a3NiTn,...ni....VN..The\nfollowing useful identity relates the mode-i product with the Kronecker product:\n$$T\u00d7_1 V_1 \u00d7_2 \u22ef\u00d7_{k\u22121} V_{k\u22121} = T_{(k)} (V_{k\u22121} \u2297 \u22ef \u2297 V_1).\n$$"}, {"title": "CANDECOMP/PARAFAC Decomposition", "content": "We refer to CANDECOMP/PARAFAC decomposition of a tensor as CP decomposition\n[27,38]. A Rank R CP decomposition factorizes a k\u2212th order tensor T\u2208 RN1\u00d7...\u00d7Nk into\nthe sum of R rank one tensors as T = \\sum_{r=1}^{R}V_{1r} \u25cb V_{2r} \u25cb\\cdots\u25cb V_{kr}, where \u25cb denotes the vector\nouter-product and v\u2081r \u2208 RN1, V2r \u2208 RN2, ..., vkr \u2208 RNk for every r = 1,2, R.\nThe decomposition vectors, v: for r = 1, ..., R, are equal in length, thus can be naturally\ngathered into factor matrices M\u2081 = [V11, ..., V1R] \u2208 RN1\u00d7R, ..., Mk = [Vk1, \u2026\u2026\u2026, VkR] \u2208 RN\u00a3\u00d7R."}, {"title": "Graph Neural Networks and Pooling Functions", "content": "Given a graph G = (V, E), a graph neural network always aggregates information in a\nneighborhood to give node-level representations. During each message-passing iteration,\nthe embedding h\u2081 corresponding to node v \u2208 V is generated by aggregating features from\nN(v) [23]. Formally, at the l-th layer of a graph neural network,\n$$m^{l}_{N(v)} = AGGREGATE^{(l)} ({h^{(l-1)}_v, \\forall v \\in N(v)}), h^{l}_v = UPDATE^{(l)} (h^{(l-1)}_v, m^{l}_{N(v)}),$$\nwhere AGGREGATE() (\u00b7) and UPDATE() (\u00b7) are differentiable functions, the former being\npermutation-invariant. In words, AGGREGATE()(\u00b7) first aggregates information from\nN(v), then UPDATE() (\u00b7) combines the aggregated message and previous node embedding\nh-1) to give a new embedding."}, {"title": "Tensorized Graph Neural Network", "content": "In this section, we introduce the CP-layer and tensorized GNNs (tGNN). For convenience,\nwe let {x1, x2, ..., xk} denote features of a node v and its 1-hop neighbors N(v) such that\n|{v} UN(v)| = k."}, {"title": "Motivation and Method", "content": "We leverage the symmetric CP decomposition to design an efficient parameterization\nof permutation-invariant multilinear maps for aggregation operations in graph neural\nnetworks, Tensorized Graph Neural Network (tGNN), resulting in a more expressive\nhigh-order node interaction scheme. We visualize the CP pooling layer and compare it\nwith sum pooling in Fig. 3.2.\nLet T \u2208 RN\u00d7N\u00d7\u2026\u00d7N\u00d7M of order k + 1 be a tensor which is partially symmetric w.r.t. its\nfirst k modes. We can parameterize T using a rank R partially symmetric CP decomposi-\ntion (see Section 3.1.2): T = [W,\u2026\u2026\u2026, W, M] where W \u2208 RN\u00d7R and M\u2208 RM\u00d7R. Such a\ntensor naturally defines a map from (RN)k to RM using contractions over the first k modes:\n$$f(x_1,\u22ef, x_k) = T \u00d7_1 x_1 \u00d7_2 \u22ef\u00d7_k x_k = [W,\u2026\u2026,W,M] \u00d7_1 x_1 \u00d7_2\u22ef \u00d7_k x_k.$$\nThis map satisfies two very important properties for GNNs: it is permutation-invariant (due\nto the partial symmetry of T) and its number of parameters is independent of k (due to the\npartially symmetric CP parameterization). Thus, using only two parameter matrices of\nfixed size, the map in Eq. 3.3 can be applied to sets of N-dimensional vectors of arbi-\ntrary cardinality. In particular, we will show that it can be leveraged to replace both the\nAGGREGATE and UPDATE functions in GNNS.\nThere are several way to interpret the map in Eq. 3.3. First, from Eq. 3.1 we have\nf(x1,\u2026\u2026,Xk) = T X1 X1 X2 \uff65\uff65\uff65 \u00d7kXk = T (k+1)(XkXk\u22121\nX1),\nwhere T (k+1) \u2208 RM\u00d7Nk is the mode-(k+1) matricization of T. This shows that each element\nof the output f (x1,\uff65\uff65\uff65, xk) is a linear combinations of terms of the form (x1)i1 (x2)12(Xk)ik (k-\nth order multiplicative interactions between the components of the vectors x1,\u2026, xk).\nThat is, f is a multivariate polynomial map of order k involving only k-th order interac-\ntions. By using homogeneous coordinates, i.e., appending an entry equal to one to each\nof the input tensors x\u2081, the map f becomes a more general polynomial map taking into"}, {"title": "Theoretical Analysis", "content": "We now analyze the expressive power of CP layers. In order to characterize the set of\nfunctions that can be computed by CP layers, we first introduce the notion of multilinear\npolynomial. A multilinear polynomial is a special kind of vector-valued multivariate\npolynomial in which no variables appears with a power of 2 or higher. More formally, we\nhave the following definition.\nDefinition 2. A function g : Rk R is called a univariate multilinear polynomial if it can be\nwritten as\n$$g(a_1, a_2,\u22ef, a_\u03ba) = \\sum_{i_1=0}^{1}\u2026 \\sum_{i_k=0}^{1} \u03c4_{i_1i_2\u2026\u2026i_k} a^{i_1}_1a^{i_2}_2\u2026a^{i_k}_k$$\nwhere each \u03c4iri2\u2026\u2026ik \u2208 R. The degree of a univariate multilinear polynomial is the maximum\nnumber of distinct variables occurring in any of the non-zero monomials \u03c4iri2...in aa...a\nA function f : (Rd)k \u2192 Rp is called a multilinear polynomial map if there exist univariate\nmultilinear polynomials gi,j1,\u2026,in for j1,\u2026\u2026\u2026, jk \u2208 [d] and i \u2208 [p] such that\n$$f(x_1,\u2026, x_k) = \\sum_{j_1,...,j_k=1}^{d} g_{i,j_1,jk} ((x_1)_{j_1}, \u2026, (x_k)_{j_k})$$\nfor all x1,...,xk \u2208 Rd and all i \u2208 [p]. The degree of f is the highest degree of the multilinear\npolynomials gi,j1,\u2026,\u0130k*\nThe following theorem shows that CP layers can compute any permutation-invariant\nmultilinear polynomial map. We also visually represent the expressive power of CP layers\nin Fig. 3.3, showing that the class of functions computed by CP layer subsumes multilinear\npolynomials (including sum and mean aggregation functions).\nTheorem 1. The function computed by a CP layer (Eq. (1)) is permutation-invariant. In addition,\nany permutation-invariant multilinear polynomial f : (RF)k \u2192 Rd can be computed by a CP\nlayer (with a linear activation function)."}, {"title": "Experiments on Real-World Datasets", "content": "In this section, we evaluate Tensorized Graph Neural Net on real-world node- and graph-\nlevel datasets. We introduce experiment setup in 3.3.1, compare tGNN with the state-of-\nthe-arts models in 3.3.2, and conduct ablation study on model performance and efficiency\nin 3.3.3. The hyperparameter and computing resources are attached in Tab 3.3, 3.4. Dataset\ninformation can be found in Tab. 3.1, 3.2."}, {"title": "Experiment Setup", "content": "In this work, we conduct experiments on three citation networks (Cora, Citeseer, Pubmed)\nand three OGB datasets (PRODUCTS, ARXIV, PROTEINS) [30] for node-level tasks, one\nOGB dataset (MolHIV) [30] and three benchmarking datasets (ZINC, CIFAR10, MNIST) [18]\nfor graph-level tasks.\nTraining Procedure For three citation networks (Cora, Citeseer, Pubmed), we run experi-\nments 10 times on each dataset with 60%/20%/20% random splits used in [14], and report\naccuracy with standard deviation in Tab. 3.5. For data splits of OGB node and graph\ndatasets, we follow [30], run experiments 5 times on each dataset (due to training cost),\nand report results in Tab. 3.5, 3.6. For benchmarking datasets, we run experiments 5 times on\neach dataset with data split used in [18], and report results in Tab. 3.6. To avoid numerical\ninstability and floating point exception in tGNN training, we sample 5 neighbors for each\nnode. For graph datasets, we do not sample because the training is already in batch thus\nnumerical instability can be avoided, and we apply the CP pooling at both node-level\naggregation and graph-level readout."}, {"title": "Real-world Datasets", "content": "In this section, we present tGNN performance on node- and graph-level tasks. We compare\ntGNN with several classic baseline models under the same training setting. For three\ncitation networks, we compare tGNN with several baselines including GCN [40], GAT [74],\nGraphSAGE [24], H2GCN [90], GPRGNN [14], APPNP [42] and MixHop [1]; for three OGB\nnode datasets, we compare tGNN with MLP, Node2vec [22], GCN [40], GraphSAGE [24]\nand DeeperGCN [48]. And for graph-level tasks, we compare tGNN with several baselines\nincluding MLP, GCN [40], GIN [78], DiffPool [83], GAT [74], MoNet [62], GatedGCN [9],\nPNA [17], PHMGNN [46] and DGN [7].\nFrom Tab. 3.5, we can observe that tGNN outperforms all classic baselines on Cora,\nPubmed, PRODUCTS and ARXIV, and have slight improvements on the other datasets but\nunderperforms GCN on Citeseer and DeeperGCN on PROTEINS. On the citation networks,\ntGNN outperforms others on 2 out of 3 datasets. Moreover, on the OGB node datasets,"}, {"title": "Ablation and Efficiency Study", "content": "In the ablation study, we first investigate the effectiveness of having the high-order non-\nlinear CP pooling and adding the linear low-order transformation in Tab. 3.7, then in-\nvestigate the relations of the model performance, efficiency, and tensor decomposition\nr"}]}