{"title": "SSRFlow: Semantic-aware Fusion with Spatial Temporal Re-embedding for Real-world Scene Flow", "authors": ["Zhiyang Lu", "Zhimin Yuan", "Ming Cheng", "Qinghan Chen", "Chenglu Wen", "Cheng Wang"], "abstract": "Scene flow, which provides the 3D motion field of the first frame from two consec-utive point clouds, is vital for dynamic scene perception. However, contemporary scene flow methods face three major challenges. Firstly, they lack global flow embedding or only consider the context of individual point clouds before embed-ding, leading to embedded points struggling to perceive the consistent semantic relationship of another frame. To address this issue, we propose a novel approach called Dual Cross Attentive (DCA) for the latent fusion and alignment between two frames based on semantic contexts. This is then integrated into Global Fusion Flow Embedding (GF) to initialize flow embedding based on global correlations in both contextual and Euclidean spaces. Secondly, deformations exist in non-rigid objects after the warping layer, which distorts the spatiotemporal relation between the consecutive frames. For a more precise estimation of residual flow at next-level, the Spatial Temporal Re-embedding (STR) module is devised to update the point sequence features at current-level. Lastly, poor generalization is often observed due to the significant domain gap between synthetic and LiDAR-scanned datasets. We leverage novel domain adaptive losses to effectively bridge the gap of motion inference from synthetic to real-world. Experiments demonstrate that our approach achieves state-of-the-art (SOTA) performance across various datasets, with particu-larly outstanding results in real-world LiDAR-scanned situations. Our code will be released upon publication.", "sections": [{"title": "1 Introduction", "content": "3D scene flow estimation captures the motion information of objects from two consecutive point clouds and produces the motion vector for each point in the source frame. It serves as a founda-tional component for perceiving dynamic environments and provides important motion features to"}, {"title": "2 Methodology", "content": ""}, {"title": "2.1 Problem Definition", "content": "The scene flow task aims to estimate point-wise 3D motion information between two consecutive point cloud frames. The input includes the source frame \\(S = \\{s_i\\}_{i=1}^{N} = \\{x_i, f_i\\}_{i=1}^{N}\\) and target frame \\(T = \\{t_j\\}_{j=1}^{M} = \\{y_j, g_j\\}_{j=1}^{M}\\), where \\(x_i, y_j \\in \\mathbb{R}^3\\) are 3D coordinates of the points, and \\(f_i, g_j \\in \\mathbb{R}^d\\) represent the feature of the corresponding point at a specific level. It should be noted that N and M may not be equal due to the uneven point density and occlusion. The prediction of the model is the 3D motion vector \\(SF = \\{sf_i \\in \\mathbb{R}^3\\}_{i=1}^{N}\\) of each source frame point, representing the non-rigid motion towards the target frame."}, {"title": "2.2 Hierarchical Feature Extraction", "content": "The overview of our proposed network is shown in Figure 1. We utilize PointConv Wu et al. (2019) as the feature extraction backbone to build a pyramid network. To extract the higher-level semantic feature \\(S^{l+1}\\) of level (l + 1), we apply a three-step process to the previous lower-level feature \\(S^{l}\\). Farthest Point Sampling (FPS) is first employed to extract \\(N_{l+1}\\) center points from \\(S^{l}\\), where \\(N_{l+1} < N_l\\). Next, K-Nearest Neighbor (KNN) is used to group the neighbor points around each center point. Finally, PointConv is utilized to aggregate the local features for each group, resulting in the desired semantic feature \\(S^{l+1}\\)."}, {"title": "2.3 Global Fusion Flow Embedding", "content": "The GF module is designed to capture the global relation between consecutive frames during the flow initialization. After performing the multi-level feature extraction, we obtain \\(S^*\\) and \\(T^*\\) at the highest level of the semantic pyramid. Following that, the global fusion flow embedding is constructed from \\(S^*\\) to all points in \\(T^*\\) in both semantic context space and Euclidean space, as shown in Figure 2. The previous algorithmsKittenplon et al. (2021); Wang et al. (2022a) merely utilized the individual and unaligned semantic features of two consecutive point clouds for hard global embedding. However, the flow embedding of a point is generated in response to the semantic context of another frame, necessitating the simultaneous consideration of the fused features in a consistent semantic space between the two frames during embedding. To enhance mutual understanding of semantic context between two frames of point clouds, we first utilize the DCA module to fuse and align semantic context from both frames. This equips each frame with the ability to perceive the global semantic environment of the other frame, leading to a more reliable latent correlation."}, {"title": "2.4 Warping Layer", "content": "We employ distance-inverse interpolation to upsample the coarse sparse scene flow from level (l + 1) to obtain the coarse dense scene flow of level l. The obtained coarse dense flow is directly accumulated onto the source frame \\(S^{l}\\) to generate the warped source frame \\(WS^{l}\\) = \\(\\{wx_i = x_i + sf_i, f_i\\}_{i=1}^{N}\\), which brings the source and target frames closer and allows the subsequent layers to only consider the estimation of residual flow Cheng and Ko (2022); Fu et al. (2023); Kittenplon et al. (2021); Wang et al. (2022a); Wei et al. (2021); Wu et al. (2020)."}, {"title": "2.5 Spatial Temporal Re-embedding", "content": "After the warping layer, the spatiotemporal relation between the consecutive frames may change. Specifically, the temporal features of points from the warped source frame to the target change since the position between the two point clouds is closer. Furthermore, dynamic non-rigid objects in the source frame may encounter surface distortion during warping, resulting in different spatial features. Therefore, it is necessary to re-embed spatiotemporal point features before the Local Flow Embedding (LFE), which is implemented in a patch-to-patch manner between the two frames following Wu et al. (2020). Based on this consideration, we re-embed the spatiotemporal features of each point \\(ws_i\\) at level l based on the warped source frame \\(WS_l\\) and the target frame \\(T_l\\), as depicted in Figure 3.\nTemporal Re-embedding First, we locate the K nearest neighbor points group \\(N_T(ws_i)\\) of point \\(ws_i\\) in \\(T_l\\). For each target point \\(t_j \\in N_T(ws_i)\\), by employing position encoder as (3), a 9D positional feature \\(PE_{ij}\\) is acquired for this group, representing the positional relation between the two frames after warping. Then, the initial temporal re-embedding feature is derived using the following formula:\n\\[TRF_{ij} = MLP(\\eta(g_j, f_i, PE_{ij})).\\]\nInstead of employing the hard aggregation method of MaxPooling, which results in flow bias due to the non-corresponding points between the two frames, we leverage local similarity map \\(LM_l = \\{LM_{ij}\\}\\) in both feature space and Euclidean space to derive the soft aggregation weights,\n\\[LM_{ij} = \\sigma(MLP(\\eta(TRF_{ij}, MLP(PE_{ij})))).\\]\n\\[TRF_i = \\sum_{j}LM_{ij}TRF_{ij}.\\]\nSpatial Re-embedding Spatial Re-embedding shares the same framework as Temporal Re- embedding, with the only distinction being that the embedding object changes to the warped source frame itself. Upon acquiring the temporal re-embedding features \\(TRF^* = \\{TRF^*\\}\\) and spatial re-embedding features \\(SRF^* = \\{SRF^*\\}\\) of each point in the warped source frame of level l, we fuse them by leveraging the Fusion Net module to derive the ultimate comprehensive features\n\\[STRF_i = MLP(\\eta(TRF^*,SRF^*)),\\]\nand the warped frame updates to \\(WS_l = \\{wx_i, STRF_i\\}_{i=1}^{N}\\). As shown in Figure 1, the STR module is followed by LFE, which computes the patch-to-patch cost volume of each point \\(ws_i\\) by utilizing the spatiotemporal re-embedding features."}, {"title": "2.6 Flow Prediction", "content": "This module is constructed by combining PointConv, MLP, and a Fully Connected (FC) layer. For each point \\(s_i\\) in the source frame, its local flow embedding feature, along with the warped coordinates and \\(STRF_i\\) are input into the module. PointConv is first employed to incorporate the local information of each point, followed by non-linear transformation in the MLP layer. The final output is the scene flow \\(sf_i\\), regressed through the FC layer."}, {"title": "3 Training Losses", "content": ""}, {"title": "3.1 Hierarchical Supervised Loss", "content": "A supervised loss is directly hooked to the GT of scene flow, and we leverage multi-level loss functions as supervision to optimize the model across various pyramid levels. The GT of scene flow at level l is represented as \\(SF_l = \\{f_i^l\\}_{i=1}^{N_l}\\), and the predicted flow is \\(SF_l = \\{sf_i^l\\}_{i=1}^{N_l}\\). The multi-level supervised loss is as follows:\n\\[L_{sup} = \\sum_{l=1}^{5} \\delta_l \\sum_{i=1}^{N_l}||f_i^l - sf_i^l||_2,\\]\nwhere \\(\\delta\\) is the penalty weight, with \\(\\delta_1 = 0.02, \\delta_2 = 0.04, \\delta_3 = 0.08, \\delta_4 = 0.16, and \\delta_5 = 0.32\\)."}, {"title": "3.2 Domain Adaptive Losses", "content": "Local Flow Consistency (LFC) Loss Dynamic objects in real-world scenes may not exhibit absolute rigid or regular motion. Instead, they typically undergo local rigid motion, which is manifested through the consistency of local flow. The degree of predicted flow difference between each point \\(s_i\\) and its KNN+Radius points group \\(N_R(s_i)\\) in the source frame at the full resolution level (\\(N_l = 8192\\)) is defined as the LFC loss, where point \\(p \\in N_R(s_i)\\) denotes \\(p \\in N_s(s_i)\\) and the \\(l_2\\) distance between p and \\(s_i\\) is less than R. The KNN+Radius search strategy effectively mitigates the influence of noise points resulting from occlusion and sparsity in point clouds, as demonstrated in Sec B.2 of Appendix. Formally, the LFC loss is represented as follows:\n\\[L_{lfc} = \\frac{1}{N_1}\\sum_{i=1}^{N_1}\\frac{1}{|N_R(s_i)|}\\sum_{j \\in N_R(s_i)}||sf_i - sf_j||_2,\\]\nwhere \\(|\\cdot|\\) is the number of points in a group.\nCross-frame Feature Similarity (CFS) Loss The semantic features of the points in the warped source frame are similar to those in the surrounding target frame, as they should be in a dynamic registered state. Specifically, we accumulate the GT scene flow \\(sf_i\\) directly onto the source frame at the full resolution level, as described by \\(ws_i = \\{x_i + sf_i, STRF_i\\}\\). Next, we utilize cosine similarity to compute the similarity between \\(ws_i\\) and \\(t_j \\in N_R(ws_i)\\) in the target frame:\n\\[CS(ws_i,t_j) = \\frac{STRF \\cdot g_j}{||STRF||_2||g_j||_2}.\\]\nWe utilize the features of source frame points derived from the last layer of the STR module (the rightmost re-embedding feature in Figure 1) as input instead of the initially extracted features. Lastly, we establish a similarity threshold TH and employ function F to penalize points that exhibit a similarity lower than TH:\n\\[L_{cfs} = \\frac{1}{N_1}\\sum_{i=1}^{N_1}\\sum_{t_j \\in N_R(ws_i)}\\frac{F(CS(ws_i,t_j) - TH)}{|N_R(ws_i)|},\\]\nwhere F(x) = -x if x < 0 and 0 otherwise, and \\(g_j\\) is updated by the Temporal Re-embedding module of the STR module with the warped source frame for a reliable and precise loss. The final loss of our model is :\n\\[L_{all} = \\lambda_1L_{sup} + \\lambda_2L_{lfc} + \\lambda_3L_{cfs},\\]\nwhere R = 0.05, \u03a4\u0397 = 0.95, and \\(\\lambda_1 = 0.7, \\lambda_2 = 0.15, \\lambda_3 = 0.15\\) by default."}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Datasets and Data Preprocessing", "content": "The experiments were performed on four datasets: the synthetic dataset FlyThings3D (FT3D) Mayer et al. (2016) and three real-world datasets including Stereo-KITTI Menze et al. (2015, 2018), SF-KITTI Ding et al. (2022), and LiDAR-KITTI Geiger et al. (2012); Gojcic et al. (2021), as shown in"}, {"title": "4.2 Experimental Settings", "content": "Implementation Details Our model is implemented with PyTorch 1.9, and both training and testing are conducted on a single NVIDIA RTX3090 GPU. The AdamW optimizer Loshchilov and Hutter (2017) with \\(\\beta_1 = 0.9\\) and \\(\\beta_2 = 0.99\\) is used for model tuning during the training phase, with an initial learning rate of 0.001 which was decayed by half every 80 epochs. We train our model in an end-to-end manner for 900 epochs (or reached convergence) with batch size 8. The cross-attention is"}, {"title": "4.3 Results and Analysis", "content": "Our method exhibits remarkable generalization ability across various scenarios, encompassing both synthetic and real-world scenes, as well as dense or sparse point clouds. In contrast, some previous methods are tailored to specific datasets.\nFT3Ds and KITTIS We compare with recent SOTA methods on the FT3Ds and KITTIs datasets. The quantitative results presented in Table 1 indicate that SSRFlow outperforms the other methods by a large margin, especially in real-world datasets. Specifically, on the FT3Ds dataset, SSRFlow is on par with previous SOTACheng and Ko (2023) while achieving a 63% reduction in inference time, as listed in Table 2. Further, our model exhibits exceptional generalization performance on the KITTIs dataset, surpassing the second place by 50% on EPE3D. Qualitative analysis is shown in Figure 5.\nFT3Do and KITTIo Similar to the above, we train our model on FT3Do and test on KITTIo without any fine-tuning. The experimental results are listed in Table 3, which reveal the good performance of our model even with occlusion. Specifically, our model achieves 39% improvement over the previous SOTA method Cheng and Ko (2023) on FT3Do. Furthermore, SSRFlow outperforms Cheng and Ko (2023) by a significant margin, surpassing it by 33% and 40% in terms of EPE3D and Out3D on the real-world occluded KITTIo dataset, respectively. Visualized experimental results are provided in the Figure 6."}, {"title": "4.4 Ablation Study", "content": "To investigate the distinct impacts of GF, STR, and DA Losses, a set of ablation experiments are conducted to perform functional analysis. The comprehensive results of the ablation experiments can be found in Table 5, while detailed information is presented in Table 6 and Table 7.\nGF In (a) of Table 7, we provide a detailed list of the importance of the DCA Fusion, location of position encoder, aggregation style, and all-to-all point-pair concatenation. Firstly, we exclude the DCA Fusion and the subsequent global attentive aggregation in GF and directly utilize the original semantic feature for global flow embedding. Secondly, we test the internal and external position encoder of cross-attention in DCA Fusion. Additionally, we replace the attentive weighted aggregation with MaxPooling. Finally, we substitute the all-to-all match method with KNN. After removing the DCA Fusion, the model experienced a substantial decline in accuracy, primarily due to its capability to fuse point features with another frame context before embedding. Moreover, the"}, {"title": "5 Conclusion", "content": "We propose SSRFlow network to accurately and robustly estimate scene flow. SSRFlow conducts global semantic feature fusion and attentive flow embedding in both Euclidean and context spaces. Additionally, it effectively re-embeds deformed spatiotemporal features within local refinement. The DA Losses enhance the generalization ability of SSRFlow on various pattern datasets. Experiments show that our method achieves SOTA performance on multiple distinct datasets."}, {"title": "A Network", "content": ""}, {"title": "A.1 Network Architecture", "content": "Our proposed network architecture mainly consists of three components: (1) Hierarchical point cloud feature extraction. (2) Global attentive flow initialization. (3) Local flow refinement. To hierarchically extract semantic features from point clouds, we begin by adopting a pyramid feature extraction network. Subsequently, we construct an attentive global flow embedding that accounts for both high-dimensional feature space and Euclidean space. The resulting sparse flow is then upsampled using upsampling and warping layers, enabling denser flows at lower levels. These flows are accumulated onto the source frame, ultimately yielding the warped source frame. Thereafter, spatiotemporal re-embedding features are used for hierarchical refinement of the residual scene flow between the warped source frame and the target frame. Through iterative refinement, full-resolution scene flow is generated as the ultimate result."}, {"title": "A.2 DCA Fusion", "content": ""}, {"title": "A.3 Local Flow Embedding", "content": "In this section, we input the warped source frame and target frame into the Local Flow Embedding (LFE) module to generate a local flow embedding, which is realized following the patch-to-patch approach Wu et al. (2020).\nThe matching cost between two frames of point clouds can be defined as follows:\n\\[cost (y_j, wx_k) = MLP(\\eta(STRF_k,g_j, y_j - wx_k)),\\]\nwhere \\(\\eta\\) stands for concatenation. \\(STRF_k\\) is the feature of warped source point \\(wx_k\\), and \\(g_j\\) is the feature of target point \\(y_j\\). Afterward, this matching cost can be aggregated into a point-to-patch cost volume between the two frames.\n\\[CV_{point} (WX_k) = \\sum_{Y_j \\in N_T(WX_k)} MLP(y_j - wx_k) cost (y_j, wx_k) .\\]\nFinally, the patch-to-patch cost volume for warped source point \\(wx_i\\) can be defined as:\n\\[CV_{patch} (WX_i) = \\sum_{wx_k \\in N_{WS}(wxi)} MLP(wx_k - wxi)CV_{point}(WX_k).\\]\nHere \\(MLP(wx_k - wxi)\\) and \\(MLP(y_j - wx_k)\\) are the aggregation weights determined by the direction vectors. Follow the notation in the main text, we use \\(N_{WS}(wxi)\\) to define the neighbors of \\(wx_i\\) in frame \\(W_s\\) and \\(N_T(wxk)\\) to define the neighbors of \\(wx_k\\) in frame T. The patch-to-patch manner employed in the cost volume is illustrated in Figure 8."}, {"title": "B Experiments Settings", "content": ""}, {"title": "B.1 Evaluation Metrics", "content": "For fair comparisons, we leverage the same set of evaluation metrics as in the previous methods Cheng and Ko (2022); Liu et al. (2019); Wang et al. (2022a); Wu et al. (2020)."}, {"title": "B.2 Search Methods", "content": "To assess the enhancement of the KNN+Radius approach compared to the pure KNN method, we visualize the flow disparities of different points under the search strategies of KNN and KNN+Radius local point clusters based on the ground truth of the scene flow."}, {"title": "B.3 Hyper-parameters in DA Losses", "content": "We conduct a comprehensive experimental analysis on the hyper-parameters K, R, and TH in the context of DA Losses, where R represents the radius truncation of neighbor search, K represents the number of neighbors in KNN, and TH represents the cross-frame feature similarity threshold. It is worth mentioning that R and K are uniform across both Local Flow Consistency (LCF) and Cross-frame Feature Similarity (CFS) loss functions since they are utilized to identify local rigid blocks with similar semantic features.\nTo analyze the local consistency of ground truth (GT) flow, we initiate the process by selecting a suitable scenario in the KITTIs dataset that exhibits minimal visible background points, as depicted in Figure 10. Subsequently, we systematically augment the ranges of R and K to acquire diverse consistency outcomes. As shown in Figure 10 and Figure10, the quantity of KNN group points exhibits a gradual decrease as the truncation radius decreases. Simultaneously, the average flow dif-ferences within each group also decrease, indicating that the KNN+Radius search method effectively eliminates the noise points.\nWe further explore the disparities between KNN and KNN+Radius search methods. The KNN+Radius search method results in certain points without any neighboring points. These isolated points, known as exceptionally sparse or invalid points, function as noise points that impede flow smoothing. However, utilizing the pure KNN search method would optimize flow consistency between noise points and mandatory surrounding points, which is an incorrect outcome. By observing the size of local rigid blocks in real-world scenarios, we finally select R = 0.05m as the truncation radius to perform ablation experiments on different KNN search numbers. The results are shown in Figure 10."}, {"title": "C Limitations", "content": "Inferring motion flow for occluded objects has always been a challenging task. Particularly, in situations where objects are completely occluded, as defined in Ouyang and Raviv (2021), motion inference relying on the consistency of local motion becomes implausible. In real-world scenarios, taking into account roadway regulations, it might be feasible to estimate the movement of occluded vehicles on the lane by considering the overall directionality of traffic flow. However, for synthetic datasets like FT3Do, where random motion is assigned, it is challenging to infer the motion of completely occluded objects even for humans due to the unpredictable and random nature of the motion generation. Our model exhibits poor performance in some completely occluded synthetic scenes, as illustrated in Figure 11. An effective approach involves integrating a specific pattern of motion (object-wise relation) during the generation of a synthesized scene flow dataset."}, {"title": "D Related Works", "content": "FlowNet3D Liu et al. (2019) pioneers in leveraging deep learning network PointNet++Qi et al. (2017) for scene flow embedding based on raw point clouds, which surpasses traditional methods by a large margin. Afterward, HPLFlowNetGu et al. (2019) proposed novel DownBCL, UpBCL, and CorrBCL operations inspired by Bilateral Convolutional layers to abstract and fuse structural information from consecutive point clouds. FlowNet3D++ Wang et al. (2020) enhances FlowNet3D by incorporating geometric constraints based on point-to-plane distance and angular alignment. FESTA Wang et al. (2021b) expands on FlowNet3D by utilizing a trainable aggregate pooling to stably down-sample points instead of Farthest Point Sampling (FPS). These above methods employ the SetConvLiu et al. (2019), composed of PointNet++, to conduct local flow embedding from two frames. However, this local embedding approach lacks global representation and fails to multi-scale processing.\nInspired by Sun et al. (2018) in optical flow, PointPWC Wu et al. (2020) incorporates the Pyramid, Warp, and Cost volume (PWC) to scene flow estimation. PointPWC utilizes semantic features from point cloud pyramids at different levels to generate the cost volume, which is then used to compute patch-to-patch local flow embedding. HALF Wang et al. (2021a) introduces a novel double attentive"}, {"title": "E Datasets", "content": "Experiments are conducted on four datasets, namely the synthetic dataset FlyThings3D (FT3D) Mayer et al. (2016) and three real-world datasets, including Stereo-KITTI Menze et al. (2015, 2018), LiDAR-KITTI Geiger et al. (2012), and SF-KITTI Ding et al. (2022).\nThe synthetic dataset FT3D is derived from a large-scale collection of stereo videos. Each pair of point cloud scenes is generated from RGB stereo images in the ShapeNet Chang et al. (2015), with random motion assigned to multiple objects within the scenes. Another stereo dataset Stereo-KITTI is a real-world dataset comprising 200 training sets and 200 testing sets. Building on the previous literature, two distinct preprocessing techniques are employed for these two datasets. A technique from Gu et al. (2019) is utilized to remove points that do not correspond between consecutive frames, resulting in processed datasets referred to as FT3Ds and KITTIs. The second technique, proposed by Liu et al. (2019), takes a different approach by preserving the occluded points. Instead of removing them, mask labels are used to indicate occluded points for evaluation purposes. The datasets generated using this method are referred to as FT3Do and KITTIo. The FT3Ds dataset comprises 19,640 pairs of training data and 3,824 pairs for evaluation. On the other hand, the FT3Do dataset consists of 20,008 point cloud pairs for training and 2,008 pairs for testing. Regarding the KITTIs and KITTIO datasets, they contain 142 and 150 pairs of test-only data, respectively.\nHowever, it is noteworthy that the FT3D and Stereo-KITTI datasets, which are derived from dense and regular disparity images, differ from real-world LiDAR-scanned datasets. To demonstrate the robust generalization of our SSRFlow on LiDAR-scanned datasets, we conducted additional experiments using datasets Ding et al. (2022); Geiger et al. (2012); Gojcic et al. (2021). Specifically, SF-KITTI Ding et al. (2022) is a large-scale real-world scene flow dataset that is based on LiDAR-scanned data. It comprises 7,185 pairs of data. Following Ding et al. (2022), we further divide the dataset into 6,400 pairs for training and 600 pairs for testing. In addition, the LiDAR-KITTI Geiger et al. (2012); Gojcic et al. (2021), dataset includes 142 pairs of real-world scenarios captured by the Velodyne 64-beam LiDAR, specifically designed for testing purposes. This dataset is particularly valuable as it highlights the sparse and non-corresponding point characteristics that are often observed between consecutive frames of the real-world LiDAR-scanned point clouds."}, {"title": "F More Results", "content": ""}]}