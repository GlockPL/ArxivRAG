{"title": "Invariant Correlation of Representation with Label", "authors": ["Gaojie Jin", "Ronghui Mu", "Xinping Yi", "Xiaowei Huang", "Lijun Zhang"], "abstract": "The Invariant Risk Minimization (IRM) approach aims to address the challenge\nof domain generalization by training a feature representation that remains invari-\nant across multiple environments. However, in noisy environments, IRM-related\ntechniques such as IRMv1 and VREx may be unable to achieve the optimal\nIRM solution, primarily due to erroneous optimization directions. To address this\nissue, we introduce ICorr (an abbreviation for Invariant Correlation), a novel\napproach designed to surmount the above challenge in noisy settings. Addition-\nally, we dig into a case study to analyze why previous methods may lose ground\nwhile ICorr can succeed. Through a theoretical lens, particularly from a causal-\nity perspective, we illustrate that the invariant correlation of representation with\nlabel is a necessary condition for the optimal invariant predictor in noisy envi-\nronments, whereas the optimization motivations for other methods may not be.\nFurthermore, we empirically demonstrate the effectiveness of ICorr by comparing\nit with other domain generalization methods on various noisy datasets.", "sections": [{"title": "1 Introduction", "content": "Over the past decade, deep neural networks (DNNs) have made remarkable progress\nin a wide range of applications, such as computer vision [1-3] and natural language\nprocessing [4, 5]. Typically, most deep learning models are trained using the Empiri-\ncal Risk Minimization (ERM) [6] approach, which assumes that training and testing\nsamples are independently drawn from an identical distribution (I.I.D. assumption).\nNevertheless, recent studies have reported increasing instances of DNN failures [7-9]\nwhen this I.I.D. assumption is violated due to distributional shifts in practice.\nInvariant Risk Minimization [10] is a novel learning approach that addresses the\nchallenge of domain generalization (also known as out of distribution problem) in\nthe face of distributional shifts. The fundamental concept behind IRM is to train a\nfeature representation that remains invariant across multiple environments [11], such\nthat a single classifier can perform well in all of them. Although obtaining the opti-\nmal invariant feature representation is challenging, previous works employ alternative\nmethods [12-14] to approximate it. The success of IRM approach in existing train-\ning environments can ensure its ability to generalize well in new environments with\nunseen distributional shifts, which is evidenced by positive empirical results [15, 16].\nHowever, in the real world, different environments (or domains) may exhibit vary-\ning levels of inherent (independent) noises, leading to various inherent losses. Even an\noptimal IRM model cannot mitigate these inherent losses, resulting in varying opti-\nmal losses across different environments. As shown in Figure 1, inherent noise (such\nas snow or water) can impact the invariant feature (dog), such as covering the face or\nblurring the body, resulting in different inherent losses. Existing IRM-related meth-\nods, such as IRMv1, VREx [17] and Rame et al. [15], focus on optimizing the model\nin different clean environments but may fail in these noisy situations.\nWe conduct an analysis in this study to identify the reasons why existing IRM-\nrelated methods may be ineffective in noisy environments. Upon examining the case\nstudy presented in Section 2.3, it has come to our attention that the optimization"}, {"title": "2 Study IRM in noisy environments", "content": ""}, {"title": "2.1 Preliminaries", "content": "Given that X and Y are the input and output spaces respectively, let $E :=\\{e_1, e_2, ..., e_m\\}$ be a collection of m environments in the sample space $X \\times Y$ with dif-\nferent joint distributions $P_e(x, y)$, where $e \\in E$. Consider $E_{tr} \\subset E$ to be the training\nenvironments and $S_e :=\\{(x^e, y^e)\\}$, to be the training dataset drawn from distribu-\ntion $P_e(x, y)$ ($e \\in E_{tr}$) with $n_e$ being dataset size. Given the above training datasets\n$S_e$ ($e \\in E_{tr}$), the task is to learn an optimal model $f (\\cdot; w) : X \\rightarrow Y$, such that $f(x_e; w)$\nperforms well in predicting y when given $x_e$ not only for $e \\in E_{tr}$ but also for $e \\in E \\setminus E_{tr}$,\nwhere w is the parameters of f.\nThe ERM algorithm [6] tries to solve the above problem via directly minimizing\nthe loss throughout training environments:\n$\\min_w \\sum_{e\\in E_{tr}} \\mathbb{R}_e(w),  \\tag{ERM}$\nwhere $ \\mathbb{R}_e(w),  R(x_e, w)$ are the expected loss of $f(\\cdot; w)$ in the environment e, the loss\nof $f(x_e; w)$ for the data $x_e$, respectively.\nIRM [10] firstly supposes that the predictor $f(\\cdot; w)$ can be made up of $g(\\cdot; \\Phi)$ and\n$h(\\cdot; v)$, i.e., $f (\\cdot; w) = h(g(\\cdot; \\Phi); v)$, where $w = \\{v, \\Phi\\}$ are the model parameters. Here,"}, {"title": "2.2 Invariant correlation of representation with label", "content": "We now formally describe our method (ICorr) to extract invariant features in noisy\nenvironments. ICorr performs robust learning via stabilizing the correlation between\nrepresentation and true label across environments:\n$\\min_{w} \\lambda \\text{Var}(p_{f, y}(w)) + \\sum_{e \\in E_{tr}} \\mathbb{R}_e(w), \\tag{ICorr}$\nwhere $p_{f, y}(w) = E_{x_e, y}(f(x_e;w)y)$ is the correlation between $f(x_e; w)$ and y in the\nenvironment e, $f(x_e;w) = f(x_e;w) - E_{x_e} (f(x_e; w))$, and $\\text{Var}(p_{f,y}(w))$ represents\nthe variance of the correlation in $E_{tr}$. Here $\\lambda\\in [0, +\\infty)$ controls the balance between\nreducing average loss and enhancing stability of correlation, with $\\lambda = 0$ recovering\nERM, and $\\lambda \\rightarrow +\\infty$ leading ICorr to focus entirely on making the correlation equal.\nIn the following, we demonstrate the power of ICorr in noisy environments through\nthe case study (Section 2.3) and the theoretical analysis of causality (Section 3),\nrespectively."}, {"title": "2.3 Why is ICorr necessary (a case study in two-bit environments)", "content": "Arjovsky et al. [10] present the Colored-MNIST task, a synthetic challenge derived\nfrom MNIST, to demonstrate the efficacy of the IRM technique and IRMv1 in partic-\nular. Although MNIST pictures are grayscale, Colored-MNIST images are colored red\nor green in a manner that strongly (but spuriously) correlates with the class label. In\nthis case, ERM successfully learns to exploit the color during training, but it fails at\ntest time when the correlation with the color is inverted.\nKamath et al. [18] study an abstract version of Colored-MNIST based on two bits\nof input, where y is the label to be predicted, $x_1$ is correlated with the label of the\nhand-written digit (0 \u2013 4 or 5 \u2013 9), and $x_2^e$ corresponds to the color (red or green)."}, {"title": "3 Theoretical analysis from causal perspective", "content": "In this section, we present our theoretical understanding of ICorr from the perspective\nof causality. Following the theoretical setting from Arjovsky et al. [10] and Peters\net al. [11], we formally prove that (1) $\\text{Var}(p_{f,y}(w)) = 0$ is a necessary condition\nfor the optimal invariant predictor in noisy environments; (2) $||\\nabla_{v|_{v=1}}R^e(w)|| = 0$,\n$\\text{Var}(R^e(w)) = 0$ and some other minimal penalty terms may not be necessary for the\noptimal invariant predictor in noisy environments.\nSetting: Consider several training environments $E_{tr} = \\{e_1, e_2, ...\\}$ and $x^e$ to be the\nobserved input of $e \\in E_{tr}$. We adopt an anti-causal framework [10] with data generation\nprocess as follows:\n$\\begin{aligned}\ny &= \\gamma^T x_{\\text{inv}} + \\eta_y, \\\\\nx_{\\text{inv}} &= x_{\\text{inv}} + \\eta_{\\text{inv}}, x^e = x_s + \\eta_s, \\\\\nx^e &= S\\begin{pmatrix}x_{\\text{inv}}\\\\\nx_s^e\\end{pmatrix}\n\\end{aligned}$\nwhere $y \\in \\mathbb{R}^{d_{\\text{inv}}}$ and $y \\neq 0$, the hidden invariant feature $x_{\\text{inv}}$ and the observed\ninvariant feature $x_{\\text{inv}}^e$ take values in $\\mathbb{R}^{d_{\\text{inv}}}$, the hidden spurious feature $x_s^e$ and the\nobserved spurious feature $x_s^e$ take values in $\\mathbb{R}^{d_s}$, and $S : \\mathbb{R}^{(d_{\\text{inv}}+d_s)} \\rightarrow \\mathbb{R}^d$ is an inherent\nmapping to mix features. The hidden spurious feature $x_s^e$ is generated by y with any\nnon-invariant relationship, $\\eta_{\\text{inv}}$ and $\\eta_s^e$ are independent Gaussian with bounded mean\nand variance changed by environments, $\\eta_y$ is an independent and invariant zero-mean\nGaussian with bounded variance. As the directed acyclic graph (DAG) in Figure 4(b)\nshows, the hidden invariant feature $x_{\\text{inv}}$ generates the true label y and y generates\nthe hidden spurious feature $x_s^e$. In consideration of environmental noise, we can only\nobserve the input $x^e$ which is a mixture of $x_{\\text{inv}}^e$ and $x_s^e$ after mapping. (Note that the\nobserved feature is generated by applying environmental noise to the hidden feature.)\nWe aim to learn a classifier to predict y based on $x^e$, i.e., $f(x^e; w) = h(g(x^e; \\Phi); v)$.\nDrawing upon the foundational assumption from IRM [10], i.e., assume that there\nexists a mapping $S : \\mathbb{R}^{d} \\rightarrow \\mathbb{R}^{d_{\\text{inv}}}$ such that $S(S(x)) = x_1$ for all $x_1 \\in \\mathbb{R}^{d_{\\text{inv}}}$ and $x_2 \\in\n\\mathbb{R}^{d_s}$, the following theorem mainly states that, in noisy environments, if there exists a\nrepresentation that elicits the optimal invariant predictor $f(\\cdot; w)$ across all possible\nenvironments $E$, then the correlation between $f(x^e; w)$ and y remains invariant for all\n$e \\in E$.\nTheorem 3.1. Assume that there exists a mapping $S : \\mathbb{R}^{d} \\rightarrow \\mathbb{R}^{d_{\\text{inv}}}$ such that\n$S(S(x)) = x_1$ for all $x_1 \\in \\mathbb{R}^{d_{\\text{inv}}}, x_2 \\in \\mathbb{R}^{d_s}$. Then, if $I$ elicits the desired (optimal)\ninvariant predictor $f(\\cdot; w) = y^TS(\\cdot)$, we have\n$p_{f,y}(w) = E_{x^e,y} [y^Tf(x^e;w)] - E[y^TS(x^e)]E[y] = Var(y^Tx_{\\text{inv}})$\nholds for all $e \\in E$. Thus we get $\\text{Var}(p_{f,y}(w)) = 0$.\nProof. See Appendix D.\nTheorem 3.1 indicates that in noisy environments, minimizing the regularization\nterm of ICorr, i.e., $\\text{Var}(p_{f,y}(w))$, is a necessary condition to find the invariant features.\nThe intuition behind Theorem 3.1 is that, the correlation between the representation\nand the true label can effectively prevent interference in noisy environments, whereas\nIRMv1 and VREx may get stuck. In the following, we would like to point out that the"}, {"title": "4 Experiments", "content": "In this section, we implement extensive experiments with ColoredMNIST [10], Cir-\ncle dataset [24], noisy DomainBed [25] framework, noisy Waterbirds [26-28] and\nCelebA [29] datasets. The first part includes comprehensive experiments on ColoredM-\nNIST using multi-layer-perceptrons (MLP) with varying environmental noises. In the\nsecond part, we conduct further experiments to verify the effectiveness of ICorr in\nmore noisy environments with more architectures."}, {"title": "4.1 MLP with ColoredMNIST", "content": "Training setting: This proof-of-concept experiment of ColoredMNIST follows the\nsettings from Arjovsky et al. [10], Krueger et al. [17]. The MLP consists of two hidden\nlayers with 256 and 256 units respectively. Each of these hidden layers is followed by a"}, {"title": "5 Related work", "content": "The domain generalization problem is initially explicitly described by Blanchard et al.\n[33] and then defined by Muandet et al. [34], which takes into account the potential\nof the target data being unavailable during model training. A large body of literature\nseeks to address the domain generalization challenge, typically through additional\nregularizations of ERM [6]. The regularizations from Sagawa et al. [28], Motiian et al.\n[35], Namkoong and Duchi [36] enhance model robustness against minor distributional\nperturbations in the training distributions, some works [37-39] further improve this\nrobustness with extra assumptions, while some other regularizations [40-45] promote\ndomain invariance of learned features."}, {"title": "6 Limitation", "content": "Our theoretical results are based on the assumption that there exists $S\\in \\mathbb{R}^{d_{\\text{inv}}\\times d}$\nsuch that $SS(x) = x_1$, for all $x_1 \\in \\mathbb{R}^{d_{\\text{inv}}}, 1,x_2 \\in \\mathbb{R}^{d_s}$, which has also been utilized in\nthe pioneering work of IRM by Arjovsky et al. [10]. Nonetheless, this $S$ may not exist\nin DNNs when facing complicated S. Although this may pose new challenges, we aim\nto advance our research by studying more possible cases of S and S in the future."}, {"title": "7 Conclusion", "content": "In this work, we introduce an IRM-related method named ICorr, which leverages the\ncorrelation between representation and label to overcome the challenge of training\nan invariant predictor in noisy environments. A detailed case study involving two-\nbit environments is conducted to elucidate why conventional methods might falter,\nwhereas ICorr maintains its efficacy in such noisy settings. Through rigorous theo-\nretical analyses of causality, we demonstrate the critical importance of maintaining\ninvariant correlation across noisy environments to achieve the optimal IRM solution.\nIn addition to our theoretical insights, we conduct extensive experiments which show\nthe superior performance of ICorr compared to other methods in such noisy cases.\nData availability statement: The research conducted in this work solely utilizes\npublicly available datasets, the code is available in the uploaded file."}, {"title": "Appendix A More case study results", "content": ""}, {"title": "Appendix B Calculation details", "content": ""}, {"title": "B.1 Calculation for IRMv1, VREx and ICorr", "content": "Following Kamath et al. [18] and L\u00e9on Bottou, we provide the calculation details of\nIRMv1, VREx and ICorr solutions as follows.\nSuppose $E_{tr}$ consists of two environments $e_1 = (\\alpha, \\beta_{e1}, \\eta_{e1})$ and $e_2 = (\\alpha, \\beta_{e2}, \\eta_{e2})$.\nFrom the definition of IRMv1, VREx, ICorr, for any $f(x) = 1\\cdot g(x^e; \\Phi) = w_1x_1+w_2x$\nwith square loss, we have that:\nwhen optimizing IRMv1 till $\\nabla_{v|_{v=1}}R^e(w) = 0$, we get\n$\\begin{aligned}\n&E_{x_{e1},y} (w_1x_1^1 + w_2x_2^1 - y) (w_1x_1^1 + w_2x_2^1) = 0, \\\\\n&E_{x_{e2},y} (w_1x_1^2 + w_2x_2^2 - y) (w_1x_1^2 + w_2x_2^2) = 0;\n\\end{aligned} \\tag{B1}$\nwhen optimizing VREx till $\\text{Var}(R^e(w)) = 0$, we get\n$E_{x_{1},y} (w_1x_1^1 + w_2x_2^1 - y)^2 = E_{x_{e2},y} (w_1x_1^2 + w_2x_2^2 - y)^2 ; \\tag{B2}$\nwhen optimizing ICorr with $\\text{Var}[p_{f,y}(w)] = 0$, we get\n$E_{x_{e1},y}(w_1x_1^1y + w_2x_2^1y) = E_{x_{e2},y} (w_1x_1^2y + w_2x_2^2y). \\tag{B3}$\nCase 1: For both $\\eta_{e1} = 0$ and $\\eta_{e2} = 0$, we have (i) $E[(x_1^e)^2] = E[(x)^2] = 1$, (ii)\n$E(x_1^ey) = \\alpha$, $E(x_2^ey) = b_i$, (iii) $E(x_1^ex_2^e) = \\alpha b_i$, where $a := 1 - 2\\alpha$ and $b_i := 1 - 2\\beta_{ei}$\nfor $i \\in \\{1,2\\}$.\nThen, according to (B1), the solutions for IRMv1 $(\\lambda = +\\infty)$ are\n$\\begin{aligned}\n&(1) \\  w_1 = 0, w_2 = 0; \\\\\n&(2) \\  w_1 = a, w_2 = 0; \\\\\n&(3) \\  w_1 = \\frac{1}{2a}, \\  w_2 = \\frac{1}{\\sqrt{2 - 4a^2}}; s.t.\\ a^2 > \\frac{1}{2},  \\\\\n&(4) \\  w_1 = \\frac{1}{2a}, w_2 = -\\frac{1}{\\sqrt{2 - 4a^2}}; s.t.\\ a^2 > \\frac{1}{2}, w_2 \\neq 0.\n\\end{aligned}$"}, {"title": "B.2 More calculation results", "content": "When optimizing IGA with $||\\nabla_wR_{e1}(w) - \\nabla_wR_{e2}(w)||^2 \\rightarrow 0$, we get\n$\\begin{aligned}\n&(E_{x_{e1},y}((x_1^1)^2w_1 + w_2x_1x_2^1 - x_1^1y) - E_{x_{e2},y}((x_1^2)^2w_1 + w_2x_1^2x_2^2 - x_1^2y))^2+ \\\\\n&(E_{x_{e1},y}((x_2^1)^2w_2 + w_1x_1x_2^1 - x_2^1y) - E_{x_{e2},y}((x_2^2)^2w_2 + w_1x_1^2x_2^2 - x_2^2y))^2 \\rightarrow 0;\n\\end{aligned} \\tag{B4}$\nwhen optimizing Fishr with $||\\text{Var}(\\nabla_wR(x^{e1}, w)) - \\text{Var}(\\nabla_wR(x^{e2}, w))||^2 \\rightarrow 0$, we have\n$\\begin{aligned}\n&E_{x_{e1},y}((x_1^1)^2w_1 + w_2x_1x_2^1 - x_1^1y - E_{x_{e1},y}((x_1^1)^2w_1 + w_2x_1x_2^1 - x_1^1y))^2 \\\\\n&- E_{x_{e2},y}((x_1^2)^2w_1 + w_2x_1^2x_2^2 - x_1^2y - E_{x_{e2},y}((x_1^2)^2w_1 + w_2x_1^2x_2^2 - x_1^2y))^2 \\rightarrow 0,\\\\\n&E_{x_{e1},y}((x_2^1)^2w_2 + w_1x_1x_2^1 - x_2^1y - E_{x_{e1},y}((x_2^1)^2w_2 + w_1x_1x_2^1 - x_2^1y))^2 \\\\\n&- E_{x_{e2},y}((x_2^2)^2w_2 + w_1x_1^2x_2^2 - x_2^2y - E_{x_{e2},y}((x_2^2)^2w_2 + w_1x_1^2x_2^2 - x_2^2y))^2 \\rightarrow 0;\n\\end{aligned} \\tag{B5}"}, {"title": "Appendix C More causality analyses", "content": "Given the theoretical setting in Section 3, we have the following corollaries.\nSetting: Consider several training environments $E_{tr} = \\{e_1, e_2, ...\\}$ and $x^e$ to be\nthe observed input of $e \\in E_{tr}$. We adopt an anti-causal framework [10] with data\ngeneration process as follows:\n$\\begin{aligned}\ny &= \\gamma^T x_{\\text{inv}} + \\eta_y, \\\\\nx_{\\text{inv}} &= x_{\\text{inv}} + \\eta_{\\text{inv}}, x^e = x_s + \\eta_s, \\\\\nx^e &= S\\begin{pmatrix}x_{\\text{inv}}\\\\\nx_s^e\\end{pmatrix},\n\\end{aligned}$\nwhere $y \\in \\mathbb{R}^{d_{\\text{inv}}}$ and $y \\neq 0$, the hidden invariant feature $x_{\\text{inv}}$ and the observed\ninvariant feature $x_{\\text{inv}}^e$ take values in $\\mathbb{R}^{d_{\\text{inv}}}$, the hidden spurious feature $x_s^e$ and the\nobserved spurious feature $x_s^e$ take values in $\\mathbb{R}^{d_s}$, and $S : \\mathbb{R}^{(d_{\\text{inv}}+d_s)} \\rightarrow \\mathbb{R}^d$ is an inherent\nmapping to mix features. The hidden spurious feature $x_s^e$ is generated by y with any\nnon-invariant relationship, $\\eta_{\\text{inv}}$ and $\\eta_s^e$ are independent Gaussian with bounded mean\nand variance changed by environments, $\\eta_y$ is an independent and invariant zero-mean\nGaussian with bounded variance. As the directed acyclic graph (DAG) in Figure 4(b)\nshows, the hidden invariant feature $x_{\\text{inv}}$ generates the true label y and y generates\nthe hidden spurious feature $x_s^e$. In consideration of environmental noise, we can only\nobserve the input $x^e$ which is a mixture of $x_{\\text{inv}}^e$ and $x_s^e$ after mapping. (Note that the\nobserved feature is generated by applying environmental noise to the hidden feature.)\nWe follow the assumption from IRM [10], i.e., assume that there exists a mapping\n$S : \\mathbb{R}^{d} \\rightarrow \\mathbb{R}^{d_{\\text{inv}}}$ such that $S(S(x)) = x_1$ for all $x_1 \\in \\mathbb{R}^{d_{\\text{inv}}}, x_2 \\in \\mathbb{R}^{d_s}$. and aim to\nlearn a classifier to predict y based on $x^e$, i.e., $f(x^e; w) = h(g(x^e; \\Phi); v)$.\nCorollary C.1. If $I$ elicits the desired invariant predictor $f(\\cdot; w) = y^TS(\\cdot)$, there\nexist noisy environments $\\{e_1, e_2\\}$ such that\n$\\nabla_wR^{e_1}(w) \\neq \\nabla_wR^{e_2}(w)$."}, {"title": "Proof C.1. If $I$ elicits the desired invariant predictor $f(\u00b7;w) = y\u00af$\\tilde$\\S(\u00b7) in noisy\nenvironments ${e1,e2}, given square loss and the fixed \u201cdummy\u201d classifier v = 1, we", "content": "dRe(w)\nExey [(f(x\u00ae; w) \u2212 y)2]\n= Eexey [(v/v=1(y\\n(Xinv + y + Ninv)) \u2212 yTXinv \u2212 Ny)2]\n= Eexey ((v/v=1YTNinvYXinv + (yTNinv)2 \u2212 yTXinvny \u2212 Ninvny),\nwhere e \u2208 {e1, 2}.\nObviously, when y \u2260 0, there exists ning \u2260 ning such thatR\u21161 (w) \u2260 DR\u00b02 (w).\n(IGA) may also be failed\nto find the optimal invariant predictor in noisy environments. Given different inherent\nlosses, it seems unreasonable to enforce all gradients to be equal across environments."}, {"title": "Appendix D Proofs", "content": "Here", "S": "Rd \u2192 Rdinv such that S(S(x)) =\nX1 for all x1 \u2208 Rdinv"}, {"title": "Invariant Correlation of Representation with Label", "authors": ["Gaojie Jin", "Ronghui Mu", "Xinping Yi", "Xiaowei Huang", "Lijun Zhang"], "abstract": "The Invariant Risk Minimization (IRM) approach aims to address the challenge\nof domain generalization by training a feature representation that remains invari-\nant across multiple environments. However, in noisy environments, IRM-related\ntechniques such as IRMv1 and VREx may be unable to achieve the optimal\nIRM solution, primarily due to erroneous optimization directions. To address this\nissue, we introduce ICorr (an abbreviation for Invariant Correlation), a novel\napproach designed to surmount the above challenge in noisy settings. Addition-\nally, we dig into a case study to analyze why previous methods may lose ground\nwhile ICorr can succeed. Through a theoretical lens, particularly from a causal-\nity perspective, we illustrate that the invariant correlation of representation with\nlabel is a necessary condition for the optimal invariant predictor in noisy envi-\nronments, whereas the optimization motivations for other methods may not be.\nFurthermore, we empirically demonstrate the effectiveness of ICorr by comparing\nit with other domain generalization methods on various noisy datasets.", "sections": [{"title": "1 Introduction", "content": "Over the past decade, deep neural networks (DNNs) have made remarkable progress\nin a wide range of applications, such as computer vision [1-3] and natural language\nprocessing [4, 5]. Typically, most deep learning models are trained using the Empiri-\ncal Risk Minimization (ERM) [6] approach, which assumes that training and testing\nsamples are independently drawn from an identical distribution (I.I.D. assumption).\nNevertheless, recent studies have reported increasing instances of DNN failures [7-9]\nwhen this I.I.D. assumption is violated due to distributional shifts in practice.\nInvariant Risk Minimization [10] is a novel learning approach that addresses the\nchallenge of domain generalization (also known as out of distribution problem) in\nthe face of distributional shifts. The fundamental concept behind IRM is to train a\nfeature representation that remains invariant across multiple environments [11], such\nthat a single classifier can perform well in all of them. Although obtaining the opti-\nmal invariant feature representation is challenging, previous works employ alternative\nmethods [12-14] to approximate it. The success of IRM approach in existing train-\ning environments can ensure its ability to generalize well in new environments with\nunseen distributional shifts, which is evidenced by positive empirical results [15, 16].\nHowever, in the real world, different environments (or domains) may exhibit vary-\ning levels of inherent (independent) noises, leading to various inherent losses. Even an\noptimal IRM model cannot mitigate these inherent losses, resulting in varying opti-\nmal losses across different environments. As shown in Figure 1, inherent noise (such\nas snow or water) can impact the invariant feature (dog), such as covering the face or\nblurring the body, resulting in different inherent losses. Existing IRM-related meth-\nods, such as IRMv1, VREx [17] and Rame et al. [15], focus on optimizing the model\nin different clean environments but may fail in these noisy situations.\nWe conduct an analysis in this study to identify the reasons why existing IRM-\nrelated methods may be ineffective in noisy environments. Upon examining the case\nstudy presented in Section 2.3, it has come to our attention that the optimization"}, {"title": "2 Study IRM in noisy environments", "content": ""}, {"title": "2.1 Preliminaries", "content": "Given that X and Y are the input and output spaces respectively, let $E :=\\{e_1, e_2, ..., e_m\\}$ be a collection of m environments in the sample space $X \\times Y$ with dif-\nferent joint distributions $P_e(x, y)$, where $e \\in E$. Consider $E_{tr} \\subset E$ to be the training\nenvironments and $S_e :=\\{(x^e, y^e)\\}$, to be the training dataset drawn from distribu-\ntion $P_e(x, y)$ ($e \\in E_{tr}$) with $n_e$ being dataset size. Given the above training datasets\n$S_e$ ($e \\in E_{tr}$), the task is to learn an optimal model $f (\\cdot; w) : X \\rightarrow Y$, such that $f(x_e; w)$\nperforms well in predicting y when given $x_e$ not only for $e \\in E_{tr}$ but also for $e \\in E \\setminus E_{tr}$,\nwhere w is the parameters of f.\nThe ERM algorithm [6] tries to solve the above problem via directly minimizing\nthe loss throughout training environments:\n$\\min_w \\sum_{e\\in E_{tr}} \\mathbb{R}_e(w),  \\tag{ERM}$\nwhere $ \\mathbb{R}_e(w),  R(x_e, w)$ are the expected loss of $f(\\cdot; w)$ in the environment e, the loss\nof $f(x_e; w)$ for the data $x_e$, respectively.\nIRM [10] firstly supposes that the predictor $f(\\cdot; w)$ can be made up of $g(\\cdot; \\Phi)$ and\n$h(\\cdot; v)$, i.e., $f (\\cdot; w) = h(g(\\cdot; \\Phi); v)$, where $w = \\{v, \\Phi\\}$ are the model parameters. Here,"}, {"title": "2.2 Invariant correlation of representation with label", "content": "We now formally describe our method (ICorr) to extract invariant features in noisy\nenvironments. ICorr performs robust learning via stabilizing the correlation between\nrepresentation and true label across environments:\n$\\min_{w} \\lambda \\text{Var}(p_{f, y}(w)) + \\sum_{e \\in E_{tr}} \\mathbb{R}_e(w), \\tag{ICorr}$\nwhere $p_{f, y}(w) = E_{x_e, y}(f(x_e;w)y)$ is the correlation between $f(x_e; w)$ and y in the\nenvironment e, $f(x_e;w) = f(x_e;w) - E_{x_e} (f(x_e; w))$, and $\\text{Var}(p_{f,y}(w))$ represents\nthe variance of the correlation in $E_{tr}$. Here $\\lambda\\in [0, +\\infty)$ controls the balance between\nreducing average loss and enhancing stability of correlation, with $\\lambda = 0$ recovering\nERM, and $\\lambda \\rightarrow +\\infty$ leading ICorr to focus entirely on making the correlation equal.\nIn the following, we demonstrate the power of ICorr in noisy environments through\nthe case study (Section 2.3) and the theoretical analysis of causality (Section 3),\nrespectively."}, {"title": "2.3 Why is ICorr necessary (a case study in two-bit environments)", "content": "Arjovsky et al. [10] present the Colored-MNIST task, a synthetic challenge derived\nfrom MNIST, to demonstrate the efficacy of the IRM technique and IRMv1 in partic-\nular. Although MNIST pictures are grayscale, Colored-MNIST images are colored red\nor green in a manner that strongly (but spuriously) correlates with the class label. In\nthis case, ERM successfully learns to exploit the color during training, but it fails at\ntest time when the correlation with the color is inverted.\nKamath et al. [18] study an abstract version of Colored-MNIST based on two bits\nof input, where y is the label to be predicted, $x_1$ is correlated with the label of the\nhand-written digit (0 \u2013 4 or 5 \u2013 9), and $x_2^e$ corresponds to the color (red or green)."}, {"title": "3 Theoretical analysis from causal perspective", "content": "In this section, we present our theoretical understanding of ICorr from the perspective\nof causality. Following the theoretical setting from Arjovsky et al. [10] and Peters\net al. [11], we formally prove that (1) $\\text{Var}(p_{f,y}(w)) = 0$ is a necessary condition\nfor the optimal invariant predictor in noisy environments; (2) $||\\nabla_{v|_{v=1}}R^e(w)|| = 0$,\n$\\text{Var}(R^e(w)) = 0$ and some other minimal penalty terms may not be necessary for the\noptimal invariant predictor in noisy environments.\nSetting: Consider several training environments $E_{tr} = \\{e_1, e_2, ...\\}$ and $x^e$ to be the\nobserved input of $e \\in E_{tr}$. We adopt an anti-causal framework [10] with data generation\nprocess as follows:\n$\\begin{aligned}\ny &= \\gamma^T x_{\\text{inv}} + \\eta_y, \\\\\nx_{\\text{inv}} &= x_{\\text{inv}} + \\eta_{\\text{inv}}, x^e = x_s + \\eta_s, \\\\\nx^e &= S\\begin{pmatrix}x_{\\text{inv}}\\\\\nx_s^e\\end{pmatrix}\n\\end{aligned}$\nwhere $y \\in \\mathbb{R}^{d_{\\text{inv}}}$ and $y \\neq 0$, the hidden invariant feature $x_{\\text{inv}}$ and the observed\ninvariant feature $x_{\\text{inv}}^e$ take values in $\\mathbb{R}^{d_{\\text{inv}}}$, the hidden spurious feature $x_s^e$ and the\nobserved spurious feature $x_s^e$ take values in $\\mathbb{R}^{d_s}$, and $S : \\mathbb{R}^{(d_{\\text{inv}}+d_s)} \\rightarrow \\mathbb{R}^d$ is an inherent\nmapping to mix features. The hidden spurious feature $x_s^e$ is generated by y with any\nnon-invariant relationship, $\\eta_{\\text{inv}}$ and $\\eta_s^e$ are independent Gaussian with bounded mean\nand variance changed by environments, $\\eta_y$ is an independent and invariant zero-mean\nGaussian with bounded variance. As the directed acyclic graph (DAG) in Figure 4(b)\nshows, the hidden invariant feature $x_{\\text{inv}}$ generates the true label y and y generates\nthe hidden spurious feature $x_s^e$. In consideration of environmental noise, we can only\nobserve the input $x^e$ which is a mixture of $x_{\\text{inv}}^e$ and $x_s^e$ after mapping. (Note that the\nobserved feature is generated by applying environmental noise to the hidden feature.)\nWe aim to learn a classifier to predict y based on $x^e$, i.e., $f(x^e; w) = h(g(x^e; \\Phi); v)$.\nDrawing upon the foundational assumption from IRM [10], i.e., assume that there\nexists a mapping $S : \\mathbb{R}^{d} \\rightarrow \\mathbb{R}^{d_{\\text{inv}}}$ such that $S(S(x)) = x_1$ for all $x_1 \\in \\mathbb{R}^{d_{\\text{inv}}}$ and $x_2 \\in\n\\mathbb{R}^{d_s}$, the following theorem mainly states that, in noisy environments, if there exists a\nrepresentation that elicits the optimal invariant predictor $f(\\cdot; w)$ across all possible\nenvironments $E$, then the correlation between $f(x^e; w)$ and y remains invariant for all\n$e \\in E$.\nTheorem 3.1. Assume that there exists a mapping $S : \\mathbb{R}^{d} \\rightarrow \\mathbb{R}^{d_{\\text{inv}}}$ such that\n$S(S(x)) = x_1$ for all $x_1 \\in \\mathbb{R}^{d_{\\text{inv}}}, x_2 \\in \\mathbb{R}^{d_s}$. Then, if $I$ elicits the desired (optimal)\ninvariant predictor $f(\\cdot; w) = y^TS(\\cdot)$, we have\n$p_{f,y}(w) = E_{x^e,y} [y^Tf(x^e;w)] - E[y^TS(x^e)]E[y] = Var(y^Tx_{\\text{inv}})$\nholds for all $e \\in E$. Thus we get $\\text{Var}(p_{f,y}(w)) = 0$.\nProof. See Appendix D.\nTheorem 3.1 indicates that in noisy environments, minimizing the regularization\nterm of ICorr, i.e., $\\text{Var}(p_{f,y}(w))$, is a necessary condition to find the invariant features.\nThe intuition behind Theorem 3.1 is that, the correlation between the representation\nand the true label can effectively prevent interference in noisy environments, whereas\nIRMv1 and VREx may get stuck."}, {"title": "4 Experiments", "content": "In this section, we implement extensive experiments with ColoredMNIST [10], Cir-\ncle dataset [24], noisy DomainBed [25] framework, noisy Waterbirds [26-28] and\nCelebA [29] datasets. The first part includes comprehensive experiments on ColoredM-\nNIST using multi-layer-perceptrons (MLP) with varying environmental noises. In the\nsecond part, we conduct further experiments to verify the effectiveness of ICorr in\nmore noisy environments with more architectures."}, {"title": "4.1 MLP with ColoredMNIST", "content": "Training setting: This proof-of-concept experiment of ColoredMNIST follows the\nsettings from Arjovsky et al. [10], Krueger et al. [17]. The MLP consists of two hidden\nlayers with 256 and 256 units respectively. Each of these hidden layers is followed by a"}, {"title": "5 Related work", "content": "The domain generalization problem is initially explicitly described by Blanchard et al.\n[33] and then defined by Muandet et al. [34], which takes into account the potential\nof the target data being unavailable during model training. A large body of literature\nseeks to address the domain generalization challenge, typically through additional\nregularizations of ERM [6]. The regularizations from Sagawa et al. [28], Motiian et al.\n[35], Namkoong and Duchi [36] enhance model robustness against minor distributional\nperturbations in the training distributions, some works [37-39] further improve this\nrobustness with extra assumptions, while some other regularizations [40-45] promote\ndomain invariance of learned features."}, {"title": "6 Limitation", "content": "Our theoretical results are based on the assumption that there exists $S\\in \\mathbb{R}^{d_{\\text{inv}}\\times d}$\nsuch that $SS(x) = x_1$, for all $x_1 \\in \\mathbb{R}^{d_{\\text{inv}}}, 1,x_2 \\in \\mathbb{R}^{d_s}$, which has also been utilized in\nthe pioneering work of IRM by Arjovsky et al. [10]. Nonetheless, this $S$ may not exist\nin DNNs when facing complicated S. Although this may pose new challenges, we aim\nto advance our research by studying more possible cases of S and S in the future."}, {"title": "7 Conclusion", "content": "In this work, we introduce an IRM-related method named ICorr, which leverages the\ncorrelation between representation and label to overcome the challenge of training\nan invariant predictor in noisy environments. A detailed case study involving two-\nbit environments is conducted to elucidate why conventional methods might falter,\nwhereas ICorr maintains its efficacy in such noisy settings. Through rigorous theo-\nretical analyses of causality, we demonstrate the critical importance of maintaining\ninvariant correlation across noisy environments to achieve the optimal IRM solution.\nIn addition to our theoretical insights, we conduct extensive experiments which show\nthe superior performance of ICorr compared to other methods in such noisy cases.\nData availability statement: The research conducted in this work solely utilizes\npublicly available datasets, the code is available in the uploaded file."}, {"title": "Appendix A More case study results", "content": ""}, {"title": "Appendix B Calculation details", "content": ""}, {"title": "B.1 Calculation for IRMv1, VREx and ICorr", "content": "Following Kamath et al. [18] and L\u00e9on Bottou, we provide the calculation details of\nIRMv1, VREx and ICorr solutions as follows.\nSuppose $E_{tr}$ consists of two environments $e_1 = (\\alpha, \\beta_{e1}, \\eta_{e1})$ and $e_2 = (\\alpha, \\beta_{e2}, \\eta_{e2})$.\nFrom the definition of IRMv1, VREx, ICorr, for any $f(x) = 1\\cdot g(x^e; \\Phi) = w_1x_1+w_2x$\nwith square loss, we have that:\nwhen optimizing IRMv1 till $\\nabla_{v|_{v=1}}R^e(w) = 0$, we get\n$\\begin{aligned}\n&E_{x_{e1},y} (w_1x_1^1 + w_2x_2^1 - y) (w_1x_1^1 + w_2x_2^1) = 0, \\\\\n&E_{x_{e2},y} (w_1x_1^2 + w_2x_2^2 - y) (w_1x_1^2 + w_2x_2^2) = 0;\n\\end{aligned} \\tag{B1}$\nwhen optimizing VREx till $\\text{Var}(R^e(w)) = 0$, we get\n$E_{x_{1},y} (w_1x_1^1 + w_2x_2^1 - y)^2 = E_{x_{e2},y} (w_1x_1^2 + w_2x_2^2 - y)^2 ; \\tag{B2}$\nwhen optimizing ICorr with $\\text{Var}[p_{f,y}(w)] = 0$, we get\n$E_{x_{e1},y}(w_1x_1^1y + w_2x_2^1y) = E_{x_{e2},y} (w_1x_1^2y + w_2x_2^2y). \\tag{B3}$\nCase 1: For both $\\eta_{e1} = 0$ and $\\eta_{e2} = 0$, we have (i) $E[(x_1^e)^2] = E[(x)^2] = 1$, (ii)\n$E(x_1^ey) = \\alpha$, $E(x_2^ey) = b_i$, (iii) $E(x_1^ex_2^e) = \\alpha b_i$, where $a := 1 - 2\\alpha$ and $b_i := 1 - 2\\beta_{ei}$\nfor $i \\in \\{1,2\\}$.\nThen, according to (B1), the solutions for IRMv1 $(\\lambda = +\\infty)$ are\n$\\begin{aligned}\n&(1) \\  w_1 = 0, w_2 = 0; \\\\\n&(2) \\  w_1 = a, w_2 = 0; \\\\\n&(3) \\  w_1 = \\frac{1}{2a}, \\  w_2 = \\frac{1}{\\sqrt{2 - 4a^2}}; s.t.\\ a^2 > \\frac{1}{2},  \\\\\n&(4) \\  w_1 = \\frac{1}{2a}, w_2 = -\\frac{1}{\\sqrt{2 - 4a^2}}; s.t.\\ a^2 > \\frac{1}{2}, w_2 \\neq 0.\n\\end{aligned}$"}, {"title": "B.2 More calculation results", "content": "When optimizing IGA with $||\\nabla_wR_{e1}(w) - \\nabla_wR_{e2}(w)||^2 \\rightarrow 0$, we get\n$\\begin{aligned}\n&(E_{x_{e1},y}((x_1^1)^2w_1 + w_2x_1x_2^1 - x_1^1y) - E_{x_{e2},y}((x_1^2)^2w_1 + w_2x_1^2x_2^2 - x_1^2y))^2+ \\\\\n&(E_{x_{e1},y}((x_2^1)^2w_2 + w_1x_1x_2^1 - x_2^1y) - E_{x_{e2},y}((x_2^2)^2w_2 + w_1x_1^2x_2^2 - x_2^2y))^2 \\rightarrow 0;\n\\end{aligned} \\tag{B4}$\nwhen optimizing Fishr with $||\\text{Var}(\\nabla_wR(x^{e1}, w)) - \\text{Var}(\\nabla_wR(x^{e2}, w))||^2 \\rightarrow 0$, we have\n$\\begin{aligned}\n&E_{x_{e1},y}((x_1^1)^2w_1 + w_2x_1x_2^1 - x_1^1y - E_{x_{e1},y}((x_1^1)^2w_1 + w_2x_1x_2^1 - x_1^1y))^2 \\\\\n&- E_{x_{e2},y}((x_1^2)^2w_1 + w_2x_1^2x_2^2 - x_1^2y - E_{x_{e2},y}((x_1^2)^2w_1 + w_2x_1^2x_2^2 - x_1^2y))^2 \\rightarrow 0,\\\\\n&E_{x_{e1},y}((x_2^1)^2w_2 + w_1x_1x_2^1 - x_2^1y - E_{x_{e1},y}((x_2^1)^2w_2 + w_1x_1x_2^1 - x_2^1y))^2 \\\\\n&- E_{x_{e2},y}((x_2^2)^2w_2 + w_1x_1^2x_2^2 - x_2^2y - E_{x_{e2},y}((x_2^2)^2w_2 + w_1x_1^2x_2^2 - x_2^2y))^2 \\rightarrow 0;\n\\end{aligned} \\tag{B5}"}, {"title": "Appendix C More causality analyses", "content": "Given the theoretical setting in Section 3, we have the following corollaries.\nSetting: Consider several training environments $E_{tr} = \\{e_1, e_2, ...\\}$ and $x^e$ to be\nthe observed input of $e \\in E_{tr}$. We adopt an anti-causal framework [10] with data\ngeneration process as follows:\n$\\begin{aligned}\ny &= \\gamma^T x_{\\text{inv}} + \\eta_y, \\\\\nx_{\\text{inv}} &= x_{\\text{inv}} + \\eta_{\\text{inv}}, x^e = x_s + \\eta_s, \\\\\nx^e &= S\\begin{pmatrix}x_{\\text{inv}}\\\\\nx_s^e\\end{pmatrix},\n\\end{aligned}$\nwhere $y \\in \\mathbb{R}^{d_{\\text{inv}}}$ and $y \\neq 0$, the hidden invariant feature $x_{\\text{inv}}$ and the observed\ninvariant feature $x_{\\text{inv}}^e$ take values in $\\mathbb{R}^{d_{\\text{inv}}}$, the hidden spurious feature $x_s^e$ and the\nobserved spurious feature $x_s^e$ take values in $\\mathbb{R}^{d_s}$, and $S : \\mathbb{R}^{(d_{\\text{inv}}+d_s)} \\rightarrow \\mathbb{R}^d$ is an inherent\nmapping to mix features. The hidden spurious feature $x_s^e$ is generated by y with any\nnon-invariant relationship, $\\eta_{\\text{inv}}$ and $\\eta_s^e$ are independent Gaussian with bounded mean\nand variance changed by environments, $\\eta_y$ is an independent and invariant zero-mean\nGaussian with bounded variance. As the directed acyclic graph (DAG) in Figure 4(b)\nshows, the hidden invariant feature $x_{\\text{inv}}$ generates the true label y and y generates\nthe hidden spurious feature $x_s^e$. In consideration of environmental noise, we can only\nobserve the input $x^e$ which is a mixture of $x_{\\text{inv}}^e$ and $x_s^e$ after mapping. (Note that the\nobserved feature is generated by applying environmental noise to the hidden feature.)\nWe follow the assumption from IRM [10], i.e., assume that there exists a mapping\n$S : \\mathbb{R}^{d} \\rightarrow \\mathbb{R}^{d_{\\text{inv}}}$ such that $S(S(x)) = x_1$ for all $x_1 \\in \\mathbb{R}^{d_{\\text{inv}}}, x_2 \\in \\mathbb{R}^{d_s}$. and aim to\nlearn a classifier to predict y based on $x^e$, i.e., $f(x^e; w) = h(g(x^e; \\Phi); v)$.\nCorollary C.1. If $I$ elicits the desired invariant predictor $f(\\cdot; w) = y^TS(\\cdot)$, there\nexist noisy environments $\\{e_1, e_2\\}$ such that\n$\\nabla_wR^{e_1}(w) \\neq \\nabla_wR^{e_2}(w)$."}, {"title": "Proof C.1. If $I$ elicits the desired invariant predictor $f(\u00b7;w) = y\u00af$\\tilde$\\S(\u00b7) in noisy\nenvironments ${e1,e2}, given square loss and the fixed \u201cdummy\u201d classifier v = 1, we", "content": "dRe(w)\nExey [(f(x\u00ae; w) \u2212 y)2]\n= Eexey [(v/v=1(y\\n(Xinv + y + Ninv)) \u2212 yTXinv \u2212 Ny)2]\n= Eexey ((v/v=1YTNinvYXinv + (yTNinv)2 \u2212 yTXinvny \u2212 Ninvny),\nwhere e \u2208 {e1, 2}.\nObviously, when y \u2260 0, there exists ning \u2260 ning such thatR\u21161 (w) \u2260 DR\u00b02 (w).\n(IGA) may also be failed\nto find the optimal invariant predictor in noisy environments. Given different inherent\nlosses, it seems unreasonable to enforce all gradients to be equal across environments."}]}]}