{"title": "NeRF-NQA: No-Reference Quality Assessment for Scenes\nGenerated by NeRF and Neural View Synthesis Methods", "authors": ["Qiang Qu", "Hanxue Liang", "Xiaoming Chen", "Yuk Ying Chung", "Yiran Shen"], "abstract": "Neural View Synthesis (NVS) has demonstrated efficacy in generating high-fidelity dense viewpoint videos using a image\nset with sparse views. However, existing quality assessment methods like PSNR, SSIM, and LPIPS are not tailored for the scenes\nwith dense viewpoints synthesized by NVS and NeRF variants, thus, they often fall short in capturing the perceptual quality, including\nspatial and angular aspects of NVS-synthesized scenes. Furthermore, the lack of dense ground truth views makes the full reference\nquality assessment on NVS-synthesized scenes challenging. For instance, datasets such as LLFF provide only sparse images,\ninsufficient for complete full-reference assessments. To address the issues above, we propose NeRF-NQA, the first no-reference\nquality assessment method for densely-observed scenes synthesized from the NVS and NeRF variants. NeRF-NQA employs a joint\nquality assessment strategy, integrating both viewwise and pointwise approaches, to evaluate the quality of NVS-generated scenes.\nThe viewwise approach assesses the spatial quality of each individual synthesized view and the overall inter-views consistency, while\nthe pointwise approach focuses on the angular qualities of scene surface points and their compound inter-point quality. Extensive\nevaluations are conducted to compare NeRF-NQA with 23 mainstream visual quality assessment methods (from fields of image, video,\nand light-field assessment). The results demonstrate NeRF-NQA outperforms the existing assessment methods significantly and it\nshows substantial superiority on assessing NVS-synthesized scenes without references. An implementation of this paper are available\nat https://github.com/VincentQQu/NeRF-NQA.", "sections": [{"title": "1 INTRODUCTION", "content": "The synthesis of photorealistic free views plays a pivotal role in enhanc-\ning user experiences in Virtual Reality (VR) and Augmented Reality\n(AR) [1,47,59]. Such realistic rendering immerses users deeply into the\nVR or AR environment, making it easier for them to engage in the vir-\ntual content [11,45]. In AR, the seamless integration of virtual objects\nwith real-world scenes is vital, and photorealistic rendering ensures that\nthese virtual elements appear natural and believable. In VR, efficient\nview synthesis techniques can generate these \"realistic\" views without\nextensive data storage for every perspective, optimizing application\nperformance [36, 53]. The adaptability of these views to real-world\nlighting conditions ensures that virtual objects reflect, refract, and cast\nshadows realistically, enhancing the immersive experience.\nHowever, the synthesis of photorealistic free views from limited\nRGB images collected from sparse viewpoints remains a pivotal chal-\nlenge in the field of image-based rendering [7, 12,21]. Recently, Neural\nView Synthesis (NVS) via implicit representations has emerged as a\npromising research field, with techniques such as Neural Radiance\nFields (NeRF) [27] and its variants [3, 10, 48, 49, 60] gaining consider-\nable attention for their exceptional fidelity and robustness. However, the\nquality assessment of NVS-generated scenes presents a complex task\nas it necessitates a comprehensive evaluation encompassing various\ndimensions, such as spatial fidelity and smoothness across consecutive\nviews. This complexity is further amplified in immersive VR/AR en-\nvironments, where users have the liberty to perceive NVS-generated\nscenes from unrestricted viewpoints [47, 59].\nCurrent quality assessment protocols for NVS-generated scenes are\ntypically based on full-reference image quality assessment methods,\nsuch as PSNR, SSIM [56], and LPIPS [64], on a subset of hold-out\nviews. Nevertheless, these methods are primarily tailored for images,\nthus may not adequately capture the comprehensive and immersive\nquality of NVS-generated scenes as perceived by human observers."}, {"title": "2 RELATED WORK", "content": "Quality assessment methods can be broadly classified into full-reference\nand no-reference, contingent upon the dependence of reference me-\ndia [39]. Full-reference methods necessitate complete access to the ref-\nerence media during quality score prediction. In contrast, no-reference\nmethods determine quality without referencing the original media. The\nno-reference methods, while more intricate in design, are better suited\nfor practical scenarios [40]. This is particularly relevant for NVS\nquality assessment, given that some datasets, like LLFF, offer sparse\nimages, rendering them inadequate for full-reference evaluations [26].\nTherefore, our research emphasizes no-reference quality assessment.\nImage Quality Assessment. The domain of image quality assessment\nis extensively studied. A plethora of full-reference methods for 2D\nimages, such as PSNR, SSIM [56], MS-SSIM [58], IW-SSIM [57],\nVIF [44], FSIM [63], GMSD [61], VSI [62], DSS [2], HaarPSI [41],\nMDSI [33], LPIPS [64], PieAPP [37], and DISTS [9], have been\ndelineated in literature. PSNR is a prevalent objective quality assess-\nment method that quantifies the quality of reconstructed images by\ncomparing the maximum possible power of a signal to the power of\ncorrupting noise, with a higher PSNR indicating a closer resemblance\nto the original image. In contrast, SSIM evaluates the perceptual quality\nof images by considering changes in structural information, luminance,\nand texture, providing a more comprehensive understanding of per-\nceived image quality [56]. VIF gauges image quality by considering\nthe mutual information shared between the reference and the distorted\nimage, offering a nuanced assessment by accounting for characteristics\nof the human visual system [62]. Lastly, the LPIPS employs deep\nlearning techniques to measure perceptual differences between images,\ncapturing intricate visual discrepancies that traditional methods might\noverlook [64]. No-reference image quality assessment methods include\nthe likes of BRISQUE [29], NIQE [31], and CLIP-IQA [52]. As one\nof the most popular, BRISQUE leverages the scene statistics of locally\nnormalized luminance coefficients to measure potential reductions in\n\"naturalness\" due to distortions [29].\nVideo Quality Assessment. Beyond standard images, quality assess-\nment methodologies exist for alternative visual media formats such\nas videos. Leading video quality methods encompass STRRED [46],\nVIIDEO [30], VMAF [22], and FovVideoVDP [24]. STRRED focuses\non the structural retention in videos, offering insights into the preser-\nvation of inherent video patterns post-processing [46]. The VIIDEO,\non the other hand, is a no-reference video quality assessment method\nthat relies solely on the video being evaluated, utilizing intrinsic sta-\ntistical regularities observed in natural videos [30]. VMAF, or Video\nMulti-Method Assessment Fusion, combines multiple algorithms to\npredict video quality, aligning closely with human perception by con-\nsidering factors like texture, luminance, and motion [22]. Meanwhile,\nFov VideoVDP is a sophisticated method tailored for video quality as-\nsessment, taking into account the viewer's field of view to provide a\nmore contextual evaluation [24]. The video quality assessment tech-\nniques are adaptable to NVS, given that the synthesized view sequence\ncan be analogously interpreted as a video.\nLight-Field Quality Assessment. Besides videos, light-field images\nare another media format that contains unique angular dimension for\nvisual content. For light-field quality assessment, cutting-edge methods\nsuch as ALAS-DADS [39] and LFACon [40] are the-state-of-the-arts"}, {"title": "3 METHODOLOGY", "content": "As depicted in Figure 3, the architecture of NeRF-NQA is principally\ndivided into three major components: the Viewwise Quality Assessment\nModule, the Pointwise Quality Assessment Module, and the Quality\nScore Estimation Module.\nThe Viewwise Quality Assessment Module is designed to evaluate\nthe spatial quality of scenes generated from NVS. This module ingests\nthe synthesized views and undergoes two primary stages: Quality\nFeature Generation per View and Inter-View Feature Extraction. The\noutput consists of viewwise quality features that encapsulate the spatial\ncharacteristics of the scene (detailed in Section 3.2).\nThe Pointwise Quality Assessment Module aims to capture angular\nquality features that are challenging for the Viewwise Module to as-\nsess. Both NVS-generated views and their corresponding camera poses\nare taken as input and processed through a sequence of operations,\nincluding Pointwise Quality Feature Extraction and Inter-Point Feature\nExtraction, to yield pointwise quality features (detailed in Section 3.3).\nFinally, the Quality Score Estimation Module employs a Multi-Layer\nPerceptron (MLP) to fuse the viewwise and pointwise features gen-\nerated by the preceding modules, resulting in the final quality scores\nto offer a comprehensive assessment of the NVS scene. The inten-\ntional use of the MLP fusion aims to highlight the effectiveness of our\nproposed features. This fusion, common in representation learning as\nshown in references [6, 13], allows us to demonstrate the strength and\ndiscriminative power of extracted features without the interference of\ncomplex fusion techniques."}, {"title": "3.2 Viewwise Quality Assessment", "content": "The quality of NVS-generated scene is intrinsically influenced by the\nquality of each synthesized view. After generating the quality features\nof individual views, it is imperative to holistically evaluate the final\nquality, factoring in the interrelation of these views. Given that NVS\noutcomes typically follow a camera trajectory, an intuitive approach is\nto analyze the quality features along this path.\nBased on this concept, we introduce a viewwise quality assess-\nment module, as depicted in Figure 4. The module starts with an\ninitial block, Quality Features Generation per View [28], to individu-\nally assess the synthesized views to produce quality features for each\nview. Then, the Inter-View Feature Extraction block extracts features\nalong the camera path, with the model structure inspired by Efficient-\nNetV2 [50]. Specifically, it integrates two repeated sets of two (Fused)\nMBConv layers (as per [42, 50]) combined with MaxPooling, two\nstandalone MBConv layers [42] followed by global MaxPooling and\na MLP. The MBConv employs the inverted bottleneck structure [42]\nand depthwise convolutional layers [14] to enhance memory efficiency.\nAdditionally, a squeeze-and-excitation unit [15] is integrated within\nthe MBConv to recalibrate channel-wise feature responses adaptively.\nThe fused MBConv variant replaces depthwise convolutional layers\nwith standard ones, proven to be more efficacious for larger spatial\ndimensions [50].All layers operate coherently along the camera path,\nallowing the viewwise module to integrate inter-view quality features.\nThis ensures that the quality of each view is evaluated in conjunction\nwith its neighboring views. The global MaxPooling layer ensures the\nmodule's compatibility with view sequences of varying lengths."}, {"title": "3.3 Pointwise Quality Assessment", "content": "While the viewwise module adeptly captures the spatial quality of syn-\nthesized views, it encounters challenges in encapsulate the important\nangular quality explicitly. The angular quality, often delineated as the\nexperience of observing a consistent location from varied angles [40],\ncan be contextualized in NVS scenes as viewing a singular surface\npoint from diverse viewpoints. To encapsulate the angular quality\ninherent in the NVS scenes, we introduce the pointwise quality assess-\nment module. This module is one of the key technical contribution of\nNeRF-NQA and its detailed design is shown in Figure 5. The module\ncommences by accepting NeRF synthesized views and camera poses,\nsubsequently sampling sparse surface points via COLMAP [43]. For\neach point, we compute pointwise quality features, elaborated in the\nsubsequent paragraph. These high-dimensional pointwise quality fea-\ntures undergo further refinement in a feature extraction block, which\ndistills the features per point and diminishes their dimensionalities.\nThis block comprises four 3D convolutional layers followed by a MLP.\nSubsequently, an Inter-Point Feature Extraction block is designed by\nemploying PointNet [38] to extract inter-point quality features based\non the spatial positioning of the points within the scene.\nPointwise Quality Feature Calculation. To encapsulate the angular\nquality inherent to NVS scenes, we introduce the Pointwise Normalized\nSpherical Gradient map (PNSG) as the foundational pointwise quality\nfeatures. The essence of PNSG lies in computing the gradient of pixel\nvalues observed from different viewpoints targeting at an identical\nsurface point. The intricate procedures underpinning the pointwise"}, {"title": "4 EXPERIMENTS", "content": "We evaluate and compare our proposed NeRF-NQA with existing qual-\nity assessment methods on three NVS datasets: Lab [23], LLFF [26],\nand Fieldwork [23]. Lab dataset features 6 real scenes captured in a\nlab setting with a 2D gantry, facilitating both horizontal and vertical\ncamera movements. Training views were taken on a uniform grid, and\nreference videos ranged from 300 to 500 frames [23]. LLFF dataset\ncomprises 8 real scenes captured via a handheld cellphone, each with\nsparse test views (20-30 images) [26]. Poses for these images were\ncomputed using the COLMAP structure from motion [43]. Fieldwork\ndataset contains 9 real scenes from outdoor urban areas and indoor\nmuseum spaces. These scenes are challenging due to intricate back-\ngrounds, occlusions, and varying lighting. Reference videos typically\nhave around 120 frames with diverse trajectories [23]. For each dataset,\nwe randomly designate four scenes for testing, while the remaining\nscenes are allocated for training. To mitigate overfitting, we conduct\nten rounds of random surface sampling on every scene, effectively\naugmenting both the training and testing samples tenfold."}, {"title": "4.2 Perceptual Quality Labels", "content": "The perceptual quality labels are derived from subjective experiments\nby Liang et al. [23]. They engaged 39 color-normal volunteers,\nwith each participant completing 4-5 batches of comparisons using\nASAP [25]. The results, scaled from pairwise comparisons, were artic-\nulated in Just-Objectionable-Difference (JOD) units via the Thurstone\nCase V observer model [35]. The JOD scores are offset by reference\nscores and thus predominantly negative values. A JOD score of 0 in-\ndicates undistorted quality. Higher JOD values suggest better quality\nperceived by human visual systems.\nThese experiments encompassed ten representative NVS methods,\nshowcasing a variety of models with both explicit and implicit geo-\nmetric representations, different rendering models, and optimization"}, {"title": "4.3 Training Setup", "content": "The model was trained utilizing the ADAM optimizer [19], over 200\nepochs with a batch size of 10. It is designed as a generalized model,\nwhich, post-training, is capable of operating across diverse scenes with-\nout necessitating scene-specific fine-tuning. The weights, established\nduring this initial training phase, are maintained consistently. In other\nwords, once the model is trained, it is supposed to be proficiently ap-\nplied to unseen scenes across different datasets. The computational\nexperiments were conducted on a desktop equipped with an AMD\n5950X processor, an RTX 3090 GPU, and 32GB of RAM, operating\non Windows 10. The implementation replied on the PyTorch [34]."}, {"title": "4.4 Metrics to Evaluate the Quality Assessment Methods", "content": "In the realm of quality assessment, several metrics are commonly\nemployed to quantify the performance of quality assessment meth-\nods [39,40]. Among these, the Root Mean Square Error (RMSE) [8]\nserves as a standard measure of the differences between predicted\nand ground-truth values, with lower RMSE values indicating more\naccurate predictions. The Spearman Rank Order Correlation Coeffi-\ncient (SRCC) [65] assesses the strength and direction of the monotonic\nrelationship between the predicted and ground-truth scores. Higher\nSRCC values signify a stronger correlation and, consequently, better\nperformance. Similarly, the Pearson Linear Correlation Coefficient\n(PLCC) [8] evaluates the linear correlation between the predicted and"}, {"title": "4.5 Ablation Study on the Design of NeRF-NQA Model", "content": "As elaborated in Section 3, the Pointwise Module is specifically de-\nsigned to capture angular quality features that are inherently difficult\nfor the Viewwise Module for assessment. To empirically validate the\nefficacy of the Pointwise Module, we construct two variants of NeRF-\nNQA: one incorporating the Pointwise Module and the other excluding\nit. Comparative performance metrics for these variants are presented in\nTable 1. Our experimental findings reveal that the NeRF-NQA variant\nwith the Pointwise Module consistently outperforms its counterpart\nacross nearly all evaluation criteria, with the exception of RMSE on\nthe Fieldwork dataset, where the results are closely aligned. Notably,\nin the LLFF dataset, the fully-equipped NeRF-NQA demonstrates a\n33.3% reduction on RMSE and a 0.1611 increase on SRCC. These\noutcomes substantiate the utility and effectiveness of the Pointwise\nModule, thereby justifying its inclusion in subsequent experiments."}, {"title": "4.6 Comparison with Other Quality Assessment Methods", "content": "Our benchmarking considered prevalent full-reference image quality as-\nsessment metrics (FR-IQA) such as PSNR, SSIM [56], MS-SSIM [58],\nIW-SSIM [57], VIF [44], FSIM [63], GMSD [61], VSI [62], DSS [2],\nHaarPSI [41], MDSI [33], LPIPS [64], PieAPP [37], and DISTS [9],\nalong with no-reference image quality assessment metrics (NR-IQA)\nsuch as BRISQUE [29], NIQE [31], and CLIP-IQA [52]. We also in-\ncluded video quality assessment methods (VQA) such as STRRED [46],"}, {"title": "4.7\nEvaluation on Different Scenes", "content": "To analyze the efficacy of the evaluated quality assessment methods\nacross different scenes, we present the scene-wise performance statis-\ntics in Table 3. As depicted in the table, the evaluation results of\nNeRF-NQA are consistently superior than others across a diverse array\nof NVS scenes. Specifically, NeRF-NQA attains the lowest RMSE\nvalues in 11 out of 12 scenes and the highest SRCC values in 10 out of\n12 scenes. For RMSE, NeRF-NQA exhibits substantial improvements\non scenes such as Dinosaur, Vespa, Horns, and Toys with enhance-"}, {"title": "4.8 Evaluations on Different NVS Methods", "content": "Table 4 lists the performance of the evaluated quality assessment meth-\nods in different NVS methods. As delineated in Table 4, NeRF-NQA\nconsistently outperforms other quality assessment methods across a\ndiverse range of NVS methods. Specifically, NeRF-NQA achieves the\nmost best RMSE values in 9 of the 10 evaluated NVS methods and the\nhighest SRCC values across all NVS methods. In the context of RMSE,\nNeRF-NQA manifests significant performance gains in methods such\nas GNT-C, GNT-S, IBRNet-C, and LFNR, registering improvements\nof 74.8%, 55.8%, 63.3%, and 38.1%, respectively, when compared to\nthe second-best performing metrics. Likewise, with respect to SRCC,\nNeRF-NQA exhibits pronounced advantages in methods like DVGO,\nNeRF, and Plenoxel, enhancing SRCC values by 0.377, 0.128, and\n0.165, respectively, relative to the next best-performing metrics."}, {"title": "4.9 Cross Dataset Evaluation", "content": "To substantiate the model's generalizability, cross-dataset evaluations\nwere conducted, and the results are presented in Table 6. This table\ndelineates the model's efficacy when trained on two datasets and sub-\nsequently tested on the third, exemplified by the 'Fieldwork' column,\nwhich reflects results from training on the LLFF and Lab datasets. The\nresults reveal that, for the case of dataset independence, the proposed\nmethod consistently surpasses competing approaches, affirming its su-\nperior generalization capabilities. Notably, when the Fieldwork dataset\nis the test set, NeRF-NQA achieves a significant 35.0% improvement\nin RMSE over the second best method. Regarding the LLFF dataset,\nit exhibits a 13.2% enhancement in RMSE, along with increments of\n0.290 in SRCC and 0.257 in PLCC. Similarly, for the Lab dataset,\nNeRF-NQA secures substantial advancements, bringing a 20.9% im-\nprovement in RMSE, a 0.133 increase in SRCC, and a 0.178 rise in\nPLCC, further evidencing its superior performance and generalization\nacross diverse datasets."}, {"title": "5 LIMITATION", "content": "A notable limitation of the proposed method is its reliance on the sparse\npoints generated by COLMAP [43] within the pointwise module. De-\nspite this dependency, the empirical evidence presented in Table 5\nsuggests that this reliance does not markedly diminish the model's per-\nformance, even in scenarios traditionally challenging for sparse point\ngeneration, such as scenes with complex shapes and specular surfaces.\nThe sustained performance in these conditions indicates a degree of\nresilience to the noise inherent in COLMAP-generated sparse points.\nFuture work will aim to address and potentially mitigate this depen-\ndency on COLMAP to further enhance the robustness and applicability\nof the proposed method. Due to time constraints, this research primar-\nily focused on front-facing scenarios. As a result, perceptual scores\nfor 360-degree scenes were not collected, and the methods related to\n360-degree scenes were not tested. Additionally, the research did not\nencompass some of the latest advancements in NVS methods, includ-\ning Instant-NGP [32], TensorRF [5], and 3D Gaussian Splatting [18].\nFuture work will aim to address these gaps by incorporating evaluations\non 360-degree scenes and integrating a wider array of NVS methods to\nprovide a more exhaustive analysis of the proposed approach."}, {"title": "6 CONCLUSION", "content": "In this paper, we introduce NeRF-NQA, an innovate quality assessment\nmethod to evaluate the quality of NVS-generated scenes without the\ndependency on reference views, addressing the prevalent challenges on\nscarce reference availability in NVS scenarios. NeRF-NQA adopts a\njoint quality assessment strategy, integrating both viewwise and point-\nwise assessment methodologies to facilitate a holistic evaluation of\nboth the spatial fidelity and the intricate angular quality of the synthe-\nsized views. Empirical results underscore the pronounced superiority\nof NeRF-NQA in gauging the quality of NVS-generated views, out-\nperforming extant quality assessment techniques for images, videos,\nand light fields. These findings accentuate the efficacy and robustness\nof NeRF-NQA as a pivotal instrument for discerning the perceptual\nquality of NVS-generated scenes."}, {"title": "Formal Definition of PNSG.", "content": "The PNSG is derived as an aggregation\nof NSG values. Consider a set of $n$ surface points, denoted as ${\\{P\\}}$,\nfor which we aim to compute the PNSG. For a given surface point $P_i$,\nwe can collate pixels from all synthesized views targeting at that point,\ngiven the respective camera poses. Each pixel is associated with both its\nviewpoint position in 3D space and its RGB values. Subsequently, we\ntransform the pixel positions from Cartesian to spherical coordinates,\nusing the surface point as the spatial origin. This transformation allows\nus to represent the view direction of each pixel using azimuthal and\npolar angles.\nInitially, we compute the NSG along the azimuthal axis by partition-\ning the polar axis into $b$ evenly spaced bins, represented as ${\\{B\\}}_{i=0}^{b-1}$.\nPixels are then grouped into the nearest bins. For a specific azimuthal\nbin $B_i$ containing $m_i$ pixels, we arrange the pixels by their azimuthal\nangles, denoted as $B_i = {\\{x_j\\}}_{j=0}^{m_i-1}$. We then compute the NSG for each\nadjacent pair of pixels, resulting in $b$ bins of NSG along the azimuthal\naxis, represented as $\\text{NSG}_{azi}$.\n$\\text{NSG}_{azi} = {\\{\\left{\\text{NSG}(x_j, x_{j+1})\\right\\}}_{j=0}^{m_i-1}}_{i=0}^{b-1}$    (2)\nIn a similar vein, we compute the NSG along the polar axis, denoted\nas $\\text{NSG}_{pol}$. The PNSG for the surface point $P_i$ is then represented\nas ${\\{\\text{NSG}_{azi}, \\text{NSG}_{pol}\\}}$. The cumulative PNSG for the entire scene is\ndefined as:\nPNSG = { {NSGazi, NSG pol} }.   (3)\nFrom the derivations presented, it is apparent that the PNSG captures\nthe dynamics within the angular domain, serving as a feature set for\nevaluating the angular quality inherent to NVS scenes."}, {"title": "4.1 Datasets for Evaluation", "content": "{\\displaystyle {\\text{NSG}}(x_{i,j})={\\frac {I(x_{i})-I(x_{j})}{\\langle x_{i}ox_{j}\\rangle }},}"}]}