{"title": "Cross-domain Few-shot Object Detection with Multi-modal Textual Enrichment", "authors": ["Zeyu Shangguan", "Daniel Seita", "Mohammad Rostami"], "abstract": "Advancements in cross-modal feature extraction and integration have significantly enhanced performance in few-shot learning tasks. However, current multi-modal object detection (MM-OD) methods often experience notable performance degradation when encountering substantial domain shifts. We propose that incorporating rich textual information can enable the model to establish a more robust knowledge relationship between visual instances and their corresponding language descriptions, thereby mitigating the challenges of domain shift. Specifically, we focus on the problem of Cross-Domain Multi-Modal Few-Shot Object Detection (CDMM-FSOD) and introduce a meta-learning-based framework designed to leverage rich textual semantics as an auxiliary modality to achieve effective domain adaptation. Our new architecture incorporates two key components: (i) A multi-modal feature aggregation module, which aligns visual and linguistic feature embeddings to ensure cohesive integration across modalities. (ii) A rich text semantic rectification module, which employs bidirectional text feature generation to refine multi-modal feature alignment, thereby enhancing understanding of language and its application in object detection. We evaluate the proposed method on three cross-domain object detection benchmarks and demonstrate that it significantly surpasses existing few-shot object detection approaches. The implementation of our method is publicly accessible at: https://github.com/zshanggu/HTRPN.*", "sections": [{"title": "1 Introduction", "content": "In real-world industrial settings, many existing deep learning-based object detection methods struggle to detect product defects effectively due to the challenge of data annotation [2-4]. These challenges primarily stem from two factors: the limited availability of domain-specific training data and the significant domain gap between pre-trained datasets which are typically sourced from everyday scenarios and the specialized domain of product defect detection. Even for humans, identifying subtle product defects can be challenging without prior visual experience in the specific domain. However, human experts often rely on detailed training manuals containing textual instructions and guidelines to bridge this gap. These manuals provide critical context and descriptive information, enabling experts to identify defects more accurately and consistently annotate images. Inspired by this process, we propose a novel methodology that integrates multi-modal rich textual information to enhance object detection performance in scenarios characterized by limited training data [5, 6] and out-of-domain generalization [7\u20139] challenges. By mimicking the human approach of combining visual and textual cues, our method seeks to improve the model's ability to generalize to new domains and effectively detect defects in industrial applications.\nWe explore the general task of few-shot object detection (FSOD), which focuses on training models capable of identifying objects from classes where only a limited number of labeled examples are available [10, 11]. This capability is particularly valuable in domains where rare or less common objects is frequent, such as in autonomous driving, where the ability to adapt to novel scenarios is critical. The typical FSOD workflow involves an initial pre-training phase on base classes with abundant annotated data, followed by fine-tuning on novel classes that have only a few labeled samples. In traditional FSOD settings, base and novel classes generally belong to the same feature domain, which simplifies adaptation but does not always reflect real-world challenges. Current FSOD methods primarily adopt one of two approaches: fine-tuning or meta-learning [10]. Fine-tuning, while computationally efficient and requiring fewer GPU resources, often exhibits limited generalization capabilities, especially when applied to novel domains. On the other hand, meta-learning methods achieve stronger generalization due to their class-agnostic training paradigm, enabling them to infer novel classes more accurately. However, this robustness comes at the cost of higher computational demands, as these methods typically require substantial GPU resources [12].\nA promising new direction in FSOD is multi-modal learning, which introduces additional modalities such as text to enrich visual feature representations. This approach not only improves performance but also opens the door to zero-shot learning [13-15], where models can recognize novel classes without direct training examples (see Figure 1). Multi-modal FSOD encompasses a range of techniques, from straightforward visual question-answering frameworks [16, 17] to more advanced systems that tightly integrate vision and text modalities [18, 19]. The broader field of multi-modal machine learning leverages diverse data sources, including vision, language, acoustics, and tactile inputs, to enhance performance across tasks. This paradigm has demonstrated remarkable success in areas such as visual question-answering [20, 21], robotics [22, 23], continual learning [24], and medical image analysis [25]. Multi-modal learning is also foundational to modern generative AI systems like GPT-4 [26] and Gemini [27], which can seamlessly process and produce text and images to perform a wide range of complex tasks."}, {"title": "", "content": "While multi-modal object detection (MM-OD) methods have shown considerable success in few-shot learning tasks, their performance often deteriorates in practical scenarios where there is a significant domain gap between the source and target data [28-30]. Domain gap presents a critical challenge, as the data distributions in the training (source) and testing (target) domains can differ substantially in real-world detection problems. Such cross-domain, data-scarce scenarios exacerbate the difficulty of maintaining high performance. As illustrated in Figure 2, object detection methods, including MM-OD approaches, experience significant performance degradation when trained and evaluated on datasets with distinct source and target domains. To address this issue, we focus on enhancing the robustness of few-shot object detection (FSOD) models by tackling the domain adaptation problem inherent in these scenarios. Specifically, our objective is to address the task of cross-domain multi-modal few-shot object detection (CDMM-FSOD), which aims to bridge the domain gap effectively while facilitating the transfer of information from the source domain to the target domain. By developing strategies to mitigate this domain shift, we seek to improve the generalization and adaptability of MM-OD methods, enabling them to perform reliably in diverse and challenging real-world environments.\nMore specifically, we hypothesize that leveraging rich, detailed text within a sentence can be significantly more effective than relying solely on simple semantic descriptions, particularly when dealing with target domain images that require technical language for accurate depiction. This hypothesis holds special relevance in scenarios where most models are pre-trained on datasets reflecting common, everyday contexts, leaving them ill-equipped to handle domain-specific technical descriptions. Despite this potential, a key challenge lies in determining how neural network models can effectively utilize such rich text information for detection tasks, as this remains an underexplored area in existing research. We aim to explore whether and how rich textual information can help address the cross-domain data gaps that hinder object detection performance. As illustrated in Figure 1, our proposed approach for cross-domain multi-modal few-shot object detection (CDMM-FSOD) differs fundamentally from traditional FSOD and multi-modal FSOD (MM-FSOD) methods. Our methodology incorporates rich text descriptions containing detailed technical information about the training data. In contrast, the novel data, such as the patch defect depicted in the example, represents a substantial domain shift. This scenario, where the target domain diverges significantly from the source domain, is not only theoretical but also prevalent in industrial applications. We believe our approach provides a practical and innovative solution to these challenges, addressing a critical gap in the ability of models to generalize effectively across domains in real-world settings.\nWe adopt Meta-DETR [12], a meta-learning variant of the Detection Transformer (DETR) [34], as our baseline due to its Transformer-based architecture and strong capability for adapting to multiple modalities. Unlike traditional multi-modal approaches, our framework incorporates a rich text description for each training category, which can include technical terminology specific to the domain. These rich text descriptions serve as a linguistic support set during the meta-learning process, functioning in a manner analogous to the image-based support set but providing complementary information through language. To effectively integrate vision and language information, we design a meta-learning multi-modal aggregated feature module, which fuses the visual and textual embeddings and projects them into a shared class-agnostic feature space. This meta-support feature serves as the foundation"}, {"title": "3 Proposed Methods", "content": "In the conventional FSOD paradigm, the label set is divided into two distinct groups: base classes, denoted as CB, and novel classes, represented as CN. Classes in CB have access to a substantial amount of labeled training data, whereas each class in CN is characterized by a limited number of labeled instances. Importantly, it is assumed that the sets CB and CN are disjoint, meaning there is no overlap between the base and novel class categories. The training dataset, DFSOD = {(x, y); }, comprises ND annotated samples. Each sample consists of an RGB image x and its corresponding ground-truth annotations y. The annotation y is structured as y = {(cj,bj) | j \u2208 {1, 2, ..., Nobj}}, where Nobj represents the number of objects present in the image x. For each object j, cj \u2208 CBU CN specifies its class label, and by defines its bounding box coordinates. The primary goal of FSOD is to train a model on DFSOD such that it performs well at test time, particularly in detecting objects that belong to the novel classes CN, despite the limited labeled instances available for those classes. This challenging task has been addressed in several notable works, such as those by Kang et"}, {"title": "3.2 Rich Text", "content": "We denote the text data corresponding to the base and novel categories as TB and TN, respectively. To construct the textual input l for the training data, we manually create detailed and fixed text descriptions for each category C\u2208 CBU CN. These descriptions form a set denoted as WC = {w1,w2,...,wC}, where C = |CB \u222a CN| represents the total number of categories. Each text description wi is composed of multiple tokens with a variable length M. Formally, wi = {wi, w, ..., w}, where the first token w\u2081 = (s) signifies the start of the sentence, and the last token w\u2081\u2081 = </> marks the end of the sentence. To construct the set WC, we employ two complementary strategies: (i) Aspect-based Description: For each category, we refer to relevant content on Wikipedia and create descriptions that detail aspects such as color, shape, attributes, material, and other intrinsic properties. For instance, a description for \"motorbikes\" might include its structural and functional features. (ii) Contextual Extension: Beyond the aspects described in the first strategy, we extend the sentences by incorporating contextual details to reflect common visual relationships between categories. For example, the original sentence, \u201cMotorbikes have two wheels, a motor, and a sturdy frame,\" is expanded to include contextual relationships: \u201cMotorbikes have two wheels, a motor, and a sturdy frame, usually ridden by a person.\" This enhancement introduces a typical visual interaction between motorbikes and persons, enriching the textual representation for cross-modal understanding. The complete set of text descriptions, covering all base and novel classes, is provided in the experimental section for each dataset that we have used."}, {"title": "3.3 Proposed Architecture", "content": "We adopt Meta-DETR [12] as the baseline network for our approach. Meta-DETR builds upon Deformable DETR [53], which itself is an advanced object detection model leveraging the transformer architecture [54]. DETR employs a ResNet [55] backbone to extract high-level features from input images. These features are then passed through an encoder that generates memory embeddings. Subsequently, a decoder processes these memory embeddings to produce object proposals, effectively identifying and localizing objects in the image. In our implementation, we follow Meta-DETR's methodology and utilize a \"weight-shared self-attention module\" to incorporate meta-learning for both the query and support images. This mechanism enables the network to generalize effectively across categories with limited labeled data by learning shared representations. The classic DETR architecture is composed of both an encoder and a decoder, each consisting of six multi-head attention layers. However, Meta-DETR introduces the meta-learning mechanism selectively, applying it only at the first multi-head attention layer of the encoder. This focused design enhances the network's ability to perform few-shot object detection while maintaining computational efficiency. By leveraging this architecture, our model can effectively integrate query-support relationships and detect objects with minimal data for novel categories.\nThe meta-learning module begins by processing both query and support images through a shared ResNet feature extractor, yielding their respective basic feature representations. These query and support features are then passed into a multi-head attention module, ensuring they are transformed into a shared feature space for effective comparison and integration. In this architecture, the support sample features act as the key (K) and value (V) matrices, while"}, {"title": "3.4 Multi-modal Feature Aggregation", "content": "The multi-modal feature aggregation module forms the core of our framework, designed to integrate information from multiple modalities effectively. This module is capable of fusing features across multiple support categories simultaneously, making it a key component in handling the complexities of multi-modal tasks. A detailed visualization of its architecture is provided in Figure 4. To extract the foundational features, we utilize the pre=trained CLIP model, which generates the primary image features for the support and query sets, as well as the primary semantic features for the rich text labels. For the support image features, a RoIAlign layer followed by an average pooling layer is applied to derive instance-level category prototypes. This operation is essential in object detection tasks because raw image features cannot accurately represent individual object instances, especially in images containing multiple foreground objects. Additionally, consistent with conventional object detection methods, the background is treated as an extra hidden category and is initialized as one of the category prototypes. This ensures that the model accounts for non-object regions during feature aggregation. The extracted features from both modalities are then projected into a shared feature space using a multi-head self-attention module, implemented as the first layer of the DETR encoder. This shared-weight mechanism ensures alignment between visual and textual representations, enhancing the model's capability to fuse multi-modal data. The importance of this shared-weight design is validated through an ablation study, as discussed in Section 4.3.\nNext, we combine the image and semantic features by concatenating and averaging them to form the comprehensive support feature. Specifically, the support vision feature is concatenated with the corresponding language feature from the same category, ensuring that both modalities contribute equally to the final representation. This concatenated feature serves as a unified input for subsequent processing. To further enhance the learning process, we apply a single-head self-attention module. This module serves two primary functions: (i) it maps the support-query feature relationships, enabling the model to align the features from the support and query sets based on their class-specific similarities, and (ii) it matches the class-agnostic task embedding-query relationship, aligning the task-related features with the query features in a generalizable manner. This dual functionality allows the model to learn both category-specific and class-agnostic representations, optimizing the feature matching process across different categories and tasks."}, {"title": "3.5 Rich Text Semantic Rectify", "content": "The rich text semantic rectify module is illustrated in 4. The core idea behind this module is to employ a transformer encoder architecture [56], denoted as fe, to generate category-level language sequence features for the fused support-query samples. Once these language features are generated, they are expected to align with the corresponding ground-truth language features. This alignment process helps refine the model's understanding and generation of the language representation. Furthermore, the language generation within this module is bidirectional, meaning that the model generates the sentence both in the forward and backward directions. This bidirectional generation ensures that the model develops a more robust understanding of the image-text relationships and enhances its ability to encode and integrate semantic knowledge from both modalities. By doing so, the module helps improve the"}, {"title": "4 Experimental Validation", "content": "We validate our method on three suitable benchmarks and compare our results against state-of-the-art methods to demonstrate its effectiveness."}, {"title": "4.1 Experimental Setup", "content": "We perform experiments using four existing benchmarks. We adopt these datasets such that they can be used in the context of our proposed method."}, {"title": "4.1.1 Datasets", "content": "The CD-FSOD benchmark, introduced by Xiong et al.[50], serves as an exceptional platform for evaluating our experimental framework. This benchmark incorporates diverse datasets, allowing us to assess few-shot learning models in distinct domains. Specifically, we leverage three datasets: ArTaxOr[57], DIOR [58], and UODD [59], as our target-domain datasets. The ArTaxOr dataset features seven arthropod categories, the DIOR dataset includes 20 classes derived from satellite imagery, and the UODD dataset comprises three underwater species, offering a rich diversity for cross-domain analysis and testing.\nThe CDSFSOD dataset, introduced by Fu et al.[48], is constructed by integrating three diverse datasets: Clipart1k[60], which features clipart-style object images, DeepFish[61], containing underwater fish images, and NEU-DET[62], designed for detecting surface defects in industrial materials. A balanced few-shot sampling strategy is applied in this dataset, meaning the k-shot setting includes exactly k instances for each category across all datasets. This design ensures consistent and equitable representation of classes, making the dataset suitable for cross-domain FSOD evaluations."}, {"title": "4.1.2 Detailed List of the Rich Text", "content": "We augment the datasets that we use in our experiments with rich texts so we can apply on our algorithm on the resulting datasets. Note that the four benchmarks that we use for FSOD evaluation are built using these datasets. To this end, we build two rich texts to describe classes in these datasets. We build rich text manually or by using an LLM [63]. As indicated in our experiments, either way could generate significant performance improvement on the out-of-domain benchmarks.\nAs depicted in Figure 5, the ArTaxOr Dataset [57] comprises seven distinct categories of insects. The images within this dataset were primarily captured using a macro lens, resulting in high-definition visuals and a distinct foreground-background boundary. Nevertheless, the inter-class differences within the ArTaxOr Dataset are relatively subtle, posing a challenge in accurately distinguishing between the various insect categories [50]. The detailed categories list and the corresponding rich texts are available in Table 1."}, {"title": "4.1.3 Implementation Details", "content": "Our experiments were conducted using hardware equipped with 8 NVIDIA 3090 GPUs to enable parallelized training and testing. We adopted hyperparameter configurations inspired by DETR [34], setting the initial learning rate to 0.001. The batch size was adjusted depending on the scenario: 4 for 1-shot, and 1 for both 5-shot and 10-shot cases. The DETR encoder and decoder each consist of six self-attention layers, with the first encoder layer integrated into our meta-learning multimodal aggregation module. We employed DETR-R101 as the vision-language backbone, and the image-text transformer decoder architecture follows the design by Afham et al. [56]. For transparency and reproducibility, our code is included in the supplementary materials to allow others to verify and build upon our results."}, {"title": "4.1.4 Baseline For Comparison and Metrics", "content": "In our comparative analysis, we evaluated our method against recent state-of-the-art approaches that demonstrate competitive performance in the field. To ensure a fair and consistent comparison, we selected methods widely recognized in the literature for their effectiveness. The evaluation metric used for benchmarking is the mean Average Precision (mAP), which is a standard metric for assessing object detection performance. This allowed us to quantify and highlight the improvements our approach offers relative to existing methods."}, {"title": "4.2 Comparative Results", "content": "We demonstrate that our proposed algorithm outperforms existing methods on all the four benchmarks we use in our experiments."}, {"title": "4.2.1 Results for CD-FSOD", "content": "Our experimental results on the CD-FSOD datasets, summarized in Table 10, align with the setup described by Xiong et al. [50]. Specifically, our model is pre-trained on COCO and fine-tuned on three out-of-domain datasets, with mean Average Precision (mAP) serving as the evaluation metric. Since no prior work addresses CDMM-FSOD, we implemented and evaluated representative FSOD and MMOD methods, including Meta-DETR, Next-Chat, GOAT, and ViLD, on the CD-FSOD benchmark using their official code. Results from Distill-CD-FSOD [50] and Fu et al. [48] are directly cited from their papers.\nOur approach significantly outperforms baseline methods, particularly on the ArTaxOr dataset, achieving mAP values of 15.1, 48.7, and 61.4 for the 1-shot, 5-shot, and 10-shot scenarios, compared to the highest baseline values of 6.5, 20.9, and 23.4, respectively. This improvement is attributed to the reduced domain shift between ArTaxOr's well-defined object boundaries and the COCO pre-training dataset, in contrast to other benchmarks. Additionally, leveraging LLM-generated rich text improves performance across all three datasets by providing more comprehensive descriptions. However, this advantage is not universal; for datasets like NEU-DET, LLMs occasionally fail to produce distinct descriptions for abstract or highly technical category names, limiting their effectiveness."}, {"title": "4.2.2 Results for PASCAL-VOC", "content": "Our few-shot detection results on the PASCAL VOC dataset are presented in Table 11. Consistent with TFA [10] and the setup outlined in Section 4.1.1, we use three fixed base-novel category splits (Split 1, 2, and 3) to minimize sampling bias. For each split, predefined n-shot sampling (n \u2208 {1, 2, 3, 5, 10}) is applied to the novel categories. The model is evaluated across these dataset splits. As detailed in Section 3.1, fine-tuning also involves base categories to mitigate forgetting, but with different sampling strategies: (i) restricted n-shot sampling for base categories, as employed by TFA [10]; and (ii) unrestricted sampling for base categories, as used in Meta-DETR [12]. Both approaches are valid for FSOD; we adopt the latter. As shown in Table 11, our method demonstrates superior performance compared to prior work, highlighting its effectiveness."}, {"title": "4.2.3 Results for Mo-FSOD", "content": "The results on the Mo-FSOD benchmark are shown in Table 12, where our approach outperforms previous methods, particularly on the Clipart1k dataset. In the 10-shot scenario, we achieve scores of 47.7 and 48.8, significantly higher than the best prior method's score of 25.6. However, on the NEU-DET dataset, LLM-generated text descriptions do not yield better performance than manually crafted descriptions. This is due to the LLM's inability to generate sufficiently distinctive descriptions for the six defect categories, resulting in text homogeneity that limits its effectiveness."}, {"title": "4.2.4 Results for CDS-FSOD", "content": "The results on the CDS-FSOD benchmark, presented in Table 13, demonstrate that our method outperforms the approach by Lee et al. [49]. This highlights the effectiveness of our method, surpassing their results across the evaluated tasks, further solidifying the potential of our approach in addressing cross-domain few-shot object detection challenges.\nAcross all benchmarks, we observe that when rich text descriptions of the target objects are available, few-shot learning performance improves significantly. However, the quality and context of the rich text are crucial for effective knowledge transfer. Users must carefully assess and fine-tune the rich text for optimal results. This process may pose challenges in two situations: (i) when the rich text includes irrelevant descriptions, or (ii) when the user lacks rich text and needs to generate it from scratch. In the first case, focusing on descriptions most relevant to the visual features of the target objects is recommended. In the second case, users could enhance and refine LLM-generated text descriptions to improve their effectiveness."}, {"title": "4.3 Analytic Experiments", "content": "We offer analytic experiments to offer a better insight about our proposed approach."}, {"title": "4.3.1 Effect of the Rich text length", "content": "We first investigate the impact of the rich text length on model performance by conducting experiments on the UODD dataset. We compare four text cases: (i) the raw category name, (ii) our manually-built rich text referencing Wikipedia, (iii) an extended version of (ii) with added image context, and (iv) LLM-generated rich text. Cases (ii) to (iv) involve longer, semantically-rich sentences, generally improving model performance, as shown in Table 14. However, not all sentence length extensions are beneficial. For instance, in case (iii), extending the description for sea cucumbers slightly reduced performance from 17.4 to 16.9, likely due to confusion caused by the inclusion of sea urchin, another category with distinct characteristics in the dataset."}, {"title": "4.3.2 Module Analysis", "content": "We evaluate the effectiveness of our proposed multi-modal aggregated feature module and the rich semantic rectify module. As shown in Table 15, our method significantly outperforms the single-modal baseline by incorporating rich text multi-modal information. Additionally, the rich text rectify module further enhances performance, demonstrating its effectiveness in improving the overall model accuracy. This indicates that leveraging both rich text and the"}, {"title": "4.3.3 Effect of Information Modalities on Performance", "content": "We perform an ablation experiment, as shown in Table 16, to assess the impact of different modalities on downstream performance. When only the image modality is used, the model performs at the baseline level. Introducing the language modality, which uses an encoder language branch to extract prototype features, leads to a performance drop. This is because the language modality is tailored to categories rather than individual images, limiting its effectiveness for task-level support. However, as demonstrated in Table 15, combining the language prototype with vision-language alignment significantly improves performance."}, {"title": "4.3.4 Effect of Shared vs Decoupled Attention", "content": "In our MM Aggregation module, we employ a shared attention mechanism. To assess the effectiveness of this choice, we also compare it with a decoupled attention mechanism, where multi-head self-attention modules are separated for the support vision and semantic prototypes. As shown in Table 17, the decoupled attention approach results in a decrease in performance. This decline occurs because the language prototypes are designed at the category level, not the image level. Thus, using separate attention modules diminishes the benefits of the language modality for task-level support."}, {"title": "4.4 Qualitative Comparison", "content": "In this section, we present qualitative detection results to illustrate our method's effectiveness. Figure 12 showcases examples from the three CD-FSOD benchmarks, where the goal is to detect araneae (ArTaxOr), sea urchins and sea cucumbers (UODD), and windmills (DIOR). Using a confidence threshold of 0.3 for all methods, we observe improvements over baseline methods. These include more precise bounding boxes (first row), fewer false positives (second row), and higher detection confidence (third row).\nThe detection results for ArTaxOr, DIOR, and UODD are shown in Figures 13, 14, and 15, respectively. Consistent improvements are observed across all datasets when compared to both the multi-modal method (Next-chat) and the single-modal method (Meta-DETR). For instance, in Figure 13, there is better classification of Lepidoptera, improved bounding box regression for Odonate, and stronger classification confidence. Similar improvements are seen in Figures 14 and 15, where additional foreground detections, like the sea cucumber and scallop, are also identified."}, {"title": "4.5 Computational Overhead", "content": "We provide a reference for our computational overhead in Table 18, with values based on 10-shot (68 images per epoch) experiments on the ArTaxOr dataset. Our method involves more than 10 times the number of training parameters compared to Meta-DETR, yet requires only about twice the training time. This is due to the calculation and caching of feature embeddings for the rich text. We conclude that the additional computational cost is reasonable, given the substantial performance improvements observed in our experiments."}, {"title": "5 Conclusion", "content": "We proposed a novel cross-domain multi-modal few-shot object detection network and demonstrated that incorporating rich text information significantly enhances detection robustness in out-of-domain, few-shot settings. Our experiments highlight that the design of rich text, particularly accurate descriptions of object appearance, plays a crucial role in improving performance. Our method outperforms existing approaches on four cross-domain datasets. We hope this work encourages further research into leveraging multi-modality to bridge domain gaps in other computer vision tasks."}, {"title": "Data Availibility", "content": "The databases used in the manuscript are publicly available."}]}