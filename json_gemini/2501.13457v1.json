{"title": "Zero-Shot Trajectory Planning for Signal Temporal Logic Tasks", "authors": ["Ruijia Liu", "Ancheng Hou", "Xiao Yu", "Xiang Yin"], "abstract": "Signal Temporal Logic (STL) is a powerful specification language for describing complex temporal behaviors of continuous signals, making it well-suited for high-level robotic task descriptions. However, generating executable plans for STL tasks is challenging, as it requires consideration of the coupling between the task specification and the system dynamics. Existing approaches either follow a model-based setting that explicitly requires knowledge of the system dynamics or adopt a task-oriented data-driven approach to learn plans for specific tasks. In this work, we investigate the problem of generating executable STL plans for systems whose dynamics are unknown a priori. We propose a new planning framework that uses only task-agnostic data during the offline training stage, enabling zero-shot generalization to new STL tasks. Our framework is hierarchical, involving: (i) decomposing the STL task into a set of progress and time constraints, (ii) searching for time-aware waypoints guided by task-agnostic data, and (iii) generating trajectories using a pre-trained safe diffusion model. Simulation results demonstrate the effectiveness of our method indeed in achieving zero-shot generalization to various STL tasks.", "sections": [{"title": "1 Introduction", "content": "Signal Temporal Logic (STL) is a formal specification language used to describe the temporal behavior of continuous signals. It has become widely adopted for specifying high-level robotic behaviors due to its expressiveness and the availability of both Boolean and quantitative evaluation measures. Controlling robots under STL task constraints, however, is a challenging problem, as it requires balancing both the satisfaction of the task and the feasibility of the system dynamics. In cases where the environment and system dynamics are fully known, several representative methods have been developed, including optimization-based approaches [Raman et al., 2014; Kurtz and Lin, 2022; Sun et al., 2022], gradient-based techniques [Gilpin et al., 2020; Dawson and Fan, 2022], and sampling-based methods [Ilyes et al., 2023]. However, these methods are often difficult to apply in practical scenarios, where the system dynamics and environment are either unknown or difficult to model.\nTo address the challenge of unknown dynamics, several learning-based approaches have been proposed. One typical method is reinforcement learning (RL) [Aksaray et al., 2016; Balakrishnan and Deshmukh, 2019; Kalagarla et al., 2021; Venkataraman et al., 2020; Ikemoto and Ushio, 2022; Wang et al., 2024], where an appropriate reward function is designed to approximate the satisfaction of the STL task. However, these methods often struggle with long-horizon STL tasks and lack generalization capabilities across different tasks. Another approach involves first learning a system model and then integrating it with model-based planning methods. For example, in [Kapoor et al., 2020], the authors trained a neural network to approximate the system dynamics and combined it with an optimization-based approach. However, this method is limited to simple short-horizon STL tasks due to its high computational cost. In [He et al., 2024], the authors used goal-conditioned RL to train multiple goal-conditioned policies, referred to as \u201cskills,\u201d to accomplish specific objectives. They then applied a search algorithm to determine the optimal sequence of \u201cskills\u201d needed to satisfy the given STL tasks. While this approach enables a certain degree of task generalizations, these tasks must be based on pre-defined objectives associated with the skills.\nMore recently, generative models, such as diffusion models [Ho et al., 2020], have emerged as a new approach for generating trajectories for systems with unknown dynamics [Janner et al., 2022; Ajay et al., 2022; Chi et al., 2023; Carvalho et al., 2023; Huang et al., 2025], gaining popularity across various applications. Compared to traditional model-based reinforcement learning methods, these generative approaches are better suited for long-horizon decision-making and offer greater test-time flexibility [Janner et al., 2022], making them particularly effective for complex tasks. For example, for finite Linear Temporal Logic (LTLf) tasks, [Feng et al., 2024a] introduced a classifier-based guidance approach to steer the sampling of diffusion models, ensuring that generated trajectories satisfy LTLf requirements. Similarly, [Feng et al., 2024b] proposed a hierarchical framework that decomposes co-safe LTL tasks into sub-tasks using hierarchical reinforcement learning. This framework employs a diffusion model with a determinant-based sampling strategy to generate diverse low-level trajectories, improving both"}, {"title": "2 Preliminaries", "content": "2.1 System Model\nWe consider a discrete time system with unknown dynamics\n$$x_{t+1} = f(x_t, a_t),$$,\nwhere $$x_t \\in \\mathbb{R}^n$$ and $$a_t \\in \\mathbb{R}^m$$ are the state and the action at time instant t, respectively. Given an initial state $$x_0$$ and a sequence of actions $$a_0 a_1 ... a_{T-1}$$, the resulting trajectory of the system is $$ \\tau = x_0 a_0 x_1 a_1 ... a_{T-1} x_T $$, where T is the horizon. The signal of the trajectory is referred to as the state sequence $$s = x_0 x_1 .. x_T$$ and we denote by $$s_t = x_t x_{t+1} ... x_T$$ the sub-signal starting from time step t.\n2.2 Signal Temporal Logic\nWe use signal temporal logic (STL) to describe the formal task imposed on the generated state sequence [Maler and Nickovic, 2004]. Specifically, we consider STL formula in the Positive Normal Form (PNF) [Sadraddini and Belta, 2015] whose syntax is as follows:\n$$\\varphi::=\\top | \\mu | \\varphi_1 \\wedge \\varphi_2 | \\varphi_1 \\vee \\varphi_2 | F_{[a,b]}\\varphi | G_{[a,b]}\\varphi | \\varphi_1 U_{[a,b]}\\varphi_2,$$,\nwhere $$\\top$$ is the true predicate and $$\\mu$$ is an atomic predicate associated with an evaluation function $$h_{\\mu} : \\mathbb{R}^n \\rightarrow \\mathbb{R}$$, i.e., predicate $$\\mu$$ is true at state $$x_t$$ iff $$h_{\\mu}(x_t) \\geq 0$$. Furthermore, $$\\wedge$$ and $$\\vee$$ are logic operators \u201cconjunction\u201d and \u201cdisjunction\u201d, respectively; $$U_{[a,b]}$$, $$F_{[a,b]}$$ and $$G_{[a,b]}$$ are temporal operators \u201cuntil\u201d, \u201ceventually\u201d and \u201calways\u201d, respectively; $$[a,b]$$ is a time interval such that $$a, b \\in \\mathbb{Z}, 0 \\leq a \\leq b < \\infty$$. Note that, negation is not used in the PNF. However, as shown in [Sadraddini and Belta, 2015], this does not result in any loss of generality as one can always redefine atomic predicates to account for the presence of negations, allowing any general STL formula to be expressed in PNF. In our work, we impose an additional restriction on the Prenex Normal Form of formulas. Specifically, for any formula of the form $$\\varphi_1 U_{[a,b]}\\varphi_2$$, $$\\varphi_1$$ can only involve temporal operator \"always\". This restriction is introduced for technical reasons, as it facilitates the decomposition of the overall formula into a set of progresses. For any signal $$s = x_0 x_1 ... x_T$$, we denote by $$s_t \\models \\varphi$$ if s satisfies STL formula $$\\varphi$$ at time t, and we denote by $$s \\models \\varphi$$ if $$s_0 \\models \\varphi$$. This is formal defined by the Boolean semantics of STL formulae as follows [Bartocci et al., 2018]:\n$$s_t \\models \\mu \\Leftrightarrow h_{\\mu}(x_t) \\geq 0,$$,\n$$s_t \\models \\varphi_1 \\wedge \\varphi_2 \\Leftrightarrow s_t \\models \\varphi_1 \\wedge s_t \\models \\varphi_2,$$,\n$$s_t \\models \\varphi_1 \\vee \\varphi_2 \\Leftrightarrow s_t \\models \\varphi_1 \\vee s_t \\models \\varphi_2,$$,\n$$s_t \\models F_{[a,b]}\\varphi \\Leftrightarrow \\exists t' \\in [t + a, t + b] s.t. s_{t'} \\models \\varphi,$$,\n$$s_t \\models G_{[a,b]}\\varphi \\Leftrightarrow \\forall t' \\in [t + a, t + b] s.t. s_{t'} \\models \\varphi,$$,\n$$s_t \\models \\varphi_1 U_{[a,b]}\\varphi_2 \\Leftrightarrow \\exists t' \\in [t + a, t + b] s.t. s_{t'} \\models \\varphi_2 \\wedge \\forall t'' \\in [t, t'], s_{t''} \\models \\varphi_1.$$\n2.3 Planning with Unknown Dynamics\nIn the context of STL planning, the objective is to determine an action sequence such that the resulting signal satisfies the specified STL formula. When the system dynamics are perfectly known, this problem can be solved using model-based optimization approaches (e.g., see [Raman et al., 2014; Kurtz and Lin, 2022; Sun et al., 2022]). In contrast, our work addresses a setting with unknown dynamics. Specifically, we assume the mapping $$f : \\mathbb{R}^n \\times \\mathbb{R}^m \\rightarrow \\mathbb{R}^n$$ is unknown, but a dataset of historical operational trajectories, consistent with the underlying unknown system dynamics, is available. Note that each trajectory in the dataset is collected from the previous task-agnostic operations and may vary in length. Our goal is to leverage these task-agnostic trajectories to generate new trajectories that satisfy any given STL formula, thereby achieving zero-shot task generalization at test time.\nProblem 1. Given a set of trajectories from the unknown system (1) and a STL formula $$\\varphi$$, find a sequence of actions $$a_0 a_1 ... a_T$$ such that the resulting signal s satisfies the STL formula, i.e., $$s \\models \\varphi$$.", "equation": "xt+1 = f(xt, at),\n\u03c6::=\u03a4 | \u03bc | 41/42 | Y1VY2 | F[a,b]Y | G[a,b]Y | 41U[a,b]\u00a52,"}, {"title": "3 Our Method", "content": "3.1 Overall Framework\nFirst, we provide an overview of our proposed planning methods, whose overall structure is illustrated in Figure 1. Specifically, our method consists of the following three parts:\n\u2022 Task Decomposition: First, we decompose the given STL into a set of spatial-temporal relevant progresses $$\\mathcal{P} = \\mathcal{P}^R \\cup \\mathcal{P}^I$$, where $$\\mathcal{P}^R$$ is the set of reachability progresses and $$\\mathcal{P}^I$$ is the set of invariance progresses. Furthermore, these progresses need to be satisfied subject to a set of time constraints $$\\mathcal{T}$$ over a set of time variables $$\\mathcal{A}$$.\n\u2022 Progress Allocation: While the decomposition above is task-focused, achieving progress in the correct order requires consideration of the underlying dynamics of the system. To address this, we employ a pre-trained time predictor derived from trajectory data to estimate the time required for the system to transition from one waypoint to another. A search-based algorithm is then used to determine a sequence of waypoints with associated timestamps $$(x_0, t_0) (x_1, t_1) ... (x_n, t_n)$$ such that each waypoint satisfies the corresponding reachability progress in $$\\mathcal{P}^R$$, and all timestamps comply with the time constraints $$\\mathcal{T}$$.\n\u2022 Trajectory Generation: The final step is to generate an executable trajectory for the system, ensuring that each waypoint is visited at the correct time while maintaining satisfaction of the invariance progresses. This problem can be framed as several reach-avoid control problems with unknown system dynamics. Our approach leverages a diffusion model, pre-trained on trajectory data, to generate task-compliant and dynamic-feasible trajectories through constraints-guided sampling. Following [Ajay et al., 2022], we utilize the diffusion model solely for generating the trajectory's state sequence. The action sequence can be obtained offline using an inverse dynamics model [Agrawal et al., 2016] or online via appropriate controllers.\nNexet, we provide the technical details of each part.\n3.2 Decompositions of STL Formulae\nEliminating Disjunctions For the given STL task $$\\varphi$$, our first step is to covert it into the disjunctive normal form (DNF) $$\\varphi = \\varphi_1 \\vee \\varphi_2 \\vee ... \\vee \\varphi_n$$, where each subformula $$\\varphi_i$$ involves no \"disjunction\". Formally, for any STL formula, one can obtain its DNF by recursively applying the following replacements:\n\u2022 replace $$F_{[a,b]} (\\varphi_1 \\vee \\varphi_2)$$ by $$F_{[a,b]}\\varphi_1 \\vee F_{[a,b]}\\varphi_2$$;\n\u2022 replace $$G_{[a,b]} (\\varphi_1 \\vee \\varphi_2)$$ by $$G_{[a,b]}\\varphi_1 \\vee G_{[a,b]}\\varphi_2$$;\n\u2022 replace $$(\\varphi_1 \\vee \\varphi_2) U_{[a,b]}(\\varphi_1 \\vee \\varphi_2)$$ by $$\\vee_{i,j\\in{1,2}} \\varphi_i U_{[a,b]}\\varphi_j$$\nNote that the last two replacements are not equivalent, making the resulting DNF stronger than the original formula $$\\varphi$$. Furthermore, to achieve the task defined by $$\\varphi$$, it suffices to satisfy one of the subformulas $$\\varphi_i$$. Without loss of generality, we will assume henceforth that the DNF contains only a single subformula, as the STL planning problem can be addressed for each subformula individually. In other words, moving forward, we will focus on STL formulae, denoted directly by $$\\varphi$$, without negations (due to the PNF) and without disjunctions (due to the DNF).\nProgresses and Constraints Next, we further decompose the overall STL task $$\\varphi$$ into a set of progresses. Specifically, we consider the following two types of progresses:\n\u2022 Reachability Progress: we denote by $$R(a, b, \\mu)$$ that $$\\exists t \\in [a, b], x_t \\models \\mu$$.\n\u2022 Invariance Progress: we denoted by $$I(a,b,\\mu)$$ that $$\\forall t \\in [a, b], x_t \\models \\mu$$.\nNote that we use subscript $$\\mathcal{A}$$ in time interval $$[a, b]$$ as $$a_{\\mathcal{A}}$$ and $$b_{\\mathcal{A}}$$ may not be a fixed time and involve variables in $$\\mathcal{A}$$ subject to time constraints. Therefore, the STL formula $$\\varphi$$ is decomposed into a tuple $$(\\mathcal{P}_\\varphi, \\mathcal{T}_\\varphi)$$, where $$\\mathcal{P}_\\varphi$$ is the set of progresses and $$\\mathcal{T}_\\varphi$$ is the set of time constraints over variable set $$\\mathcal{A}$$. Such decomposition is defined recursively as follows:\n\u2022 If $$\\varphi = F_{[a,b]}\\mu$$, then we have $$\\mathcal{P}_\\varphi = {R(\\lambda_i, \\lambda_i, \\mu)}$$ and $$\\mathcal{T}_\\varphi = {\\lambda_i \\in [a, b]}$$, where $$\\lambda_i$$ is a new time variable.\n\u2022 If $$\\varphi = G_{[a,b]}\\mu$$, then we have $$\\mathcal{P}_\\varphi = {I(a, b, \\mu)}$$ and $$\\mathcal{T}_\\varphi = \\emptyset$$.\n\u2022 If $$\\varphi = \\mu_1 U_{[a,b]}\\mu_2$$, then we have $$\\mathcal{P}_\\varphi = {I(a, \\lambda_i, \\mu_1), R(\\lambda_i, \\lambda_i, \\mu_2)}$$ and $$\\mathcal{T}_\\varphi = {\\lambda_i \\in [a, b]}$$, where $$\\lambda_i$$ is a new time variable.\n\u2022 If $$\\varphi' = \\varphi_1 \\wedge \\varphi_2$$, then we merge the progresses and time constraints, i.e., $$\\mathcal{P}_\\varphi = \\mathcal{P}_{\\varphi_1} \\cup \\mathcal{P}_{\\varphi_2}$$ and $$\\mathcal{T}_\\varphi = \\mathcal{T}_{\\varphi_1} \\cup \\mathcal{T}_{\\varphi_2}$$.\n\u2022 If $$\\varphi' = F_{[a,b]}\\varphi$$, then we (i) introduce a new time variable $$\\lambda_i$$; (ii) add a new time constraint $$\\mathcal{T}_{\\varphi'} = \\mathcal{T}_\\varphi \\cup {\\lambda_i \\in [a, b]}$$; (iii) increase each time indices in each progress by $$\\lambda_i$$, i.e., $$\\mathcal{P}_{\\varphi'} = {P(c_{\\mathcal{A}} + \\lambda_i, d_{\\mathcal{A}} + \\lambda_i, \\mu) | P(c_{\\mathcal{A}}, d_{\\mathcal{A}}, \\mu) \\in \\mathcal{P}}$$.\n\u2022 If $$\\varphi' = G_{[a,b]}\\varphi$$, then (i) the time constraints remain unchanged, i.e., $$\\mathcal{T}_{\\varphi'} = \\mathcal{T}_\\varphi$$; (ii) modify each invariance progress $$I(c_{\\mathcal{A}}, d_{\\mathcal{A}}, \\mu) \\in \\mathcal{P}_\\varphi$$ to $$I(c_{\\mathcal{A}} + a, d_{\\mathcal{A}} + b, \\mu)$$ in $$\\mathcal{P}_{\\varphi'}$$; (iii) modify each reachability progress $$R(c_{\\mathcal{A}}, d_{\\mathcal{A}}, \\mu) \\in \\mathcal{P}_\\varphi$$, to b-a progresses $$R(c_{\\mathcal{A}} + k, d_{\\mathcal{A}} + k, \\mu)$$ in $$\\mathcal{P}_{\\varphi'}$$, where $$k = a, a + 1, ..., b$$.\n\u2022 If $$\\varphi' = \\varphi U_{[a,b]}\\psi$$, then (i) introduce a new time variable $$\\lambda_i$$; and (ii) add a new time constraint, i.e., $$\\mathcal{T}_{\\varphi'} = \\mathcal{T}_\\varphi \\cup {\\lambda_i \\in [a, b]}$$; and (iii) increase each time indices in each progress in $$\\mathcal{P}$$ by $$\\lambda_i$$ and modify each invariance progress $$I(c_{\\mathcal{A}}, d_{\\mathcal{A}}, \\mu) \\in \\mathcal{P}$$ to $$I(c_{\\mathcal{A}} + a, d_{\\mathcal{A}} + \\lambda_i, \\mu)$$, i.e., $$\\mathcal{P}_{\\varphi'} = {P(c_{\\mathcal{A}} + \\lambda_i, d_{\\mathcal{A}} + \\lambda_i, \\mu) | P(c_{\\mathcal{A}}, d_{\\mathcal{A}}, \\mu) \\in \\mathcal{P}} \\cup {I(c_{\\mathcal{A}} + a, d_{\\mathcal{A}} + \\lambda_i, \\mu) | I(c_{\\mathcal{A}}, d_{\\mathcal{A}}, \\mu) \\in \\mathcal{P}}$$.", "equation": "Xt+1 = f(xt, at),\n\u03c6::=\u03a4 | \u03bc | 41/42 | Y1VY2 | F[a,b]Y | G[a,b]Y | 41U[a,b]\u00a52,"}, {"title": "3.3 Progress Allocation", "content": "Without considering the system dynamics, the STL task planning problem is essentially a constraint satisfaction problem for the decomposed progresses and constraints $$(\\mathcal{P}_\\varphi, \\mathcal{T}_\\varphi)$$. However, the unknown system dynamics introduce additional challenges, as allocating progress arbitrarily may not be feasible for the system. To address this issue, we use a search algorithm to allocate the progresses, where the feasibility of each assignment is determined based on the trajectory data.\nTo perform the search-based allocation process, we further split the progresses $$\\mathcal{P}$$ as follows. For each invariance progress $$I(a_{\\mathcal{A}}, b_{\\mathcal{A}}, \\mu) \\in \\mathcal{P}_\\varphi$$, we decompose it into a reachability progress $$R(a_{\\mathcal{A}},a_{\\mathcal{A}},\\mu)$$ and an invariance progress $$I(a_{\\mathcal{A}} + 1, b_{\\mathcal{A}}, \\mu)$$. For simplicity, we will denote the further decomposed progresses as $$(\\mathcal{P}, \\mathcal{T})$$ without subscripts, where $$\\mathcal{P} = \\mathcal{P}^R \\cup \\mathcal{P}^I$$. Due to this further decomposition, each invariance progress follows a unique reachability progress.\nMain Allocation Algorithm The main algorithm for progress allocation is presented in Algorithm 1. The algorithm employs a depth-first search (DFS) to sequentially assign satisfaction times and waypoint for each reachability progress in $$\\mathcal{P}^R$$. When the algorithm terminates, it returns a sequence of waypoints with associated timestamps of form $$(x_0, t_0)(x_1, t_1)...(x_n, t_n)$$ such that each waypoint corresponds to the satisfication of a reachability progress in $$\\mathcal{P}^R$$. During the search process, we maintain the current state x, the current time step t, the set of remaining reachability progresses $$\\mathcal{P}^R$$, the set of all time constraints $$\\mathcal{T}$$, and the searched sequence of waypoints with associated timestamps s."}, {"title": "3.4 Trajectory Generation", "content": "In the above part, a sequence of waypoints s is obtained and the time intervals for each invariance progress have also been determined. The remaining task is to generate executable trajectories that connect the waypoints while ensuring the trajectories satisfy the invariance progresses. Specifically, the trajectory $$\\\\\\tau$$ between two adjacent waypoints $$(x_i, t_i)$$ and $$(x_{i+1}, t_{i+1})$$ must be feasible under the current system dynamics and satisfy the following conditions:\n1. The trajectory length is $$t_{i+1} - t_i + 1$$.\n2. It starts at state $$x_i$$ and ends at state $$x_{i+1}$$.\n3. It satisfies all invariance progresses active within the time interval $$[t_i, t_{i+1}]$$.\nWe employ the diffusion model to solve this conditional trajectory generation problem since it can learn the distribution of system trajectories from system trajectory data, enabling it to generate feasible trajectories under the current system. Additionally, by guiding the sampling process, the generated trajectories can satisfy additional constraints.\nThe technical details of the diffusion model employed are provided in the Appendix B. Here, we briefly outline the main idea. To satisfy the first condition, we control the length of the generated trajectory by adjusting the length of the initial noise during the denoising process. This is feasible due to the structural properties of the backbone network used in the diffusion model [Janner et al., 2022]. For the second condition, we treat it as an inpainting problem [Janner et al., 2022], ensuring that the generated trajectory meets the requirement by replacing the start and end states of the trajectory with $$x_i$$ and $$x_{i+1}$$, respectively, after each denoising step. To address the last condition, we decompose each invariance progress $$I(a, b, \\mu_i)$$ into state constraints of the form $$h_{\\mu_i}(x_t) \\geq 0$$ for $$t = a, a + 1, . . ., b$$, commonly referred to as \u201csafety\u201d constraints. We employ SafeDiffuser [Xiao et al., 2023] to generate trajectories that comply with these safety constraints. SafeDiffuser integrates control barrier functions (CBFs) [Nguyen and Sreenath, 2016] to enforce finite-time diffusion invariance directly within the sampling process."}, {"title": "4 Case Study", "content": "To illustrate the workflow of our algorithm, we consider a sequential visit and region avoidance task in the Maze2D (Large) environment [Fu et al., 2020]."}, {"title": "5 Experiments", "content": "To evaluate the performance of our algorithm, we further conduct experiments in the Maze2D environment. Specifically,\nto validate the zero-shot generalization capability of our algorithm for STL tasks, we test it on a set of testing cases containing randomly generated STL tasks in three different Maze2D environments: U-Maze, Medium, Large. The experimental setup follows the same framework described in the Case Study. The agent starts from a randomly generated position and must complete the randomly generated STL tasks by reaching the target region within the specified time interval. All experiments were conducted on a PC running Ubuntu 22.04, equipped with an Intel i7-13700K CPU and an Nvidia 4090 GPU.\nSTL Tasks Generation In order to generate random STL tasks, we design nine STL task templates as shown in Table 2. For each template, we randomly generated time intervals and the positions and sizes of circular regions corresponding to atomic predicates in the template, resulting in randomized STL tasks. Specifically, for each pair of Maze2D environment (U-Maze, Medium, Large) and each task template listed in Table 2, we generate 150 feasible random STL formulae.\nBaseline Algorithm We compare our algorithm with the method proposed in [Zhong et al., 2023], which adopts classifier-based guidance and directly leverages the gradient of the trajectory's robustness value to guide the sampling process of the diffusion model, thereby optimizing the robustness of the generated trajectory. The gradient of robustness is calculated by the STLCG method proposed in [Leung et al., 2023]. In the following text, we refer to this algorithm as the Robustness Guided Diffuser (RGD).\nExperiment Settings The diffusion models used in both RGD and our algorithm are trained following the procedure in [Janner et al., 2022] using the D4RL dataset [Fu et al., 2020]. A simple multilayer perceptron (MLP) with four fully connected layers is used as the TimePredict model in our algorithm and it is also trained using the D4RL dataset. In our experiments, we employ diffusion model to generate only the state sequence of the trajectory and use a simple PD controller to follow the state sequence during running to get the actual execution trajectory.\nEvaluation Metrics For each environment and each STL task template, we test RGD and our algorithm on all randomly generated test cases and record the average of the following metrics across all cases:\n\u2022 Execution Success Rate (SR): The proportion of cases where the actual execution trajectory achieve non-negative robustness values."}, {"title": "A Related Works", "content": "A.1 STL Decomposition\nTo address the high complexity of STL control synthesis problems, several decomposition-based methods have been proposed [Leahy et al., 2023; Yu et al., 2023; Kapoor et al., 2024]. In [Leahy et al., 2023], the authors proposed a formula transformation-based method for multi-agent STL planning. This approach jointly decomposes an STL specification and team of agents. In [Yu et al., 2023], the authors decompose STL tasks into several subtasks with non-overlapping time intervals using time interval decomposition and sequentially apply the shrinking horizon Model Predictive Control (MPC) algorithm to each short-time-interval subtask. However, this method is limited to handling STL fragments that do not include nested temporal logic operators.\nThe work most similar to our STL decomposition framework is [Kapoor et al., 2024]. This work first decomposes STL tasks into several spatio-temporal subtasks with time variable constraints. Then, through time variable simplification, partial ordering, and slicing, the subtasks are segmented into several time intervals. Finally, a planning algorithm is subsequently used to sequentially solve the atomic tasks within each time interval. Our algorithm similarly begins by decomposing the STL task into several spatio-temporal progresses and time variable constraints. However, we adopt a search-based approach to determine the completion times and corresponding states to achieve each progress. During the search process, our method dynamically maintains the time variable constraints on-the-fly according to the completion of progresses. By incorporating the search mechanism, our algorithm achieves greater completeness compared to the incremental planning approach used in [Kapoor et al., 2024]. Additionally, the dynamic maintenance of time variable constraints enables a more natural handling of relationships between subtasks and extends the applicability of our approach to more complex STL task fragments that allows \"until\" operator.\nA.2 Planning with Diffusion Model\nRecent advancements in diffusion-based planning methods highlight their remarkable flexibility, as they rely exclusively on offline trajectory datasets and do not require direct interaction with or access to the environment. By leveraging guided sampling, these methods can address a wide range of objectives without the need for retraining. This approach has been widely applied to long-horizon task planning and decision-making, facilitating the generation of states or actions for control purposes [Janner et al., 2022; Ajay et al., 2022; Chi et al., 2023].\nIn the domain of diffusion-based planning for temporal logic tasks, several significant studies have been conducted [Zhong et al., 2023; Meng and Fan, 2024; Feng et al., 2024a; Feng et al., 2024b]. For instance, [Feng et al., 2024a] proposed a classifier-based guidance approach to direct the sampling process of the diffusion model, enabling the generation of trajectories that fulfill finite Linear Temporal Logic (LTLf) tasks. Similarly, [Feng et al., 2024b] introduced a data-driven hierarchical framework that decomposes co-safe LTL tasks"}, {"title": "B Diffusion Models for Trajectory Planning", "content": "The core concept of diffusion-model-based trajectory planning[Janner et al., 2022; Ajay et al., 2022] is to employ the diffusion model to learn the distribution of pre-collected trajectories $$q (\\tau^0)$$ in the current environment under the system dynamics, thereby transforming planning or control synthesis problems into conditional trajectory generation.\nDiffusion models consist of two processes: the diffusion process and the denoising process.\nThe diffusion process gradually adds Gaussian noise to a trajectory $$\\tau^0$$, transforming it into noise. At each timestep i, the noisy trajectory is given by:\n$$q(\\tau^i | \\tau^{i-1}) = \\mathcal{N}(\\tau^i; \\sqrt{1 - \\beta_i}\\tau^{i-1}, \\beta_i\\mathcal{I}),$$,\nwhere $$\\beta_i$$ controls the noise scale, and N is the total number of diffusion steps. The noisy trajectory $$\\\\\\tau^i$$ at any step can be directly computed as:\n$$\\tau^i = \\sqrt{\\bar{\\alpha_i}}\\tau^0 + \\sqrt{1 - \\bar{\\alpha_i}}\\epsilon, \\epsilon \\sim \\mathcal{N}(0, 1),$$,\nwith $$\\bar{\\alpha_i} = \\Pi_{j=1}^{i}(1 \u2013 \\beta_j)$$.\nThe denoising process reverses the diffusion by iteratively recovering the trajectory from Gaussian noise. The reverse distribution is approximated as:\n$$p_\\theta(\\tau^{i-1} | \\tau^i) = \\mathcal{N}(\\tau^{i-1}; \\mu_\\theta(\\tau^i, i), \\Sigma_\\theta)$$,\nwhere $$\\mu_\\theta(\\tau^i, i)$$ is parameterized by the model, and $$\\\\\\Sigma_\\theta$$ is typically fixed. Instead of learning $$\\mu_\\theta$$ directly, the model typically predicts the noise $$\\epsilon_\\theta (\\tau^i, i)$$ and gets $$\\\\\\mu_\\theta(\\tau^i, i)$$ according to the relationship[Ho et al., 2020]:\n$$\\mu_\\theta(\\tau^i, i) = \\frac{1}{\\sqrt{\\alpha_i}} (\\tau^i - \\frac{1 - \\alpha_i}{\\sqrt{1 - \\bar{\\alpha_i}}}\\epsilon_\\theta(\\tau^i, i)).$$", "equation": "q(\u03c4i | \u03c4i\u22121) = N(\u03c4i; \u221a1 \u2212 \u03b2i\u03c4i\u22121, \u03b2iI),\n\u03c4i = \u221a\u03b1i\u03c40 + \u221a1 \u2212 \u03b1\u03b5, \u03b5 ~ N(0, 1),\np\u03b8(\u03c4i\u22121 | \u03c4i) = N(\u03c4i\u22121; \u03bc\u03b8(\u03c4i, i), \u03a32),\n\u03bc\u03b8(\u03c4i, i) =\n1\n\u221a\u03b1i\n(\u03c4i \u2212\n1 \u2212 \u03b1i\n\u221a1 \u2212 \u03b1\n, i))"}, {"title": "B.1 Safe Planning with Diffusion Model", "content": "Previous studies [Xiao et al.", "2024": "have demonstrated the effectiveness of diffusion models in generating safety constraint-compliant trajectories. In this work", "2023": "which integrates control barrier functions (CBFs) [Nguyen and Sreenath", "2016": "to enforce finite-time diffusion invariance directly within the sampling process.\nSafeDiffuser models the denoising process as a dynamic system:\n$$ \\frac{\\partial \\tau^i"}, {"as": "n$$\\frac{db (x_t)"}, {"problem": "n$$ \\underset{u}{minimize} \\| \\frac{\\tau^{i+1} - \\tau^{i}}{\\Delta \\tau} - u \\|_2, $$,\n$$ \\underset{}{subject to}  \\frac{"}]}