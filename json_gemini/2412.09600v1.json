{"title": "Owl-1: Omni World Model for Consistent Long Video Generation", "authors": ["Yuanhui Huang", "Wenzhao Zheng", "Yuan Gao", "Xin Tao", "Pengfei Wan", "Di Zhang", "Jie Zhou", "Jiwen Lu"], "abstract": "Video generation models (VGMs) have received extensive attention recently and serve as promising candidates for general-purpose large vision models. While they can only generate short videos each time, existing methods achieve long video generation by iteratively calling the VGMs, using the last-frame output as the condition for the next-round generation. However, the last frame only contains short-term fine-grained information about the scene, resulting in inconsistency in the long horizon. To address this, we propose an Omni World modeL (Owl-1) to produce long-term coherent and comprehensive conditions for consistent long video generation. As videos are observations of the underlying evolving world, we propose to model the long-term developments in a latent space and use VGMs to film them into videos. Specifically, we represent the world with a latent state variable which can be decoded into explicit video observations. These observations serve as a basis for anticipating temporal dynamics which in turn update the state variable. The interaction between evolving dynamics and persistent state enhances the diversity and consistency of the long videos. Extensive experiments show that Owl-1 achieves comparable performance with SOTA methods on VBench-I2V and VBench-Long, validating its ability to generate high-quality video observations.", "sections": [{"title": "1. Introduction", "content": "With the success of image generative models [2, 12, 23, 27, 28, 41], video generation [14, 15, 17, 29, 32] have also garnered increasing attention. While existing video generation models (VGMs) [3, 7, 26, 35] have achieved commercial-grade performance, the durations of videos are still short. The long video generation methods [18, 20, 34, 37, 43] remedies this issue by focusing on improving the length and consistency of generated videos, facilitating a variety of newly rising tasks such as video extension [35], film generation [40] and world simulation [24].\nDespite the promising applications, how to increase the video length while preserving consistency remains an open question. Several work [1, 43] investigates the 3D vari-"}, {"title": "2. Related Work", "content": "Short video generation. In the realm of computer vision, video generation has emerged as a pivotal area of research, garnering significant attention due to its broad applications. Short video generation investigates how to generate videos based on text (and/or image) conditions, where the alignment between the generated video and the given conditions is one of the primary evaluation criteria. For text conditions, most methods [3, 15, 35] encode them with pretrained text encoders [22, 25], and incorporate the textual features using cross attention. In addition, image-to-video models requires the generated video to incorporate the specified image conditions. In order to effectively fuse the fine-grained visual information, several approaches [13, 43] directly replace or concatenate the diffusion features with the encoded features of the image condition. Other methods [35] also transform the image condition into tokens similar to the textual features, and apply cross attention between the diffusion features and image tokens to preserve coarser level of details such as visual styles and background. Our Owl-1 uses both the latent state and optional image conditions from the last clip for consistent and smooth generation of the next clip.\nLong video generation. As an important extension of the application scope of video generation models, long video generation focuses on improving the length and consistency of generated videos. To achieve this, several work attempts to enhance the video durations in a single generation process, by designing 3D VAEs that are able to compress longer videos [1, 43] or investigating the temporal modules in VGMs for efficient generation [35]. Although the end-to-end generation pipeline inherently guarantees the video consistency, the length of generated videos is constrained by limited computational resources. To remedy this issue, the divide-and-conquer approach simplifies the task by first identifying key frames that outline the main narrative and then generating the intervening frames to create a cohesive long video. However, these methods are dependent on training video data of long durations which are still"}, {"title": "Video generation world models.", "content": "Video generation models are promising candidates for world models [44] which aims to model the evolution of the environment. For videos of short durations, the generated content may reflect certain physical laws [21], indicating that the video generation model has learned some general knowledge about the world. For a longer horizon, the emphasis of video generation world models lies in capturing overall dynamics that drive environment evolution [44]. Although such models have been proposed in autonomous driving [33, 42] and embodied intelligence [5], they can only predict structured actions instead of general world dynamics in the form of natural language. As for general video generation, most existing methods focus on improving the alignment of generated videos and given text conditions, lacking the ability to anticipate the world dynamics. In addition to conditional video generation, our Owl-1 is capable of predicting future dynamics to generate long videos with diverse content."}, {"title": "3. Proposed Approach", "content": "In this section, we present our method of omni world model for consistent long video generation. To formulate this task mathematically, we aim to generate a long video consisting of a sequence of video clips v = {..., ot\u22121, ot, ot+1, ...} given a starting image I and a text description do as input."}, {"title": "3.1. Omni World Model", "content": "Videos are fundamentally recorded observations of the underlying evolving world, whose long-term consistency is inherently guaranteed in the coherence of the world itself. Therefore, maintaining consistency in long videos from the perspective of the implicit world is a more reasonable and essential approach, compared with the explicit pixel-space methods. However, the real world constitutes a complex high-dimensional system, and the cost of directly modeling such a system is unacceptable. Inspired by the world models in the field of embodied intelligence [4], we represent the world using a set of latent state variables {..., st-1, st, ...}. Each state st not only encodes information about the world at the current moment t, but also incorporates historical information about the evolution of the world, i.e. {..., st-2, st-1}. Since state variables serve as an purely implicit representation of the world, we introduce a state decoder D to obtain explicit video observations {..., ot\u22121, ot, ...} from the state variables:\not = D(st, ot-1),\nwhere we incorporate the last observation ot-1 to ensure short-term fine-grained smoothness of successive observations, while the current state st is primarily responsible for the long-term consistency."}, {"title": "3.2. Comprehensive Condition from Latent State", "content": "The key challenge in the temporal autoregressive paradigm for long video generation lies in the design of the condition used for generating the next clip. Most existing methods directly take the last frames of the previous clip as condition, which only considers the short-term smoothness between consecutive clips, and overlooks consistency issues in the long-term such as style, character identity, background etc. Our Owl-1 takes the latent state variable as a comprehensive condition for the long-term consistency, because the derivation of the current state st inherently includes the information of all previous observations:\nst+1 = g(st, f(st, ot)) = h(so, oo, \u2026\u2026\u2026, ot\u22121, ot),\nwhich is derived by plugging Eq. (2) into Eq. (3) and iteratively replacing st with st-1 and ot-1.\nFor implementation of our Owl-1, we take advantage of a large multimodal model (LMM) to instantiate the functions f(.) and g(\u00b7), in order to take advantage of its common knowledge from the large scale pretraining on textual and visual data, and its large receptive field as well. And we instantiate the state decoder D with a pretrained video diffusion model for their capability to generate short videos of high quality. To incorporate the state-observation-dynamics triplet into the framework of LMM, we design the format of the input and output sequences:\nSeq = [..., st, ot, dt, ...],"}, {"title": "3.3. Anticipation of Future Dynamics", "content": "In the context of long video generation, anticipating future dynamics is crucial for maintaining consistency and coherence across extended video sequences. Our Owl-1 predicts and integrates these future dynamics dt into the evolution of the latent state s, thereby enriching the content diversity and ensuring temporal consistency in the generated videos. As indicated by Eq. (2), the prediction of world dynamics dt relies on the current video observation of as short-term reference, and the current latent state st as source of long-term information. Once the current dynamics dt is predicted, we integrate it into the latent state variable st to update the world state for the next-round generation. To train the dynamics anticipation ability of the LMM, we adopt the next-token prediction paradigm and use the textual ground truth dynamics for teacher-forcing supervision.\nThe anticipation of future dynamics is important for content diversity of generated long videos and world modeling. By enabling the anticipation of subsequent events within a video sequence, our Owl-1 enhances the richness of the generated content, moving beyond repeated generation of homogeneous content to capturing the essence of dynamic scenarios. Furthermore, future dynamics prediction serves as a cornerstone for constructing plausible world models, which are instrumental in simulating and understanding complex environments. Our Owl-1 not only predicts real-world behaviors but also allow for the incorporation of control mechanisms by replacing the anticipated dynamics with user-input control signals, facilitating the generation of content that is not only predictable but also controllable."}, {"title": "3.4. Multi-Stage Training", "content": "Several challenges exist in the training process of our Owl-1: 1) Since the LMM and the video diffusion model are separately pretrained, it is nontrivial to align these two models. 2) Our Owl-1 is designed for long-term world modeling, which requires video data with long duration and dense captions. However, given the scarcity of such high-quality data, it would be infeasible to train these large models with billions of parameters directly for the purpose of world model. Therefore, we carefully design a multi-stage training scheme for our Owl-1 which consists of alignment, generative pretraining and world model training.\nThe alignment stage primarily enforces the consistency between the state variables st from the LMM and the textual conditions of the video diffusion model, which serves as a good initialization for the subsequent generative pretraining stage. Specifically, we freeze the video diffusion model to preserve its ability of generating short videos and only trains the LMM at this stage. We use general datasets for video generation in this stage, which provide videos of varying lengths and one single description for each video. For each sample (v, t), we first segment the video into short clips of fixed length v = {..., ot\u22121, ot, ot+1, ...}, and construct the input sequence as:\nSeqalign = [I, t, so, oo, t, ..., st, ot, t, ...],\nwhere I represents the first frame, and we use the same text dynamics t for every triplet since the general video generation datasets do not provide dense captions for every clip and the content of the video remains largely unchanged throughout its duration. To train the LMM to align with the textual conditions of video diffusion model, we minimize the L2 distance between the latent state st and the text features from the text encoder of video diffusion model T:\nLalign = MSE(st, T(t)).\nThe alignment stage enforces the consistency between the state variable and the textual conditions of the video diffusion model, which is pivotal for the stability of subsequent training given the distinction between the LMM and the video diffusion model.\nThe generative pretraining stage finetunes the LMM and the video diffusion model in a joint manner, to train the ability of the video diffusion model as the state decoder (Eq. 1), which translates the latent state st into explicit video observations ot. We adopt the same general video generation datasets and thus the same input sequence in Eq. (6) for this stage. Since the purpose of the MSE loss in the alignment stage (Eq. (7)) is only to provide an initialization, we discard it in the generative pretraining stage and substitute the latent state st for the original text condition of the video diffusion model. We supervise these two models with only the denoising target of diffusion models:\nLpretrain = ||e \u2013 \u00caD(ot,m, M, st, ot\u22121)||2,\nwhere m, ot,m represent the denoising timestamp and the noisy video observation, respectively. By training the video diffusion model with the latent state st as conditional input, we turn the video diffusion model into a photographer who films the latent world into explicit videos.\nThe world model training stage mainly incorporates the prediction of world dynamics dt into our Owl-1. It is based on the large scale pretraining of the second stage, which unifies the LMM and the video diffusion model as a preliminary Owl-1 capable of generating latent states st as comprehensive conditions for video clip generation. Now we further finetune the LMM and video diffusion model on a small amount of video data with longer duration and dense captions due to its scarcity. To achieve this, we change the input sequence of the LMM as:\nSeq = [I, t, so, oo, do, ..., st, ot, dt, ...],\nwhich incorporates the provided dense caption of each video clip as world dynamics dt. For supervision, we employ the next-token prediction paradigm and supervise dt with its textual ground truth in a teacher-forcing style. Also, we still keep the denoising target in Eq. (8) at this stage."}, {"title": "4. Experiments", "content": "4.1. Datasets and Benchmarks\nGeneal video generation datasets. We use two general purpose video generation datasets in the first two training stages. The WebVid dataset [1] comprises over 10 million captioned videos sourced from the internet, totaling approximately 52K hours of footage. This large-scale text-video dataset encompasses a diverse range of content across multiple domains, making it highly suitable for tasks such as video-text retrieval and video generation. We take around 400K randomly sampled videos from this dataset. The Panda70m dataset [8] includes 70 million videos with an average length of 8s along with their high-quality textual captions from an automatic captioning pipeline leveraging multimodal inputs and multiple cross-modal teacher models. We randomly sample 2M videos from this dataset.\nDense video captioning datasets. Due to the lack of datasets specifically focusing on the dynamics driving the progression of videos, we utilize dense video caption datasets as an alternative. The ActivityNet Captions dataset [6] contains 20K YouTube videos with 100K caption annotations and an average duration of 120 seconds. The majority of the videos contain more than three annotated events, each associated with corresponding time span and manually written sentences, averaging 13.5 words per annotation. The Vript dataset [36] represents a large-scale, fine-grained video-text dataset comprising 12K high-resolution videos and over 400K segments, which are densely annotated in the form of video scripts. The average lengths of video clips and captions are 11s and 145 words, respectively. We use the training splits of these two datasets.\nVBench. VBench [17] is a comprehensive and hierarchical benchmark framework, which dissects video generation quality into 16 specific and disentangled dimensions, such as subject identity inconsistency, motion smoothness, temporal flickering, and spatial relationship, each equipped with tailored prompts and evaluation methodologies. VBench possesses three key attributes: its comprehensive coverage of diverse video generation aspects, alignment with human perception, and insights into current models' performance across various dimensions and content."}, {"title": "4.2. Implementation Details", "content": "We use the Chameleon model [31] as the LMM, and the DynamiCrafter-1024 [35] as the video diffusion model. For the trainable parameters, we finetune the LMM using LORA [16] and finetune all the parameters of the video diffusion model. For the segmentation of videos, we divide each video into equal clips of 4 seconds as observations ot, and sample 2 frames from each clip as input to the LMM. We set the length of learnable state queries st as 128. For the alignment and generative pretraining stages, we train on a total of 2.4M videos from WebVid and Panda10m for 10K and 10K iterations, respectively. For the world model training stage, we train on a total of 20K videos from ActivityNet Captions and Vript for 1K steps."}, {"title": "4.3. General Video Generation", "content": "We evaluate our Owl-1 on two benchmarks of VBench [17], i.e. VBench-I2V and VBench-Long, for its ability of generating short and long videos, respectively. We report the results on VBench-I2V in Table 1, for which we generate 2s short videos. Our Owl-1 achieves comparable performance with state-of-the-art methods for short video generation, excelling at the aspects of motion smoothness, background consistency and temporal flickering. This proves the effectiveness of the state decoding mechanism which films latent state varibles into explicit video observations. However, we do observe a decrease in the score for dynamic degree compared with DynamiCrafter, which we attribute to the lack of training video data with high motion levels.\nWe report the results on VBench-Long in Table 2, where we generate videos of 7s long, similar to the other methods. Since the video diffusion model we use, i.e. DynamiCrafter, requires both an image and a text description as input to generate the first clip, we adopt an image diffusion model SD2.1-v [27] to generate the first frame of the video from the given text prompt. Our model achieves comparable performance with the open-sourced video generation models, e.g. OpenSora, on this benchmark. Similar to the results on VBench-I2V, our Owl-1 performs better at subject and background consistency, temporal flickering and motion smoothness, while its dynamic degree is lower than other methods, which could be improved through further training with videos of higher motion level."}, {"title": "4.4. World Model Based Video Generation", "content": "Given the current absence of benchmarks for evaluating world models in video generation, we assess the capabilities of our model through qualitative means. We provide the visualization results of generated long videos in Figure 5. We generate 3 scenes for a given prompt, and sample 2 frames from each scene. Every scene lasts for 8 seconds, and the whole video is 24 seconds long. When transitioning from one scene to another, we manually discard the image condition from the last frame and depend solely on the latent state variable as condition for geneartion, which is challenging because the latent state has to include information about the style and context of the previous video clips to generate the next clip in a consistent manner. We observe that Owl-1 is able to generate consistent long videos with reasonable dynamics anticipation. The video in the fourth row features a man engaged in gardening, where he utilizes tools to prune branches. The video we generated initially focuses on his hand movements and subsequently showcases the overall pruning effect, demonstrating a certain degree of logic. This reflects the modeling and prediction of the evolution of the world. However, we do notice that the predicted dynamics exhibit a certain degree of repetition, which we hypothesize is due to the inherent repetitiveness in the dense captions of the training video data. Even so, the videos generated by Owl-1 maintain good consistency across different scenes."}, {"title": "5. Conclusion", "content": "In this paper, we have proposed an Omni World Model (Owl-1) for consistent long video generation. Our Owl-1 approaches this task from the perspective of world model, which models the evolution of the world with a sequence of state variables. We have introduced a closed-loop state-observation-dynamics triplet, in which the latent states encode both current and historical information about the world and serve as comprehensive long-horizon conditions for video generation. Explicit video observations are then decoded from latent state variables with a video diffusion model. To drive the world evolution, we incorporate the anticipation of the world dynamics during the generation process, which is beneficial for the diversity of generated content. Furthermore, we have devised an effective mutli-stage training scheme for our Owl-1 to take advantage of the vast amount of short video data and only finetune on a relatively small amount of long video data which reflects the evolution of the world. Owl-1 shows impressive capabilities in generating long and consistent videos. The visualizations further validate Owl-1's ability to capture fine-grained details and generate videos with reasonable dynamics anticipation.\nLimitations and future work. From the evaluation and visualization results, we do notice some limitations of the current Owl-1, especially the decreased dynamic degree after fintuning the video diffusion model and repetitive world dynamics. Future work could investigate into these drawbacks and the scale up of our model on a large amount of high-quality video data with dense captions featuring the evolution process of the world. We believe the proposed paradigm for video generation world model is one of the approaches to realize multimodal general intelligence."}, {"title": "A. Additional Implementation Details", "content": "When finetuning Chameleon [31] as the LMM, We employ LORA [16] and set the rank of LoRA to 8, resulting in approximately 798M trainable parameters. Together with all the parameters from DynamiCrafter [35] as the video diffusion model, the total amount of trainable parameters is about 2B. We train the Owl-1 using 8 A800 GPUs with 80G memory, and the training time for the three stages is 1 day, 5 days, and 1 day, respectively."}, {"title": "C. Controllability over Scene Transitions", "content": "Due to the scarcity of high quality video data with varying temporal content and devoid of scene transitions, we adopt the datasets of dense video captioning as the training data for the world model training stage. However, these datasets, e.g. Vript [36] and ActivityNet Captions [6], often incorporate scene transitions in a long video, which poses challenge for the training process. To address this issue, we manually discard the concatenating image conditions when generating the next clip belonging to a new scene during training. This strategy also endows our model with the capability to perform controllable scene transitions. Similar to the training phase, we only need to omit the concatenating image conditions to transit into a new scene. When generating longer videos in Figure 6 and Fig. 4 in the main paper, we set the interval between scene transitions to about 2 short clips generated by the video diffusion model, resulting in the duration of each scene being about 4 seconds."}]}