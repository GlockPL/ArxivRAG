{"title": "REAL-TIME SUB-MILLIWATT EPILEPSY DETECTION\nIMPLEMENTED ON A SPIKING NEURAL NETWORK EDGE\nINFERENCE PROCESSOR *", "authors": ["RuiXin Li", "Guoxu Zhao", "Dong Wang", "Yannan Xing", "Yuya Ling", "Ning Qiao", "Dylan Richard Muir", "Karla Burelo", "Mina Khoei"], "abstract": "Analyzing electroencephalogram (EEG) signals to detect the epileptic seizure status of a subject\npresents a challenge to existing technologies aimed at providing timely and efficient diagnosis. In\nthis study, we aimed to detect interictal and ictal periods of epileptic seizures using a spiking neural\nnetwork (SNN). Our proposed approach provides an online and real-time preliminary diagnosis of\nepileptic seizures and helps to detect possible pathological conditions.\nTo validate our approach, we conducted experiments using multiple datasets. We utilized a trained\nSNN to identify the presence of epileptic seizures and compared our results with those of related\nstudies. The SNN model was deployed on Xylo, a digital SNN neuromorphic processor designed to\nprocess temporal signals. Xylo efficiently simulates spiking leaky integrate-and-fire neurons with\nexponential input synapses. Xylo has much lower energy requirments than traditional approaches to\nsignal processing, making it an ideal platform for developing low-power seizure detection systems.\nOur proposed method has a high test accuracy of 93.3% and 92.9% when classifying ictal and\ninterictal periods. At the same time, the application has an average power consumption of 87.4 \u03bcW\n(IO power) + 287.9 \u03bcW (compute power) when deployed to Xylo. Our method demonstrates excellent\nlow-latency performance when tested on multiple datasets. Our work provides a new solution for\nseizure detection, and it is expected to be widely used in portable and wearable devices in the future.", "sections": [{"title": "Introduction", "content": "The measurement of electroencephalogram (EEG) is a safe and non-invasive method for recording brain signals\nby attaching electrodes on the scalp [1], where the continuously recorded data can reflect the electrical activity of\nneurons and the electrical potential across the entire brain surface. As the characteristic patterns of EEG signals are\ndistinguishable at different neural states, brain activities can be analyzed and studied using EEG [2].\nEpilepsy is a common neurological disorder characterized by recurrent episodes of abnormal brain discharges, behavioral\nseizures, or abnormalities in sensation, emotion or consciousness [3]. Currently, there are more than 50 million epileptic\npatients in the worldwide. Thus, the diagnosis and treatment of epilepsy are of great social and economic value. By\ndetecting the EEG signals, the different degrees of abnormal discharge in epileptic patients can be diagnosed and\nmonitored. This can help to determine the type and location of epilepsy and to choose appropriate treatment options\nduring epilepsy diagnosis, and help to monitor treatment effect, adjust treatment plan timely and avoid unnecessary\ndrug side effects during epilepsy treatment. In recent years, with the development of computer and artificial intelligence\n(AI) technologies, EEG detection has been widely used in epilepsy researches. By analyzing and mining a large amount\nof EEG data, it becomes realizable to explore the pathophysiological characteristics of epilepsy and develop more\naccurate methods for the diagnosis and treatment of epilepsy [4, 5].\nReal-time EEG monitoring plays a crucial role in medical diagnosis by providing essential information about the health\nstatus of human bodies [6, 7]. This can aid doctors in timely disease detection and is vital for formulating and adjusting\ntreatment plans. Moreover, real-time monitoring of biological signals is essential in scientific research fields, such\nas neuroscience. Researchers can investigate the relationship between specific behaviors or cognitive tasks and brain\nactivity by using EEG signals. Given the time-consuming and labor-intensive nature of manual detection, an artificial\nintelligence model is urgently required to perform real-time monitoring while accurately identifying abnormal EEG\nsignals.\nArtificial neural networks (ANNs) are now widely used for solving problems in signal analysis, and have been widely\nemployed in numerous fields, such as speech processing, computer vision and natural language processing. [8].\nHowever, there are significant differences between traditional ANNs and real biological neural networks. Traditional\nANNs have inputs and outputs that are both real numbers, while the information transmission in the human brain is\nin the form of discrete action potentials or spikes. Spiking neural network (SNN) is the third generation of neural\nnetworks, and models the behavior of biological neurons, where information is transmitted in the form of discrete\nelectrical impulses, known as spikes. SNNs have shown promise in accurately identifying patterns in time-series data,\nsuch as EEG signals, due to their ability to efficiently handle temporal information. Our research focuses on real-time\ndetection of EEG signals using low-power neuromorphic chips which deploy spiking neural networks for inference.\nThese chips can quickly and efficiently detect epileptic signals, and trigger an appropriate response.\nOur study utilized the CHB-MIT dataset from Boston Children's Hospital [9] and the Siena Scalp EEG dataset from\nthe University of Siena [10]. The Wavesense neural network model [11] was combined with time-series analysis and\nDelta-Sigma coding techniques for detecting and identifying epilepsy signals based on spikes characteristics. The\nfeasibility and accuracy of this method were further verified by experiments on Xylo, a low-power neuromorphic\nprocessor for signal processing inference. Our experimental results demonstrate an ultra-low power consumption and\nhigh accuracy when deployed to the Xylo processor, suggesting that the great promise of this SNN-based epilepsy\nidentification method for serving as a new epilepsy diagnostic tool in the future. Our work can enhance the efficiency of\nepilepsy diagnosis, reduce the power consumption during signal acquisition, and provide guidance for the practical\napplication and dissemination of spiking neural networks."}, {"title": "Related Work", "content": "Historically EEG signal identification has been performed manually, which is a time-consuming and subjective process,\nand susceptible to human error. Although linear signal processing techniques such as time-domain, frequency-domain,\nand time-frequency analysis have become popular [12], they fail to accurately capture the nonlinear characteristics of\ncomplex electrical activity in the brain. However, with the development of nonlinear EEG data classification methods,\nartificial neural networks have become an essential tool for nonlinear analysis and identification of EEG signals.\nVarious neural network models have been explored, including spatiotemporal convolutional neural networks [13],\nfuzzy function-based classifiers [14], and long short-term memory recurrent neural networks [15]. Traditional ANNs\nare highly dependent on feature extraction, and the quality of feature extraction can greatly affect their classification\naccuracy. Therefore, past researchers have spent a lot of time and effort developing suitable feature extraction techniques.\nIn contrast, SNN models show potential for processing complex spatiotemporal patterns without the need for manual\nfeature extraction.\nAs early as 2007, Ghosh Dastidar developed an efficient SNN model for classification and epilepsy detection, which\nuses RProp as the training algorithm and achieved a classification accuracy of 92.5% [16]. In 2009, Adeli developed a\nnew multi-spike neural network (MuSpiNN) model and a new supervised learning algorithm called Multi-SpikeProp,\nwhich is used to train MuSpiNN, which can achieve complex EEG classification problem achieves a classification"}, {"title": "Spiking Neural Network", "content": "Spiking Neurons are a mathematical construct inspired by the dynamics and behaviour of biological neurons. Spiking\nNeurons receive inputs and communicate with other neurons through discrete binary events known as Spikes. Neurons\ntemporally integrate these signals in small dynamical systems representing synapses and neuron membranes, and when\na membrane state passes a configurable threshold, emit a Spike to communicate with other connected neurons. Similar\nto an ANN, layers of spiking neurons are connected to each other through linear weights, which effectively scale the\nstrength of inputs received by synapses [20]. Here we describe an LIF (Leaky integration and Fire) neuron [21], one of\nthe simplest spiking neuron models. The state of the neuron, known as the membrane potential Vm (t), evolves over\ntime depending on its previous state as well as its inputs [22].The equivalent circuit of a LIF neuron is shown in the\nFigure. 1. The dynamics of this state can be described as follows Equation 1.\nIn the equation, $RC \\frac{dV_{mem}(t)}{dt}$ represents the rate of voltage change across the capacitor, $-V_{mem}(t)$ represents the decay\nof the membrane potential due to the leakage resistance, and $I(t) R$ represents the voltage change caused by the input\ncurrent. Overall, this equation reflects how the membrane potential increases when the neuron receives input current\nand decreases due to the leakage effect when there is no input current.The working mechanism of the LIF neuron model\nis: when the membrane potential $V_{mem}(t)$ exceeds a certain threshold, the neuron generates a \"spike\" (i.e., \"fires\"),\nand then the membrane potential is reset to a lower value, simulating the discharge process of a biological neuron.\nFigure 2 provides a detailed illustration of this process. The input current $I_{app}(t)$ is the sum of weighted input signals\nfiltered by a set of synapses, where each input $x(t)$ is weighted independently by $w$, and can be positive or negative,\nand subject to synaptic filtering with time constant 7. The result is a spatially summed time series. The input current\n$I_{app}(t)$ travels to the neuron soma, which acts as a low-pass filter and integrates input over time, updating the internal\nstate variable Vm. The soma performs integration and applies a threshold to make a decision on whether to spike or not.\nAfter a spike is produced, the voltage Vm is reset to a value $V_{reset}$. Finally, the resulting spike is transmitted to the other\nneurons in the network, this network consisting of many similar LIF neurons is called a SNN. SNNs can adopt a wide\nrange of network architectures, similar to standard ANNs. Neurons in different layers are connected through synapses\nwith multiple dynamically adjustable weights, which transmit signals from the input layer to the next layer. Different\ntopological structures have different characteristics. Feedforward SNNs are composed of an input layer, one or more"}, {"title": "Xylo neuromorphic processor", "content": "Xylo is an application-specific integrated circuit (ASIC) that utilizes an all-digital approach for simulating spiking leaky\nintegrate-and-fire neurons with exponential input synapses. It is designed to be energy efficient and highly configurable,\nwith the ability to adjust synaptic and membrane time-constants, thresholds, and biases for individual neurons. Xylo\nsupports a wide range of network architectures, including recurrent networks, residual spiking networks, and other\narbitrary configurations. The overall logical architecture of Xylo is illustrated in Figure 3. It has up to 1000 digital\nLIF hidden neurons with independently configurable time constants and thresholds, and each neuron supports up to 31\nspikes generated per time-step. Xylo also has 8-bit input, recurrent, and readout weights with bit-shift decay on synaptic\nand membrane potentials. It supports one output alias per hidden neuron, one input synapse, and one output spike per\ntime-step in the readout layer. Furthermore, the Xylo ASIC allows for various clock frequencies, and the network time\nstep dt can be chosen freely. Overall, Xylo is a flexible and powerful architecture suitable for many applications [27].\nTo minimize power consumption and improve performance and reliability, the Xylo chip incorporates a low-power digital\ncircuit design. This chip features sparse recurrent weights, with a maximum of 32 targets per hidden neuron, reducing\nenergy and computation by connecting only a few neurons. Compared to dense connections, sparse connections also\ndecrease memory bandwidth requirements, computation time, and power consumption. To measure the aforementioned\npower consumption, the Xylo development kit seamlessly integrates an onboard current monitor, allowing users to\nsample real-time current data at a frequency of 1000Hz via an ADC (Analog-to-Digital Converter). The kit includes two\ndistinct power tracks: \"core\" and \"IO.\" The core power track covers the total power consumption of the Xylo frontend\nand processing core, including RAM read/write operations, logic circuit operations, event routing, and other essential\nprocesses. Conversely, the IO power track pertains to the chip interface power, primarily facilitating Serial Peripheral\nInterface (SPI) operations.\nThe Xylo chip employs an off-chip training and on-chip inference approach to learn data samples. Off-chip training\nrefers to the phase where we use an external computing platform to train our neural network model. During this phase,\nthe model learns features and weights through a large amount of labeled data, often involving the backpropagation\nalgorithm to optimize network parameters. Once the model is trained and the parameters are set, we convert these\nparameters (weights, biases, thresholds, etc.) into a format suitable for the Xylo chip. This may include quantization\nand encoding of the parameters to fit the hardware architecture of Xylo. In the on-chip inference phase, the converted\nmodel parameters are loaded onto the Xylo chip. The Xylo chip utilizes its digital spiking neural network to perform"}, {"title": "Architecture and Functional capabilities", "content": "Xylo is an application-specific integrated circuit (ASIC) that utilizes an all-digital approach for simulating spiking leaky\nintegrate-and-fire neurons with exponential input synapses. It is designed to be energy efficient and highly configurable,\nwith the ability to adjust synaptic and membrane time-constants, thresholds, and biases for individual neurons. Xylo\nsupports a wide range of network architectures, including recurrent networks, residual spiking networks, and other\narbitrary configurations. The overall logical architecture of Xylo is illustrated in Figure 3. It has up to 1000 digital\nLIF hidden neurons with independently configurable time constants and thresholds, and each neuron supports up to 31\nspikes generated per time-step. Xylo also has 8-bit input, recurrent, and readout weights with bit-shift decay on synaptic\nand membrane potentials. It supports one output alias per hidden neuron, one input synapse, and one output spike per\ntime-step in the readout layer. Furthermore, the Xylo ASIC allows for various clock frequencies, and the network time\nstep dt can be chosen freely. Overall, Xylo is a flexible and powerful architecture suitable for many applications [27].\nTo minimize power consumption and improve performance and reliability, the Xylo chip incorporates a low-power digital\ncircuit design. This chip features sparse recurrent weights, with a maximum of 32 targets per hidden neuron, reducing\nenergy and computation by connecting only a few neurons. Compared to dense connections, sparse connections also\ndecrease memory bandwidth requirements, computation time, and power consumption. To measure the aforementioned\npower consumption, the Xylo development kit seamlessly integrates an onboard current monitor, allowing users to\nsample real-time current data at a frequency of 1000Hz via an ADC (Analog-to-Digital Converter). The kit includes two\ndistinct power tracks: \"core\" and \"IO.\" The core power track covers the total power consumption of the Xylo frontend\nand processing core, including RAM read/write operations, logic circuit operations, event routing, and other essential\nprocesses. Conversely, the IO power track pertains to the chip interface power, primarily facilitating Serial Peripheral\nInterface (SPI) operations.\nThe Xylo chip employs an off-chip training and on-chip inference approach to learn data samples. Off-chip training\nrefers to the phase where we use an external computing platform to train our neural network model. During this phase,\nthe model learns features and weights through a large amount of labeled data, often involving the backpropagation\nalgorithm to optimize network parameters. Once the model is trained and the parameters are set, we convert these\nparameters (weights, biases, thresholds, etc.) into a format suitable for the Xylo chip. This may include quantization\nand encoding of the parameters to fit the hardware architecture of Xylo. In the on-chip inference phase, the converted\nmodel parameters are loaded onto the Xylo chip. The Xylo chip utilizes its digital spiking neural network to perform\nreal-time event-driven inference. Designed for energy efficiency and real-time processing, the Xylo chip is capable of\nexecuting complex signal processing tasks with very low power consumption."}, {"title": "Mapping and quantization", "content": "For deploying SNNs in Rockpool [28], the structure of an SNN is converted to a computational graph. Nodes in the\ncomputational graph contain parameters of a neuron model, such as thresholds, biases, etc.; high-level structures such\nas dense weights; as well as representing the connections between individual layers in the network. When a SNN\nis deployed to a Neuromorphic Chip, the topology of the neural network needs to be mapped to the actual physical\nlayout of the chip. This enables the implementation of the same computational process as the original SNN on the chip.\nThe mapping process converts the neuron graph to a form which matches the hardware architecture. In order to build\nthe hardware-equivalent configuration, neuron IDs, weights, inputs and outputs, and other required information are\nextracted from the graph. In addition, when performing calculations on Xylo, it is necessary to quantise floating-point\nparameters in the trained SNN to low-precision integer values. To convert weights and thresholds, one needs to find the\nabsolute maximum of all input weights to a neuron, and calculate a scaling factor such that this value can be mapped\nto a range of \u00b1128, and the threshold is scaled by the same scaling factor. After scaling, weights and thresholds are\nrounded to the nearest integer, which is the process of quantization."}, {"title": "Experiments", "content": null}, {"title": "Dataset", "content": null}, {"title": "Siena scalp EEG database", "content": "The first dataset used in this work is the Scalp Electroencephalography Database of the Department of Neurology and\nNeurophysiology at the University of Siena, Italy [10]. This database contains EEG records from 14 patients, including\n8 males (aged 25\u201371) and 6 females (aged 20\u201358). This experiment used Video-EEG monitoring under a sampling"}, {"title": "CHB-MIT scalp EEG database", "content": "The second dataset we used was collected by Boston Children's Hospital and contains 23 cases involving 22 children\nwith intractable epilepsy [9, 29]. These patients recorded their epileptic seizures after discontinuing antiepileptic drugs\nand undergoing monitoring for several days to evaluate the potential for surgical intervention. The subjects in this\ndatabase include 5 males and 17 females, aged between 1.5 and 22 years old. Cases chb21 and chb01 are data from the\nsame female subject, with a gap of 1.5 years between them."}, {"title": "Preprocessing", "content": "We begin by describing the preprocessing of the Siena Scalp EEG dataset. In this dataset, the arrangement of electrodes\nis placed according to the standard10-20 system. However, the order of electrodes differs between subjects. In addition,\nthe researchers also captured ECG signals from some subjects that were unrelated to the experiment. The common\nelectrodes of all subjects in the standard 10-20 system are kept, and the order of all electrodes is uniformly sorted. After\nthese operations, the electrode positioning is performed on the original signal once, and the electrode reference point is\nreselected. The data is re-referenced according to the average value of all signals using this function. Subsequently, the\ninfluence of the acquisition environment, such as power frequency interference in the acquisition system, needs to be\neliminated. In addition, studies have shown that EEG signals only contain valid information at 1\u201380 Hz. Due to the\nneed for a large number of repetitive preprocessing operations on the data in the study, the EEG data of all patients were\nprocessed with 1\u201380Hz band-pass filtering, and then 50Hz notch processing to eliminate power frequency interference.\nIn order to verify the data processing effect of filtering and notching, We show the comparison effect of power spectral\ndensity maps before and after filtering Figure 4A.\nIndependent components analysis (ICA) is widely used to remove ECG, eye movement, myoelectricity and head\nmovement signals in EEG, and the effect is remarkable [30]. ICA is a method of linear transformation based on"}, {"title": "Network", "content": "During the training process, the employed model is WaveSense [11]. This model takes spike time series data as\ninput and is constructed using a network architecture based on spiking neural networks. It draws inspiration from\nthe architecture of WaveNet [35]. The model adopts a stacked network architecture, which stacks multiple blocks\ntogether to gradually extract higher-level features of the input signals. Each block consists of multiple computational\nmodules, including dilated temporal convolutional layers, gated convolutional layers, and pooling layers. The dilated\ntemporal convolutional layer uses multiple synapse projections with different time constants to achieve dilated temporal\nconvolution. The gated convolutional layer uses gate mechanisms to regulate information flow, and the pooling layer\nis used to reduce the dimensionality of feature maps. These computational modules are used to extract the temporal\nfeatures of the input signal and transform them into classification or regression results. The WaveSense model achieves\naudio and signal processing tasks by converting the input signal into a pulse sequence and processing it with a series of\npulse neural layers. The model has been tested on multiple datasets and has achieved excellent performance. Advantages\nof the network include its strong generalization ability and robustness, suitability for various low-dimensional signal\nprocessing tasks, and ability to be implemented on neuromorphic hardware with low power consumption and high\nefficiency. The WaveSense model can be trained using the backpropagation algorithm and compared to other deep\nlearning models. In this experiment, we set the number of neurons in the hidden layer of the readout to 16, the number\nof synapses in the dilated layer of the block to 2, and the membrane potential time constant of neurons to 0.002. The\nthreshold for firing spikes of all neurons was set to 0.6. Figure. 6 shows the overview of the model architecture,"}, {"title": "Training and deploying", "content": "While experiencing reduced average latency, experiments demonstrate that optimal model prediction accuracy is\nachieved by segmenting the signal into 5-second samples. Consequently, the initial dataset undergoes partitioning into\nnumerous 5-second trials, each accompanied by a corresponding label. These trials are then processed to create spike\ntime series data, which is fed into the network. The processed samples are randomly split into training and testing\ndatasets at a 4:1 ratio, trained for 150 training epochs, a learning rate of 0.0005, and an output dimensionality of 2.\nBack Propagation Through Time (BPTT) [36] is used to train the SNN, and the peak current of the neurons on each\nsegment is calculated by extracting the synaptic current of the output layer. The prediction is made by choosing the\nneuron with the highest peak current, and cross-entropy loss [37] is calculated with the label to obtain $L_{CE}$. The\noptimization technique employed is Adam[38]. It is noteworthy that the model is intended to be used in streaming\nmode, which necessitates an appropriate loss function. The activity of LIF neurons may change substantially during\nlearning, resulting in either a lack of spikes or excessive energy consumption in neuromorphic implementations [39].\nTo maintain sparse activity and limit the activity of these neurons, an activity regularizer term is included in the loss\nfunction. The final loss function is given by Equation 2:\n$L = L_{CE} + \\sum_{i} \\sum_{t} \\sum_{N_i} \\sum_{N_i} \\frac{\\Theta(N_i-l)^2}{T. N_{neurons}}$\nThe activation loss is determined by the network's population size $N_{neurons}$ and the total number of excess spikes\nproduced in response to an input of duration T time steps. The excess spikes are those that exceed a certain threshold l,\nand are summed over all neurons $N_i$ and time bins t. where $\\Theta$ is the Heaviside step function.\nThe network performance was evaluated using four evaluation criteria. Accuracy measures overall correctness,\nsensitivity measures correct positive identification, specificity measures correct negative identification, and F1 score\nconsiders both precision and recall for accuracy assessment. The model parameter curve and loss change curve during\ntraining are shown in the Figure 7, Since the results on the test set can better evaluate the generalization ability of the\nmodel on unseen data, we only show the curves of the test set. Once training is finished, the network is deployed onto\nthe neuromorphic chip Xylo. This process primarily involves the mapping and quantization techniques mentioned\nearlier [28]. Afterwards, the original data is imported into the Xylo processor and the model prediction results are\nobserved."}, {"title": "Results", "content": null}, {"title": "Network performance", "content": "We visualized the above parameters. Additionally, the loss value was observed during the iteration process, and the\ncurves are shown in the figure. Since the results on the test set can better evaluate the generalization ability of the model\non unseen data, only the curves of the test set are shown. The model parameter curves corresponding to the two data\nsets are depicted. In both datasets, the following measures were obtained: Accuracy of 93.3% and 92.9%, sensitivity of\n90.4% and 89.7%, specificity of 96.7% and 90.1%, and F1 score of 91.2% and 92.3%. However, it is worth noting that\nwhen the model is deployed on the Xylo processor, a certain loss in accuracy rate relative to the simulation will occur.\nAfter the final test on the two data sets, the accuracy can reach 89.87% and 88.62%.\nIn deep learning, model latency refers to the time interval between when a model receives input data and when it outputs\nresults. Model latency is critical for real-time applications [40], in this application, for EEG monitoring of epilepsy\npatients, the model must quickly detect epilepsy signals and sound an alarm in a short time. Therefore, evaluating the\nlatency of the SNN model in this experiment can help determine its usability and practicality in real-time applications,\nas well as optimize the real-time performance of the model.\nThe SNN was deployed on the Xylo processor, and measurements on delays were performed using the CHB-MIT\ndataset. Each time step was set to 0.5 seconds, and it was found that the majority of 5-second epileptic signals were\ndetected within 0\u20131 seconds, with a median delay of 0.5 seconds. Favorable results were also achieved using the Siena\nscalp dataset with a delay of 0.75 seconds. The visualization of our findings is presented in Figure 8.\nIn order to fully showcase the methods and implementation of our research, we have publicly released the complete\nsource code of the project on GitHub. The code for the project related to the Siena scalp EEG database can be accessed at\nhttps://github.com/liruixinxinxin/siena_work, and for the CHB-MIT scalp EEG database, it is available at\nhttps://github.com/liruixinxinxin/epilepsy_complete. These repositories include detailed documentation\nof the algorithms and implementation procedures, aimed at facilitating further verification and replication of our research\nresults."}, {"title": "Real-time monitoring and power consumption", "content": "For power consumption measurement, we enable the power recording function by setting \"power_record = True\"\nin the code. actually calls a function related to the Samna interface, allowing the system to automatically read voltage\nand current information from the chip's registers. This method, which obtains precise data directly from the hardware,\nenables researchers to monitor and evaluate the chip's energy consumption in real time under various operating\nconditions. A real-time epilepsy detection experiment was conducted and the power consumption during the monitoring\nprocess was measured. As an example, The first seizure of patient No. 1 from CHB-MIT was used (show in Fig. 9). The\ndata sample reveals that patient No. 1 experienced an epileptic seizure from 2996 seconds to 3036 seconds. The model\nissued a red alarm at 2997.5 seconds, and ended the alarm at 3033.5 seconds. False alarms during this process may\noccur, which were addressed by carrying out post-processing in the algorithm. Following an analysis of the model's\naccuracy and the trade-off between false alarms and sensitivity, the algorithm triggers or cancels alerts only when four\nconsecutive test values are either 1 or 0. The purpose behind selecting this specific threshold is to minimize false alarms,\nwhile retaining a high sensitivity to epileptic seizures. The device exhibited a consistent average I/O power range\nof 80.2\u201395.5 \u03bcW, with an average consumption of 87.4 \u03bcW. The digital SNN core and control logic showed stable\npower consumption within 280.3\u2013292.1 \u03bcW, averaging at 287.9 \u03bcW. Our experimental results confirmed that the power\nconsumption of device is in the microwatt range, significantly lower than the power consumption reported in previous\nresearch involving traditional on-chip biosignal detection. Table 1 displays a comparison of power consumption and\nother metrics with similar works."}, {"title": "Performance comparison with existing work", "content": "By conducting several experiments and summarising the results in Table. 2, we observe that the overall performance\nof the SNN model is somewhat lower than that of the traditional artificial neural network. However, in this project,\nthe scale of the SNN model is relatively small, which enables the model to better simulate the neural system, more\neffectively process time series data, and use computing resources more efficiently. This is because in deep learning\nmodels, the number of weights and the power consumption of the model are usually related. The more weights there\nare, the greater the complexity and computational load of the model, which may require more computing resources\nand higher power consumption during deployment. In addition, the number of weights also affects the model's storage\nand data transmission requirements, which may also lead to higher power consumption. Therefore, using a very small\nnumber of weights in this model, the experiment results demonstrate that microwatt-level power loss can be achieved.\nCurrently, many related studies on power consumption measurement remain at the theoretical estimation stage[57, 58].\nIn contrast, we have measured and visualized the power values in real-time epilepsy detection. While some studies\nmay achieve similarly low power consumption, they often suffer from significantly higher latency[58, 59]. Our study,\nhowever, achieved a balanced outcome by ensuring low power consumption, low latency, and high accuracy."}, {"title": "Discussion and future work", "content": "In this research, a real-time epilepsy detection method based on SNN is proposed and implemented on a hardware\nplatform using the neuromorphic processor Xylo. This study presents several advantages and innovations. Firstly, a"}]}