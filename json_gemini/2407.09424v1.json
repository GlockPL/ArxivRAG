{"title": "TelecomGPT: A Framework to Build Telecom-Specfic Large Language Models", "authors": ["Hang Zou", "Qiyang Zhao", "Yu Tian", "Lina Bariah", "Faouzi Bader", "Thierry Lestable", "Merouane Debbah"], "abstract": "Large Language Models (LLMs) have the potential to revolutionize the Sixth Generation (6G) communication networks. However, current mainstream LLMs generally lack the specialized knowledge in telecom domain. In this paper, for the first time, we propose a pipeline to adapt any general purpose LLMs to a telecom-specific LLMs. We collect and build telecom-specific pre-train dataset, instruction dataset, preference dataset to perform continual pre-training, instruct tuning and alignment tuning respectively. Besides, due to the lack of widely accepted evaluation benchmarks in telecom domain, we extend existing evaluation benchmarks and proposed three new benchmarks, namely, Telecom Math Modeling, Telecom Open QnA and Telecom Code Tasks. These new benchmarks provide a holistic evaluation of the capabilities of LLMs including math modeling, Open-Ended question answering, code generation, infilling, summarization and analysis in telecom domain. Our fine-tuned LLM TelecomGPT outperforms state of the art (SOTA) LLMs including GPT-4, Llama-3 and Mistral in Telecom Math Modeling benchmark significantly and achieve comparable performance in various evaluation benchmarks such as TeleQnA, 3GPP technical documents classification, telecom code summary and generation and infilling.", "sections": [{"title": "I. INTRODUCTION", "content": "Recent advances of Large Language Models (LLMs) have attracted significant attention across different domains including telecommunications community. LLMs, such as GPT-4 [1], the Llama series [2]\u2013[4], the Mistral series [5], [6], and the Falcon series [7], have demonstrated remarkable capabilities in both natural language understanding and generation tasks. These LLMs have the potential to revolutionize the 6G communication systems. Unlike conventional Deep Learning (DL) techniques which are already widely applied in telecom domain, LLMs offer enhanced generalization capabilities and the emerging abilities, making them suitable tools for applications such as Radio Resource Management (RRM) in Open Radio Access Network (O-RAN). Additionally, LLMs can facilitate more sophisticated and adaptive communication protocols, thereby enhancing the efficiency and resilience of telecom networks.\nHowever, integrating mainstream LLMs into current telecom systems presents various challenges. Firstly, mainstream LLMs possess a large model size, hindering their direct deployment in wireless networks. Model compression techniques such as LLM-QAT [8], one bit quantization in BitNet [9] makes it possible to deploy mainstream LLMs on edge devices, e.g., mobile phones. Second, the long inference time of LLMs is unbearable to meet the requirement of Ultra Reliable and Low Latency Communication (URLLC) in beyond 5G networks. Taking the example of Vehicle to Everything (V2X) communication networks, it would be impossible for autonomous vehicles to wait for the generation completion of LLMs when taking crucial decision or transmitting important information to surrounding vehicles. Inference acceleration techniques and architectures on both system level and algorithm level [10] e.g., KV caching [11], FlashAttention [12] and Mixture of Experts (MoEs) [13] could largely increase the throughput of LLMs (tokens per second) to alleviate this issue. Finally, even physical challenges such as insufficient memory and low high latency are mitigated by the combination of various techniques, it remains a fundamental difficulty for LLMs to accomplish telecom-specific tasks in wireless networks due to a general lack of knowledge in telecom domain. Therefore, it would be natural to anticipate the existence of telecom-specialized or telecom-specfic LLMs, which is exactly the core problem this paper tries to tackle with. Before diving into the technical details of our proposed methods, we briefly review the recent advances including domain-specific LLMs, applications of LLMs in telecom and the the challenge of building telecom-specific LLMs."}, {"title": "A. Related Works", "content": "Domain-Specific LLMs: Although general-purpose LLMS demonstrate considerable capabilities in various tasks, their performance degenerates seriously for tasks which necessitates domain-specific knowledge, e.g., math, finance and Tele-com. Therefore, domain-specification will be mandatory to enhance LLMs's performance in these domains. For instance, BloombergGPT [14] is the first financial LLMs which is pre-trained on massive mixed data of financial and general corpora. Due to the high cost of pre-training LLMs from the scratch, FinGPT was proposed by fine-tuning general-purpose LLMs on 34 online curated data sources and through Retrieval Aug-mented Generation (RAG) [15]\u2013[19]. To enhance math problem solving capabilities of LLMs, Reinforcement Learning from Evol-Instruct Feedback (RLEIF) is proposed in [20] to train WizardMath which outperforms various general purpose LLMs and math-specialized LLMs in GSM8k [21] and MATH [22]. Other examples of domain-specific LLMs include MedPalM2 in medical domian, ChatLaw [23] and SaulLM [24] in legal domain, and Code Llama [25] in code generations. For more details of domain-specific LLMs such as training dataset and training methods, see examples in Table I.\nLLMs in Telecom: LLMs has gathered a lot of attentions in telecom domain recently. It is not surprising that LLMs can perform Natural Language Processing (NLP) tasks such as fault analysis [42] and Technical Documents (Tdoc) classifications [43] within the context of telecom. In [44], it is shown that two distinct nodes interact seamlessly for enhanced network optimization, demonstrating the potential of LLMs in varied telecom-relevant scenarios. Other examples of applying general purposed LLMs to use cases such as generating network config-uration [45], [46] and assisting FPGA-based wireless hardware development [47] showcase the versatility of LLMs in tackling domain-specific tasks within the telecommunications sector. Apart from existing applications, LLMs are envisioned to be key enabling techniques such as semantic communications [48]\u2013[50], intent-driven networks and self-evolving networks [51]. In [52], LLMs can be effectively deployed in end user and vicinity thorough edge training and edge inference. RAG is used in [53] to align the math and reasoning capabilities of multi-modal LLMs to wireless system design. A semantic-native LLM-based network is proposed in [54], where Telecom agents use LLMs to extract semantic information from raw data (observations), learn reasoning path towards decisions (responses), distillate knowledge from memory and exchange in a network to reduce the overall energy consumption of the system. These early explorations demonstrate an obvious paradigm shift compared to conventional Deep Neural Network (DNN) approaches.\nChallenges of building Telecom-specific LLMs: Despite the promising benefits of deploying LLMs in a centralized or decentralized manner in different networks, mainstream LLMs lack seriously knowledge and know how of telecom domain. Practical telecom networks operates in proto-cols and standards in pre-defined in Standards Developing Organizations (SDOs) such as Third Generation Partnership Project (3GPP), Institute of Electrical and Electronics En-gineers (IEEE) and International Telecommunication Union (ITU). However, SOTA models such as GPT-4 fails almost half part of the specification-related problem in TeleQnA [55], hindering the potential deployment in networks. Therefore, it is crucial to enhance current LLMs telecom knowledge in all aspects to be integrated into current networks seamlessly. To achive this taraget, there are (not limited to) several main challenges to build a Telecom-specific LLM:\n\u2022 Missing of Telecom text datasets: despite various amount of textual documents available in telecom domain, there are almost no open-source dataset available for pre-training or fine-tuning a telecom-specific LLM.\n\u2022 Technical documents dominance: Tdocs, specifications and research papers written in a professional manner dominates the overall text files, making the training of LLMs challenging.\n\u2022 Multi-modality contents: important concepts, scenarios and methods are usually explained and represented in different formats, e.g., figures, tables in standards and patents.\n\u2022 Frequent knowledge updating: regular release announce-ment from SDO and the rapid advancement of research publications make it difficult for LLMs, with their high training cost and long training time, to be updated in a timely manner.\nTherefore, instead of pre-training a Telecom LLM from scratch which is expensive and unprepared for the time being, it would be efficient and reasonable to consider adapting general-purpose LLMs to telecom domain under acceptable cost and training time, which is exactly the target of this paper."}, {"title": "B. Contributions", "content": "Our main contributions are summarized as follows:\n\u2022 We collect and build a telecom-specific pre-training dataset to build a telecom-specific LLMs.\n\u2022 We construct a high quality telecom-specific instruction dataset with instructions for diverse tasks in telecom domain.\n\u2022 We propose effective benchmarks to evaluate crucial ca-pabilities of LLMs such as such as knowledge query, math modeling, Tdocs classification, code generation and analysis in telecom context.\n\u2022 We propose a pipeline to adapt a general purpose LLMs to a telecom-specific LLM which consists of continual pretraining, instruction tuning and alignment tuning within the context of telecom.\n\u2022 We train a telecom-specific LLM which can be used for multiple downstream tasks in telecom domain.\nThe remaining of the paper is organized as follows. Firstly the methodology of adapting a general purpose LLM to a telecom-specific LLM is detailed in Sec. II. In Sec. III, we present our pipeline of building datasets for different stages of training. After that, we present the evaluation benchmarks to evaluate the key capabilities of an LLM for telecom-relevant tasks in Sec. IV. Then, our training setting and the main results obtained are in Sec. V and Sec. VI. Finally, we conclude the paper in Sec. VII."}, {"title": "II. METHODOLOGY", "content": "Our method of building telecom-specific LLM consists of three standard stages, namely, domain specific continual pre-training, instruction tuning and alignment tuning."}, {"title": "A. Telecom-Specific Continual Pre-training", "content": "Different from pre-training from the scratch for domain-specific LLMs such as BloombergGPT for finance and Galac-tica [33] for scientific domain, continual pre-training [56], offers an alternative approach in contrast which requires considerably less cost to specialize general-purpose LLMs, e.g., Code Llama [25] Deepseek Math [37] and SaulLM [24]. Similar to pre-training stage, the training objective is the causal language modeling, i.e., predicting the next token conditioned on previous tokens. Denote \\(x = (x_1,...,x_T)\\) a sequence of tokens representing a text input and \u03b8 the parameter of an LLM, the causal language modeling task is to minimize the negative log-likelihood loss expressed as:\n\\(L (x, \\theta) = \\sum_{t=1}^Tlog P (x_t|x_{<t}),\\)  (1)\nwhere \\(x_{<t}\\) denotes the token sequence before token \\(x_t\\)."}, {"title": "B. Telecom-Specific Instruct Tuning", "content": "During (domain-specific) pre-training, LLMs learn and ac-quire general knowledge but might not not excel in user interactions. Therefore, pre-trained LLMs should be further instruct tuned to follow instructions to interact with users and other LLMs through Supervised Fine-Tuning (SFT). With instruct tuning, LLMs are capable to follow instructions even for unseen tasks without explicit examples. Instruct tuned LLMs generally have better performance in zero-shot or few-shot scenarios and fewer refusals when responding to users' requests. For a given instruction dataset consisting of multiple instruction-response pairs: \\(I = \\{x^{(i)},y^{(i)}\\}_{i=1}^n\\) with instruction \\(x^{(i)}\\) and corresponding response \\(y^{(i)}\\), the training objective is to minimize the negative log-likelihood of a response conditioned on its instruction:\n\\(L (y^{(i)}, \\theta) = -\\sum_{t=1}^{y^{(i)}} log P (y_t^{(i)}|x^{(i)}).\\)  (2)"}, {"title": "C. Telecom-Specific Alignment Tuning", "content": "After supervised fine-tuning of pre-trained LLMs, then we need to further align the model with human preference. Reinforcement Learning with Human Feedback (RLHF) is proposed in InstructGPT to improve the helpfulness of LLMs [58]. However, RLHF requires preference data collected from human labeler and building reward model which is both costly and unstable during the reinforcement learning due to the existence of Kullback-Leibler (KL) divergence in the training objective and discrete nature of language generation. Instead, Direct Preference Optimization (DPO) [41] is proposed to mitigate the difficulty in RLHF. Instead of constructing an explicit reward function, a Bradley-Terry model [59] is ap-plied to reparameterize the reward so that the probability of preference of the ground truth and the generated answer is independent of the reward model, significantly simplying the training procedure. For DPO, only a simple binary preference dataset consisting of pairs of chosen and rejected samples, is required. The objective of DPO is formulated as:\n\\(L_{DPO} (\\pi_{\\theta}; \\pi_{ref}) = E_{(x,y_w,y_l)}[log \\sigma (\\beta log \\frac{\\pi_{\\theta}(y_w|x)}{\\pi_{ref}(y_w|x)} - \\beta log \\frac{\\pi_{\\theta}(y_l|x)}{\\pi_{ref}(y_l|x)})] ,\\)  (3)\nwhere \\((x, y_w, y_l)\\) denotes a preference pair, with x being the prompt. In this pair, \\(y_w\\) is the response preferred by the human labeler or LLM judge, and \\(y_l\\) is the less preferred response; \u03b2 is a parameter controlling the deviation from the base reference policy \\(\\pi_{ref}\\); \\(\\sigma(\\cdot)\\) stands for the logistic function. We select the reference model \\(\\pi_{ref}\\) as the SFT model we obtained in previous stage."}, {"title": "III. DATASET", "content": "Following our training method, we need to build three datasets, namely the pre-training dataset for continual pre-training, the instruction dataset for instruction tuning and the preference dataset for alignment tuning."}, {"title": "A. Pre-training Dataset - OpenTelecom", "content": "The pre-training dataset is collected mainly from web, which includes Telecom standards, research papers, books, patents, Question Answerings (QAs), wiki, and codes, as detailed in Table II. We developed the following two preprocessing pipelines according to the source of these data.\nTelecom Standards: 3GPP is the main SDO in the area of Telecommunication. The universal standards for 3G, 4G and 5G have been developed by 3GPP since 1999. 3GPP works with Tdoc contributed by companies during the development phase and produces technical specifications as a final output. The specification work is carried out in Technical Specifica-tion Groups (TSGs). There are three Technical Specifications Groups: Radio Access Network (RAN), system architecture (SA), and core network and terminals (CT). Each TSG consists of multiple working groups (WGs) focused on specific areas, ranging from radio access network specifications, core network specifications, service requirements and specifications, and architecture and protocols for mobile communication systems, to Quality of Service (QoS) and performance requirements, se-curity and privacy in mobile communication systems, interop-erability and compatibility requirements, network management and operation, and testing and certification procedures. We scrap the technical specifications, reports, and documents from release 8 to 19 on 3GPP FTP site. IEEE is another important SDO developing popular Telecom standards, such as WiFi and Ethernet. We scrap the standard documents of IEEE 802.11, 802.3, 802.15, and C95.1 series from IEEE Get program.\nWe utilize similar methods in [43] to preprocess the raw standard documents, including: 1) Parse the HTML tags in the text and return the text content without any HTML tags using BeautifulSoup; 2) Remove any URLs (web links) from the text: identify the regex pattern that matches URLs starting with either \"http\" or \"https\" and may include alphanumeric char-acters, special characters, and encoded characters. 3) Remove tables from the parsed HTML document using BeautifulSoup. 4) Divide each document into multiple text segments with dif-ferent number of words extracted from natural language toolkit (NLTK). This allows us to evaluate the model's capability of understanding technical descriptions in different lengths. 5) Remove headers, footers, captions, and pseudo codes. Also, we eliminate the references section and all the text afterward. 6) Remove Change Requests (CRs), drafts, templates due to their limited technical information.\nTelecom Content Filtering from General Dataset: Despite the fact that no open-sourced telecom-specific pre-training dataset exists, there are gigantic amount of telecom content in general purpose pre-training dataset such as C4 [60], Re-finedWeb [61] and Redpajama [35]. We select Rejama-1T to demonstrate how to filter telecom-relevant content from general dataset to take advantage of its categorical structure and also due to our hardware limits. RedPajama-1T is an open pre-training dataset consisting 7 subsets: Commoncrawl, C4, Github, Books, Arxiv, Wikipedia and StackExchange. The entire dataset contains 1.2 billion text tokens. In order to filter all telecom-relevant contents from open dataset, there are two steps in our framework: keyword filtering and deduplication.\nTo select Telecom-specific data from a general-purpose dataset, we collect and select a non-exhaustive 700 keywords from telecom textbook, section of definitions and abbreviations from open 3GPP technical specifications and etc. We propose 6 criteria for selecting a keyword so that it can effectively help us to filter telecom-specific content:\n\u2022 Domain Specificity: Keywords are closely associated with the telecommunications industry and theory. Positive Ex-amples: \"5G\", \u201cVoIP\u201d, \u201cMIMO\u201d and \u201cLTE Advanced\"; Negative Examples: \u201cNetwork\", \"Service\" and \"Platform\".\n\u2022 Frequency in Telecom Discourse: Keywords frequently appearing in telecom discussions and publications. Pos-itive Examples: \"Broadband\u201d, \u201cLTE\u201d, \u201cRoaming\" and \"Signal strength\"; Negative Examples: \"Connection\", \u201cSpeed\u201d and \"Improvement\".\n\u2022 Distinctiveness within Telecom: Keywords uniquely iden-tifying telecom technologies or services. Positive Exam-ples: \"Spectrum allocation\u201d, \u201cFiber optic\u201d, \u201cBeamform-ing\", \"Cellular handoff\"; Negative Examples: \"Data trans-mission\", \"Wireless\u201d, \u201cCommunication system\"\n\u2022 Authority and Standards: Keywords from telecom stan-dards and regulatory documents. Positive Examples: \u201c3GPP\u201d, \u201cIEEE 802.11\u201d, \u201cITU-T\u201d, \u201cE.164\"; Negative Examples: \"Software update\u201d, \u201cUser interface design\"\n\u2022 Timeliness and Emerging Technologies: Keywords reflect-ing the latest advancements in telecom. Positive Examples: \"Network slicing\u201d, \u201cSemantic communication\u201d, \u201cQuantum cryptography\", \"5G NR (New Radio)\"; Negative Exam-ples: Technologies no longer at the forefront like \"2G\", \"PSTN\", \"ISDN\".\n\u2022 Clarity and Avoidance of Ambiguity: Avoid vague or broadly used terms. Positive Examples: \"VoLTE\u201d, \u201cWi-Fi 6\u201d, \u201cSD-WAN\u201d, \u201cIPv6 adoption\"; Negative Examples: Broad or ambiguous terms like \u201cTransformation\u201d, \u201cEffi-ciency\", \"Fourier Transform\", \u201cConvex Optimization\u201d.\nMost keywords in our keyword list satisfy more than 4 criteria out of 6. Both the keywords and their abbreviations is included in the keywords list. However, to avoid introducing ir-relevant content due to the potential polysemy of abbreviations, we test our keyword list on a small portion of the entire dataset, and remove those abbreviations frequently introducing noise, e.g., DL (deep learning v.s. downlink) and SAP (Service Access Point v.s. Systems Applications and Products). For each data sample, we introduce two quantities to evaluate its relevance to telecom domain: number of telecom keywords (same keyword will only be counted once) and telecom keyword density. The density for a text of N words with a total number of M telecom keywords matches is M/log(N + 1). The reason for using the logarithm of the number of total words rather itself is to compensate those long documents. For example, a long text of 1000 words with 30 telecom keywords would be considered to be less relevant to telecom compare to a short text of only 100 words with only 1 telecom keyword which is an absurd. Besides, the number of total keywords is also an effective metric to avoid obtain false positive data samples in the final dataset.\nDeduplication refers to the process of removing redundant or repeated contents in a dataset. Deduplication is crucial for improving efficiency of pre-training [62] and for mitigating diversity decreasing [63]. For telecom-specific LLMs, this pro-cess is unavoidable since most standards or protocols are gener-ally approved by multiple Standards Developing Organizations (SDOs), e.g., technical specification of 3GPP and European Telecommunication Standards Institute (ESTI). Moreover, mul-tiple releases of some topics introduces redundant contents naturally. For the sake of simplicity, we directly apply existing deduplication tool in xorbits [64]."}, {"title": "B. Instruct Tuning Dataset - TelecomInstruct", "content": "To build an effective instruction dataset, it is widely shown that the diversity of different tasks in the dataset is more important than the number of instructions of same type. Our instruction dataset, referred as Telecom Instruct in what follows consists of different critical tasks in telecom domain:\n\u2022 Multiple-Choice Question (MCQ) Answering: Select all correct answers from a MCQ.\n\u2022 Open-ended Question Answering: Answer telecom-relevant question from standards, research papers or patents in an open-ended manner.\n\u2022 Technical documents classification: Classify text from different Tdocs into the related working group as docu-mented in the library of SDOS."}, {"title": "C. Alignment Tuning Dataset - TelecomAlign", "content": "After SFT on our telecom instruction dataset, we found that the SFT models have learned how to perform telecom-relevant tasks in general. However, we can still observe undesired response such as repeated generation, too short response and telecom-irrelevant content generation. Rather than collecting real human preference data which is costly and inefficient, we simply define our preference as to provide concise and accurate answer with minimum amount of information unless requested especially. Such preference is reasonable to reduce the latency introduced in a LLM-based communication system and aligns with concepts such as semantic communication in a naive sense. Other response preferences are also possible. Here we just demonstrate how to align LLMs's response with human preference for telecom-relevant tasks. Besides, such preference can be easily measured by comparing metrics like Rouge scores and length of the ground truth and LLMs' response. Therefore, our preference dataset is obtained by selecting instructions with low performance metrics. Pre-trained LLMs can be utilized to select preferred response when provided with clear preference description."}, {"title": "IV. EVALUATION BENCHMARKS", "content": "After continual pre-training, instruct and alignment tuning, a general purpose LLMs is adapted to telecom domain. We would like to evaluate the performance of obtained telecom-specific LLM in some critical tasks in telecom domain including both the academic and industrial applications. Unfortunately, there are few telecom-specific evaluation benchmarks available, e.g., TeleQnA [55]. Moreover, we believe that an LLM with extremely high accuracy in MCQ alone might not meet the requirements of practical applications such as the tasks we mentioned in the instruct tuning phase. Therefore, it would be mandatory to construct reasonable benchmarks to evaluate the capabilities of a telecom-specific LLM in various downstream tasks."}, {"title": "A. Telecom Question Answering", "content": "Multiple-Choice QA: Based on the pipeline of creating TeleQnA dataset, we utilize GPT-4 to generate the questions. We first clean the raw telecom-relevant texts such as ArXiv paper, Wikipedia pages, patents and textbooks by removing the references in case that the generated questions are related to any specific literature. After that we use two LLM agents to automate the generation process. The cleaned text is given to the first GPT-4 agent with the prompt illustrated in Fig. 2. After that, we feed the original text and the generated question into the second GPT-3.5 agent to validate the correctness of the generated question. If a correct answer is obtained, we will keep the question-answer pair, otherwise it is removed. Finally, we conduct human validation to ensure the correctness of generated question-answer pairs.\nOpen-ended QA: We utilize the same method to create an evaluation dataset of non-contextual QA, where all options are removed. In this benchmark, an LLM generates open-ended answer to a question, which is evaluated over the ground-truth answer using Rouge scores. It makes the evaluation close to the real QA use cases. The dataset is also used to align LLM with Telecom preferred responses during instruct and alignment tuning."}, {"title": "B. Telecom Standard Documents Classification", "content": "One key capability of a telecom-specific LLM is to under-stand and manage technical documents. There are three aspects why such knowledge is vital. Firstly, when envisaging an LLM (agent) acting as a decision maker for the network management, it is mandatory for an LLM to understand understand the specific scenario it is facing within the established standards and protocols. Second, the knowledge of the valid range of an action is essential. For example, the uplink transmit power control (TPC) command filed with reference to certain accumu-lated and absolute power tuning as defined in 3GPP TS38.213. Lastly, the knowledge of standards and protocols allows an LLM to predict the impact of its action and the reaction of other entities. In summary it is difficult for any LLMs to operate in practical networks if a deep and accurate understanding of the technical documents is missing. To evaluate these capabilities, a simple but efficient way to evaluate the basic knowledge of LLMs is to first identify the relevance of any text to a given Tdoc. Therefore we follow the same methodology in our previous work [43] to classify a random text extracted from a Tdoc into one unique working group of 3GPP: CT1, CT3, CT4, CT6, RAN1, RAN2, RAN3, RAN4, RAN5, SA1, SA2, SA3, SA4, SA5 and SA6. No context information will be further provided to LLMs. Despite the extreme simplicity of this task, as we will show later in numerical results, it is difficult for most mainstream general purpose LLMs to predict accurately working group in zero-shot setting."}, {"title": "C. Telecom Math Modeling", "content": "To undertake a telecom-related task, a critical capability for a telecom-specific LLM is to accurately model the pertinent environment or problem using telecom terminology and to formulate the task as rigorously defined problems expressed through precise mathematical equations. Requesting an LLM to perform system modeling and problem formulation requires both in-depth telecom knowledge and factual reasoning capa-bility. Therefore, we believe a benchmark capable of evaluating the math modeling capabilities of LLMs within the context of telecom provides an indispensable perspective to a compre-hensive evaluation of a telecom-specific LLM. Nevertheless, due to the flexibility of modeling (considering the myriad potential assumptions and extensive variety of existing models in the literature), it is challenging to directly evaluate the effectiveness of a system model or the problem formulation generated by LLMs. Hence, building a simple-but-effective benchmark is crucial to evaluate the problem modeling and formulation capabilities of an LLM.\nTherefore, we propose the following masked equations in-filling task for LLMs: for a research paper or any technical document, we first extract those paragraphs relevant to the system modeling and problem formulation. Then we mask some crucial equations for the modeling and problem formu-lation process, e.g., system model, channel model, coding and decoding schemes. This type of masked language modeling task is widely used for language understanding task, e.g., in BERT [65] and its variants and has been proven to be useful while equations are masked entirely here. A masked equation will be replaced by a special placeholder < MASK > to indicate that there is a missing equation. In the meantime, due to the uncountable possibilities of different notations, we try to avoid mask those equations related to definitions, referring other documents (typically defined inline). Finally, an LLM will be asked to recover these equations and the predicted equation would be compared to the ground-truth to obtain a score based on the quality of prediction. To further simplify the task, an LLM is merely asked to predict one equation at a time. The context, along with the ground-truth equations (rather than the predicted equations from previous rounds), is provided before the equation to be predicted.\nIt remains how to evaluate a predicted equation to its ground truth. We utilize MathBERT [66], a variant of BERT fine-tuned on a large dataset of math equations, to evaluate the average cosine similarity between the embeddings of the predicted equation and the ground truth. The cosine similarity of MathBERT is adapted to the operation trees of the equations and thus can be used as a semantic similarity in the sense of math structure. For a given equation represented by y and a predicted equation y given by an LLM, we define the MathBERT score as:\nscore (y, \\hat{y}) = max \\{ \\frac{cos (e (y), e (\\hat{y})) - cos (e (y), e (\\O)) }{1 - cos (e (y), e (\\O))}, 0 \\} \u00d7 100% (4)\nwhere e (.) is the embedding output of the MathBERT; cos denotes the cosine similarity between two embedding vectors; \\(\u00d8\\) represents an empty equation, i.e., an LLM returns nothing. The MathBERT is actually a normalized cosine similarity of range [0, 100%] w.r.t. the difference between the ground truth and the \"empty\" answer. This normalization is introduced since the cosine similarity between the ground truth and an empty answer is usually greater than 0.7. During our tests, a random generated equation can easily achieve a cosine similarity greater than 0.8. Therefore, we believe the raw cosine similarity metric of MathBERT is not aligned with common sense of human. See appendices for example demonstrations with fundamental equations in telecom domain."}, {"title": "D. Telecom Code Understanding and Generation", "content": "Practical telecom systems operate in codes written in dif-ferent languages. To understand and generate codes for a well-defined telecom task given in standards or protocols are vital for telecom-specific LLMs. Besides, code analysis and generation capabilities are important for an LLM to interact with existing simulators. To start with, we consider four common program-ming languages used in telecom domain: C, C++, Python and Matlab. Our Code evaluation benchmark include four tasks mentioned in our instruction dataset.\nOther standard code-relevant task, e.g., code fixing/editing is not included for lack of relevant open-sourced dataset. The data samples used to build our benchmark are selected following the same logic in the telecom content filtering process for pre-training dataset. We further select those code files with larger number of telecom keywords, number of copies and keyword density to ensure the quality of data and the relevance to tele-com domain. Then we extract telecom-relevant function blocks from these files. For code generation, we prompt GPT-40 to generate corresponding generation requests using the prompt illustrated in Fig. 3. The summary and analysis instructions can be obtained using similar prompts and we omit them here to avoid repetitions. For code infilling task, several successive lines in the middle of a code file are randomly replaced with a special placeholder < FILL >, the missing part of code is considered to be ground truth. Due to the complexity of telecom-relevant script, it is difficult to directly evaluate the effectiveness of the script similar to common benchmark such as HumanEval [67]. Instead, we utilize Rouge score to compare the similarity between generated script and the ground truth. Although being hard to judge the quality of generating code lines for code generation and infilling tasks, it provides a reasonable metric on the relevance of generating scripts in telecom domain rather the true effectiveness."}, {"title": "E. Telecom Instruct Following", "content": "Using LLMs to improve the productivity of Telecom system research, design, and development is a key application in Telecom industry. For example, it can help researchers to find solutions in the literature for a Telecom problem, or guide engineers to identify the key steps to implement a Telecom feature in the standard. In order to enhance TelecomGPT's instruct following capabilities preferred by Telecom users, we further develop the following two benchmarks:\n\u2022 General Instruction: We utilize GPT models to create a number of instruction tasks from standard documents, specifications, research papers, and patents. Example tasks include identify issues in a specified scenario, explain a technical feature in standard, find possible solutions of a research problem, and so on. The prompt used to generate the data comprises: 1) Description of the task, formats and rules to avoid referring to specific figures, tables, sections, etc.; 2) Some examples of the instruction tasks; 3) Text segments from the pre-train dataset. The output includes instruction of the task, input of context information such as scenario description, and response of the expected answer.\n\u2022 Protocol Instruction: We create a dataset to produce the protocol workflows defined in telecom standards follow-ing a human instruction. For example, an instruction of \"Identify the different types of APN/DNN supportedm for UE implementation capabilities.\u201d, the expected response created from standard is \"1. Verify the default APN/DNN setup; 2. Validate APN IDs for different purposes; 3. Align APN/DNN configurations; 4. Submit APN/DNN IDs\u201d.\nWe further conduct model and human validation over the cre-ated telecom instruct following datasets. This includes rephras-ing or removing ambiguous information referring context in the original pre-train data or not in the input filed."}, {"title": "V. TRAINING DETAIL", "content": "Due to the constraint of available GPU resources", "Pretraining": "We continue pre-train a general-purpose LLMs with the OpenTelecom dataset on 8 AWS ml.p4d.24xlarge instances with a training time of approximately 6 hours. For Llama3-8B", "reasons": "i) the pre-trained dataset of Llama-3 series contains 15 TB tokens", "Tuning": "We use QLoRA [68] to instruct tune a pre-trained LLM. The reasons for selecting QLoRA are as follows: i) QLORA is applied as a regularizer to avoid the catastrophic forgetting of fine-tuned LLMs; ii) Due to the rapid advance of the telecom domain"}, {"Tuning": "Similar to instruction tuning stage", "TP,\u201d \u201cTI,\u201d and \u201cTA\u201d, respectively. Finally, the best model obtained by{\n      ": "itle", "VI. RESULTS": "content"}, {"title": "A. Traning Performance", "content": "Continual Pretraining: Fig. 5 shows the training and eval-uation loss during continual pre-training on LlaMA2-7B. The evaluation loss first drop significantly then saturates gradually, suggesting a better domain adaptation in Telecom.\nInstruct Tuning: Fig. 6 shows the training and evaluation loss during instruct tuning on LlaMA3-8B. The notable eval-uation loss shows that base model gradually learn all types of instructions in our instruction dataset.\nAlignment Tuning: Fig. 7 shows the reward margin (differ-ence of reward) of chosen and rejected answers during align-ment tuning on LlaMA3-8B, demonstrating clearly a tendency that responses similar to chosen samples are preferred than those similar to rejected samples."}, {"title": "B. Evaluation Benchmark Performance", "content": "Telecom MCQ: In table III, the accuracies of SOTA LLMS and our LLMs in different training stage are demonstrated in different categories of TeleQnA benchmark. After instruct tuning of our Telecom Instruct dataset, there is an obvious in-crease of accuracy for all base models. Moreover, our telecom-specific models outpeforms the instruct tuned version of the base models and achieve a comparable performance compare to GPT-4 while the model size is notably smaller. After the alignment tuning stage, there is a slight accuracy drop as well. This decrease might be caused by the selection strategy of alignment dataset (both correct and wrong option follow the same format of response). Moreover, for each category, there exist models with significant accuracy increase showing the impact of fine-tuning. Nevertheless, the average accuracy for standard specifications is still limited for all tested LLMs since questions related to standard specifications contains techinical details such as the precise value of important qualities, the procedure of a defined scenario. For Llama2-7B, before instruct tuning, we add the one more continual pretraining stage. The effectiveness of continual pretraining is confirmed by an increase of accuracy by approximately 4%.\nTelecom Standard Classification: Table V summarizes the average accuracy of classifying 3GP Tdocs using SOTA LLMs and our telecom-specific LLMs on a test dataset with 2000 texts of all 16 working groups. With only 1000 data samples per working group used during instruct tuning stage,"}, {"title": "VII. CONCLUSION", "content": "In this paper, we propose a full pipeline to build telecom-specific LLMs. Our data processing pipeline can effectively se-lect telecom-relevant contents from general pretraining dataset. Then we can build instruction dataset and preference dataset to further instruct tune LLMs to perform various downstream tasks and align with human preference. Besides, effective benchmarks to evaluate LLMs's capabilities in telecom context were proposed and verified with experimental results. Models obtained by telecom-specific fine-tuning pipeline outperform the corresponding base models and the start of the art models with significantly larger model size. Due to the resource limits, our experiments remain in small scale. Pre-training from scratch with even larger dataset looks promising to build better telecom-specific LLM. Moreover, our models can only treat textual data which limit the potential of current framework. One promising future direction is to integrate other data modalities, especially radio signal."}]}