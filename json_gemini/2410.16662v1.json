{"title": "Visual Question Answering in Ophthalmology: A Progressive and Practical Perspective", "authors": ["Xiaolan Chen", "Ruoyu Chen", "Pusheng Xu", "Weiyi Zhang", "Xianwen Shang", "Mingguang He", "Danli Shi"], "abstract": "Accurate diagnosis of ophthalmic diseases relies heavily on the interpretation of multimodal ophthalmic images, a process often time-consuming and expertise-dependent. Visual Question Answering (VQA) presents a potential interdisciplinary solution by merging computer vision and natural language processing to comprehend and respond to queries about medical images. This review article explores the recent advancements and future prospects of VQA in ophthalmology from both theoretical and practical perspectives, aiming to provide eye care professionals with a deeper understanding and tools for leveraging the underlying models. Additionally, we discuss the promising trend of large language models (LLM) in enhancing various components of the VQA framework to adapt to multimodal ophthalmic tasks. Despite the promising outlook, ophthalmic VQA still faces several challenges, including the scarcity of annotated multimodal image datasets, the necessity of comprehensive and unified evaluation methods, and the obstacles to achieving effective real-world applications. This article highlights these challenges and clarifies future directions for advancing ophthalmic VQA with LLMs. The development of LLM-based ophthalmic VQA systems calls for collaborative efforts between medical professionals and AI experts to overcome existing obstacles and advance the diagnosis and care of eye diseases.", "sections": [{"title": "Introduction", "content": "Accurate diagnosis of ophthalmic diseases often relies on the comprehensive analysis of multimodal ophthalmic images, including color fundus photographs (CFP), optical coherence tomography (OCT), fundus fluorescein angiography (FFA), scanning laser ophthalmoscopy (SLO), anterior segment photographs and corneal topography, etc. However, interpreting ocular imaging information is time-consuming and experience-dependent in such an image-extensive specialty. Visual question answering (VQA) is an emerging interdisciplinary field that combines computer vision (CV) with natural language processing (NLP). In the past few years, VQA has been introduced in medicine and attracted extensive attention from researchers. [1] Medical visual question answering system is expected to understand free-form questions and generate corresponding answers related to the medical images after image feature extraction, question feature extraction, and feature fusion.[2]\nWhile VQA systems have made certain progress in the biomedical field,[3] research in ophthalmology is still in its early stages, limited to creating essential image captions and reports. [45] Despite this, the field shows great potential, suggesting broad future applications. VQA systems can be integrated into basic ophthalmology education, assisting laypeople, or inexperienced young trainees in identifying and understanding clinical images, and could serve as a knowledge assistant for human doctors, providing image-assisted information to improve diagnostic efficiency and reduce misdiagnoses.[67] Moreover, for patients, a well-developed ophthalmic VQA system could offer instant professional feedback, reducing frequent hospital visits, thereby alleviating financial burdens economically and lessening the pressure on public healthcare systems.\nDespite the significant potential of ophthalmic VQA systems, we still face several challenges during their development.[8] These include the need for accurately annotated multimodal ophthalmic image datasets to train models, the design of advanced frameworks and algorithms to understand and process clinical questions, and the importance of achieving harmonious interaction between artificial intelligence and human doctors.\nNotably, large language models (LLMs) have recently been introduced in medical field.[9] LLMs, such as MetaAI's Llama and OpenAI's GPT-4, have shown promising performance in question answering of eye conditions.[1011] Integrating LLMs into the VQA framework presents new possibilities for database construction, enhanced image understanding, and output optimization, marking a significant leap toward more intelligent VQA systems.[12]\nThis paper aims to provide the theoretical basis and potential applications of establishing LLM-based VQA systems in ophthalmology. We begin with an overview of key technologies of VQA systems, then summarize the latest progress and potential applications of VQA in the medical field, particularly in ophthalmology. Additionally, we propose potential strategies for optimizing the VQA framework with advanced technologies like LLMs. Finally, we discuss the challenges in developing LLM-based ophthalmic VQA models and their future directions."}, {"title": "Overview of VQA", "content": "VQA systems synergize advancements in image recognition, NLP, and machine learning to interpret visual data and answer related questions. This section will briefly introduce the core architecture of VQA models, understanding how they process and analyze visual inputs in conjunction with textual queries."}, {"title": "Current Progress and Potential Uses of VQA in Ophthalmology", "content": "Ophthalmic VQA datasets\nCurrently, most existing ophthalmological datasets consist of eye images with brief labels, and there is a scarcity of comprehensive ophthalmic VQA datasets. The DME VQA dataset, derived from the IDRID and eOphta datasets, encompasses both healthy and unhealthy fundus images. Each image is accompanied by a set of predefined questions, including inquiries about specific regions (e.g., \"Are there hard exudates in this region?\"), along with corresponding masks indicating the region's location.[36] OphthalVQA represents an initial effort to develop a multimodal VQA dataset. [11] It comprises 600 image-QA pairs across six imaging modalities: slit lamp, SLO, CFP, OCT, FFA, and ultrasound images. These VQA pairs cover a wide range of common questions related to modality recognition, disease diagnosis, examination, and treatment, thus serving as a valuable resource for training and evaluating ophthalmic VQA models.\nAdvancements in Ophthalmic Visual Question Answering\nClosed-form VQA: Closed-form VQA tasks are commonly regarded as classification tasks that aim to provide a limited number of answers. These answers are typically presented as predetermined options, such as a short list of multiple choices, yes/no responses, or numerical ratings. For instance, Tascon et al.[36] incorporated question relationships into their VQA model training and evaluated the model's performance in grading diabetic macular edema using fundus images. The ChatFFA model fine-tuned the BLIP framework with FFA images to answer multiple-choice and true/false questions, demonstrating strong performance in both automated and human evaluations.[37]\nFree-form VQA: Free-form VQA tasks facilitate responses in the form of phrases or sentences, enabling detailed and fine-grained answers without being restricted by predefined options. Early research on image captioning and report generation laid the groundwork for generating free-text descriptions. Initially, technologies such as ResNet-LSTM, CNN-BLSTM models, and Expert-defined Keywords were utilized to generate preliminary descriptive captions for ophthalmic images, enhancing the efficiency of diagnosis and disease management.[38-41] As NLP progressed, more complex VQA systems were able to automatically generate comprehensive reports from ophthalmic images,[42-44] thereby improving the efficiency of medical report writing for healthcare professionals.\nHowever, image captioning or report generation models can only generate fixed formats of image-text QA and fail to fulfill the requirements for complex question answering in smart clinics. Researchers began exploring the integration of LLMs to construct more human-like QA dialogue systems. OphGLM constructed a pipeline for diagnosing common ophthalmic diseases and lesion segmentation based on fundus images, successfully developing an ophthalmic-specific vision-language assistant by integrating visual capabilities into LLMs.[45] Additionally, FFA-GPT and ICGA-GPT systems, which combine image-to-text conversion models with LLMs to provide interactive image-based question-answering for FFA and ICGA images through a two-step approach.[1246] Another study leveraged a large number of image-QA pairs to develop a true bilingual VQA model based on FFA (ChatFFA), which has shown great potential in various tasks.[37]"}, {"title": "Potential Application of VQA in Ophthalmology", "content": "The successful application of ophthalmic VQA systems holds promise for benefiting both healthcare professionals and patients (Figure 1b).\nOphthalmic Education: Ophthalmic VQA systems serve as interactive learning tools for medical students and general practitioners, enhancing their understanding of ophthalmic diseases and management. These systems provide personalized answers based on diverse imaging, enabling learners to extract key points from ophthalmology textbooks and clinical guidelines. This facilitates exam revision and self-assessment for general practitioners.[47] While the interpretation of multimodal images is crucial in ophthalmology education, traditional training methods often provide limited exposure to typical cases. Consequently, medical students encountering real-world images may have numerous unanswered questions that ophthalmic specialists cannot promptly address. In such instances, students can upload their images and engage with the VQA system to access personalized learning opportunities.\nClinical Assistant: In real-world clinical practice, generating professional reports based on ophthalmic images is a highly specialized and time-consuming task, [48] particularly for junior doctors. VQA systems can assist junior doctors by providing accurate interpretations of ophthalmic images and generating automated preliminary reports. This assistance can lead to improved diagnostic accuracy, reduced diagnostic time, and faster decision-making processes. However, it's important to note that the outputs of the VQA system may still require modification by ophthalmologists to ensure accuracy and patient-specific relevance. Furthermore, the dynamic and interactive nature of the VQA system makes it a valuable resource for ophthalmologists in busy clinics, helping to minimize errors and enhance communication skills.\nPatient Consultant: Ophthalmic VQA systems have the potential to revolutionize patient consultations by enabling remote triage and streamlining the consultation process. Firstly, integrating the VQA system into mobile devices or hospital websites can provide patients in regions with limited medical resources quick access to professional guidance and assistance in scheduling appointments at nearby hospitals. This approach promotes a more rational distribution of ophthalmic medical resources and mitigates the issue of overcrowding at tertiary ophthalmic centers. [49] Secondly, deploying VQA systems in outpatient consultations can aid patients by addressing their pre-consultation inquiries, identifying key questions to be discussed during face-to-face appointments, and providing clear explanations of relevant reports and prescriptions in plain language after the visit. This helps minimize the need for multiple physical queues and face-to-face interactions with doctors, thereby resulting in decreased outpatient burden, enhanced patient satisfaction, and reduced economic and time costs related to unnecessary follow-up."}, {"title": "Utilizing LLM in Ophthalmic VQA", "content": "LLMs, such as ChatGPT and Llama, [50 51] are trained on extensive datasets encompassing diverse topics, showcasing advanced capabilities in understanding, generating, and interacting with human language. These models have already shown potential applications in various medical contexts, including assisting in the generation of streamlining clinical documentation, [52 53] enhancing patient communication,[54] and answering ophthalmic questions[55]. By integrating LLMs into the framework of VQA, the analysis and interpretation of ophthalmic images can be significantly enhanced (Figure 2).\nLLMs as Data Creators\nIn the field of ophthalmology, accurately annotated professional datasets are scarce but crucial for training effective VQA systems. Here, LLMs play a vital role as key data generators, enriching existing datasets or creating new ones by generating descriptive captions, synthetic patient inquiries, or detailed reports based on ophthalmic images. For instance, ophthalmic images and their corresponding reports, which are relatively easier to obtain, can be leveraged by LLMs to generate image-QA pairs, bridging the gap in dedicated VQA datasets.[37] Moreover, LLMs not only increase the quantity of training data for VQA models but also enhance their diversity, encompassing a wider range of eye diseases and scenarios. This is particularly beneficial for rare conditions like Stargardt disease and Leber hereditary optic neuropathy,[56] which are difficult to accumulate in the real world. The application of LLMs could facilitate the development of more robust and versatile VQA models.\nLLMs as Base Models\nRecently, general domain multimodal LLMs, such as LlaVA and GPT 4Vision, [28 57] which can process image data, have demonstrated commendable performance in VQA tasks on general images. However, their effectiveness when applied directly to ophthalmic images is suboptimal, with only 30.6% accuracy, 21.5% high usability, and 55.6% posing no harm. [11] Despite this limitation, LLMs, which have undergone extensive pretraining on a large scale of general images, possess a vast knowledge base and linguistic abilities, making them suitable as base models for further fine-tuning into specialized medical VQA systems. For example, models like LlaVA-Med have utilized large-scale biomedical image-text datasets extracted from PubMed Central. They have employed GPT-4 for prompt engineering and implemented novel fine-tuning methods to enhance the performance of the LLM LlaVA. This dedicated effort has resulted in the creation of a comprehensive language-vision assistant tailored for biomedicine.[58] Another notable model, Visual Med Alpaca, builds upon the LLaMA-7B architecture and is trained using diverse medical datasets sourced from the BigBIO repository.[59] Furthermore, Med PaLM M utilizes multimodal datasets comprising clinical texts, radiology and dermatology images, pathology slides, and genomics. It is built upon the PaLM-E model architecture and exhibits strong performance in various biomedical tasks.[60] Apart from fine-tuning, another approach to infusing ophthalmic expertise into LLMs is retrieval-augmented generation (RAG). For instance, ChatDoctor introduced online Wikipedia and manual datasets to enhance professional information integration, thus significantly improving the accuracy of responses.[61] Lewis et al. successfully combined pre-trained parameters with external knowledge for text generation.[62] Similarly, the RAG method was also explored to enhance the ophthalmic capabilities of Llama 2.[56] These explorations have yielded encouraging results in employing RAG technology to LLM-assisted VQA systems in ophthalmology.\nLLM as a part of the VQA pipeline\nLLMs can serve as a crucial part of the VQA pipeline, especially when it comes to optimizing free-form text outputs. This was demonstrated in FFA-GPT and ICGA-GPT systems. [12 46] These systems achieved this by combining LLMs with a fine-tuned image-text conversion model based on BLIP, enabling interactive QA after generating reports from FFA and ICGA images. This approach can enrich the output of the VQA pipeline by providing users with easily comprehensible language descriptions.\nLLMs as the Controllers\nLLMs can enhance VQA systems by acting like a human brain to schedule different modules to finish a task collaboratively. By combining LLMs with other Al models, these agents handle user requests and generate final answers. [63] When users ask questions, Al agents utilize LLMs for natural language understanding, along with image-text conversion models, to extract relevant information from images and texts and complete VQA tasks. This approach can enhance the precision and efficiency of vision-language information processing and leverage the strengths of diverse AI models, thereby enhancing the versatility and practicality of the VQA system."}, {"title": "Challenges and Future Direction", "content": "Despite the initial attempts at VQA in ophthalmology and the integration of LLMs into ophthalmic VQA systems, which have demonstrated great potential, we still face several challenges when it comes to real-world applications. Furthermore, the progress in this field has revealed several promising future directions that merit further exploration.\nVOA Datasets\nOphthalmology, as a distinct branch of clinical medicine, encompasses a vast array of specialized terminology and imaging techniques. There is a critical need to establish high-quality datasets that specifically focus on ophthalmology and include annotations for training VQA models in this domain. There are two effective ways to establish these datasets: expanding existing ophthalmic image datasets and collecting real-world dialogues. Firstly, existing pure image datasets and multimodal image-text datasets can be utilized,[64] including classification,[65-67] segmentation,[68 69] and report generation datasets [70 71]. While these datasets may not explicitly provide QA pairs for training, they serve as valuable resources that can be leveraged to expand these pairs. For instance, by leveraging the excellent generation capabilities of LLMs, we can generate additional QA pairs from existing structured reports, thereby enriching the VQA dataset.[37] However, it is important to note that due to the potential for hallucinations generated by LLMs and it can be challenging to cover all possible scenarios. Therefore, it is important to also collect real dialogues from clinical practice to further enhance the dataset.\nEvaluation Methods\nDue to the unique nature of medicine, the evaluation specifically designed for medical applications of VQA is particularly important. The current evaluation methods for VQA systems mainly consist of automated assessment and human evaluation. [72] Automated assessment is a widely used method that employs several benchmarks, including MultiMedQA, [73] PubMedQA, [74] MedMCQA,[75] USMLE, [76] among others. The common evaluation metrics can be divided into two types: classification-based metrics, such as accuracy, recall, and F1 score; and language-based metrics, such as\nClinical Application\nAlthough some initial ophthalmic VQA systems have demonstrated their capabilities in various scenarios, applying them to actual clinical environments requires addressing a series of practical issues. First, it is crucial to further optimize the VQA systems by fully leveraging the multimodal nature of ophthalmology. This involves exploring more effective ways to integrate and analyze different types of ophthalmic images and text data,[81] leading to more accurate disease diagnosis and management. Secondly, it is essential to improve the explainability of the VQA system. This will enable doctors and patients to understand the decision-making process of the model, thereby enhancing trust in the system's output.[82] Next, system integration and deployment need to be considered, particularly in relation to existing clinical workflows. For example, in eye disease screening, ophthalmic VQA combined with automatic imaging systems can be utilized for generating large-scale eye disease screening reports. Moreover, for telemedicine, ophthalmic VQA can serve patients in remote areas with limited medical resources. Finally, ethical considerations in the development of VQA-LLM are crucial.[83] Compliance with medical insurance regulations is paramount, and measures must be taken to ensure that these systems do not increase the workload on doctors. Furthermore, patient privacy and data security should be prioritized and carefully addressed.\nBy overcoming these challenges and exploring the aforementioned future directions, LLM-assisted ophthalmic VQA systems are expected to play a significant role in improving the accuracy of eye diagnoses, advancing medical education, enhancing patient experiences, and promoting global eye health."}, {"title": "Conclusion", "content": "VQA systems in ophthalmology, when enhanced with LLMs, have the potential to introduce a new era of precision, efficiency, and accessibility in medical practice and patient education. While developing specialized ophthalmology VQA systems poses challenges, such as the scarcity of VQA datasets and the complexities of adapting general Al models for medical-specific domains, the opportunities outweigh these barriers. We emphasize that widespread adoption of these systems in the medical field requires collaborative efforts from the global medical and Al research communities, coupled with rigorous clinical validation. By responsibly harnessing this potential, we envision a future where the obstacles to high-quality eye care are significantly reduced worldwide."}, {"title": "Contributors", "content": "DS developed the concept of the manuscript. XC, RC and PX drafted the manuscript. All authors have commented on the manuscript. DS and MH supervised the entire work. DS is the guarantor."}, {"title": "Funding", "content": "The study was supported by the Start-up Fund for RAPs under the Strategic Hiring Scheme (P0048623) from HKSAR, Global STEM Professorship Scheme (P0046113), and Henry G. Leong Endowed Professorship in Elderly Vision Health. The sponsors or funding organizations had no role in the design or conduct of this research."}, {"title": "Competing Interests", "content": "None declared."}]}