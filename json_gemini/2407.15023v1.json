{"title": "ViT LOS V2X: Vision Transformers for Environment-aware LoS Blockage Prediction for 6G Vehicular Networks", "authors": ["Ghazi Gharsallah", "Georges Kaddoum"], "abstract": "As wireless communication technology progresses towards the sixth generation (6G), high-frequency millimeter-wave (mmWave) communication has emerged as a promising candidate for enabling vehicular networks. It offers high data rates and low-latency communication. However, obstacles such as buildings, trees, and other vehicles can cause signal attenuation and blockage, leading to communication failures that can result in fatal accidents or traffic congestion. Predicting blockages is crucial for ensuring reliable and efficient communications. Furthermore, the advent of 6G technology is anticipated to integrate advanced sensing capabilities, utilizing a variety of sensor types. These sensors, ranging from traditional RF sensors to cameras and Lidar sensors, are expected to provide access to rich multimodal data, thereby enriching communication systems with a wealth of additional contextual information. Leveraging this multimodal data becomes essential for making precise network management decisions, including the crucial task of blockage detection. In this paper, we propose a Deep Learning (DL)-based approach that combines Convolutional Neural Networks (CNNs) and customized Vision Transformers (ViTs) to effectively extract essential information from multimodal data and predict blockages in vehicular networks. Our method capitalizes on the synergistic strengths of CNNs and ViTs to extract features from time-series multimodal data, which include images and beam vectors. To capture temporal dependencies between the extracted features and the blockage state at future time steps, we employ a Gated Recurrent Unit (GRU)-based architecture. We train and evaluate our proposed method on the DL dataset framework for vision-aided wireless communications (ViWi) and demonstrate its potential for predicting blockages in vehicular networks through simulations. Our results show that the proposed approach achieves high accuracy and outperforms state-of-the-art solutions, achieving more than 95% accurate predictions. This contribution is expected to advance the development of ultra-reliable and low-latency communication in vehicular networks.", "sections": [{"title": "I. INTRODUCTION", "content": "IRELESS communication has shown one of the fastest growth rates in the previous decades, with the commercial deployment of the fifth-generation wireless communication (5G) in 2020. 5G is a revolutionary technology that offers ultra-reliable and low-latency communications (uRLLC), as well as enhanced mobile broadband (eMBB) services. Researchers are now turning their attention towards sixth-generation (6G) wireless communication systems, which aim to offer \"connected intelligence\" instead of \"connected things\" in various sectors [1], [2], [3], [4]. The transition to 6G demands higher data rates and channel capacity, requiring the use of higher frequencies, including millimeter-wave (mmWave) and sub-terahertz communications [5] [6].\nHowever, these frequencies present new challenges, such as their vulnerability to blockage which results in signal-to-noise ratio (SNR) dips and frequent disconnections. Therefore, Line-of-Sight (LoS) and Non-Line-of-Sight (NLoS) predictions are vital for future high-frequency communication applications, where we can predict if the future connection between a base station and a given user will be blocked or not, especially in highly dynamic networks, such as vehicular networks, which are one of the most vulnerable applications to user disconnections.\nWith the development of 6G sensing and the availability of various sensor types, such as high-resolution cameras, LiDAR, radar, and acoustic sensors [7], [8]. These sensors facilitate the collection of diverse data types, including visual imagery, depth information, and acoustic signals, thereby providing a comprehensive understanding of the surrounding environment. This data represents a significant opportunity to enhance network management decisions, including the crucial task of blockage detection. Leveraging this multimodal data is essential for ensuring the reliability and efficiency of vehicular communication networks.\nThe vehicle-to-everything (V2X) technology, which enables vehicles to communicate with other vehicles, infrastructure, and road users, is a key enabler of intelligent transportation systems [9] [10]. The primary aim of V2X communication is to improve road safety, traffic efficiency, and driver experience by providing real-time information about road conditions, traffic flow, and nearby vehicles. However, V2X networks face significant challenges, such as high mobility, intermittent connectivity, and limited resources, which affect the performance of the network. The complexity introduced by these challenges will create highly intricate vehicular network scenarios, making it extremely difficult, if not impossible, to model such systems for LoS prediction using traditional probabilistic methods and optimization models [11], [12], [13], [14], [15]. Contextualization in V2X communication involves the integration and analysis of environmental, vehicular, and network contexts to enhance communication reliability and efficiency [16]. This approach is particularly relevant to LoS prediction solutions in V2X networks by understanding the context in which vehicles operate, including dynamic changes"}, {"title": "A. Background and Motivation", "content": "IRELESS communication has shown one of the fastest growth rates in the previous decades, with the commercial deployment of the fifth-generation wireless communication (5G) in 2020. 5G is a revolutionary technology that offers ultra-reliable and low-latency communications (uRLLC), as well as enhanced mobile broadband (eMBB) services. Researchers are now turning their attention towards sixth-generation (6G) wireless communication systems, which aim to offer \"connected intelligence\" instead of \"connected things\" in various sectors [1], [2], [3], [4]. The transition to 6G demands higher data rates and channel capacity, requiring the use of higher frequencies, including millimeter-wave (mmWave) and sub-terahertz communications [5] [6].\nHowever, these frequencies present new challenges, such as their vulnerability to blockage which results in signal-to-noise ratio (SNR) dips and frequent disconnections. Therefore, Line-of-Sight (LoS) and Non-Line-of-Sight (NLoS) predictions are vital for future high-frequency communication applications, where we can predict if the future connection between a base station and a given user will be blocked or not, especially in highly dynamic networks, such as vehicular networks, which are one of the most vulnerable applications to user disconnections.\nWith the development of 6G sensing and the availability of various sensor types, such as high-resolution cameras, LiDAR, radar, and acoustic sensors [7], [8]. These sensors facilitate the collection of diverse data types, including visual imagery, depth information, and acoustic signals, thereby providing a comprehensive understanding of the surrounding environment. This data represents a significant opportunity to enhance network management decisions, including the crucial task of blockage detection. Leveraging this multimodal data is essential for ensuring the reliability and efficiency of vehicular communication networks.\nThe vehicle-to-everything (V2X) technology, which enables vehicles to communicate with other vehicles, infrastructure, and road users, is a key enabler of intelligent transportation systems [9] [10]. The primary aim of V2X communication is to improve road safety, traffic efficiency, and driver experience by providing real-time information about road conditions, traffic flow, and nearby vehicles. However, V2X networks face significant challenges, such as high mobility, intermittent connectivity, and limited resources, which affect the performance of the network. The complexity introduced by these challenges will create highly intricate vehicular network scenarios, making it extremely difficult, if not impossible, to model such systems for LoS prediction using traditional probabilistic methods and optimization models [11], [12], [13], [14], [15]. Contextualization in V2X communication involves the integration and analysis of environmental, vehicular, and network contexts to enhance communication reliability and efficiency [16]. This approach is particularly relevant to LoS prediction solutions in V2X networks by understanding the context in which vehicles operate, including dynamic changes in the environment and vehicle behavior.\nThe introduction of advanced machine learning architectures, such as Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) [17], into the realm of vehicular communications opens new avenues for addressing these challenges. CNNs, known for their ability to capture spatial patterns and correlations [18], [19], can be instrumental in analyzing time-series data derived from vehicular sensors. Meanwhile, ViTs offer a complementary strength by adeptly handling image-based inputs, capturing intricate patterns and long-range dependencies that are often present in vehicular scenarios, such as V2X cooperative perception [20]. The synergy between these architectures can offer a more nuanced understanding of the environment, thereby enhancing the predictive accuracy of vehicular network management systems.\nMoreover, the dynamic nature of vehicular networks, with constantly changing environments and vehicular movements, necessitates the use of models that can adapt over time. Gated Recurrent Units (GRUs) [21] emerge as a suitable choice in this context, given their capability to model temporal dependencies and retain relevant information across time steps. Integrating GRUs allows for the effective capture of the temporal evolution of vehicular network states, providing a robust framework for making informed predictions about potential blockages.\nIn light of these considerations, the motivation for integrating CNNs, ViTs, and GRUs in our approach is driven by the need to effectively utilize the rich multimodal data available in 6G-enabled vehicular networks. By harnessing the strengths of these advanced machine learning architectures, we aim to address the complex challenge of blockage prediction, ensuring seamless communication in the intelligent transportation systems of the future."}, {"title": "B. Related Work", "content": "The aforementioned challenges have motivated numerous researchers to propose innovative solutions for blockage detection and prediction to address the LoS blockage issue in V2X networks. These proposed solutions make use of various approaches as well as different types of data, such as visual data from cameras [22] and tabular data, including channel and beamforming vectors [23]. Moreover, with the rapid evolution of artificial intelligence (AI) and machine learning techniques, researchers have employed advanced tools to predict LoS using different data sources.\nInitially, the solutions were based on the multi-connectivity approach, such as the one proposed by [24], which introduced a centralized multi-cell solution to improve the quality of connections in the face of disconnections caused by LoS blockages in Heterogeneous Networks (HetNets). The solution is based on keeping track of the connection link between the user and multiple base stations, where the centralized unit collects information from all the base stations to make a decision about the quality of the connection. However, these solutions detect the disconnections without anticipating them, which causes the user to get disconnected for a while until the base station recovers the connection link, causing communication latency. Therefore, more information is needed to allow the base station to predict the NLoS state and the optimal beamforming codebook to be used before losing the connection. For this reason, different tabular data sources and types were explored to extract the needed information for the machine-learning model to anticipate blockage.\nFor instance, in [12], the authors used a series of channel state information (CSI) measured by signal transmissions in an indoor office environment with a Recurrent Neural Network (RNN) model consisting of a Long Short-Term Memory (LSTM) block [25] to predict the connection state (LoS or NLoS). Also, the authors of [13] proved the ability of neural networks to detect the current link status using the sub-6GHz channel from the DeepMIMO dataset script [26], which generates a labeled sub-6GHz and mmWave channels set. On the other hand, authors in [15], provided a predictive solution to anticipate the stationary blockage for a single mmWave user using past observations of beamforming vectors, with an approach based on the GRU network [23]. The mentioned approaches achieved outstanding performance in predicting stationary blockages. However, in many practical scenarios, such as vehicular networks, the movement of the environment and the users can cause blockages to occur and disappear rapidly. In such cases, predicting only stationary blockages may not be sufficient to ensure reliable and efficient communication. Therefore, it is important to also consider the prediction of dynamic or non-stationary blockages\nDue to the limitations of previous solutions, the research community tried to explore other data types to reach better prediction performance for static and dynamic future blockages. As a novel solution, the authors in [22] proposed a blockage prediction framework using a publicly available vision-aided wireless communications (ViWi) framework dataset [27], which provides RGB images of different communication scenes and a tabular wireless data generator script. However, it considered only single-user communication settings.\nFinally, at the time of writing this paper, the state-of-the-art solution was proposed by [28], where the authors consider both tabular and vision data to form a multimodal data approach for multi-user communication settings. This approach is based on a centralized DL solution that takes advantage of the extracted information from both beam vectors and the images, first to detect the presence of a user, and then detect whether there is a possible future blockage or not using the position of the detected user. This solution has shown an impressive performance improvement in terms of detecting future LoS blockages. However, it is extremely dependent on the performance of the object detection component which is sometimes inaccurate and impacts the overall performance.\nThe methodologies referenced in previous research aimed to optimize the utilization of wireless data to forecast future LoS blockages. Despite these efforts, the inclusion of alternative data sources, such as images, became necessary. However, the integration of multiple data modalities poses challenges in extracting relevant information. Therefore, a more robust feature extraction architecture is needed to effectively handle multimodal data and enable the extraction of valuable information for a more comprehensive understanding of the environment, leading to more precise predictions of future connection states."}, {"title": "C. Contribution", "content": "In this paper, we present a multimodal vision transformer-aided predictive framework that anticipates future connection states using both images collected using an RGB camera and beam vectors collected from the perspective of the base station. Our approach focuses on developing an advanced AI-driven framework that addresses the challenges of anticipating blockages, including both static and dynamic obstacles. This framework efficiently leverages the rich information contained in multimodal data. In the development of our vision-aided blockage prediction method, we have employed the ViWi dataset [27], as it was the most comprehensive and pertinent dataset available at the time of conducting this research.\nTo establish a robust and efficient predictive framework, we introduce a unique time series DL architecture that incorporates a feature extraction component. Specifically, our framework utilizes a customized architecture of CNN and ViT to extract needed information from the input images and beam vectors. The proposed architecture allows the extraction of a comprehensive and detailed representation of the multimodal input data. This, in turn, produces more accurate and robust predictions compared to existing methods. Specifically, this paper's contributions are summarized as follows:\n\u2022 Our approach employs a time-series DL model, specifically a GRU neural network, to predict future LoS connection states. What sets our approach apart is the quality of the features extracted by our unique feature extraction architecture. By leveraging advanced DL techniques such as CNNs and ViTs, our framework embeds multimodal data efficiently and provides a substantial volume of pertinent information for the learning process.\n\u2022 Our proposed approach goes a step further by reducing dependence on the object detection component commonly used in vision-aided predictive frameworks. By doing so, we create a more resilient predictive framework that is less susceptible to potential object detection failures or misinterpretations. This reduction in dependency results in a higher degree of consistency and reliability in predictions, ensuring that the system performs well even in scenarios with challenging visual conditions or limitations in the computer vision component's performance. Through experiments and comparative analysis, we demonstrate that our solution leads to significantly improved prediction accuracy and stability compared to the state-of-the-art solution."}, {"title": "D. Organization", "content": "This paper is organized as follows. In Section II, we describe the system model considered in our study. In Section III, we formulate the V2X blockage prediction problem and provide a detailed analysis of the challenges. Section IV presents our proposed methodology, which includes feature extraction using ViT and classification using a GRU neural network. In Section V, we provide details on the implementation of the proposed solution and the obtained numerical results, including a comparison with the state-of-the-art works. Finally, in Section VI, we present our conclusion and highlight directions for future work."}, {"title": "II. SYSTEM AND CHANNEL MODELS", "content": "In this section, we provide a detailed description of the system and channel models used for our proposed V2X blockage prediction methodology."}, {"title": "A. System Model", "content": "In a vehicle network, the environment is formed by dynamic users and dynamic objects that could be detected as possible obstacles, as presented in Figure 1, with a base station equipped with a standard-resolution RGB camera.\nWe consider a highly dynamic vehicular network environment, where base stations use the beamforming codebook technique, which is helpful to target the desired user directly without losing signal energy in undesired directions. Beamforming vectors can provide information about the direction of the user, this provides the base station with 3D awareness, which is significantly helpful for both detecting the existence of the user, as well as tracking its location. Moreover, images serve as a data type rich with valuable information about the environment. Visual data is highly informative, offering essential details about the user, including their position, distance, velocity, size, and more.\nFormally, at a time step t' in a time interval of p steps and for a given user u, an observation X_u is defined as follows:\nX_u[t'] = {(A_u[t], b_u[t])}_{t=t'-p+1}^{t'}.\nwhere A_u is a series of image frames of the environment taken from the perspective of the base station, and b_u is the beam index of the corresponding optimal beamforming vector from the predefined beam codebook F while communicating with user u.\nOur vehicular network consists of an outdoor environment including small-cell mmWave base stations using beamforming. The decision about the optimal beam vector index while communicating with a user u is extracted from a predefined beamforming codebook F:\nF = {w_n}_{n=1}^{N},\nwhere N denotes the number of the beamforming vectors present in the codebook and w_n \u2208 \\mathbb{C}^{M\u00d71} is given by:\nw_n = \\frac{1}{\\sqrt{M}}[1, e^{jd \\sin(\\phi_n)},..., e^{j (M-1)d \\sin(\\phi_n)}]^T,"}, {"title": "B. Channel Model", "content": "In our research, the experimental results are derived from data samples obtained using the ViWi data generation framework, which employs the ray-tracing software Wireless InSite to simulate realistic mmWave channel conditions. The channel model adopted throughout this paper is a geometric mmWave channel model with L clusters. The communication system utilizes orthogonal frequency division multiplexing (OFDM) with K subcarriers and a cyclic prefix of length D. For each user u, we define the downlink channel with received signal Y_{u,k} as follows:\nY_{u,k} = h_{u,k}w_n^*X + n_k,\nwhere h_{u,k} \u2208 \\mathbb{C}^{M\u00d71} denotes the channel between the base station and user u at sub-carrier k, and w_n^* is the optimal beam vector that maximizes the received SNR at the receiver selected from the predefined beam codebook F, and the noise sample n_k follows a complex Gaussian distribution \\mathcal{N}_c (0, \u03c3\u00b2), given by [28]\nh_{u,k} = \\sum_{d=0}^{D-1} \\sum_{l=1}^{L} a_l e^{-j d \\omega_l} e^{-j p (d T_s - \\tau_l)} a(\\theta_l, \\phi_l),\nwhere a_l is the path gain including path loss, p is the pulse shaping filter, T_l is the delay, T_s is the sampling period, and a, \u03b8_l, \u03c6_l are the array manifold vector, the azimuth and elevation angles of arrival."}, {"title": "III. PROBLEM FORMULATION", "content": "In this section, we formally define the problem of V2X blockage prediction, which is the main focus of this paper. Let us consider a highly dynamic 6G vehicular network where vehicles are communicating with a base station (BS) through a mmWave channel. We assume directional communication with the aid of beamforming techniques where the vehicles and BS are equipped with a beamforming codebook containing a set of predefined beam vectors, and they adapt the beam direction according to the CSI to optimize the communication performance. The goal of the beamforming algorithm is to maximize the signal-to-interference-plus-noise ratio (SINR) by steering the main beam toward the intended receiver. Our goal is to predict the occurrence of these blockages and their duration in advance, hence allowing the vehicles and the BS to switch to a more suitable beam direction to maintain the communication link.\nConsidering a future interval in which we predict the blockage. Let f be the size of the future interval, and let l_u[t] \u2208 {0,1} be the LoS connection state indicator for the upcoming time step t, which is equal to 0 if an LoS is available and 1 if not (NLOS: blockage detected), and L_u[t'] = {l_u[t]}_{t=t'+1}^{t'+f} is the set of connection statuses in the future interval. The time step referred to aligns with the data generation framework of the ViWi dataset, where a 'time instance' does not specify an exact duration in conventional units like seconds or minutes. It reflects the duration used to generate each data point in the dataset.\nWe define the global future link status s_u as a binary indicator of whether we have an LoS blockage in the future f time steps, where:\ns_u[t'] = \\begin{cases} 0, & \\text{if } l_u[t] = 0, \\forall t \u2208 {t' + 1, . . .,t' + f} \\\\ 1, & \\text{otherwise} \\end{cases},\nwhich is equal to 0 if a LoS connection will be maintained for the next f time steps and 1 if not (a blockage is predicted to occur in the future interval). The principal objective is to design a DL framework capable of extracting the needed information from the multimodal data to maximize the prediction accuracy of future connection states \u015d_u for all users u in the set of users U:\n\\max \\sum_{u=1}^{U} [ P (\u015d_u = s_u | X_u)."}, {"title": "IV. PROPOSED SOLUTION METHODOLOGY", "content": "The proposed methodology described in Figure 2 aims to predict future obstacles in LoS vehicular communications between base stations and vehicles using multimodal data that includes images and beamforming vectors. This task is becoming increasingly complex due to the higher frequency bands that will be used in future networks, the high mobility of users, and the heterogeneity of the used data to make the decision.\nThere are mainly two challenges that need to be addressed to realize the full potential of this methodology. The first one is related to the type of data we are dealing with. In fact, since the data comes from different modalities, such as images and beamforming vectors, there may be differences in the data format, size, resolution, and type of information that could be extracted from each data type. This can make it difficult to integrate the data and extract meaningful features.\nThe other challenge, proven by the available state-of-the-art solution [28], is the high dependency on the object detection component, adversely affecting the overall performance with any small errors caused by the object detection model, which can limit the scalability of the methodology.\nTo address these challenges effectively, we propose an efficient feature extraction technique capable of handling the heterogeneity of the data and extracting essential information without relying on an object detection component. Additionally, we optimize the training process of the feature extraction component to accommodate different data types and utilize it as an embedding component."}, {"title": "A. Feature Extraction", "content": "To achieve efficient information extraction essential for the final decision-making process, we propose a novel transformer-based architecture for feature extraction from multimodal data for blockage detection in V2X, as shown in Figure 2. In this design, we apply CNNs for processing tabular wireless data and use ViTs to handle images. This hybrid approach takes advantage of the unique strengths of both architectures, facilitating effective information capture and extraction from diverse modalities. The feature extraction component is designed to extract useful information Z_u from the input data X_u, which consists of beamforming codebook vectors and camera images (A_u[t], b_u[t]). The proposed architecture consists of two parallel branches: one branch for CNN-based feature extraction and another branch for ViT-based feature extraction, then merged to provide the extracted information to the final predictive element. These two branches will be described in detail in the following subsections.\n1) CNN Branch: The CNN branch focuses on processing tabular wireless data by extracting essential features from beamforming vectors {b_u[t]}_{t=t'-p+1}^{t'}. Our CNN architecture is designed to capture intricate spatial dependencies within the beam vectors. As shown in Table I, it consists of 4 convolutional layers, with each layer equipped with an increasing number of filters, ranging from 32 to 256. A 3x3 filter size is"}, {"title": "B. Time Series Prediction", "content": "Time series forecasting has traditionally been dominated by linear and ensemble methods. These methods are highly interpretable and efficient for a variety of problems, particularly when coupled with feature engineering. However, with the advent of RNN in the 1980s, followed by more advanced RNN structures, such as LSTM in 1997 [25], and more recently, GRU in 2014 [23], DL techniques have enabled the learning of complex relationships between sequential inputs and outputs with limited feature engineering. These RNN techniques have enormous potential for analyzing large-scale time series in previously unfeasible ways.\nGRUs, a newer version of RNNs, show similar performances as RNNs and LSTMs while being significantly faster to compute. GRUs record long-term dependencies without any cell state by using reset and update gates. The update gate identifies how much past information must be kept, while the reset gate determines how much past information must be ignored. GRUs are often faster and require less memory than LSTMs since they require fewer tensor operations.\nIn our methodology, after extracting relevant features using our customized architecture, the next step is to pass these features through a time series computer vision classification component to make a binary decision.\nFor this purpose, we use a GRU as the temporal modeling component of our framework. Our GRU network consists of 2 stacked layers, each layer comprising 256 and 128 hidden units, respectively. The GRU architecture operates in a unidirectional manner, as bidirectional processing is deemed unnecessary for the given task due to the inherently causal nature of the LoS prediction task, where future information cannot influence past events. Hyperbolic Tangent (tanh) activation functions are applied, and a dropout probability of 0.3 is introduced to mitigate overfitting. The final layer consists of a fully connected layer with a single neuron, for the final binary classification, and the sigmoid activation function ensures that predictions fall within the [0, 1] range. Binary Cross-Entropy is chosen as the loss function, making it well-suited for our binary LoS prediction task. The GRU network serves as the decision-maker in our multimodal LoS prediction system, synthesizing both spatial and temporal information to predict future LoS states accurately."}, {"title": "V. IMPLEMENTATION AND NUMERICAL RESULTS", "content": "In this section, we elaborate on the vehicular communication environment that was used for data generation, as well as the process followed to prepare the data used in the proposed framework. We also describe the training of the feature extraction components and the time series computer vision classifier. Additionally, we provide numerical results and a detailed evaluation of the performance of the proposed methodology in order to evaluate its effectiveness in predicting future obstacles in vehicular communications."}, {"title": "A. Environment and Data Setup", "content": "In this section, we illustrate the framework used to build the needed environment and generate the wireless and image datasets. Specifically, we start with the definition of the scenarios from the ViWi dataset generator [27], followed by the detailed process for raw data generation, and finally, the data processing is applied to our raw data to provide the needed data sequences for the DL time series model.\n1) Environment Preparation: Our environment is built using the \"ASUDT1\" scenario from ViWi open-source dataset [27], which contains an outdoor multi-user environment. As illustrated in Figure 1, the scenario illustrates a busy downtown street, along with its various elements, such as cars, buses, trucks, skyscrapers, buildings, lamp posts, etc. The experimental setup consists of a pair of base stations with uniform linear array mmWave antennas. The base stations are placed 80 meters apart, facing opposite directions, and operating at a frequency of 28 GHz. A custom Matlab script is utilized to optimize the size and shape of the antenna array.\nIn addition, each base station is equipped with three cameras, which are strategically placed to capture the surrounding environment from different angles. The cameras are labeled from 1 to 6 and have overlapping fields of view. Specifically, cameras 3 and 4, situated on the left side of the first base station and the right side of the second base station, respectively, share almost identical fields of view. The use of multiple cameras with overlapping views enables us to capture different perspectives of the environment and ensures a more comprehensive and accurate dataset for our predictive model.\nThe ViWi data generation script enables the manipulation of the number of users and the number of scenes that the scenario has, where a scene is one frame in a video captured by the RGB camera. Each vehicle, equipped with a mmWave radio receiver, continuously moves in one direction in one of the four road lines with a variable velocity."}, {"title": "2) Data Generation Pipeline", "content": "As depicted in Figure 5, the data generation pipeline for this project begins with the ViWi raw data generator, which utilizes the parameters provided in Table II. The ViWi generator produces a 4-tuple data point, which includes an image of a selected scene, an mmWave channel for each user for each OFDM subcarrier for each of the M antennas at time step t, the coordinates of the user position in the visual field of the selected base station, and the connection state indicating whether there is a LoS connection or not.\nThe beam codebook vectors are then obtained using a uniform planar array (UPA) codebook generation Matlab script, which generates a beam-steering codebook design. Using the generated channels, we select the beam vector that maximizes the received SNR at the receiver from the codebook, resulting in the following optimal beam vector for a total of K subcarriers.\nw^* = argmax_{W \\in F} \\sum_{k=1}^{K} | (h_k)^T W |^2,\nFinally, using the LoS state of the future f steps, the pipeline constructs the target of the data and embeds the resulting dataset in the form of observation and target. The observation is a pair of sequences of images and beam vectors (A_u[t], b_u[t]), while the target is the future LoS state s_u. This comprehensive pipeline generates a large and diverse dataset to train and test the proposed solution for V2X blockage prediction."}, {"title": "3) Data Processing", "content": "To render the data suitable for a time series DL model, we need to form a dataset composed of sequences. Each data point consists of p successive pairs of images and beam vectors (A_u[t], b_u[t]) for t \u2208 {t' \u2212 p + 1,...,t'}, and the label s_u which is a binary state indicating whether we have a blockage in the next f time steps. Thus, we have a sequence of p pairs with information about the past p time steps, and the target s_u presenting the information about the future f time steps. In our experiments, we set p to 8 and f to 3.\nAfter creating feature columns, such as time-lagged observations, making roughly 7000 data points, we divide the dataset into three parts: training, validation, and test sets. It is noted that since our data is time-dependent, it is crucial to preserve the temporal sequence. Therefore, no shuffling has been applied to our dataset."}, {"title": "B. Implementation", "content": "1) Features Extraction: The feature extraction component of our multimodal LoS prediction framework plays a crucial role in processing the multimodal data, which includes both images and beam vectors. We adopt a two-branch approach, leveraging the power of both ViT and CNN architectures for extracting relevant spatial and structural features. For the ViT branch, we employ a pre-trained model that has been fine-tuned on the ImageNet dataset [30]. During fine-tuning, as shown in Table II, we set the learning rate to le-5 and utilized a batch size of 32 to efficiently adapt the model to our specific LoS prediction task.\nIn parallel, the CNN branch focuses on extracting features from the beam vectors. Training parameters for the CNN include a learning rate of le-3, a batch size of 64, and a dropout probability of 0.2 to prevent overfitting. We train the CNN for 1000 epochs, allowing it to learn complex spatial dependencies within the beamforming data effectively. The Adam optimizer is employed for efficient weight updates during training. These two branches operate synergistically to provide a rich set of spatial and structural features, ensuring that the multimodal data is comprehensively processed for subsequent LoS state prediction.\n2) Detection: In our framework, we employ a baseline GRU framework to detect blockages in the future f time slots using the extracted information from the prepared sequences of past information during the last p time slots. We trained the model using the parameters listed in Table II, and the results are displayed in the following section.\nAs a baseline solution, we implement the object detection approach proposed in [28]. This method proposes an object detection component to detect possible obstacles in the environment by extracting the coordinates of the bounding boxes of the detected objects and using only these coordinates with the beamforming vectors in the training process of the GRU model. For the implementation of this solution, we used the latest object detection You Only Look Once (YOLO) [31] model: YOLOv7 [32], followed by the same GRU model considered in our solution."}, {"title": "C. Numerical Results and Evaluation", "content": "1) Comparison to State-of-the-art: Using the described experimental setup", "28": "that uses an object detection component as a baseline solution.\nDuring the training process", "Evaluation": "To assess the precision of the model, we depict the confusion matrix, which provides a"}]}