{"title": "TSS GAZ PTP: Towards Improving Gumbel AlphaZero with Two-stage Self-play\nfor Multi-constrained Electric Vehicle Routing Problems", "authors": ["Hui Wang", "Xufeng Zhang", "Xiaoyu Zhang", "Zhenhuan Ding", "Chaoxu Mu"], "abstract": "Recently, Gumbel AlphaZero (GAZ) was proposed\nto solve classic combinatorial optimization prob-\nlems such as TSP and JSSP by creating a carefully\ndesigned competition model (consisting of a learn-\ning player and a competitor player), which lever-\nages the idea of self-play. However, if the com-\npetitor is too strong or too weak, the effectiveness\nof self-play training can be reduced, particularly\nin complex CO problems. To address this prob-\nlem, we further propose a two-stage self-play strat-\negy to improve the GAZ method (named TSS GAZ\nPTP). In the first stage, the learning player uses\nthe enhanced policy network based on the Gumbel\nMonte Carlo Tree Search (MCTS), and the com-\npetitor uses the historical best trained policy net-\nwork (acts as a greedy player). In the second stage,\nwe employ Gumbel MCTS for both players, which\nmakes the competition fiercer so that both players\ncan continuously learn smarter trajectories. We first\ninvestigate the performance of our proposed TSS\nGAZ PTP method on TSP since it is also used\nas a test problem by the original GAZ. The re-\nsults show the superior performance of TSS GAZ\nPTP. Then we extend TSS GAZ PTP to deal with\nmulti-constrained Electric Vehicle Routing Prob-\nlems (EVRP), which is a recently well-known real\napplication research topic and remains challenging\nas a complex CO problem. Impressively, the exper-\nimental results show that the TSS GAZ PTP out-\nperforms the state-of-the-art Deep Reinforcement\nLearning methods in all types of instances tested\nand outperforms the optimization solver in tested\nlarge-scale instances, indicating the importance and\npromising of employing more dynamic self-play\nstrategies for complex CO problems.", "sections": [{"title": "1 Introduction", "content": "In recent decades, the development of transportation infras-\ntructure has become vital to the socioeconomic growth of the\narea, but it has also brought many environmental issues at\nthe same time, especially carbon emissions [Fan et al., 2018;\nKucukoglu et al., 2021]. Path planning, such as the clas-\nsic Travel Salesman Problem (TSP) is a typical and simple\nmodel to reduce carbon emissions by minimizing the dis-\ntance traveled. In addiction, Vehicle Routing Problem (VRP)\nis usually viewed as a model of logistics service planning.\nAs a classic combinatorial optimization (CO) problem, it has\na lot of variants [Xu et al., 2022; Imran and Won, 2024;\nLi et al., 2021]. However, most of these variations are built\non traditional fuel vehicles. The consumption of fossil fuels\nremains one of the main contributors to large-scale carbon\nemissions [Li et al., 2024]. However, with the rapid devel-\nopment of electric vehicles and the construction of charging\nstations, electric vehicles have offered a viable substitute for\ntraditional fossil fuel vehicles [Conrad and Figliozzi, 2011].\nUnlike fuel vehicles that have a long range and possess\nthe significant benefit of widespread fuel station deployment,\ncharging electric vehicles is still a problem due to the fact\nthat electric vehicles usually have a range of about 500-700\nkm [Zhang et al., 2023a]. Consequently, it is more challeng-\ning to plan the delivery routes with electric vehicles, which\nleads to a new variant called the Electric Vehicle Routing\nProblem (EVRP) [Yang et al., 2025] [Moradi and Boroujeni,\n2025]. Recently, several methods have been studied to offer\noptimal solutions for EVRP, including adaptive large neigh-\nborhood search (ALNS) [Goeke and Schneider, 2015; Sistig\nand Sauer, 2023], variable neighborhood search (VNS) [Mao\net al., 2020], ant colony optimization (ACO) [Zhang et al.,\n2018]. These algorithms perform well on small-scale prob-\nlems and commonly find an approximate optimal solution,\nbut they have to adjust numerous parameters and can easily\nget stuck in poor local optimal.\nWith the advent of deep reinforcement learning (DRL),\nthe landscape of addressing complex optimization problems\nhas changed significantly to find new avenues for efficient\nand innovative solutions by training a deep neural network\nmodel. Although DRL could provide desirable solutions to\nboth small- and large-scale problems with lower computa-\ntional costs and outperform traditional techniques. Most of\nthem concentrate on fewer features and less constrained prob-\nlems, such as the TSP and VRP, which are basically lin-\near programming problems. In comparison, the research on\nmulti-constrained EVRP is more practical and more complex\nbecause it is a nonlinear problem that takes into account a\nwide range of real-world constraints."}, {"title": "2 Related Work", "content": "In recent years, Transformer has shown superior performance\nin the area of Natural Language Processing (NLP), particu-\nlarly machine translation and sequence-to-sequence tasks. A\nCO problem is essentially a sequence optimization problem,\nso their solutions can be described as sequential decisions.\nTherefore, TSP and EVRP, as typical CO problems, are also\nbeing studied using DRL methods. For example, [Wang et al.,"}, {"title": "3 EVRP", "content": "3.1 Problem Formulation\nThis section introduces the multi-constrained EVRP. Follow-\ning [Tang et al., 2023], we define it with a directed graph\nG = (V, E), V = CUDU\u00ca, where C = {1 + s, . . ., n+s}\nis a set of n customers, D = {0} represents the depot,\nf = {1, . . ., s} is a set of recharge stations, and E = {(i, j) |\ni, j \u2208 V} is a set of edges connecting two nodes. Each cus-\ntomer i with demand $c_i$ can only be served once. Our goal is\nto visit all customers and plan completed routes for electric\nvehicles to minimize the total distance or total energy con-\nsumption while satisfying all relevant constraints described\nin the following equations. As shown in Figure 2, electric ve-\nhicles start from the depot D with a maximum load capacity\nL and a maximum battery capacity Q, and then return to the\ndepot D after serving all customers. During the whole jour-\nney, we are supposed to ensure that the capacity of the electric\nvehicles is not less than 0, and the electric vehicles must visit\nthe recharge stations at the appropriate time to avoid being\nstranded while driving. We also take into account the maxi-\nmum serving time of the driver, $T_{max}$.\nmin $f(x) = \\sum_{i\\in V, j \\in V, i \\neq j}  c_{ij} x_{ij}$   (1)\ns.t.\n$\\sum_{j \\in V} x_{ij} = 1  \\forall i \\in C$  (2)\n$\\sum_{j \\in V} x_{ij} \\leq 1  \\forall i \\in \\hat{E}$  (3)\n$\\sum_{j \\in V, i \\neq j} X_{ij} - \\sum_{j \\in V, i \\neq j} X_{ji} = 0  \\forall i \\in C \\cup \\hat{E}$  (4)\n$T_0 = 0$   (5)\n$0 \\leq T_i \\leq T_{max},  \\forall i \\in C \\cup D \\cup \\hat{E}$  (6)\n$T_i + (g_i + t_{ij}) x_{ij} - T_{max} (1 - x_{ij}) \\leq T_j,  \\forall i, j \\in V, i \\neq j$   (7)\n$l_j \\leq l_i - c_i x_{ij} + L (1 - x_{ij}) \\forall i, j \\in V, i \\neq j$   (8)\n$0 \\leq l_i \\leq L  \\forall i \\in C \\cup D \\cup \\hat{E}$  (9)\n$u_0 = L$   (10)\n$e_j \\leq e_i - E_{ij} + Q(1 - x_{ij})  \\forall i \\in C  \\forall j \\in V, i \\neq j$   (11)\n$e_i = Q   \\forall i \\in D \\cup \\hat{E}$   (12)\n$0 \\leq e_i \\leq Q   \\forall i \\in C \\cup D \\cup \\hat{E}$   (13)\n$x_{ij} \\in \\{0,1\\}   \\forall i, j \\in V, i \\neq j$   (14)\nThe objective function of the multi-constrained EM-EVRP\nis depicted in Equation (1). Constraints (2) and (3) stipu-\nlate that each customer should only be served once, and the\ncharging stations can be visited multiple times separately.\nConstraint (4) indicates the route's continuity, while Con-\nstraints (5) and (6) ensure that the driver's serving time is\nno longer than $T_{max}$ and recount after returning to the de-\npot. Constraint (7) states that the driver's serving time is con-\nstantly updated when arriving at a site. Constraint (8) tracks\nelectric vehicle cargo, while Constraints (9) and (10) ensure\nthat electric vehicles leave the depot fully loaded and have a\ncargo capacity of L. Constraint (11) states that the battery\ncapacity of electric vehicles is constantly updated when they\narrive at a site, while Constraint (12) and (13) ensure that the\nremaining battery capacity is not greater than Q and is fully\ncharged when arriving at the depot or recharge stations. Con-\nstraint (14) defines the decision variables. The parameters\nused in these equations and their corresponding explanation\nare described in detail in Table 4, see the Appendix.\nThe multi-constrained DM-EVRP model shares the same\nconstraints (equation 2 to 14), but the objective function for\nmulti-constrained DM-EVRP is defined as (15).\nIn addition, we model the multi-constrained EVRP as a\ntwo-player game, then the Markov decision process (MDP)\ncan be defined as a tuple (S, A, R, P), consisting of state S,\naction A, reward R and state transition P, see the detailed\ndefinition of each element in Appendix.\nmin $f(x) = \\sum_{i\\in V, j \\in V, i \\neq j}  d_{ij} x_{ij}$ (15)"}, {"title": "3.2 Energy Consumption", "content": "We calculate the energy consumption of the electric vehicles\nbetween node i and node j as follows:\n$P_{ij} = (m_{ij} (a + gsin (\\alpha_{ij}) + C_r cos (\\alpha_{ij})) + S) V_{ij}$  (16)\n$S = 0.5 \\cdot C_a. \\rho . A. v_{ij}^2$ (17)\nwhere $m_{ij}$ represents the capacity of the electric vehicles be-\ntween the node i and node j, a is the acceleration of the vehi-\ncle and is set as 0 since we assume the speed is constant in our\nexperiments. Gravity is shown by g, air density is indicated\nby $\\rho$, resistance coefficient is indicated by $C_r$, aerodynamic\ndrag coefficient is indicated by $C_a$, and slope between nodes i\nand j is indicated by $\\alpha_{ij}$. After getting the mechanical power\n$P_{ij}$, the energy consumption is calculated as follows:\n$C_{ij} =\\begin{cases}\n\\frac{\\phi_a . \\phi_a . d . P_{ij} t_{ij}}{P_\\text{kw}} & P_{ij} (m_{ij}) \\geq O_{kw}\n\\\\ P_{ij} t_{ij} P_{ij} (m_{ij})\n< O_{kw}\n\\end{cases}$   (18)\nSince the effect of slope is a factor, the formula for energy\nconsumption is divided into two cases: electric vehicles need\nmore energy when traveling uphill and are allowed to charge\nfor power recovery when traveling downhill. While $t_{ij}$ rep-\nresents the travel time between node i and node j, $\\phi_d$ and $\\phi_r$\ndenote the charging and discharging efficiency of the battery,\nrespectively."}, {"title": "4 Methodology", "content": "4.1 Two-stage Self-play\nThe pseudo code of the original GAZ PTP method can be\nfound in the Appendix, Algorithm 2. This part presents\nour new GAZ method with Two-stage self-play strategy. As\nshown in Figure 3, on stage 1, the learning player uses Gum-\nbel MCTS to choose actions, while the competitor chooses\nthe action from the best historical policy network. In this\nstage, only the learning player can take the state of the com-\npetitor into consideration when in the expansion phase and\nupdate the node information through backpropagation. Af-\nter a period of training episodes, the learning player is unable\nto find a better trajectory because it has a great advantage\nto employ Gumbel MCTS in complex tasks, leading to un-\nbalanced competition. Therefore, we introduce the second\nstage. Both players use Gumbel MCTS, which ensures that\nthey try to find a better trajectory during the competition. It\nalso increases the depth of MCTS because both players can\ntake the other player's state into account and update the node\ninformation through backpropagation.\n4.2 Algorithm\nThe algorithm 1 illustrates our proposed training framework.\nIn each episode, we divide the two players into the learning\nplayer and the competitor, while we also divide the training\ninto two stages. In the first stage, the learning player chooses\nthe action according to the policy network $\\pi_\\theta$ based on GAZ\nMCTS [Danihelka et al., 2022], while the competitor chooses\nthe action greedily from the policy network $\\pi_{\\theta_B}$, which is the\nbest historical policy network of the learning player. The $\\pi_{\\theta_B}$\nonly updates periodically in the arena mode when the out-\ncome of greedily rolling out $\\pi_\\theta$ is better than $\\pi_{\\theta_B}$. Our aim is\nto train the model to converge quickly and save computational\nresources simultaneously due to the competitor does not em-\nploy Monte Carlo simulations and Gumbel MCTS only needs\nfewer simulations. After n episodes of training, the learning\nplayer performs much better than the competitor.\nTherefore, to ensure that the learning player learns stronger\ntrajectories, in the second stage, each player uses the Gum-\nbel MCTS based policy network $\\pi_\\theta$ and $\\pi_{\\theta_B}$ to select ac-\ntions. Our proposed Two-stage Self-play strategy can make\nthe learning player constantly compete with an opponent with\nsimilar playing strength so that the learning player can learn\nsmarter trajectories from games. In each episode, there is a"}, {"title": "4.3 Network Architecture", "content": "The policy and value network are based on the Transformer\narchitecture and the Transformer block is a little different\nfrom the Vanilla Transformer block, as shown in Figure 4.\nWe employ batch normalization (BN) before the Multi-Head\nAttention (MHA) and add gate aggregation [Xu et al., 2022]\nafter MHA and feedforward network (FFN) to replace the ad-\nditive aggregation. For the policy network, we use a pointing\nmechanism based on state attention to compute the probabil-"}, {"title": "5 Experiments", "content": "5.1 Validation on TSP\nIn order to compare the performance of our proposed TSS\nGAZ PTP and the original GAZ method, we first performed\nexperiments on TSP instances with 20, 50 and 100 nodes,\nwhich are also tested by [Kool et al., 2018] and [Pirnay et al.,\n2023]. The coordinates for each instance are sampled from\n[0,1]2. For problems of different scales, we use the same\nhyper-parameter settings based on GAZ PTP [Pirnay et al.,\n2023]. As shown in the Tabel 1, the experimental results\ndemonstrate that our method achieves the best performance\ncompared to other historically best learning-based methods\non TSP problems.\n5.2 Extension to EVRP\nSince the advantage of our TSS GAZ PTP method on TSP has\nbeen validated, our aim is to extend the method to variable-\nstep problems. Therefore, we further conducted experiments\non multi-constrained EVRP instances with n = 10, 20 and\n50 nodes, each category consists of 512 different instances,\nwhich is the same as [Tang et al., 2023]. For example, in-\nstances with 10 and 20 customers have four recharge stations,\nwhile instances with 50 customers have eight. Both customer\nsites and recharge stations are uniformly distributed in the\n[0, 100]2 kilometer area, and the depot is randomly distributed\nin the [25, 75]2 kilometer area. The demand of each customer"}, {"title": "5.3 Baselines", "content": "We compare the proposed TSS GAZ PTP framework with the\nfollowing methods.\n\u2022 Gurobi: A commercial optimization solver.\n\u2022 ACO: An improved ant colony algorithm based meta-heuristics to solve EVRP. [Zhang et al., 2018]\n\u2022 ALNS: Adaptive large neighborhood search algorithm,"}, {"title": "5.4 Results on EVRP", "content": "Our techniques are implemented on two NVIDIA GeForce\nRTX 4090 GPUs and an Intel i9-14900K CPU running at\n6.00 GHz. With a total of 50k training episodes, after the\nlight-weight test, we divided our training into two stages: 20k\nand 30k, with a simulation budget of 100 since [Tang et al.,\n2023] and [Kool et al., 2018] train on 128M trajectories. The\nfollowing are the training durations for certain problems: 15\nhours for C10-S4, 40 hours for C20-S4, and 120 hours for\nC50-S8.\nTo present the training curve, we select the category C10-\nS4 EM-EVRP as an example and present the results in Fig-\nure 5. We see that along with training, total energy consump-\ntion gradually decreases. For our TSS GAZ PTP method,\nSOC consumption drastically decreased after 20K episodes,\ndemonstrating the efficacy of the second training phase. The\ntraining curve eventually converges, indicating that, despite\nsporadic oscillations during the training phase, our approach\nhas learned the best consistent policy.\nIn addition, we conducted convergence analysis experi-"}, {"title": "6 Conclusion", "content": "In this paper, we proposed TSS GAZ PTP, a novel DRL\nframework inspired by GAZ, for solving multi-constrained\nEVRP. Unlike reconstructing network architecture or adding\nstate feature modules for specific tasks, our aim is to design\na new training strategy based on self-play that can be applied\nto more complex and general tasks and enhance the diver-\nsity of training data. Our training strategy ensures that the\nplayer continues to explore a smarter trajectory based on self-\nplay and provides an efficient way to eliminate local optimal.\nThe experimental results show that our method outperforms\nboth current state-of-the-art DRL methods and heuristic al-\ngorithms, especially in large-scale instances. Importantly, the\nproposed method achieved a significant improvement com-\npared to the original GAZ PTP method on multi-constrained\nEVRP.\nFor future work, it is still promising to further explore more\nefficient self-play strategies for GAZ-type methods and ap-\nply these methods to solve more multi-constrained tasks, es-\npecially for their large-scale instances. Another direction is\nto explore alternative ways to reduce the computational cost\ncaused by Monte Carlo simulations in large-scale problems.\nIn addition, combining the proposed approach with the multi-agent methods [Hao et al., 2024] for more complex tasks re-"}, {"title": "7 Appendix", "content": "7.1 GAZ PTP\nAccording to Algorithm 2, we see that the original GAZ PTP\nalgorithm only has one stage of self-play. Throughout the\ntraining process, the learning player selects actions using a\npolicy network guided by MCTS planning, while the greedy\nplayer greedily selects the action with the highest probability\naccording to the policy network. The greedy player's strat-\negy performs reasonably well in problems like TSP, which\nonly requires considering the simple constraint that the next\nselected point has not been visited before. However, for prob-\nlems with multiple constraints like EVRP, the greedy player's\ngreedy strategy is insufficient to execute a near-optimal ac-\ntion. This leads to a situation where the competition between\nthe learning player and the greedy player fails to produce\nbetter-learned strategies.\n7.2 EVRP Parameters\n7.3 Markov Decision Process of Multi-constrained\nEVRP\nIn this paper, we model the multi-constrained EVRP as a two-\nplayer game, then the Markov decision process (MDP) can be\ndefined as a tuple (S, A, R, P), consisting of state S, action\nA, reward R and state transition P.\n\u2022 State: S represents the state space. In a two-player\ngame, $S = \\{s_t, s_t^{-1}\\}$, each player starts from the\ndepot with the initial state $s_0, s_0^{-1}$, respectively, and\n$s_t, s_t^{-1}$ represent the states of two players at time\nstep t. The graph node state and the electric vehicle\nstate, represented as $\\{s_t, s_t^{-1}\\} = \\{v_t, x_t, v_t^{-1}, x_t^{-1}\\}$.\nFor each node i, $x_t = (x_t^s, x_t^d)$, $x_t^{-1} = (x_t^{s,-1}, x_t^{d,-1})$ are the\nstatic and dynamic information of the node i, respec-\ntively. The static information is composed of the two-\ndimensional coordinates of the node $x_t^s$ and the demand\nof each customer $c_i$. For the vehicle state $\\{v_t, v_t^{-1}\\} =$\n$\\{e_t, \\tau_t, u_t, e_t^{-1}, \\tau_t^{-1}, u_t^{-1}\\}$, $e_t, e_t^{-1}$ is the remaining battery\nof the electric vehicles; $\\tau_t$ is the current travel time; and\n$u_t$ is the remaining capacity of the electric vehicles.\n\u2022 Action: A represents the action space. In the two-player\ngame $\\mathcal{A} = \\{a_t^1, a_t^2, ..., a_t^n, a_t^{-1,1}, a_t^{-1,2}, ..., a_t^{-1,n}\\}$, the\naction $a_t$ represents the action that has been chosen at\nthe time step t.\n\u2022 Reward: Unlike a single task that sets the reward as\nminimization or maximization of the objective function,\nthe reward R is reshaped into a binary $\\pm 1$ based on\nself-competition, which we compare the trajectory\n(so, a,..., -1, ar\u22121, s7) for the player p \u2208 {1, \u22121}\nat the time step t, R = 0 if t < T \u2013 1.\n\u2022 Transition: The state transitions deterministically to\n$P = F(s_t, a_t)$ due to the deterministic state transition\nfunction $F: S \\times \\mathcal{A} \\rightarrow S$."}]}