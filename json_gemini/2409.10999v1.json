{"title": "Enhancing Low-Resource Language and Instruction Following Capabilities of Audio Language Models", "authors": ["Potsawee Manakul", "Guangzhi Sun", "Warit Sirichotedumrong", "Kasima Tharnpipitchai", "Kunat Pipatanakul"], "abstract": "Audio language models can understand audio inputs and perform a range of audio-related tasks based on instructions, such as speech recognition and audio captioning, where the instructions are usually textual prompts. Audio language models are mostly initialized from pre-trained audio encoders and large language models (LLMs). Although these pre-trained components were developed to support multiple languages, audio-language models are trained predominantly on English data, which may limit their usability to only English instructions or English speech inputs. First, this paper examines the performance of existing audio language models in an underserved language using Thai as an example. This paper demonstrates that, despite being built on multilingual backbones, audio language models do not exhibit cross-lingual emergent abilities to low-resource languages. Second, this paper studies data mixture for developing audio language models that are optimized for a target language as well as English. In addition. this paper integrates audio comprehension and speech instruction-following capabilities into a single unified model. Our experiments provide insights into data mixture for enhancing instruction-following capabilities in both a low-resource language and English. Our model, Typhoon-Audio, outperforms existing open-source audio language models by a considerable margin, and it is comparable to state-of-the-art Gemini-1.5-Pro in both English and Thai languages.", "sections": [{"title": "I. INTRODUCTION", "content": "Recent advancements in multimodal large language models (LLMs) have involved augmenting audio encoders, allowing models to extend their capabilities beyond textual inputs to include the understanding of speech and audio events [1]\u2013[7]. Audio language models are developed for different use cases with common types as follows. Audio-understanding models, e.g., Qwen-Audio [1], SALMONN [2], and LTU [3], can process various types of audio inputs, such as speech, environ-mental sounds, or music, and generate textual outputs. These models are primarily designed for tasks that require a deep un-derstanding of audio content, prompted by human instructions such as speech recognition, speech translation, or audio cap-tioning. Alternatively, models such as SpeechGPT [5] accept speech as input and produce speech as output, functioning as voice agents. These models are optimized for seamless voice interactions rather than complex audio understanding. This work focuses on audio-understanding and instruction-following models, which we refer to as audio language models."}, {"title": "II. RELATED WORK", "content": "Audio Language Models: SALMONN integrates three pri-mary components: an LLM based on Vicuna [8], a speech encoder based on the encoder of Whisper-large-v2 [9], and BEATS [10] for audio events. The representations from Whis-per and BEATS are concatenated and passed through an adapter (connection module based on Q-Former) to obtain the audio representation as the input to the LLM. Trainable modules are the Q-Former and the LoRA weights of the LLM. Train data consists of various tasks such as speech recognition, translation, audio captioning, or spoken question-answering. Qwen-Audio, similarly, uses the encoder of Whisper-large-v2, and its LLM is based on Qwen [11]. No connection module (i.e., adapter) is employed between these components; however, a variety of special tokens are incorporated. The pre-training phase involves training the audio encoder while freez-ing the LLM. During the fine-tuning phase, the audio encoder is frozen, and only the LLM is trained. LTU (first version) [3] was developed to support audio events with Llama-7B (LLM) and an AST audio encoder [12] as backbones. The AST encoder maps audio into 32 tokens, with a connection module adjusting dimensions from 768 to 4096. Subsequently, LTU-AS (second version) [4] extends support to both speech and audio, using the Whisper-encoder, similar to SALMONN and Qwen-Audio. The training involved a curated dataset, with around 5 million triples (audio, question, answer) in the first version, and expanded to 9 million triples in the second version. These training examples were generated using GPT-3.5 based on audio metadata. Alternatively, AudioChatLlama aligns the semantic space of speech and text for direct speech-to-response generation. Since AudioChatLlama is optimized for responding to speech input, it cannot be controlled to perform other tasks through text instructions like SALMONN, Qwen-Audio, or LTU.\nLLMs for Low-Resource Language: Previous studies have adapted English-centric unimodal LLMs into bilingual uni-modal LLMs, enabling them to perform effectively in both English and a target language [13]-[16]. For example, Ty-phoon [14] is a Mistral/Llama3-based LLM enhanced on Thai. Typhoon was continually pre-trained on 50% English and 50% Thai data where Thai texts were sourced from the MC4 and OSCAR datasets before supervised fine-tuning. This process showed an improvement in Thai evaluation benchmarks. We note that while adapting LLMs to a new language is well-explored in unimodal LLMs, research on adapting audio language models to a new language remains underexplored."}, {"title": "III. METHODOLOGY", "content": "Model Architecture (in Fig. 1): We follow the SALMONN architecture [2]. With Thai and English as target languages, our model is based on Typhoon-1.5-8B-Instruct [14] as the LLM, Whisper-large-v3 fine-tuned to Thai [17] coupled with BEATS [10] as the audio encoder, and Q-Former [18] trained from scratch as the adapter. Note that we examine other variants of LLM and audio encoder backbones in Section V-B.\nTraining Strategies: The audio encoder maps the spectrogram into a representation, which is then transformed into the audio representation a in the text embedding space via an adapter. The model \\( \\theta \\) is trained to maximize the probability of the next word \\( y_t \\) in the textual response, conditioned on previous words \\( Y_{1:t-1} \\), textual prompt input x and the audio input \\( \\alpha \\): \\( P(y_t | Y_{1:t-1}, x, \\alpha; \\theta) \\). Training occurs in two phases:\n\u2022 Pre-training: As the adapter is the only component initial-ized with random weights, this phase trains only the adapter to align audio and textual representations. We use ASR and audio captioning data shown in Tab. I in this phase.\n\u2022 Supervised Fine-Tuning (SFT): This phase trains both the adapter and the LoRA weight [19] of the LLM (r=8, a=32). During SFT, the model is trained on diverse tasks and instruc-tion prompts to enhance its instruction-following capabilities. Tab. II presents the data used in our final model (Typhoon-Audio), and Section V-C studies the SFT data mixture."}, {"title": "IV. EXPERIMENTAL SETUP", "content": "Data: Each example has {audio, textual_prompt}. For pre-training data (Tab. I), a few prompts were pre-defined for each task, e.g., \"Transcribe this audio\" for ASR, and we sample a prompt language that matches the response language. Since there is no Thai audio-captioning data, we translate AudioCaps and Clotho to Thai. For SFT data (Tab. II), we translate 10% of prompts/responses in existing QA data to Thai. To increase the diversity of prompts, we generate a prompt for each example in ASR, translation, and audio-captioning, using GPT-40. For speech instruction following, the model is meant to listen to spoken instructions and respond, thus, the prompt is null. Next, we describe newly created datasets, grouped by tasks:\n\u2022 Spoken Document QA: The QA examples in SALMONN and LTU are primarily based on short spoken documents, typically comprising single utterances under 10 seconds. To extend this to support longer understanding capabilities, we chunked longer audio in Yodas2 [33] into 30-second segments and prompted GPT-40 to generate question-answer pairs from the reference texts. In addition to standard extractive QA pairs, we also generated multiple-choice questions (MCQ), as we found that this approach improved the SpokenQA perfor-mance, which will be discussed in Section V-C. Furthermore, we focus on the Thai subset of Yodas2, as existing QA datasets predominantly consist of English spoken documents. Since Yodas2 emphasizes the content of speech, to capture the unique characteristics of voices, we additionally generated QA pairs from the VISTEC-SER dataset [32], which includes metadata such as speaker gender and emotional state.\n\u2022 Audio Caption: AudioCaps is used for pre-training, but its short ground-truth captions limit detailed response generation. To address this, we provide Gemini-1.5-Pro with both audio input and the short caption, prompting it to generate detailed responses. This augmented data is called AudioCaps (Gemini).\n\u2022 Speech Instruction Following (SpeechIF): This task re-quires models to listen to spoken instructions and directly respond. Current models like SALMONN lack specific data for this ability. We propose two methods for generating SpeechIF data (Fig. 2). Typel leverages ASR datasets to generate text responses from transcripts. However, since ASR data typically contains non-question utterances, LLMs often default to safe responses such as \"I'm sorry, as an AI assistant I cannot...\" in up to 30% of cases. While it offers voice diversity, it does not fully reflect real-world interactions. Type2 synthe-sizes speech from instruction-response pairs (e.g., Alpaca, Airoboros), providing more practical commands but struggles with unsuitable instructions like math or coding. Though lacking voice diversity, it represents real interactions better. For evaluation, we selected instructions from AlpacaEval (English) and SelfInstruct (Thai), creating the first SpeechIF benchmark for both languages. The prompt for baseline models (e.g., SALMONN) is \u201cListen to the audio and answer the question\".\n\u2022 Complex Instruction Following (ComplexIF): We pro-pose ComplexIF to assess models' ability to follow unseen, compound instructions, where each instruction involves two to three audio tasks (such as transcribe, then translate). In ComplexIF, models have to respond in specific formats (e.g., JSON, XML), with format templates provided in the instruc-tion prompt. As it evaluates the general instruction following ability, only English speech data is used. ComplexIF is used exclusively for evaluation, without additional training."}, {"title": "V. RESULTS AND KEY FINDINGS", "content": "Evaluation: For existing tasks, we use standard metrics. For SpeechIF and ComplexIF, we follow MT-Bench [35] in using an LLM judge (GPT-40), and we adapt the single-turn evaluation prompt from MT-Bench and score responses on a scale from 1.0 to 10.0. For ComplexIF, we prompt the judge to evaluate the response on two aspects: Quality and Format.\nBaselines: Competitive audio language models include,\n\u2022 Open-weights: Qwen-Audio (Qwen-7B) [1], SALMONN (Vicuna-13B) [2], and DiVA (Llama3-8B) [36]. As of this writing, DiVA has shown strong performance with publicly available model weights; however, its technical paper has not yet been released. Thus, the specifics of its training strategies and data remain unknown.\n\u2022 Proprietary: Gemini-1.5-Pro (Audio)\nFor open models, we use available weights on Hugging-Face. For Gemini-1.5-Pro, we use gemini-1.5-pro-001 through Google API with {audio, text_instruction} as input.\nA. Existing Audio Language Models on English versus Thai\nThe results in Table IV demonstrate that: (1) Baselines using multilingual backbones exhibit significant performance degradation in Thai, while Gemini-1.5-Pro maintains strong performance across both Thai and English. (2) Among the baselines, DiVA is the only model that performs well on the SpeechIF task, but it experiences a notable drop when tested on Thai. Thus, the subsequent experiments aim to develop a model that can effectively handle these tasks in both English and a low-resource language such as Thai.\nB. Pre-training: Speech Encoder and LLM Backbones\nThis experiment focuses on selecting backbones, and com-paring Whisper with its English+Thai fine-tuned variant. Simi-larly, Typhoon is a Llama-3 model fine-tuned to English+Thai. Our results (in Tab. III) show that for ASR, models where both backbones are matched with the target language yield the best results. However, for audio captioning, the performance dif-ference between these models is marginal. As a low-resource language such as Thai is our goal, Whisper+Th coupled with Typhoon-1.5 are selected for subsequent experiments.\nC. Supervised Fine Tuning (SFT): Data Mixture\nThis experiment focuses on data mixture to enhance broad instruction-following abilities across tasks and languages."}, {"title": "VI. CONCLUSIONS", "content": "This paper demonstrates the limitations of low-resource language capabilities in open-source audio language models, using Thai as an example. Through our data mixtures which combine audio content understanding and speech instruction understanding, we show that it is possible to achieve perfor-mance on par with the proprietary state-of-the-art Gemini-1.5-Pro in a range of audio tasks in both English and Thai with only around 1.82M pre-training and 0.64M SFT examples."}]}