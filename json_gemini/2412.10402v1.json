{"title": "TANGO: Training-free Embodied AI Agents for Open-world Tasks", "authors": ["Filippo Ziliotto", "Tommaso Campari", "Luciano Serafini", "Lamberto Ballan"], "abstract": "Large Language Models (LLMs) have demonstrated excellent capabilities in composing various modules together to create programs that can perform complex reasoning tasks on images. In this paper, we propose TANGO, an approach that extends the program composition via LLMs already observed for images, aiming to integrate those capabilities into embodied agents capable of observing and acting in the world. Specifically, by employing a simple PointGoal Navigation model combined with a memory-based exploration policy as a foundational primitive for guiding an agent through the world, we show how a single model can address diverse tasks without additional training. We task an LLM with composing the provided primitives to solve a specific task, using only a few in-context examples in the prompt. We evaluate our approach on three key Embodied AI tasks: Open-Set ObjectGoal Navigation, Multi-Modal Lifelong Navigation, and Open Embodied Question Answering, achieving state-of-the-art results without any specific fine-tuning in challenging zero-shot scenarios.", "sections": [{"title": "1. Introduction", "content": "Large Language Models (LLMs) have gained significant attention in the field of AI due to their remarkable capability to generalize across unseen tasks [5, 23, 45, 46]. Systems like VisProg and ViperGPT [18, 44] demonstrated strong performance on a broad range of vision-and-language tasks by just relying on a few numbers of in-context examples to generate complex compositional programs without requiring any specific training. Nevertheless, the evaluation of these concepts to Embodied AI, particularly for navigation tasks, remains largely unexplored. This approach may be crucial in building agents capable to navigate and operate efficiently in unfamiliar environments.\nPreviously, methods such as Neural Module Networks (NMN) [2, 19] have demonstrated promising compositional properties for high-level tasks like visual question answering, via end-to-end training of networks and specialized, differentiable neural modules. However, they required the combination of semantic parsers and predefined templates to learn to solve the compositional tasks. Alternative approaches leverage LLMs through predefined task modules or APIs for action execution [21, 22, 31, 49, 55]. While effective, these methods often lack a compositional structure to address multiple tasks or primarily focus on manipulation and robotic planning, with limited emphasis on navigation.\nIn this work, we present TANGO (see Figure 1) a novel neuro-symbolic compositional approach which utilizes primitives and employs a LLM as a planner to sequence these primitives within photorealistic 3D environments, where agents must perceive and act. This framework integrates high-level planning with low-level action execution, without the need for training. TANGO makes use of diverse modules designed for visual navigation and question-answering tasks, resulting in a system that seamlessly addresses both challenges while achieving SoA performance without requiring any prior adjustments. By providing a few in-context examples that show how to tackle multiple tasks, TANGO is capable of generalizing to the specific task at hand. This is facilitated by the LLM, which effectively combines the individual modules available within the TANGO system. Moreover, it extends [52] exploration policy by incorporating a memory mechanism in the form of a stored feature map that retains information about previously explored areas, supporting for efficient life-long navigation tasks.\nTo demonstrate the flexibility of the proposed framework, TANGO has been tested on three popular benchmarks: namely, i) Open-Vocabulary ObjectGoal Navigation [53], ii) Multi-Modal Lifelong Navigation [25], and iii) Embodied Question Answering [15, 33]. Results match or surpass previous approaches, without requiring specialized training. Summing up, the contributions of this paper are as follows:\n\u2022 Introduction of a neuro-symbolic compositional LLM-based framework for EAI leveraging specialized primitive modules.\n\u2022 Demonstration of robust generalization capabilities across multiple tasks without the need for any specific training or fine-tuning.\n\u2022 Extension of the exploration policy presented in [52] to multi-goal scenarios through the incorporation of a memory mechanism stored as a feature vector map.\n\u2022 Achievement of state-of-the-art results, underscoring the effectiveness of the proposed framework."}, {"title": "2. Related Works", "content": "Our work takes inspiration from modular approaches for visual tasks, like the seminal Neural Module Networks [2, 19], as well as the recent VisProg [18] and ViperGPT [44] frameworks. The main contribution of this paper is to extend these ideas to embodied visual navigation, that usually require heavy end-to-end learning, although modular approaches have recently shown promising results [7, 17]. Therefore, we discuss prior work in the area of embodied AI, the use of language models in robotics, and program generation approaches for image recognition tasks.\nEmbodied AI and Visual Navigation. The field of Embodied AI has recently undergone a paradigm shift, fuelled by the emergence of highly efficient simulators [26, 35, 41\u201343]. These simulators enable processing numerous parallel simulations in photorealistic indoor environments, facilitating large-scale testing that was otherwise challenging in classical robotics. Alongside these simulators, a large variety of tasks (and benchmarks) has been proposed: PointGoal Navigation [47], where an agent navigates from point A to point B in an unknown environment; Object-Goal Navigation [3], requiring the agent to locate and navigate to an object in the scene; Instance-Image Goal Navigation [27], akin to ObjectGoal Navigation, but in which the agent should find a specific object depicted in a given image; Multi-Modal Lifelong Navigation [25] involving navigating to a sequence of target objects that can be specified through labels, images, or textual descriptions; Embodied Question Answering [15, 33], in which an agent navigates an environment to gather information needed to answer a question, and Vision and Language Navigation (VLN) [1].\nVarious approaches have been proposed to tackle these tasks, primarily falling into three categories: end-to-end [6, 50, 51], modular [11\u201313, 28, 38, 39] and LLM-based [16, 29, 54\u201356]. However, the main limitation of these paradigms is their dependence on task-specific architectures. In the case of end-to-end approaches, a model is trained for a specific problem, often requiring days of training, such as in [47], where a policy for PointGoal Navigation was trained for 2.5 billion steps. To adapt the learned model to a new problem, adjustments and retraining are necessary, and this comes with a high cost. Modular approaches share a similar challenge, usually relying on policies designed for specific tasks, although common elements, such as exploration and navigation modules, are often shared [11, 12, 28, 38, 39]. Adapting these solutions to different tasks requires a manual adjustment of the approach to fit the new domain, often involving the addition or modification of modules.\nLanguage models for Robotics In robotics, foundation models (LLMs) trained on vast internet-scale datasets [5, 23, 45, 46] have the potential to equip robots with real-world priors and advanced reasoning abilities without the need for extensive task-specific training. Early approaches equipped agents with learned language embeddings, requiring large amounts of training data [4, 31]. Recent studies, on the other hand, have explored zero-shot and few-shot solutions mainly focusing on robotic planning and manipulation tasks [21, 22, 31]. In the context of visual navigation, LLM-based approaches leverage the powerful LLMS priors and reasoning capabilities to guide navigation within the environment [16, 29, 54\u201356]; however, despite their remarkable contributions, they often lack a modular design that allows for flexible integration and extensibility, as they are typically optimized for single tasks rather than a comprehensive, adaptable framework.\nIn contrast, our work deviates from these paradigms by relying on a series of diverse pre-trained modules that, thanks to a LLM, can be combined to potentially solve various tasks, compositionally. None of our modules are fine-tuned explicitly for the target problems, placing us in a zero-shot setting. Moreover, TANGO does not only rely on an exploration policy based entirely on LLM output, as in [16, 55, 56]. Instead, it combines a frontier exploration policy with LLM priors and memory guidance to enhance navigation performance. To achieve this, we use a pre-trained PointGoal policy as the base waypoint navigation module of TANGO and extend the exploration policy introduced in [52] by equipping the agent with a memory mechanism stored as a feature vector for each pixel of the map."}, {"title": "Modular vision and program composition for visual tasks", "content": "Neural Module Networks (NMN) [2, 19] introduced modular and compositional methodologies for visual question answering (VQA). NMNs integrate neural modules into an end-to-end differentiable network; the approach originally relied on pre-existing parsers [2], whereas more recent methods [19, 20, 24] have evolved to learn the layout generation model concurrently with the neural modules, employing reinforcement learning [48] and weak supervision. Stack-NMN [20] extends N2NMN by transitioning from discrete to soft layout generation, incorporating a weighted average of predictions from all modules at each step, determined by a layout generation network.\nRecently, [18] introduced VisProg, a framework that offers two key advantages over NMNs. Firstly, it constructs high-level programs that invoke state-of-the-art neural models and Python functions at intermediate steps, diverging from the conventional approach of generating end-to-end neural networks. This design facilitates the integration of symbolic, non-differentiable modules. Secondly, it capitalizes on the in-context learning ability of large language models (LLMs) [5] to generate programs. This is achieved by prompting the LLM with a natural language instruction (or a visual question or a statement to be verified), along with a few examples of similar instructions and their corresponding programs. This approach eliminates the need to train specialized program generators for each task. Almost concurrently, ViperGPT [44] presented a similar approach to VisProg, but in this case the model directly generates Python code, instead of composing pre-defined modules and routines. In contrast, in our framework, the LLM is provided only with high-level knowledge of the functions of each primitive, which is encoded directly in the primitive's name. Therefore, it is built similarly to VisProg, as directly creating code to solve EAI tasks is a much harder problem than composing pre-defined primitives. It is noteworthy that both VisProg and ViperGPT were tested on images, while our model is specifically designed for embodied agents operating in an interactive environment, relying on sensor-based perception."}, {"title": "3. Method", "content": "Neuro-symbolic approaches offer the possibility to address a broad range of diverse complex tasks efficiently. Systems like VisProg [18] operate on images and have demonstrated excellent results in zero-shot settings. Understanding how a similar approach works in an action-oriented scenario is key to enable robots to navigate autonomously in novel environments. Therefore, we develop this paradigm further into EAI, where the integration with action execution adds several challenges. Inspired by [18], given an input prompt, we rely on a LLM to generate synthetic pseudo-programs that can be executed by the embodied agent in the environment. Our TANGO framework is therefore able to generalize to new tasks, without any direct training on task-specific datasets, thus effectively mitigating the heavy computational training costs inherent to embodied navigation tasks. The procedure that enables generating these new programs is based on \u201cin-context examples\" that are fed to the LLM, alongside a natural language instruction. An outline of TANGO is shown in Figure 2.\nTANGO Interpreter. The first key component of the proposed framework is referred to as \"Program Interpreter\". It comprises visual recognition modules that can be used by the agent to extract the semantics of the scene, as well as to provide an understanding of the visual context.\nAs shown in Figure 1, users can ask questions or give specific tasks to the agent; the natural language prompt is then processed by the LLM (in our implementation GPT-40 [5]), which serves as a planner and outputs a step-by-step executable program. To ensure the LLM delivers a reasonable output, it is fed with 15 \u201cin-context examples\" across diverse tasks. This enables the LLM to make use of its reasoning capabilities effectively, identifying the most suitable planning for the required task. Moreover, the examples remain the same regardless of the task identified in the prompt; it is the LLM's responsibility to output the specific program target for the given question. Programs use a higher level of abstraction than previous modular attempts such as Neural Module Networks (NMN) [2, 19]. Each program is constructed as a sequence of primitives (e.g., detect, answer, match, etc.) that invoke corresponding TANGO modules. These modules are either powered by pre-trained state-of-the-art vision models or implemented as simple Python subroutines (e.g., count, is_found, eval, etc.), with additional navigation-specific modules designed to \"steer\" the agent's movements (e.g. navigate_to, return, turn, etc.). Figure 3 provides a comprehensive list of all the currently implemented modules."}, {"title": "Navigation Module", "content": "In order to navigate the environment, we define a module employing a PointGoal navigation agent as our foundational module [39, 52]. This model achieved nearly perfect PointGoal performance, both in terms of success (~ 99%) and efficiency (a forward pass that takes a fraction of a second) on standard datasets [9, 37]. The agent starts the exploration of the environment until it locates its target goal. Once the target is identified, the focus of exploration transitions to reaching the designated goal. This PointNav policy exclusively uses the egocentric depth image and the robot's relative distance and heading towards the desired goal point as input.\nExploration Policy. Exploration is performed using the policy outlined in [52]. This method builds occupancy maps based on depth observations to identify exploration frontiers. It further leverages RGB observations and a pre-trained vision-language model (BLIP2 [30]) to generate a language-grounded \"value map\". This value map is then used to guide the exploration, facilitating an efficient search for instances of a given target. Notably, as in [52], the agent performs a 360\u00b0 turn at the start of navigation to initialize frontiers.\nWe extend this policy for sequential goals by incorporating a memory mechanism, represented as a \u201cfeature map\u201d in which each pixel of the value map is encoded as a vector and updated at each step. This feature map then updates the language-grounded original value map when a new target is specified, by calculating the cosine similarity between each pixel's vector and the new target embeddings (either from text or image). We sample the highest value in the updated value map; if this value exceeds a predefined threshold, indicating the agent has previously encountered and \u201cremembers\" the target's location, the agent navigates to it, supporting lifelong navigation. The process is also computationally efficient, as the feature map similarity calculations are performed only if the target changes, rather than at each step during navigation."}, {"title": "3.1. Navigation Tasks with TANGO", "content": "TANGO leverages pre-trained multimodal vision-language and vision-only models as foundational components to extract semantic information from the scene, making it well-suited for several Embodied AI tasks.\n(Open-set) ObjectGoal Navigation [3, 53], in particular, emerges as a suitable testbed to evaluate the efficacy of our method. A key module to tackle this problem involves utilizing an object detector (Owlv2 in our implementation [34] or DETR [8] for target objects that fall within the COCO classes [32]) to identify objects within the image. Subsequently, navigation towards the detected objects is facilitated by waypoints, leveraging depth distance calculation to guide the agent effectively. For simplicity, we use the center of the bounding box to determine the target waypoint. Hence, our approach effectively addresses this task without requiring prior knowledge of target labels for previously encountered objects, making it suitable for real-world applications.\nTANGO also integrates a specialized module for matching target objectives with ongoing visual observations in image-based scenarios, resulting in accurate navigation towards an instance image. To evaluate our agent's performance in this context, we use SuperGlue network [40], optimized for indoor environments (i.e. using hyperparameters recommended in the respective paper). We rely on SuperGlue because of its real-time matching capabilities and effortless integration into Embodied systems.\nTANGO being task-agnostic, can process any type of navigation target, regardless of the order in which it is presented. In this context, the GOAT benchmark [25] is well-suited for evaluating our system, as it operates in scenarios where targets are specified through images, text, or descriptive phrases, provided in random sequence. It also heavily relies on memory to find previously seen targets when sequential goals are given.\nAn agent should also be able to answer user queries like \"can you check if the kitchen table is clean?\". Hence, EQA serves as an excellent benchmark to assess TANGO's capabilities and robustness. EQA consists of three phases: understanding the semantic structure of the prompt, locating the target object(s), and analyzing the visual semantics to generate an accurate response. To this end, TANGO integrates a specific answer module, relying on BLIP2 [30] as its core foundation."}, {"title": "4. Experiments", "content": "TANGO provides a flexible framework that can be applied to various embodied navigation problems. We evaluate our approach on three popular tasks that require a wide range of capabilities, including efficient environment exploration, path planning, scene and context understanding, and image similarity comparison."}, {"title": "4.1. Experimental Setup", "content": "Agent Configuration. Prior research on visual navigation commonly uses different agent configurations depending on the considered task [15, 25, 53]. The configuration typically employed for (OPEN-SET) OBJNAV closely resembles that of a LoCoBot, with an agent height of 0.88m, a radius of 0.18m, and a single 640 \u00d7 480 RGB sensor with a 79\u00b0 hfov, positioned 0.88m above the ground.\nIn the context of GOAT Benchmark, some of the default settings differ, featuring a camera situated 1.31m above the ground. The agent has a height of 1.41m and is given 360 \u00d7 640 RGB images. Both the above tasks' configurations use a step size of 0.25m and a left and right turning angle of 30\u00b0. Lastly, the settings for EQA mirror those of PointGoal Navigation as specified in the Habitat-Lab configuration files. All other settings remain consistent with those of [15].\nDatasets and Evaluation Protocol. We assess the TANGO performance across task-specific datasets using the Habitat simulator [42]. As the system is not trained in any way (except for the PointGoal Navigation model, which has been pre-trained on the training set of HM3D, never used in our tests), each scene within episodes is novel to the agent. Therefore, the entire validation set can be categorized as \"unseen\". We now describe in detail the tasks and datasets used:\n\u2022 OPEN VOCABULARY OBJECT NAVIGATION (OVON) [53]: a large-scale benchmark featuring over 15,000 annotated household objects across 379 categories, derived from real-world 3D scans [37]. The agent is initialized at a random location within a scene and tasked with navigating to a target object category within a time limit of 500 steps. To ensure comparability with other state-of-the-art (SoA) methods, we evaluate our approach across all episodes included in the \"val unseen\" set.\n\u2022 MULTI-MODAL LIFELONG NAVIGATION (GOAT-BENCH) [25]: an agent is tasked with sequentially navigating to five to ten target objects identified by category names, descriptions, or images. Each target represents a subtask executed within an open-vocabulary framework that spans over 312 categories. The agent is required to reach a goal within a specified time constraint and is assigned new targets upon the completion of each subtask. We evaluate our approach across all episodes included in the \"val unseen\" set.\n\u2022 EMBODIED QUESTION ANSWERING (OPENEQA) [33]: containing over 1,600 question-answer pairs sourced from more than 180 real-world environments and scans [14, 37]. It is divided into two task categories: Episodic Memory-EQA (EM-EQA) and Active-EQA (A-EQA). Our focus is on the latter, in which the agent must autonomously navigate and answer questions within a time constraint of 500 steps. Questions span over seven categories, including world knowledge, attribute recognition, spatial reasoning, and object localization. Examples of questions from the episodes include: \"Is the microwave door propped open?\" and \"What is left of the kitchen pass-through?\u201d. We evaluate our approach across all episodes belonging to the A-EQA task category.\nEvaluation Metrics. In all tasks, we use the standard metrics as in prior works [3, 15, 33, 53]. Namely:\n\u2022 Success Rate (SR): measures the ratio of episodes where the agent succesfully reached its target (Open ObjNav, GOAT-Bench).\n\u2022 Success weighted by Path Length (SPL): measures the optimality of the path taken by the agent w.r.t. the optimal path (Open ObjNav, GOAT-Bench).\n\u2022 Distance to Goal (DTG): measures the average distance to goal of the agent at the end of the episode (Open ObjNav, GOAT-Bench).\n\u2022 Score (LLM-Match): an LLM compares the ground-truth $GT_i$ answers with model output $A_i$ given a question $Q_i$ and assigns a score $\\sigma_i$ on a scale of 1 to 5. On this scale, 1 indicates an incorrect response, 5 represents a correct response, and intermediate values reflect varying levels of similarity. For example, the answer to the question \"What color is the bed\" could be correctly answered either as \"white\" or \"the bed is white\". The final results aggregation is as follows:\n$S = \\frac{1}{N} \\sum_{i=1}^{N} \\frac{\\sigma_i - 1}{4} \\times 100\\%$         (1)\n\u2022 Answer Accuracy: the average accuracy of the answers provided by the agent for the EQA task. This metric, used in [15], is presented with the related results in the supplementary material.\nImplemented Modules. TANGO contains descriptive and easily understandable names for the modules, arguments, and variables to facilitate the LLM comprehension of the input and output. Figure 3 showcases a list of all available modules. Each module exclusively outputs predefined object variables to the next one, enabling the possibility to monitor the progression of the agent at each step. Therefore, dramatically enhancing the explainability of failure cases, as it allows for a deep examination of the agent's decision-making process through its interactions with the environment. Figure 6 provides a general overview of the failure analysis in EQA task. As presented in Section 3, \"navigate_to\" and \"explore_scene\u201d modules are specifically engineered to navigate the environment, leveraging inputs from depth and RGB sensors as well as the PointGoal GPS + compass sensors. If this sensor is not available for a specific task, it can be derived from the current agent pose and the exploration goal location. The agent is provided with the actions: Forward, Move Left, Move Right, Look Up, Look Down and Stop.\nFor the \"detect\" module, we employ Owlv2 [34] object detector for general classes and use DETR [8] for categories within the COCO classes [32]. Notably, in synthetic 3D scenes, we noticed numerous false positives. To address the problem, bounding boxes produced by the detector are forwarded to a \"classify\" module. Within this module, each detection undergoes classification to differentiate between categories of similar instances. For example, the class \"chair\u201d encompasses diverse subcategories such as \"armchair\", \"couch\", and \"other\". Subclasses are outputted under the hood directly by the LLM requiring no human annotation. We utilize a CLIP-based classifier in its default configuration [36].\nEmbodied Question Answering (EQA) task uses an \u201canswer\u201d module based on BLIP2 [30], capable of performing various multi-modal tasks, including Visual Question Answering, Image-Text retrieval, and Image Captioning."}, {"title": "4.2. Experimental Results", "content": "Open-set ObjectGoal Navigation We evaluated our method in the standard Open-Set OBJNAV setting, with targets sampled directly from objects in the scene. As shown in Table 1, our approach achieves performance on par with state-of-the-art methods (rows 4-5) on the validation-unseen split both in SR (35.5%) and SPL (19.5%). Instead of focusing solely on numerical results, these findings highlight TANGO 's flexibility in solving navigation tasks in unfamiliar environments, underscoring the potential of our approach within the OBJNAV framework.\nEmbodied Question Answering. In EQA, the agent is queried with a natural language question and must autonomously explore the environment and gather information in order to answer accordingly. In recent literature, this task shifted from being regarded as a purely classification problem aimed at determining the most suitable answer from a set of pre-defined possibilities (i.e., class labels) to an open-vocabulary benchmark where the agent must answer with natural language [15, 33]. We compare TANGO results, using the score from Eq. 1, against other zero-shot approaches. Table 2 shows that our method ranks second and closely matches the performance of the leading SoA method, with a gap of less than 1%. Notably, these approaches [33] utilize LLMs to interpret scene objects step-by-step (rows 3-4), but they do not leverage LLMs to effectively guide navigation towards specific targets. Hence, our approach demonstrates that SoA results can still be achieved by pre-planning a path that prioritizes relevant areas for a given question. Moreover, the results indicate that human agents achieve a significantly higher score of 85%, which remains well above the performance achieved by using large language models (LLMs), particularly in open-world settings. This gap underscores the challenges LLMs face in handling the complexity and variability of open-world scenarios compared to human agents.\nGOAT. We assess our method within the GOAT-Bench setting, where the agent is spawned randomly and tasked with sequential targets. Each goal can be specified either by its category name, a description, or an image (e.g. the input could be \"gas boiler\", \"the gas boiler on the corner of the room. The gas boiler is located on the left of the washing machine and freezer\" or an image of the \"gas boiler\"). Table 3 compares our approach to the methods in [10]. TANGO significantly outperforms other state-of-the-art techniques (rows 1-4), achieving a +2.6% gain in the success metric. Additionally, it ranks second in navigation efficiency, with only a slight -0.7% difference from the top-performing method. We primarily attribute these results to the LLM's ability to accurately transform target descriptions into a sequence of selected goals, rather than relying solely on potentially \u201cnoisy\u201d text embeddings. Moreover, the use of a memory mechanism is essential for the task (Fig. 5); however, if the memorized target object is incorrect, the path efficiency can significantly decrease."}, {"title": "4.3. TANGO Failure analysis", "content": "Due to the high explainability of our system, we conducted an analysis of the failures encountered during EQA episodes (Fig. 6). We extracted a significant subsample for the task and manually classified the instances where the model failed. We considered scores below 3 as failures, i.e. incorrect answers. We observed that the main cause of failure can be attributed to the \"detect\u201d module (stopped at wrong object or Ignored goal object in the image). Furthermore, the exploration policy appears to perform well given the targets produced by the LLM, as it fails in only ~ 10% of the cases (labeled as: didn't see target goal in the image). The LLM generates incorrect pseudo-code around 18% of the time, with 6.9% failures leading to ambiguous or incorrect targets, and 11.2% due to incorrect primitive ordering or usage which instead leads directly to episode failure. Notably, only 11.2% of errors are actually due to the LLM generating incorrect code, while the remaining is attributable to prompt-related issues."}, {"title": "5. Conclusion", "content": "We conducted a systematic analysis of our compositional approach, offering valuable insights into the future of modular, neuro-symbolic systems. Moreover, TANGO highlights the versatility of simple PointGoal Navigation agents equipped with specific task-oriented modules, yielding promising results in zero-shot scenarios across all considered tasks. TANGO effectively handles diverse multi-modal prompts, following instructions to complete tasks, underscoring the potential of LLMs in robotic navigation. Its integrated memory supports lifelong navigation, suggesting improved capabilities with increased exploration. However, it may struggle with overly complex or ambiguous prompts, which can limit the LLM's ability to identify the correct primitives or targets. Future work could extend this system to tasks such as Visual Language Navigation (VLN) and explore the use of open-source LLMs. Additionally, refining the memory mechanism\u2014particularly through improved sampling strategies for identifying high-value pixels linked to objects' memory\u2014could further enhance the system's effectiveness."}]}