{"title": "LeapVAD: A Leap in Autonomous Driving via Cognitive Perception and Dual-Process Thinking", "authors": ["Yukai Ma", "Tiantian Wei", "Naiting Zhong", "Jianbiao Mei", "Tao Hu", "Licheng Wen", "Xuemeng Yang", "Botian Shi", "Yong Liu"], "abstract": "While autonomous driving technology has made remarkable strides, data-driven approaches still struggle with complex scenarios due to their limited reasoning capabilities. Meanwhile, knowledge-driven autonomous driving systems have evolved considerably with the popularization of visual language models. In this paper, we propose LeapVAD, a novel method based on cognitive perception and dual-process thinking. Our approach implements a human-attentional mechanism to identify and focus on critical traffic elements that influence driving decisions. By characterizing these objects through comprehensive attributes including appearance, motion patterns, and associated risks - LeapVAD achieves more effective environmental representation and streamlines the decision-making process. Furthermore, LeapVAD incorporates an innovative dual-process decision-making module miming the human-driving learning process. The system consists of an Analytic Process (System-II) that accumulates driving experience through logical reasoning and a Heuristic Process (System-I) that refines this knowledge via fine-tuning and few-shot learning. LeapVAD also includes reflective mechanisms and a growing memory bank, enabling it to learn from past mistakes and continuously improve its performance in a closed-loop environment. To enhance efficiency, we develop a scene encoder network that generates compact scene representations for rapid retrieval of relevant driving experiences. Extensive evaluations conducted on two leading autonomous driving simulators, CARLA and DriveArena, demonstrate that LeapVAD achieves superior performance compared to camera-only approaches despite limited training data. Comprehensive ablation studies further emphasize its effectiveness in continuous learning and domain adaptation.", "sections": [{"title": "I. INTRODUCTION", "content": "Since the early 21st century, humans have been exploring the use of computer algorithms to replace human drivers. Recent data-driven approaches [1], [2], [3] have achieved remarkable success. However, they often rely heavily on the distribution of training data, which can result in a superficial understanding of underlying semantics and lead to misinterpretations in complex or unfamiliar scenarios. Data-driven approaches typically generalize observed patterns without inferential capabilities, limiting their performance to the scope of annotated data. Consequently, there is a pressing need for a system that can reason beyond the boundaries of its training data and imitate human cognitive processes. Several knowledge-based methods [4], [5], [6], [7] employ large language models (LLMs) and vision language models (VLMs) as driving agents, marking a significant step towards more advanced autonomous systems. However, current evaluation methods for these approaches, such as open-loop testing, fall short of capturing the dynamic interactions between the self-driving car and its environment [8]. As a result, the responsiveness and adaptability of driving agents may not be adequately assessed, highlighting the need for more comprehensive evaluation methodologies.\nHuman learning to drive involves continuous interaction in closed-loop environments, where decisions are made based on surroundings and feedback is received. According to dual-process theory [9], [10], [11], human intelligence operates on two systems: 1) System-I (i.e., Heuristic Process, which is characterized by being fast, automatic, and empirical) and 2) System-II (i.e., Analytic Process, which is characterized by being slow, rational, and logical). This dual-process thinking is evident as novice drivers transition to experienced ones. Initially, they depend on common sense, but with training, they develop skills through trial and error and rational analysis (Analytic Process), leading to muscle memory that allows for quick, instinctive reactions in familiar situations (Heuristic Process). Even after obtaining a driver's license, individuals continue to learn from experiences and accidents to improve their driving skills.\nOur initial work in [12] introduced a dual-process driving system inspired by the human attention mechanism. This system aimed to achieve knowledge-driven autonomous driving with limited training data, emulating the human ability to learn from experience and refine skills over time. While the previous method demonstrated promising performance, it had several limitations, such as supporting only single-frame image inputs and lacking precise motion prediction for traffic participants. To overcome these challenges, we propose an enhanced version named LeapVAD. Specifically, the algorithm has been extended to support multi-view and multi-frame inputs. Furthermore, we enhance the scene encoder to extract scene tokens that are more closely related to driving actions, thus improving the accuracy of the overall system.\nBuilding on the dual-process continuous learning framework, this paper presents the following new contributions:\n\u2022 Temporal Scene Understanding. We extend our approach to"}, {"title": "II. RELATED WORKS", "content": "Built on LLMs such as LLaMA [16], [17] and Vicuna [18], a wide range of VLMs [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30] has emerged, extending their capabilities to multimodal understanding. These models have introduced innovative pretraining and fine-tuning techniques to bridge the gap between vision and language and raise extraordinary emergent abilities, typically including instruction following [31], in-context learning [19], and chain-of-thought [30] on multimodal data. Typically, VLMs utilize Q-former-like connectors [32], [29] or MLP-like connectors [31], [30] for modal alignment and undergo three stages of training: pretraining, instruction-tuning, and optional alignment tuning to develop full-fledged capabilities. For example, in addition to being pre-trained in large image-text corpora, models like LLaVA [25] and MiniGPT-4 [23] leverage instruction tuning to create versatile and interactive visual agents. Further advances include Qwen-VL [29], which adopts a multistage training strategy to improve multilingual and fine-grained visual comprehension. InternVL [30] introduces a progressive alignment training strategy that facilitates the integrated understanding of multiple modalities, including text, images, videos, and others. These models have significantly broadened the scope of integration in vision languages, enabling more sophisticated applications.\nFollowing VLMs' great success, supervised fine-tuning and visual adaptation are conducted for various downstream vision tasks [33]. In the realm of autonomous driving, numerous studies [34], [35], [4], [5], [6], [36] have explored the application of large foundation models, leveraging their embedded world knowledge and advanced reasoning capabilities. For example, DriveLM [37] presents a graph visual question answering benchmark and incorporates VLMs to perform planning using a designed graph-based chain of thought, thereby enhancing the interpretability of autonomous driving systems. DriveMLM [38] utilizes VLMs to generate decision-making processes based on human instructions within simulated environments. ELM [39] introduces a VLM tailored for embodied understanding in driving scenarios, while RAG-Driver [6] improves driving interpretation and signal prediction by combining retrieval-augmented generation (RAG) with in-context learning. Recent innovations, such as DriveVLM-Dual [7], further integrate VLMs with data-driven planning pipelines, offering promising solutions for real-world deployment.\nIn contrast to the approaches mentioned above, we draw inspiration from human driving behaviors. Through the dual-process system, the memory bank, and the reflection mechanism, we implement the storage, updating, and transfer of experiences, enabling continuous exploration, learning, and improvement in closed-loop scenarios."}, {"title": "III. METHODOLOGY", "content": "Our proposed LeapVAD framework consists of four main components: the VLM for scene understanding (Section III-B), Scene Encoder for extracting scene token (Section III-C), a dual-process decision-making module consisting of the Analytic Process (Section III-D) and the Heuristic Process (Section III-E), which operates in conjunction with a controller detailed in Appendix V-A. As illustrated in Algorithm 1 and Figure 1, LeapVAD employs a VLM to analyze multi-frame images and describe key objects in the closed-loop simulator. The Scene Encoder creates scene tokens based on the current image and vehicle state. These object descriptions and scene tokens are passed to the Dual-Process Decision-Making Module, which performs scenario reasoning and determines driving actions. The resulting high-level decisions are then sent to the action actuator to generate control signals. Once an accident occurs, the system will automatically activate the reflection mechanism (Section III-D) for self-updating and continuous improvement.\nHuman drivers focus on key objects around the vehicle to avoid information overload, improve reaction times, and reduce cognitive load. This strategy enhances concentration and lowers accident risks. Inspired by this, the scene understanding module in LeapVAD selectively identifies critical objects, simplifying the environment's description and easing the decision-making process.\nOff-the-shelf foundation VLMs often lack domain-specific knowledge for driving. To address this, we perform supervised"}, {"title": "C. Scene Token", "content": "To facilitate retrieving similar scenes in the memory bank for few-shot prompting, we propose a more efficient and precise method for extracting scene tokens. Our approach differs from LeapAD [12] in that we can avoid the ambiguity, synonyms, and paraphrasing of scene descriptions, resulting in improved performance compared to text embeddings. In this paper, LeapVAD generates meta-actions for steering and speed regulation, mirroring the primary control mechanisms employed by human drivers. Our key insight is that scenes requiring similar human control responses (regarding steering and braking) can be considered analogous at the control level. Building upon this observation and inspired by ACO [62], we develop a comparative learning approach that operates in two distinct spaces: the Action space (ACT) for steering and the Acceleration space (ACC) for braking. This dual-space comparative learning framework enables us to derive scene tokens that capture the underlying control similarities.\nAs illustrated in Fig. 3, given a batch of image $I \\in R^{B \\times H \\times W \\times 3}$ and the ego state $A \\in R^{B \\times 9}$, we process the input through the Scene Encoder $E$ to derive the Scene Token $t \\in R^{B \\times 256}$, which encompasses $g_{act} \\in R^{B \\times 128}$ and $g_{acc} \\in R^{B \\times 128}$. A momentum-updated Scene Encoder $E_m$\nis used to extract feature vector $t' \\in R^{B \\times 256}$. The updated rule of the encoder's parameters is:\n$\\theta_{E_m} = \\alpha \\theta_{E_m} + (1 - \\alpha) \\theta_E$\nwhere $\\alpha$ is the momentum coefficient and $E_m, \\theta_E$ are the parameters of $E_m$ and $E$.\nSpecifically, the intent is derived through one-hot encoding. The features are concatenated with ego velocity $V$ via an MLP to generate the ego state features $f_{ego} \\in R^{B \\times 256}$. The image is processed to extract ViT features $f_{img} \\in R^{B \\times N \\times C}$, which are then compressed into the scene features $f_{scene} \\in R^{B \\times 256}$ using either max-pooling or attention mechanisms. The final Scene Token $t$ is produced by concatenating $f_{ego}$ and $f_{scene}$ after applying the MLP.\nFollowing MoCo [63], we use a key dictionary $H$ to store historical encoded Scene Token $t'$ to enable larger contrastive batch size. In this study, we utilize the Scene Encoder $E_m$ to generate the Scene Token $t'$. This process establishes positive pairs with the current training features. It ensures that these tokens are stored in the memory bank, $H$, to form positive and negative sample pairs in subsequent training iterations. When the size of $H$ exceeds its predetermined capacity, the samples within the dictionary are systematically replaced.\nDuring training, a batch of images and their corresponding ego states are sampled. Following data augments, these images are encoded by both $E$ and $E_m$ to obtain the query features $g_{sp} \\in R^{B \\times 128}$ and key features $g_{sp} \\in R^{B \\times 128}$. Simultaneously, $N$ key features are sampled from both $H$ and $g_{sp}$ to form the key set $K$. The losses $L_{act}$ and $L_{acc}$ are then computed as:\n$L_{sp} = -log \\frac{\\sum_{z^+ \\in P_{sp}(g_{sp})} exp(g_{sp} \\cdot z^+/\\tau)}{\\sum_{z^- \\in N_{sp}(g_{sp})} exp(g_{sp} \\cdot z^-/\\tau)}$\nwhere $P_{sp}(t_{sp})$ and $N_{sp}(t_{sp})$ are the positive and negative keys in \"sp\" space respectively. They are represented as:\n$P_{sp}(g_{sp}) = \\{z ||| g_{sp} - a_{sp} || < \\zeta_{sp}, (z, a_{sp}) \\in K\\}$,\n$N_{sp}(g_{sp}) = K \\setminus P_{sp}(g_{sp}),$\n$sp \\in \\{act, acc\\},$\nwhere $a_{sp}$ is the label of the query feature, and $a_{sp}$ is the label of the key feature, with the values of steering and brakes in the ACT and ACC spaces, respectively. Also, $\\zeta_{sp}$ is the distance threshold in \"sp\" space. In the ACC space, a negative pair is identified when one vehicle brakes and the other does not.\nThe overall contrastive loss of Scene Token is:\n$L = \\lambda_{act} L_{act} + \\lambda_{acc} L_{acc}$\nwhere $\\lambda_{act}$ and $\\lambda_{acc}$ are hyper-parameters used to adjust the weights of $L_{act}$ and $L_{acc}$."}, {"title": "D. Analytic Process", "content": "Leveraging scene descriptions offered by the VLM, we develop the Analytic Process, a framework designed to emulate the logical reasoning processes characteristic of human drivers. The Analytic Process utilizes logical reasoning to navigate complex scenarios, employing structured analysis and rational decision-making to ensure safety in driving tasks.\nThrough extensive pre-training on diverse datasets, LLMs inherently accumulate a vast repository of world knowledge, equipping them to address complex problems with nuanced reasoning and understanding [35]. This capability meets the demand of the Analytic Process, which relies on thorough analysis and contextual awareness to make informed decisions in driving scenarios. Our Analytic Process leverages the world knowledge embedded in LLMs to interpret scene descriptions and produce high-quality driving decisions. Empirical results indicate that incorporating specific traffic rules, as outlined in Appendix V-B, further enhances the system's safety and reliability in real-world driving scenarios. Moreover, we combine the VLM with the Analytic Process to conduct closed-loop experiments, enabling the collection of high-quality decision-making data and outcomes. These results, stored as \"experience\" in a memory bank, are incrementally accumulated and can be effectively transferred to the Heuristic Process. This transfer empowers the Heuristic Process to leverage prior experience for rapid response in analogous scenarios, as elaborated in Section III-E.\nThe Analytic Process is employed to facilitate reflection on traffic accidents, as demonstrated at the bottom of Algorithm 1. Precisely, in a closed-loop driving scenario integrating the VLM and Heuristic Process, the occurrence of any accident O activates a reflective mechanism. The Analytic Process subsequently analyzes the scene description D, reasoning R, and decision S from the historical frames Q preceding the incident. This analysis identifies causal"}, {"title": "E. Heuristic Process", "content": "Although the Analytic Process excels at providing precise driving reasoning and decisions through its detailed analysis and thorough evaluation, it is inherently slow processing often results in duplicated and redundant efforts, limiting its practicality in real-world driving scenarios. Drawing inspiration from human driving behavior, where drivers develop muscle memory through repeated practice that enables efficient reactions with minimal cognitive load, we introduce a Heuristic Process within LeapVAD incorporating a lightweight language model.\nTo enable effective knowledge transfer, we apply supervised fine-tuning (SFT) using the data accumulated in the memory bank, as described in Section III-D. This process distills knowledge from the Analytic Process into the lightweight model, allowing the Heuristic Process to adapt its behavior to diverse scenarios while operating significantly faster (approximately five times faster in our experiments). Our previous results [12] reveal that the lightweight model without SFT fails to produce reliable driving decisions.\nWe utilize few-shot prompting [35], [12] to enhance the Heuristic Process's generalization for unseen scenes and reduce hallucinations, leading to more robust decisions. This approach allows the Heuristic Process to effectively draw on insights from its memory bank, improving the accuracy of future driving decisions.\nFollowing Section III-C, we can obtain the scene token $t_q$ for the current scene. Subsequently, the cosine similarity between $t_q$ and the scene tokens $\\{t_i\\}_{i=0}^{M-1}$ in the memory bank with the size of M is calculated by:\n$cosine(t_q, t_i) = \\frac{t_q \\cdot t_i}{||t_q|| ||t_i||}$\nWe select the top-k samples with the highest similarity scores as queried scenes. The scene descriptions $\\{D\\}_{i=0}^{k-1}$, reasoning $\\{R\\}_{i=0}^{k-1}$, and decisions $\\{S\\}_{i=0}^{k-1}$ of these samples, along with the current scene description $D_e$, are input into the Heuristic Process for final reasoning $R_e$ and decision $S_c$."}, {"title": "IV. EXPERIMENTS", "content": "We constructed an instruction-following dataset for the supervised fine-tuning of VLM by integrating data collected from Rank2Tell [61], DriveLM [37], and CARLA [13]. We standardized the referring format for key objects as follows:  properties. Each dataset was processed with the original labels to obtain unique question-answer pairs. Overall, the dialogue is constructed summary-elaborated, as shown in Figure 2. The elaboration includes four aspects of key object properties: semantic attributes, spatial attributes, motion attributes, and importance. Apart from multi-view dialogues, we added summaries for multi-frame images, introducing properties such as distance, speed, and motion trends of key objects. We constructed dialogues using five frames, with Rank2Tell operating at 10 Hz and the others at 2 Hz. We collected 5K of multi-view summary data and 2K of multi-frame summary data from CARLA Towns 01-04, 06, 07, and 10 to train the VLM.\nWe accumulate experience in a closed-loop setting and store it in a memory repository by integrating Analytic Process and VLM, which serves as few-shot examples for subsequent SFT and Heuristic Process prompting. Our approach also incorporates a dynamic updating mechanism to address issues encountered by Heuristic Process, as detailed in Section III-D. It is worth noting that these samples are collected in a closed-loop environment without human intervention. As with LeapAD [12], we used a memory bank size of 18.1k in Table I and used it as the default in this paper.\nWe utilized data collected from CARLA and the nuScenes [64] (train set) to train the Scene Encoder. Specifically, we collected 90K driving frames from the CARLA simulator, of which 70% were designated for training. Each frame includes an image captured by the front-view camera and data on intent, velocity, steering, and braking. Steering values are normalized within the [-1, 1] range, whereas braking values range from [0, 1]. We use steering and braking as labels for the ACT and ACC spaces, respectively, and set the distance threshold $\\zeta_{sp}$ ($sp \\in \\{act, acc\\}$) to 0.04."}, {"title": "V. CONCLUSION", "content": "In this paper, we propose a novel, human-like, knowledge-driven autonomous driving framework that employs a VLM to focus on critical objects in the driving environment. Our framework mimics the way human eyes perceive and prioritize information. We have designed a dual-process decision-making module to emulate the learning process of human drivers. Additionally, we have incorporated reflective mechanisms and cumulative memory banks to enable continuous self-improvement of algorithms. We introduce an efficient scene token to represent the scene relevant to driving actions. This scene token retrieves similar samples to guide the current decision. Through our evaluations, our method demonstrates impressive closed-loop performance with reduced training data requirements. Specifically, we achieved performance that is near state-of-the-art (SOTA) using only 1/73 of the data. Additionally, we improved the driving scores on the Town05 short and long benchmarks by 5.3% and 42.6%, respectively, compared to LeapAD [12], and attained the best performance in DriveArena [14]. Furthermore, extensive ablation studies validate the efficiency of the proposed modules and highlight the continuous learning ability and transferability of knowledge within our system."}, {"title": "APPENDIX", "content": "Our dual-process decision-making module outputs meta-actions (e.g., \u201cAC\u201d, \u201cDC\u201d, \u201cLCL\u201d, \u201cLCR\u201d, \u201cIDLE\u201d, \u201cSTOP\u201d), which serve as inputs for trajectory generation. A PID controller then tracks these trajectories to compute the final control signals, such as steering, throttle, and brake.\nTo enhance the effectiveness of route tracking, we addressed the issue of the potentially large gap (up to several dozen meters) between the default waypoints provided by CARLA. By leveraging high-definition maps, we converted these sparse waypoints into dense path points, spaced 1 meter apart, thus creating a precise reference path for the vehicle. Furthermore, when needed, we incorporated information about adjacent lanes along this reference path to support lane-changing operations, such as overtaking or obstacle avoidance. This enhancement not only allows the vehicle to deviate from the navigation waypoints as required temporarily but also significantly improves the flexibility and execution capability of the control system.\nCombining detailed lane data, our controller uses a planner provided by LimSim [15] to generate trajectories for the next 5 seconds, ensuring that the vehicle travels on the desired path. The actions \"AC\" and \"DC\" determine the target state by calculating the target acceleration from the current acceleration, establishing the speed and position 5 seconds ahead. The actions \u201cLCL\u201d and \u201cLCR\u201d use a spatio-temporal sampling strategy to sample target positions within the target lane and the speed range to determine the target state. These target states are inputted into the trajectory optimizer in Fren\u00e9t coordinate [70], generating quintic polynomial trajectories. As a result, the trajectory is optimized and selected based on cost factors such as smoothness, speed matching, acceleration, jerk, and obstacle avoidance, resulting in the optimal trajectory.\nOur approach can utilize alternative methods rather than relying on high-definition maps. Techniques like those in DriveCot [71] and TransFuser [42] leverage distinct neural networks to predict future paths from camera images and sparse navigation data, seamlessly aligning with controller design while preserving our core methodology."}]}