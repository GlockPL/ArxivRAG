{"title": "Optimus: Accelerating Large-Scale Multi-Modal LLM Training by Bubble Exploitation", "authors": ["Weiqi Feng", "Yangrui Chen", "Shaoyu Wang", "Yanghua Peng", "Haibin Lin", "Minlan Yu"], "abstract": "Multimodal large language models (MLLMs) have extended\nthe success of large language models (LLMs) to multiple\ndata types, such as image, text and audio, achieving signifi-\ncant performance in various domains, including multimodal\ntranslation, visual question answering and content generation.\nNonetheless, existing systems are inefficient to train MLLMs\ndue to substantial GPU bubbles caused by the heterogeneous\nmodality models and complex data dependencies in 3D par-\nallelism. This paper proposes Optimus, a distributed MLLM\ntraining system that reduces end-to-end MLLM training time.\nOptimus is based on our principled analysis that schedul-\ning the encoder computation within the LLM bubbles can\nreduce bubbles in MLLM training. To make scheduling en-\ncoder computation possible for all GPUs, Optimus searches\nthe separate parallel plans for encoder and LLM, and adopts\na bubble scheduling algorithm to enable exploiting LLM bub-\nbles without breaking the original data dependencies in the\nMLLM model architecture. We further decompose encoder\nlayer computation into a series of kernels, and analyze the\ncommon bubble pattern of 3D parallelism to carefully opti-\nmize the sub-millisecond bubble scheduling, minimizing the\noverall training time. Our experiments in a production cluster\nshow that Optimus accelerates MLLM training by 20.5%-\n21.3% with ViT-22B and GPT-175B model over 3072 GPUs\ncompared to baselines.", "sections": [{"title": "1 Introduction", "content": "Multimodal Large Language Models (MLLMs) continue the\nhot of Large Language Models (LLMs) and further extend\nLLM's capability to understand and generate content from\nmultiple modalities (e.g., text, images, and audio). MLLMs,\nsuch as GPT-4V [22], Google Gemini [29], Grok-1.5 Vi-\nsion [33] and LLava [19], have achieved remarkable progress\nin various domains, such as visual question answering [2, 20],\nmultimodal translation [28, 34], and content generation and\nunderstanding [22, 29, 39]. Notably, the computational de-\nmands of MLLMs are substantial, emphasizing the urgent\nneed to enhance training performance to fully leverage their\ncapabilities.\nMLLMs typically involve the integration of multiple encoders,\neach tailored to process specific modalities, combined with\na giant language model component. The multimodal data is\npassed to respective encoders, and the output is combined to\nserve as the input of the language model.\nThe multimodal encoders and the language model vary greatly\nin functionalities, architectures, and data input sizes, leading\nto different resource demands. However, existing distributed\ntraining systems are mainly designed for sequential unimodal\n(e.g., MegaScale [14], Megatron-LM [21], Chimera [17]), and\nfall short in MLLMs training with over 40% idle GPU cycles\nwhen we train a large MLLM (several hundred of billions of\nparameters) using Megatron-LM and more than 3,000 GPUs.\nAfter analyzing typical MLLM training tasks, we made two\nkey observations. (1) The communication of 3D parallelism\nis extensive and frequent, leading to long GPU idle time. (2)\nThe pipeline stages of MLLM are imbalanced and the data\ndependency between adjacent pipeline stages results in long\ndata waiting time. Existing solutions can be classified into two\ncategories: (1) optimizing LLM, e.g., Megatron-LM and Zero-\nbubble pipeline [24]; (2) optimizing multimodal encoders,\ne.g., DistMM [13]. Nonetheless, none of the existing works\nconsider LLM and encoders together and we will show in\nSection 2.2 that around 48% GPU cycles are wasted in our\ninternal large-scale MLLM training task.\nIn this paper, we propose Optimus, a distributed MLLM train-\ning system that enables the scheduling of encoder compu-\ntation within LLM bubbles to achieve performant 3D paral-\nlism. However, it is difficult to schedule encoder computa-\ntion within LLM bubbles based on existing training frame-\nworks because of three main reasons.\nFirst, existing training frameworks, e.g., Megatron-LM [21],\nMegaScale [14], and zero-bubble pipeline [24], apply unified\nparallel strategies to MLLM models, distributing encoder\nand LLM layers across different GPUs. As a result, most"}, {"title": "2 Background", "content": "2.1 Multimodal LLM Characteristics\nMultimodal LLMs are increasingly important. These mod-\nels inherit the foundational principles of LLMs, integrating\nadvanced natural language processing techniques while ex-\npanding their scope to encompass diverse data modalities.\nGPT-4 [22] represents a prominent example of a multimodal\nmodel that extends the capabilities and success of its predeces-\nsors to encompass multimodal understanding and generation,\ndemonstrating human-level performance in various bench-\nmark tests with inputs of both images and text.\nMultimodal large language model (MLLM) comprises three\nkey parts: one or multiple modality encoders, input projectors,\nand a large language model backbone [36]. The Modality\nEncoders are designed to encode inputs from non-textual\nmodalities into respective features, while the input projector\naligns features from these modalities with the text feature\nspace. Ultimately, the LLM backbone utilizes aligned fea-\ntures from various modalities and textual features as its input.\nFigure 1 illustrates the architecture of the MLLM. We exclude\nthe input projector from our discussion due to its relatively\nminor computational demand compared to the encoder and\nthe LLM (refer to Llava [19]). Additionally, we treat the input\nprojector as the final layer of the modality encoder in our\nanalysis.\nDifferent from homogeneous LLM architecture, multimodal\nLLM has the following unique characteristics.\nDominant Model Size of LLM Backbone: In multimodal\nLLMs, the LLM backbone has a significantly larger number of\nparameters compared to other components such as encoders\nand projectors. For instance, Flamingo [4] boasts a total of 80\nbillion parameters, with its LLM backbone alone comprising\n70 billion parameters.\nDependency between Encoders and LLM Backbone: In\nMLLM training, there are two types of data dependencies be-\ntween encoders and LLM. During the forward pass, encoders\nmust complete the generation of encoded features before the\nLLM backbone can proceed with forwarding. Conversely, in\nthe backward pass, the LLM backbone calculates gradients\nbefore the encoders initiate the backward pass."}, {"title": "2.2 Bubbles in MLLM Training", "content": "Existing LLM pipeline optimizations are not model-agnostic,\nand fall short in MLLM training tasks. In our internal large-\nscale MLLM training tasks with ViT encoder and GPT back-\nbone (over 100B parameters), we train Megatron-LM with\nmore than 3,000 NVIDIA GPUs and observe more than 48%\nGPU cycle idleness when applying multiple SOTA techniques,\nincluding MegaScale [14], Zero Bubble Pipeline [24], fine-\ngrained communication-computation overlapping [32]. We\nanalyze the profiled timeline to identify and investigate the\noccurrences of GPU idleness (i.e., bubbles). Table 1 shows\nthe total time and percentage of average training step time\n(5.12s) occupied by different types of bubbles.\nThese bubbles can be classified into three categories based on\ntheir underlying causes.\n(1) Communication in Data Parallelism (DP). Data paral-\nlism requires communication to aggregate gradients, lead-\ning to GPU idle time during the communication. Specifically,\nMegaScale [14] and Megatron-LM [26] use the distributed\noptimizer (similar to Pos+g in ZeRO [25]) to save memory\nfor large model training, which performs two collective com-\nmunications (all-gather and reduce-scatter). At the start of\neach training step, an all-gather operation gathers updated\nparameters from all data parallel (DP) ranks, resulting in a\nDP all-gather bubble (occupying 3.3% of the training time).\nAt the end of the training step, reduce-scatter is performed\nto aggregate gradients, leading to a DP reduce-scatter bubble\n(occupying 8.9% of the training time). It should be noted\nthat overlapping optimization in data parallelism proposed\nin Megascale [14] have already been applied and above DP\ncommunications are required for the first model chunk which\ncan not be hidden because of the nature of synchronous train-\ning [14].\n(2) Dependency in Pipeline Parallelism (PP). Despite apply-\ning pipeline send-receive overlap optimization from Megas-\ncale [14], pipeline bubbles still occur due to the inherent data\ndependencies between stages during the forward and back-\nward passes. It should be noted that Zero Bubble Pipeline\ncannot eliminate pipeline bubbles in MLLM training, owing\nto the required changes in the optimizer [24] (refer to discus-\nsions in \u00a77). Figure 2 illustrates the MLLM training pipeline\nschedule, which consists of three phases: warm-up (forward\nonly), steady (one forward and one backward), and cool-down\n(backward only). Throughout pipeline training, three types of\nbubbles arise:\n\u2022 PP warm-up bubbles occur at all stages except the initial\none due to the forward dependency of the first forward\npass, averaging 5.0% of the training time.\n\u2022 PP cool-down bubbles occur at all stages except the\ninitial one due to the backward dependency of the final\nbackward pass, averaging 9.2% of the training time.\n\u2022 Other PP bubbles manifest at all stages except the last\none due to dependencies of other forward and backward\npasses, occupying 8.7% of training time. For instance,\nPP bubbles emerge immediately after the PP warm-up\nphase due to the backward dependency of the initial back-\nward pass. Additionally, in cases of imbalanced pipeline\nstages caused by MLLM's heterogeneous model, there\nare additional pipeline bubbles not depicted in Figure 2.\n(3) Communications in Tensor Parallelism (TP). Ten-\nsor parallelism entails partitioning individual model layers\nacross multiple GPUs, necessitating communication during\nforward and backward passes to synchronize between GPUs.\nIn Megatron-LM, each forward or backward pass of a trans-\nformer layer involves two all-gather and two reduce-scatter\nkernels [15]. Figure 3 provides a detailed view of CUDA\ncomputation and communication kernels during two GPT-\n175B [6] layer forward passes. In the CUDA communication\nstream, green kernels represent all-gather communications,\nwhile blue kernels denote reduce-scatter communications.\nThe compute stream idles during these communications. Typ-\nically, these TP bubbles last for sub-millisecond durations,\naveraging around 300 \u00b5s. However, during MLLM training,\nthere are thousands of TP bubbles, totaling 11.2% of the train-\ning time."}, {"title": "2.3 Challenges", "content": "To minimize bubbles in MLLM training, we aim to lever-\nage the distinct dual-component structure of MLLM, which\nincludes encoders and the LLM backbone. We have noted\ntwo key observations. Firstly, the majority of bubbles dur-\ning MLLM training tend to occur during the forward and\nbackward passes of the LLM backbone, with around 90% of\nthese bubbles arising from LLM communication, as indicated\nin Table 1. Secondly, the encoders require fewer computa-\ntional operations (FLOPs) than the LLM backbone due to\ntheir smaller number of parameters [5, 8, 11, 18, 19].\nIn response, we propose to schedule encoder computation\nin LLM bubbles (occurring during communication in LLM)\nto reduce bubbles throughout the MLLM training process.\nWe identify three main challenges of scheduling encoder com-\nputation to LLM bubbles.\nChallenge 1: Only a few GPUs have both encoder and\nLLM model states. Current training systems [21, 38] use\npipeline parallelism to parallelize the MLLM as a single\npipeline. Due to the dependency between the encoder and\nLLM, encoder layers are assigned to earlier pipeline stages,\nwhile LLM layers are assigned to later pipeline stages. Con-\nsequently, only one pipeline stage typically contains both\nencoder and LLM layers. To illustrate, Figure 4 demonstrates\nthe application of 3D parallelism (DP=1, PP=4, TP=2) to\nparallelize MLLM across 8 GPUs, where only 2 GPUs in\npipeline stage 1 possess both encoder and LLM model states.\nThe remaining 6 GPUs are incapable of executing encoder\ncomputations during LLM bubbles because they lack encoder\nmodel states.\nChallenge 2: Complex Dependencies in MLLM Train-\ning. The intricate dependencies inherent in MLLM training\npose significant challenges when scheduling encoder com-\nputation within LLM bubbles. Firstly, in synchronous train-\ning, the utilization of LLM bubbles is restricted to executing\nthe required encoder computation solely within the current\ntraining iteration (iteration dependency). Secondly, the depen-\ndency within the encoder pipeline requires scheduling the for-\nward computation of the current encoder pipeline stage i after\nthe completion of the previous encoder stage, and schedul-\ning the backward computation after the subsequent encoder\nstage concludes. Lastly, the encoder-LLM dependency en-\ntails a microbatch-level dependency, where the encoder must\ncomplete the forward pass of microbatch i before the LLM\npipeline initiates the forward pass of microbatch i, and simi-\nlarly, the encoder can commence the backward pass of micro-\nbatch i after the LLM pipeline completes the backward pass\nof microbatch i.\nChallenge 3: Sub-millisecond LLM bubbles. Existing\nframeworks like MegaScale [14] and Megatron-LM [21] typ-\nically schedule in the unit of layers. However, bubbles in"}, {"title": "3 Design Decisions and System Overview", "content": "We discuss the core design decisions that drive Optimus de-\nsign and provide an overview of Optimus. The next section\ndiscusses the detailed design.\n3.1 Design Decisions\nDesign decision 1: Colocate encoders and LLM with sep-\narate parallelism. To ensure that each GPU possesses both\nencoder and LLM model states, we propose assigning sep-\narate parallel plans to encoders and LLMs across all GPUs.\nThis strategy is illustrated in Figure 5, where using parallel\nplan (DP=2, PP=2, TP=2) for encoders and (DP=1, PP=4,\nTP=2) for LLM. Each GPU retains both encoder and LLM\nmodel states, and then it becomes feasible for all GPUs to\nexecute encoder computations during LLM bubbles. Note\nthat colocating both the encoder and LLM states may require\nmore GPU memory and we analyze the memory overhead in\nSection 4.5.\nDesign decision 2: Dual-Stage Dependency Management.\nWe use two stages to handle complex dependencies in MLLM\ntraining: local scheduling and global ordering. Each encoder\npipeline undergoes local scheduling, which schedules en-\ncoder computations with available LLM bubbles, adhering\nto the iteration-dependency and encoder-internal dependen-\ncies. Global ordering ensures microbatch-level dependency\nbetween encoders and LLM by sequencing the encoder's\nending times forward and the encoder's starting times back-\nward across microbatches. This involves comparing times-\ntamps to verify encoder-LLM dependency compliance. As\nshown in Figure 6, local scheduling is applied independently\nto two encoder pipelines, maintaining iteration dependency\nand encoder-internal dependency. In global ordering, times-\ntamps across all microbatches (totaling 8) are checked to\nconfirm that encoder-LLM dependencies are met.\nDesign Decision 3: Schedule encoder computation at Ker-\nnel Level. Decomposing the encoder layer into kernels en-\nables efficient utilization of sub-millisecond bubbles. How-\never, TP communication kernels in the encoder layer compete\nfor link bandwidth during LLM TP bubbles, causing longer\ntime per iteration. To resolve this, we must additionally sched-\nule encoder communication kernels during LLM compute\nOptimus is a distributed training system designed for MLLM,\nenabling the scheduling of encoder computation within LLM\nbubbles to improve end-to-end training latency. To tackle chal-\nlenges in Section 3.1, Optimus has two components, which\nare the model planner and bubble scheduler.\nModel Planner. The model planner partitions encoders and\nthe LLM backbone separately to all given GPUs (address-\ning Challenge 1 in \u00a73.1). Initially, the planner determines\nthe 3D parallelism plan (DPllm,PPllm,TPllm) for the LLM\nbackbone based on insights in Megatron-LM [21]. Subse-\nquently, the planner enumerates potential 3D parallelism plans\n(DPenc, PP enc, TPenc) for the encoders, considering the avail-\nable GPU memory after the deployment of the LLM. With the\nmodel planner, each GPU holds both LLM and encoder model\nstates, enabling encoder computation during LLM bubbles.\nThe encoder and LLM model parallel plans are provided as\ninput to the bubble scheduler, where Optimus selects parallel\nplans based on the output schedule with the shortest execution\ntime.\nBubble Scheduler. Bubble scheduler is responsible for\nscheduling encoder computation into LLM bubbles. Given\nthat the LLM training pipeline divides data into multiple mi-\ncrobatches, the scheduler schedules encoder computations\non a per-microbatch basis and satisfies encoder-LLM data\ndependency at microbatch level (addressing Challenge 2 in\n\u00a73.1). In addition, the scheduler breaks down encoder com-\nputation into kernel granularity, to enable the utilization of\nsub-millisecond bubbles (TP bubbles) during LLM training\n(addressing Challenge 3 in \u00a73.1).\nOptimus uses the model planner to devise parallel plans for\nboth encoders and LLMs. Subsequently, for each encoder par-\nallel plan, Optimus utilizes the bubble scheduler to generate\na schedule and estimate the latency. Ultimately, Optimus se-\nlects the schedule with the shortest training time to schedule\nencoder computation into LLM bubbles. The workflow of\nOptimus is outlined in Algorithm 1."}, {"title": "4 Optimus Design", "content": "Section 4.1 describes how the model planer searches the par-\nallel plans for the encoder, Section 4.2 details how the bub-\nble scheduler exploits the coarse-grained and fined-grained\nbubbles through local scheduling, Section 4.3 discusses how\nthe bubble scheduler handles encoder-LLM data dependen-\ncies through global ordering, Section 4.4 designs the bubble\nscheduling in multi-branch encoder models, and Section 4.5\nanalyzes the memory consumption of the bubble scheduling\nalgorithm.\n4.1 Model Planner\nSearching separate parallel plans. Initially, the planner de-\ntermines the 3D parallelism plan (DP1lm, PPllm, TPilm) for the\nLLM backbone based on insights in Megatron-LM [21]. Sub-\nsequently, the planner enumerates potential 3D parallelism\nplans (DPenc, PPenc, TPenc), ensuring that PPenc is a factor of\nPPlum and TPenc is a factor of TPIlm. In practice, PPIlm can reach up to 64 and TPIlm up to 8 for training large language\nmodels (LLMs) [21]. Consequently, there are generally no\nmore than 28 encoder parallel plans available, with up to 7\noptions for PPenc and 4 for TPenc\nColocating encoders and LLM. To guarantee that each GPU\ncan perform encoder computations during LLM downtime,\nthe model planner assigns both encoder and LLM model states\nto every GPU. As illustrated in Figure 5, all GPUs contain\nmodel states for both the encoder (depicted in green) and the\nLLM (shown in red). Without such colocation, many GPUs\nwould lack the necessary encoder model states to execute\nencoder computations.\nPrune parallel plans based on memory constraint. As we\ncolocate the encoder and LLM stages on GPUs, we calculate\nthe memory requirements for both encoder and LLM states\nbased on the chosen parallelism plan, referencing memory\nanalysis in [15]. Plans that violate GPU memory capacity are\nimmediately pruned.\nConstructing separate microbatches. Due to the different\nparallel plans for encoders and LLMs, there are m = $\\frac{DP_{enc}}{DP_{llm}}$\ntimes more encoder pipelines than LLM pipelines for a given\nset of GPUs (e.g. m = 2 in Figure 5). For GPUs belonging\nto the same LLM pipeline, there are m encoder pipelines\ncolocated. Depending on the number of microbatches $N_{mb}$\nutilized in LLM pipeline training, the data from these $N_{mb}$\nmicrobatches needs to be distributed among these m encoder\npipelines, where each encoder pipeline i handles forward and\nbackward computations for $\\frac{N_{mb}}{m}$ microbatch data. The model\nplanner enumerates possible ways to partition these $N_{mb}$ mi-\ncrobatches among the m encoder pipelines. For instance, if\nthere are 8 microbatches in the LLM training and m = 2 en-\ncoder pipelines, there are a total of 7 possible partitioning\noptions, such as [1,7], [2,6], ..., [7, 1].\n4.2 Bubble Scheduling\nAlthough LLM bubbles in different GPUs have different start\ntimes and duration, there is one common pattern of LLM\nbubbles as shown in Figure 8. There is one single big bubble\n(the sum of DP all-gather bubble and PP-warm bubble) before\nany LLM computation starts and one single big bubble (the\nsum of PP-cooldown bubble and reduce-scatter bubble) after\nall LLM computation finishes. And there are many small\nbubbles (PP bubbles and TP bubbles) [15, 21, 26] interleaved\nwith LLM computation.\nDesign decision 2: The bubble scheduler, as described in Algo-\nrithm 2, initially engages in coarse-grained bubble exploita-\ntion by creating initial schedules that incorporate encoder\ncomputations within the bubbles positioned before and after\nLLM computations (line 2). However, it's possible that these\ntwo bubbles may not allow sufficient time to complete all\nencoder computations, leading to some encoder computations\nbeing unscheduled within bubbles. To reduce the total training\ntime, the bubble scheduler then executes fine-grained bubble\nexploitation. This involves refining the schedule by allocating\nencoder forward computations to the bubbles that alternate\nwith LLM computations (line 7), followed by assigning en-\ncoder backward computations to these same bubbles (line 8).\nThe final output of the bubble scheduler is the schedule that\nachieves the shortest possible runtime.\nCoarse-grained bubble exploitation. For each potential data\npartitioning approach, the bubble scheduler initializes the\nschedule by scheduling encoder forward operations to occur\nbefore LLM computations and encoder backward operations\nto occur after LLM computations. Figure 9 illustrates the\ninitialized schedule when there are m = 2 encoder pipelines\nand the data partitioning approach is [3, 5], i.e., 3 microbatches\nis allocated to the first encoder pipeline and 5 for the second\nencoder pipeline.\nFine-grained bubble exploitation. The OptimizeSchedule\nfunction (line 15 at Algorithm 2) refines the initial sched-\nule through an iterative approach. Initially, the bubble sched-\nuler employs findCritical to identify the encoder pipeline\nwhose computation is on the critical path of the end-to-\nend MLLM training (line 17). Subsequently, it utilizes\nAssignKernels to allocate one microbatch of this encoder\ncomputation to bubbles interleaved with LLM computations\n(line 18). If there are sufficient bubbles available for schedul-\ning encoder computation and encoder-LLM data dependen-\ncies are met, the bubble scheduler repeats this process. Other-\nwise, it returns the current optimized schedule.\nWhen optimizing the schedule for encoder forward compu-\ntation (line 7 in Algorithm 2), findCritical identifies the\nencoder pipeline whose forward computation is critical. As\nshown in the left portion of Figure 10, encoder pipeline 2's\nforward computation (microbatch 8 forward) is initially on the\ncritical path in the initial schedule. After successfully schedul-\ning that microbatch forward to later bubbles, encoder pipeline\n1 assumes the critical path position. This iterative process\nleads to a reduction in the end-to-end MLLM training time\nafter each step. Similarly, encoder pipelines whose backward\ncomputation is critical are illustrated in the right portion of\nFigure 10. After each step, the bubble scheduler must verify\nif it still satisfies the encoder-LLM data dependency before\nproceeding with the next steps.\nWhen scheduling encoder computation to bubbles interleaved\nwith LLM compute (AssignKernels at line 18), the bubble"}, {"title": "4.3 Address Encoder-LLM dependency", "content": "The model planner provides different parallel strategies for\nencoders and LLM backbone, including the number of micro-\nbatches, resulting in complex data dependencies both between\nand within the encoder and LLM. Also, the communication"}]}