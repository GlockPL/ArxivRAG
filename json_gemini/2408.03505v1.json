{"title": "Optimus: Accelerating Large-Scale Multi-Modal LLM Training by Bubble Exploitation", "authors": ["Weiqi Feng", "Yangrui Chen", "Shaoyu Wang", "Yanghua Peng", "Haibin Lin", "Minlan Yu"], "abstract": "Multimodal large language models (MLLMs) have extended\nthe success of large language models (LLMs) to multiple\ndata types, such as image, text and audio, achieving signifi-\ncant performance in various domains, including multimodal\ntranslation, visual question answering and content generation.\nNonetheless, existing systems are inefficient to train MLLMs\ndue to substantial GPU bubbles caused by the heterogeneous\nmodality models and complex data dependencies in 3D par-\nallelism. This paper proposes Optimus, a distributed MLLM\ntraining system that reduces end-to-end MLLM training time.\nOptimus is based on our principled analysis that schedul-\ning the encoder computation within the LLM bubbles can\nreduce bubbles in MLLM training. To make scheduling en-\ncoder computation possible for all GPUs, Optimus searches\nthe separate parallel plans for encoder and LLM, and adopts\na bubble scheduling algorithm to enable exploiting LLM bub-\nbles without breaking the original data dependencies in the\nMLLM model architecture. We further decompose encoder\nlayer computation into a series of kernels, and analyze the\ncommon bubble pattern of 3D parallelism to carefully opti-\nmize the sub-millisecond bubble scheduling, minimizing the\noverall training time. Our experiments in a production cluster\nshow that Optimus accelerates MLLM training by 20.5%-\n21.3% with ViT-22B and GPT-175B model over 3072 GPUs\ncompared to baselines.", "sections": [{"title": "1 Introduction", "content": "Multimodal Large Language Models (MLLMs) continue the\nhot of Large Language Models (LLMs) and further extend\nLLM's capability to understand and generate content from\nmultiple modalities (e.g., text, images, and audio). MLLMs,\nsuch as GPT-4V [22], Google Gemini [29], Grok-1.5 Vi-\nsion [33] and LLava [19], have achieved remarkable progress\nin various domains, such as visual question answering [2, 20],\nmultimodal translation [28, 34], and content generation and\nunderstanding [22, 29, 39]. Notably, the computational de-\nmands of MLLMs are substantial, emphasizing the urgent\nneed to enhance training performance to fully leverage their\ncapabilities.\nMLLMs typically involve the integration of multiple encoders,\neach tailored to process specific modalities, combined with\na giant language model component. The multimodal data is\npassed to respective encoders, and the output is combined to\nserve as the input of the language model.\nThe multimodal encoders and the language model vary greatly\nin functionalities, architectures, and data input sizes, leading\nto different resource demands. However, existing distributed\ntraining systems are mainly designed for sequential unimodal\n(e.g., MegaScale [14], Megatron-LM [21], Chimera [17]), and\nfall short in MLLMs training with over 40% idle GPU cycles\nwhen we train a large MLLM (several hundred of billions of\nparameters) using Megatron-LM and more than 3,000 GPUs.\nAfter analyzing typical MLLM training tasks, we made two\nkey observations. (1) The communication of 3D parallelism\nis extensive and frequent, leading to long GPU idle time. (2)\nThe pipeline stages of MLLM are imbalanced and the data\ndependency between adjacent pipeline stages results in long\ndata waiting time. Existing solutions can be classified into two\ncategories: (1) optimizing LLM, e.g., Megatron-LM and Zero-\nbubble pipeline [24]; (2) optimizing multimodal encoders,\ne.g., DistMM [13]. Nonetheless, none of the existing works\nconsider LLM and encoders together and we will show in\nSection 2.2 that around 48% GPU cycles are wasted in our\ninternal large-scale MLLM training task.\nIn this paper, we propose Optimus, a distributed MLLM train-\ning system that enables the scheduling of encoder compu-\ntation within LLM bubbles to achieve performant 3D paral-\nlelism. However, it is difficult to schedule encoder computa-\ntion within LLM bubbles based on existing training frame-\nworks because of three main reasons.\nFirst, existing training frameworks, e.g., Megatron-LM [21],\nMegaScale [14], and zero-bubble pipeline [24], apply unified\nparallel strategies to MLLM models, distributing encoder\nand LLM layers across different GPUs. As a result, most"}, {"title": "2 Background", "content": "GPUs contain only LLM model states, unable to perform\nencoder computation during LLM bubbles. In contrast, we\nuse separate parallel plans for encoders and LLM to colocate\nencoder and LLM model states on each GPU. We enumerate\npotential 3D parallelism plans for the encoder and prune plans\nthat violate the GPU memory constraint.\nSecond, the presence of complex data dependencies within\nMLLM imposes constraints on the scheduling of encoder com-\nputation within LLM bubbles. There are dependencies related\nto synchronous training iterations and internal dependencies\nwithin the encoder (see Section 2.3). The most intricate of\nthese is the encoder-LLM microbatch-level data dependency,\nwhich necessitates that the encoder completes its forward pass\nbefore the LLM begins its forward pass for each microbatch\nand that the encoder begins its backward pass only after the\nLLM has completed its backward pass for each microbatch.\nTo manage these dependencies, we employ a two-stage depen-\ndency management approach: local scheduling to address the\nfirst two types of dependencies and global ordering to handle\nthe encoder-LLM microbatch-level dependencies.\nThird, the LLM bubble duration varies from sub-milliseconds\nto hundreds of milliseconds, making bubble reduction a hur-\ndle to overcome. Existing frameworks [17, 21, 24] schedule\nin the unit of layers, and the sub-millisecond bubble is too\nshort to complete even a single encoder layer forward or back-\nward. Hence, we decompose encoder layer computation into\na series of kernels to utilize the sub-millisecond bubbles. Fur-\nther, we analyze the common patterns of LLM bubbles, and\noptimize the bubble schedule by scheduling encoder kernel\ncomputation to bubbles interleaved with LLM computation\nto minimize the overall training time.\nWe have implemented Optimus based on Megatron-LM, in-\ncluding the above design points. We conduct extensive ex-\nperiments using multiple representative MLLM models. The\nresults are promising - Optimus outperforms state-of-the-art\nbaselines by 20.3% on average and Optimus also scales well\nwith the size of models and GPUs. Our experiments in a pro-\nduction cluster show that Optimus accelerates MLLM training\nby 20.5%-21.3% with ViT-22B and GPT-175B model over\n3072 GPUs compared to baselines."}, {"title": "2.1 Multimodal LLM Characteristics", "content": "Multimodal LLMs are increasingly important. These mod-\nels inherit the foundational principles of LLMs, integrating\nadvanced natural language processing techniques while ex-\npanding their scope to encompass diverse data modalities.\nGPT-4 [22] represents a prominent example of a multimodal\nmodel that extends the capabilities and success of its predeces-\nsors to encompass multimodal understanding and generation,\ndemonstrating human-level performance in various bench-\nmark tests with inputs of both images and text.\nMultimodal large language model (MLLM) comprises three\nkey parts: one or multiple modality encoders, input projectors,\nand a large language model backbone [36]. The Modality\nEncoders are designed to encode inputs from non-textual\nmodalities into respective features, while the input projector\naligns features from these modalities with the text feature\nspace. Ultimately, the LLM backbone utilizes aligned fea-\ntures from various modalities and textual features as its input.\nWe exclude\nthe input projector from our discussion due to its relatively\nminor computational demand compared to the encoder and\nthe LLM (refer to Llava [19]). Additionally, we treat the input\nprojector as the final layer of the modality encoder in our\nanalysis.\nDifferent from homogeneous LLM architecture, multimodal\nLLM has the following unique characteristics.\nDominant Model Size of LLM Backbone: In multimodal\nLLMs, the LLM backbone has a significantly larger number of\nparameters compared to other components such as encoders\nand projectors. For instance, Flamingo [4] boasts a total of 80\nbillion parameters, with its LLM backbone alone comprising\n70 billion parameters.\nDependency between Encoders and LLM Backbone: In\nMLLM training, there are two types of data dependencies be-\ntween encoders and LLM. During the forward pass, encoders\nmust complete the generation of encoded features before the\nLLM backbone can proceed with forwarding. Conversely, in\nthe backward pass, the LLM backbone calculates gradients\nbefore the encoders initiate the backward pass."}, {"title": "2.2 Bubbles in MLLM Training", "content": "Existing LLM pipeline optimizations are not model-agnostic,\nand fall short in MLLM training tasks. In our internal large-\nscale MLLM training tasks with ViT encoder and GPT back-\nbone (over 100B parameters), we train Megatron-LM with\nmore than 3,000 NVIDIA GPUs and observe more than 48%\nGPU cycle idleness when applying multiple SOTA techniques,\nincluding MegaScale [14], Zero Bubble Pipeline [24], fine-\ngrained communication-computation overlapping [32]. We\nanalyze the profiled timeline to identify and investigate the"}, {"title": "2.3 Challenges", "content": "occurrences of GPU idleness (i.e., bubbles).\nTo minimize bubbles in MLLM training, we aim to lever-\nage the distinct dual-component structure of MLLM, which\nincludes encoders and the LLM backbone. We have noted\ntwo key observations. Firstly, the majority of bubbles dur-\ning MLLM training tend to occur during the forward and\nbackward passes of the LLM backbone, with around 90% of\nthese bubbles arising from LLM communication, as indicated"}, {"title": "3 Design Decisions and System Overview", "content": "in Table 1. Secondly, the encoders require fewer computa-\ntional operations (FLOPs) than the LLM backbone due to\ntheir smaller number of parameters [5, 8, 11, 18, 19].\nIn response, we propose to schedule encoder computation\nin LLM bubbles (occurring during communication in LLM)\nto reduce bubbles throughout the MLLM training process.\nWe identify three main challenges of scheduling encoder com-\nputation to LLM bubbles.\nChallenge 1: Only a few GPUs have both encoder and\nLLM model states. Current training systems [21,38] use\npipeline parallelism to parallelize the MLLM as a single\npipeline. Due to the dependency between the encoder and\nLLM, encoder layers are assigned to earlier pipeline stages,\nwhile LLM layers are assigned to later pipeline stages. Con-\nsequently, only one pipeline stage typically contains both\nencoder and LLM layers. To illustrate, demonstrates\nthe application of 3D parallelism (DP=1, PP=4, TP=2) to\nparallelize MLLM across 8 GPUs, where only 2 GPUs in\npipeline stage 1 possess both encoder and LLM model states.\nThe remaining 6 GPUs are incapable of executing encoder\ncomputations during LLM bubbles because they lack encoder\nmodel states.\nChallenge 2: Complex Dependencies in MLLM Train-\ning. The intricate dependencies inherent in MLLM training\npose significant challenges when scheduling encoder com-\nputation within LLM bubbles. Firstly, in synchronous train-\ning, the utilization of LLM bubbles is restricted to executing\nthe required encoder computation solely within the current\ntraining iteration (iteration dependency). Secondly, the depen-\ndency within the encoder pipeline requires scheduling the for-\nward computation of the current encoder pipeline stage i after\nthe completion of the previous encoder stage, and schedul-\ning the backward computation after the subsequent encoder\nstage concludes. Lastly, the encoder-LLM dependency en-\ntails a microbatch-level dependency, where the encoder must\ncomplete the forward pass of microbatch i before the LLM\npipeline initiates the forward pass of microbatch i, and simi-\nlarly, the encoder can commence the backward pass of micro-\nbatch i after the LLM pipeline completes the backward pass\nof microbatch i.\nChallenge 3: Sub-millisecond LLM bubbles. Existing\nframeworks like MegaScale [14] and Megatron-LM [21] typ-\nically schedule in the unit of layers. However, bubbles in"}, {"title": "3.1 Design Decisions", "content": "Design decision 1: Colocate encoders and LLM with sep-\narate parallelism. To ensure that each GPU possesses both\nencoder and LLM model states, we propose assigning sep-\narate parallel plans to encoders and LLMs across all GPUs.\nThis strategy is illustrated in Figure 5, where using parallel\nplan (DP=2, PP=2, TP=2) for encoders and (DP=1, PP=4,\nTP=2) for LLM. Each GPU retains both encoder and LLM\nmodel states, and then it becomes feasible for all GPUs to\nexecute encoder computations during LLM bubbles. Note\nthat colocating both the encoder and LLM states may require\nmore GPU memory and we analyze the memory overhead in\nSection 4.5.\nDesign decision 2: Dual-Stage Dependency Management.\nWe use two stages to handle complex dependencies in MLLM\ntraining: local scheduling and global ordering. Each encoder\npipeline undergoes local scheduling, which schedules en-\ncoder computations with available LLM bubbles, adhering\nto the iteration-dependency and encoder-internal dependen-\ncies. Global ordering ensures microbatch-level dependency\nbetween encoders and LLM by sequencing the encoder's\nending times forward and the encoder's starting times back-\nward across microbatches. This involves comparing times-\ntamps to verify encoder-LLM dependency compliance. As\nshown in Figure 6, local scheduling is applied independently"}, {"title": "3.2 Optimus Overview", "content": "to two encoder pipelines, maintaining iteration dependency\nand encoder-internal dependency. In global ordering, times-\ntamps across all microbatches (totaling 8) are checked to\nconfirm that encoder-LLM dependencies are met.\nOptimus is a distributed training system designed for MLLM,\nenabling the scheduling of encoder computation within LLM\nbubbles to improve end-to-end training latency. To tackle chal-\nlenges in Section 3.1, Optimus has two components, which\nare the model planner and bubble scheduler.\nModel Planner. The model planner partitions encoders and\nthe LLM backbone separately to all given GPUs (address-\ning Challenge 1 in \u00a73.1). Initially, the planner determines\nthe 3D parallelism plan $(DP_{llm}, PP_{llm}, TP_{llm})$ for the LLM\nbackbone based on insights in Megatron-LM [21]. Subse-\nquently, the planner enumerates potential 3D parallelism plans\n$(DP_{enc}, PP_{enc}, TP_{enc})$ for the encoders, considering the avail-\nable GPU memory after the deployment of the LLM. With the\nmodel planner, each GPU holds both LLM and encoder model\nstates, enabling encoder computation during LLM bubbles.\nThe encoder and LLM model parallel plans are provided as\ninput to the bubble scheduler, where Optimus selects parallel\nplans based on the output schedule with the shortest execution\ntime.\nBubble Scheduler. Bubble scheduler is responsible for\nscheduling encoder computation into LLM bubbles. Given\nthat the LLM training pipeline divides data into multiple mi-\ncrobatches, the scheduler schedules encoder computations\non a per-microbatch basis and satisfies encoder-LLM data\ndependency at microbatch level (addressing Challenge 2 in\n\u00a73.1). In addition, the scheduler breaks down encoder com-\nputation into kernel granularity, to enable the utilization of\nsub-millisecond bubbles (TP bubbles) during LLM training\n(addressing Challenge 3 in \u00a73.1).\nOptimus uses the model planner to devise parallel plans for\nboth encoders and LLMs. Subsequently, for each encoder par-\nallel plan, Optimus utilizes the bubble scheduler to generate\na schedule and estimate the latency. Ultimately, Optimus se-\nlects the schedule with the shortest training time to schedule\nencoder computation into LLM bubbles. The workflow of\nOptimus is outlined in Algorithm 1."}, {"title": "4 Optimus Design", "content": "Section 4.1 describes how the model planer searches the par-\nallel plans for the encoder, Section 4.2 details how the bub-\nblescheduler exploits the coarse-grained and fined-grained\nbubbles through local scheduling, Section 4.3 discusses how\nthe bubble scheduler handles encoder-LLM data dependen-\ncies through global ordering, Section 4.4 designs the bubble\nscheduling in multi-branch encoder models, and Section 4.5\nanalyzes the memory consumption of the bubble scheduling\nalgorithm."}, {"title": "4.1 Model Planner", "content": "Searching separate parallel plans. Initially, the planner de-\ntermines the 3D parallelism plan $(DP_{llm}, PP_{llm}, TP_{llm})$ for the\nLLM backbone based on insights in Megatron-LM [21]. Sub-\nsequently, the planner enumerates potential 3D parallelism\nplans $(DP_{enc}, PP_{enc}, TP_{enc})$, ensuring that $PP_{enc}$ is a factor of\n$PP_{llm}$ and $TP_{enc}$ is a factor of $TP_{llm}$. In practice, $PP_{llm}$ can"}, {"title": "4.2 Bubble Scheduling", "content": "reach up to 64 and $TP_{llm}$ up to 8 for training large language\nmodels (LLMs) [21]. Consequently, there are generally no\nmore than 28 encoder parallel plans available, with up to 7\noptions for $PP_{enc}$ and 4 for $TP_{enc}$\nColocating encoders and LLM. To guarantee that each GPU\ncan perform encoder computations during LLM downtime,\nthe model planner assigns both encoder and LLM model states\nto every GPU. As illustrated in Figure 5, all GPUs contain\nmodel states for both the encoder (depicted in green) and the\nLLM (shown in red). Without such colocation, many GPUs\nwould lack the necessary encoder model states to execute\nencoder computations.\nPrune parallel plans based on memory constraint. As we\ncolocate the encoder and LLM stages on GPUs, we calculate\nthe memory requirements for both encoder and LLM states\nbased on the chosen parallelism plan, referencing memory\nanalysis in [15]. Plans that violate GPU memory capacity are\nimmediately pruned.\nConstructing separate microbatches. Due to the different\nparallel plans for encoders and LLMs, there are $m = \\frac{DP_{enc}}{DP_{llm}}$\ntimes more encoder pipelines than LLM pipelines for a given\nset of GPUs (e.g. $m = 2$ in Figure 5). For GPUs belonging\nto the same LLM pipeline, there are m encoder pipelines\ncolocated. Depending on the number of microbatches $N_{mb}$\nutilized in LLM pipeline training, the data from these $N_{mb}$\nmicrobatches needs to be distributed among these m encoder\npipelines, where each encoder pipeline i handles forward and\nbackward computations for $\\frac{N_{mb}}{m}$ microbatch data. The model\nplanner enumerates possible ways to partition these $N_{mb}$ mi-\ncrobatches among the m encoder pipelines. For instance, if\nthere are 8 microbatches in the LLM training and $m = 2$ en-\ncoder pipelines, there are a total of 7 possible partitioning\noptions, such as [1,7], [2,6], ..., [7, 1].\nAlthough LLM bubbles in different GPUs have different start\ntimes and duration, there is one common pattern of LLM\nbubbles as shown in Figure 8. There is one single big bubble\n(the sum of DP all-gather bubble and PP-warm bubble) before\nany LLM computation starts and one single big bubble (the\nsum of PP-cooldown bubble and reduce-scatter bubble) after\nall LLM computation finishes. And there are many small\nbubbles (PP bubbles and TP bubbles) [15, 21, 26] interleaved\nwith LLM computation.\nDesign decision 2: The bubble scheduler, as described in Algo-\nrithm 2, initially engages in coarse-grained bubble exploita-\ntion by creating initial schedules that incorporate encoder\ncomputations within the bubbles positioned before and after\nLLM computations (line 2). However, it's possible that these\ntwo bubbles may not allow sufficient time to complete all\nencoder computations, leading to some encoder computations"}, {"title": "4.3 Address Encoder-LLM dependency", "content": "being unscheduled within bubbles. To reduce the total training\ntime, the bubble scheduler then executes fine-grained bubble\nexploitation. This involves refining the schedule by allocating\nencoder forward computations to the bubbles that alternate\nwith LLM computations (line 7), followed by assigning en-\ncoder backward computations to these same bubbles (line 8).\nThe final output of the bubble scheduler is the schedule that\nachieves the shortest possible runtime.\nCoarse-grained bubble exploitation. For each potential data\npartitioning approach, the bubble scheduler initializes the\nschedule by scheduling encoder forward operations to occur\nbefore LLM computations and encoder backward operations\nto occur after LLM computations.  illustrates the\ninitialized schedule when there are $m = 2$ encoder pipelines\nand the data partitioning approach is [3, 5], i.e., 3 microbatches\nis allocated to the first encoder pipeline and 5 for the second\nencoder pipeline.\nFine-grained bubble exploitation. The OptimizeSchedule\nfunction (line 15 at Algorithm 2) refines the initial sched-\nule through an iterative approach. Initially, the bubble sched-\nuler employs findCritical to identify the encoder pipeline\nwhose computation is on the critical path of the end-to-\nComplexity. Our bubble scheduling algorithm has low com-\nplexity. Given n GPUs and the number of prime factors of n is\n$n_p$, the search space of parallel plans is $C_{n}^{2n_p +1}$. The number of\nmicrobatch partitioning is $O(N_{mb}^{m-1})$. Hence, the complexity\nfor scheduling bubbles is $O(C_{n}^{2n_p +1} *N_{mb} * (F+B))$. For our\nexperimented settings, it usually takes around several minutes\nto calculate the optimal schedule (see \u00a75.3.2), which is also a\none-time cost.\nThe model planner provides different parallel strategies for\nencoders and LLM backbone, including the number of micro-\nbatches, resulting in complex data dependencies both between\nand within the encoder and LLM. Also, the communication"}, {"title": "4.4 Multi-Branch Encoder Scheduling", "content": "of symbols frequently used in the paper\nto, the scheduler integrates necessary peer-to-peer (P2P) communications into\nthe training schedule between the last stage of the encoder\npipeline and the first stage of the LLM pipeline. For instance,\nif encoder pipeline j completes the forward pass for micro-\nbatch i, the scheduler will insert a P2P send (sending activa-\ntions) at the last stage of encoder pipeline j and a P2P receive\n(receiving activations) at the first stage of the LLM pipeline.\nSimilarly, when the LLM pipeline completes the backward\npass for microbatch i, the scheduler adds a P2P send (sending\ngradients) at the first stage of the LLM pipeline and a P2P re-\nceive (receiving gradients) at the last stage of encoder pipeline\nj. In the scenario depicted in Figure 13, the scheduler inserts\n8 pairs of P2P send-receive at devices 1 and 2 to manage\nthe dependencies between encoder pipeline 1 and the LLM\npipeline, with 4 pairs allocated for forward dependencies and\n4 pairs for backward dependencies. Likewise, an additional 8\npairs of P2P send-receive are inserted at devices 3 and 4 to\naddress the dependencies between encoder pipeline 2 and the\nLLM pipeline.\nTo support MLLM with multiple encoders [7, 35],\nthe model planner applies an encoder parallelism plan\n$(DP_{enc}, PP_{enc}, TP_{enc})$ independently for each encoder. For\npipeline parallelism, layers within each encoder are divided\ninto $PP_{enc}$ stages (as illustrated in Figure 14). Each layer of\nevery encoder is then parallelized according to $TP_{enc}$. The\nbubble scheduler breaks down the layers of distinct encoders\ninto kernel-level granularity and arranges their scheduling as\nif these kernels were part of a single encoder. This is because\nthe encoders within MLLM operate independently, without\nany data dependencies between them."}, {"title": "4.5 Memory Analysis", "content": "To, the scheduler integrates necessary peer-to-peer (P2P) communications into\nthe training schedule between the last stage of the encoder\npipeline and the first stage of the LLM pipeline. For instance,\nif encoder pipeline j completes the forward pass for micro-\nbatch i, the scheduler will insert a P2P send (sending activa-\ntions) at the last stage of encoder pipeline j and a P2P receive\n(receiving activations) at the first stage of the LLM pipeline.\nSimilarly, when the LLM pipeline completes the backward\npass for microbatch i, the scheduler adds a P2P send (sending\ngradients) at the first stage of the LLM pipeline and a P2P re-\nceive (receiving gradients) at the last stage of encoder pipeline\nj. In the scenario depicted in Figure 13, the scheduler inserts\n8 pairs of P2P send-receive at devices 1 and 2 to manage\nthe dependencies between encoder pipeline 1 and the LLM\npipeline, with 4 pairs allocated for forward dependencies and\n4 pairs for backward dependencies. Likewise, an additional 8\npairs of P2P send-receive are inserted at devices 3 and 4 to\naddress the dependencies between encoder pipeline 2 and the\nLLM pipeline.\nTo support MLLM with multiple encoders [7, 35],\nthe model planner applies an encoder parallelism plan\n$(DP_{enc}, PP_{enc}, TP_{enc})$ independently for each encoder. For\npipeline parallelism, layers within each encoder are divided\ninto $PP_{enc}$ stages (as illustrated in Figure 14). Each layer of\nevery encoder is then parallelized according to $TP_{enc}$. The\nbubble scheduler breaks down the layers of distinct encoders\ninto kernel-level granularity and arranges their scheduling as\nif these kernels were part of a single encoder. This is because\nthe encoders within MLLM operate independently, without\nany data dependencies between them."}, {"title": "5 Evaluation", "content": "When utilizing $n_{gpu}$ GPUs for MLLM training, the model\nplanner requires $DP_{enc}$ replicated encoder model states and\n$DP_{llm}$ replicated LLM model states based on parallel plans.\nSuppose the number of parameters in the encoder is $\\phi_{enc}$ and\nthe number of parameters in the LLM is $\\phi_{llm}$, with each param-\neter requiring k bytes of memory. The average GPU memory\nusage $MEM_{model}$ for storing model states is calculated as\nfollows:\n$MEM_{model} = \\frac{k \\cdot (DP_{enc} \\cdot \\phi_{enc} + DP_{llm} \\cdot \\phi_{llm})}{n_{gpu}}$"}, {"title": "5.1 Methodology", "content": "In comparison to existing 3D parallel training solutions, where\n$DP_{enc} = DP_{llm}$, the estimated memory overhead $MEM_{overhead}$\ncan be expressed as:\n$MEM_{overhead} = \\frac{k(DP_{enc} \u2013 DP_{llm})\\phi_{enc}}{n_{gpu}}$\nWith a larger value of $DP_{enc}$, there is a higher memory over-\nhead due to more replicated encoder model states. However,\nthis results in less complex encoder internal dependencies\nduring scheduling (indicated by a smaller $PP_{enc}$). Model plan-\nner filters the encoder parallel plans based on the estimated\nmemory usage $MEM_{model}$, ensuring adherence to GPU mem-\nory constraints. In practice, the memory overhead typically\namounts to less than 12% in our evaluation (\u00a75.3.1) because\n$\\phi_{enc}$ is small (e.g., the largest vision encoder has 22 billion pa-\nrameters [10]) and k is small (e.g., k = 6 when using bf 16 pa-\nrameters and fp32 gradients with distributed optimizer [1]).\nWe have developed Optimus based on the open-source\nMegatron-LM framework [1] and evaluate Optimus on train-\ning large-scale mulitimodal LLMs.\nTestbed. We conduct our experiments in a production training\ncluster with thousands of NVIDIA Hopper GPUs. Each GPU\nhas 80GB memory and 989TFLOPS computing performance.\nThe intra-server connection is NVLink and the inter-server\nconnection is a high-bandwidth RDMA network.\nMLLM models. We examine the performance of Optimus\nusing various sizes of image encoders and LLM backbones.\nThe image encoders include three sizes: ViT-22B [10], ViT-\n11B, and ViT-5B, which are scaled-down versions of ViT-\n22B with smaller hidden sizes. For the language models, we\nemploy two sizes: LLAMA-70B [31] and GPT-175B [6].\nAppendix A includes detailed model configurations.\nBaselines. We use three open-sourced MLLM training sys-\ntems with one strawman method as our baselines for compar-\nison."}, {"title": "5.2 End-to-End Performance", "content": "\u2022 PyTorch FSDP [37]: FSDP is a distributed data-parallel\ntraining module designed to scale PyTorch models across mul-\ntiple GPUs with minimal code changes. It shards the model\nacross GPUs, runs All_Gather to collect all shards from all\nranks to recover the full parameter for forward and backward\ncomputation, and runs Reduce_Scatter to synchronize gradi-\nents.\n\u2022 Alpa [38]: Alpa is a compiler system for distributed DL\ntraining that automatically generates parallel execution plans\ncovering 3D parallelisms.\n5.2.1 Weak-Scaling Experiment\nExperiment Setup. To study the ability to train large models,\nwe follow common ML practice to scale the model size along\nwith the number of GPUs. We evaluate the weak-scaling train-\ning performance of Optimus and baselines based on model\nconfigurations in Table 3.\nResults. Figure 15 presents a comparison between Optimus\nand baseline methods across various sizes of MLLM. Optimus\nachieves a speedup of up to 1.22\u00d7 compared to Megatron-\nLM and 1.18\u00d7 compared to the Megatron-LM balanced. Alpa\nand FSDP face GPU out-of-memory (OOM) issues with these\nmodels."}, {"title": "5.2.2 Strong-Scaling Experiment", "content": "\u2022 Megatron-LM [21]: Megatron-LM is a state-of-the-art LLM\ntraining framework that integrates 3D parallelism techniques.\nMegatron-LM is designed for symmetric transformer models,\nand we place multimodal encoders to the preprocess in the\nfirst pipeline stage to adapt to MLLM training.\n\u2022 Megatron-LM balanced: In this strawman method, we bal-\nance the layer partitioning among different pipeline stages\nwith an interleaved 1F1B pipeline schedule. Considering the\nheterogeneity in MLLM submodules, we use a dynamic pro-\ngramming algorithm to assign different layers of submodules\nto pipeline stages and achieve approximately the same com-\nputation amount. The DP algorithm is a simplified version\nof Alpa's inter-operator DP algorithm and is included in Ap-\npendix B.\nExperiment setup. We assess the strong-scaling training per-\nformance of Optimus and Megatron-based baselines using the\nViT-22B+GPT-175B model. Following [14], we progressively\nincrease the number of GPUs used (1536, 2048, and 3172)\nwhile keeping the batch size constant at 1536.\nResults. Table 5 compares training performance between Op-\ntimus and Megatron-LM based baselines with an increasing\nnumber of GPUs. Optimus reduces iteration time by up to\n21.3% compared to Megatron-LM, and by up to 20.5% com-\npared to the Megatron-LM balanced. With the increase in\nGPU count, Optimus exhibits a more pronounced speedup\nrelative to baseline solutions. This enhanced performance\nis anticipated since the constant batch size coupled with an\nincreased GPU count escalates the bubble ratio, enabling Op-\ntimus to allocate a larger proportion of encoder computations\nto LLM bubbles. It is also evident that Optimus maintains a\nstable MFU, whereas the baseline MFU declines when scaling\nto more GPUs."}, {"title": "5.3 Microbenchmarks", "content": "We use iteration time and Model Flops Utilization (MFU) [9"}]}