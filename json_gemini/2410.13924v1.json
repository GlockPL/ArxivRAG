{"title": "ARKit LabelMaker: A New Scale for Indoor 3D Scene Understanding", "authors": ["Guangda Ji", "Silvan Weder", "Marc Pollefeys", "Francis Engelmann", "Hermann Blum"], "abstract": "The performance of neural networks scales with both their\nsize and the amount of data they have been trained on. This\nis shown in both language and image generation. However,\nthis requires scaling-friendly network architectures as well\nas large-scale datasets. Even though scaling-friendly archi-\ntectures like transformers have emerged for 3D vision tasks,\nthe GPT-moment of 3D vision remains distant due to the\nlack of training data. In this paper, we introduce ARKit La-\nbelMaker, the first large-scale, real-world 3D dataset with\ndense semantic annotations. Specifically, we complement\nARKitScenes [4] dataset with dense semantic annotations\nthat are automatically generated at scale. To this end, we\nextend LabelMaker Weder et al. [33], a recent automatic\nannotation pipeline, to serve the needs of large-scale pre-\ntraining. This involves extending the pipeline with cutting-\nedge segmentation models as well as making it robust to\nthe challenges of large-scale processing. Further, we push\nforward the state-of-the-art performance on ScanNet and\nScanNet200 dataset with prevalent 3D semantic segmen-\ntation models, demonstrating the efficacy of our generated\ndataset.", "sections": [{"title": "1. Introduction", "content": "Recent progress in deep learning has been mostly focused\non language [5, 25, 26] and 2D image generation [27]. Be-\ncause for these two modalities, there is vast amount of train-\ning data available on the web. For text and image gen-\neration, you can simply scrape the internet for all avail-\nable data and train your model in an auto-regressive (in the\ncase of language) or diffusion process (image generation),\nwhere the supervision signal comes from self-supervision\nthat does not require any labelling. This type of train-\ning revealed surprising properties [34] when scaling it to\nbillions of data points in both text and image generation\nand leads to unseen performance gains enabling completely\nnew use cases. Yet, this type of training is not avail-\nable for scene understanding as objects and other prim-\nitives have to be classified in separate categories. This\nusually requires ground-truth annotations for the training\ndata. While there are efforts to relax this requirement\nthrough self-supervision [12, 42] or open-set scene under-\nstanding [17, 36], state-of-the-art methods [19, 36] all have\nsome form of direct supervision during training. Thus, an-\nnotated data is required for learning these tasks. Yet, creat-\ning datasets of similar scales as used in language and image\ngeneration is far from trivial. In this paper, we contribute\nthe largest 3D real-world indoor semantic dataset and in-\nvestigate the following questions: Is there a benefit from\npreferring real-world data over synthetic data? How can the\nlabeling effort be reduced? Do current models profit from\nmore real-world data?\nTo answer these questions, we make use of ARK-\nitScenes [4], a large collection of RGB-D trajectories man-\nually captured with consumer tablets. While these trajecto-\nries are annotated with sparse object bounding boxes, they\nare not complete enough to train competitive models for\nscene understanding and lack dense labels that could poten-\ntially serve as supervision for semantic segmentation mod-\nels. Therefore, we supplement this dataset with dense se-\nmantic labels that we automatically generate using an auto-\nmated pipeline. This allows us to create the first large-scale,\ndense 3D semantic segmentation training dataset that can\nbe used to (pre-)train any 3D semantic segmentation model.\nTo demonstrate the value of these vast yet imperfect anno-\ntations, we use these labels to re-train different models and\nextensively evaluate them on popular 3D semantic segmen-\ntation benchmarks."}, {"title": "2. Related Works", "content": "Datasets for 3D semantic segmentation. 3D seman-\ntic segmentation is to classify each point in a 3D point\ncloud into a set of predefined semantic categories. Promi-\nnent datasets for training and evaluation include Scan-\nNet [10]/ScanNet200 [29], and the Stanford 3D Indoor\nScene Dataset [1] (S3DIS), which comprises 6 large-scale\nindoor areas with 271 rooms. Both datasets includes\nRGB-D frames captured from real-world. In addition\nto real-world datasets, Structured3D [41] offers a photo-\nrealistic synthetic dataset with 3.5K house designs, and\nReplica [31] provides 18 high-quality reconstructed scenes.\nARKitScenes [4] is the most extensive collection of in-\ndoor scenes to date, featuring 5047 captures of 1661 unique\nscenes. RGB-D data is captured with Apple LiDAR scan-\nner. High-quality surface reconstruction and the bounding\nbox for object detection are also provided. However, dense\nannotations for semantic or instance segmentation is absent\nfrom this dataset. Therefore, it cannot be directly used in\ntraining models for 3D semantic segmentation.\nLabelMaker Weder et al. [33] is an automatic semantic\nsegmentation annotation pipeline that consolidates outputs\nfrom state-of-the-art 2D segmentation models with an addi-\ntional feature for translating frame-wise 2D labels into co-\nhesive 3D point cloud labels. In this work, we employ an\nenhanced version of LabelMaker to generate dense seman-\ntic segmentation annotations for ARKitScenes.\n3D semantic segmentation models. The neural network\narchitecture for processing 3D input data can be classified\ninto three main categories: voxel-based, point-based, and\ntransformer-based methods. Voxel-based methods trans-\nform points into fixed-sized voxel grids before passing\nthem through the neural network. MinkowskiNet [6] is\none of the most well-known model. Mix3D [21] en-\nhances MinkowskiNet through effective 3D data augmen-\ntation techniques. PonderV2 [42] explores self-supervised\nlearning from RGB-D data to improve the performance of\nthe MinkowskiNet architecture. Point-based methods in-\ncludes [3, 11, 15, 16, 23, 24, 32, 38]. However, there is a\nrecent shift from models based on point-wise convolutions\nto point-based transformer models [14, 22, 30, 40]. No-\ntable examples include Point Transformer [40], PTv2 [35],\nand PTv3 [36], which are developed towards better effi-\nciency and scaling ability for 3D inputs. Point Prompt\nTraining [37](PPT) introduces a novel training paradigm\nenabling the simultaneous training of multiple datasets with\ndiverse label spaces. Combining PTv3 with PPT achieves\nstate-of-the-art performance on the ScanNet/ScanNet200\nsemantic segmentation benchmark.\nIn this paper, we address the main limitation of existing\ndatasets for 3D semantic segmentation: their limited size.\nWe suspect that this limited size negatively impacts the per-\nformance of commonly used models as their performance is\nlimited without additional training data available."}, {"title": "3. method", "content": "As we build on top of LabelMaker [33], we briefly review\nthe essential steps of the pipeline proposed by Weder et al.. \nLabelMaker is an automatic labelling pipeline for 2D and\n3D semantic annotation. As shown in [33], it generates la-\nbels that are on par with the human annotators in [9]. It\nautomatically generates these labels by exploiting an en-\nsemble of base models that predict semantic maps for every\ninput frame in an RGB-D trajectory. Since the base models\npredict segmentation in different label spaces based on their\ntraining data, they are then mapped to a unified label space.\nOnly through this mapping, the different base models can be\nused in a subsequent ensemble. Thus, [33] defined a map-\nping from every label space into a carefully curated label"}, {"title": "3.2. Improving Labelmaker to make it scale to\nARKitScenes [4]", "content": "While LabelMaker [33] presented an automatic labelling\ntool that produces annotations on par with human annota-\ntors, we enhance the pipeline with two changes to further\nimprove its performance that is necessary to robustly gen-\nerate high-quality annotations for large-scale datasets. The\ncomplete pipeline is shown in Figure 1.\nIntegrating Grounded-SAM LabelMaker [33] used\nseveral state-of-the-art base models in its esemble. Yet,\nthey did not leverage the potential of Segment Anything\n(SAM) [13], a 2D segmentation model that was trained on\na large-scale datasets and robustly generalizes to many sce-\nnarios. As we want to scale LabelMaker to any environ-\nment, our aim is to integrate this prior into the pipeline.\nHowever, it is in not straight forward to efficiently use this\nmodel for semantic segmentation. To this end, Grounded\nSAM combines Grounding DINO [18] with SAM [13].\nGrounding DINO locates instances' bounding boxes given\nsemantic labels or natural languages, while the later model\ngenerates high quality segmentation masks for these bound-\ning boxes. We integrate this model by adapting it to Label-\nMaker's unified labelspace such that it can act as an addi-\ntional vote in the ensemble.\nAligning to gravity For ideal performance, many se-\nmantic segmentation models require alignment of the grav-\nity direction with the coordinate system of the data they\nwere trained on. Yet, large-scale data is not aligned with\ngravity by default. For example in ARKitScenes, occa-\nsional phone rotation leads to inconsistencies in the orien-\ntation of 2D images. Passing those rotated images to La-\nbelMaker's base models results in decreased performance\nand misclassifications (e.g. mistaking the floor with the\nceiling). Therefore, we project sky direction, which cor-\nresponds to the z-axis of the pose coordinate system from\nARKit (that uses the IMU), onto each 2D frame. Then, we\ncompute the angle between sky direction and upward direc-\ntion $\\alpha$. Given this angle, we rotate the image by $k*\\frac{\\pi}{2}$, where\n$k = \\arg \\min_k (s - \\alpha)$ to make the sky direction roughly\nupward, and rotate the predicted segmentation back to its\noriginal orientation after passing it through model to align\nit with its coordinate system.\nOptimizing compute resource scheduling. We op-\ntimize the code to deploy each individual piece of the\npipeline of Figure 1 as individual jobs to a GPU cluster, with\nSLURM as a dependency manager between the pipeline\npieces. To optimize the overall execution time, it is there-\nfore important to be able to estimate the processing time\nof each piece of the pipeline at the point of job submis-\nsion. ARKitScenes contains scenes of a wide range of sizes,\nspanning from a minimum of 65 frames to a maximum of\n13796, and different parts of the pipeline scale differently\nwith increasing scene size. To figure out the minimum re-\nsources requirements, we select 11 scenes of varied sizes\nuniformly distributed within the minimum and maximum\nrange and record their resources usage. While most jobs are\nnot sensitive to scene size and can suffice with a fixed re-\nsource allocation, the base models exhibit greater sensitivity\nto scene size. We interpolate resource needs with respect to\nscene size and summarize the empirical numbers in the Ap-\npendix. Through this, we ensure that we request minimal-\nrequired resources, so that we have lowest job waiting time\nand less idle compute power."}, {"title": "3.3. Scaling beyond existing datasets", "content": "In this paper, we demonstrate the effectiveness of the au-\ntomated labels generated for a large-scale dataset for pre-\ntraining 3D semantic segmentation models. Yet, ARK-\nitScenes is still limited in terms of variance in scene type.\nThrough modern mobile devices, RGB-D scanning is ubiq-\nuitously available and a excellent source for 3D data. Yet,"}, {"title": "4. Results", "content": "We evaluate the effectiveness of our ARKitScenes La-\nbelMaker dataset on two distinct network architectures:\nMinkowskiNet [6] and Point Transformer [8, 35-37].\nMinkowskiNet is the most established architecture. Many\nmodifications [19, 42] have been proposed to Minkowsk-\niNet and it is still the underlying architecture of most top-\nperforming models in 3D Semantic Segmentation bench-\nmarks. Point Transformer [36] is a very recently proposed\narchitecture and the current state-of-the-art on the ScanNet\nand ScanNet200 benchmarks. Since transformers are in\ngeneral known to profit from large-scale training data, we\nalso train on this architecture. From these two architectures,\nwe derive our three relevant baselines:\nVanilla MinkowskiNet. This is the standard Minkowsk-\niNet model based on [6], which most 3D semantic segmen-\ntation methods compare to. In this paper, we use the com-\nmonly used 'Res16UNet34C'variant of MinkowskiNet to\nguarantee fair comparison to all other baselines.\nPonderV2 [42]. PonderV2 [42] is an unsupervised pre-\ntraining strategy for semantic segmentation. For large-scale\ntraining data, manual labelling of training data is infeasible\nand leaves essentially two alternatives: Automatic labelling"}, {"title": "4.2. Datasets and Metrics for Evaluation", "content": "ScanNet [9]. ScanNet comprises 1513 densely annotated\nscans across 707 distinct indoor scenes, totaling 2.5 mil-\nlion RGB-D frames. It stands as one of the most widely\nused and influential benchmark datasets for indoor 3D scene\nunderstanding. ScanNet is annotated by humans using the\nNYU40 label space and evaluated on a subset of 20 classes\nfrom NYU40.\nScanNet200 [28]. While only 20 classes are used in the\nScanNet benchmark, the original dataset is annotated with\nmany more classes. ScanNet200 [28] leverages these an-\nnotations and organizes them into a new benchmark with\n200 classes that are of higher-resolution than the original\nScannet classes. Given the large-number of different cat-\negories generated by our LabelMakerv2 pipeline, we also\npre-train the models for this task and evaluate them on the\nScanNet200 benchmark.\nScanNet++ [39]. ScanNet++ is a dataset of 460 high-resolution 3D indoor scenes with dense semantic and in-\nstance annotations, captured using a high-precision laser\nscanner and registered images from a DSLR camera and\nRGB-D streams. It focuses on long-tail and multi-labeled\nannotations. In its semantic segmentation benchmark, mod-\nels are evaluated over 100 labels.\nS3DIS [2]. S3DIS is a 3D semantic dataset containing 6\nlarge-scale indoor areas from 3 different buildings, labeled\nwith 13 semantic classes. We follow the practice of [36] and\ncreate a dataset with an effective size of 406. We only use\nthis dataset during the training of the PTv3 [36] baseline.\nStructured3D [41] is a large-scale indoor synthetic\nRGB-D dataset featuring 6519 training scenes and 1697 test\nscenes. It is annotated with a label space of 25 classes.\nStructured3D and S3DIS only used in PTv3+PPT joint\ntraining and we adopt pre-processed version of these two"}, {"title": "4.3. Experiment Settings", "content": "We adopt three approaches to evaluate the effectiveness of\nour ARKitScenes LabelMaker dataset.\nPre-training. To investigate whether automatic labels\nare useful to learn strong features from imperfect annota-\ntions, we pre-train both, Minkowskinet [7] and PointTrans-\nformerV3, on our generated ALS200 dataset. Afterwards,\nwe fine-tune the pretrained models on the ScanNet and\nScanNet200 dataset, respectively.\nFor MinkowskiNet, we employ the Res16UNet34C ar-\nchitecture as our backbone model. During pre-training, we\nutilize the AdamW optimizer with a learning rate of 0.01\nand OneCycleLR scheduler, training the network for 600\nepochs. If the label space is changed for fine-tuning, we\nreplace the classification head and exclusively train it with\nthe same learning rate setting until convergence while the\nrest of the model is fixed. Then, the entire network under-\ngoes fine-tuning with a learning rate of 0.001, while other\nsettings are kept unchanged.\nFor PTv3, we adhere to the settings outlined in [36] em-\nploying the AdamW optimizer with OneCycleLR for 800\nepochs of training. However, the learning rate during pre-\ntraining is adjusted to 0.0016. Similar to the fine-tuning\nof MinkowskiNet, we initially freeze the backbone during\nfine-tuning and solely train the classification head until con-\nvergence. Then, we fine-tune on ScanNet or ScanNet200\nwith a reduced learning rate of 0.0006. Besides ALS200,\nwe also pre-train PTv3 with ALC label space as the map-\nping from wordnet to ScanNet200 may reduce neural stim-\nulation.\nCo-training with ALS200. With this experiment, we\naim to investigate if our ALS200 dataset can be simply com-\nbined with existing datasets for increasing the dataset size\nand, therefore, the performance of the resulting model. To\nthis end, we combine ALS200 with the ScanNet200 dataset\nand train a MinkwoskiNet from scratch. Due to resource\nlimitations, we could only train MinkowskiNet with this\ncombined dataset. The training setting is exactly same as\nthe pre-training stage of MinkowskiNet described above.\nJoint-training. We employ PTv3+PPT for joint train-\ning on multiple datasets across multiple label spaces. Be-\nsides ScanNet/ScanNet200, ScanNet++, S3DIS and Struc-\ntured3D, we add our ALC dataset. We choose Label-\nMaker's wordnet label space in order to provide the net-\nwork with maximum possible semantic class stimulation.\nWe use the exact same setting of PTv3+PPT from [36]. We\nuse AdamW optimizer with OneCycleLR scheduler and a\nlearning rate of 0.05. Additionally, we incorporate the La-\nbelMaker WordNet label space into the norm layer and the\nfinal classification head."}, {"title": "4.4. Results", "content": "In Table 2, we present the results for the ScanNet dataset.\nIn the case of MinkowskiNet [7], we can not only show\nthat pre-training on our large-scale, real-world ALS200\ndataset significantly improves the mean intersection-over-\nunion compared to vanilla training, but it also significantly\noutperforms to other variants of pre-training. Pre-training\non our imperfect yet automatically generated labels is su-\nperior to self-supervised pre-training (PonderV2 [42]) and\nextensive data augmentation (Mix3D [20]). This indicates\nthat direct supervision with scale in training data for super-\nvised learning is essential for 3D semantic segmentation.\nAdditionally, the ALS200/ALC pre-trained PTv3 exhibits\ncomparable or superior performance to large-scale multi-\ndataset joint training. This proves that our dataset is more"}, {"title": "4.5. LabelMakerv2 is generalizing beyond ARK-itScenes", "content": "In order to demonstrate the effectiveness of our LabelMak-\nerv2, we utilize it to process two self-captured scenes-a\nkitchen and a fireplace-taken in a holiday cottage using an\niPhone 12 Max. In Figure 2, we present the reconstructed\ncolored scenes alongside their semantic segmentation. It\ndemonstrates visual accuracy and plausibility, confirming\nthe effectiveness of our pipeline."}, {"title": "4.6. Limitations & Broader Impact", "content": "While we extend LabelMaker [33] with a better pointcloud\npipeline, we leave out the part that generates 2D segmenta-\ntion maps. The computational cost of the NeRF-based lift-\ning over the whole ArKitScenes dataset is beyond our avail-\nable resources. It would be an interesting future direction of\nresearch to explore if training 2D models on this data yields\nsimilar performance gains as it is the case for 3D models.\nFurthermore, 20 scenes in ARKitScenes processing are\nexcluded due to lack of pose data. Our LabelMakerv2 re-\nquires accurate poses, but future iterations could integrate\ntechniques such as bundle adjustments to reconstruct miss-\ning pose data.\nLike the original LabelMaker [33], also our improved\npipeline does not have perfect accuracy. While [33] showed\nthat the accuracy is on par with crowd-sourced human anno-\ntations, there is always a danger of introducing systematic\nmistakes when training on noisy labels. For safety critical\napplications, rigorous testing on accurately annotated data\nis even more important when using tools like ours to source\ntraining data.\nDoes large-scale pre-training with automatic labels show\nsimilar trends as it does for language and image generation\ntasks? The discussed results point in this direction, with a\nmeasurable improvement to different models when pretrain-\ning on ArKitLabelMaker. However, training on large-scale\nreal-world data 'only' achieves test results on par with the\ncurrent SOTA based on synthetic data of even larger scale.\nOur conclusion is that real-world data is much more effec-\ntive than syntethic data, but even larger overall scale is nec-\nessary to push the performance beyong state-of-the-art. Our\ndeveloped pipeline makes it easy to provide training data\nonce more scans are available."}, {"title": "5. Conclusion", "content": "In this paper, we presented the largest, real-world 3D RGB-D dataset with dense semantic annotations. The dense an-\nnotations are automatically generated using an improved\nversion of LabelMaker [33], which we dub LabelMakerv2.\nWhile these labels are automatically generated and there-\nfore imperfect, we demonstrate their value to pre-training\ncommonly used 3D semantic segmentation methods sig-\nnificantly improving the performance of existing models\ntrained with traditional, self-supervised, or augmentation-heavy training strategies. This allows to draw parallels to\nrecent advances in language and image generation, where\nscaling up the size of training data led to huge gains in per-\nformance. Therefore, we also provide an integration of a\ncommonly available 3D scanning software for iOS to en-\nable the usage of mobile devices to easily generate more\ndata for training and evaluation."}, {"title": "C. Detailed process of applying LabelMaker to\nARKitScenes", "content": "ARKitScenes is one of the largest indoor 3D scenes dataset.\nIt consists of 5047 parsable scenes of various size. We con-sider a scene parsable if the RGB-D trajecotry comes with\nassociated pose data. Processing these scenes with our im-proved LabelMaker pipeline requires deliberate engineering\nwith the following goals: a) Bring the data in to the format\nrequired by LabelMaker [33] b) Robust processing to not\nwaste compute on failures, c) Improved parallelization to\nspeed up processing. d) Accurate resource estimation to\nprevent waste of compute resources and longer job waiting\ntime. e) Fast failure identification and results inspection.\nTransforming the data LabelMaker [33] requires a spe-cific data format to be able to reliably process all data. All\ntrajectories require posed RGB-D data and a denoised 3D\nmodel that is used by Mask3D. ARKitScenes comprises\ndata of varying resolutions and sampling rates. Depth data\nis captured at 256 \u00d7 192 and 60 FPS, while the RGB frames\nare recorded at 640 \u00d7 480 and 30 FPS. Therefore, synchro-nization is required to process the data with LabelMaker. To\nthis end, we match each RGB frame with the closest depth\nframe in time and we resize the depth frame to RGB frame's\nresolution. Pose data, originally at 10 FPS, is interpolated\nusing rotation splines to synchronize with each RGB frame.\nTo obtain a 3D mesh of each scene that can be processed by\nMask3D, we reconstruct the 3D model by fusing the syn-chornized posed RGB-D data using TSDF fusion and then\nextract mesh with marching cube algorithm. We empirically\nchose a voxel size of 8mm and a truncation distance of 4cm\nfor fusion.\nBuilding a scalable pipeline LabelMaker [33] can be\ndecomposed into individual modules such as the individual\nbase models, the consensus computation, and the 3D lift-ing. This modular nature allows to build a scalable pipeline\nusing popular high-performance computing toolboxes. As\nthe different base models have different runtimes, we had to\nleverage dependency management system that can handle\ndifferent dependencies of the pipeline steps. This architec-ture allows us to effectively leverage large-scale computing\nand distribute the processing across many different nodes.\nIn more detail, we decompose the pipeline into several\njobs (ordered by dependency) for each scene:\n1. Preprocessing: Downloading the original scene data,\ntransforming it into LabelMaker format, and run TSDF\nfusion to get the 3D mesh of the scene.\n2. Forwarding 2D images or 3D meshes to each base mod-els: Grounded-SAM, Mask3D, OVSeg, CMX, InternIm-age. (all jobs depends on step 1.)\n3. Getting the consensus label from base models' labels.\n(depends on all elementary jobs in step 2.)\n4. Lifting the 2D consensus label into 3D. (depends on step\n3.)\n5. Rendering the outputs of base models or consensus into\nvideos for visualization. (depends on steps 2. or 3.)\n6. Post-processing, including removing temporary files and\nget statistics of each tasks. (depends on all steps above)\nOptimizing compute resource scheduling. ARKitScenes contains scenes of a wide range of sizes, spanning\nfrom a minimum of 65 frames to a maximum of 13796, and\ndifferent parts of the pipeline scale differently with increas-ing scene size. To figure out the minimum resources re-"}, {"title": "C.1. Implementation Details", "content": "We use a CentOS 7 based SLURM cluster to process all\nthe data, which is capable of handling task dependencies\nand parallel processing. Before submitting jobs for a single\nscene, we first check the progress of each job and gener-ate a SLURM script to submit only those unfinished jobs.\nThis ensures that no compute resource is used in rerunning\ncompleted tasks.\nWe employ test time augmentation by forwarding all\nmodels twice, with Mask3D using two different random\nseeds and other models being mirror flipped. Following the\npractice of LabelMaker [33], we assign equal weights to\neach model when calculating the consensus, although these\nweights are configurable in our pipeline code. Since we\nare primarily interested in the 3D labels that can be used\nfor pre-training 3D semantic segmentation models, SDFS-tudio training and rendering are omitted due to their lengthy\nprocessing times. Further, we enhance the pipeline by stor-ing both the most and second most voted predictions along-\nside their respective vote counts. This information is useful\nwhen we want to investigate on the uncertainty across the\nbase models. We leave the exploitation of this information\nas potential future direction of research."}]}