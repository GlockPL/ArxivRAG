{"title": "IMPROVING THE LANGUAGE UNDERSTANDING CAPA-\nBILITIES OF LARGE LANGUAGE MODELS USING REIN-\nFORCEMENT LEARNING", "authors": ["Bokai Hu", "Sai Ashish Somayajula", "Xin Pan", "Pengtao Xie", "Zihan Huang"], "abstract": "Large language models (LLMs), primarily built on decoder-only transformer ar-chitectures, excel in natural language generation tasks and have shown promisein adapting to diverse downstream tasks using zero-shot and few-shot promptingtechniques. However, these prompting methods often fall short on natural languageunderstanding (NLU) tasks, where smaller encoder-only models like BERT-baseconsistently outperform LLMs on benchmarks such as GLUE and SuperGLUE.In this paper, we explore two approaches\u2014supervised fine-tuning and proximalpolicy optimization (PPO)\u2014to enhance the NLU capabilities of LLMs. To reducerthe computational cost of full-model fine-tuning, we integrate low-rank adaptation(LoRA) layers, restricting updates to these layers during both supervised fine-tuningand PPO stages. In the supervised fine-tuning approach, task-specific prompts areconcatenated with input queries and ground-truth labels from the NLU trainingcorpus, optimizing the model using the next-token prediction objective. Despitethis, LLMs still underperform compared to encoder-only models like BERT-baseon several NLU tasks. To address this gap, we employ PPO, a reinforcementlearning technique that treats each token generation as an action and evaluates thesequence of generated tokens using a reward function based on their alignmentwith ground-truth answers. PPO then updates the model to maximize these re-wards, effectively aligning its outputs with the correct labels. Our experimentswith the LLAMA2-7B model demonstrate that PPO-based fine-tuning significantlyimproves performance, delivering an average gain of 6.3 points over supervisedfine-tuning on the GLUE benchmark. PPO surpasses zero-shot prompting by 38.7points and few-shot prompting by 26.1 points on GLUE, while also outperformingthese baselines by 28.8 and 28.5 points on SuperGLUE. Additionally, PPO exceedsthe performance of BERT-large, a strong baseline, with an average improvementof 2.7 points on GLUE and 9.3 points on SuperGLUE. These improvements areconsistent across models such as Qwen2.5-7B and MPT-7B, highlighting PPO'srobustness and effectiveness in enhancing the NLU capabilities of LLMs.", "sections": [{"title": "1 INTRODUCTION", "content": "Large language models (LLMs) Radford et al. (2019); Brown (2020); Touvron et al. (2023b) have\nrevolutionized natural language processing (NLP) with their powerful text generation capabilities,\ndriven by their decoder-only transformer architecture Radford (2018). Pretrained on large amounts\nof unlabeled text, LLMs can generate coherent and contextually relevant content. Using prompt-based strategies like zero-shot and few-shot prompting Brown (2020), LLMs can tackle various\ndownstream tasks without requiring task-specific fine-tuning. However, these methods often un-derperform on natural language understanding (NLU) tasks compared to encoder-only models like\nBERT Devlin (2018), which consistently excel on benchmarks such as GLUE Wang et al. (2019)\nand SuperGLUE Wang et al. (2020). For instance, our evaluations on LLAMA2-7B showed that\nzero-shot prompting with task-specific prompts yielded an average performance of 46.1 across all"}, {"title": "2 RELATED WORKS", "content": ""}, {"title": "2.1 NATURAL LANGUAGE UNDERSTANDING", "content": "Natural language understanding (NLU) tasks are crucial for evaluating a model's ability to com-prehend and process human language in various contexts, such as classification, inference, andreasoning. The GLUE benchmark Wang et al. (2019) serves as a key standard for NLU performance,covering tasks like CoLA, SST-2, MRPC, MNLI, and so on, which assess grammatical acceptability,sentiment analysis, paraphrase detection, and textual entailment. For more complex challenges, theSuperGLUE benchmark Wang et al. (2020) was introduced, featuring more difficult tasks that requireadvanced reasoning and comprehension. Together, GLUE and SuperGLUE provide a comprehensiveassessment of a model's language understanding capabilities.\nModels such as BERT Devlin (2018), which utilize a bidirectional encoder architecture, have achievedstate-of-the-art performance in NLU tasks. BERT's architecture allows it to capture bidirectionalcontext. Its pretraining strategy, which uses masked language modeling (MLM), helps the model learn\ndeep semantic representations. This combination makes BERT highly effective across a wide range\nof NLU tasks. The success of encoder-only models in benchmarks such as GLUE and SuperGLUE\ncan largely be attributed to their ability to capture rich bidirectional context during pretraining, which\nis critical for NLU tasks.\nIn contrast, LLMs like GPT-2 Radford et al. (2019), GPT-3 Brown (2020), and LLAMA Touvron et al.(2023b) rely on scaling model size with decoder-only architectures, achieving significant successin text generation tasks. However, their zero-shot performance with task-specific prompts remain\nsuboptimal on NLU tasks, such as those in the GLUE benchmark. This underperformance is attributedto their autoregressive nature, which limits their ability to capture the bidirectional dependencies\ncrucial for deep contextual understanding Devlin (2018); Radford et al. (2019); Brown (2020). Efforts\nto adapt LLMs for NLU have focused on prompt-based methods like few-shot prompting Brown\n(2020), which show promise but still fall short of the performance achieved by encoder-only models\nlike BERT on these tasks."}, {"title": "2.2 POLICY-BASED REINFORCEMENT LEARNING", "content": "Policy-based reinforcement learning (RL) directly optimizes an agent's policy by learning its pa-rameters to maximize long-term rewards. Unlike value-based methods like Q-learning Watkins &\nDayan (1992) and DQN Hester et al. (2018), which indirectly derive policies through value functions,\npolicy-based methods represent the policy as a parameterized function. This function, $p_{\\theta}(a|s)$, defines\nthe probability of taking action a in state s, where $\\theta$ represents the policy parameters. The goal is to\nlearn optimal parameters $\\theta^*$ that maximize the expected cumulative reward, typically through policy\ngradient methods Sutton et al. (1999). These methods excel in high-dimensional or continuous action\nspaces, where value-based methods can struggle Deisenroth et al. (2013).\nPolicy-based methods in reinforcement learning (RL) have evolved significantly over time, starting\nwith REINFORCE Williams (1992), which optimizes policies using the policy gradient theorem\nbut suffers from high variance due to its reliance on Monte Carlo estimates of the reward. Monte\nCarlo estimates refer to calculating the total reward based on full episodes of interaction, meaning\nupdates are made only after an entire sequence of actions and rewards is observed, which can lead\nto noisy and slow learning. To address this, actor-critic methods like A2C and A3C Mnih (2016)\nintroduced a critic that estimates the value of the current state, allowing for smoother updates by\nreducing the variability in policy updates and speeding up convergence. However, these methods still\nfaced instability when large updates caused the new policy to diverge too far from the previous one.\nTrust Region Policy Optimization (TRPO) Schulman (2015) tackled this by limiting the size of policy\nupdates using a KL divergence constraint, but its implementation was complex and computationally\nexpensive. Proximal policy optimization (PPO) Schulman et al. (2017a) simplified this process by"}, {"title": "3 PRELIMINARIES ON APPLICATION OF PPO FOR FINE-TUNING LLMS", "content": "Proximal policy optimization (PPO) Schulman et al. (2017b) is an online reinforcement learning\nalgorithm. In this section, we describe the process to fine-tune an LLM using PPO. During training,\nat each timestep t, the LLM (policy) generates a token prediction $a_t$ (action) based on the state $s_t$,\nwhich consists of the sequence of generated tokens up to timestep t \u2013 1. The final generated output is\nevaluated in the context of the downstream task, where the environment provides feedback in the\nform of rewards. The model updates its parameters based on these rewards to improve its ability to\ngenerate accurate predictions over time.\nPPO uses gradient ascent to optimize the following objective, aiming to maximize cumulative rewards:\n$J(\\theta) = E_{(s_t, a_t)\\sim \\pi_{\\theta_{old}}} [min(r_t(\\theta)A_t, clip(r_t(\\theta), 1-\\epsilon, 1+\\epsilon)A_t)]$,\nwhere $r_t(\\theta) = \\frac{P_{\\theta}(a_t| s_t)}{P_{\\theta_{old}}(a_t| s_t)}$\nHere, $p_{\\theta}(a_t | s_t)$ is the probability of taking action $a_t$ in state $s_t$ under the current policy, while\n$p_{\\theta_{old}}(a_t | s_t)$ represents this probability under the old policy. In PPO, the training data\u2014specifically,\nthe state-action pairs $(s_t, a_t)$\u2014are sampled using the old policy $\\pi_{\\theta_{old}}$ (the LLM before it is updated),\nrather than the new policy currently being optimized. Thus, the ratio $\\frac{P_{\\theta}(a_t| s_t)}{P_{\\theta_{old}}(a_t| s_t)}$ accounts for how\nmuch the new policy has changed relative to the old policy and adjusts the likelihood of an action\naccordingly. This ratio is multiplied by $A_t$, the Generalized Advantage Estimation (GAE) Schulman\net al. (2018), which measures how much better or worse an action $a_t$ is compared to the expected\noutcome under the current policy.\n$A_t = R_t + \\gamma V_{t+1} - V_t + \\gamma \\lambda \\hat{A}_{t+1}$,\nHere, $R_t + \\gamma V_{t+1} - V_t$ represents the temporal difference (TD) error Sutton (1988). In this expression,\n$R_t$ is the immediate reward received after taking action $a_t$, $V_t$ is the expected reward before the action,\nand $V_{t+1}$ is the discounted estimate of the future reward after the action. This term reflects how the\naction $a_t$ performed when compared to the expected return at state $s_t$. The second term, $\\gamma \\lambda \\hat{A}_{t+1}$, is\nthe smoothing factor in GAE, where $\\lambda$ is the trade-off parameter. This recursive estimate allows the\nmodel to incorporate future information, making the advantage estimate more stable. Smaller values\nof $\\lambda$ emphasize on immediate rewards, while larger values capture longer-term dependencies. The\ndiscount factor $\\gamma$ controls how much emphasis is placed on future rewards compared to immediate\nones, with higher values of $\\gamma$ giving more weight to future rewards. $V_t$, which represents the expected\nfuture reward from state $s_t$, is estimated by a critic model.\nThe clipping function $clip(ratio, 1-\\epsilon, 1+\\epsilon)$ limits the change between the current and old policy,\nensuring stable updates by preventing large deviations. This helps avoid too-large policy changes that\ncould destabilize training. In summary, PPO optimizes the policy using gradient ascent to maximize\ncumulative rewards while ensuring stable updates through clipping, with the GAE providing a more\nstable and accurate advantage estimate by incorporating future information recursively.\nCritic Model The critic model consists of a value head, which is a multi-layer perceptron attached\nto the final layer of the LLM. It takes the LLM's representation of the generated token sequence up to\ntimestep t (i.e., the state $s_t$) and predicts a scalar value representing the value function $V_t$ for that\nstate. The critic model is updated using the square of TD error, which is computed as:\n$\\delta_t = (R_t + \\gamma V_{t+1} - V_t)^2$, (1)"}, {"title": "4 METHOD", "content": "To enhance the performance of LLMs on NLU tasks, we adopt two distinct fine-tuning methods.\nThe first approach involves supervised fine-tuning, where the input consists of a concatenation of\nthe task-specific prompt, query and the ground truth answer, with the model optimized using the\nnext-token prediction objective. The second approach utilizes PPO, framing response generation\nas a reinforcement learning problem. In this setup, the sequence of input tokens till timestep t - 1\nrepresents the state $s_t$, and each token generated at timestep t is treated as an action $a_t$. After\ngenerating the entire sequence, a heuristic-based process extracts the final answer from this generated\nsequence, and is compared to the ground truth. PPO is then employed to optimize the model\nby maximizing the cumulative reward derived from this comparison. To reduce computational\ncomplexity, we fine-tune LoRA layers instead of the full model."}, {"title": "4.1 TASK-SPECIFIC PROMPT DESIGN", "content": "We detail the construction of task-specific prompts used to query the LLM for NLU tasks. Each\nprompt begins with a clear task description, outlining the necessary background information to\nguide the model in solving the task. Following this, we specify strict requirements for the output\nformat, ensuring that the response is encapsulated within a predefined structure, specifically between\n'<Judgement></Judgement>' tags. This structure ensures consistency in the model's responses,\nfacilitating easier extraction and evaluation of the results.\nFor example, in the CoLA task, which assesses grammatical acceptability, the prompt is structured as\nfollows:\nSystem_prompt:\nYou are an assistant to analyze the linguistic properties\nof a sentence. The task is to decide the linguistic acceptability\nof a sentence. If the sentence is linguistically correct then it\nis acceptable, else it is not.\nThe result you give should have the following form:\n<Judgement> {Insert only \"Yes\" or \"No\" here} </Judgement>\nPrompt:\nNow judge if the sentence \"{sentence}\" is linguistically acceptable.\nAssistant:\n<Judgement>\nThe prompt starts with background information about CoLA, specifies restrictions on the output\n(such as labeling a sentence as acceptable or unacceptable), and concludes with a special start token,\n<Judgement>, to initiate the model's response generation."}, {"title": "4.2 SUPERVISED FINE-TUNING OF LLM ON NLU TASKS", "content": "Given an NLU training dataset, $D^{(tr)} = \\{(x_i, y_i)\\}_{i=1}^N$, where $x_i$ represents the input text and $y_i$ the\nground truth label, we fine-tune the LLM on a sequence consisting of the task-specific prompt p\n(described in section 4.1) concatenated with the input $x_i$ and the ground truth answer $y_i$. The model\nis trained using the next-token prediction objective, where it predicts the next token in the sequence\nby conditioning on all preceding tokens. This objective trains the model to learn to predict the correct\nanswer for the NLU task conditioned on the task-specific prompt and input."}, {"title": "4.3 PROXIMAL POLICY OPTIMIZATION FOR LLM FINE-TUNING ON NLU TASKS", "content": "We utilize PPO to fine-tune the LLM on NLU tasks, following the training protocol outlined in\nsection 3. The reward function is specifically designed for each NLU task. In this work, we use a\nsimple reward function, where a reward is assigned at the end of the generation based on alignment\nwith the ground truth labels. We use regular expression matching to extract answers from the LLM's\noutputs by first locating the text within the \u2018<Judgement></Judgement>' tags. Depending on the task,\nwe then search for task-specific keywords (such as \u201cyes\u201d, \u201cno\u201d, \u201cacceptable\u201d, or \u201cnot acceptable\u201d) to\nidentify the answer. These extracted answers are compared with the ground truth to determine the\nappropriate rewards.\nFor instance, CoLA, which is a classification task, answers are categorized as acceptable, unaccept-able, or exceptional (incorrect format). For STS-B, which is a regression task, the extracted answer is\na floating-point number between 0 and 5. The reward per generation for classification tasks is given\nby $R = 1(\\hat{y} == y)$, where $\\hat{y}$ is the model's prediction and y is the ground truth. For STS-B, a\nregression task, the reward per generation is calculated based on how close the prediction is to the\nground truth: $R = 2.5 \u2013 |\\hat{y_i} \u2013 y_i|$. Incorrectly formatted responses are penalized with a value of -1\nfor classification tasks and -2.5 for regression tasks."}, {"title": "4.4 Low-RANK ADAPTATION", "content": "To mitigate the computational cost of full-model fine-tuning, we employ LoRA Hu et al. (2021b)\nduring both the supervised fine-tuning and PPO stages. Instead of updating the entire model, we\nrestrict the updates to LoRA layers, which significantly reduces the number of trainable parameters\nby decomposing the weight matrices into low-rank matrices."}, {"title": "5 EXPERIMENTS", "content": ""}, {"title": "5.1 EXPERIMENTAL SETUP", "content": "We trained and evaluated our models on the GLUE Wang et al. (2019), and SuperGLUE Wang\net al. (2020) benchmarks. All experiments were conducted using instruction-tuned LLAMA2-7B\nmodels Touvron et al. (2023a)\u00b9. We perform both single task and multi-task fine-tuning: 1) Single-task Fine-tuning: For each subtask within GLUE, and SuperGLUE, a separate task-specific LORA\nmodule was trained independently. 2) Multi-task Fine-tuning: In the multi-task setting, datasets from\ndifferent subtasks within each benchmark were combined, and a single LoRA module was trained to\nhandle all tasks simultaneously.\nHyperparameter Settings For PPO-based fine-tuning, gird search is performed to select the batch\nsize in 4, 8, 12, and 16 for each task. A batch size of 24 was used across all tasks during supervised\nfine-tuning (SFT). The PPO epochs is set to 4, that is each sampled batch is used for updating the\nmodel four times. The initial learning rate for all tasks was set to 9 \u00d7 10-6. We utilized the Adafactor\noptimizer for PPO training and AdamW for SFT. A cosine annealing learning rate scheduler with a\nwarmup phase was employed, where the learning rate was gradually increased during the first 10%\nof training steps and then reduced to one-tenth of the initial value by the end of training. We use a\nrank r = 16 for the LoRA layers. We trained both PPO and SFT models until convergence on the\nvalidation set. The best hyperparameters were selected based on performance on the validation set.\nThe final reported results for the GLUE and SuperGLUE are from their corresponding evaluation\nserver. For evaluation, multinomial sampling with a temperature of 1 was used to generate a single\nresponse per data sample. The model generated responses with lengths between 12 and 32 tokens,\nwith the generation process concluding using a special identifier \u201c</Judgement>\u201d."}, {"title": "5.2 BASELINES", "content": "We evaluated the performance of our approach against three baselines:"}, {"title": "5.3 RESULTS ON GLUE BENCHMARK", "content": "In this section, we present our experiments on the GLUE benchmark, comparing the results with\nencoder-only models such as BERT Devlin et al. (2019). We use the LLAMA2-7B model as the LLM\nfor our evaluations. The baselines include zero-shot prompting and few-shot prompting (5-shot).\nFor fine-tuning methods, we compare both supervised fine-tuning and PPO across single-task and\nmulti-task settings. The results are summarized in Table 1. From the results, we make the following\nobservations.\nFirst, we observed that zero-shot prompting of the LLAMA2-7B model with task-specific prompts\nconsistently underperformed compared to the smaller BERT-base model. LLAMA2-7B struggled\nnotably on simpler tasks like SST-2, which only required classifying sentiment as positive or negative.\nThis underscores the model's weak language understanding capabilities, with zero-shot prompting\nproving inadequate compared to BERT-base. Second, few-shot prompting showed improvements\nover the zero-shot baseline, achieving an average score of 58.7 compared to 46.1, but it still lagged\nsignificantly behind the BERT-base model's score of 79.6. Third, supervised fine-tuning (SFT) using\nLORA modules for each task further boosted performance, bringing it closer to BERT's level with\nan average score of 78.5, though still slightly behind BERT-base's 79.6. Fourth, fine-tuning with\nPPO delivered the best results, achieving an average score of 84.6, surpassing even BERT-large's\n82.1. Moreover, zero-shot and few-shot prompting of LLAMA2-7B displayed a noticeable output\nimbalance, with a tendency to favor certain classes or values. In contrast, models fine-tuned with\nPPO showed no significant bias. Fifth, the computational time for PPO is approximately 1.32 times\nthat of SFT, indicating only a marginal increase in computational costs.\nAdditionally, we compared the results with multi-task training, where a single LoRA module was\ntrained across all datasets using both SFT and PPO to reduce time complexity. We found that SFT\non individual tasks outperformed its multi-task fine-tuning counterpart. However, while PPO on\nmulti-task training did not perform as well as PPO on single-task training, it still outperformed\nBERT-large in average performance, achieving a score of 82.9 compared to BERT-large's 82.1.\nThese results demonstrate that while single-task fine-tuning yields the best performance, multi-task\ntraining with PPO can still achieve competitive results, even surpassing state-of-the-art models like\nBERT-large. The training curves for PPO is presented in appendix A."}, {"title": "5.4 RESULTS ON SUPERGLUE BENCHMARK", "content": "We fine-tuned the LLAMA2-7B model using PPO on the SuperGLUE dataset and compared its\nperformance against several baselines, including BERT-large, BERT-large++, and zero-shot and\nfew-shot prompting of LLAMA2-7B. The term \u201cBERT++\" refers to a BERT model fine-tuned using\nthe supplementary training on intermediate labeled-data tasks (STILTs) approach Phang et al. (2018),\nwhere the model is first fine-tuned on related transfer tasks before being fine-tuned on SuperGLUE\ntasks. For example, MNLI from the GLUE benchmark Wang et al. (2019) is used as an intermediate\ntask for CB, RTE, and BoolQ Wang et al. (2020). In contrast, our experiments with LLM did not"}, {"title": "5.5 PERFORMANCE COMPARISON ACROSS DIFFERENT LLMS", "content": "To assess the consistency of our findings across different models, we evaluated Qwen2.5-7B and\nMPT-7B alongside LLAMA2-7B on the STS-B dataset from the GLUE benchmark and the COPA\ndataset from the SuperGLUE benchmark. The results confirm that PPO-based fine-tuning consistently\noutperforms the BERT-large model, as well as the zero-shot and few-shot prompting baselines for all\nLLMs, highlighting its effectiveness across different LLMs. Notably, another independent observation\nis that for all LLMs, few-shot prompting underperforms zero-shot prompting on the COPA dataset."}, {"title": "6 CONCLUSION", "content": "Prompting-based approaches, such as zero-shot and few-shot prompting, have gained popularity for\nadapting LLMs to downstream tasks. However, when applied to LLAMA2-7B, these methods under-perform on NLU tasks compared to smaller encoder-only models like BERT-base and BERT-large.\nTo address this limitation, we explore two fine-tuning strategies that leverage LoRA layers to reduce\ncomputational overhead. First, we employ supervised fine-tuning by concatenating task-specific\nprompts, input queries, and ground-truth labels, optimizing the model with the next-token prediction\nobjective. While this approach improves LLAMA2-7B's performance over prompting-based methods,\nit still lags behind BERT-base on the GLUE benchmark. To further enhance performance, we adopt\nPPO, treating the LLM as a policy that generates the next token (action) based on the current input\nsequence (state). A reward function then evaluates how closely the generated tokens match the\nground-truth labels, guiding updates to the policy. PPO based fine-tuning of LLAMA2-7B, tested\nacross benchmarks like GLUE, and SuperGLUE, resulted in significant performance gains, outper-forming strong baselines like BERT-large. Similar trends were observed in other LLMs, including\nQwen2.5-7B and MPT-7B, showcasing the robustness of this approach. These findings underscore\nthe effectiveness of PPO in enhancing NLU capabilities in LLMs. Future work could extend these\ntechniques to more diverse datasets and refine reward functions for handling complex NLU tasks."}, {"title": "7 REPRODUCIBILITY STATEMENT", "content": "We provide our codes at https://anonymous.4open.science/r/LLM_NLU-BE83. In\nthe code repo, we provide instructions on how to reproduce experimental results. Furthermore, we\nalso provided the hyperparameter settings in section 5.1."}]}