{"title": "Parallel Strategies for Best-First Generalized Planning", "authors": ["Alejandro Fern\u00e1ndez-Alburquerque", "Javier Segovia-Aguas"], "abstract": "In recent years, there has been renewed interest in closing the performance gap between state-of-the-art planning solvers and generalized planning (GP), a research area of AI that studies the automated synthesis of algorithmic-like solutions capable of solving multiple classical planning instances. One of the current advance-(BFGP), a GP algorithm based on a novel solution space that can ments has been the introduction of Best-First Generalized Planning be explored with heuristic search, one of the foundations of mod-ern planners. This paper evaluates the application of parallel search techniques to BFGP, another critical component in closing the per-formance gap. We first discuss why BFGP is well suited for paral-lelization and some of its differentiating characteristics from classi-cal planners. Then, we propose two simple shared-memory parallel strategies with good scaling with the number of cores.", "sections": [{"title": "1 Introduction", "content": "Generalized planning (GP) has been a longstanding area of research in Artificial Intelligence (AI) [6, 15]. The foundation of GP is auto-(commonly known as plans) to go from a specific initial state to a mated planning, which studies how to construct sequences of actions goal [3]. Since planning is a hard problem (PSPACE-complete) [1], solving multiple problem instances from the same domain is compu-tationally expensive. Given this realization, GP studies the synthesis of general plans that can solve multiple problem instances from the same domain, reducing the computational complexity to a one-time up-front cost [5, 14].\nThe state-of-the-art planning solvers are often heuristic-based planners [16]. These planners guide the combinatorial search of reaching a goal state from the initial state with heuristics, usually based on computing the cost of solving a relaxed plan as an estimate of the actual cost of reaching the solution [2]. Given the success of heuristic search in planning, Segovia-Aguas et al. [11, 13] propose a heuristic-based approach to generalized planning, which they call Best-First Generalized Planning (BFGP). BFGP leverages a set of novel GP native heuristics and a new solution space, independent of the number of input instances, to compute general algorithmic solu-tions.\nParallel programming is tightly coupled with Al's recent success, as CPU manufacturers have transitioned to multi-core processors due to single-core performance stagnation [7]. As a consequence, there has been a notable effort to parallelize fundamental algorithms like Best-First Search (BFS), and these techniques have already been suc-cessfully applied to planning domains [1, 7, 9]. One of the most im-"}, {"title": "2 Suitness of Best-First Generalized Planning for parallelization", "content": "A classical planning problem [4] is defined as P = \u3008D,I), where D = (F, A) is the domain that comprises the set of lifted predicates F and actions A, and I = (\u03a9, I, G) is the instance that specifies the set of constant objects, the initial state I, and the goal condition. A solution to P is a sequence of actions or plan that maps the initial state to a goal state where the goal condition holds.\nGP is formalized as the problem of finding an algorithm-like solu-tion \u03a0 (also known as generalized plan) to a set of T planning prob-lems, that is P = {P1, P2, ..., PT}, where all planning problems share the domain, but may differ in the instances (different objects, initial state and/or goal condition). We focus on a special kind of gen-eralized plans named planning programs. Formally, a planning pro-gram [10] is a sequence of n instructions \u03a0 = (\u03c90, ..., \u03c9n\u22121), where each instruction \u03c9i is associated with a program line 0 \u2264 i < n and can be one of the following three types:\nA planning action \u03c9i \u2208 A, where A is the set of deterministic functions of the planning domain.\nA goto instruction \u03c9i = goto(i', !y), where i' is a program line such that 0 \u2264 i' < n and i' \u2260 i, and y is a proposition. Propo-sition y can be the result of an arbitrary expression on state vari-ables.\nA termination instruction \u03c9i = end. The last instruction of a planning program is always a termination instruction.\nA planning program \u03a0 is a solution to P iff the execution of \u03a0 on every Pi \u2208 P generates a classical plan \u03c0i that is a solution to the original planning problem.\nIn this work, we use the generalized planner BFGP [13], which has shown good performance for computing planning programs via"}, {"title": "3 Parallel Best-First Generalized Planning", "content": "The following section presents our two strategies for parallelizing BFGP\u00b9 and evaluates their performance in 9 different classical plan-ning domains; 3 of them are propositional (corridor, gripper, and visitall) and the other 6 are numeric domains (fibonacci, find, reverse, select, sorting, and triangular sum).\nParallel strategy #1. It sequentially expands nodes until there are at least N nodes per thread\u00b2. Then, it starts a parallel search in which each thread is independent and does not share nodes with other threads. To ensure a balanced workload distribution, N should be larger for planning domains that require more program lines to reach a solution.\nParallel strategy #2. In this strategy, threads distribute promising nodes during the parallel search phase, so there is a tradeoff between searching the most promising states and minimizing the communi-cation overhead. In our solution, we compute the cost-to-go value of each generated node, and if it is equal to or better than the last ex-panded node, we send the new node to another thread. In contrast to HDA*, where a hash function determines which process will re-ceive the node, we simply cycle between all threads. This approach is viable because BFGP does not need to perform duplicate detection."}, {"title": "4 Discussion", "content": "The first strategy performs very well, with speedups ranging from ~4x to ~98x in the most complex domains. Furthermore, increas-ing the number of threads always results in better performance. On the other hand, no parallel strategy strictly dominates. In some do-mains (like Visitall), the second strategy gets better scaling and per-formance than the first strategy, but in others, it gets slower execu-tion times. We believe that a better prioritization of promising nodes and the use of asynchronous communication would help the second strategy perform better than the first one. To conclude, our results show that BFGP is well-suited for parallelization, and further de-velopments could make BFGP capable of handling more complex problems from IPC planning domains[16]."}]}