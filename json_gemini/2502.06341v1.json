{"title": "Facial Analysis Systems and Down Syndrome", "authors": ["Marco Rondina", "Fabiana Vinci", "Antonio Vetr\u00f2", "Juan Carlos De Martin"], "abstract": "The ethical, social and legal issues surrounding facial analysis technologies have been widely debated in recent years. Key critics have argued that these technologies can perpetuate bias and discrimination, particularly against marginalized groups. We contribute to this field of research by reporting on the limitations of facial analysis systems with the faces of people with Down syndrome: this particularly vulnerable group has received very little attention in the literature so far.\nThis study involved the creation of a specific dataset of face images. An experimental group with faces of people with Down syndrome, and a control group with faces of people who are not affected by the syndrome. Two commercial tools were tested on the dataset, along three tasks: gender recognition, age prediction and face labelling.\nThe results show an overall lower accuracy of prediction in the experimental group, and other specific patterns of performance differences: i) high error rates in gender recognition in the category of males with Down syndrome; ii) adults with Down syndrome were more often incorrectly labelled as children; iii) social stereotypes are propagated in both the control and experimental groups, with labels related to aesthetics more often associated with women, and labels related to education level and skills more often associated with men.\nThese results, although limited in scope, shed new light on the biases that alter face classification when applied to faces of people with Down syndrome. They confirm the structural limitation of the technology, which is inherently dependent on the datasets used to train the models.", "sections": [{"title": "Introduction and motivation", "content": "In recent years, the ethical, social and legal implications of Facial Analysis Systems (FASs) arose in several parts of the world. Several municipalities and governments banned the use of facial recognition technologies in public spaces, such as the city of San Francisco [17]. In Italy, a moratorium [14] suspended the\nThis preprint has not undergone peer review (when applicable) or any post-submission improvements or corrections. The Version of Record of this con-tribution is published in \"Machine Learning and Principles and Practice ofKnowledge Discovery in Databases. ECML PKDD 2023. Communications inComputer and Information Science, vol 2133. Springer, Cham.\", and is avail-able online at https://doi.org/10.1007/978-3-031-74630-7_10"}, {"title": "Related Works", "content": "Several scholars highlighted the ethical issues of FASs. Crawford in Atlas ofAI [3] reconstructed the history and development of AI in relationships with avariety of impacts (e.g., on the environment, work, health) and highlighted theepistemic issues of FASs and their controversial historical origins.\nOther studies focused on the failures of FASs and the associated negativeimpacts on society. In terms of gender and ethnicity, Buolamwini and Gebru[2] evaluated different commercial gender classification systems, and found thatdarker-skinned women were the most misclassified group. Similar findings arereported by Klare et al. [9], who found that commercial and non-trainable al-gorithms performed worse for women, blacks and young people. The issue ofdiversity and inclusion in FASs can arise from the lack of examples of subpop-ulations in the train dataset, but also from the definition of classes that incor-"}, {"title": "Study Design", "content": "We describe the research questions that drove the entire analysis (Section 3.1),how the test set was constructed (Section 3.2) and how the FASs were selected(Section 3.3)."}, {"title": "Research Questions", "content": "RQ1: How does the FASs work, with images of Down-syndrome peo-ple, regarding the predictions of a) gender and b) age?\nPrevious work has shown how FASs can fail to predict age and gender forvulnerable people (Section 2), but never for people with disabilities. In this work,we are interested in understanding whether predicting gender and age for peoplewith Down syndrome work in the same way as for people without the syndrome.The consequences of incorrect gender and age predictions are diverse and dependon the decisions that are made according to these predictions (e.g., hiring, wrongassociations for personalized contents, etc.).\nRQ2: Are the labels assigned differently to people with or withoutDown syndrome by image recognition models?\nThe labels generated by the models and associated with an input image, canbe used by companies for many purposes. The labels are generally correlatedwith gender, objects in the image and emotions of the people. The aim of thispart of the study is to analyse the labels of the images and to evaluate any"}, {"title": "Dataset", "content": "In the past, some researchers have used datasets of faces of people with Downsyndrome. They focused their experiments on classifying people with or withoutthe syndrome [1,16]. However, they focused exclusively on children, who are notrepresentative of the population as a whole.\nIt was therefore necessary to build a set of facial images of people with Downsyndrome from scratch: this set is the experimental group (EG). Due to a lackof resources, it wasn't possible to build a sample by taking photos directly orby contacting people and asking them to send their photos (in both cases withtheir explicit consent). It was therefore necessary to use images already availableon the web. The images come from Google searches and from websites that offerfree stock images, such as iStock and Pexels. In this way, it is impossible toknow whether the individuals have given their explicit consent: for this reason,we err on the side of caution and do not redistribute them (see the discussion insection 5). Not every EG image has a referenced age: the age is known for 66 EGmales and 64 EG females. It is also important to note that the life expectancyof people with Down syndrome is currently around 65 years [7]. The resultingEG set consisted of 200 images.\nThe control group (CG, i.e. images of people not affected by Down syndrome)also consisted of 200 images: these were collected by selecting some of the bestquality images from the UTKFace dataset\u00b9 [21]. This dataset is a well-knownlarge-scale face dataset with a long age range (from 0 to 116 years), constructedfrom images of famous people.\nA total of 400 images make up the dataset, 100 for each of the followingcategories: EG male, EG female, CG male and CG female. Each image in thedataset was stored in two different ways, according to the reference rule of one ofthe tools used (see section 3.3 that require cropped - representing only the partof the images with the face - for gender and age detection 2). Thus, on the onehand, the cropped version of the images was used for gender and age recognition.On the other hand, the not cropped version of the images, was used for labeldetection.\nAll images in the dataset were paired with gender and age to test the pre-dictions of the models. In line with the categories used by most FASs, gender isconsidered to be binary: we are aware of the limitations of such a representation.Age is the corresponding age of the person at the time the photo was taken. Theinformation on age was the most difficult to find. For the EG, we knew the agefor 66 and 64 pictures of the female and male, respectively. Instead, the CG was"}, {"title": "Models", "content": "The selection of services offering facial analysis, was based on a study of themost widely used and well-known commercial services, that meet some initialconstraints:\n-the images should not be retained for use for other purposes, or at a minimumbe deleted with account suspension;\n-it should be possible to predict gender and age;\n-there should be a free amount of test operations.\nThe two services that met the previous conditions are: ClarifAI and AWS Rekog-nition 3. Some other services were considered, but not selected because they didnot meet the above conditions. In particular: the services retain images indefi-nitely, as in the case of Face++ and Mega Matcher; they require a payment forthe operations, Face++ and Cognitec's Face VACS; they do not provide genderrecognition, Microsoft Face API, or they are deprecated, IBM Watson VisualRecognition.\nBoth ClarifAI and AWS Rekognition provide different models that are usedduring the analysis. The models and their details are shown and summarizedin the Supplementary Material, Table 1, Appendix B. The selected services alsoprovide some suggestions, called Rules of reference, for the correct use of models:they are summarized and reported in the Supplementary Material, Appendix A.The tests were operationalized using the SDKs for Python provided by AWSRand ClarifAI4 5. The output of the models is a JSON file containing differenttypes of information.\nAccording to the research questions codified in Section 3.1, this paper anal-yses three different tasks: gender recognition, age prediction and image recog-nition. As previously mentioned, gender is categorized in a binary format: maleand female. Age prediction is performed differently by the two models. ClarifAI,assigns a probability to each of the possible age intervals for each image: the"}, {"title": "Results and Discussion", "content": null}, {"title": "RQ1.a - Gender Recognition", "content": "Table 1 presents the values of Accuracy, Recall, Precision and F1-score, for bothgender recognition models and groups. The accuracy scores of the EG were lowerthan those of the CG for both models: the discrepancy between these scores wasabout 7% and 4% respectively. In general, the results of the EG were lower thanthose of the CG for both models. The F1-scores of the EG were lower than thoseof the CG. Female Precision and male Recall showed lower values between EGand CG.\nA closer examination of the misclassified images was carried out. On theone hand, all misclassified images of the EG male group represent children andadolescents. On the other hand, the misclassified images of EG females by theClarifAI model represent old people. The rule of reference number 2 (AppendixA.1 in the Supplementary Material) regarding the AWSR model suggests thatthe confidence value assigned to each prediction of gender should be checkedand taken into account. The threshold considered safe for sensible subjects is setat 99.00% by the rules of the model. Following the previous recommendation, adetailed examination of the confidence values is carried out on each group of thedataset, as shown in Figure 1.\nFigures 1a and 1b illustrate the distribution of confidence values for eachclass of the dataset, including the values of correct and incorrect predictions:the colours green and purple represent the correctness and incorrectness of themodel prediction in the study, respectively. The different shades of these coloursrepresent the confidence levels and their own level of error. Both models per-formed poorly for the EG male category. The AWSR model correctly classified66% of the images with a confidence level greater than or equal to 99.00%. TheClarifAI model correctly classified 60% of the images with a confidence levelgreater than or equal to 99.00%. We observe that the categories with the bestperformance were the female ones. In particular the category EG female, forthe AWSR model, does not contain misclassification of gender. Looking at theincorrect predictions, we found that about a 5% of the predictions of the EGmale classes had a confidence value greater than or equal to 99.00%. This meansthat the model misclassified images for which it was very confident about theprediction.\nFinally, Table 2 shows the accuracy values considering only the predictionwith a confidence value greater or equal to 99.00%. Within females, the dis-crepancy between EG and CG was reduced. In fact, in the case of ClarifAI, thedifference between EG and CG is about 6%, while in the case of AWSR, EGperformed better than CG (2%). The males highlight large performance discrep-ancies between EG and CG. In the AWSR case, the difference in accuracy was24%, while in the case of ClarifAI the difference in accuracy was equal to 15%."}, {"title": "RQ1.b - Age Prediction", "content": "In the models analysed, age prediction is a classification problem and the result ofthe prediction consists of a range of ages rather than a precise value, as describedin section 3.3. The accuracy values are presented in Table 3. The results showthat the performances of both models are low: only half of the samples arepredicted correctly.\nThe truth tables were constructed using the ClarifAI ranges for both thetrue range and the predicted range. Each value of the truth tables 4a, 4b, 4c,4d represent the number of images predicted in the corresponding range of ages.Looking at the ClarifAI model (Tables 4b and 4a) we can observe some differ-ences between EG and CG. The EG performed worst in the ranges: 20-29, 30-39,40-49, while the CG performed worst in the ranges between: 40-49, 50-59, 60-69,> 70. The AWSR predictions (Tables 4d and 4c) were slightly more accurate.\nMost of the errors for the EG were in the ranges: 30-39, 40-49 whereas forthe CG they were in the ranges: 30-39, 60-69, \u2265 70. The comparison should takeinto account that the average life expectancy of people with Down syndrome isaround 65 years, and the EG dataset contains images of people with a maximumage in the range 50-59.\nFocusing on the predictions of ClarifAI on the EG, Table 4a shows that thepredicted ranges 3-9 and 10-19 are the two intervals with higher variance in thedataset. This means that some images with a true age range of 20-29 and 30-39are labelled with the age range 3-9 or 10-19. The same can be verified for theage range 40-49 with predictions of 10-19. Differently, for the CG, Table 4b, theage ranges with higher variance are 20-29 and 30-39. Some images with a trueage range of 50-59 or 60-69 are labelled with age ranges of 20-29 and 30-39. Forthe CG, the predicted age ranges are lower than the real ones, but they nevercoincide with the age ranges of children. The predictions corresponding to a lowerage could be due to the fact that the dataset is made up of images of famouspeople, i.e. people with facial surgery and make-up who make themselves lookyounger. Instead, the images representing adult people with Down syndrome areclassified with children in age ranges such as 3-9 and 10-19.\nThe AWSR model is more stable in the ranges of its predictions. The rangeswith higher variance for the EG are 3-9 and 10-19, whereas for the CG almostall ranges have the same variance. However, some errors in the AWSR model arequite similar to those in the ClarifAI model for the experimental group. Some"}, {"title": "RQ2. Image labelling", "content": "Aesthetics and Education Figure 2a shows that the EG had a higher numberof occurrences than the CG for every concept except the label Sexy, althoughthe difference is very small. Looking at the gender distinction, Figure 2b, it isnoticeable that for both the EG and the CG, women were more likely to beassociated with the aesthetic labels than men. In terms of numbers, 316 vs.135 labels were assigned to females and males, respectively. Furthermore, thelabel Sexy is only assigned to those images that were classified as female in thegender recognition task. The description of this label is linked to the ability toarouse sexual desire or interest, which is only associated with the female gender.Regarding the labels of the category Education, Figure 3b, most of the labelswere associated with images representing males rather than females. The overallsituation reflects the typical stereotypes associated with gender, with aestheticlabels associated with females and educational labels associated with males.\nPerson Descriptors The name Person description is taken from one of thepredefined categories of the AWSR model. All the labels are common to bothmodels, so a comparison can be made as shown in Figure 4a and Figure 4b.\nBoth models assigned the label Child more often to the EG than to the CG,although the number of images representing children is quite balanced betweenthe two groups. The same consideration can be made for the label Adult, wherethe situation is reversed: this result is consistent with the observations from theage prediction task, i.e. the models were more likely to consider a person in theEG as a child rather than an adult.\nEach image represents only one person, and according to the definition givenby the ClarifAI model for the label Person one human being - and the labelPeople - (plural) any group of people (men or women or children) together - thelabel the label Person is the correct one for each image in the dataset. ClarifAIused the correct label for only 56 images out of a total of 400 images, whileAWSR used the correct label for 399 images out of a total of 400 images.\nThe other labels are gendered in the sense that they refer strictly to onegender rather than the other. For this reason, it may be useful to look at theFigure 4c and 4d where a comparison is made of the labels assigned between allfour different classes of the dataset: EG male, EG female, CG male, CG female.All of these gendered labels occurred for both genders, meaning that some images"}, {"title": "Threats to Validity and Ethical Concerns", "content": "Most of the limitations and ethical concerns of the study relate to the construc-tion of the dataset and the choice of models.\nStarting from the limitations of the dataset, some images are of poor quality.The resizing of the images to obtain the cropped versions inevitably lowered thequality of about 15% of the images belonging to the CG. The impact of the lowerquality of these images on the classification results cannot be excluded, but thisproblem affected a limited number of images.\nAs described in Section 3.2, some images of the EG did not include informa-tion about the year in which they were taken or the age of the person depicted.The resulting limitation is an unbalanced comparison between the two groups,EG and CG, in terms of the different number of samples for each age range.\nSince there is no information and no details on which datasets were used forthe learning phase of the models, we cannot exclude the possibility that the sameimages, or some of them, were used in the training process. Building a test setwith homemade images requires relevant economic and organizational resources.\nAs the dataset has been created using resources available online, we do nothave the explicit consent of the people depicted in the images. For this reason,the dataset created is not available to the public and it will remain private.\nAnother important limitation relates to the ethnicity of the people repre-sented. We carefully constructed the dataset to include people from differentbackgrounds, so that a variety of ethnicities are included in the dataset. How-ever, given the difficulties in finding images for the EG, we didn't aim at anequal distribution of different ethnicities, leaving this aspect to future work."}, {"title": "Conclusions", "content": "The goal of this study was to understand whether people with Down syndromemay experience problems when their facial image is automatically classified.By focusing on this specific group of vulnerable people, we identified a gap inthe literature on bias in facial analysis systems and further contributed to theinvestigation of the inherent limitations of this technology.\nTo achieve our goal, we created a test set by collecting facial images alreadyavailable on the web. We collected 400 images, 200 faces of people with Down syn-drome (experimental group, EG) and 200 faces of people without the syndrome(control group, CG). We then compared the performance of two commercial facerecognition tools. The results showed that overall the tools performed less wellwith the EG. We also found that: i) the gender prediction showed a higher errorrate towards the Down male sample, with an accuracy value of 85% for bothtools; ii) people with Down syndrome were assigned a younger age in relationto their real age; iii) the labels assigned to the experimental group reflected thesame gender stereotypes observed in the labels of the control group, in certaincases with a higher frequency."}, {"title": "Future Work", "content": "The way in which the dataset was constructed was, for the time being, the mostfeasible way of investigating the research questions. One of the first improve-ments in the construction of the dataset is to involve people from the selectedcommunities and ask them to take photographs of themselves, thus obtainingtheir consent and some valuable information. This could be a valid solution tosome limitations mentioned above, such as knowing the exact age of each personand getting their explicit consent to be part of the research. Furthermore, someproblems encountered during the process, such as pose, lighting and quality, canbe solved by using appropriate cameras and rules for taking photographs.\nAnother important improvement concerns ethnicity. An idea to construct anequally balanced dataset can be inspired by the Pilot Parliaments Benchmarkdataset [2].\nIn terms of models, it would be interesting to increase the number of modelsstudied. This will make it possible to get a broader picture of how FASs areperforming in relation to groups that are under-represented.\nFurther progress could be made on active measures to reduce discriminationagainst under-represented groups (such as people with Down syndrome). Sincethe subgroup performance issues that lead to dangerous discrimination stemmost probably from under-representation and unbalanced data, it would be in-teresting to explore a way to measure data characteristics (such as balance) andprovide ad hoc designed labels that provide ethically relevant information. Sucha tool could be integrated into the AI pipeline to allow developers to be awareof the data issues and take into consideration meaningful countermeasures. Thiscould be also useful if it is used to publish and disseminate relevant informationrelated to public datasets that are widely used in the AI community, such asthose used or mentioned in this article."}]}