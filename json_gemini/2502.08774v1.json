{"title": "Exploring Test Time Adaptation for Subcortical Segmentation of the Fetal Brain in 3D Ultrasound", "authors": ["Joshua Omolegan", "Pak Hei Yeung", "Madeleine K. Wyburd", "Linde Hesse", "Monique Haak", "Ana I. L. Namburete", "Nicola K. Dinsdale"], "abstract": "Monitoring the growth of subcortical regions of the fetal brain in ultrasound (US) images can help identify the presence of abnormal development. Manually segmenting these regions is a challenging task, but recent work has shown that it can be automated using deep learning. However, applying pretrained models to unseen freehand US volumes often leads to a degradation of performance due to the vast differences in acquisition and alignment. In this work, we first demonstrate that test time adaptation (TTA) can be used to improve model performance in the presence of both real and simulated domain shifts. We further propose a novel TTA method by incorporating a normative atlas as a prior for anatomy. In the presence of various types of domain shifts, we benchmark the perfor- mance of different TTA methods and demonstrate the improve- ments brought by our proposed approach, which may further facilitate automated monitoring of fetal brain development.", "sections": [{"title": "I. INTRODUCTION", "content": "During pregnancy, ultrasound (US) is routinely used to assess the development of certain brain structures (e.g. sub- cortical structures), whose volume can serve as biomarkers for neurological conditions [9]. However, since these assessments are normally performed in 2D, valuable 3D information is lost, which could provide richer diagnostic information [6]. To utilise 3D shape information, a method to segment sub- cortical structures is required. However, this is challenging due to the low levels of soft tissue contrast, reverberation artefacts and the presence of speckle. As a result, structural boundaries are often hard to distinguish, causing high levels of inter- and intra-rater variability in manual annotations [6]. Furthermore, subcortical segmentation is not a task usually completed in clinical practice, and thus even trained sonographers can have difficulty in accurately segmenting subcortical structures. Studies have shown that deep learning (DL) methods can be successfully applied to segmentation tasks in 3D US volumes of the fetal brain [6], [17], outperforming traditional image analysis methods. In particular, [6] proposed a DL framework for segmenting subcortical regions in the second trimester, achieving promising performance. However, the difficulty of obtaining manual annotations for these structures presents challenges for training DL models. Furthermore, as there is a high variability in possible US acquisition settings, a model trained for a given set of US images may not perform well for images acquired with a different set-up. Due to the tendency for DL models to overfit to their training data, changes in image characteristics cause a domain shift, normally resulting in a drop in performance and poor quality segmentations. Domain adaptation methods aim to overcome this domain shift by learning a shared feature representation between source and target domains [5], [14]. However, they assume simultaneous and constant access to the source and target data which is normally infeasible: the source data may not be available for the adaptation phase, for instance, due to confidentiality agreements [1], and model retraining is compu- tationally expensive. Source Free Domain Adaptation [1], [8] relaxes this requirement, enabling the adaptation of models without access to the source data. However, it is assumed that the target data belongs to a single domain: that is, all images are collected with the same acquisition protocol. This assumption is not valid for US data, as the acquisition is often changed for each individual scan to produce the best image. To overcome the domain shift commonly seen in US, we explore the use of test-time adaptation (TTA) to adapt an existing subcortical segmentation model [6]. TTA adapts models to new data during testing, with access only to the testing data and the pretrained model [7], [16] and can produce a differently adapted model for each test sample or batch. We hypothesise that TTA should be able to overcome the large diversity of domain shifts between US scans. Our contributions are as follows: \u2022 We demonstrate that TTA can be used to adapt models to US specific domain shifts, without needing additional manual segmentation, and explore the use of novel adap- tations to the existing Test Entropy Minimisation (TENT) method [16] to make TTA more suitable for fetal US. \u2022 We propose a novel TTA method (EntropyKL) to in- corporate an US atlas [11] as a prior for the expected volume of the each subcortical region respectively, and show how its incorporation increases the performance of model adaptation across a range of domain shifts. \u2022 We benchmark the performance of a range of TTA approaches in the presence of simulated domain shifts,"}, {"title": "II. METHOD", "content": "We assume access to a pretrained source model, f(X, Ys; \u03a6), where \u03a6 are learned weights and {Xs, Ys} are the labelled training data pairs from the dataset Ds. We wish to adapt the model, f, as to achieve the maximum performance for each sample in a target data Dt = {Xt} for which no labels are available. A schematic demonstrating the approach can be seen in Fig. 1. In this section, we first introduce the adaptation of different TTA approaches to fetal US subcortical segmentation. Then, we describe our proposed TTA technique to incorporate an US atlas [11] as a prior for the expected volume of the subcortical regions.\nA. Test Entropy Minimisation\nWe use Test Entropy Minimisation (TENT) [16] as our baseline TTA approach, as it has shown success with medical images [15]. The goal of TENT is to minimise the entropy, H(\u0176t), of the model predictions, where \u0176t = f(Xt, \u03a6') and \u03a6' are the adapted model weights \u03a6 + \u2206, where \u2206 is a small change, based on the observation that model predictions with the highest entropy are the most likely to be incorrect [16]. Thus, TENT minimises the Shannon entropy:\n$L_{TENT} = -\\sum_{c} p(\\hat{Y}_c)log(p(\\hat{Y}_c))$ (1)\nwhere p(\u0176c) is the softmax output from f for class c\u2208 C and C is the total number of segmentation classes.\nHowever, na\u00efve minimisation of this loss would lead to the model moving too far away from the source learned feature parameters and thus the loss of the source model knowledge. Therefore, TENT only optimises the weights of the batch normalisation layers as these are linear and low-dimensional, constraining the model weight updates and reducing the like- lihood of collapsing to a trivial solution.\nB. LayerInspect\nModifying the parameters of only the batch normalisation layers potentially limits the types of distribution shift we can adapt to. Thus, to allow for more flexible adaptation, we consider updating layers other than the batch normalisation layers. As we still need to not deviate too far from the original model, inspired by pruning methods [4], we propose a new TTA method, LayerInspect. In this, we select the m layers with the largest difference in magnitude between the source and target activations to be updated. The source activations need only be calculated once (at the end of model training), so the source data does not need to be stored. Following the notation in [4], consider the feature activation maps in a network, f, denoted by $z_l^{(k)}$ where $l \\in 1...L$ is current layer in the model f and k refers to the kth filter in layer l. To assess the magnitude of the activations, we calculate the Taylor approximation proposed in [10]:\n$\\Theta_{TE}(z_l^{(k)}) = \\frac{1}{N} \\sum_{i=1}^N z_l^{(k)} \\frac{\\partial L_{TENT}}{\\partial z_{lin}^{(k)}}$ (2)\nwhere N is the total number of data points. We utilise the Taylor approximation as it accounts for gradient information that may be useful for determining the layers with the largest impact on the models output. The activations are then nor- malised using L2 normalisation, to account for differences in scale with model depth [10], and finally the difference is found as $\u0398_{TE}^s - \u0398_{TE}^t$\nC. Incorporating Atlas Prior via EntropyKL\nFinally, following [1], we considered the incorporation of a class ratio prior to the loss function, providing the model with an estimate of the amount of each tissue that should be present, preventing the model from collapsing to the trivial solution. In [11], the authors used fetal brain ultrasound scans collected as part of the INTERGROWTH-21st Project [12] to create a normative atlas of the average healthy fetal brain at various weeks of pregnancy. Additionally, the authors gener- ated segmentation maps of the atlas by manual segmentation. We use this as our estimate of the expected amount of each tissue, and integrate it into our TTA methods by combining it with the Entropy minimization objective from TENT. Our new loss function is:\n$L_{ENTROPYKL} = L_{TENT} + \\lambda KL(\\hat{t}||T_t)$ (3)\nwhere KL(p||q) = $\\sum_{i=1}^n p(i) log(\\frac{p(i)}{q(i)})$ is the KL divergence - a measure of the difference between two probability distributions, Tt(k) is the proportion of the atlas that was class k, \u02c6t(k) is the average probability of a pixel in the image belonging to class k, and \u03bb is a hyperparameter chosen to weight the contribution of the KL divergence term. This prior gives our model some idea of how much of each class should be present and penalises updates to the model parameters that drastically change the ratio of classes. Note that we still only update the batch norm parameters using this new objective for the same reasons as in TENT.\nIf the image we wish to adapt our model to is not a healthy fetal brain (e.g. if a disease or birth defect caused the domain shift), the ratio of classes may not be similar to the ratio of classes in our atlas. To address this, we use the"}, {"title": "III. EXPERIMENTAL SETUP", "content": "Source Model: We used the model from [6] as the source model, which used few shot learning to segment four subcor- tical regions: choroid plexus (CP), lateral posterior ventricle horn (LPVH), cerebellum (CB) and cavum septum pellucidum et vergae (CSPV) during the second trimester. The model was trained from manual annotations on 3D US images acquired by the INTERGROWTH-21st Fetal Growth Longitudinal Study [12], collected using a Philips HD9 curvilinear probe.\nSimulated Domain Shifts: We first considered a range of sim- ulated domain shifts, designed to capture expected variation between US scans, going beyond the magnitude of the aug- mentations used in [6]. We used 59 pre-aligned volumes at 21 gestational weeks from the INTERGROWTH-21st study [12], with propagated annotations from an atlas [6], [11]. All images are of size 160 \u00d7 160 \u00d7 160 voxels, with an isotropic voxel size of 0.6mm. The considered domain shifts were: rotations, scaling, Gaussian smoothing and contrast changes (Gamma Correction), implemented using TorchIO [13]. These simulated shifts and propagated annotations allow us to quantitatively evaluate the performance of the TTA approaches.\nAugmentation Details: Rotations and scaling are both performed along each dimension. Given a value x, each image is rotated by a value randomly selected from the range [-x, x]. For gamma correction, this value of log \u03b3 is also sampled from [-x,x]. The value of \u03c3 for Gaussian smoothing is sampled from [0,x]. Note that augmentations were used during the training of our base model, and so we chose ranges that exceed the range of augmentation seen during training, but are still realistic. This ensures that the data is out-of-distribution.\nGestational Age: The source model was trained on fetal images acquired between 18+0days and 26+6days gestational weeks (GW). However, the appearance and size of the sub- cortical structures we consider in this work develop drastically over this period [11], and a single model may be suboptimal across this variation. Thus, we consider using TTA to adapt the model across this age range, considering the performance separately across each week. This gave a dataset of 529 labelled images with atlas-propagated annotations.\nUnseen Datasets: Finally, we consider data from two unseen scanners (Canon and GE) collected at the Leiden University Medical Centre [18]. We again consider scans from 21 GW and both datasets have been preprocessed in an identical way to the INTERGROWTH-21st study, and so have been aligned, are of size 160\u00d7160\u00d7160 voxels, and have an isotropic voxel size of 0.6mm. This resulted in 31 volumes from the Canon dataset and 22 from the GE dataset. Manual annotations are not available so only qualitative analysis is completed.\nImplementation: For each experiment, TTA was performed on an Nvidia A10 GPU. The Adam optimiser was used with a learning rate of 10\u22123 for both TENT and EntropyKL, and a learning rate of 10\u22124 for LayerInspect. Due to memory"}, {"title": "IV. RESULTS", "content": "Simulated Domain Shifts: We test both adapting to the full dataset and to a single volume at a time. Although it may be reasonable in many domains to assume all data from a site is a single domain [3], the flexible nature of US scan acquisition means this is unlikely. However, as TENT assumes access to batches of data [16], we compare both approaches. We also present two baselines: the original source model and histogram matching, as standard preprocessing approach. We compare Dice scores, averaged across the subcortical regions. Fig. 2 shows the results. All TTA methods provided sig- nificant improvement (paired t-test, p < 0.05) over the base model, indicating their effectiveness in overcoming the simulated domain shifts. They each have similar performance, but the proposed adaptations to TENT in LayerInspect and EntropyKL lead to improvement, with EntropyKL being the most robust to large transformations. Histogram matching provides little advantage over the base model apart from Gamma correction, where it effectively removes the applied augmentation. Comparing the results, we see TTA performs better when adapting to a single volume than to a batch across the range of transformations. This improvement, and the nature of each US image effectively being its own imaging domain due to the flexible nature of US imaging, motivates the use of single volume adaptation going forward. Gestational Age: Figure 4 compares our baselines with the TTA methods across gestational age. Considering three ages from the atlas, we see the structures change significantly throughout gestation, especially in the volume of the brain they"}, {"title": "V. DISCUSSION", "content": "Our results demonstrate that TTA can adapt subcortical segmentation models to domain shifts in US scans, showing improvements of up to 0.6 in Dice score for synthetic aug- mentations, and 0.1 for gestational age. Given the challenges of manually segmenting these regions in US images this may be key for the identification of abnormal fetal development. We further proposed a novel TTA technique to incorporate the normative altas [11] as a prior for expected volume, which improved performance over the other TTA methods, especially for the scans from unseen sites. However, since the atlas corresponds to healthy fetal growth, it may bias the model incorrectly in the presence of fetal abnormality. Future work will explore its use in this case."}]}