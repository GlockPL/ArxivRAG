{"title": "PREFERENCE DIFFUSION FOR RECOMMENDATION", "authors": ["Shuo Liu", "An Zhang", "Guoqing Hu", "Hong Qian", "Tat-Seng Chua"], "abstract": "Recommender systems predict personalized item rankings based on user preference distributions derived from historical behavior data. Recently, diffusion models (DMs) have gained attention in recommendation for their ability to model complex distributions, yet current DM-based recommenders often rely on traditional objectives like mean squared error (MSE) or recommendation objectives, which are not optimized for personalized ranking tasks or fail to fully leverage DM's generative potential. To address this, we propose PreferDiff, a tailored optimization objective for DM-based recommenders. PreferDiff transforms BPR into a log-likelihood ranking objective and integrates multiple negative samples to better capture user preferences. Specifically, we employ variational inference to handle the intractability through minimizing the variational upper bound and replaces MSE with cosine error to improve alignment with recommendation tasks. Finally, we balance learning generation and preference to enhance the training stability of DMs. PreferDiff offers three key benefits: it is the first personalized ranking loss designed specifically for DM-based recommenders and it improves ranking and faster convergence by addressing hard negatives. We also prove that it is theoretically connected to Direct Preference Optimization which indicates that it has the potential to align user preferences in DM-based recommenders via generative modeling. Extensive experiments across three benchmarks validate its superior recommendation performance and commendable general sequential recommendation capabilities.", "sections": [{"title": "INTRODUCTION", "content": "The recommender system endeavors to model the user preference distribution based on their historical behaviour data (He & McAuley, 2016; Wang et al., 2019; Rendle, 2022) and predict personalized item rankings. Recently, diffusion models (DMs) (Sohl-Dickstein et al., 2015; Ho et al., 2020; Yang et al., 2024) have gained considerable attention for their robust capacity to model complex data distributions and versatility across a wide range of applications, encompassing diverse input styles: texts (Li et al., 2022; Lovelace et al., 2023), images (Dhariwal & Nichol, 2021; Ho & Salimans, 2022) and videos (Ho et al., 2022a;b). As a result, there has been growing interest in employing DMs as recommenders in recommender systems.\nThese DM-based recommenders utilize the diffusion-then-denoising process on the user's historical interaction data to uncover the potential target item, typically following one of three approaches: modeling the distribution of the next item (Yang et al., 2023b; Wang et al., 2024b; Li et al., 2024), capturing the user preference distribution (Wang et al., 2023b; Zhao et al., 2024; Hou et al., 2024a; Zhu et al., 2024), or focusing on the distribution of time intervals for predicting the user's next action (Ma et al., 2024a). However, prevalent DM-based recommenders often routinely rely on standard generative loss functions, such as mean squared error (MSE), or blindly adapt established recommendation objectives, such as Bayesian personalized ranking (BPR) (Rendle et al., 2009) and (binary) cross entropy (Sun et al., 2019) without any modification. Despite their empirical"}, {"title": "PRELIMINARY", "content": "In this section, we begin by formally introducing the task of sequential recommendation and then introduce the foundations of DM-based recommenders who model the next-item distribution.\nSequential Recommendation. Suppose each user has a historical interaction sequence {1,2,...,in-1}, representing their interactions in chronological order and in is the next target item. For each sequence, we randomly sample negative items from batch or candidate set result in H = {i}. Moreover, each item i is associated with a unique item ID or additional descriptive information (e.g., title, brand and category). Via ID-embedding or text-embedding module, items can be transformed into its corresponding vectors e \u2208 R1\u00d7d. Therefore, the historical interaction sequence and negative items' set can be transformed to c = {e1, e2, . . ., en\u22121} and H = {ev}v=1. The goal of sequential recommendation is to give the personalized ranking on the whole candidate set, namely, predict the next item in user may prefer given the sequence c and negative items' set H.\nDiffusion models for Sequential Recommendation. In this section, we introduce the use of guided DMs to model the conditional next-item distribution p(in | i<n), following the DreamRec (Yang et al., 2023b). For clarity, we denote the vector representation of the next item in as e instead of en and negative items in as e\u014dr result in H = {e}41. The subscript denotes the timesteps in DM, where \"0\" indicates that no noise has been added, and the superscript represents whether the item is positive or negative, denoted by \"+\" or \"-\" respectively in recommendation. Notably, these notations will be used consistently in the subsequent sections.\n\u2022 Forward Process. DMs add Gaussian noise to the positive item embedding er with noise scale {\u03b11,\u03b12, , \u03b1\u03c4} over the pre-defined timesteps T, namely, q(e+ | ef) = N(\u221a\u0101te, (1 \u2013 \u0101t)I). If T\u2192 +\u221e, e+ asymptotically converges to the standard Gaussian distribution. q(e+ | ef) can be easily derived through applications of the reparameterization trick (Kingma & Welling, 2014).\n\u2022 Reverse Process. The reverse process aims to recover the target item embedding e from the standard Gaussian distribution through the denoising process with the personalized guidance c. Concretely, following the classical DMs' paradigm introduced in DDPM (Ho et al., 2020), we choose the simple objective which minimizes the KL divergence between the true denoising transition q(e-1et, e) and the intractable denoising transition pe(et_1 | e+, c). Leveraging the favorable properties of the Gaussian distribution, we can derive the following closed-form objective:\nLSimple = E(et,c,t) [||Fo(et, t, M(c)) - ||2],\nwhere e, c come from the training data. t ~ U(1,T) is the sampled timestep. M(\u00b7) denotes the arbitrary sequence encoder utilized in sequential recommendation (e.g., GRU (Hidasi et al., 2016), Transformer (Kang & McAuley, 2018), Bert (Sun et al., 2019)). Fe(\u00b7) serves as denoising network which is commonly parameterized by a simple MLP and 0 denotes the trainable parameters. Classifier-free guidance scheme (Ho & Salimans, 2022) can be utilized here to replace M(c) with"}, {"title": "METHODOLOGY: THE PROPOSED PREFERDIFF", "content": "In this section, we introduce PreferDiff, a novel loss for DM-based recommenders that can instill preference information. First, we extend the classical BPR loss to a probabilistic one, defining a new loss LBPR-Diff. To address the inherent intractability, we derive a variational upper bound Lupper for LBPR-Diff and optimize this bound instead. Furthermore, we explore the incorporation of multiple negative samples and propose an efficient strategy by lowering the likelihood of the negative samples' centroid, which avoids multiple denoising steps. Lastly, we make a trade-off between learning generation and learning preference to ensure training stability, resulting in the final loss, LPreferDiff."}, {"title": "CONNECT DIFFUSION MODELS WITH BAYESIAN PERSONALIZED RANKING", "content": "In this subsection, we explore the integration of DMs with the classical BPR loss (Rendle et al., 2009), which has been proven to be highly effective in real-world industrial recommendation scenarios. As BPR is designed to optimize personalized ranking by modeling user preferences in a pairwise fashion, it has been extensively applied in contemporary recommendation researches (Kang & McAuley, 2018; He et al., 2020). It can be formulated as\nLBPR = -E(e+e,c) [logo (fo(et | c) \u2013 fo (eo |c))],\nwhere e, e represents the positive item and one negative item in H, we omit v for brevity. c represents the historical item sequences. o is the Sigmoid function. fo (eo | c) is the predicted rating of item eo conditioned on the historical item sequence c. As DMs are part of the family of likelihood-based generative models (Yang et al., 2024) and are employed here to maximize the log-likelihood of the next item distribution log pe(e+ | c), it is clear that equation 2 does not meet this need. Therefore, we put forward to change the rating to probability distribution.\nFrom Rating to Probability Distribution. Here, we define the probability distribution of the next-item eo given historical item sequences c via a softmax over the arbitrarily flexible, parameterizable, rating function fe(\u00b7). It can be formulated as pe (eo | c) = exp(fo(eo|c))/Ze, where Ze is normalizing constant (a.k.a, partition function), defined as fexp(fe(e | c)) de. Then, by substituting it into equation 2, we obtain the following result, which we refer to as LBPR-Diff, as we utilize the DMs to model that distribution. The detailed derivation is provided in the Appendix C.1.\nLBPR-Diff(0) = -E(e,eo,c) [logo (logpo(et | c) \u2013 log pe (eo |c))] .\nIntuitively, LBPR-Diff seeks to widen the gap between the log-probability distributions of positive and negative items given c. However, the challenge is that equation 3 is intractable due to the need to marginalize over all possible diffusion paths as DMs are latent variable models. Therefore, like previous work (Sohl-Dickstein et al., 2015; Ho et al., 2020), we propose to minimize the LBPR-Diff via variational inference through minimizing the derived variational upper bound.\nMinimize the LBPR-Diff through Variational Upper Bound. Therefore, like previous work (Sohl-Dickstein et al., 2015; Ho et al., 2020) we introduce latent variables (e1,..., \u0435\u0442) result in po (eo | c) = fpe(eo:T | c) de1:T. Then, we substitute pe (e1:T | eo) with q(e1:T | e0) which is typically modeled as a Gaussian distribution with predefined mean and variance at each timestep, due to the intractability of directly sampling from the former distribution. The objective can be expressed as follows\nLBPR-Diff(0) = -E(et,eo,c) log [log Eq(ere0) \u0440\u043e(\u0435\u0442 | \u0441)/q(ere)  - log Eq(ere0) \u0440\u04e9(\u0435\u043e:\u0442 | \u0441)/q(ere)]"}, {"title": "DEEP ANALYSIS OF LBPR-DIFF", "content": "In this subsection, we demonstrate the two properties of LBPR-Diff by analyzing the gradient with respect to @ and connect it with recent popular Direct preference optimization. We also reveal the connection between rating function and score function in Appendix equation C.2 which bridges the objective of recommendation with generative modeling in DMs.\nGradient Analysis. Here, we analysis the gradients of LBPR-Diff to understand its impact on the training process of DMs for sequential recommendation.\n\u2202 LBPR-Diff (0)/\u2202\u03b8 = -E(ete,c) [wo (\u2202log po (e+ | c)/\u2202\u03b8 - Velog po(e|c))], where we = 1 \u2212 \u03c3 (log pe (e+ | c) \u2013 log po (eo | c)) represents the gradient weight. Obviously, if given certain item sequences, the DM incorrectly assigns higher likelihood to the negative items than positive items, the gradient weight we will be higher. Therefore, optimizing LBPR-Diff is capable of handling hard negatives, which has become increasingly important in recent researches (Chen et al., 2022; Fan et al., 2023; Zhang et al., 2023).\nConnection with Direct Preference Optimization. After determining how to minimize LBPR-Diff using the aforementioned upper bound and analyzing the gradient, we proceed to validate the rationality of LBPR-Diff. Here, we establish a connection with the recently prominent Direct Preference Optimization (DPO) (Rafailov et al., 2023; Wallace et al., 2024; Meng et al., 2024), which has been shown to effectively align human feedback with large language models. For further details on DPO, we refer readers to (Rafailov et al., 2023). The equation of DPO is expressed as follows"}, {"title": "EXTEND TO MULTIPLE NEGATIVES", "content": "As previous works have demonstrated that incorporating multiple negatives during the training phase can better capture user preferences, we extend LBPR-Diff to support multiple negatives for instilling more fruitful rank information. Suppose that for each sequence, we have negative items' set H introduce in Section 2, according to equation 7, we can directly derive that:\nLBPR-Diff-V = -logo(-|H|\u00b7(S(\u00eat, e\uc555) - 1/|H| \u03a3v=1S(eo, eo\")) .\nFor brevity, we omit the expectation term. However, the above equation applies the noising and denoising process to all negative samples, which significantly reduces the model's training speed and increases susceptibility to false negatives. Therefore, we propose to replace the |H| negative samples with their centroid\u0113 = \u03a3H v=1\u03a3\u03c5e as the diffusion target and derive the following:\nLBPR-Diff-C = \u2212 logo(-|H|\u00b7 [S(\u00ea+, ef) \u2013 S(Fo(\u0113\u012b, t, M(c)), \u0113\u014d)]) .\nAssuming that F(\u00b7) is a convex function, we can apply Jensen's inequality and derive that LBPR-Diff-VLBPR-Diff-C. Therefore, minimizing LBPR-Diff-C can efficiently increase the likelihood of the positive items while simultaneously distancing them from the centroid of the negative items. Intuitively, this aligns with the phenomenon that users may not explicitly indicate dislike for specific items, but rather for a certain category of items. Detailed derivation can be found in Appendix C.4.\nTraining and Inference of PreferDiff. Here, we introduce the training and inference details of PreferDiff, as demonstrated in Algorithm 1 and Algorithm 2 in Appendix. Empirically, we find that solely using the proposed LBPR-Diff-C leads to instability during training. This may be due to an overemphasis on ranking information, which can neglect the more accurate generation of the next item. Therefore, we balance the trade-off between learning generation and learning preference with hyperparameter \u5165, with the following:\nLPerferDiff = ALSimple + (1 \u2212 1)LBPR-Diff-C\""}, {"title": "EXPERIMENTS", "content": "In this section, we aim to answer the following research questions:\n\u2022 RQ1: How does PreferDiff perform compared with other sequential recommenders?\n\u2022 RQ2: Can PreferDiff leverage pretraining to achieve commendable zero-shot performance on unseen datasets or datasets from other platforms just like DMs in other fields?\n\u2022 RQ3: What is the impact of factors (e.g., \u03bb) on PreferDiff's performance?"}, {"title": "PERFORMANCE OF SEQUENTIAL RECOMMENDATION", "content": "Baselines. We comprehensively compare PreferDiff with five categories of sequential recommenders: traditional sequential recommenders, including GRU4Rec (Hidasi et al., 2016), SASRec (Kang & McAuley, 2018), and BERT4Rec (Sun et al., 2019); contrastive learning-based recommenders, such as CL4SRec (Xie et al., 2022); generative sequential recommenders like TIGER (Rajput et al., 2023); DM-based recommenders, including DiffRec (Wang et al., 2023b), DreamRec (Yang et al., 2023b) and DiffuRec (Li et al., 2024); and text-based sequential recommenders like MoRec (Yuan et al., 2023) and LLM2Bert4Rec (Harte et al., 2023). See Appendix D.3 for details on the introduction, selection and hyperparameter search range of the baselines.\nDatasets. We evaluate the proposed PreferDiff on three public real-world benchmarks (i.e., Sports, Beauty and Toys), utilizing the Amazon Reviews 2014 (He & McAuley, 2016), which spans user reviews and item metadata from May 1996 to October 2014. Detailed statistic of three benchmarks can be found in Table 5. Here, we utilize the common five-core datasets, filtering out users and items with fewer than five interactions. Following prior work (Yang et al., 2023b), we first sort all sequences chronologically for each dataset, then split the data into training, validation, and test sets with an 8:1:1 ratio, while preserving the last 10 interactions as the historical sequence. More Details about data prepossessing can be found in Appendix D.1. Notably, we also give comparison under another setting (i.e., leave-one out) to provide more insights which can be found in Appendix D.4.\nImplementation Details. For PerferDiff, for each user sequence, we treat the other next-items (a.k.a., labels) in the same batch as negative samples. We set the default diffusion timestep to 2000, DDIM step as 20, pu = 0.1, and the \u1e9e linearly increase in the range of [1e-4, 0.02] for all DM-based sequential recommenders (e.g., DreamRec). For all text-based recommenders, we utilize OpenAI-3-Large (Neelakantan et al., 2022) to obtain the text embeddings. We fix the embedding dimension to 64 for all models except DM-based recommenders, as the latter only demonstrate strong performance with higher embedding dimensions. The former does not gain much from high embedding dimensions, which will be discussed in Section 4.3. Refer to Appendix D.2 for more implementation details about baselines. Notably, PreferDiff can be applied to any sequence encoder, M(.). We provide the results of PreferDiff with other backbones in Appendix D.3.\nEvaluation Metrics. We evaluate the recommendation performance in full-ranking manner (Yang et al., 2023b) using Recall (Recall@K) and Normalized Discounted Cumulative Gain (NDCG@K) with K = 5, 10, following the widely adopted top-K protocol as the primary metrics for sequential recommendation (Kang & McAuley, 2018; Rajput et al., 2023).\nResults. Table 1 presents the performance of PreferDiff compared with five categories sequential recommenders. For brevity, R stands for Recall, and N stands for NDCG. The top-performing and runner-up results are shown in bold and underlined, respectively. \u201cImprov\u201d represents the relative improvement percentage of PreferDiff over the best baseline. \u201c*\u201d indicates that the improvements are statistically significant at the 0.05, according to the t-test. We can have the following observations:\n\u2022 DM-based recommenders have exhibited substantial performance gains over other sequential recommenders across most metrics. This is consistent with prior research, which demonstrates that the powerful generation and generalization capabilities (Yang et al., 2023b) or noise robust-ness (Wang et al., 2023b; Li et al., 2024) of DM can better capture user behavior distributions compared to other sequential recommenders and alleviate the false negative or false positive issue in recommendation (Sato et al., 2020; Chen et al., 2023b).\n\u2022 PreferDiff significantly outperforms other DM-based recommenders across all metrics on three public benchmarks. PreferDiff demonstrates an improvement ranging from 6.41% to 19.35%"}, {"title": "GENERAL SEQUENTIAL RECOMMENDATION (RQ2)", "content": "Given that DMs have exhibited exceptional zero-shot inference capabilities after pretraining on large, high-quality datasets in other fields (Khachatryan et al., 2023; Clark & Jaini, 2023), we aim to explore how PreferDiff can effectively zero-shot recommendation on unseen datasets, either within the same platform (e.g., Amazon) or across different platforms (e.g., Steam), without any overlap of users or items (Ding et al., 2021; Hou et al., 2022a; 2023; Li et al., 2023a), which distinguishes it from traditional ID-based cross-domain recommendation (Zhu et al., 2021; Ma et al., 2024b).\nBaselines. Here, we compare PreferDiff with two baselines which towards general sequential recom-mendations, namely UniSRec (Hou et al., 2022a) and MoRec (Yuan et al., 2023). See Appendix D.5 for details on the introduction, selection, and hyperparameter search range of the baselines. For a fair comparison, we employ the text-embedding-3-large model from OpenAI (Neelakantan et al., 2022) as the text encoder to convert identical item descriptions (e.g., title, category, brand) into representations, as it has been proven to deliver commendable performance in recommendation (Harte et al., 2023). More additional experiments about different text encoder can be found in Appendix E.3.\nDatasets and Evaluation Metrics. Following the previous work (Hou et al., 2022a; Li et al., 2023a), we select five different product reviews from Amazon 2018 (Ni et al., 2019), namely, \u201cAutomotive\u201d, \"Cell Phones and Accessories\u201d, \u201cGrocery and Gourmet Food\u201d, \u201cMusical Instruments\" and \"Tools and Home Improvement\u201d, as pretraining datasets. \u201cOffice Products\u201d is selected as the validation dataset for early stopping when Recall@5 (i.e., R@5) shows no improvement for 20 consecutive epochs. Here, we consider three scenarios for the incoming evaluated target datasets. (1) \u201cIn Domains\" refers to target datasets that are part of the pretraining dataset. (2) \u201cOut Domains", "CDs and Vinyl\u201d and \u201cMovies and TV\u201d. (3) \u201cOther Platform": "efers to target datasets that are neither in the pretraining dataset nor from the same platform. Here, we select a commonly used game dataset collected from Steam (Kang & McAuley, 2018). Detailed dataset statistics can be found in Table 5.\nResults. Tables 3 present the performance of PreferDiff compared with the chosen two general sequential recommenders. We can observe that:\n\u2022 Without any additional components, PreferDiff-T outperforms other general sequential recommenders. Unlike UniSRec, which employs mixture of experts technique for whitening, and MoRec, which uses dimension transformation, PreferDiff-T directly utilizes raw semantic text embeddings. This results in improvements of 2% to 8% in in-domain scenarios, 2% to 10% in"}, {"title": "STUDY OF PREFERDIFF (RQ3)", "content": "In this subsection, we study the important factors (e.g., \u03bb, embedding size and S(\u00b7)) which may impact the recommendation performance of PreferDiff. Others can be found in Appendix E.1 and Appendix E.2. We also provide visualization of learned item embeddings via t-SNE in Appendix E.4.\nImportance of A for PreferDiff A controls the balance between learning generation and learning preference in PreferDiff. As shown in Figure 3, PreferDiff performs best when x = 0.4 or x = 0.6, highlighting the importance of enabling DMs to understand negatives in the recommendation task.\nDimension of Embedding for PreferDiff. As shown in Figure 4, we empirically observe that the recommendation performance of both PreferDiff and DreamRec improves significantly as the embedding size increases. This finding contrasts with previous observations in some non-DM-based recommenders (Liu et al., 2020; Qu et al., 2023; Guo et al., 2024). We attribute this phenomenon to the dynamic feature space of ID embeddings during the training phase, which DMs requires higher dimension to capture the user preference and ensure the stability of embedding space.\nMeasure Function for PreferDiff. As the final recommendation is ranked by maximal inner prod-uct search, we replace MSE with cosine error, as introduced in equation 7. The results presented in Table 4 demonstrate the superiority of using set co-sine error as the default measurement function over MSE in PreferDiff."}, {"title": "CONCLUSIONS AND LIMITATIONS", "content": "We propose PreferDiff, an optimization objective specifically designed for DM-based recommenders which can integrate multiple negative samples into DMs via generative modeling paradigm. Optimization is achieved through variational inference, deriving a variational upper bound as a surrogate objective. However, PreferDiff has limitations: (1) Dimension Sensitivity: The recommendation performance of PreferDiff is highly dependent on the embedding dimension. Empirical results show a sharp decline in performance when the embedding size is reduced to 64, a common dimension in existing studies. This dependency may lead to increased computational resources and slower training times when larger embedding sizes are required. (2) Hyperparameter \u5165 Dependence: PreferDiff heavily relies on the hyperparameter A to balance the generation and preference learning in DMs.\nEthic Statement. This paper aims to develop a specially tailored objective for DM-based recom-menders through generative modeling. We do not anticipate any negative social impacts or violations of the ICLR code of ethics.\nReproducibility Statement. All results in this work are fully reproducible. The hyperparameter search space is discussed in Table 11, and further details about the hardware and software environment are provided in Appendix D.2. We provide the code and the best hyperparameters for our method at https://anonymous.4open.science/r/PreferDiff and Table 12."}, {"title": "RELATED WORK", "content": "We highlight key related works to contextualize how PreferDiff fits within and contributes to the broader literature. Specifically, our work aligns with research on sequential recommendation and DMs based recommenders.\nSequential Recommendation have gained significant attention in both academia (Rendle, 2022; Liu et al., 2024) and industry (Wang et al., 2019; Fang et al., 2020) due to their ability to capture user preferences from historical interactions and recommend the next item. One common research line has focused on developing more efficient network architectures, such as GRU (Hidasi et al., 2016), convolutional neural networks (Tang & Wang, 2018), Transformer (Kang & McAuley, 2018; Fan et al., 2021), Bert4Rec (Devlin et al., 2019), and HSTU (Zhai et al., 2024). Another research line focuses on leveraging additional unsupervised signals (Xie et al., 2022; Wang et al., 2023a; Ren et al., 2024a) or reshaping sequential recommendation into other tasks such as retrieval (Rajput et al., 2023; Wang et al., 2024a) and language generation (Bao et al., 2023; Li et al., 2023b; Liao et al., 2024).\nDM-based Recommenders have been explored in recent studies due to the powerful generative and generalization capabilities of DMs (DMs) (Lin et al., 2024). These recommenders either focus on modeling the distribution of the next item (e.g., (Yang et al., 2023b; Wang et al., 2024b; Li et al., 2024)), capture the probability distribution of user interactions (e.g., (Wang et al., 2023b; Zhao et al., 2024)), or focus on the distribution of time intervals between user behaviors (e.g., (Ma et al., 2024a)). However, existing approaches often rely on conventional objectives, such as mean squared error (MSE), or standard recommendation-specific objectives like Bayesian Personalized Ranking (BPR) (Rendle et al., 2009) and Cross Entropy (CE) (Klenitskiy & Vasilev, 2023). We argue that the former may diverge from the core objective of accurately modeling user preference distributions in recommendation tasks (Rendle, 2022), as DMs often lack an adequate understanding of negative items. While the latter leverages DMs' noise resistance to mitigate noisy interactions in recommendations which might fall short of fully exploiting the generative and generalization capabilities of DMs."}, {"title": "SAMPLING ALGORITHM IN PREFERDIFF", "content": "We utilize DDIM (Song et al., 2021a) as the default sampler in PreferDiff, replacing the DDPM used in DreamRec, as we empirically find that DDIM is faster and performs better, requiring only a few denoising steps. Here, we briefly introduce how DDIM is employed in PreferDiff; Detailed derivations can be found in (Song et al., 2021a), and the code implementation is available at https://github.com/lswhim/PreferDiff.\nDetails. Specifically, in PreferDiff, the training is to predict the original data eo. The sampling process should be reparameterized to predict eo directly instead of the noise e. Starting from the original DDIM update equation (Song et al., 2021a):\net-1 = \u221aat-1 (et - \u221a1 \u2013 at eo (et, t)/\u221aat) + \u221a1 - \u03b1\u03c4\u22121 \u2212 \u03c3\u1f76 \u03b5\u03b8(et, t) + \u03c3\u03c4z, where z ~ N(0, I), \u03c3\u03c4 controls the stochasticity of the process, and eo (et, t) is the predicted noise at time step t.\nIn PreferDiff, since our model is trained to predict the original data eo directly, we use the relationship between et, eo, and the noise \u0454:\net = \u221aat eo + \u221a1 \u2013 \u03b1\u03c4 \u20ac.\nSolving for e, we obtain:\n\u20ac = et - \u221aat eo / \u221a1 - at.\nSince eo is predicted by our model as \u00ea0 = Fe(et, c, t), we can estimate the noise as:"}]}