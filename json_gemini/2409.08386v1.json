{"title": "SELF-SUPERVISED INFERENCE OF AGENTS IN TRUSTLESS ENVIRONMENTS", "authors": ["Vladyslav Larin", "Ivan Nikitin", "Alexander Firsov"], "abstract": "In this paper, we propose a novel approach where agents can form swarms to produce high-quality responses effectively. This is accomplished by utilizing agents capable of data inference and ranking, which can be effectively implemented using LLMs as response classifiers. We assess existing approaches for trustless agent inference, define our methodology, estimate practical parameters, and model various types of malicious agent attacks. Our method leverages the collective intelligence of swarms, ensuring robust and efficient decentralized Al inference with better accuracy, security, and reliability. We show that our approach is an order of magnitude faster than other trustless inference strategies reaching less than 125 ms validation latency.", "sections": [{"title": "Introduction", "content": "Decentralized AI inference represents a transformative shift in the deployment and utilization of artificial intelligence. Traditional AI systems rely heavily on centralized infrastructures, which can lead to computational bottlenecks and single points of failure. These centralized systems often face challenges related to scalability and adaptability in meeting the growing demand for AI inference.\nTo address these limitations, a distributed approach has emerged, spreading AI computation across multiple nodes in a network. This method enhances system resilience and enables more efficient resource utilization. In such decentralized systems, tasks such as model training, inference, and data processing are performed across a distributed network. Consequently, this eliminates the need for a central authority, allowing for greater flexibility and potentially reducing operational costs. [1].\nThe evolution of decentralized AI has been facilitated by advances in blockchain technology, which provides a robust framework for trustless operations. Smart contracts enable the automation of service-level agreements (SLAs) and ensure that all parties adhere to predefined rules without the need for intermediaries. This is particularly beneficial in scenarios where trust and transparency are paramount, such as financial services and healthcare [2].\nSeveral innovative frameworks and protocols have been developed to support decentralized AI inference. The AI Protocol's Decentralized Inference Clusters (DePIN) exemplify the shift from traditional centralized models to distributed frameworks, enabling efficient and scalable AI services by leveraging decentralized resources [3]. Similarly, platforms like Nesa offer decentralized query marketplaces that support high-security and privacy measures, facilitating the deployment of AI models in trustless environments [4].\nDespite these advancements, significant challenges remain in achieving fast, trustless inference on large artificial neural networks (ANNs). The computational demands and latency associated with decentralized operations can hinder performance, especially for real-time applications. Additionally, ensuring the integrity and security of the inference process in the presence of malicious actors is a complex problem that requires robust solutions.\nThis paper explores the current state-of-the-art in decentralized AI inference, highlighting the advantages and limitations of various approaches. In light of existing challenges, we propose a novel method for self-supervised inference of agents in trustless environments, leveraging the collective intelligence of agent swarms to ensure high-quality responses. We demonstrate that Large Language Models (LLMs) can efficiently and quickly perform the response ranking necessary for swarm consensus. Additionally, our approach incorpo-"}, {"title": "Related Work", "content": "Various approaches have been proposed for trustless AI inference in decentralized environments. Each method offers unique advantages but also has significant drawbacks, particularly when applied to large artificial neural networks (ANNs).\nProof of Quality (PoQ). PoQ ensures robust and efficient deployment of generative AI models on blockchain architecture by focusing on validating model responses using lightweight, simple model assessors. However, the trade-off between guaranteed accuracy and inference latency is significant, leading to less than 70 percent accuracy for fast inference, making it less suitable for quality-demanding applications [5].\nZero-Knowledge Machine Learning (ZKML). ZKML combines zero-knowledge proofs (ZKPs) with machine learning for verifiable Al model inferences. Despite its strong guarantees, ZKML's computational overhead can be significant, limiting its practicality for large-scale applications [6].\nHalo2 ZK-SNARK Protocol. The Halo2 protocol employs Plonkish arithmetization to efficiently express DNN inference and generate zero-knowledge proofs. While it ensures high security at lower costs than ZKML, the computational expenses are still prohibitive for very large neural networks [7].\nOptimistic Machine Learning (OPML). OPML introduces a novel framework that leverages optimistic rollups for machine learning inferences on the blockchain, significantly reducing computational costs and improving efficiency. Unlike ZKML, which uses zero-knowledge proofs to achieve verifiable AI inferences, OPML relies on a fraud-proof protocol that separates execution from proving, thereby optimizing resource usage while maintaining security. However, this trade-off comes with the need for a challenge period, which may delay finality compared to instant ZKP verification [8].\nFederated Learning. Federated learning enables decentralized training by aggregating model updates instead of raw data. However, communication overhead and synchronization challenges can slow down the overall process in heterogeneous environments [9].\nBlockchain-based Model Verification. Smart contracts on blockchains can verify AI model inference by checking the consistency of model outputs. The execution of verification algorithms can be slow and resource-intensive, hindering real-time inference [10].\nEnclaves and Trusted Execution Environments (TEEs). TEEs, such as Intel SGX and ARM TrustZone, provide secure environments for AI model inference. Their limited scalability and availability overhead make them less suitable for large-scale, real-time applications [12].\nHomomorphic Encryption. Fully Homomorphic Encryption (FHE) allows computations on encrypted data, preserving privacy and validity. However, the significant performance overhead and resource requirements limit its practicality for real-time AI inference [13].\nVerifiable Computation. Interactive proofs and Probabilistically Checkable Proofs (PCPs) enable verification of computations in a decentralized manner. The complexity and resource intensity of these methods limit their practicality for large-scale neural networks [14]."}, {"title": "Self-Supervised Agents", "content": "Collaborative agent-based frameworks have demonstrated strong results in improving both the accuracy and efficiency of AI systems, particularly when compared to monolithic solutions, by distributing tasks across specialized agents [11]. Our approach leverages the collective intelligence of swarms of agents capable of both inference and ranking. These agents form dynamic networks that ensure high-quality responses while effectively mitigating the risk of malicious behavior. In this section, we describe the architecture, practical parameters, and methods for detecting and mitigating malicious attacks."}, {"title": "Agent Architecture", "content": "Each agent in the swarm performs data inference and quality ranking. Agents communicate with each other to form a consensus on the best responses, ensuring robustness and accuracy.\nThere is a valid question whether we can use the single LLM both for content generating and response ranking. Recent studies [15] answer positively to this question, moreover MetaRanking approach achieves over 80 percent accuracy with ranking complex GPT4 responses using lightweight Phi2 model[16]. Thus, each agent can easily provide reasonable ranking capabilities to the swarm using its lightweight expert LLM.\nThe proposed system employs a multi-agent architecture, where each agent is designed as a composite entity encompassing several key components:\n1. Primary Cognitive Module: This core component is responsible for both content generation and ranking tasks. It can be implemented as either:\n\u2022 A Large Language Model (LLM), or\n\u2022 An Expert System\n2. Auxiliary Processing Unit (Optional): This module augments the primary cognitive capabilities by performing:\n\u2022 Pre-processing operations on input data\n\u2022 Post-processing operations on generated content\nAuxillary Unit can connect to the external world and be represented like tools (interpreters, calculators, knowledge bases, filters) as well as third-party services (Math processors, Internet access and Search Engine, Cloud ML Providers and Co-Pilots)\nThis modular architecture allows flexibility in agent implementation while maintaining a standardized interface for inter-agent communication and system integration.\nThe primary cognitive module's dual functionality in content generation and ranking enables efficient task execution, while the optional auxiliary processing unit provides enhanced adaptability to diverse input and output requirements."}, {"title": "Swarm-based Consensus Mechanism for Optimal Response Selection", "content": "This section outlines a novel swarm-based consensus mechanism designed to coordinate agents and select the most appropriate response from a pool of generated answers. The process is divided into three main phases: response generation, selective ranking, and final selection.\nResponse Generation Phase The process initiates with the following steps:\nA client request is broadcast to the swarm. Participating agents generate and submit encrypted responses to the swarm. It's important to note that only willing agents can take part in the response generation, filtering client requests for their expertise and applicability. After a predetermined time interval, At, the responses are decrypted using keys submitted by each participating agent.\n$R_i = E(r_i, k_i), D(R_i, k_i) = r_i$ (1)\nwhere $R_i$ is the encrypted response, $r_i$ is the original response, $k_i$ is the encryption key, E is the encryption function, and D is the decryption function. This encryption-decryption mechanism mitigates potential attacks involving the copying of highly-rated nodes' responses.\nSelective Ranking Phase Following the response generation, a selective ranking process is employed:\nA subset of agents, $S_j \\subset A$, is pseudo-randomly selected to rank each agent's response, where A is the set of participating agents. The selection is based on a recent block hash, $H_b$, ensuring randomness and fairness. Each agent $a_i \\in A$ ranks responses from approximately one-third of other agents, excluding its own response.\n$|S_j = f(H_b, A \\setminus a_j), |S_j| \\approx \\frac{|A|-1}{3}$ (2)\nwhere f is the selection function based on the block hash. This selective ranking approach reduces the risk of collusion and prevents any single agent from exerting undue influence over the consensus.\nFinal Selection Phase The final selection of the optimal response proceeds as follows:\nRankings are submitted to the swarm in encrypted form to prevent copying attacks. After a decryption round, the best response is selected based on weighted rankings. The weight of each ranking is determined by the rating of the ranking agent.\n$r_{best} = arg max_{r_i} \\sum_{j \\in S_i} w_j rank(r_i, a_j)$ (3)\nwhere $w_j$ is the weight (rating) of agent $a_j$, and $rank(r_i, a_j)$ is the rank assigned to response $r_i$ by agent $a_j$.\nThe selected best response, $r_{best}$, is returned to the client.\nThis mechanism ensures a fair, decentralized, and robust selection process that leverages the collective intelligence of the swarm while mitigating potential vulnerabilities."}, {"title": "Agents Rating - Ranking Ability and Quality Estimation", "content": "Trustless systems need to be robust and stable with the highest amount of noise being introduced by malicious or lazy actors. While the smarm's ranking algorithm guarantees consensus with enough swarm size, an effective agent ranking ability and answer quality estimation enhance these mechanisms by ensuring that only the most reliable nodes participate in the consensus process. Our approach leverages the statistical properties of score deviations from the mean, assuming that the collective rankings by nodes conform to a normal distribution, a consequence of the Central Limit Theorem."}, {"title": "Methodology", "content": "Each agent in the network ranks other agents' contributions based on specific criteria. For each ranking cycle, the deviation of a node's score from the mean score is computed. The standard deviation of these score deviations across multiple cycles is used as a metric to estimate the node's ranking ability:\n$\\sigma_i = \\sqrt{\\frac{1}{N-1} \\sum_{j=1}^{N} (x_{ij} - \\bar{x_j})^2}$ (4)\nwhere $\\sigma_i$ is the standard deviation of score deviations for node i, $x_{ij}$ is the score given by node i to node j, and $\\bar{x_j}$ is the mean score received by node j.\nLimiting this metric only to the blocks with a high number of participants, we assume that the sum of scores given to each node approximates a normal distribution (the consequence of the Central Limit Theorem). This assumption allows us to use statistical methods to systematically analyze the ranking behaviors.\nNodes with lower values of $\\sigma_i$ are considered more aligned with the collective decision-making process, indicating a higher reliability in their ranking assessments or response quality. These nodes are given more influence in the consensus process, leading to a more robust and secure agent swarm.\nFinally, to estimate the node's rating (ranking ability) we can calculate the inverted $\\sigma_i$ value:\n$Rating_i = 1 - \\sigma_i$"}, {"title": "Nodes' Rating Estimation", "content": "We conducted several simulations to estimate ranking ability. In our setup, we used 10 test agents with different actual ranking abilities, and we simulated various numbers of test consensus rounds to calculate the proposed ranking metric. As we observed, even 10 simulation rounds are sufficient to provide a rough estimation of an agent's ranking ability.\nThis ranking estimation mechanism provides a statistically sound method for assessing the reliability and accuracy of agents in consensus. By integrating this system, agent swarm enhances their security and efficiency, ensuring that only the most competent nodes govern the consensus process."}, {"title": "Adversarial Agent Detection and Mitigation", "content": "We model various types of malicious attacks, including those producing random outputs (lazy agents), inconsistent quality outputs (buggy agents), and deliberate attempts by malicious agents to forge outputs for personal gain. While lazy and buggy agents' outputs are mitigated using ranking and rating systems, our approach includes mechanisms for detecting and isolating malicious agents' influence on the cluster, ensuring the integrity and reliability of the inference process."}, {"title": "Sybil Attacks", "content": "A Sybil attack poses a significant security threat in trustless networks, where a single entity creates multiple fake identities to gain disproportionate influence or control. A Self-Supervised Agent Inference approach necessitates a balanced incentive model to render Sybil attacks financially unfeasible."}, {"title": "Methodology", "content": "Each participating node in the network is tasked with completing a specific job, which involves computing LLM requests. This job completion is essential as it directly influences the consensus process by providing a measurable output that can be verified and ranked by other nodes.\nNodes that wish to participate in the solution process are required to purchase a ticket. This economic disincentive is a critical component in preventing Sybil attacks, as the cost of creating numerous fake identities becomes prohibitively high due to the required token price for each participating identity.\nThe reward mechanism in this consensus model is designed to promote the best contributions. Only the node whose solution wins the majority approval from the ranked nodes receives the major portion of the rewards, which include both the intrinsic value derived from solving the problem (e.g., transaction fees, block rewards) and part of the ticket value from nodes that offered poorer solutions. This further aligns the incentives of the nodes with the overall health and security of the network."}, {"title": "Sybil Attack Profitability Simulation", "content": "The following graph displays the profitability of Sybil attacks in an agent swarm, with varying numbers of total nodes and token deposit requirements. The x-axis represents the total number of nodes in the network, ranging from 10 to 500. The y-axis shows the range of token deposits required by each node, from 0 to 2 tokens. In this scenario, we simulate a reward of 20 tokens for the winning node.\nThe color coding indicates the financial outcome of the Sybil attack:\n\u2022 White areas represent scenarios where the attack results in negative profits, making them unprofitable or unattractive for attackers.\n\u2022 Shades of green signify varying levels of positive profits, with darker greens indicating higher profits from the attack.\nA red line on the graph marks the break-even point, where the profits from the attack shift from positive to zero. Above this line, attacks are not profitable (white area), providing a clear visual guide on setting token deposit thresholds to deter malicious activities effectively. This line is crucial to understanding how to scale security measures based on network size and potential rewards to maintain the integrity of the network.\nIn summary, the graph demonstrates that setting a ticket price as low as 1% of the potential reward is sufficient to make an attack unprofitable, even in a moderately sized agent swarm, effectively enhancing the network's resilience against Sybil attacks.\nThe selective ranking based on hash functions reduces the overhead typically associated with each node evaluating every other node's submission, which can be particularly burdensome in large networks. Furthermore, by decentralizing the ranking and not allowing nodes to rank their own solutions, the system inherently guards against self-promotion and favoritism, thus enhancing the security and integrity of the consensus process.\nThis consensus mechanism leverages economic incentives and algorithmic randomness to create a robust defense against Sybil attacks, ensuring that the blockchain maintains its decentralized, secure, and transparent nature."}, {"title": "Prompt Engineering Attacks", "content": "One of the potential vulnerabilities in the swarm of agents approach is the prompt engineering attacks. These attacks occur when malicious nodes attempt to manipulate the inference process by crafting inputs (prompts) that can exploit the ranking mechanism of other agents in the network. The goal of such an attack is to have the malicious agent's output ranked higher than it deserves, thereby influencing the overall consensus of the swarm and potentially compromising the system's integrity.\nPrompt engineering attacks can take various forms, but they generally fall into two main categories: low-frequency token attacks and common-sense prompt attacks. Both types of attacks aim to deceive or manipulate the underlying language models (LLMs) used by other agents to skew the ranking of responses in favor of the attacker."}, {"title": "Low-Frequency Token Attacks", "content": "Low-frequency token attacks involve the insertion of rare or special tokens, such as unique Unicode characters, into the generated text by the malicious agent. The premise behind this attack is that by introducing these low-frequency tokens, the malicious response might exploit biases or vulnerabilities in the language models of other agents, causing them to rank the manipulated response more favorably. This tactic could potentially work in scenarios where the language models disproportionately weight the presence of rare tokens as indicative of novel or important information.\nHowever, a key countermeasure against such attacks in our decentralized AI framework is the heterogeneity of the agents' LLMs. Since each agent in the swarm operates either a varied or uniquely tuned LLM, the effectiveness of low-frequency token attacks is significantly diminished. An insertion that may trigger a favorable bias in one model is unlikely to have the same effect across a diverse set of models. The diversity of LLM architectures, training data, and tokenization processes means that these rare tokens do not consistently influence the ranking process, thereby reducing the chances of a successful attack. This diversity acts as a natural defense mechanism, ensuring that no single token-based strategy can universally deceive the swarm."}, {"title": "Common-Sense Prompt Attacks", "content": "Common-sense prompt attacks are another form of manipulation where the malicious agent embeds statements within the prompt that are intended to influence the other agents' LLMs to rank their response higher. Examples of such manipulative statements include phrases like \u201cthis answer is the best\" or other similar assertions that aim to exploit basic common-sense reasoning or self-referential biases within the LLMs.\nEach agent in the swarm is inherently motivated to protect itself against manipulations to maximize its received incentive. Luckily such a self-referential protection mechanism is grounded in the \"common-sense\" capabilities of the agents' LLMs, and can be utilized to penalize responses that attempt to unduly influence ranking through non-substantive means."}, {"title": "Evaluation", "content": "We estimate the computational resources required for our approach, considering factors such as network latency, processing power, and data throughput. Our model shows that a swarm of agents can operate very efficiently in a decentralized environment."}, {"title": "Inference Latency Comparison", "content": "Table 1 compares the inference latencies of various decentralized AI approaches, revealing significant differences in performance. Proof of Quality (PoQ) demonstrates the lowest latency at 50 ms for MobileNet v2, though it is achieved with low accuracy guarantees (< 70%) due to validation using a simple BERT transformer [5]. In stark contrast, ZKML exhibits extremely high latency, taking over 24 hours for ResNet-50 inference, rendering it impractical for large-scale models [6]. The Halo2 ZK-SNARK protocol shows improvement but still requires 2457.5 seconds (about 41 minutes) for MobileNet v2, which remains prohibitive for very large neural networks [7]. OPML offers a significant reduction compared to ZKML, completing ResNet-50 inference in 3.6 hours, due to its challenge period [8]. Trusted Execution Environments like Intel SGX achieve relatively low latency at 230 ms for VGG-16, but struggle with scalability [12]. Homomorphic Encryption, while preserving privacy, incurs high latency at 788 seconds for SqueezeNet, making it unsuitable for real-time applications [13]. Federated Learning, Blockchain-based Verification, and Verifiable Computation approaches lack specific latency figures in the literature for direct comparison, but are generally understood to face significant overhead due to communication costs, resource-intensive operations, and computational complexity, respectively [9, 10, 14]. This comparison highlights the ongoing challenge in achieving both low-latency and secure decentralized AI inference. While some approaches like PoQ and TEEs offer promising latency figures, they come with their own limitations. The trade-off between security, efficiency, and practicality remains a key area for further research and development in decentralized AI, and Self-Supervised Agent Inference shows a promising approach to this problem."}, {"title": "Ultra-Low Latency Inference on Large Language Models", "content": "Our swarm-based consensus mechanism demonstrates remarkable efficiency when applied to large language models such as Llama 3 405B, achieving inference latencies of under 125 milliseconds. This exceptional performance is attributed to several key factors:\n\u2022 Parallel Processing: The swarm architecture allows for massively parallel response generation. All agents taking part in the round can simultaneously process the input query, effectively avoiding latency on the initial inference step.\n\u2022 Selective Ranking: By having each agent rank only a subset of responses, we drastically reduce the time required for the evaluation phase without compromising the quality of selection.\n\u2022 Rapid Ranking Process: To rank a response, only a single token needs to be inferred, which is orders of magnitude faster than generating a full response. This allows for quick and efficient quality assessment (ranking) without significantly impacting overall latency.\n\u2022 Weighted Ranking Aggregation: The final selection phase employs a computationally efficient weighted ranking system that quickly identifies the optimal response.\n\u2022 Asynchronous Operations: Many of the consensus mechanism's steps occur asynchronously, allowing for overlapping operations that further reduce overall latency.\nThis combination of parallelism, efficient cryptographic operations, intelligent agent coordination, and our consensus technique enables our system to leverage the full power of the Llama 3 405B model while maintaining inference latencies below 125 ms. The single-token ranking approach is particularly crucial, as it allows for quality assessment at a fraction of the time required for full response generation. This represents a significant advancement in decentralized AI, offering performance comparable to centralized solutions while preserving the benefits of swarm-based consensus and decentralization."}, {"title": "Conclusion", "content": "This paper presents a novel AI inference approach that leverages the collective intelligence of agent swarms in decentralized environments. By employing agents capable of data inference and response ranking, our method enables Large Language Models (LLMs) to function effectively as response classifiers while addressing critical challenges including security, reliability, and rapid response times.\nOur proposed method demonstrates significant improvements over existing trustless inference strategies in terms of speed, accuracy, and resilience to malicious attacks. By modeling various types of malicious agent behavior, we have developed and validated mechanisms to detect and mitigate such threats, ensuring the integrity of the inference process. This approach provides a scalable solution adaptable to the growing demands of AI applications."}]}