{"title": "MapTune: Advancing ASIC Technology Mapping via Reinforcement Learning Guided Library Tuning", "authors": ["Mingju Liu", "Yingjie Li", "Daniel Robinson", "Cunxi Yu"], "abstract": "Technology mapping involves mapping logical circuits to a library\nof cells. Traditionally, the full technology library is used, leading to\na large search space and potential overhead. Motivated by randomly\nsampled technology mapping case studies, we propose MapTune\nframework that addresses this challenge by utilizing reinforcement\nlearning to make design-specific choices during cell selection. By\nlearning from the environment, MapTune refines the cell selec-\ntion process, resulting in a reduced search space and potentially\nimproved mapping quality.\nThe effectiveness of MapTune is evaluated on a wide range of\nbenchmarks, different technology libraries and technology map-\npers. The experimental results demonstrate that MapTune achieves\nhigher mapping accuracy and reducing delay/area across diverse cir-\ncuit designs, technology libraries and mappers. The paper also dis-\ncusses the Pareto-Optimal exploration and confirms the perpetual\ndelay-area trade-off. Conducted on benchmark suites ISCAS 85/89,\nITC/ISCAS 99, VTR8.0 and EPFL benchmarks, the post-technology\nmapping and post-sizing quality-of-results (QoR) have been sig-\nnificantly improved, with average Area-Delay Product (ADP) im-\nprovement of 22.54% among all different exploration settings in\nMapTune. The improvements are consistently remained for four\ndifferent technologies (7nm, 45nm, 130nm, and 180 nm) and two\ndifferent mappers.", "sections": [{"title": "1 Introduction", "content": "Targeted specialization of functionality in hardware has become ar-\nguably the best means for enabling improved compute performance\nand energy efficiency. However, as the complexity of modern hard-\nware systems explodes, fast and effective hardware explorations\nare hard to achieve due to the lack of guarantee in the existing in\nelectronic design automation (EDA) toolflow. Several major lim-\nitations prevent practical hardware explorations [1-3]. First, as\nthe hardware design and technology advance, the design space of\nmodern EDA tools has increased dramatically. Besides, evaluating\na given design point is extremely time-consuming, such that only a\nvery small sub-space of the large design space can be explored. Last\nbut not least, while the initialization of design space exploration is\nimportant for the final convergence, it is difficult to initialize the\nsearch for unseen designs effectively.\nRecent years have seen increasing employment of decision in-\ntelligence in EDA, which aims to reduce the manual efforts and\nboost the design closure process in modern toolflows [1, 2, 4-12].\nFor example, various of machine learning (ML) techniques have\nbeen used to automatically configure the tool configurations of in-\ndustrial FPGA toolflow [1, 4, 5, 13-15] and ASIC toolflow [2, 7? -9].\nThese works focus on end-to-end tool parameter space exploration,\nwhich are guided by ML models trained based on either offline [2]\nor online datasets [1, 4]. Moreover, exploring the sequence of syn-\nthesis transformations (also called synthesis flow) in EDA has been\nstudied in an iterative training-exploration fashion through Convo-\nlutional Neural Networks (CNNs) [8] and reinforcement learning\n[9]. While the design quality is very sensitive to the sequence of\ntransformations [8], these approaches are able to learn a sequen-\ntial decision making strategy to achieve better quality-of-results\n[8, 9]. Moreover, [15, 16] demonstrate the effectiveness of light-\nweight Multi-Arm Bandit (MAB) models in identifying the optimal\nsynthesis flow and it achieves a balance between exploring and\nexploiting arms through multiple trials to maximize overall payoffs.\nIn addition, neural network based image classification and image\nconstruction techniques have been leveraged in placement and\nroute (PnR), in order to accelerate design closure in the physical\ndesign stage [17-23]. As the design of digital circuits continues to\ngrow in complexity, technology mapping process faces an increas-\ningly large search space due to the vast number of cells contained in\nmodern technology libraries. Utilizing the entire technology library\nas an input for technology mapping can result in excessive runtime\nand sub-optimal results, which has been demonstrated with our\ncomprehensive case studies using 7nm ASAP library [24-26].\nTo address this issue, we argue that carefully calibrated partial\ntechnology libraries that contain a subset of cells have been pro-\nposed to mitigate the search space and reduce runtime. However,\nthe selection of an optimal subset of cells requires significant ex-\npertise and experience to carefully consider the design goals, target\ntechnology, and characteristics of each cell. This process is known\nas cell selection and represents a significant challenge for EDA"}, {"title": "2 Background", "content": "Technology mapping is a critical phase in the logic synthesis pro-\ncess, converting high-level circuit descriptions, such as those at the\nRegister Transfer Level (RTL), into technology-specific gate-level\nnetlists, particularly for Application-Specific Integrated Circuits\n(ASIC) designs. It involves selecting appropriate cells from an EDA\nlibrary to realize a circuit in a chosen technology, effectively bridg-\ning high-level design with physical implementation. This step not\nonly follows logic optimization but is also essential for optimizing\nPower, Performance, and Area (PPA), significantly influencing the\ncost, performance, and manufacturability of a circuit by determin-\ning the optimal gates and their interconnections.\nVarious algorithms have been developed to address the tech-\nnology mapping problem, crucial in logic synthesis for ASIC de-\nsign. These include tree-based approaches [35, 36], which focus on\nmapping trees or sub-trees to specific gates, and Directed Acyclic\nGraph (DAG)-based methods [37-39] that consider the entire cir-\ncuit topology for enhanced optimization. Additionally, genetic al-\ngorithms [40], inspired by natural selection, use a population of\nsolutions evolved over time to find optimal or near-optimal config-\nurations for technology mapping. Recently, ML approaches have\nalso been involved in improving the technology mapping process,\neither by correlating the technology-independent representation"}, {"title": "2.1 Technology Mapping and Library", "content": "to technology-dependent PPAs (prediction models) [25, 41], or by\ndirectly optimizing the technology mapping algorithms [26]. These\ntechniques aim to optimize PPA by minimizing gate count and in-\nterconnections, reducing power consumption, and meeting timing\nconstraints. Thus, technology mapping is essential for producing\nefficient, cost-effective, and high-performance circuits in modern\nelectronic systems.\nTechnology libraries are critical for technology mapping, as they\nprovide predefined gates and components optimized for specific\nfabrication processes. Consequently, significant efforts have been\nmade to optimize or generate standard cell libraries to improve the\nPPAs in the design flow [42]. However, there has been no effort to\nanalyze the impact of these libraries on the technology mapping\nprocedure. Specifically, in this work, we observe and demonstrate\na counter-intuitive finding: a partially selected set of cells from the\nfull technology library might significantly improve the technology\nmapping PPAs. The main intuition behind this is that the complexity\nand algorithmic space of technology mapping algorithms increase\nas the number of cells in the libraries increase, while most practi-\ncal technology mappers in academic and industrial toolflows are\nheavily heuristic-based. Our comprehensive case studies in Section\n3 will first discuss the impact of specific cell selection within a\nfull library on the technology mapping PPA results using the ABC\nframework."}, {"title": "2.2 Learning-based techniques in EDA", "content": "Learning-based techniques have found widespread application across\nvarious aspects of the EDA area, including synthesis, placement and\nrouting, and design space exploration for different design stages.\nNumerous studies have achieved substantial success in addressing\nplacement problems using reinforcement learning [20, 43-45]. Ad-\nditionally, various studies such as [46], [47], [48], have explored\ndifferent learning techniques, including Convolutional Neural Net-\nworks (CNN) and Generative Adversarial Networks (GAN), during\nthe routing phase. Another notable application is Design Space\nExploration (DSE) for logic synthesis. The optimization of quality-\nof-results (QoR) in logic synthesis often requires extensive tuning\ntime, making efficient DSE a challenging task due to the expo-\nnential number of potential design permutations. In response to\nthis challenge, [44, 49] have applied GAN and CNN techniques to\nautomate design space exploration and synthesis design flow."}, {"title": "3 MapTune Case Studies", "content": "Technology mapping plays a crucial role in the logic synthesis\nprocess within the domain of EDA. An excessively large technology\nmapping library can impose significant pressure on the exploration\nof design space and make the search for optimal gate selection\nchallenging. Therefore, reducing the library size in a reasonable\nmanner presents opportunities for optimizing technology mapping.\nThus, to explore the impact of library sampling size on technology\nmapping performance, we conduct a comprehensive case study that\ninvolves random sampling from the 7nm ASAP library [24].\nIn particular, we use the complete 7nm ASAP technology library\nwith 161 cells as the baseline. This baseline approach represents the\nconventional method that is widely employed in the industry. Then,"}, {"title": "4 Approach", "content": "An overview of the MapTune framework is shown in Figure 2.\nMapTune is dedicated to addressing a cell selection problem with\na pool of N candidates, denoted as L (LibSet). Each candidate in\nL is associated with two performance metrics: Delay and Area.\nIn this work, we use the Area-Delay Product (ADP) as a single\nmetric to assess overall circuit efficiency. By definition, ADP = Delay \u00d7 Area. In each decision iteration, a subset of n candidates is\nselected from L, forming a potential solution. Each candidate i\u2208 L\ncorresponds to a binary decision variable $S_i \\in \\{0,1\\}$. A solution\nis then represented by a multi-hot encoded vector $S \\in \\{0,1\\}^N$,\nwhere $S = [S_0, S_1, ..., S_{N-1}]$ and $\\sum_{i=0}^{N-1} S_i = n$. Each distinct S\nrepresents a unique combination of candidates, each associated\nwith a specific reward.\nHere are the essential model formulation configurations:\nAction Space: The action space A consists of a discrete and\nfinite set of actions, where each action corresponds to selecting a\nspecific cell to form a subset of cells from the technology library,\nsubsequently utilized for the technology mapping of a design. The\ncardinality of this action space is equal to N, denoting the total\nnumber of unique cells in the initial technology library. Here, we\ndefine each action as $a^i: S_i = 1, i \\in [0, N \u2013 1], a^i \\in A$, where\ntaking action $a^i$ means selecting cell i from the original library.\nState: During each iteration of forming a subset of cells from\nthe original library with all candidate cells, the state S represents\nthe current data collection condition, where $S = [S_0, S_1, ..., S_{N-1}]$.\nAnd when $\\sum_{i=0}^{N-1} S_i = n$, where n is the required subset size, it\nrefers to a complete state.\nReward: The reward function corresponds to the cell selections\nmade by the agent. It is defined as $R_S = -ADP_S = -\\big|\\frac{A_S}{A_{Base}} \\times \\frac{D_S}{D_{Base}}\\big|$"}, {"title": "4.1 Formulation of MapTune", "content": ", where $D_S$ and $A_S$ are the metrics derived from the technology map-\nping of the design using the selected subset of cells indicated at\nstate S. The terms $D_{Base}$ and $A_{base}$ represent the baseline metrics,\nestablished using all the cells in the original library for technology\nmapping. In this case, $ADP_S$ is a product of normalized $D_S$ and\n$A_S$. The negative function is employed to invert the metric, encour-\naging maximizing the reward by minimizing ADP, thus optimizing\nboth metrics concurrently.\nFormally, we define the probability vector as $p = [p_{a^0}, p_{a^1},...,\np_{a^{N-1}}]$. The probability $p_{a^i}$ is defined as the likelihood of\nselecting cell i for the sampled library can maximize the\nreward. These vectors are updated at each decision epoch based\non the observed performance metric, i.e., ADP. The objective is to\niteratively refine p such that the probability of selecting candidates\nthat lead to minimizing ADP is maximized."}, {"title": "4.2 Implementation", "content": "In the realm of Reinforcement Learning, we choose two direc-\ntions to formulate the MapTune Framework: Multi-Armed Bandit\n(MapTune-MAB) and Q-Learning (MapTune-Q). More specifically,\nour prominent MapTune-MAB algorithms are with e-greedy strat-\negy [51] and Upper Confidence Bound strategy [52], denoted as\nMapTune-e and MapTune-UCB, respectively. For MapTune-Q meth-\nods, we implement and compare both the Deep Q-Network (DQN)\n[53] and the Double Deep Q-Network (DDQN) [54]."}, {"title": "4.2.1 MapTune-MAB", "content": "As for the bandit problem settings, we refer\nto each cell i \u2208 L in the library as an arm i, during each iteration,\nif an action $a^i$ is taken, this means arm i is selected for this action.\nMapTune-e Agent. The MapTune-e agent utilizes e-greedy to\nbalance exploration and exploitation via a parameter \u0454 \u2208 [0, 1].\nThis parameter dictates the probability of random action selection\n(exploration) vs. choosing the action with the highest probability\nleads to higher reward based on historical data (exploitation). For--\nmally, the agent selects action $a^i$ \u2208 A according to the following\nrule:\n$a = \\begin{cases}\n    arg \\underset{a^i \\in A}{max} p_{a^i} & \\text{with parameter } 1 \u2013 \u0454\\\\\n    \\text{a random selection } a^i \\in A & \\text{with parameter } e\n\\end{cases}$\nMapTune-UCB Agent. The MapTune-UCB agent integrates a\nconfidence interval around the reward estimates based on historical"}, {"title": "4.2.2 MapTune-Q", "content": "trial data to tackle the similar exploration-exploitation problem\neffectively. Action selection is governed by the following formula:\n$a = arg \\underset{a^i \\in A}{max} \\big(p_{a^i} + c\\sqrt{\\frac{log(t)}{n_{a^i}}}\\big)$(1)\nwhere t is the current iteration, $n_{a^i}$ is the number of times that\naction $a^i$ is taken during t iterations, c is the coefficient that mod-\nulates the extent of exploration.\nNote that, for both MapTune-e and MapTune-UCB agents, the\nprobability vector p are updated as following:\n$p_{a^i}(t + 1) = \\frac{p_{a^i}(t)n_{a^i}(t) + R_{S}(t)}{n_{a^i}(t)}$(2)\nwhere $p_{a^i}(t)$ is the probability of action $a^i$ at iteration t, $n_{a^i}(t)$ is\nthe number of times that action $a^i$ is taken during t iterations. In\nthis context, the probability of taking action $a^i$ leads to minimizing\nADP is an average of the obtained reward during the data trial\nprocess. Note that, we ensure every time when updating $p_{a^i}$ for\nthe next iteration (t + 1) at state S(t), a required number of arms\nhas been selected, so that the reward $R_S$ is influencing a subset of\narms during each iteration.\nFollowing the same action space, state and re-\nward setup, we can implement MapTune-Q Agent similarly. Instead\nof influencing the probability of action decision directly from the\nobserved reward, our prominent MapTune-Q methods facilitate the\nDQN to approximate the optimal state-action value function with\nthe associated reward. Both MapTune-DQN Agent and MapTune-\nDDQN Agent are implemented as following by adapting the same\nenvironment settings:\nMapTune-DQN Agent.Given a state vector S and action $a_i$ as\nthe input of the DQN, the model will predict a Q-value $Q(S, a^i)$. To\nkeep the consistency, we use the same probability $p_{a^i} = Q(S, a^i)$ to\nrefer to the predicted Q-value of the taken action $a^i$. The model also\nderives a target Q-value, denoted as $p_{a^i}^{tar}$ through Bellman equation\nas follows:\n$p_{a^i}^{tar} = R + \u03b3 \\underset{a^i}{max} p_{a^i}$(3)\nwhere R is the current reward introduced by taking action $a^i$, and\n\u03b3 is the discount factor to emphasize the significance of the future"}, {"title": "5 Results", "content": "We demonstrate the proposed approach on designs from five bench-\nmark suites: ISCAS 85, ISCAS 89, ITC/ISCAS 99, VTR8.0 and EPFL\nbenchmarks, to evaluate the performance of MapTune after technol-\nogy mapping and gate sizing are performed. Specifically, MapTune\nexplores the technology libraries and evaluates it by mapping on\nthe library using ABC with the same command as in Section 3.\nNote that, by default, the ABC built-in map command is a Delay-\ndriven mapper. To provide more robustness to this work, we also\nuse map -a command in ABC which is an Area-driven mapper by"}, {"title": "5.1 Technology Mapping Results", "content": "Due to the variations of the proposed approaches, we structure\nthe experimental results to answer the three following research\nquestions (RQ):\nRQ1: How effective is MapTune in optimizing ADP?\nMapTune shows a stable convergence rate regardless of meth-\nods/designs. In Section 4, we employ Normalized ADP as a single\nmetric to guide our MapTune framework. Here, we compare the\nnormalized ADP optimization trends across selected designs and\nmethods. Due to page limits, we choose nine representative designs\nmapped on ASAP7 library with Delay-driven mapper, as depicted\nin Figure 3. We focus on the first 1200 seconds time span to empha-\nsize the rapid convergence rates of the different methods within\nMapTune.\nAs illustrated in Figure 3, across nine selected designs, various\nMapTune methods are able to converge to a lower achivable ADP\nwithin the given optimization time span. Take design bar in Figure\n3b as an example, all four MapTune methods can achieve at least\n~15% ADP reduction within 300 seconds, while MapTune-UCB can\nobtain an over 20% ADP reduction in the first 15 seconds which is\nsignificantly rapid.\nDespite the rapid convergence feature among various methods\nand their variational settings, we do notice MapTune-MAB methods\nshow a slight superiority than MapTune-Q methods in terms of\ngeneral convergence rates and lowest achievable ADP. For instance,\nfor design c880 as shown in Figure 3e, MapTune-e converges to\nthe lowest ADP within 160 seconds, while MapTune-DDQN even-\ntually achieves a ~5% higher ADP despite a similar convergence\ntime. MapTune-DQN converges to a similar ADP as of MapTune-e\nhowever suffers more than 5\u00d7 convergence time.\nRQ2: Are MapTune adaptive to different technologies?\nMapTune is effective regardless of various technology li-\nbraries. In Figure 4, we showcase the final converged ADP for\neight selected designs on four libraries tuned by MapTune. Note\nthat, in this figure, we choose the one with the best final converged"}, {"title": "5.2 Pareto-Optimal Exploration", "content": "While we have confirmed that MapTune is able to identify design\npoint that significantly improves the quality-of-results (QoR), we\nwant to see whether MapTune is able to identify Pareto frontier\nof technology mapping. One of the key aspects in this domain is\nthe trade-off between delay and area, two primary metrics that\ndictate the efficiency and compactness of a given solution. With the\nresults at hand, the focus lies on discussing how the algorithm has\nmanaged to identify a new frontier that is superior to the baseline\nand confirming the perpetual trade-off between delay and area.\nWe present the exact Delay and Area results in Table 2 and\nTable 3 from the Delay-driven mapper, and Area-driven mapper,\nrespectively. For space constraints, we showcase results from 20\nselected designs across the entire benchmark suites. The final map-\nping results for each method were obtained at the one-hour timeout,"}, {"title": "6 Conclusions", "content": "This paper explores the use of partially sampled technology libraries\nto reduce the search space for better QoR in technology mapping.\nOur case study empirically demonstrates the importance of this\nprocess, given its potential impact on the trade-off between area\nand delay, as well as its capability to reveal new performance Pareto\nfrontiers. In response to this challenge, we introduce MapTune, a\nnovel sampling framework based on Reinforcement Learning by\nleveraging both MAB and Q-Learning and seamlessly integrated\nwithin the ABC framework. Extensive evaluations using five dis-\ntinct benchmark suites confirmed the effectiveness of MapTune\nframework for technology library tuning. By solely focusing on\nlibrary optimization, MapTune is able to achieve an average ADP\nimprovement of 22.54% and identify pareto-optimal results. Fu-\nture work will concentrate on cross-design library exploration and\nintegration with automatic library generation tools."}]}