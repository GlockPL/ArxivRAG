{"title": "MapTune: Advancing ASIC Technology Mapping via Reinforcement Learning Guided Library Tuning", "authors": ["Mingju Liu", "Yingjie Li", "Daniel Robinson", "Cunxi Yu"], "abstract": "Technology mapping involves mapping logical circuits to a library of cells. Traditionally, the full technology library is used, leading to a large search space and potential overhead. Motivated by randomly sampled technology mapping case studies, we propose MapTune framework that addresses this challenge by utilizing reinforcement learning to make design-specific choices during cell selection. By learning from the environment, MapTune refines the cell selection process, resulting in a reduced search space and potentially improved mapping quality.\nThe effectiveness of MapTune is evaluated on a wide range of benchmarks, different technology libraries and technology mappers. The experimental results demonstrate that MapTune achieves higher mapping accuracy and reducing delay/area across diverse circuit designs, technology libraries and mappers. The paper also discusses the Pareto-Optimal exploration and confirms the perpetual delay-area trade-off. Conducted on benchmark suites ISCAS 85/89, ITC/ISCAS 99, VTR8.0 and EPFL benchmarks, the post-technology mapping and post-sizing quality-of-results (QoR) have been significantly improved, with average Area-Delay Product (ADP) improvement of 22.54% among all different exploration settings in MapTune. The improvements are consistently remained for four different technologies (7nm, 45nm, 130nm, and 180 nm) and two different mappers.", "sections": [{"title": "Introduction", "content": "Targeted specialization of functionality in hardware has become arguably the best means for enabling improved compute performance and energy efficiency. However, as the complexity of modern hardware systems explodes, fast and effective hardware explorations are hard to achieve due to the lack of guarantee in the existing in electronic design automation (EDA) toolflow. Several major limitations prevent practical hardware explorations [1-3]. First, as the hardware design and technology advance, the design space of modern EDA tools has increased dramatically. Besides, evaluating a given design point is extremely time-consuming, such that only a very small sub-space of the large design space can be explored. Last but not least, while the initialization of design space exploration is important for the final convergence, it is difficult to initialize the search for unseen designs effectively.\nRecent years have seen increasing employment of decision intelligence in EDA, which aims to reduce the manual efforts and boost the design closure process in modern toolflows [1, 2, 4-12]. For example, various of machine learning (ML) techniques have been used to automatically configure the tool configurations of industrial FPGA toolflow [1, 4, 5, 13-15] and ASIC toolflow [2, 7? -9]. These works focus on end-to-end tool parameter space exploration, which are guided by ML models trained based on either offline [2] or online datasets [1, 4]. Moreover, exploring the sequence of synthesis transformations (also called synthesis flow) in EDA has been studied in an iterative training-exploration fashion through Convolutional Neural Networks (CNNs) [8] and reinforcement learning [9]. While the design quality is very sensitive to the sequence of transformations [8], these approaches are able to learn a sequential decision making strategy to achieve better quality-of-results [8, 9]. Moreover, [15, 16] demonstrate the effectiveness of lightweight Multi-Arm Bandit (MAB) models in identifying the optimal synthesis flow and it achieves a balance between exploring and exploiting arms through multiple trials to maximize overall payoffs.\nIn addition, neural network based image classification and image construction techniques have been leveraged in placement and route (PnR), in order to accelerate design closure in the physical design stage [17-23]. As the design of digital circuits continues to grow in complexity, technology mapping process faces an increasingly large search space due to the vast number of cells contained in modern technology libraries. Utilizing the entire technology library as an input for technology mapping can result in excessive runtime and sub-optimal results, which has been demonstrated with our comprehensive case studies using 7nm ASAP library [24-26].\nTo address this issue, we argue that carefully calibrated partial technology libraries that contain a subset of cells have been proposed to mitigate the search space and reduce runtime. However, the selection of an optimal subset of cells requires significant expertise and experience to carefully consider the design goals, target technology, and characteristics of each cell. This process is known as cell selection and represents a significant challenge for EDA"}, {"title": "Background", "content": "Technology mapping is a critical phase in the logic synthesis process, converting high-level circuit descriptions, such as those at the Register Transfer Level (RTL), into technology-specific gate-level netlists, particularly for Application-Specific Integrated Circuits (ASIC) designs. It involves selecting appropriate cells from an EDA library to realize a circuit in a chosen technology, effectively bridging high-level design with physical implementation. This step not only follows logic optimization but is also essential for optimizing Power, Performance, and Area (PPA), significantly influencing the cost, performance, and manufacturability of a circuit by determining the optimal gates and their interconnections.\nVarious algorithms have been developed to address the technology mapping problem, crucial in logic synthesis for ASIC design. These include tree-based approaches [35, 36], which focus on mapping trees or sub-trees to specific gates, and Directed Acyclic Graph (DAG)-based methods [37-39] that consider the entire circuit topology for enhanced optimization. Additionally, genetic algorithms [40], inspired by natural selection, use a population of solutions evolved over time to find optimal or near-optimal configurations for technology mapping. Recently, ML approaches have also been involved in improving the technology mapping process, either by correlating the technology-independent representation to technology-dependent PPAs (prediction models) [25, 41], or by directly optimizing the technology mapping algorithms [26]. These techniques aim to optimize PPA by minimizing gate count and interconnections, reducing power consumption, and meeting timing constraints. Thus, technology mapping is essential for producing efficient, cost-effective, and high-performance circuits in modern electronic systems.\nTechnology libraries are critical for technology mapping, as they provide predefined gates and components optimized for specific fabrication processes. Consequently, significant efforts have been made to optimize or generate standard cell libraries to improve the PPAs in the design flow [42]. However, there has been no effort to analyze the impact of these libraries on the technology mapping procedure. Specifically, in this work, we observe and demonstrate a counter-intuitive finding: a partially selected set of cells from the full technology library might significantly improve the technology mapping PPAs. The main intuition behind this is that the complexity and algorithmic space of technology mapping algorithms increase as the number of cells in the libraries increase, while most practical technology mappers in academic and industrial toolflows are heavily heuristic-based. Our comprehensive case studies in Section 3 will first discuss the impact of specific cell selection within a full library on the technology mapping PPA results using the ABC framework."}, {"title": "Learning-based techniques in EDA", "content": "Learning-based techniques have found widespread application across various aspects of the EDA area, including synthesis, placement and routing, and design space exploration for different design stages. Numerous studies have achieved substantial success in addressing placement problems using reinforcement learning [20, 43-45]. Additionally, various studies such as [46], [47], [48], have explored different learning techniques, including Convolutional Neural Networks (CNN) and Generative Adversarial Networks (GAN), during the routing phase. Another notable application is Design Space Exploration (DSE) for logic synthesis. The optimization of quality-of-results (QoR) in logic synthesis often requires extensive tuning runtime, making efficient DSE a challenging task due to the exponential number of potential design permutations. In response to this challenge, [44, 49] have applied GAN and CNN techniques to automate design space exploration and synthesis design flow."}, {"title": "MapTune Case Studies", "content": "Technology mapping plays a crucial role in the logic synthesis process within the domain of EDA. An excessively large technology mapping library can impose significant pressure on the exploration of design space and make the search for optimal gate selection challenging. Therefore, reducing the library size in a reasonable manner presents opportunities for optimizing technology mapping.\nThus, to explore the impact of library sampling size on technology mapping performance, we conduct a comprehensive case study that involves random sampling from the 7nm ASAP library [24].\nIn particular, we use the complete 7nm ASAP technology library with 161 cells as the baseline. This baseline approach represents the conventional method that is widely employed in the industry. Then,"}, {"title": "Approach", "content": "An overview of the MapTune framework is shown in Figure 2. MapTune is dedicated to addressing a cell selection problem with a pool of N candidates, denoted as L (LibSet). Each candidate in L is associated with two performance metrics: Delay and Area. In this work, we use the Area-Delay Product (ADP) as a single metric to assess overall circuit efficiency. By definition, ADP = Delay \u00d7 Area. In each decision iteration, a subset of n candidates is selected from L, forming a potential solution. Each candidate i\u2208 L corresponds to a binary decision variable $S_{i} \\in \\{0,1\\}$. A solution is then represented by a multi-hot encoded vector S \u2208 {0,1}$^{N}$, where S = [S0, S1, ..., SN-1] and $\\sum_{i=0}^{N-1} S_{i} = n$. Each distinct S represents a unique combination of candidates, each associated with a specific reward.\nHere are the essential model formulation configurations:\nAction Space: The action space A consists of a discrete and finite set of actions, where each action corresponds to selecting a specific cell to form a subset of cells from the technology library, subsequently utilized for the technology mapping of a design. The cardinality of this action space is equal to N, denoting the total number of unique cells in the initial technology library. Here, we define each action as $a^{i} : S_{i} = 1, i \\in [0, N \u2013 1], a^{i} \\in A$, where taking action $a^{i}$ means selecting cell i from the original library.\nState: During each iteration of forming a subset of cells from the original library with all candidate cells, the state S represents the current data collection condition, where S = [S0, S1, ..., SN-1]. And when $\\sum_{i=0}^{N-1} S_{i} = n$, where n is the required subset size, it refers to a complete state.\nReward: The reward function corresponds to the cell selections made by the agent. It is defined as $R_{S} = -ADP_{S} = -\\frac{D_{S}}{D_{Base}} \\times \\frac{A_{S}}{A_{Base}}$ where Ds and As are the metrics derived from the technology mapping of the design using the selected subset of cells indicated at state S. The terms $D_{Base}$ and $A_{base}$ represent the baseline metrics, established using all the cells in the original library for technology mapping. In this case, ADPs is a product of normalized Ds and As. The negative function is employed to invert the metric, encouraging maximizing the reward by minimizing ADP, thus optimizing both metrics concurrently.\nFormally, we define the probability vector as p = [$P_{a^{0}}$, $P_{a^{1}}$,..., $P_{a^{N-1}}$]. The probability $P_{a^{i}}$ is defined as the likelihood of selecting cell i for the sampled library can maximize the reward. These vectors are updated at each decision epoch based on the observed performance metric, i.e., ADP. The objective is to iteratively refine p such that the probability of selecting candidates that lead to minimizing ADP is maximized."}, {"title": "Implementation", "content": "In the realm of Reinforcement Learning, we choose two directions to formulate the MapTune Framework: Multi-Armed Bandit (MapTune-MAB) and Q-Learning (MapTune-Q). More specifically, our prominent MapTune-MAB algorithms are with e-greedy strategy [51] and Upper Confidence Bound strategy [52], denoted as MapTune-e and MapTune-UCB, respectively. For MapTune-Q methods, we implement and compare both the Deep Q-Network (DQN) [53] and the Double Deep Q-Network (DDQN) [54].\nAs for the bandit problem settings, we refer to each cell $i \\in L$ in the library as an arm i, during each iteration, if an action $a^{i}$ is taken, this means arm i is selected for this action.\nThe MapTune-e agent utilizes e-greedy to balance exploration and exploitation via a parameter \u0454 \u2208 [0, 1]. This parameter dictates the probability of random action selection (exploration) vs. choosing the action with the highest probability leads to higher reward based on historical data (exploitation). Formally, the agent selects action $a^{i} \\in A$ according to the following rule:\n$a = \\begin{cases} arg \\underset{a^{i} \\in A} {max}  P_{a^{i}} & \\text{with parameter 1 \u2013 \u0454} \\\\a \\text{ random selection } a^{i} \\in A & \\text{with parameter e} \\end{cases}$\nThe MapTune-UCB agent integrates a confidence interval around the reward estimates based on historical trial data to tackle the similar exploration-exploitation problem effectively. Action selection is governed by the following formula:\n$a = arg \\underset{a^{i} \\in A} {max}(P_{a^{i}} + c \\sqrt{\\frac{log(t)}{n_{a^{i}}}})$ (1)\nwhere t is the current iteration, $n_{a^{i}}$ is the number of times that action $a^{i}$ is taken during t iterations, c is the coefficient that modulates the extent of exploration.\nNote that, for both MapTune-e and MapTune-UCB agents, the probability vector p are updated as following:\n$P_{a^{i}} (t + 1) = \\frac{P_{a^{i}} (t)n_{a^{i}} (t) + R_{S}(t)}{n_{a^{i}}(t)}$ (2)\nwhere $P_{a^{i}}(t)$ is the probability of action $a^{i}$ at iteration t, $n_{a^{i}}(t)$ is the number of times that action $a^{i}$ is taken during t iterations. In this context, the probability of taking action $a^{i}$ leads to minimizing ADP is an average of the obtained reward during the data trial process. Note that, we ensure every time when updating $P_{a^{i}}$ for the next iteration (t + 1) at state S(t), a required number of arms has been selected, so that the reward $R_{S}$ is influencing a subset of arms during each iteration.\nFollowing the same action space, state and reward setup, we can implement MapTune-Q Agent similarly. Instead of influencing the probability of action decision directly from the observed reward, our prominent MapTune-Q methods facilitate the DQN to approximate the optimal state-action value function with the associated reward. Both MapTune-DQN Agent and MapTune-DDQN Agent are implemented as following by adapting the same environment settings:\nGiven a state vector S and action a\u00a1 as the input of the DQN, the model will predict a Q-value Q(S, $a^{i}$). To keep the consistency, we use the same probability $P_{a^{i}} = Q(S, a^{i})$ to refer to the predicted Q-value of the taken action a\u00b9. The model also derives a target Q-value, denoted as $P_{a^{i}}^{tar}$ through Bellman equation as follows:\n$P_{a^{i}}^{tar} = R + \\gamma max \\ P_{a^{i}}$ (3)\nwhere R is the current reward introduced by taking action $a^{i}$, and \u03b3 is the discount factor to emphasize the significance of the future rewards. Note that, only after a certain number of actions have been taken reaches the required selection size, the R will be reflected as the actual -ADP as aforementioned.\nDuring each iteration, we use the DQN by parameter 0 to calculate the Q-function and choose the action with the highest probability (i.e., $P_{a^{i}}$) leads to maximizing the reward. For the trainable parameters of DQN, we use Mean Squared Error (MSE) as the loss function to calculate between the predicted Q-value ($P_{a^{i}}$) and target Q-value ($P_{a^{i}}^{tar}$), then through backward propagation, the network parameters are updated hence affecting further probabilities of chosen actions. It can defined as follows:\n$L_{e} = MSE(P_{a^{i}}, P_{a^{i}}^{tar})$ (4)\nTo mitigate the overestimation bias in the DQN due to the maximization step in the Bellman Equation 3, we use DDQN to optimize this process. DDQN incorporates additional network, the target DQN ($Q_{target}$), which mirrors the online DQN ($Q_{online}$) in terms of number and configurations of layers but with only periodically updated weights. When taking a state S and action $a^{i}$, both $Q_{target}$ and $Q_{online}$ will produce a Q-value denoted as, $P_{a^{i}}$ = $Q_{online}$(S, $a^{i}$) and $P^{'}_{a^{i}} = Q_{target}$(S, arg $\\underset{a^{i}} {max} P_{a^{i}}$), respectively. Consequently, we calculate the target Q-value $P_{a^{i}}^{tar}$ as following:\n$P_{a^{i}}^{tar} = R + \\gamma P^{'}_{a^{i}}$ (5)\nWhile $Q_{online}$ update the parameters using the same settings as in MapTune-DQN Agent, the weight update of $Q_{target}$ is under a soft update rule as follows:\n$O_{target} \\leftarrow \\tau O_{online} + (1 - \\tau) O_{target}$ (6)\nwhere \u03c4 is a small coefficient that controls the rate of the update hence stabilizing the learning process."}, {"title": "Results", "content": "We demonstrate the proposed approach on designs from five benchmark suites: ISCAS 85, ISCAS 89, ITC/ISCAS 99, VTR8.0 and EPFL benchmarks, to evaluate the performance of MapTune after technology mapping and gate sizing are performed. Specifically, MapTune explores the technology libraries and evaluates it by mapping on the library using ABC with the same command as in Section 3. Note that, by default, the ABC built-in map command is a Delay-driven mapper. To provide more robustness to this work, we also use map -a command in ABC which is an Area-driven mapper by default for experiments. All experiments are conducted with an Intel\u00ae Xeon\u00ae Gold 6418H CPU and NVIDIA RTXTM A6000 GPU. We evaluate the mapped results with different sampling sizes in MapTune, which searches for the best achievable results. Through the observed results, we evaluate the design space exploration using MapTune, which aims to search for Pareto Frontier in area-delay trade-offs. All experiments are conducted with the 7nm ASAP library [24], FreePDK45 45nm libary [32], SKYWATER 130nm library [33], GlobalFoundries 180nm MCU library [34]. For simplicity, we will refer to them as ASAP7, NAN45, SKY130, and GF180 respectively.\nAll results presented in this section are conducted with MapTune framework given a one-hour timeout constraint. All MapTune-MAB and MapTune-Q methods are implemented with a batch size of 10. Following the approach used in the motivating case studies, MapTune is evaluated by fixing its sampling size range over the same technology library among different optimization methods. To ensure a fair comparison, sampling sizes are set as follows: 1) 45 - 135 cells; 2) 35 - 75 cells; 3) 220 - 310 cells; and 4) 40 - 130 cells for ASAP7, NAN45, SKY130, and GF180 library, respectively, with a step size of 10 within the sampling range."}, {"title": "Technology Mapping Results", "content": "Due to the variations of the proposed approaches, we structure the experimental results to answer the three following research questions (RQ):\nMapTune shows a stable convergence rate regardless of methods/designs. In Section 4, we employ Normalized ADP as a single metric to guide our MapTune framework. Here, we compare the normalized ADP optimization trends across selected designs and methods. Due to page limits, we choose nine representative designs mapped on ASAP7 library with Delay-driven mapper, as depicted in Figure 3. We focus on the first 1200 seconds time span to emphasize the rapid convergence rates of the different methods within MapTune.\nAs illustrated in Figure 3, across nine selected designs, various MapTune methods are able to converge to a lower achivable ADP within the given optimization time span. Take design bar in Figure 3b as an example, all four MapTune methods can achieve at least ~15% ADP reduction within 300 seconds, while MapTune-UCB can obtain an over 20% ADP reduction in the first 15 seconds which is significantly rapid.\nDespite the rapid convergence feature among various methods and their variational settings, we do notice MapTune-MAB methods show a slight superiority than MapTune-Q methods in terms of general convergence rates and lowest achievable ADP. For instance, for design c880 as shown in Figure 3e, MapTune-e converges to the lowest ADP within 160 seconds, while MapTune-DDQN eventually achieves a ~5% higher ADP despite a similar convergence time. MapTune-DQN converges to a similar ADP as of MapTune-e however suffers more than 5\u00d7 convergence time."}, {"title": "Are MapTune adaptive to different technologies?", "content": "MapTune is effective regardless of various technology libraries. In Figure 4, we showcase the final converged ADP for eight selected designs on four libraries tuned by MapTune. Note that, in this figure, we choose the one with the best final converged ADP within MapTune-MAB and MapTune-Q methods respectively, denoted as Best MAB and Best QL.\nAs shown in Figure 4, for the same design mapped on different technologies, MapTune achieves at least a 4% ADP reduction (as shown in Figure 4a, design b14 mapped on GF180 tuned by MapTune-MAB with Delay-driven mapper) over the baseline (i.e., all cells retained in the original library, depicted by the red dashed line). More specifically, for design s838a mapped on various technology libraries tuned by MapTune, an average of 36% ADP reduction can be obtained. This highlights effectiveness of MapTune on diverse technology libraries."}, {"title": "How effective is MapTune regarding different technology mappers?", "content": "MapTune is effective regardless of different technology mappers. Besides various technology libraries are compared in Figure 4, both ABC built-in Delay-driven and Area-driven mappers are also evaluated for MapTune framework. For the eight selected designs with both mappers on different technology libraries tuned by MapTune, the final converged ADP are all brought to a lower level comparing to the baseline. For example, for design s35932 shown in Figure 4f, MapTune achives an average of 40.12% ADP reduction with Delay-driven mapper while an average of 40.00% ADP reduction with Area-driven mapper highlighting that MapTune is effective regardless of technology mappers."}, {"title": "Pareto-Optimal Exploration", "content": "While we have confirmed that MapTune is able to identify design point that significantly improves the quality-of-results (QoR), we want to see whether MapTune is able to identify Pareto frontier of technology mapping. One of the key aspects in this domain is the trade-off between delay and area, two primary metrics that dictate the efficiency and compactness of a given solution. With the results at hand, the focus lies on discussing how the algorithm has managed to identify a new frontier that is superior to the baseline and confirming the perpetual trade-off between delay and area.\nWe present the exact Delay and Area results in Table 2 and Table 3 from the Delay-driven mapper, and Area-driven mapper, respectively. For space constraints, we showcase results from 20 selected designs across the entire benchmark suites. The final mapping results for each method were obtained at the one-hour timeout, corresponding to the best achieved ADP for each MapTune-MAB and MapTune-Q method.\nAnalyzing the tables reveals that MapTune emphasize on delay optimization with both ABC technology mappers. Across designs, libraries, and methods, we observe an average delay reduction of 21.77%, while area reductions average only 0.79% for Delay-driven mapper. Similarly, for Area-driven mapper, the average delay reduction is 22.30%, but there exists an area penalty of 0.5% on average. In fact, delay tends to benefit more from the tuned technology libraries optimized by MapTune with modest area trade-offs. This is evident in cases such as the design multiplier mapped with SKY130 library tuned by MapTune-MAB which achieves a substantial 39.96% delay improvement with a 4.03% area penalty as shown in Table 2. There still exists opposite cases that indicating trade-off delay for area optimization. As for design sqrt mapped on GF180 library tuned by MapTune-MAB with Area-driven Mapper shown in Table 3, MapTune introduces 1.25% delay penalty resulting a 14.90% area reduction ultimately. Such Pareto-Optimal trade-offs are often desirable, as significant delay/area reductions can potentially benefits more throughout the design flow than the introduced modest area/delay penalty."}, {"title": "Conclusions", "content": "This paper explores the use of partially sampled technology libraries to reduce the search space for better QoR in technology mapping. Our case study empirically demonstrates the importance of this process, given its potential impact on the trade-off between area and delay, as well as its capability to reveal new performance Pareto frontiers. In response to this challenge, we introduce MapTune, a novel sampling framework based on Reinforcement Learning by leveraging both MAB and Q-Learning and seamlessly integrated within the ABC framework. Extensive evaluations using five distinct benchmark suites confirmed the effectiveness of MapTune framework for technology library tuning. By solely focusing on library optimization, MapTune is able to achieve an average ADP improvement of 22.54% and identify pareto-optimal results. Future work will concentrate on cross-design library exploration and integration with automatic library generation tools."}]}