{"title": "Uncertainty Calibration with Energy Based Instance-wise Scaling in the Wild Dataset", "authors": ["Mijoo Kim", "Junseok Kwon"], "abstract": "With the rapid advancement in the performance of deep neu-ral networks (DNNs), there has been significant interest in deployingand incorporating artificial intelligence (AI) systems into real-world sce-narios. However, many DNNs lack the ability to represent uncertainty,often exhibiting excessive confidence even when making incorrect pre-dictions. To ensure the reliability of AI systems, particularly in safety-critical cases, DNNs should transparently reflect the uncertainty in theirpredictions. In this paper, we investigate robust post-hoc uncertaintycalibration methods for DNNs within the context of multi-class classifi-cation tasks. While previous studies have made notable progress, theystill face challenges in achieving robust calibration, particularly in sce-narios involving out-of-distribution (OOD). We identify that previousmethods lack adaptability to individual input data and struggle to accu-rately estimate uncertainty when processing inputs drawn from the wilddataset. To address this issue, we introduce a novel instance-wise calibra-tion method based on an energy model. Our method incorporates energyscores instead of softmax confidence scores, allowing for adaptive consid-eration of DNN uncertainty for each prediction within a logit space. Inexperiments, we show that the proposed method consistently maintainsrobust performance across the spectrum, spanning from in-distributionto OOD scenarios, when compared to other state-of-the-art methods.", "sections": [{"title": "1 Introduction", "content": "Despite the impressive performance demonstrated by recent AI systems, their de-ployment should be carefully considered, particularly in safety critical situations(e.g. autonomous driving, finance, health care, and medical diagnosis), becausethese systems cannot consistently ensure accurate predictions. For example, inthe field of medical diagnosis, incorrect predictions have the potential to resultin catastrophic outcomes. To address this concern, the system must transpar-ently reveal the uncertainty associated with its prediction. Recently, DNNs relyon confidence as a way of expressing uncertainty, but they tend to assign higher"}, {"title": "2 Related work", "content": ""}, {"title": "2.1 Post-hoc Calibration", "content": "Confidence calibration can be divided into two categories. The first category isknown as training-time calibration, such as focal loss andlabel smoothing. These methods train DNNs to exhibit calibrated behaviorduring training. The second category is referred to as post-hoc calibration. In this approach, pre-trained neural networks are utilized along withhold-out validation datasets to learn calibration mappings in a post-hoc manner.Post-hoc calibration can be further categorized into non-parametric andparametric approaches. Non-parametric approaches include Histogram Binning(HB) and Isotonic Regression (IR). HB divided predicted probabili-ties into multiple intervals, each associated with representative confidences. IRutilized isotonic regression with uncalibrated confidences as the x-axis and theexpected accuracy values as the y-axis. BBQ, non-parametric extension ofHB, incorporated Bayesian model averaging to enhance calibration. On the otherhand, the most common parametric calibration method is Temperature Scaling(TS). As the temperature increases, the distribution of logits becomes moreuniformly distributed, resulting in a decrease in the confidence score associatedwith the predicted label. TS has a significant advantage in terms of accuracypreservation, as it maintains the originally predicted label with the highest con-fidence score unchanged. However, TS exhibits limited expressiveness as it relieson only a single parameter that is fixed on the validation set."}, {"title": "2.2 Beyond In-distribution Calibration", "content": "Conventional research has mainly focused on investigating post-hoc uncertaintycalibration methods. However, these methods often overlook scenarios involvingdistribution shifts. Due to their dependence on a fixed calibration map optimizedfor ID validation sets, they struggle to effectively handle unknown test samples.Recently, the importance of the robustness of post-hoc calibration methodacross various distribution shift scenarios has been emphasized. In,various degrees of Gaussian perturbations were injected into the ID validationdataset. The parameters of the calibration method were then adjusted usingthe perturbed validation data, resulting in enhanced robustness against shifteddistributions. However, this method tends to achieve notable performance onlywithin scenarios where a certain degree of distribution shift is present. Moreover,in a complete ID scenario, it exhibits even worse calibration compared to thepre-calibration state. To solve this problem, DAC has been proposed as apre-processing step before employing the existing post-hoc calibration methods.Notably, it leveraged additional output information for uncertainty estimationand enhanced the calibration performance in distribution shift scenarios.Similar to these methods, we focus on a broad range of scenarios, rangingfrom ID to various OOD scenarios. However, unlike these methods, our approachcan facilitate robust calibration without requiring any additional DNN's layerinformation except for the last layer. In experiments, we demonstrate that ourproposed method performs comparably to, and in some cases even surpasses,other state-of-the-art methods that utilize DAC as a preprocessing step."}, {"title": "3 Problem Setup", "content": "In this section, we establish preliminaries for uncertainty calibration. We de-fine key notations in the context of multi-class classification and present a rep-resentative calibration metric derived from the concept of perfect calibration.Additionally, we address calibration in OOD scenarios."}, {"title": "3.1 Notation", "content": "Let \\(x \\in \\mathcal{X}\\) and \\(y \\in \\mathcal{Y} = \\{1, ..., K\\}\\) be random variables that denote d-dimensionalinputs and labels, respectively, in multi-class classification tasks with K classes.These random variables follow a joint distribution \\(\\pi(x,y) = \\pi(y|x)\\pi(x)\\). Thedataset \\(\\mathcal{D} = \\{(x_n, y_n)\\}_{n=1}^N\\) consists of the N number of i.i.d. samples drawnfrom \\(\\pi(x, y)\\). Let f be a pre-trained neural network and \\(f(x) = (\\hat{y}, z)\\) be theoutput of the neural network, where \\(\\hat{y}\\) is a predicted label and z is an originalnon-probabilistic output of the network, referred to as the logit. The logit z isconverted into a probabilistic value \\(p\\) using the softmax function \\(\\sigma_{SM}\\). Thus, p represents a confidence score associated with the predicted label \\(\\hat{y}\\). To summa-rize, the output of the neural classifier f, p and \\(\\hat{y}\\), can be obtained as follows:\\[p = \\max_k \\sigma_{SM}(z), \\quad \\text{and} \\quad \\hat{y} = \\mathop{\\text{argmax}}\\limits_k \\sigma_{SM}(z) \\quad \\text{for } k \\in \\{1, ..., K\\}.\\]"}, {"title": "3.2 Calibration Metric", "content": "Perfect calibration is achieved when the predicted probability (i.e. confidence)matches with the actual likelihood of a correct prediction. If a neural networkpredicts a label as y with confidence p, the actual likelihood of the predictionshould ideally be p. Thus, the perfect calibration in multi-class classification canbe represented as follows:\\[P(\\hat{y} = y | p = p) = p, \\quad \\forall p \\in [0, 1].\\]The goal of uncertainty calibration is to minimize the gap between the ground-truth likelihood and the predicted confidence by calibrating the confidence value.Using this definition of perfect calibration, the calibration error can be com-puted by modifying (1):\\[\\mathbb{E}_p[ |P(\\hat{y} = y | p = p) - p| ].\\]Subsequently, the expected calibration error (ECE) empirically approxi-mates the calibration error in (2) using a binning technique. By discretizing theconfidence interval into M equally sized bins, i.e. \\(\\{B_m\\}_{m=1}^M\\), the ECE calculatesa weighted average of differences between accuracy \\(acc(\\cdot)\\) and confidence \\(conf(\\cdot)\\)in each bin. With N samples, the ECE is defined as follows:\\[ECE = \\sum_{m=1}^M \\frac{|B_m|}{N} |acc(B_m) - conf(B_m)|,\\]where all ECE values in this paper are calculated with M = 15 and multiplied by100. In addition to ECE, there are other metrics such as Maximum CalibrationError (MCE), which represents the highest error among bins, Static Calibra-tion Error (SCE) that evaluates calibration errors on a classwise manner,and KDE-ECE that utilizes Kernel Density Estimation (KDE). As ECEis the most representative metric, we primarily evaluate the proposed methodusing ECE, but we also employ other metrics."}, {"title": "3.3 Calibration in OOD Scenarios", "content": "In general, OOD refers to a distribution that differs from the training distribution. In this paper, the term OOD includes two types of distribution shifts:covariate shift and semantic shift. Covariate-shifted samples are drawn froma different joint distribution \\(\\pi_{OOD}(x, y)\\) such that \\(\\pi_{OOD} \\neq \\pi\\). In other words,while the samples may belong to the same class, they are presented in differentforms. In the case of semantic shift, the data is drawn from \\(\\pi_{\\mathcal{Y}^{\\'}}(x, y)\\), where\\(\\mathcal{Y}^{\\'} \\cap \\mathcal{Y} = \\emptyset\\), indicating that the data is from classes not present in the training set \\(\\mathcal{D}\\). Therefore, in semantic shift scenarios, all predictions by a pre-trainedclassifier may be incorrect, as they may correspond to one of the K in-distributionclasses in \\(\\mathcal{Y}\\). From the perspective of calibration, in such scenarios, the lower theconfidence, the better the calibration."}, {"title": "4 Proposed Method", "content": "To overcome the limitations of conventional calibration methods when dis-tribution shifts exist, we propose a robust calibration method that exhibits cal-ibration improvements across various OOD scenarios. Previous approaches that initially addressed shift scenarios in post-hoc calibration could not properlyhandle ID inputs. In contrast, our method achieves calibration improvementsin both OOD and ID scenarios by adaptively capturing the uncertainty of pre-trained neural networks for each input. To accomplish this, our method leveragesthe concept of the energy model, which is technically derived from energy-basedOOD detection methods. Before introducing our calibration method, we layout the mathematical motivation behind the proposed method by establishing acorrelation between our method and the energy model. The overall pipeline forunderstanding the propose method is illustrated in Fig.2."}, {"title": "4.1 Mathematical Motivation", "content": "The energy function makes scores on ID and OOD more distinguishable than thesoftmax function. As demonstrated in, there is a connection betweenGibbs distributions (i.e. Boltzmann distributions) and softmax functions:\\[P(y|x) = \\frac{e^{-\\beta E(x, y)}}{\\sum_{i \\in \\mathcal{Y}} e^{-\\beta E(x, y)}}, \\quad \\text{and} \\quad \\sigma_{SM}(y|x) = \\frac{e^{f_y(x)}}{\\sum_{i=1}^K e^{f_i(x)}},\\]where \\(P(y|x)\\) denotes the Gibbs distribution with the energy \\(E(x, y) : \\mathbb{R}^D \\rightarrow \\mathbb{R}\\)and \\(\\sigma_{SM}(y|x)\\) indicates the softmax function for the K-class classifier \\(f(x) :\\mathbb{R}^D \\rightarrow \\mathbb{R}^K\\). In (4), \\(f(x)\\) outputs a vector of length K and \\(f_i(x)\\) denotes the i-thelement of the vector.\nBy comparing \\(P(y|x)\\) and \\(\\sigma_{SM}(y|x)\\) in (4), we can derive the energy as\\[E(x, y) = -f_y(x),\\]where the positive constant value \\(\\beta\\) is set to 1. The denominator of \\(P(y|x)\\) in(4) is a partition function, transforming each energy value corresponding to yinto a probability value within the range of [0, 1]. In particular, Helmholtz freeenergy is defined as a log partition function. Then, the free energy F canbe represented using the connection between the Gibbs distribution and thesoftmax function:\\[F(x) = \\log \\sum_{i=1}^K e^{f_i(x)}.\\]A mathematical relationship between the energy and the negative log likeli-hood (NLL) loss has been derived in [17]. Based on this, it is demonstrated thatthe NLL loss inherently decreases the energy for ID samples, while increasingthe energy for OOD samples. From these findings, we define the NLL loss,\n\\[\\mathcal{L}_{NLL} = -\\log \\sigma_{SM}(y|x),\\]as a combination of \\(E(x, y)\\) in (5) and \\(-F(x)\\) in (6).\\[\\mathcal{L}_{NLL} = -\\log \\frac{e^{f_y(x)}}{\\sum_{i=1}^K e^{f_i(x)}} = -f_y(x) + \\log \\sum_{i=1}^K e^{f_i(x)} = E(x,y) - F(x),\\]where the free energy F can be interpreted as a contrastive term that aggregatesthe energies for all classes of \\(i \\in \\{1, ..., K\\}\\). From the third equation in (7), wecan see that the NLL loss inherently lowers the energy for the correct label yand raises the energy for the other labels. Additionally, the derivative of \\(\\mathcal{L}_{NLL}\\)over the network parameter \\(\\theta\\) is calculated as follows.\\[\\frac{\\partial \\mathcal{L}_{NLL}}{\\partial \\theta} = \\frac{\\partial E(x,y)}{\\partial \\theta} - \\frac{\\partial F(x)}{\\partial \\theta} = \\frac{\\partial E(x, y)}{\\partial \\theta} - \\sum_{i=1}^K \\frac{\\frac{\\partial E(x, i)}{\\partial \\theta}}{e^{-E(x,i)} \\sum_{j=1}^K e^{-E(x,j)}}\\]\\[ = \\frac{\\partial E(x, y)}{\\partial \\theta} - \\sum_{i=1}^K \\frac{\\partial E(x, i)}{\\partial \\theta} P(i|x),\\]\\[= \\frac{\\partial E(x, y)}{\\partial \\theta} - \\sum_{i \\neq y}^K \\frac{\\partial E(x, i)}{\\partial \\theta} (1 - P(y|x)) - \\frac{\\partial F(x,i)}{\\partial \\theta} P(i|x),\\]"}, {"title": "4.2 Robust Instance-wise Calibration", "content": "Most existing calibration methods are limited by the assumption of the samedistribution on which the classifier has been trained. As the parameters areoptimized using the consistent distribution of the validation set, these methodslack the adaptiveness to effectively address distribution shift scenarios. To solvethis problem, an uncertainty calibration method should possess the capability tocapture the uncertainty of the neural network for each individual sample. In thiscontext, the energy score can effectively fulfill this role within the framework ofpost-hoc calibration. As shown in the motivation and Fig.1, it is evident thatthe energy score is capable of producing distinct values for both cases: ID andOOD samples, as well as correct and incorrect samples.\nBy facilitating our motivation, we adjust the scaling factor for each inputsamples to achieve uncertainty calibration. Our method is fundamentally builtupon the temperature scaling (TS) technique introduced in to incorporatethe advantages of accuracy-preserving property. The proposed scaling factor isdefined as follows:\\[h(T_{ts}, x; \\theta) = \\frac{T_{ts}}{-\\lambda_1\\theta_1 + \\lambda_2\\theta_2},\\\\]where \\(T_{ts}\\) denotes the temperature parameter obtained by the TS technique, which is fixed on the validation set, and \\(\\theta \\triangleq \\{\\theta_1, \\theta_2\\}\\) denotes trainable"}, {"title": "Algorithm 1 Training Calibration Parameters", "content": ""}, {"title": "Algorithm 2 Proposed Instance-wise Calibration Method", "content": "input sample is crucial for capturing the uncertainty of a particular prediction,especially in the presence of distribution shifts. This is the main reason on therobust calibration performance exhibited by our method across various test data,including ID samples and various types of distribution shifted samples (Fig.4),which is empirically demonstrated in the experiment section. Then, using ourinstance-wise scaling with the scaling factor in (9), the calibrated probability ofsample x for K classes, \\(p_c \\in \\mathbb{R}^K\\), can be expressed as follows:\\[p_c = \\sigma_{SM}(f(x)/h(T_{ts}, x; \\theta)),\\]where \\(h(T_{ts}, x; \\theta)\\) denotes the proposed scaling factor defined in (9). To train theparameters \\(\\theta = \\{\\theta_1,\\theta_2\\}\\) in (11), we design a mean squared error loss function:\\[\\mathcal{L}_o = \\frac{1}{N} \\sum |\\hat{y}^{(i)} - p_c^{(i)}|^2_2,\\]where \\(\\hat{y}^{(i)} \\in \\mathbb{R}^K\\) denotes the one-hot encoded ground-truth label for the ith sample, \\(p_c^{(i)} \\in \\mathbb{R}^K\\) denotes c in (11) for the i-th sample, and N is thetotal number of training samples. Using the trained parameter \\(\\theta\\), the calibratedconfidence for a test sample x can be calculated in an instance-wise manner:\\[\\hat{q} = \\mathop{\\text{max}}(\\sigma_{SM}(f(x)/h(T_{ts}, x; \\theta)).\\] describes the entire procedure for training calibration parame-ters, while Algorithm 2 outlines the procedure of implementing the instance-wisecalibration method with the trained parameters."}, {"title": "5 Experiment", "content": ""}, {"title": "5.1 Experimental Settings", "content": "For experiments, we trained classification DNNs including VGGNet, ResNet, WideResNet, DenseNet and SE-ResNet on the CIFAR10/CIFAR100datasets. We employed pre-trained weights implemented in PyTorch for"}, {"title": "5.2 Ablation Study on Energy Score", "content": "We conducted an ablation study on the energy score in (6). To demonstrate the capability of our method in capturing network uncertainty, we evaluated the energy score on various samples from complete ID dataset (CIFAR10), corrupteddataset (CIFAR10-C) and semantic OOD dataset (SVHN). As shown in Fig.3,the energy scores tend to decrease with higher variances as the degree of distri-bution shift increases. Since the tendency for energy scores to decrease followsthe trend of diminishing accuracy, it implies that the energy scores can indeedefficiently capture the uncertainty of DNNS."}, {"title": "5.3 Comparison with Baseline Methods", "content": "We compared our method with the aforementioned baseline methods by eval-uating them across various datasets and backbone architectures in terms ofECE. Because our method emphasizes robust calibration performance on diversedatasets, we comprehensively conducted experiments on a variety of distributionshift scenarios, spanning from complete in-distribution to heavily corrupted sce-narios. For this purpose, we employed a corrupted dataset comprising severitylevels ranging from 1 to 5, with complete ID test data added as severity level 0.Table 1 shows the averaged ECE across all severity levels. Our method out-performs other baseline methods for various backbone networks and datasets.Furthermore, as shown in Fig.4, our method surpassed other approaches in mostindividual severity levels. It is noteworthy that our method shows consistent per-formance not only in scenarios involving corruptions but also in cases of completeID scenarios. This is in contrast to, which exhibited greater miscalibra-tion even than the uncalibrated one in the context of complete in-distributiondata. Fig.5 shows a comparison of ECE by corruption type, similar to [7,32].Our method demonstrates robust calibration across various corruption types.Additional results on diverse calibration metrics and transformer-basedmodels are available in the supplementary materials."}, {"title": "5.4 Exploring Synergies with Applicable State-of-the-art Method", "content": "We analyzed the results of applying an applicable post-hoc calibration methodto our proposed method. DAC, similar to our objective, aims for robustcalibration performance even in OOD scenarios. Unlike most methods, includingours, which use only the output of the last layer of the DNN, DAC additionallyutilized the output of other layers. Since DAC is designed to be used alongsidepost-hoc calibration methods, we applied it to our proposed method. We followedthe DAC's layer selection method proposed by Tomani et al. Table 2 showsthe averaged ECE for each corrupted dataset. We compared our method withETS+DAC and SPLINE+DAC, both of which primarily achieved state-of-the-art in. While our method showed good synergy with DAC, it alone achievedthe best performance even without DAC. Notably, our approach can attain theseresults without needing the additional output information from each classifierlayer used by DAC."}, {"title": "5.5 Evaluating Robustness on Semantic OOD", "content": "We measured the calibrated confidence scores for semantic OOD test samples,which are different from OOD train samples used to adjust our calibration pa-rameters. For this experiment, we utilized DenseNet201. Fig.6 demonstrates thatour method produces the lowest confidence scores for OOD samples, because allpredictions for OOD samples are incorrect.\nFurthermore, we conducted an experiment to investigate the potential ex-tension into OOD detection. To accomplish this, we utilized key evaluation met-rics commonly employed in OOD detection, such as AUROC, AUPR-in, and"}, {"title": "6 Conclusion", "content": "In this paper, we addressed the limitations of existing post-hoc calibration meth-ods on the wild datasets, including in-distribution, covariate shift, and semanticshift. Conventional methods could not consider all these scenarios in achievingrobust calibration. To solve this problem, we introduced a novel instance-wisecalibration method using the energy score. Our method adaptively captureduncertainty for each instance by leveraging the energy score. Through experi-ments conducted across various networks and datasets, we demonstrated thatour method outperforms existing calibration methods in scenarios involving var-ious types of distribution shifts, while consistently maintaining calibration effectin the complete in-distribution dataset.\nAs the reliability of AI in safety-critical situations becomes increasingly im-portant, we believe that our method can contribute to the safer deployment ofAI systems in real-world scenarios. By offering a promising direction with ourmethod, we hope to inspire future research efforts for enhancing trustworthy AI."}, {"title": "A Evaluation on Additional Calibration Metrics", "content": "Table 4: Averaged KDE-ECE over all severity levels of corruption. The low-est KDE-ECE results were highlighted in bold and the parentheses indicates the OODdataset used for tuning our method, same as in the main text. It can be also observedthat our method has a noticeable performance across various DNN architectures andcorrupted datasets, when evaluated using KDE-ECE."}, {"title": "B Evaluation on Transformer-based Models", "content": "Table 6: Calibration effect across diverse transformer-based models. Results on averaged ECE and KDE-ECE over all severity levels of corruption. It demonstratescalibration effect of proposed method across diverse transformer-based models."}, {"title": "C Evaluation on Additional Covariate OOD Dataset", "content": "Table 7: ECE results on additional covariate shifted datasets. This ta-ble presents ECE using accuracy-preserving methods. It confirms that our proposedmethod exhibits the best calibration performance across ImageNet-R, ImageNet-A,and ImageNet-Sketch.\nIn the main text, we utilized the ImageNet-C dataset, which features pro-gressively diverging stages of corruption from the in-distribution (ID), to repre-sent covariate-shifted test sets and effectively demonstrate the characteristics ofour method. In this section, we introduce additional experimental results usingadditional covariate OOD datasets as test sets beyond ImageNet-C. We em-ployed different covariate OOD datasets including ImageNet-Renditions (R),ImageNet-Adversarial (A) and ImageNet-Sketch."}, {"title": "D Ablation Study on Semantic OOD Data", "content": "This section provides interesting detailed examinations through various ablationstudies related to the semantic OOD dataset, which was utilized for parametertuning in our method. Firstly, Section D.1 discusses the impacts of differenttypes of semantic OOD datasets utilized for tuning. Lastly, Section D.2 offers acomprehensive analysis of the results when semantic OOD is not used for tuningat all."}, {"title": "D.1 Semantic OOD type", "content": "Table 8: Results when using different tuning semantic OOD datasets. Withthe use of different semantic OOD datasets for tuning, the calibration performanceremains consistently good without significant changes in performance.\nFor the ID dataset, we used CIFAR10, CIFAR100, and ImageNet-1k, whilefor the semantic out-of-distribution (OOD) datasets, we utilized SVHN and theTexture dataset. When tuning the calibration parameter, if SVHN was used,then another dataset, Texture, was employed as the test set to prevent any dataleakage to the test dataset. We highlight that the semantic OOD data used fortuning was not used as the test dataset. In this section, we conducted an ab-lation study on the types of semantic OOD used for parameter tuning, usingDenseNet201. Table 8 indicates that regardless of the semantic OOD used fortuning the parameters of our method, it consistently maintained high perfor-mance without significant changes in the results."}, {"title": "D.2 Analysis of Results Without Using Semantic OOD for Tuning", "content": "Our intuition suggests that using energy to distinguish between correct and in-correct predictions should be effective with our algorithm alone. However, expo-"}]}