{"title": "Uncertainty Calibration with Energy Based Instance-wise Scaling in the Wild Dataset", "authors": ["Mijoo Kim", "Junseok Kwon"], "abstract": "With the rapid advancement in the performance of deep neu- ral networks (DNNs), there has been significant interest in deploying and incorporating artificial intelligence (AI) systems into real-world sce- narios. However, many DNNs lack the ability to represent uncertainty, often exhibiting excessive confidence even when making incorrect pre- dictions. To ensure the reliability of AI systems, particularly in safety- critical cases, DNNs should transparently reflect the uncertainty in their predictions. In this paper, we investigate robust post-hoc uncertainty calibration methods for DNNs within the context of multi-class classifi- cation tasks. While previous studies have made notable progress, they still face challenges in achieving robust calibration, particularly in sce- narios involving out-of-distribution (OOD). We identify that previous methods lack adaptability to individual input data and struggle to accu- rately estimate uncertainty when processing inputs drawn from the wild dataset. To address this issue, we introduce a novel instance-wise calibra- tion method based on an energy model. Our method incorporates energy scores instead of softmax confidence scores, allowing for adaptive consid- eration of DNN uncertainty for each prediction within a logit space. In experiments, we show that the proposed method consistently maintains robust performance across the spectrum, spanning from in-distribution to OOD scenarios, when compared to other state-of-the-art methods. The source code is available at https://github.com/mijoo308/Energy- Calibration.", "sections": [{"title": "1 Introduction", "content": "Despite the impressive performance demonstrated by recent AI systems, their de- ployment should be carefully considered, particularly in safety critical situations (e.g. autonomous driving, finance, health care, and medical diagnosis), because these systems cannot consistently ensure accurate predictions. For example, in the field of medical diagnosis, incorrect predictions have the potential to result in catastrophic outcomes. To address this concern, the system must transpar- ently reveal the uncertainty associated with its prediction. Recently, DNNs rely on confidence as a way of expressing uncertainty, but they tend to assign higher"}, {"title": "2 Related work", "content": ""}, {"title": "2.1 Post-hoc Calibration", "content": "Confidence calibration can be divided into two categories. The first category is known as training-time calibration [18, 22, 23, 30, 41], such as focal loss [22] and label smoothing [23]. These methods train DNNs to exhibit calibrated behavior during training. The second category is referred to as post-hoc calibration [4,7, 28,35,40]. In this approach, pre-trained neural networks are utilized along with hold-out validation datasets to learn calibration mappings in a post-hoc manner. Post-hoc calibration can be further categorized into non-parametric and parametric approaches. Non-parametric approaches include Histogram Binning (HB) [37] and Isotonic Regression (IR) [38]. HB divided predicted probabili- ties into multiple intervals, each associated with representative confidences. IR utilized isotonic regression with uncalibrated confidences as the x-axis and the expected accuracy values as the y-axis. BBQ [24], non-parametric extension of HB, incorporated Bayesian model averaging to enhance calibration. On the other hand, the most common parametric calibration method is Temperature Scaling (TS) [4]. As the temperature increases, the distribution of logits becomes more uniformly distributed, resulting in a decrease in the confidence score associated with the predicted label. TS has a significant advantage in terms of accuracy preservation, as it maintains the originally predicted label with the highest con- fidence score unchanged. However, TS exhibits limited expressiveness as it relies on only a single parameter that is fixed on the validation set."}, {"title": "2.2 Beyond In-distribution Calibration", "content": "Conventional research has mainly focused on investigating post-hoc uncertainty calibration methods. However, these methods often overlook scenarios involving distribution shifts. Due to their dependence on a fixed calibration map optimized for ID validation sets, they struggle to effectively handle unknown test samples. Recently, the importance of the robustness of post-hoc calibration method across various distribution shift scenarios has been emphasized [32, 33]. In [32], various degrees of Gaussian perturbations were injected into the ID validation dataset. The parameters of the calibration method were then adjusted using the perturbed validation data, resulting in enhanced robustness against shifted distributions. However, this method tends to achieve notable performance only within scenarios where a certain degree of distribution shift is present. Moreover, in a complete ID scenario, it exhibits even worse calibration compared to the pre-calibration state. To solve this problem, DAC [33] has been proposed as a pre-processing step before employing the existing post-hoc calibration methods. Notably, it leveraged additional output information for uncertainty estimation and enhanced the calibration performance in distribution shift scenarios. Similar to these methods, we focus on a broad range of scenarios, ranging from ID to various OOD scenarios. However, unlike these methods, our approach can facilitate robust calibration without requiring any additional DNN's layer information except for the last layer. In experiments, we demonstrate that our proposed method performs comparably to, and in some cases even surpasses, other state-of-the-art methods that utilize DAC as a preprocessing step."}, {"title": "3 Problem Setup", "content": "In this section, we establish preliminaries for uncertainty calibration. We de- fine key notations in the context of multi-class classification and present a rep- resentative calibration metric derived from the concept of perfect calibration. Additionally, we address calibration in OOD scenarios."}, {"title": "3.1 Notation", "content": "Let $x \\in X$ and $y \\in Y = {1, ..., K}$ be random variables that denote d-dimensional inputs and labels, respectively, in multi-class classification tasks with K classes. These random variables follow a joint distribution $\\pi(x,y) = \\pi(y|x)\\pi(x)$. The dataset $D = {(x_n, y_n)}_{n=1}^N$ consists of the N number of i.i.d. samples drawn from $\\pi(x, y)$. Let $f$ be a pre-trained neural network and $f(x) = (\\hat{y}, z)$ be the output of the neural network, where $\\hat{y}$ is a predicted label and $z$ is an original non-probabilistic output of the network, referred to as the logit. The logit $z$ is converted into a probabilistic value $p$ using the softmax function $\\sigma_{SM}$. Thus, $p$ represents a confidence score associated with the predicted label $\\hat{y}$. To summa- rize, the output of the neural classifier $f$, $p$ and $\\hat{y}$, can be obtained as follows:\n$p = \\max_k \\sigma_{SM}(z)$, and $\\hat{y} = \\underset{k}{\\operatorname{argmax}} \\sigma_{SM}(z)$ for $k\\in {1, ..., K}$."}, {"title": "3.2 Calibration Metric", "content": "Perfect calibration is achieved when the predicted probability (i.e. confidence) matches with the actual likelihood of a correct prediction. If a neural network predicts a label as $\\hat{y}$ with confidence $p$, the actual likelihood of the prediction should ideally be $p$. Thus, the perfect calibration in multi-class classification can be represented as follows:\n$\\mathbb{P}(\\hat{y} = y | p = p) = p, \\forall p \\in [0, 1]$.\n(1)\nThe goal of uncertainty calibration is to minimize the gap between the ground- truth likelihood and the predicted confidence by calibrating the confidence value. Using this definition of perfect calibration, the calibration error can be com- puted by modifying (1):\n$\\mathbb{E}_p [|\\mathbb{P}(\\hat{y} = y | p = p) - p|]$.\n(2)\nSubsequently, the expected calibration error (ECE) [24] empirically approxi- mates the calibration error in (2) using a binning technique. By discretizing the confidence interval into M equally sized bins, i.e. ${B_m}_{m=1}^M$, the ECE calculates a weighted average of differences between accuracy $acc(\\cdot)$ and confidence $conf(\\cdot)$ in each bin. With N samples, the ECE is defined as follows:\n$\\text{ECE} = \\sum_{m=1}^M \\frac{|B_m|}{N} |acc(B_m) - conf(B_m)|$,\n(3)\nwhere all ECE values in this paper are calculated with M = 15 and multiplied by 100. In addition to ECE, there are other metrics such as Maximum Calibration Error (MCE) [24], which represents the highest error among bins, Static Calibra- tion Error (SCE) [26] that evaluates calibration errors on a classwise manner, and KDE-ECE [40] that utilizes Kernel Density Estimation (KDE). As ECE is the most representative metric, we primarily evaluate the proposed method using ECE, but we also employ other metrics."}, {"title": "3.3 Calibration in OOD Scenarios", "content": "In general, OOD refers to a distribution that differs from the training distribution [11,36]. In this paper, the term OOD includes two types of distribution shifts: covariate shift and semantic shift. Covariate-shifted samples are drawn from a different joint distribution $\\pi_{OU}(x, y)$ such that $\\pi_{OU} \\neq \\pi$. In other words, while the samples may belong to the same class, they are presented in different forms [27]. In the case of semantic shift, the data is drawn from $\\pi_{\\mathcal{O}}(x, y)$, where $\\mathcal{O} \\cap Y = \\emptyset$, indicating that the data is from classes not present in the training set $D$ [11]. Therefore, in semantic shift scenarios, all predictions by a pre-trained classifier may be incorrect, as they may correspond to one of the K in-distribution classes in Y. From the perspective of calibration, in such scenarios, the lower the confidence, the better the calibration."}, {"title": "4 Proposed Method", "content": "To overcome the limitations of conventional calibration methods when dis- tribution shifts exist, we propose a robust calibration method that exhibits cal- ibration improvements across various OOD scenarios. Previous approaches [32] that initially addressed shift scenarios in post-hoc calibration could not properly handle ID inputs. In contrast, our method achieves calibration improvements in both OOD and ID scenarios by adaptively capturing the uncertainty of pre- trained neural networks for each input. To accomplish this, our method leverages the concept of the energy model, which is technically derived from energy-based OOD detection methods [19]. Before introducing our calibration method, we lay out the mathematical motivation behind the proposed method by establishing a correlation between our method and the energy model. The overall pipeline for understanding the propose method is illustrated in Fig.2."}, {"title": "4.1 Mathematical Motivation", "content": "The energy function makes scores on ID and OOD more distinguishable than the softmax function [19]. As demonstrated in [17], there is a connection between Gibbs distributions (i.e. Boltzmann distributions) and softmax functions:\n$\\mathbb{P}(y|x) = \\frac{e^{-\\beta E(x,y)}}{\\sum_{u \\in y} e^{-\\beta E(x,y)}}$, and $\\sigma_{SM}(y|x) = \\frac{e^{f_y(x)}}{\\sum_i^K e^{f_i(x)}}$,\n(4)\nwhere $\\mathbb{P}(y|x)$ denotes the Gibbs distribution with the energy $E(x, y) : \\mathbb{R}^D \\to \\mathbb{R}$ and $\\sigma_{SM}(y|x)$ indicates the softmax function for the K-class classifier $f(x) : \\mathbb{R}^D \\to \\mathbb{R}^K$. In (4), $f(x)$ outputs a vector of length K and $f_i(x)$ denotes the i-th element of the vector.\nBy comparing $\\mathbb{P}(y|x)$ and $\\sigma_{SM}(y|x)$ in (4), we can derive the energy as\n$E(x, y) = -f_y(x)$,\n(5)\nwhere the positive constant value $\\beta$ is set to 1. The denominator of $\\mathbb{P}(y|x)$ in (4) is a partition function, transforming each energy value corresponding to $y$ into a probability value within the range of [0, 1]. In particular, Helmholtz free energy is defined as a log partition function [17]. Then, the free energy $\\mathcal{F}$ can be represented using the connection between the Gibbs distribution and the softmax function:\n$\\mathcal{F}(x) = \\log \\sum_{i=1}^K e^{f_i(x)}$.\n(6)\nA mathematical relationship between the energy and the negative log likeli- hood (NLL) loss has been derived in [17]. Based on this, it is demonstrated that the NLL loss inherently decreases the energy for ID samples, while increasing the energy for OOD samples [19]. From these findings, we define the NLL loss, $\\mathcal{L}_{NLL} = - \\log \\sigma_{SM}(y|x)$, as a combination of $E(x, y)$ in (5) and $-\\mathcal{F}(x)$ in (6).\n$\\mathcal{L}_{NLL} = - \\log \\frac{e^{f_y(x)}}{\\sum_{i=1}^K e^{f_i(x)}} = -f_y(x) + \\log \\sum_{i=1}^K e^{f_i(x)} = E(x,y) - \\mathcal{F}(x)$,\n(7)\nwhere the free energy $\\mathcal{F}$ can be interpreted as a contrastive term that aggregates the energies for all classes of $i \\in {1, ..., K}$. From the third equation in (7), we can see that the NLL loss inherently lowers the energy for the correct label $y$ and raises the energy for the other labels. Additionally, the derivative of $\\mathcal{L}_{NLL}$ over the network parameter $\\theta$ is calculated as follows.\n$\\frac{\\partial \\mathcal{L}_{NLL}}{\\partial \\theta} = \\frac{\\partial E(x,y)}{\\partial \\theta} - \\frac{\\partial \\mathcal{F}(x)}{\\partial \\theta} = \\frac{\\partial E(x, y)}{\\partial \\theta} - \\sum_{i=1}^K \\frac{\\partial E(x, i)}{\\partial \\theta}  \\frac{e^{-E(x,i)}}{\\sum_{j=1}^K e^{-E(x,j)}}$\n$= \\frac{\\partial E(x,y)}{\\partial \\theta} - \\sum_{i=1}^K \\frac{\\partial E(x, i)}{\\partial \\theta} \\mathbb{P}(i|x)$\n$= \\sum_{i \\neq y}^K \\frac{\\partial E(x, i)}{\\partial \\theta} \\left(1 - \\mathbb{P}(y|x) - \\sum_{i\\neq y}^K  \\frac{\\partial E(x,i)}{\\partial \\theta} \\mathbb{P}(i|x),\\right.$\n(8)"}, {"title": "4.2 Robust Instance-wise Calibration", "content": "Most existing calibration methods are limited by the assumption of the same distribution on which the classifier has been trained. As the parameters are optimized using the consistent distribution of the validation set, these methods lack the adaptiveness to effectively address distribution shift scenarios. To solve this problem, an uncertainty calibration method should possess the capability to capture the uncertainty of the neural network for each individual sample. In this context, the energy score can effectively fulfill this role within the framework of post-hoc calibration. As shown in the motivation and Fig.1, it is evident that the energy score is capable of producing distinct values for both cases: ID and OOD samples, as well as correct and incorrect samples.\nBy facilitating our motivation, we adjust the scaling factor for each input samples to achieve uncertainty calibration. Our method is fundamentally built upon the temperature scaling (TS) technique introduced in [4] to incorporate the advantages of accuracy-preserving property. The proposed scaling factor is defined as follows:\n$h(T_{ts}, x; \\theta) = \\frac{T_{ts}}{-\\lambda_1 \\theta_1 + \\lambda_2 \\theta_2}$,\n(9)\nFixed on validation set Adaptive for each input\nwhere $T_{ts}$ denotes the temperature parameter obtained by the TS technique [4], which is fixed on the validation set, and $\\theta = {\\theta_1, \\theta_2}$ denotes trainable"}, {"title": "5 Experiment", "content": ""}, {"title": "5.1 Experimental Settings", "content": "For experiments, we trained classification DNNs including VGGNet [29], ResNet [6], WideResNet [39], DenseNet [13] and SE-ResNet [12] on the CIFAR10/CIFAR100 datasets [14]. We employed pre-trained weights implemented in PyTorch for"}, {"title": "5.2 Ablation Study on Energy Score", "content": ""}, {"title": "5.3 Comparison with Baseline Methods", "content": "We compared our method with the aforementioned baseline methods by eval- uating them across various datasets and backbone architectures in terms of ECE. Because our method emphasizes robust calibration performance on diverse datasets, we comprehensively conducted experiments on a variety of distribution shift scenarios, spanning from complete in-distribution to heavily corrupted sce- narios. For this purpose, we employed a corrupted dataset comprising severity levels ranging from 1 to 5, with complete ID test data added as severity level 0. Table 1 shows the averaged ECE across all severity levels. Our method out- performs other baseline methods for various backbone networks and datasets. Furthermore, as shown in Fig.4, our method surpassed other approaches in most individual severity levels. It is noteworthy that our method shows consistent per- formance not only in scenarios involving corruptions but also in cases of complete ID scenarios. This is in contrast to [32], which exhibited greater miscalibra- tion even than the uncalibrated one in the context of complete in-distribution data. Fig.5 shows a comparison of ECE by corruption type, similar to [7,32]. Our method demonstrates robust calibration across various corruption types. Additional results on diverse calibration metrics [26,40] and transformer-based models [3,20] are available in the supplementary materials."}, {"title": "5.4 Exploring Synergies with Applicable State-of-the-art Method", "content": "We analyzed the results of applying an applicable post-hoc calibration method to our proposed method. DAC [33], similar to our objective, aims for robust calibration performance even in OOD scenarios. Unlike most methods, including ours, which use only the output of the last layer of the DNN, DAC additionally utilized the output of other layers. Since DAC is designed to be used alongside post-hoc calibration methods, we applied it to our proposed method. We followed the DAC's layer selection method proposed by Tomani et al. [33]. Table 2 shows the averaged ECE for each corrupted dataset. We compared our method with ETS+DAC and SPLINE+DAC, both of which primarily achieved state-of-the- art in [33]. While our method showed good synergy with DAC, it alone achieved the best performance even without DAC. Notably, our approach can attain these results without needing the additional output information from each classifier layer used by DAC."}, {"title": "5.5 Evaluating Robustness on Semantic OOD", "content": "We measured the calibrated confidence scores for semantic OOD test samples, which are different from OOD train samples used to adjust our calibration pa- rameters. For this experiment, we utilized DenseNet201. Fig.6 demonstrates that our method produces the lowest confidence scores for OOD samples, because all predictions for OOD samples are incorrect.\nFurthermore, we conducted an experiment to investigate the potential ex- tension into OOD detection. To accomplish this, we utilized key evaluation met- rics commonly employed in OOD detection, such as AUROC, AUPR-in, and"}, {"title": "6 Conclusion", "content": "In this paper, we addressed the limitations of existing post-hoc calibration meth- ods on the wild datasets, including in-distribution, covariate shift, and semantic shift. Conventional methods could not consider all these scenarios in achieving robust calibration. To solve this problem, we introduced a novel instance-wise calibration method using the energy score. Our method adaptively captured uncertainty for each instance by leveraging the energy score. Through experi- ments conducted across various networks and datasets, we demonstrated that our method outperforms existing calibration methods in scenarios involving var- ious types of distribution shifts, while consistently maintaining calibration effect in the complete in-distribution dataset. As the reliability of AI in safety-critical situations becomes increasingly im- portant, we believe that our method can contribute to the safer deployment of AI systems in real-world scenarios. By offering a promising direction with our method, we hope to inspire future research efforts for enhancing trustworthy AI."}]}