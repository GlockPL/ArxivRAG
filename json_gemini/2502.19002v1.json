{"title": "The Sharpness Disparity Principle in Transformers for Accelerating Language Model Pre-Training", "authors": ["Jinbo Wang", "Mingze Wang", "Zhanpeng Zhou", "Junchi Yan", "Weinan E", "Lei Wu"], "abstract": "Transformers consist of diverse building blocks, such as embedding layers, normal-\nization layers, self-attention mechanisms, and point-wise feedforward networks.\nThus, understanding the differences and interactions among these blocks is impor-\ntant. In this paper, we uncover a clear sharpness disparity across these blocks,\nwhich emerges early in training and intriguingly persists throughout the training\nprocess. Motivated by this finding, we propose Blockwise Learning Rate (LR),\na strategy that tailors the LR to each block's sharpness, accelerating large lan-\nguage model (LLM) pre-training. By integrating Blockwise LR into AdamW,\nwe consistently achieve lower terminal loss and nearly 2\u00d7 speedup compared\nto vanilla AdamW. We demonstrate this acceleration across GPT-2 and LLaMA,\nwith model sizes ranging from 0.12B to 1.1B and datasets of OpenWebText and\nMiniPile. Finally, we incorporate Blockwise LR into Adam-mini (Zhang et al.,\n2024c), a recently proposed memory-efficient variant of Adam, achieving a com-\nbined 2x speedup and 2\u00d7 memory saving. These results underscore the potential\nof exploiting the sharpness disparity to improve LLM training.", "sections": [{"title": "1 Introduction", "content": "Transformers (Vaswani et al., 2017) have achieved remarkable success across fields, including\nnatural language processing (Brown et al., 2020), vision (Dosovitskiy et al., 2020), and scientific\ncomputing (Jumper et al., 2021). They have become the de facto design in modern AI models (Team\net al., 2023; Achiam et al., 2023; Liu et al., 2024a).\nCompared to traditional architectures, e.g., multilayer perceptrons (MLPs), convolutional neural\nnetworks (CNNs), and recurrent neural networks (RNNs), transformers exhibit distinctive alloy-\nlike characteristics, where diverse types of blocks synergistically combine to achieve superior\nperformance. A transformer at minimum includes self-attention (further broken down into query-key\n(QK) and value-output (VO)) blocks, point-wise feedforward networks (FFN), normalization layers\n(Norm), and embedding layers (Emb). Uncovering the distinct properties of these blocks, as well\nas the differences and interactions among them, is thus crucial for gaining a deeper insight into\ntransformer models (Wang et al., 2024b)."}, {"title": "2 Related Works", "content": "Sharpness structures in transformers. Recent work has started to investigate blockwise sharpness\npatterns in transformer models through Hessian-based analyses. For example, Zhang et al. (2024b)\nempirically observed the sharpness' blockwise heterogeneity but did not establish a clear principle\nregarding the sharpness disparity among different blocks. Meanwhile, Ormaniec et al. (2024) provided\na Hessian analysis for a single self-attention (SA) layer, focusing only on the sharpness disparity\nbetween the query-key (QK) and value-output (VO) blocks within the same layer.\nIn contrast, we examine sharpness at the block-type level across the entire transformer architecture,\nrather than focusing on individual blocks (as in Zhang et al. (2024b)) or a single layer (as in Ormaniec\net al. (2024)). This coarse-grained perspective reveals a consistent disparity, as formalized by\nPrinciple (1), which persists throughout most of the training process except during the initial steps.\nEfficient optimizers for LLM pre-training. AdamW (Adam with decoupled weight de-\ncay) (Loshchilov and Hutter, 2017) has become the default optimizer in LLM pre-training. Efforts to\ndesign more efficient optimizers generally fall into two main categories: accelerating convergence and\nreducing memory footprint. Accelerations have been developed using techniques such as Nesterov\nmomentum (Xie et al., 2022), diagonal second-order estimates (Liu et al., 2024b; Wang et al., 2024a),\nvariance reduction (Yuan et al., 2024), and matrix-based preconditioners (Keller et al., 2024; Vyas\net al., 2024). Memory-efficient optimizers utilize sign-based methods (Chen et al., 2024), reduced\nusage of second moments in Adam (Zhang et al., 2024c), and gradient low-rank projection (Zhao\net al., 2024). The closest work to our Blockwise LR is Wang et al. (2024a), which also increases the\nLR along low-sharpness directions. A detailed comparison is deferred to Section 5.\nThe edge of stability (EoS) phenomenon. Neural network training typically occurs at the EoS\nstage (Wu et al., 2018; Jastrzebski et al., 2020; Cohen et al., 2021; 2022), where the optimizer\nexhibits oscillatory behavior along sharp directions without diverging, while steadily progressing\nalong flat directions, leading to loss reduction. Several works (Wen et al., 2024; Song et al., 2024;\nCohen et al., 2024; Wang et al., 2024a) have highlighted the crucial role of the dynamics along flat\ndirections (referred to as river directions by Wen et al. (2024), bulk directions by Song et al. (2024),\nand stable direction in Wang et al. (2024a)) in reducing total loss. Notably, Wen et al. (2024) further\ndemonstrated that this picture is essential for understanding LLM pre-training. Building on these\ninsights, our Blockwise LR approach is designed to accelerate training by amplifying the dynamics\nparticularly along the flat river directions."}, {"title": "3 Preliminaries", "content": "Notations. Let $||\\cdot||_2$, $||\\cdot||_F$, and Tr(\u00b7) denote the spectral norm, Frobenius norm and trace for\nmatrices, respectively. Given $A \\in \\mathbb{R}^{m \\times n}$, its row-wise vectorization is defined as vec(A) =\n$(a_{1,1},\\cdots, a_{1,n},\\cdots, a_{m,1},\\cdots, A_{m,n}) \\in \\mathbb{R}^{mn}$. The Kronecker product and Hadamard product are\ndenoted by $\\otimes$ and $\\odot$, respectively. The row-wise mean and covariance of $A \\in \\mathbb{R}^{m \\times n}$ are denoted\nby $E_r[A] \\in \\mathbb{R}^{m \\times n}$ and $V_r[A] \\in \\mathbb{R}^{m \\times n}$, respectively. Specifically, they are defined as: for all\n$i\\in [m]$, $j\\in [n]$, $(E_r[A])_{i,j} = \\frac{1}{n}\\sum_{k=1}^n A_{i,k}$, $(V_r[A])_{i,j} = (A_{i,j} - \\frac{1}{n}\\sum_{k=1}^n A_{i,k})^2$. We will use\nstandard big-O notations like $O(\\cdot)$, $\\Omega(\\cdot)$, and $\\Theta(\\cdot)$ to hide problem-independent constants.\nJacobian matrix. Given a vector-valued function: $b \\rightarrow a(b)$ with $b \\in \\mathbb{R}^n$ and $a(b) \\in \\mathbb{R}^m$,\nthe Jacobian is defined as $\\frac{\\partial a}{\\partial b} = (\\frac{\\partial a^{(i)}}{\\partial b^{(j)}})_{i,j} \\in \\mathbb{R}^{m\\times n}$. Analogously, for a matrix-valued function:"}, {"title": "3.1 The Transformer Architecture", "content": "Given an n-token input sequence $X = (x_1,\\ldots,x_n) \\in \\mathbb{R}^{n \\times d}$, where $d$ refers to the vocabulary\nsize in LLM and each $x_i$ corresponds to the token's one-hot encoding, an L-layer transformer TF\nprocesses it as follows.\nEmbedding layer. First, each input token is embedded into the latent space through an embedding\nlayer with parameters $W_E \\in \\mathbb{R}^{d \\times D}$, $b_e \\in \\mathbb{R}^{1\\times D}$:\n$x_s^{(0)} = x_sW_E + b_e$, $s \\in [n]$,\nwhere the bias $b_e$ is omitted in LLMs such as nanoGPT (Karpathy, 2022).\nL-layer SA-FFN blocks. Then the embedded sequence $X^{(0)}$ is processed by L-layer SA-FFN\nblocks, and the output of the final layer is taken as the output sequence $TF(X) = X^{(L)} \\in \\mathbb{R}^{n \\times D}$.\nFor each layer $l \\in [L]$, the computations are as follows:\n$X^{(l-\\frac{1}{2})} = X^{(l-1)} + SA^{(l)}(Norm^{(l-\\frac{1}{2})}(X^{(l-1)}))$;\n$X^{(l)} = X^{(l-\\frac{1}{2})} + FFN^{(l)}(Norm^{(l)}(X^{(l-\\frac{1}{2})}))$ .\\newline\nNorm blocks. Here, $Norm^{(v)}$ ($v \\in {l-\\frac{1}{2}, l}$) denote normalization layers (e.g., LayerNorm (Lei Ba\net al., 2016) and RMSNorm (Zhang and Sennrich, 2019)) with learnable parameters $\\gamma^{(v)}$, $\\beta^{(v)} \\in\n\\mathbb{R}^{1\\times D}$. For LayerNorm, the computation for a token $x \\in \\mathbb{R}^{1\\times D}$ is:\n$Norm^{(v)}(x) = \\frac{x - E_r[x]}{\\sqrt{V_r[x]}} \\odot \\gamma^{(v)} + \\beta^{(v)}$.\nwhere the bias $\\beta$ is omitted in LLMs such as nanoGPT.\nFFN blocks. $FFN^{(l)}$ denotes a (token-wise) two-layer FFN of width $M$, comprising parameters\n$W_1^{(l)} \\in \\mathbb{R}^{D \\times M}$, $W_2^{(l)} \\in \\mathbb{R}^{M \\times D}$, and using activation function $\\sigma(\\cdot)$ such as ReLU. For any token\n$x \\in \\mathbb{R}^{1\\times D}$, the operation is:\n$FFN^{(l)}(x) = \\sigma(xW_1^{(l)})W_2^{(l)}$.\nSA blocks. $SA^{(l)}$, a multi-head self-attention, has parameters $W_Q^{(l)}$, $W_K^{(l)}$, $W_V^{(l)}$, $W_O^{(l)} \\in \\mathbb{R}^{D \\times D}$.\nWhen applied to a sequence $Z \\in \\mathbb{R}^{n \\times D}$, it operates as:\n$SA^{(l)}(Z) = \\sum_{h=1}^{H} SA^{(l,h)}(Z)W_O^{(l,h)}$, $SA^{(l,h)}(Z) =$\n$\\text{softmax}\\left(\\frac{(ZW_Q^{(l,h)}), ZW_K^{(l,h)})}{\\sqrt{D/H}} + M\\right)ZW_V^{(l,h)}$\nwhere $H$ is the head number, and $W_Q^{(l,h)}$, $W_K^{(l,h)}$, $W_V^{(l,h)} \\in \\mathbb{R}^{D \\times (D/H)}$, $W_O^{(l,h)} \\in \\mathbb{R}^{(D/H) \\times D}$ are\nsplit from $W_Q^{(l)}$, $W_K^{(l)}$, $W_V^{(l)}$, $W_O^{(l)}$ by heads, respectively. The operator $\\text{softmax}(\\cdot)$ represents the\nrow-wise softmax normalization. For the next-token prediction, the mask $M \\in \\mathbb{R}^{n \\times n}$ satisfies\n$M_{i,j} = -\\infty$ if $i<j$ and $M_{i,j} = 0$ otherwise."}, {"title": "3.2 Blockwise Sharpness and the Efficient Estimation", "content": "Measuring sharpness requires accessing the Hessian matrix, which is computationally expensive due\nto the high dimensionality of the parameter space. Consequently, approximate methods are needed to\nreduce computational complexity.\nLet $l(\\cdot,\\cdot)$ denote the cross-entropy loss. For an input data $x \\in \\mathbb{R}^d$ and label $y \\in \\mathbb{R}^{d_y}$, let the\nmodel's prediction be $f(x; \\theta) \\in \\mathbb{R}^{d_y}$. The Fisher (Gauss-Newton) matrix $F(\\theta)$ is widely recognized\nas a good approximation of the Hessian, particularly near minima. Thus, the diagonal Hessian can be estimated\nas $h(\\theta) = \\text{diag}(F(\\theta))$, a popular technique in deep learning optimization (Martens and Grosse, 2015;"}, {"title": "4 The Sharpness Disparity Principle", "content": "4.1 Main Findings\nWe first investigate the sharpness disparity across different types of building blocks (Emb, QK, VO,\nFFN, Norm) in transformer-based LLMs. Specifically, we pre-trained GPT-2 (Radford et al., 2019)\nand LLaMA (Touvron et al., 2023) models on the OpenWebText dataset using default configurations.\nBlockwise diagonal Hessians are analyzed at various checkpoints using the Hessian estimator (3).\nThe experimental details can be found in Appendix A.1."}, {"title": "4.2 Theoretical Insights", "content": "To provide theoretical insights into explaining Principle (1), we derive analytic expressions of\n$S(\\cdot)$ and analyze their dependence on parameter magnitudes and numbers of each block. For\nsimplicity, we denote $Q(\\theta) := L_B(\\theta)$, where $L_B(\\theta)$ is defined in (3). Then from (4), we have\n$S(\\bullet) = \\frac{B ||\\nabla_{\\bullet} Q||_F^2}{\\#(\\bullet)}$. Without loss of generality, we set $B=1$. Our calculations for $\\nabla Q$ apply\nto general Q.\nConsidering blocks across different layers is complicated. Therefore, we focus on comparisons within\nthe same layer. Specifically, we examine the following sharpness comparisons: (i) FFN vs. Norm\nwithin the same layer; (ii) SA (comprising QK and VO) vs. Norm within the same layer; and (iii)\nEmb vs. the adjacent Norm."}, {"title": "Theorem 4.1 (FFN vs. Norm).", "content": "Consider the $l$-th layer in a transformer (2). Omitting the layer index\nfor simplicity, let $Y = X + FFN(Norm(X; \\gamma); W_1, W_2)$, where FFN utilizes the (Leaky) ReLU\nactivation $\\sigma$. Then, the gradients of $Q$ w.r.t. $W_1, W_2$, and $\\gamma$ are:\n$\\frac{\\partial Q}{\\partial W_2} = \\frac{\\partial Q}{\\partial \\mathcal{A}} \\frac{\\partial \\mathcal{A}}{\\partial M} (X_{Norm}W_1 \\otimes I_d)$;\n$\\frac{\\partial Q}{\\partial W_1} = (I_n \\otimes W_2^T) \\frac{\\partial \\mathcal{A}}{\\partial M} (I_n \\otimes W_1)$;\n$\\frac{\\partial Q}{\\partial \\gamma} = \\frac{\\partial Q}{\\partial \\mathcal{A}} \\frac{\\partial \\mathcal{A}}{\\partial M} \\text{diag}(vec(X_{std})) (1_{n \\times 1} \\otimes I_d)$,\nwhere $X_{std} := \\frac{X-\\mathbb{E}[X]}{\\sqrt{V[X]}}$, $X_{Norm} := \\text{Norm}(X; \\gamma) = X_{std} (1_{n \\times 1} \\otimes \\gamma)$, $\\mathcal{A} := \\sigma(M)$, $M :=\nX_{Norm} W_1$. Let $\\Psi := \\frac{n}{\\sqrt{D}} ||\\gamma||_F ||W_1||_F ||W_2||_F ||\\sigma'(Y)||_F$. Then, the blockwise average\nsharpness can be bounded as:\n$S(W_\\cdot) = O(\\frac{\\Psi^2}{D^2 ||W_\\cdot||^2}), \\bullet \\in \\{1,2\\}$;\n$S(\\gamma) = O(\\frac{\\Psi^2}{D ||\\gamma||^2})$,\nwhere the denominators ($D^2$ or $D$) reflect the number of parameters in each group."}, {"title": "Theorem 4.2 (QK, VO vs. Norm).", "content": "Consider the $(l - \\frac{1}{2})$-th layer in (2). Omitting the layer in-\ndex for simplicity, let $Y = X + SA(Norm(X; \\gamma); W_K, W_Q, W_V, W_O)$. Consider a single-\nhead attention (i.e., $H=1$) for simplicity. Then, the gradients of $Q$ w.r.t. different blocks\n$(W_K, W_Q, W_V, W_O, \\gamma)$ are provided in Appendix B.2. Furthermore, there exist two problem-\ndependent constants $\\Phi, \\Psi > 0$ (detailed in Appendix B.2), such that:\n$S(W_\\cdot) = O(\\frac{\\Phi^2}{D^2 ||W_\\cdot||^2}), \\bullet \\in \\{K, Q\\}$;\n$S(W_\\cdot) = O(\\frac{\\Psi^2}{D^2 ||W_\\cdot||^2}), \\bullet \\in \\{V, O\\}$;\n$S(\\gamma) = O(\\frac{\\Phi^2 + \\Psi^2}{D ||\\gamma||^2})$\nwhere the denominators ($D^2$ or $D$) reflect the number of parameters in each group."}, {"title": "Theorem 4.3 (Emb v.s. Norm).", "content": "Consider the embedding layer and its adjoint normalization layer\nof a transformer (2). Omitting the layer index for simplicity, let: $Y := Norm(XW_{emb}; \\gamma)$. The\ngradients of $Q$ w.r.t $W_{emb}$ and $\\gamma$ are derived in Appendix B.3. Moreover, there exists a problem-\ndependent constant $\\Psi > 0$ (also detailed in Appendix B.3), such that:\n$S(W_E) = O(\\frac{\\Psi^2}{D_d \\sum_{i \\in [d]} ||W_{E_i}||^2})$;\n$S(\\gamma) = O(\\frac{\\Psi^2}{D ||\\gamma||^2})$"}, {"title": "5 The Blockwise LR Strategy", "content": "Recalling Figure 3, the sharpness disparity across different blocks, as described in (1), emerges\nearly in training and persists until convergence. This insight can be leveraged to accelerate LLM\npre-training, as elaborated later.\nFast-slow dynamics at EoS. As discussed in Section 2, recent studies (Wen et al., 2024; Song\net al., 2024; Wang et al., 2024a) have highlighted the distinct roles of the dynamics along high- and\nlow-sharpness directions during EoS. The main picture is summarized as follows:\nFast dynamics: Along high-sharpness directions, the optimizer exhibits significant fluctuations\nwithout converging or diverging. These components of dynamics govern training stability, as\nfurther increasing the LR in these directions can lead to instability, while contributing little to\nloss reduction.\nSlow dynamics: Along low-sharpness directions, the optimizer progresses steadily, making the\nprimary contribution to loss reduction, albeit at a slow rate.\nInspired by the above picture, a promising approach to accelerating training is as follows: given a base\noptimizer, increase the LRs along low-sharpness directions while keeping the LR of high-sharpness\ndirections unchanged. This strategy aims to speed up loss reduction without compromising training\nstability.\nWang et al. (2024a) has implemented this idea by adjusting the LR of each parameter based on its\nsharpness. However, this approach faces two key challenges: 1) it requires frequent diagonal Hessian\nestimation, which imposes significant computational and memory overhead; 2) sharpness estimates\nat the individual parameter level can be unreliable.\nThe Blockwise LR. Unlike Wang et al. (2024a), we propose adjusting LRs at the block-type level, as\nour Principle (1) reveals a consistent sharpness disparity at this granularity. Specifically, let $\\eta_{base}$\ndenote the LR for base optimizers such as AdamW, the LR for each block type is then adjusted as\nfollows:\nNorm blocks (the sharpest directions): we still use the base LR, $\\eta_{Norm} = \\eta_{base}$, to keep training\nstability;\nOther blocks (low-sharpness directions): we adjust the LRs of these blocks by $\\eta_\\bullet = r(\\bullet) \\cdot \\eta_{base}$,\nwhere $\\bullet \\in \\{Emb, QK, FFN, VO\\}$, where $r(\\bullet)$ denotes the adjusting ratio for the block type $\\bullet$.\nNaturally, we can set $r(\\bullet) \\propto S(Norm)/S(\\bullet)$. However, in practice, we find that manually tuning\nr()'s-involving only four hyperparameters-while following the qualitative trend described by\nPrinciple (1) is more effective. Further details are provided in Section 6.\nIt is also worth noting that due to its simplicity, Blockwise LR can be seamlessly integrated into\nmodern LLM training frameworks such as Megatron (Shoeybi et al., 2019)."}, {"title": "6 Experiments", "content": "Models and datasets. We evaluate our proposed Blockwise LR in the pre-training of decoder-only\nLLMs across various model types, model sizes, and datasets. Specifically, we consider two widely\nused LLMs: LLAMA and GPT-2; we experiment with model sizes ranging from 0.12B to 1.1B"}, {"title": "7 Conclusion and Outlook", "content": "In this paper, we uncovered a sharpness disparity principle among different types of blocks in\ntransformers, as formalized in Eq. (1). Notably, this blockwise sharpness disparity persists throughout\nthe entire training process, except during the initial few steps. Building on this discovery, we proposed\na novel Blockwise LR adjustment principle, which effectively accelerates base optimizers such as\nAdamW and Adam-mini in LLM pre-training tasks.\nFuture works. It would be valuable to investigate the applicability of our Blockwise LR to non-LLM\ntasks, such as computer vision, and its compatibility with other optimizers, such as Muon (Keller\net al., 2024) and other alloy-like architectures such as Mamba (Gu and Dao, 2023). Furthermore,\nour findings open up opportunities to develop other block-adaptive optimization strategies, such as\nblockwise weight decay and gradient clipping, which could further enhance training efficiency and\nperformance."}]}