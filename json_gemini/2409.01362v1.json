{"title": "Correlating Time Series with Interpretable Convolutional Kernels", "authors": ["Xinyu Chen", "HanQin Cai", "Fuqiang Liu", "Jinhua Zhao"], "abstract": "This study addresses the problem of convolutional kernel learning in univariate, multivariate, and multidimensional time series data, which is crucial for interpreting temporal patterns in time series and supporting downstream machine learning tasks. First, we propose formulating convolutional kernel learning for univariate time series as a sparse regression problem with a non-negative constraint, leveraging the properties of circular convolution and circulant matrices. Second, to generalize this approach to multivariate and multidimensional time series data, we use tensor computations, reformulating the convolutional kernel learning problem in the form of tensors. This is further converted into a standard sparse regression problem through vectorization and tensor unfolding operations. In the proposed methodology, the optimization problem is addressed using the existing non-negative subspace pursuit method, enabling the convolutional kernel to capture temporal correlations and patterns. To evaluate the proposed model, we apply it to several real-world time series datasets. On the multidimensional rideshare and taxi trip data from New York City and Chicago, the convolutional kernels reveal interpretable local correlations and cyclical patterns, such as weekly seasonality. In the context of multidimensional fluid flow data, both local and nonlocal correlations captured by the convolutional kernels can reinforce tensor factorization, leading to performance improvements in fluid flow reconstruction tasks. Thus, this study lays an insightful foundation for automatically learning convolutional kernels from time series data, with an emphasis on interpretability through sparsity and non-negativity constraints.", "sections": [{"title": "1 INTRODUCTION", "content": "TIME series data are one of the most important data types encountered in real-world systems, capturing intrinsic temporal correlations and patterns that are essential for understanding and forecasting various phenomena. Accurately modeling these correlations and patterns is fundamental in many domains, such as spatiotemporal prediction and control systems. To achieve this, it is common to formulate time series coefficients using both linear and nonlinear machine learning approaches, in the meantime providing a flexible framework for analyzing and predicting time-dependent behaviors. In statistics, autoregression (AR) models have been extensively applied to time series analysis, offering an efficient approach to modeling temporal dependencies [1], [2]. The classical AR framework also leads to vector autoregression (VAR) for multivariate time series, which captures the interdependencies among a sequence of time series [2]. One classical counterpart that takes the form of VAR\u2013dynamic mode decomposition [3], [4], [5], [6]\u2014combines the concepts from fluid dynamics and machine learning to characterize complex dynamical systems. This method is effective in applications such as fluid flow analysis, where it decomposes the dynamics into a set of modes that describe the system's behavior over time.\nWhen applying AR to a circulant time series\u2014where the AR order equals the length of the time series\u2014the AR operation can be equivalently viewed as a circular convolution between the time series and its coefficients. The convolution operation is vital for filtering and signal processing tasks [7], where the convolution theorem relates the convolution in the time domain to multiplication in the frequency domain via the discrete Fourier transform. Recent advancements in machine learning have further expanded the use of convolution operations in sequence modeling [8]. Convolutional kernel methods such as Laplacian convolutional representation [9] enable characterizing the complex temporal dependencies. To summarize, the aforementioned regression methods, including AR, VAR, and convolution, take linear equations in an unsupervised learning framework.\nAnother aspect of enhancing model interpretability in machine learning is using sparsity-induced norms. Typically, structured sparsity regularization offers an effective way to select features and improve model interpretability [10]. The LASSO method [11] is particularly useful for identifying key features when only a subset of features (i.e., input variables) is relevant or correlated with the target variables (i.e., output variables), as it lets the coefficients of irrelevant feature be zero. Since sparsity-inducing norms such as the lo- and l\u2081-norm enforce sparsity patterns, the resulting algorithms are particularly useful for tasks such as sparse signal recovery [12], outlier detection [13], variable selection in genetic fine mapping [14], and nonlinear system identification [15], to name but a few.\nHowever, manually designed kernels (e.g., kernels refer-"}, {"title": "2 RELATED WORK", "content": "2.1 Solving Sparse Regression\nIn the fields of signal processing and machine learning, a classical optimization problem involves learning sparse representations [12] from a linear regression model with measurements $x \\in R^m$ and $A \\in R^{m \\times n}$, such that\n$\\min_{w} ||x - Aw||_2$\ns.t. $||w||_0 <\\tau, \\tau \\in Z^+$,\n(1)\nis of great significance for many scientific areas (e.g., compressive sensing [12], [18]) due to the lo-norm on the decision variable w, which counts the number of non-zero entries. As shown in Figure 1, it becomes a classical least squares problem [19] if \u00e6 and A are known variables and the vector w is not required to be sparse. In the fields of signal processing and information theory, a large manifold of iterative methods and algorithms have been developed for this problem because the problem (1) is typically NP-hard. These include some of the most classical iterative greedy methods, such as orthogonal matching pursuit (OMP) [20], [21], compressive sampling matching pursuit (CoSaMP) [22], and subspace pursuit (SP) [17]. Both CoSaMP and SP are fixed-cardinality methods whose support set has a fixed cardinality, while the support set in OMP is appended incrementally during the iterative process. In the case of inferring causality, if the matrix A has n explanatory variables, then the sparse regression problem becomes a classical variable selection technique [14]. When w is assumed to be non-negative, the methods derived from OMP, such as non-negative orthogonal greedy algorithms, require one to resolve the non-negative least squares problem [23], [24].\n2.2 Learning Kernels from Time Series\nIn the field of statistics, time series problems have been well investigated via the use of AR methods [2]. The coefficients in the AR methods represent the correlations at different times. For certain purposes such as modeling of local temporal dependencies, time series smoothing using random walk can minimize the errors of first-order differencing on the time series. Instead of time series smoothing, Laplacian kernels are more flexible for characterizing the temporal dependencies [9], in which the temporal modeling is in the form of a circular convolution between Laplacian kernel and time series. The probability product kernel, constructed based on probabilistic models of the time series data, can evaluate exponential family models such as multinomials and Gaussians and yields interesting nonlinear correlations [25]. The auto-correlation operator kernel can discover the dynamics of time series by evaluating the difference between autocorrelations [26]. Besides, Gaussian elastic matching kernels possess a time-shift and nonlinear representation in time series analysis [27]. However, setting the aforementioned kernels requires certain assumptions and prior knowledge,"}, {"title": "3 PRELIMINARIES", "content": "In this work, we summarize the basic symbols and notations in Table 1. Here, R denotes the set of real numbers, while Z+ refers to the set of positive integers. The definitions of tensor unfolding and modal product (or mode-k product as shown in Table 1) are well explained in [28], [29]. For those symbols and notations related to tensor computations, we also follow the conventions in [28], [29].\nIn particular, circular convolution is essential when dealing with periodic signals and systems [6], and it is also an important operation in this work. Given two vectors $\\theta = (\\theta_1, \\theta_2,\\dots,\\theta_T)^T \\in R^T$ and $x = (x_1,x_2,\\dots,x_T)^T \\in R^T$ of length T, the circular convolution of \u03b8 and \u00e6 is denoted by\n$y = \\theta * x \\in R^T$,\n(2)\nelement-wise, this gives\n$y_t = \\sum_{k \\in [T]} \\theta_{t-k+1}x_k, \\forall t \\in [T]$,\n(3)\nwhere yt is the tth entry of y, and $\\theta_{t-k+1} = \\theta_{t-k+1+T}$ for t +1 < k. Since the results of circular convolution are computed in a circulant manner, the circular convolution can therefore be rewritten as a linear transformation using a circulant matrix. In this case, we have\n$y = \\theta * x = x * \\theta = C(x)\\theta$,\n(4)\nwhere C : RT \u2192 RT\u00d7T denotes the circulant operator [9], [30]. For example, on the vector x \u2208 RT, the circulant matrix can be written as follows,\n$C(x) = \\begin{pmatrix}x_1 & x_T & x_{T-1} & \\dots & x_2\\\\x_2 & x_1 & x_T & \\dots & x_3\\\\x_3 & x_2 & x_1 & \\dots & x_4\\\\\\vdots & \\vdots & \\vdots & & \\vdots\\\\x_T & x_{T-1} & x_{T-2} & \\dots & x_1\\end{pmatrix} \\in R^{T \\times T}$.\n(5)"}, {"title": "4 METHODOLOGIES", "content": "In this study, we present a convolutional kernel learning method to characterize the temporal patterns of univariate, multivariate, and multidimensional time series data. First, we formulate the optimization problem for learning temporal kernels as a linear regression with sparsity and non-negativity constraints. Then, we solve the optimization problem by using the non-negative SP method.\n4.1 On Univariate Time Series\n4.1.1 Model Description\nIn real-world systems, time series often exhibit complex correlations among both local and nonlocal data points. In this study, we propose characterizing the time series correlations using circular convolution, an approach inspired by the temporal regularization with Laplacian kernels introduced in [9]. Formally, for the univariate time series x = (x1,x2,\u2026,xT)T \u2208 RT with T time steps, we formulate the learning process as an optimization problem. The objective function involves the circular convolution (denoted by *) between the temporal kernel \u03b8 (i.e., convolutional kernel) and the time series x, i.e.,\n$\\min ||\\Theta*x||^2$\nw\u22650\ns.t. $\\begin{cases} \\Theta[1] = 1\\\\ ||w||_0 \\le \\tau, \\tau \\in Z^+ ,\\end{cases}$\n(6)\nin which the (7 + 1)-sparse kernel 0 is designed to capture temporal correlations. In the parameter setting, we assume the first entry of 0 is 1, while the remaining T - 1 entries are non-positive values, parameterized by non-negative vector w \u2208 RT-1. The sparsity constraint applies to w, allowing no more than 7 positive entries, where t is referred to as the sparsity level. The sparsity assumption is meaningful for parameter pruning, preserving only the most remarkable coefficients to characterize local and nonlocal temporal patterns. In the circular convolution + x within the objective function, the temporal kernel 8 can also be interpreted as a graph filter with time-shift operator, as seen in the field of graph signal processing (e.g., [31], [32]). By leveraging the property of circular convolution, 0 + x = Ox, the matrix \u2299 \u2208 RT\u00d7T can be expressed as a matrix polynomial:\n$\\Theta = I_T - w_1F - w_2F^2 - ... - w_{T-1}F^{T-1}$,\n(7)\nwith the T-sparse representation (i.e., a sequence of coefficients)\nw = (w_1, w_2,...,w_{T-1})^T \\in R^{T-1},\n(8)\nand the time-shift matrix\n$F = \\begin{pmatrix}0&0&0&\\dots&0&1\\\\1&0&0&\\dots&0&0\\\\0&1&0&\\dots&0&0\\\\\\vdots&\\vdots&\\vdots& &\\vdots&\\vdots\\\\0&0&0&\\dots&1&0\\\\ \\end{pmatrix} \\in R^{T \\times T}$.\n(9)\nHerein, IT is the identity matrix of size T \u00d7 \u0422.\nAs a result, we can express the temporal kernel 0 in the circular convolution 0 * \u00e6 and the corresponding circulant matrix O in the matrix-vector multiplication O\u00e6 as follows,\n$\\theta = (1, -w_1,-w_2,\\dots,-w_{T-1}) = \\begin{pmatrix}1\\\\-w\\\\\\end{pmatrix}$,\n(10)\nand\n$\\Theta = \\begin{pmatrix}1 & -w_1 & -w_2 & -w_3 & \\dots & -w_{T-1}\\\\-w_{T-1} & 1 & -w_1 & -w_2 & \\dots & -w_2\\\\ -w_2 & -w_{T-1} & 1 & -w_1 & \\dots & -w_3\\\\\\vdots & \\vdots & \\vdots & \\vdots & & \\vdots\\\\-w_1 & -w_2 & -w_3 & \\dots & -w_{T-1} & 1\\end{pmatrix}$,\n(11)\nrespectively. Due to the property of circulant matrix, the temporal kernel 0 is indeed the first column of matrix \u0398. As mentioned above, the temporal kernel O can be reinforced for capturing local and nonlocal correlations of time series automatically. Using the structure of 0 described in Eq. (10), the problem (6) is equivalent to\n$\\min ||x - Aw||$\nw\u22650\ns.t. $||w||_0 \\le \\tau, \\tau \\in Z^+$,\n(12)\nwhere the auxiliary matrix A is comprised of the last T \u2013 1 columns of the circulant matrix C(x) \u2208 RT\u00d7T (see Eq. (5)), namely,\n$A = \\begin{pmatrix}x_T & x_{T-1} & x_{T-2} & \\dots & x_2\\\\x_1 & x_T & x_{T-1} & \\dots & x_3\\\\x_2 & x_1 & x_T & \\dots & x_4\\\\\\vdots & \\vdots & \\vdots & & \\vdots\\\\x_{T-2} & x_{T-3} & x_{T-4} & \\dots & x_T\\\\x_{T-1} & x_{T-2} & x_{T-3} & \\dots & x_1\\\\ \\end{pmatrix} \\in R^{T \\times (T-1)}$.\n(13)\nAs can be seen, one of the most intriguing properties is the circular convolution 0 * \u00e6 can be converted into the expression x Aw, which takes the form of linear regression, as illustrated in Figure 2. Thus, our problem aligns with sparse linear regression on the data pair {x, A} in Figure 1, if not mentioning the non-negativity constraint.\n4.1.2 Solution Algorithm\nTo solve the optimization problem in Eq. (12), one should consider both non-negativity and sparsity of the vector w. In this study, we present Algorithm 1 as the implementation using a non-negative SP method that adapted from [17], where non-negative least squares is treated as a subroutine.\nThe temporal kernel 0 is constructed using the T-sparse representation w (see Eq. (10)). Here, S = supp(w) = {t : wt \u2260 0} represents the support set of the vector w, with S denoting the cardinality of S. Notably, we compute ar, Vi \u2208 'i \u2208 [T \u2013 1], where the vector a\u017c is defined as\n$a_i = (x_{T-i+1}, \\dots, x_T, x_1, \\dots, x_{T-i})^T \\in R^T$,\n(14)\nwhere the entries of the first phase start from XT-i+1 to \u0445\u0442, as the remaining T \u2013 i entries start from x\u2081 to XT-i. Such structure is consistent with the matrix A in Eq. (13).\nThus, it suffices to compute the linear transformation\n$A_S w_S = \\sum_{l \\in S} w_l a_l$,\n(17)\nin a memory-efficient manner. Since the matrix A is derived from the circulant matrix C(x), it is possible to avoid explicitly constructing a memory-consuming matrix of size T \u00d7 (T \u2212 1). In some extreme cases, where the time series is particularly long, directly computing with T \u00d7 (T \u2212 1) matrices becomes challenging."}, {"title": "4.2 On Multivariate Time Series", "content": "4.2.1 Model Description\nFor univariate time series, a t-sparse representation w \u0395 RT-1 can effectively capture temporal correlations and patterns. However, for multivariate time series, the case becomes more complicated because it is unnecessary to learn a separate T-sparse representation for each individual time series. Instead, a single sparse vector w \u2208 RT-1 is expected to capture consistent correlations and patterns across all time series. For any multivariate time series X \u2208 RN\u00d7T, where N and T are the number and the length of time series, respectively, the learning process of the temporal kernel \u03b8 can be formulated as follows,\n$\\min \\sum_{n \\in [N]} ||\\Theta*x_n||$\nw\u22650\n$\\Theta = \\begin{pmatrix}1\\\\-w\\\\\\end{pmatrix}$,\ns.t. $||w||_0 \\le \\tau, \\tau \\in Z^+$,\n(18)\nwhere xn \u2208 RT is the n-th row vector of X, corresponding to a single time series. In the objective function, according to the property of circular convolution in Eq. (4), the circular convolution takes the following form:\n$\\Theta * x_n = C(x_n)\\Theta = X_n A_n w$,\n(19)\nwith An \u2208 RT\u00d7(T-1) consisting of the last T 1 columns of the circulant matrix C(xn). By constructing the matrix An for each time series an independently, we propose representing An, n \u2208 [N] as slices of a newly constructed tensor A\u2208 RNXT\u00d7(T\u22121). Equivalently, we have\n$\\min ||X - A \\times_3 w ||$\nw\ns.t. $||w||_0 \\le \\tau, \\tau \\in Z^+$,\n(20)\nwhere \u00d73 denotes the modal product along the third mode, namely, mode-3 product. In this case, we have a linear regression with known time series matrix X and dictionary tensor A. The regression expression is particularly written with the modal product.\nBy utilizing the properties of tensor unfolding and modal product as described in [28], the optimization problem in Eq. (20) can be equivalently expressed as the following one:\n$\\min || vec(X) - A_{(3)} w||$\nw\u22650\ns.t. $||w||_0 \\le \\tau, \\tau \\in Z^+$,\n(21)\nwhere vec() denotes the vectorization operator. In tensor computations, A(3) is the tensor unfolding of A at the third dimension, resulting in A(3) having dimensions (T \u2013 1) \u00d7 (NT). As can be seen, the original regression problem in Eq. (20) is actually converted into a standard sparse regression problem that is analogous to Eq. (12). Consequently, the previously mentioned algorithm can be applied to the multivariate time series case.\n4.2.2 Solution Algorithm\nBefore using Algorithm 1, it is necessary to adjust the algorithm settings. There are some procedures to follow: 1) Set x := vec(X) \u2208 RNT as the input. 2) Compute the inner product ar, Vi \u2208 [T \u2013 1], where we have\n$a = (x_{T-i+1}^T, \\dots, x_T^T, x_1^T, \\dots, x_{T-i}^T)^T \\in R^{NT}$,\n(22)\nwhere xt \u2208 RN, t \u2208 [T] are the column vectors of X \u2208 RNXT. In the vector ar, the entries of the first phase start from XT-i+1 to xT, as the remaining N(T \u2013 i) entries start from x1 to xT-i. Notably, this principle is analogous to Eq. (14).\nFinally, suppose S = {l1,2,..., ls|} be the support set, then the most important procedure is constructing the sampling matrix As \u2208 R(NT)\u00d7|S|, which consists of the selected columns of A(3) \u2208 R(NT)\u00d7(T-1) corresponding to the index set S. This matrix is given by\n$A_s = \\begin{pmatrix}a_{l_1} & a_{l_2} & & a_{l_{|s|}}\\\\&\\end{pmatrix}$.\n(23)\nIn this case, if i \u2208 S represents the index i in the support set S, then constructing a\u017c in Eq. (22) allows one to build the column vectors of As.\n4.3 On Multidimensional Time Series\nFor any multidimensional time series X \u2208 RM\u00d7N\u00d7T in the form of a tensor, we use the tensor fiber Xm,n,: \u2208 RT to represent each individual time series of length T. The challenge is to learn a temporal kernel 0 from matrix-variate time series. To address this, we propose formulating the circular convolution as 0 * Xm,n,: over m\u2208 [M] and n\u2208 [N]. Consequently, the optimization problem can be written as follows,\n$\\min \\sum_{m \\in [M]} \\sum_{n \\in [N]} ||\\Theta*x_{m,n,:}||\n$\\Theta = \\begin{pmatrix}1\\\\-w\\\\\\end{pmatrix}$,\nw>0\ns.t. $||w||_0 \\le \\tau, \\tau \\in Z^+$,\n(24)"}, {"title": "5 EXPERIMENTS", "content": "In this section, we evaluate the proposed method for learning convolutional kernels using real-world time series data. In what follows, we consider several multidimensional time series datasets, including the rideshare and taxi trip data collected from New York City (NYC) and Chicago, which capture human mobility in urban areas, as well as fluid flow dataset that shows temporal dynamics. We use these datasets to identify interpretable temporal patterns and support downstream machine learning tasks, such as tensor completion in fluid flow analysis.\n5.1 On Human Mobility Data\nHuman mobility in urban areas typically exhibits highly periodic patterns on a daily or weekly basis, with a significant number of trips occurring during morning and afternoon peak hours and relatively fewer trips during off-peak hours. The NYC TLC trip data provides records of rideshare and taxi trips projected onto the 262 pickup/dropoff zones across urban areas.\u00b9 Each trip is recorded with spatial and temporal information, including pickup time, dropoff time, pickup zone, and dropoff zone. For privacy concerns, the detailed trajectories (e.g., latitude and longitude) of rideshare vehicles and taxis are removed. By aggregating these trips on an hourly basis, the trip data can be represented as mobility tensors such as X of size M \u00d7 N \u00d7 T, in which the number of zones is M = N = 262. For numerical experiments, we choose the datasets covering the first 8 weeks starting from April 1, 2024. As a result, the number of time steps is T = 8 \u00d7 7 \u00d7 24 = 1344.\n5.2 On Fluid Flow Data\n5.2.1 Learning Convolutional Kernels\nThe dynamics of fluid flow often exhibit complicated spatiotemporal patterns, allowing one to interpret convolutional kernels in the context of temporal dynamics. We use a fluid flow dataset collected from the fluid flow passing a circular cylinder with laminar vortex shedding at Reynolds number, using direct numerical simulations of the Navier-Stokes equations. This dataset is a multidimensional tensor of size 199 \u00d7 449 \u00d7 150, representing 199-by-449 vorticity fields with 150 time snapshots as shown in Figure 7.\nSpecifically, the circulant matrix is defined for each time series Xm,n,: independently. Thus, the problem takes the form of tensor regression with known variables being tensors. By utilizing the properties of tensor unfolding and modal product, we can find an equivalent optimization as follows,\n$\\min || vec(X) - A_{(4)} w||^2$\nw\u22650\ns.t. $||w||_0 \\le \\tau$,\n(26)\nwhere the vectorization on the third-order tensor x is vec(x) = vec(X (1)) with the tensor unfolding of X at the first dimension being X (1) \u2208 RM\u00d7(NT). The matrix A(4) is the tensor unfolding of A at the fourth dimension, which is of size (T-1) \u00d7 (MNT).\nAs mentioned above, the optimization problem can be converted into an equivalent sparse regression on data pair {vec(X), A(4)} using vectorization and tensor unfolding operations. In the algorithmic implementation, the vector a\u017c \u2208 RMNT used in inner product ar, Vi \u2208 [T \u2013 1] can be defined as follows,\n$a_i = (vec(X_{T-i+1}), \\dots, vec(X_T), vec(X_1), ..., vec(X_{T-i})) \\in R^{MNT}$,\n(27)\nwhere Xt \u2208 RM\u00d7N, t \u2208 [T] are the frontal slices of the tensor X \u2208 RM\u00d7N\u00d7T. In this case, we introduce the vectorization operation to make the vector a\u017c identical to the ith column of the matrix A(4) A(4) \u2208 R(MNT)\u00d7(T-1). The essential idea of constructing a can be generalized to the column vectors of As by letting i \u2208 S over a sequence of indices in the support set S.\nTherefore, the optimization problem now becomes\n$\\min ||x - A \\times_4 w ||_F$\nw\u22650\ns.t. $||w||_0 \\le \\tau$,\n(25)\nwhere A ERM\u00d7N\u00d7T\u00d7(T\u22121) is a fourth-order tensor constructed from X.\n5.2.2 Fluid Flow Reconstruction with Tensor Factorization\nFor any partially observed tensor Y \u2208 RM \u00d7 N\u00d7T in the form of multidimensional time series, we consider the problem of fluid flow reconstruction using CP tensor factorization which is a classical formula in tensor computations [28], [29]. To emphasize the significance of learning convolutional kernels from time series data, we reformulate the optimization problem of tensor factorization by incorporating spatiotemporal regularization terms such that\n$\\min_{W,U,V} \\frac{1}{2} ||P_{\\Omega} (Y_{(1)} - W(VU)) ||_F^2$\n$+ \\gamma (||\\partial_wW||_F + ||\\partial_UU||_F+ ||\\partial_VV||_F)$,\n(28)\nwhere (1) is the mode-1 tensor unfolding of size M \u00d7 (NT), and \u03a9 denotes the observed index set of Y(1). Since the data is partially observed, Po(\u00b7) denotes the orthogonal projection supported on \u03a9, while P\ub9d8(\u00b7) denotes the orthogonal projection supported on the complement of 2. In this tensor factorization, given a rank R \u2208 Z+, there are three factor matrices W \u2208 RM\u00d7R, U \u2208 RN\u00d7R, and V\u2208RT\u00d7R Accordingly, if one accounts for the temporal correlations, the matrix \u2208 RTXT is the circulant matrix with the first column being the temporal kernel \u03b8 \u2208 RT. Instead of temporal kernel 0, the proposed method can also learn the spatial kernels \u03b8w and ou from the fluid flow data. Thus, one can construct the spatial regularization terms with matrices Ow \u2208 RM\u00d7M and \u0472u \u2208 RN\u00d7N. Notably, these regularization terms are weighted by y \u2208 R.\nThe optimization problem in Eq. (28) can be solved by the alternating minimization method, in which the variables {W,U,V} would be updated iteratively with the following principle:\n$\\begin{cases} W := {W | \\partial f/\\partial W = 0},\\\\U := {U | \\partial f/\\partial U = 0},\\\\V := {V | \\partial f/\\partial V = 0},\\end{cases}$\n(29)\nwhere the objective function is denoted by f. Each sub-problem can be resolved by the conjugate gradient method efficiently [33]."}, {"title": "6 CONCLUSION", "content": "In this study, we propose a unified machine learning framework for temporal convolutional kernel learning to model univariate, multivariate, and multidimensional time series data and capture interpretable temporal patterns. Specifically, the optimization problem for learning temporal kernels is formulated as a linear regression with T-sparsity (i.e., using lo-norm on the sparse representation w) and non-negativity constraints. The temporal kernel @ takes the first entry as one and the remaining entries as -w. To ensure the interpretable temporal kernels, the constraints in optimization are solved by the non-negative SP method, which is well-suited to produce a sparse and non-negative sparse representation w.\nIn the modeling process, the challenge arises as the time series switched from univariate cases to multivariate and even multidimensional cases due to the purpose of learning a single kernel 0 from a sequence of time series. To address this, we propose formulating the optimization problem with tensor computations, involving both modal product and tensor unfolding operations in tensor computations. Eventually, we show that the optimization for multivariate and multidimensional time series can be converted into an equivalent sparse regression problem. Thus, the non-negative SP method can be seamlessly adapted for solving these complex optimization problems.\nThrough evaluating the proposed method on the real-world human mobility data, we show the interpretable temporal kernels for characterizing multidimensional rideshare and taxi trips in both NYC and Chicago, allowing one to uncover the local and nonlocal temporal patterns such as weekly periodic seasonality. The comparison between different cities and transportation modes provides insightful evidence for understanding the periodicity of urban systems. On the fluid flow data, convolutional kernels that obtained along spatial and temporal dimensions can reinforce the tensor completion in fluid flow reconstruction problems.\nAlthough this work focuses on how to learn a temporal kernel from univariate, multivariate, and multidimensional time series data, the essential idea can be easily generalized to other machine learning tasks on relational data. For future work, possible directions for extending the proposed methods include: 1) Learning convolutional kernels from sparse or irregular time series due to the challenge of biased sampling of data points. 2) Inferring causality from time series data."}]}