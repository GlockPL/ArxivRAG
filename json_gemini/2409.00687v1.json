{"title": "When Heterophily Meets Heterogeneous Graphs: Latent Graphs Guided Unsupervised Representation Learning", "authors": ["Zhixiang Shen", "Zhao Kang"], "abstract": "Unsupervised heterogeneous graph representation learning (UHGRL) has gained increasing attention due to its significance in handling practical graphs without labels. However, heterophily has been largely ignored, despite its ubiquitous presence in real-world heterogeneous graphs. In this paper, we define semantic heterophily and propose an innovative framework called Latent Graphs Guided Unsupervised Representation Learning (LatGRL) to handle this problem. First, we develop a similarity mining method that couples global structures and attributes, enabling the construction of fine-grained homophilic and heterophilic latent graphs to guide the representation learning. Moreover, we propose an adaptive dual-frequency semantic fusion mechanism to address the problem of node-level semantic heterophily. To cope with the massive scale of real-world data, we further design a scalable implementation. Extensive experiments on benchmark datasets validate the effectiveness and efficiency of our proposed framework. The source code and datasets have been made available at https://github.com/zxlearningdeep/LatGRL.", "sections": [{"title": "I. INTRODUCTION", "content": "Heterogeneous graphs, prevalent in various domains such as social networks, bibliographic networks, and knowledge graphs, represent complex semantic relationships [1]. In a heterogeneous graph, nodes represent entities of multiple types and edges demonstrate various kinds of relations. Most cutting-edge methods model heterogeneous graphs based on meta-paths [2]. These techniques employ predefined meta-paths to extract a series of homogeneous graphs that pertain to a specific type of nodes for subsequent node representation learning, which aligns with the principles of multi-view graph learning [3]. Recently, Unsupervised Heterogeneous Graph Representation Learning (UHGRL) has gained considerable attention due to its importance in handling large amounts of real-world graphs [4]. UHGRL leverages unsupervised techniques, such as contrastive learning, to obtain high-quality node representations, avoiding the reliance on labeled data. These representations aid in tasks such as node classification and clustering, finding practical applications in recommendation systems, and bibliographic network mining [5]. However, the semantic heterophily, which refers to the phenomenon that nodes of the same type connected by meta-"}, {"title": "II. PRELIMINARY", "content": "A. Unsupervised Heterogeneous Graph Representation Learning\nA heterogeneous graph can be defined as a graph G = (\u03bd,\u03b5,\u03c6, \u03c8), where V is the node set and E is the edge set. \u00a2 : V \u2192 T is the node-type mapping function where T = {\u03c6(\u03c5) : \u03c5v \u2208 V}, and \u03c8 : E \u2192 R is the edge-type mapping function where R = {\u03c8(e) : e \u2208 E}.\nDefinition 1. Mete-path. A meta-path P : T\u2081 R1\u2192 T2 R2, \u00b7\u00b7\u00b7Ri Ti+1 (abbreviated as T\u2081T2... Ti+1) defines a composition relation R = R10 R20 \u00b7\u00b7\u00b7 0 Ri between nodes of type T\u2081 and Ti+1, where o denotes the composition operator."}, {"title": "B. Homophily versus Heterophily", "content": "Two nodes are considered similar if they share the same label [11]. In a homogeneous graph, edges connecting two"}, {"title": "C. Graph Filtering", "content": "From a spectral perspective, the Laplacian filter amplifies the high-frequency components and suppresses the low-frequency components in the graph, while affinity matrices, such as the renormalized adjacency matrix, exhibit the opposite behavior [13]. On the other hand, from a spatial perspective, the application of filters \u0100sym and \u0139sym to the graph signal x \u2208 RN can be interpreted as operations of aggregation and diversification. Formally:\n$(\\text{Asym}x)_i = \\frac{1}{|N_i|}\\sum_{j \\in N_i} X_j$ \n$(\\text{Lsym}x)_i = \\frac{1}{|N_i|}\\sum_{j \\in N_i}(X_i \u2013 X_j)$ \nwhere Ni is the neighbor set of node vi with self-loop. Therefore, these two operations aim to smooth and sharpen the node features, respectively, and then capture the commonalities and differences among neighborhoods [14]. Most existing GNN encoders are essentially equivalent to low-pass graph filters [15], which limits their effectiveness to graphs with high homophily levels. Some research has attempted to introduce high-pass filters to handle heterophilic graphs [16]. These filters can sharpen the features of nodes across heterophilic edges, thereby avoiding the ambiguity of representations between nodes of different categories."}, {"title": "III. EMPIRICAL STUDY", "content": "We propose the notion of Semantic Homophily in heterogeneous graphs, which refers to the tendency for nodes of the same type, connected by meta-paths, to possess similar features or identical labels. Conversely, when nodes of the same type exhibit dissimilar features or different labels, we refer to it as Semantic Heterophily. To manifest the distinctions among different semantic relations, we calculate the homophily ratio separately for each homogeneous meta-path subgraph extracted from different meta-paths and refer to it as the meta-path-level semantic homophily ratio.\nDefinition 3. Meta-path-level Semantic Homophily ratio (MHR). Given a meta-path \u03a6 = T1T2\u2026\u2026 Ti+1 with T\u2081 = Ti+1"}, {"title": "IV. UHGRL UNDER SEMANTIC HETEROPHILY", "content": "To overcome the above challenges, we propose Latent Graphs Guided Unsupervised Representation Learning (Lat-GRL). As illustrated in Fig. 3, LatGRL comprises three main modules: (1) construction of latent graphs by coupling structure and feature similarity, (2) adaptive dual-frequency semantic fusion using dual-pass graph filtering, and (3) latent graphs guided learning that maximizes mutual information between the original semantically rich graph and latent graphs. Through the collaborative efforts of these three components, LatGRL performs adaptive semantic fusion and generates high-quality representations for nodes."}, {"title": "A. Latent Graphs Construction", "content": "Inspired by recent research on unsupervised homogeneous graph learning that leverages prior structures and features for positive sampling [20], we present a similarity mining approach that couples structures and features in heterogeneous graphs to achieve superior positive sample extraction.\nTwo nodes sharing similar neighborhood structures often possess identical characteristics and labels [21]. Hence, we begin by focusing on structure similarity and propose a simple yet effective methodology. Within the heterogeneous graphs, gauging the structure similarity amongst nodes necessitates a comprehensive evaluation of their topological structures spanning various relations. Based on this, we propose the concept of global structure similarity. Formally, the similarity of the global structure between two nodes of identical type is evaluated by considering all meta-path subgraphs {$G_\u03a6$ | p = 1,2,..., P}:\n$M^\u03a6p = \u010e_{\u03a6p}\u00c3^\u03a6p, M = \\frac{1}{P}\\sum_{p=1}^P M^\u03a6p$, \n$\\text{sim} (v_i, v_j) = \\frac{(M_{i.}, M_{j.})}{\\|| M_{i.} \\|| \\|| M_{j.} \\||}$ \nwhere $M^\u03a6p$ represents random walk normalized adjacency matrix of meta-path subgraph $G_\u03a6$. We obtain the probability diffusion matrix M by averaging over all meta-paths. The element of M represents the combined probability of each node randomly walking to other nodes through multiple relations, capturing its local structural information. The structure similarity between two nodes is defined as the cosine value of their diffusion vectors. A larger value indicates a higher proportion of shared neighbors along all meta-paths."}, {"title": "B. Adaptive Dual-frequency Semantic Fusion", "content": "Due to the numerous heterophilic connections, we introduce high-pass graph filtering to sharpen the features of neighboring nodes, thereby avoiding the confusion of representations from different categories. Learning complex filters often requires supervision signals from label information, which is not suitable for unsupervised tasks [22]. In contrast, we propose a simple and interpretable mechanism. We use \u0100sym as the low-pass filter to capture low-frequency information and preserve commonalities among neighborhoods. Simultaneously, Lsym is employed as the high-pass filter to retain high-frequency information and sharpen the representations of nodes along the edges. Graph filtering is applied on the node embedding:\n$H^\u03a6p,l = (\u0100^{sym})^r f(X)$\n$H^\u03a6p,h = (L^{sym})^r f(X)$\n$\u0124^\u03a6p,l = norm(H^\u03a6p,l), \u0124^\u03a6p,h = norm(H^\u03a6p,h)$\nwhere f(.): Rdf \u2192 Rd denotes the node encoder, which is implemented using MLP in this study. $H^\u03a6p,l$ and $H^\u03a6p,h$ are the low-frequency smoothed representations and the high-frequency sharpened representations in the context of meta-path \u03a6p, respectively. r is the filter order. To eliminate the influence of representation magnitude, we apply L2 Normalization to each node representation, given as $norm(h) = \\frac{h}{\\|| h \\||_2}$. Subsequently, the two representations of each meta-path view are collectively fed into a shared decoder g(\u00b7) : Rd \u2192 Rdf. The Scaled Cosine Error (SCE) [23] is employed as the reconstruction loss to ensure that the representations of each meta-path preserve sufficient information. The contribution of simple samples can be controlled by adjusting the sharpening parameter \u03b3:\n$\\text{LSCE} = \\frac{1}{NP}\\sum_{p=1}^P\\sum_{i=1}^N [1 - \\frac{<X_{i.}, X^P_{i.}>}{\\|| X_{i.} \\|| \\|| X^P_{i.} \\|| } ]_{+}$,\n$X^P = g([\u0124^\u03a6p,l || \u0124^\u03a6p,h])$\nwhere [ ||] denotes the row-wise concatenation operation. The decoder is also implemented using an MLP in this study.\nSection III emphasizes that the node-level semantic homophily ratios based on various meta-paths exhibit diversity across distinct nodes. This finding underscores the necessity of employing an adaptive encoding mechanism to handle the varied patterns of node neighborhoods. Therefore, we propose an innovative approach, dubbed the adaptive dual-frequency semantic fusion method for node-wise modeling:\n$w^l_{i,p} = \u03c3(q_l \u00b7 (\u0124^\u03a6p,l)_i), w^h_{i,p} = \u03c3(q_h \u00b7 (\u0124^\u03a6p,h)_i)$,\n$\u03b2^l_{i,p} = \\frac{\\text{exp}(w^l_{i,p})}{\\sum_{j=1}^P \\text{exp}(w^l_{j,p}) + \\sum_{j=1}^P \\text{exp}(w^h_{j,p})}$\n$\u03b2^h_{i,p} = \\frac{\\text{exp}(w^h_{i,p})}{\\sum_{j=1}^P \\text{exp}(w^l_{j,p}) + \\sum_{j=1}^P \\text{exp}(w^h_{j,p})}$\\\n$Z_i = \\sum_{p=1}^P \u03b2^l_{i,p}\u0124^\u03a6p,l + \\sum_{p=1}^P \u03b2^h_{i,p}\u0124^\u03a6p,h$\nwhere qu and qr denote the learnable attention vectors associated with low-frequency and high-frequency information, respectively.  $\u03b2^l_{i,p}$ and $\u03b2^h_{i,p}$ are the fusion weights for the low-pass and high-pass representations of node vi in the meta-path \u03a6\u03c1. \u03c3(\u00b7) is the non-linear activation function. Z\u017c represents the final representation of node vi, which would be used in the downstream tasks. Many existing UHGRL methods use shared semantic fusion weights for all nodes [7], [8]. On the contrary, our method focuses on adaptive dual-frequency fusion, tailored for nodes with different neighborhood patterns."}, {"title": "C. Latent Graphs Guided Learning", "content": "We address the lack of label information in UHGRL by exploring self-supervised signals from the data. Unlike existing methods that rely on predefined data augmentation techniques like random edge deletion or random feature removal to generate contrastive views [10], which could be harmful in heterophily graphs [12], our approach leverages latent graphs to provide valuable supervision signals. The homophilic latent graph mainly encompasses nodes with shared class neighbors, so we use low-pass filtering to extract common traits within the same category. In contrast, the heterophilic latent graph comprises nodes whose neighbors represent different categories, motivating high-pass filtering to reveal differentiating information among distinct categories. The process of latent graph encoding is as follows:\n$Z^l = (\u0100^{sym}) f(X), Z^h = (\u0139^{sym}) f(X)$\nwhere \u0100sym and Lsym are the renormalized versions. Then, we maximize the mutual information between the semantic fusion representations Z and the latent graph representations Zl and Zh, where Z\u00b9 provides communal information within the same category and Zh captures distinction characteristics among different categories. Both of them collectively guide the representation learning process. The optimization objective can be written as follows:\nmax I (Z, Z\u00b9) + I(Z, Z)\nwhere @ is the parameters of the model. The InfoNCE objective is used as the lower bound of mutual information [24]. We also employ commonly used positive sampling techniques in contrastive learning. Unlike previous methods that relied on neighboring nodes [7] or employed the KNN algorithm [25], we leverage the previously defined similarity measurement coupling global structures and features to select positive samples. Specifically, for a given node vi, the positive samples set Pi = arg top;(sim(vi, vj), kpos), which means selecting"}, {"title": "D. Scalable Implementation", "content": "To accommodate large-scale heterogeneous graph data, we develop a scalable implementation called LatGRL-S, which consists of the following two steps.\nScalable Latent Graph Construction. In the process of latent graph construction, the calculation of the similarity between each node and all other nodes is impractical for real-world graphs. Therefore, we only use first-order neighboring nodes to construct the homophilic latent graph, as the number of first-order neighbors in large-scale graphs is typically substantial. In contrast, for the heterophilic latent graph, we employ an anchor-based construction approach. Formally:\n$N^S_i = \\text{arg top}_{vj\\in N_i} (\\text{sim}^S (v_i, v_j) \u00b7 \\text{sim}^F (v_i, v_j), K)$\n$N^W_i = \\text{arg top}_{vj\\in U} ([1 \u2013 \\text{sim}^S (v_i, v_j)] \u00b7 [1 \u2013 \\text{sim}^F (v_i, v_j)], K)$\nwhere Ni = \u222ap=1 Np, represents all first-order neighbors of node vi based on all meta-paths and U is the anchor set consisting of m randomly selected nodes. NS and NW denote the neighbor sets of node vi in the homophilic and heterophilic latent graph, respectively. It is worth noting that we are not performing simple neighbor sampling. The homophilic latent graph primarily focuses on local information, constructing personalized homophilic neighborhoods for each node based on its local context. For the heterophilic latent graph, the anchor graph construction could emphasize global information to capture inter-class differences.\nMini-Batch Training with Pre-Filtering. In the implementation of LatGRL, graph filtering is performed on the embeddings, resulting in filtering calculations being executed during each training epoch. To enhance training efficiency, we first pre-filter the raw features to obtain low-frequency and high-frequency features before training. Subsequently, during the mini-batch training, the two features are fed to the encoding layer. The entire process is illustrated as follows:\nPre-Filtering:\nXpl = (Am)X, Xp,h = (Lm) X\nMini-Batch Training:\nHB\u03a6p,l = f(XB\u03a6p,l), HB\u03a6p,h = f(XB\u03a6p,h)\nZB = Fusion(H1..., HP, Hish..., Hp,h)\nwhere Hand Hoph represents the low-frequency and high-frequency representations of the sampled nodes in each batch under the meta-path \u03a6p, respectively. ZB denotes the final representations of these nodes. The node representations of latent graphs are also obtained through pre-filtering and dimensionality reduction processes. This decoupled implementation approach enhances training efficiency by eliminating the need for time-consuming and resource-intensive neighbor sampling and message aggregation operations in each mini-batch. Following LatGRL, we then employ the latent graphs guided learning method to achieve model training. The loss is computed only for the nodes within the batch."}, {"title": "V. EXPERIMENTS", "content": "A. Experimental setup\nDatasets. We employ four benchmark heterogeneous attributed graph datasets and a large-scale heterogeneous graph dataset. The statistics of these datasets are presented in Table IV.\nBaselines. We compare LatGRL with 11 other state-of-the-art methods, which can be grouped into three categories:\nSemi-supervised Learning Methods: GAT [30] incorporates neighborhood attention mechanisms for homogeneous graphs, while HAN [31] uses hierarchical attention for heterogeneous graphs.\nUnsupervised Heterogeneous Graph Representation Learning Methods: DGI [19] maximizes agreement between node representations and global representations. GraphMAE [23] is a masked graph autoencoder that incorporates graph masking and feature reconstruction. DGI and GraphMAE are both methods for homogeneous graphs. Mp2vec [32] is a shallow heterogeneous graph embedding method using meta-path-based random walks. DMGI [6] maximizes the mutual information between local and global information and includes semantic consistency constraints. HeCo [7] constructs contrastive loss between meta-paths and network schemas. MEOW [8] contrasts coarse-grained and fine-grained views and incorporates negative sample importance mining. DuaLGR [33] is a multi-view graph clustering method that introduces structural and feature pseudo labels. HGMAE [9] explores masked autoencoders in heterogeneous graphs.\nUnsupervised Learning Methods For Heterophily: GREET [25] addresses neighborhood heterophily in homogeneous graphs by using an edge discriminator to differentiate between homophilic and heterophilic edges.\nSettings. To ensure fair comparisons, we conduct 10 runs of all experiments and report the average results. For each dataset, we use the features of target nodes and set the final representation dimension to 64. For random-walk-based methods like Mp2vec, we set the number of walks per node to 40, the walk length to 100, and the window size to 5. For the homogeneous graph methods like GAT, DGI, GraphMAE, and GREET, we evaluate them on each meta-path subgraph and select the optimal results, following [7]\u2013[9]. A portion of the results for comparative methods is cited from [9].\nWe initialize parameters using Kaiming initialization [34] and train the model with Adam optimizer. We conduct experiments with different learning rates ranging from {1e-3, 5e-4} and penalty weights for L2-norm regularization from {1e-3, 1e-4, 0}. Early stopping is used with a patience of 10 epochs, stopping training if the total loss did not decrease for patience"}, {"title": "B. Performance on Node Classification", "content": "The node representations learned through unsupervised methods are used to train a linear classifier. To comprehensively assess our methods, we follow [7] and select 20, 40, and 60 labeled nodes per class as the training set, while using 1000 nodes for validation and 1000 other nodes for testing in each dataset. We employ standard evaluation metrics, including Ma-F1, Mi-F1, and AUC, where higher values indicate better model performance. As shown in Table V, among all the evaluated datasets, LatGRL achieves the best performance except for the Yelp dataset, where LatGRL-S outperforms it. Even though all other comparative methods employ full-graph training schemes, which often yield better results, LatGRL-S consistently ranks in the top three for each"}, {"title": "C. Performance on Node Clustering", "content": "In this task, we further perform the k-means algorithm to the learned representations of all nodes and adopt normalized mutual information (NMI) and adjusted rand index (ARI) to assess the quality of the clustering results. For both metrics, the larger, the better. To alleviate the instability due to different initial values, we repeat the process 10 times and report average results in Table VI. We can observe that LatGRL"}, {"title": "D. Ablation Study", "content": "1) Effectiveness of Each Loss: To examine the effectiveness of each component of LatGRL, we conduct experiments on variants of LatGRL. We first remove key components from"}, {"title": "E. Time Complexity", "content": "Due to the sparsity of real-world heterogeneous graphs, we implement graph filtering with sparse matrix techniques. For simplicity, we use the same D as the dimensionality of input features and node representations, and B as the batch size of mini-batch training and the number of anchors for scalable latent graph construction. N and E are the number of target nodes and edges. r is the filter order. Since latent graph construction only needs to be done once before training, we divide the analysis into two stages: Preprocessing and Training. For LatGRL, the complexity of the preprocessing stage is O (N2(N + D)). The training stage includes graph filtering and loss calculation, with a complexity of O (D(N\u00b2 + rE + ND)). For LatGRL-S, the preprocessing stage involves scalable latent graph construction and pre-filtering, with a complexity of O (E(N + rD) + NB(N + D)). The training stage has a complexity of O (ND(B + D)). We compare our complexity with baselines in Table III. It can be observed that existing methods require a complexity of at least O(N2) during the training phase. In contrast, our method is only linear, demonstrating the significant advantage of LatGRL-S in handling large-scale graph data."}, {"title": "F. Hyper-parameter Analysis", "content": "We further analyze the impact of two important hyper-parameters in LatGRL: the filtering order r and the number of positive samples kpos. Fig. 6 illustrates the results on the Micro-F1 scores with 40 labeled nodes per class. It can be observed that when the filtering order is relatively small, better results can be achieved. As the filtering order increases, the performance tends to decrease on most datasets. Smaller filtering orders also improve computational efficiency in practical applications.\nFurthermore, in the ACM and IMDB datasets, as kpos increases, the decrease in model performance becomes more pronounced. This could be attributed to the fact that higher values of kpos result in a decrease in the precision of the positive samples extracted. In contrast, the model is not sensitive to changes in kpos in the remaining two datasets."}, {"title": "G. Latent Graph Homophily Analysis", "content": "We conduct a comprehensive analysis to showcase the HR of latent graphs. Table X presents the HR of the latent graphs constructed in our study. For ACM and Yelp datasets, the HR of the homophilic latent graph surpasses the MHR based on any meta-path. Moreover, the HR of the homophilic latent graph in DBLP is very close to the highest MHR observed among meta-paths. Conversely, the HR of the heterophilic latent graph is consistently lower than the MHR of any meta-path in any dataset. In DBLP, it even approaches zero. Concerning the IMDB dataset, the general low semantic homophily ratios and the weak discriminative capability of the original features shown in Table II may have jointly affected the HR of the latent graphs. However, LatGRL still"}, {"title": "VI. RELATED WORK", "content": "A. Unsupervised Heterogeneous Graph Learning\nUHGRL aims to learn low-dimensional node representations on heterogeneous graphs without labeled data. Early methods utilize structure information to maximize the representation similarity of proximal nodes within the same link [38], the same random walk [32], or the same subgraph [39]. With"}, {"title": "B. Heterophily and Graph Neural Network", "content": "Recently, many works have discussed the performance degradation of GNNs on non-homophilic graphs and proposed some solutions. Geom-GCN [44] expands the node neighborhood by mining the geometric relationships of the input graph. FAGCN [15], SPCNet [45], and ACM [16] adopt adaptive message passing through graph filtering on the original graph structure. In addition, [46] has discussed the relationship between heterophily and over-smoothing in GNNs. It should be noted that, in unsupervised graph learning, heterophily has also received attention in recent studies like MUSE [47], RGSL [48], and GREET [25]. However, most studies have focused mainly on homogeneous graphs. Although there exists some research that has explored homophily/heterophily in heterogeneous graphs [49], [50], it focuses only on the meta-path-level, disregarding the diversity of heterophily at the node level. Moreover, their solutions are heavily dependent on node labels. On the contrary, we are the first to tackle the issue of heterophily in unsupervised heterogeneous graph learning, which is more challenging."}, {"title": "VII. CONCLUSION", "content": "In this paper, we delve into an extensive analysis of semantic heterophily in heterogeneous graphs and present a pioneering effort to resolve it in an unsupervised manner. We propose the concept of semantic homophily and heterophily, along with the corresponding evaluation metrics, and conduct an empirical study to analyze the prevalent manifestation of semantic heterophily. To overcome these challenges, we propose a novel unsupervised heterogeneous graph representation learning framework called LatGRL, which utilizes a similarity mining approach that couples global structures and features to construct homophilic and heterophilic latent graphs to guide representation learning. LatGRL also incorporates an adaptive dual-frequency semantic fusion mechanism with dual-pass graph filtering to handle semantic heterophily. Extensive experiments on four public datasets and a large-scale dataset demonstrate that it outperforms existing state-of-the-art models, confirming its effectiveness and high efficiency in addressing semantic heterophily in heterogeneous graph learning."}]}