{"title": "Benchmarking Deep Reinforcement Learning for Navigation in Denied Sensor Environments", "authors": ["Mariusz Wisniewski", "Paraskevas Chatzithanos", "Weisi Guo", "Antonios Tsourdos"], "abstract": "Deep Reinforcement learning (DRL) is used to enable autonomous navigation in unknown environments. Most research assume perfect sensor data, but real-world environments may contain natural and artificial sensor noise and denial. Here, we present a benchmark of both well-used and emerging DRL algorithms in a navigation task with configurable sensor denial effects. In particular, we are interested in comparing how different DRL methods (e.g. model-free PPO vs. model-based DreamerV3) are affected by sensor denial. We show that Dreamer-erV3 outperforms other methods in the visual end-to-end navigation task with a dynamic goal and other methods are not able to learn this. Furthermore, Dreamer V3 generally outperforms other methods in sensor-denied environments. In order to improve robustness, we use adversarial training and demonstrate an improved performance in denied environments, although this generally comes with a performance cost on the vanilla environments. We anticipate this benchmark of different DRL methods and the usage of adversarial training to be a starting point for the development of more elaborate navigation strategies that are capable of dealing with uncertain and denied sensor readings.", "sections": [{"title": "1 Introduction", "content": "Autonomous navigation is a fundamental challenge in unmanned systems. Conventional autonomous systems typically use Simultaneous Localisation and Mapping (SLAM) alongside Kalman filters that fuse data from multiple sources to map the environment, localize the agent, and generate trajectories to the goal. In recent years, deep reinforcement learning (DRL) methods have been used to train end-to-end policies that can navigate using a camera. These improvements have been made aided by advances in digital image processing largely due to convolutional neural networks trained using backpropagation [1-3], and parametrization of RL using deep neural networks [4].\nWhilst autonomous visual navigation is achievable in simulated environments [5], most of the research assumes perfect sensor readings. But, in real-world scenarios, information about the environment may be uncertain, incomplete, or even intentionally denied. This can limit the ability of the navigation system to adapt to changes in the environment and make optimal decisions. These failure modes and how they affect the systems need to be understood for safety-critical operations, such as unmanned aerial vehicles or autonomous road vehicles. In the case of DRL agents, which are notoriously hard to explain, it is important to study how the sensor artefacts or failures affect the training and evaluation.\nFurther, real systems typically use a suite of sensors. Whilst a lot of the cutting-edge work in DRL focuses on end-to-end visual navigation, in practice it is important to understand how the models learn across different modalities, by taking in data from multiple sensors. In this work, we study how sensor failure affects the training and evaluation of policies that use a camera or Lidar as the observation space.\nTo understand the effects of sensor noise and failures, we compare several DRL architectures: TD3, PPO, PPO-LSTM, and DreamerV3. TD3 and PPO are a commonly used for evaluating environments with continuous action spaces, PPO-LSTM is the recurrent version of PPO (recurrent models were shown to outperform non-recurrent counterparts by Mirowski et al. [6]), and DreamerV3 which uses autoencoders to create an internal world model of the environment, and was shown to outperform PPO on certain environments.\nWe generate a baseline of their performance in a 'vanilla' environment, with no perturbations - noise or failures - to the sensors. We then progressively add random areas to the environment in which the sensors are perturbed. The policies are then evaluated to understand how successful they are in navigation under varying degrees of noise. To train and evaluate the methods we use a modified version of the ROS-based gymnasium environment for training navigation policies, DRL-Robot-Navigation [7]. It contains a 3D maze in which a robot is tasked with navigating to a goal.\nThis publication is an extension of the conference paper on autonomous navigation presented at ICUAS 2024 [8]."}, {"title": "1.1 Review of DRL Applications to Navigation", "content": "Several studies have achieved end-to-end navigation using DRL with the camera as the sole sensor in a simulation environment [9]. Popular environments include the DeepMind Lab [10], ViZDoom [11], and DRL-Robot-Navigation [7].\nMirowski et al. [6] present an application of the A3C [12] algorithm to the Deepmind Lab environment. They use the camera as the sole sensor and their best-performing architecture consists of the image encoder, 2 LSTM layers, depth and loop predictions. The agent is rewarded for reaching the goal, or subgoals in the form of a fruit. The agent is not penalized for wall collisions, allowing strategies such as 'wall-following' that do not require memory. DRL can also be used to race FPV unmanned aerial vehicles (UAVs) against expert human pilots [13] by using the image sensor combined with the IMU (which are combined to detect gates and generate VIO states input to the DRL algorithm) in static environments. Polvara et al. show how a hierarchy of DQNs can be used for the autonomous landing of UAVs [14]. Their method handles a large variety of simulated environments and outperforms human pilots in some conditions. Interpretable deep reinforcement learning methods for end-to-end autonomous driving can handle complex urban scenarios [15]. Their method outperforms many baselines including Deep Q-Networks (DQNs), deep deterministic policy gradient (DDPG), Twin Delayed DDPG (TD3), and Soft Actor Critic (SAC) in urban scenarios with surrounding vehicles. The memory of FPV agents in 3D mazes is evaluated by Pasukonis et al. [16]. They find that models such as Dreamer and IMPALA can match humans on smaller mazes (9x9), but their performance significantly drops off on larger mazes (15x15). Lample and Chaplot [17] show that DRL can be used to train agents to play the first-person-shooter game DOOM, in which the agent is required to navigate a 3D maze, collect items, and shoot at enemies. They found that by using DQN and DQRN with an LSTM layer, the agent can converge, but required an addition of predicting game features (using a separate loss function) to be able to reliably shoot at enemies. Liquid neural networks [18] have been proposed in recent years to achieve robust navigation for changing environments [19]. In most of these cases, whilst the physical environment is unknown or stochastic, the papers did not consider compromised sensor readings (bar liquid neural networks that consider noise and other perturbations)."}, {"title": "1.2 Review of Sensor Fusion under Attacks", "content": "The first approach deals with intermittent attacks or denial, for example when GNSS coverage is lost and a Kalman filter (or more advanced recurrent neural network) is used to hold over the trajectory estimation until coverage recovers [20, 21]. This is useful for brief outages. The same idea can preemptively discover other V2V networks to improve localisation [22]. However, if the environment requires constant sharp manoeuvring to avoid dynamic objects, this scenario rules out trajectory estimation approaches. Also, if the attacks are continuous and erode the holdover accuracy, this would be unsuitable."}, {"title": "1.2.1 Trajectory Estimation Approaches", "content": "The first approach deals with intermittent attacks or denial, for example when GNSS coverage is lost and a Kalman filter (or more advanced recurrent neural network) is used to hold over the trajectory estimation until coverage recovers [20, 21]. This is useful for brief outages. The same idea can preemptively discover other V2V networks to improve localisation [22]. However, if the environment requires constant sharp manoeuvring to avoid dynamic objects, this scenario rules out trajectory estimation approaches. Also, if the attacks are continuous and erode the holdover accuracy, this would be unsuitable."}, {"title": "1.2.2 Sensor Fusion and Generative AI Approaches", "content": "The alternative is suitable for high-end platforms that can afford a diverse sensor suite, performing sensor fusion to alleviate the impact of one sensor being compromised. This is usually unsuitable for low-end drones and does not guarantee success [23]. A cheaper alternative is to use generative AI to generate alternative sensor representations using a single cheap sensor. For example, infrared images can be generated using a camera sensor and then fused together to achieve more robust recognition of the environment [24]. However, these approaches require an onboard diverse sensor suite or a powerful GAN operating onboard, which does not guarantee success."}, {"title": "1.2.3 Adversarial Environmental Perturbations in DRL", "content": "First-person-view (FPV) navigation problems are typically considered partially observable [25] because only a portion of the entire state is visible to the agent at any given time. But, sensor faults also have to be considered. Robust partially observable MDP [26] considers the POMDP to have uncertain parameters. Other approaches [27] consider the interaction between the adversary and the agent to be a zero-sum minmax game and define it as a Probabilistic Action Robust MDP. Zhang et al. [28] consider the addition of adversarial perturbations as a state-adversarial MDP (SA-MDP). In this framing, the state returned by the environment may be perturbed by an adversary (an adversary may be another agent or an environmental process that perturbs the state observations), making the observation uncertain and resulting in the action returned by the agent being potentially sub-optimal. Because we consider that the adversary can be static (e.g. it is part of the environment), we prefer the SA-MDP framing for our problem.\nWhen building robust RL models, some of these works focus on physical perturbations [29] that may affect the physics of the agent (e.g. by introducing uncertain forces), whilst others consider perturbations to the sensor state [30]. Korkmaz [31, 32] argues that vanilla training results in more robust policies compared with state-of-the-art adversarial and robust training techniques. Havens et al. [33] propose a hierarchical RL method of switching sub-policies in the presence of adversaries. Applications of adversarial training [34] show how a zero-sum action-robust RL method can be used to reduce conflicts in air mobility traffic management scenarios."}, {"title": "1.3 Gap and Novelty", "content": "Although research on DRL and sensor attacks exists, we have not found elaborate studies combining the effect of sensor failure on DRL performance during learning navigation policies. We aim to understand the effects of sensor failures a potentially disastrous failure mode in autonomous systems on the ability of DRL agents to learn navigation policies. To do this we:\n\u2022 Benchmark different reinforcement learning algorithms on the DRL-Robot-Navigation environment.\n\u2022 Study the effect of observation spaces Lidar and camera on the navigation capabilities."}, {"title": "2 Methodology", "content": "We build on top of the DRL-Robot-Navigation environment [7] which simulates a robot navigating around a 10 m x 10 m maze, populated with static obstacles. The robot has a suite of sensors - Lidar, camera, and odometry - but only a subset of these was used as the state observation to train the reinforcement learning model in the original paper. The original paper uses Lidar, distance to goal, and angle to goal, concatenated together as the observation space, and the camera is not used. An illustration of the environment, which shows the DRL-Robot-Navigation environment along with sensor readings, is presented in Fig. 1.\nAs the focus of this study is on studying adversarial sensor perturbations and test-ing different sensor suites, the changes to the environment are described in section 2.1. The benchmarking procedure, of training the different RL algorithms - TD3, PPO, PPO LSTM, and Dreamer V3, using different modalities and with differing sen-sor noise, is illustrated in figure 2. The reinforcement learning algorithms used in the benchmark are described in section 2.2. Training experimental settings, includ-ing hyperparameters, are shown in section 2.3. The evaluation process is described in section 2.4."}, {"title": "2.1 Changes to the DRL-Robot-Navigation Environment", "content": "To enable our experiments, the DRL-Robot-Navigation environment required several changes. A primary change to the environment is the addition of a vision-based observation space. To do this, the image from the robot is outputted from the environment, and a visual marker is added to mark the position of the goal. A pink sphere of diameter 0.5 m is added where the goal resides and is shown in Fig. 1 (top left image). The collision system is updated, as in the original implementation, it relied on the reading from the Lidar sensor. This is not a reliable way of detecting collisions, and adding noise to the Lidar sensor could trigger collisions. Instead, we generate a flag and detect any physical collisions between objects by triggering ROS topic that can be subscribed."}, {"title": "2.1.1 Modelling Adversarial Sensor Perturbations", "content": "Autonomous vehicles often use a camera as a primary sensor for navigation, but physical sensor attacks are rarely considered. Kim et al. [36] perform a review of drone attacks, including laser attacks on the camera sensor. They find that when applied, the camera pixels are distorted. This shows that camera sensors are susceptible to"}, {"title": "2.2 Reinforcement Learning Benchmark", "content": "A benchmark of the following RL methods is performed on the DRL-Robot-Navigation environment:\n\u2022 Twin Delayed DDPG (TD3) [37] is an off-policy, model-free algorithm designed to work on continuous action spaces. It employs two Q-value approximators (critics) and uses the smaller of the two Q-values to update the policy, which helps to mitigate the problem of overestimation bias. TD3 also introduces a delay in updating the policy network to reduce the variance of policy updates and adds noise to the target action to encourage exploration. The hyperparameters used to train TD3 are shown in table 1.\n\u2022 Proximal Policy Optimization (PPO) [38] is an on-policy, model-free algorithm designed to work on continuous spaces. Recurrent PPO, a modification of PPO with the introduction of a long short-term memory (LSTM) layer [39], is also used. The hyperparameters used to train PPO and Recurrent PPO are shown in table 2. The recurrent architecture is similar to the PPO, but contains an LSTM layer before the Actor-Critic Multilayer Perceptron (but after the convolutional layers for the camera observation).\n\u2022 DreamerV3 [40] is an off-policy, model-based algorithm that learns a world model from the experience by incorporating an autoencoder that encodes the input into a discrete representation. It uses recurrence and predicts the dynamics, reward, and the continuity of the episode. The hyperparameters are fixed, hence tuning is not required. It has shown to outperform PPO on several environments, including navigation on DeepMind Lab [10]. Based on this, it is expected that Dreamer V3 should outperform the other algorithms on our navigation problems. The default model is used.\nStable-Baselines3 [41] implementations of TD3, PPO, and recurrent PPO were used for the experiments. The general setup for TD3 and PPO is shown in Fig. 4, showing the networks for both camera and Lidar observations. For the camera observation, the image is initially passed through a convolutional network to reduce the dimensionality to 256 features. Those features are then passed into the actor-critic MLP. In the case of the recurrent PPO, the features are first passed through an LSTM layer, before going to the actor-critic. For the Lidar, because it only contains 24 data points, the features are passed straight to the actor-critic. We perform:\n\u2022 A benchmark of the RL methods.\n\u2022 A learning-rate discovery to find the optimal LR for each algorithm as it is the most sensitive hyperparameter (apart from Dreamerv3 as it does not require hyperparameter tuning). This is shown in the appendix.\n\u2022 A study into the performance of the agents across different observation spaces in sensor-perturbed areas."}, {"title": "2.3 Experimental Scenario Settings", "content": "The experimental settings are shown in table 3. The Lidar observation space comprises 20 discrete points, taken at even intervals in a 180\u00b0 arc around the robot. The camera image produces a 160x160 px RGB image and 64x64 px RGB image for Dreamer V3 (the input to the DreamerV3 is required to be a power of two\u00b2). Lidar noise is Gaussian, and camera noise results in a complete blackout. The agent position input (for Lidar"}, {"title": "2.4 Training and Evaluation", "content": "The reinforcement learning (RL) models described in section 2.2 are trained with the following configurations and environments:\n\u2022 Lidar (this includes distance and angle to goal). The models are trained on variety of 0x0 (no noise) maps, as well as maps with varying amounts of noise: 3x3, 5x5, and 7x7 metre noise zone.\n\u2022 Camera only, static goal. The models are trained on variety of 0x0 (no noise) maps, as well as maps with varying amounts of noise: 3x3, 5x5, and 7x7 metre noise zone.\nUnless otherwise stated, camera configurations contain a static goal to enable better convergence, as described in section 2.3. Each model is then evaluated on maps specified in the section 2.4, but generally this consists of being tested on each of the"}, {"title": "2.4.1 Camera Evaluation Maps", "content": "For evaluation of camera policies, two different maps are used:\n\u2022 DRL-Robot-Navigation map (same as training).\n\u2022 A simplified DRL-Robot-Navigation map no obstacles, shown in Fig. 6. The goal is always static in the centre. The noise area is static in the centre of the map.\nThe purpose of using the simplified map is to better understand the behaviour of the policies. As the obstacles are taken out, the only task left is to get through the sensor denied area to the goal. As the noise is static, it always obstructs the goal. In this evaluation task we are ignoring the ability of the agent to avoid obstacles, but are focusing on whether the agent has learned to get through the noise area. The simplified map is only used for evaluation and is never used for training of the policies. By default, the DRL-Robot-Navigation is used for evaluation. The use of the simplified maps is explicitly specified in the results section."}, {"title": "3 Results", "content": "Several experiments are conducted to benchmark the different DRL algorithms on the environment and understand how different training regimes (vanilla vs adversarial)"}, {"title": "3.1 Modality Study", "content": "The modality study shown in Fig. 7 shows the differences in model performance for the PPO algorithm across Lidar (with a random goal), camera with a static goal, and camera with a random goal. Lidar and camera with a static goal are quick to converge to a solution close to the maximum episode reward. Camera with a random goal does not converge to a solution in the given amount of steps. As we attempt to understand the effects of sensor perturbations, we will continue using camera with a static goal for the camera noise experiments."}, {"title": "3.2 Lidar Navigation with Noise", "content": "Although different algorithms are proposed in section 2.2, we were unable to find an appropriate learning rate to get PPO-LSTM to converge as shown in appendix A3. While it is possible to train DreamerV3 on the Lidar observation space, it is a large model for such a small observation space and we did not find that its performance increased over the other models. Hence, only TD3 and PPO are compared in this section."}, {"title": "3.3 Lidar Noise Training Regimes, Qualitative Results", "content": "As shown in section 3.2, the training regimes affect the evaluation performance of the algorithms. To better understand the behaviour of Lidar-specific policies, we can more closely investigate the effects of training in noisy regimes for each algorithm and investigate the trajectories of the robot during evaluation."}, {"title": "3.3 Lidar Noise Training Regimes, Qualitative Results", "content": "As shown in section 3.2, the training regimes affect the evaluation performance of the algorithms. To better understand the behaviour of Lidar-specific policies, we can more closely investigate the effects of training in noisy regimes for each algorithm and investigate the trajectories of the robot during evaluation."}, {"title": "3.4 Algorithm Training Comparison (Camera Observation)", "content": "As described in section 2.2, different RL methods are trained on the DRL-Robot-Navigation environment: TD3, PPO, Recurrent PPO (or PPO-LSTM), and Dreamer V3.\nFig. 13 shows the averaged episode reward during training for the different algo-rithms, across 500,000 steps of training. For the static goal scenario, TD3, PPO, and PPO-LSTM converge to a solution quickly, each one reaching 0.5 mean reward in less than 100,000 steps. The only model that was able to learn to navigate to a ran-dom goal was DreamerV3. By comparison, PPO failed to learn to navigate even after 500,000 steps. Nonetheless, to better understand the effects of noise on training, a static goal will be used for the camera policies."}, {"title": "3.5 Camera Navigation with Sensor Denial", "content": "Fig. 14 shows the evaluation results of the different algorithms, TD3, PPO, PPO-LSTM, and Dreamer V3 using the camera observation space. The algorithms are trained on the vanilla 0x0 map, and evaluated on environments of increasing camera denial area: 0x0 (sensor denial), 3x3, 5x5, and 7x7 - referring to the size of the rectangle of the sensor denied area in meters. On the 0x0 evaluation scenario, Dreamer V3 achieved the highest mean success rate of 90.8%, with TD3 achieving a similar performance of 88.8%. PPO-LSTM was third with a mean of 82.2% and PPO performed the worst with a mean success of 71.6%. On the 3x3 evaluation scenario, the pat-tern remains the same, with Dreamer V3 achieving the highest mean of 73.2% (with a much larger standard error of 6.8), TD3 close behind with 68.4%, PPO-LSTM third"}, {"title": "3.6 Camera Navigation with Sensor Denial (No Obstacle Map)", "content": "Section 3.5 shows that the training regimes have an effect on policy behaviour in sensor-denied areas. To better understand this behaviour, they are evaluated on the simplified, no obstacle map, as described in section 2.4.\nFig. 16 shows the evaluation results of the different algorithms, TD3, PPO, PPO-LSTM, and Dreamer V3 using the camera observation space. The algorithms are trained on the vanilla 0x0 (no sensor denial) default map, and evaluated on the simpli-fied, no obstacle map, with variations of increasing, static camera denial area which is located in the centre of the map: 0x0 (no sensor denial), 3x3, 5x5, and 7x7 - referring to the size of the rectangle of the sensor denied area in meters. All four of the models exhibit similar behaviour of successfully navigating in nearly all of the episodes on the 0x0 evaluation, but then consistently failing when any noise is introduced (with a few successes likely caused by spawning near the goal).\nSimilarly, Fig. 17 shows the evaluation results of the different algorithms trained on the default map with random 3x3 noise areas. The 0x0 evaluation does not significantly change, with TD3, PPO, and DreamerV3 still successfully navigating to the target 98, 98, and 96 times out of 100. For the 3x3 evaluation, TD3 reached the goal 74 times, PPO 71 times, PPO-LSTM 66 times, and DreamerV3 showed the best performance"}, {"title": "3.7 Discussion", "content": "The overall best performer for the camera navigation was Dreamer V3. DreamerV3 consistently outperformed other models for the camera observations for both 0x0 and 3x3 noise area evaluations, when trained on both 0x0 and 3x3 environments (Fig. 14 and 15). This might result from the DreamerV3 having a different structure to the other models and building a world model due to the autoencoder structure. We hypothesized that it might be able to overcome the sensor denied areas due to the autoencoder/recurrent structure. While it did outperform other models when trained and evaluated on the 3x3 environment (it reached a mean of 80.6% in Fig. 14) that still does not match the baseline performance of 90.8% when trained and evaluated on the no noise environments. Dreamer V3 was also the only model to have learned to navigate to a random goal using the camera-only observation space within 500,000 steps, as shown in section 3.4. Other models either required the Lidar policy (which contained information about distance and angle to target), or required a static goal to be able to converge to a solution using the camera observation space. Dreamer V3 was also convenient to use as it did not require any hyperparameter tuning. The best performer (between TD3 and PPO) for the Lidar modality was TD3. The 0x0 trained TD3 model generally outperformed most variations on PPO across different evaluations.\nIn sections 3.2 and 3.6 we have learned that training policies with some noise present in the environment present an interesting result: these policies are more successful at navigating to the goal, but also take more high-risk actions when sensor readings are uncertain or completely denied, which may lead to colliding with obstacles more often a terminating event in our scenario. This is a limitation of training"}, {"title": "4 Conclusion and Further Work", "content": "This study has shown quantitative results of how common reinforcement learning algorithms perform with different modalities camera and Lidar on the DRL-Robot-Navigation environment, and the different effects of training and evaluating on environments containing noisy and sensor denied areas. We observed that training DRL models on noisy environments creates policies that, generally perform better on noisy environments, compared with models trained on vanilla, no noise environments. However, we found that these policies opted for riskier actions when presented with a noisy or faulty observation: ones that might result in successful navigation to the goal, but also in collision with another object. While these models result in better performance by quantitative metrics, this behaviour is unacceptable from the point of view of safety-critical scenarios and shows a need for more robust RL methods that are resilient to noise or sensor outages in the scenarios. Hence, we open-source the environ-ments that we built on top of the DRL-Robot-Navigation to enable other researchers to evaluate their model performance on these scenarios. The results presented here should serve as a benchmark for any future studies on strategies of how to deal with faulty sensors. While we present evaluation for Gaussian noise to Lidar and camera outages, different perturbations also should be integrated (e.g. [32] investigates trans-formations such as compression artefacts, brightness and contrast, and blur for camera observations). We consider that work on autoencoders could lead to identifying sen-sor failures, and in potentially serving as a backup that is able to fill out the gaps in case of sensor outages. Sensor fusion strategies should also be investigated. Hierarchi-cal reinforcement learning could lead to better performance (as proposed by Havens et al. [33]) as it allows for the top hierarchy to detect adversaries. This could lead to"}, {"title": "Appendix A Learning Rate Sensitivity", "content": "As the algorithms (apart from DreamerV3) are sensitive to learning rate, we initially perform a learning rate study - we then continue using this learning rate throughout."}], "equations": ["R(s, a) = \\begin{cases}1, & \\text{goal reached} \\\\ -1, & \\text{collision} \\\\ -1/50, & \\text{otherwise}\\end{cases}"]}