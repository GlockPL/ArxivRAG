{"title": "S2-Diffusion: Generalizing from Instance-level to Category-level Skills in Robot Manipulation", "authors": ["Quantao Yang", "Michael C. Welle", "Danica Kragic", "Olov Andersson"], "abstract": "Recent advances in skill learning has propelled robot manipulation to new heights by enabling it to learn complex manipulation tasks from a practical number of demonstrations. However, these skills are often limited to the particular action, object, and environment instances that are shown in the training data, and have trouble transferring to other instances of the same category. In this work we present an open-vocabulary Spatial-Semantic Diffusion policy (S2-Diffusion) which enables generalization from instance-level training data to category-level, enabling skills to be transferable between instances of the same category. We show that functional aspects of skills can be captured via a promptable semantic module combined with a spatial representation. We further propose leveraging depth estimation networks to allow the use of only a single RGB camera. Our approach is evaluated and compared on a diverse number of robot manipulation tasks, both in simulation and in the real world. Our results show that S2-Diffusion is invariant to changes in category-irrelevant factors as well as enables satisfying performance on other instances within the same category, even if it was not trained on that specific instance. Full videos of all real-world experiments are available in the supplementary material.", "sections": [{"title": "I. INTRODUCTION", "content": "Imitation learning (IL) [1], [2] has shown potential in en-abling robotic manipulation in challenging real-world scenar-ios by learning complex skills from human demonstrations. Still, existing IL methods often struggle to generalize beyond the specific training environments from which the demonstra-tions are derived. This is an important obstacle as each new environment requires labor-intensive data collection, model fine-tuning, and retraining to adapt the learned policies.\nFor humans, transferring knowledge between tasks and skills, such as transferring the scooping skill from rice to cereals, is rather straightforward. Scooping rice or cereals may be considered as different instances of the same task for current IL methods. The ability to generalize over such instances is still a challenge and requires rather advanced spatial-semantic understanding [3]. The ability to transfer and generalize over instances removes the necessity for extensive training and also allows for assessing what type of instances one can transfer over for example, scooping ice cream may be very different from scooping granular material such as cereals or rice. Thus, granular materials may be seen as the same category as rice and cereals, while ice cream is an instance of another category which requires a very different policy when executing the scooping task.\nLarge pre-trained Visual-Language-Action (VLA) mod-els [4], [5], [6] generalize simple skills such as pick-and-place over a wide range of environments and objects. How-ever, more complex non-prehensile manipulation such as scooping, mug-flipping, cooking shrimp or opening a bottle with a bottle opener stay elusive for such large general mod-els. Recent imitation learning approaches address learning of instances of challenging manipulation tasks [1], [2], [7], [8], [9], [10], [11] but the integration with semantic knowledge in highly-complex manipulation tasks remains a challenge [9]. The aforementioned methods often rely on raw perceptual features and environmental conditions, limiting their applicability to the instances observed during the training. Training a skill via imitation learning, such as a diffusion policy [9], relies on expert demonstrations that often do not cover several instances of the same task. We show that when trained only on a particular instance of a task such as wiping red scribbles from a whiteboard - the skill fails to transfer already when the scribbles are now green, even if the required action and environment for wiping is exactly the same. This is because the policy did not learn a whiteboard-wiping category skill but a single instance of this category namely red-whiteboard-wiping.\nMotivated by the above challenges, we present a novel approach that integrates spatial prediction with semantic segmentation features from large pre-trained models [12], [13] to generalize from expert demonstrations on a single"}, {"title": "II. RELATED WORK", "content": "In recent studies, visual imitation learning methods [16], [17], [18], [19] have shown great potential in diverse robot manipulation tasks. The recent work of Zhu et al. [3] demonstrates the potential of object-based representations for manipulation by using a conventional closed-set object detector. In [20], the authors validate that large-scale video generative pre-training is able to effectively benefit visual robot manipulation learning. However, in their works, the finetuned policy is not demonstrated to generalize to novel instances of the task. 3D vision based imitation learning [21], [22], [23], [24] has shown to improve performance but typically requires multiple posed views with RGB-D sensors to reconstruct the scene. Another work [25] also ulitizes pretrained VLMs to provide scene representation for robot tasks, but their approach needs to construct a 3D neural feature field by scanning part of the scene. In our paper, we propose a visuomotor imitation learning method that takes advantage of object-aware semantic representation to focus on task-relevant objects for training a visuomotor policy.\nRecently, Wang et al. [26] introduce a novel framework that enhances generalization capabilities by incorporating explicit spatial and semantic information into policy learn-ing. This method uses multi-view RGB-D observations to generate 3D descriptor fields and then constructs semantic fields that highlight task-relevant parts of objects (e.g., a knife handle or soda can tab). By combining 3D geometric and semantic information, this approach achieves significant improvements in generalization at the cost of requiring multiple RGB-D cameras.\nVision-Language Models (VLMs), trained on vast amounts of internet data, have found widespread use in robotic applications [27], [28], [29]. Some works [30], [31], [32], [26] have explored leveraging 2D foundation vision mod-els to construct open-vocabulary 3D representations. These approaches involve utilizing visual features from pretrained foundation models like CLIP [27] or SAM [33] to scene rep-resentations that benefit learning downstream robot policy. The most recent works [12], [34], [35] demonstrate supe-rior open-vocabulary classification ability which succeed in generalizing to unseen classes. Pretrained foundation models are also utilized by either being prompted or fine-tuned to generate high-level task plans [36], [31] and to shape reward functions [37], [38], [39] for reinforcement learning. In [40], the authors introduce a multimodal diffusion transformer policy for solving long-horizon tasks. In contrast, our ap-proach concentrates on improving generalization ability for open-vocabulary manipulation policy. MOKA [41] addresses robotic manipulation by using VLMs to mark open-world keypoint affordances. The core idea behind both MOKA and our method is similar, that is guiding the policy to focus on task-relevant objects. However, our method utilizes a more generalizable semantic representation and also depth information for the entire scene.\nLarge policies or Vision-Language Actions (VLAs), pre-trained on a combination of Internet-scale data and diverse robot demonstrations, is another promising research direc-tion for generalizable policy learning in robotics. Recent works [42], [5], [43], [44] propose to co-finetune robot trajectory data alongside vision-language data, demonstrat-ing impressive generalization capabilities with novel objects and instructions. However, these are complex and computa-tionally costly attempts at general manipulation that while providing encouraging results at task reasoning have so far not been able to compete with imitation-based skill learning on non-prehensile and contact-rich manipulation tasks. To enhance generalization ability, [45] further proposes a cross-embodied robot policy trained on the diverse robot dataset to control robots with varying observation and action types. For comparison, our method focuses on policy generalization across different tasks by concentrating solely on the segmen-tation masks of target objects."}, {"title": "III. TOWARDS GENERALIZABLE ROBOTIC SKILLS", "content": "We start with a question: \"What does it mean for a skill to be generalizable?\". Commonly, \"mug flipping\" trained on a specific mug with distinct appearance and geometry will not generalize to flipping other mugs. Instead, the learned skill remains instance-specific, as neural network-based methods"}, {"title": "IV. PROBLEM FORMULATION", "content": "Let \\(I\\) be the space of RGB images. Consider a manipula-tion task given the language instruction of the target object \\(L\\) (e.g., \"coffee mug\"). Our goal is to learn a generalizable imitation learning policy by leveraging spatial-semantic rep-resentation from pretrained Vision-Language models."}, {"title": "V. METHOD", "content": "Our objective is to develop an open-vocabulary spatial-semantic visuomotor policy that can generalize from an individual instance to other unseen instances resulting in a category-level skill. We propose open-vocabulary Spatial-Semantic Diffusion policy (S2-Diffusion), an approach that leverages three main components in policy learning: a se-mantic segmentation model, a depth prediction model and a diffusion policy shown in Fig. 3. The policy is trained with demonstrations from expert teleoperation, using only RGB images and robot proprioception as the state, and end-effetor velocities as the commanded actions respectively.\nFollowing previous works [9], [49], we formulate the visuomotor policy as a conditional Denoising Diffusion Probabilistic Model (DDPM) [50], [51]. Starting from ran-dom action \\(a_{k}\\) sampled from Gaussian noise, the diffusion probabilistic model \\(\\epsilon\\) performs \\(K\\) iterations of denoising. This process gradually produces a sequence of actions with decreasing noise levels, \\(a^{K}, a^{K-1}, ..., a^{0}\\), until the noise-free action \\(a^{0}\\). Each action denoising iteration is described as:\n\\begin{equation}\na^{k-1} = \\alpha_{k}(a^{k} - \\gamma_{k}\\epsilon_{\\theta}(a^{k}, o, k)) + \\sigma_{k}N(0, I), \\tag{1}\n\\end{equation}\nwhere \\(o\\) is the observation for the policy. \\(\\alpha_{k}\\), \\(\\gamma_{k}\\) and \\(\\sigma_{k}\\) are referred as noise schedule for each iteration \\(k\\), and \\(N(0, I)\\) is the Gaussian noise added to the action.\nTo learn the action predicting model \\(\\pi_{\\theta}\\), we randomly sample the robot action \\(a^{0}\\) from the demonstration dataset \\(D\\) and add the noise \\(\\epsilon_{k}\\) for a random iteration \\(k\\). The training loss for the diffusion model is formulated as:\n\\begin{equation}\nL = MSE(a^{0}, \\pi_{\\theta}(a^{0} + \\epsilon_{k}, o, k)),\\tag{2}\n\\end{equation}\nwhere we use an action sampling approach rather than a noise prediction model to enhance the generation of high-dimensional actions.\nWe aim to design spatial-semantic-aware representation that is leveraged for the observation of the above denois-ing probabilistic model. We utilize two pretrained VLMs, Grounded-SAM2 [14] and DepthAnythingV2 [13], for open-vocabulary semantic segmentation and depth map estimation respectively. We use Grounded-SAM2 model to perform zero-shot semantic segmentation, leveraging CLIP-based [27] mask classification to segment unseen classes. We combine the extracted features to construct a spatial-semantic repre-sentation that is leveraged as the input for the visuomotor diffusion policy.\nWe utilize Grounded-SAM2 to segment an image into a set of semantic masks (\\(Z_{1}, Z_{2}, ..., Z_{n}\\)) from visual observations based on text descriptions. We apply pixel-wise maximum pooling for each pixel location \\((i, j)\\) across all segmentation masks:\n\\begin{equation}\nz_{f}(i, j) = \\max_{s=1}^{n} z_{s}(i, j),\\tag{3}\n\\end{equation}"}, {"title": "VI. EVALUATION", "content": "The goals of all our experiments are three-fold:\n1) to evaluate and compare the performance of our method on challenging robotic manipulation tasks;\n2) to validate that generalization from instance-level to category-level skill is achieved in a real-world setting;\n3) to perform ablations in order to investigate the role of the semantic and spatial components.\nWe first describe the experiments performed in the simula-tion, followed by the experiments on physical hardware.\nFor our simulation experiments, we take advantage of a recent open-sourced large-scale simu-lation environment, RoboCasa [52], which provides expert demonstrations for diverse everyday tasks. We evaluate our method and the baselines in a set of single-stage tasks as shown in Fig. 4: ServeMug, CloseDoor, TurnOnMicrowave, TurnOffFaucet, MoveSoda using the provided 50 expert demonstrations from RoboCasa.\nWe compare our method with three baseline methods in simulation: 1) BC-RNN: a behavior cloning method with recurrent neural network implementation; 2) BC-Transformer: a behavior cloning imitation learning method with transformer architecture as the policy im-plementation [54]; 3) Diffusion Policy: the image-based diffusion policy of [9]. For real-world experiments, we compare our method against the diffusion policy. In all our experiments, we train for 500 epochs on an NVIDIA RTX 4090 GPU. We use a batch size of 64 and a learning rate of le-4.\nWe take advantage of the open-sourced implementation for the baseline methods from RoboMimic [54]. In the demonstration datasets for each task, we assume variations set by RoboCasa in the color and shape of the target object, as well as differences in the background environment. To test the performance of our method and the baselines, we evaluate 50 trials for each task and the corresponding success rates are listed in Table I, demonstrating the superior performance of the proposed S2-Diffusion approach.\nDue to the challenging variations in target object and back-ground seen within the RoboCasa dataset, classical imitation learning policies struggle to solve the specific single-stage manipulation task. While the baseline methods-BC-RNN, BC-Transformer, and Diffusion Policy-show relatively poor performance, particularly on tasks like ServeMug, Close-Door, and MoveSoda, S\u00b2-Diffusion consistently achieves the highest success rates. For instance, S2-Diffusion outperforms the other methods with a success rate of 0.72 on ServeMug"}, {"title": "VII. LIMITATIONS", "content": "As Fig. 2 shows, the functional goal of flipping or scooping is discretized in different category-level skills such as bowl-to-bowl and pile-to-container scooping. While our method does generalize from instance-level tasks to category-level tasks, in its current form it is not able to generalize well from one instance-level skill to an instance of a different category task. i.e. when only training on rice-bowl-to-bowl the performance will degrade on a sand-pile-to-container task.\nWhile Fig. 2 makes the ordering of skills into instance and category levels look straightforward, there are some corner cases that would need to be addressed if this view was expanded into a more complex ontology. For one, the functional goal can depend on more context than what is shown in our example. For instance, flipping is presented in our example as a way to turn over an object, but it could also be interpreted as an action the robot should perform with its own body. Furthermore, given how current learning-from-demonstration techniques are implemented, the category bowl-to-bowl scooping might require additional qualifiers to accurately encompass the correct instances. For example, if we were to generalize this skill to ice cream bowl-to-bowl scooping, the policy would fail, as the forces required are completely different from those observed in the training instances. Ice cream, especially when frozen, requires significantly more force to be scooped in the first place. However, as the ice cream begins to melt, and the required forces become more similar to those seen during training, there is a point at which our category skill bowl-to-bowl scooping would likely work. We leave exploring such nuances to future work.\nAnother limitation of our work is its dependency on the performance of pre-trained vision foundation models. Natu-rally if the semantic or spatial estimate is poor, the resulting"}, {"title": "VIII. CONCLUSION", "content": "In this work, we have introduced S2-Diffusion, an open-vocabulary spatial-semantic diffusion policy that enables generalization from instance-level training data to category-level skills in robotic manipulation. Our method integrates semantic understanding with spatial representations, lever-aging vision foundation models to learn a policy that is invariant to task-irrelevant visual changes and generalizes across different instances of the same category. This allows robots to transfer learned skills beyond their training data without requiring additional fine-tuning or retraining.\nThrough extensive simulations and real-world evaluations, we demonstrated that S2-Diffusion outperforms the base-lines. In particular we showed that:\n\u2022 Spatial-Semantic representations enhance generalization By combining spatial information with semantic segmentation, our method enables robots to focus on task-relevant features, improving instance-to-category transfer.\n\u2022 Efficient real-time execution using only a single RGB camera Unlike approaches that require multi-view setups or additional depth sensors, S2-Diffusion extracts meaningful scene representations from a single RGB image, making it practical for real-world deployment.\n\u2022 Category-level generalization Our evaluations on a number of robotic manipulation tasks, including real-world wiping and scooping, demonstrate that the method successfully generalizes across unseen instances within the same category, achieving high performance where baseline policies fail.\nIn summary, S2-Diffusion policies are less constrained by their training distribution and constitute a step toward the ability of humans to generalize skills across variations in objects, materials, and environments instances."}]}