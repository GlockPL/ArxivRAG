{"title": "DALL-M: Context-Aware Clinical Data Augmentation with LLMs", "authors": ["Chihcheng Hsieh", "Catarina Moreira", "Isabel Blanco Nobre", "Sandra Costa Sousa", "Chun Ouyang", "Margot Brereton", "Joaquim Jorge", "Jacinto C. Nascimento"], "abstract": "X-ray images are vital in medical diagnostics, but their effectiveness is limited without clinical context. Radiologists often find chest X-rays insufficient for diagnosing underlying diseases, necessitating comprehensive clinical features and data integration. We present a novel technique to enhance the clinical context through augmentation techniques with clinical tabular data, thereby improving its applicability and reliability in AI medical diagnostics. To address this, we introduce a pioneering approach to clinical data augmentation that employs large language models (LLMs) to generate patient contextual synthetic data. This methodology is crucial for training more robust deep learning models in healthcare. It preserves the integrity of real patient data while enriching the dataset with contextually relevant synthetic features, significantly enhancing model performance.\nDALL-M uses a three-phase feature generation process: (i) clinical context storage, (ii) expert query generation, and (iii) context-aware feature augmentation. DALL-M generates new, clinically relevant features by synthesizing chest X-ray images and reports. Applied to 799 cases using nine features from the MIMIC-IV dataset, it created an augmented set of 91 features. This is the first work to generate contextual values for existing and new features based on patients' X-ray reports, gender, age and to produce new contextual knowledge during data augmentation.\nEmpirical validation with machine learning models, including Decision Trees, Random Forests, XGBoost, and TabNET, showed significant performance improvements. Incorporating augmented features increased the F1 score by 16.5% and Precision and Recall by approximately 25%. DALL-M addresses a critical gap in clinical data augmentation, offering a robust framework for generating contextually enriched datasets.", "sections": [{"title": "I. INTRODUCTION", "content": "In deep learning, data augmentation is crucial for enhancing the diversity and robustness of training datasets. For image data, techniques such as rotating, flipping, and altering pixel values enable models to extract novel features that would not be present in unaltered images. Similarly, text data can be augmented by masking, replacing, or repositioning words, which helps models learn better contextual representations. However, when it comes to tabular data, particularly in clinical settings, the approach to data augmentation is less straightforward and fraught with challenges.\nA common method for augmenting tabular data involves introducing noise by randomly permuting values, which can corrupt the data and necessitate model reconstruction [3]. While this may work in some contexts, it poses significant risks in clinical settings where measurements are highly specific to a patient's condition. Random permutations can lead to biased and erroneous representations, undermining the model's reliability. Moreover, unlike image augmentation, where new information such as edges, textures, and positions are generated, permuting clinical tabular data does not inherently produce new knowledge. For example, consider a 70-year-old female patient with a consolidation in the left lung, high blood pressure, and high temperature. We lack mechanisms to augment this knowledge by inferring additional clinical features, such as the likelihood of the patient experiencing shortness of breath or chest pain, or even more advanced insights, like the probability of the patient being a smoker. This example highlights a significant gap in our ability to generate enriched, contextually relevant data in clinical settings. Tabular data augmentation remains an open research question with profound implications on the accuracy, reliability, and generalizability of deep learning models, particularly in critical domains such as healthcare, finance, and scientific research, where the integrity and context of the data are essential.\nLarge Language Models (LLMs) have triggered a transformative shift within the Artificial Intelligence (AI) and deep learning (DL) communities due to their model architectures and extensive pre-training across diverse corpora.\nIn healthcare applications, LLMs can use their expansive repository of medical knowledge to hypothesize additional clinical symptoms that might manifest based on specific patient characteristics. This capability introduces an innovative method for contextual data augmentation in clinical settings. Applying probabilistic reasoning and pattern recognition, we argue that LLMs can extrapolate and synthesize relevant clinical features that, while not explicitly documented, are logically conceivable given the patient's existing data profile. This approach can preserve the factual integrity of the original dataset while augmenting it with predictive insights, thereby enhancing its dimensional depth. Such enriched data facilitates the training of deep learning models, enabling them to develop more accurate and robust diagnostic tools. Additionally, anchoring LLMs to authoritative medical sources, such as Radiopaedia, mitigates their tendency to generate inaccurate or fabricated information, commonly called \"hallucinations.\"\nThis paper addresses a critical and unexplored topic in the medical context: how can LLMs be effectively utilized to augment clinical datasets with contextually relevant features that are not explicitly present but are inferable from existing data? This research question investigates the potential of LLMs to bridge the gap between raw clinical data and the understanding required for advanced diagnostic and prognostic applications, thereby enhancing the depth and applicability of clinical datasets in predictive modeling.\nWe propose an innovative solution for balancing images and tabular data to address this. Specifically, we introduce a tabular Data Augmentation with large language models termed DALL-M, designed to generate context-aware and clinically relevant features by synthesizing chest X-ray images and their reports. Our approach leverages the analytical power of LLMs combined with radiologists' medical knowledge to extract context-aware, clinically relevant features and reason with heterogeneous information sources. This method enhances the dataset contextually and semantically, improving model performance and reliability in clinical applications.\nOur contributions are as follows:\n1) Framework for Data Augmentation (DALL-M) that generates contextually relevant data and new clinical features by combining LLMs with radiologist insights;\n2) Three-Phase Feature Generation comprising (i) clinical context storage, (ii) expert query generation, and (iii) context-aware feature augmentation;\n3) Validation with several machine learning (ML) Models using Decision trees (DT), Random Forests (RF), XGBoost, and TabNET [2];\n4) Empirical Evidence of Relevance through clinically relevant and contextually appropriate features, and\n5) Improvements in Clinical Data Integration into medical diagnostics, addressing the challenge of augmenting tabular data with clinical precision and context awareness."}, {"title": "II. RELATED WORK", "content": "Medical Large Language Models (Med-LLMs) can be a trustworthy tool in clinical workflow settings as they bring (i) enhanced medical knowledge comprehension, (ii) accuracy improvement in the diagnosis, and (iii) recommendation for personalized patient treatment. The above capabilities allow for more accurate decisions and improve patient care quality and treatment outcomes.\nIn the healthcare arena, we can easily find several Med-LLMs e.g., ChiMed-GPT [31], MedicalGPT [25], HuatuoGPT-II [8], or ChatMed [41], which have garnered increasing interests in healthcare and biomedical research fields. Existing works can be framed into the following categories [22]: (i) existing Med-LLMs in the medical field, and (ii) in providing effective clinical and patient treatment.\nRegarding (i), a notable class of existing Med-LLMs methodologies exist, and we now summarize their background. HuatuoGPT [8] is based on reinforcement learning and aims to align LLMs with distilled and real-world data. ClinicalT5 [23] aims to leverage general LLMs and the specialized nature of clinical text. Towards this, the model is pre-trained using the span-mask denoising objective on a large corpus of clinical notes from the MIMIC-III database. ClinicalGPT [33] incorporates quite different sources of information, comprising diverse real-world data in the training process, (i.e. medical records), domain-specific knowledge, and multi-round dialogue consultations. ChiMed-GPT [31] is built upon Ziya-13B-v2, a general domain Chinese LLM, and uses a comprehensive training regime. It is a holistic approach capturing domain-specific knowledge and aligning human preferences. BioGPT [24] specifically designed for biomedical text generation, brings inspiration from GPT-based models. BioGPT aims to improve the limited generation ability for biomedical domain-specific language models, which primarily focus on BERT-based architectures. PubMedBERT [14] is pre-trained from scratch on a large biomedical text being trained only in-domain text. This allows for a better capture of biomedical language's subtle distinctions and complexity. GatorTron [40] is specifically designed to extract and capture patient characteristics from electronic health records (EHRs) and to answer medical queries. Med-PaLM [28] improves the application of Med-LLMs, through timely adjustments to PaLM, show promising prospects in the MultiMedQA benchmark tests and highlight areas needing improvement to meet healthcare standards. MedAlpaca [15] promotes model performance significantly by fine-tuning the open-source AI model LLaMa with specialized training datasets. MedAlpaca aims to facilitate research and development in medical imaging, ultimately contributing to patient care and medical research advancements. LLaVA-Med [20] demonstrates effectiveness in medical image analysis, enabling it to interpret visual medical data within a contextual framework. Specifically, using X-rays and CT scans, can cause pathological conditions in the patients. This Med-LLM is equipped with multimodal dialogue capabilities employing cost-effective learning strategies.\nFrom the above, it is evident that the scope of application in using LLMs is wide. Despite this, in this paper, we propose to use the Med-LMMs in an orthogonal and new context. To our knowledge, and up to now, the Med-LMMs have never been used to augment clinical features as we propose herein.\nConcerning the second point in (ii) above, and as in this work, the main focus is identifying the patient-specific characteristics to provide efficient clinical decision-making support. The available works fall into four categories:\n(1) Medical Corpus, which focus on a collection of rich high-quality medical data from clinical scenarios [7], [21];\n(2) Medical-specific methodologies, where the learning optimization is the central aspect, and where the adaptation LLMs are used to improve its capacity for understanding medical language and context awareness. Examples of this class include structured medical knowledge graphs [39], to enhance Med-LLMs' clinical reasoning ability, and e.g., Chain-of-Thought [35] to improve suggestions reliability;\n(3) Clinical Role, tailored to address on the practical applications of Med-LLMs across various settings (e.g., [34], [37]);\n(4) Ethics, that concentrates to establishing the regulatory guidelines [38], necessary to guarantee personal health information protection.\nFraming our work with the points above, our proposal does not rely on the richness of the dataset (see (1)). Also, we do not adapt (i.e., pre-training and fine-tuning) the contents of the LLMs (see (2)). Although our proposal does not address ethical concerns as in (4), it aligned with (3), as we focus on the practical applications of Med-LLMs (e.g., applied to the diagnosis in X-ray images). However, we achieve this in a different way: we use Med-LMMs to promote knowledge generating new clinical features through tabular data augmentation. Indeed, some previous works testify to the usefulness of LMMs to model tabular data, e.g. for the Generation of REAlistic Tabular Data (GReat) [6], for high-fidelity Synthetic electronic health records (HALO) [29] and for inferring tabular features from textual medical reports TEMED-LLM [5]. Following [6], [29], we also argue that LLMs are realistic at generating clinical features. However, we tackle the problem differently, providing an answer to a question that has not been answered so far: How to precisely perform a clinical realistic \u201cdata augmentation\" with tabular data? Carrying this thought further, we present a novel methodology to accomplish the challenge above, i.e. augmenting the knowledge by inferring additional and \"new\" clinical features.\nOur proposal involved collaboration with domain experts to analyze the importance of features, highlighting our framework's ability to generate clinically significant features. Our novel framework can potentially revolutionize clinical data integration in medical diagnostics through ML methodologies and LLMs."}, {"title": "III. DALL-M: DATA AUGMENTATION WITH LLMs", "content": "The framework encompasses three main phases: (1) Clinical Context Extraction and Storage, (2) Expert Input Queries and Prompt Generation, and (3) Context-Aware Feature Augmentation. This pipeline ensures a comprehensive data augmentation process, from extracting clinical context to generating augmented features, all while leveraging the analytical power of LLMs.\nA. Phase I: Clinical Context Extraction and Storage\nThis phase is specifically designed for the systematic extraction and secure storage of clinical context and medical history relevant to each patient. Initiating this phase with a particular patient case, the proposed pipeline consists of constructing a comprehensive repository encapsulating the contextual clinical knowledge associated with the patient's conditions. Here, \u201cconditions\" refer to lesions identified through ground truth labels derived from the REFLACX dataset. Utilizing these lesion labels as a foundation, we query and retrieve documents from two distinguished online resources: Radiopedia [27] and Wikipedia [36]. Radiopedia, a collaborative online platform dedicated to radiology, offers an extensive compilation of radiology cases, articles, and reference materials, serving as a pivotal educational and knowledge-sharing tool within the radiology domain. On the other hand, Wikipedia stands as a vast, freely accessible web-based encyclopedia orchestrated by the Wikimedia Foundation, catering to a wide array of topics across multiple languages.\nTo process and systematically organize the information gleaned from these sources, we employ a Retrieval Augmented Generation (RAG) approach, further refined through applying LLMs. This strategy enables the transformation of the retrieved documents into a semantically rich and structured format. Our exploration of various database technologies, including NetworkX Graph, Neo4j Graph, and Neo4j Vector Index, determined that Neo4j with vector index facilitates superior information extraction and storage capabilities. Accordingly, this methodology was selected for our project. This RAG process was meticulously executed for each of the five lesion labels, yielding a domain-specific clinical dataset that encapsulates the relevant clinical context and ensures efficient storage.\nB. Phase II: Expert Input Queries and Prompt Generation\nThis phase focuses on generating contextually rich domain knowledge resembling radiologists' process when evaluating an X-ray image. Through structured interviews with radiologists, we sought to understand the key questions that arise when they encounter a new case. Their insights were distilled into seven principal questions covering various clinical dimensions:\n1) General Clinical Domain Knowledge\nWhat are the symptoms associated with {lesion}?\nWhat can cause {lesion}?\n2) Observational Analysis: What the patient complains about\nWhat are the patient's symptoms that are relevant for {lesion}?\n3) Physical Analysis: What the radiologists observe\nWhat are the relevant clinical signs for the etiological diagnosis of {lesion}?\nWhat are the relevant clinical characteristics for the etiological diagnosis of {lesion}?\n4) Laboratory Analysis: What do the exams say?\nWhat are the relevant laboratory data for the etiological diagnosis of {lesion}?\n5) Patient Characteristics: Patient's demographics\nWhat is the patient's personal, relevant history for the etiological diagnosis of {lesion}?\nUtilizing these expert-informed questions, we construct prompts that seek to address them through an RAG approach, wherein LLMs serve as intermediaries. These LLMs solicit and retrieve responses from the clinical domain vector database established in Phase I. The responses offer enriched and augmented clinical insights specific to the patient's case.\nThis approach ensures a comprehensive and understanding of the patient's condition, blending expert radiological inquiry with advanced computational techniques. This allows the generation of an Augmented Clinical Knowledge (ACK) source capturing valuable context information, such as explanations of the lesions, symptoms that may cause the lesion, and potential patient-specific attributes that may contribute to the lesions (for instance, overweight, etc.). This information will be processed in Phase III to extract new contextual and relevant clinical features for each patient's condition.\nC. Phase III: Context-Aware Feature Augmentation\nThe overarching goals of this phase are twofold: (1) to identify and generate new clinical features and (2) to assign values to these newly identified features. Leveraging the ACK corpus developed in Phase II, this phase extracts novel clinical features from the accumulated corpus. Recognizing these features directly from textual data poses significant challenges, even with advanced techniques such as named entity recognition. To navigate these complexities, we employ LLMs equipped with few-shot learning capabilities to discern clinical features, identifying m distinct features extracted from the Phase II documents.\nHowever, merely identifying relevant clinical features is insufficient for augmenting a clinical dataset. Generating values for these features is equally crucial to ensure their applicability and utility. To this end, we revisit the RAG approach, formulating prompts that incorporate four pivotal sources of information: (1) prior contextual knowledge derived from the ACK in Phase II; (2) the newly identified features; (3) the domain knowledge database established in Phase I; and (4) the patient's radiology report alongside demographic details. Consequently, our prompts returned a list of features and corresponding values, which were integrated into our original dataset, culminating in enriched and augmented context-aware datasets."}, {"title": "IV. EXPERIMENTAL SETUP", "content": "A. Dataset\nIn this work, we utilize instances from the MIMIC-IV dataset [19], comprising radiographs, radiology reports from chest X-rays, and clinical data. We gathered 799 patient cases from these datasets, with each instance accompanied by corresponding ground-truth information on clinical features, facilitating the evaluation of the generated synthetic data. The labels used for evaluation in Sect. IV-D are extracted from the REFLACX dataset [4], annotated by five radiologists. In the dataset, we have the following clinical features: (1) temperature (in Fahrenheit degrees), (2) heartrate (beats rate per minute), (3) resprate (breaths rate per minute), (4) o2sat (peripheral oxygen saturation as a percentage), (5) sbp, dbp (systolic and diastolic blood pressure, respectively, measured in millimeters of mercury (mmHg)). Additionally, the dataset incorporates ground truth labels indicating the presence/absence of lesions within the chest X-ray images of patients. We selected five distinct lesions for consideration: atelectasis, consolidation, enlarged cardiac silhouette, pleural effusion, and pleural abnormality. The rationale behind selecting these specific lesions stems from their prevalence as the most frequently occurring conditions within the REFLACX dataset.\nB. Experiments\nTo evaluate our proposed DALL-M framework, we conducted a set of experiments with two main objectives: (1) to evaluate the capacity of LLMs to generate contextual synthetic values for existing clinical features and (2) their ability to create entirely new clinically relevant features. Our experimental setup was anchored by high-powered NVIDIA 4090 24GB GPUs and Intel i9-13900K CPUs.\nTo achieve these objectives, we designed the following experimental setup.\nExperiment I. We aimed to understand whether LLMs can generate values for existing clinical features based on the patient's X-ray report. We compared the LLM-generated values to those produced by traditional Gaussian-based permutation methods. We hypothesized that LLMs could generate clinically contextual and patient-specific values for the existing features in our dataset better than context-agnostic approaches such as Gaussian-based permutation methods.\nAfter validating that LLMs can generate contextual values for existing features, the next step was to understand the effectiveness of each module in the proposed DALL-M framework (Figure 1). In Phase I of DALL-M (Section III-A), we conducted an information retrieval step where we queried Radiopaedia and Wikipedia for information about specific pathologies, such as atelectasis, and stored the retrieved information in a database. This database will be used to answer the seven key questions covering various clinical dimensions, from general clinical knowledge to patient-specific details (Section III-B).\nExperiment II. We aimed to understand the impact of using Radiopaedia versus Wikipedia in retrieval and determine the best data representation for storing our information. We hypothesize that using a vector database, combined with documents from both Wikipedia and Radiopaedia, will enable the LLM to provide better answers to the seven domain expert questions for several reasons. First, Wikipedia provides broad medical knowledge, while Radiopaedia offers detailed radiology-specific information, ensuring the LLM accesses general and specialized knowledge. Radiopaedia's specialized content aligns with radiological practices, helping the LLM generate more relevant and specific responses. Additionally, vector databases capture semantic relationships more effectively, enabling better storage and retrieval of high-dimensional data, which improves the LLM's ability to generate accurate responses. Combining general and specialized sources enriches the LLM's information base, enhancing the quality and depth of its answers.\nAfter validating the best information representation mechanism, we aimed to understand the quality of the answers generated by the LLM when addressing the seven key questions covering various clinical dimensions, from general clinical knowledge to patient-specific details (Section III-B). The LLM's answers are stored in the Augmented Clinical Knowledge (ACK)module and used to build prompts for the Synthetic Clinical Value Generation module. After identifying the optimal choices for each module in our framework, we tested the overall DALL-M framework for generating new synthetic features.\nExperiment III. This experiment evaluated the DALL-M framework's overall performance in generating new synthetic features and its impact on clinical data augmentation. We processed 799 patient case data points, initially comprising only eight features, to include new features and their corresponding synthetic values, as established in Experiment I. To rigorously assess the contribution of these augmented features to the dataset, we employed three distinct analytical approaches. First, we evaluated the augmented dataset using various ML models (Decision Trees, Random Forest, XGBoost, and TabNet) and measured performance metrics such as accuracy and AUC. Second, we conducted an ablation study to determine the impact of incorporating ACK into the prompt generation phase. Finally, we performed a feature importance analysis, which medical experts reviewed to validate the clinical relevance of the identified features."}, {"title": "C. Experiment I: Augmenting existing clinical features with synthetic values", "content": "This experiment evaluated the potential of LLMs to accurately generate synthetic values for pre-existing clinical features within the MIMIC-IV dataset. The criticality of this task cannot be overstated, given the propensity of LLMs to produce \"hallucinations\u201d or inaccuracies that, within a medical context, could lead to significant ramifications for patient care. To mitigate these risks and ensure the generation of clinically relevant values, we employed a structured approach involving generating a prompt (see Fig. S2 in Supplementary Material). We set the temperature parameter to 0.1 and applied eight clinical features from the MIMIC-IV dataset (oxygen saturation, temperature, gender, age, heart rate, o2sat, systolic (sbp), and diastolic blood pressure (dbp)) to 799 patient cases.\nWe analyzed the capabilities of different types of LLMs to generate clinically relevant synthetic data. We tested nine different LLMs ranging from medical domain-specific models (such as BioGPT [24], ClinicalBERT [16], BioClinicalBERT [1], and Medtiron [9]) to more general models (Mistral [18], Zehyr [32], Llama2 [17], GPT-3.5, and GPT-4). Figure S2 of Supplementary Material presents an example of a prompt to generate values for existing features based on the patient's clinical context.\nTo quantify the deviation of LLM-generated values from actual clinical data, we employed the Mean Squared Error (MSE) metric, comparing the generated values against the ground truth for each feature. We use this metric to indicate the LLM's accuracy and reliability in synthesizing clinical data.\nWe used a simple Gaussian distribution to generate clinical agnostic feature values as a baseline. Furthermore, to evaluate the performance of LLMs against advanced ML models, we introduced a comparison with a multimodal transformer (MMTF) inspired by ViT. ViTs [11], which leverage self-attention mechanisms to process image data, represent a cutting-edge approach to analyzing visual inputs. In this context, we utilized a multimodal variant to integrate both limited clinical data and the textual chest X-ray reports with the corresponding chest X-ray images. This integration assessed whether a multimodal approach could surpass the LLMs' accuracy and relevance of generated clinical feature values.\nThe experiment was structured to run each query through the multimodal vision transformer, mirroring the approach taken with the LLMs to ensure comparability. Table I reports the results using MIMIC-IV, where GPT-4 provides the best performance using mean MSE (i.e., average overall features)."}, {"title": "D. Experiment II: Information Retrieval and Data Storage", "content": "In Experiment II, we evaluated the effectiveness of information retrieval and data storage mechanisms by comparing LLMs' performance using data from Radiopaedia and Wikipedia. The experiment focused on two main aspects: the information sources and the data representation types.\nUsing the best LLM from Experiment I, GPT-4, we conducted queries using three configurations: Wikipedia alone, Radiopaedia alone, and combined. Due to the high costs of using GPT-4, we limited our experiments to a single query: \"Atelectasis.\u201d The following conditions were tested: (a) all documents from the first result page of Wikipedia (approximately 10 documents); (b) all documents from the first result page of Radiopaedia (approximately 20 documents); (c) the top result from Wikipedia (1 document); (d) the top result from Radiopaedia (1 document); and (e) all documents from the first result pages of both Wikipedia and Radiopaedia (approximately 30 documents total). For each setting, we tested three types of data representation: NetworkX, Neo4j Vector databases, and Neo4j Graph databases. The goal was to retrieve information for the domain questions outlined in Phase II: (1) What is Atelectasis?, (2) What are the symptoms of Atelectasis?, (3) What can cause Atelectasis?, (4) What clinical features do radiologists need for diagnosing Atelectasis?, and (5) What are the radiographic features of Atelectasis?\nThe best information was extracted when combining the results of both Wikipedia and Radiopaedia (Supplementary Material, Table SI) presents the best results in terms of the number of new features identified by the LLM, which were achieved using a mix of Wikipedia and Radiopaedia documents.\nOur experiments show that the best data representation is the Neo4jVector. This is due to its ability to capture and retrieve semantic relationships within high-dimensional data efficiently. The Neo4j Vector database outperformed other methods by providing more contextually relevant and accurate responses to the domain questions. Its advanced indexing and search capabilities allowed for better integrating diverse information sources, ultimately enhancing the LLM's ability to generate high-quality, clinically relevant features. This demonstrates that leveraging a combination of broad and specialized knowledge stored in a semantically rich vector format significantly improves the performance of the LLM in clinical data augmentation tasks."}, {"title": "E. Experiment III: Context-Aware Feature Augmentation", "content": "Building on the initial assessment of LLMs to generate realistic values for existing clinical features, this experiment evaluates DALL-M's capability to create new, clinically relevant features to enhance the dataset comprehensively. This investigation involved processing 799 patient case data points, originally comprising only eight features, to include new features and their corresponding synthetic values, as established in Experiment I. We termed this method Augmented, containing 78 features. Furthermore, we consulted medical experts to leverage clinical expertise and enrich the dataset. They reviewed the newly generated features and recommended including specific clinical features based on their professional experience. The subsequent generation of values, denoted as Augmented with Expert Input, yielded 91 features.\nTo rigorously assess the contribution of these augmented features to the dataset, three distinct analytical approaches were employed: (1) We evaluated the augmented dataset using various ML models, including Decision Trees, Random Forest, XGBoost, and TabNet. This analysis used metrics such as accuracy and area under the curve (AUC); (2) An ablation study was conducted to discern the impact of incorporating ACK into the prompt generation phase on the overall effectiveness of the augmentation process; (3) The ML models employed underwent a feature importance analysis. Our medical experts then reviewed the results to validate the clinical relevance of the identified features.\nWe documented the outcomes in Table II. An example of feature importance distribution for TabNet appears in Figure 2, with expert commentary on the identified features' clinical validity discussed in Sect. V. For the feature importance results for the remaining ML algorithms (Decision Tree, Random Forest, and XGBoost, please see Figure S4 in Supplementary material)."}, {"title": "V. DISCUSSION", "content": "Our experiments with the DALL-M framework highlight the accuracy of LLMs in generating synthetic clinical values and creating new clinically relevant features. The precision of models like GPT-4 and the enhancements from integrating expert insights are key focuses. We examine the impact of these advancements on predictive modeling, clinical decision-making, and patient care and contemplate the future role of AI in healthcare. The main contributions are as follows:\n(1) LLMs Can Generate Clinically Relevant Synthetic Values: Experiment I demonstrates the potential of LLMs, particularly GPT-3.5 and GPT-4, to accurately generate synthetic values for existing clinical features. Using structured prompts, our evaluation through MSE suggests the models' ability to produce values close to real clinical data, with GPT-4 showing the best performance among LLMs.\n(2) Multimodal Approaches Offer Further Improvements: Integrating LLMs with other data modalities, such as images, could enhance the accuracy and relevance of generated clinical feature values. Although we explored this only in Experiment I, further research could incorporate these models in Phase III.\n(3) General LLMs perform better than Domain-Specific ones: Experiment I shows that general LLMs outperform domain-specific LLMs in generating synthetic clinical values. This may be due to several factors: (i) general LLMs are trained on a vast and diverse dataset, including ample medical literature, which may surpass the performance of models trained on narrower datasets, and (ii) general LLMs are updated regularly, leading to up-to-date information.\n(4) Creation of New Clinically Relevant Features with Improved Dataset Performance: Experiment II demonstrates DALL-M's capability to augment existing datasets with synthetic values and expand these data with entirely new, clinically relevant features. Without medical expert input, DALL-M extended the eight original features to 78 features, enhancing the classifiers' performance across all metrics, with a significant improvement of approximately 25% in Precision and Recall, and 16.5% in F1 score for XGBoost. Additionally, the data augmentation with specific expert knowledge generated 13 new features, totaling 91 features. It contributed to an increase in Precision of approximately 3% for Random Forests, indicating the practical benefits of DALL-M in feature augmentation for clinical predictive modeling. XGBoost consistently outperformed others across most metrics, confirming the superiority of tree-based models in tabular data [12], [13].\n(5) Ablation studies confirm Augmented Clinical Knowledge is vital: The ablation study indicates that integrating augmented clinical knowledge into the feature generation process yields benefits. As shown in Table II, the absence of augmented clinical knowledge results in a notable performance decline across all models. This underscores the role of augmented clinical knowledge in enhancing the ability of LLMs to produce reliable and precise synthetic values. Nevertheless, our approach requires ethical scrutiny for future iterations, to ensure accuracy, transparency, and mitigate biases [10], [26], [30]."}, {"title": "VI. CONCLUSION", "content": "This study proposes two distinct strategies for augmenting clinical datasets. The first approach addresses the issue of missing values by generating realistic synthetic values using trained LLMs and multimodal transformers for supervised learning. This method ensures the dataset is complete and robust, allowing machine learning models to learn more effectively.\nThe second approach focuses on generating clinically relevant features by leveraging the combined power of LLMs and medical experts' domain knowledge. This method goes beyond simply filling in gaps; it creates new, valuable data points that enhance the overall quality of the dataset. By incorporating expert insights, we ensure the generated features are statistically sound and clinically meaningful."}]}