{"title": "DALL-M: Context-Aware Clinical Data Augmentation with LLMs", "authors": ["Chihcheng Hsieh", "Catarina Moreira", "Isabel Blanco Nobre", "Sandra Costa Sousa", "Chun Ouyang", "Margot Brereton", "Joaquim Jorge", "Jacinto C. Nascimento"], "abstract": "X-ray images are vital in medical diagnostics, but their effectiveness is limited without clinical context. Radiologists often find chest X-rays insufficient for diagnosing underlying diseases, necessitating comprehensive clinical features and data integration. We present a novel technique to enhance the clinical context through augmentation techniques with clinical tabular data, thereby improving its applicability and reliability in Al medical diagnostics. To address this, we introduce a pioneering approach to clinical data augmentation that employs large language models (LLMs) to generate patient contextual synthetic data. This methodology is crucial for training more robust deep learning models in healthcare. It preserves the integrity of real patient data while enriching the dataset with contextually relevant synthetic features, significantly enhancing model performance.\nDALL-M uses a three-phase feature generation process: (i) clinical context storage, (ii) expert query generation, and (iii) context-aware feature augmentation. DALL-M generates new, clinically relevant features by synthesizing chest X-ray images and reports. Applied to 799 cases using nine features from the MIMIC-IV dataset, it created an augmented set of 91 features. This is the first work to generate contextual values for existing and new features based on patients' X-ray reports, gender, age and to produce new contextual knowledge during data augmentation.\nEmpirical validation with machine learning models, including Decision Trees, Random Forests, XGBoost, and TabNET, showed significant performance improvements. Incorporating augmented features increased the F1 score by 16.5% and Precision and Recall by approximately 25%. DALL-M addresses a critical gap in clinical data augmentation, offering a robust framework for generating contextually enriched datasets.", "sections": [{"title": "I. INTRODUCTION", "content": "In deep learning, data augmentation is crucial for enhancing the diversity and robustness of training datasets. For image data, techniques such as rotating, flipping, and altering pixel values enable models to extract novel features that would not be present in unaltered images. Similarly, text data can be augmented by masking, replacing, or repositioning words, which helps models learn better contextual representations. However, when it comes to tabular data, particularly in clinical settings, the approach to data augmentation is less straightforward and fraught with challenges.\nA common method for augmenting tabular data involves introducing noise by randomly permuting values, which can corrupt the data and necessitate model reconstruction [3]. While this may work in some contexts, it poses significant risks in clinical settings where measurements are highly specific to a patient's condition. Random permutations can lead to biased and erroneous representations, undermining the model's reliability. Moreover, unlike image augmentation, where new information such as edges, textures, and positions are generated, permuting clinical tabular data does not inherently produce new knowledge. For example, consider a 70-year-old female patient with a consolidation in the left lung, high blood pressure, and high temperature. We lack mechanisms to augment this knowledge by inferring additional clinical features, such as the likelihood of the patient experiencing shortness of breath or chest pain, or even more advanced insights, like the probability of the patient being a smoker. This example highlights a significant gap in our ability to generate enriched, contextually relevant data in clinical settings. Tabular data augmentation remains an open research question with profound implications on the accuracy, reliability, and generalizability of deep learning models, particularly in critical domains such as healthcare, finance, and scientific research, where the integrity and context of the data are essential.\nLarge Language Models (LLMs) have triggered a transformative shift within the Artificial Intelligence (AI) and deep learning (DL) communities due to their model architectures and extensive pre-training across diverse corpora."}, {"title": "II. RELATED WORK", "content": "Medical Large Language Models (Med-LLMs) can be a trustworthy tool in clinical workflow settings as they bring (i) enhanced medical knowledge comprehension, (ii) accuracy improvement in the diagnosis, and (iii) recommendation for personalized patient treatment. The above capabilities allow for more accurate decisions and improve patient care quality and treatment outcomes. In the healthcare arena, we can easily find several Med-LLMs e.g., ChiMed-GPT [31], MedicalGPT [25], HuatuoGPT-II [8], or ChatMed [41], which have garnered increasing interests in healthcare and biomedical research fields. Existing works can be framed into the following categories [22]: (i) existing Med-LLMs in the medical field, and (ii) in providing effective clinical and patient treatment.\nRegarding (i), a notable class of existing Med-LLMs methodologies exist, and we now summarize their background. HuatuoGPT [8] is based on reinforcement learning and aims to align LLMs with distilled and real-world data. ClinicalT5 [23] aims to leverage general LLMs and the specialized nature of clinical text. Towards this, the model is pre-trained using the span-mask denoising objective on a large corpus of clinical notes from the MIMIC-III database. ClinicalGPT [33] incorporates quite different sources of information, comprising diverse real-world data in the training process, (i.e. medical records), domain-specific knowledge, and multi-round dialogue consultations. ChiMed-GPT [31] is built upon Ziya-13B-v2, a general domain Chinese LLM, and uses a comprehensive training regime. It is a holistic approach capturing domain-specific knowledge and aligning human preferences. BioGPT [24] specifically designed for biomedical text generation, brings inspiration from GPT-based models. BioGPT aims to improve the limited generation ability for biomedical domain-specific language models, which primarily focus on BERT-based architectures. PubMedBERT [14] is pre-trained from scratch on a large biomedical text being trained only in-domain text. This allows for a better capture of biomedical language's subtle distinctions and complexity. GatorTron [40] is specifically designed to extract and capture patient characteristics from electronic health records (EHRs) and to answer medical queries. Med-PaLM [28] improves the application of Med-LLMs, through timely adjustments to PaLM, show promising prospects in the MultiMedQA benchmark tests and highlight areas needing improvement to meet healthcare standards. MedAlpaca [15] promotes model performance significantly by fine-tuning the open-source Al model LLaMa with specialized training datasets. MedAlpaca aims to facilitate research and development in medical imaging, ultimately contributing to patient care and medical research advancements. LLaVA-Med [20] demonstrates effectiveness in medical image analysis, enabling it to interpret visual medical data within a contextual framework. Specifically, using X-rays and CT scans, can cause pathological conditions in the patients. This Med-LLM is equipped with multimodal dialogue capabilities"}, {"title": "III. DALL-M: DATA AUGMENTATION WITH LLMs", "content": "The framework encompasses three main phases: (1) Clinical Context Extraction and Storage, (2) Expert Input Queries and Prompt Generation, and (3) Context-Aware Feature Augmentation. This pipeline ensures a comprehensive data augmentation process, from extracting clinical context to generating augmented features, all while leveraging the analytical power of LLMs.\n\nA. Phase I: Clinical Context Extraction and Storage\nThis phase is specifically designed for the systematic extraction and secure storage of clinical context and medical history relevant to each patient. Initiating this phase with a particular patient case, the proposed pipeline consists of constructing a comprehensive repository encapsulating the contextual clinical knowledge associated with the patient's conditions. Here, \u201cconditions\u201d refer to lesions identified through ground truth labels derived from the REFLACX dataset. Utilizing these lesion labels as a foundation, we query and retrieve documents from two distinguished online resources: Radiopedia [27] and Wikipedia [36]. Radiopedia, a collaborative online platform dedicated to radiology, offers an extensive compilation of radiology cases, articles, and reference materials, serving as a pivotal educational and knowledge-sharing tool within the radiology domain. On the other hand, Wikipedia stands as a vast, freely accessible web-based encyclopedia orchestrated by the Wikimedia Foundation, catering to a wide array of topics across multiple languages.\nTo process and systematically organize the information gleaned from these sources, we employ a Retrieval Augmented Generation (RAG) approach, further refined through applying LLMs. This strategy enables the transformation of the retrieved documents into a semantically rich and structured format. Our exploration of various database technologies, including NetworkX Graph, Neo4j Graph, and Neo4j Vector Index, determined that Neo4j with vector index facilitates superior information extraction and storage capabilities. Accordingly, this methodology was selected for our project. This RAG process was meticulously executed for each of the five lesion labels, yielding a domain-specific clinical dataset that encapsulates the relevant clinical context and ensures efficient storage.\nB. Phase II: Expert Input Queries and Prompt Generation\nThis phase focuses on generating contextually rich domain knowledge resembling radiologists' process when evaluating an X-ray image. Through structured interviews with radiologists, we sought to understand the key questions that arise when they encounter a new case. Their insights were distilled into seven principal questions covering various clinical dimensions:\n1) General Clinical Domain Knowledge\n\u2022\tWhat are the symptoms associated with {lesion}?\n\u2022\tWhat can cause {lesion}?\n2) Observational Analysis: What the patient complains about\n\u2022\tWhat are the patient's symptoms that are relevant for {lesion}?\n3) Physical Analysis: What the radiologists observe\n\u2022\tWhat are the relevant clinical signs for the etiological diagnosis of {lesion}?\n\u2022\tWhat are the relevant clinical characteristics for the etiological diagnosis of {lesion}?\n4) Laboratory Analysis: What do the exams say?\n\u2022\tWhat are the relevant laboratory data for the etiological diagnosis of {lesion}?\n5) Patient Characteristics: Patient's demographics\n\u2022\tWhat is the patient's personal, relevant history for the etiological diagnosis of {lesion}?\nUtilizing these expert-informed questions, we construct prompts that seek to address them through an RAG approach, wherein LLMs serve as intermediaries. These LLMs solicit and retrieve responses from the clinical domain vector database established in Phase I. The responses offer enriched and augmented clinical insights specific to the patient's case.\nThis approach ensures a comprehensive and understanding of the patient's condition, blending expert radiological inquiry with advanced computational techniques. This allows the generation of an Augmented Clinical Knowledge (ACK) source capturing valuable context information, such as explanations of the lesions, symptoms that may cause the lesion, and potential patient-specific attributes that may contribute to the lesions (for instance, overweight, etc.). This information will be processed in Phase III to extract new contextual and relevant clinical features for each patient's condition.\nC. Phase III: Context-Aware Feature Augmentation\nThe overarching goals of this phase are twofold: (1) to identify and generate new clinical features and (2) to assign values to these newly identified features. Leveraging the ACK corpus developed in Phase II, this phase extracts novel clinical features from the accumulated corpus. Recognizing these features directly from textual data poses significant challenges, even with advanced techniques such as named entity recognition. To navigate these complexities, we employ LLMs equipped with few-shot learning capabilities to discern clinical features, identifying m distinct features extracted from the Phase II documents.\nHowever, merely identifying relevant clinical features is insufficient for augmenting a clinical dataset. Generating values for these features is equally crucial to ensure their applicability and utility. To this end, we revisit the RAG approach, formulating prompts that incorporate four pivotal sources of information: (1) prior contextual knowledge derived from the ACK in Phase II; (2) the newly identified features; (3) the domain knowledge database established in Phase I; and (4) the patient's radiology report alongside demographic details. Consequently, our prompts returned a list of features and corresponding values, which were integrated into our original dataset, culminating in enriched and augmented context-aware datasets. The overall structure of our prompt is presented in Supplementary material (Figure S1)."}, {"title": "IV. EXPERIMENTAL SETUP", "content": "A. Dataset\nIn this work, we utilize instances from the MIMIC-IV dataset [19], comprising radiographs, radiology reports from chest X-rays, and clinical data. We gathered 799 patient cases from these datasets, with each instance accompanied by corresponding ground-truth information on clinical features, facilitating the evaluation of the generated synthetic data. The labels used for evaluation in Sect. IV-D are extracted from the REFLACX dataset [4], annotated by five radiologists. In the dataset, we have the following clinical features: (1) temperature (in Fahrenheit degrees), (2) heartrate (beats rate per minute), (3) resprate (breaths rate per minute), (4) o2sat (peripheral oxygen saturation as a percentage), (5) sbp, dbp (systolic and diastolic blood pressure, respectively, measured in millimeters of mercury (mmHg)). Additionally, the dataset incorporates ground truth labels indicating the presence/absence of lesions within the chest X-ray images of patients. We selected five distinct lesions for consideration: atelectasis, consolidation, enlarged cardiac silhouette, pleural effusion, and pleural abnormality. The rationale behind selecting these specific lesions stems from their prevalence as the most frequently occurring conditions within the REFLACX dataset.\nB. Experiments\nTo evaluate our proposed DALL-M framework, we conducted a set of experiments with two main objectives: (1) to evaluate the capacity of LLMs to generate contextual synthetic values for existing clinical features and (2) their ability to create entirely new clinically relevant features. Our experimental setup was anchored by high-powered NVIDIA 4090 24GB GPUs and Intel i9-13900K CPUs. To achieve these objectives, we designed the following experimental setup.\nExperiment I. We aimed to understand whether LLMs can generate values for existing clinical features based on the patient's X-ray report. We compared the LLM-generated values to those produced by traditional Gaussian-based permutation methods. We hypothesized that LLMs could generate clinically contextual and patient-specific values for the existing features in our dataset better than context-agnostic approaches such as Gaussian-based permutation methods."}, {"title": "V. DISCUSSION", "content": "Our experiments with the DALL-M framework highlight the accuracy of LLMs in generating synthetic clinical values and creating new clinically relevant features. The precision of models like GPT-4 and the enhancements from integrating expert insights are key focuses. We examine the impact of these advancements on predictive modeling, clinical decision-making, and patient care and contemplate the future role of AI in healthcare. The main contributions are as follows:\n(1) LLMs Can Generate Clinically Relevant Synthetic Values: Experiment I demonstrates the potential of LLMs, particularly GPT-3.5 and GPT-4, to accurately generate synthetic values for existing clinical features. Using structured prompts, our evaluation through MSE suggests the models' ability to produce values close to real clinical data, with GPT-4 showing the best performance among LLMs.\n(2) Multimodal Approaches Offer Further Improvements: Integrating LLMs with other data modalities, such as images, could enhance the accuracy and relevance of generated clinical feature values. Although we explored this only in Experiment I, further research could incorporate these models in Phase III.\n(3) General LLMs perform better than Domain-Specific ones: Experiment I shows that general LLMs outperform domain-specific LLMs in generating synthetic clinical values. This may be due to several factors: (i) general LLMs are trained on a vast and diverse dataset, including ample medical literature, which may surpass the performance of models trained on narrower datasets, and (ii) general LLMs are updated regularly, leading to up-to-date information.\n(4) Creation of New Clinically Relevant Features with Improved Dataset Performance: Experiment II demonstrates DALL-M's capability to augment existing datasets with synthetic values and expand these data with entirely new, clinically relevant features. Without medical expert input, DALL-M extended the eight original features to 78 features, enhancing the classifiers' performance across all metrics, with a significant improvement of approximately 25% in Precision and Recall, and 16.5% in F1 score for XGBoost. Additionally, the data augmentation with specific expert knowledge generated 13 new features, totaling 91 features. It contributed to an increase in Precision of approximately 3% for Random Forests, indicating the practical benefits of DALL-M in feature augmentation for clinical predictive modeling. XGBoost consistently outperformed others across most metrics, confirming the superiority of tree-based models in tabular data [12], [13].\n(5) Ablation studies confirm Augmented Clinical Knowledge is vital: The ablation study indicates that integrating augmented clinical knowledge into the feature generation process yields benefits. As shown in Table II, the absence of augmented clinical knowledge results in a notable performance decline across all models. This underscores the role of augmented clinical knowledge in enhancing the ability of LLMs to produce reliable and precise synthetic values. Nevertheless, our approach requires ethical scrutiny for future iterations, to ensure accuracy, transparency, and mitigate biases [10], [26], [30]."}, {"title": "VI. CONCLUSION", "content": "This study proposes two distinct strategies for augmenting clinical datasets. The first approach addresses the issue of missing values by generating realistic synthetic values using trained LLMs and multimodal transformers for supervised learning. This method ensures the dataset is complete and robust, allowing machine learning models to learn more effectively.\nThe second approach focuses on generating clinically relevant features by leveraging the combined power of LLMs and medical experts' domain knowledge. This method goes beyond simply filling in gaps; it creates new, valuable data points that enhance the overall quality of the dataset. By incorporating expert insights, we ensure the generated features are statistically sound and clinically meaningful."}, {"title": "VII. SOURCE CODE", "content": "The DALL-M source code is publicly available at:\nhttps://github.com/ChihchengHsieh/DALL-M"}]}