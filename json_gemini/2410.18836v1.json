{"title": "From English-Centric to Effective Bilingual: LLMs with Custom Tokenizers for Underrepresented Languages", "authors": ["Artur Kiulian", "Anton Polishko", "Mykola Khandoga", "Yevhen Kostiuk", "Guillermo Gabrielli", "\u0141ukasz G\u0105ga\u0142a", "Fadi Zaraket", "Qusai Abu Obaida", "Hrishikesh Garud", "Wendy Wing Yee Mak", "Dmytro Chaplynskyi", "Selma Belhadj Amor", "Grigol Peradze"], "abstract": "In this paper, we propose a model-agnostic\ncost-effective approach to developing bilingual\nbase large language models (LLMs) to sup-\nport English and any target language. The\nmethod includes vocabulary expansion, initial-\nization of new embeddings, model training and\nevaluation. We performed our experiments\nwith three languages, each using a non-Latin\nscript-Ukrainian, Arabic, and Georgian.\nOur approach demonstrates improved language\nperformance while reducing computational\ncosts. It mitigates the disproportionate penaliza-\ntion of underrepresented languages, promoting\nfairness and minimizing adverse phenomena\nsuch as code-switching and broken grammar.\nAdditionally, we introduce new metrics to eval-\nuate language quality, revealing that vocabulary\nsize significantly impacts the quality of gener-\nated text.", "sections": [{"title": "1 Introduction", "content": "The discovery of the Transformer architec-\nture (Vaswani et al., 2017) has opened doors for\ncreating large language models (LLMs) with bil-\nlions of parameters, trained on datasets of trillions\nof tokens. One of the notable features of the LLMS\nis cross-lingual language understanding (XLU),\nwhich allows models to possess multilingual capa-\nbilities. However, the XLU ability is restricted by\nthe so-called curse of multilinguality, which refers\nthe difficulties and constraints encountered in creat-\ning multilingual LLMs. Studies showed that a sub-\nstantial drop in performance occurs as the number\nof languages increases, due to the model's limited\ncapacity to adequately capture and represent the\nnuances of each language (Conneau et al., 2020).\nThe efforts to examine and address the problem\nhave highlighted two key factors: the composition\nof the dataset and vocabulary composition (Pfeif-\nfer et al., 2022; Blevins et al., 2024). Some stud-\nies (Chang et al., 2023) suggest that the natural\nlimitations on the model capacity, vocabulary and\ntraining dataset sizes along with differences in lan-\nguage structures do not allow the creation of the\nultimate multilingual model to perform equally in\nmany languages, favoring the creation of custom\nmodels targeted at specific languages instead.\nThe most obvious yet often overlooked conse-\nquence of low language representation in a model's\nvocabulary is a much higher cost of language pro-\ncessing. A sentence in Ukrainian requires about\n3 times more tokens for the GPT-4 model (Ope-\nnAI et al., 2024) than the same sentence in En-\nglish due to higher tokenization fertility (see Sec-\ntion 6.1). Three times higher fertility means three\ntimes smaller context window, three times higher\nmemory usage, and nine times higher computation\ncost due to attention's quadratic dependence on the\nsequence length. On the other hand, high computa-\ntional costs are not the only ramifications of a poor\nvocabulary. Recent studies (Rust et al., 2021a) in-\ndicate that representation in an LLM vocabulary\nof a specific language directly relates to the per-\nformance of the model in that language (Petrov\net al., 2023). In particular, it may be a reason\nfor the generation of non-existing words, code-\nswitching (Winata et al., 2021; Zhang et al., 2023),\nand broken grammar. Languages that use a non-\nLatin alphabet are particularly affected by poor vo-\ncabulary representation since they cannot rely even\non the overlapping tokens with better represented\nlanguages.\nAn insufficient training dataset affects the per-\nformance of LLMs as much as it does any other\ndeep learning model. The model might generate a\nresponse in the wrong language, probably the one\nit is most familiar with, such as English (Marchisio\net al., 2024). In this work, exposing the model to\nadditional data in the target language via continual\npre-training helped mitigate these effects.\nIn this paper, we present a model-agnostic\nresource-effective method to create a base bilin-"}, {"title": "2 Related Work", "content": "The shortcomings of existing multilingual LLMs\nhave motivated numerous scholars and practition-\ners to address the insufficient performance of un-\nderrepresented languages.\nPerhaps the most fundamental approach is to\ndesign and train a model from scratch, as demon-\nstrated by EuroLLM (Martins et al., 2024). While\nthis method offers maximal flexibility, it is highly\ndemanding in terms of effort and computational\nresources.\nMore commonly, available open-source LLMs\nare used as a starting point, leveraging transfer\nlearning and building on available weights (Tejaswi\net al., 2024). This can still involve significant ar-\nchitectural changes compared to other methods, as\nseen in the SOLAR model (Kim et al., 2024). De-\nspite utilizing transfer learning, such approaches\noften require pre-training on vast datasets, some-\ntimes reaching trillions of tokens.\nA number of publications (Cui et al., 2024; \"he-\nmanth kumar\"; Nguyen et al., 2023; Vo, 2024)\nsuggest a more lightweight approach, where the\nmodel's vocabulary is extended by 10,000-20,000\ntokens, entailing the extension of the embedding\nlayer and the language modeling head, while leav-\ning the rest of the architecture unchanged. This\nmethod reduces the required training dataset to hun-\ndreds, or even tens, of billions of tokens, while still\ndelivering notable improvements in the model's\nlanguage abilities and computational efficiency.\nFinally, instruction fine-tuning (Basile et al.,\n2023; Azime et al., 2024; Kohli et al., 2023) offers\na highly resource-efficient alternative by skipping\nthe base model composition step. While this ap-\nproach can yield some improvements, it does not\nenhance the model's factual knowledge or address\ntokenization issues.\nOur approach, in contrast, maintains the overall\nvocabulary size and keeps the model architecture\nintact. To create a bilingual model, we extend the\nvocabulary of the target language at the expense of\nother languages in the model, except English. This\nallows us to reduce the pre-training dataset to as\nlittle as 2 billion tokens while still improving the\nmodel's factual knowledge, enhancing the dataset,\nand achieving visible improvements in target lan-\nguage generation."}, {"title": "3 Methodology", "content": "Our proposed pipeline for training of bilingual\nLLMs supporting English and a target language\nL consists of the following steps:\n1.  Vocabulary Extension. The aim of this step\nis to create a new bilingual tokenizer T that\nretains the exact tokenization for English as\nin the original model, while incorporating an\nextended vocabulary for the target language\nL, thus reducing fertility.\n2.  Embeddings Initialization. Initialize new\nembedding vectors for the newly added L-\nspecific tokens."}, {"title": "3.2 Embeddings Initialization", "content": "Upon the vocabulary extension, the embedding vec-\ntors for the new tokens must be reinitialized. A\nproper embedding initialization can significantly\nimprove the training convergence speed, while fail-\ning to do so might lead to a slower convergence or\neven non-convergence (Glorot and Bengio, 2010).\nIn our experiments, we have tried a number of\nembedding initialization techniques, such as ran-\ndom, mean (Hewitt, 2021), FOCUS (Dobler and\nde Melo, 2023) and technique we called Natural\nCharacter Overlap Segmentation (NACHOS). We\nselected NACHOS because it has shown better con-\nvergence during training (see Appendix A). NA-\nCHOS works as follows. New tokens in $T_{en-L}$\nare expressed through the tokens that have already\nexisted in the original tokenizer model $T_o$. Every\nlonger token $t_{new}$ can be split into a n of shorter\ntokens t: $t_{new} \\rightarrow (t_1...t_n)$, with shorter tokens be-\nlonging to the overlapping vocabulary. We then\ninitialize the embeddings of these new tokens by\ncomputing the mean of the shorter tokens embed-\ndings (see Eq. 1):\n$E(t_{new}) = \\frac{1}{n} \\sum_{1}^{n} E(t_n)$,\nwhere $E(t_{new})$ represents the embedding vector of\nthe new token, E(tn) denotes the embeddings of\nthe overlapping token tn into which the new token\nis segmented."}, {"title": "3.3 Continual pre-training", "content": "As a final step, the newly composed model with the\nextended vocabulary and initialized embeddings\nis trained on the bilingual parallel corpora. This\nallows the model to fully adopt the new tokens,\nwhich we have verified by checking the token IDs\nof the model output. This process of new token\nadoption is put under scrutiny and discussed in\ndetail in Section 7.2."}, {"title": "4 Datasets", "content": "Vocabulary Extension Datasets The monolin-\ngual language-specific tokenization models $T_c$\nhave been trained on monolingual datasets. For\nthe Ukrainian language we've trained on the pub-\nlicly available UberText 2.0 (Chaplynskyi, 2023),\nthat contains 3.274B words and consists of 8.59M\ntexts.\nTo train an Arabic tokenizer we have used a pri-\nvate dataset of non-fiction books of 430 million\nwords based on (ACRPS). For Arabic, we inte-\ngrated one more additional preprocessing step. As\nan Arabic word could correspond to several words\nin another language transmitting the same meaning,\nit is the best practice to perform light stemming\nto allow the models to pick the similarity of the\nsemantics of the main parts of words (Larkey et al.,\n2002). For example, we consider \u0627\u0644\u0640 )English trans-\nlation: the) as a separate token when it prefixes a\nword. We processed attached pronouns and gender\nspecifiers in similar way.\nFor our experiments with Georgian we have\nused the Georgian section of the public OSCAR\ndataset (OSCAR), which contains 171.9M words.\nThis dataset has been used for both tokenizer\ntraining and continual pre-training of the English-\nGeorgian Mistral model for token adoption experi-\nments.\nContinual Pre-training Datasets For continual\npre-training we created parallel datasets, consisting\nof both English and target language.\nFor Ukrainian and Arabic, we considered\nWikipedia parallel dataset dump from June 20th\n2024 archive dump\u00b9. For Ukrainian, the size of\nthe datasets is approximately 2B tokens. The total\nnumber of articles was 2.1M (791,336 in Ukrainian\nand 1,327,709 in English). The total number of\nUkrainian tokens was 1.02B and the total number\nof English tokens was 1.05B. For Arabic, the size\nof the datasets is approximately 1.8B tokens. The\ntotal number of articles was 2.1B (1.2B in Ara-\nbic and 882,534 in English). The total number of\nArabic tokens was 621.51M and the total number\nof English tokens was 1.1B. For Georgian token\nadoption experiments, we trained a model on par-\nallel corpora from the same dump. The dataset\nwas much smaller due to a sparsity of resources\nin Georgian. It contained 107,123 and 169,602 ar-\nticles in English and Georgian, respectively. The\ntotal number of tokens was approximately 395.2M\n(219.88M in English and 175.32M in Georgian).\nThe articles were shuffled to create the train-\ning dataset with equal representation of the target\nlanguage (Arabic or Ukrainian) and English. To de-\ntermine the amount of tokens, we used the Gemma\n2 tokenizer."}, {"title": "5 Experimental Setup", "content": "We continually pre-trained bilingual models on the\nnext token prediction task on the parallel corpora\nutilizing HuggingFace (Wolf et al., 2019; Tunstall\net al.) instructions for 8x80Gb GPUs. To launch\ntraining, we used the SkyPilot framework (Yang\net al., 2023). In order to isolate the effects of ex-\ntended vocabulary and additional pre-training we\nhave conducted the same pre-training for the vanilla\nmodels and then compared the performances. For\nhyper-parameter optimization we used grid search.\nThe selected set of hyper-parameters can be found\nin our GitHub repository\u00b3."}, {"title": "6 Evaluation Metrics", "content": "Since we work on the base completion model, we\nfocused mainly on the metrics that reflect the text\ncompletion performance: tokenizer fertility, code\nswitching score, non-existing words ratio, and man-\nually evaluated grammar correctness score."}, {"title": "6.1 Tokenizer Fertility", "content": "Fertility is the most common metric for evaluat-\ning tokenizer performance (Scao et al., 2023; Rust\net al., 2021b). This is an intrinsic metric of the\ntokenization model and is defined as the average\nnumber of tokens required to represent a word. For\na tokenizer T and a dataset D, fertility is calculated\nby dividing the total number of tokens in T(D) by\nthe total number of words in D."}, {"title": "6.2 Non-Existing Words Ratio (NEWR)", "content": "We used a following heuristic to detect non-existent\nwords generated by LLMs. A word is considered\nnon-existent if it is absent from a large language-\nspecific corpus or vocabulary. For Ukrainian, we"}, {"title": "6.3 Code Switching Word Ratio (CSWR)", "content": "In linguistics, code switching is a phenomenon,\nwhen a speaker uses (or \u201cswitches\" between) two\nor more different languages in a conversation. To\ndetect code switching in LLM outputs, we intro-\nduced a novel metric: Code Switching Word Ra-\ntio (CSWR). Unlike previous token-based meth-\nods (Marchisio et al., 2024), our approach uses\nlanguage-specific rules to better identify code\nswitching. The implementations are available in\nthe GitHub repository5.\nCSWR is a ratio of words in the text that in-\ncludes at least one foreign symbol (outside of the\nalphabet of the language, not a number or punctua-\ntion) and does not fit the rules of the correct code\nswitching usage. The lower this ratio is - the better\nperformance model showed from a code switching\nperspective.\nThe correct instances of code switching are de-\ntected depending on the language. A detailed ex-\nplanation and a list of rules are provided in the\nAppendix B."}, {"title": "6.4 Grammar Correctness Score (GCS)", "content": "To evaluate grammar correctness, the model gener-\nated text was evaluated by experts for the particular\nlanguage on the following criteria: usage of incor-\nrect words (e.g. wrong gender of the word, plu-"}, {"title": "7 Results", "content": "The comparison of the original model tokenizer\nwith the customized bilingual tokenizers developed\nby us via the procedure described in Section 3.1\ncan be found in Table 2. Besides Mistral with its\n32,768 tokens in the vocabulary we have also ex-\nperimented with Gemma 2, which has a vocabulary\n8 times larger. That has allowed us to substan-\ntially extend the target language vocabulary with-\nout changing the model architecture. Naturally, in\nevery case the extended vocabulary has improved\nthe tokenization fertility in the target language, al-\nlowing the model to process the same amount of\ntext at lower computational cost. The non-linear\nfertility improvement is expected due to the loga-\nrithmic character of its dependence on the vocabu-\nlary size (Tao et al., 2024)."}, {"title": "Ukrainian", "content": "In the case of the Ukrainian language,\nit was challenging to estimate the exact number\nof the language-specific tokens in the original vo-\ncabulary due to possible confusions with other lan-\nguages that use the Cyrillic alphabet. The number\npresented in the Table 2 is a lower estimate. Fertil-\nity has been measured with 13 million words from\nthe Ukrainian section of the OSCAR dataset. No-\ntably in the case of Gemma 2 we have developed\na tokenizer that ensures the same fertility for the\nEnglish and Ukrainian languages, thus reaching\nparity between the two. Parallel fertility has been\nmeasured using the Paracrawl parallel English-\nUkrainian dataset (Espl\u00e0 et al., 2019)."}, {"title": "Arabic", "content": "For the Arabic language, fertility was\nmeasured using a stemmed dataset (see Section 4).\nDue to this, the numerical fertility results for Ara-\nbic differ from those of the other languages and\ncan't be directly compared to them."}, {"title": "Georgian", "content": "The original Mistral vocabulary did\nnot cover 6 letters from the Georgian alphabet,\nwhich has forced the model to resort to byte fall-\nback (see also Section 7.2), which affected the orig-\ninal model's fertility in Georgian. Extending the\nvocabulary by 5,500 tokens has allowed to improve\ntoken usage by nearly three times. Due to Geor-\ngian dataset size limitations we were not able to\nproperly train and evaluate a Gemma-compatible\ntokenizer for the Georgian language."}, {"title": "7.2 Token Adoption Process", "content": "In this subsection, we investigate the token com-\nposition of the Mistral model output during the\ncontinual pre-training that followed the vocabu-\nlary extension for Ukrainian (Mean initialization),\nGeorgian (NACHOS initialization), and Arabic lan-\nguages respectively. The output tokens have been\nsplit into 5 categories:\n\u2022 Existing: tokens of the target language that\nexist in the default Mistral vocabulary.\n\u2022 New: tokens of the target language that were\nadded to the vocabulary.\n\u2022 English: tokens used to represent English.\n\u2022 Byte-encoded: 256 byte fallback tokens used\nto encode characters absent in the vocabulary\nin UTF-8 format.\n\u2022 Other: tokens that do not belong to any of the\nabove-mentioned categories (e.g. tokens of\nother languages, punctuation, etc.).\nOn Figure 1, Y axis of the plot corresponds to the\nrelative fraction of the tokens in each category (all\ncategories sums up to 1). In general, we observed\nsimilar phenomena in all three languages. Being\nprompted in a target language, the original Mistral\nmodel is likely to produce a response in English,\nmost probably due to insufficient pre-training on\nthe target language corpus. Once our pre-training\nstarts, the model learns to produce responses in the\ntarget language and after a few hundred training\nsteps it outputs little to no English tokens.\nAt first, the model favors the usage of familiar\ntokens that already existed in its vocabulary before\nthe extension. Subsequent pre-training teaches the\nmodel to use the new tokens along with the famil-\niar ones. After 2,000 training steps, the process\nstabilizes and becomes nearly static between 5,000\nand 10,000 steps.\nThe same pattern holds in all three of the con-\nsidered languages, though with some differences\nwhich we would like to discuss in more detail. We\nexperimented with Ukrainian, Georgian, and Ara-\nbic."}, {"title": "Ukrainian", "content": "Ukrainian is much better represented\nin Mistral model than Arabic and Georgian. The\noriginal Mistral vocabulary contains 1,731 Cyrillic\ntokens, with about 1,600 of them suitable for the\nUkrainian language representation. The original\nmodel occasionally replies in English if prompted\nin Ukrainian, producing about 35% of English\ntokens in the output. Upon the start of the pre-\ntraining the model learns to use Ukrainian tokens,\nthough initially the model tends to use the existing\nUkrainian tokens. After 200 training steps, this\nratio increases to about 65%. With more train-\ning, this number drops to 50%, indicating that the\nmodel fully adopted new tokens. However, despite\nthe new tokens make about 75% of the extended\nUkrainian vocabulary, the fraction of existing to-\nkens remains dominant due to higher frequency of\noccurrence."}, {"title": "Arabic", "content": "Qualitatively, the situation with the Ara-\nbic language is similar to that of the Ukrainian, but\nwith two important differences. When prompted in\nArabic, original Mistral is more likely to respond\nin English, with the fraction of produced English\ntokens reaching 60%. In the original Mistral vocab-\nulary there is 70 Arabic tokens, which is enough\nto avoid byte fallback, but is still a relatively small\nnumber. That is why the fraction of the new tokens\novertakes as early as 200 training steps and remains\ndominant afterwards."}, {"title": "Georgian", "content": "There are 29 Georgian tokens in the\noriginal Mistral vocabulary, which does not even\ncover the Georgian alphabet (35 letters). That\nforces the model to resort to byte fallback when\ngenerating text in Georgian more frequent than in\nUkrainian or Arabic. The fraction of the byte en-\ncodings grows when the model learns to respond in\nGeorgian and then drops along with the adoption of\nthe new tokens, similarly to previously discussed\nlanguages. In case if Georgian, the token adap-\ntation takes longer, as the model resorts to using\nthe byte encodings for the text prediction while\nlearning new tokens. Byte encodings are always\nencoded with a pair of tokens and that might ex-\nplain a longer period of adopting the new Georgian\ntokens."}, {"title": "7.3 Performance Metrics", "content": "The results for the trained model of Grammar Cor-\nrectness Score (GCS), Non-Existing Words Ratio\n(NEWR), and Code Switching Word Ratio (CSWR)\nare presented in Table 1.\nThe results showed that the model trained with\nour approach outperformed both Vanilla and Tuned\nmodels in terms of GCS in Ukrainian and Arabic.\nNotably, the vanilla model struggled with gram-\nmatical accuracy, achieving a score of 0.264 on\nUkrainian compared to the our model's score of\n0.503. Tuned English-Ukrainian model achieved\nGCS of 0.388. For Arabic, tuned model achieved\n0.238 and 0.04 for the vanilla model, demonstrat-\ning lack of grammatical knowledge. Our model\nachieved GCS score of 0.548.\nOur method demonstrated NEWR of 3%, which\nis not significantly different from the score of the\ntuned model (3.2%) for Ukrainian. The reason\nfor such similarity could be in a better representa-\ntion of Ukrainian tokens in Mistral (see Figure 1).\nVanilla model showed 8.9% of non-existing words\nin its generated texts. On the other hand, for Ara-\nbic our approach obtained NEWR of 5%, when\nvanilla and tuned models obtained 86.3% and 7.9%\nrespectively. The vanilla model's performance was\nreally poor when it comes to generating existing\nmodern Arabic words. The tuning improved the\nperformance in more than 10 times, but our model\noutperformed it.\nFinally, we achieved a score of 0.001 for CSWR\nfor Ukrainian, which indicates a very little incor-\nrect usage of foreign languages in the text. The\nsecond best score was obtained for tuned model\n(0.002). The vanilla model performed significantly\nworse: 0.515, indicating that more than half of gen-\nerated words are used incorrectly in terms of code\nswitching. For Arabic, the situation is similar. Our\nmodel obtained a score of 0.002, outperforming\ntuned model (0.004) and vanilla model (0.45)."}, {"title": "7.4 Preventing catastrophic forgetting in\nEnglish", "content": "After a series of experiments, we found that after\njust 1 epoch of training on the bilingual corpora, the\nmodels showed improvement in the target language\nbut experienced a substantial drop in the English\nMMLU benchmark (Hendrycks et al., 2021b,a).\nHowever, by lowering the learning rate from 1.5e-5 to 2e\n6, training resulted in a much smaller\nloss in MMLU benchmark points. These important\nresults demonstrate that, with the right training,\nthe model can retain its English performance and\nremain bilingual, as shown in Table 3."}, {"title": "8 Discussion", "content": "The obtained results highlight a subject that has\nbeen largely overlooked, particularly in the context\nof generative LLMs: the impact of vocabulary size\nand composition an on the quality of generated\ntext.\nOur experiments with the vanilla model pre-"}, {"title": "9 Conclusions", "content": "In this work, we introduced a model-agnostic, cost-\neffective method for developing bilingual base com-\npletion LLMs that support English and a target lan-\nguage, including low-resource or underrepresented\nlanguages. Our approach, centered on vocabulary\nextension and efficient embedding initialization,\nwas validated by creating two bilingual LLMs:\nEnglish-Ukrainian and English-Arabic. Moreover,\nwe conducted experiments with Georgian tokeniza-\ntion and explored token adoption process during\nthe training of a English-Georgian model. Geor-\ngian has a unique underreprsentation in the Mistral\ntokenizer.\nWe demonstrated that extending the vocabulary\nof a pre-trained model enhances its performance in\ntarget language while maintaining its English per-\nformance. Specifically, the grammar correctness re-\nsults indicate that pre-training alone provides only\nlimited improvement. The comparison between\nUkrainian and Arabic further emphasizes the limi-\ntations of poor vocabulary for the underrepresented\nlanguage. Expanding the tokenizer's vocabulary\nwith target language tokens reduced tokenizer fer-\ntility, resulting in lower computational costs and\nimproved processing efficiency. Finally, retaining\nthe original English tokens in the custom tokenizer\nwhile adding new language-specific tokens lead to\npreservation of the model's English performance\non the MMLU benchmark, while also improving its\nperformance in the target language from perspec-\ntive of grammar, code switching and non-existant\nword ratios.\nOur approach promotes a more equitable and\ninclusive NLP ecosystem, contributing to the revi-\ntalization of underrepresented languages. By lower-\ning the barrier to developing more literate and gram-\nmatically capable models, we believe our work also\npaves the way for enhanced economic viability of\nusing LLMs in non-English languages."}, {"title": "10 Limitations", "content": "In this work, we have focused on creating a min-\nimal working example of a base bilingual model\nwith an extended vocabulary in a cost-effective way.\nWhile our approach is model-agnostic, it has yet\nto be tested with models other than Mistral 7B.\nGemma 2 is the most likely candidate, as we have\nalready concluded tokenizer experiments. How-\never, applying the method to other open-source\nmodels, such as Llama 3 or Qwen, would provide\nfurther validation for our approach.\nAnother important limitation is that the method\nwas eventually tested only for English-Ukrainian\nand English-Arabic models. Due to the limited\navailability of Georgian corpora, we were un-\nable to complete the experiment with the English-\nGeorgian model.\nThe retention of English language capabilities\nhas only been tested with the English-Ukrainian\nmodel. We are currently in the process of testing it\nfor the English-Arabic model.\nFurther experiments with the vocabulary size and\ncomposition could help to find the optimal parame-\nters along with their dependence on the available\ndataset size and individual language properties.\nTo fully evaluate the model across a variety of\ndownstream tasks, such as machine translation,\nquestion answering, summarization, or text com-\npletion, instruction tuning will be required. This\nstep, however, goes beyond the scope of our current\nwork.\nWhile we believe that the proposed metrics for\nassessing the language quality are an important\nstep, they leave enough space for refinement. In\nparticular, the code-switching metric for Ukrainian\nand Arabic might benefit from implementing addi-\ntional rules."}, {"title": "Acknowledgements", "content": "We would like to express our gratitude to the fol-\nlowing organizations for their generous support,\nwhich made this work possible:\n\u2022 Observea for providing access to a 16xTesla\nH100 cluster, which significantly enhanced\nour computational resources.\n\u2022 NVIDIA for providing DGX Workstation\nwith 4xTesla V100 used for inference and eval-\nuations.\n\u2022 HotAisle for granting access to the 8xAMD\nMI300x node, enabling critical model training\nexperiments.\n\u2022 AWS for offering cloud credits that supported\nthe use of Tesla H200 instances for training.\n\u2022 GCP (Google Cloud) for providing credits\nused for model training and inference.\n\u2022 TPU Research Cloud (Google Cloud) for\nproviding TPU VMs for our training pipeline\nexperiments.\n\u2022 A.I. Hero for providing access to a 8xA100\ninstance for our original set of experiments.\n\u2022 Doha Graduate Studies University and\nthe Arab Center for Research and Policy\nStudies for being key strategic collaborators,\nwhose guidance and expertise greatly con-\ntributed to the development of this work.\nAll of the contributions above were instrumental in\nachieving the results presented in this paper, and\nwe look forward to continued collaborations."}, {"title": "2.0.1 Ukrainian CSWR Rules", "content": "In Ukrainian, the usage of code switching is al-\nlowed if it respects the following rules. All the\nmentions of the following entities are allowed in a\nforeign language:\n1.8\n1.7\n1.6\n\u2022 Proper names: names of the music bands, loca-\ntions, restaurants, libraries, cities, titles, iden-\ntification numbers etc. For example, Pythago-\nras, California, MIT, Metallica, F-16 and so\non.\n\u2022 Medical terms, additives and vitamins. For\nexample, (vitamin) B12, (food additive) E110\netc."}, {"title": "2.0.2 Arabic CSWR Rules", "content": "Arabic follows the following rules.\n\u2022 Arabic does not have capital letters which\nrenders named entity detection especially for\nproper names a specialized task.\n\u2022 In Arabic, both Indian or Arabic numerals can\nbe used.\n\u2022 Some Arabic characters are non-connecting\ncharacters and are written separately from the\nnext word, even if there is no space between\nthem. Arabic is written right to left, but Ara-\nbic words followed by non-Arabic words writ-\nten in the other direction (sometimes with no\nwhite space separation).\nTo address these issues, we utilized a different\nensemble of NER models, specifically Flair (Akbik\net al., 2019) pre-trained Arabic NER model (Mega-\nhed, 2021)10, transformer-based Arabic NER mod-\nels (Lan et al., 2020; Inoue et al., 2021)11, and\nStanza (Qi et al., 2020) Arabic model. Resources\nand algorithms to identify medical terms, additives,\nvitamins, hashtags, encoding names, URL links,\nfile formats, roman integers and quotes are the same\nas we introduced in the Ukrainian Code Switching\nMetric."}]}