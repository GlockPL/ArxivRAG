{"title": "Early Joint Learning of Emotion Information Makes MultiModal Model Understand You Better", "authors": ["Mengying Ge", "Mingyang Li", "Dongkai Tang", "Pengbo Li", "Kuo Liu", "Shuhao Deng", "Songbai Pu", "Long Liu", "Yang Song", "Tao Zhang"], "abstract": "In this paper, we present our solutions for emotion recognition in the sub-challenges of Multimodal Emotion Recognition Challenge (MER2024). For the tasks MER-SEMI and MER-NOISE, participants are required to recognize discrete emotions. Particularly, in MER-NOISE, the test videos are corrupted with noise, necessitating the consideration of modality robustness. We developed our Emotion ViT based on large-scale data pre-training and fine-tune, a vision feature extractor adapted to emotion recognition tasks. In addressing the modal competition between audio and text, we implemented an early fusion methodology underpinned by a large language model. This design facilitates full interaction between audio and text, thereby harmonizing their contributions. The joint Audio-Text representation can be late-fused with other features extracted from our specific unimodal encoders. To solve the problems of data insufficiency and class imbalance, we employed multiple rounds of multi-model voting for data mining. To ensure the high quality of audio features, we introduce a speech source separation method to denoise the audios. Our model secured 2nd in both MER2024-SEMI and MER2024-NOISE categories, affirming that the robustness and validity of our strategies in advancing the field of multimodal emotion recognition.", "sections": [{"title": "1 Introduction", "content": "In the rapidly advancing field of human-machine interaction, emotion recognition stands as a pivotal bridge facilitating emotional communication between humans and machines. Previous works cues from facial expressions, vocal intonations, and textual content to deeply analyze the emotional states of individuals [1]. Among these approaches, multimodal technology with its capability to integrate different modal information sources for comprehensive analysis, has garnered significant attention[2][3]. However, the emotional complexity, scarcity of high-quality annotated data and inevitable environmental noise in practical applications pose substantial challenges to the accuracy and robustness of multimodal emotion recognition. Efficient and precise feature extractors have emerged as a crucial means to overcome these bottlenecks. Traditional handcrafted features and machine learning classifiers often falter when confronted with complex nonlinear relationships, while the rise of deep learning and large-scale pre-trained models has infused new vitality into the domain of emotion recognition, exemplified by models such as VideoMAE-large[4], CLIP-large [5] for vision, wav2vec2.0 [6], HUBERT[7] for audio, and RoBERTa[8], Baichuan-13B[9] for text, which have all demonstrated remarkable performance. Although training of feature extractors and classifiers simultaneously can enhance results further, the large scale parameters in multimodal contexts typically entail prohibitive computational costs. Hence, a more efficient approach involves optimizing feature extractors during pre-training and subsequently freezing their parameters while only training fusion layers and classifier layers[10].\nWithin the realm of multimodal information processing, the information from different modalities complements one another yet may also contain redundancy or conflicts. Effectively fusing these multiple sources of information remains a core challenge in the field[11][12]. Notably, late fusion strategies incorporating attention mechanisms have shown exceptional performance in semantic-level feature integration[13]. Nonetheless, it is often observed in practice that even with improvements in individual modality models, the performance of fused models may fall short of expectations, potentially due to modality conflicts, overfitting of unimodal models, and an overreliance on a single modality at the expense of exploiting complementary advantages across modalities [14]. Thus, exploring effective joint training strategies to optimize multimodal feature representations becomes imperative.\nThe MER-SEMI and MER-NOISE tracks specifically address the challenges of semi-supervised learning in multimodal emotion recognition and enhancing robustness under noisy conditions. The organizers have generously provided meticulously annotated high-quality datasets and abundant unlabeled data resources, laying a solid foundation for participants and researchers to delve into these critical areas. Additionally, comprehensive research materials have been released, offering invaluable insights into understanding and addressing these challenges [15]. Initially, extensive baseline experiments were conducted to systematically assess the performance of unimodal models in emotion recognition tasks, conclusively demonstrating the substantial performance boost from more powerful feature extractors. Subsequently, in pursuit of further exploiting the potential of multimodal information, feature-level late fusion strategies were attempted to consolidate features from different modalities and enhance overall recognition capabilities. However, reliance solely on late fusion strategies often fails to adequately address potential conflicts and redundancies among modalities, thereby limiting the optimization of fusion effectiveness. In response to this challenge, we propose a novel joint training framework, particularly focusing on early fusion exploration of speech and text modalities.\nThis framework incorporates attention mechanisms and employs a hybrid fusion approach to handle multimodal features with greater finesse, effectively alleviating modality conflicts, and fully leveraging the complementary strengths between modalities, ultimately realizing efficient and precise integration and recognition of multimodal emotion features. Our key contributions are summarized as follows:\nEmotion ViT Model: We have trained a Vision Transformer (ViT) model proficient in characterizing human emotional features. Leveraging self-supervised learning, this model efficiently harnesses vast amounts of unlabeled data during pre-training and, through subsequent fine-tuning on a broad range of emotional data, significantly enhances its capacity for emotion feature expression.\nAudio-Text Joint Training Architecture: To tackle potential expression conflicts in multimodal emotion recognition, we introduce an innovative joint training structure for speech and text. By adopting early fusion, this structure effectively integrates information from both speech and text modalities, circumventing information loss associated with late fusion and surpassing simplistic dual-modal late fusion methods in performance.\nNoise Robustness Enhance: Addressing the interference of noise in complex environments on emotion recognition performance, audio denoise strategies and optimizations to ASR system's noise resilience have been implemented. These measures collectively elevate the model's stability and recognition precision in noisy environments, thereby reinforcing its noise robustness.\nEfficient Utilization of Unlabeled Data: To fully exploit the potential of unlabeled data, we incorporate a cyclic boosting data mining method. Through iterative utilization of unlabeled data for model training, this approach significantly enhances model accuracy and generalization capabilities, achieving efficient utilization of unlabeled data."}, {"title": "2 Proposed Method", "content": "As shown in Figure 1, we employ audio, text, and vision encoders to extract features from input audio, text, and facial sequence data, respectively. These encoders have been pre-trained or fine-tuned on emotion datasets. Further, we introduce a novel joint Audio-Text modality feature extraction module, specifically designed to harness the synergistic information between audio and text, thereby enhancing the expressivity of cross modalities.\nSubsequently, we leverage the distinct characteristics of these extracted features by combining them based on their differential properties. A cross-modal attention mechanism is then employed to fuse these combined features, allowing the system to dynamically weigh and focus on the most salient aspects for emotion recognition across each modality. Each fusion branch yields its independent prediction. Finally, a tailored ensemble strategy is invoked to ascertain the ultimate predicted result.\nAs shown in formulas (1)-(7), where $f_t$, $f_a$, and $f_v$ respectively represents the features extracted from text, audio, and visual modalities.\n$f_{a+t+v} = f_t \\oplus f_a \\oplus f_v$ (1)\n$f_{a+v+j} = f_a \\oplus f_v \\oplus f_i$ (2)\n$f_{t+v+j} = f_t \\oplus f_v \\oplus f_i$ (3)\n$Z^{*+} = W^{*+} \\cdot f^{*+} + b_z$ (4)\n$P^{*+} = Softmax(W_{smax}Z^{*+} + b_{smax})$ (5)\n$y^{*+} = arg max(P^{*+})$ (6)\n$y_o = Ensemble(y_{a+t+v}, y_{a+v+j}, y_{t+v+j}, ...)$ (7)\n$f_i$ indicates the early joint Audio-Text modality feature, and $f^{*+}$ signifies the fused features based on multimodal attention of different feature groups. Each $y_i$ signifies the prediction outcome from individual fusion branch. $y_o$ signifies the final sentiment prediction after our specific ensemble strategy."}, {"title": "2.2 Unimodal Emotional Encoder", "content": "2.2.1 Emotion ViT.\nFor MER2024, a large amount of labeled emotion data is not available. Considering this limitation, we adopt a self supervised training approach and propose EmotionViT, which mainly includes two parts:\nViT-Base[16]: We employ the MAE[17] self-supervision methodology for pre-training, utilizing ViT-Base as the backbone architecture. Our training dataset comprises a substantial 8 million images, including facial recognition, facial attributions, facial occlusion, and facial expression datasets. In particular, we have trained the ViT from scratch without leveraging any publicly available pre-trained models, such as models pre-trained on ImageNet1K[18]. We believe that although ImageNet1K covers most scenarios, facial expressions are the most important basis for emotion recognition. Therefore, our entire dataset for pre-training contains a large amount of facial data. After pre-trained, the ViT-Base model was fine-tuned on the labeled data of MER2024 and public emotion recognition datasets. Add a classification head to the vit base structure for training facial expression classification, and finally use the fine-tuned model to extract facial features as input for subsequent model fusion. During the process of creating data, we found that, The dataset for fine-tuning comes from the training set of MER2024, which consists of segments of video with video level labels instead of image level labels. For example, the annotated video is labeled as happy, but after the video is divided into frames, although the main expression is happy, not every frame is happy. Therefore, when this data is added to the training, it becomes a negative sample. Based on this, we innovatively distinguish the training data. For the source of a single image, we still use the original input method, as shown in Figure 2 (a), while for the training data of a video, we change the input of vit base from a single image to multiple frames from the same video concatenated together. The resulting grids picture is shown in Figure 2 (b), and this modification can greatly improve the final accuracy of the model and obtain better feature representation capabilities.\nInternVIT-6B: In addition to self-supervised pre-training methods such as MAE, the CLIP leverages image-text pairs for contrastive learning, is widely adopted in multi-modal models as a visual encoder. For our visual feature fusion, we introduced vit-base pre-trained model and CLIP pre-trained model for feature extraction. The InternVL-chat-V1.5[19], outstanding on the opencompass multi-modal benchmark, employs InternVIT-6B, a visual encoder pre-trained on a large-scale, high-quality image-text dataset with a substantial 6 billion parameters, exhibiting exceptional feature representation capabilities. We enhanced InternVIT-6B with a classification head and fine-tuned it on labeled data of MER2024 and public emotion datasets. Given the large scale parameters of InternVIT-6B, we only trained the projector layers.\nIn summary, EmotionViT mainly includes ViT-Base and InternViT-6B, which are pre trained and fintuned on a large amount of human centered data and integrated for use on the MER-SEMI and MER-NOISE tracks. Then the facial frames from a video is transformed into a sequence of high-dimensional video embeddings through Emotion ViT, denoted as $F_v \\in R^{N_f \\times 768}$"}, {"title": "2.2.2 fine-tuned Chinese_HuBERT_large.", "content": "To extract features from raw audio inputs, we employ a fine-tuned variant of the large-scale Chinese version of HuBERT_large. This model has been meticulously adapted to the MSA and Multimodal Emotional Recognition 2023 Audio Datasets, enhancing its capability to comprehend nuanced linguistic and emotional content in the Chinese language. The feature extraction process harnesses the concatenated hidden representations averaged over the last four layers of the Chinese_HuBERT_large architecture. Consequently, each audio input is transformed into a sequence of high-dimensional audio embeddings, denoted as $F_a \\in R^{N_a \\times 1024}$"}, {"title": "2.2.3 fine-tuned BaiChuan13b_Chat.", "content": "We extract features for a text input by leveraging a large language model Baichuan13b_Chat, fine-tuned on 200w Chinese texts related to emotion which contains WeiBo, ECB and MER2023 [20] Dataset, and use the averaged hidden representations across the last four layers of the LLM, resulting in a sequence of text representations $F_t \\in R^{N_t \\times 5120}$ for each text input."}, {"title": "2.3 Joint Audio-Text Module", "content": "In most cases, the emotion expressed by the individual in videos can be discerned through their auditory signals, encompassing not only the words but also the tone. Emotion is a product of both textual content and acoustic characteristics. For multimodal models, the crux lies in effectively integrating these modalities-textual and vocal features. Our experimental findings revealed that while features extracted using audio encoder in Section 2.2.2 and text encoder in Section 2.2.3 performed exceptionally well in their respective unimodal settings, the fusion of these two modalities based on attention mechanisms resulted in slight improvement.\nWe posit that sometimes there are discrepancies between audio and text representations, bringing additional competition between modalities, which potentially leads to the model under-fitting. This suggests that relying solely on feature-level fusion of cross-modal vocal-textual data might be insufficient to fully harness their complementary strengths.\nTo this end, we have augmented the QwenAudio[21] framework with the inclusion of a text modality, implementing an early fusion strategy tailored for emotion-laden data. Upon training this joint audio-text framework, it evolves into a novel, unified feature extractor for audio-text modalities based on a Large Language Model, aimed at more accurately capturing and representing the intricate interplay of emotions conveyed through both audio and text. This approach endeavors to transcend the limitations of conventional multimodal fusion by fostering a deeper integration at the very onset of the processing pipeline, thereby enhancing the model's capacity to interpret emotional expressions in a multimodal context.\nAs shown in Figure 3, The whisper-large-v2 model is used as Audio Encoder and the decoder-only Qwen-7B model is the Large Language Model(LLM). In our methodology, audio inputs are processed through an audio feature extractor to yield audio embeddings. Concurrently, the transcribed text from the audio is transformed into text embeddings. These embeddings, along with the prompt's embedding, are then concatenated and fed into the LLM for joint training across the audio and text modalities. The high-quality dataset comprising 5,000 samples from MER2023 serves as our training data, with a notable preprocessing step that filters out instances lacking sound in an audio. Full parameter fine-tuning is adopted as the training strategy, and our experiments affirm its superiority over the LoRA method.\nDespite the multitask-adapted QwenAudio model's inherent capability for speech recognition, it encounters inaccuracies when confronted with background noise or ambient sounds. Furthermore, the model's transcription output lacks punctuation, which is vital for effective emotion recognition in the textual modality as MERBench[13] paper asserts. Consequently, furnishing the model with the correct, punctuated text aligned with the audio not only compensates for this limitation but also preserves and does not interfere with the model's innate speech recognition functionality, thereby enhancing its overall performance in multimodal emotion recognition tasks. We use our own ASR model in Section 2.4.1 to extract text from audios and punctuate the text by CT-Transformer[22] model.\nAs a joint audio-text modality feature extractor, the last four layers of the large language model's decoder are employed to derive feature representations for each audio-text pair denoted as $F_{at} \\in R^{N_{at} \\times 4096}$"}, {"title": "2.4 Noise Robustness Enhance", "content": "2.4.1 Noise-Robust ASR.\nOur noise-robust ASR model is improved based on Paraformer[23]. We optimized the ASR model from following three aspects.\nFirstly, we added noise to the training set of the ASR model for data augmentation. We employ the MUSAN [24] noise database as the environmental noise to expand the training set, with signal-to-noise ratios ranging from 5db to 12db. The MUSAN noise database contains various types of environmental noise, including laughter, music sounds, etc., which are very suitable for simulating noise situations in daily life. In addition, we also used the training set combined with room impulse response(RIR) [25] to produce background human voice noise to further augment our training set, with signal-to-noise ratios ranging from 10db to 30db. We train our ASR model in a 2:1:1 ratio of clean speech, ambient noise speech, and background human voice noise speech. Secondly, we use semi-supervised method to further improve the robustness of the ASR model in noisy environments. Specifically, we use the ASR model to infer large-scale unlabeled noisy speech data, obtaining pseudo labels and beam search scores for each segment of speech. By calculating the average beam search score of each speech segment, we selected pseudo labeled data with high confidence for model self training. We iteratively trained the model until its performance did not improve. Finally, we further improved the performance of the ASR model by using a language model to avoid recognition errors. We trained an n-gram language model using a large amount of text data. Combining with Weighted Finite-State Transducers (WFST) decoding, we rescore the beams earch path of the ASR"}, {"title": "2.4.2 Speech Source Separation Denoise.", "content": "For multimodal emotion recognition, audio denoising can elevate model performance significantly. Audio noise is broadly categorized into background noise and vocal noise, with separate handling of each potentially complicating the process and risking compounded audio degradation through successive denoising steps. To this end, we propose a denoising method based on MossFormer2 [26], an attention-based speech separation model capable of decomposing a noisy audio into two distinct monaural audio.\nThe workflow commences by subjecting the noisy audio to noise-robust ASR model(2.4) to obtain a template text, designated as $Text_{temp}$. Subsequently, the noisy audio is processed through Moss-Former2, yielding two separated audios. Each of the audios is then passed through the ASR model above to get transcripts $Text_{ido}$ and $Text_{id1}$.\nA comparative analysis ensues, calculating the similarity between $Text_{ido}$ and $Text_{temp}$ against $Text_{temp}$, quantified as $Sim_{ido}$ and $Sim_{id1}$, respectively. If the absolute difference $|Sim_{ido} - Sim_{id1}|$ exceeds 0.1, the audio corresponding to the higher similarity score is selected; otherwise, the original audio is the chosen one. This strategy ensures a reduction in audio degradation after denoised.\nThe adoption of a speech source separation denoising scheme effectively mitigates the detrimental impact of noise on audio features, thereby enhancing multimodal emotion recognition model's performance."}, {"title": "2.5 Data Mining and Model Ensemble", "content": "This study proposes an innovative semi-supervised learning framework based on difference, aimed at effectively mining high confidence pseudo labeled samples from a large unlabeled dataset $D_u$, thereby promoting iterative improvement of model performance. Specifically, we first use a labeled dataset $D_l$ to train an initial weak learner through supervised learning. In order to enhance the diversity and complementarity among learners, we carefully designed four multimodal fusion models $M_1$, $M_2$, $M_3$, and $M_4$ with different feature combinations, and trained four independent weak learners $M_{l1}$, $M_{l2}$, $M_{l3}$, and $M_{l4}$ based on these models.\nSubsequently, these four weak learners were synergistically applied to the unlabeled dataset $D_u$ to predict each unlabeled sample. In order to generate reliable pseudo labels, we adopted a majority voting strategy: only when the prediction results of at least three learners are completely consistent, the predicted label is adopted as the pseudo label of the sample, thus constructing the pseudo label dataset $D_w$.\nNext, in order to fully utilize these newly generated pseudo labeled data, we evenly divide $D_w$ into four subsets and merge them with the original labeled dataset $D_l$ to form four extended and differentiated labeled datasets $D_{lw1}$, $D_{lw2}$, $D_{lw3}$, $D_{lw4}$.\nSubsequently, the four extended datasets were used as training sets for supervised and refined training of models $M_1$ to $M_4$, resulting in four more powerful learners $M_{lw1}$, $M_{lw2}$, $M_{lw3}$, $M_{lw4}$. We used the updated learner to vote on the remaining unlabeled data to generate new pseudo labeled samples and repeat the training and enhancement steps above for N iterations. As the number of iterations increases, the model's performance improve gradually driven by the continuous enrichment of data.\nAfter N iterations, we obtained 4 strong learners. These learners adopt different strategies in feature extraction and different combinations of datasets during training, resulting in significant differences in model characteristics and knowledge representation.\nBased on four strong learners, we designed a multimodel ensemble strategy, as shown in Figure 4. We rank four models from high to low according to the indicators on the test set, and then count the mode(most common categories) of the output results of multiple models. If the number of modes is 1, we choose the mode as the voting result, otherwise we ignore the last model's result, and repeat the above steps until the mode is unique."}, {"title": "3 Experiments and Analysis", "content": "3.1 Dataset\nThe data details of MER-SEMI and MER-NOISE tracks are shown in Table 1.\nThis dataset includes 5030 carefully labeled data samples and 115595 unlabeled data samples, which together form the foundation of the training and validation stages. Participants can fully utilize this data to optimize and improve the performance of their models. In the testing phase, in order to encourage participants to pay more attention to the generalization ability of the model, it is required to make predictions on all submitted models on a sample set of 20000. However, for the MER-SEMI track, we only evaluated 1169 samples; For the MER-NOISE track, 1170 noisy samples were evaluated to comprehensively test the performance of the model in complex noise environments.\nWe have trained two vision feature extractors, based on VIT-Base and InternVIT-6B respectively. Training VIT-Base encoder includes pre-train and fine-tune stages. In pre-train stage,"}, {"title": "3.2 Ablation Study", "content": "For ViT-Base, we choose mmpre-train[?] framework for large-scale pre-training and fine-tune it by adding a classification header. In the pre-train stage, the parameters are set as follows: input resolution 224, batch size 1024, initial learning rate 2e-4, maximum training rounds 300, mixed precision training enabled, gradient accumulation 4, and training on 8*A800. In the finuetune stage, the final learning is 2e-3, the maximum number of training rounds is 100, and the other settings are the same as pre-train stage.\nFor InternViT-6B, the experimental settings are as follows, batch size 512, input resolution 224, learning rate 2e-5, optimizer SGD, attenuation parameter 0.9, maximum training rounds 50.\ntext feature extracted from fine-tuned Baichua_13B, Joint Audio-Text is the joint Audio-Text feature based on enhanced QwenAudio (Qwen_JAT), and 'Denoise' represents denoising preprocessing.\nFrom the table, we can find that Joint Audio-Text feature fused with Baichuan_13Bft and Chinese_Hubert_largeft reached 0.8606 score in MER2024-semi track. Tri-feature fusion can maximize the performance of audio and text modality. In addition, using Joint Audio-Text features after audio denoising can simultaneously improve our model's performance in both tracks of MER2024.However, due to the reduction of the speech sampling rate after denoising (16k to 8k), the feature quality of Chinese_Hubert_largeft (pre-trained on audios sampling rate 16k) decreased, which cause the score in noise-track dropped after the tri-feature fusion.\nNext, we conduct features fusion experiments extracted from all our modal feature encoder based on MER2024 labeled dataset sized 5030. The Table 4 shows the comparison of different modality feature groups with the baseline in the multi-modal fusion model. It's clear that our fine-tuned audio encoder (Chinese_Hubert_large*), fine-tuned text encoder (Baichuan_13B*), EmotionViT(collective name for vit_base_emo and internvit_6B_emo) and Qwen_JAT all have significantly improved the overall performance of multi-modal models respectively.\nSpecifically, the encoder groups including Baichuan_13B*, vit_base_emo and Qwen_JAT increase the WAF by up to 3.41% compared to the baseline, which is a very considerable improvement.\nMoreover, our denoise preprocess method for audios before Qwen_JAT showed outstanding results on the MER2024-NOISE track, with a WAF increase of 2.9%, proving the effectiveness and robustness of our approach. What's even more gratifying is that in the MER2024-SEMI track, our denoise method also brings a 0.39% increasement."}, {"title": "4 Conclusion and Limitations", "content": "In this paper, we present our solutions for emotion recognition in the sub-challenges of MER2024. Firstly, We launched our Emotion ViT based on large-scale data pre-training and fine-tune, a vision feature extractor adapted to emotion recognition tasks. To mitigate the modal competition issue between audio and text, we adopt an early fusion strategy based on a large language model, where joint training of audio and text is conducted initially. And the joint Audio-Text modal feature will be late-fused with other unimodal features. In order to solve the problems of data insufficiency and class imbalance, We use multiple turns of multi-model voting for data mining. Moreover, to enhance the quality of audio features, we employ speech source separation to preprocess audios. The efficacy of our methods has been demonstrated through outstanding performances in both MER2024-SEMI and MER2024-NOISE, achieving scores of 0.9001 and 0.8383 respectively.\nOur research into the competition among visual modality and other modalities is insufficient, and furthermore, the ambiguity inherent in Chinese text poses a particular challenge for emotion recognition. These issues above may need the utilization of large language models for resolution. In the future, we will investigate emotion recognition based on Multimodal Large Language Models."}]}