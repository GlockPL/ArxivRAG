{"title": "Streamlining Conformal Information Retrieval via Score Refinement", "authors": ["Yotam Intrator", "Regev Cohen", "Ori Kelner", "Roman Goldenberg", "Ehud Rivlin", "Daniel Freedman"], "abstract": "Information retrieval (IR) methods, like retrieval augmented generation, are fundamental to modern applications but often lack statistical guarantees. Conformal prediction addresses this by retrieving sets guaranteed to include relevant information, yet existing approaches produce large-sized sets, incurring high computational costs and slow response times. In this work, we introduce a score refinement method that applies a simple monotone transformation to retrieval scores, leading to significantly smaller conformal sets while maintaining their statistical guarantees. Experiments on various BEIR benchmarks validate the effectiveness of our approach in producing compact sets containing relevant information.", "sections": [{"title": "Introduction", "content": "Information retrieval (IR) methods lie at the heart of numerous modern applications, ranging from search engines and recommendation systems to question-answering platforms and decision support tools. These methods facilitate the identification and extraction of relevant information from vast collections of data, enabling users to access the knowledge they seek efficiently and effectively.\nA popular example of IR is Retrieval Augmented Generation (RAG), a technique for reducing hallucinations in large language models (LLMs) by grounding their responses on factual information retrieved from external sources.\nWhile IR methods have been widely adopted, they traditionally lack statistical guarantees on the relevance of retrieved information. This limitation can lead to uncertainty regarding the reliability and correctness of the retrieved information. Conformal prediction (Angelopoulos and Bates, 2021; Angelopoulos et al., 2021) is an uncertainty quantification framework that can be used with any underlying model to construct sets that are statistically guaranteed to contain the ground truth with a user-specified probability. Conformal prediction has expanded far beyond its initial classification focus (Vovk et al., 2005; Angelopoulos and Bates, 2021; Ringel et al., 2024), now encompassing diverse applications like regression, image-to-image translation (Angelopoulos et al., 2022b; Kutiel et al., 2023), and foundation models (Gui et al., 2024), advancing to enable control of any monotone risk function (Angelopoulos et al., 2022a). In the context of IR, recent methods (Xu et al., 2024; Li et al., 2023; Angelopoulos et al., 2023) have incorporated conformal prediction into ranked retrieval systems to ensure the reliability and quality of retrieved items. However, existing conformal methods often produce excessively large retrieved sets, implying high computational costs and slower response times.\nIn this work, we address this limitation by introducing a novel score refinement method that employs a simple yet effective monotone transformation, inspired by ranking measures, to adjust the scores of any given information retrieval system. By applying standard conformal prediction methods to these refined scores, we deliver significantly smaller retrieved sets while preserving their statistical guarantees, striking a crucial balance between efficiency and accuracy. We validate the effectiveness of our method through experiments on three of BEIR (Thakur et al., 2021) benchmark datasets, demonstrating its ability to outperform competing approaches in producing compact sets that contain the relevant information."}, {"title": "Background", "content": "To lay the groundwork for our work, we present a simplified description of the operation of information retrieval systems and how conformal inference can be seamlessly integrated within this context."}, {"title": "Information Retrieval: Overview", "content": "Consider a large information database \\(D = \\{d_1, d_2, ..., d_v\\}\\). At inference time, an IR model \\(R : Q \\rightarrow D\\) accepts a query \\(q \\in Q\\) as input and returns a subset of candidates \\(S \\subset D\\). To do this, the IR model computes a semantic embedding \\(e_q = E(q)\\) for the query and compares it to pre-computed embeddings \\(e_i = E(d_i)\\) for each item in the database using a similarity metric:\n\\[\ns_i = sim(e_q, e_i),\n\\]\nwhere \\(E\\) is the chosen representation model (e.g., a neural network encoder) and \\(sim\\) is a similarity metric, such as cosine similarity. Subsequently, the items are typically ranked based on their similarity scores, and the top ranked items are retrieved, forming the following set\n\\[\nS^{(K)} = \\{d_i \\in D : s_i \\geq s_{(K)}\\}\n\\]\nwhere \\(s_{(K)}\\) denotes the Kth largest similarity score, for a predefined \\(K > 0\\) constant across all queries.\nThe approach above suffers from two key limitations. First, using a fixed K can be problematic: it might be too restrictive for some queries, leading to the omission of relevant information, while for others, it might be too permissive, resulting in the retrieval of numerous redundant or irrelevant items. The latter scenario significantly impacts efficiency and prolongs response times. Second, this approach lacks guarantees that truly relevant information, such as a specific item \\(d^*\\) within the database \\(D\\), will be included in the retrieved set S."}, {"title": "Conformal Prediction for Retrieval", "content": "Conformal prediction can be seamlessly integrated into IR systems by constructing calibrated prediction sets designed to include, on average, the desired information with a user-specified high probability. Formally, given a query q and its corresponding similarity scores si, we construct a prediction set parameterized by \\(\\tau > 0\\) as follows:\n\\[\nC_\\tau(q) = \\{d_i \\in D : c_i < \\tau\\},\n\\]\nwhere \\(c_i = -s_i\\) represents a non-conformity score. To appropriately set the value of \\(\\tau\\), we utilize a held-out calibration dataset \\(D_c\\) consisting of n samples \\((q_i, d_i) \\in Q \\times D\\) drawn exchangeably from an underlying distribution P. Here, \\(q_i\\) represents a query whose most relevant information is assumed to be a single item \\(d_i\\) from the database, for simplicity. Given a user-chosen error rate \\(\\alpha \\in [0, 1]\\), we set \\(\\tau\\) as the \\(\\frac{(n+1)(1-\\alpha)}{n}\\)-th quantile of the calibration non-conformity scores. This ensures that for a new exchangeable test sample \\((q_{n+1}, d_{n+1})\\), we have the following marginal coverage guarantee:\n\\[\nP(d_{n+1} \\in C_\\tau(q_{n+1})) \\geq 1 - \\alpha\n\\]\nfor any distribution P. The probability above is marginal (averaged) over all n + 1 calibration and test samples. This ensures that the IR model retrieves sets of adaptive size, guaranteed to contain the relevant information at least \\(\\alpha\\)-fraction of the time, thereby overcoming the limitation above.\nWhile the conformal sets above use a calibrated threshold, other parameterizations are possible, such as setting the calibration parameter to the set size K, as in (2). Furthermore, it is important to note that the description above merely presents conformal prediction in its simplest, most common form. However, there have been significant advancements in the field in recent years, leading to the development of more involved and efficient conformal methods (Romano et al., 2020; Angelopoulos et al., 2020) and to extensions that provide guarantees beyond marginal coverage (Angelopoulos et al., 2022a; Fisch et al., 2020; Li et al., 2023)."}, {"title": "Method", "content": "Integrating conformal prediction to IR systems enhances their reliability by providing statistical guarantees. However, CP methods prioritize trustworthiness and are not optimized for efficiency, thus they often produce excessively large retrieval sets. Following the above, our goal is to improve the predictive efficiency of CP methods by reducing the average size of the retrieved sets \\(E_q[|C_\\tau(q)|]\\), while maintaining their coverage guarantees. In contrast to approaches that focus on improving the IR model or developing more efficient conformal methods, we propose an alternative approach that introduces an intermediate step of score refinement.\nSpecifically, given a query q and its scores \\(S = \\{s_1, s_2,..., s_N\\}\\), we adjust them prior to employing conformal prediction \\(T(S) = \\{t_1, t_2,..., t_N\\}\\).\nIn designing the transformation T, we identify that scores from different queries can vary significantly in scale. This can cause the calibration threshold \\(\\tau\\) to be dominated by queries with small scores, leading to excessively large prediction sets. To mitigate this issue, we first normalize the retrieval scores by dividing them by their maximum, ensuring that scores across all queries are comparable in scale. We remark that the maximum score \\(s_{max}\\) can be interpreted as the IR model's confidence. When this value is small, it suggests a lack of relevant information for the given query, suggesting that ideally no items should be retrieved. Thus, normalization in such scenarios may be counter-productive, resulting in irrelevant items. However, we assume the corpus is sufficiently extensive to contain at least one relevant item for any query, an assumption particularly valid for the calibration.\nNext, assume without loss of generality that the scores are sorted in decreasing order: \\(S = \\{s_{(1)}, s_{(2)},..., s_{(N)}\\}\\), where \\(s_{(r)}\\) is the rth largest score and \\(r > 1\\) represents its rank. Inspired by ranking measures (Yining et al., 2013), we define our transformation as follows\n\\[\nT(s_{(r)}, r) = \\frac{s_{(r)}}{s_{max}} W(r)\n\\]\nwhere \\(W(r) \\in [0, 1]\\) is a discount function that penalizes scores based on their rank. We specifically employ the inverse logarithm decay \\(W(r) = \\frac{1}{\\log(1+r)}\\), which offers a balance between emphasis on top items and exploration of lower-ranked items. To offer additional flexibility, we introduce a hyperparameter \\(\\lambda \\in [0, 1]\\):\n\\[\nT(s_{(r)}, r) = \\frac{s_{(r)}}{s_{max}} \\frac{1}{\\log(1 + r^\\lambda)}.\n\\]\nWe tune \\(\\lambda\\) by performing a search over a sequence of values to minimize the set size on a validation set. Note the transformation is monotone, preserving the IR model's induced order and maintaining its core functionality. Furthermore, it is simple to implement, computationally efficient, and easily integrated into existing systems. As demonstrated in the following section, the proposed transformation is highly effective in reducing the size of the conformal retrieved sets."}, {"title": "Experiments", "content": "Datasets For our evaluation, we utilized BEIR (Thakur et al., 2021), a large collection of information retrieval benchmarks. Specifically, we focus on the following datasets: FEVER (Thorne et al., 2018), SCIFACT (Wadden et al., 2020), and FIQA (Maia et al., 2018). Data statistics are presented at Table 1. It is important to note that each query within these datasets may have multiple relevant documents within the corpus. For this study, we adopted a pragmatic approach, considering the document with the highest score among the relevant documents as the ground truth. This ensures that a successful retrieval implies at least one relevant document is present in the inference set.\nTo simulate real-world production environments, we employ a vector store, specifically FAISS-GPU (Johnson et al., 2019) for its efficiency and performance in handling large-scale databases. We retrieve the top 2,000 documents for each query and apply our refinement process exclusively to these initially retrieved documents.\nEmbedders Initial semantic scores were derived using deep sentence embedders, which encode textual input into a fixed-dimensional latent space where semantic similarity is represented by vector proximity. We employ two models: BGE-large-1.5 (Xiao et al., 2023) (326M parameters) and E5-Mistral-7b model (Wang et al., 2023) (7B parameters). BGE-large-1.5 is a smaller model with a latent representation dimension of 1024, whereas E5-Mistral, a finetuned encoder version of the mistral-7b model, has a latent representation dimension"}, {"title": "Results", "content": "We first conduct experiments on the smaller SCIFACT dataset to optimize the hyperparameter \\(\\lambda\\). The results, shown in Figure 2, reveal a favorable value for \\(\\lambda\\), prompting us to set \\(\\lambda\\) = 0.05."}, {"title": "Conclusion", "content": "We addressed the challenge of large prediction sets in conformal prediction for IR by introducing a novel score refinement method. Our experiments on the BEIR benchmark demonstrated its effectiveness in generating compact, statistically reliable prediction sets, enabling the deployment of conformal prediction in real-world IR systems without sacrificing performance."}, {"title": "Limitations", "content": "The conclusions of this study could be further strengthened by evaluating the method on a wider range of datasets and employing diverse embedding models. Currently, our method does not handle cases where no relevant information exists in the database, potentially limiting its applicability. Additionally, while we introduced a simple transformation, exploring more involved or even parameterized functions, e.g. neural networks, could further enhance efficiency and statistical guarantees."}, {"title": "Additional Experiments", "content": "Here evaluate our method with the E5-Mistral embedder on SCIFACT and FIQA datasets. Results, presented in Table 3, show our method consistently outperforms competitors. Moreover, using E5-Mistral leads to improved performance in both empirical coverage and average group size compared to BGE-large-1.5.\nIn addition to the aforementioned experiments, we compared our method against alternative transformations: Max Score, where scores are normalized by dividing each by the maximum score, and Z-Score, which standardizes the initial retrieved scores. The results, summarized in Table 4, show that our score refinement transformation outperforms these other refinement methods in the vast majority of cases."}]}