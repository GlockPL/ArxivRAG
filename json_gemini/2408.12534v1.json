{"title": "Automatic Organ and Pan-cancer Segmentation\nin Abdomen CT: the FLARE 2023 Challenge", "authors": ["Jun Ma", "Yao Zhang", "Song Gu", "Cheng Ge", "Ershuai Wang", "Qin Zhou", "Ziyan Huang", "Pengju Lyu", "Jian He", "Bo Wang"], "abstract": "Organ and cancer segmentation in abdomen Computed Tomography (CT) scans is the prerequisite for precise cancer\ndiagnosis and treatment. Most existing benchmarks and algorithms are tailored to specific cancer types, limiting their ability\nto provide comprehensive cancer analysis. This work presents the first international competition on abdominal organ and\npan-cancer segmentation by providing a large-scale and diverse dataset, including 4650 CT scans with various cancer types\nfrom over 40 medical centers. The winning team established a new state-of-the-art with a deep learning-based cascaded\nframework, achieving average Dice Similarity Coefficient (DSC) scores of 92.3% for organs and 64.9% for lesions on the\nhidden multi-national testing set. The dataset and code of top teams are publicly available, offering a benchmark platform to\ndrive further innovations https://codalab.lisn.upsaclay.fr/competitions/12239.", "sections": [{"title": "INTRODUCTION", "content": "Abdomen organs are quite common cancer sites, such as colorectal cancer and pancreatic cancer, which are the second\nand third most common cause of cancer death [1]. Computed Tomography (CT) scanning yields important prognostic\ninformation for cancer patients and is a widely used imaging technology for cancer diagnosis and treatment monitoring [2].\nIn both clinical trials and daily clinical practice, radiologists and clinicians measure the tumor and organ on CT scans based\non manual measurements (e.g., Response Evaluation Criteria In Solid Tumors (RECIST) criteria) [3]. However, this manual\nassessment is inherently subjective with considerable inter- and intra-expert variability and cannot measure the 3D tumor\nmorphology.\nDeep learning-based methods have shown great potential for automatic tumor segmentation and quantification. Many\nchallenges have been established to benchmark algorithm performance by providing standard datasets and fair evaluation\nplatforms, such as the brain tumor segmentation (BraTS) [4], liver and liver tumor segmentation [5], kidney and kidney\ntumor segmentation [6], and pancreas and colon lesion segmentation [7]. These challenges have greatly advanced algorithm\ndevelopment [8], [9], but they only focus on one type of lesion (e.g., liver cancer, kidney cancer, or pancreas cancer), which\ncannot provide holistic lesion analysis. Pan-cancer segmentation in abdomen CT plays an important role in clinical practice\nbecause lesions can spread from one organ to another organ. For example, pancreas cancer and colorectal cancer could\ntransfer to the liver, leading to liver metastases. There is a great need for general algorithms that can segment all kinds of\nlesions from CT scans.\nRecently, universal lesion segmentation algorithms for abdomen CT have received increasingly attention [10]\u2013[13].\nHowever, these algorithms are developed and evaluated under various datasets, leading to difficulties in fairly comparing\nthem. The main barrier is the lack of a general benchmark platform and publicly available dataset. In this work, we\naddressed the limitation by providing the largest abdominal pan-cancer dataset and organized the first international\ncompetition to prompt the development of universal abdominal organ and lesion segmentation algorithms. In particular,\nwe curated a diverse abdominal pan-cancer dataset with 4650 CT scans, covering various abdomen cancers from 50 medical\ncenters, which is the most comprehensive abdomen pan-cancer dataset to date. The competition attracted 292 participants"}, {"title": "RESULTS", "content": "Challenge design\nThis challenge aimed to prompt the methodology development of fully automatic abdominal organ and lesion segmen-\ntation algorithms in CT scans. Different from the previous FLARE challenges designed for pure organ segmentation [14],\n[15], the FLARE 2023 challenge introduced three improvements (1a). First, the challenge task was expanded to joint organ"}, {"title": "Overview of evaluated algorithms", "content": "47 teams from 292 participants joined the challenge and we received 37 successful algorithm docker container submissions\nduring the testing phase, where four submissions failed and the remaining six teams did not submit. We analyzed three key\ncomponents of the employed deep learning model among the 37 teams, including network architectures, loss functions, and\noptimizers. All teams used 3D networks and 60% of the teams used U-Net [9] as the main architecture. The combination of\nDice loss and cross-entropy loss was the most popular loss function, used by 87% of the teams. Stochastic gradient descent\nwas usually used for optimizing the U-Net while Transformer-based networks usually used Adam [21] and its variant\n(AdamW [22]).\nBest-performing algorithm. Team aladdin5 (T1 [23]) designed an efficient cascaded framework that localized the region\nof interest first followed by fine-grained segmentation for organs and lesions. The organ pseudo labels of the unlabeled\norgans, generated by the best-accuracy-algorithm [24] in FLARE 2022 [15] were used to enlarge the annotations in the\ntraining set. In particular, a lightweight nnU-Net [9] was first trained on the combination of labeled cases and unlabeled\ncases with pseudo labels for binary segmentation of the region of abdominal organs. After that, two individual models\nwere trained with the cropped images with different image spacing for organ and tumor segmentation, respectively. The\nsegmentation models were further fine-tuned with selected patches where the organ or tumor was centered. To improve\ninference efficiency, the prediction interpolation was implemented on GPU and multithreading was employed to pre-process images and load model checkpoints simultaneously.\nSecond-best-performing algorithm. Team citi (T2 [25]) presented a partially supervised framework with nnU-Net [9] to\nleverage the partially labeled data. During partially supervised training, the images were first grouped by their annotated\nclasses and then a batch of training images were selected from one group with a probability of its proportion. In this way,\nthe images in a training batch should have identically labeled classes. When calculating the loss between the prediction and\nthe ground truth, the output channels of unlabeled classes were merged to one channel by max pooling and corresponded\nto the background channel in the reference standard. The pseudo labels were selected by uncertainty estimation and further\ncleaned by filtering out small isolated regions. Due to the lack of tumor annotations, a CutMix augmentation strategy was\nexploited to copy tumor regions from the cases with labeled tumors to those without labeled tumors. To preserve the\ncontext of each tumor region, the neighboring regions were cropped together with the tumor object."}, {"title": "Segmentation accuracy and efficiency analysis on the testing set", "content": "We show the testing set segmentation accuracy and efficiency performance of the 37 teams in Fig. 2a (Supplementary\nTable 6). The majority of teams are concentrated on the top left of the plot, indicating that most participants aimed to\ndevelop accurate and efficient algorithms. However, we also noticed that some teams only pursue the unilateral metric.\nFor example, T25 and T30 consumed around 1500MB GPU memory, but the segmentation accuracy is inferior to the others.\nT24 achieved very competitive DSC score but the inference speed is slow, costing 99 seconds for each case. In contrast, the\nwinning team (T1) stands out with an average DSC of 92.3% and 64.9% for organ and lesion segmentation, respectively,\nand an inference speed of 8.6s by consuming 3561.6 MB of GPU RAM.\nNext, we compared the performance of top five algorithms across seven metrics (organ DSC and NSD, lesion DSC and\nNSD, runtime, GPU memory consumption, and final rank) in terms of the number of algorithms it outperformed, aiming to\nprovide a comprehensive comparison of the algorithms' strengths and weaknesses across the multiple criteria (Fig. 2b). The\nradar plot reveals that the top five algorithms outperformed most others across these metrics, as evidenced by the significant\noverlap in the plotted areas. However, closer inspection highlights subtle variations in performance. In particular, T1-aladdin5 demonstrated the highest accuracy in lesion segmentation, while T2-citi excelled in organ segmentation. They all\nobtained the perfect GPU consumption metric, but T4-hmi406 achieved the fastest inference speed. Overall, T1-aladdin5\nsecured the best final rank with a better balance between segmentation accuracy and computational efficiency.\nWe also analyzed the ranking stability of all the employed metrics regarding testing case sampling variability. Specifi-\ncally, the bootstrap approach was used by generating 1000 bootstrap samples where each sample contained 400 randomly\nselected testing cases with replacement from the testing set. Then, we compute Kendall's \\(\tau\\) values for all the metrics and\nFig. 2c shows the corresponding distributions with violin and box plots. The Kendall's \\(\tau\\) values for all metrics are clustered\naround 1.0, implying that the rankings of the algorithms are highly consistent and stable."}, {"title": "Organ-wise performance analysis", "content": "We further present a detailed analysis of the performance of top five teams in segmenting a diverse set of 13 organs (Fig. 3,\nSupplementary Table 7). Specifically, we categorized these organs into three groups based on their size, morphology, and\nsegmentation challenges: large solid organs, small organs, and tubular organs. This classification allows us to highlight\nthe specific difficulties and successes associated with each category, providing insights into how the algorithms perform\ndifferently across varying anatomical structures.\nThe first group consists of large solid organs, including the liver, kidneys, spleen, pancreas, and stomach. These organs\ngenerally have well-defined structures, making them more straightforward targets for segmentation algorithms. The liver,\nkidneys, and spleens demonstrated high and consistent performance in all five teams, with average DSC and NSD scores\nabove 95% in all five teams (Fig. 3a-c). Their box plots also show tight clusters, suggesting that these organs are easier\nfor segmentation due to the relatively large size and distinct boundaries. The pancreas, despite being part of this group,\nexhibited greater variability with average DSC scores ranging from 88.9% to 91.3%, reflecting the anatomical complexity\nand variability in shape and size across patients, which presents additional challenges for accurate segmentation (Fig. 3d).\nThe stomach also performs well, with mean DSC scores between 93.4% (T4) and 94.9% (T1), indicating overall good\nsegmentation accuracy across the teams (Fig. 3e).\nThe second group focuses on small organs, specifically the gallbladder and adrenal glands (Fig. 3f-g). These organs are\ncharacterized by their smaller size and less distinct boundaries, making them more difficult to segment. The DSC and NSD\nscores exhibit greater variability than large organs, ranging from 83.5% (T1-aladdin5) to 86.9% (T4-hmi306). The adrenal\nglands show a similar trend, with mean DSC scores from 79.0% (T3-blackbean) to 89.5% (T2-citi). These results suggest that\nthe small size and less distinct boundaries of these organs pose substantial challenges for accurate and robust segmentation,\nleading to increased variability in performance across the different algorithms."}, {"title": "Lesion performance analysis", "content": "The challenge task approached the lesion as a semantic segmentation problem, categorizing each voxel as either part of\na lesion or not. This approach aligns with organ segmentation tasks, enabling uniform evaluation across different cases.\nAlternatively, the task can be framed as an instance segmentation problem, which not only captures category information\nbut also distinguishes between different lesions. This allows for the identification and analysis of multiple distinct lesions\nwithin the same image.\nWe evaluated the lesion segmentation results of the top five teams using both semantic segmentation metrics (DSC and\nNSD) and instance segmentation metrics (Sensitivity, Specificity, and F1 score), where we separated the disconnected lesions\nas individual entities with connected components analysis. The dot-box plots show the testing set performance distribution\nfor each team across the five metrics (Fig. 4a, Supplementary Table 8). All the top three teams achieved a median DSC score\nof over 70% where T1-aladdin5 had the highest median DSC score of 75.9 (interquartile range (IQR):49.8-86.2%). However,\nthe median NSD scores dropped below 60% for all the teams, indicating that lesion boundaries may not be accurately\ndelineated and small lesions could be missed.\nIn terms of instance segmentation metrics, all the teams achieved high precision but low recall. This imbalance indicates\nthat the algorithms are conservative in their segmentations. In particular, T1-aladdin5 and T4-hmi306 obtained the best\nspecificity with median scores of 50.0% (IQR: 25.0-100.0%) and 50.0% (IQR: 0.0-100.0%), respectively. However, T4-hmi306\nhad the lowest sensitivity with a median score of 20.0% (IQR: 0.0-50.0%), indicating that many lesions were missed in\nthe segmentation results. The F1 scores follow a similar trend to the DSC scores with T1-aladdin5 achieving the highest\nmedian score of 40.0% (IQR: 22.2-66.7%). Overall, the F1 scores across teams show a broad range, reflecting inconsistencies\nin balancing lesion detection precision and recall.\nWe also employed the majority vote approach to generate the ensemble results of the top three and top five algorithms,\nrespectively, and computed the panoptic quality metric to understand both segmentation and detection quality. As shown\nin Fig. 4b (Supplementary Table 9), the ensemble models generally show comparable or slightly improved panoptic quality\ncompared to the individual teams, with Ensemble-3 slightly outperforming the others in terms of median score. However,\nall teams and ensembles exhibit a wide range of panoptic quality scores, indicating variability in performance across\ndifferent cases.\nNext, we analyzed the relationship between lesion volume and segmentation accuracy (DSC) of the winning algorithm\nT1-aladdin5 (Fig 4c). There is a clear trend where larger lesion volumes tend to correspond with higher DSC scores,\nsuggesting that the algorithm performs better as the lesion volume increases. For smaller lesions, particularly those with\nvolumes below approximately 100, the DSC values are widely dispersed, ranging from near 0 to around 80% or higher.\nThis indicates that the segmentation algorithm struggles more with smaller lesions, leading to less consistent and generally\nlower DSC scores.\nTumor volume is an important image biomarker and we compared the predicted lesion volume to the true lesion volume\n(Fig. 4d). The result reveals that while the model generally performs well in predicting lesion volumes, with a strong\noverall correlation between true and predicted volumes, it exhibits variability in accuracy, particularly at the extremes\nof the volume range. Smaller lesions tend to be underestimated, while larger lesions are occasionally overestimated, as\nindicated by the spread of data points around the diagonal line in the scatter plot.\nFinally, we present a visualization of typical examples (Fig 4e) that contains two successful cases (the 1st and 2nd\ncolumns) and three failure examples (the 3rd to 5th columns). These examples demonstrate that the algorithm is capable\nof accurately identifying and segmenting both large and small lesions when their appearances and boundaries are well-defined. However, the algorithm often struggles with small, heterogeneous lesions, as evidenced by its complete failure to\ndetect the pancreas and colorectal lesions leading to poor performance."}, {"title": "DISCUSSION", "content": "AI has revolutionized medical image segmentation tasks, but most algorithms rely on a large number of human expert\nannotations, which are extremely hard and expensive to collect. Moreover, the performance of existing algorithms is\nmainly evaluated based on accuracy-related metrics on limited cohorts while the generalization ability, running efficiency,\nand resource consumption are overlooked. These barriers hinder the wider adoption of AI algorithms in clinical practice.\nThe main goal of this study was to address these critical issues. In particular, we created the largest abdomen organ and\npan-cancer CT dataset with a well-defined segmentation task to benchmark algorithms. Furthermore, we organized an"}, {"title": "METHODS", "content": "Challenge schedule\nThe FLARE 2023 challenge was preregistered [19] and the proposal passed peer review at the 25th International Conference\non Medical Image Computing and Computer-Assisted Intervention (MICCAI 2023). We launched the challenge on April\n1st 2023 on the CodaLab platform [48]. During the development phase, each team can submit up to three tuning set\nsegmentation results every day to the online platform and get the segmentation accuracy scores. Moreover, each team also\nhad five chances to submit docker containers to challenge organizers and obtain segmentation efficiency scores. During the\ntesting phase, participants were required to submit the final algorithm docker by August 25th 2023. We manually evaluated\nall the submitted dockers on the hidden testing set and announced the results on October 8th 2023 at MICCAI.\nData standardization and annotation protocol\nThe data standardization followed the common practice in the other 3D medical image segmentation challenges [5]\u2013[7] and\nthe past FLARE challenges [14], [15]. We curated the CT scans from public datasets based on the license permission. Detailed\ninformation on these datasets is presented in Supplementary Table 1-4. All CT scans were converted to the standard\nNIfTI format (https://nifti.nimh.nih.gov/) and preserved the original CT HU values. The orientation was standardized\nas canonical 'RAS', which means that the first, second, and third voxel axes go from left to Right, posterior to Anterior,\nand inferior to superior, respectively. Organ annotation protocol remained the same as for the FLARE 2022 challenge [15],\nwhich adhered to the radiation therapy oncology group consensus (RTOG) panel guideline [49] and Netter's anatomical\natlas [50]. In the training set, the lesion annotations in the source datasets were directly used. In the tuning and testing set,\nall visible lesions were annotated by a senior radiologist with the assistance of ITK-SNAP [51] and MedSAM [52]."}, {"title": "Evaluation protocol", "content": "All algorithms were sequentially run on the same GPU desktop workstation for a fair evaluation. The workstation was\na Ubuntu 20.04 desktop with one central processing unit (CPU, Intel Xeon(R) W-2133 CPU, 3.60GHz \u00d7 12 cores), one\ngraph processing unit (GPU, NVIDIA QUADRO RTX5000, 16G), 32G of memory, and 500G of hard disk drive storage. We\nused two groups of metrics to evaluate segmentation accuracy and efficiency. Following the recommendations in Metrics\nReloaded [53], the segmentation accuracy metrics contained Dice Similarity Coefficient (DSC) and Normalized Surface\nDistance (NSD), measuring the region and boundary overlap between segmentation mask and reference standards. The\nsegmentation efficiency metrics included runtime and Area Under the Curve of GPU memory-time (AUC GPU), where the\nGPU memory consumption was recorded every 0.1s. In addition, we also analyzed commonly used instance segmentation\nmetrics for lesion segmentation, including precision, recall, F1 score, and panoptic quality."}, {"title": "Ranking scheme", "content": "The final rank was computed with both segmentation accuracy and efficiency metrics. We give runtime and GPU memory\nconsumption a tolerance of 15s and 4GB", "steps": "n\u2022\nStep 1. Compute the six metrics for each case in the testing set (N=400)", "metrics": "average\nDSC and NSD scores for 13 abdominal organs; two lesion-wise metrics: DSC and NSD scores; two efficiency metrics:\nruntim"}]}