{"title": "Automatic Organ and Pan-cancer Segmentation in Abdomen CT: the FLARE 2023 Challenge", "authors": ["Jun Ma", "Yao Zhang", "Song Gu", "Cheng Ge", "Ershuai Wang", "Qin Zhou", "Ziyan Huang", "Pengju Lyu", "Jian He", "Bo Wang"], "abstract": "Organ and cancer segmentation in abdomen Computed Tomography (CT) scans is the prerequisite for precise cancer diagnosis and treatment. Most existing benchmarks and algorithms are tailored to specific cancer types, limiting their ability to provide comprehensive cancer analysis. This work presents the first international competition on abdominal organ and pan-cancer segmentation by providing a large-scale and diverse dataset, including 4650 CT scans with various cancer types from over 40 medical centers. The winning team established a new state-of-the-art with a deep learning-based cascaded framework, achieving average Dice Similarity Coefficient (DSC) scores of 92.3% for organs and 64.9% for lesions on the hidden multi-national testing set. The dataset and code of top teams are publicly available, offering a benchmark platform to drive further innovations https://codalab.lisn.upsaclay.fr/competitions/12239.", "sections": [{"title": "INTRODUCTION", "content": "Abdomen organs are quite common cancer sites, such as colorectal cancer and pancreatic cancer, which are the second and third most common cause of cancer death [1]. Computed Tomography (CT) scanning yields important prognostic information for cancer patients and is a widely used imaging technology for cancer diagnosis and treatment monitoring [2]. In both clinical trials and daily clinical practice, radiologists and clinicians measure the tumor and organ on CT scans based on manual measurements (e.g., Response Evaluation Criteria In Solid Tumors (RECIST) criteria) [3]. However, this manual assessment is inherently subjective with considerable inter- and intra-expert variability and cannot measure the 3D tumor morphology.\nDeep learning-based methods have shown great potential for automatic tumor segmentation and quantification. Many challenges have been established to benchmark algorithm performance by providing standard datasets and fair evaluation platforms, such as the brain tumor segmentation (BraTS) [4], liver and liver tumor segmentation [5], kidney and kidney tumor segmentation [6], and pancreas and colon lesion segmentation [7]. These challenges have greatly advanced algorithm development [8], [9], but they only focus on one type of lesion (e.g., liver cancer, kidney cancer, or pancreas cancer), which cannot provide holistic lesion analysis. Pan-cancer segmentation in abdomen CT plays an important role in clinical practice because lesions can spread from one organ to another organ. For example, pancreas cancer and colorectal cancer could transfer to the liver, leading to liver metastases. There is a great need for general algorithms that can segment all kinds of lesions from CT scans.\nRecently, universal lesion segmentation algorithms for abdomen CT have received increasingly attention [10]\u2013[13]. However, these algorithms are developed and evaluated under various datasets, leading to difficulties in fairly comparing them. The main barrier is the lack of a general benchmark platform and publicly available dataset. In this work, we addressed the limitation by providing the largest abdominal pan-cancer dataset and organized the first international competition to prompt the development of universal abdominal organ and lesion segmentation algorithms. In particular, we curated a diverse abdominal pan-cancer dataset with 4650 CT scans, covering various abdomen cancers from 50 medical centers, which is the most comprehensive abdomen pan-cancer dataset to date. The competition attracted 292 participants"}, {"title": "RESULTS", "content": ""}, {"title": "Challenge design", "content": "This challenge aimed to prompt the methodology development of fully automatic abdominal organ and lesion segmen-\ntation algorithms in CT scans. Different from the previous FLARE challenges designed for pure organ segmentation [14], [15], the FLARE 2023 challenge introduced three improvements (1a). First, the challenge task was expanded to joint organ"}, {"title": "Segmentation accuracy and efficiency analysis on the testing set", "content": "We show the testing set segmentation accuracy and efficiency performance of the 37 teams in Fig. 2a (Supplementary Table 6). The majority of teams are concentrated on the top left of the plot, indicating that most participants aimed to develop accurate and efficient algorithms. However, we also noticed that some teams only pursue the unilateral metric. For example, T25 and T30 consumed around 1500MB GPU memory, but the segmentation accuracy is inferior to the others. T24 achieved very competitive DSC score but the inference speed is slow, costing 99 seconds for each case. In contrast, the winning team (T1) stands out with an average DSC of 92.3% and 64.9% for organ and lesion segmentation, respectively, and an inference speed of 8.6s by consuming 3561.6 MB of GPU RAM.\nNext, we compared the performance of top five algorithms across seven metrics (organ DSC and NSD, lesion DSC and NSD, runtime, GPU memory consumption, and final rank) in terms of the number of algorithms it outperformed, aiming to provide a comprehensive comparison of the algorithms' strengths and weaknesses across the multiple criteria (Fig. 2b). The radar plot reveals that the top five algorithms outperformed most others across these metrics, as evidenced by the significant overlap in the plotted areas. However, closer inspection highlights subtle variations in performance. In particular, T1-aladdin5 demonstrated the highest accuracy in lesion segmentation, while T2-citi excelled in organ segmentation. They all obtained the perfect GPU consumption metric, but T4-hmi406 achieved the fastest inference speed. Overall, T1-aladdin5 secured the best final rank with a better balance between segmentation accuracy and computational efficiency.\nWe also analyzed the ranking stability of all the employed metrics regarding testing case sampling variability. Specifically, the bootstrap approach was used by generating 1000 bootstrap samples where each sample contained 400 randomly selected testing cases with replacement from the testing set. Then, we compute Kendall's \u03c4 values for all the metrics and Fig. 2c shows the corresponding distributions with violin and box plots. The Kendall's \u03c4 values for all metrics are clustered around 1.0, implying that the rankings of the algorithms are highly consistent and stable."}, {"title": "Organ-wise performance analysis", "content": "We further present a detailed analysis of the performance of top five teams in segmenting a diverse set of 13 organs (Fig. 3, Supplementary Table 7). Specifically, we categorized these organs into three groups based on their size, morphology, and segmentation challenges: large solid organs, small organs, and tubular organs. This classification allows us to highlight the specific difficulties and successes associated with each category, providing insights into how the algorithms perform differently across varying anatomical structures.\nThe first group consists of large solid organs, including the liver, kidneys, spleen, pancreas, and stomach. These organs generally have well-defined structures, making them more straightforward targets for segmentation algorithms. The liver, kidneys, and spleens demonstrated high and consistent performance in all five teams, with average DSC and NSD scores above 95% in all five teams (Fig. 3a-c). Their box plots also show tight clusters, suggesting that these organs are easier for segmentation due to the relatively large size and distinct boundaries. The pancreas, despite being part of this group, exhibited greater variability with average DSC scores ranging from 88.9% to 91.3%, reflecting the anatomical complexity and variability in shape and size across patients, which presents additional challenges for accurate segmentation (Fig. 3d). The stomach also performs well, with mean DSC scores between 93.4% (T4) and 94.9% (T1), indicating overall good segmentation accuracy across the teams (Fig. 3e).\nThe second group focuses on small organs, specifically the gallbladder and adrenal glands (Fig. 3f-g). These organs are characterized by their smaller size and less distinct boundaries, making them more difficult to segment. The DSC and NSD scores exhibit greater variability than large organs, ranging from 83.5% (T1-aladdin5) to 86.9% (T4-hmi306). The adrenal glands show a similar trend, with mean DSC scores from 79.0% (T3-blackbean) to 89.5% (T2-citi). These results suggest that the small size and less distinct boundaries of these organs pose substantial challenges for accurate and robust segmentation, leading to increased variability in performance across the different algorithms."}, {"title": "Lesion performance analysis", "content": "The challenge task approached the lesion as a semantic segmentation problem, categorizing each voxel as either part of a lesion or not. This approach aligns with organ segmentation tasks, enabling uniform evaluation across different cases. Alternatively, the task can be framed as an instance segmentation problem, which not only captures category information but also distinguishes between different lesions. This allows for the identification and analysis of multiple distinct lesions within the same image.\nWe evaluated the lesion segmentation results of the top five teams using both semantic segmentation metrics (DSC and NSD) and instance segmentation metrics (Sensitivity, Specificity, and F1 score), where we separated the disconnected lesions as individual entities with connected components analysis. The dot-box plots show the testing set performance distribution for each team across the five metrics (Fig. 4a, Supplementary Table 8). All the top three teams achieved a median DSC score of over 70% where T1-aladdin5 had the highest median DSC score of 75.9 (interquartile range (IQR):49.8-86.2%). However, the median NSD scores dropped below 60% for all the teams, indicating that lesion boundaries may not be accurately delineated and small lesions could be missed.\nIn terms of instance segmentation metrics, all the teams achieved high precision but low recall. This imbalance indicates that the algorithms are conservative in their segmentations. In particular, T1-aladdin5 and T4-hmi306 obtained the best specificity with median scores of 50.0% (IQR: 25.0-100.0%) and 50.0% (IQR: 0.0-100.0%), respectively. However, T4-hmi306 had the lowest sensitivity with a median score of 20.0% (IQR: 0.0-50.0%), indicating that many lesions were missed in the segmentation results. The F1 scores follow a similar trend to the DSC scores with T1-aladdin5 achieving the highest median score of 40.0% (IQR: 22.2-66.7%). Overall, the F1 scores across teams show a broad range, reflecting inconsistencies in balancing lesion detection precision and recall.\nWe also employed the majority vote approach to generate the ensemble results of the top three and top five algorithms, respectively, and computed the panoptic quality metric to understand both segmentation and detection quality. As shown in Fig. 4b (Supplementary Table 9), the ensemble models generally show comparable or slightly improved panoptic quality compared to the individual teams, with Ensemble-3 slightly outperforming the others in terms of median score. However, all teams and ensembles exhibit a wide range of panoptic quality scores, indicating variability in performance across different cases.\nNext, we analyzed the relationship between lesion volume and segmentation accuracy (DSC) of the winning algorithm T1-aladdin5 (Fig 4c). There is a clear trend where larger lesion volumes tend to correspond with higher DSC scores, suggesting that the algorithm performs better as the lesion volume increases. For smaller lesions, particularly those with volumes below approximately 100, the DSC values are widely dispersed, ranging from near 0 to around 80% or higher. This indicates that the segmentation algorithm struggles more with smaller lesions, leading to less consistent and generally lower DSC scores.\nTumor volume is an important image biomarker and we compared the predicted lesion volume to the true lesion volume (Fig. 4d). The result reveals that while the model generally performs well in predicting lesion volumes, with a strong overall correlation between true and predicted volumes, it exhibits variability in accuracy, particularly at the extremes of the volume range. Smaller lesions tend to be underestimated, while larger lesions are occasionally overestimated, as indicated by the spread of data points around the diagonal line in the scatter plot.\nFinally, we present a visualization of typical examples (Fig 4e) that contains two successful cases (the 1st and 2nd columns) and three failure examples (the 3rd to 5th columns). These examples demonstrate that the algorithm is capable of accurately identifying and segmenting both large and small lesions when their appearances and boundaries are well-defined. However, the algorithm often struggles with small, heterogeneous lesions, as evidenced by its complete failure to detect the pancreas and colorectal lesions leading to poor performance."}, {"title": "DISCUSSION", "content": "AI has revolutionized medical image segmentation tasks, but most algorithms rely on a large number of human expert annotations, which are extremely hard and expensive to collect. Moreover, the performance of existing algorithms is mainly evaluated based on accuracy-related metrics on limited cohorts while the generalization ability, running efficiency, and resource consumption are overlooked. These barriers hinder the wider adoption of AI algorithms in clinical practice. The main goal of this study was to address these critical issues. In particular, we created the largest abdomen organ and pan-cancer CT dataset with a well-defined segmentation task to benchmark algorithms. Furthermore, we organized an"}, {"title": "METHODS", "content": ""}, {"title": "Challenge schedule", "content": "The FLARE 2023 challenge was preregistered [19] and the proposal passed peer review at the 25th International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI 2023). We launched the challenge on April 1st 2023 on the CodaLab platform [48]. During the development phase, each team can submit up to three tuning set segmentation results every day to the online platform and get the segmentation accuracy scores. Moreover, each team also had five chances to submit docker containers to challenge organizers and obtain segmentation efficiency scores. During the testing phase, participants were required to submit the final algorithm docker by August 25th 2023. We manually evaluated all the submitted dockers on the hidden testing set and announced the results on October 8th 2023 at MICCAI."}, {"title": "Data standardization and annotation protocol", "content": "The data standardization followed the common practice in the other 3D medical image segmentation challenges [5]\u2013[7] and the past FLARE challenges [14], [15]. We curated the CT scans from public datasets based on the license permission. Detailed information on these datasets is presented in Supplementary Table 1-4. All CT scans were converted to the standard NIfTI format (https://nifti.nimh.nih.gov/) and preserved the original CT HU values. The orientation was standardized as canonical 'RAS', which means that the first, second, and third voxel axes go from left to Right, posterior to Anterior, and inferior to superior, respectively. Organ annotation protocol remained the same as for the FLARE 2022 challenge [15], which adhered to the radiation therapy oncology group consensus (RTOG) panel guideline [49] and Netter's anatomical atlas [50]. In the training set, the lesion annotations in the source datasets were directly used. In the tuning and testing set, all visible lesions were annotated by a senior radiologist with the assistance of ITK-SNAP [51] and MedSAM [52]."}, {"title": "Evaluation protocol", "content": "All algorithms were sequentially run on the same GPU desktop workstation for a fair evaluation. The workstation was a Ubuntu 20.04 desktop with one central processing unit (CPU, Intel Xeon(R) W-2133 CPU, 3.60GHz \u00d7 12 cores), one graph processing unit (GPU, NVIDIA QUADRO RTX5000, 16G), 32G of memory, and 500G of hard disk drive storage. We used two groups of metrics to evaluate segmentation accuracy and efficiency. Following the recommendations in Metrics Reloaded [53], the segmentation accuracy metrics contained Dice Similarity Coefficient (DSC) and Normalized Surface Distance (NSD), measuring the region and boundary overlap between segmentation mask and reference standards. The segmentation efficiency metrics included runtime and Area Under the Curve of GPU memory-time (AUC GPU), where the GPU memory consumption was recorded every 0.1s. In addition, we also analyzed commonly used instance segmentation metrics for lesion segmentation, including precision, recall, F1 score, and panoptic quality."}, {"title": "Ranking scheme", "content": "The final rank was computed with both segmentation accuracy and efficiency metrics. We give runtime and GPU memory consumption a tolerance of 15s and 4GB, respectively, because they are acceptable in clinical practice. The employed metrics cannot be directly merged because of the dimension difference. Thus, we used rank-then-aggregation to obtain the final rank. Specifically, the ranking scheme had three steps:\n\u2022 Step 1. Compute the six metrics for each case in the testing set (N=400), including two organ-wise metrics: average DSC and NSD scores for 13 abdominal organs; two lesion-wise metrics: DSC and NSD scores; two efficiency metrics: runtime and area under GPU memory-time curve.\n\u2022 Step 2. Rank algorithms for each of the 400 testing cases and each metric. Each algorithm has 2400 (400x6) rankings.\n\u2022 Step 3. Compute the final rank for each algorithm by averaging all the rankings."}, {"title": "Ranking stability and statistical analysis", "content": "We applied the bootstrapping approach and computed Kendall's \u03c4 [54] to quantify the variability of the ranking scheme. Specifically, we first extracted 1000 bootstrap samples from the international validation set and computed the ranks again for each bootstrap sample. Then, the ranking agreement was quantified by Kendall's \u03c4. Kendall's \u03c4 computes the number of pairwise concordances and discordances between ranking lists. Its value ranges [-1,1] where -1 and 1 denote inverted and identical order, respectively. A stable ranking scheme should have a high Kendall's \u03c4 value that is close to 1. Wilcoxon signed rank test was used to compare the performance of different algorithms. Results were considered statistically significant if the p-value is less than 0.05. The following packages were used in the analysis: ChallengeR [55], Python 3 [56], Numpy [57], Pandas [58], Scipy [59], PyTorch [60], and matplotlib [61]."}, {"title": "Data availability", "content": "All the datasets have been publicly available on the challenge website https://codalab.lisn.upsaclay.fr/competitions/12239."}, {"title": "Code availability", "content": "The code, method descriptions, and docker containers of the top ten teams are available at https://codalab.lisn.upsaclay.fr/competitions/12239#learn_the_details-awards."}]}