{"title": "An Ad-hoc graph node vector embedding algorithm for general knowledge graphs using Kinetica-Graph\u2020", "authors": ["B. Kaan Karamete", "Eli Glaser"], "abstract": "This paper discusses how to generate general graph node embeddings from knowledge graph representations. The embedded space is composed of a number of sub-features to mimic both local affinity and remote structural relevance. These sub-feature dimensions are defined by several indicators that we speculate to catch nodal similarities, such as hop-based topological patterns, the number of overlapping labels, the transitional probabilities (markov-chain probabilities), and the cluster indices computed by our recursive spectral bisection (RSB) algorithm. These measures are flattened over the one dimensional vector space into their respective sub-component ranges such that the entire set of vector similarity functions could be used for finding similar nodes. The error is defined by the sum of pairwise square differences across a randomly selected sample of graph nodes between the assumed embeddings and the ground truth estimates as our novel loss function defined by Equation 3. The ground truth is estimated to be a combination of pairwise Jaccard similarity and the number of overlapping labels. Finally, we demonstrate a multi-variate stochastic gradient descent (SGD) algorithm to compute the weighing factors among sub-vector spaces to minimize the average error using a random sampling logic.", "sections": [{"title": "1. Introduction", "content": "There is not a definitive way to represent fixed di- mension vector node embed-dings from variable di- mension general knowledge graph connections (re- lations as edges). The simple reasoning is that there is really no rule in a general graph connection sense. Nevertheless, in the last decade, researchers have been pushing the envelope to apply the success of vector embedding advancements in Large Lan- guage Models (LLM) over to the general context knowledge graphs. A number of novel algorithms, namely, node2vec and word2vec [1, 2] devised with lots of success in language semantics and undoubt- edly opened doors for today's many AI applications that seem to be championed as the master key so- lution for most of our engineering problems, if not all (exclusions are mostly in multi constraint opti- mization problems, such as supply chain logistics and optimal fleet routes). Though, possibly a more humble acceptance of LLM's superiority is when there is supposedly a hidden pattern among word pairings that can be put in a neural net machinery to minimize the error between the assumed solution and the known ground truth based on either some training data (supervised) or a logic of differentia- tion (unsupervised/reinforced) [3]. However different in its details of minimizing er- rors to create these sophisticated language models that would produce intended outcomes, its supe- riority mainly lies in the deterministic nature of the input and the output with the additions of some fuzziness so that near-reality outcomes could be achieved [4, 5]. In a general knowledge graph sense, however, there is no language ruling for how a node 'Tom' is connected to 'Bill' or 'Jane' or to the country of his/her birth place, certainly none better than mere connections as node to node 'relation's."}, {"title": "2. Sub-vector features", "content": "The vector space is divided into sub-group range of indices that are indicative of ad-hoc graph pred- icates as shown in Figure 1. This is crucial since a value at an index location would have a specific meaning for every node and a share in similarity score when it is inner product-ed with that of an- other node. These predicates are specifically chosen to capture the local and remote affinities. The fol- lowing predicates are chosen per graph node:\nHop-patterns\nLabel index associations\nCluster index\nTransitional probability\nThese predicates are explained in detail below."}, {"title": "2.1. hop-patterns", "content": "The first feature predicate encompasses a range of indices to depict hop based pattern numbers as shown in Figure 2. Hop pattern of a node is defined by the number of forks and the number of nodes in each fork arm as shown with the respective col- ors per hop; e.g., second hop depicted as cyan has two forks with two nodes at each fork arm. This is not full-fledged topological pattern matching, since that would require the node indices instead of the number of nodes at breadth-first search (bfs) adja- cency traversal. The reason why we can not use the node indices in the vector is that it does not have a meaning as a value subject to inner product. If we can find a better means to universally reflect node indices in vector embeddings, this sub-feature could be replaced with much accurate values, but for now, we'll be using this light weight topologi- cal feature. Maximum number of hops is added as an option to the embedding algorithm as the set of pattern based numbers can slide within the array based on this option."}, {"title": "2.2. Label indices", "content": "We have devised in [10] an efficient mechanism to attach multiple labels to nodes and edges. The labels are stored with their unique indexes in the graph db. This feature has as many sub-range in- dices in the vector as there are unique labels in the graph (that has node-associations). The idea as similar to hop patterns is for these array indexes to have an absolute meaning throughout the nodes, i.e., if a label index is common to a number of nodes, the specific array index for that label should be turned on for all those nodes that share the same label. The label indices are depicted in Figure 1 as k, m, n, p for each sub-feature, respectively."}, {"title": "2.3. Cluster indices", "content": "A recursive spectral bisection algorithm is used to compute the cluster indexes of graph nodes. The idea in this Kinetica-Graph implementation is in- spired from [8] where the sorted second smallest eigenvector of the graph Laplacian is used in bisect- ing at the median location at each recursive split as depicted in Algorithm 6. The choice of partitioning for the endpoint match/graph with 'spectral' option is particularly preferred for its speed of execution and low resource allocation requirement compared to the Louvain clustering [14] (another option in the solver) as shown in the Figure 8. A geometri- cal example of the RSB method with three levels of bisections can be seen in Figure 7.\nThe cluster index per node is pushed into the re- spective sub-range allocated for this feature. The width of this feature over the vector space can be scaled by overriding the default value of 8 via the maximum number of clusters option to the embed- ding solver. The output of the RSB clustering is shown as a DB table in Figure 9."}, {"title": "2.4. Transitional Probabilities", "content": "Inspired from the Pagerank algorithm [15], our novel probability ranking solver uses the same equa- tion depicted in 1 with a modified transition proba- bility flux $p_{ij}$ where it is computed to be the ratio of incoming adjacent edge weights (connecting nodes i and j) within the immediate neighbor B(i) to each node i. These nodal scalar $p_i$ values are iterated at each traversal loop converging to a steady state where the maximal change in $p_i$ is less than a small threshold. The ranking factor r is assumed to be 0.15 so that every node will have a small amount of uniform probability (r divided by the number of graph nodes $N_V$) to account particularly for nodes with no incoming edges.\n$p_i = (1 - r) \\sum_{j \\in B(i)} p_{ij} + \\frac{r}{N_V}$\nThese transitional probabilities are scaled be- tween zero and one and the sub-range for this fea- ture over the vector is allocated to a preset division. For example, if a particular node's probability is 0.25, with the default range of ten, the third index within the sub-vector range is turned on. It is also worth mentioning that this solver behaves exactly like a Pagerank solver for the uniform identical edge weights scenario where the transitional probability defined above simply becomes the incoming valence rank."}, {"title": "3. Flattening", "content": "Hop-pattern numbers are per each fork of a hop. Therefore, primary flattening occurs in mapping this two dimensional information over the one di- mensional vector space. Furthermore, a secondary flattening happens for laying the sub-feature's own indexing after the previous feature's flattened in- dex location. Label and cluster indices do not re- quire flattening but their respective vector loca- tions have to be shifted after the prior feature's index range. Transitional probability, however, would require to map the continuous scalar prob- ability values to be bucketed over a preset num- ber of interval indices. This is easily accomplished by partitioning the unit range equally over a pre- set number of intervals (can be modified by the user). The vector size formula is given in Equa- tion 2 where each sub-feature's respective ranges are summed. The parameters max_num_clusters, max_hops are user driven and max_forks_perhop, max_edges_perfork are set implicitly to minimize the number of pattern indices within the overall vector dimension.\n$Vector\\_size = max\\_patterns + max\\_labels + max\\_num\\_clusters + num\\_probabilities$\\n$where$\\n$max\\_patterns = max\\_hops \\times max\\_forks\\_perhop \\times max\\_edges\\_per fork$\nBefore the normalization process, these vector values within each sub-feature are multiplied by a feature specific weight parameter. We can ex- tend the number of sub-features in our embed- ding framework, however, at the time of writing this manuscript, we are currently having four sub-features, hence, we have only four weight parame- ters that we will use for the purpose of finding out"}, {"title": "4. Loss Function", "content": "The concept of assuming the total error dis- tributed evenly across the nodal pairs is inspired from the computational mechanics field [16], specif- ically in finite element analysis defined as z-square where the elemental errors are aggregated over the entire domain and then divided equally over the fi- nite elements. Similarly, in our sub-feature weight optimization, we can define the total error as the aggregated sum of the differences between the in- ner product of each node against every other in a subset of the graph and the ground truth esti- mates for each pair. Specifically, the error is defined by the sum of pairwise square differences across a randomly selected sample of graph nodes between the assumed embeddings and the ground truth es- timates as our novel loss function defined by Equa- tion 3. The ground truth is estimated to be a combination of pairwise Jaccard similarity and the number of overlapping labels.\nLoss function is defined per node i such that the goal is to find the average difference aggregated over all the pairs from the node i to all other $n_k$ num- ber of nodes. The similarity, i.e., the inner product between the vector embeddings of $f_i$ and $f_k$ is sub- tracted from the pairwise sum of Jaccard similarity"}, {"title": "5. Stochastic Gradient Descent", "content": "The sum of pairwise differences between the inner product of pairs for node i, node k and the ad-hoc ground truth from Jaccard scores with overlapping label count ratio as depicted in Equation 3(a) is dependent on the unknown terms $w_j$ as the weight factor of each four sub-features. The selection of the set of graph nodes where each node is paired with every other in the set is important in finding these optimal weights. The process needs to include nodes to have a good representation of the entire graph behavior. We have opted to sample this set randomly (stochastic) with a caveat of picking the batch of nodes from each cluster index group where we have already computed in constructing the sub-features. The number of nodes in this random se- lection process is user specified, however, it needs to be much less than the original graph size so that the SGD iterations would not be prohibitive.\nThe next step is taking the derivative of the nodal average of the loss per each of these weight param- eters and move against the direction of the gradient of each weight to minimize the loss. The incremen- tal update on each unknown weigth is immediately made to reflect its impact on the next unknown weight variable computation as shown in Equa- tion 4 and 5 in which $w_j^{(k+1)}$ is the next (k + 1)th iteration on the jth weight parameter. This ap- proach is not a guaranteed outcome in accelerat- ing the convergence. Other alternative approaches such as batching or mini-batching discussed exten- sively in [13] might provide better computational outcome. However, we think this is beyond the scope of this study and our findings are satisfac- tory computationally for the cases we have tried so far. The iterations are continued if the relative incremental iteration delta of all unknown weights go below a preset user threshold (default value is 0.001) or the number of epoch iterations reaches the upper limit (default is 100) which is also a user prescribed parameter of the solver as shown in Fig- ure 12. The convergence history plot between the number of iterations and the error is also shown in Figure 13 for the knowledge graphs of varying sizes from a few hundred to 100 million nodes. The trend in all cases is with early unstable fluctuations and rapid descending to the optimal as expected. The initial weight values and the rate of iteration, also known as training or learning rate, namely, $\\beta$ as shown in Equation 4, can both also be overridden by the user.\n$\\frac{\\partial \\sum(Loss_i)}{\\partial w_j}$ / $NV$\\n$w_j^{(k+1)} \\leftarrow - \\beta - \\frac{\\frac{\\partial \\sum(Loss_i)}{\\partial w_j}}{\\frac{\\partial \\sum(Loss_i)}{\\partial w_j}}$\n$\\frac{\\partial \\sum(Loss_i)}{\\partial w_j} = 4 \\sum_i \\sum_m ( <f_i, f_m> \\times w_j s_{ir} s_{mj})$"}, {"title": "6. Discussion and Conclusions", "content": "We have tried to map unlimited dimen- sion general knowledge graph topology onto a 1-dimensional vector embeddings by constructing the vector space from features that we think would best resemble local affinity and remote structure so that any vector similarity (inner product) be- tween a node pair would result in the same sim- ilarity behavior if we had computed the Jaccard score with the number of common labels between the two nodes of every pair. To this end, the sub- features are chosen to be the predicates of hop- pattern numbers, cluster indices (computed by the recursive spectral bisection (RSB)), associated la- bel indices, and the transitional probability (or the Pagerank score if weights are uniform) as explained in Section 2 above. The impact of the vector com- ponent sub-features on the similarity can be found by adding a weight parameter to multiply each of the sub-feature elements within the respective vec- tor sub-range and test the result against an es- timated ground truth as explained in Section 4. We have formulated the difference between the in- ner product of the assumed embeddings and the combined common-labels and Jaccard score as the ground truth, as our ad-hoc Loss function as shown in Equation 3. We then tried to optimize the nodal average of the total loss by applying a stochastic gradient descent (SGD) algorithm to find the un- known weights so that this total average loss is min- imized as explained in Section 5 and Equations 4 and 5.\nStochastic process is the selection of the random nodes that will be used in SGD to find the unknown weights. We have chosen these random nodes from each graph cluster in equally numbers. We use this smaller sub-set of nodes in computing the unknown weights. The assumption of picking this narrow set of nodes from each cluster is to increase the like- lihood of better representation of the entire graph since SGD on the entire graph is computationally prohibitive. SGD converges very similarly in our testing of many graphs as shown in Figure 13. The banking graph shown, is in 10+ million range (4+ billion case is also used), and its ontology is de- picted in Figure 14.\nThe output of the embedding solver is a database table with a vector per graph node as depicted in Figure 15. These embedding results can be used in any vector similarity functions; such as a co- sine similarity as depicted in Figure 16. A com- mon use case for vector similarity is, for instance, in recommendation engines for various industries, from friend recommendations in social networks to the next likely item in your shopping chart. The efficiency and accuracy of these embeddings, how- ever, depends on the rich-ness of the vector sub- features and the sophistication of the randomly se- lected training sets in optimizing the vector con- tents. We argue that even the best embedding algo- rithm would be less accurate compared to the pre- cise connections and labels depicted in the graph topology itself. However, mapping of graphs to vectors has a distinct advantage that they can be applied in a standard manner using simple vector functions in many AI modules. The alternative of using knowledge graph analytics has almost no standardization in many downstream AI applica- tions provided by various graph vendors.\nThe stipulation of the existing four sub-features representative of the graph topology can certainly be mitigated by either adding more features or a different set of predicates. One other criterion that seems to make sense to include is the distance met- ric as discussed in [6, 7]; which considers similarity for nodes at an equal distance from a set source. However, this statement implies to include all nodes to be similar lying on the same ring-radius distance (hop or weight distance) from the center as the source. This is however a wrong postulate since we know that the nodes on the same ring may be at equal distance away from a source at the center, but they are no-where close to each other particularly for the nodes across each other at any section of the ring. However, along with the cluster index as is already a sub-feature, the combined effect (always consider the inner-product sense) might move the argument to a more acceptable and even preferred state. Another area of future development is in the dynamic additions to the graph, and how to up- date graph embeddings for the new additions that should be calculable instantly and ready for vector analysis in order for it to be useful in real-time sim- ulations. We are considering to eliminate recom- puting of the embeddings for new node insertions by caching and using the results of already com- puted weight parameters and interpolating proba- bility and cluster indexes from adjacent nodes in- stead of running compute heavy cluster and proba-"}]}