{"title": "An Ad-hoc graph node vector embedding algorithm for general knowledge graphs using Kinetica-Graph\u2020", "authors": ["B. Kaan Karamete", "Eli Glaser"], "abstract": "This paper discusses how to generate general graph node embeddings from knowledge graph represen-\ntations. The embedded space is composed of a number of sub-features to mimic both local affinity and\nremote structural relevance. These sub-feature dimensions are defined by several indicators that we spec-\nulate to catch nodal similarities, such as hop-based topological patterns, the number of overlapping labels,\nthe transitional probabilities (markov-chain probabilities), and the cluster indices computed by our recursive\nspectral bisection (RSB) algorithm. These measures are flattened over the one dimensional vector space into\ntheir respective sub-component ranges such that the entire set of vector similarity functions could be used\nfor finding similar nodes. The error is defined by the sum of pairwise square differences across a randomly\nselected sample of graph nodes between the assumed embeddings and the ground truth estimates as our\nnovel loss function defined by Equation 3. The ground truth is estimated to be a combination of pairwise\nJaccard similarity and the number of overlapping labels. Finally, we demonstrate a multi-variate stochastic\ngradient descent (SGD) algorithm to compute the weighing factors among sub-vector spaces to minimize\nthe average error using a random sampling logic.", "sections": [{"title": "1. Introduction", "content": "There is not a definitive way to represent fixed di-\nmension vector node embed-dings from variable di-\nmension general knowledge graph connections (re-\nlations as edges). The simple reasoning is that\nthere is really no rule in a general graph connection\nSense. Nevertheless, in the last decade, researchers\nhave been pushing the envelope to apply the success\nof vector embedding advancements in Large Lan-\nguage Models (LLM) over to the general context\nknowledge graphs. A number of novel algorithms,\nnamely, node2vec and word2vec [1, 2] devised with\nlots of success in language semantics and undoubt-\nedly opened doors for today's many AI applications\nthat seem to be championed as the master key so-\nlution for most of our engineering problems, if not\nall (exclusions are mostly in multi constraint opti-\nmization problems, such as supply chain logistics\nand optimal fleet routes). Though, possibly a more\nhumble acceptance of LLM's superiority is when\nthere is supposedly a hidden pattern among word\npairings that can be put in a neural net machinery\nto minimize the error between the assumed solution\nand the known ground truth based on either some\ntraining data (supervised) or a logic of differentia-\ntion (unsupervised/reinforced) [3].\nHowever different in its details of minimizing er-\nrors to create these sophisticated language models\nthat would produce intended outcomes, its supe-\nriority mainly lies in the deterministic nature of\nthe input and the output with the additions of\nsome fuzziness so that near-reality outcomes could\nbe achieved [4, 5]. In a general knowledge graph\nsense, however, there is no language ruling for how\na node 'Tom' is connected to 'Bill' or 'Jane' or to the\ncountry of his/her birth place, certainly none better\nthan mere connections as node to node 'relation's.\nIt is always possible however, the problem can be\ncast into a language model by connecting these gen-\neral nodal relations around building sentences and\nparagraphs. These embeddings are broadly gener-\nalized into 'translational' and 'semantic' categories\nand extensively surveyed in [6, 7] where the former\nis distance based and the latter is relation centric.\nThe intention of this paper is not to find this\nmapping from unstructured knowledge graphs to\nlanguage graph models so we could apply the cel-\nebrated node2vec method to create nodal embed-\ndings for similarity analysis. Our goal, however,\nis to create an ad-hoc mapping framework that is\nbased on each graph's own analytics and using as\nmuch machinery as possible from LLM technology\nto mimic similarities between the node pairs and\ncombine 'translational' and 'semantic' mappings to-\ngether. Perhaps, one could rephrase that a graph is\nits own AI model where its connections reveal the\nunstructured information in its most true form and\nany other representation is just an approximation\nat best.\nIn this spirit, we try to come up with a vector em-\nbedding in which we use a number of graph pred-\nicates, such as topological hop-patterns, common\nlabels, transitional probabilities via Markov-chain\n(MC) probabilities, and clustering indices via the\nrecursive spectral bisection (RSB) solver [8] using\nKinetica-Graph [9, 10, 11, 12]. We would refer these\nas sub-features and explain each group in Section 2\nin detail. We then describe a flattening procedure\nto spread these sub-feature predicates onto the sub-\nranges of the vector embedding space in Section 3.\nA novel loss function definition is described in Sec-\ntion 4 where an average embedding error is assumed\nto be the sum of square differences between the in-\nner product of nodal vector pairings and pairwise\nsum of Jaccard scores combined with pairwise com-\nmon labels. Finally, we will show a stochastic gra-\ndient descent (SGD) algorithm [13] that minimizes\nthis average error by adjusting the weights among\nsub-feature groups in the embedded space in Sec-\ntion 5."}, {"title": "2. Sub-vector features", "content": "The vector space is divided into sub-group range\nof indices that are indicative of ad-hoc graph pred-\nicates as shown in Figure 1. This is crucial since\na value at an index location would have a specific\nmeaning for every node and a share in similarity\nscore when it is inner product-ed with that of an-\nother node. These predicates are specifically chosen\nto capture the local and remote affinities. The fol-\nlowing predicates are chosen per graph node:\nHop-patterns\nLabel index associations\nCluster index\nTransitional probability\nThese predicates are explained in detail below."}, {"title": "2.1. hop-patterns", "content": "The first feature predicate encompasses a range\nof indices to depict hop based pattern numbers as\nshown in Figure 2. Hop pattern of a node is defined\nby the number of forks and the number of nodes\nin each fork arm as shown with the respective col-\nors per hop; e.g., second hop depicted as cyan has\ntwo forks with two nodes at each fork arm. This is\nnot full-fledged topological pattern matching, since\nthat would require the node indices instead of the\nnumber of nodes at breadth-first search (bfs) adja-\ncency traversal. The reason why we can not use the\nnode indices in the vector is that it does not have\na meaning as a value subject to inner product. If\nwe can find a better means to universally reflect\nnode indices in vector embeddings, this sub-feature\ncould be replaced with much accurate values, but\nfor now, we'll be using this light weight topologi-\ncal feature. Maximum number of hops is added as\nan option to the embedding algorithm as the set of\npattern based numbers can slide within the array\nbased on this option."}, {"title": "2.2. Label indices", "content": "We have devised in [10] an efficient mechanism\nto attach multiple labels to nodes and edges. The\nlabels are stored with their unique indexes in the\ngraph db. This feature has as many sub-range in-\ndices in the vector as there are unique labels in the\ngraph (that has node-associations). The idea as\nsimilar to hop patterns is for these array indexes to\nhave an absolute meaning throughout the nodes,\ni.e., if a label index is common to a number of\nnodes, the specific array index for that label should\nbe turned on for all those nodes that share the same\nlabel. The label indices are depicted in Figure 1 as\nk, m, n, p for each sub-feature, respectively."}, {"title": "2.3. Cluster indices", "content": "A recursive spectral bisection algorithm is used\nto compute the cluster indexes of graph nodes. The\nidea in this Kinetica-Graph implementation is in-\nspired from [8] where the sorted second smallest\neigenvector of the graph Laplacian is used in bisect-\ning at the median location at each recursive split as\ndepicted in Algorithm 6. The choice of partitioning\nfor the endpoint match/graph with 'spectral' option\nis particularly preferred for its speed of execution\nand low resource allocation requirement compared\nto the Louvain clustering [14] (another option in\nthe solver) as shown in the Figure 8. A geometri-\ncal example of the RSB method with three levels of\nbisections can be seen in Figure 7.\nThe cluster index per node is pushed into the re-\nspective sub-range allocated for this feature. The\nwidth of this feature over the vector space can be\nscaled by overriding the default value of 8 via the\nmaximum number of clusters option to the embed-\nding solver. The output of the RSB clustering is\nshown as a DB table in Figure 9."}, {"title": "2.4. Transitional Probabilities", "content": "Inspired from the Pagerank algorithm [15], our\nnovel probability ranking solver uses the same equa-\ntion depicted in 1 with a modified transition proba-\nbility flux $p_{ij}$ where it is computed to be the ratio of\nincoming adjacent edge weights (connecting nodes i\nand j) within the immediate neighbor B(i) to each\nnode i. These nodal scalar $p_i$ values are iterated\nat each traversal loop converging to a steady state\nwhere the maximal change in $p_i$ is less than a small\nthreshold. The ranking factor r is assumed to be\n0.15 so that every node will have a small amount\nof uniform probability (r divided by the number of\ngraph nodes $N_V$) to account particularly for nodes\nwith no incoming edges.\n$P_{i} = (1 - r) \\sum_{j \\in B(i)} P_{ij} + \\frac{r}{N_V}$ (1)\nThese transitional probabilities are scaled be-\ntween zero and one and the sub-range for this fea-\nture over the vector is allocated to a preset division.\nFor example, if a particular node's probability is\n0.25, with the default range of ten, the third index\nwithin the sub-vector range is turned on. It is also\nworth mentioning that this solver behaves exactly\nlike a Pagerank solver for the uniform identical edge\nweights scenario where the transitional probability\ndefined above simply becomes the incoming valence\nrank."}, {"title": "3. Flattening", "content": "Hop-pattern numbers are per each fork of a hop.\nTherefore, primary flattening occurs in mapping\nthis two dimensional information over the one di-\nmensional vector space. Furthermore, a secondary\nflattening happens for laying the sub-feature's own\nindexing after the previous feature's flattened in-\ndex location. Label and cluster indices do not re-\nquire flattening but their respective vector loca-\ntions have to be shifted after the prior feature's\nindex range. Transitional probability, however,\nwould require to map the continuous scalar prob-\nability values to be bucketed over a preset num-\nber of interval indices. This is easily accomplished\nby partitioning the unit range equally over a pre-\nset number of intervals (can be modified by the\nuser). The vector size formula is given in Equa-\ntion 2 where each sub-feature's respective ranges\nare summed. The parameters max_num_clusters,\nmax_hops are user driven and max_forks_perhop,\nmax_edges_perfork are set implicitly to minimize the\nnumber of pattern indices within the overall vector\ndimension.\n$Vector\\_size = max\\_patterns + \\newline max\\_labels + \\newline max\\_num\\_clusters + \\newline num\\_probabilities$ \\newline $where$ \\newline $max\\_patterns = max\\_hops \\times \\newline max-forks\\_perhop \\times \\newline max\\_edges\\_per fork$ (2)\nBefore the normalization process, these vector\nvalues within each sub-feature are multiplied by\na feature specific weight parameter. We can ex-\ntend the number of sub-features in our embed-\nding framework, however, at the time of writing\nthis manuscript, we are currently having four sub-\nfeatures, hence, we have only four weight parame-\nters that we will use for the purpose of finding out"}, {"title": "4. Loss Function", "content": "The concept of assuming the total error dis-\ntributed evenly across the nodal pairs is inspired\nfrom the computational mechanics field [16], specif-\nically in finite element analysis defined as z-square\nwhere the elemental errors are aggregated over the\nentire domain and then divided equally over the fi-\nnite elements. Similarly, in our sub-feature weight\noptimization, we can define the total error as the\naggregated sum of the differences between the in-\nner product of each node against every other in\na subset of the graph and the ground truth esti-\nmates for each pair. Specifically, the error is defined\nby the sum of pairwise square differences across a\nrandomly selected sample of graph nodes between\nthe assumed embeddings and the ground truth es-\ntimates as our novel loss function defined by Equa-\ntion 3. The ground truth is estimated to be a\ncombination of pairwise Jaccard similarity and the\nnumber of overlapping labels.\nLoss function is defined per node i such that the\ngoal is to find the average difference aggregated over\nall the pairs from the node i to all other nk num-\nber of nodes. The similarity, i.e., the inner product\nbetween the vector embeddings of $f_i$ and $f_k$ is sub-\ntracted from the pairwise sum of Jaccard similarity"}, {"title": "5. Stochastic Gradient Descent", "content": "The sum of pairwise differences between the inner\nproduct of pairs for node i, node k and the ad-hoc\nground truth from Jaccard scores with overlapping\nlabel count ratio as depicted in Equation 3(a) is\ndependent on the unknown terms $w_j$ as the weight\nfactor of each four sub-features. The selection of\nthe set of graph nodes where each node is paired\nwith every other in the set is important in finding\nthese optimal weights. The process needs to include\nnodes to have a good representation of the entire\ngraph behavior. We have opted to sample this set\nrandomly (stochastic) with a caveat of picking the\nbatch of nodes from each cluster index group where\nwe have already computed in constructing the sub-\nfeatures. The number of nodes in this random se-\nlection process is user specified, however, it needs\nto be much less than the original graph size so that\nthe SGD iterations would not be prohibitive.\nThe next step is taking the derivative of the nodal\naverage of the loss per each of these weight param-\neters and move against the direction of the gradient\nof each weight to minimize the loss. The incremen-\ntal update on each unknown weigth is immediately\nmade to reflect its impact on the next unknown\nweight variable computation as shown in Equa-\ntion 4 and 5 in which $w_j^{(k+1)}$ is the next (k + 1)th\niteration on the jth weight parameter. This ap-\nproach is not a guaranteed outcome in accelerat-\ning the convergence. Other alternative approaches\nsuch as batching or mini-batching discussed exten-\nsively in [13] might provide better computational\noutcome. However, we think this is beyond the\nscope of this study and our findings are satisfac-\ntory computationally for the cases we have tried\nso far. The iterations are continued if the relative\nincremental iteration delta of all unknown weights\ngo below a preset user threshold (default value is\n0.001) or the number of epoch iterations reaches\nthe upper limit (default is 100) which is also a user\nprescribed parameter of the solver as shown in Fig-\nure 12. The convergence history plot between the\nnumber of iterations and the error is also shown in\nFigure 13 for the knowledge graphs of varying sizes\nfrom a few hundred to 100 million nodes. The trend\nin all cases is with early unstable fluctuations and\nrapid descending to the optimal as expected. The\ninitial weight values and the rate of iteration, also\nknown as training or learning rate, namely, $\\beta$ as\nshown in Equation 4, can both also be overridden\nby the user.\n$\\omega_{j}^{(k+1)} \\leftarrow - \\beta - \\frac{\\sum(Loss_{i})}{\\partial\\omega_{j}} / NV$ (4)\n$\\frac{\\partial \\sum(Loss_{i})}{\\partial\\omega_{j}} = 4 \\sum \\sum ( < f_i, f_m > \\times \\omega_{j}s_{i}s_{j} )$ (5)"}, {"title": "6. Discussion and Conclusions", "content": "We have tried to map unlimited dimen-\nsion general knowledge graph topology onto a\n1-dimensional vector embeddings by constructing\nthe vector space from features that we think would\nbest resemble local affinity and remote structure\nso that any vector similarity (inner product) be-\ntween a node pair would result in the same sim-\nilarity behavior if we had computed the Jaccard\nscore with the number of common labels between\nthe two nodes of every pair. To this end, the sub-\nfeatures are chosen to be the predicates of hop-\npattern numbers, cluster indices (computed by the\nrecursive spectral bisection (RSB)), associated la-\nbel indices, and the transitional probability (or the\nPagerank score if weights are uniform) as explained\nin Section 2 above. The impact of the vector com-\nponent sub-features on the similarity can be found\nby adding a weight parameter to multiply each of\nthe sub-feature elements within the respective vec-\ntor sub-range and test the result against an es-\ntimated ground truth as explained in Section 4.\nWe have formulated the difference between the in-\nner product of the assumed embeddings and the\ncombined common-labels and Jaccard score as the\nground truth, as our ad-hoc Loss function as shown\nin Equation 3. We then tried to optimize the nodal\naverage of the total loss by applying a stochastic\ngradient descent (SGD) algorithm to find the un-\nknown weights so that this total average loss is min-\nimized as explained in Section 5 and Equations 4\nand 5.\nStochastic process is the selection of the random\nnodes that will be used in SGD to find the unknown\nweights. We have chosen these random nodes from\neach graph cluster in equally numbers. We use this\nsmaller sub-set of nodes in computing the unknown\nweights. The assumption of picking this narrow set\nof nodes from each cluster is to increase the like-\nlihood of better representation of the entire graph\nsince SGD on the entire graph is computationally\nprohibitive. SGD converges very similarly in our\ntesting of many graphs as shown in Figure 13. The\nbanking graph shown, is in 10+ million range (4+\nbillion case is also used), and its ontology is de-\npicted in Figure 14.\nThe output of the embedding solver is a database\ntable with a vector per graph node as depicted in\nFigure 15. These embedding results can be used\nin any vector similarity functions; such as a co-\nsine similarity as depicted in Figure 16. A com-\nmon use case for vector similarity is, for instance,\nin recommendation engines for various industries,\nfrom friend recommendations in social networks to\nthe next likely item in your shopping chart. The\nefficiency and accuracy of these embeddings, how-\never, depends on the rich-ness of the vector sub-\nfeatures and the sophistication of the randomly se-\nlected training sets in optimizing the vector con-\ntents. We argue that even the best embedding algo-\nrithm would be less accurate compared to the pre-\ncise connections and labels depicted in the graph\ntopology itself. However, mapping of graphs to\nvectors has a distinct advantage that they can be\napplied in a standard manner using simple vector\nfunctions in many AI modules. The alternative\nof using knowledge graph analytics has almost no\nstandardization in many downstream AI applica-\ntions provided by various graph vendors.\nThe stipulation of the existing four sub-features\nrepresentative of the graph topology can certainly\nbe mitigated by either adding more features or a\ndifferent set of predicates. One other criterion that\nseems to make sense to include is the distance met-\nric as discussed in [6, 7]; which considers similarity\nfor nodes at an equal distance from a set source.\nHowever, this statement implies to include all nodes\nto be similar lying on the same ring-radius distance\n(hop or weight distance) from the center as the\nsource. This is however a wrong postulate since we\nknow that the nodes on the same ring may be at\nequal distance away from a source at the center, but\nthey are no-where close to each other particularly\nfor the nodes across each other at any section of the\nring. However, along with the cluster index as is\nalready a sub-feature, the combined effect (always\nconsider the inner-product sense) might move the\nargument to a more acceptable and even preferred\nstate. Another area of future development is in the\ndynamic additions to the graph, and how to up-\ndate graph embeddings for the new additions that\nshould be calculable instantly and ready for vector\nanalysis in order for it to be useful in real-time sim-\nulations. We are considering to eliminate recom-\nputing of the embeddings for new node insertions\nby caching and using the results of already com-\nputed weight parameters and interpolating proba-\nbility and cluster indexes from adjacent nodes in-\nstead of running compute heavy cluster and proba-"}]}