{"title": "EAGLE: Enhanced Visual Grounding Minimizes Hallucinations in Instructional Multimodal Models", "authors": ["Andr\u00e9s Villa", "Juan Le\u00f3n Alc\u00e1zar", "Motasem Alfarra", "Vladimir Araujo", "Alvaro Soto", "Bernard Ghanem"], "abstract": "Large language models and vision transformers have demonstrated impressive zero-shot capabilities, enabling significant transferability in downstream tasks. The fusion of these models has resulted in multi-modal architectures with enhanced instructional capabilities. Despite incorporating vast image and language pre-training, these multi-modal architectures often generate responses that deviate from the ground truth in the image data. These failure cases are known as hallucinations. Current methods for mitigating hallucinations generally focus on regularizing the language component, improving the fusion module, or ensembling multiple visual encoders to improve visual representation. In this paper, we address the hallucination issue by directly enhancing the capabilities of the visual component. Our approach, named EAGLE, is fully agnostic to the LLM or fusion module and works as a post-pretraining approach that improves the grounding and language alignment of the visual encoder. We show that a straightforward reformulation of the original contrastive pre-training task results in an improved visual encoder that can be incorporated into the instructional multi-modal architecture without additional instructional training. As a result, EAGLE achieves a significant reduction in hallucinations across multiple challenging benchmarks and tasks.", "sections": [{"title": "1. Introduction", "content": "Large-scale pre-trained architectures have significantly advanced the fields of Natural Language Processing (NLP), and Computer Vision. In the language domain, Large Language Models (LLMs) have demonstrated remarkable zero-shot performance in multiple NLP tasks [3, 7, 8, 10, 25, 30, 35]. This success has resulted in a shift in NLP research, moving from task-specific pre-training to task-agnostic representation learning. In the image domain, visual transformers (ViT) [12] have driven significant advances in zero-shot transferability [4, 6, 14, 17, 27, 29, 33, 39], and enabled new state-of-the-art results on downstream tasks.\nBuilding upon the improvements attained by transformer architectures in vision and NLP, Instruction Tuning Vision and Language Models (IT-VLM) have emerged as the ensemble of an LLM and a large-scale ViT fine-tuned with instructional data. IT-VLMs leverage the zero-shot capabilities of both modalities by learning a small fusion sub-network known as adapter or connector [1, 9, 11, 20, 23, 28, 31, 42]. Although the fusion module is far smaller than the other components, it enables IT-VLM models to achieve notable zero-shot performance in multi-modal tasks, such as VQA, Image Captioning, and Image Classification.\nThe open-set natural language responses of IT-VLM allow for the generation of erroneous outputs unrelated to image ground-truth, these false positive errors are known as hallucinations [15, 21, 34, 37]. This phenomenon is often a result of language biases in the decoder module, as the decoding component would generate outputs consistent with the current stream of generated tokens while ignoring the semantic information in the visual modality [37].\nExisting methods for reducing hallucinations in IT-VLMs primarily focus on the language model (LLM) component or the adapter module. These methods include optimizing the adapter module's training strategy [1, 28], improving the adapter module's architecture [9, 20, 23], leveraging extensive data to pre-train robust LLMs [1, 5, 23], and enhancing instructional training data through commercial LLMs [23, 24]. In contrast, we address the hallucination problem from an orthogonal direction by focusing exclusively on the visual component of the IT-VLM.\nPrevious approaches to enhance the visual representation in IT-VLMs [19, 34] introduce additional visual encoders, which prevent scalability and require further fine-tuning of the IT-VLM. We depart from that practice and develop a method that directly enforces visual grounding on the Vision Transformer (ViT) for a subset of common object classes while preserving the global feature descriptor. This refined ViT effectively encodes fine-grained object class distributions and seamlessly integrates into an IT-VLM model without additional fine-tuning or adaptation. Empirical results confirm that our ViT module consistently reduces hallucinations across diverse benchmarks, demonstrating compatibility and effectiveness across IT-VLM architectures. In Figure 1, we provide some qualitative examples that show the baseline (left pink box) and improved response (right orange box) of IT-VLMs. Notably, when our ViT is incorporated, we observe significantly more factual and visually grounded responses.\nIn this paper, we introduce Enhanced Visual Grounding Minimizes Hallucinations in Instructional Multimodal ModEls (EAGLE), a pre-training strategy that significantly reduces the hallucinations in IT-VLMs. Unlike other methods, EAGLE does not target a specific IT-VLM architecture, or a particular LLM. In fact, EAGLE is fully agnostic to the other IT-VLM components. At training time, EAGLE optimizes the ViT component in isolation from the LLM and the fusion module; the improved visual transformer is then plugged into the trained IT-VLM at inference time without any further modification. Without any bells and whistles, our tuned ViTs reduce the hallucinations in common IT-VLMs. Across three well-established hallucination benchmarks and 6 different IT-VLMs models, we show direct improvement by simply replacing their default ViT with our optimized version.\nContributions. Our contributions are two-fold: i) We mitigate the hallucination problem in IT-VLMs by directly enhancing the grounding of the visual encoder, tuning it to capture fine-grained visual details. We obtain 11.2% relative improvement in the MMVP benchmark and 6.3% relative improvement in the MERLIM benchmark ii) EAGLE is a straightforward and effective approach that reduces hallucinations in IT-VLMs without requiring any additional alignment or tuning. Therefore, EAGLE visual modules can be directly integrated into an IT-VLM.\nTo ensure reproducibility and to foster future research, upon acceptance, we will make available all the resources related to this paper, including training code, checkpoints, and official benchmark results."}, {"title": "3. Methodology", "content": "EAGLE improves the local patch embeddings allowing the ViT to encode refined information about objects in the image using a modified contrastive learning framework. We motivate our strategy by comparing the language alignment of the visual features used in a VLM and an IT-VLM. Both models build upon the same visual encoder, however, VLMS perform alignment with the projection of a single visual token (the CLS token), while IT-VLMs drop the CLS token, and proceed to prompt the language stream with features extracted from the embedded tokens.\nAs outlined in Figure 2, EAGLE ensures that, the ViT module encodes a fine-grained visual representation where individual visual tokens encode a better object representation that remains highly aligned with the language component. In addition, our proposal also preserves the global structure of the feature embedding thus retaining the zero-shot and linear probing capabilities of the original transformer. The EAGLE training scheme benefits the visual encoder in two key aspects. First, the visual transformer has almost the same global feature representation and can be readily plugged into any pre-trained IT-VLM. Second, the improved visual representation in the feature sequence will be aware of the object semantics and their spatial alignment in the visual data, thus reducing hallucinations in IT-VLMs."}, {"title": "3.1. Language Feature Alignment", "content": "To generate a language-aligned representation over the feature sequence of the ViT, we rely on image data with instance level segmentations. Formally, given a trained VLM with visual encoder v and language encoder l, we use v to obtain the feature sequence $I$ of image $x$ $I = v(x)$. $I$ has the same sequence length as the tokenized version of $x$ (i.e we drop the CLS token). The architecture of $l$ remains unchanged, thus the embedding of the language remains at"}, {"title": "Loss Function.", "content": "We can not directly use contrastive learning [16] as our training objective, the standard contrastive loss considers all the elements in the batch as negatives except one. This is incompatible with our setup as the training batch might contain multiple masks with the same semantic class (c).\nWe decompose our loss into two terms. $L_{ins}$ operates at instance level, since there is a single ground-truth class for every mask. $L_{ins}$ is a standard contrastive loss minimizing the distance between the visual features of a single image and the ground-truth language embedding of all the classes. For an image x, segmentation mask m, and mask's class c, we obtain a pair of tokens {$\u03a6(v(x), m), l(c)$} and define the instance level loss as:\n$L_{ins} = L_{con}(\u03a6(I,m),l(c_j)),$ (1)\nwhere $L_{con}$ is the contrastive loss. Note that $c_j$ is composed of a prefix and the class (j) of a mask in natural language, $c_j$ follows a prompt template where we prepend \"This is an image of <mask_class>\".\nOur second loss function $L_{ce}$ allows for multiple masks to be associated to a single class. To this end, we leverage a cross-entropy loss over a distance measure ($d_j$) between the modalities, normalized with a sigmoid function.\n$d_j = 1 - \u03c3(\u03a6(I, m) \u2013 l(c_j))$ (2)\n$L_{ce} = \\frac{1}{K} \\sum_{j=1}^K - c_j \\log(d_j) + (1 - c_j) \\log(1 \u2013 d_j)$ (3)\nwhere $\u03c3$ is the sigmoid function, and K is the total number of classes in the training set. Our final loss function integrates both losses $L_{ce}$ and $L_{ins}$ without extra hyperparameters.\n$L = L_{ce} + L_{ins}$ (4)\nEmpirically, we find that it is beneficial to have an approximate uniform distribution of training classes. Therefore, we resample such that masks with uncommon classes are more likely to be sampled than masks belonging to the most common classes.\nTraining Dataset. EAGLE requires image data with instance segmentation labels. We leverage the labeled data in the OpenImages V7 dataset [2], which includes 944,037 images annotated with object segmentations distributed over 350 object classes. Although the data available in the segmented set of OpenImages is three orders of magnitude smaller than the datasets used to pre-train VLM models [29, 33], we show that the training strategy in EAGLE suffices to learn fine-grained visual representations at the patch"}, {"title": "4. Experimental Evaluation", "content": "Before we check the capabilities of ViTs enhanced with EAGLE in the instructional domain of IT-VLMs, we test the effectiveness of our method in the VLM domain. We fine-tune v and l from the pre-trained EVA-01-CLIP-g-14 and OpenAI CLIP-L-14-336 VLMs [29, 33] and verify if their feature sequence is any more effective at recognizing multiple objects in an image. We validate our VLMs in the MS-COCO dataset [22], and check for improvements regarding the number of False Positives predictions. Our testing methodology follows the zero-shot protocol of [33], and we rank the top K predictions. Within this rank, we calculate the ratio of false positives.\nTable 3 summarizes the results of our preliminary evaluation on VLMs. EAGLE ViTs significantly reduce the number of false positive predictions in VLMs, with up to 19.53% less false positive errors.\nImplementation Details. We tune the VLMs EVA01-CLIP-g-14 [33] and OpenAI CLIP-L-14-336 [29]. The ViT in these models corresponds to the visual module in MiniGPT-4 [42], BLIP-2 [20], InstructBLIP [9], and LLaVA-v1.5 [23]. We apply GaLore [40], setting the rank to 128 and the learning rate to 4e-6. The training is performed with two A100 GPUs (80GB) with a batch size of 512. We train until convergence of the $L_m$ loss. EVA01-CLIP-g-14 and OpenAI CLIP-L-14-336 converge at epoch 8, and training requires about 20 hours."}, {"title": "4.1. Visual Representation Quality", "content": "We now proceed with an in-depth empirical evaluation testing the EAGLE-tuned VLMs in the MMVP-VLM benchmark [34]. MMVP-VLM designs nine challenging scenarios where CLIP-based models typically fail. The failure case is designed by pairing visually distinct images that have a highly similar CLIP feature embedding. As a consequence, VLMs fail to align the textual description of those image pairs. MMVP-VLM characterizes nine possible sce-"}, {"title": "4.2. Hallucinations in Instructional Models", "content": "We now proceed with a comprehensive empirical evaluation of state-of-the-art IT-VLMs enhanced with EAGLE on multi-modal instructional benchmarks. We select three representative hallucination benchmarks: POPE [21], MMVP [34], and MERLIM [37]. We run these benchmarks on six different IT-VLMs which are augmented with EAGLE-tuned visual encoders.\nHallucination Benchmarks. POPE [21] assesses hallucination events using yes/no questions about objects in an image. For this benchmark, we focus on POPE's most challenging subset: Adversarial SEEM [43] from A-OKVQA [32], which uses the SEEM segmentation model to detect object segmentations in A-OKVQA images. In POPE, questions with \"yes\" answers are generated based on ground truth objects, while questions with \u201cno\u201d answers are formulated from the top-k most frequent objects in the dataset, which are not present in the image. POPE is a direct tool to evaluate if the visual embeddings generated by EAGLE perform better with queries addressing individual objects instead of the image's global appearance.\nMMVP [34] measures hallucinations using 150 image pairs and 300 corresponding (a) or (b) questions. The image pairs are designed such that they have highly similar CLIP embeddings. MMVP scores favorably only if both questions for each image pair are answered correctly. Following MMVP's methodology, we used a GPT-based scoring approach, replacing the now deprecated \u201cGPT-4-0314\u201d with its closest current successor, \u201cGPT-4o\u201d [26]. MMVP offers a direct way to evaluate whether or not EAGLE improves the model's recognition of fine-grained visual details.\nIn MERLIM [37], we evaluate EAGLE using a subset of original and edited images. We target those images where an entire object category was removed from the edited image, i.e. there exists only 1 instance of the object in the image and was removed. This subset results in 5608 edited images and 3037 corresponding originals. MERLIM incorporates open-ended questions with equivalent meanings, to inquire about all the objects present in the image. This design choice provides a more realistic scenario to assess both EAGLE's ability to capture fine-grained visual information"}, {"title": "Training Strategy Analysis", "content": "We conclude the empirical evaluation of EAGLE and examine the contribution of the key design choices in EAGLE's.\nAs outlined in Table 6, the GaLore component is the most important one for maintaining zero-shot accuracy on ImageNet-1K while also enabling the sequence embeddings to capture detailed visual features, as evidenced by a substantial reduction in the false positive rate on MS-COCO. A second important design choice is to refrain from training the CLS token in favor of focusing exclusively on fine-tuning the feature sequence. Although training the CLS token further reduces the false positive rate of sequence embeddings, it compromises the zero-shot performance and reduces the generalization capability of the new feature representation."}, {"title": "5. Conclusions and Limitations", "content": "This paper introduces EAGLE, a straightforward and scalable approach for reducing hallucinations in IT-VLMs. Unlike traditional methods that rely on additional instructional data, fine-tuning the LLM component, enhancing the adapter module, or ensembling multiple visual encoders to improve visual representation, EAGLE employs a tuning strategy to enhance fine-grained visual grounding directly within the visual encoder. We demonstrate that EAGLE-tuned visual encoders can integrate seamlessly into an IT-VLM without requiring additional alignment or instructional training. EAGLE improves hallucination metrics across three standard benchmarks and six diverse architectures, which is also consistent with the significant improvements in the challenging scenarios for VLM proposed in the MMVP-VLM benchmark.\nLimitations. EAGLE utilizes object segmentations to encourage fine-grained object representation within the sequence embeddings. However, the relative scarcity of image data with instance segmentations poses a challenge. To address this, we employ a parameter-efficient fine-tuning strategy, such as GaLore, which helps prevent model overfitting and preserves generalization capabilities. We encourage future work to explore surrogate sources of fine-grained supervision to further improve model grounding and performance."}]}