{"title": "COMMA: A COMMUNICATIVE MULTIMODAL MULTI-AGENT BENCHMARK", "authors": ["Timothy Ossowski", "Jixuan Chen", "Danyal Maqbool", "Zefan Cai", "Tyler Bradshaw", "Junjie Hu"], "abstract": "The rapid advances of multi-modal agents built on large foundation models have largely overlooked their potential for language-based communication between agents in collaborative tasks. This oversight presents a critical gap in understanding their effectiveness in real-world deployments, particularly when communicating with humans. Existing agentic benchmarks fail to address key aspects of inter-agent communication and collaboration, particularly in scenarios where agents have unequal access to information and must work together to achieve tasks beyond the scope of individual capabilities. To fill this gap, we introduce a novel benchmark designed to evaluate the collaborative performance of multimodal multi-agent systems through language communication. Our benchmark features a variety of scenarios, providing a comprehensive evaluation across four key categories of agentic capability in a communicative collaboration setting. By testing both agent-agent and agent-human collaborations using open-source and closed-source models, our findings reveal surprising weaknesses in state-of-the-art models, including proprietary models like GPT-40. These models struggle to outperform even a simple random agent baseline in agent-agent collaboration and only surpass the random baseline when a human is involved.", "sections": [{"title": "INTRODUCTION", "content": "The field of multimodal agents is experiencing rapid growth (Xu et al., 2024; Xie et al., 2024; Cao et al., 2024), with research efforts expanding at an unprecedented pace. However, amidst this growth, a critical gap in research has emerged: the lack of focus on collaborative work (Gurcan, 2024; Park et al., 2023; Hong et al., 2024; Liu et al., 2024) among multiple multimodal agents. Synergistic operation of such agents is a highly promising but largely unexplored domain. Language agents can collaboratively finish complex tasks such as software development by assuming functional roles such as system designer, function generator, etc (Qian et al., 2023; Du et al., 2024)). Current research on multimodal agents (Xu et al., 2024; Xie et al., 2024; Cao et al., 2024) has predominantly focused on individual agent capabilities, neglecting the potential for inter-agent collaboration. This limitation is further compounded by existing benchmarks such as VisualWebArena (Koh et al., 2024) and MME-RealWorld (Zhang et al., 2024), which fail to adequately assess collaborative performance between agents. As a result, our ability to evaluate and improve multi-agent systems remains constrained, hindering progress in this crucial area.\nSeveral critical questions emerge in the context of multimodal agent collaboration. How can different agents effectively communicate multimodal information through language when they have varying levels of access to information? In scenarios where different agents possess diverse task-specific capabilities, how can they collaborate to accomplish objectives that are beyond the scope of any individual agent? These research settings remain largely uncharted and present significant challenges. Furthermore, the ability of agents to handle incomplete information is of paramount importance, particularly when working with sensitive data (Li et al., 2024) (i.e. Agent application in healthcare where privacy concerns are critical (Tang et al., 2023)). The exploration of these questions is crucial for advancing the field of multimodal agent collaboration. By addressing these challenges, we can expand the applicability of multimodal agents in real-world scenarios (Zhang et al., 2024), particularly those involving sensitive or restricted information.\nMotivated by these aforementioned issues, we propose a novel benchmark for evaluating collaborative multi-modal multi-agent frameworks to address critical gaps in current approaches (see Figure 1). Our evaluation setting simulates a scenario where an in-house agent with direct access to sensitive data (i.e., the AI solver) collaborates with external expert agents (i.e., the AI expert) to analyze information without compromising privacy. This evaluation setting could revolutionize how we handle and extract insights from sensitive datasets across various domains.\nWe assess multi-modal multi-agent systems using a series of carefully designed collaborative puzzle games. These scenarios typically involve two-player setups where agents have access to different, complementary information. (i.e., in a bomb defusal game, one agent possesses details about the bomb, while the other has access to a disarming manual). By employing such diverse and interactive scenarios, we aim to provide a thorough assessment of multi-modal multi-agent performance.\nOur benchmark includes 10 distinct, easily customizable puzzles with thousands of unique solutions. We tested two different settings (AI-AI and AI-Human) and evaluated several popular multimodal models, including closed-source models (GPT-4V, GPT-4O, and GPT-401) and open-source models (Qwen-VL (Bai et al., 2023), and InternVL(Chen et al., 2024)). Surprisingly, the GPT series does not outperform even a simple random baseline in the AI-AI setting, highlighting a potential growth area for future model development. Our contributions are as follows:\n\u2022 We propose an evaluation framework called COMMA, a multimodal agent benchmark focusing on language communication between multiple agents (Section 3).\n\u2022 Using COMMA, we carefully record conversations and performance metrics between state-of-the-art multimodal models such as QWenVL, InternVL, GPT-40, GPT-401, etc (Section 4)."}, {"title": "RELATED WORK", "content": "Multi-agent Frameworks:\nThere are many emergent agent collaboration works (Gurcan, 2024; Park et al., 2023; Hong et al., 2024; Liu et al., 2024; Ghafarollahi & Buehler, 2024; Li et al., 2023; Wu et al., 2023) among multiple language agents. Multi-agent systems arise mainly in two different scenarios: (1) role-playing different task executors (e.g., software development requiring different roles of agents, such as program manager, software architect, programmer Du et al. (2024); Qian et al. (2023); Hong et al. (2024), scientific discovery simulation Wu et al. (2023), and social simulation Park et al. (2023); Gurcan (2024)); (2) communicating between agents with different pieces of information Wu et al. (2023); Li et al. (2023) (e.g., consulting experts without sharing some sensitive or confidential data. In our case, the AI solver has some private multimodal data, and the AI expert has domain-specific knowledge or instructions).\nInstruction-based Agent Benchmarks: Instruction-based agent benchmarks evaluate an agent's capability of following a human instructions to finish a task (e.g., navigating on a website, interacting with an operating system Xu et al. (2024); Xie et al. (2024); Cao et al., 2024)). However, our benchmark focuses more on a communication-based evaluation where two clients engage in multi-turn conversations to solve a task collaboratively."}, {"title": "BENCHMARK", "content": "Our benchmark is inspired by the cooperative gameplay scenario in Keep Talking and Nobody Explodes Games (2015). In this game, two players work together to defuse a bomb under time pressure. One player, the defuser, can see the bomb but lacks the instructions to disarm it. The other player, the expert, has access to the bomb's manual but cannot see the bomb itself. The players must rely on effective communication to exchange information, navigate challenges, and defuse the bomb.\nWe generalize this dynamic for our benchmark, shifting the focus to solving vision-language puzzles in a communication-based agent framework. As multimodal agent systems gain momentum, our goal is to create a benchmark that rigorously evaluates their reasoning, communication, and collaborative abilities. The design of our benchmark revolves around the following core principles:\nLanguage communication: A critical aspect of our benchmark is evaluating natural language communication between agents. Similar to how players in the original game exchange information verbally, agents in our framework must use language to share observations, clarify ambiguities, and reason about tasks. In order for the agents to succeed, they must display clarity, efficiency, and depth of communication, making it an essential factor in task completion.\nMulti-agent collaboration: In our benchmark, agents must work together, much like the two-player dynamic of Keep Talking and Nobody Explodes. The collaboration element ensures that tasks require mutual dependency, where each agent contributes unique information or capabilities that are critical to solving the puzzle. This principle highlights how well agents can cooperate and leverage each other's strengths.\nMultimodality: Our benchmark emphasizes the integration of multiple sensory inputs and outputs, such as vision, language, and audio. The puzzles involve visual elements that agents must perceive, describe, and interpret, alongside linguistic interactions. This principle assesses an agent's ability to handle and synthesize multimodal information, a skill crucial to real-world applications."}, {"title": "CATEGORIES OF AGENT CAPABILITY", "content": "We benchmark agents working under different roles to solve various tasks in multiple settings, each requiring different capabilities. Specifically, the Solver agent must demonstrate strong instruction-"}, {"title": "TASKS", "content": "We create 10 puzzles across 4 different categories briefly summarized below. A more comprehensive description along with example images and instruction manuals can be found in Appendix A.\n\u2022 ButtonPuzzle (RT): The solver must hold a colored button for a specific number of seconds based on the button's color, the strip's color when pressed, and the time remaining on a timer.\n\u2022 ColorPuzzle (MR, MSR): The solver presses squares of the least common color in a 4x4 grid, then follows a sequence based on a table, aiming to turn all squares white.\n\u2022 KeypadPuzzle (MG, MSR): The solver must describe the symbol of each button in a 2x2 grid. The expert must then identify a column in the manual containing these four unique symbols and tell the solver to press the symbols in the correct order from top to bottom.\n\u2022 LedPuzzle (MR, MSR): The solver presses a button if the value of its letter, when multiplied by a stage's LED color multiplier and taken modulo 26, matches the value of the letter diagonally opposite it. At each stage, the letters on the buttons change.\n\u2022 MazePuzzle (MG, MSR): The solver navigates a mouse through a maze to a colored sphere, pressing the correct button to disarm the module based on the maze's layout.\n\u2022 MemoryPuzzle (MR, MSR): The solver presses buttons according to specific positional and label-based rules over five stages, with incorrect presses resetting progress.\n\u2022 PasswordPuzzle (MG, MSR): The solver cycles through letters to form a valid word from a predefined list, submitting the correct word to complete the puzzle.\n\u2022 DogPuzzle (RT): The solver is presented with an image containing 0-4 dogs. Based on the number of dogs in the image, the solver must press the submit button when the last digit of the timer matches the number of dogs in the image.\n\u2022 WhoPuzzle (MG): The solver must tell the value on a display to the expert, who will identify a button label. The solver must then tell this label to the expert, and then press the correct button based on a detailed list of instructions.\n\u2022 WirePuzzle (MG): The solver must cut one of the wires on the display. There are 3 to 6 colored wires, and the correct wire to cut changes depending on the number and order of colors."}, {"title": "EXPERIMENTAL SETUP", "content": "In this section, we describe the experimental settings of our multi-agent interaction environment where two distinct agents, namely the Solver agent and the Expert agent, engage in iterative dialogue sessions. The primary aim of this setup is to assess the collaborative problem-solving capabilities between different agents or multimodal large language models (MLLMs). During our experiments, we limit the number of conversation turns to 20, allowing for a unified and systematic assessment of interactions. We use greedy decoding when available to maintain consistent agent output across runs and run inference on a single NVIDIA A100 GPU with 80GB RAM. We parse the solver's chosen actions at each conversation turn using exact string matching, and use PyAutoGUI (Sweigart, 2023) to directly perform the action on the interface if the solver outputs a valid action. Our exact prompts for both the solver and expert agent can be found in Appendix D."}, {"title": "EVALUATION METRICS", "content": "We meticulously recorded several key performance metrics through multiple iterations of the experiments described below:\n\u2022 Success Rate (SR): The solver agent is assigned a 0 or 1 value for each puzzle depending on if it completed it. These values are averaged across all puzzles to obtain the success rate.\n\u2022 Partial Success Rate (PSR): Because our benchmark includes puzzles with multi-step reasoning, some puzzles can have a more precise success rate evaluation. For these multi-step puzzles, we assign the solver a number between 0 and 100 to indicate its progress towards the solution, and average this number across puzzles to obtain partial success rate. For single-step puzzles, partial success rate is identical to success rate.\n\u2022 Average Mistakes (AM): After an action is chosen by the solver, the environment checks if the action was a mistake. We tally up the mistakes made during each puzzle and take a global average across puzzles to obtain average mistakes.\n\u2022 Average Conversation Length (ACL): We count the number of conversation turns the Solver took to arrive at the solution, or default to the maximum of 20 if the solver failed. This count is averaged across all puzzles to get Average Conversation Length."}, {"title": "MODELS", "content": "Open-Source Models\n\u2022 Human: We conduct several experiments in which a human plays as the solver or expert to provide a strong baseline. As hiring participants was prohibitively expensive and time consuming, we role played as agents ourselves across 30 sampled puzzles as a preliminary study, and leave further human participation to future work.\n\u2022 InternVL (Chen et al., 2024): A vision-language model by Shanghai AI Lab, designed for cross-modal tasks like visual question answering and image-text retrieval. We evaluate both the 26b and 8b variants of the model.\n\u2022 QwenVL (Bai et al., 2023): We use version 2 of QWenVL (QWen-VL2), offering enhanced pretraining for improved performance on vision-language tasks. We use the 2b and 7b variants.\nClosed-Source Models\n\u2022 GPT-4V: A version of OpenAI's GPT-4, GPT-4V incorporates visual processing, enabling it to interpret both text and images.\n\u2022 GPT-40: An optimized, faster, and more cost-effective variant of GPT-4, used for applications requiring speed and efficiency.\n\u2022 GPT-401: The most recent version of OpenAI's GPT series models, which claims to have improved reasoning capability via internal chain of thought."}, {"title": "RESULTS AND ANALYSIS", "content": "Table 1 illustrates the performances of all combinations of solver and expert pairs we evaluated. We evaluate some combinations of different open-source models because they are free, and leave pairings of separate closed-source models for future work. Intrerestingly, increasing model size does not always improve performance, as QWenVL 2b (38% PSR) outperforms QWenVL 7b (32% PSR). We observe that the random agent performs well on simple puzzles which only require one correct action (e.g. 100% on WirePuzzle, 100% on WhoPuzzle) and even outperforms all of the AI-AI setting performances. However, a random agent struggles with puzzles which require several correct actions in a row, as evidenced by its 0 percent success rate and low partial success rate in Memory, Maze, Password, Keypad, and Color puzzles.\nOur results show that GPT-401 is the most powerful agent to use, although we only run a single iteration of each puzzle with a Human-AI setting due to rate limit issues. It makes the most progress in the color puzzle, achieving a partial success rate of 44%, followed by GPT-40 at 35%. In this puzzle, we observed that GPT-401's \u201creasoning process\u201d made correct connections with different parts of the puzzle's manual, though sometimes it made incorrect conclusions at the end of the process. GPT-4V is the second best agent, with an average partial score of 53% in the AI-AI setting, although the performance is still far worse than human performance. For example, switching from a gpt-4o solver to a human solver increases overall partial success rate from 53% to 74%."}, {"title": "QUALITATIVE ANALYSIS ON MODEL FAILURES", "content": "In this section, we highlight key takeaways and common failure modes displayed by the agents during their conversations. We manually classify errors across 50 conversations into the following categories, and report the distribution of these errors across the benchmark in Figure 3 and Figure 4.\n\u2022 Roleplay: The expert thinks it is the solver or vice versa. Figure 2 illustrates how the expert can misunderstand its role assignment, leading to miscommunication and failure to solve the puzzle.\n\u2022 Misinterpretation: The solver misunderstands the current puzzle state/signal, resulting in failure. For instance, Figure 2 showcases the solver misinterpreting the number of dogs in the image, leading to incorrect instructions from the expert.\n\u2022 Repetition Loop: The solver sometimes repeats its past incorrect actions, even if is in a situation it has encountered before. We classify any repeated incorrect state, action pair into this category.\n\u2022 Miscommunication: As shown in Figure 2, the agent occasionally does not listen to the expert's instructions, attempting to solve the puzzle on its own as if it were the expert. We also observed some open source models such as LLaVA don't have instruction following capability for this task"}, {"title": "FINE-GRAINED ANALYSIS", "content": "Multimodal Agents Struggle to Learn from Past Mistakes An important skill for humans is to learn from past mistakes to adapt to new situations. Here we analyze if agents can display a similar capability and recover when exploring a bad trajectory when solving a puzzle. Figure 5 plots the number of allowed conversation turns to solve a puzzle, along with the overall success and mistakes rate of several multimodal agents. We note the following observations. First, incorporating a human in the pipline in the form of a human solver significantly improves overall success rate, being the only agent to achieve over random baseline performance at the 20-turn conversation mark. This is also supported by the mistakes plot, in which the human solver setting generally displays lower mistakes as the conversation progresses compared to the full AI setting. In fact, the human solver, gpt-40 expert setting shows zero mistakes over the course of most conversations, with the main reason for failure being the conversation limit. Second, humans appear to have greater ability to recover, as indicated by the faster increase in their success rate as conversation length increases, as well as the fact that they make less mistakes over time in the mistakes chart.\nPerformance Based on Capability Here we group the model performance based on the category tested: Memory (MR), Grounding (MG), Reasoning (MSR), and Reaction (RT).\nMultimodal Agents Excel at Simple Realtime Tasks Figure 6 gives a more nuanced look at how well multimodal agents are equipped to deal with puzzles of different nature. The agents performed"}, {"title": "CONCLUSION", "content": "In this paper, we address a critical gap in the field of multi-modal agents by introducing a novel benchmark specifically designed to evaluate communication in a multi-modal, multi-agent system. While substantial progress has been made in developing individual multi-modal agents, collaborative frameworks remain under-explored, particularly in scenarios requiring secure communication and the handling of sensitive data. Our benchmark aims to bridge this gap by simulating real-world conditions where agents possess complementary information and must work together to achieve complex goals. We comprehensively evaluate metrics such as partial success rate, mistake rate, and document common failure modes for both AI-AI and AI-Human interactions. Our findings show that multimodal agents struggle to communicate with each other, often falling short of even a simple random baseline due to poor communication and frequently repeated bad actions. These findings emphasize the need for deeper investigation into enhancing inter-agent collaboration. We hope the insights from our benchmark lay the foundation for future research on multi-modal agent collaboration and inspires the community to explore innovative approaches to improve multimodal agent capabilities this emerging field of communicative multimodal systems."}, {"title": "LIMITATIONS", "content": "While we aim to construct a holistic framework for multimodal agent communication, our experiments may not represent all possible scenarios in our puzzles. We conduct a preliminary study by sampling puzzle configurations and conversations between agents, and we leave more comprehensive evaluation and expansion of puzzle categories to future work. Additionally, there will inevitably be a simulation-to-reality gap from our benchmark to real-world situations, thus a high score on our benchmark may not perfectly generalize to real-world communication scenarios. Lastly, we acknowledge that there is inherent risk to using multimodal agents when handling private data. Given that LLMs have been shown to be prone to jailbreaking, it is critical to take additional safety measures before deploying an agent in practice, even if it achieves a high score on our benchmark."}, {"title": "ADDITIONAL STATISTICS", "content": "We report additional metrics recorded during evaluation such as Average Mistakes (Table 2) and Conversation Length (Table 3)"}]}