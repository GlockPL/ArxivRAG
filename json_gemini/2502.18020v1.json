{"title": "AfroXLMR-Comet: Multilingual Knowledge Distillation with Attention Matching for Low-Resource languages", "authors": ["Joshua Sakthivel Raju", "Sanjay S", "Jaskaran Singh Walia", "Srinivas Raghav", "Vukosi Marivate"], "abstract": "Language model compression through knowledge distillation has emerged as a promising approach for deploying large language models in resource-constrained environments. However, existing methods often struggle to maintain performance when distilling multilingual models, especially for low-resource languages. In this paper, we present a novel hybrid distillation approach that combines traditional knowledge distillation with a simplified attention matching mechanism, specifically designed for multilingual contexts. Our method introduces an extremely compact student model architecture, significantly smaller than conventional multilingual models. We evaluate our approach on five African languages: Kinyarwanda, Swahili, Hausa, Igbo, and Yoruba. The distilled student model-AfroXLMR-Comet-successfully captures both the output distribution and internal attention patterns of a larger teacher model (AfroXLMR-Large) while reducing the model size by over 85%. Experimental results demonstrate that our hybrid approach achieves competitive performance compared to the teacher model, maintaining an accuracy within 85% of the original model's performance while requiring substantially fewer computational resources. Our work provides a practical framework for deploying efficient multilingual models in resource-constrained environments, particularly benefiting applications involving African languages.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have become a pillar of modern Natural Language Processing (NLP), achieving state-of-the-art results across various tasks (Devlin, 2018; Liu et al., 2019; Lewis, 2019). Their performance only continues to improve with the expansion of computational power, the availability of vast datasets, and the scaling of model architectures (Kaplan et al., 2020; Brown et al., 2020; Touvron et al., 2023). But, despite their success, a significant portion of the world's languages, particularly low-resource languages (LRLs), remain underrepresented in NLP research. These languages, numbering in the thousands, lack the necessary linguistic resources and data required for traditional statistical methods to be effectively applied (Singh, 2008; Cieri et al., 2016; Tsvetkov, 2017). The challenges associated with LRLs are substantial and multifaceted. These languages often suffer from a lack of computational tools, limited digital presence, and insufficient educational infrastructure, which hinders their incorporation into modern NLP systems. While advancements in NLP for LRLs present immense potential, especially in regions such as Africa and India, where over 2.5 billion people speak these languages, the barriers are high. Addressing these challenges not only promises economic and cultural benefits but also plays a vital role in preserving linguistic heritage and improving societal outcomes, such as aiding in emergency response and enhancing educational and cultural exchanges (Tsvetkov, 2017)."}, {"title": "1.1 Knowledge Distillation", "content": "Knowledge distillation (KD) refers to the process where a smaller model learns from a larger one, transferring knowledge from a teacher model to a student model to enhance the student's performance. The central concept is that the student model emulates the teacher model, using the teacher's insights to achieve competitive or even superior results. A typical KD framework includes three main components: knowledge, a distillation algorithm, and the teacher-student model architecture. The key challenge is to effectively convey knowledge from the large teacher model to the smaller student model while ensuring the student retains or improves its performance (Bucilu\u0103 et al., 2006; Ba and Caruana, 2014; Hinton, 2015; Urban et al., 2017). Response-based knowledge distillation focuses on aligning the final output (logits) of the student model with the teacher's logits, typically using soft targets and temperature scaling to capture the \"dark knowledge\" from the teacher. This approach has been widely applied in tasks like image classification, but it is limited by only utilizing the output of the last layer, potentially missing intermediate-level supervision that could benefit deeper networks (Hinton, 2015; Ba and Caruana, 2014). To address this, feature-based knowledge distillation extends the method by transferring feature maps from intermediate layers of the teacher model to the student model, enhancing representation learning. Techniques like attention maps and activation boundaries are used to match features between layers, but challenges persist in selecting the right layers and handling size differences between layers (Romero et al., 2015; Zagoruyko and Komodakis, 2017). In this work, we propose a novel hybrid distillation framework that integrates knowledge distillation and attention matching to improve multilingual model compression. Existing approaches often focus on either response-based distillation, which transfers knowledge through soft output distributions, or feature-based distillation, which aligns internal representations. Our method integrates both, enabling a more comprehensive transfer of knowledge from teacher to student models. Additionally, we introduce a highly compact multilingual student model with a significantly smaller hidden dimension, optimized for low-resource African languages. Our contributions can be summarized as follows:\n\u2022 Hybrid Distillation Framework: We propose a novel distillation approach that integrates knowledge distillation with attention matching, enabling the student model to learn both the output distribution and internal attention patterns of the teacher model.\n\u2022 Compact Multilingual Model: We introduce an extremely compact multilingual architecture with a hidden dimension of 256, significantly smaller than existing models (typically 768 or higher), while maintaining reasonable performance.\n\u2022 Simplified Attention Matching: Our mean-pooled attention matching mechanism effectively transfers knowledge between teacher and student models while reducing computational overhead compared to complex relation-based methods.\n\u2022 Evaluation on African Languages: We conduct a systematic evaluation of our hybrid distillation approach on five African languages-Kinyarwanda (rw), Swahili (sw), Hausa (ha), Igbo (ig), and Yoruba (yo)-addressing gaps in model compression research for low-resource languages.\n\u2022 Empirical Analysis: We analyze trade-offs between model size, computational efficiency, and performance, demonstrating that effective knowledge transfer is possible despite substantial architectural differences between teacher and student models."}, {"title": "2 Related Work", "content": "Alabi et al. (2022) proposed AfroXLMR, a multilingual pre-trained language model specifically adapted for African languages through multilingual adaptive fine-tuning (MAFT). Their approach has shown to enhance the performance of pre-trained models, such as XLM-R and AfriBERTa, on a variety of tasks for African languages by fine-tuning on monolingual texts from 17 high-resource African languages, alongside three widely spoken high-resource languages in Africa. One of the key innovations is the reduction of model size by removing tokens for non-African writing scripts from the embedding layer, which decreases the model size by approximately 50%. The resulting model not only performs competitively with language-adaptive fine-tuning (LAFT) on individual languages but also improves the cross-lingual transfer (Thangaraj et al., 2024) abilities of models like XLM-R, while requiring significantly less disk space. This makes the AfroXLMR approach more efficient for practical deployment on tasks such as Named Entity Recognition (NER), topic classification, and sentiment analysis, especially in low-resource African languages. In MiniLM (Wang et al., 2020), deep self-attention distillation is used to compress pre-trained Transformers by transferring knowledge from the teacher model to the student model. MiniLMv2 (Wang et al., 2021) builds on this by introducing multi-head self-attention relation distillation for task-agnostic compression, where attention relations are defined as the scaled dot-product between query, key, and value pairs within the self-attention module. Unlike previous methods, MiniLMv2 allows the student model to have a different number of attention heads than the teacher, which removes the constraint of matching attention head numbers. By concatenating queries from multiple attention heads and splitting them to match the desired number of relation heads, MiniLMv2 enables more fine-grained attention knowledge transfer, leading to a deeper mimicry of the teacher's attention mechanisms. Moreover, MiniLMv2 examines layer selection beyond just the last layer, and finds that transferring knowledge from an upper-middle layer results in improved performance, especially for large models. Experimental results demonstrate that MiniLMv2, applied to both monolingual and multilingual pre-trained models, outperforms state-of-the-art methods, achieving better performance with fewer training examples and faster execution times."}, {"title": "3 Methodology", "content": ""}, {"title": "3.1 Model Architecture", "content": ""}, {"title": "3.1.1 Teacher Model", "content": "For our teacher model, we utilize the Afro-XLM-ROBERTa-Large (AfroXLMR-Large) architecture, a multilingual language model specifically pre-trained on on 17 African languages (Afrikaans, Amharic, Hausa, Igbo, Malagasy, Chichewa, Oromo, Naija, Kinyarwanda, Kirundi, Shona, Somali, Sesotho, Swahili, isiXhosa, Yoruba, and isiZulu) covering the major African language families and 3 high-resource languages (Arabic, French, and English). This model follows the standard transformer architecture with a hidden size of 1024, 16 attention heads, and 24 transformer layers (refer Table 1). The model was pre-trained on a diverse corpus of African language texts, making it particularly suitable for our target languages."}, {"title": "3.1.2 Student Model", "content": "Our student model architecture is derived from XLM-ROBERTa-COMET-small, leveraging the mMiniLM-L12xH384 XLM-R model proposed by Wang et al. (2021). However, we introduce significant modifications to further reduce the model size while maintaining performance (refer to Table 1). The key architectural changes include:\n1. Hidden Size Reduction: We reduce the hidden size from the original 384 to 256 dimensions, to further decreasing the model's parameter count.\n2. Intermediate Layer: To maintain architectural balance with the reduced hidden size, we adjust the intermediate layer size to 1024, compared to the original 1536 dimensions.\n3. Attention Head: We configure the model with 8 attention heads, ensuring that each head operates on 32-dimensional key, query, and value vectors (256/8 = 32), maintaining the standard practice of having head dimensions that are factors of the hidden size. This architectural configuration results in a substantially more compact model while preserving the essential multi-head attention mechanism necessary for capturing complex linguistic patterns. The reduced dimensionality is compensated for through our attention matching mechanism, which enables the student model to learn effective representations despite its smaller size."}, {"title": "3.2 Dataset Preparation", "content": "In this work, we focus on a multilingual dataset-the MADLAD-400 dataset (Kudugunta et al., 2023), a manually audited, general domain 3T token monolingual dataset based on CommonCrawl, spanning 419 languages. We employ a multilingual subset of the dataset that represents African languages, specifically Kinyarwanda (rw), Swahili (sw), Hausa (ha), Igbo (ig), and Yoruba (yo). The dataset is then split into 80% for training and 20% for evaluation. All sequences are tokenized to a maximum length of 128 tokens."}, {"title": "3.3 Attention Matching Mechanism", "content": "Our work implements a simplified attention matching approach that focuses on the aggregate attention patterns rather than individual head-level interactions. While previous work, such as MiniLMv2 (Wang et al., 2021), addresses head count mismatches through relation heads and KL-divergence, we propose a more streamlined approach that captures the overall attention behavior. We extract the attention matrices from the final layer of both teacher and student models. To manage the dimensional differences between the models, we first compute the mean across attention heads, producing a single attention map per sequence. This averaging operation reduces the attention tensors from shape [batch_size, num_heads, sequence_length, sequence_length] to [batch_size, sequence_length, sequence_length], effectively capturing the aggregate attention patterns across all heads. The attention matrices are then flattened to vectors of shape [batch_size, sequence_length x sequence_length]. To address the remaining dimensional differences between teacher and student models, we employ a learnable linear projection layer that maps the flattened student attention patterns to the teacher's dimensional space. The projection operation can be formalized as:\n$A_{student\\_proj} = W \\cdot A_{student\\_flat}$ (1)\nwhere:\n\u2022 Astudent_flat is the flattened student attention matrix.\n\u2022 W is the learned projection matrix.\n\u2022 Astudent_proj is the projected student attention pattern. The training objective for attention matching is computed using Mean Squared Error (MSE) loss between the projected student attention and the flattened teacher attention:\n$L_{attention} = MSE(A_{student\\_proj}, A_{teacher\\_flat})$ (2)\nwhere:\n\u2022 Lattention is the attention loss function.\n\u2022 MSE is the Mean Squared Error function.\n\u2022 Astudent_proj is the projected student attention pattern.\n\u2022 Ateacher_flat is the flattened teacher attention matrix. This approach, while simpler than the relation-based methods, proves effective in practice. By focusing on the aggregate attention patterns rather than individual head interactions, we reduce computational complexity while still capturing the essential aspects of the teacher's attention mechanism. The learned projection layer allows the student to adapt its attention patterns to match the teacher's higher-dimensional space, facilitating effective knowledge transfer despite the architectural differences between the models. Our final loss function combines this attention matching loss with traditional knowledge distillation:\n$L_{total} = \\alpha \\cdot L_{distill} + (1 - \\alpha) \\cdot L_{attention}$ (3)\nwhere:\n\u2022 \u03b1 = 0.5 balances between the distillation and attention matching objectives."}, {"title": "3.4 Training Process", "content": "To facilitate effective knowledge transfer between these architecturally different models, we employ a two-stage training process (refer Figure 1). First, we initialize the student model with the reduced configuration parameters. Then, we apply our combined distillation approach, using both soft target probabilities and attention matching to transfer knowledge from the teacher to the student model. Instead of task-specific distillation, we follow a task-agnostic knowledge distillation framework, where the student model is trained to generalize knowledge from the teacher across multiple tasks, rather than being optimized for a single downstream task. By distilling general linguistic representations instead of task-specific knowledge, our model remains adaptable to various NLP applications without requiring retraining for different use cases. For soft target distillation, we use a temperature parameter T=2.0 to create smoothed probability distributions from both models' logits, then measure their difference using Kullback-Leibler (KL) divergence. This smoothing reveals the teacher's underlying knowledge through relative probabilities across all classes, while KL divergence effectively captures how the student's probability distribution diverges from the teacher's desired distribution. The soft probability for class i is calculated as:\n$P_i = \\frac{exp(z_i/T)}{\\sum_{j=1}^N exp(z_j/T)}$ (4)\nwhere:\n\u2022 zi is the logit (raw score) for class i\n\u2022 T is the temperature parameter (typically T>1)\n\u2022 N is the total number of classes\n\u2022 pi is the resulting soft probability for class i The training process is optimized using AdamW optimizer with a learning rate of 5e-5. We implement early stopping with a patience of 3 epochs"}, {"title": "4 Experiments", "content": "We evaluate our distilled student model-AfroXLMR-Comet-using the AfriSenti dataset from the AfriSenti-SemEval(Muhammad et al., 2023a,b, 2022; Yimam et al., 2020) Shared Task 12, which covers 17 African languages: Hausa, Yoruba, Igbo, Nigerian Pidgin, Amharic, Tigrinya, Oromo, Swahili, Algerian Arabic, Kinyarwanda, Twi, Mozambican Portuguese, Moroccan Arabic, Fongbe, Lingala, Kamba, and Luganda. For our comparative analysis with AfroXLMR models, we focus on five major languages: Kinyarwanda, Swahili, Hausa, Igbo, and Yoruba. The dataset consists of manually annotated tweets categorized into positive, negative, and neutral sentiments (refer Table 4). We maintain the original data splits provided by the task organizers for consistent comparison with benchmark models (refer Table 5). Each model was fine-tuned for 5 epochs at a learning rate of 2e-5 on each language dataset provided by the task and then retrained for Task A. Our benchmark results are presented in Table 6."}, {"title": "5 Results and Discussion", "content": "The distilled student model-AfroXLMR-Comet-achieved a substantial parameter reduction of 87.69%, compressing the teacher model's 559,890,432 parameters down to 68,937,216. This reduction in size results in a trade-off in performance, as the student model achieves an average F1 score of 65.28% compared to AfroXLMR-Large's 73.22% on the AfriSenti-SemEval sentiment classification benchmark. Despite this drop in accuracy, the student model significantly reduces inference latency and memory footprint, making it particularly suitable for deployment in resource-constrained environments. Additionally, the student model's compact size of 262.99 MB (compared to 2.09 GB for Afro-XLMR-Large) highlights its efficiency in handling low-resource languages. Notably, the distilled student model's performance is very close to Afro-XLMR-Mini, which achieves an F1 score of 66.94% but is almost twice the size at 448.79 MB, further showcasing that the student model balances efficiency and performance, achieving comparable results with a significantly smaller memory footprint. The simplification of attention mechanisms further emphasizes the practicality of this approach, ensuring efficient knowledge transfer while maintaining competitive performance across multiple African languages."}, {"title": "6 Conclusion", "content": "In this work, we introduced a hybrid distillation framework for the efficient compression of pre-trained multilingual transformer models, specifically focusing on African languages. By integrating task-agnostic knowledge distillation, attention matching, and adaptive learning mechanisms, we successfully distilled AfroXLMR-Large into a lightweight yet effective student model. Our approach achieves a significant reduction in computational and memory requirements while maintaining competitive performance, demonstrating a viable trade-off between efficiency and accuracy. Looking ahead, future work will explore extending this methodology to other language families and investigating domain-specific fine-tuning to further enhance the adaptability of compressed multilingual models. Additionally, we aim to refine attention transfer mechanisms to improve performance retention in extreme compression settings. Our findings highlight the transformative potential of efficient model compression in democratizing access to NLP for low-resource languages, ensuring that cutting-edge language technologies become more accessible to linguistic communities worldwide."}, {"title": "7 Limitations", "content": "While our framework shows promising results, several important limitations should be acknowledged:\n\u2022 Projection Layer Computation: The projection layer matching between teacher and student attention matrices introduces additional computational overhead. For a sequence length of 128, the projection layer requires maintaining and computing gradients for a large parameter matrix of size (1282) x (1282), which can be memory-intensive during training. This limitation becomes more pronounced when dealing with longer sequences or larger batch sizes, leading to memory allocation challenges during both fine-tuning and inference. This limitation, while less severe than head-level matching approaches, still impacts the model's deployability in resource-constrained environments.\n\u2022 Data and Resource Constraints: A funda-mental challenge in our work is the scarcity of high-quality dataset samples for many low-resource languages, which inherently limits the extent of multilingual generalization. Due to computational constraints, our experimental validation was limited to five languages.\n\u2022 Temperature Sensitivity: The distillation process relies heavily on the temperature parameter T in the softmax computations. While we use a fixed temperature of 2.0, the optimal temperature may vary across different languages and tasks. Our approach lacks adaptive temperature scaling that could potentially improve knowledge transfer for specific language combinations."}]}