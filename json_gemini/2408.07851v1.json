{"title": "SER Evals: In-domain and Out-of-domain Benchmarking for Speech Emotion Recognition", "authors": ["Mohamed Osman", "Daniel Z. Kaplan", "Tamer Nadeem"], "abstract": "Speech emotion recognition (SER) has made significant strides with the advent of powerful self-supervised learning (SSL) models. However, the generalization of these models to diverse languages and emotional expressions remains a challenge. We propose a large-scale benchmark to evaluate the robustness and adaptability of state-of-the-art SER models in both in-domain and out-of-domain settings. Our benchmark includes a diverse set of multilingual datasets, focusing on less commonly used corpora to assess generalization to new data. We employ logit adjustment to account for varying class distributions and establish a single dataset cluster for systematic evaluation. Surprisingly, we find that the Whisper model, primarily designed for automatic speech recognition, outperforms dedicated SSL models in cross-lingual SER. Our results highlight the need for more robust and generalizable SER models, and our benchmark serves as a valuable resource to drive future research in this direction.", "sections": [{"title": "1. Introduction", "content": "Speech emotion recognition has garnered significant attention due to its potential to enable more natural and empathetic human-computer interaction. Recent advancements in self-supervised learning have led to powerful speech representation models like wav2vec2 [1], HuBERT [2], and WavLM [3], which have shown impressive performance on various speech processing tasks. However, the generalization of these models to diverse languages and emotional expressions remains a critical challenge [4].\nExisting SER benchmarks often focus on a limited set of well-studied datasets, which may not accurately reflect real-world scenarios [5]. Moreover, the emphasis on in-domain evaluation fails to capture the crucial aspect of out-of-domain generalization, which is essential for deploying SER systems in practical applications. For our paper's purpose, we define in-domain as evaluating on the same data distribution seen in training, and out-of-domain as evaluating on a different data distribution. This can manifest as different speakers, tones, decision boundaries, etc. To address these limitations, we propose a large-scale benchmark that evaluates SER models on a diverse collection of multilingual datasets, emphasizing zero-shot performance.\nOur benchmark focuses on less commonly used datasets to mitigate overfitting and encourage the development of more robust and adaptable models. We employ state-of-the-art speech representation models, including Whisper [6], an automatic speech recognition model, and CLAP [7, 8], a contrastive learning model, to analyze their performance in cross-lingual SER. Interestingly, our results show that Whisper consistently outperforms dedicated SSL models across most datasets, challenging the common belief that ASR models are suboptimal for SER due to their focus on phoneme recognition.\nThe main contributions of this work are as follows:\n\u2022 We introduce a large-scale benchmark for evaluating the robustness and generalization of SER models across diverse languages and emotional expressions.\n\u2022 We curate a collection of multilingual datasets and establish targetted subsets for systematic in-domain and out-of-domain evaluation.\n\u2022 We employ logit adjustment[9] to account for varying class distributions and ensure fair comparisons across datasets.\n\u2022 We conduct extensive experiments with state-of-the-art speech representation models and provide insights into their cross-lingual SER performance.\n\u2022 We open source our entire code base, our full un-reduced results and training logs, as well as all implementation details at the following url: https://github.com/\nspaghettiSystems/serval."}, {"title": "2. Related Work", "content": "Self-supervised learning has revolutionized speech representation learning, enabling models to capture rich acoustic features without relying on labeled data. Models like wav2vec 2.0 [1], HuBERT [2], and WavLM [3] have achieved state-of-the-art performance on various speech processing tasks, including speech recognition, speaker identification, and emotion recognition [29].\nCross-lingual SER has gained attention as a means to develop models that can generalize across languages. Several studies have explored the use of SSL models for cross-lingual SER [30, 4]. However, these works often focus on a limited set of languages and datasets, making it difficult to assess the true generalization capabilities of the models.\nExisting well-known SER benchmarks, such as IEMOCAP [31] and MSP-Podcast [32], have played a crucial role in advancing the field. However, these benchmarks often emphasize in-domain evaluation and may not adequately capture the challenges of real-world deployment [5]. Our work aims to address these limitations by introducing a large-scale benchmark that focuses on out-of-domain generalization and includes a diverse set of multilingual datasets.\nRecent works such as EMO-SUPERB [33] and SERAB [34] have made notable contributions to the field of Speech Emotion Recognition (SER). However, these works have limitations in terms of the diversity of languages, datasets, and the emphasis on out-of-domain generalization.\nOur benchmark significantly advances the state-of-the-art in SER evaluation by addressing these limitations. We curate an extensive collection of multilingual datasets, carefully selected to cover diverse linguistic and cultural contexts, ensuring a thorough evaluation of SER models in real-world scenarios. Moreover, our benchmark places a strong emphasis on out-of-domain generalization, a crucial aspect that has been largely overlooked in previous works. We evaluate SER models in both in-domain and out-of-domain settings, providing valuable insights into their ability to adapt to unseen data distributions. This focus on generalizability is essential for developing SER models that can be effectively deployed in real-world applications, where the variability in speech patterns, emotions, and recording conditions is vast."}, {"title": "3. Methodology", "content": "The primary objectives of this section are to detail the dataset selection and preprocessing steps, introduce the backbone models employed, describe the model architecture and training process, explain the logit adjustment technique, and outline the evaluation protocol. The methodology is designed to ensure a comprehensive and fair evaluation of state-of-the-art SER models across diverse languages and emotional expressions."}, {"title": "3.1. Dataset Selection and Preprocessing", "content": "We curate a diverse collection of multilingual datasets for our benchmark, covering various languages and emotional expressions. Table 1 provides an overview of the datasets used in our evaluation. We focus on less commonly used datasets to mitigate overfitting and encourage the development of more robust models.\nThe datasets are preprocessed to ensure consistency and compatibility with our evaluation protocol. We set the maximum audio length to 30 seconds, and process the audios with appropriately for each backbone model we test (detailed in the next section). We rely on the Huggingface library for model preprocessing and inference implementations. Additionally, we remap the label space by mapping the original emotion labels to a unified eight-class space, facilitating cross-dataset comparisons. Due to complexity, detailing the exact remapping for each dataset is relegated to the open-source code.\nThe datasets used for out-of-domain evaluations are matched by having the same classes (excluding 'other') and their eligibility is indicated in the 'OOD Eligible' column of Table 1. These datasets were found to have the same exact classes after the class mapping, making them eligible for out-of-domain testing. When calculating out-of-domain metrics, samples with the 'other' label were discarded, and models were banned from predicting the 'other' class."}, {"title": "3.2. Backbone Models", "content": "We employ state-of-the-art speech representation models as backbones for our benchmark, as listed in Table 2. These models are selected based on their strong performance on various speech processing tasks and their ability to capture rich acoustic features.\nIn addition to the SSL models, we also evaluate MERT [37], a music recognition model, and CLAP [38, 8], a contrastive learning model. Including these models allows us to assess the effectiveness of different learning paradigms for cross-lingual SER. Lastly, we evaluate the Whisper[6] encoder which is trained under an encoder-decoder setup for ASR."}, {"title": "3.3. Model Architecture and Training", "content": "We employ a simple multilayer perceptron (MLP) architecture with approximately 500K parameters for emotion classification. The MLP consists of two hidden layers and is trained for 100 epochs. Due to the small parameter size and shallow depth, we do not expect substantial overfitting. We apply label smoothing with a factor of 0.1 to improve generalization.\nInstead of the typical approach of averaging the features before classification, we execute the MLP on every feature frame and then take the mean of the predictions. We find that this approach preserves more information and leads to stronger and more consistent results."}, {"title": "3.4. Logit Adjustment", "content": "To account for the varying class distributions across datasets, we employ logit adjustment during evaluation. This technique adjusts the model's output logits based on the difference between the training and testing dataset distributions, mitigating the impact of class imbalance and enabling fair comparisons."}, {"title": "3.5. Evaluation Protocol", "content": "Figure 1 provides an overview of our benchmark's methodology. As we described in Subsection 3.1, we establish a subset of our datasets as OOD eligible, which have the same exact classes after the class mapping. Effectively, all datasets are accounted in in-domain tests. Only OOD-eligible datasets are accounted for our out of domain metrics.\nFor each model, we construct a performance matrix where the rows represent the training datasets and the columns represent the evaluation datasets. When the training and evaluation datasets are the same (diagonal elements), it indicates in-domain performance. Off-diagonal elements correspond to out-of-domain zero-shot performance.\nWe assess the quality of the backbone models based on three key metrics:\n1. In-domain separability: We compute the mean of the diagonal elements to measure how well the features learned by a model can separate emotions within a dataset.\n2. Out-of-domain performance given training dataset: We calculate the mean of each row, excluding the diagonal element, to evaluate the model's ability to generalize to unseen datasets when trained on a specific dataset.\n3. Average performance on unseen datasets: We compute the mean of each column, excluding the diagonal element, to assess the average performance on a dataset when the model is not trained on it.\nAll metrics are reported in terms of macro-averaged F1 score to account for class imbalance."}, {"title": "4. Results and Discussion", "content": "The results of our benchmark provide valuable insights into the performance and generalization capabilities of state-of-the-art SER models across diverse languages and emotional expressions."}, {"title": "4.1. In-domain separability", "content": "The second and third column in Table 3 present the in-domain separability performance of various models, focusing on their ability to distinguish between different emotional states in speech. Performance is quantified by two metrics: the mean average performance across datasets (Mean) and the variability of performance across these datasets (Standard Deviation). From the table, Whisper-Large-v2 leads the evaluated models in in-domain SER performance, with the highest mean accuracy and low variability across datasets, closely followed by the original Whisper-Large. Other models like Whisper-Large-v3, Whisper-medium, WavLM-Large, and CLAP Music & Speech show competent but slightly more variable performances. Conversely, Hubert Large, MERT v1 330M, and w2v-bert-2.0 exhibit the lowest accuracies with higher fluctuations in their effectiveness across different datasets, indicating potential limitations in generalization capabilities for speech emotion contexts.\nThe outcome of this evaluation highlights a clear hierarchy among the models in terms of both accuracy and consistency in emotion recognition within the same domain. Whisper-Large variants stand out as the most effective, with their newer versions, particularly Whisper-Large-v2, slightly improving upon the original's already high benchmark. Lower-ranked models, though less consistent and accurate overall, may still offer valuable insights or perform well in specific niches or datasets. This analysis underscores the importance of choosing the right model for specific SER applications, balancing between performance and consistency across diverse emotional speech datasets."}, {"title": "4.2. Out-of-domain performance given training dataset", "content": "Figure 2 shows the average out-of-domain performance for each model, obtained by row-wise reduction of the performance matrix. The Whisper models demonstrate the highest out-of-domain performance, indicating their superior generalization capabilities compared to the SSL models. However, there is high variability in OOD performance across training sets. Training on some datasets like BAUM leads to much better OOD generalization than others like MELD. This warrants further investigation into what properties of datasets lead to more generalizable models. The strong performance of Whisper challenges the common belief that ASR models are suboptimal for SER and highlights the potential of leveraging ASR models for emotion recognition tasks."}, {"title": "4.3. Average performance on unseen datasets", "content": "Figure 3 presents the average performance of the evaluated models on each dataset when the models are not trained on that dataset. The results highlight the varying levels of difficulty across datasets, with some datasets posing greater challenges for out-of-domain generalization. Notably, EMOVO, MELD, and MEAD are the most challenging for models not trained on them, suggesting they have unique characteristics that are harder to learn indirectly. On the other hand, models generalize best to URDU and AESDD, indicating these datasets share more common features with others. Interestingly, the Whisper model consistently achieves strong performance across most datasets, surpassing the SSL models in many cases."}, {"title": "4.4. General outcomes", "content": "Table 3 provides a summary of the key performance metrics for the evaluated models. The second and third columns show the average and standard deviations of the in-domain results, while the next two columns show the out-of-domain performance. The weighted average column is calculated as follows:\nWeighted Average = $\\frac{\\text{Average OOD + Average ID}}{2} - \\frac{\\text{Std. Dev. OOD + Std. Dev. ID}}{2} A_{\\text{factor}}$\nwhere $A_{\\text{factor}}$ is the discounting factor, which we set to 1.0.\nThe Whisper models consistently achieve the highest scores across all metrics, further confirming their effectiveness in cross-lingual SER. However, the high standard deviations indicate that performance is quite variable depending on the specific train/test combination. This suggests that model robustness is still a challenge and there is room for improvement in developing models that perform consistently well across diverse datasets.\nOur benchmark also demonstrates the effectiveness of logit adjustment in addressing the challenges posed by varying class distributions across datasets. By incorporating this technique, we ensure fair comparisons and mitigate the impact of class imbalance on model performance."}, {"title": "5. Conclusion", "content": "In this paper, we introduced a comprehensive benchmark for evaluating the robustness and generalization of speech emotion recognition models across diverse languages and emotional expressions. Our benchmark focuses on less commonly used datasets to mitigate overfitting and encourage the development of more robust models. Through extensive experiments with state-of-the-art speech representation models, we found that the Whisper model, primarily designed for automatic speech recognition, outperforms dedicated SSL models in cross-lingual SER. This finding challenges the common belief that ASR models are suboptimal for SER and highlights the potential of leveraging ASR models for emotion recognition tasks.\nOur benchmark, along with the released code and evaluation protocol, serves as a valuable resource for the research community to assess and advance the state of cross-lingual SER. The insights gained from our work can guide future research efforts in developing more robust and generalizable SER models."}, {"title": "6. Future Works", "content": "Future directions include exploring advanced techniques for domain adaptation, few-shot learning, and meta-learning to further improve the generalization capabilities of SER models. Additionally, investigating the specific characteristics of datasets that contribute to better generalization can provide valuable insights for dataset design and selection.\nWe hope that our benchmark and findings will inspire researchers to push the boundaries of cross-lingual SER and develop models that can effectively handle the diversity of languages and emotional expressions encountered in real-world applications."}]}