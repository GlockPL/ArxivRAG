{"title": "Beyond Vision: How Large Language Models Interpret Facial Expressions from Valence-Arousal Values", "authors": ["Vaibhav Mehra", "Guy Laban", "Hatice Gunes"], "abstract": "Large Language Models primarily operate through text-based inputs and outputs, yet human emotion is communicated through both verbal and non-verbal cues, including facial expressions. While Vision-Language Models analyze facial expressions from images, they are resource-intensive and may depend more on linguistic priors than visual understanding. To address this, this study investigates whether LLMs can infer affective meaning from dimensions of facial expressions-Valence-Arousal values, structured numerical representations, rather than using raw visual input. VA values were extracted using Facechannel from images of facial expressions and provided to LLMs in two tasks: (1) categorizing facial expressions into basic (on the IIMI dataset) and complex emotions (on the Emotic dataset) and (2) generating semantic descriptions of facial expressions (on the Emotic dataset). Results from the categorization task indicate that LLMs struggle to classify VA values into discrete emotion categories, particularly for emotions beyond basic polarities (e.g., happiness, sadness). However, in the semantic description task, LLMs produced textual descriptions that align closely with human-generated interpretations, demonstrating a stronger capacity for free-text affective inference of facial expressions.", "sections": [{"title": "I. INTRODUCTION", "content": "Large Language Models (LLMs) are predominantly text-based models, designed to process and generate human language naturally. When used as interactive agents, these models rely heavily on verbal inputs and outputs to infer and express emotions. However, human emotion extends beyond words; non-verbal cues, such as facial expressions, convey crucial affective meanings essential to communication [18], [40]. Affective communication with artificial agents should include both verbal and non-verbal cues [23] and is influenced by users' emotional states [24], underscoring the need for Al systems that can interpret and generate meaningful representa- tions of human emotions. As LLMs are increasingly employed in applications requiring emotional intelligence [25], it is vital to assess their ability to move beyond merely language processing. Accordingly, this work examines the extent to which LLMs can understand and interpret facial expressions, addressing the gap between verbal and non-verbal affective communication in intelligent system design.\nVision-Language Models (VLMs) are used to analyse facial expressions by processing raw visual inputs. These models extract information from images and videos to infer emotions [31], [44]. However, relying on raw image processing presents several challenges: it requires significant computational re- sources and raises privacy concerns in sensitive contexts. Moreover, while VLMs demonstrate strong performance in emotion recognition [28], [45], their reliance on visual input remains unclear. Many of these models integrate multimodal data, but their outputs may be predominantly shaped by linguistic priors rather than genuine visual understanding [30], [33]. This lack of transparency makes it difficult to assess the role of visual information in their predictions. An alter- native approach is to represent emotional information in a structured format, rather than relying on raw visual inputs. One such representation is Valence-Arousal (VA) values [3], which quantify expressions along two dimensions: Valence (positivity/negativity) and Arousal (intensity). If models can interpret facial expressions effectively using only VA values, this would reduce reliance on direct image processing while maintaining emotional interpretability.\nThis approach allows us to assess whether LLMs can generalize affective meaning from structured numerical repre- sentations of facial expressions, rather than relying on explicit image features. Therefore, this study explores whether LLMs' semantic reasoning can be extended beyond language to struc- tured affective data, offering insights into their latent capac- ity for cross-modal inference. Specifically, we evaluate their ability in two key tasks: (1) categorizing facial expressions into discrete emotional labels and (2) generating semantic de- scriptions of these expressions. By comparing LLM-generated outputs to human annotations, this study provides insights into the strengths and limitations of LLMs in non-verbal emotion recognition. To achieve these objectives, the study addresses the following research questions:"}, {"title": "II. METHOD", "content": "In this study, we conducted two experiments with two distinct datasets. The IIMI dataset [39] and the Emotic dataset [22] as detailed in Sections III-A1 and III-B1. Each dataset included images of facial expressions that were processed us- ing FaceChannel [6], an off the shelf package that predicts VA values ranging from -1 to 1 and categorizes facial expressions into basic emotional categories. The extracted VA values were input into LLMs through custom prompts to classify these into categories of emotion (in Experiment 1) or describe the expressions semantically (in Experiment 2). Accordingly, the outputs were analyzed to (Experiment 1) show LLMs' ability to classify expressions from VA values into emotional categories, and (Experiment 2) to demonstrate the extent of similarity between textual descriptions of facial expressions generated by LLMs (based on VA values) to those given by humans (based on their observation)."}, {"title": "III. EXPERIMENT 1: CATEGORIZATION TASK", "content": "Humans often describe facial expressions via variety of different categories of emotion [11], [13], [19]. To understand LLMs' ability to classify VA values to categories of emotions, a categorization experiment was conducted. The experiment included two sub-experiments. Experiment 1.1 tested LLMs' ability to classify VA values into basic emotions (see [12]). Considering that expressions can correspond to a complex range of emotions, where an expression may align with multiple categories [32], Experiment 1.2 evaluated LLMs' ability of mapping VA values also to complex emotions (see [7]) via a multi-class categorisation task with a larger dataset."}, {"title": "A. Experiment 1.1: Basic Emotion Categorization", "content": "1) IIMI dataset: The IIMI dataset [39] contains 700 images of Indian individuals expressing seven basic emotions defined by Ekman's model (see [12]). The dataset includes 100 images per category, each assigned to a single emotion, making it ideal for single-class classification tasks [31].\n2) Methodology: All images from the IIMI dataset [39], were processed with the two models of FaceChannel [6]. The categorization model classified images into basic emotion cate- gories: Neutral, Happiness, Surprise, Sadness, Anger, Disgust, Fear, and Contempt, consistent with the IIMI dataset. The dimensional model extracted VA values, which were input into the LLM model, GPT-4o-mini [36], using the following prompt:"}, {"title": "B. Experiment 1.2: Complex Emotion Categorization", "content": "1) Emotic Dataset: The Emotic dataset [22] includes di- verse scenarios with individual faces, multiple faces, and social situations. The dataset consists of 12,821 images in the training subset and 3,663 images in the test subset. Since the study includes an evaluation task rather than a training task, we used the test subset, which includes 3,047 images with clear facial expressions (after manual inspection). This sample provided sufficient power for statistical analysis ($\\alpha$ = .05, 1 - $\\beta$ = .8, d = .2) while also minimizing computational costs and environmental impact [15]. Each image in the dataset is annotated with 1 to 9 categories of emotion (according to [22]) out of 26 categories, ranging from basic [12] to more complex emotions [7], [32]. Each image in the dataset is annotated with VA values by humans while following the method of Mou et al. [34]. The Emotic dataset was ideal for the task as it includes diverse facial expressions and multi-class emotion labels, enabling an evaluation of LLMs' multi-class categorization abilities in this affective domain.\n2) Methodology: The images from the Emotic dataset [22] were processed using FaceChannel's dimensional model [6] extracting VA values for each image, which were then pro- vided to the LLMs using the following prompt:"}, {"title": "IV. EXPERIMENT 2: SEMANTIC DESCRIPTION TASK", "content": "LLMs perform better at generating semantically descriptive outputs compared to outputs that are syntactically correct but lack meaningful semantic content [27]. This is because their primary use case has been language generation, and they are trained accordingly. Moreover, facial expressions do not always align with discrete emotion categories, as the same ex- pression can convey different emotions and social information [5]. As a result, describing expressions in words-capturing their intensity, subtlety, and affective dimensions-may pro- vide a more accurate and flexible representation than rigid classification into predefined emotion labels [26], [29]. In addition, comparing AI-generated affective explanations to human explanations provides insight into how well AI systems align with human reasoning and social norms [10]. Thus, to address the limitations of Experiment 1, Experiment 2 aimed at evaluating LLMs' performance in semantically describing facial expressions using only VA values extracted from images."}, {"title": "A. Methodology", "content": "We used the FaceChannel dimensional model [6] to extract VA values from 3,047 images in the test subset (see Section III-B1) of the EMOTIC dataset [22]. The Emotic dataset was ideal for the task as it includes diverse facial expressions with human-annotated explanations, enabling a comparison to LLMs' semantic descriptions. These were then submitted to the LLMs to generate n semantic descriptions for each unit in the dataset, corresponding to the number of human-annotated descriptions of facial expressions, using the following prompt:"}, {"title": "B. Analysis", "content": "Semantic similarity between the original and LLM- generated descriptions was calculated using two methods and three models. The first method, combined semantic similarity, compares the full LLM-generated description with the con- catenated definitions of all human-assigned categories. The second method, separate semantic similarity, treats each LLM sentence and category definition independently, calculates an n \u00d7 n similarity matrix, and averages the values. Vector repre- sentations of sentences were created using Transformers [41], Word2Vec [8], and BERT [20]. Word2Vec represents words as dense vectors based on co-occurrence patterns in a corpus, capturing local semantic relationships but lacking contextual awareness. In contrast, Transformer-based models dynamically adjust word embeddings based on surrounding context, al- lowing for a deeper understanding of sentence structure and meaning. BERT, specifically, leverages bidirectional context, making it particularly effective at capturing nuanced semantic relationships [2]. Cosine similarity was used to compute the final scores."}, {"title": "C. Results", "content": "For the GPT-40-mini, applying the Word2Vec model with the combined method of similarity calculation yielded an average cosine similarity of M = .81, 95%CI [.81, .82]. A one-sample t-test confirmed that this mean similarity was significantly higher than the baseline value of .5, t(3046) = 262.57, p < .001. Using the separate method of similarity calculation for the same model resulted in an average cosine similarity of M = .72, 95%CI [.72, .73], also significantly higher than the baseline, t(3046) = 259.84, p < .001.\nWhen using the Transformer-based embeddings, the com- bined method produced a lower similarity of M = .42, 95%CI [.42, .43], and a one-sample t-test indicated that this result was not significantly different from the baseline value, t(3046) = \u221240.06,p = 1. The separate method with Transformer embeddings yielded M = .31, 95%CI [.31, .32], t(3046) = -31.6,p = 1. With BERT-based embeddings, the combined method showed an average similarity of M = .79, 95%CI [.78, .79], t(3046) = 555.14, p < .001, while the separate method resulted in M = .62, 95%CI [.62, .63], t(3046) = 182.94, p < .001.\nFor the GPT-40, Word2Vec embeddings with the combined method yielded an average similarity of M = .80, 95%CI [.80, .81], significantly higher than the baseline (t(3046) = 227.54, p < .001). The separate method resulted in M = .74, 95%CI [.73, .74], t(3046) = 225.68, p < .001. Using Transformer embeddings, the combined method resulted in M = .39, 95%CI [.39, .40], t(3046) = -60.23, p = 1, while the separate method produced M = .28, 95%CI [.28, .29], t(3046) = -67.84, p = 1. For BERT embeddings, the combined method produced M = .79, 95%CI [.79, .80], t(3046) = 520.12, p < .001, while the separate method re- sulted in M = 0.62, 95%CI [.61, .62], t(3046) = 473.61, p < .001.\nFor the LLAMA 3.2 8B Instruct, Word2Vec embeddings with the combined method produced an average similarity of M = .77, 95%CI [.76, .77], t(3046) = 174.54, p < .001. The separate method resulted in M = .75, 95%CI [.74, .75], (3046) = 173.28, p < .001. For Transformer embeddings, the combined method produced M = .35, 95%CI [.34, .35], t(3046) = -80.49, p = 1, while the separate method resulted in M = .32, 95%CI [.32, .33], t(3046) = \u22122.2,p = .98. With BERT embeddings, the combined method showed M = .75, 95%CI [.75, .76], t(3046) = 285.61, p < .001, while the separate method resulted in M = .66, 95%CI [.65, .66], t(3046) = 239.87, p < .001."}, {"title": "V. DISCUSSION", "content": "Our findings highlight both the potential and limitations of LLMs in inferring facial expressions from VA values alone. In Experiment 1, LLMs struggled to map VA values to discrete emotions. Biases were evident, with better perfor- mance for polarized emotions (e.g., happiness, sadness) but poor recognition of others (e.g., anger, surprise). Multi-class categorization of complex emotions improved performance slightly, yet exact matches were low, suggesting difficulty in capturing nuanced and complex emotions. In contrast, Experiment 2 showed that LLMs perform significantly better when generating open-ended semantic descriptions of facial expressions. This aligns with prior research indicating LLMs excel in free-text generation over rigid classification [43], [38]. BERT and Word2Vec performed better than Transformers, suggesting that pre-trained embeddings capturing contextual and semantic relationships are more effective than purely structural representations for mapping VA values to mean- ingful affective descriptions, highlighting the importance of leveraging linguistic priors when using LLMs for structured affective inference tasks.\nThe stronger performance in the semantic description task suggests that LLMs are more effective at inferring general affective meanings from VA values rather than rigidly catego- rizing them. This aligns with theories of emotion and affect, which posit that affective perception is often more gradient-based than categorical [4], [17], [37], also when observing fa- cial expressions [16]. Our findings also highlight the potential for LLMs to complement multimodal emotion recognition sys- tems by providing descriptive information rather than binary classifications. However, LLMs' reliance on linguistic priors may lead to oversimplifications. Future work should explore integrating additional context (e.g., speech, action units) and comparing LLMs with VLMs to enhance emotion recognition."}, {"title": "VI. CONCLUSIONS", "content": "In this study, we explored the ability of LLMs to classify and describe facial expressions based solely on VA values, shedding light on their potential for affective inference without direct visual input. LLMs performed notably better at gener- ating semantic descriptions of expressions than at categorising emotions, indicating their strength in free-text descriptions over rigid classification tasks. These findings suggest that LLMs can process structured affective data, yet their reliance on linguistic priors may limit their ability to fully capture nuanced emotions. Overall, LLMs show promise for affective computing and facial expression research (e.g., see [46]), but require further refinement for nuanced emotional understand- ing. Hybrid approaches combining structured affective data with multimodal inputs could improve robustness. Future work should integrate multimodal inputs and refine LLMs' affective reasoning capabilities to enhance their application in privacy- conscious emotion recognition and social interactions."}, {"title": "ETHICAL IMPACT STATEMENT", "content": "This study did not involve human participants or personal data collection. This study utilized publicly available datasets, ensuring compliance with ethical standards for data use. By leveraging structured data rather than raw visual inputs, this research contributes to the advancement of privacy-conscious approaches in emotion recognition. The methods employed promote ethical AI development by reducing reliance on personally identifiable data and mitigating potential biases associated with direct human observation. A key ethical con- sideration is the potential for LLM-generated interpretations of affective data to reflect linguistic biases present in their training data. Future research should ensure that models are evaluated across diverse datasets to enhance fairness and generalizability in affective computing applications. Another consideration is the interpretability of LLMs' affective in- ferences. While this study examines their ability to describe emotions based on structured data, these models may not fully capture the complexity of human affective states. Over-reliance on LLM-generated interpretations in sensitive applications, such as mental health, should be approached with caution to avoid misleading conclusions."}]}