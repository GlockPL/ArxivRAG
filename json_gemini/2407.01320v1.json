{"title": "INCREASING MODEL CAPACITY FOR FREE: A SIMPLE STRATEGY FOR PARAMETER EFFICIENT FINE-TUNING", "authors": ["Haobo Song", "Hao Zhao", "Soumajit Majumder", "Tao Lin"], "abstract": "Fine-tuning large pre-trained foundation models, such as the 175B GPT-3, have attracted more attention for downstream tasks recently. While parameter-efficient fine-tuning methods have been proposed and proven effective without retraining all model parameters, their performance is limited by the capacity of incremental modules, especially under constrained parameter budgets.\nTo overcome this challenge, we propose CAPABOOST, a simple yet effective strategy that enhances model capacity by leveraging low-rank updates through parallel weight modules in target layers. By applying static random masks to the shared weight matrix, CAPABOOST constructs a diverse set of weight matrices, effectively increasing the rank of incremental weights without adding parameters. Notably, our approach can be seamlessly integrated into various existing parameter-efficient fine-tuning methods. We extensively validate the efficacy of CAPABOOST through experiments on diverse downstream tasks, including natural language understanding, question answering, and image classification. Our results demonstrate significant improvements over baselines, without incurring additional computation or storage costs.", "sections": [{"title": "1 INTRODUCTION", "content": "In recent years, the prevailing training paradigm has revolved around pre-training models on large-scale datasets and subsequently fine-tuning them for diverse downstream tasks, yielding remarkable achievements. However, the increasing size of popular pre-trained models, such as LLaMA2 (Touvron et al., 2023) and GPT3 with a size of 175B (Floridi & Chiriatti, 2020), poses significant challenges for full-sized model fine-tuning. Memory and storage limitations restrict its practicality and applicability.\nParameter Efficient Fine-Tuning (PEFT) emerges as a compelling solution to address these challenges head-on. Unlike the resource-intensive nature of full-size fine-tuning, PEFT adopts a judicious approach by either fine-tuning a small subset of the original model's parameters or introducing a limited number of additional parameters during the fine-tuning process. This strategy effectively alleviates the memory and computation burdens, providing a cost-effective alternative that can match or surpass the performance of full-size fine-tuning. Prominent PEFT techniques employed today include Prompt Learning (Sun & Lai, 2020), Prefix-Tuning (Li & Liang, 2021), Adapters (Houlsby et al., 2019; Pfeiffer et al., 2021), and LoRA (Hu et al., 2022).\nThe core concept of these PEFT techniques lies in approximating a single layer's heavy-weight matrix w \u2208 Rdi\u00d7d2 by two consecutive layers possessing much smaller inner dimension r such as B\u2208 Rd1xr, A \u2208 Rr\u00d7d2. This approach results in a significant reduction in parameter count (evident in Adapters and LoRA), achieving performance comparable to full-size fine-tuning while retaining only 1% of trainable parameters. However, the inner dimension cannot be arbitrarily small, as its impact on performance is substantial. For instance, in LoRA, the trainable parameter BA is constrained by rank(BA) \u2264 r (Hu et al., 2022), setting an upper bound on the model's capacity. These capacity constraints often lead to suboptimal performance when r is small (He et al., 2021)."}, {"title": "2 RELATED WORK", "content": "We provide a compact summary here due to space issues. A complete discussion is in Appendix J.\nParameter-Efficient Fine-tuning (PEFT). In the realm of PEFT, the initial approach involves selecting a subset of the original model parameters for updating, such as top layers (Donahue et al., 2014), specific model layers (Gheini et al., 2021), or internal modules (Zaken et al., 2022). While these brute-force selection methods effectively reduce the number of trainable parameters, they often result in sub-optimal performance. Consequently, various scoring functions have been employed to assess the importance of each parameter (Sung et al., 2021; Ansell et al., 2022; Guo et al., 2021). However, these scoring functions are typically task-specific and involve additional computational overhead.\nAnother line of work proposes sharing the pre-trained network and incorporating task-specific trainable modules, significantly reducing storage costs (Zhao et al., 2020). Notably, HAdapter (Houlsby et al., 2019) introduces adapter modules after each feed-forward and attention layer, while PAdapter (Pfeiffer et al., 2021) further suggests using adapters after FFN and LayerNorm modules (Ba et al., 2016) for improved efficiency. Drawing inspiration from textual prompting methods (Sun & Lai, 2020; Liu et al., 2019; Jiang et al., 2020; Shin et al., 2020), Prefix-Tuning (Li & Liang, 2021) employs additional prefix vectors that are prepended to the keys and values in the attention layer. Building upon this, He et al. (2021) conducts a systematic exploration of existing advancements and proposes a diverse range of Adapter variants by incorporating design elements from Prefix-Tuning.\nTo alleviate the inference latency caused by incremental parameters, Hu et al. (2022) employ a bottleneck structure with a low-rank constraint, enabling the merging of learned weights into the pre-trained network. However, these methods are limited by the prescribed rank values, which impose constraints on the maximum rank of trainable parameter matrices, thus limiting the model's capacity. In contrast, our work aims to overcome such capacity limitations and enhance parameter efficiency for fine-tuning.\nLow-rank Properties in Deep Neural Networks. In the over-parameterized regime, it has been widely observed in various deep learning tasks that neural networks exhibit low-rank properties following training (Oymak et al., 2019). Building upon this insight, several works (Jaderberg et al., 2014; Sainath et al., 2013; Khodak et al., 2020) propose explicitly incorporating low-rank constraints during neural network training, leading to notable successes in CNNs. Following a similar approach,"}, {"title": "2.1 PARAMETER PRUNING FOR PEFT METHODS", "content": "Weight-tied models, also referred to as weight-sharing or weight-tying models, represent a parameter-efficient neural network architecture where the same set of weights is shared across different layers or segments of the input (Dehghani et al., 2019; Dabre & Fujita, 2019; Xia et al., 2019; Lan et al., 2020; Li et al., 2021; Takase & Kiyono, 2021). Serving as the foundation for most implicit models, this architecture has garnered significant attention in recent years across a wide range of tasks (Wang et al., 2019; Liu et al., 2020; Yang et al., 2018; Lan et al., 2020; Takase & Kiyono, 2021; Zhang et al., 2020; Bender et al., 2020; Xie et al., 2021; Li et al., 2021). Pruning, an orthogonal yet extensively employed strategy, aims to enhance neural network efficiency by identifying and eliminating redundant parameters (Wang et al., 2022a; Frankle & Carbin, 2019; Lin et al., 2020; Su et al., 2020; Frankle et al., 2021; Wang et al., 2022b; He et al., 2022b). To the best of our knowledge, our idea of introducing various deterministic random masks to a parallel weight-tied model is novel.\nThe most relevant work to our method might be Bai et al. (2022), which extends the codebook idea and learns masks on top of a fixed random weight vector to represent diverse dense layers. Specifically, masks are utilized to select values from a random vector (i.e., codebook), resulting in layers with distinct structures. Simultaneously, Zeng et al. (2023) introduce ProPETL, a method that also incorporates shared weights but employs different masks to encode layer-specific and task-specific knowledge. Our work, in contrast to these approaches, learns shared weights with deterministic random binary masks, leading to stronger model capability.\nAdaLoRA (Zhang et al., 2022a) offers an alternative approach to achieving parameter efficiency by dynamically allocating the parameter budget across different layers. This is accomplished through iterative pruning of singular values of incremental parameters based on an importance metric. However, it is worth noting that AdaLoRA entails additional computational overhead and necessitates a higher initial budget of trainable parameters, making it less suitable for low-resource scenarios."}, {"title": "3 CAPABOOST LEARNING FRAMEWORK", "content": "3.1 MOTIVATION: RANK MATTERS IN PEFT METHODS\nLORA (Hu et al., 2022), Adapter (Li & Liang, 2021), and Prefix-Tuning (Houlsby et al., 2019) are the representative PEFT methods in the community. These PEFT methods share similar insights and can be unified (He et al., 2021). More specifically,\n\u2022 The core idea of LoRA is proposing to model the incremental update of the pre-trained weights Wpre-trained \u2208 Rd1\u00d7d2 through low-rank approximations z = x (Wpre-trained + BA), where BA is trainable and in low rank, x \u2208 Rd1, B \u2208 Rd1\u00d7r, A \u2208 Rr\u00d7d2, and r < {d1,d2}.\n\u2022 In Adapter or Prefix-Tuning, a module is inserted into a pre-trained model with structure z = xWpre-trained + f(xB)A, where B\u2208 Rd\u0131\u00d7r and A \u2208 Rr\u00d7d2 are trainable low-rank parameters. x \u2208 Rd1 is the input data, fnon-linear(\u00b7) is the non-linear function, and r < {d1,d2}.\nWhile the compact dimensions of B and A offer notable parameter efficiency benefits, they also impose significant constraints on model capacity due to the inherent low-rank nature of their product, particularly when inner dimension r is low. This restriction inevitably leads to suboptimal model performance, as evidenced by the findings presented in Figure 4 of He et al. (2021), Figure 2 of Zhang et al. (2022a), and our Figure 3(b).\nTo unlock superior performance, a crucial imperative emerges: elevating the rank of BA.\n3.2 INCREASING MODEL CAPACITY BY INCREASING THE RANK\nHereby, we propose the following theorem which indicates an efficient potential to increase rank."}, {"title": "3.3 CAPABOOST: INCREASING MODEL CAPACITY FOR FREE!", "content": "In addition to increasing the rank of low-rank-based PEFT methods to enhance model capacity, we explore a complementary approach in this section based on the insights from Theorem 3.1.\nA naive solution. Theorem 3.1 intuitively motivates us to add more parallel trainable parameters. In detail, the capacity of a general linear layer w with z = wx + b can be improved by:\n$$z = \\sum_{i=1}^{d} w_i x + b = (w := \\sum_{i=1}^{d} w_i)x + b,$$\nwhere rank(w) \u2243 d\u00b7 rank(w), once {wi} can satisfy the distribution assumption in Theorem 3.1. However, this approach becomes impractical due to the d-1 times increase in the number of additional parameters, rendering PEFT methods inefficient. As a remedy, we introduce the CAPABOOST, a cost-free solution, detailed below.\nCAPABOOST framework. We leverage the idea of weight-tying to alleviate the parameter-inefficient overhead caused by the naive solution discussed above. As intuitively visualized in Figure 2(a) and equations below, CAPABOOST constructs {wi} for free using diverse pruning masks {mi} with original weights w:\n$$z = \\sum_{i=1}^{d} (w\\odot m_i) x + b = (w\\odot \\sum_{i=1}^{d} m_i) x + b,$$\nwhere d is the number of parallel tied models, and mi is the corresponding pruning mask. Unless specified otherwise, we utilize a sparsity of 0.5 in each mi by default. These boolean masks {mi} are distinct, static, and non-trainable across the training, and should be determined before the training phase. The storage overhead of these boolean masks can be avoided by using a random generator to generate deterministic masks on the fly with several scalar seeds per forward and backward pass."}, {"title": "Benefits of CAPABOOST", "content": "With pruning masks, w \u2299 m\u2081 and w \u2299 mj become different matrices and could potentially increase model rank following Theorem 3.1. The additional study shown in Appendix I suggests that the superiority of CAPABOOST cannot solely be attributed to the implicit regularization induced by the random masking, which emphasizes the advantage of boosting the model capacity. The CAPABOOST framework not only results in an increased model capacity but also allows a further reduction in the number of parameters and even FLOPs. The original parameter w will be pruned by a ratio of 1 \u2212 s\u00b7d with independently generated masks, where s represents the sparsity ratio of masks. The pruning mask storage can also be avoided by the random generator mentioned above. An illustration is given in Figure 2(a).\nBenefiting from the sparse matrix, the inference phase of CAPABOOST enjoys both reduced parameter numbers and FLOPs. Though the total FLOPS of CAPABOOST during the training is a factor of s \u00d7 d to that of a dense layer, it could remain unchanged or lower than the dense one by appropriately selecting values for s and d. Moreover, the computation involving sparse matrices can be accelerated using NVIDIA's sparse tensor core technology for both training and inference (Choquette et al., 2021; Zhang et al., 2022c). In Appendix H, we demonstrate that CAPABOOST can enjoy the hardware acceleration while preserving the performance gains from the increased model capacity.\nTherefore, with proper selection of s and d, we can implement CAPABOOST nearly for free as it can achieve fewer parameter numbers with similar FLOPs compared to original PEFT methods.\nAdapting CAPABOOST to PEFT methods. In Figure 2(b), we illustrate how to utilize CAPABOOST as a plugin for various existing algorithms to further enhance the model capacity.\n\u2022 CAPABOOST-LoRA (adapt z = x(Wpre-trained + BA)):\n$$z = xW_{\\text{pre-trained}} + x \\left(\\sum_{i=1}^{d} (B \\odot m_{b_i}) (A \\odot m_{a_i})\\right).$$\n\u2022 CAPABOOST-Adapter / CAPABOOST-Prefix-Tuning (adapt z=xWpre-trained+fnon-linear(xB)A):\n$$z = xW_{\\text{pre-trained}} + \\sum_{i=1}^{d} (f_{\\text{non-linear}} (x (B\\odot m_{b_i})) (A \\odot m_{a_i})).$$\nRemark 3.3 (The consistency between intuition and practice). To verify the idea of rank expansion, we measure the rank in LoRA and CapaBOOST-LoRA separately, denoted as rank(BA) and rank (\u2211di=1 (B\u2299mbi)(A\u2299mai)). Using RoBERTa-base model as the base model following settings in Zeng et al. (2023), we train LoRA and CAPABOOST-LORA on the CoLA (i.e., GLUE subtask dataset (Wang et al., 2018)) and compare their ranks at the final epoch. The results in Table 1 consistently show that the rank of CAPABOOST-LoRA is always d times of that in LoRA, regardless of any changes in the inner rank r. This observation confirms our theoretical intuition."}, {"title": "4 EXPERIMENTAL SETUP", "content": "We briefly summarize the experimental setup in this sec-tion. More details can be found in Appendix B.\nModels and Datasets. We conduct a broad range of experiments on various tasks, including (a) Natural Language Understanding (NLU), (b) Question Answering (QA), and (c) Computer Vision (CV). In NLU, we evaluate the fine-tuning performance of ROBERTa-base (Liu et al., 2019) and DeBERTaV3-base (He et al., 2022a) on GLUE (Wang et al., 2018) benchmark using a diverse array of PEFT algorithms. In QA, we evaluate the performance of the proposed PEFT method on SQUADv1.1 (Rajpurkar et al., 2016) and SQUADv2.0 (Rajpurkar et al., 2018). For the CV task, we use ViT-B/16 (Dosovitskiy et al., 2020) as our pre-trained model and evaluate the efficacy of our method on the VTAB-1k (Zhai et al., 2019) benchmark.\nBackbones. For NLU tasks, similar to ProPETL (Zeng et al., 2023) and AdaLoRA (Zhang et al., 2022a), we implement CAPABOOST-LORA and CAPABOOST-PAdapter for fine-tuning ROBERTa-base (Liu et al., 2019) and DeBERTaV3-base (He et al., 2022a) on GLUE. During fine-tuning, only parameters in incremental modules and the text classification head are trainable and the other model parameters remain frozen. For QA tasks, we use the publicly available fine-tuning example scripts"}, {"title": "5 RESULTS", "content": "Our CAPABOOST is able to unleash the potential of PEFT methods across both NLP and CV domains, by increasing the model capacity without introducing additional parameters and computational cost.\n5.1 NATURAL LANGUAGE UNDERSTANDING\nFine-tuning RoBERTa-base on GLUE. We summarize the performance of CAPABOOST-LORA, CAPABOOST-PAdapter, and other baselines on the GLUE benchmark in Table 2, following the evaluation setup as ProPETL (Zeng et al., 2023) on RoBERTa-base model. Both CAPABOOST-LORA and CAPABOOST-PAdapter achieve better performance compared to their respective counterparts, while containing even less trainable parameters. Specifically, (1) CAPABOOST-PAdapter significantly outperforms the original PAdapter (86.31 v.s. 84.30) in terms of the average score on the GLUE benchmark, while reducing the trainable parameters by 25.3% (0.71% v.s. 0.95%); (2) CAPABOOST-LORA also improves the score of LoRA by 1.9 with the same extent of parameter reduction as CAPABOOST-PAdapter. These results reveal that CAPABOOST randomly masking weights in parallel tied modules is capable of enhancing the fine-tuning performance on downstream tasks via increasing the actual rank of incremental weights."}, {"title": "5.2 QUESTION ANSWERING", "content": "To verify CAPABOOST can also work in more difficult tasks, we evaluate our method on two QA datasets: SQUADv1.1 and SQuADv2.0, under four different budget settings: 0.08%, 0.16%, 0.32%, and 0.65%. The results are presented in Table 4. We find that"}, {"title": "5.3 VISUAL TASK ADAPTATION BENCHMARK", "content": "We further evaluate the efficacy of CAPABOOST on the benchmark with a different modality, VTAB-1k (Zhai et al., 2019), which includes several visual tasks from three categories: natural, specialized, and structured. Experimental results summarized in Table 5 show that our method(d=2) brings consistent improvements over LoRA on all downstream tasks, leading to an average improvement of around 1% on VTAB-1k, while maintaining the same or less storage and computation efficiency. More parallel tied modules achieve larger performance gain from 73.20 to 73.41 as evidenced in Table 5, which shows vision tasks enjoy more benefits from higher rank updates."}, {"title": "6 DISCUSSION", "content": "In this section, we present ablation study results based on the CoLA dataset to validate the effective-ness of our method. Additional ablation studies on the SST-2 dataset can be found in Appendix G.\nDifferent mask sparsity. Figure 3(a) illustrates the results of fine-tuning the RoBERTa-base model with different pruning mask sparsity on CoLA. Notably, we observe that CAPABOOST-LORA reaches peak performance when the density of random masks equals 0.6. This observation highlights the"}, {"title": "On the influence of mask types", "content": "Dropout (Hinton et al., 2012) addresses overfitting by randomly setting parameters or activations to zero (Wan et al., 2013; Kingma et al., 2015; Gal & Ghahramani, 2016), similar to our masking to some extent. However, our method aims to boost the capacity of dense weights through learning with deterministic binary masks, rather than relying on regularization. Figure 3(a) validates the efficacy of our design choice, Diff-mask, by comparing it with two alter-natives: Same-mask and Dropout. Our findings reveal that Diff-mask consistently outperforms both alternatives across different sparsity levels, suggesting that different random but deterministic masks in CAPABOOST play vital roles in the performance gain. In the case of employing new random weight masking in each iteration like Dropout, it does result in poor performance when the density is low."}, {"title": "Tradeoff between parameter budget and the number of parallel tied modules", "content": "As discussed in Section 3.1, the rank of incremental weight matrix bottlenecks the model capacity, especially in the case with a limited parameter budget. We illustrate this point by conducting an ablation study over the inner dimension r of LoRA modules as shown in Figure 3(b). We find that\n\u2022 The performance of CAPABOOST-LORA with one single layer on CoLA monotonically increases with the inner dimension r. It is aligned with our intuition in Section 3.1 that the performance is limited by the rank of incremental weight matrices when the model capacity is insufficient.\n\u2022 CAPABOOST-LORA with 2 or 3 parallel layers significantly outperforms the case with only one parallel layer. Specifically, the fine-tuning performance of 2-layer-CAPABOOST-LORA with r = 32 surpasses that of 1-layer-CAPABOOST-LoRA with r = 64, where the former one has 25% fewer trainable parameters than that of 1-layer-CAPABOOST-LoRA.\nThese results demonstrate that the fine-tuning performance is bottlenecked by the model capacity and CAPABOOST can alleviate this problem by increasing the model capacity for free. However, we also notice that the performance of CAPABOOST-LORA starts to degrade as we further increase the rank of incremental weights, which might be attributed to potential optimization difficulties e.g. overfitting."}, {"title": "Limitations and future work", "content": "Although CAPABOOST has demonstrated its prowess in enhancing model capacity, it becomes susceptible to overfitting when the internal dimension is set to a large value. Addressing this issue through adaptive weight allocation across layers remains a promising avenue for future research. Furthermore, CAPABOOST's remarkable parameter efficiency positions it as an ideal solution for tackling demanding applications, like fine-tuning Large Language Models. We also defer exploration of this potential to future work."}, {"title": "A Proof of Theorem 3.1", "content": "We first rewrite the theorem:\nTheorem A.1. Assume two matrices X and Y are randomly generated by X = XcolXrow and Y = Ycolyrow respectively. Xcol := [xcol,xcol,...,xcol] \u2208 Rdxr, where column vector basis {xcol,xcol,...,xcol} are sampled from N(0, Id). Similarly, Xrow = [xrow,xrow,...,xrow] \u2208 Rr\u00d7d by sampling row vector basis {xrow,xrow,...,xrow} from N(0, Id). Id \u2208 Rd\u00d7d denotes an identity matrix. For matrices X \u2208 Rd\u00d7d and Y \u2208 Rd\u00d7d, we have\nrank(X+Y) = rank(X)+rank(Y) with probability equal to 1 almost surely when 2r < d. (6)\nWe rely on the following two lemmas to finish our proof.\nLemma A.2 (Marsaglia (1964)). Let X and Y be two matrices of the same size, R\u2081 and R2 are their row spaces, C\u2081 and C2 are their column spaces, \u2229 is the linear space intersection. The rank of two matrices sum can be bound as:\nrank(X + Y) >= rank(X) + rank(Y) \u2013 dim(R1 \u2229 R2) \u2013 dim(C1 \u2229 C2),\nLemma A.3 ((Bogachev & Ruas, 2007)). If r < d, then every r-dimensional subspace of Rn has d-dimensional Lebesgue measure zero.\nProof. Step (1). Fix X, define column space of Xcolas\nColumnSpace(Xcol) = {x \u2208 Rd | x = \u2211\u03bb\u03af\u03c7\u03bf\u03b9, \u03bb\u03af\u2208R}. (7)"}, {"title": "B IMPLEMENTATION DETAILS", "content": "The implementation of our algorithm builds on the publicly available PyTorch (Paszke et al., 2019) and Adapter-Transformers (Pfeiffer et al., 2020)."}, {"title": "C DATASETS", "content": "C.1 GLUE BENCHMARK\nThe General Language Understanding Evaluation (GLUE) is a benchmark of nine sentence- or sentence-pair language understanding tasks (Wang et al., 2018), designed to evaluate and analyze the performance of language model with respect to a broad range of linguistic phenomena found in natural language tasks. Similar to two most recent work (Zeng et al., 2023; Zhang et al., 2022a), we conduct extensive experiments on 8 GLUE tasks, including CoLA, SST-2, MRPC, QQP, STS-B, MNLI, QNLI, and RTE. We present the dataset statistics of GLUE (Wang et al., 2018) in Table 6."}, {"title": "C.2 SQUADv1.1 AND SQUADv2.0", "content": "Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions collected by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable. The statistics of SQuADv1.1 and SQuADv2.0 are shown in Table 7."}, {"title": "C.3 VTAB-1K", "content": "Visual Task Adaptation Benchmark (VTAB) proposed by Zhai et al. (2019), has good representations as those that adapt to diverse, unseen tasks with few examples. It includes CIFAT-100, DTD, Flower102, Pets, SVHN, Eurosat, Resisc45, Clevr-Count, Clevr-Dist, DMLab, dSpr-Ori and sNORB-Azim datasets and classify them into Natural, Specialized and Structured catagories."}, {"title": "D Natural Language Understanding", "content": "D.1 ROBERTA-BASE\nROBERTa (Liu et al., 2019) uses an optimized pre-training recipe based on the one originally proposed in BERT (Devlin et al., 2018) and improves the performance on the same tasks with BERT without introducing many more tunable model parameters. It is a competitive choice of pre-trained model and broadly used by prior work for comprehensive evaluations (Hu et al., 2022; Zeng et al., 2023). We take the pre-trained ROBERTa-base model from the HuggingFace Transformers library (Wolf et al., 2020) and fine-tune it on the GLUE benchmark (Wang et al., 2018). We initialize the model to the pre-trained model at the beginning of every fine-tuning task."}, {"title": "E QUESTION ANSWERING", "content": "E.1 BUDGET CONFIGURATION\nWe present the budget configuration for experiments on DeBERTaV3-base in Table 14."}, {"title": "E.2 TRAINING DETAILS", "content": "We tune the learning rate from {1 \u00d7 10\u22124,3 \u00d7 10\u22124,5 \u00d7 10\u22124,8 \u00d7 10\u22124,1 \u00d7 10\u22123} and pick the learning rate with the best performance for each dataset. The details of hyper-parameters are shown in Table 15."}, {"title": "F VISUAL TASK ADAPTATION", "content": "For the VTAB-1k(Zhai et al., 2019), we follows its default augmentation settings. All input images are resized to 224 \u00d7 224 and apply normalization with ImageNet means and standard deviation. The inner dimension of LORA is set to 8."}, {"title": "GADDITIONAL ABLATION STUDY", "content": "In this section, we implement additional ablation studies on SST-2-10k dataset, which contains 10k data examples uniformly sampled from the original SST-2 dataset, to further analyze the effectiveness of our proposed method. We use the same hyper-parameters as in section 6 for all experiments in this section. The results are shown in Figure 4."}, {"title": "\u0397 HARDWARE ACCELARATION UTILIZATION", "content": "The hardware accelaration for sparse matrix operation in CAPABOOST relies on NVIDIA's sparse tensor core technology. This requires sparse matrix(e.g. pruning mask in CAPABOOST) in a shape of N:M sparsity, for example, 2:4 sparsity. To verify our proposed CAPABOOST still enjoys performance benefits in N:M sparsity matrix, we present GLUE benchmark experimental results in Table 16.\nThe results indicate that CapaBoost performs similarly with both fully random masks and N:M sparsity random masks. Thus, we can directly replace fully random masks with N:M sparsity random masks and benefit from the hardware acceleration provided by the Nvidia sparse tensor core. In our experiment, the fine-tuned weight matrix with N:M sparsity displays the same rank as with fully random masks."}, {"title": "I COMPARISON TO REGULARIZATION-BASED BASELINES", "content": "One potential question arising from our CAPABOOST approach is whether the performance gain is primarily due to the implicit regularization effect induced by the random masking. We address this concern via comparing CAPABOOST with three new baselines: AdaMix (Wang et al., 2022c), Dropout (Hinton et al., 2012), and same masking. In particular, AdaMix proposes to randomly route training examples from a batch of inputs via stochastically chosed PEFT modules and adopt a consistency regularization loss. The performance improvements of Dropout and same masking compared to the underlying PEFT modules are purely coming from regularization effects. Experiment results are shown in Table 17. Note that we use PAdapter as the underlying PEFT mechanism for all baselines in this experiment."}, {"title": "JADDITIONAL RELATED WORK", "content": "J.1 PARAMETER-EFFICIENT FINE-TUNING (PEFT)\nThe first line of work in PEFT picks up a subset of the original model parameters to update, such as top layers (Donahue et al., 2014), specific model layers (Gheini et al., 2021), and internal modules (Zaken et al., 2022). These brute-force selection approaches are effective in reducing trainable parameters, but only leading to sub-optimal performance. Thus, various scoring functions are used to measure the importance of each parameter (Sung et al., 2021; Ansell et al., 2022; Guo et al., 2021). However, these scoring functions are usually task-specific and require additional computation."}, {"title": "J.2 LOW-RANK PROPERTIES IN DEEP NEURAL NETWORKS", "content": "In the over-parameterized regime, it has been demonstrated in many deep learning tasks that neural networks enjoy low-rank properties after training (Oymak et al., 2019). Inspired by this insight, some works (Jaderberg et al., 2014; Sainath et al., 2013; Khodak et al., 2020) propose to reinforce neural network training performance by explicitly injecting the low-rank constraints and achieve great success in CNNs. Similarly, LoRA (Hu et al., 2022) and a number of follow-up works (Dettmers et al., 2023; Zhang et al., 2022a; Chavan et al., 2023; Chen et al., 2023) borrow this idea and suggest applying low-rank updates to a frozen pre-trained network for fine-tuning on downstream tasks. While state-of-the-art model architectures like transformers have been shown to present a low-rank dimensionality and representations (Aghajanyan et al., 2021; Wang et al., 2020), Bhojanapalli et al. (2020) point out that the low-rank of key and query projections in the multi-head attention modules bottlenecks the performance of transformers. Experiments in Lialin et al. (2023) also demonstrate that transformers with low-rank updates perform significantly worse than full-rank baseline in the training stage."}, {"title": "J.3 WEIGHT-TIED MODELS", "content": "Weight-tied models, also known as weight-sharing or weight-tying models, are a type of parameter-efficient neural network architecture, in which the same set of weights is used across different layers or parts of the input (Dehghani et al., 2019; Dabre & Fujita, 2019; Xia et al., 2019; Lan et al., 2020; Li et al., 2021; Takase & Kiyono, 2021). This architecture, as the backbone of most implicit models, has been widely studied in recent years for various tasks (Wang et al., 2019; Liu et al., 2020; Yang et al., 2018; Lan et al., 2020; Takase & Kiyono, 2021; Zhang et al., 2020; Bender et al., 2020; Xie et al., 2021; Li et al., 2021). For instance, the Universal Transformer proposed in Dehghani et al. (2019) ties the parameters through one Transformer layer; such an idea was later employed in Dabre & Fujita (2019); Lan et al. (2020). In addition, Xia et al. (2019) introduce an encoder-decoder architecture that shares parameters between"}]}