{"title": "Augmented Lagrangian-Based Safe Reinforcement Learning Approach for Distribution System Volt/VAR Control", "authors": ["Guibin Chen"], "abstract": "This paper proposes a data-driven solution for Volt-VAR control problem in active distribution system. As distribution system models are always inaccurate and incomplete, it is quite difficult to solve the problem. To handle with this dilemma, this paper formulates the Volt-VAR control problem as a constrained Markov decision process (CMDP). By synergistically combining the augmented Lagrangian method and soft actor critic algorithm, a novel safe off-policy reinforcement learning (RL) approach is proposed in this paper to solve the CMDP. The actor network is updated in a policy gradient manner with the Lagrangian value function. A double-critics network is adopted to synchronously estimate the action-value function to avoid overestimation bias. The proposed algorithm does not require strong convexity guarantee of examined problems and is sample efficient. A two-stage strategy is adopted for offline training and online execution, so the accurate distribution system model is no longer needed. To achieve scalability, a centralized training distributed execution strategy is adopted for a multi-agent framework, which enables a decentralized Volt-VAR control for large-scale distribution system. Comprehensive numerical experiments with real-world electricity data demonstrate that our proposed algorithm can achieve high solution optimality and constraints compliance.", "sections": [{"title": "I. INTRODUCTION", "content": "With the integration of large-scale distributed energy resources (DERs), modern distribution networks face significant operational challenges, such as voltage violations and excessive active power losses. Reactive voltage control, as a critical component of distribution management systems (DMS), is employed to optimize the control actions of all voltage regulation and reactive power control devices (e.g., voltage regulators, load tap changers, and switchable capacitor banks) to minimize system losses and operational costs, while adhering to operational constraints such as voltage limits and line loading restrictions. Effectively utilizing the integrated large-scale DERs to achieve voltage regulation and reduce line losses is a pressing issue in the operation and control of modern power grids. In particular, with the increasing penetration of inverter-based energy resources (IB-ERs), which can provide rapid reactive power support, leveraging IB-ERs for reactive voltage control has attracted widespread attention.\nConventional reactive voltage control methods are primarily optimization-based approaches that rely"}, {"title": "II. PROBLEM FORMULATION", "content": "This section will first introduce the concept of CMDP and then model the reactive voltage control problem in distribution networks as a CMDP."}, {"title": "A. Preliminaries of Constrained Markov Decision Process", "content": "The Markov decision process (MDP) is widely adopted to formulate the sequential decision process with its predefined transition probability to model how the current state-action pair influences the state in the next step. By allowing for the inclusion of constraints that model the concept of safety, the MDP is further generalized as a constrained Markov decision process (CMDP). Generally, the CMDP can be represented by a tuple, which consists of a state space S, an action space A, a reward function R, a cost function $R^c$, a transition probability $P_r$, and a discount factor $\\gamma \\in [0,1]$.\nUnder a CMDP setting, an agent learns the optimal policy by interacting with the environment at each discrete time step, t=1,2,...,T. In each time step t, the agent first observes the current state of the environment $s_t \\in S$, then selects an action $a_t \\in A$ guided by its policy. The action adopted at time step t lead to the next state according to the unknown transition probability function $P_r(s_{t+1}|s_t, a_t)$. After the transition, the agent then receives the reward $R(s_t, a_t, s_{t+1}) \\in R$ and the cost $R^c(s_t, a_t, s_{t+1}) \\in R^c$ from the environment. In a CMDP, the reward and cost are associated with each action and state pair experienced by the agent, and the safety is maintained only if the expected discounted cost is below a certain threshold.\nInstead of focusing on the reward and cost associated with individual action-state pairs, the agent aims to find the control policy $\\pi$ that maximizes the long-term return and safety, which are represented by the expected discounted return $J(\\pi)$ and the expected discounted cost $J^c(\\pi)$:\n$\\max_{\\pi} J(\\pi)$ (1)\ns.t.\n$J^c(\\pi) \\leq \\overline{J}$ (2)\nwhere $\\pi$ represents a mapping from a state space S to an action space A for a stochastic or deterministic policy. The expected discounted return is defined as $J(\\pi) = E_{\\tau \\sim \\pi}[\\sum_{t=0}^{T} \\gamma^t R_t]$, where $\\tau$ is a trajectory or sequence of states and actions, {$s_0, a_0, s_1, a_1,...s_{T-1}, a_{T-1}, s_T$}. $R_t$ is the short name for $R(s_t, a_t, s_{t+1})$. The expected discounted cost under policy $\\pi$ in trajectory $\\tau$ is defined in a similar manner:\n$J^c(\\pi) = E_{\\tau \\sim \\pi}[\\sum_{t=0}^{T} \\gamma^{t} R_t^c]$, where $R_t^c$ is $R^c(s_t, a_t, s_{t+1})$ for short."}, {"title": "", "content": "The state-action value function $Q^{\\pi}(s,a)$ is defined as follows:\n$Q^{\\pi}(s,a) = E_{\\tau \\sim \\pi}[\\sum_{t=0}^{T} \\gamma^t r_t | s_0 = s, a_0 = a]$ (3)\nwhere $Q^{\\pi}(s,a)$ denotes the expected discounted reward starting from state s, taking action a, and thereafter following policy $\\pi$. The value function satisfies the Bellman equation:\n$Q^{\\pi}(s_t,a_t) = E_{a_{t+1} \\sim \\pi, s_{t+1} \\sim P_r}[R_t + \\gamma Q^{\\pi}(s_{t+1},a_{t+1})]$ (4)"}, {"title": "B. Formulating Volt/Var Control Problem as CMDP", "content": "In the Volt/Var control problem, the distribution network's dispatch control center can be regarded as an agent interacting with the power grid system. By issuing control commands to the inverter-based energy resources (IB-ERs) in the grid, it optimizes both voltage levels and network losses. Consequently, the reactive power voltage control problem in the distribution network can be formulated as a CMDP. The state space, action space, reward function, and cost function are defined as follows:\n1) State Space: the state space is defined as s=[P,Q,V], where P,Q,V represent the active power, reactive power, and voltage magnitude at the nodes, respectively.\n2) Action Space: The action space is denoted as $a = Q_G$, where $Q_G$ represents the reactive power outputs of all IB-ERs and static Var compensators (SVCs). For IB-ERs, the range of $Q_G$ is constrained by $|Q_G| \\leq \\sqrt{S_G^2 - P_G^2}$, where $S_G$ is the apparent power limit of the IB-ERs, and $P_G^o$ is the active power output upper bound. For SVCs, the range of $Q_G$ is $\\underline{Q_G} \\leq Q_G \\leq \\overline{Q_G}$, where $\\underline{Q_G}$ and $\\overline{Q_G}$ represent the lower and upper bounds of the SVC output, respectively.\n3) Reward and cost function: According to the definition of reinforcement learning, the reward R($s_t, a_t, s_{t+1}$) and cost are $R^c$($s_t, a_t, s_{t+1}$) functions of the state s. The objective of the reactive power voltage control problem is to minimize network losses while maintaining acceptable voltage levels. Therefore, the reward function is defined as:\n$R_t = -\\sum P^t$\nThe definition of cost function is:\n$R^c = -2[max(V^t - \\overline{V}, 0) + max(\\underline{V} - V^t, 0)]$\nwhen $R_t^c$ = 0, it indicates that all node voltages are within acceptable limits, and when $R_t^c$ > 0,it indicates that some node voltages violate the voltage level constraints.\nThrough the reward and cost functions defined above, the distribution network dispatch control center, replaced by a safe reinforcement learning agent, interacts with the power grid system and learns the optimal control policy. This enables the minimization of network losses while ensuring that voltage levels remain within acceptable limits."}, {"title": "III. AUGMENTED LAGRANGIAN-BASED SAFE REINFORCEMENT LEARNING APPPROACH", "content": "In this section, we develop an innovative constrained deep reinforcement learning algorithm based on theaugmented Lagrangian method, thus we named it as augmented Lagrangian soft actor-critic (AL-SAC). To solve the examined problem, the adopted reinforcement learning algorithm should be sample efficient and constraints satisfied.\nSample efficiency: The training of DRL algorithms generally requires the agent to interact with the environment to collect sufficient samples. However, collecting a tremendous amount of operation experiences for EV charging scheduling problem repeatedly in real world is not realistic. Since the off- policy DRL algorithm's learned control policy (target policy) and the policy that generates instantaneous control action (behavior policy) are different, it allows the reuse historical operational experiences. Compared to on-policy ones, off-policy DRL algorithms are much more sample-efficient and appropriate for the examined problem.\nConstraints compliance: Traditional RL methods have not considered safe exploration. While giving a RL agent complete freedom is unacceptable since certain exploratory behaviors may cause physical damage. For example, in the EV charging control problem, over-discharging will lead to significant SOC violations in the EV causing battery damage or dissatisfaction with the driver's charging demand. Thus, it is crucial to develop a RL algorithm, which can always achieve near constraint satisfaction.\nIn the following subsections, we first introduce the state-of-the-art maximum-entropy-based off-policy RL algorithm, soft actor-critic (SAC) [29]. Then we present the proposed constrained RL algorithm and discuss the algorithm design."}, {"title": "A. Soft Actor-Critic", "content": "Actor-critic algorithms such as PPO [30], A3C [31], and DDPG [32] have been widely adopted in the DRL application. However, the first two are featured with sample inefficient, as they require samples generated by the latest policy at each gradient step. While the DDPG often suffers from hyperparameter sensitivity. To tackle the abovementioned challenges, [29] introduces the maximum-entropy concept in [33] into the actor-critic framework and develops the soft actor-critic (SAC) algorithm, which outperforms aforementioned algorithms.\nBy combining the entropy into the value function, SAC achieves a better tradeoff on exploration and exploitation and therefore avoids the suboptimal. The entropy for a probabilistic policy at state s, is defined as H($\\pi(\\cdot|s_t)$) = -$\\sum_a \\pi(a|s_t)log \\pi(a|s_t)$. Then the state-action value function of SAC is defined as :"}, {"title": "", "content": "$Q^{\\pi} (s_t,a_t) = E_{a_{t+1} \\sim \\pi, s_{t+1} \\sim P_r}[R_t + \\gamma (Q^{\\pi} (s_{t+1}, a_{t+1}) +\\alpha H(\\pi(\\cdot|s_{t+1})))]$ (7)\nThe policy function is defined as a probability distribution $\\pi(\\cdot |s_t)$ in a stochastic manner as:\n$\\pi(\\cdot | s_{t+1}) = \\frac{e^{\\frac{Q^{\\pi} (s,a)}{\\alpha}}}{\\sum_a e^{\\frac{Q_t (s,a)}{\\alpha}}}$ (8)"}, {"title": "B. Augmented Lagrangian SAC", "content": "Even though the SAC has achieved excellent performance on a range of challenging control tasks, it can only solve the MDPs. A popularly adopted approach to tackle CMDPs with DRL algorithms is revising the reward function via adding penalties associated with infeasible control over constraint. However, simply adding the product of fixed penalty coefficient and constraint violation into the reward function will lead to an infeasible or too consecutive control policy. In addition, identifying the penalty coefficient requires trial and error, which is low-efficient and time-consuming. In this subsection, we propose AL-SAC by extending the SAC algorithm to satisfy the operational constraints in CMDPs.\nThe optimal EV charging/discharging scheduling problem can be formulated as follows:\n$\\max_\\pi I(\\pi) = E_{\\tau \\sim P}[\\sum_t \\gamma^t R_t]$ (9)\ns.t.\n$\\underline{a} \\leq a \\leq \\overline{a}$ (10)\n$E_{(s,a_t) \\sim p} [-log(\\pi_t(a_t | s_t))] \\geq H$ (11)\n$J^c(\\pi) = E_{\\tau \\sim P}[\\sum_t \\gamma^t R_t^c] \\leq \\overline{J_c}$ (12)\nwhere the objective function is to maximize the negative of charging cost. The first line of constraints denotes the action bound, in which the $\\underline{a}$ and $\\overline{a}$ denote the lower and upper bound, respectively. The second line of constraints is the lower bound of entropy and the third line is the upper bound for the SOC deviation. The threshold value of entropy is a hyperparameter which is used to balance the exploration and exploitation. For continuous action space, a general entropy threshold value is set as the negative value of the action space's dimension [29].\nNote that the action bound has already been included in the action space's definition. We adopt the augmented Lagrangian method to transfer the original constrained optimization problem into the unconstrained formula:"}, {"title": "", "content": "$\\max_{\\pi}\\min_{\\alpha \\geq 0, \\lambda \\geq 0} J(\\pi) + \\alpha(-H-log(\\pi_t(a_t | s_t))) + \\lambda(\\overline{J_c} - J^c(\\pi)) + \\frac{\\delta_{\\lambda}}{2}(\\overline{J_c} - J^c(\\pi))^2$ (13)\nwhere $\\alpha, \\lambda$ are multipliers for entropy constraint and cost constraint, respectively. The $\\delta_{\\lambda}$ denotes the updating step size of$\\lambda$, acts as the penalty coefficient in the augmented Lagrangian equation here. The entropy threshold and the upper bound for the SOC deviations are determined for specific optimization problems. In traditional RL-based methodologies, the SOC deviation is penalized directly in the reward function. In other word, they treat the variables $\\alpha$ and $\\lambda$ as penalty coefficients. However, the values for the penalty coefficients are hard to ensure. They can be either too conservative or infeasible. To overcome such a dilemma, we adopt the augmented Lagrangian method to solve the optimization problem, which guarantees that both primal and dual variables reach their optimal values and therefore guarantee the constraint compliance."}, {"title": "C. Algorithm Design", "content": "Since the proposed AL-SAC is an off-policy DRL algorithm, parameters of actor and critic networks can be updated in an iterative manner using historical transitions. The overall framework of the AL-SAC is summarized in Algorithm 1. In each iteration, critic networks for evaluating expected discounted reward and cost are firstly trained by using stochastic gradient descent. Then, the Lagrange multipliers of corresponding constraints are updated using gradient ascent and the actor network is updated in a policy gradient manner with the unconstrained Lagrangian function."}, {"title": "IV. CASE STUDIES", "content": "In this section, we apply the proposed AL-SAC algorithm for reactive voltage control across distribution networks of varying scales under real load fluctuation conditions. The performance of the proposed algorithm is evaluated by comparing it with methods proposed in previous studies."}, {"title": "A. Environment Settings", "content": "The algorithm was tested on 33-bus, 69-bus, and 118-bus distribution networks to demonstrate the advantages of the proposed TC-DRL method. In the 33-bus test network, three IB-ERs with a reactive power capacity of 2 MVar and active power of 1.5 MW were connected at nodes 17, 21, and 24, while one SVC with a reactive power capacity of 2 MVar was connected at node 32. In the 69-bus test network, four IB-ERs with a reactive power capacity of 2 MVar and active power of 1.5 MW were connected at nodes 5, 22, 44, and 63, with one SVC of 2 MVar reactive power capacity connected at node 13. In the 118-bus test"}, {"title": "B. Benchmark Methods", "content": "We selected several state-of-the-art deep reinforcement learning algorithms and model-based optimization methods as baseline approaches for comparison. These deep reinforcement learning algorithms include DDPG and SAC. All of these algorithms are capable of handling high-dimensional continuous state and action spaces. However, DDPG and SAC cannot be directly applied in a CMDP setting. To address the constraints in the problem under study, we manually selected penalty coefficients in the reward functions of these algorithms. It is important to note that the value of the penalty coefficients is difficult to determine directly and typically requires trial and error for evaluation. In our study, the reward functions of DDPG and SAC were defined as $R_t - \\delta R_t^c$, in which the penalty coefficient $\\delta$ is set as 5 and 0.5. Since DDPG, SAC, and AL-SAC are all off-policy deep reinforcement learning algorithms, the sample outcomes from each training iteration can be stored as historical data for model updates.\nTo better illustrate the effectiveness of the proposed AL-SAC algorithm, the neural network architecture and parameters of all these off-policy actor-critic DRL algorithms are set the same. The details of neural networks are presented in Table I. As discussed in Section III, to balance the exploration and exploitation, the value of target entropy is set as the negative value of action space's dimension."}, {"title": "C. Numerical Results of Centralized Framework", "content": "Under the centralized control scheme, all reactive support devices (IB-ERs and SVCs) in the distribution network are scheduled and controlled by a reinforcement learning agent representing the distribution network control center. To verify the convergence and satisfaction of safety constraints of the proposed safe reinforcement learning method in the reactive voltage control task, this section analyzes the numerical results of various methods on the 33-bus and 69-bus systems. For the aforementioned reinforcement learning methods, the models are trained using 200 days of grid data. Since the dataset contains 96 data points per day, the training dataset consists of a total of 19,200 samples. To eliminate the impact of randomness caused by model training, all reinforcement learning algorithms are trained under three fixed random seed settings, and the average of the results is taken as the final model output.\nFig. 1-4 illustrates the training process and convergence of all reinforcement learning algorithms and the model-based optimization method on the 33-bus distribution network. Specifically, Figures 1 and 2 show the network losses caused by reactive voltage control across all methods on the 200-day training dataset, while Fig.3 and Fig.4 depict the voltage limit violations corresponding to the control strategies in the training dataset. For the reinforcement learning methods, the models tend to converge as the training steps increase. As shown in Fig. 1 and Fig. 2, the proposed AL-SAC algorithm converges in the latter half of the training process, with its control strategy resulting in network losses close to the theoretical optimal solution obtained by the model-based optimization method. In contrast, for the baseline reinforcement learning methods such as DDPG and SAC, it can be observed that these methods, which heavily rely on the selection of penalty coefficients in the reward function, struggle to effectively minimize network losses while simultaneously ensuring voltage limit compliance. Fig. 1 shows that when larger penalty coefficients are selected, the network losses caused by DDPG and SAC are higher than those of the AL-SAC algorithm, indicating that the strategies are overly conservative, prioritizing voltage limit compliance. Fig. 3 shows the voltage limit violations under various methods with larger penalty coefficients. It can be observed that the voltage violations caused by DDPG and SAC are relatively low, approximating those of the AL-SAC algorithm and the theoretical optimal result."}, {"title": "D. Numerical Results of Decentralized Framework", "content": "To demonstrate the scalability of the proposed reactive voltage control scheme, this paper further introduces a distributed reactive voltage control approach under a multi-agent setting. In larger-scale distribution networks, the system measurement information from each local area is delayed when uploaded to the control center. Distributed control agents can only observe the system measurements from adjacent subregions and then control the nearby reactive support devices (IB-ERs and SVCs) based on local observations. Under the distributed control framework, all agents collaborate to minimize network transmission losses while avoiding voltage limit violations."}, {"title": "", "content": "To implement the distributed reactive voltage control, a centralized training and distributed execution strategy is adopted based on the proposed AL-SAC algorithm. The policy network for reactive power regulation in each local control area determines the corresponding reactive power adjustment actions upon"}, {"title": "", "content": "receiving the local area measurement information and implements these actions. The local measurement information is then uploaded to the control center. The neural network for evaluating the reward function and cost function is trained on the global measurement dataset provided by the control center. Fig. 9 illustrates the framework of the extended multi-agent reactive voltage control scheme presented in this paper."}, {"title": "", "content": "To validate the proposed distributed reactive voltage control scheme under the multi-agent setting, experiments were conducted on the 118-bus distribution network system. Similar to the experimental setup for the 33-bus and 69-bus systems, the load and generation data were sourced from real grid measurements, spanning 200 days with 96 sample data points per day, resulting in a training set of 19,200 samples. In the 118-bus distribution network, each reactive compensation device is controlled by an AL-SAC agent, totaling 10 agents. Each distributed AL-SAC agent only receives the measurement information (including voltage magnitude, active power, and reactive power) from its respective node. Figure 10 shows the network losses of the AL-SAC algorithm compared to the theoretical optimal solution obtained by the model-based optimization method in the 118-bus system. Fig. 11 displays the corresponding voltage limit violations. The experimental results indicate that the proposed AL-SAC-based reactive voltage control algorithm demonstrates good scalability. Under the centralized training and distributed execution multi- agent framework, it achieves network losses close to the theoretical optimal solution while satisfying voltage level constraints. Table III presents the network losses and voltage limit violations of the trained distributed AL-SAC model on the same test dataset used in the previous experiments. The numerical results in Table III demonstrate that the proposed distributed reactive voltage control method for distribution networks achieves near-optimal control strategies, meeting the operational control requirements of the distribution network."}, {"title": "V. CONCLUSION", "content": "In this paper, we propose a model-free offline policy-based safe reinforcement learning algorithm, AL-SAC, to address the reactive voltage control problem in distribution networks. The algorithm is designed to minimize network losses while ensuring voltage level constraints are met, even in the absence of accurate distribution network model information. To prevent voltage violations, the reactive voltage control problem is formulated as a constrained Markov decision process. By incorporating the augmented Lagrangian method, the original constrained optimization problem is transformed into an unconstrained one. Through alternating updates of the neural network parameters and Lagrangian multipliers within the reinforcement learning model, the proposed safe reinforcement learning algorithm converges to the optimal solution. To validate the effectiveness and novelty of the proposed method, experiments were conducted on the 33-bus and 69-bus distribution networks. The experimental results demonstrate that the proposed method significantly outperforms other reinforcement learning approaches in reactive power optimization for distribution networks. To further verify the scalability of the proposed method, we introduced a centralized learning, distributed execution multi-agent framework, which successfully implemented distributed reactive voltage control in the 118-bus distribution network system."}]}