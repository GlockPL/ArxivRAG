{"title": "BENCHAGENTS: Automated Benchmark Creation with Agent Interaction", "authors": ["Natasha Butt", "Varun Chandrasekaran", "Neel Joshi", "Besmira Nushi", "Vidhisha Balachandran"], "abstract": "Evaluations are limited by benchmark availability. As models evolve, there is a need to create benchmarks that can measure progress on new generative capabilities. However, creating new benchmarks through human annotations is slow and expensive, restricting comprehensive evaluations for any capability. We introduce BENCHAGENTS, a framework that methodically leverages large language models (LLMs) to automate benchmark creation for complex capabilities while inherently ensuring data and metric quality. BENCHAGENTS decomposes the benchmark creation process into planning, generation, data verification, and evaluation, each of which is executed by an LLM agent. These agents interact with each other and utilize human-in-the-loop feedback from benchmark developers to explicitly improve and flexibly control data diversity and quality. We use BENCHAGENTS to create benchmarks to evaluate capabilities related to planning and constraint satisfaction during text generation. We then use these benchmarks to study seven state-of-the-art models and extract new insights on common failure modes and model differences.", "sections": [{"title": "1 Introduction", "content": "AI advancements are progressing rapidly, with new models frequently showing enhanced capabilities. Evaluation datasets are essential for testing these claims, but they are expensive to produce and can quickly become saturated, due to the fast pace of model improvements (Balachandran et al., 2024), or contaminated (Zhang et al., 2024a). In the absence of benchmarks, new capabilities are often demonstrated with anecdotal, qualitative examples or small, non-comprehensive test sets; this offers limited insight into actual model performance. This highlights the need for scalable, dynamic benchmarking methods to enable fast and reliable model evaluation.\nTraditionally, benchmark creation involved designing data requirements and recruiting human annotators to provide test instances. Whilst ensuring quality, this process is costly, time-consuming, and difficult to scale. Previous work proposed methods for synthetic test data generation via prompt templates (Wang et al., 2024; Xia et al., 2024a; Yuan et al., 2024) or by using programmatic workflows for narrow domains (Zhu et al., 2024; Zhang et al., 2024b). These methods, however, are unable to easily generalize to a broader set of complex and generative tasks. Parallel efforts for training data synthesis have been proposed (Li et al., 2023b; Mitra et al., 2024; Li et al., 2023a). However, they do not usually transfer for generating evaluation datasets, due to stricter quality and diversity requirements.\nWe propose BENCHAGENTS, a multi-agent evaluation framework for automated, high-quality, and diverse benchmark creation. BENCHAGENTS breaks down benchmark creation into four components and instantiates each component via dedicated LLM agents as shown in Fig. 1. The Planning Agent creates a high-level plan/specification based on the problem and user requirements, breaks it down to tasks, and communicates the plan with the other agents. The plans may contain elements including but not limited to parameters and their values to guide the data generation and enable dis-aggregations along important dimensions, definitions for quality checks for data verification, and metrics for model evaluation.\nThe Data Generation Agent implements the plan programmatically and generates diverse benchmark data. Next, the Verification Agent formulates and executes fine-grained data quality checks to ensure quality control for the generated examples. Finally, the Evaluation Agent produces evaluation code and prompts for one or more metrics used for assessing target model performance. Grounding the data generation on a shared plan (that consists of a structured set of parameters) across agents enables precise control on the diversity of the data distribution. It also facilitates the creation of data quality checks and model evaluation criteria that are consistent with the initial plan. Whenever applicable, BENCHAGENTS by design allows for additional developer feedback at each stage of the process to ensure transparency, control and quality in the produced benchmarks.\nWe demonstrate the utility of BENCHAGENTS, by generating benchmarks on two complex and generative problem settings - calendar scheduling (BA-CALENDAR) and constrained long-form text generation (BA-TEXT) - each with 2,000 test instances. These are tasks where current benchmarks are lacking and state-of-the-art (SOTA) LLMs perform poorly. We then evaluate seven SOTA LLMs on both benchmarks. The generated benchmarks enable fine-grained dis-aggregations along multiple important dimensions, such as complexity. We find that (i) all LLMs struggle with joint constraint satisfaction across both datasets, with performance decreasing as the number of constraints increases; (ii) LLMs differ in their prioritisation of constraints when all cannot be met; and (iii) failures often involve constraints requiring numerical or logical reasoning.\nIn summary, our contributions are:\n\u2022 We introduce BENCHAGENTS\u2014a multi-agent framework which utilises interacting LLM agents to design and create benchmarks for complex and generative capabilities, while ensuring data and metric quality (\u00a7 3).\n\u2022 Using BENCHAGENTS, we create two diverse and high-quality benchmarks, BA-CALENDAR and BA-TEXT, to evaluate LLMs on two complex problems (\u00a7 4).\n\u2022 Evaluating seven SOTA LLMs on the two benchmarks, we offer insights on models' capabilities for generative and complex tasks (\u00a7 6)."}, {"title": "2 Related Work", "content": "A growing body of literature looks at leveraging algorithms and LLMs to automate parts of benchmark creation. This can be divided in two areas: dynamic benchmark creation for narrow domains, and extending existing benchmarks.\nDynamic Benchmark Creation: Zhang et al. (2024b) create a data generation algorithm that selects images and scene graphs from a corpus and generates input-output pairs based on question-answer templates for custom multi-modal evaluations. Yuan et al. (2024) propose AutoBench for aligning vision-language model evaluation, annotating images with question-answer pairs using LLMs for skill-based analysis. Zhu et al. (2024) design an evaluation data generation algorithm for reasoning tasks using graphs. These methods offer a dynamic way for users to produce fine-grained evaluation data based on pre-defined tasks. However, the data generators are manually designed, limiting generalisability and scalability.\nBenchmark Extension: Li et al. (2024) propose AutoBencher to optimize existing benchmarks to improve diversity and quality. It does so by generating question-answer pairs by proposing topics and retrieving relevant information from databases using an LLM. AutoBencher is apt for improving existing benchmarks, but is non-trivial to extend outside of question-answering domains to generative settings. Wang et al. (2024) present a multi-agent framework for dynamically augmenting benchmarks for scalability and robustness. Xia et al. (2024a) look at evolving existing coding benchmarks into different coding domains using LLM-based augmentation and verification with manual examination. Though dynamic and scalable, these approaches mandatorily require a seed dataset to bootstrap the process. Table 1 summarizes these comparisons."}, {"title": "3 Design of BENCHAGENTS", "content": "BENCHAGENTS automates benchmark creation for complex NLP tasks using LLM agents, while accommodating developer-in-the-loop (DIL) feedback. An LLM agent is defined as the combination of an LLM and an agent configuration (i.e., a set of prompts) provided by the developer or another agent. An LLM agent is specialised for a particular task in the workflow of benchmark creation.\nAt a high-level, BENCHAGENTS takes as input a description of the task to be evaluated and optionally a seed set of prompts representing the type of evaluation benchmark intended. BENCHAGENTS then uses multiple LLM agents\u2014Planning Agent for benchmark planning (P-AGENT: \u00a7 3.1), Data Generation Agent for instance generation (G-AGENT: \u00a7 3.2), Verification Agent for instance quality verification (V-AGENT: \u00a7 3.3) and Evaluation Agent for response evaluation (E-AGENT: \u00a7 3.4)-sequentially, for benchmark creation. The final output of BENCHAGENTS consists of (i) verified and diverse instances, and (ii) metrics to evaluate outputs of a (target) model on these benchmark instances. By dividing responsibilities across agents, BENCHAGENTS enables more precise debugging of the benchmark creation process.\nBENCHAGENTS follows a hybrid LLM and code execution approach to support automation. Such an approach is effective as some automation tasks are best handled by code (even if the code is generated by an LLM) while others are more easily managed through LLM calls. An overview of the framework is described in Fig. 1, considering the task of calendar planning as an example."}, {"title": "3.1 Planning Agent (P-AGENT)", "content": "Each instance BENCHAGENTS generates contains (i) a prompt to be used for (target) model evaluation, (ii) task-specific parameters, and (iii) constraints. Parameters are defined as variables on which a prompt is grounded. In Fig.1 box b, parameters include the number of participants or the earliest meeting start time. In contrast, constraints are defined as restrictions placed on the solution to a prompt. For example, in Fig.1 box b, the meeting duration constrains the space of possible to so- lutions to those with a specific meeting duration.\nP-AGENT takes a task description as an input and a set of optional seed prompts and proposes a plan for the other agents to execute. As shown in Fig.1 box b, for data generation, P-AGENT proposes and defines multiple parameters including the range and distribution of values for each pa- rameter. In addition to parameters, the plan also includes a set of constraints. After knowing the set of input parameters and query constraints, G- AGENT can then proceed with sampling the inputs from the parameters' range and the corresponding queries from the constraints' range (more details in \u00a7 3.2). This controlled sampling process is es- sential in ensuring benchmark data diversity.\nP-AGENT also guides V-AGENT by proposing a suite of quality checks that each instance in the benchmark should pass, including clarity, completeness, consistency, feasibility, and complexity (Fig.1 box c); more details are in \u00a7 3.3. At their core, these checks ensure that the gen- erated instances are exemplar representatives of the task, and that they can support reliable eval- uations. Finally, P-AGENT proposes evaluation metrics to E-AGENT for assessing the quality of model responses on the generated benchmark prompts (Fig.1 box d); more details are in \u00a7 3.4.\nUpon plan creation, developers can further steer the benchmark creation to better align it with their measurement goals by refining elements like parameters (and ranges), constraints, or metrics."}, {"title": "3.2 Data Generation Agent (G-AGENT)", "content": "G-AGENT transforms the plan into concrete benchmark instances (Fig. 1 box e). G-AGENT first generates the code needed for parameter sampling given the ranges from P-AGENT (Fig. 1 box b). It then adapts the code to sample and apply the constraints in the plan. G-AGENT then gener- ates the instance prompt by grounding on parame- ters and constraints. This supports augmenting the prompt to ensure semantic equivalence but syntactic diversity."}, {"title": "3.3 Verification Agent (V-AGENT)", "content": "V-AGENT analyses generated instances and evaluates their fitness for use in the benchmark. To do so, it uses the following quality checks by generating code for programmatic checks or by generating prompts for model-based checks:\n1. Clarity: The prompt should be understandable and unambiguous to developers and target models.\n2. Completeness: The prompt should contain all the listed constraints. For the example in Fig. 1, meeting duration should be present in all prompts.\n3. Consistency: When a parameter or constraint is realised in the prompt, the value should be consistent. For the example in Fig. 1, the \u201cnumber of participants\" parameter should be consistent with the number of participants in the schedules.\n4. Feasibility: The constraints should define a feasible problem. For e.g, in Fig. 1, a common time slot should exist that satisfies all constraints.\n5. Complexity: The constraints should be associ- ated with a measure of how challenging they make the problem. To capture this, a task-specific metric should be defined by the P-AGENT. For the example in Fig. 1, the metric involves the ratio of feasible slots to all slots.\nFor the example in Fig. 1, box f highlights the quality checks by V-AGENT for verifying feasibility in calendar scheduling using the specification provided by P-AGENT in box c. The generated code (used for verification) is manually reviewed for correctness. The same is done for the model- based verification methods."}, {"title": "3.4 Evaluation Agent (E-AGENT)", "content": "E-AGENT evaluates the solutions generated by target models. This is necessary in generative set- tings since we cannot always simply compare a so- lution to ground truth. E-AGENT operates based on the evaluation metrics defined by P-AGENT, which are all grounded on the set of constraints present in the plan. More specifically, for each constraint, there exists an evaluation metric that marks whether the constraint was satisfied or not (pass vs. fail) by a proposed solution. E-AGENT can implement both model-based and program- matic metrics. Developers can choose either of the options and mark their preference in the plan. For example, for calendar planning we generated both options and decided to go with programmatic met- rics as they were fully implementable for this task. For the example in Fig. 1, E-AGENT is required to check if the solution conforms to the duration con- straint (box g). To achieve this, it generates code to extract and check the duration from the solution, given access to parameters and constraints (box d).\nIn addition to metrics associated to a single con- straint, E-AGENT also computes the fraction of constraints satisfied from the whole list of con- straints (i.e., \"fraction passed\") as well as whether all constraints were satisfied (i.e., \"pass all\")."}, {"title": "4 Benchmark Generation", "content": "Next, we describe how we leveraged BENCHA- GENTS to generate benchmarks for two challenging tasks: calendar scheduling and constrained long-form text generation. For all agents, we use GPT-40 as the LLM model and the specific agent configurations are reported in Appendix A."}, {"title": "4.1 Calendar Scheduling (BA-CALENDAR)", "content": "Calendar scheduling is an important task that is relevant for several calendar and mail applications. In addition, it also constitutes a domain where planning and reasoning are important. Previous work (Zheng et al., 2024) has proposed initial benchmarks on the task (NATURALPLAN) but the scheduling part of the benchmark was shown to be saturated in evaluations of the ol-preview model (Valmeekam et al., 2024), with the majority of instances containing only two participants and one day of the week (see Appendix C). Therefore, we generate BA-CALENDAR that simulates a challenging and closer to real-world setting, where the problem involves more constraints.\nAs part of the plan, P-AGENT proposes (i) various parameters including the number of participants, number of days with availability, days of week, and (ii) multiple constraints like each participant's availability, required meeting duration, buffer times. G-AGENT writes code for generating diverse data based on P-AGENT's proposed parameters and constraints. Fig. 8 (in Appendix A.1) shows an example prompt from a gener- ated instance. V-AGENT initialises (i) model- based checks for clarity, completeness, and consistency, and (ii) programmatic checks for feasibility. Further, V-AGENT also implements a task- specific programmatic check for constrainedness as a measure of complexity: the inverse ratio of number of feasible solutions to number of time slots where at least one participant is available. Finally, E-AGENT initialises programmatic metrics for the satisfaction of each constraint defined by P-AGENT. For a full list of parameters and con- straints along with details of each agent's implementation see Appendix A.1, where we also differentiate between what was provided by the developer and what was generated by the model."}, {"title": "4.2 Constrained Long-form Text Generation (BA-TEXT)", "content": "Constrained long-form text generation requires models to plan and produce a long response to a user query that meets all the constraints in the query. This capability is relevant for creative and technical writing, as important pillars of pro- ductivity applications. Existing datasets that aim to test these capabilities evaluate only on format constraints (Xia et al., 2024b), short-form solutions (Zhou et al., 2023) and include relatively simple constraints (Yao et al., 2023). In comparison, our BA-TEXT focuses on long-form generations with complex content-based constraints.\nP-AGENT proposes (i) parameters like user, role, task, and (ii) constraints including:\n\u2022 Positive constraints: inclusion of certain content like topics or entities in the generation.\n\u2022 Negative constraints: exclusion of content from the generation.\n\u2022 Positional constraints: inclusion at a specific position (e.g., paragraph) in the generation.\n\u2022 Sequencing constraints: inclusion of certain content in a specific sequence in the generation.\n\u2022 Conditional constraints: inclusion or exclusion based on some conditions.\n\u2022 Iterative constraints: any previously defined constraints applied iteratively.\nG-AGENT generates data generation code that aligns with P-AGENT's proposed parameters and constraints. An example prompt from a generated instance may be found in Fig. 11 (Appendix A.2). Amongst all quality checks from V-AGENT, the only programmatic check is for constrained- ness defined as number of constraints applied to total number of constraints. The model-based quality checks include clarity, completeness, consistency, and feasibility. Finally, E-AGENT initialises model-based metrics\u00b9 for the satisfaction of each constraint and topic consistency based on proposed metrics from P-AGENT. For a full list of parameters and constraint examples along with details of the data generation procedure, see Appendix A.2, where we also distinguish between what was supplied by the agent configuration, model and developer-in-the-loop feedback."}, {"title": "5 Benchmark Quality Assessment", "content": "To validate the quality and diversity of benchmarks produced by BENCHAGENTS, we conduct a quality assessment consisting of automatic and human based assessments."}, {"title": "5.1 Are Generated Instances High Quality?", "content": "We measure quality of the generated benchmark using two measures: (i) conformance with the verification checks specified in \u00a7 3.3, and (ii) coverage of important parameters useful for comprehensive evaluation. Table 2 shows the pass rate for each of these quality checks. We observe that the G-AGENT produces high-quality instances for both tasks, indicating good quality of our benchmark. We see higher quality for all criteria in BA- CALENDAR compared to BA-TEXT. For BA- TEXT, we note that the \u201cfeasibility\u201d criterion is substantially lower that the other criteria. On investigation, we find that the feasibility test fails sometimes due to conflicting constraints. For example, the G-AGENT has a tendency to gener- ate positional and sequencing constraints that con- tradict each-other. To control for quality, BEN- CHAGENTS excludes any instance from the final benchmark that fail any of the quality checks.\nWe discuss the coverage of various parameters for BA-CALENDAR in Appendix C, and for BA- TEXT in Appendix D. We observe that BENCHA- GENTS generates diverse instances ensuring superior coverage, even compared to manually designed benchmarks (see Appendix C)."}, {"title": "5.2 Are Model-based Checks Reliable?", "content": "Recall that for certain constraints and parameters, the V-AGENT employs model-based verification checks. We conduct a human assessment of the V-AGENT for the model-based checks to evalu- ate their reliability. In this study, for each generated dataset, we take 50 instances produced from the G-AGENT (before filtering by the V-AGENT). For each instance, we collect two human anno- tations for each model-based verification check performed (more details in \u00a7 4). For the ground truth, we consider an instance to pass a verification check only if both annotators mark it as passed."}, {"title": "5.3 Are Generated Instances Difficult?", "content": "While we did not explicitly optimize for difficulty, any good benchmark should not be trivial to solve. To ensure there is a range of difficulty over our task instances, we borrow insight from prior work (Abdin et al., 2023; Yuksekgonul et al., 2023) and aim to assess if the respective constrainedness metrics (refer \u00a7 4.1 and \u00a7 4.2) from the V-AGENT act as a reliable proxy for difficulty. We do so by comparing the average \"pass all\" from evaluating GPT-40 on both datasets.\nIn Fig. 2, we bucketize task instances by their constrainedness measures and report both the average \"pass all\" and number of task instances in each bucket. We observe a monotonic decrease in \"pass all\" as constrainedness increases for buckets with more than 10 task instances across both datasets. This suggests that adding constraints indeed increases difficulty."}, {"title": "6 Model Analysis", "content": "We evaluated the generated benchmarks on OpenAI ol-preview (OpenAI, 2024b), GPT-40 (\u041e\u0440\u0435- nAI, 2024a), Claude 3.5 (Anthropic, 2024), Gemini 1.5 Pro (Reid et al., 2024), Llama 3.1 70B and 405B (Dubey et al., 2024), and Mistral 2407 (MistralAI, 2024). For both datasets, we report (i) fraction passed: the fraction of constraints passed\u00b3 per instance and solution and (ii) pass all: whether the solution satisfies all constraints. We discuss salient findings below. Note that such findings are only possible due to the diverse set of parameters and constraints found and supported by BENCHAGENTS."}, {"title": "6.1 Model Performance", "content": "Insight 1. Models struggle to satisfy multiple constraints simultaneously. Fig. 4a and Fig 4d show that the \"fraction passed\" rate across all models is always substantially higher than the \"passed all\" rate. Most models see a drop of nearly 50% in performance when comparing the two metrics, showing that while they can satisfy some constraints in the query, reliably satisfying all of them is a challenge. Of all models evaluated, o1-preview shows the smallest gap between the two metrics for BA-CALENDAR showing progress in this space with improved focus on reasoning and planning. However, ol-preview also struggles with satisfying all constraints in BA-TEXT.\nInsight 2. Models' prioritisation of constraint satisfaction varies. For BA-CALENDAR, in Fig. 4b, we observe that in contrast to other strong models, o1-preview, Gemini-1-5-Pro and Mistral-2407 have lower performance with respect to simple constraints such as meeting duration or time re- strictions. On further inspection, we see that these models are the most likely to respond with \u201cno solution exists\" for both feasible and infeasible instances (Fig. 5c). We observe that for challenging problems, these models choose to not give a solution while other models provide a solution that meets some constraints. In these cases, other models might prioritise simpler constraints like meeting duration compared to availability.\nInsight 3. Bigger models are better at joint constraint satisfaction for tasks with strict criteria. For BA-CALENDAR and models with known size, in Fig. 4a, we observe \u201cpass all\" increases w.r.t. model size for Llama 3-1-70b, Mistral-2407 and Llama 3-1-405b, as expected. However, this trend is not consistent with BA-TEXT in Fig. 4d where we observe Mistral-2407 outperforming Llama 3- 1-70b and Llama 3-1-405b, and matching perfor- mance of GPT-40. We posit this difference to the more ambiguous and open-ended nature of BA-TEXT; BA-CALENDAR has well defined and stricter satisfaction criteria, allowing large models to plan and execute the task well.\nInsight 4. As the complexity in a query increases, individual and joint constraint satisfaction performance decreases. This is observed in Fig. 5a and Fig. 5b for joint constraint satisfaction and Fig. 19 (in Appendix G) for individual constraint satisfaction. All models have very similar trends in pass rate at different constrainedness levels for BA-TEXT. Here, even strong reasoning models like ol-preview struggle at high levels of constrainedness. The trend while downward is more varied for BA-CALENDAR with ol-preview performing more similarly at different constrained- ness levels. These results on performance w.r.t. constrainedness are consistent with findings in the literature; Yao et al. (2023); Abdin et al. (2023); Yuksekgonul et al. (2023) also find, when text generation tasks incorporate more constraints, GPT-4 only partially satisfies them. Similarly, in Fig. 20 (Appendix G), model performance monotonically decreases with the number of days and the number of participants. This finding is consistent with prior work (Zheng et al., 2024) showing the challenge with increasing complexity in search space."}, {"title": "6.2 Model Failures", "content": "Insight 5. Numerical and logical reasoning requiring state tracking is challenging for most mod- els. In Fig. 4b and Fig. 4e respectively, we observe lower performance for constraints that involve logical reasoning such as availability, buffer time and priority for BA-CALENDAR and conditional, iterative and sequencing constraints for BA-TEXT. While o1-preview's performance exceeds other models with respect to these con- straints (highlighting the importance of inference- time compute scaling (Snell et al., 2024)), we see that its performance on these constraints is still low indicating further room for improvement.\nInsight 6. Most models struggle with recognizing infeasible problems. In Fig. 5c we see that most models have less than 40% accuracy in correctly identifying infeasible problems and responding with a correct \"no solution\" response. Here, o1-preview improves over prior models significantly pushing the rate to 70%. This shows that other models are strongly inclined to always respond with a solution even when no solution exists.\nInsight 7. All models struggle with specific combination of constraints. Comparing Fig. 4e and Fig. 4f, we see that while model performance on positional and sequencing constraints individually is higher than others, all models struggle with the combination of the two where the best model performance is less than 60%. Similarly, models also jointly struggle with negative and positional constraints, especially smaller models like Mistral 2407. In BA-CALENDAR, for all tasks involving priority, performance is consistently low, potentially since priority itself is a challenging constraint. We pose that the performance gap for certain constraint combinations is due to ambiguity in how these constraints should be applied together."}, {"title": "7 Conclusion", "content": "We introduce BENCHAGENTS, a framework for automatic benchmark creation, which employs multiple LLM agents to interact with each other and developers to create high-quality, diverse and challenging NLP benchmarks. BENCHAGENTS reduces developer effort while maintaining an ap- propriate level of human oversight. Further, the hybrid LLM and code execution approach enables flexibility to adapt to the generation of new com- plex and generative datasets in an efficient and controllable way. In addition, BENCHAGENTS enables the computation of dis-aggregated eval- uation metrics. We highlight the advantage of this by producing two new benchmarks to evaluate planning and constraint satisfaction capabilities of models and present insights of the common failure models across seven SOTA models."}, {"title": "Limitations", "content": "LLMs for Planning: P-AGENT uses an LLM to interpret user requirements and generate plans. To this end, the LLM may misinterpret ambiguous developer instructions. Further, the quality of the plan depends on the training data of the LLM, which may not have sufficient knowledge about the task to generate high-quality plans. Thus, the plan may not fully capture the developer's intended application. However, we highlight that in BENCHAGENTS the develop reviews and edits all plans before they are passed to the other agents, ensuring that the plans meet the desired use case.\nLLMs for Initialisation: G-AGENT, V-AGENT and E-AGENT all use LLMs to initialise the respective data generation, verification, and evaluation processes with code and prompts for future LLM calls. This relies on the LLM to have coding capabilities and to understand user intent from the plan passed to it from the P-AGENT in order to write high-quality prompts. Again, we emphasise that the DIL reviews and edits these code and prompts before execution for quality control.\nLLMs for Instance Generation: While executing the data generation processes, G-AGENT makes LLM calls, for example, in flexible tem- plate creation to generate task prompts from sets of parameters and constraints. Here, there is risk of hallucination and other inconsistencies between the desired parameters and constraints and the content of the prompt. This compounds the need for a verification process to ensure completeness and consistency between the prompt and the parameters and constraints. Further, the LLM may produce prompts that are unclear or infeasible, again highlighting the need for quality checks for clarity and feasibility. Finally, there is a risk that using LLMs for instance generation will result in instances that are not challenging enough since we use LLMs to create the data that we then evaluate them on. However, it is important to note the compositional nature of the data generation where BENCHAGENTS uses both LLM calls and code execution, with DIL for the initialisation stage. Our results show that both datasets we create are in fact challenging across all models. Further, since V-AGENT computes a metric for complexity, BENCHAGENTS supports filtering by complexity to ensure instances in the benchmark are sufficiently challenging.\nLLMs for Model-based Quality Checks and Evaluation Metrics: BENCHAGENTS supports the use of model-based quality checks via the V- AGENT and evaluation metrics via the E-AGENT. There is a growing body of literature on the effectiveness of using LLM-as-Judge (Zheng et al., 2023; Verga et al., 2024). There is a risk that LLMs provide inaccurate and/or biased responses resulting in low quality instances in our datasets and/or inaccurate evaluation metrics. However, we highlight that we conduct a human assessment of both the V-AGENT model-based quality checks and the E-AGENT evaluation metrics for both of our generated datasets (see Appendix E) and any further innovations to improve the reliability of model-based evaluations can be directly integrated into our framework.\nComputational Costs: Running multiple LLM agents in BENCHAGENTS can be computationally expensive. As a result, the overall computational cost is higher than synthetic data generation frameworks that rely solely on code execution or single inference calls. This could limit the accessibility to developers with limited computational resources. Research on eliciting multi-agent capabilities with smaller models would improve the cost and efficiency of the entire framework.\nSingle LLM Used in Agents: For the generation of BA-CALENDAR and BA-TEXT, we use GPT-40 as the model in all our LLM agents. However, BENCHAGENTS is flexible and the compositional nature enables the use of different mod- els for different agents. Again, we highlight that this is not a limitation of the framework but an implementation decision given the nature of the two generated benchmarks and resource availability. Future work can explore the benefit of using different LLMs for different agents.\nEthical Considerations\nThe aims of BENCHAGENTS are to assist in the automation and scalability of benchmark creation. However, since LLMs can hallucinate or misinterpret instructions, caution should be taken not to use model outputs without verification. BENCHAGENTS incorporates DIL feedback such that developers check the intermediate steps of the benchmark creation process and ensure the resulting benchmark meets the desired user requirements."}, {"title": "Appendix A More Details on Benchmark Generation (\u00a7 4)", "content": "A.1 BA-CALENDAR\nFigure 6 displays the task description for BA-CALENDAR from the agent configuration.\nQuestions will include an availability schedule for participants and additional constraints to increase difficulty.\nThe goal is to find one day and common time slot when all participants are available. Schedules will be provided as a dictionary with participants as keys and availability schedules as values. Each schedule will be a dictionary with days of the week as keys and a list of time blocks as values. For example:\navailability = {\n\"p1\": {\n},\n\"Monday\": [9:00-12:00, 14:00-17:00],\n\"Tuesday\": [10:00-15:00]\n},\n\"p2\": {\n},\n\"Monday\": [15:00-18:00],\n\"Tuesday\": [09:00-12:00, 14:00-17:00]\n},\n\"p3\": {\n\"Monday\": [10:00-11:00, 13:00-16:00, 18:00-19:00],\n\"Tuesday\": [11:00-14:00]\n}\n}\nA time block will be a string in the format \"start_time-end_time\" where a participant is available. A time slot refers to a single time block of length meeting duration. Granularity refers to the start and end times we consider for blocks. For example, if granularity is 30 minutes, we consider 9:00, 9:30, 10:00, etc. Scheduling parameters refers to the parameters used to generate availability schedules such as Minium length of time block etc. Constraints refer to additional conditions that must be met for any correct solution slot such as meeting duration, etc. Constraints will always have a default value of None, False or 0. For evaluation, solutions will be in the format: \"[day] [start_time]-[end_time]\" or \"No common time slot available\".\nA.1.1 P-AGENT Plan\nParameters:\nMinimum length of time block: 15, 30, 45, 60 minutes\nMaximum length of time block: 60, 90, 120, 180, 240 minutes\nNumber of participants: 2-10\nNumber of days in the schedule: 1-7\nMaximum number of time blocks per day: 1-5\nMinimum number of time blocks per day: 1-5\nEarliest start time: 6:00 7:00 8:00, 9:00\nLatest end time: 17:00, 18:00, 19:00, 20:00\nConstraints:\nMeeting duration: 15, 30, 45, 60, 90, 120 minutes\nBuffer time before and after meeting: None, 5, 10, 15, 30 minutes\nNo meetings on weekends: True, False\nNo meetings before: None, 8:00, 9:00, 10:00\nNo meetings after: None, 17:00, 18:00, 19:00\nHigh priority meeting (must be scheduled in first available slot): True, False\nNo meetings during specific times: None, 12:00-13:00, 16:00-17:00"}, {"title": "A.1.2 BA-CALENDAR: Parameters & Constraints", "content": "A.1.3 Data Generation with G-AGENT\nAs described in \u00a7 3.1, P-AGENT takes the agent configuration (see Appendix A.1) as input and generates a plan for data generation (see Appendix A.1.1). Note, that the DIL may update the plan. The plan includes a list of parameters and constraints, along with the range of values each may take. Tables 4 and 5 contains the full list of parameters and constraints from the plan along with whether each was provided by the agent configuration, LLM or DIL.\nThe G-AGENT takes the agent configuration and data generation plan as input and writes the code to initialise the data generation procedure. The data generation procedure is initialised as follows. First, the G-AGENT writes functions, sample"}]}