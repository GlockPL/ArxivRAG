{"title": "From a Tiny Slip to a Giant Leap: An LLM-Based Simulation for Fake News Evolution", "authors": ["Yuhan Liu", "Zirui Song", "Xiaoqing Zhang", "Xiuying Chen", "Rui Yan"], "abstract": "With the growing spread of misinformation online, research has increasingly focused on detecting and tracking fake news. However, an overlooked issue is that fake news doesn't naturally exist in social networks-it often originates from distorted facts or deliberate fabrication by malicious actors. Understanding how true news gradually evolves into fake news is critical for early detection and prevention, reducing its spread and impact. Hence, in this paper, we take the first step toward simulating and revealing this evolution, proposing a Fake News evolution Simulation framEwork (FUSE) based on large language models (LLMs). Specifically, we employ LLM as agents to represent individuals in a simulated social network. We define four types of agents commonly observed in daily interactions: spreaders, who propagate information; commentators, who provide opinions and interpretations; verifiers, who check the accuracy of information; and bystanders, who passively observe without engaging. For simulated environment, we model various social network structures, such as high-clustering networks and scale-free networks, to mirror real-world network dynamics. Each day, the agents engage in belief exchanges, reflect on their thought processes, and reintroduce the news accordingly. Given the lack of prior work in this area, we developed a FUSE-EVAL evaluation framework to measure the deviation from true news during the fake news evolution process. The results show that FUSE successfully captures the underlying patterns of how true news transforms into fake news and accurately reproduces previously discovered instances of fake news, aligning closely with human evaluations. Moreover, our work provides insights into the fact that combating fake news should not be delayed until it has fully evolved; instead, prevention in advance is key to achieving better outcomes. We hope our work inspires further research and responsibility in the early detection and prevention of fake news in social networks.", "sections": [{"title": "1 INTRODUCTION", "content": "The rapid spread of fake news has become a significant global concern, with social media accelerating the production, consumption, and propagation [25, 41]. Addressing the evolution of fake news is therefore critical to mitigating its far-reaching effects and is particularly important for fostering a responsible web environment [1, 2, 46]. Most existing studies on fake news focus on detecting misinformation or simulating its spread once it has already been generated [15, 57]. For instance, Piqueira et al. [44] categorized individuals into four types and used mathematical models to simulate the spread of fake news, as depicted in Figure 1(a), showing the dynamics of different actor populations over time. On a micro-level, Jalili and Perc [18] defined numerical conditions for opinion change to study fake news dissemination, as shown in Figure 1(b). The above work follows a key assumption common in much of the existing research: fake news inherently exists within social networks.\nHowever, a significantly overlooked issue in these works is that fake news does not naturally exist within social networks. In reality, it may originate from a piece of true news that, over time, becomes distorted, misinterpreted, or altered as it spreads among individuals,"}, {"title": "2 RELATED WORK", "content": "2.1 Fake News Evolution\nRecent research into fake news evolution has focused on how misinformation spreads and transforms over time. Zhang et al. [63] found that rumors evolve as they are repeatedly modified, becoming shorter and more shareable, while Guo et al. [16] empirically tracked fake news evolution, noting how sentiment and text similarity change as truth transitions into misinformation. Xia et al. [60] developed a sentiment analysis pipeline that addresses sarcasm and out-of-domain input, helping track shifts in public opinion related to fake news. Other studies have emphasized structural and behavioral aspects of fake news propagation. Zhao et al. [64] proposed a dynamic learning method that captures temporal changes in rumor propagation, revealing how rumor patterns evolve. Wang et al. [55] demonstrated that slight content changes during the COVID-19 pandemic aided the spread of misinformation, while Li et al. [26] examined how user behaviors, particularly the role of verified accounts, influence the evolution and dissemination of rumors. These studies underscore the dynamic nature of fake news evolution and the need for comprehensive approaches.\nHowever, there has not been a detailed and comprehensive study on how true news evolves into fake news, with only some superficial linguistic analyses [16, 63].\n2.2 Simulations of Fake News Propagation\nFake news propagation modeling plays a vital role in understanding the dynamics of misinformation spread and is key to developing intervention strategies such as early warning systems [15], misinformation blocking [50], and truth confrontation [57]. Current models can be broadly categorized into three types: epidemic models, point"}, {"title": "3 METHODOLOGY", "content": "3.1 Problem Formulation\nIn this study, we aim to simulate the gradual evolution of true news into fake news within a social network by leveraging LLMs as agents. We construct a simulation environment with a pool of N agents A = (a1, ..., an), each endowed with a unique persona that includes their role type (spreader, commentator, verifier, or bystander), personality traits, demographic information, and initial opinion toward a news topic.\nAt time t = 0, a piece of true news So is introduced into the network. The agents are connected according to a predefined social network structure, G = (A, E), which may represent high-clustering, scale-free, or random networks to reflect real-world dynamics. On each day t = 1,2,..., T, agents interact with their neighbors as defined by the network structure. During these interactions, they exchange information and opinions, influenced by their personas and prior knowledge. After interactions, agents reintroduce the news content considering the new information received. The evolution of the news content held by agent a\u00a1 at time t, denoted as S, depends on their previous news content and the information received from their neighbors. This update process is defined by:\nS_i^t = f(S_i^{t-1}, \\{S_j^{t-1}|a_j \\in N_i\\}, P_i), (1)\nwhere f() represents the agent's information processing function, influenced by their persona Pi and the interactions with neighboring agents Ni.\nOur objective is to analyze how the true news So transforms over time due to the agents' interactions and personal biases. This simulation allows us to research the roles of different agent types, network structures, news topics, and individual personality traits in evolution. By tracking the changes in S, we can quantify the deviation from the original news and understand the factors influencing the transformation into fake news."}, {"title": "3.2 Our Simulation Framework", "content": "As depicted in Figure 2, our FUSE framework integrates two core components: the Propagation Role-Aware agents (PRA) and the News Evolution Simulator (NES). The PRA module equips each agent with role-based decision-making capabilities, enabling them to process and disseminate information according to their assigned roles and personal attributes. The NES constructs the interaction environment, simulating the social network through which news propagates and evolves. Within the PRA module, each agent is powered by an LLM and is assigned a specific role type along with attributes. It influences how agents interpret information, interact with others, and update their opinions. Daily interactions are facilitated by the NES, which determines which agents interact based on the predefined social network structure, G = (A,E). The NES simulates various network types. During each day, agents engage with their neighbors, exchanging news content and opinions influenced by their roles and personal attributes. In scenarios where interventions are introduced-such as official announcements after the deviation level of news reaches a set threshold, aiming to correct misinformation and release credible information sources to relevant agents. At the end of each day, the simulation progresses by a one-time step, and the agents' states are updated.\n3.3 Propagation Role-Aware Agent\nThe PRA is the core component of our FUSE framework, designed to simulate individual human behaviors in the context of news evolution. By equipping agents with specific roles and personal attributes, we aim to closely mimic the diversity and complexity of human interactions in social networks.\n3.3.1 Personal Information. According to Sun et al. [51], they classify the roles in fake news propagation into four types: spreaders, who propagate information; commentators, who provide opinions and interpretations; verifiers, who check the accuracy of information; and bystanders, who passively observe without engaging. however, they failed to model this in their numerical simulation."}, {"title": "3.3.2 Role-Specific Behaviors.", "content": "At each time step ti, agent a\u00a1 holds a version of the news content S. When interacting with neighboring agents Ni as defined by the network G, agent a\u00a1 receives news content {S-1|aj \u2208 Ni}. The agent then reintroduces news based on their role and persona through a role-specific update function:\nf_{role} = f_r(S_i^{t-1}, \\{S_j^{t-1}|a_j \\in N_i\\}, P_i). (2)\nFor the role in our model, spreaders may combine and amplify sensational aspects of the news, commentators may add personal interpretations or opinions, verifiers may cross-reference information before accepting or sharing it, and bystanders typically retain their previous news content S = S-1 unless significantly influenced [51]. The prompt for the role-specific reintroduction function fr, is in Appendix A.\n3.3.3 Memory and Reflection. In our simulation, agents engage with their neighbors each day, leading to an updated version of the news. However, owing to the potentially vast volume of interactions, storing all of them in detail is impractical. Therefore, agents are equipped with a hierarchical memory system comprising short-term memory (STM) M and long-term memory (LTM) M. The STM stores recent interactions and immediate information received, while the LTM accumulates knowledge over time, influencing future information processing and news updates. After interactions, agents reflect and reintroduce the news through a memory consolidation function:\nM_i^t = g(f_L(M_i^{L, t-1}), f_S(M_i^{S, t})), (3)"}, {"title": "3.3.4 Decision-Making Process.", "content": "In our FUSE framework, each agent's opinion evolves through a reasoning process influenced by their role, persona, and interactions with others. After daily interactions and memory updates, agents reflect on the news content they hold, leading to gradual changes in their opinions. This process integrates the agent's role in propagation-whether they are a spreader, commentator, verifier, or bystander-with their cognitive functions. The decision-making process for agent a\u00a1 at time t is modeled as:\nS_i^t = f_{am}(S_i^{t-1}, M_i^{t-1}, r_i, P_i). (4)\nThe function prompt fam is in Appendix A, which captures how agents integrate new information with their existing opinions, taking into account their role in the decision-making process. For example, the reasoning of spreaders may lead to greater changes in S. Commentators often add subjective nuances to the news content through their reasoning. Verifiers are aiming to correct inaccuracies and potentially reduce misinformation. Bystanders have minimal engagement; their reasoning leads to minor or no changes in their held content.\n3.4 News Evolution Simulator\nThe News Evolution Simulator (NES) serves as the core environment where news content propagates and evolves over time through"}, {"title": "3.4.1 Intervention Mechanisms.", "content": "A critical feature of the NES is its ability to simulate interventions aimed at countering the evolution of fake news, thereby contributing to the development of a more responsible web environment. When the degree of deviation between the current news content S and the original news So surpasses a predefined threshold, an official agent is introduced into the system to correct misinformation. This official agent represents authoritative sources that provide verified information, to mitigate the spread and impact of distorted or false narratives.\nThe intervention process starts with continuous monitoring of the deviation between the news content held by each agent and the original news. Once the deviation exceeds a critical threshold, the official agent is triggered to take action. This agent issues official announcements based on reliable sources, targeting agents most likely to propagate or exacerbate misinformation. The prompt for the official agent is in Appendix A."}, {"title": "4 FUSE-EVAL: News Evolution Analysis", "content": "To systematically measure how true news evolves into fake news within our simulation, we propose a comprehensive evaluation framework named FUSE-EVAL. This framework consists of two sets of metrics: Content Deviation Metrics and Statistical Deviation Metrics, which together provide a detailed understanding of how misinformation evolves within the simulated environment.\n4.1 Content Deviation Metrics\nThe Content Deviation Metrics assess the deviation of the news content across multiple dimensions by quantifying changes in specific aspects of the news. FUSE-EVAL evaluates the news content based on six key dimensions. (1) Sentiment Shift (SS) measures the change in emotional tone between the original news content and its evolved version [33, 35]. Sentiment plays a crucial role in how information is perceived and shared; shifts in sentiment can indicate the introduction of bias or emotional manipulation. (2) New Information Introduced (NII) assesses the extent to which additional information, not present in the original news, has been incorporated during the evolution process [4, 56]. The introduction of new facts, details, or claims can significantly alter the original"}, {"title": "4.2 Statistical Deviation Metrics", "content": "The Statistical Deviation Metrics are derived from the Total Deviation scores and provide insights into the overall trends and patterns of news evolution within the network. We then analyze several statistical metrics based on the TD. The ADeviation represents the difference in Average Deviation between the final and initial day of the simulation, highlighting the overall growth in deviation throughout the simulation period. The Average Deviation is the mean of TD across all agents at each time step, providing insight into the general trend of news evolution within the network. The Deviation Variance measures the statistical variance of TD among agents, indicating the degree of dispersion and how uniformly the news content deviates across the network. The Final Deviation is the average TD at the final simulation time step t, representing the cumulative effect of news evolution over the entire simulation duration. The Maximum Deviation and Minimum Deviation refer to the highest and lowest average TD observed during the simulation, respectively. These metrics indicate the points at which the news content has deviated most and least significantly from the original. The Peak Deviation Time indicates the percentage of the total simulation time taken to reach the Peak Deviation Rate. It provides insight into how quickly the maximum deviation occurs"}, {"title": "4.3 Implementation Details", "content": "Our simulation framework was developed using Python scripts, leveraging various libraries to model the agents and their environment effectively. The LLM used is gpt-40-mini, accessed via OpenAI API calls. When creating the network structure, we used the Python library networkx to construct different social network structures. The simulation includes 40 agents, whose traits were based on the Big Five personality dimensions commonly used in psychology [5]. Each agent was assigned scores on these traits to introduce variability in behaviors and interactions within the simulation. For further details, please refer to our code. We ensured that our framework is adaptable to different language models and have provided details on API costs and compatibility with other models in Appendix E and F.\n5 Validation of the FUSE Framework"}, {"title": "5.1 Alignment with Real-World Patterns", "content": "5.1.1 Topic Comparison. We compared the evolution of fake news across five topics: politics, science, finance, terrorism, and urban legends. Table 1 shows that political fake news spreads the fastest, with average deviation peaking rapidly within four days and remaining high. Terrorism follows a similar pattern. In contrast, financial and science-related fake news spread more slowly, with science-related misinformation evolving the slowest and average deviation remaining consistently low. Table 1 shows the final deviation for political news is approximately 90% higher than that of science news. These results indicate that political fake news is more prone to rapid distortion and widespread belief, while science-related misinformation spreads more cautiously, consistent with prior research [25].\n5.1.2 Social Network Comparison. Using a terrorism topic, we compared fake news evolution across different social network structures: random, scale-free, and high-clustering networks. Table 1 shows that high-clustering networks exhibit the fastest and most extensive spread of misinformation, with deviation peaking rapidly and remaining high. This indicates that tightly connected communities are particularly susceptible to rapid belief distortion, aligning with the \"echo chamber\" effect [10]. Random networks show the slowest evolution of fake news, with lower variance suggesting individuals are less likely to form polarized beliefs. Scale-free networks are intermediate. Peak deviation time is the longest in random networks and shortest in high-clustering networks, illustrating that clustering accelerates fake news evolution, consistent with prior research [29, 54].\n5.1.3 Spread Type Comparison. Using a terrorism topic, we compared fake news evolution across three spread types: normal spread,"}, {"title": "5.1.4 Personality Traits Comparison.", "content": "Using a terrorism topic, we compared the impact of personality traits on fake news evolution. Based on the Big Five personality traits [5], individuals with high agreeableness and neuroticism (Impressionable) are more likely to believe rumors, while those with low levels (Vigilant) are less susceptible. Table 1 show that impressionable agents are more prone to accepting and spreading misinformation. Vigilant agents exhibit stronger resistance, with beliefs remaining relatively stable. These results align with previous studies and demonstrate our framework's effectiveness in simulating the influence of personality traits on fake news spread.\n5.2 Alignment with Real-World Fake News"}, {"title": "6 ANALYSIS AND DISCUSSION", "content": "6.1 Ablation Study\nWe chose a terrorism topic to demonstrate the effectiveness of our model's components and conducted two ablation studies to evaluate the contribution of key components in the FUSE framework."}, {"title": "6.1.1 The Impact of Hierarchical Memory and Propagation-Role.", "content": "As shown in Figure 3(b), the full FUSE framework demonstrates a clear accumulation effect in deviation, indicating its effectiveness in simulating fake news evolution. After removing hierarchical memory, the deviation significantly drops, with a 39.8% reduction throughout the simulation. The absence of the accumulation effect indicates the simulation fails [45]. This highlights the critical role hierarchical memory plays in capturing the depth and persistence of belief distortion, as it enables agents to store and reflect on both short-term and long-term information. Similarly, removing the propagation role further decreases the deviation. This underscores the importance of agent roles (such as spreader, commentator, verifier, and bystander) in shaping the propagation and evolution of information. Without these roles, the agents behave more uniformly, and the accumulation effect of deviation disappears, meaning that the news does not evolve.\n6.1.2 The Impact of Propagation Role Types. After the first ablation study showed that removing the propagation role leads to simulation failure, we conducted a second study to assess the impact of different role types (spreader, commentator, verifier, and bystander) on fake news evolution. As shown in Figure 3(c), removing the commentator agent had the most significant impact, with a sharp drop in average deviation, leading to simulation failure. This confirms the central role of the commentator in amplifying and disseminating misinformation. In contrast, removing the spreader slightly reduced the deviation, as spreaders do not provide explanations or opinions, which are key drivers of news distortion but may still contribute to its evolution. Interestingly, removing the verifier increased deviation, as verifiers play a crucial role in checking the accuracy of information. Without verifiers, the system becomes more susceptible to misinformation, leading to a greater spread of fake news. Finally, removing the bystander had the least impact, as bystanders passively observe without actively participating in the information spread.\nOverall, these ablation studies demonstrate the effectiveness of FUSE. Hierarchical memory, propagation roles, and specific agent types play important roles in shaping the evolution of fake news within the FUSE framework."}, {"title": "6.2 Fake News Intervention", "content": "6.2.1 Intervention Strategy. Based on the previous experimental results, we chose to introduce the official agent at high-degree nodes. Table 1 presents the comparative experimental results between the no-intervention and intervention conditions. Since fake news evolution reached a high point on the sixth day, we implemented the intervention strategy, where the official agent released a statement.\nThe deviation under the intervention condition decreased by 37.8% compared to the no-intervention condition. This early intervention effectively blocked the initial spread of fake news, preventing a large number of agents from believing in the misinformation. However, as the news continued to propagate, the effectiveness of the intervention gradually weakened. By the 12th day, although the deviation under the intervention condition was still lower than that under the no-intervention condition, the gap narrowed to 22.3%, suggesting that the impact of a single intervention diminishes over time, as agents continue to interact and may revert to their previous beliefs or encounter conflicting information. Around the 16th day, we conducted a second intervention, which again reduced the deviation by 31.8% compared to the no-intervention condition. This further demonstrates the importance of continuous and timely fact-checks to suppress the spread of fake news. Compared to the no-intervention condition, the final deviation under the intervention strategy was reduced by approximately 28.6% and the variance in deviation was also reduced by 61.8%. Moreover, the peak deviation time under the intervention strategy occurred approximately 0.56-time units earlier than in the no-intervention case.\nOverall, the intervention strategy consistently maintained a lower average deviation than the no-intervention condition. The results highlight the importance of not only introducing early interventions but also maintaining regular interventions over a longer period to effectively combat the continuous spread and evolution of fake news."}, {"title": "6.3 Factors in Fake News Evolution", "content": "The analysis of experimental results and charts indicates varying contributions of different factors to fake news evolution. The Figure 5 (a) shows that PS contributes the most (22.3%), suggesting that altering reporting angles or distorting original information is the key driver of fake news evolution. NII follows with 18%, highlighting its significant role in this process. SS an STS contribute 17.2% and 17.5%, respectively, while TS has the smallest impact at 11%. The Figure 5 (b) reveals topic-specific patterns. Political and terrorism-related fake news evolves across multiple dimensions, especially new information, perspective, and sentiment shifts. In contrast, science-related fake news is driven mainly by new information, with less influence from temporal or style shifts. Urban"}, {"title": "7 CONCLUSION", "content": "We introduced the FUSE framework, a simulation environment that models the evolution of true news into fake news using agent-based modeling and LLMs. By implementing the FUSE-EVAL evaluation framework, which quantifies content deviation across six dimensions, we systematically analyzed how misinformation propagates and transforms within social networks. Our experiments demonstrated alignment with real-world observations across various settings, reinforcing existing theories on misinformation spread, such as the rapid distortion of political news, the influence of network clustering, the impact of super spreaders and emotional content, and the role of individual personality traits in susceptibility to fake news. By automating the evaluation process with LLMs like GPT-40-mini, our framework achieves scalable and consistent analysis. By enhancing our understanding of misinformation spread, our framework supports the development of a more responsible and trustworthy web environment."}, {"title": "A Prompt Set", "content": "Here, we present a detailed description of the prompts employed in our FUSE framework to model the dynamics of fake news evolution.\n1. The prompt for the role-specific reintroduction function fr, is as:\nfspr: share information quickly without verifying its accuracy.\nfcom: modifies or adds their views before sharing news.\nfuer: performs some verification before spreading news.\nfbys: consume news without participating in its dissemination.\n2. The prompt for Short-Term Memory function fs is as:\nSummarize the opinions you have heard in a few sentences, including their own perspective on the news.\n3. The prompt for Long-term memory function fL is as:\nReview the previous long-term memory and today's short-term summary. Please update the long-term memory by integrating today's summary, ensuring continuity and incorporating any new insights.\n4. The prompt for the reasoning function is as:\nAs a [role], you combine your [previous personal opinion] with the new information stored in your [long memory]. You process this information in the following manner: [role behavior], and then reintroduce the [news].\n5.The prompt for \"Official Statement\" is as:\nAccording to the current investigation, That [news] is true. We have noticed that some social media platforms and certain media outlets are spreading false information, claiming that [news] is fake. We firmly state that such claims are baseless. The government is committed to transparency and will provide timely updates on the investigation. We urge the public to seek accurate information from official channels, and necessary actions will be taken against those who intentionally spread false information.\nB Human Evaluation\nTo efficiently evaluate the deviation of news content across the multiple dimensions defined in FUSE-EVAL, we employ large language models (LLMs) to automate the assessment process. This approach provides consistent and scalable evaluations, reducing the reliance on time-consuming human evaluation. We utilize two versions of OpenAI's language models: gpt-3.5-turbo and GPT-4. For each agent's news content at various time steps, we prompt the LLMs to evaluate the six FUSE-EVAL dimensions by comparing the evolved content with the original news article, which is as follows:\n\u2022 Sentiment Shift (SS)\n\u2022 New Information Introduced (NII)\n\u2022 Certainty Shift (CS)"}, {"title": "C Alignment Between Simulated and Real-World Fake News", "content": "Additionally, our framework generates fake news narratives that closely mirror those found in the real world. This alignment validates the realism of our simulation and demonstrates its potential as a tool for studying misinformation dynamics. By producing content that reflects actual fake news, our framework enables researchers to better understand how such information originates and spreads, thereby aiding in the development of effective strategies to combat misinformation.\nThe specific case is as follows:\n\u2022 For terrorism topic, our framework generates fake news such as \"Trump was not attacked, it's a dramatic effect,\" which is also a widely circulated piece of fake news in the real world."}, {"title": "D Various Topics and Simulation Results", "content": "In our experiments, we compared the evolution of fake news across five different topics: politics, science, finance, terrorism, and urban legends. As shown in Figure 6(a), political fake news spreads the fastest, with average deviation rapidly peaking within just four days"}, {"title": "E Analysis of Experimental Costs", "content": "In this section, we analyze the costs associated with our experiments utilizing the GPT-40-mini APIs. At the time of our experiments, OpenAl's pricing model was as follows: for gpt-40-mini, the cost was 0.15 USD for every 1M input tokens and 0.6 USD for every 1M output tokens.\nOur simulations involved multiple agents interacting over several days, with each agent generating and processing textual content. For a simulation with 40 agents over 30 days, it involved approximately 3 to 5M input tokens and 5 to 10M output tokens. This resulted in an estimated cost of 4 USD to 8 USD for the entire simulation phase using gpt-40-mini combining both the simulation and evaluation phases.\nConducting comparable research in real-world settings typically involves significantly higher expenses. Real-world studies require funding for participant recruitment, compensation, data collection tools, infrastructure setup, and extended durations to gather and analyze data. Depending on the scale and scope, such studies can cost from several thousand to hundreds of thousands of dollars. By leveraging GPT-40-mini, we can simulate complex social interactions and the evolution of information without the logistical challenges and high costs associated with real-world experiments. This approach allows for rapid iteration and scalability, enabling us to explore various scenarios and intervention strategies efficiently. This cost analysis highlights the economic advantages of our simulation-based methodology-FUSE. The ability to conduct extensive experiments at a fraction of the cost demonstrates the practicality and accessibility of using LLMs for research in misinformation dynamics. It opens avenues for researchers with limited resources to contribute valuable insights into the field, fostering a more inclusive and innovative research environment.\nSocial networks in real life can generally be categorized into three types: high clustering networks, scale-free networks, and random networks, which correspond respectively to Figure 7 (a), (b), and (c)."}, {"title": "F Simulation on Different Backbones", "content": "To further validate the robustness and adaptability of our FUSE framework, we conducted additional experiments using different LLMs as the backbone. Specifically, we implemented simulations with both GPT-40-mini and GPT-4 to assess whether the choice of LLM affects the effectiveness of our framework."}, {"title": "G Social Network", "content": "High clustering networks are characterized by nodes that tend to form tightly knit groups or communities, where neighbors of a node are likely to be neighbors themselves. The degree of clustering can be quantified by the clustering coefficient Cv, which is defined for a node v as:\nC_v = \\frac{2T(v)}{k_v(k_v - 1)}, where T(v) is the number of triangles passing through node v and kv is the degree of v. The clustering coefficient for the whole network is the average of Cv over all nodes v.\nScale-free networks are characterized by a power-law degree distribution, where the probability P(k) that a randomly selected node has k connections to other nodes follows:\nP(k) \\sim k^{-\\gamma}, where \u03b3 is a parameter typically in the range 2 < \u03b3 < 3. This distribution implies that most nodes have few connections, while a few hub nodes have a large number of connections. This heterogeneity in node connectivity is a hallmark of scale-free networks.\nRandom networks, also known as Erd\u0151s-R\u00e9nyi networks, each edge is included in the network with a fixed probability p independent of the other edges. For a network with n nodes, the probability P(k) that a randomly selected node has k connections is given by the binomial distribution:\nP(k) = \\binom{n-1}{k} p^k (1-p)^{n-1-k}. For large n, this can be approximated by the Poisson distribution:\nP(k) \\approx \\frac{\\lambda^k e^{-\\lambda}}{k!}, where \u03bb = p(n - 1) is the expected degree of a node. These three types of networks are used in the environment simulation of news evolution within our FUSE framework."}]}