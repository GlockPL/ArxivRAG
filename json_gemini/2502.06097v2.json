{"title": "NLGR: Utilizing Neighbor Lists for Generative Rerank in Personalized Recommendation Systems", "authors": ["Shuli Wang", "Xue Wei", "Senjie Kou", "Chi Wang", "Wenshuai Chen", "Qi Tang", "Yinhua Zhu", "Xiong Xiao", "Xingxing Wang"], "abstract": "Reranking plays a crucial role in modern multi-stage recommender systems by rearranging the initial ranking list. Due to the inherent challenges of combinatorial search spaces, some current research adopts an evaluator-generator paradigm, with a generator generating feasible sequences and an evaluator selecting the best sequence based on the estimated list utility. However, these methods still face two issues. Firstly, due to the goal inconsistency problem between the evaluator and generator, the generator tends to fit the local optimal solution of exposure distribution rather than combinatorial space optimization. Secondly, the strategy of generating target items one by one is difficult to achieve optimality because it ignores the information of subsequent items.\nTo address these issues, we propose a utilizing Neighbor Lists model for Generative Reranking (NLGR), which aims to improve the performance of the generator in the combinatorial space. NLGR follows the evaluator-generator paradigm and improves the generator's training and generating methods. Specifically, we use neighbor lists in combination space to enhance the training process, making the generator perceive the relative scores and find the optimization direction. Furthermore, we propose a novel sampling-based non-autoregressive generation method, which allows the generator to jump flexibly from the current list to any neighbor list. Extensive experiments on public and industrial datasets validate NLGR's effectiveness and we have successfully deployed NLGR on the Meituan food delivery platform.", "sections": [{"title": "1 Introduction", "content": "E-commerce platforms, such as Meituan and Taobao, need to provide users with personalized services from millions of items. To improve recommendation efficiency, personalized recommendation systems generally include three stages: matching, ranking, and reranking. The ranking models (e.g.,Wide&Deep [10], DeepFM [14], DIN [35]) evaluate the point-wise items respectively based on the Click-Through Rate (CTR), but they ignore the crucial mutual influence among items [7, 27]. Research [1, 5, 6, 25] indicates that optimizing a list-wise utility is a more advantageous strategy, as it capitalizes on the mutual influences between items within the list to enhance overall performance.\nThe key challenge of reranking is to explore the optimal list in the huge combinatorial space [21, 28]. Initially, some list-wise methods [2, 26, 36] re-evaluate and score items within lists by modeling the context. These list-wise methods can obtain more accurate scores than point-wise methods, then they use a greedy strategy to reorder based on the list-wise score. However, these methods face the evaluation-before-reranking problem [13, 32] and cannot achieve optimization in combinatorial space. To resolve the problem, a straightforward solution is to evaluate every possible permutation, which is global-optimal but is too complex to meet"}, {"title": "2 Related Work", "content": "Typical reranking methods can be divided into two categories [29]. The first category is the one-stage reranking methods, which only generates one list as output by capturing the mutual influence among items. Seq2slate [4] utilizes pointer-network and MIRNN [37] utilizes GRU to determine the item order one-by-one. Methods such as PRM [26] and DLCM [1] take the initial ranking list as input, use RNN or self-attention to model the context-wise signal, and output the predicted value of each item. Such methods bring an evaluation-before-reranking problem [32] and lead to suboptimal. Similarly, methods such as EXTR [8] estimate pCTR of each candidate item on each candidate position, which are substantially pointwise models and thus limited in extracting exact context. MIR [33] capturing the set2list interactions by a permutation-equivariant module Another category is the two-stage reranking methods, which tries to evaluate every possible permutation through a welldesigned context-wise model. This is a global-optimal method but is too complex to meet the strict inference time constraint in industrial systems. To reduce the complexity, PRS [12] adopts beam-search to generate a few candidate permutations first, and score each permutation through a permutation-wise ranking model. PIER [29] applies SimHash to select top-K candidates from the full permutation."}, {"title": "2.2 Generative Reranking Solutions", "content": "In recent years, the generative reranking model [16, 18, 22] for listwise recommendation has been a topic of discussion. To manage the vast combinatorial output space of lists, the generative approach directly models the distribution of recommended lists and employs deep generative models to generate a list as a whole. For instance, ListCVAE [18] utilizes conditional variational autoencoders (CVAE) to capture the positional biases of items and the interdependencies within the list distribution. But Pivot-CVAE [22] indicates that ListCVAE suffers from a trade-off dilemma between accuracy and diversity, and proposes an \"elbow\" performance to enhance the accuracy-based evaluation.\nGFN [21] uses a flow-matching paradigm that maps the list generation probability with its utility. Essentially it is still studying list distributions rather than directly modeling the permutation space, so it still has the challenge mentioned above. GRN [13] proposes an evaluator-generator framework to replace the greedy strategy, but it can't avoid the evaluation-before-reranking problem [32] because it takes the rank list as input to the generator. DCDR [20] introduces diffusion models into the reranking stage and presents a discrete conditional diffusion reranking framework. NAR4Rec [28]"}, {"title": "3 Problem Definition", "content": "In the Meituan food delivery platform, we adhere to the matching, ranking, and reranking recommendation paradigms to present items to users in a list format. Initially, we define a user set U and an item set I. We utilize the session-level users' historical interacted lists B and candidate set C to represent the user's features u \u2208 U, which consistently holds in our reranking scenario C\u2208 I, and ultimately select the list L for users.\nReranking introduces a combination space with exponential size, represented as O(A^{m}), where n represents the size of the candidate set C and m represents the size of the output list L. Our optimization objective is to learn a strategy \u03c0: C \u2192 L by maximizing the list score reward R(u, \u03c0). The list score reward R(u, \u03c0) takes into account factors such as click-through rate (CTR) and conversion rate (CVR).\nNeighbor List. If the distance between two lists is 1, that is, the two lists only differ by 1 item, we define the two lists as neighbor lists. If we swap two items in a list, the new list will have a distance of 2."}, {"title": "4 Proposed Method", "content": "In this section, we present a detailed introduction of NLGR. We first introduce the evaluator (in Section 4.1) and generator (in Section 4.2) of NLGR, denoted as NLGR-E and NLGR-G respectively. Then, we demonstrate the offline training process of NLGR in Section 4.3. The evaluator is only assisting in the offline training of the generator, so NLGR is a fast inference method."}, {"title": "4.1 Evaluator Model", "content": "We use NLGR-E to evaluate a ranked sequence, such as the exposed list which is displayed to the user, or the candidate lists generated by NLGR-G. The structure of NLGR-E is shown on the left side of Figure 2.\nNLGR-E includes two inputs: the exposed list to be evaluated and the user session-level behavior sequence, where each session in the user session-level behavior sequence is the user's historical exposed list. The evaluation process of NLGR-E is as follows:\nFirst, we use an embedding layer to get the embedding of the original input, donated as X \u2208 R^{m\u00d7F\u00d7D} and M \u2208 R^{H\u00d7m\u00d7F\u00d7D} respectively, where H represents the number of history sessions, m represents the number of items in each list, F represents the number of feature fields for each item (i.g., ID, category, position index), and D represents the dimension of the embedding. Inspired by DIF [34], to avoid feature interference, we propose the D-Attention unit to decouple the feature context information. We first calculate the attention score on i-th attribute X_{i} \u2208 R^{m\u00d7D}:\nAtt_{i} = \u03c3\\left(\\frac{(X_{i}W_{Q})(XW_{K})^{T}}{\\sqrt{D}}\\right), \\forall i \\in [F],\nwhere W_{Q} and W_{K} \u2208 R^{D\u00d7D} denote the weight matrices."}, {"title": null, "content": "Then we aggregate all attention matrices Att_{i} \u2208 R^{m\u00d7m} and get the item-level attention score Att_{all} \u2208 R^{m\u00d7m}:\nAtt_{all} = \\frac{1}{F} \\sum_{f=1}^{F} Att_{i}.\nSubsequently, we aggregate ID feature embeding X_{id} \u2208 R^{m\u00d7D} and obtain each exposed list's representation e^{l} \u2208 R^{D}:\ne^{l} = reduce\\_mean\\left(Att_{all} \\left(X_{id}W_{V}\\right)\\right).\nSimilarly, by performing the above operations on each session in the user session-level behavior sequence, we can obtain the representation e^{h} \u2208 R^{D} of each session. Then we input e^{h} into a Self-Attention layer (SA) [19, 30] to get the user representation e^{u} \u2208 R^{D}:\ne^{u} = SA(e_{1} || e_{2} || ... || e_{H}),\nwhere || represents concatenate operate.\nFinally, the predicted Click-Through Rate (pCTR) of the j-th item can be represented as:\n\u0177_{j} = \u03c3(Tiled\\_MLP(X_{j} || e^{l} || e^{u} || PE_{j})),\nwhere PE_{j} denotes j-th position embedding. Similarly, the pCVR and other evaluations also follow the above process."}, {"title": "4.2 Generator model", "content": "We utilize NLGR-G to generate the optimal list in combinatorial space. The structure of NLGR-G is shown on the middle side of Figure 2.\nSimilar to NLGR-E, NLGR-G includes two inputs: the candidate list to be reranked and the user session-level behavior sequence, where each session in the user session-level behavior sequence is the user's historical exposed list. Theoretically, the candidate list can be any in the combinatorial space, but generally, we use the ranking list as the initial input. The generation process of NLGR-G is as follows:\nFirst, we obtain the user representation e^{u} through Eq. 4. These parameters are shared from NLGR-E to ensure that the remaining parameters can be optimized more focused.\nThen, we propose a sampling-based non-autoregressive generation method. It first determines the position of the item that needs to be replaced through the Position Decision Unit (PDU), then retrieves new replacement items from the candidate item set through the Candidate Retrieval Unit (CRU)."}, {"title": "4.2.1 Position Decision Unit (PDU)", "content": "First, we use the embedding layer to get the embedding of the candidate list, donated as X\u2208 R^{m\u00d7F\u00d7D}. Then, we flatten j-th item embedding X_{j} \u2208 R^{F\u00d7D} and use a Fully Connected layer (FC) to calculate the selected logit of the j-th position:\nh_{j} = FC_{1}(X_{j} || e^{l} || e^{u} || PE_{j}).\nTo solve the non-differentiable problem of the sampling distribution, inspired by [15, 17, 23], we use the Gumbel-softmax trick for sampling:\nr_{j} = softmax\\left(\\frac{log(h_{j}) + \u03b7}{\u03c4}\\right), \\forall j \\in [m],"}, {"title": null, "content": "where \u03c4 > 0 is a temperature parameter, \u03b7 = -log(-log(u))) represents random noise sampled from the Gumbel distribution, u is a uniform distribution between [0, 1]. During backpropagation, the gradient is calculated using Eq. 7. While during forward propagation, the replaced position is j = argmax(r).\nAfter determining the po-sition j to be replaced, we need to select a suitable one from n candidate items and place it at position j. Since this operation will be repeated multiple times during the generation process, we propose leveraging retrieval-based techniques to quickly achieve this goal for efficiency. First, we mask the position j of the candidate list, denoted as X_{mask} \u2208 R^{m\u00d7F\u00d7D}, and then extract the list representation e^{mask} \u2208 R^{D}:\ne^{mask} = SA(X_{mask}).\nWe then compute the representation of each candidate item at position j:\ne_{k} = FC_{2} (flatten(X_{C}^{k}) || PE_{j}), \\forall k \u2208 [n],\nwhere X_{C} \u2208 R^{n\u00d7F\u00d7D} denotes embedding of candidate set C, and X_{C}^{k} denotes k-th candidate item's embedding.\nThen we use an FC layer to calculate the selected logit of the j = k-th candidate item:\ng_{k} = FC_{3} (e_{k} || e^{mask} || e^{u} || PE_{j}).\nSimilarly, to overcome non-differentiable problems, we use the Gumbel-softmax trick for sampling:\nr_{k} = softmax\\left(\\frac{log(g_{k}) + \u03b7}{\u03c4}\\right), \\forall k \u2208 [n],"}, {"title": null, "content": "During backpropagation, the gradient is calculated using Eq. 10. While during forward propagation, the newly inserted item is c = argmax(r).\nStop condition. Note that the generation process may be repeated many times until the newly inserted item equals the replaced item or the values of r_{j} and r_{k} are too low."}, {"title": "4.3 Utilizing Neighbor Lists Training", "content": "In this section, we elaborate on the offline training process of NLGR, which includes the training procedures for NLGR-E and NLGR-G. As mentioned before, the evaluator is trained to fit list-wise scores of items, and the generator is tasked with transforming any ranking list into the optimal one. This goal inconsistency between the evaluator and the generator complicates the transfer of guidance. We will introduce our solution in detail below."}, {"title": "4.3.1 Training of NLGR-E", "content": "To accurately evaluate the return of the exposure list and estimate the listwise pCTR value of the exposure list, we train NLGR-E using real data collected from online logs. The input is the features of the recommended advertisement sequences exposed in reality online, and the advertising return situation, including exposure, click, conversion, and other performance indicators, is used as the label to supervise the training of NLGR-E, enabling it to accurately evaluate the return of the recommended sequence. The loss of NLGR-E is calculated as follows:\nL_{E} = \\sum_{j=1}^{m}(-y_{j} log(\u0177_{j}) \u2013 (1 \u2013 y_{j}) log (1 \u2013 \u0177_{j})),\nwhere y_{j} represents the real label, \u0177_{j} represents the predicted value of NLGR-E, and the evaluation is carried out for the m items in the exposure list in turn."}, {"title": "4.3.2 Training of NLGR-G", "content": "To address the problem of goal inconsistency mentioned before, we use neighbor lists to guide NLGR-G within the counterfactual space. For each list generated by NLGRG, NLGR-E simulates human feedback and provides a reward R to guide NLGR-G training.\nThen, we use the trained NLGR-E to evaluate the candidate list L^{0} and the neighbor list L^{*}, and indicators such as pCTR and pCVR will be estimated. We convert the estimated value into list reward based on business indicators as follows:\n\\tilde{r} = \\begin{cases}\ne^{w-1-1} & \\text{if } w > 1 \\\\1 & \\text{if } w = 1, \\\\1-e^{1-w} & \\text{if } w < 1 \\end{cases}\nw = k_{1} L_{ctr} + k_{2} L_{ctr} L_{cvr},\nwhere L_{ctr} and L_{cvr} represent the list total pCTR and pCVR which are evaluated by NLGC-E, respectively. The parameters k_{1} and k_{2} are business parameters that depend on the click bid and conversion price in the specific business.\nThrough Eq. 13, we can get the rewards of the neighbor lists L^{*} and the original candidate list L^{0}, denoted as \\vec{r} = [r_{1}, r_{2}, ..., r_{m}] and r_{0} respectively. NLGR-G is tasked to iterate from the candidate list to a more optimal list, so we calculate the relative reward for each position j:\nr_{j} = \\tilde{r_{j}} - r_{0}, \\forall j \u2208 [m].\nThe authentic evaluation R for the candidate list L^{0} is obtained by aggregating the relative rewards of all positions. And we define the counterfactual loss of NLGR-G as -R:\nL = -R = - \\sum_{j=1}^{m} r_{j}.\nFurthermore, to increase the stability of the NLGR-G's generation process, we propose using the position reward r_{j} to provide additional guidance to the Position Decision Unit (PDU). Specifically, we introduce cross-entropy loss as an auxiliary loss to measure the"}, {"title": null, "content": "distribution difference of position sampling r_{p} and position reward r_{j}:\nL^{r}_{p} = - \\sum_{j=1}^{m} Norm(r_{j}) logr_{p},\nwhere Norm(r_{j}) = \\sum_{j=1}^{m} r_{j}.\nThe final loss of NLGR-G in each batch is:\nL_{G} = \\frac{1}{B} (L + \u03b1L^{r}_{p}),\nwhere \u03b1 is a coefficient to balance the two losses."}, {"title": "5 Experiments", "content": "To validate the superior performance of NLGR, we conducted extensive offline experiments on the Meituan dataset and verified the superiority of NLGR in online A/B tests. In this section, we first introduce the experimental setup, including the dataset and baseline. Then, in Section 5.2, we present the results and analysis of various reranking methods in both offline and online A/B tests."}, {"title": "5.1 Experimental Setup", "content": "5.1.1 Dataset. In order to verify the effectiveness of NLGR, we conduct sufficient experiments on both public dataset and industrial dataset. For public dataset, we choose Taobao Ad dataset. For the industrial dataset, we use real-world data collected from Meituan food delivery platform.\n5.1.2 Baseline. The following six baselines are chosen for comparative experiments and divided into three groups. We select DNN and DeepFM as point-wise baselines (Group I), PRM and MIR as list-wise baselines (Group II), and GRN and DCDR as generative baselines (Group III). A brief introduction of these methods is as follows:\n5.1.3 Evaluation Metrics. We adopt several metrics, i.e., AUC (Area Under ROC Curve), Logloss and NDCG (normalized discounted cumulative gain) to evaluate NLGR-E in offline experiments. A larger AUC and NDCG indicate better recommendation performance, while Logloss performs the opposite.\nWe use HR (Hit Ratio) [3] to evaluate NLGR-G in offline experiments. It is worth noting that only one list produced by reranking algorithms can be presented to the user. As a result, the generator cannot be fully and fairly evaluated. A practical workaround is to employ the evaluator to assess the performance of the generator. For each data, we evaluate all candidate lists using NLGR-E. HR@10% is 1 only when the rerank list produced by NLGR-G is ranked within the top 10% as sorted by NLGR-E. The HR metrics are only meaningful with evaluator-based reranking methods. The results of HR can be seen in Table 3.\nIt is worth noting that AUC and HR can measure the evaluator and generator respectively. AUC measures the model's ability to"}, {"title": "5.1.4 Implementation Details", "content": "We implement all the deep learning baselines and NLGR with TensorFlow 1.15.0 using NVIDIA A100-SXM4-80GB. For all comparison models and our NLGR model, we adopt Adam as the optimizer with the learning rate fixed to 0.001 and initialize the model parameters with normal distribution by setting the mean and standard deviation to 0 and 0.01, respectively. The batch size is 1024, the embedding size is 8, the \u03b1 is 0.2. The hidden layer sizes of tiled MLP are (1024, 256, 128). For the Taobao Ad dataset, the length of the ranking list and reranking list are both 5, thus the length of full permutation is 120. For Metuan dataset, we select 4 items from the initial ranking list which contains 12 items, thus the length of full permutation is A^{4}_{12} = 11, 880. All experiments are repeated five times and the averaged results are reported."}, {"title": "5.2 Experimental Results", "content": "i) PRM in Group II outperforms all models in Group I, which verifies the impact of the mutual influence among contextual items. DCDR in Group III outperforms all the other models in Groups I and II, which verifies the effectiveness of generative methods.\nii) DCDR indeed exhibits robust generative capabilities, thanks to the incorporation of the diffusion model, and achieves the second-best result. Nevertheless, DCDR overlooks the significance of full sight and falls short of leveraging the evaluator's potential to its fullest, which limits its effect. Our proposed NLGR significantly and consistently outperforms the state-of-the-art approaches in all metrics on both datasets. As presented in Table 2, our proposed NLGR brings 0.8349/0.6344 absolute AUC, 0.2857/0.2323 absolute NDCG@10, 0.2431/0.1830 absolute NDCG@5 on Metuan/Ad dataset, gains significant improvement in industrial recommendation system. NLGR has greater improvements on Meituan dataset because Meituan dataset has more realistic reranking scenarios and richer features."}, {"title": "5.2.2 Ablation Study", "content": "To verify the impact of different units, we study three ablated variants of NLGR on Meituan dataset.\ni) The HR of NLGR w/o r drops the most (8%/17.2%), indicating that neighbor list training is the most important part of NLGR. ii) The HR of NLGR w/ AG dropped significantly (2.2%/4.8%), indicating that the sampling-based nonautoregressive generation method in NLGR-G can significantly improve the generation effect. iii) The HR of NLGR w/o L also decreases (1.1%/3.2%), indicating that auxiliary loss can enhance the generation ability of NLGR-G."}, {"title": "5.3 Hyperparameter Analysis", "content": "We analyze the sensitivity of two hyperparameters: \u03b1 and \u03b2, corresponding to the generation process and training process of NLGR. Among them, \u03b1 is the weight of NLGR-G loss in Eq. 18, and \u03b2 is the sampling ratio at each position when constructing the neighbor list L^{*}. By default \u03b2 = 1 means that each position is sampled 1 time. The result is shown in Table 5, showing the same trend on the public dataset and industrial dataset and we have the following findings:\ni) Hyperparameter \u03b1 significantly affects the generator's HR@10%\nmetric. When \u03b1 = 0, NLGR is equivalent to the method of Group"}, {"title": null, "content": "III, which means the generator has no full sight. As \u03b1 increases, HR@10% first increases and then decreases.\nii) We tested several values for \u03b2. When \u03b2 < 1, we randomly select \u03b2m positions in m positions to construct rewards. When \u03b2 > 1, we construct \u03b2 neighbor lists at each position. Increasing \u03b2 within a certain range can quickly improve the HR@10%\nperformance. As \u03b2 continues to increase, HR@10% remains stable but increases offline training time. The results show that counterfactual rewards considering all positions are important."}, {"title": "5.4 Online A/B test", "content": "We deployed NLGR in Meituan App, where Figure 4 shows the online serving system architecture. It is worth noting that although we involve the evaluator guiding the generator multiple times during offline training, we only need to use the generator when serving online. In this way, we ensure that its model complexity is comparable to the online model without adding additional calculations to the online service."}, {"title": "6 Conclusion", "content": "In this paper, we make the first attempt to solve the goal inconsistency problem in reranking systems. We propose a novel framework called Neighbor List Generative Reranking (NLGR), which uses the relative scores of candidate list and neighboring lists to guide the generator. Furthermore, we propose a sampling-based non-autoregressive generator that can flexibly jump from the current list to any neighbor list. Both offline experiments and online A/B tests show that NLGR significantly outperformed other existing reranking baselines, and we have deployed NLGR on the Meituan food delivery platform."}]}