{"title": "Deep Learning Through A Telescoping Lens: A Simple Model Provides Empirical Insights On Grokking, Gradient Boosting & Beyond", "authors": ["Alan Jeffares", "Alicia Curth", "Mihaela van der Schaar"], "abstract": "Deep learning sometimes appears to work in unexpected ways. In pursuit of a deeper understanding of its surprising behaviors, we investigate the utility of a simple yet accurate model of a trained neural network consisting of a sequence of first-order approximations telescoping out into a single empirically operational tool for practical analysis. Across three case studies, we illustrate how it can be applied to derive new empirical insights on a diverse range of prominent phenomena in the literature - including double descent, grokking, linear mode connectivity, and the challenges of applying deep learning on tabular data \u2013 highlighting that this model allows us to construct and extract metrics that help predict and understand the a priori unexpected performance of neural networks. We also demonstrate that this model presents a pedagogical formalism allowing us to isolate components of the training process even in complex contemporary settings, providing a lens to reason about the effects of design choices such as architecture & optimization strategy, and reveals surprising parallels between neural network learning and gradient boosting.", "sections": [{"title": "1 Introduction", "content": "Deep learning works, but it sometimes works in mysterious ways. Despite the remarkable recent success of deep learning in applications ranging from image recognition [KSH12] to text generation [BMR+20], there remain many contexts in which it performs in apparently unpredictable ways: neural networks sometimes exhibit surprisingly non-monotonic generalization performance [BHMM19, PBE+22], continue to be outperformed by gradient boosted trees on tabular tasks despite successes elsewhere [GOV22], and sometimes behave surprisingly similarly to linear models [FDRC20]. The pursuit of a deeper understanding of deep learning and its phenomena has since motivated many subfields, and progress on fundamental questions has been distributed across many distinct yet complementary perspectives that range from purely theoretical to predominantly empirical research.\nOutlook. In this work, we take a hybrid approach and investigate how we can apply ideas primarily used in theoretical research to investigate the behavior of a simple yet accurate model of a neural network empirically. Building upon previous work that studies linear approximations to learning in neural networks through tangent kernels (e.g. [JGH18, COB19], see Sec. 2), we consider a model that uses first-order approximations for the functional updates made during training. However, unlike most previous work, we define this model incrementally by simply telescoping out approximations to individual updates made during training (Sec. 3) such that it more closely approximates the true behavior of a fully trained neural network in practical settings. This provides us with a pedagogical lens through which we can view modern optimization strategies and other design choices (Sec. 5), and a mechanism with which we can conduct empirical investigations into several prominent deep learning phenomena that showcase how neural networks sometimes generalize seemingly unpredictably."}, {"title": "2 Background", "content": "Notation and preliminaries. Let $f_{\\theta} : \\mathcal{X} \\subseteq \\mathbb{R}^d \\rightarrow \\mathcal{Y} \\subseteq \\mathbb{R}^k$ denote a neural network parameterized by (stacked) model weights $\\theta \\in \\mathbb{R}^P$. Assume we observe a training sample of $n$ input-output pairs $\\{(x_i, Y_i)\\}_{i=1}^n$, i.i.d. realizations of the tuple $(X, Y)$ sampled from some distribution $P$, and wish to learn good model parameters $\\theta$ for predicting outputs from this data by minimizing an empirical prediction loss $\\frac{1}{n}\\sum_{i=1}^n l(f_{\\theta}(x_i), Y_i)$, where $l : \\mathbb{R}^k \\times \\mathbb{R}^k \\rightarrow \\mathbb{R}$ denotes some differentiable loss function. Throughout, we let $k = 1$ for ease of exposition, but unless otherwise indicated our discussion generally extends to $k > 1$. We focus on the case where $\\theta$ is optimized by initializing the model with some $\\theta_0$ and then iteratively updating the parameters through stochastic gradient descent (SGD) with learning rates $\\gamma_t$ for $T$ steps, where at each $t \\in [T] = \\{1, ..., T\\}$ we subsample batches $\\mathcal{B}_t \\subseteq [n] = \\{1, ..., n\\}$ of the training indices, leading to parameter updates $\\Delta \\theta_t := \\theta_t - \\theta_{t-1}$ as:\n$$\\theta_t = \\theta_{t-1} + \\Delta \\theta_t = \\theta_{t-1} - \\gamma_t \\sum_{i \\in \\mathcal{B}_t} \\nabla_{\\theta}l(f_{\\theta_{t-1}}(x_i),y_i) = \\theta_{t-1} - \\gamma_t T_t g_t,$$\nwhere $g_{it} = \\frac{\\partial l(f_{\\theta_{t-1}}(x_i),y_i)}{\\partial f_{\\theta_{t-1}}(x_i)}$ is the gradient of the loss w.r.t. the model prediction for the $i^{th}$ training example, which we will sometimes collect in the vector $g_t = [g_{1t},..., g_{nt}]^T$, and the $p \\times n$ matrix $T_t = [\\mathbf{1}\\{i \\in \\mathcal{B}_t\\}\\nabla_{\\theta}f_{\\theta_{t-1}}(x_1),..., \\mathbf{1}\\{n \\in \\mathcal{B}_t\\}\\nabla_{\\theta}f_{\\theta_{t-1}}(x_n)]$ has as columns the gradients of the model prediction with respect to its parameters for examples in the training batch (and 0 otherwise). Beyond vanilla SGD, modern deep learning practice usually relies on a number of modifications to the update described above, such as momentum and weight decay; we discuss these in Sec. 5.\nRelated work: Linearized neural networks and tangent kernels. A growing body of recent work has explored the use of linearized neural networks (linear in their parameters) as a tool for theoretical [JGH18, COB19, LXS+19] and empirical [FDP+20, LZB20, OJMDF21] study. In this paper, we similarly make extensive use of the following observation (as in e.g. [FDP+20]): we can linearize the difference $\\Delta f_t(x) := f_{\\theta_t}(x) - f_{\\theta_{t-1}}(x)$ between two parameter updates as\n$$\\Delta f_t(x) = \\nabla_{\\theta}f_{\\theta_{t-1}}(x)^T \\Delta \\theta_t + O(||\\Delta \\theta_t||^2) \\approx \\nabla_{\\theta}f_{\\theta_{t-1}}(x)^T \\Delta \\theta_t := \\tilde{\\Delta}f_t(x)$$\nwhere the quality of the approximation $\\tilde{\\Delta}f_t(x)$ is good whenever the parameter updates $\\Delta \\theta_t$ from a single batch are sufficiently small (or when the Hessian product $||\\Delta \\theta_t \\nabla^2f_{\\theta_{t-1}}(x)\\Delta \\theta_t||$ vanishes). If Eq. (2) holds exactly (e.g. for infinitesimal $\\gamma_t$), then running SGD in the network's parameter space to obtain $\\Delta \\theta_t$ corresponds to executing steepest descent on the function output $f_{\\theta}(x)$ itself using the neural tangent kernel $K_{\\theta}(x, x_i)$ at time-step $t$ [JGH18], i.e. results in functional updates\n$$\\Delta f_t(x) = -\\gamma_t \\sum_{i \\in [n]} K_{\\theta}(x, x_i) g_{it} \\text{ where } K_{\\theta}(x, x_i) := \\mathbf{1}\\{i \\in \\mathcal{B}_t\\}\\nabla_{\\theta}f_{\\theta_{t-1}}(x)^T\\nabla_{\\theta}f_{\\theta_{t-1}}(x_i).$$\nLazy learning [COB19] occurs as the model gradients remain approximately constant during training, i.e. $\\nabla_{\\theta}f_{\\theta_t}(x) \\approx \\nabla_{\\theta}f_{\\theta_0}(x), \\forall t \\in [T]$. For learned parameters $\\theta_T$, this implies that the approxi-mation $f_{\\theta_T}(x) \\approx f_{\\theta_0}(x) + \\nabla_{\\theta}f_{\\theta_0}(x)^T (\\theta_T - \\theta_0)$ holds \u2013 which is a linear function of the model parameters, and thus corresponds to a linear regression in which features are given by the model gradients $\\nabla_{\\theta}f_{\\theta_0}(x)$ instead of the inputs $x$ directly \u2013 whose training dynamics can be more eas-ily understood theoretically. For sufficiently wide neural networks the $\\nabla_{\\theta}f_{\\theta_t}(x)$, and thus the tangent kernel, have been theoretically shown to be constant throughout training in some settings [JGH18, LXS+19], but in practice they generally vary during training, as shown theoretically in [LZB20] and empirically in [FDP+20]. A growing theoretical literature [GPK22] investigates con-stant tangent kernel assumptions to study convergence and generalization of neural networks (e.g."}, {"title": "3 A Telescoping Model of Deep Learning", "content": "In this work, we explore whether we can exploit the approximation in Eq. (2) beyond the laziness assumption to gain new insight into neural network learning. Instead of applying the approximation across the entire training trajectory at once as in $f_{\\theta_T}(x)$, we consider using it incrementally at each batch update during training to approximate what has been learned at this step. This still provides us with a greatly simplified and transparent model of a neural network, and results in a much more reasonable approximation of the true network. Specifically, we explore whether \u2013 instead of studying the final model $f_{\\theta_T}(x)$ as a whole \u2013 we can gain insight by telescoping out the functional updates made throughout training, i.e. exploiting that we can always equivalently express $f_{\\theta_T}(x)$ as:\n$$f_{\\theta_T}(x) = f_{\\theta_0}(x) + \\sum_{t=1}^T[f_{\\theta_t}(x) - f_{\\theta_{t-1}}(x)] = f_{\\theta_0}(x) + \\sum_{t=1}^T\\Delta f_t(x)$$\nThis representation of a trained neural network in terms of its learning trajectory rather than its final parameters is interesting because we are able to better reason about the impact of the training procedure on the intermediate updates $\\Delta f_t(x)$ than the final function $f_{\\theta_T}(x)$ itself. In particular, we investigate whether empirically monitoring behaviors of the sum in Eq. (4) while making use of the approximation in Eq. (2) will enable us to gain practical insights into learning in neural networks, while incorporating a variety of modern design choices into the training process. That is, we explore the use of the following telescoping model $\\hat{f}_{\\theta_T}(x)$ as an approximation of a trained neural network:\nTelescoping model of a trained neural network\n$$\\hat{f}_{\\theta_T}(x) := f_{\\theta_0}(x) + \\sum_{t=1}^T \\nabla_{\\theta}f_{\\theta_{t-1}}(x)^T \\Delta \\theta_t = f_{\\theta_0}(x) - \\sum_{t=1}^T\\sum_{i \\in [n]} K^T_{\\theta}(x, x_i) g_{it}$$\nwhere $K^T_{\\theta}(x, x_i)$ is determined by the neural tangent kernel as $\\gamma_t K_{\\theta}(x, x_i)$ in the case of standard SGD (in which case (ii) can also be interpreted as a discrete-time approximation of [Dom20]'s path kernel), but can take other forms for different choices of learning algorithm as we explore in Sec. 5.\nPractical considerations. Before proceeding, it is important to emphasize that the telescoping approximation described in Eq. (5) is intended as a tool for (empirical) analysis of learning in neural networks and is not being proposed as an alternative approach to training neural networks. Obtaining $\\hat{f}_{\\theta_T}(x)$ requires computing $\\nabla_{\\theta}f_{\\theta_{t-1}}(x)$ for each training and testing example at each training step $t \\in [T]$, leading to increased computation over standard training. Additionally, these computational costs are likely prohibitive for extremely large networks and datasets without further adjustments; for this purpose, further approximations such as [MBS23] could be explored. Nonetheless, computing $\\hat{f}_{\\theta_T}(x)$ \u2013 or relevant parts of it \u2013 is still feasible in many pertinent settings as later illustrated in Sec. 4."}, {"title": "4 A Closer Look at Deep Learning Phenomena Through a Telescoping Lens", "content": "Next, we turn to applying the telescoping model. Below, we present three case studies revisiting existing experiments that provided evidence for a range of unexpected behaviors of neural networks. These case studies have in common that they highlight cases in which neural networks appear to generalize somewhat unpredictably, which is also why each phenomenon has received considerable attention in recent years. For each, we then show that the telescoping model allows us to construct and extract metrics that can help predict and understand the unexpected performance of the networks. In particular, we investigate (i) surprising generalization curves (Sec. 4.1), (ii) performance differences between gradient boosting and neural networks on some tabular tasks (Sec. 4.2), and (iii) the success of weight averaging (Sec. 4.3). We include an extended literature review in Appendix A, a detailed discussion of all experimental setups in Appendix C, and additional results in Appendix D."}, {"title": "4.1 Case study 1: Exploring surprising generalization curves and benign overfitting", "content": "Classical statistical wisdom provides clear intuitions about overfitting: models that can fit the training data too well - because they have too many parameters and/or because they were trained for too long \u2013 are expected to generalize poorly (e.g. [HTF09, Ch. 7]). Modern phenomena like double descent [BHMM19], however, highlighted that pure capacity measures (capturing what could be learned in-stead of what is actually learned) would not be sufficient to understand the complexity-generalization relationship in deep learning [Bel21]. Raw parameter counts, for example, cannot be enough to under-stand the complexity of what has been learned by a neural network during training because, even when using the same architecture, what is learned could be wildly different across various implementation choices within the optimization process \u2013 and even at different points during the training process of the same model, as prominently exemplified by the grokking phenomenon [PBE+22]. Here, with the goal of finding clues that may help predict phenomena like double descent and grokking, we explore whether the telescoping model allows us to gain insight into the relative complexity of what is learned.\nA complexity measure that avoids the shortcomings listed above \u2013 because it allows to consider a specific trained model \u2013 was recently used by [CJvdS23] in their study of non-deep double descent. As their measure $\\rho_{\\Theta}$ builds on the literature on smoothers [HT90], it requires to express learned predictions as a linear combination of the training labels, i.e. as $f(x) = s(x)y = \\sum_{i \\in [n]} s_i(x)Y_i$. Then, [CJvdS23] define the effective parameters $\\rho$ used by the model when issuing predictions for some set of inputs $\\{x_j\\}_{j \\in \\mathcal{I}_0}$ with indices collected in $\\mathcal{I}_0$ (here, $\\mathcal{I}_0$ is either $\\mathcal{I}_{train} = \\{1, ..., n\\}$ or $\\mathcal{I}_{test} = \\{n+1, ..., n + m\\}$) as $\\rho = \\rho(\\mathcal{I}_0, s(\\cdot)) = \\sum_{j \\in \\mathcal{I}_0} ||s(x_j)||^2$. Intuitively, the larger $\\rho$, the less smoothing across the training labels is performed, which implies higher model complexity.\nDue to the black-box nature of trained neural networks, however, it is not obvious how to link learned predictions to the labels observed during training. Here, we demonstrate how the telescoping model allows us to do precisely that \u2013 enabling us to make use of $\\rho$ as a proxy for complexity. We consider the special case of a single output ($k = 1$) and training with squared loss $l(f(x), y) = (y \u2212 f(x))^2$,"}, {"title": "4.2 Case study 2: Understanding differences between gradient boosting and neural networks", "content": "Despite their overwhelming successes on image and language data, neural networks are \u2013 perhaps surprisingly - still widely considered to be outperformed by gradient boosted trees (GBTs) on tabular data, an important modality in many data science applications. Exploring this apparent Achilles heel of neural networks has therefore been the goal of multiple extensive benchmarking studies [GOV22, MKV+23]. Here, we concentrate on a specific empirical finding of [MKV+23]: their results suggest that GBTs may particularly outperform deep learning on heterogeneous data with greater irregularity in input features, a characteristic often present in tabular data. Below, we first show that the telescoping model offers a useful lens to compare and contrast the two methods, and then use this insight to provide and test a new explanation of why GBTs can perform better in the presence of dataset irregularities.\nIdentifying (dis)similarities between learning in GBTs and neural networks. We begin by introducing gradient boosting [Fri01] closely following [HTF09, Ch. 10.10]. Gradient boosting (GB) also aims to learn a predictor $f^{GB} : X \\rightarrow \\mathbb{R}^k$ minimizing expected prediction loss $l$. While deep learning solves this problem by iteratively updating a randomly initialized set of parameters that transform inputs to predictions, the GB formulation iteratively updates predictions directly without requiring any iterative learning of parameters \u2013 thus operating in function space rather than parameter space. Specifically, GB, with learning rate $\\gamma$ and initialized at predictor $h_0(x)$, consists of a sequence $f^t(x) = h_0(x) + \\sum_{t=1}^T h_t(x)$ where each $h_t(x)$ improves upon the existing predictions $f^{t-1}(x)$. The solution to the loss minimization problem can be achieved by executing steepest descent in function space directly, where each update $h_t$ simply outputs the negative training gradients of the loss function with respect to the previous model, i.e. $h_t(x_i) = -g_{it}$ where $g_{it} = \\partial l(f^{t-1}(x_i), y_i)/\\partial f^{h}(x_i)$."}, {"title": "4.3 Case study 3: Towards understanding the success of weight averaging", "content": "The final interesting phenomenon we investigate is that it is sometimes possible to simply average the weights $\\theta_1$ and $\\theta_2$ obtained from two stochastic training runs of the same model, resulting in a weight-averaged model that performs no worse than the individual models [FDRC20, AHS22] \u2013 which has important applications in areas such as federated learning. This phenomenon is known as linear mode connectivity (LMC) and is surprising as, a priori, it is not obvious that simply averaging the weights of independent neural networks (instead of their predictions, as in a deep ensemble [LPB17]), which are highly nonlinear functions of their parameters, would not greatly worsen performance. While recent work has demonstrated empirically that it is sometimes possible to weight-average an even broader class of models after permuting weights [SJ20, ESSN21, AHS22], we focus here on understanding when LMC can be achieved for two models trained from the same initialization $\\theta_0$.\nIn particular, we are interested in [FDRC20]'s observation that LMC can emerge during training: the weights of two models $\\theta_{t'}^{j} \\in \\{1,2\\}$, which are initialized identically and follow identical optimization routine up until checkpoint $t'$ but receive different batch orderings and data augmentations after t', can be averaged to give an equally performant model as long as $t'$ exceeds a so-called stability point $t^*$, which was empirically discovered to occur early in training in [FDRC20]. Interestingly, [FDP+20, Sec. 5] implicitly hint at an explanation for this phenomenon in their empirical study of tangent kernels and loss landscapes, where they found an association between the disappearance of loss barriers between solutions during training and the rate of change in $K(\\cdot, \\cdot)$. We further explore potential implications of this observation through the lens of the telescoping model below.\nWhy a transition into a constant-gradient regime would imply LMC. Using the weight-averaging representation of the telescoping model, it becomes easy to see that not only would stabilization of the tangent kernel be associated with lower linear loss barriers, but the transition into a lazy regime during training - i.e. reaching a point $t^*$ after which the model gradients no longer change \u2013 can be sufficient to imply LMC during training as observed in [FDRC20] under a mild assumption on the performance of the two networks' ensemble. To see this, let $L(f) := E_{X,Y \\sim P}[l(f(X), Y)]$ denote the expected loss of $f$ and recall that if $\\sup_{\\alpha \\in [0,1]}L(f_{\\alpha \\theta_{t'}^1+(1-\\alpha)\\theta_{t'}^2}) - [\\alpha L(f_{\\theta_{t'}^1}) + (1 - \\alpha)L(f_{\\theta_{t'}^2})] \\leq 0$ then LMC is said to hold. If we assume that ensembles $f_{\\alpha}(x) := \\alpha f_{\\theta_{t'}^1}(x) + (1 - \\alpha)f_{\\theta_{t'}^2}(x)$ perform no worse than the individual models (i.e. $L(f_{\\alpha}^0) \\leq \\alpha L(f_{\\theta_{t'}^1})+(1-\\alpha)L(f_{\\theta_{t'}^2}) \\forall \\alpha \\in [0, 1]$, as is usually the case in practice [ABPC23]), then one case in which LMC is guaranteed is if the predictions of weight-averaged model and ensemble are identical. In Appendix B.2, we show that if there exists some $t^* \\in [0, T)$ after which the model gradients $\\nabla_{\\theta}f_{\\theta_{t'}}(x)$ no longer change (i.e. for all $t' > t^*$ the learned updates $\\Delta \\theta_{t'} \\in$ lie in a convex set $\\Theta_{stable}$ in which $\\nabla_{\\theta}f_{\\theta_{t'}}(\\cdot) \\approx \\nabla_{\\theta}f_{\\theta_{t^*}}(\\cdot)$), then indeed\n$$f_{\\frac{\\alpha \\theta_{t'}^1+(1-\\alpha)\\theta_{t'}^2}(x) \\approx f_{\\theta_{t^*}}(x) \\approx f_{\\theta_0}(x) + \\nabla_{\\theta}f_{\\theta_{t^*}}(x) \\sum_{t'=t^*+1}^T(\\alpha \\Delta \\theta_{t'}^1 + (1 - \\alpha)\\Delta \\theta_{t'}^2).$$That is, transitioning into a regime with constant model gradients during training can imply LMC because the ensemble and weight-averaged model become near-identical. This also has as an immediate corollary that models with the same $\\theta_0$ which train fully within this regime (e.g. those discussed in [JGH18, LXS+19]) will have $t^* = 0$. Note that, when using nonlinear (final) output activation $\\sigma(\\cdot)$ the post-activation model gradients will generally not become constant during training (as we discuss in Sec. 5 for the sigmoid and as was shown theoretically in [LZB20] for general nonlinearities). If, however, the pre-activation model gradients become constant during training and the pre-activation ensemble \u2013 which averages the two model's pre-activation outputs before applying $\\sigma(\\cdot)$ \u2013 performs no worse than the individual models (as is also usually the case in practice [JLCvdS24]), then the above also immediately implies LMC for such models."}, {"title": "5 The Effect of Design Choices on Linearized Functional Updates", "content": "The literature on the neural tangent kernel primarily considers plain SGD, while modern deep learning practice typically relies on a range of important modifications to the training process (see e.g. [Pri23, Ch. 6]) - this includes many of the experiments demonstrating surprising deep learning phenomena we examined in Sec. 4. To enable us to use modern optimizers above, we derived their implied linearized functional updates through the weight-averaging representation $\\Delta f_t(x) = \\nabla_{\\theta} f_{\\theta_{t-1}}(x)^T \\Delta \\theta_t$, which in turn allows us to define $K^T(\\cdot, \\cdot)$ in Eq. (5) for these modifications using straightforward algebra. As a by-product, we found that this provides us with an interesting and pedagogical formalism to reason about the relative effect of different design choices in neural network training, and elaborate on selected learnings below.\n$\\bullet$ Momentum with scalar hyperparameter $\\beta_1$ smoothes weight updates by employing an exponentially weighted average over the previous parameter gradients as $\\tilde{\\Delta} \\theta_t = \\frac{1}{1-\\beta_1} [\\sum_{k=1}^t \\beta_1^{t-k} (\\gamma_k \\mathbf{1}\\{i \\in \\mathcal{B}_k\\})] - \\frac{1}{1-\\beta_1}$ instead of using the current gradients alone. This implies linearized functional updates\n$$\\Delta f_t(x) = -\\gamma_t \\sum_{i \\in [n]} (K_{\\theta}^t(x, x_i) g_{it} + \\frac{\\beta}{1-\\beta_1} \\sum_{k=1}^{t-1} \\mathbf{1}\\{i \\in \\mathcal{B}_k\\} K_{\\theta}^{t,k}(x, x_i) g_{ik})$$\nwhere $K_{\\theta}^{t,k}(x, x_i) := \\mathbf{1}\\{i \\in \\mathcal{B}_k\\} \\nabla_{\\theta}f_{\\theta_{t-1}}(x) \\nabla_{\\theta} f_{\\theta_{k-1}}(x_i)$ denotes the cross-temporal tangent kernel. Thus, the functional updates also utilize previous loss gradients, where their weight is determined"}, {"title": "6 Conclusion", "content": "This work investigated the utility of a telescoping model for neural network learning, consisting of a sequence of linear approximations, as a tool for understanding several recent deep learning phenomena. By revisiting existing empirical observations, we demonstrated how this perspective provides a lens through which certain surprising behaviors of deep learning can become more intelligible. In each case study, we intentionally restricted ourselves to specific, noteworthy empirical examples which we proceeded to re-examine in greater depth. We believe that there are therefore many interesting opportunities for future research to expand on these initial findings by building upon the ideas we present to investigate such phenomena in more generality, both empirically and theoretically."}, {"title": "Appendix A Additional literature review", "content": "In this section, we present an extended literature review related to the phenomena we consider in Sec. 4.1 and Sec. 4.3.\nA.1 The model complexity-performance relationship (Sec. 4.1)\nClassical statistical textbooks convey a well-understood relationship between model complexity historically captured by a model's parameter count and prediction error: increasing model complexity is expected to modulate a transition between under- and overfitting regimes, usually represented by a U-shaped error-curve with model complexity on the x-axis in which test error first improves before it worsens as the training data can be fit too well [HT90, Vap95, HTF09]. While this relationship was originally believed to hold for neural networks as well [GBD92], later work provided evidence that \u2013 when using parameter counts to measure complexity \u2013 this U-shaped relationship no longer holds [NMB+18, Nea19].\nDouble descent. Instead, the double descent [BHMM19] shape has claimed its place, which postulates that the well-known U-shape holds only in the underparameterized regime where the number of model parameters p is smaller than the number of training examples n; once we reach the interpolation threshold p = n at which models have sufficient capacity to fit the training data perfectly, increasing p further into the overparametrized (or: interpolation) regime leads to test error improving again. While the double descent shape itself had been previously observed in linear regression and neural networks in [VCR89, BO96, ASS20, NMB+18, SGd+18] (see also the historical note in [LVM+20]), the seminal paper by [BHMM19] both popularized it as a phenomenon and highlighted that the double descent shape can also occur tree-based methods. In addition to double descent as a function of the number of model parameters, the phenomenon has since been shown to emerge also in e.g. the number of training epochs[NKB+21] and sparsity [HXZQ22]. Optimal regularization has been shown to mitigate double descent [NVKM20].\nDue to its surprising and counterintuitive nature, the emergence of the double descent phenomenon sparked a rich theoretical literature attempting to understand it. One strand of this literature has focused on modeling double descent in the number of features in linear regression and has produced precise theoretical analyses for particular data-generating models [BHX20, ASS20, BLLT20, DLM20, HMRT22, SKR+23, CMBK21]. Another strand of work has focused on deriving exact expressions of bias and variance terms as the total number of model parameters is increased in a neural network by taking into account all sources of randomness in model training [NMB+18, AP20, dRBK20, LD21]. A different perspective was presented in [CJvdS23], who highlighted that in the non-deep double descent experiments of [BHMM19], a subtle change in the parameter-increasing mechanism is introduced exactly at the interpolation threshold, which is what causes the second descent. [CJvdS23] also demonstrated that when using a measure of the test-time effective parameters used by the model to measure complexity on the x-axes, the double descent shapes observed for linear regression, trees, and boosting fold back into more traditional U-shaped curves. In Sec. 4.1, we show that the telescoping model enables us to discover the same effect also in deep learning.\nBenign overfitting. Closely related to the double descent phenomenon is benign overfitting (e.g. [BMM18, MBB18, BLLT20, CL21, MSA+22, WOBM17, HHLS24]), i.e. the observation that, incompatible with conventional statistical wisdom about overfitting [HTF09], models with perfect training performance can nonetheless generalize well to unseen test examples. In this literature, it is often argued in theoretical studies that overparameterized neural networks generalize well because they are much more well-behaved around unseen test examples than examples seen during training [MSA+22, HHLS24]. In Sec. 4.1 we provide new empirical evidence for this by highlighting that there is a difference between $\\rho^{train}$ and $\\rho^{test}$."}, {"title": "Understanding modern model complexity", "content": "Understanding modern model complexity. Many measures for model complexity capture some form of capacity of a hypothesis class, which gives insight into the most complex function that could be learned - e.g. raw parameter counts and VC dimensions [BBL03]. The double descent and benign overfitting phenomena prominently highlighted that complexity measures that consider only what could be learned and not what is actually learned for test examples, would be unlikely to help understand generalization in deep learning [Bel21]. Further, [CJvdS23] highlighted that many other measures for model complexity \u2013 so-called measures of effective parameters (or: degrees of freedom) including measures from the literature of smoothers [HT90, Ch. 3.5] as well as measures relying on the model's Hessian [Moo91, Mac91] (which have been considered for use in deep learning in [MBW20]) \u2013 were derived in the context of in-sample prediction (where train- and test inputs would be the same) and do thus not allow to distinguish differences in the behavior of learned functions on training examples from new examples. [Cur24] highlight that this difference in setting \u2013 the move from in-sample prediction to measuring performance in terms of out-of-sample generalization \u2013 is crucial for the emergence of apparently counterintuitive modern machine learning phenomena such as double descent and benign overfitting. For this reason, [CJvdS23] proposed an adapted effective parameter measure for smoothers that can distinguish the two, and highlighted that differentiating between the amount of smoothing performed on train- vs test examples is crucial to understanding double descent in linear regression, trees and gradient boosting. In Sec. 4.1, we show that the telescoping model makes it possible to use [CJvdS23]'s effective parameter measure for neural networks, allowing interesting insight into implied differences in train- and test-time complexity of neural networks.\nGrokking. Similar to double descent in the number of training epochs as observed in [NKB+21] (where the test error first improves then gets worse and then improves again during training), the grokking phenomenon [PBE+22] demonstrated the emergence of another type of unexpected be-havior during the training run of a single model. Originally demonstrated on arithmetic tasks, the phenomenon highlights that improvements in test performance can sometimes occur long after perfect training performance has already been achieved. [LMT22] later demonstrated that this can also occur on more standard tasks such as image classification. This phenomenon has attracted much recent attention both because it appears to challenge the common practice of early stopping during training and because it showcases further gaps in our current understanding of learning dynamics. A number of explanations for this phenomenon have been put forward recently: [LKN+22] attribute grokking to delayed learning of representations, [NCL+23] use mechanistic explanations to examine case studies of grokking, [VSK+23] attribute grokking to more efficient circuits being learned later in training, [LMT22] attribute grokking to the effects of weight decay setting in later in training and [TLZ+22] attribute grokking to the use of adaptive optimizers. [KBGP24] highlight that the latter two explanations cannot be the sole reason for grokking by constructing an experiment where grokking occurs as the weight norm grows without the use of adaptive optimizers. Instead, [KBGP24, LJL+24] conjecture that grokking occurs as a model transitions from the lazy regime to a feature learning regime later in training. Finally, [LBBS24] show analytically and experimentally that grokking can also occur in simple linear estimators, and [MOB24] similarly study grokking outside neural net-works, including Bayesian models. Our perspective presented in Sec. 4.1 is complementary to these lines of work: we highlight that grokking coincides with the widening of a gap in effective parameters used for training and testing examples and that there is thus a quantifiable benign overfitting effect at play."}]}