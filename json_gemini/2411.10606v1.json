{"title": "AmoebaLLM: Constructing Any-Shape Large Language Models for Efficient and Instant Deployment", "authors": ["Yonggan Fu", "Zhongzhi Yu", "Junwei Li", "Jiayi Qian", "Yongan Zhang", "Xiangchi Yuan", "Dachuan Shi", "Roman Yakunin", "Yingyan (Celine) Lin"], "abstract": "Motivated by the transformative capabilities of large language models (LLMs) across various natural language tasks, there has been a growing demand to deploy these models effectively across diverse real-world applications and platforms. However, the challenge of efficiently deploying LLMs has become increasingly pronounced due to the varying application-specific performance requirements and the rapid evolution of computational platforms, which feature diverse resource constraints and deployment flows. These varying requirements necessitate LLMs that can adapt their structures (depth and width) for optimal efficiency across different platforms and application specifications. To address this critical gap, we propose AmoebaLLM, a novel framework designed to enable the instant derivation of LLM subnets of arbitrary shapes, which achieve the accuracy-efficiency frontier and can be extracted immediately after a one-time fine-tuning. In this way, AmoebaLLM significantly facilitates rapid deployment tailored to various platforms and applications. Specifically, AmoebaLLM integrates three innovative components: (1) a knowledge-preserving subnet selection strategy that features a dynamic-programming approach for depth shrinking and an importance-driven method for width shrinking; (2) a shape-aware mixture of LoRAs to mitigate gradient conflicts among subnets during fine-tuning; and (3) an in-place distillation scheme with loss-magnitude balancing as the fine-tuning objective. Extensive experiments validate that AmoebaLLM not only sets new standards in LLM adaptability but also successfully delivers subnets that achieve state-of-the-art trade-offs between accuracy and efficiency. Our code is available at https://github.com/GATECH-EIC/AmoebaLLM.", "sections": [{"title": "Introduction", "content": "The remarkable abilities and transformative impacts of large language models (LLMs) [1, 2, 3, 4] have been paralleled by a growing interest in deploying them across a wide range of real-world applications and diverse platforms. However, given the rapid evolution of computational platforms and varying application-specific requirements, the challenge of deploying LLMs efficiently on various platforms with differing specifications has become more pronounced. This is because diverse platforms often feature different resource constraints and deployment flows, necessitating LLMs with varying structures and shapes (i.e., depth and width) to achieve maximized execution efficiency, as affirmed by our profiling in Sec. 2. Moreover, even the same platform may have varying requirements for LLMs' execution efficiency depending on factors such as on-device battery status. These varying requirements demand a flexible framework capable of adapting to both the intrinsic hardware constraints and the extrinsic demands of diverse application scenarios."}, {"title": "Motivation and Profiling", "content": "Before introducing our framework, we first conduct a profiling of generation latency across different devices, deployment flows, and use cases to examine the demand for LLMs with adaptable structures that meet the diverse needs of various platforms and applications."}, {"title": "The Proposed AmoebaLLM Framework", "content": ""}, {"title": "AmoebaLLM: Methodology Overview", "content": "To address the challenges associated with traditional one-for-all network training, as mentioned in Sec. 1, our AmoebaLLM is equipped with three components: a knowledge-preserving subnet selection strategy, an SMoL adapter, and an in-place distillation fine-tuning objective with loss-magnitude balancing. We illustrate our overall framework in Fig. 2: given a target LLM, our AmoebaLLM endows it with the capability of instantly deriving capable subnets via a two-stage process.\nIn the first stage, AmoebaLLM generates the subnet selection strategy. Specifically, given the target depth/width remaining ratios, this step decides which layers/neurons to maintain, respectively. To maximally preserve the knowledge and language modeling capabilities of pre-trained LLMs, we propose employing dynamic programming [22] to determine the retained layers under different remaining ratios and leverage neuron importance metrics [8] to retain important neurons in a structured manner, as detailed in Sec. 3.2. After this stage, the subnet selection strategy is determined and fixed.\nIn the second stage, we insert our proposed SMoL adapter into the target LLM for a one-time, one-for-all fine-tuning. Specifically, SMoL is composed of a set of LoRAs [18] and employs a gating function to sparsely activate different subsets of LoRAs for different subnets, thus mitigating their gradient conflicts. This process is elaborated upon in Sec. 3.3. In addition, our fine-tuning objective enhances the sandwich sampling and in-place distillation methods described in [11, 13, 14] by adding a loss-magnitude balancing scheme. This prevents bias towards specific subnets in pre-trained LLMs, as detailed in Sec. 3.4. At deployment time, the selected LoRAs, which are determined by the extracted subnet shape favorable to the target platform, can be merged into the LLM weights."}, {"title": "AmoebaLLM: The Proposed Knowledge-Preserving Subnet Selection Strategy", "content": "Motivation. As detailed in Sec. 5, previous one-for-all training techniques [11, 12, 13, 14, 15] often select the first layers of a model or the first channels of a layer. These techniques, intended for models trained from scratch, are unsuitable for pre-trained LLMs with rich knowledge encoded in their weights. Considering the difficulty of recovering lost knowledge through fine-tuning or our"}, {"title": "DP-based depth shrinking.", "content": "Previous works have made diverse observations regarding the layer locations that store knowledge in different series of language models [24, 25, 26, 27, 28, 29]. As such, it is highly desirable to have a principled strategy to evaluate the joint contributions of different layer combinations in a target LLM to derive the optimal layer selection strategy for each remaining ratio. To achieve this goal, we propose a DP-based depth shrinking strategy.\nProblem formulation. Given a target LLM with N decoder layers, we define the layer selection strategy by a vector s \u2208 {0,1}N. Here, s[n] = 1 indicates that the n-th layer is retained; otherwise, the layer is removed. The objective is to determine the selection strategy s that achieves the optimal target metric, such as maximal accuracy or minimal perplexity (PPL), on a calibration dataset C, subject to the constraint that M layers are removed.\nKey hypothesis. Thanks to the residual structure [30] of common LLMs [1, 2, 3] and the observations that LLMs' knowledge is compositional across layers [24, 31, 32], we hypothesize that the layer selection problem described above can be divided into smaller and approximately independent sub-problems. Consequently, we can employ dynamic programming [22] to effectively and efficiently solve the layer selection problem.\nOur DP-based methodology. We define a DP table D[n][m] (where n \u2208 [1, N] and m\u2208 [1, M]), which stores the best target metric on the calibration dataset when exactly m layers are removed from the first n layers of the target LLM. The corresponding layer selection strategy is denoted as S[n][m] \u2208 {0,1}\u00d1. Consequently, S[N][M] represents the final strategy derived for removing M layers out of all N layers. Next, we elaborate on how to obtain D[n][m] and S[n][m], where we assume the target metric is such that larger values are better, without losing generality.\nAs illustrated in Fig. 2 (a), similar to general DP problems [22], D[n][m] can be derived by a recurrence relationship. Specifically, to derive each D[n][m], we compare the metrics achieved by the following two cases: (1) removing m layers from the first n 1 layers, and (2) removing m 1 layers from the first n - 1 layers and removing the n-th layer. The strategy yielding better metrics is adopted. More formally, this process can be formulated as follows:\n$D[n][m] = max (D[n-1][m], P(n, m))$  (1)\nwhere P(n, m) is the metric obtained by removing the n-th layer on top of the best-known strategy S[n-1][m-1] for removing m 1 layers from the first n 1 layers. This is computed as follows:\n$P(n,m) = evaluate(remove(S[n \u2013 1][m \u2013 1], n), C')$ (2)\nwhere remove(s, n) is a function that sets the n-th layer to 0 in a strategy s. Leveraging this recurrence relationship, after initializing the DP table with the base cases, i.e., D[i][0] = \u221e and S[i][0] = {1}N (Vi \u2208 [1, N]), since no layer is removed, the full DP table can be established with a complexity of O(MN). In practice, we set M as the maximum number of layers allowed to be removed in our"}, {"title": "Differences from and connections with previous methods.", "content": "The most relevant direction is layer pruning for LLMs. Pioneering works along this direction either utilize single-layer importance to determine which layers to prune [9], failing to measure the layers' joint contribution and thus aggressively losing factual knowledge as shown in Sec. 4, or rely on pre-defined rules [33], i.e., removing consecutive layers among the last ones, which is suboptimal and may not be generalizable to future LLMs. In contrast, our DP-based strategy can measure the joint impact of different layer combinations and is principled and fully automated without relying on human priors for specific LLMs. More importantly, the two aforementioned works [9, 33] are subsets of our DP-based strategy's solution space. Thus, it can serve as a new SOTA layer pruning method, as demonstrated in Sec. 4.3, in addition to serving as a component (i.e., the subnet selection strategy) in one-for-all fine-tuning. Previous works [34, 35] have also applied dynamic programming for pruning at different granularities with varied formulations, and our work is the first to introduce this classical approach to LLMs."}, {"title": "Importance-driven width shrinking.", "content": "Compared to layer selection, removing neurons (and all corresponding weight connections) for width shrinking is more fine-grained and involves a much larger design space. Therefore, instead of using dynamic programming, we directly employ existing structured pruning metrics for LLMs to obtain importance scores for each neuron and select the most important ones during one-for-all training, given a width remaining ratio.\nTo achieve this, we employ the metric in [8] due to its outstanding performance compared to other alternatives. Specifically, we adopt the same width remaining ratio for linear layers within the same block (either the self-attention block or the feed-forward network), where the importance scores of input neurons of the last linear layer in each block are used to determine the width configuration of this block. In other words, when a subset of input neurons to the last linear layer is removed, all associated neurons and weights [7] within this block will be removed in a structured manner.\nImportance metric. The importance score F of the i-th input neuron in the last linear layer We of the l-th block is computed as $F = E(X_{k,t,i}-\\bar{X}_{j,i})^2 ||W^e_{j,i}||^2_2$, where j is the index of the output neurons, Xti is the input features of the t-th token in the k-th batch, and X is the averaged input features across these two dimensions, both received by the i-th input neuron. To better maintain the LLMs' capability under a given remaining ratio: (1) the importance score is further normalized over all input neurons in each layer and globally sorted for non-uniform width shrinking; (2) a pre-computed bias term Be is added to the output neurons to compensate for the removed input neurons, i.e., $B^e = W^e((1 \u2013 M^e) X^e)$, where M\u00ba is the binary mask indicating whether the input neurons are retained. We refer the readers to [8] for more details."}, {"title": "AmoebaLLM: The Proposed Shape-aware Mixture of LoRAs", "content": "Motivation. As demonstrated in Sec. 4.3, joint weight fine-tuning of different subnets on small-scale datasets can lead to severe gradient conflicts [16, 17], resulting in poor performance across all subnets. One potential solution is to adopt parameter-efficient adapters like LoRA [18]. However, accumulating gradients from all subnets onto the same LoRA still suffers from gradient conflicts, making fine-tuning unstable and causing some subnets to underperform. On the other hand, tuning a separate LoRA for each subnet configuration is infeasible due to the large design space. To address this, we propose an intermediate solution to balance performance and efficiency: the SMoL adapter that features a set of LoRAs, which are sparsely activated based on the subnet shape.\nSMoL adapter design. Our SMoL adapter consists of a set of T LoRAs {AW; = B\u00bfAi}=1, which are sparsely activated for each subnet shape using a gating function G. Specifically, we employ a one-hot mask M to indicate the shape, i.e., the layer/width configuration, of the subnet. This mask is fed into the gating function G to calculate a score for each LoRA. Only the top k LoRAS are activated and weightedly averaged for each subnet shape during fine-tuning, thus mitigating the gradient conflicts among different subnets.\nImplementation. We extend the noisy top-K gating mechanism from [36] to implement our SMOL. Specifically, G(M) = Softmax(KeepTopK(H(M), k)), where KeepTopK(v, k)i is vi if vi is in the"}, {"title": "AmoebaLLM: The Fine-tuning Objective", "content": "Motivation. Strategically sampling and jointly fine-tuning different subnets is necessary to ensure high-performance subnets within the same LLM, where the sandwich sampling and in-place distilla-tion mechanisms [11, 13, 14] can serve as promising sampling and fine-tuning schemes, respectively. However, naively doing so can cause the larger subnets to gradually underperform during fine-tuning. We identify that this is due to notable differences in the loss magnitudes of different subnets, with smaller subnets, which diverge more from the well pre-trained full model, exhibiting much higher losses than larger subnets. To address this, we propose a simple but effective solution: equipping the in-place distillation with a loss-magnitude balancing mechanism, which we elaborate on as follows.\nIn-place distillation with loss-magnitude balancing. During each fine-tuning iteration, we employ the sandwich sampling [11, 13, 14] to sample K subnets {T}1 with different layer/width remaining ratios, including the largest/smallest ones and K \u2013 2 random ones from our design space. Detailed layer/width configurations of sampled subsets can be obtained from the strategies derived in Sec. 3.2. We fine-tune our SMoL adapter as detailed in Sec. 3.3 by accumulating the gradients from all sampled subnets using in-place distillation, where only the loss of the largest subnet T\u2081 is calculated using ground truth, while those of other subnets {T}2 use distillation from the largest one [11]. To balance the loss magnitude from different subnets, we normalize all subnets' loss magnitudes to that of the largest subnet, as visualized on the rightmost side of Fig. 2 (b). In this way, the final loss direction is jointly determined by all subnets' loss directions, falling on a unisphere without being severely impacted by their unbalanced magnitudes. We formulate the fine-tuning objective as follows:\n$L_{total} = L_{CE} (T_1(x), y) + \\sum_{i=2}^{K} \\frac{||L_{CE} (T_1(x), y) ||}{||L_{CE} (T_i(x), T_1(x)) ||} -L_{CE}(T_i(x), T_1(x))$ (3)\nwhere x and y are the input and ground truth, respectively, and LCE is a cross-entropy loss function.\nFinal subnet search after fine-tuning. We adopt a simple hierarchical search strategy to select sub-nets from the fine-tuned LLM to satisfy the target efficiency constraint while maximizing achievable accuracy. Specifically, we first perform a coarse grid search across uniformly spaced depth and width settings based on a small calibration set (e.g., 40 samples from the MMLU dataset) to identify subnets that meet the efficiency constraint with maximized accuracy. Next, we conduct a more fine-grained grid search within depth and width ranges surrounding the optimal subnet identified in the coarse grid search stage. More advanced search strategies, such as evolutionary search [11, 12, 13, 14], are left for future work."}, {"title": "Experimental Results", "content": ""}, {"title": "Experiment Setup", "content": "Baselines. Our baselines include two SOTA structured width pruning methods: LLM-Pruner [7] and FLAP [8], and one layer pruning method Shortened LLaMA [9]. All these baselines are open-sourced and we apply their official code to different LLMs. All baselines, including FLAP which were not fine-tuned in their original paper, are fine-tuned using the settings below for a fair comparison.\nFine-tuning setting. Following [7, 9], we adopt 50K samples from Alpaca [40] for our one-for-all fine-tuning as well as for fine-tuning all baselines. For both our method and the baselines, we adopt a constant learning rate of 2e-4 with an AdamW optimizer and a LoRA rank of 64, and fine-tune for 10K iterations. It takes 40 GPU hours on an NVIDIA A5000 GPU for our one-for-all fine-tuning."}, {"title": "Benchmark with SOTA LLM Compression Methods", "content": "We benchmark our AmoebaLLM against SOTA LLM width/layer pruning methods on LLaMA2 7B/Vicuna 7B v1.5 in Tab. 1/Tab. 2, respectively. Note that the subnets produced by AmoebaLLM are instantly extracted from the same one-for-all fine-tuned LLM, where the (depth, width scale) settings for 80%/65%/50% remaining ratios, determined by the final subnet search in Sec. 3.4, are (30, 0.875)/(28, 0.75)/(22, 0.75), respectively. We also provide the per-subnet fine-tuned counterparts of our delivered subnets, denoted as AmoebaLLM\u2020, to compare one-for-all and individual fine-tuning.\nBenchmark under comparable model sizes. As shown in Tab. 1 and Tab. 2, we observe that (1) the subnets instantly extracted by AmoebaLLM can achieve higher MMLU accuracy compared to all baselines, suggesting that our method better preserves the factual knowledge acquired during pre-training, as further analyzed in Sec. 4.3 and Sec. 4.4; (2) AmoebaLLM's delivered subnets, extracted from the same model, can also achieve better or comparable average commonsense reasoning accuracy compared to the strongest baselines, each trained separately; (3) AmoebaLLM\u2020 achieves the best performance across all metrics and tasks compared to the baselines, indicating that AmoebaLLM\u2020 can serve as a new SOTA LLM compression framework in addition to its one-for-all functionality, thus advancing the achievable accuracy-efficiency trade-off; (4) compared to our per-subnet fine-tuned variant AmoebaLLM\u2020, the instantly delivered subnets achieve comparable performance. This demonstrates the effectiveness of our one-for-all fine-tuning scheme, as further ablated in Sec. 4.3."}, {"title": "Ablation Study: Effectiveness of Each Component", "content": "We perform ablation studies to validate the effectiveness of each component of AmoebaLLM.\nThe DP-based depth shrinking strategy. We benchmark our DP-based strategy against two existing LLM layer pruning methods, Shortened LLaMA [9] and Unreasonable [33], on LLaMA2 7B. Specifically, we employ the three methods to select important layers using Wikitext2/MMLU with PPL/accuracy as calibration metrics, respectively. We directly report the achieved PPL/MMLU accuracy after calibration under various layer remaining ratios without fine-tuning to indicate their effectiveness in identifying important layers\nObservations and analysis. As shown in Tab. 3, we observe that our DP-based strategy outperforms the other two strategies on both calibration datasets and metrics, especially under small remaining ratios, e.g., a +9.4% MMLU accuracy and a -33.1 PPL over the strongest baseline when remaining 18 layers. This demonstrates the superiority of our method over the two baselines in selecting important layers that optimize the target calibration metric, thus significantly contributing to knowledge preservation.\nRemark. This set of experiments supports our analysis in Sec. 3.2 that the superiority of our method arises from its consideration of different layers' joint contributions, rather than focusing on single-layer importance [9], and its avoidance of reliance on pre-defined rules [33], thus ensuring generality.\nThe SMoL adapter. To assess the efficacy of our SMOL adapter, we substitute it with full model fine-tuning or the standard LoRA [18] and benchmark it against our AmoebaLLM with SMOL as"}, {"title": "Benchmark accuracy-latency trade-offs on real devices.", "content": "We further benchmark the achieved trade-off between average common-sense reasoning accuracy and measured latency of LLaMA2 7B using MLC-LLM and PyTorch as the deployment flows on an NVIDIA A5000 GPU, following the settings in Sec. 2. For our method, we select the subnet shape that favors the hardware characteristics based on the profiling in Sec. 2 from the one-for-all fine-tuned LLM. As shown in Fig. 3, we observe that (1) our method consistently achieves the best trade-off on both deployment scenarios; and (2) al-though FLAP achieves higher accuracy than Shortened LLaMA under comparable model sizes, its real-device speed is limited when using vanilla PyTorch, aligning with observations in [9]. In contrast, our method can instantly deliver subnets that favor the hardware/deployment flow characteristics, thus enjoying both high accuracy and real-device friendliness."}, {"title": "Ablation Study: The Selection of Calibration Datasets", "content": "We conduct an ablation study on the choice of calibration datasets for our DP-based depth shrinking introduced in Sec. 3.2. Specifically, we use accuracy on the training set of MMLU [49] and PPL on the training sets of Wikitext2 [50]/BookCorpus [51] as target metrics. We report the evaluation metrics, including MMLU test accuracy and Wikitext2 test PPL, under different layer remaining ratios both after calibration and after one-for-all fine-tuning under a depth-shrinking-only setting.\nObservations. As shown in Tab. 5, we observe that (1) after calibration without fine-tuning, the subnets perform well on the evaluation metric for which they were calibrated and underperform in terms of the other metric; (2) after fine-tuning, the subnets calibrated using PPL continue to perform poorly on MMLU accuracy, indicating a severe loss of factual knowledge. In contrast, the subnets calibrated using MMLU accuracy achieve notably lower PPL compared to before fine-tuning, even on par with the subnets calibrated using PPL, while still maintaining high MMLU accuracy.\nThe key insight. This set of experiments indicates that the loss of factual knowledge during com-pression is hard to restore during fine-tuning, echoing the observations in [23], while the language modeling capability is easier to recover through fine-tuning. As such, we adopted MMLU as the calibration dataset throughout the previous experiments, and we believe this insight could inspire future LLM compression frameworks and calibration metrics."}, {"title": "Limitations and Future Work", "content": "One limitation of our work is that due to the limited fine-tuning data and resources, our method-ology is applied to parameter-efficient fine-tuning, which mitigates gradient conflicts under small data conditions while limiting the achievable accuracy-efficiency trade-off. We anticipate that by leveraging more extensive fine-tuning data beyond our current use of Alpaca [40] and extending our design insights regarding subnet selection and gradient conflict mitigation, more aggressive accuracy-efficiency trade-offs can be achieved, which will be the focus of our future work."}, {"title": "Related Work", "content": "Large language models. Before the advent of LLMs, transformer-based language models [52, 53, 54, 55] demonstrated their ability to effectively analyze relationships among tokens in complex input sequences, facilitated by the attention mechanism [52]. These models also exhibit notable scalability [56, 57, 58] with respect to model size and the scale of pre-training datasets. This decent scalability has led to the emergence of LLMs, such as GLM [59], OPT [60], BLOOM [61], the Llama family [1, 1, 2], Gemma [3], and GPT-4 [4], which exhibit impressive zero-shot and few-shot in-context learning capabilities. However, these LLMs often feature billions of parameters and prohibitive computation complexity, which limits their widespread use across diverse platforms.\nLarge language model compression. To facilitate efficient deployment of LLMs in real-world applications, existing works primarily focus on compressing LLMs by extending traditional compres-sion techniques such as knowledge distillation [62, 63], quantization [64, 65, 66, 67, 68, 69], system acceleration [70, 71], and pruning [6, 5, 72]. Our work is most closely related to LLM pruning. Along this direction, early works [5, 6] employ unstructured and semi-structured pruning [73] by zeroing out connections among neurons. Despite their plausible performance, these methods require specialized support to achieve real-device speedup. To benefit commodity platforms, structured LLM pruning methods remove more coarse-grained components, such as all connections related to a single neuron [7, 8, 10] or even entire layers [9, 33]. For instance, LLM-Pruner [7] and FLAP [8] reduce LLM width by eliminating identified redundant neurons, while Sheared-LLaMA [10] learns a set of binary masks to reduce both the width and depth of LLMs. However, these methods either focus on a single dimension of compression (i.e., depth or width) with limited efficiency improvements, or they require a costly fine-tuning process for each target configuration and platform. In contrast, our AmoebaLLM can instantly extract subnets of arbitrary shapes that reach the accuracy-efficiency frontier, thus facilitating rapid deployment across devices.\nOne-for-all networks. Slimmable networks [74, 75] are pioneering works that enable a single model to operate at varying widths. Follow-up works [11, 76, 12, 13, 14, 15] further extend this approach to train more general one-for-all networks with switchable depth and width, thus enabling tasks like neural architecture search. In particular, BigNAS [11] builds one-for-all networks using a sandwich sampling strategy that samples a random set of subnets and jointly trains them in each iteration through in-place distillation, where the largest model guides the learning of the smaller ones. This approach has been inherited by subsequent works [12, 13, 14]. Additionally, this idea has been extended to any-precision networks [77, 78, 79] that allow switchable precision at runtime. A very recent work [80] has further extended this concept to any-precision LLMs.\nNevertheless, directly applying these methods to the depth and width of pre-trained LLMs would likely fail because their subnet sampling strategies often select the first layers of a model or the first channels of a layer, which are intended for models trained from scratch. This approach is unsuitable for pre-trained LLMs, as it may omit layers or neurons containing crucial knowledge. Our AmoebaLLM framework addresses these challenges by developing three key components: the subnet selection strategy, the trainable adapter design, and the fine-tuning objective. One concurrent work [81] also aims to train many-in-one LLMs that support instant subnet derivation, targeting a full-model continual training setting on 90 billion tokens. In contrast, our method targets a parameter-efficient tuning setting with only 8.5 million fine-tuning tokens."}, {"title": "Conclusion", "content": "In this work, we present a framework called AmoebaLLM that grants a given LLM the capability to instantly deliver subnets of arbitrary shapes, which achieve the accuracy-efficiency frontier and can be extracted immediately after a one-time fine-tuning. This is achieved by the development of three dedicated components that enable AmoebaLLM's one-for-all fine-tuning scheme: a knowledge-preserving subnet selection strategy, an SMoL adapter, and an in-place distillation objective with loss-magnitude balancing. Extensive experiments validate that our AmoebaLLM framework can deliver efficient LLMs with instantly serviceable subnets of any shape, which outperform SOTA LLM compression techniques in terms of the accuracy-efficiency trade-off. We believe this work is promising to facilitate the wider use of existing and emerging public LLMs by making them instantly deployable on varying platforms and applications, and by providing a new perspective on efficient LLM deployment, thus inspiring future solutions."}]}