{"title": "3D Multi-Object Tracking with Semi-Supervised GRU-Kalman Filter", "authors": ["Xiaoxiang Wang", "Jiaxin Liu", "Miaojie Feng", "Zhaoxing Zhang", "Xin Yang"], "abstract": "3D Multi-Object Tracking (MOT), a fundamental component of environmental perception, is essential for intelligent systems like autonomous driving and robotic sensing. Although Tracking-by-Detection frameworks have demonstrated excellent performance in recent years, their application in real-world scenarios faces significant challenges. Object movement in complex environments is often highly nonlinear, while existing methods typically rely on linear approximations of motion. Furthermore, system noise is frequently modeled as a Gaussian distribution, which fails to capture the true complexity of the noise dynamics. These oversimplified modeling assumptions can lead to significant reductions in tracking precision. To address this, we propose a GRU-based MOT method, which introduces a learnable Kalman filter into the motion module. This approach is able to learn object motion characteristics through data-driven learning, thereby avoiding the need for manual model design and model error. At the same time, to avoid abnormal supervision caused by the wrong association between annotations and trajectories, we design a semi-supervised learning strategy to accelerate the convergence speed and improve the robustness of the model. Evaluation experiment on the nuScenes and Argoverse2 datasets demonstrates that our system exhibits superior performance and significant potential compared to traditional TBD methods.The code is available at https://github.com/xiang-1208/GRUTrack.", "sections": [{"title": "I. INTRODUCTION", "content": "Multi-Object Tracking (MOT) [1] is a crucial research topic within the field of computer vision and serves as a foundational technology in numerous intelligent applications, such as autonomous driving, traffic flow analysis, security surveillance, robotics, and action recognition.\nAt present, with the increasing performance of Multi-Object Detection (MOD) [2]\u2013[5], Multi-Object Tracking (MOT) methods based on the \"Tracking-by-Detection\" (TBD) [1], [6], [7] have demonstrated superior accuracy and robustness. These TBD methods follow the motion process of the tracked object, contrasting with \"Joint Detection and Tracking\" (JDT) approaches [2], [8], [9] which do not perform as well. In general, TBD methods update the state of tracked objects incrementally by constructing motion model and employing recursive Bayesian filter estimator. However, due to the varied motion characteristics of different objects within the scene, a single state space (SS) and estimator parameter cannot well match the different motion characteristics between various categories, which reduces the consistency between the motion state update and the actual, leading to false matching and inaccurate state update. Some approaches [7], [10], [11] take note of this and design motion parameters or association strategies for each class to be more relevant to the different characteristics of different classes.\nHowever, these methods fail to fundamentally solve the problem of multi-category differences. On the one hand, with the continuous addition or refinement of categories, it is not only tedious to design motion models for each category, but also too dependent on the designer's experience. On the other hand, although it is possible to capture different types of motion characteristics by continuously refining the categories, most methods still use model-based state filters, such as Kalman Filter (KF) [12] and Extended Kalman Filter (EKF) [13], [14]. The effectiveness of these models depends on the accuracy of the state model and the validity of the motion hypothesis. In the actual MOT, the latent state of the system is nonlinear and complex, and it is even difficult to be accurately described as a tractable state model. In this case, model-based state estimators typically simplify the motion dynamics by linearizing the process and assuming the system noise follows a Gaussian distribution. This assumption does not match the actual situation, and these modeling inaccuracies often bring the loss of tracking precision to the system.\nTo this end, we propose a partially learnable MOT method by introducing a Gated Recurrent Unit (GRU)-based Kalman filter into the motion module of TBD. This method can replace the traditional manual model design in a data-driven manner, thus eliminating the need to design a unique SS and estimator for each class. Specifically, we use multiple GRUs to simulate the loops in Recursive Bayesian Filter-ing. The model automatically learns the noise distribution, state transition matrix and observation matrix. It avoids the mismatch of noise modeling and the loss of precision caused by linearizing the state transition and observation function. This is doable in theory because neural network-based state estimation has been shown to capture the motion characteristics of complex processes [15], [16], which is also applicable to state transitions in MOT.\nHowever, it is not feasible to directly use the learnable Kalman Filter in MOT. On the one hand, due to the partial annotation of the dataset, the amount of trainable data is small, which is easy to overfitting. On the other hand, since the annotations and trajectories are associated by a hand-designed association strategy, the errors of association will introduce error supervision into the system. Therefore, we propose to parallel a Kalman filter during the training process to generate pseudo-labels for those unlabeled data for semi-supervised training. The experimental results prove the effectiveness of our strategy.\nSpecifically, our contributions are as follows:\n\u2022 We propose a data-driven MOT method by using a GRU-based motion module to avoid the precision loss of traditional methods for noise mismatch modeling and motion process linearization.\n\u2022 We design a pseudo-label-based semi-supervised method, which greatly expands the amount of available training data and label robustness, so that the system can converge in fewer training cycles.\n\u2022 We evaluate our method on the nuScenes [17] and Argoverse2 [18] datasets. Our system demonstrates performance comparable to traditional MOT systems and strong level of generalization, while obviating the need for manually designing a model for each object category."}, {"title": "II. RELATED WORK", "content": "Lidar-based 3D MOT has a similar form to image-based MOT. However, the lidar point cloud provides more precise spatial and depth information, significantly enhancing track-ing precision. This improvement has led to its widespread adoption in robotic sensing and other applications. Most Lidar-based 3D MOT algorithms can be categorized into two primary paradigms: \u201cTracking-by-Detection\" and \"Joint Detection and Tracking\".\nA. TBD\nLeveraging increasingly advanced detectors, AB3DMOT [1] establishes a baseline method for 3D MOT based on filters and 3D Intersection over Union (3D IoU), which provides a foundation for the design of TBD methods. SimpleTrack [6] offers a detailed analysis of the strengths and limitations of various models, including the widely used Kalman filter and the constant velocity model, as well as different data association metrics. It proposes specific improvements for each module and integrates them into a streamlined baseline method, achieving competitive results on the Waymo Open Dataset and nuScenes.\nAdditionally, Poly-MOT [7] incorporates geometric con-straints into the motion model and develops multiple motion models tailored to the distinct characteristics of different ob-ject categories. It introduces three custom similarity measures and a novel two-stage data association strategy, enabling the identification of the most suitable similarity measure for each object category and reducing mismatches. This approach further optimizes TBD trackers, resulting in superior tracking performance on the NuScenes dataset.\nSome methods aim to improve tracking performance by integrating image features. For instance, EagerMOT [10] combines image detection results with Lidar detection data to achieve comprehensive scene perception. CAMO-MOT [11] also leverages both camera and Lidar data to significantly mitigate tracking failures caused by occlusions and false de-tections. Additionally, CAMO-MOT introduces an occlusion head to effectively select optimal object appearance features multiple times, further reducing the impact of occlusions.\nB. JDT\nMotiontrack [19] proposed an end-to-end Transformer-based [20] JDT algorithm, which was based on the previous end-to-end detection work Transfusion [21], and further proposed a Transformer-based data association module and query enhancement module. In addition, following Guillem et al. [22], OGR3MOT [9] uses graph neural networks to solve the 3D MOT problem, and achieves the best IDS metrics so far.\nC. Recursive Bayesian Filtering\nIn MOT, predicting and updating existing trajectories is a crucial aspect. KF [12] is used for linear Gaussian state models. However, in practice, many problems do not perfectly adhere to the linear Gaussian model. Consequently, nonlinear filters, such as the EKF [13], are employed for approximate processing. It performs recursive computation by linearizing the state forward function and the observation function.\nThe model-based algorithms aforementioned, which rely on accurate knowledge of the SS model, often experience significant performance degradation when there is a mis-match between the actual motion model and the modeled one [3]. Recently, there has been an increasing focus on integrating machine learning with SS models. DNN-based algorithms typically encode observations to fit a simplified SS model, and then track the parameters of these implicit SS, as exemplified by KFNet [23]. Additionally, some approaches [24] incorporate graph neural network (GNN) alongside Kalman filters to enhance filter accuracy through neural augmentation. However, such algorithms are often designed for unknown or highly complex SS models that lack mathematical interpretability and are generally not suited for real-time estimation due to their computational demands.\nIn contrast, KalmanNet [25] offers a novel approach by combining model-based Kalman filtering with RNN to address model mismatch and nonlinearity. Our work builds upon KalmanNet, applying it to practical MOT scenarios. This approach eliminates the need for manual design of multiple SS models and the selection of various filters for different classes, while avoiding additional computational complexity."}, {"title": "III. METHOD", "content": "A. 3D MOT Pipeline\nOur system can be divided into four parts: the pre-processing module, motion module, association module, and trajectory management module, as shown in Fig. 1.\n1) Pre-processing Module: 3D MOD typically generates multiple bounding boxes for the same detection to minimize the risk of missed and false detections. To prevent these detections from causing redundant ID switches, we prepro-cess the original detections $D_n$ to reduce false matches. This preprocessing generally involves applying score filter-ing and non-maximum suppression (NMS) to each frame's detections, retaining only the bounding box with the highest confidence for each object. After preprocessing, our detection boxes $D_n = [x, y, z, w,l, h, v, \\theta]$ include the center position of the bounding box, its dimensions, its velocity, and the heading angle.\n2) Motion Module: The motion module is primarily responsible for predicting the state $X_{n-1} = [\\hat{x}_{n-1}^1,...,\\hat{x}_{n-1}^{num-tra}]$ of the tracked trajectories and updating the state based on observations $Y_n = [y_n^1,..., y_n^{num\\_det}]$. Our design focuses on this module.\nThe process of traditional motion and prediction, taking EKF as an example, is shown in Fig. 2. The system con-tinuously updates the state through the prior state $\\hat{x}_{n-1}$, the observation $y_n$, the process noise covariance matrix Q, the measurement noise covariance matrix R, and the continuously maintained state covariance P. Specifically, there are two steps:\na) Prediction Step: In this step, the system predicts the current prior state based on the posterior state from the previous time step, following the predefined SS model:\n$\\hat{x}_{n|n-1} = f(\\hat{x}_{n-1}).$ (1)\nSimultaneously, the uncertainty in the state, represented by the covariance, must also be predicted:\n$P_{n|n-1} = F \\cdot P_{n-1} F^T + Q.$ (2)\nb) Update Step: The first step involves calculating the Kalman gain $K_n$, which balances the weights between the prediction and the observation:\n$K_n = P_{n|n-1} \\cdot H^T \\cdot (H \\cdot P_{n|n-1} \\cdot H^T + R)^{-1}$, (3)\n$S_{n|n-1} = H \\cdot P_{n|n-1} \\cdot H^T + R$, (4)\nwhere $S_{n|n-1}$ is the observation error covariance.\nAgain according to the current observation, update the current state of posterior:\n$\\hat{x}_{n} = \\hat{x}_{n|n-1} + K_n\\cdot (y_n - h(\\hat{x}_{n|n-1})).$ (5)\nMeanwhile, the error covariance is updated as follows:\n$P_n = P_{n|n-1} - K_n S_{n|n-1} K_n^T$ (6)\nHere, the EKF uses the Jacobian matrix F and H to linearize the differentiable functions f(x) and h(x) in a time-dependent manner.\nThis approach relies heavily on the accuracy of the SS model setup, which often depends on the designer's expe-rience and is difficult to transfer. Moreover, for nonlinear motion, the linearization of the state transition and observa-tion equations can introduce additional errors. Additionally, in MOT, the Kalman filter requires manual adjustment of the observation noise and process noise, assuming they follow a multi-dimensional Gaussian distribution. However, in MOT, observations are derived from upstream object detection results, which do not necessarily conform to a Gaussian distribution. Thus, explicitly modeled noise may not adequately address the needs of MOT.\nIt is reasonable to believe that the differences in motion for different classes are often also reflected in the state. If the model can adaptively obtain different Kalman gain results based on the observations and state, it can then be applied to all classes and adjust the noise accordingly.\nGRU-Kalman Filter: We introduce the learnable Kalman filter [25] into our system, using a uniform SS model across all classes. The specific neural network architecture is illustrated in Fig. 3. Following the design of KalmanNet, we represent each second-order statistical moment of the Kalman filter using separate GRU, with fully connected (FC) layers interspersed between the GRU, and dedicated input and output layers.\nSpecifically, we use the first GRU to track the process noise covariance matrix, which combines the previous pro-cess noise covariance matrix $Q_{n-1}$ and the state forward update difference $\\Delta \\hat{x}_n = \\hat{x}_n - \\hat{x}_{n|n-1}$ to infer the current process noise covariance matrix $Q_n$.\nThe second GRU is employed to simulate the reasoning process of Eq. 2, thereby circumventing the need to linearize f(x) in order to design F. It combines the process noise co-variance matrix and the forward evolution difference $\\Delta \\hat{x}_n = \\hat{x}_n - \\hat{x}_{n|n-1}$, along with the previous state error covariance $P_{n-1|n-2}$, to infer the current state error covariance $P_{n|n-1}$.\nThe third GRU is designed to simulate the reasoning process of the observation formula in Eq. 4, avoiding the linearization of h(x) required to design H. It integrates the state covariance $P_{n|n-1}$, the observation difference$\\Delta \\tilde{y}_n = y_{n-1}$, the innovation difference $\\Delta y_n = y_n - \\hat{y}_{n|n-1}$, and the previous observation error covariance $\\tilde{S}_{n-1|n-2}$ to derive the current observation error covariance $\\tilde{S}_{n|n-1}$.\nFinally, simulating the traditional Kalman filter process, we learn the H matrix through an output layer, which takes the state error covariance and observation error covariance as inputs and ultimately outputs the Kalman gain $K_n$. Com-pared with end-to-end DNN, this network structure emulates part of the model-based Kalman filter process, making it more parameter-efficient and easier to train.\n3) Association Module: Following the baseline Poly-MOT, we use a two-stage association strategy to reduce false negative associations. In the One-stage Association, we use the 3D Generalized Intersection over Union (GIOU3D) to as the match metric between the tracked trajectories and the detections. After One-stage Association, we perform a wider threshold Two-stage Association between mismatched trajectories and mismatched detections. This association will be performed in Bird's Eye View.\nGIOU3D (T_{n-1}, D_n) = \\frac{T_{n-1} \\cap D_n}{T_{n-1} \\cup D_n} - \\frac{|Hull(T_{n-1}, D_n) - T_{n-1} \\cup D_n|}{|T_{n-1} \\cup D_n|}.$ (7)\nBy combining the two-stage association, we effectively obtain three results: updated trajectories $T^D$, mismatched trajectories$T^{\\tilde{T}}$, and mismatched detections $D^u$.\n4) Trajectory Management Module: Like most 3D-MOT methods, our system employs a confidence-based trajectory lifecycle management approach [26]. Specifically, when the tracker receives a detection $D^u$ that is not associated with an existing trajectory, it is initialized as a new trajectory. During subsequent tracking, if a observation is associated with this trajectory, corresponding to $T^D$, the state of the trajectory is updated based on the new observation. However, if the trajectory is not observed in consecutive framesT_1, its confidence decays following an exponential function until it is ultimately deleted.\nB. Training methods\nOur design also extends to the training methodology. The prototype system, KalmanNet, is trained in a supervised manner using labeled datasets. The squared error is computed between the estimated value $\\hat{X}_n$ and the true value $X_n$, as shown below:\n$\\mathcal{L}=\\sum_{seq} \\sum_{n=0}^{\\text{seq}}||X_n - \\hat{X}_n||_2^2$ (8)\nAlthough the system output is the Kalman gain, supervising with respect to the state variable $X_n$ is not direct supervision. However, it is still effective because:\n$\\frac{\\partial ||K_n \\Delta y_n - \\Delta X_n||^2}{\\partial L_n} = 2 \\cdot (K_n \\cdot \\Delta y_n - \\Delta X_n) \\cdot \\Delta \\frac{\\partial L_n}{\\partial K_n}$ (9)\nwhere $\\Delta X_n = X_n - \\hat{X}_{n|n-1}$. The loss is differentiable with respect to the Kalman gain, meaning that KalmanNet can be trained end-to-end to compute the Kalman gain by minimizing the squared error.\nHowever, in KalmanNet, the training data is automati-cally generated and can be collected in unlimited quantities. Automatically generated labels ensure a strict correspon-dence between labels and samples. However, in MOT, the annotations is provided by the labeled bounding boxes in the dataset. On the one hand, the number of annotations labels is typically smaller than the number of objects being tracked. On the other hand, associating annotations bounding boxes with trajectories often relies on methods such as Euclidean distance or IoU. Regardless of the association method used, there is no guarantee that the annotations strictly correspond to the system's trajectories, which brings anomalous supervision to the system.\nTo address these issues, we adopt a semi-supervised train-ing method based on pseudo-labels. Specifically, in addition to using the annotations to provide the true state values for supervised training, we also parallelize the training process with an EKF. For trajectories that are not associated with annotated boxes, we use the system state obtained by the EKF as a pseudo-label for training. Consequently, our final loss function is as follows:\n$\\mathcal{L} = \\sum_{n} ||X_n - \\hat{X}_n^i||_2^2 + \\lambda \\sum_n ||\\hat{X}_n - X_n||_1^2,$ (10)\nwhere $X_n$ is the true annotation of the dataset, $\\hat{X}_n$is the update output of traditional Kalman filter, and $\\hat{X}_n^i$ is the update output of GRU-Kalman filter."}, {"title": "IV. EXPERIMENTS", "content": "A. The dataset\n1) NuScenes: The nuSceness [17] dataset comprises 850 training sequences and 150 validation sequences, each con-sisting of approximately 40 frames. Keyframes are sam-pled at a rate of 2Hz, with annotations provided for each keyframe. These annotations include geometric details of the object bounding boxes and their unique identifiers within the scene. The official evaluation primarily employs accuracy AMOTA as the key performance metric, precision AMOTP, ID Switch (IDS) as the secondary performance metric.\n2) Argoverse2: Argoverse2 [18] expands on Argoversel [31] by collecting 1000 scene clips in six US cities. Each sequence lasted 15 seconds, sampled and annotated at 10Hz, with an average of 75 annotated objects per frame. The dataset features over 30 object classes and encompasses multiple complex urban environments. The official evaluation uses HOTA as the key metric.\nB. Implementation Details\nThe system input consists of the detection results from the 3D MOD. For dataset splitting, we follow the official division of training, validation, and test sets. Regarding training parameters, we employ the AdamW optimizer with a maximum learning rate le-5 and weight decay le-5. The CosineAnnealingLR adjustment method and smooth-loss su-pervision are applied to help the model avoid local minima. Gradient accumulation is performed across each object pair per frame in the sequence, followed by backpropagation at the end of the sequence.\nC. Experimental Results\n1) Comparative evaluation:: We compare our approach with several methods on the validation and test sets of the NuScenes dataset, as well as on the validation set of the Argoverse2 dataset.\nNuScenes Test Set: We compared our system with other tracking algorithms, such as the TBD baselines AB3DMOT, CenterTrack, SimpleTrack, Poly-MOT, and JDT method OGR3MOT. The selected algorithms include both traditional tracking methods and machine learning-based approaches.\nAs shown in TABLE I, according to the AMOTA results, our proposed GRU-based semi-supervised method achieves better performance than most mainstream hand-designed methods using the same detector (CenterPoint [2]). Com-pared to OGR3MOT with GNN, it also demonstrates superior performance. Furthermore, we obtain the best AMOTP re-sults, indicating that our system provides more accurate final trajectory box information than the traditional Kalman model and constant velocity model. Overall, compared with most TBDs and JDTs, we exhibit excellent performance without the need for manually designing motion models, highlighting great potential for future development.\nNuScenes Val Set: As shown in Table III, we evaluated our system on the validation set using the same model configuration, and once again, it demonstrated stable and consistent performance.\nArgoverse2 Val Set: We evaluated our method on the Argoverse2 validation set in Table II, which includes ground-truth labels for 30 object classes. Given that most MOT algorithms require modification before being applied to Ar-goverse2, we compared our method with the Argoverse2 baseline, LT3D, using tracking method such as greedy and ab3dmot tracker. We present results for the seven most preva-lent dynamic object categories in Argoverse2 and report the overall average HOTA score. Importantly, we used the model trained on the NuScenes training set, and its performance on Argoverse2 demonstrates a strong level of generalization.\n2) Ablation Study: In this part, we conduct extensive ablation experiments to evaluate the performance of the pro-posed module. We used the same preprocessing parameters and association parameters, and then replaced the motion module as well as different training strategies for a series of experiments.\nGRU-based Motion Module: We select the two most rep-resentative categories to investigate the tracking performance performance of different SS and optimizers on these two categories separately. As can be seen from the Table IV, the performance of the traditional methods represented by Poly-MOT will vary under different SS models and optimizers, which is precisely caused by the high dependence of the traditional motion module on accurate modeling. However, our data-driven based method, achieves the best tracking results in both categories. And changing SS does not affect the tracking accuracy, which reflects the excellent robustness of our motion module.\nSemi-Supervised: To verify the effectiveness of our pro-posed semi-supervised method, we compared the fully su-pervised and semi-supervised training processes and results on the validation set. As shown in Figure 4, our semi-supervised approach achieved convergence within 1,700 steps, or roughly 2 epochs, reaching an AMOTA score close to 0.7. In contrast, when training solely with only annotations from dataset, the system took 3 epochs to converge, and the final performance was lower. This discrepancy is attributed to the limited amount of training data and errors introduced during the association between annotations and the tracked samples, which semi-supervised learning helps to mitigate."}, {"title": "V. CONCLUSIONS", "content": "In this work, we propose a partially learnable MOT method by introducing a GRU-based Kalman filter [25] into the motion module of the TBD framework. This method eliminates the adverse effects of inaccurate noise parameter-ization and reduces the error of linearization. Our findings demonstrate that the GRU-based motion module is well-suited for MOT in autonomous driving and robotic sensing environments. Additionally, we introduce a semi-supervised training strategy that leverages pseudo-labels, which accel-erates training by increasing data volume and minimizing association errors. We believe that combining deep learning methods with interpretable mathematical models can enhance 3D MOT performance, and foresee extending this approach to other modules, potentially leading to the development of parameter-free trackers."}]}