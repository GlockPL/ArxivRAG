{"title": "Maverick:\nEfficient and Accurate Coreference Resolution Defying Recent Trends", "authors": ["Giuliano Martinelli", "Edoardo Barba", "Roberto Navigli"], "abstract": "Large autoregressive generative models have\nemerged as the cornerstone for achieving the\nhighest performance across several Natural\nLanguage Processing tasks. However, the urge\nto attain superior results has, at times, led\nto the premature replacement of carefully de-\nsigned task-specific approaches without exhaus-\ntive experimentation. The Coreference Res-\nolution task is no exception; all recent state-\nof-the-art solutions adopt large generative au-\ntoregressive models that outperform encoder-\nbased discriminative systems. In this work,\nwe challenge this recent trend by introducing\nMaverick, a carefully designed - yet simple\npipeline, which enables running a state-of-\nthe-art Coreference Resolution system within\nthe constraints of an academic budget, outper-\nforming models with up to 13 billion param-\neters with as few as 500 million parameters.\nMaverick achieves state-of-the-art performance\non the CoNLL-2012 benchmark, training with\nup to 0.006x the memory resources and ob-\ntaining a 170x faster inference compared to\nprevious state-of-the-art systems. We exten-\nsively validate the robustness of the Maver-\nick framework with an array of diverse ex-\nperiments, reporting improvements over prior\nsystems in data-scarce, long-document, and\nout-of-domain settings. We release our code\nand models for research purposes at https:\n//github.com/SapienzaNLP/maverick-coref.", "sections": [{"title": "Introduction", "content": "As one of the core tasks in Natural Language Pro-\ncessing, Coreference Resolution aims to identify\nand group expressions (called mentions) that refer\nto the same entity (Karttunen, 1969). Given its\ncrucial role in various downstream tasks, such as\nKnowledge Graph Construction (Li et al., 2020),\nEntity Linking (Kundu et al., 2018; Agarwal et al.,\n2022), Question Answering (Dhingra et al., 2018;\nDasigi et al., 2019; Bhattacharjee et al., 2020;\nChen and Durrett, 2021), Machine Translation\n(Stojanovski and Fraser, 2018; Voita et al., 2018;\nOhtani et al., 2019; Yehudai et al., 2023) and Text\nSummarization (Falke et al., 2017; Pasunuru et al.,\n2021; Liu et al., 2021), inter alia, there is a press-\ning need for both high performance and efficiency.\nHowever, recent works in Coreference Resolution\neither explore methods to obtain reasonable per-\nformance optimizing time and memory efficiency\n(Kirstain et al., 2021; Dobrovolskii, 2021; Otmaz-\ngin et al., 2022), or strive to improve benchmark\nscores regardless of the increased computational\ndemand (Bohnet et al., 2023; Zhang et al., 2023).\nEfficient solutions usually rely on discriminative\nformulations, frequently employing the mention-\nantecedent classification method proposed by Lee\net al. (2017). These approaches leverage relatively\nsmall encoder-only transformer architectures (Joshi\net al., 2020; Beltagy et al., 2020) to encode docu-\nments and build on top of them task-specific net-\nworks that ensure high speed and efficiency. On\nthe other hand, performance-centered solutions\nare nowadays dominated by general-purpose large\nSequence-to-Sequence models (Liu et al., 2022;\nZhang et al., 2023). A notable example of this\nformulation, and currently the state of the art in\nCoreference Resolution, is Bohnet et al. (2023),\nwhich proposes a transition-based system that incre-\nmentally builds clusters of mentions by generating\ncoreference links sentence by sentence in an autore-\ngressive fashion. Although Sequence-to-Sequence\nsolutions achieve remarkable performance, their\nautoregressive nature and the size of the underly-\ning language models (up to 13B parameters) make\nthem dramatically slower and memory-demanding\ncompared to traditional encoder-only approaches.\nThis not only makes their usage for downstream ap-\nplications impractical, but also poses a significant\nbarrier to their accessibility for a large number of\nusers operating within an academic budget.\nIn this work we argue that discriminative\nencoder-only approaches for Coreference Reso-"}, {"title": "Related Work", "content": "We now introduce well-established approaches to\nneural Coreference Resolution. Specifically, we\nfirst delve into the details of traditional discrimi-\nnative solutions, including their incremental varia-\ntions, and then present the recent paradigm shift for\napproaches based on large generative architectures."}, {"title": "Discriminative models", "content": "Discriminative approaches tackle the Coreference\nResolution task as a classification problem, usu-\nally employing encoder-only architectures. The\npioneering works of Lee et al. (2017, 2018) intro-\nduced the first end-to-end discriminative system for\nCoreference Resolution, the Coarse-to-Fine model.\nFirst, it involves a mention extraction step, in which\nthe spans most likely to be coreference mentions\nare identified. This is followed by a mention-\nantecedent classification step where, for each ex-\ntracted mention, the model searches for its most\nprobable antecedent (i.e., the extracted span that ap-\npears before in the text). This pipeline, composed\nof mention extraction and mention-antecedent clas-\nsification steps, has been adopted with minor modi-\nfications in many subsequent works, that we refer\nto as Coarse-to-Fine models."}, {"title": "Coarse-to-Fine Models", "content": "Among the works that\nbuild upon the Coarse-to-Fine formulation, Lee\net al. (2018), Joshi et al. (2019) and Joshi et al.\n(2020) experimented with changing the underlying\ndocument encoder, utilizing ELMO (Peters et al.,"}, {"title": "Incremental Models", "content": "Discriminative systems\nalso include incremental techniques. Incremen-\ntal Coreference Resolution has a strong cognitive\ngrounding: research on the \"garden-path\" effect\nshows that humans resolve referring expressions\nincrementally (Altmann and Steedman, 1988).\nA seminal work that proposed an automatic in-\ncremental system was that of Webster and Curran\n(2014), which introduced a clustering approach\nbased on the shift-reduce paradigm. In this formula-\ntion, for each mention, a classifier decides whether\nto SHIFT it into a singleton (i.e., single mention\ncluster) or to REDUCE it within an existing cluster.\nThe same approach has recently been reintroduced\nin ICoref (Xia et al., 2020) and longdoc (Toshniwal\net al., 2021), which adopted SpanBERT and Long-\nFormer, respectively. In these works the mention\nextraction step is identical to that of Coarse-to-Fine\nmodels. On the other hand, the mention clustering\nstep is performed by using a linear classifier that\nscores each mention against a vector representa-\ntion of previously built clusters, in an incremental\nfashion. This method ensures constant memory us-\nage since cluster representations are updated with\na learnable function. In Section 3.2 we present\na novel performance-driven incremental method\nthat obtains superior performance and generaliza-\ntion capabilities, in which we adopt a lightweight\ntransformer architecture that retains the mention\nrepresentations."}, {"title": "Sequence-to-Sequence models", "content": "Recent state-of-the-art Coreference Resolution sys-\ntems all employ autoregressive generative ap-\nproaches. However, an early example of Sequence-\nto-Sequence model, TANL (Paolini et al., 2021),\nfailed to achieve competitive performance on\nOntoNotes. The first system to show that the au-\ntoregressive formulation was competitive was ASP\n(Liu et al., 2022), which outperformed encoder-\nonly discriminative approaches. ASP is an autore-\ngressive pointer-based model that generates actions\nfor mention extraction (bracket pairing) and then\nconditions the next step to generate coreference\nlinks. Notably, the breakthrough achieved by ASP\nis not only due to its formulation but also to its us-\nage of large generative models. Indeed, the success\nof their approach is strictly correlated with the un-\nderlying model size, since, when using models with\na comparable number of parameters, the perfor-\nmance is significantly lower than encoder-only ap-\nproaches. The same occurs in Zhang et al. (2023),\na fully-seq2seq approach where a model learns to\ngenerate a formatted sequence encoding corefer-\nence notation, in which they report a strong positive\ncorrelation between performance and model sizes.\nFinally, the current state-of-the-art system on\nthe OntoNotes benchmark is held by Link-Append\n(Bohnet et al., 2023), a transition-based system that\nincrementally builds clusters exploiting a multi-\npass Sequence-to-Sequence architecture. This ap-\nproach incrementally maps the mentions in previ-\nously coreference-annotated sentences to system\nactions for the current sentence, using the same\nshift-reduce incremental paradigm presented in\nSection 2.1. This method obtains state-of-the-art\nperformance at the cost of using a 13B-parameter\nmodel and processing one sentence at a time, drasti-\ncally increasing the need for computational power.\nWhile the foregoing models ensure superior per-\nformance compared to previous discriminative ap-\nproaches, using them for inference is out of reach\nfor many users, not to mention the exorbitant cost\nof training them from scratch."}, {"title": "Methodology", "content": "In this section, we present the Maverick frame-\nwork: we propose replacing the preprocessing and\ntraining strategy of Coarse-to-Fine models with a\nnovel pipeline that improves the training and infer-\nence efficiency of Coreference Resolution systems.\nFurthermore, with the Maverick Pipeline, we elim-"}, {"title": "Maverick Pipeline", "content": "The Maverick Pipeline combines i) a novel mention\nextraction method, ii) an efficient mention regular-\nization technique, and iii) a new mention pruning\nstrategy."}, {"title": "Mention Extraction", "content": "When it comes to extract-\ning mentions from a document D, there are differ-\nent strategies to model the probability that a span\ncontains a mention. Several previous works follow\nthe Coarse-to-Fine formulation presented in Sec-\ntion 2.1, which consists of scoring all the possible\nspans in D. This entails a quadratic computational\ncost in relation to the input length, which they miti-\ngate by introducing several pruning techniques.\nIn this work, we employ a different strategy. We\nextract coreference mentions by first identifying\nall the possible starts of a mention, and then, for\neach start, extracting its possible end. To extract\nstart indices, we first compute the hidden represen-\ntation $(x_1,...,x_n)$ of the tokens $(t_1, ..., t_n) \\in D$\nusing a transformer encoder, and then use a fully-\nconnected layer F to compute the probability for\neach $t_i$ being the start of a mention as:\n$F_{start}(x) = W'_{start}(GeLU(W_{start}x))$\n$P_{start}(t_i) = \\sigma(F_{start}(x_i))$\nwith $W'_{start}, W_{start}$ being the learnable parameters,\nand $\\sigma$ the sigmoid function. For each start of a men-\ntion $t_s$, i.e., those tokens having $P_{start}(t_s) > 0.5$,\nwe then compute the probability of its subsequent\ntokens $t_j$, with $s < j$, to be the end of a mention\nthat starts with $t_s$. We follow the same process\nas that of the mention start classification, but we\ncondition the prediction on the starting token by\nconcatenating the start, $x_s$, and end, $x_j$, hidden\nrepresentations before the linear classifier:\n$F_{end}(x, x') = W'_{end}(GeLU(W_{end}[x, x']))$\n$P_{end}(t_j|t_s) = \\sigma(F_{end}(x_s,x_j))$\nwith $W'_{end}, W_{end}$ being learnable parameters. This\nformulation handles overlapping mentions since,"}, {"title": "Mention Regularization", "content": "To further reduce the\ncomputation demand of this process, in the Mav-\nerick Pipeline we use the end-of-sentence (EOS)\nmention regularization strategy: after extracting\nthe span start, we consider only the tokens up to\nthe nearest EOS as possible mention end candi-\ndates. Since annotated mentions never span across\nsentences, EOS mention regularization prunes the\nnumber of mentions considered without any loss of\ninformation. While this heuristic was initially in-\ntroduced in the implementation of Lee et al. (2018),\nall the recent Coarse-to-Fine have abandoned it in\nfavor of the maximum span-length regularization,\nwhich is a manually-set hyperparameter that reg-\nulates a threshold to filter out spans that exceed a\ncertain length. This implies a large overhead of\nunnecessary computations and introduces a struc-\ntural bias that does not consider long mentions that\nexceed a fixed length. In our work, we not only\nreintroduce the EOS mention regularization, but\nwe also study its contribution in terms of efficiency,\nas reported in Table 1, second row."}, {"title": "Mention Pruning", "content": "After the mention extraction\nstep, as a result of the Maverick Pipeline, we con-\nsider an 18x lower number of candidate mentions\nfor the successive mention clustering phase (Table\n1). This step consists of computing, for each men-\ntion, the probability of all its antecedents being in\nthe same cluster, incurring a quadratic computa-\ntional cost. Within the Coarse-to-Fine formulation,\nthis high computational cost is mitigated by con-\nsidering only the top k mentions according to their\nprobability score, where k is a manually set hyper-"}, {"title": "Mention Clustering", "content": "As a result of the Maverick Pipeline, we obtain a\nset of candidate mentions $M = (m_1, m_2,\u2026\u2026\u2026, m_l)$,\nfor which we propose three different clustering\ntechniques: Mavericks2e and Maverickmes, which\nuse two well-established Coarse-to-Fine mention-\nantecedent techniques, and Maverickincr, which\nadopts a novel incremental technique that lever-\nages a light transformer architecture."}, {"title": "Mention-Antecedent models", "content": "The first proposed\nmodel, Mavericks2e, adopts an equivalent mention\nclustering strategy to Kirstain et al. (2021): given\na mention $m_i = (x_s, x_e)$ and its antecedent $m_j$\n= $(x_{s'}, x_{e'})$, with their start and end token hidden\nstates, we use two fully-connected layers to model\ntheir corresponding representations:\n$F_s(x) = W_s(GeLU(W'_sx))$\n$F_e(x) = W_e(GeLU(W'_ex))$\nwe then calculate their probability to be in the same\ncluster as:\n$P_c(m_i, m_j) = \\sigma(F_s(x_s)\\cdot W_{ss} \\cdot F_s(x_{s'})+\nF_e(x_e)\\cdot W_{ee} \\cdot F_e(x_{e'})+\nF_s(x_s)W_{se}F_e(x_{e'})+\nF_e(x_e)W_{es}F_s(x_{s'}))$"}, {"title": "Incremental model", "content": "Finally, we introduce a novel\nincremental approach to tackle the mention clus-\ntering step, namely Maverickincr, which follows\nthe standard shift-reduce paradigm introduced in\nSection 2.1. Differently from the previous neu-\nral incremental techniques (i.e., ICoref (Xia et al.,\n2020) and longdoc (Toshniwal et al., 2021)) which\nuse a linear classifier to obtain the clustering prob-\nability between each mention and a fixed length\nvector representation of previously built clusters,\nMaverickincr leverages a lightweight transformer\nmodel to attend to previous clusters, for which\nwe retain the mentions' hidden representations.\nSpecifically, we compute the hidden representa-\ntions $(h_1,..., h_l)$ for all the candidate mentions\nin M using a fully-connected layer on top of the\nconcatenation of their start and end token repre-\nsentations. We first assign the first mention $m_1$\nto the first cluster $C_1 = (m_1)$. Then, for each\nmention $m_i \\in M$ at step i we obtain the proba-\nbility of $m_i$ being in a certain cluster $c_j$ by encod-\ning $h_i$ with all the representations of the mentions\ncontained in the cluster $c_j$ using a transformer ar-\nchitecture. We use the first special token ([CLS])\nof a single-layer transformer architecture T to ob-\ntain the score $S(m_i, c_j)$ of $m_i$ being in the cluster\n$C_j = (m_f,..., m_g)$ with $f \\leq g < i$ as:\n$S(m_i, c_j) = W_c\\cdot(ReLU(T_{CLS}(h_i, h_f,...,h_g)))$\nFinally, we compute the probability of $m_i$ belong-\ning to $c_j$ as:\n$P_c(m_i \\in c_j|c_j = (m_f, ..., m_g)) = \\sigma(S(m_i, c_j))$\nWe calculate this probability for each cluster $c_j$ up\nto step i. We assign the mention $m_i$ to the most\nprobable cluster $c_j$ having $p_c(m_i \\in c_j) > 0.5$ if"}, {"title": "Training", "content": "To train a Maverick model, we optimize the sum of\nthree binary cross-entropy losses:\n$L_{coref} = L_{start} + L_{end} + L_{clust}$\nOur loss formulation differs from previous\ntransformer-based Coarse-to-Fine approaches,\nwhich adopt the marginal log-likelihood to opti-\nmize the mention to antecedent score (Lee et al.,\n2018; Kirstain et al., 2021). Since their formulation\n\"makes learning slow and ineffective, especially for\nmention detection\" (Zhang et al., 2018), we directly\noptimize both mention extraction and mention clus-\ntering with a multitask approach. $L_{start}$ and $L_{end}$\nare the start loss and end loss, respectively, of the\nmention extraction step, and are defined as:\n$L_{start} = \\sum_{i=1}^{N} -(y_i log(P_{start}(t_i))+\n(1 - y_i) log(1 - P_{start}(t_i)))$\n$L_{end} = \\sum_{s \\in S} \\sum_{j=1}^{E_s} -(y_i log(p_{end}(t_j|t_s))+\n(1 \u2013 y_i) log(1 \u2013 P_{end}(t_j|t_s)))$\nwhere N is the sequence length, S is the number of\nstarts, Es is the number of possible ends for a start\ns and $p_{start}(t_i)$ and $p_{end}(t_j|t_s)$ are those defined\nin Section 3.1.\nFinally, $L_{clust}$ is the loss for the mention clus-\ntering step. Since we experiment with two dif-\nferent mention clustering formulations, we use a\ndifferent loss for each clustering technique, namely\n$L_{clust}^{anist}$ for the mention-antecedent models, i.e.,\nMavericks2e and Maverickmes, and $L_{clust}^{incr}$ for the\nincremental model, i.e., Maverickincr:\n$L_{clust}^{anist} = \\sum_{i=1}^{|M|} \\sum_{j=1}^{|M|} (y log(p_c(m_i|m_j))+\n(1 - y_i) log(1 - P_c(m_i|m_j)))$\n$L_{clust}^{incr} = \\sum_{i=1}^{M} \\sum_{j=1}^{C_i} -(y_i log(p_c(m_i \\in c_j))+\n(1 \u2013 y_i) log(1 \u2013 P_c(m_i \\in c_j)))$"}, {"title": "Experiments Setup", "content": "We train and evaluate all the comparison systems\non three Coreference Resolution datasets:"}, {"title": "Datasets", "content": "OntoNotes (Pradhan et al., 2012), proposed in\nthe CoNLL-2012 shared task, is the de facto stan-\ndard dataset used to benchmark Coreference Reso-\nlution systems. It consists of documents that span\nseven distinct genres, including full-length docu-\nments (broadcast news, newswire, magazines, we-\nblogs, and Testaments) and multiple speaker tran-\nscripts (broadcast and telephone conversations).\nLitBank (Bamman et al., 2020) contains 100 lit-\nerary documents typically used to evaluate long-\ndocument Coreference Resolution.\nPreCo (Chen et al., 2018) is a large-scale dataset\nthat includes reading comprehension tests for mid-\ndle school and high school students.\nNotably, both LitBank and PreCo have differ-\nent annotation guidelines compared to OntoNotes,\nand provide annotation for singletons (i.e., single-\nmention clusters). Furthermore, we evaluate mod-\nels trained on OntoNotes on three out-of-domain\ndatasets:\n\u2022 GAP (Webster et al., 2018) contains sentences\nin which, given a pronoun, the model has to\nchoose between two candidate mentions.\n\u2022 LitBankns and PreCons, the datasets' test-set\nwhere we filter out singleton annotations.\n\u2022 WikiCoref (Ghaddar and Langlais, 2016),\nwhich contains Wikipedia texts, including doc-\numents with up to 9,869 tokens."}, {"title": "Comparison Systems", "content": "Discriminative Among the discriminative sys-\ntems, we consider c2f-coref (Joshi et al., 2020) and\ns2e-coref (Kirstain et al., 2021), which build upon\nthe Coarse-to-Fine formulation and adopt different\ndocument encoders. We also report the results of\nLingMess (Otmazgin et al., 2023), which is the pre-\nvious best encoder-only solution, and f-coref (Ot-\nmazgin et al., 2022), which is a distilled version of\nLingMess. Furthermore, we include CorefQA (Wu\net al., 2020), which casts Coreference as extractive\nQuestion Answering, and wl-coref (Dobrovolskii,\n2021), which first predicts coreference links be-\ntween words, then extracts mentions spans. Finally,\nwe report the results of incremental systems, such\nas ICoref (Xia et al., 2020) and longdoc (Toshniwal\net al., 2021).\nSequence-to-Sequence We compare our models\nwith TANL (Paolini et al., 2021) and ASP (Liu\net al., 2022), which frame Coreference Resolu-\ntion as an autoregressive structured prediction. We\nalso include Link-Append (Bohnet et al., 2023), a\ntransition-based system that builds clusters with a\nmulti-pass Sequence-to-Sequence architecture. Fi-\nnally, we report the results of seq2seq (Zhang et al.,\n2023), a model that learns to generate a sequence\nwith Coreference Resolution labels."}, {"title": "Maverick Setup", "content": "All Maverick models use DeBERTa-v3 (He et al.,\n2023) as the document encoder. We use DeBERTa\nbecause it can model very long input texts effec-\ntively (He et al., 2021). Moreover, compared to the\nLongFormer, which was previously adopted by sev-\neral token-level systems, DeBERTa ensures a larger\ninput max sequence length (e.g., DeBERTalarge\ncan handle sequences up to 24,528 tokens while\nLongFormer only 4096) and has shown better per-\nformances empirically in our experiments on the\nOntoNotes dataset. On the other hand, using"}, {"title": "Results", "content": "We report in Table 3 the average CoNLL-F1 score\nof the comparison systems trained on the English\nOntoNotes, along with their underlying pre-trained\nlanguage models and total parameters. Compared\nto previous discriminative systems, we report gains\nof +2.2 CoNLL-F1 points over LingMess, the best\nencoder-only model. Interestingly, we even out-\nperform CorefQA, which uses additional Question\nAnswering training data.\nConcerning Sequence-to-Sequence approaches,\nwe report extensive improvements over systems\nwith a similar amount of parameters compared to\nour large models (500M): we obtain +3.4 points\ncompared to ASP (770M), and the gap is even\nwider when taking into consideration Link-Append\n(3B) and seq2seq (770M), with +6.4 and +5.6,\nrespectively. Most importantly, Maverick mod-\nels surpass the performance of all Sequence-to-\nSequence transformers even when they have sev-\neral billions of parameters. Among our proposed\nmethods, Maverickmes shows the best performance,\nsetting a new state of the art with a score of 83.6\nCONLL-F1 points on the OntoNotes benchmark.\nMore detailed results, including a table with M\u1ee4C,\nB\u00b3, and CEAF4 scores and a qualitative error anal-\nysis, can be found in Appendix C."}, {"title": "PreCo and LitBank", "content": "We further validate the robustness of the Mav-\nerick framework by training and evaluating sys-\ntems on the PreCo and LitBank datasets. As re-\nported in Table 4, our models show superior per-\nformance when dealing with long documents in a"}, {"title": "Out-of-Domain Evaluation", "content": "In Table 5, we report the performance of Maver-\nick systems along with LingMess, the best encoder-\nonly model, when dealing with out-of-domain texts,\nthat is, when they are trained on OntoNotes and\ntested on other datasets. First of all, we report\nconsiderable improvements on the GAP test set,\nobtaining a +1.2 F1 score compared to the previous\nstate of the art. We also test models on WikiCoref,\nPreCons and LitBankns (Section 4.1). However,\nsince the span annotation guidelines of these cor-\npora differ from the ones used in OntoNotes, in\nTable 5 we also report the performance using gold\nmentions, i.e., skipping the mention extraction step\n(gold column). On the WikiCoref benchmark, we\nachieve a new state-of-the-art score of 67.2 CONLL-\nF1, with an improvement of +4.2 points over the\nprevious best score obtained by LingMess. On\nthe same dataset, when using pre-identified men-\ntions the gap increases to +5.8 CoNLL-F1 points\n(76.6 vs 82.4). In the same setting, our models\nobtain up to +7.3 and +10.1 CONLL-F1 points on\nPrecons and LitBankns, respectively, compared to\nLingMess. These results suggest that the Maver-\nick training strategy makes this model more suit-\nable when dealing with pre-identified mentions\nand out-of-domain texts. This further increases\nthe potential benefits that Maverick systems can\nbring to many downstream applications that ex-\nploit coreference as an intermediate layer, such\nas Entity Linking (Rosales-M\u00e9ndez et al., 2020)\nand Relation Extraction (Xiong et al., 2023; Zeng"}, {"title": "Speed and Memory Usage", "content": "In Table 3, we include details regarding the train-\ning time and the hardware used by each com-\nparison system, along with the measurement of\nthe inference time and peak memory usage on\nOntoNotes the validation set. Compared to Coarse-\nto-Fine models, which require 32GB of VRAM, we\ncan train Maverick systems under 18GB. At infer-\nence time both Maverickmes and Mavericks2e, ex-\nploiting DeBERTalarge, achieve competitive speed\nand memory consumption compared to wl-coref\nand s2e-coref. Furthermore, when adopting\nDeBERTabase, Maverickmes proves to be the most\nefficient approach among those directly trained"}, {"title": "Maverick Ablation", "content": "In Table 6, we compare Mavericks2e and\nMaverickmes models with s2e-coref and LingMess,\nrespectively, using different pre-trained encoders.\nInterestingly, when using DeBERTa, Maverick sys-\ntems not only achieve better speed and memory\nefficiency, but also obtain higher performance\ncompared to the previous systems. When using\nthe LongFormer, instead, their scores are in the\nsame ballpark, showing empirically that the Mav-\nerick training procedure better exploits the ca-"}, {"title": "Conclusion", "content": "In this work, we challenged the recent trends of\nadopting large autoregressive generative models to\nsolve the Coreference Resolution task. To do so,\nwe proposed Maverick, a new framework that en-\nables fast and memory-efficient Coreference Reso-\nlution while obtaining state-of-the-art results. This\ndemonstrates that the large computational overhead\nrequired by Sequence-to-Sequence approaches is\nunnecessary. Indeed, in our experiments Maver-\nick systems demonstrated that they can outperform\nlarge generative models and improve the speed\nand memory usage of previous best-performing\nencoder-only approaches. Furthermore, we intro-\nduced Maverickiner, a robust multi-step incremental\ntechnique that obtains higher performance in the\nout-of-domain and long-document setting. By re-\nleasing our systems, we make state-of-the-art mod-\nels usable by a larger portion of users in different\nscenarios and potentially improve downstream ap-\nplications."}, {"title": "Limitations", "content": "Our experiments were limited by our resource set-\nting i.e., a single RTX 4090. For this reason, we\ncould not run Maverick using larger encoders, and\ncould not properly test Sequence-to-Sequence mod-\nels as we did with encoder-only models. Neverthe-\nless, we believe this limitation is a common sce-\nnario in many real-world applications that would\nbenefit substantially from our system. We also\ndid not test our formulation on multiple languages,\nbut note that both the methodology behind Mav-\nerick and our novel incremental formulation are\nlanguage agnostic, and thus could be applied to any\nlanguage."}, {"title": "Acknowledgements", "content": "We gratefully acknowledge the sup-\nport of the PNRR MUR project\nPE0000013-FAIR.\nRoberto Navigli also gratefully acknowledges the\nsupport of the CREATIVE project (CRoss-modal\nunderstanding and gEnerATIon of Visual and tEx-\ntual content), which is funded by the MUR Progetti\ndi Rilevante Interesse Nazionale programme (PRIN\n2020). This work has been carried out while Giu-\nliano Martinelli was enrolled in the Italian National\nDoctorate on Artificial Intelligence run by Sapienza\nUniversity of Rome."}, {"title": "Multi-Expert Scorers", "content": "In Maverickmes", "namely": "n1. PRON-PRON-C. Compatible pronouns based\non their attributes, such as gender or number\n(e.g. (I, I), (I, my) (she, her)).\n2. PRON-PRON-NC, Incompatible pronouns\n(e.g. (I, he), (She, my), (his, her)).\n3. ENT-PRON. Pronoun and non-pronoun (e.g.\n(George, he), (CNN, it), (Tom Cruise, his)).\n4. MATCH. Non-pronoun spans with the same\ncontent words (e.g. Italy, Italy).\n5. CONTAINS. One contains the other (e.g.\n(Barack Obama, Obama)).\n6. OTHER. The Other pairs.\nTo detect pronouns we use string match with a full\nlist of English pronouns."}]}