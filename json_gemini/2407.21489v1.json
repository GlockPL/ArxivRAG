{"title": "Maverick: Efficient and Accurate Coreference Resolution Defying Recent Trends", "authors": ["Giuliano Martinelli", "Edoardo Barba", "Roberto Navigli"], "abstract": "Large autoregressive generative models have emerged as the cornerstone for achieving the highest performance across several Natural Language Processing tasks. However, the urge to attain superior results has, at times, led to the premature replacement of carefully designed task-specific approaches without exhaustive experimentation. The Coreference Resolution task is no exception; all recent state-of-the-art solutions adopt large generative autoregressive models that outperform encoder-based discriminative systems. In this work, we challenge this recent trend by introducing Maverick, a carefully designed - yet simple - pipeline, which enables running a state-of-the-art Coreference Resolution system within the constraints of an academic budget, outperforming models with up to 13 billion parameters with as few as 500 million parameters. Maverick achieves state-of-the-art performance on the CoNLL-2012 benchmark, training with up to 0.006x the memory resources and obtaining a 170x faster inference compared to previous state-of-the-art systems. We extensively validate the robustness of the Maverick framework with an array of diverse experiments, reporting improvements over prior systems in data-scarce, long-document, and out-of-domain settings. We release our code and models for research purposes at https://github.com/SapienzaNLP/maverick-coref.", "sections": [{"title": "1 Introduction", "content": "As one of the core tasks in Natural Language Processing, Coreference Resolution aims to identify and group expressions (called mentions) that refer to the same entity (Karttunen, 1969). Given its crucial role in various downstream tasks, such as Knowledge Graph Construction (Li et al., 2020), Entity Linking (Kundu et al., 2018; Agarwal et al., 2022), Question Answering (Dhingra et al., 2018; Dasigi et al., 2019; Bhattacharjee et al., 2020; Chen and Durrett, 2021), Machine Translation (Stojanovski and Fraser, 2018; Voita et al., 2018; Ohtani et al., 2019; Yehudai et al., 2023) and Text Summarization (Falke et al., 2017; Pasunuru et al., 2021; Liu et al., 2021), inter alia, there is a pressing need for both high performance and efficiency. However, recent works in Coreference Resolution either explore methods to obtain reasonable performance optimizing time and memory efficiency (Kirstain et al., 2021; Dobrovolskii, 2021; Otmazgin et al., 2022), or strive to improve benchmark scores regardless of the increased computational demand (Bohnet et al., 2023; Zhang et al., 2023).\nEfficient solutions usually rely on discriminative formulations, frequently employing the mention-antecedent classification method proposed by Lee et al. (2017). These approaches leverage relatively small encoder-only transformer architectures (Joshi et al., 2020; Beltagy et al., 2020) to encode documents and build on top of them task-specific networks that ensure high speed and efficiency. On the other hand, performance-centered solutions are nowadays dominated by general-purpose large Sequence-to-Sequence models (Liu et al., 2022; Zhang et al., 2023). A notable example of this formulation, and currently the state of the art in Coreference Resolution, is Bohnet et al. (2023), which proposes a transition-based system that incrementally builds clusters of mentions by generating coreference links sentence by sentence in an autoregressive fashion. Although Sequence-to-Sequence solutions achieve remarkable performance, their autoregressive nature and the size of the underlying language models (up to 13B parameters) make them dramatically slower and memory-demanding compared to traditional encoder-only approaches. This not only makes their usage for downstream applications impractical, but also poses a significant barrier to their accessibility for a large number of users operating within an academic budget.\nIn this work we argue that discriminative encoder-only approaches for Coreference Resolution have not yet expressed their full potential and have been discarded too early in the urge to achieve state-of-the-art performance. In proposing Maverick, we strike an optimal balance between high performance and efficiency, a combination that was missing in previous systems. Our framework enables an encoder-only model to achieve top-tier performance while keeping the overall model size less than one-twentieth of the current state-of-the-art system, and training it with academic resources. Moreover, when further reducing the size of the underlying transformer encoder, Maverick performs in the same ballpark as encoder-only efficiency-driven solutions while improving speed and memory consumption. Finally, we propose a novel incremental Coreference Resolution method that, integrated into the Maverick framework, results in a robust architecture for out-of-domain, data-scarce, and long-document settings."}, {"title": "2 Related Work", "content": "We now introduce well-established approaches to neural Coreference Resolution. Specifically, we first delve into the details of traditional discriminative solutions, including their incremental variations, and then present the recent paradigm shift for approaches based on large generative architectures."}, {"title": "2.1 Discriminative models", "content": "Discriminative approaches tackle the Coreference Resolution task as a classification problem, usually employing encoder-only architectures. The pioneering works of Lee et al. (2017, 2018) introduced the first end-to-end discriminative system for Coreference Resolution, the Coarse-to-Fine model. First, it involves a mention extraction step, in which the spans most likely to be coreference mentions are identified. This is followed by a mention-antecedent classification step where, for each extracted mention, the model searches for its most probable antecedent (i.e., the extracted span that appears before in the text). This pipeline, composed of mention extraction and mention-antecedent classification steps, has been adopted with minor modifications in many subsequent works, that we refer to as Coarse-to-Fine models."}, {"title": "Coarse-to-Fine Models", "content": "Among the works that build upon the Coarse-to-Fine formulation, Lee et al. (2018), Joshi et al. (2019) and Joshi et al. (2020) experimented with changing the underlying document encoder, utilizing ELMO (Peters et al., 2018), BERT (Devlin et al., 2019) and SpanBERT (Joshi et al., 2020), respectively, achieving remarkable score improvements on the English OntoNotes (Pradhan et al., 2012). Similarly, Kirstain et al. (2021) introduced s2e-coref that reduces the high memory footprint of SpanBERT by leveraging the LongFormer (Beltagy et al., 2020) sparse-attention mechanism. Based on the same architecture, Otmazgin et al. (2023) analyzed the impact of having multiple experts score different linguistically motivated categories (e.g., pronouns-nouns, nouns-nouns, etc.). While the foregoing works have been able to modernize the original Coarse-to-Fine formulation, training their architectures on the OntoNotes dataset still requires a considerable amount of memory.\u00b9 This occurs because they rely on the traditional Coarse-to-Fine pipeline that, as we cover in Section 3.1, has a large memory overhead and is based on manually-set thresholds to regulate memory usage."}, {"title": "Incremental Models", "content": "Discriminative systems also include incremental techniques. Incremental Coreference Resolution has a strong cognitive grounding: research on the \"garden-path\" effect shows that humans resolve referring expressions incrementally (Altmann and Steedman, 1988).\nA seminal work that proposed an automatic incremental system was that of Webster and Curran (2014), which introduced a clustering approach based on the shift-reduce paradigm. In this formulation, for each mention, a classifier decides whether to SHIFT it into a singleton (i.e., single mention cluster) or to REDUCE it within an existing cluster. The same approach has recently been reintroduced in ICoref (Xia et al., 2020) and longdoc (Toshniwal et al., 2021), which adopted SpanBERT and LongFormer, respectively. In these works the mention extraction step is identical to that of Coarse-to-Fine models. On the other hand, the mention clustering step is performed by using a linear classifier that scores each mention against a vector representation of previously built clusters, in an incremental fashion. This method ensures constant memory usage since cluster representations are updated with a learnable function. In Section 3.2 we present a novel performance-driven incremental method that obtains superior performance and generalization capabilities, in which we adopt a lightweight transformer architecture that retains the mention representations."}, {"title": "2.2 Sequence-to-Sequence models", "content": "Recent state-of-the-art Coreference Resolution systems all employ autoregressive generative approaches. However, an early example of Sequence-to-Sequence model, TANL (Paolini et al., 2021), failed to achieve competitive performance on OntoNotes. The first system to show that the autoregressive formulation was competitive was ASP (Liu et al., 2022), which outperformed encoder-only discriminative approaches. ASP is an autoregressive pointer-based model that generates actions for mention extraction (bracket pairing) and then conditions the next step to generate coreference links. Notably, the breakthrough achieved by ASP is not only due to its formulation but also to its usage of large generative models. Indeed, the success of their approach is strictly correlated with the underlying model size, since, when using models with a comparable number of parameters, the performance is significantly lower than encoder-only approaches. The same occurs in Zhang et al. (2023), a fully-seq2seq approach where a model learns to generate a formatted sequence encoding coreference notation, in which they report a strong positive correlation between performance and model sizes.\nFinally, the current state-of-the-art system on the OntoNotes benchmark is held by Link-Append (Bohnet et al., 2023), a transition-based system that incrementally builds clusters exploiting a multi-pass Sequence-to-Sequence architecture. This approach incrementally maps the mentions in previously coreference-annotated sentences to system actions for the current sentence, using the same shift-reduce incremental paradigm presented in Section 2.1. This method obtains state-of-the-art performance at the cost of using a 13B-parameter model and processing one sentence at a time, drastically increasing the need for computational power. While the foregoing models ensure superior performance compared to previous discriminative approaches, using them for inference is out of reach for many users, not to mention the exorbitant cost of training them from scratch."}, {"title": "3 Methodology", "content": "In this section, we present the Maverick framework: we propose replacing the preprocessing and training strategy of Coarse-to-Fine models with a novel pipeline that improves the training and inference efficiency of Coreference Resolution systems. Furthermore, with the Maverick Pipeline, we eliminate the dependency on long-standing manually-set hyperparameters that regulate memory usage. Finally, building on top of our pipeline, we propose three models that adopt a mention-antecedent classification technique, namely Mavericks2e and Maverickmes, and a system that is based upon a novel incremental formulation, Maverickincr."}, {"title": "3.1 Maverick Pipeline", "content": "The Maverick Pipeline combines i) a novel mention extraction method, ii) an efficient mention regularization technique, and iii) a new mention pruning strategy.\nMention Extraction When it comes to extracting mentions from a document D, there are different strategies to model the probability that a span contains a mention. Several previous works follow the Coarse-to-Fine formulation presented in Section 2.1, which consists of scoring all the possible spans in D. This entails a quadratic computational cost in relation to the input length, which they mitigate by introducing several pruning techniques.\nIn this work, we employ a different strategy. We extract coreference mentions by first identifying all the possible starts of a mention, and then, for each start, extracting its possible end. To extract start indices, we first compute the hidden representation $(x_1,...,x_n)$ of the tokens $(t_1, ..., t_n) \\in D$ using a transformer encoder, and then use a fully-connected layer F to compute the probability for each $t_i$ being the start of a mention as:\n$F_{start}(x) = W'_{start}(GeLU(W_{start}x))$\n$P_{start}(t_i) = \\sigma(F_{start}(x_i))$\nwith $W'_{start}$, $W_{start}$ being the learnable parameters, and $\\sigma$ the sigmoid function. For each start of a mention $t_s$, i.e., those tokens having $P_{start}(t_s) > 0.5$, we then compute the probability of its subsequent tokens $t_j$, with $s < j$, to be the end of a mention that starts with $t_s$. We follow the same process as that of the mention start classification, but we condition the prediction on the starting token by concatenating the start, $x_s$, and end, $x_j$, hidden representations before the linear classifier:\n$F_{end}(x, x') = W'_{end}(GeLU(W_{end}[x, x']))$\n$P_{end}(t_j|t_s) = \\sigma(F_{end}(x_s,x_j))$\nwith $W'_{end}$, $W_{end}$ being learnable parameters. This formulation handles overlapping mentions since,"}, {"title": "3.2 Mention Clustering", "content": "As a result of the Maverick Pipeline, we obtain a set of candidate mentions $M = (m_1, m_2,\u2026\u2026\u2026, m_l)$, for which we propose three different clustering techniques: Mavericks2e and Maverickmes, which use two well-established Coarse-to-Fine mention-antecedent techniques, and Maverickincr, which adopts a novel incremental technique that leverages a light transformer architecture.\nMention-Antecedent models The first proposed model, Mavericks2e, adopts an equivalent mention clustering strategy to Kirstain et al. (2021): given a mention $m_i = (x_s, x_e)$ and its antecedent $m_j = (x_{s'}, x_{e'})$, with their start and end token hidden states, we use two fully-connected layers to model their corresponding representations:\n$F_s(x) = W'_s(GeLU(W_sx))$\n$F_e(x) = W'_e(GeLU(W_ex))$\nwe then calculate their probability to be in the same cluster as:\n$P_c(m_i, m_j) = \\sigma(F_s(x_s)\u00b7W_{ss}\u00b7 F_s(x_{s'})+\nF_e(x_e)\u00b7 W_{ee}\u00b7 F_e(x_{e'})+\nF_s(x_s)\u00b7W_{se}\u00b7 F_e(x_{e'})+\nF_e(x_e)\u00b7 W_{es}\u00b7F_s(x_{s'}))$"}, {"title": "3.3 Training", "content": "To train a Maverick model, we optimize the sum of three binary cross-entropy losses:\n$L_{coref} = L_{start} + L_{end} + L_{clust}$\nOur loss formulation differs from previous transformer-based Coarse-to-Fine approaches, which adopt the marginal log-likelihood to optimize the mention to antecedent score (Lee et al., 2018; Kirstain et al., 2021). Since their formulation \"makes learning slow and ineffective, especially for mention detection\" (Zhang et al., 2018), we directly optimize both mention extraction and mention clustering with a multitask approach. $L_{start}$ and $L_{end}$ are the start loss and end loss, respectively, of the mention extraction step, and are defined as:\n$L_{start} = -\\sum_{i=1}^{N} (y_i log(P_{start}(t_i))+\n(1 - y_i) log(1 \u2013 P_{start}(t_i)))$\n$L_{end} = \\sum_{s=1}^{S}\\sum_{j=1}^{E_s}-(y_i log(p_{end}(t_j|t_s))+\n(1 \u2013 y_i) log(1 \u2013 P_{end}(t_j|t_s)))$\nwhere N is the sequence length, S is the number of starts, Es is the number of possible ends for a start s and $p_{start}(t_i)$ and $p_{end}(t_j|t_s)$ are those defined in Section 3.1.\nFinally, $L_{clust}$ is the loss for the mention clustering step. Since we experiment with two different mention clustering formulations, we use a different loss for each clustering technique, namely $L_{clust}^{anust}$ for the mention-antecedent models, i.e., Mavericks2e and Maverickmes, and $L_{clust}^{incr}$ for the incremental model, i.e., Maverickincr:\n$L_{clust}^{anist} = \\sum_{i=1}^{|M|}\\sum_{j=1}^{|M|}(y log(p_c(m_i|m_j))+\n(1 - y_i) log(1 \u2013 P_c(m_i|m_j)))$\n$L_{clust}^{incr} = \\sum_{i=1}^{M}\\sum_{j=1}^{C_i}-(y_i log(p_c(m_i \\in c_j))+\n(1 \u2013 y_i) log(1 \u2013 P_c(m_i \\in c_j)))$"}, {"title": "4 Experiments Setup", "content": "We train and evaluate all the comparison systems on three Coreference Resolution datasets:"}, {"title": "4.1 Datasets", "content": "OntoNotes (Pradhan et al., 2012), proposed in the CoNLL-2012 shared task, is the de facto standard dataset used to benchmark Coreference Resolution systems. It consists of documents that span seven distinct genres, including full-length documents (broadcast news, newswire, magazines, weblogs, and Testaments) and multiple speaker transcripts (broadcast and telephone conversations).\nLitBank (Bamman et al., 2020) contains 100 literary documents typically used to evaluate long-document Coreference Resolution.\nPreCo (Chen et al., 2018) is a large-scale dataset that includes reading comprehension tests for middle school and high school students.\nNotably, both LitBank and PreCo have different annotation guidelines compared to OntoNotes, and provide annotation for singletons (i.e., single-mention clusters). Furthermore, we evaluate models trained on OntoNotes on three out-of-domain datasets:\n\u2022 GAP (Webster et al., 2018) contains sentences in which, given a pronoun, the model has to choose between two candidate mentions.\n\u2022 LitBankns and PreCons, the datasets' test-set where we filter out singleton annotations.\n\u2022 WikiCoref (Ghaddar and Langlais, 2016), which contains Wikipedia texts, including documents with up to 9,869 tokens."}, {"title": "4.2 Comparison Systems", "content": "Discriminative Among the discriminative systems, we consider c2f-coref (Joshi et al., 2020) and s2e-coref (Kirstain et al., 2021), which build upon the Coarse-to-Fine formulation and adopt different document encoders. We also report the results of LingMess (Otmazgin et al., 2023), which is the previous best encoder-only solution, and f-coref (Otmazgin et al., 2022), which is a distilled version of LingMess. Furthermore, we include CorefQA (Wu et al., 2020), which casts Coreference as extractive Question Answering, and wl-coref (Dobrovolskii, 2021), which first predicts coreference links between words, then extracts mentions spans. Finally, we report the results of incremental systems, such as ICoref (Xia et al., 2020) and longdoc (Toshniwal et al., 2021).\nSequence-to-Sequence We compare our models with TANL (Paolini et al., 2021) and ASP (Liu et al., 2022), which frame Coreference Resolution as an autoregressive structured prediction. We also include Link-Append (Bohnet et al., 2023), a transition-based system that builds clusters with a multi-pass Sequence-to-Sequence architecture. Finally, we report the results of seq2seq (Zhang et al., 2023), a model that learns to generate a sequence with Coreference Resolution labels."}, {"title": "4.3 Maverick Setup", "content": "All Maverick models use DeBERTa-v3 (He et al., 2023) as the document encoder. We use DeBERTa because it can model very long input texts effectively (He et al., 2021). Moreover, compared to the LongFormer, which was previously adopted by several token-level systems, DeBERTa ensures a larger input max sequence length (e.g., DeBERTalarge can handle sequences up to 24,528 tokens while LongFormer only 4096) and has shown better performances empirically in our experiments on the OntoNotes dataset. On the other hand, using"}, {"title": "5 Results", "content": "We report the average CoNLL-F1 score of the comparison systems trained on the English OntoNotes, along with their underlying pre-trained language models and total parameters. Compared to previous discriminative systems, we report gains of +2.2 CoNLL-F1 points over LingMess, the best encoder-only model. Interestingly, we even outperform CorefQA, which uses additional Question Answering training data.\nConcerning Sequence-to-Sequence approaches, we report extensive improvements over systems with a similar amount of parameters compared to our large models (500M): we obtain +3.4 points compared to ASP (770M), and the gap is even wider when taking into consideration Link-Append (3B) and seq2seq (770M), with +6.4 and +5.6, respectively. Most importantly, Maverick models surpass the performance of all Sequence-to-Sequence transformers even when they have several billions of parameters. Among our proposed methods, Maverickmes shows the best performance, setting a new state of the art with a score of 83.6 CONLL-F1 points on the OntoNotes benchmark. More detailed results, including a table with MUC, B\u00b3, and CEAF4 scores and a qualitative error analysis, can be found in Appendix C."}, {"title": "5.2 PreCo and LitBank", "content": "We further validate the robustness of the Maverick framework by training and evaluating systems on the PreCo and LitBank datasets. As reported in Table 4, our models show superior performance when dealing with long documents in a data-scarce setting such as the one LitBank poses. On this dataset, Maverickincr achieves a new state-of-the-art score of 78.3, and gains +1.0 CONLL-F1 points compared with seq2seq. On PreCo, Maverickincr outperforms longdoc, but seq2seq still shows slightly better performance. This is mainly due to the high presence of singletons in PreCo (52% of all the clusters). Our systems, using a mention extraction technique that favors precision rather than recall, are penalized compared to high recall systems such as seq2seq. Among our systems, Maverickincr, leveraging its hybrid architecture, performs better on both PreCo and LitBank."}, {"title": "5.3 Out-of-Domain Evaluation", "content": "In Table 5, we report the performance of Maverick systems along with LingMess, the best encoder-only model, when dealing with out-of-domain texts, that is, when they are trained on OntoNotes and tested on other datasets. First of all, we report considerable improvements on the GAP test set, obtaining a +1.2 F1 score compared to the previous state of the art. We also test models on WikiCoref, PreCons and LitBankns (Section 4.1). However, since the span annotation guidelines of these corpora differ from the ones used in OntoNotes, in Table 5 we also report the performance using gold mentions, i.e., skipping the mention extraction step (gold column). On the WikiCoref benchmark, we achieve a new state-of-the-art score of 67.2 CONLL-F1, with an improvement of +4.2 points over the previous best score obtained by LingMess. On the same dataset, when using pre-identified mentions the gap increases to +5.8 CoNLL-F1 points (76.6 vs 82.4). In the same setting, our models obtain up to +7.3 and +10.1 CONLL-F1 points on Precons and LitBankns, respectively, compared to LingMess. These results suggest that the Maverick training strategy makes this model more suitable when dealing with pre-identified mentions and out-of-domain texts. This further increases the potential benefits that Maverick systems can bring to many downstream applications that exploit coreference as an intermediate layer, such as Entity Linking (Rosales-M\u00e9ndez et al., 2020) and Relation Extraction (Xiong et al., 2023; Zeng"}, {"title": "5.4 Speed and Memory Usage", "content": "In Table 3, we include details regarding the training time and the hardware used by each comparison system, along with the measurement of the inference time and peak memory usage on OntoNotes the validation set. Compared to Coarse-to-Fine models, which require 32GB of VRAM, we can train Maverick systems under 18GB. At inference time both Maverickmes and Mavericks2e, exploiting DeBERTalarge, achieve competitive speed and memory consumption compared to wl-coref and s2e-coref. Furthermore, when adopting DeBERTabase, Maverickmes proves to be the most efficient approach among those directly trained"}, {"title": "5.5 Maverick Ablation", "content": "In Table 6, we compare Mavericks2e and Maverickmes models with s2e-coref and LingMess, respectively, using different pre-trained encoders. Interestingly, when using DeBERTa, Maverick systems not only achieve better speed and memory efficiency, but also obtain higher performance compared to the previous systems. When using the LongFormer, instead, their scores are in the same ballpark, showing empirically that the Maverick training procedure better exploits the capabilities of DeBERTa. To test the benefits of our novel incremental formulation, Maverickincr, we also implement a Maverick model with the previously adopted incremental method used in longdoc and ICoref (Section 2.1), which we call Maverickprev-incr. Compared to the previous formulation we report an increase in score of +3.9 CONLL-F1 points. The improvement demonstrates that exploiting a transformer architecture to attend to all the previously clustered mentions is beneficial, and enables the future usage of hybrid architectures when needed.\nAs a further analysis of whether the efficiency improvements of our systems stem from using DeBERTa or are attributable to the Maverick Pipeline, we compared the speed and memory occupation of a Maverick system using as underlying encoder either DeBERTalarge or LongFormerlarge. Our experiments show that using DeBERTa leads to an increase of +77% of memory space and +23% of time to complete an epoch when training on OntoNotes. An equivalent measurement, attributable to the quadratic memory attention mechanism of DeBERTa, was observed for the inference time and memory occupation on the OntoNotes test set. These results highlight the efficiency contribution of the Maverick Pipeline, which is agnostic to the document encoder and can be applied to future coreference systems to ensure higher efficiency."}, {"title": "6 Conclusion", "content": "In this work, we challenged the recent trends of adopting large autoregressive generative models to solve the Coreference Resolution task. To do so, we proposed Maverick, a new framework that enables fast and memory-efficient Coreference Resolution while obtaining state-of-the-art results. This demonstrates that the large computational overhead required by Sequence-to-Sequence approaches is unnecessary. Indeed, in our experiments Maverick systems demonstrated that they can outperform large generative models and improve the speed and memory usage of previous best-performing encoder-only approaches. Furthermore, we introduced Maverickincr, a robust multi-step incremental technique that obtains higher performance in the out-of-domain and long-document setting. By releasing our systems, we make state-of-the-art models usable by a larger portion of users in different scenarios and potentially improve downstream applications."}, {"title": "7 Limitations", "content": "Our experiments were limited by our resource setting i.e., a single RTX 4090. For this reason, we could not run Maverick using larger encoders, and could not properly test Sequence-to-Sequence models as we did with encoder-only models. Nevertheless, we believe this limitation is a common scenario in many real-world applications that would benefit substantially from our system. We also did not test our formulation on multiple languages, but note that both the methodology behind Maverick and our novel incremental formulation are language agnostic, and thus could be applied to any language."}, {"title": "A Multi-Expert Scorers", "content": "In Maverickmes, the final coreference score between two spans is calculated using 6 linguistically motivated multi-expert scorers. This approach was introduced by Otmazgin et al. (2023), which demonstrated that linguistic knowledge and symbolic computation can still be used to improve results on the OntoNotes benchmark. In Maverickmes we adopt this approach on top of the Maverick Pipeline. We use the same set of categories, namely:\n1. PRON-PRON-C. Compatible pronouns based on their attributes, such as gender or number (e.g. (I, I), (I, my) (she, her)).\n2. PRON-PRON-NC, Incompatible pronouns (e.g. (I, he), (She, my), (his, her)).\n3. ENT-PRON. Pronoun and non-pronoun (e.g. (George, he), (CNN, it), (Tom Cruise, his)).\n4. MATCH. Non-pronoun spans with the same content words (e.g. Italy, Italy).\n5. CONTAINS. One contains the other (e.g. (Barack Obama, Obama)).\n6. OTHER. The Other pairs.\nTo detect pronouns we use string match with a full list of English pronouns.\nTo perform mention clustering, we dedicate a mention-pair scorer for each of those categories.\nSpecifically, for the mention $m_i = (x_s, x_e)$ and its antecedent $m_j = (x_{s'}, x_{e'})$, with their start and end token hidden states, we first detect their category kg using pattern matching on their spans of texts. Then we compute their start and end representations, using the specific fully-connected layers for the category kg:\n$F_{kg}^s(x) = W_{kg,s}'(GeLU(W_{kg,s}x))$\n$F_{kg}^e(x) = W_{kg,e}'(GeLU(W_{kg,e}x))$\nThe probability $p_{kg}$ of $m_i$ and $m_j$ is then calculated as:\n$p_{kg}(m_i, m_j) = \\sigma(F_{kg}^s(x_s). W_{ss}. F_{kg}^s(x_{s'})+\nF_{kg}^e(x_e). W_{ee}. F_{kg}^e(x_{e'})+\nF_{kg}^s(x_s). W_{se}. F_{kg}^e(x_{e'})+\nF_{kg}^e(x_e). W_{es}.F_{kg}^s(x_{s'}))$\nWith $W_{ss}$, $W_{ee}$, $W_{se}$, $W_{es}$ being four learnable matrices and $W_{kg,e}'$, $W_{kg,s}'$, $W_{kg,e}$, $W_{kg,s}$ the learnable parameters of the two fully-connected layers. In this way, each mention-pair scorer learns to model the probability for its specific linguistic categories."}, {"title": "B.1 Datasets", "content": "We report technical details of the adopted datasets.\n\u2022 OntoNotes contains several items of metadata information for each document, such as genre, speakers, and constituent graphs. Following previous works, we incorporate the speaker's name into the text whenever there is a change in speakers for datasets that include this metadata.\n\u2022 LitBank contains 100 literary documents and is available in 10 different cross-validation folds. Our train, dev, and test splits refer to the first cross-validation fold, LB0. We report comparison systems results on the same splits. Since training DeBERTalarge is particularly computationally expensive, as introduced in Section 4.3, we train on LitBank by splitting in half each LitBank training document.\n\u2022 The authors of PreCo have not released their official test set. To evaluate our models consistently with previous approaches, we use the official 'dev' split as our test set and retain the last 500 training examples for model validation."}, {"title": "C.1 Error Analysis", "content": "To better understand the quality of Maverick predictions, we conduct an error analysis on our best system trained on OntoNotes, Maverickmes. In Table 8, we report the score of performing only mention extraction (F1) or mention clustering with gold mention (CONLL-F1) with our systems. Our results highlight that our models have strong capabilities of clustering pre-identified mentions, but limited performance in the identification of correct spans. We investigated this phenomenon by conducting a qualitative evaluation of the outputs of our best system, Maverickmes, and found out that OntoNotes contains several annotation errors. We report examples of errors in Table 9. The main inconsistency we found in the gold test set is that many documents have incomplete annotations, which directly correlates with the mention extraction error."}]}