{"title": "Where AI Assurance Might Go Wrong\nInitial lessons from engineering of critical systems", "authors": ["Robin E. Bloomfield", "John Rushby"], "abstract": "We draw on our experience working on system and software assurance and\nevaluation for systems important to society to summarise how safety engineer-\ning is performed in traditional critical systems, such as aircraft flight control.\nWe analyse how this critical systems perspective might support the develop-\nment and implementation of AI Safety Frameworks. We present the analysis\nin terms of: system engineering, safety and risk analysis, and decision analysis\nand support.\nWe consider four key questions: What is the system? How good does it\nhave to be? What is the impact of criticality on system development? and\nHow much should we trust it? We identify topics worthy of further discussion.\nIn particular, we are concerned that system boundaries are not broad enough,\nthat the tolerability and nature of the risks are not sufficiently elaborated, and\nthat the assurance methods lack theories that would allow behaviours to be\nadequately assured.\nWe advocate the use of assurance cases based on Assurance 2.0 to support\ndecision making in which the criticality of the decision as well as the criticality\nof the system are evaluated. We point out the orders of magnitude difference in\nconfidence needed in critical rather than everyday systems and how everyday\ntechniques do not scale in rigour.\nFinally we map our findings in detail to two of the questions posed by\nthe FAISC organisers and we note that the engineering of critical systems has\nevolved through open and diverse discussion. We hope that topics identified\nhere will support the post-FAISC dialogues.", "sections": [{"title": "Introduction", "content": "There are three main ways in which AI Assurance might go wrong: one is that it\naddresses the wrong risks, second is that its techniques are inadequate for the risks\nit does address, and finally it may fail to communicate its claims effectively to the\npublic and other stakeholders.\nWe are neither AI researchers nor developers, although we are users; we are\noutsiders who work on general system and software assurance (and evaluation),\nparticularly for systems important to society. We were among the first to propose\nstructured safety cases [6] (see [38, Section 2] for some history) and experience has\nled us to develop a more rigorous and critical interpretation that we call Assurance\n2.0 [8,10,43] (see the Appendix for an overview and the collection of papers at [12]\nfor more details). However, we have long been interested in the topic of AI assurance\nand wrote on the topic more than 35 years ago [5,35], and also recently [9]. This\npaper draws on our experience with critical systems and also with systems using AI.\nIn Sections 2 and 3 we present a description and analysis of these topics from\nexperience of engineering critical systems: learning from success and failure, and\nwhat are still hard problems. In Section 4 we then relate them to two specific\nquestions posed in the conference Call for Submissions.\n1. Improving existing safety frameworks: How can existing safety frameworks be\nstrengthened? How can we adapt best practices from other industries?\n2. Building on safety frameworks (Seoul Commitment V): How will safety frame-\nworks need to change over time as AI systems' capabilities improve? How do\nthey need to change when AI systems become capable of posing intolerable\nlevels of risk?\nThe engineering of critical systems is of course a rich and varied topic and we have\nbeen selective in this analysis, making a judgment of what we think might be fruitful\nat this stage in the development and implementation of AI safety frameworks.\nAt this point we should digress a little into terminology: in particular, the AI\ncommunity uses \"safety\" as an umbrella term to address a wide range of harms. In\nthis paper we use the terms safety and security in the classical sense, as used in\ncritical systems: safety can be seen as the possibly harmful impact of the system on\nits environment and security as harmful impact of the environment on the system.\nWe note that general caution is needed in terminology: in addition to safety and\nsecurity, hazard, confidence, risk, model and several other terms all have different\nand specific meanings in different communities."}, {"title": "Engineering Critical Systems", "content": "To set the context, we summarise how safety engineering is performed in traditional\nsystems, such as aircraft flight control. This is the dependability perspective [24,27]\nand we discuss its application to AI in a recent report [9]. Below, we enumerate\n(simplified) steps of the traditional process.\n1. There is a system context, or world model, generally called the Environment.\nFor a flight system such as an autopilot, this might refer to or describe the\noverall airplane, aerodynamics, aircraft behavior, structures, weather, sensors\nwith their failure modes and frequencies, and so on, including other systems\nand people that the system interacts with. Note that many critical systems\nare operated by trained pilots/operators and are not engineered to be used by\nthe untrained public.\n2. There are System Requirements that describe what the system (e.g., an\nautopilot) is to do, principally in terms of the effects it is to have on the\nenvironment (e.g., in ALTHLD mode it maintains airplane altitude within some\nrange). Note, requirements describe what is to be done, not how to do it.\n3. There is the process of Hazard Analysis that identifies circumstances in\nthe conjunction of the environment and the system requirements that have\nthe potential to lead to harm (e.q., uncommanded pitch down). In the nuclear\nindustry a \"design basis\u201d is defined as the worst-case environmental challenges\nto be addressed by the system [18] and the financial sector also uses the concept\nof \"design basis threats.\"\n4. There are safety specifications (generally called Safety Requirements) that\ndescribe constraints on the system requirements and on acceptable failure rates\n(e.g., \"not anticipated to occur in the entire lifetime of all aircraft of one type\")\nand that also identify requirements for defence in depth (for example, no single\nfault may precipitate a \u201ccatastrophic failure condition\u201d in a civil aircraft).\n5. There is a process of Requirements Validation that seeks to establish that\nthe safety requirements eliminate or mitigate all the hazards. As the system\nspecification develops, new hazards may be introduced (e.g., to mitigate a fire\nin the electronics bay, we add a fire suppression system and must then consider\nnew hazards associated with failure of that system). The whole process iterates\nuntil it stabilizes. The iteration will assess the extent of defence in depth\nneeded and the role of other subsystems and people.\n6. There is a System Specification that describes how the system is to work.\nWhile developing the system specification (or, later, its implementation) the"}, {"title": "Preliminary Analysis", "content": "We provide an analysis of whether there are concepts and techniques from the criti-\ncal systems perspective that might support the development and implementation of\nSafety Frameworks. We will present the analysis under subsections with the broad\nheadings: system engineering, safety and risk analysis, implementation, and deci-\nsion analysis and support. First, however, we consider aspects of the report and\ncommitments of the Seoul Conference from the systems engineering viewpoint.\nThe interim report from the Seoul conference [2] is comprehensive and identifies\na broad range of risks. It also, correctly, points out that safety is a system property:\nthe safety of a mechanism such as AI must be considered in the context of the envi-\nronment in which it is to be deployed, as its risks (or, more accurately, its hazards)\nare located in the environment. Unfortunately, the report explicitly chooses not to\naddress \"Narrow AI\" that is \"used in a vast range of products and services. . . and\ncan pose significant risks in many of them\" and focuses on Frontier (i.e., advanced\nand \"wide\") AI. It states that this is due to \"the limited timeframe for writing\nthis interim report.\u201d However, we suggest that the capabilities of Frontier AI have\nnow reached a stage where they (or systems based on similar technology) may be\ndeployed in \"narrow\" applications in preference to custom solutions, and hence the\nsafety risks in these applications should be considered part of Frontier AI safety.\nThis has two implications: one is that the term \"safety\" should be interpreted in\nits traditional sense as referring to any unintended consequence that harms the sys-\ntem's environment; the other is that we must consider consequences initiated by\nfaults within the system (e.g., \u201challucinations\"), as well as those initiated in the\nenvironment (e.g., by rogue users). Thus, we find that the Seoul Report considers\nan insufficiently broad range of systems and associated hazards.\""}, {"title": "Boundaries, socio-technical issues, and overall service perspective", "content": "In defining the safety properties and associated hazards we need to explore system\nboundaries. A cause of failure in complex systems is to draw the system boundary\ntoo narrowly and consider just \"equipment\" or in AI the \"algorithm\" rather than\nconsidering the overall socio-technical system that is important for delivering the\nservice [1].\nPeople are part of the wider system and may be a threat due to malicious be-\nhaviour or have unintentional impacts through human behaviour and error. The\nrole of the individual in achieving safety is addressed by human factors in a variety\nof industries [21] but this guidance typically focuses on the trained operator\u2014where\nwe also need to consider use by the general public and disparate sets of users. Broad-\nening out from human factors we need to consider how people adapt to the system,\nand use it in off-label or imaginative ways. In terms of people and organisations\nthere are two complementary views: Normal Accidents [32] and High Reliability\nOrganisations [34] that have both been explored in the context of nuclear weapons\nsafety [40].\nIn terms of AI as a service, we must also consider the delivery of this service\nand the possible harms and failures from running a globalised infrastructure (e.g.,\nhow are updates distributed\u2014recall the CrowdStrike crash). Evaluation of these\nmight seem more mundane than new AI capabilities but they must be assessed as a\nsource of risks and will also consume a risk budget, thereby increasing the criticality\nassigned to other (AI) aspects."}, {"title": "Architecture: use of models, guards, and defence in depth", "content": "To design and assure systems we need validated models (in the sense of abstracted\ndescriptions of how things work). The models need to be valid for the task they are\nbeing used for. At the very least, we require adequate models for safety analysis,\nand these can be implicit or explicit or both (e.g. an explicit system architecture\ndiagram with implicit behaviours that are assessed by experts in a hazard analysis\nactivity). To simplify the modelling task and make it feasible, we may constrain\nthe world to reflect our models (e.g., make it synchronous or time triggered [25]), or\ndesign systems so they can be modelled. The role of models in engineering is neatly\nsummed up by paraphrasing [29]: scientists use models to understand the world,\nengineers to change it.\nTo simplify modeling we can define viability domains: regions of operation for\nthe system where safety is maintained. These allow us to model safety of the system\nwith limited understanding of its components and also reduce the sensing challenges"}, {"title": "Safety and Risk Analysis: How Good Does it Have to Be?", "content": "We acknowledge that for generic AI components, and foundation models in partic-\nular, the eventual applications and their environments may be unknown and so it\nis hard to perform Steps 1 to 5 of the engineering and assurance outline presented\nearlier. A plausible approach is to consider worst-case possibilities, and the Seoul\nCommitments appear to do this: namely, hostile environments, highly hazardous\napplications, and catastrophic consequences. However, it is not articulated how\nthese specific applications, hazards, and environments were selected. Other well-\ncited studies consider different applications such as healthcare, law, and education,\nand identify very different hazards such as threats to fairness, environment, and\neconomics [14].\nAI Assurance could fail in some of its purposes if the risks addressed do not\ncoincide with societal concerns. Hence, we suggest a systematic and open assessment\nshould be undertaken of hazards and associated risks in a wide range of potential\napplications, and the judgments \u201ccatastrophic\" and \"existential\" should be carefully\ndelineated. And we suggest that corporate and national commitments should focus\non representative risks and not only those considered worst-case. In particular,\nthe impact of everyday AI as a force-multiplier should be kept in mind: minor\nrisks may become intolerable when replicated on a vast scale. Furthermore, those\nrepresentative cases should be examined in the framework of the traditional 8-step\nsafety engineering process, particularly the performance of hazard analysis (Step 3)\nand the construction (Step 4) and validation (Step 5) of safety requirements. It is\nonly by considering representative cases in some detail that we can identify whether\nproposed assurance techniques are likely to be adequate or beneficial.\nObserve that concerns in this section stem from the genericity of foundation\nmodels and their lack of a defined context, while concerns in Section 3.3 on imple-\nmentation stem from its basis in machine learning."}, {"title": "Design basis events and threats", "content": "In defining safety requirements and associated hazards we need to address a multi-\nplicity of different environments and events. One approach to exploring this issue\nis to consider a range of scenarios, as has been done for the undermining of democ-\nracy and for Chemical, Biological, Radiological, and Nuclear (CBRN) applications.\nBut if we are to move from illustrative scenarios to a necessary and sufficient a set\nof events to be addressed, we need to model this infinite set of possibilities. One\napproach, common to evaluations in engineering complex systems, is to define justi-\nfied worst-case events that bound the space (e.g., the biggest projectile crash on the\nreactor building, or the largest credible tsunami) or to have distributions of these\nevents. These events are known as the Design Basis Events [18]. Of course, there are\nmany epistemic issues here in judging what are credible events, and characterizing\nour uncertainty in the world model they reflect, but they do provide a systematic\nway of addressing the multiplicity of environments and events. Events can occur\nalong many dimensions (e.g., projectile impact, earthquake, flooding) and for AI\napplications it may be difficult to enumerate a set with adequate coverage."}, {"title": "Integrated safety and security", "content": "Similarly, when we talk about security, we need an environment context that de-\nscribes the threat actors and their interaction and potential harm to the system. De-\nsign Basis Events can be generalised to or for security as Design Basis Threats [18].\nThere is an interesting symmetry where safety can be seen as the possible impact\nof the system on its environment and security as the impact of the environment on\nthe system. Security attacks can also lead to harm to the environment (whether in\nterms of classical safety, or release or compromise of information). There is work on\nthe integration of functional safety and security with guidance published by the UK\nNational Protective Security Authority (NPSA)\u00b9 with the slogan \u201cif it's not secure\nit's not safe.\""}, {"title": "Identification and shaping of risk and tolerability", "content": "Safety and risk analysis derive the safety properties and functions required of the\nsystem but we also need to consider how good does the system have to be, and how\nconfident do we need to be about this. The first of these is often captured in terms\nof criticality levels and the probability of something bad happening: for a plane\ncrash the \"bad\" can be obvious, a major loss of life, but its tolerable likelihood is\na more complex socio-technical question. This can be expressed both qualitatively"}, {"title": "Recovery, resilience and adaptation", "content": "In national critical infrastructure risk assessment, it has been recommended [33]\nthat we recognise both chronic and acute risks and their interplay. Chronic risks,\nsuch as a lack of social cohesion, undermining trust in institutions, and long term\ncognitive changes, are all significant factors in societal risk and social acceptability\nof AI. Another lesson from national critical infrastructure assessment is the need\nto consider resilience and recovery: by explicitly considering resilience, some harms\nmay be recoverable with tolerable losses while others may lead to long term toxicity,\nor once occurred cannot be recovered (e.g., loss of secret information).\nResilience is most broadly defined as the capacity of a system to return to its\noriginal state after shocks. It can be useful to distinguish two subtypes within\nthis [7].\nType 1: resilience to design basis threats and events. This could be expressed in\nthe usual terms of fault-tolerance, availability, robustness, etc.\nType 2: resilience beyond design basis threats, events and use. This might be split\ninto known threats that are considered incredible or ignored for some reason,\nand other \"black swan\" threats that are true unknowns.\nOften we are able to engineer systems successfully to cope with Type 1 re-\nsilience using methods of redundancy and fault tolerance. Type 2 resilience is a\nmore formidable challenge. We may choose to make systems more heterogeneous\nand interconnected and with more resources to support the second type, but doing\nso might make them more expensive and suboptimal in terms of the first type of\nresilience.\nFurthermore, complex systems can be challenged not just by exogenous events\nbut also by those internal to the system: some of these might be the traditional fail-\nure of components (either technical or human). However, increasingly, it is claimed,\nsignificant failures are due to an accumulation of normal variability that in some\ninstances become correlated and, because of the very non-linear response of the\nsystem, leads to unexpected and/or unwanted behaviours: so-called emergent mis-\nbehaviours [31]. In this last scenario, reductionism is a much less successful strategy,\nand a more holistic approach is necessary or even essential.\nThe \"normal accidents\" school identifies tight coupling and interactive complex-\nity as key organizational factors in accidents that runaway into catastrophes [32] and"}, {"title": "Summarising and communicating the dependability strategy", "content": "In engineering critical systems, we often use a 4-state model (see Figure 2) to describe\nthe different approaches to achieving dependability. To achieve dependable systems,\nwe can minimise the transition from the OK to error state and we can have fault\ntolerance and management that return the system to an OK state without loss of\nservice. If the error state escalates, we can design the system so that it fails to a\nsafe or minimum loss state and then recovers. If it does fail and leads to loss, then\nwe can plan and perform incident recovery. The balance between these transitions\nvaries from system to system. This model can be applied recursively to components\nwithin the system design.\nFor critical systems, AI needs to focus not just on keeping operation within the\nOK state but should consider the other transitions in this dependability model.\nDesign that takes recovery into account is key to achieving resilience. We suggest\nthis may be best performed in an architecture with guards and defence in depth."}, {"title": "System Implementation: Impact of Criticality on Development", "content": "Having considered what exactly is the system and how much we should trust it, we\nnow consider its implementation.\nFor a system implementation to be considered safe, we need to perform system\nsafety engineering so that it is safe for the identified hazards, and we need to provide"}, {"title": "Decision Analysis and Support: How Much Should we Trust?", "content": "Assurance serves (at least) three purposes: it helps the developers ensure that their\nsystem is safe, it provides a basis whereby external evaluators can assess and approve\ndeployment of the system, and for those who do deploy the system it communicates\nreasons for trust, the assumptions and limitations of that trust and, hence, condi-\ntions on the context of deployment. Ideally, these activities proceed in parallel as\nparts of the co-design of systems and their evaluation.\nGiven how broad the application of AI can be, there is a need to define carefully\nwhat decision an assurance case is supporting and to understand the needs of the\nstakeholders in communicating the story that the case is telling. Cases can support\nboth risk communication, and the building of trust and elicitation of values. As we\nhave discussed above, there can be a wide range of views (see debate on existential\nrisks) and so counter-cases (those that argue a contrary or negative claim) can have\na role in understanding and explaining the different perspectives. There is potential\nbenefit in a case (or set of linked cases) that addresses the socio-technical aspects,\nintegrates impact of security on safety, and addresses resilience and adaptation.\nTaken together these factors can calibrate the rigour needed in the case."}, {"title": "Assurance cases for reasoning and communication", "content": "As already mentioned, assurance cases (generalizing the earlier notion of a safety\ncase) provide a framework for constructing\u2014and a lens for viewing\u2014assurance, and\nare recognized as a potential approach within the AI community. However, we find\ntheir application is often less sceptical than we would prefer. The primary hazards\nin assurance are complacency and confirmation bias. Those who construct, review,\nand use assurance cases would do well to recall Lakatos' dictum: \"The virtue of a\nlogical proof is not that it compels belief but that it suggests doubts\" [26, page 48].\nFrom our experience with the demands for innovation in complex safety-critical\nsystems, we have been developing an approach dubbed \"Assurance 2.0\" that sup-\nports sceptical analysis and is being transitioned in a number of application areas\n(see collection of papers on Assurance 2.0 [12] and [10] in particular for references\nto the technical and scientific terms used in the following paragraphs).\nAssurance 2.0 provides a framework for assurance around claims, (structured)\nargument, and evidence (building on the existing CAE approach). Arguments are\nconstructed from just five building blocks or steps (concretion, substitution, decom-\nposition, calculation, and evidence incorporation), which reduces the \u201cbewilderment\nof choice\" in free-form arguments. Argument steps are generally expected to be de-\nductive: that is to say, the conjunction of child subclaims to each argument step\nmust entail the parent claim\u2014because otherwise there is a \"gap\" in the reasoning.\nSide-claims (logically no different to other subclaims, but conceptually distinct) fac-"}, {"title": "Identify decision criticality not just system criticality", "content": "We need to consider how good the evaluation has to be, and this is of course linked\nto the decision being made. We need to understand that decision and, as with\nsystems, consider the resilience aspects: how much harm might done before we can\ndetect a bad decision and whether we can recover from such a bad decision.\nAs noted above, the tolerable failure rates for safety properties will vary by or-\nders of magnitude between everyday systems and critical systems that might pose\nsignificant harm. Similarly, the confidence required in our evaluation will also in-\ncrease as the criticality of the system increases. A qualitative illustration of this\nis the difference between a cut down CAE approach to confidence building for re-\nsilience of commodity devices (such as home fridges) using Principles Based Assur-\nance (PBA)\u00b2 contrasted with the rigors of an Assurance 2.0 case for a safety critical\nsystem. Some illustrative quantitative modelling of how confidence increases with\ncriticality is provided in [4]."}, {"title": "Explicit approach to confidence in safety claims", "content": "There are a number of technical approaches to evaluating how confident we are in\nsafety claim: one is to use a structured approach to modelling the justification [10]\nin which confidence in parts of the evaluation can be combined in a conservative\nmanner. This could be by approximate worst case propagation of doubts or by the\nuse of more nuanced theories that explicitly deal with confidence. There are also\ntechnical methods like the \"chain of confidence\" that help model the impact of being\nwrong by modelling how wrong we might be, and applying this recursively (see [10]\nand [22])."}, {"title": "Explicit approach to judgment bias", "content": "Assurance 2.0 provides an explicit approach to addressing confirmation bias through\nthe use of defeaters, confirmation theory, and explicit counter cases. This can be\naugmented with surrounding processes that also provide for independence and di-\nversity of opinion."}, {"title": "Distinguish different types of argument and inherent strengths", "content": "A high-level factoring of argument approaches is to use the \"strategy triangle\" that\ndescribes justifications in terms of rule-based, goal-based, and risk-informed ap-\nproaches that focus on compliance, behaviors, and vulnerabilities, respectively [20].\nFor systems that pose very significant potential harm all three aspects will be rele-\nvant with those addressing behaviours more compelling. When dealing with extreme\nbehaviours (e.g. catastrophic failure of a nuclear reactor pressure vessel) arguments\nabout the incredibility of failure may combine deterministic, analytical and prob-\nabilistic reasoning. In general, analytic arguments are stronger than probabilistic\nones. In Assurance 2.0 we have an approach that combines them where the de-\nductive part is supported by inductive argument about the assumptions. A simple\nexample is the unfounded fear that a civil nuclear reactor might cause a nuclear\nexplosion. The argument that by design there is not enough fissile material in\nthe core is much stronger than a probabilistic argument in which the core has sig-\nnificant fissile material, but we rely on probabilistic evaluations to show it is very\nunlikely that a core meltdown will produce a critical configuration. Similar examples\ncome from computer science: a proof of absence of critical defects, with assumption\ndoubts addressed, is stronger in principle than statistical testing because it covers\nall conditions."}, {"title": "Automation and tempo", "content": "The abundance of uncertainties on the evolution of capabilities, knowledge of risks\nand benefits, and attitudes to risk tolerability all emphasise the need to frequently\nupdate individual assurance cases. The need for greater tempo in the use of cases\nhas been apparent for some time with the need for innovation to support the \"com-\npile to combat\" doctrine and DevSecOps. The DARPA ARCOS programme spon-\nsored a number of projects on automation of certification. We were part of the\nClarissa project building on the Adelard ASCE platform. The program adopted a\n\"documents as data\" paradigm where legacy and new analysis methods updated a\nsemantic web triple store. This was then used by assurance case tools to feed evi-\ndence into case construction and analysis tools [43]. Although some of the tooling is\nat low TRL, the lessons learned show that greater automation is feasible, and this\nshould be embraced."}, {"title": "Evolution of AI Safety Frameworks", "content": "It is recognised that the development and implementation of AI Safety Frameworks\nwould benefit from adopting and adapting best practices from other industries. A\nfirst step towards this is to identity where there might be fruitful areas of interest\nthat can be developed further, building on success, failures and open issues within\nthe dependability engineering of critical systems.\nWe have provided an initial analysis driven by following questions\n\u2022 What is the system?\n\u2022 How good does it have to be?\n\u2022 What is the impact of criticality on system development?\n\u2022 How much should we trust it?\nThe topics we have identified can be grouped according to whether they address\nsystem engineering, risk analysis, or decision analysis and support. The topics are\nsummarised in Figure 3 and are as follows.\nSystem engineering\n\u2022 System evaluation and compositional assurance\n\u2022 Boundaries and sociotech, open systems perspective\n\u2022 Overall service perspective\n\u2022 Use of models, guards and defence in depth\n\u2022 Recovery, resilience and adaptation\nRisk analysis\n\u2022 Identification and shaping of risk tolerability\n\u2022 Holistic harms and risk aggregation\n\u2022 Design Basis Events and Threats\n\u2022 Integrated safety and security\n\u2022 Recovery, resilience and adaptation\nDecision analysis and support\nScope and use of assurance cases\n\u2022 Holistic approach to safety, security and resilience\n\u2022 Communication via cases and counter cases\n\u2022 Use of cases to understand disparate views\nUse of Assurance/Safety cases and Assurance 2.0, in particular"}, {"title": "Summary and Conclusions", "content": "There are three main ways in which AI Assurance might go wrong: one is that it\naddresses the wrong risks, second is that its techniques are inadequate for those\nrisks that are addressed, and finally it may fail to communicate its claims effectively\nto the public and other stakeholders.\nWe summarised how safety engineering is performed in traditional critical sys-\ntems, such as aircraft flight control. We provided an analysis of whether there are\nconcepts and techniques from this critical systems perspective that might support\nthe development and implementation of AI Safety Frameworks. We presented the\nanalysis under the broad headings: system engineering, safety and risk analysis,\nimplementation, and decision analysis and support. We have been selective in this\nanalysis, making a judgment of what we think might be fruitful at this stage in the\ndevelopment and implementation of AI safety frameworks.\nOur analysis considered four key questions: What is the system? How good\ndoes it have to be? What is the impact of criticality on system development? How\nmuch should we trust it? We identified a number of topics we think worthy of\nfurther discussion. In particular, we are concerned that in the system boundaries\nare not broad enough, the tolerability and nature of the risks are not sufficiently\nelaborated, and that the assurance methods lack theories that allow behaviours to be\nassured with adequate confidence. We advocate the use of assurance cases based on\nAssurance 2.0 to support decision making in which the criticality of the decision as\nwell as the criticality of the system is evaluated. Finally, we point out the orders of\nmagnitude difference in confidence needed in critical rather than everyday systems,\nand how everyday techniques do not scale in rigour.\nWe mapped our findings in some detail to two of the questions posed by the\nFAISC organisers \"How can we adapt best practices from other industries?", "and": "ow do they (AI safety frameworks) need to change when AI systems become\ncapable of posing intolerable levels of risk?\"\nFor the question \"What are common challenges for companies that are yet to\nproduce a frontier AI system and/or a safety framework?\" we suggest a significant"}, {"title": "Assurance 2.0 in a Nutshell", "content": "Robin Bloomfield (City, Univ. of London) and John Rushby (SRI)\nSRI CSL Technical Note, 14 October 2024\nThis is intended as a memory aid, not a replacement for reading the longer documents that can\nbe found (as can this) at https://www.csl.sri.com/users/rushby/assurance2.0.\nPurpose of Assurance 2.0: it's a rigorous and systematic approach to developing, presenting, and\nexamining assurance cases to support indefeasible confidence in safety or other critical properties\n\u2022 Structure: Claims, Argument, Evidence (CAE), plus Theories and Defeaters\nClaims: precise and meaningful statements about system and environment, presented\nas atomic propositions in natural language. Some may be marked as assumptions\n* Claims may state probabilistic properties and uncertainties (e.g., pfd < 10-4)\nArgument: typically presented as a tree-like structure of nodes; each node has a parent\nclaim, one or more subclaims, and usually a side-claim\n* Just 5 kinds of (building) blocks for argument nodes: concretion, substitution,\ndecomposition, calculation, evidence incorporation. See Figure 1\n* Conjunction of subclaims and side-claim should deductively entail parent claim;\notherwise flag as inductive & apply special care such as confirmation theory (below)\n* Disjunctive decompositions are available (useful in refutational subcases, see over)\n* Side-claim typically factors out deductiveness conditions (e.g., subclaims partition\nparent claim, or parent claim distributes over components enumerated in subclaims)\n* A narrative justification. . . justifies all this; may cite an external theory\n* LLMs can interpret claims as knowledge graphs over standardized ontology, which\ncan then be checked for consistency using answer set programming [1]\nEvidence: a coherent assembly of reviews, analyses, tests etc. that measures some\nproperty of the system. The measurement in turn supports some useful inference. This\nis justified by a narrative description that may cite an external theory\n* Parent claim of an evidence incorporation block is called the measured claim: it\nsays what the evidence is (e.g., testing achieved MC/DC coverage with no faults)\n* Above that is a substitution block that derives a useful claim from the measured\nclaim; it says what the evidence means (e.g., there is no unreachable code)\n* Weight of evidential support for the useful claim is examined using the measures of\nconfirmation theory, e.g., (Keynes): $log \\frac{P(C|E)}{P(C)}$, or (Good): $log \\frac{P(E|C)}{P(E|\\neg C)}$\n\u2022 Theories are self-contained technical descriptions and assurance arguments for specific assur-\nance methods (e.g., static analysis) or (sub)systems (e.g., altitude hold). They include narra-\ntive justifications for their arguments and may serve as templates for assurance (sub)cases\nSubcases can be instantiations of parameterized (and ideally pre-certified) theories\nInstantiations can be expanded in place (like a macro), or referenced (like a subroutine)\nMuch of a case can be synthesized from a library of such parameterized theories\nStandards bodies should deliver theories not guidelines.\nOverall case can be summarized by enumerating its theories"}]}