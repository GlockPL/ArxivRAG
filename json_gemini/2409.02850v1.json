{"title": "OOPS, I SAMPLED IT AGAIN: REINTERPRETING\nCONFIDENCE INTERVALS IN FEW-SHOT LEARNING", "authors": ["Raphael Lafargue", "Luke Smith", "Franck Vermet", "Mathias L\u00f6we", "Ian Reid", "Vincent Gripon", "Jack Valmadre"], "abstract": "The predominant method for computing confidence intervals (CI) in few-shot learning (FSL) is based\non sampling the tasks with replacement, i.e. allowing the same samples to appear in multiple tasks.\nThis makes the CI misleading in that it takes into account the randomness of the sampler but not the\ndata itself. To quantify the extent of this problem, we conduct a comparative analysis between CIs\ncomputed with and without replacement. These reveal a notable underestimation by the predominant\nmethod. This observation calls for a reevaluation of how we interpret confidence intervals and the\nresulting conclusions in FSL comparative studies. Our research demonstrates that the use of paired\ntests can partially address this issue. Additionally, we explore methods to further reduce the (size\nof the) CI by strategically sampling tasks of a specific size. We also introduce a new optimized\nbenchmark, which can be accessed at https://github.com/RafLaf/FSL-benchmark-again.", "sections": [{"title": "1 Introduction", "content": "The recent surge of interest in few-shot learning (FSL), driven by its potential applications in many real-world scenarios,\nhas led to a proliferation of new methods and novel experimental protocols (Sung et al., 2018; Snell et al., 2017; Bendou\net al., 2022; Zhang et al., 2020). If in conventional machine learning it is common to benchmark methods using a\nfixed split into training and validation sets, FSL presents unique challenges due to its reliance on extremely small and,\nconsequently, biased training datasets. In fact, the performance of FSL can dramatically depend on the choice of the\ngiven labeled training samples (Arnold et al., 2021).\nOne question that FSL shares with conventional machine learning is that of the best performing methods. Especially\nrelevant to FSL, the high variance of measured performance based on the choice of labeled data has led practitioners to\nquickly adopt the standard of aggregating statistics over a large number of artificially generated tasks, stemming from\na single (or a few distinct) dataset(s). The predominant approach is to generate artificial few-shot tasks by randomly\nsampling the same dataset with replacement, i.e. permitting the same samples to appear across multiple tasks. The\noutcome of these numerous tasks is the calculation of an average accuracy and its associated confidence interval (CI)\nfor each method, thereby providing researchers with a statistically relevant basis for comparing the efficacy of different\nmethods.\nBy allowing the same samples to appear in multiple tasks, the computed CIs account for the randomness of the sampler\nbut not the data itself. In fact, the computed CIs use the usual Lindeberg-L\u00e9vy Central Limit Theorem (CLT). Hence,\nfor these CIs to be statistically valid, the underlying random variables must be independent and identically distributed\n(IID). This means that the currently reported CIs should be understood as a likely range of outcomes if the experiment\nwere reproduced using exactly the same data, which we will refer to in the remainder of this paper as Closed CIs\n(CCIs). This contrasts with what is often of interest in many areas of machine learning: the range of outcomes if the"}, {"title": "2 Closed CIs vs. Open CIs", "content": "In this section, we are interested in better quantifying the difference between CCIs and OCIs. For this purpose, we\ndefine notations and outline algorithms for task sampling. We present a theoretical analysis of the differences between\nCCIs and OCIs. Finally, we empirically compare their ranges on real datasets."}, {"title": "2.1 A mathematical description of the problem", "content": ""}, {"title": "2.1.1 Standard Evaluation and Notations", "content": "The predominant method of evaluation in the field of few-shot classification is described in Algorithm 1. A few-\nshot classification task T = (K, S, Q) comprises a set of classes K, a support set S = {Sc}c\u2208K and a query set\nQ = {Qc}c\u2208x where Sc, Qe denote the sets of support and query examples for each class c\u2208 K. Let K = |K| denote\nthe number of ways (i.e. classes in a few-shot task), S = |Sc| the number of shots per class, and Q = |Qc| the number\nof queries per class (for simplicity, we assume the classes to be balanced). Few-shot evaluation is typically performed by\nconstructing many tasks from a larger evaluation dataset. An evaluation dataset D = (C, X) comprises a set of classes C\nand examples for all classes X = {Xc}c\u2208c. Let C = |C| > K denote the number of classes, and N = \\X\\ > S + Q\nthe number of examples per class. As highlighted in the introduction, the rationale followed by the CI computation\npredominant method is to consider D to be fixed and non-probabilistic."}, {"title": "2.1.2 Computing Confidence Intervals", "content": "We compute the accuracy At of the method on each task, and then take the mean across tasks \u0100 = \\frac{1}{T}(A_1 + \u2026 + A_T).\nAs such, we obtain the following formula for the variance:\n$Var[A] = \\frac{Var[A_1] + \u2026 + Var [A_T]}{T^2} = \\frac{Var[A]}{T}$ (2)\nAssuming a sufficient sample size and that the mean \u0100 is normally distributed according to the Central Limit Theorem,\nthe 95% confidence interval is obtained as A \u00b1 1.96\u03c3\u03b1 using the formula for standard error (standard deviation of the\nsample mean):\n$CI = 1.96 \\sigma_{\\overline{A}} = 1.96 \\frac{\\sigma_A}{\\sqrt{T}}$   (3)\nNote that in the case of a very small number of tasks, Student's distributions can be used instead. Also, we took the\nexample of 95% CIs, which is arbitrary but very common in the literature. For more generality, we consider a probability\nPlimit in the following for all theoretical considerations, and stick with the 95% value for experiments."}, {"title": "2.2 Are OCIs larger than CCIs? An empirical study", "content": "In contrast to Algorithm 1, the task sampling without replacement is presented in Algorithm 2 (see Appendix). Note\nthat we make an explicit use of a Student's law estimator in this algorithm as it is expected that for some small datasets\nit can only generate but a few independent tasks. In the latter, the total number of tasks T is determined directly by the\nsampling process, thanks to a specific stopping condition, based on the exhaustion of the dataset. This is done in an\neffort to minimize the obtained CIs' ranges. Since samples cannot be sampled twice, we can consider that the classes\nand examples are drawn IID from an underlying data distributions p(C) and p(X | C). This is why OCIs also account\nfor the randomness of the data.\nIn our experiments, we utilize datasets from the Metadataset Benchmark as referenced in Triantafillou et al. (2019). This\nbenchmark comprises 10 datasets, out of which we employ 9, excluding Imagenet, to focus on cross-domain results\nin line with the recent trend in the literature (Zhou et al., 2022b). These include Omniglot (handwritten characters),\nAircraft, CUB (birds), DTD (textures), Fungi, VGG Flowers, Traffic Signs, Quickdraw (crowd-sourced drawings) and\nMSCOCO (common objects) (Lake et al., 2015; Maji et al., 2013; Wah et al., 2011; Cimpoi et al., 2014; Schroeder &\nCui, 2018; Nilsback & Zisserman, 2008; Houben et al., 2013; Jongejan et al., 2016; Lin et al., 2014).\nLuo et al. (2023) details few-shot accuracies for 2000 tasks with 5-shots, 5 ways, and 15 queries in a comprehensive\ntable covering various works on the Metadataset datasets. Our study's only difference lies in the adoption of the\nT = 600 setting, a more prevalent choice in existing literature. If CCIs are found to be narrower than OCIs with this\nsmaller T, it will be even starker with T = 2000 tasks as shown in Equation 3. Our primary reference for methods\nand models is the comprehensive compilation provided by Luo et al. (2023), a foundational starting point for our\nexperiments.\nOur findings are detailed in Table 2, showcasing results across different few-shot methods and datasets. Firstly, there\nis a noticeable homogeneity in CCIs, arising from the fixed number of tasks set at T = 600, which contrasts with\nthe variability observed in OCIs. Interestingly, CCIs are substantialy narrower than OCIs for small datasets such as\nAircraft and DTD. Conversely, in the case of larger datasets like Quickdraw, CCIs become larger than OCIs due to\nT = 600 being insufficient to deplete the dataset. Indeed, Aircraft and DTD's test splits contain 1,500 and 840 samples\nrespectively, whereas the test splits for MSCOCO and Quickdraw have much larger sizes of 152,000 and 7.7 million\nsamples respectively. Across various datasets, models and methods, CCIs are on average 3.8 times larger than OCIs.\nThese results highlight the imperative need for accurate interpretation of Confidence Intervals, given the dramatic\ndifferences between OCIs and CCIs ranges, that undoubtedly lead to disagreeing conclusions if misinterpreted.\nWe also notice that for cases where methods reach accuracies near 100%, like adaptation methods using CLIP (unlike\nthose using DINO) on the CUB dataset, both types of CIs become narrower. This is due to accuracy saturation at 100%,\nwhich reduces the standard deviation of accuracies.\nIn the following, we delve into the conclusiveness of comparative studies using CCIs or OCIs."}, {"title": "2.3 Impact on Conclusiveness", "content": "First let us recall how confidence intervals are used to draw conclusions when comparing methods. Suppose we have\ntwo variables of interest x\u2081 and x\u2082, with their corresponding $P_{limit}$ - confidence intervals (a generalized version of\n95%-CIs) $[x_1 \u2013 \\delta_1, x_1 + \\delta_1]$ and $[x_2 - \\delta_2, x_2 + \\delta_2]$. To draw conclusions about the fact x\u2081 is smaller than x\u2082, we\nproceed as follows: if the two intervals do not intersect, and $x_1 + \\delta_1 < x_2 - \\delta_2$, then:\n$P(x_1 < x_2) > P(x_1 < X_1 + \\delta_1 \\land X_2 > X_2 - \\delta_2) = (1 - \\frac{1 - P_{limit}}{2})^2 > P_{limit}$,  (4)\nwhere the $(1 \u2013 \\frac{1 - P_{limit}}{2})^2$ part comes from the symmetry of the Gaussian distribution."}, {"title": "3 Paired tests", "content": ""}, {"title": "3.1 Definitions", "content": "As an effort to reduce the ranges of Open Confidence\nIntervals (OCIs), we propose to make use of paired tests.\nIndeed, as we pointed out in the introduction, FSL tasks\nhave a vast diversity in difficulty, leading to a high vari-\nance in accuracy across tasks. It is noteworthy that a task\ndeemed hard for method A often aligns in difficulty for"}, {"title": "4 Sizing tasks to narrow OCIs", "content": "If increasing Q reduces the number of tasks, it also changes\nVar(At) which is proportional to the CI. As such, there ex-\nists a trade-off between the number of queries and the feasible\nnumber of tasks that can be generated to minimize OCIs for\nany given dataset. Intuitively, measuring A with a small T (and\nconsequently a high Q) results in extensive CI ranges, a phe\nnomenon depicted in Equation 3. Conversely, measuring with\nQ = 1 may generate many tasks (large T) with an extremely\nhigh variance because the accuracy per class becomes either\n0% or 100%. In the following, we aim to identify the optimal\nnumber of queries, denoted Q*, that effectively minimizes the\nvariance of the average accuracy Var(A) and thus the obtained\nCIs. We first demonstrate mathematically the existence of such\nminimum by deriving Var(A).\nWe show in the Appendix that Var(A) can be written as:\nVar (A) = \\frac{K}{NC} \\frac{\\beta}{(\\alpha Q + \\gamma)},  (7)\nwith \u03b1, \u03b2 and \u03b3 also defined in the Appendix, and \u03b2 > 0.\nThese parameters are difficult to estimate in particular when\ndealing with real datasets and methods. If a \u2264 0, then Var (A)\nis decreasing as a function of Q since Q \u2208 N. In the following,\nwe focus only on cases where a > 0. This choice is supported\nby empirical evidence, which we will present later, indicating\na U-shaped relationship between the variance of A and Q for a"}, {"title": "4.1 Effect of S and N on Q*", "content": "Given the definition of \u03b1 and \u03b2 obtained in Equation 10, we find that Q* is an increasing function of S and a constant\nfunction of N. In this section, we show that these results are confirmed empirically on real datasets.\nWe propose to study the aforementioned variance model of A with respect to Q in a simplistic 1D representation of\nsamples. In our model, two class' distribution are represented as two Gaussians ($N_i = N(\\mu_i, \\sigma_i)$ with i\u2208 {1,2}).\nWe then sample an artificial balanced dataset of fixed size N. Next, we sample tasks within this artificial dataset until\nexhaustion with the procedure described in Algorithm 2 setting K = C = 2. Using the NCC classifier we obtain a\nset of accuracies on which we can compute the average accuracy A. This procedure consisting of instantiating the\nsynthetic dataset from Gaussians and measuring A is iterated. This yields a set of {$A_j$}$_j$ for a given set of parameters\n{S, Q, N, N1, N2} from which we compute an empirical variance Var(A).\nIn Figure 2, we show the measured Var(A) vs Q. We use 1000 samples in the datasets, split between the two classes.\nWe set the number of shots to S = 5. The model in Equation 10 is fitted very precisely. The discretisation effect seen at\nhigh Q is due to the low number of tasks. Next, we study the effect of S and N and compare it to our experiments with\nsynthetic data.\nIncreasing S shifts the curve's minimum from Q* = 1 towards Q* \u2192 +\u221e as depicted in Figure 3. This aligns with\nour model's predictions. At S = 1, opting for Q* = 1 effectively has two effects: (a) the high variance of At due to\nsmall support and query sets increases the variance of A and (b) low Q allows a significantly larger T, thus reducing\nthe overall variance of A as shown in Equation 3. Conversely, for S \u2265 20, the setting boils down to classical transfer\nlearning. Indeed, the narrowest CI is attained with one task with a large support and query set task. Finally, we find a\nthird regime where, for a range of values of S, Q* is nontrivial. It corresponds to what is shown in the S = 5 regime in\nFigure 3.\nWhen increasing N, Q* should not be affected according to Equation 10. However, we observe a hardly perceptible\nshift of Q* in Figure 3 when N is increased. We consider this effect to be sufficiently small and therefore negligible.\nNext, we explore how these results extend to real-world datasets."}, {"title": "4.1.1 Real Dataset Experiments", "content": "We now shift our focus to the findings derived from real image datasets. These datasets are often unbalanced and exhibit\na variety of class numbers. Our objective is to determine whether the earlier conclusions remain valid in this context.\nFirst, we observe that Q* does not depend on the size of datasets. However the size of the dataset scales T which in\nturn scales CI95%. Similar to the results with synthetic data, we observe a discretisation of the confidence interval in\nthe high Q (low T) regime."}, {"title": "5 Benchmark Proposal", "content": "Building on previously obtained results, we propose a simple benchmark where Paired Tests are used and the value of\nQ is chosen as the minimum found in the previous paragraph. Implicitly, we are assuming that the minimum of A's\nOCI is also reached at Q*. More precisely, we are assuming the independence of the covariance with respect to Q.\nThese assumptions are backed by the improved number of conclusive comparisons in Figure 6 when optimizing Q and\nusing paired tests. Indeed, while PT yieled 94 conclusive comparisions, PT with optimized Q yields a little more with\n97 conclusive comparisons.\nWe present our results with the baselines adaptation methods previously studied in Table 6 using DINOv2 as our\nbaseline model. Our experiments consistently show that, for a given model, fine-tuning tends to be less effective than\nboth logistic regression and the nearest class centroid methods. We also observe the choice of model is primordial with\na clear advantage of DINOv2 over CLIP and DINO on most datasets. Our benchmark, including the code, seed values,\ntask descriptions, and accuracy results, is available for use."}, {"title": "6 Related Work", "content": "Few-Shot Learning Since the seminal works of Vinyals et al. (2016) and Snell et al. (2017), the field of few-shot\nlearning has known many contributions. Most solutions rely on the use of a pretrained feature extractor, trained on a\ngeneric abundant dataset (Wang et al., 2020; Bendou et al.; Antoniou et al., 2018). The feature extractor can then be\nused as is on the target problem (Wang et al., 2019), or adapted before classifying (Zhang et al., 2021). Most proposed\nmethods differ in the way they combine the pretrained feature extractor and their adaptation to the target problem Luo\net al. (2023). Over the years, multiple benchmarks have been proposed, including MiniImageNet (Vinyals et al., 2016),\nOmniglot (Lake et al., 2015) and TieredImegenet (Ren et al., 2018). If initially, these benchmarks focused on the\nin-domain case, where the feature extractor is trained on disjoint classes from the target problem, but all drawn from the\nsame initial dataset, the trend evolved with the introduction of Meta-dataset (Triantafillou et al., 2019) (MD) and later\nCOOP (Zhou et al., 2022a), where the feature extractor is trained on a large generic dataset and applied to various other\ndomains, including fine-grain problems, embodying a cross-domain evaluation.\nA few papers focus on the sampling of tasks of targeted difficulty for few-shot learning (Arnold et al., 2021; Zhou et al.,\n2020). In Zhou et al. (2020), the authors claim that model performance can be improved by sampling meta-learning\ntasks of increasing difficulty. In other works, failed meta-training tasks (deemed hard) are sampled again (Sun et al.,\n2020) or previously misclassified samples/classes are more likely to be sampled in following tasks Liu et al. (2020) to\nfocus the model on difficult tasks. Estimating task difficulty can itself be a difficult task, and several solutions have\nbeen proposed Arnold et al. (2021). The idea of difficulty-based sampling proposed in Arnold et al. (2021) is relevant\nto this paper since it enables the sampling of groups of tasks with homogeneous difficulty, effectively reducing the\nconfidence interval ranges. In contrast, our research adopts paired tests, a method that obviates the need for such\ndependencies and provides a more universally applicable approach. Paired tests is not a novel contribution of our work.\nThey were introduced over a century ago to study the evolution of small populations with time (Student, 1908; Hedberg\n& Ayers, 2015). In these seminal studies, the authors shown that individual differences provide more statistical power\nand insights than changes in averages over the whole population.\nConfidence Intervals Confidence intervals were established by Polish mathematician Jerzy Neyman (Neyman, 1937)\nin the early 1930's coinciding with Fisher's ideas although supported by a different framework. They were more\ngenerally used and then required in medical research around the 1980's. They require assuming a specific model for the\ndistribution of the considered data. In contrast, the bootstrap method, introduced in DiCiccio & Efron (1996), offers a\ndistribution-agnostic approach for estimating ranges. We focused on traditional confidence intervals as they are better\nunderstood and more often implemented in the literature of few-shot learning, but similar conclusions could be drawn\nusing Boostrap instead.\nChallenges in Statistical Interpretation and Methodological Biases This issue of misleading CIs is not isolated\nto our domain. Belia et al. (2005) have similarly criticized the common misinterpretations surrounding confidence"}, {"title": "7 Limitations", "content": "A first limitation of our study we would like to point out is that for large datasets such as MSCOCO or QuickDraw, the\npredominant method of CI calculation leads to intervals that are actually larger than our proposed OCI. As such, on\nsuch large datasets using CCI may not be an unfair approximation.\nFurthermore, our mathematical modeling only explains the origin of the minimum of CI with respect to Q but does not\nprovide a way to find it analytically since we cannot easily estimate \u03b1, \u03b2 and \u03b3 in Equation 10.\nAs mentioned at the end of paragraph 2.2, saturation at 100% accuracy may negatively impact the computation of\nconfidence intervals and particularly the value of paired tests CI in Equation 5.\nFinally, we point out that paired tests introduce complexity as they require a fixed seed and necessitate saving and\npublishing individual task accuracies when using the benchmark and comparing methods."}, {"title": "8 Conclusion", "content": "In our study, we demonstrated the stark contrast between Open and Closed measurements of method accuracy in\nFew-Shot Learning. Notably, OCIs take into account data randomness but are far wider than CCIs. We identified two\nmajor approaches that contribute to narrowing the OCIs and subsequently introduced a benchmark which uses these\napproaches. Our findings underscore the importance of using confidence intervals that account for data randomness in\nevaluations, a practice we advocate extending beyond classification and vision to encompass all domains employing\ntask-based few-shot learning assessments."}, {"title": "A Mathematical derivation of Var(A)", "content": "Suppose tasks are drawn IID without replacement, we write the variance of A as:\n$Var (\\overline{A}) = \\frac{1}{T} Vart (A_t)$,   (8)\nwith At the accuracy for an arbitrary task t. By definition of the variance,\n$Var (A_t) = E[(A_t)^2] \u2013 (E[A_t])^2$. 2.  (9)\nFor a given support set, the expected accuracy for some class c is denoted $ \\mu_{t,c}$.\n$\\mu_{t,c} E_Q (1[f_{S_t} (x) = c] | S_t)$.\nThen the expectation of At becomes\n$E[A_t] = E_{S_t} [\\mu_{t,c}]$,\nAlong the same lines, we can derive E[($A_t$)$^2$],\n$E[(A_t)^2] = E_{S_t} E_Q[(A_t)^2|S_t]$.\nFor a fixed $S_t$, 1[$f_{S_t}(x) = c$] and 1[$f_{S_t}(x') = c'$] are not independent and their distribution will depend on the classes\nc and c'. Using Equation 1, we obtain .\n$E[(A_t)^2] = \\frac{1}{(KQ)^2} \\sum_c \\sum_x \\sum_{c'} \\sum_{x'} E_{S_t} E_Q,[1[f_{S_t}(x) = c]1[f_{S_t}(x') = c']]$.\nWe now separate cases where x = x' from cases where c = c' and finally cases where both are different.\n$E[(A_t)^2] = \\frac{1}{KQ} E_{S_t} [\\mu_{t,c}] +  \\frac{Q -1}{KQ} E_{S_t} [(\\mu_{t,c})^2] +  \\frac{1}{K^2}\\sum_c \\sum_{c'\\neq c} E_{S_t} [\\mu_{t,c}\\mu_{t,c'}]$.\nUsing Equation 9, we find:\n$Var (A_t) =  \\frac{1}{KQ} -E_{S_t} [\\mu_{t,c}] +  \\frac{Q -1}{KQ} E_{S_t} [(\\mu_{t,c})^2] +  \\frac{1}{K^2}\\sum_c \\sum_{c'\\neq c} E_{S_t} [\\mu_{t,c}\\mu_{t,c'}] \u2013 (E_{S_t} [\\mu_{t,c}])^2$.\nLet us define some parameters,\n$m_1 = E[\\overline{A}] = \\frac{1}{K} \\sum_c E_{S_t} [\\mu_{t,c}]$,\n$m_2 = \\frac{1}{K} \\sum_c E_{S_t} [(\\mu_{t,c})^2]$,\n$m_3 = \\frac{1}{K^2} \\sum_c \\sum_{c'\\neq c} E_{S_t} [\\mu_{t,c} \\mu_{t,c'}]$.\nWe get that\n$E[(A_t)^2] =  \\frac{1}{KQ} m_1 +  \\frac{Q -1}{KQ} m_2 + m_3$.\nThis gives\n$Var (A_t) =   \\frac{1}{KQ} m_1 +  \\frac{Q -1}{KQ} m_2 + m_3 (E_{S_t} [\\mu_{t,c}])^2$\nThen, using Equation 6 (removing the rounding) and 8, we approximate:\n$Var (\\overline{A}) = \\frac{K}{NC} \\frac{\\beta}{(\\alpha Q + \\gamma)}$,  (10)\nwith $\\alpha = \\frac{m_2}{K} - m_1$, $\\beta =  (\\frac{m_1}{K} - m_2)$ and $\\gamma =  -\\frac{m_2}{K} + m_2 + S(m_3 - (m_1)^2)$.\nFirst, let us notice that \u03b2 > 0 since for \u03bc \u2208 [0, 1], $\u03bc^2 < \u03bc$."}]}