{"title": "To Train or Not to Train: Balancing Efficiency and Training Cost in Deep Reinforcement Learning for Mobile Edge Computing", "authors": ["Maddalena Boscaro", "Federico Mason", "Federico Chiariotti", "Andrea Zanella"], "abstract": "Artificial Intelligence (AI) is a key component of 6G networks, as it enables communication and computing services to adapt to end users' requirements and demand patterns. The management of Mobile Edge Computing (MEC) is a meaningful example of AI application: computational resources available at the network edge need to be carefully allocated to users, whose jobs may have different priorities and latency requirements. The research community has developed several AI algorithms to perform this resource allocation, but it has neglected a key aspect: learning is itself a computationally demanding task, and considering free training results in idealized conditions and performance in simulations. In this work, we consider a more realistic case in which the cost of learning is specifically accounted for, presenting a new algorithm to dynamically select when to train the Deep Reinforcement Learning (DRL) agent that allocates resources. Our method is highly general, as it can be directly applied to any scenario involving a training overhead, and it can approach the same performance as an ideal learning agent even under realistic training conditions.", "sections": [{"title": "I. INTRODUCTION", "content": "The 6G paradigm will revolutionize mobile networks by integrating communication and computing in a holistic fashion, offering specialized services that constantly adapt to the specific needs of end users [1]. This extreme customization of network applications will be allowed by the wide diffusion of Artificial Intelligence (AI), which will become a core component of network management [2]. The AI native nature of 6G will therefore transform networks into proactive entities that respond to user requests while pursuing various optimization goals, far beyond human capabilities.\n\nA crucial use case of AI in 6G is the orchestration of computational jobs in Mobile Edge Computing (MEC) [3]. The offloading of tasks to MEC-capable Base Stations (BSs) will support autonomous vehicles, holographic communication, and many other breakthrough services, which require not only a huge bandwidth for data transmission, but also computationally intensive operations with strict timing deadlines [4]. The high system load that these tasks will impose makes AI algorithms necessary for automatically handling the allocation of computational resources, prioritizing critical applications and adapting to network dynamics without human intervention [5].\n\nOver the past decade, the benefits of AI for MEC optimization have been highlighted in many circumstances [6], [7]. In particular, the Deep Reinforcement Learning (DRL) paradigm can find decision-making strategies to allocate computational resources to the pending jobs, outperforming traditional heuristic approaches like Shortest Job First (SJF) [8], which do not take into account the specific demand patterns of each MEC server. Traditional DRL frameworks assume that the target scenario is stationary in time, which enables the use of a pre-existing dataset for training and does not require modifying the learning model after its deployment. This assumption is ill-suited to the expected requirements for 6G networks, which make extreme flexibility one of the key points of their operation [9]. This then requires adopting a Continual Learning (CL) approach, so that the DRL agents constantly acquire new information and retrain when the environment is subject to significant changes [10].\n\nHowever, updating learning agents with new information makes it necessary to devote resources to train the agents themselves. As learning is a computationally intensive operation, the optimization of CL systems can have a strong impact on the performance of networks and computing facilities. In traditional DRL problems, the agent is considered as being outside the environment: its training and decision-making are assumed to be instantaneous, with no impact on the environment evolution. However, this assumption becomes unrealistic in CL systems, in which DRL training is both computationally expensive, requiring the computation of gradients over batches of experience samples, and online.\n\nIn this manuscript, we consider the problem of allocating computational jobs in a MEC server via a DRL agent. Following our previous work [11], we define the cost of learning as the overhead incurred by the allocation of training jobs to improve the agent policy. Since the ultimate goal of the agent is to maximize the MEC efficiency and serve as many users as possible within their deadlines, this poses a dilemma: should we use resources to train, and improve the agent accuracy in the long term, or maximize the number of resources assigned to the users, i.e., the immediate reward?"}, {"title": "II. SYSTEM MODEL", "content": "We consider a MEC server connected to a cellular BS and equipped with various types of computational resources that we consider as a unified pool with capacity C. We assume that the system operates with discrete time slots 7 and that a MEC scheduler allocates the available resources at the beginning of each slot. We also assume that the BS area covers Nuser users, each of which generates computing jobs according to a Bernoulli process $X_n \\sim B(p)$. We define $p = \\rho/(\\mu N_{\\text{user}})$, where p, named average load, is the average fraction of the cluster capacity C requested by the users, and u is the average number of resources each job requires.\n\nEach job j requires a certain amount of resources $c_j$ and a fixed execution time $e_j$, which must be allocated when it is scheduled. It also has a fixed deadline $T_j$, i.e., the maximum time that the job can wait before execution without violating the user Quality of Service (QoS) requirements. Thus, if a job request is not satisfied before the deadline expires, it is discarded by the MEC server. We assume that, at the beginning of each slot, the MEC scheduler can allocate resources to a single new job. Once MEC resources are assigned to a job, they remain allocated to it until its completion.\n\nWe can then formulate the job scheduling problem as an Markov Decision Process (MDP), following the framework proposed in [12]. According to this model, the MEC server is provided with a finite buffer that can contain up to L jobs. If a new job arrives when the buffer is already full, it is discarded by the system and marked as failed. The state of each job in the buffer is encoded by a tuple $(e_j, c_j, w_j, T_j)$, where $w_j$ is the time that job j has already spent in the buffer. Hence, the buffer state can be represented by a $4 \\times L$ matrix B.\n\nThe full state of the MEC server depend on both the buffer conditions and the already allocated computational resources: we can represent the reserved resources for the next M slots as a vector $g \\in {0,1,...,C}^M$, where g(m) indicates the amount of allocated resources at slot m. At the end of each time step, we set $g_{t+1}(m) = g_t(m+1) \\forall m \\in {1, ..., M}$, and $g_{t+1}(M) = 0$. This notation is a more compact representation of the binary matrix defined in [12]. The time evolution of the buffer is rather simple: while $e_j$, $c_j$, and $T_j$ remain constant for each job in the buffer, $w_j$ is incremented by 1 at each time slot, and the job is discarded if $w_j > T_j$. If the MEC scheduler allocates a job on the server, it is removed from the buffer and marked as successfully executed.\n\nAt the beginning of each slot, the agent observes the system state $s = (B,g) \\in S$ and chooses an action $a \\in A$. The action space is defined as $A = {0, 2, ..., B - 1} \\cup {\\emptyset}$, where B is the buffer capacity, a = i indicates scheduling job in position i, while a = \u00d8, i.e., the void action, indicates that no job is scheduled in the current time slot. Therefore, in each time unit, the learning agent can choose from B + 1 possible actions. We observe that some actions might be invalid, i.e., the chosen position in the buffer may be empty, or the job may require the allocation of resources that are already busy. Invalid actions are equivalent to the void action, while valid actions make jobs be immediately assigned to MEC resources.\n\nWe now introduce a delay penalty function $\\varphi(\\omega, T)$, which describes the satisfaction level of a job with a deadline T"}, {"title": "III. COST OF LEARNING FRAMEWORK", "content": "In real scenarios, MEC scheduling policies need to continuously adapt to time-varying demand patterns and user requirements. In the case of learning-based optimization, as the one we consider, this requires to adopt a CL approach in which the MEC scheduler is trained in real time. Hence, we must take into account the computing overhead due to the cost of learning, i.e., the need to allocate part of the MEC resources to the training jobs used for the agent optimization.\n\nIn the following, we assume that the D-DQN algorithm can improve the DRL agent by processing B batches of experience samples only when a training job is allocated by a meta-scheduler. This latter is separated from the agent itself to avoid the convergence issues we discussed in the introduction. We then consider a training job to require $C_{tr}$ resources for a single time slot, and present two heuristics that are able to dynamically decide when to allocate training jobs without harming the performance of the MEC system.\n\nFirstly, we consider a stationary MEC scenario, where the average load is fixed and equal to $p = 0.3$. We simulate\nthe resource allocation system over $N_{train} = 1000$ episodes,\neach consisting of $N_{slot} = 1000$ time slots. When using\nlearning-based methods, we employ the \\epsilon-greedy strategy with\nexponential decay for the first 350 episodes and then maintain\na constant value $\\epsilon = 0.1$ for the remaining episodes."}, {"title": "A. Periodic Training Strategy (PTS)", "content": "The first heuristic we propose is named Periodic Training Strategy (PTS) and schedules training jobs at regular intervals, referred to as training periods and denoted as $T_c$. At the start of each training period, a training job is immediately scheduled in the cluster, regardless of the system state. Determining the optimal $T_c$ is then a key issue: lower values accelerate the training process, but also increase the load on the system, making the agent's task more difficult and taking computing resources away from the users. On the other hand, if $T_c$ is too high, the learning algorithm converges too slowly, using a suboptimal policy for extended periods."}, {"title": "B. Adaptive Training Strategy (ATS)", "content": "The major limit of PTS is that it operates independently of the current conditions of the MEC server. To address this limitation, we design Adaptive Training Strategy (ATS), which aims at identifying when to allocate training resources. The ATS algorithm leverages the current estimates of the Q-values to determine which states are the best for training, enabling the system to make more informed decisions. Indeed, states with higher Q-values are, by definition, more favorable to the agent: we can reasonably expect states with a higher expected long-term reward to be more resilient to the disruption caused by reserving resources to the training.\n\nAt the beginning of each slot, ATS simulates the effects of inserting a training job into the server, which would move the system from state s to state s* (also called training state), as shown in Fig. 1. This can be simply done by marking $C_{tr}$ resources as occupied at the earliest possible moment.\nAfterwards, ATS compares the Q-values for both the current and training state, getting\n$\\psi(s) = \\max_a Q(s^*, a) + \\beta (\\mathbb{I} (\\max_a Q(s^*, a) - \\max_a Q(s, a))$,\nwhere $\\beta \\in \\mathbb{R^+}$ is a parameter that balances the contributions of the two equation terms. The first term represents the maximum"}, {"title": "IV. SIMULATION SETTINGS AND RESULTS", "content": "In this section, we discuss the results of the simulations we performed to evaluate the proposed cost of learning framework in the MEC system described in Sec. II. We compare the proposed PTS and ATS approaches in both stationary and dynamic environments. We also consider two benchmarks, namely the classical SJF algorithm [14], which is commonly used as a standard approach for resource allocation, and an ideal DRL solution with no cost of learning, which provides an upper bound for DRL performance in realistic settings."}, {"title": "V. CONCLUSION AND FUTURE WORK", "content": "In this work, we proposed a framework to handle the cost of learning in DRL-based resource allocation problems. We considered a MEC system, and designed a framework to mitigate the significant but often overlooked computing overhead associated with learning-based optimization strategies. Unlike traditional DRL solutions, our framework considers that the training of learning agents has a direct impact on the system that these agents aim to optimize. Hence, we proposed an adaptive strategy that identifies the best moments to carry out training, exploiting information derived from the learning process itself. Our strategy outperforms traditional DRL approaches and can be readily generalized to other scenarios where the training impact is a critical factor, such as edge network optimization.\n\nFuture extension of this work will involve the investigation of the relation between training and exploration decisions, since training effectiveness is unavoidably dependent on the efficiency of the environment exploration. Besides, we plan to validate our framework and the proposed training strategy in more scenarios, considering data from real applications."}]}