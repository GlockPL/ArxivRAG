{"title": "Koel-TTS: Enhancing LLM based Speech Generation with Preference Alignment and Classifier Free Guidance", "authors": ["Shehzeen Hussain", "Paarth Neekhara", "Xuesong Yang", "Edresson Casanova", "Subhankar Ghosh", "Mikyas T. Desta", "Roy Fejgin", "Rafael Valle", "Jason Li"], "abstract": "While autoregressive speech token generation models produce speech with remarkable variety and naturalness, their inherent lack of controllability often results in issues such as hallucinations and undesired vocalizations that do not conform to conditioning inputs. We introduce Koel-TTS, a suite of enhanced encoder-decoder Transformer TTS models that address these challenges by incorporating preference alignment techniques guided by automatic speech recognition and speaker verification models. Additionally, we incorporate classifier-free guidance to further improve synthesis adherence to the transcript and reference speaker audio. Our experiments demonstrate that these optimizations significantly enhance target speaker similarity, intelligibility, and naturalness of synthesized speech. Notably, Koel-TTS directly maps text and context audio to acoustic tokens, and on the aforementioned metrics, outperforms state-of-the-art TTS models, despite being trained on a significantly smaller dataset.", "sections": [{"title": "1. Introduction", "content": "The advancement of large language models (LLMs) has brought transformative improvements to speech synthesis, enabling more natural and contextually adaptive speech generation. In particular, there has been a recent surge in the use of LLMs for various applications such as text-to-speech (TTS) and speech-to-speech translation. LLM-based TTS systems enable prompt-based customization, generating speech with human-like intonation while adapting to stylistic cues, contexts, and expressive nuances. This allows for diverse applications, from conversational interfaces to expressive narration, without extensive retraining. These advancements have been largely driven by the emergence of discrete neural audio codecs, which compress raw audio into token-based representations while preserving high fidelity. Despite these advances, LLM-based TTS systems face challenges, with hallucinations being a prominent issue. For example, when encountering text with repeated or redundant phrases, LLM-based TTS models may overemphasize these repetitions or fail to capture the intended flow and naturalness of the sentence. Additionally, among the multiple outputs sampled for the same input, there can be significant variation in quality, with some outputs sounding more natural, accurate, and appealing than others. This issue is akin to challenges faced in text-generation LLMs, where outputs may range from highly coherent to erroneous, depending on the model's response to complex prompts.\nFor text-generation, preference alignment techniques have been proposed to guide models to produce outputs that better match human preferences in coherence, relevance, and clarity. This is achieved through training with human feedback or automated scoring, based on criteria such as factual correctness and fluency. Driven by these advances, recent research employs preference alignment algorithms, including RLHF and offline preference ranking methods, to refine audio LLM outputs. For instance, SpeechAlign proposes an iterative strategy to align speech language models with human preferences by addressing the distribution gap between golden AR tokens (from real speech) and synthetic AR tokens (generated during inference).\nBuilding upon the above insights, we propose preference alignment and CFG techniques to enhance contextual coherence of LLM-based TTS models. We introduce Koel-TTS, a transformer-based TTS model that leverages a low-frame-rate (21.5 FPS) audio codec to enable low-latency autoregressive speech generation. To perform preference alignment, we first identify key metrics that strongly correlate with human judgments of generated speech: transcription accuracy and target speaker similarity. Each metric captures distinct aspects of the generated output and can be evaluated using automatic speech recognition (ASR) and speaker verification (SV) models. We integrate these metrics into a reward system that ranks the generated outputs. With this foundation, we then explore preference alignment algorithms, focusing on pairwise ranking methods and scalar reward optimization. Our findings show that fine-tuning the base model with preference alignment significantly improves speaker similarity, intelligibility, and generalization to unseen speakers. Notably, our method also enhances naturalness, despite not explicitly optimizing for this metric.\nTo further enhance synthesis quality with CFG, we train the Koel-TTS model with both conditional and unconditional inputs, by randomly dropping out conditional information (text and context audio) during training. During inference, the unconditional logits are combined with conditional logits using a CFG scale, to achieve significant improvement in intelligibility, speaker similarity, and naturalness of generated speech. Furthermore CFG can be applied independently to the base model or the preference aligned model, yielding substantial improvements across all evaluation metrics for both. Combining preference alignment with CFG, we train a 1.1 billion parameter multilingual Koel-TTS model that achieves state-of-the-art zero-shot TTS results across several human and automatic evaluation metrics."}, {"title": "2. Methodology", "content": "Our proposed framework is an autoregressive speech token generation model that is conditioned on a text input and a context audio. In this section, we first describe the tokenization scheme employed for representing speech and text. Next, we detail the model architecture and describe three model designs we explore for context-conditioning. Finally, we propose two key techniques to improve the robustness and speaker similarity of our model using preference optimization algorithms and classifier free guidance."}, {"title": "2.1. Tokenization", "content": "Speech: We employ a neural audio codec model to transform raw speech signals into tokenized representations. For a given audio signal a, the codec model outputs a two-dimensional acoustic matrix $C_{T\\times N} = \\text{CodecModel}(a)$. In this representation, $C_{T\\times N}$ consists of m-bit discrete codes, where T corresponds to the downsampled sequence length, and N represents the number of codebooks per timestep. We utilize the Low Frame-rate Speech Codec, which achieves high-quality audio compression at"}, {"title": "2.3. Training Objective", "content": "The output of each decoder-timestep is mapped to a vector of size N x 2m using a linear layer to obtain the logits of all N codebooks (each of size m-bits) at that timestep. Thereby, for all decoder time-steps, we obtain logits l of size T \u00d7 N \u00d7 2m and calculate cross-entropy as follows:\n$\\mathcal{L}_{\\text{token}} = \\text{CE} (\\text{softmax} (l), \\text{target}_{N\\times T})$"}, {"title": "2.4. Preference Alignment", "content": "We employ preference optimization methods to steer the outputs of Koel-TTS towards more desirable results. For a given text and context audio input x = (Xtext, Xaudio), the model's response distribution \u03c0(y|x) encompasses a range of potential outputs y with varying levels of alignment to the desired criteria. By constructing a dataset that explicitly labels certain responses yc as chosen and yl as rejected, we can leverage preference-based optimization algorithms to shift the model's distribution toward producing more preferred responses.\nOne such approach is Direct Preference Optimization (DPO). DPO uses preference comparisons to modify the policy \u03c0 by contrasting it against a reference policy \u03c0ref. Specifically, given an input x and a chosen response yc that is preferred over a rejected response yl, DPO seeks to increase the likelihood ratio $\\frac{\\pi(y_c|x)}{\\pi_{ref}(y_c|x)}$ relative to $\\frac{\\pi(y_l|x)}{\\pi_{ref}(y_l|x)}$. The core objective can be expressed as:\n$\\mathcal{L}_{DPO} = \\mathbb{E}_{x,y_c,y_l} [-\\beta \\log \\frac{\\pi(y_c|x)}{\\pi_{ref}(y_c|x)} - \\beta \\log \\frac{\\pi(y_l|x)}{\\pi_{ref}(y_l|x)} ]$\nwhere \u03b2 is a parameter for controlling the deviation from the base reference policy \u03c0ref. The above formulation, encourages \u03c0 to produce responses more similar to yc than yl, effectively aligning the model with the desired preferences.\nBuilding upon DPO, we also leverage Reward-aware Preference Optimization (RPO) which considers the magnitude of reward differences in the optimization process. Rather than treating all chosen versus rejected distinctions as equal, RPO utilizes scalar rewards to measure how much better the chosen response is compared to the rejected one. The RPO objective introduces a factor that scales the preference updates based on the reward gap r* (x, yc) - r* (x, yl) as follows:\n$\\mathcal{L}_{RPO} (x, y_c, y_l) \\coloneqq D \\Big[ \\sigma \\Big( -\\eta (r^*(x,y_c) - r^*(x,y_l) \\Big) \\Big|\\Big| \\frac{\\pi(y_c|x)}{\\pi_{ref}(y_c|x)} - \\beta \\log \\frac{\\pi(y_l|x)}{\\pi_{ref}(y_l|x)} \\Big]$\nwhere \u03b7 is a scaling parameter and D is a distance metric given by $D [a||b] := \\sigma(b) \\log (\\frac{\\sigma(b)}{\\sigma(a)}) + (1 \u2013 \\sigma(b)) \\log \\frac{1-\\sigma(b)}{1-\\sigma(a)}$. Thereby, RPO mitigates overfitting to narrowly better responses since the loss value is scaled as per the reward difference.\nPreference Data Creation and Reward System: To construct the preference dataset (x, yc, yl), we begin by selecting a diverse set of text and speaker prompt combinations that challenge the model's ability to produce accurate and natural speech. The text data includes a mix of regular sentences from standard TTS datasets, and carefully curated challenging transcripts generated by prompting text LLMs. These challenging texts are designed to test the model's robustness and include elements such as repeated words, numbers, and phonetically complex sequences. The inclusion of standard texts ensures generalizability, while the challenging examples target specific weaknesses of the TTS model."}, {"title": "2.5. Classifier Free Guidance", "content": "To adapt CFG for autoregressive token prediction models, we train both a conditional and an unconditional model simultaneously by randomly dropping out the text and context/speaker conditioning during training. At inference time, conditional and unconditional outputs are combined to guide the speech generation process. This approach allows for more precise control over the generated speech, which can lead to improved pronunciation, prosody, robustness, and overall audio quality.\nlcfg = y * lc + (1 \u2212 y) * lu\nwhere y \u2265 1 is the CFG interpolation scale controlling the strength of guidance. Higher scale values steer the generation to follow the text/audio inputs, while lower scale values allow more variations. In practice, we sweep around a range of values to find the optimal scale \u03b3."}, {"title": "3. Experiments", "content": "For our primary experiments, we train the models on a data-blend containing 18k hours of English TTS data from the following datasets: train-clean-360 and train-clean-100 subsets of LibriTTS, HiFiTTS, a 17k-hour subset of the LibriVox MLS dataset and a proprietary, 2-speaker, 63-hour dataset. With this dataset we create (context audio, transcript, target audio) triplets where context and target audio are distinct utterances from the same speaker. During training, we use a random 5 second context slice for the decoder context model and 3 to 8 seconds context slice for"}, {"title": "3.2. Architecture Comparison", "content": "Table 1 presents the baseline results of different Koel-TTS architectures on seen and unseen English speakers, without incorporating preference alignment training or CFG inference. All three architectures achieve similar intelligibility, but the decoder context model outperforms the multi encoder model on unseen speaker similarity, while the latter performs slightly better on seen speakers. These results suggest that decoder context model generalizes better to unseen speakers making it a more suitable choice for zero-shot TTS. The multi encoder architecture tends to overfit to the training speakers, as indicated by consistently worse speaker similarity on unseen speakers, and better speaker similarity on seen speakers across all our experiments. While SV conditioned model also achieves similar SSIM as decoder context, perceptually, we find the decoder context model captures the intended style of the context audio better. We encourage readers to listen to audio examples on our website."}, {"title": "3.3. Preference Alignment", "content": "To perform preference alignment, we create a preference dataset using the procedure described in Section 2.4. Specifically, we first curate 800 challenging texts generated using Llama-8b . It is prompted to generate texts with repeated words and alliterations. The complete"}, {"title": "3.4. Classifier Free Guidance", "content": "By controlling the CFG scale y during inference, we can steer the generations to be better aligned with conditional inputs. We vary y between 1 to 3 at 0.2 intervals and show the results of this experiment on unseen speakers in Figure 4. Increasing y significantly reduces the CER and simultaneously increases SSIM across all models. From these observations, we set y=2.5 as the optimal value."}, {"title": "3.5. Multilingual TTS", "content": "For multilingual TTS, we investigate six languages English, Spanish, German, French, Italian, and Dutch. For non-English languages, we use the CML dataset that contains 1,562 hours of German, 642 hours of Dutch, 476 hours of Spanish, 283 hours of French, 131 hours of Italian speech data. Additionally, we incorporate 42 hours of internal Spanish data from two speakers. Combining this with our 18k hours of English TTS data, we create a final blend of 21k hours of multilingual TTS data.\nFor preference alignment of the multilingual model, we create 10k text and context audio pairs per language (by pairing texts with a random context audio), from the CML training data of each language. We combine these pairs with 20k English text and context audio pairs randomly sampled from the 58k pairs used in our primary experiments. We utilize the whisper-large-v3 ASR model in our reward system to create preference pairs and perform DPO finetuning with \u03b2=0.05.\nFigure 5(right) presents the results of multilingual-TTS evaluations on unseen speakers from each language. For evaluations on non-English languages, we use 100 utterances per language from the CML test set of each language. For English evaluation, we use the same 180 utterances from LibriTTS test-clean subset as used in our primary experiments. As shown by the results, both preference alignment and CFG (with y=2.5) yield substantial improvement in both intelligibility and speaker similarity metrics across various languages and tokenizers. More interestingly, CFG inference on a DPO finetuned checkpoint, yields substantial speaker similarity improvements over using either DPO or"}, {"title": "3.6. Comparison against Past Work", "content": "We benchmark two candidate Koel-TTS models: English-only decoder context model and the larger multilingual decoder context model against past work and open source models. We evaluate all models for zero-shot TTS on the unseen speaker evaluation set (test-clean LibriTTS subset), using the same evaluation procedure as described in Section 3.1. We also compute three human evaluation metrics on Amazon Mechanical Turk namely Naturalness Mean Opinion Score (MOS), Speaker similarity MOS (SMOS) and Comparative MOS (CMOS). As shown in Table 3, Koel-TTS achieves state-of-the-art intelligibility scores (CER/WER) despite being trained on significantly less data than competing models. While Koel-TTS outperforms LLM-based baselines (VALLE-X and XTTS-v2) in SSIM scores, it slightly underperforms CFM-based systems (F5-TTS and E2-TTS), which leverage 100k+ hours of speech data, compared to 21k hours for our largest model. Human evaluations of naturalness (MOS) and speaker similarity (SMOS) show Koel-TTS to be equally or more preferred compared to all other models. We attribute the difference between SSIM scores and SMOS to SSIM's emphasis on timbre similarity, whereas human ratings consider additional factors such as style and accent. CMOS results in Figure 6, further confirm that Koel-TTS is preferred over all competing approaches."}, {"title": "4. Conclusion", "content": "We introduce Koel-TTS, a suite of encoder-decoder transformer models that map text and context audio to acoustic speech tokens. By incorporating preference alignment driven by transcription accuracy and speaker similarity, and Classifier Free Guidance, we improve intelligibility, speaker similarity, and naturalness of generated speech, achieving state-of-the-art zero-shot TTS performance. Koel-TTS excels in multilingual TTS, delivering high-quality, low-latency speech with a simplified model design. Finally, through audio examples on our webpage, we demonstrate that Koel-TTS can be effectively fine-tuned for long-form multi-turn dialogue generation, by adapting the model for contextually aware podcast synthesis."}, {"title": "A. Pareto optimal ranking for creating preference pairs", "content": "Pareto optimal ranking is a technique for multi-attribute decision making. The key idea is to find non-dominated solutions and removing them from the current set recursively till we have ranked all items. When we find multiple items in the same pareto front, we break the ties by prioritizing our preference for more robust examples (lower CER), and we break any remaining ties by preferring higher SSIM. Below is the python code for ranking for this procedure."}, {"title": "B. DPO and RPO on all model architectures", "content": "We perform DPO and RPO preference optimization on all models and evaluate the preference aligned checkpoint with and without CFG. Results are reported in Table 4. We observe a significant improvement in CER, WER and SSIM across all baseline models, when preference alignment or CFG is done in isolation. Across most architectures, the best metrics are achieved by CFG inference on a preference aligned checkpoint (DPO + CFG or RPO + CFG). Both DPO and RPO perform similarly, but in practice, we find DPO to be more sensitive to \u03b2 hyperparameter as compared to RPO."}]}