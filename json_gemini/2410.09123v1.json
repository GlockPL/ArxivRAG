{"title": "Context-Aware Adapter Tuning for Few-Shot Relation Learning in Knowledge Graphs", "authors": ["Ran Liu", "Zhongzhou Liu", "Xiaoli Li", "Yuan Fang"], "abstract": "Knowledge graphs (KGs) are instrumental in various real-world applications, yet they often suffer from incompleteness due to missing relations. To predict instances for novel relations with limited training examples, few-shot relation learning approaches have emerged, utilizing techniques such as meta-learning. However, the assumption is that novel relations in meta-testing and base relations in meta-training are independently and identically distributed, which may not hold in practice. To address the limitation, we propose RelAdapter, a context-aware adapter for few-shot relation learning in KGs designed to enhance the adaptation process in meta-learning. First, RelAdapter is equipped with a lightweight adapter module that facilitates relation-specific, tunable adaptation of meta-knowledge in a parameter-efficient manner. Second, RelAdapter is enriched with contextual information about the target relation, enabling enhanced adaptation to each distinct relation. Extensive experiments on three benchmark KGs validate the superiority of RelAdapter over state-of-the-art methods.", "sections": [{"title": "1 Introduction", "content": "Knowledge graphs (KGs) (Bollacker et al., 2008; Suchanek et al., 2007; Vrande\u010di\u0107 and Kr\u00f6tzsch, 2014) have been widely adopted to describe real-world facts using triplets in the form of (head entity, relation, tail entity). However, curating and maintaining all the possible ground-truth triplets is impossible, and various approaches for knowledge graph completion (Bordes et al., 2013; Yang et al., 2014; Trouillon et al., 2016; Sun et al., 2019) have been proposed to discover missing facts. Many of these methods adopt a supervised learning paradigm, which require abundant training data for each relation. In real-world settings, novel and emerging relations, along with many relations in the long tail, are associated with very few instances (Xiong et al., 2018), limiting their performance.\nSubsequently, few-shot relation learning (FSRL) on KGs has emerged to handle novel relations with only a few known instances. An established line of work (Chen et al., 2019; Niu et al., 2021) employs meta-learning, most notably Model-Agnostic Meta-Learning (MAML) (Finn et al., 2017). MAML aims to learn a prior from a series of meta-training tasks, which can be rapidly adapted to downstream meta-testing tasks. The meta-training tasks are specifically constructed in a few-shot setup to mimic downstream tasks. In the context of FSRL, each task contains only a few training instances of a single relation, and the objective is to predict more instances for a novel task (i.e., relation1) not seen in meta-training. For example, MetaR (Chen et al., 2019) aims to learn a relation-specific prior (also called meta-knowledge) during the meta-training stage, using a series of meta-training tasks constructed from a set of base relations with abundant instances. Subsequently, the meta-knowledge is leveraged for rapid adaptation, through a lightweight fine-tuning step, for few-shot predictions on novel relations in meta-testing.\nLimitation of prior work. The major limitation of FSRL methods based on meta-learning lies in the assumption that the meta-training and meta-testing tasks are independently and identically distributed (i.i.d.). However, different relations may diverge significantly in their underlying distributions, thereby weakening the i.i.d. task assumption. To investigate this hypothesis, we randomly sample a large number of relation pairs from standard benchmark datasets, namely, WIKI, FB15K-237 and UMLS2. We perform a mean pooling across all entities within each relation task to derive an average embedding as the task representation. For every pair of relations, we plot their cosine similar-"}, {"title": "2 Related Work", "content": "Supervised relation learning. Knowledge graph embedding aims to transform entities and relations into a low-dimensional continuous vector space while preserving their semantic meaning. Conventional knowledge graph completion models can generally be classified into three main categories: (1) Translation-based methods, such as TransE (Bordes et al., 2013), TransH (Wang et al., 2014), and TransD (Ji et al., 2015), which are additive models that use distance-based constraint to optimize entity and relations embedding. (2) Semantic matching-based methods, such as DistMult (Yang et al., 2014) and ComplEx (Trouillon et al., 2016), which are multiplicative models that exploit the interaction between entity and relation vectors. (3) Graph-based models, include graph neural networks such as GCN (Kipf and Welling, 2017) and RGCN (Schlichtkrull et al., 2018), which considers higher-order structures in KGs. However, these supervised approaches rely on a large amount of training data and are not well-suited for few-shot relation learning.\nFew-shot relation learning. To address one- or few-shot relation learning, many models have been proposed recently in two main categories: (1) Metric-based models that calculates a similarity score between support and query sets to learn the matching metrics. GMatching (Xiong et al., 2018) uses a one-hop neighbor encoder and a matching network, but assumes that all neighbors contribute equally. FSKGC (Zhang et al., 2020) extends the setting to more shots and seeks to merge information learnt from multiple reference triplets with a fixed attention mechanism. FAAN (Sheng et al., 2020) introduces an relation-specific adaptive neighbor encoder for one-hop neighbors. (2) Optimization-based models aim to learn an initial meta prior that can be generalized to a new rela-"}, {"title": "3 Preliminaries", "content": "In this section, we introduce the problem of few-shot relation learning (FSRL) and the meta-learning framework for this problem.\nProblem formulation. A knowledge graph G = (V, R) comprises a set of triplets. Each triplet is represented by the form (h, r, t) for some h, t \u2208 V and r\u2208 R, where V is the set of entities and R is the set of relations.\nConsider a novel relation r \u2209 R w.r.t. a knowledge graph G. Further assume a support set S = {(hi, r, ti) | i = 1, 2, . . ., K} for some hi, ti \u2208 V for the novel relation r. The goal is to predict the missing tail entities in a query set, Qr = {(hj, r, ?) | j = 1, 2, . . ., }, w.r.t. the given hj \u2208 V and r.\nIn the paper, for each triplet in (hj, r,?) \u2208 Qr, a type-constrained candidate set Chj,r is provided and the objective is to rank the true tail entities highest among the candidates. Together, the support and query sets form a task Tr = (Sr, Qr) for r. The support set provides a few training instances, known as K-shot relation learning, where |Sr| = K is typically a small number.\nMeta-learning framework. Given the success of meta-learning in few-shot problems, we adopt MetaR (Chen et al., 2019), a popular meta-learning-based approach for FSRL, as our learning framework. MetaR consists of two stages: meta-training and meta-testing, aiming to learn a prior \u03a6 from the meta-testing stage that can be adapted to the meta-testing stage. On one hand, meta-training involves a set of seen relations Rtr, and operates on their task data Dtr = {Tr | r \u2208 Rtr}. On the other hand, meta-testing involves a set of novel relations Rte such that Rtr \u2229 Rte = \u00d8, and operates on their task data Dte = {Tr | r \u2208 Rte}. Note that the ground-truth tail entities are provided in the query sets of the meta-training tasks Dtr, whereas the objective is to make predictions for the query sets of the meta-testing tasks Dte.\nMeta-training. During meta-training, the model learns a prior \u03a6, which serves as a good initialization to extract a shared relation meta, RT, \u2208 Rd,"}, {"title": "4 Methodology", "content": "In this section, we introduce the proposed approach RelAdapter. As depicted in Fig. 2, RelAdapter has two important components, namely, (a) adapter network and (b) entity context. On one hand, the adapter network aims to facilitate relation-specific and tunable adaptation of meta-learned prior in a parameter-efficient manner. On the other hand, the entity context aims to enrich the adapter with contextual information pertinent to the target relation, enabling more precise adaptation to each distinct relation. The two components are integrated into a context-aware adapter, which enhances FSRL for the novel relations in meta-testing.\nIn the rest of the section, we first introduce the context-aware adapter, followed by the details of the meta-training and meta-testing stages.\n4.1 Context-aware Adapter\nWe enhance the adaptation to novel relations at the model level through an adapter module, and at the data level through context-aware adaptation.\nAdapter. The objective of the adapter is to achieve a relation-specific adaptation for the novel relations in meta-testing, to overcome the divergence from the seen relations in meta-training. Specifically, we adapt the relation meta RT, to the target relation r through the adapter module, as follows.\n$R_{A} = Adapter (R_{T} ; \\Theta_{r}) = \\alpha \\cdot FFN(R_{T} ; \\Theta_{r}) + (1 - \\alpha) \\cdot R_{T}$         (4)\nwhere the output from the adapter is $R_{A}^{'}$, the adapted relation meta specific to the relation r. The adapter module consists of a lightweight feed-forward network (FFN) and a residual layer, as shown in Fig. 2(a), where \u03b1 is a hyper-parameter to balance the FFN and the residual, and \u0398r is the r-specific parameter of the adapter. Note that the FFN typically adopts a bottleneck structure, which projects the input dimension d into a smaller dimension m, reducing the number of parameters to achieve parameter-efficient adaptation.\nContext-aware adaptation. At the data level, we augment the embedding of each entity (head or tail) by additional pre-trained contextual information from their related entities, as shown in Fig. 2(b). The contextual information enables more tailored adaptation to each distinct novel relation.\n$e_{c} = \\mu. Mean({f(e_{k}) | e_{k} \\in N_{e}}) + (1 - \\mu) \\cdot emb(e)$         (5)"}, {"title": "5 Experiments", "content": "In this section, we conduct comprehensive experiments on our proposed approach RelAdapter.\n5.1 Experiment Setup\nDatasets. We utilize three benchmark datasets, namely, WIKI, FB15K-237 and UMLS. Table 1 depicts the dataset details, including the pre-train/train/validation/test splits on the relations. The pre-trained encoder, f(\u00b7), which provides initial entity embeddings for the FSRL models, are based on the pre-train split. Note that the four splits are mutually exclusive to avoid information leakage (Zhang et al., 2020). Additional details for the dataset can be found in Appendix B.\nMetrics. We employ two popular evaluation metrics, mean reciprocal rank (MRR) and hit ratio at top N (Hit@N) to compare our approach against the baselines. Specifically, MRR reflects the absolute ranking of the first relevant item in the list and Hits@N calculates the fraction of candidate lists in which the ground-truth entity falls within the first N positions.\nBaselines. RelAdapter is compared with a series of baselines in two major categories. (1) Super-vised relation learning. They learn one model for all the relations in a supervised manner. We choose four classic and popular supervised methods: TransE (Bordes et al., 2013), DistMult (Yang et al., 2014), ComplEx (Trouillon et al., 2016) and RGCN (Schlichtkrull et al., 2018). We follow the same setup in GMatching (Xiong et al., 2018), which trains on the triplets combined from the pre-train and train splits, as well as the support sets of the test splits. (2) Few-shot relation learning (FSRL). They are designed for few-shot relation prediction tasks. We choose several state-of-the-art FSRL methods, as follows: GMatching (Xiong et al., 2018), FSKGC (Zhang et al., 2020), GANA (Niu et al., 2021), FAAN (Sheng et al., 2020), HiRe (Wu et al., 2023), MetaR (Chen et al., 2019) and the details can be found in Appendix C.\nImplementation Details. For a fair comparison,"}, {"title": "5.2 Comparison with Baselines", "content": "Table 2 reports the quantitative comparison of RelAdapter against other baselines in the 3-shot setting, i.e., the size of the support set is 3. (We study the effect of the number of shots in Sect. 5.5).\nOverall, our model RelAdapter outperforms other baselines across all the three datasets, which demonstrates the benefit of incorporating context-aware adapter for FSRL. In particular, RelAdapter outperforms the most competitive baseline HiRe by 9.84% in terms of average MRR and 2.22% in terms of average Hit@10. Furthermore, we also draw the following observations.\nFirst, supervised relation learning methods tend to perform worse as compared to FSRL methods as they are not designed to handle novel relations in few-shot setting. Meanwhile, as RGCN considers the neighborhood aggregated information, it consistently outperforms other supervised relation learning models across all the three datasets.\nSecond, among the FSRL methods, GMatching is designed for one-shot setting, and a simple mean pooling is applied to handle multiple shots, resulting in unsatisfactory performance. FSKGC generally performs better than GMatching as it extends the one-shot setting to more shots and explores new ways to encode neighbors with an attention mechanism. Although both GMatching and FSKGC consider neighborhood information, the simple neighborhood aggregation design is not expressive enough to capture complex relations. On the other hand, GANA and FAAN outperform GMatching and FSKGC as they consider the neighborhood information via more expressive neigh-"}, {"title": "5.3 Ablation Study", "content": "To investigate the impact of various modules, we study four variants of our model, as shown in Table 3. (1) W/o A: We remove the adapter module entirely, while retaining the contextual information for entities. It shows a pronounced drop in per-"}, {"title": "5.4 Efficiency Analysis", "content": "We analyze the parameter and runtime efficiency of our adapter module.\nParameter efficiency. We first study the parameter overhead from the addition of adapter module. As shown in Table 4, the number of parameters in the adapter module is negligible w.r.t. MetaR. The parameter-efficient design implies that our adapter tuning is less likely to overfit to the few-shot examples. Also note that, compared to MetaR, the only new parameters of RelAdapter belong to the adapter module.\nRuntime efficiency. As reported in Table 5, our approach RelAdapter generally incurs a lower or"}, {"title": "5.5 Sensitivity Analysis", "content": "Lastly, we conduct a sensitivity analysis for various settings and hyperparameters in Fig. 3. In particular, we vary the number of shots K while fixing the hyperparamters, as well as several key hyperparam-eters while fixing K = 3. In the figures, the x-axis refers to the range of K or parameters against the MRR metric in y-axis. The error bars represent the spread of standard deviation for each data point.\nFew-shot size K. The few-shot size K refers to the number of triplets in the support set of each relation. As shown in Fig. 3(a), when K increases, we consistently observe performance improvement as more data becomes available for training.\nAdapter ratio \u03b1. As given in Eq. (4), Sect. 4.1, RelAdapter employs a hyperparameter \u03b1 which controls the weight of the residual in the context-aware adapter. A bigger \u03b1 means less residual, giving the adapter more transformative power to adapt to each relation. As shown in Fig. 3(b), the"}, {"title": "6 Conclusion", "content": "In this paper, we proposed RelAdapter, a context-aware adapter for few-shot relation learning (FSRL). We investigated the limitation of FSRL methods in prevailing meta-learning frameworks, which rely on the i.i.d. assumption. This assumption may not hold for novel relations with distribution shifts from the seen relations. Based on this insight, we introduced a context-aware adapter module, enabling relation-specific, tunable and fine-grained adaptation for each distinct relation. Extensive experiments were conducted on three benchmark datasets, demonstrating the superior performance of RelAdapter.\nLimitations\nIn RelAdapter, one potential limitation is that the context-aware adapter module is currently only integrated into the MetaR framework. Despite the significant improvement in performance, it would be ideal to explore the integration of RelAdapter with other meta-learning frameworks in general."}, {"title": "A Meta-training stage", "content": "In the meta-train stage as illustrated in Fig. 4, we update the model with the query loss LQr in the same way as MetaR. In summary, we formulate the trainable parameters of the meta-train stage as \u03a6, \u0398, emb, where \u03a6 is the set of trainable parameters in relation-meta learner RML, \u0398 is the set of trainable parameters of the adapter module and emb is the embedding learner. The output of the meta-training stage includes the set of meta-trained entity embedding emb as well as the relation-meta learner RML, which will be further used to adapt to downstream tasks in the meta-testing stage. The purpose of introducing the adapter network into the meta-training stage is to maintain consistency with the architecture of the meta-testing stage, which can improve performance."}, {"title": "B Datasets", "content": "We utilize three benchmark datasets, namely, WIKI, FB15K-237 and UMLS. On each dataset, the relations are divided into four subsets: pre-training, training, validation and test, as shown in Table 1. For the smaller dataset UMLS, all relations with less than 50 triplets are removed."}, {"title": "C Baselines", "content": "Few-shot relation learning (FSRL) are designed for few-shot relation prediction tasks, where the testing relations are previously unseen in pre-training or training. We choose several state-of-the-art FSRL methods, as follows: GMatching (Xiong et al., 2018) uses a neighbor encoder and a matching network, assuming that all neighbors contribute equally. FSKGC (Zhang et al., 2020) encodes neighbors with a fixed attention mechanism, and applies a recurrent autoencoder to aggregate the few-shot instances in the support set. GANA (Niu et al., 2021) improves on FSKGC by having a gated and attentive neighbor aggregator to capture valuable contextual semantics of each few-shot relation. FAAN (Sheng et al., 2020) introduces an adaptive neighbor encoder for different relation tasks. HiRe (Wu et al., 2023) brings in a hierarchical relational learning framework which considers triplet-level contextual information in contrastive learning. MetaR (Chen et al., 2019) utilizes a MAML-based framework, which aims to learn a good initialization for the unseen relations, followed by an optimization-based adaptation."}, {"title": "D Implementation details", "content": "We train RelAdapter for 100,000 epochs, and select the most optimal model based on the validation relations every 1,000 epochs with early stopping for a patience setting of 30. The mini-batch gradient descent is applied with batch size set as 64 for FB15K-237 and UMLS, and 128 for WIKI. The number of hidden neurons is set as 50 for all datasets. We use Adam (Kingma et al., 2015) with the initial learning rate of 0.001 to update parameters. The intensity of gradient update is fixed at 5. The number of positive and negative triplets in each query set is 3 in FB15K-237 and UMLS, and 10 in WIKI. All experiments are conducted on an RTX3090 GPU server in Linux."}, {"title": "E Number of hops in Ne", "content": "As observed in Table 7, increasing the number of hops considered in neighborhood contexts Ne for"}]}