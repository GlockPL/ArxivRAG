{"title": "Prompt2DeModel: Declarative Neuro-Symbolic Modeling with Natural Language", "authors": ["Hossein Rajaby Faghihi", "Aliakbar Nafar", "Andrzej Uszok", "Hamid Karimian", "and Parisa Kordjamshidi"], "abstract": "This paper presents a conversational pipeline for crafting domain knowledge for complex neuro-symbolic models through natural language prompts. It leverages large language models to generate declarative programs in the DomiKnowS framework. The programs in this framework express concepts and their relationships as a graph in addition to logical constraints between them. The graph, later, can be connected to trainable neural models according to those specifications. Our proposed pipeline utilizes techniques like dynamic in-context demonstration retrieval, model refinement based on feedback from a symbolic parser, visualization, and user interaction to generate the tasks' structure and formal knowledge representation. This approach empowers domain experts, even those not well-versed in ML/AI, to formally declare their knowledge to be incorporated in customized neural models in the DomiKnowS framework.", "sections": [{"title": "1 Introduction", "content": "With the rise of large language models (LLM), the community is getting closer to the dream of natural language programming interfaces [30, 36]. Nonetheless, developing systems capable of seamlessly and accurately interpreting and executing complex programming tasks using natural language remains a challenging goal. Conversely, declarative programming has endeavored to express programs at the high-level problem specifications [24]. Within this programming paradigm, emphasis on the logic of what needs to be accomplished, as opposed to the intricacies of implementation, reduces the gap between natural and declarative languages. Inspired by this idea, our research envisions a natural language interface for a declarative learning-based programming framework [35, 21, 20]. Within the interface, developers articulate their learning tasks using natural language, where the underlying system translates them into code in the declarative language of DomiKnowS [9]. DomiKnowS further establishes connections between symbolic knowledge and neural learning components (currently designed in PyTorch), facilitating the integration of domain knowledge into deep learning models.\nGiven the rise of LLMs and their ability to be universal computational models, the need for customized models can be debatable. However, we follow two main arguments that drive the need for developing customized learning-based models: 1) Despite their impressive performance across various tasks [48], Large Language Models (LLMS) trail behind smaller, domain-specific models [43, 42, 19, 26], and 2) incorporating domain knowledge into models, apart from the performance gains, improves the interpretability and reliability of the models by adhering to domain constraints [31, 34, 45]. Imposing domain knowledge has the potential to leverage explicit alignment of the models with safety, fairness, and ethical constraints [14].\nIn this work, we leverage the progress on specialized frameworks for knowledge integration [9, 1, 25, 16], particularly those with declarative interfaces [22] and the advancements in tailoring neural model architectures using LLMs [42].\nPrior research on tailoring deep learning models with natural language prompts has either focused on fine-tuning simple text-to-text models [42] or composing complex architectures during inference [38]. However, our proposal facilitates the development of complex deep-learning models and enables further fine-tuning them on the task-specific data within one framework.\nAdapting LLMs to generate DomiKnowS declarative programs presents several challenges. These include limited resources for fine-tuning (There are few training samples available in the declarative language of DomiKnowS, unlike popular programming languages such as Python or frameworks such as Py-Torch.), the requirement to adjust LLMs to a new coding style specific to DomiKnows, the complexity of translating domain knowledge into First-Order Logic (FOL) statements [13], let alone the specific DomiKnowS' Python-based FOL language, and the potential lack of user familiarity with the DomiKnowS language, hindering meaningful feedback in a human-in-the-loop process.\nTo tackle these issues, we propose an interactive pipeline where we utilize an underlying LLM with techniques such as prompt templates for user interactions, dynamic few-shot in-context learning, intermediary mapping of natural language (NL) to FOL statements, and iterative refinement based on feedback from symbolic semantic/syntactic verification functions to generate the final declarative modeling code. The pipeline's interface 3 can be accessed online."}, {"title": "2 Related Research", "content": "We aim to facilitate the development of neuro-symbolic models leveraging background knowledge expressed via logical constraints. Using logical knowledge, explored both during inference [11, 37, 12, 33] and training [15, 28, 44] of neural networks, has been encapsulated in other libraries like DeepProbLog [25], PyLon [1], and Scallop [16]. DomiKnowS [9] provides a declarative interface for"}, {"title": "3 DomiKnowS Interface", "content": "Our tool aims to translate natural language descriptions of structured prediction tasks into corresponding representations within the DomiKnowS framework. DomiKnowS programs include three components: knowledge declaration (domain/task structure and constraints), model declaration (computational units), and program execution (knowledge integration, learning, and inference). This paper focuses on the knowledge declaration, as it often requires input from domain experts who may lack familiarity with neural architectures or programming languages and benefit significantly from a natural language interface. The knowledge declaration stage results in a concept graph that represents the structure of tasks and incorporates domain-specific logical constraints. A snippet of code in DomiKnowS, illustrating the representation of the Natural Language Inference (NLI) task graph structure and logical constraints, is provided in Figure 1."}, {"title": "3.1 Graph Structure", "content": "The graph illustrates the input-output structure and dependencies of the task. It includes nodes representing different concepts and edges denoting their relationships. In NLP tasks, input concepts consist of sentences, paragraphs, phrases, words, and tokens. Vision tasks involve concepts related to images and bounding boxes. In structured prediction, the output concepts are task labels. Each decision concept needs to be anchored to an input concept. For example, the concept of sentence_class is connected to the sentence concept, and named entity tags are associated with the phrase concept. DomiKnowS uses \"is_a\" to represent connections between a concept and its parent, \"contains\" for relations between a concept and its same-type children, and \"has_a\" for many-to-many relations."}, {"title": "3.2 Logical Constraints", "content": "DomiKnowS introduces a domain-specific Python-based language that employs concepts, edges, variables, and logical operators to declare dependencies in first-order logic between concepts, which defines the tasks' domain knowledge in the form of constraints (See Fig 1 for a sample code.). Significantly, the syntax for specifying first-order logic constraints in DomiKnows is not observed during the pre-training of large language models, presenting a difficulty in generating outputs that adhere to this format in a zero or few-shot setting."}, {"title": "4 Natural Language to Knowledge Declaration", "content": "Our interactive interface employs prompt templates from the LangChain framework and GPT-3.5-turbo. At each stage, user input fills in a predefined template that the model consumes to generate the information needed for subsequent steps. We adjust the conversation history to prevent excessive context accumulation, especially where we have an iterative process.\nFigure 2 provides an overview of the pipeline. The model can consume a series of underlying natural language instructions, in-context demonstrations, and execution feedback, in addition to the user input and information from prior interactions at each step. The process begins by gathering basic information, such as the task's name, domain, and dataset name (see step 1 in the figure). Subsequently, the model formulates a clear task description, including the task's input/output and decision types. Users have the flexibility to modify this description. Next, the model compiles a list of concepts (nodes in the graph) representing the task's structure (the input/output concepts, decision types, and more; See step 2). Users can modify items on this list, which is the foundation for generating an initial concept graph (in Python; See step 3).\nThe generated code might not be accurate/executable due to the model's lack of familiarity with DomiKnowS' language syntax. Therefore, we implement a loop for parsing the code and detecting the errors while providing the LLM with feedback for refinement. At each step, the model receives errors from the code's parse and execution, which evaluates the graph structure and uses this feedback"}, {"title": "Sampling Strategy", "content": "We draw multiple samples from the underlying LLM at each step and select the most accurate response through user feedback or automated metrics, such as error counts from the execution. Additionally, if some samples do not have any errors generated by the symbolic processing engine, we prune out the remaining erroneous samples after a certain amount of iterations."}, {"title": "Dynamic In-Context Retrieval", "content": "Our pipeline involves in-context demonstrations for various tasks in the NLP and vision domains at each step. To enhance model execution speed and minimize noise from irrelevant examples, we selectively include a subset of in-context demonstrations relevant to the target task. We achieve this by creating a vector database of in-context demonstration representations using the OpenAI Embedding service and identifying the most relevant ones for inference based on cosine similarity. This approach reduces computational costs and noise in LLM's generation."}, {"title": "Iterative Symbolic Refinement", "content": "We propose an iterative refinement process in response to LLMs' inability to execute code directly to ensure its sanity. Leveraging the representations of structure and constraints in DomiKnowS, we develop syntactic and semantic evaluation parsers to provide feedback. This symbolic parser generates graph structure and constraints, pinpointing semantic and syntactic errors. The parser is a custom-designed algorithm that processes the written code in DomiKnows and finds errors and inconsistencies in its logic and syntax. The parser's feedback serves as instructions for the LLM to refine its initial answer, addressing the challenge of limited prior exposure to DomiKnowS syntax and ensuring accurate responses."}, {"title": "FOL Mapping", "content": "To enhance constraint generation precision, we utilize an intermediary FOL mapping process. The model translates natural language descriptions into FOL statements and then converts them into DomiKnowS FOL syntax, leveraging the model's exposure to FOL statements during pre-training. Logical predicates/arguments are symbolically extracted and included in the input to help in the prediction. FOL mapping assists LLMs in capturing semantics through FOL representations, enabling users to validate constraints based on these statements, assuming perfect alignment between the two representations."}, {"title": "5 Evaluation and Analysis", "content": "We assessed our method for various tasks in NLP, Vision, and Constraint Satisfaction Problems (CSP) using both automatic (similarity-based) and human judgments."}, {"title": "5.1 Automatic Evaluation", "content": "We devise a set of metrics customized for each step of the process to evaluate the expected output at each stage. This evaluation covers a benchmark of 14 tasks, spanning Entity/Relation Extraction [40], Hierarchical Classification [23], Sudoku, Procedural Reasoning [10, 8], Causal Reasoning [39], Natural Language Inference [3], and more. At each stage, we compare with the ground truth input from the dataset for evaluation. A distinct evaluation approach for end-to-end mode, incorporating human intervention, will be discussed in the next section."}, {"title": "Task, Concept, and Graph Generation", "content": "Table 1 reports the results using automatic metrics that evaluate task description, concepts, and initially generated and corrected graphs. The outcomes demonstrate the success of our proposed tool in generating the information at each step. The large number of error-free tasks after refinement and the higher performance of the model using sampling indicate the effectiveness of both of these techniques in enhancing the model's performance. Notably, generating the concept list shows a higher error rate, especially in tasks like image classification in CIFAR-100 [23], where decision space is large, and the model often falls short by adding 'etc.' to the label set instead of listing all classes. The limited differences in predicted nodes and edges compared to the ground truth suggest that human intervention can effectively address graph errors by instructing the model to add or remove small components. Although dynamic retrieval can help reduce the cost and increase the speed of the process, it slightly hurts the model's performance at these phases."}, {"title": "Constraints Generation", "content": "Table 2 displays experimental results for evaluating the model's ability to generate semantically/syntactically correct logical constraints within the graph structure and Python syntax of DomiKnowS. As indicated by the high percentage of resolved errors (%S in %E in Resolved E) detected by our custom-designed parser, it plays a significant role in improving program accuracy. This is accomplished through the feedback loop, by providing clear descriptions of both syntactic and semantic errors. It is notable that a small sampling factor (3 samples) enhances model accuracy, and FOL mapping serves as a valuable intermediary layer, reducing the need for iterative constraint adjustments. Manual evaluation reveals that while most generated constraints (even in erroneous tasks) are semantically accurate, issues arise in adapting to the DomiKnowS language due to limited prior exposure of the LLM to its specific syntax. While dynamic retrieval aids in direct constraint generation, it hinders performance using FOL due to the lack of exposure to FOL-to-DomiKnowS mappings, which encourages the use of unsupported FOL operations in DomiKnowS."}, {"title": "5.2 Human Evaluation", "content": "The human evaluation assesses two key aspects of our demo. Firstly, it measures the comparative ease and utility of our proposed demo in contrast to the conventional method of navigating the documentation and manually crafting programs within the Python package of the DomiKnowS library. Secondly, the evaluation provides insights into the accuracy of each step in the interactive process, detailing the quantity and nature of user interventions required for satisfactory performance across diverse tasks. Additionally, we aim to assess whether the proposed tool facilitates users unfamiliar with the DomiKnowS programming language in validating the model's responses."}, {"title": "Interface V.S. Coding", "content": "We assess user experiences in crafting structured prediction tasks in DomiKnowS with two volunteer groups (5 people per group). Each group received a 20-minute introduction to structured prediction and a high-level overview of DomiKnowS. The first group used the demo interface, while the second group, with access to DomiKnows documents and examples, manually composed concept graphs and logical constraints. Tasks included hierarchical image classification, sentiment analysis, and entity mention and relation extraction."}, {"title": "Groups' Sub-tasks", "content": "Group 1: Describe the task and interact with the interface. Group 2: Read the documentation and examples; Write the code."}, {"title": "Findings", "content": "Human effort using the interface is reduced 5 times. Opting for the local LLM version instead of the API can further accelerate final code generation. Notably, the second group often sought additional guidance on task modeling, while the tools integrated into the demo interface effectively guided the first group. Additionally, we instructed the second group to revisit the task using the demo interface, allocating them a 10-15 minute timeframe. The consensus within this group was that the demo interface provided a significantly superior and more user-friendly experience."}, {"title": "Interactive Setting", "content": "To evaluate the demo in an end-to-end interactive setting, we asked two volunteers to implement a total of seven tasks while recording their every interaction with the interface."}, {"title": "Duration", "content": "The average time for the process was 17 minutes. The most time-consuming part was the constraint generation, taking more time for both the user to write constraints and the model to generate them (averaging 150 seconds for model responses)."}, {"title": "Task Descriptions", "content": "In 6 out of 7 tasks, the user removed only additional and unnecessary information. In one task, the user extended the decision set by replacing the word 'etc.' with actual labels."}, {"title": "Concept List", "content": "In 4 out of 7 tasks, one sample was correct, while in the remaining tasks, the user had to remove or add less than two concepts to the list."}, {"title": "Concept Graph", "content": "In 4 out of 7 tasks, the correct graph was generated without user intervention. In 2 out of 7 tasks, one interaction was needed. In 1 out of 7 tasks, the user interacted five times to remove a wrongly included relationship between concepts where multiple relationships existed. The visualization tool notably provided users with all the necessary information to evaluate the graph."}, {"title": "Constraints", "content": "Most of the constraints were both semantically and syntactically correct. In cases with erroneous results, the verification process could capture the error but not resolve it correctly. Errors were mainly due to using syntax close to FOL for operations not directly supported in DomiKnowS, like equality between multiple variables. Remarkably, in cases such as constraints in a sudoku table, the model detected similar patterns and used for-loops to implement multiple constraints with similar logical structures but variant logical predicates."}, {"title": "6 Conclusion", "content": "We proposed a natural language interface for declarative programming to facilitate the design of neuro-symbolic models by domain experts. We exploit LLMs and prompt engineering techniques to convert natural language into declarative code, written in a specific syntax for the DomiKnowS neuro-symbolic modeling framework. It includes the structure of concepts and relationships related to the problem, as well as formal logical constraints expressing domain knowledge. Building on top of these declarations, DomiKnowS integrates domain knowledge with deep models of any kind designed in PyTorch. This framework is a step forward in making the design of deep models accessible to domain experts, allowing them to inject their domain knowledge into the model using natural language."}]}