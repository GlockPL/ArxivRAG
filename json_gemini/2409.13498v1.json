{"title": "A Deep Learning Approach for Pixel-level Material Classification via Hyperspectral Imaging", "authors": ["Savvas Sifnaios", "George Arvanitakis", "Fotios K. Konstantinidis", "Georgios Tsimiklis", "Angelos Amditis", "Panayiotis Frangos"], "abstract": "Recent advancements in computer vision, particularly in detection, segmentation, and classification, have significantly impacted various domains. However, these advancements are tied to RGB-based systems, which are insufficient for applications in industries like waste sorting, pharmaceuticals, and defense, where advanced object characterization beyond shape or color is necessary. Hyperspectral (HS) imaging, capturing both spectral and spatial information, addresses these limitations and offers advantages over conventional technologies such as X-ray fluorescence and Raman spectroscopy, particularly in terms of speed, cost, and safety.\nThis study evaluates the potential of combining HS imaging with deep learning for material characterization. The research involves: i) designing an experimental setup with HS camera, conveyor, and controlled lighting; ii) generating a multi-object dataset of various plastics (HDPE, PET, PP, PS) with semi-automated mask generation and Raman spectroscopy-based labeling; and iii) developing a deep learning model trained on HS images for pixel-level material classification. The model achieved 99.94% classification accuracy, demonstrating robustness in color, size, and shape invariance, and effectively handling material overlap. Limitations, such as challenges with black objects, are also discussed. Extending computer vision beyond RGB to HS imaging proves feasible, overcoming major limitations of traditional methods and showing strong potential for future applications.", "sections": [{"title": "INTRODUCTION", "content": "The field of material classification has evolved significantly over the past few decades, transitioning from traditional techniques to the application of deep learning methodologies. Traditional methods, such as thresholding, edge detection, and classical machine learning algorithms (e.g., k-nearest neighbors, support vector machines), rely heavily on manually crafted features and heuristic rules. These methods are often effective for simple tasks but struggle with complex and fine-grained material differentiation due to their inherent limitations in capturing shape and colour variations\nImage classification and segmentation have undergone a paradigm shift with the introduction of deep learning, especially convolutional neural networks (CNNs). Material classification tasks have improved in accuracy and robustness through deep learning models that automatically learn hierarchical features from raw data. However, despite these advancements, challenges remain when using conventional RGB images, especially in scenarios were the color or the shape of the object is not that informative regarding its material.\nRGB images solely rely on spatial features such as edges, textures, and colours visible to the human eye. This reliance often leads to misclassification when objects of different classes have similar appearances. For instance, in the sorting industry, accurately distinguishing between materials with subtle colour differences or complex shapes is crucial for efficiency and quality control. In the pharmaceutical industry, ensuring the purity and correct identification of compounds necessitates fine segmentation capabilities [1]. Similarly, in agriculture, RGB imaging often falls short in detecting diseases in crops that present with subtle spectral variations not visible in the RGB spectrum [2]. In defense applications, accurate material classification can aid in the detection and identification of hazardous substances, ensuring safety and operational efficiency.\nHyperspectral imaging has emerged as a promising solution to these challenges. Unlike traditional RGB imag-"}, {"title": "RELATED WORK", "content": "Material classification has historically relied on several approaches that cut across many domains, including electromechanical and chemical analysis techniques. Raman and Near-Infrared spectroscopy, coupled with a multivariate analysis, have widely been used to identify the materials' composition [6], [7], [8]. In another work, the utilisation of X-Ray Diffraction (XRD), Energy Dispersive X-ray Spectroscopy (EDS) and Atomic Absorption Spectroscopy (AAS) techinques have been proposed, because their high sensitivity and accuracy in detecting microstructural features and hence identifying the element composition of a sample [9]. Zhang et al. [10] highlighted the use of Differential Scanning Calorimetry (DSC) and Thermogravimetric Analysis (TGA) in accurately determining the thermal properties of various materials. Zhang and Shao [11] emphasized the role of optical microscopy in material.\nIn an attempt to further increase the performance and robustness of material classification pipelines, various studies explored the potential of deep learning. In [12] a deep convolutional neural network (CNN) was introduced for classifying 60-GHz radar data with an accuracy of 97%. Deep neural networks have, also, been utilised to analyse surface haptic data [13], achieving a precision of 94% and recall of 92%. Moreover, CNNs have been proposed for the case of visual RGB images [14], [15], [16], [17] in various use cases, e.g. commercial waste, steel products, etc. In another work, Konstantinids et al. [18] proposed a multimodal deep classifier based on ResNet-18 that jointly extracts information from RGB and multispectral cameras to classify plastic polymers, as well as wood by products with an object accuracy of 96%.\nWhile the cited work above demonstrates impressive classification performance, these methods are often slow and computationally intensive. Hyperspectral imaging has become a powerful means for in situ material classification, since they offer the advantage of real-time and efficient analysis. Shaikh and Th\u00f6rnberg [19] investigated the impact of water vapor on polymer identification by means of shortwave infrared hyperspectral imaging yielding an accuracy rate around 88%. According to Shaban [20], hyperspectral imaging has been used to determine different characteristics pertaining to concrete without disturbing its structure at sites, where it would not be possible to take samples back to laboratories for analysis; the typical accuracies are above 90%. With about 93% accuracy Capobianco et al. [21] have characterized ancient roman wall paintings using Hyperspectral Imaging as well as aid in artwork authentication (Polak et al. [22]), achieving a precision of 95%.\nThe combination of deep learning and hyperspectral imaging has shown great promise in improving the material classification capability in Earth Observation applications. Notable examples include models, e.g. Xception-based, CNN-based systems, and R-VCANet achieving accuracies up to 99%, with precision and recall values around 94% and 93%, and F1-scores reaching 91% [23], [24], [25], [26], [27]. Venkatesan et al. [28] applied deep recurrent neural"}, {"title": "HYPERSPECTRAL IMAGING SETUP", "content": "This section provides a detailed description of the physical infrastructure of the cyber-physical system where the dataset was created using materials that were conveyed under the vision and hyperspectral sensors."}, {"title": "Imaging Spectroscopy", "content": "Hyperspectral imaging combines conventional imaging and spectroscopic methodologies with the goal to simultaneously obtain spatial and spectral information from various wavelengths of the electromagnetic spectrum for every individual pixel in an image of a scene, with the objective of locating objects, classifying materials, or detecting processes [35]. Pushbroom sensors capture spectral information across a swath as the sensor moves, line scan sensors capture data one line at a time, while whiskbroom sensors, scan point-by-point to build an image, and snapshot sensors capture the entire spectral image in a single exposure."}, {"title": "Camera", "content": "The SPECIM FX17 line scan camera, following push broom technology, utilises a matrix detector and an imaging spectrograph to capture spectral data efficiently. Light enters through high-performance optics and an entrance slit, forming a line image that the spectrograph disperses into a spectrum (900 - 1700 nm, across 224 bands). This setup allows each axis of the detector to record spatial position and spectral information simultaneously. It ensures measurement stability despite sample or camera movement, requires less illumination power while achieving higher intensity, and is significantly more efficient than filter-based cameras, yielding a purer spectrum."}, {"title": "Conveyor", "content": "Once the line scan camera is selected for capturing, the system comprises a conveyor belt that facilitates the horizontal movement of objects at an adjustable speed, in order to achieve synchronised sampling frequency of the hyperspectral sensor and objects' movements. The camera was placed 0.73 cm from the conveyor, while the illumination source was placed 0.5 meters vertically above the conveyor belt, at a 45-degree angle, and 0.3 meters horizontally from the center of the camera."}, {"title": "Illumination", "content": "The delpoyed custom-made LDL-222X42CIR Full-Spectrum bar light manufactured by CCS Inc., offers better performance for Imaging spectroscopy challenges in contrast with Led lights [36]. Specifically, it includes four different halogen bulbs that emit light in distinct regions of the spectrum and a power rating of 87W. This permits it to span the complete range of wavelengths from 400nm to 2400nm. This light source contains a dispersion layer that uniformly distributes the output light in multiple directions, resulting in more consistent illumination of the scene and the objects within it."}, {"title": "Acquired Data", "content": "The HS camera employed in this work is ample of achieving a maximum sampling rate of 400 FPS, with each scanned line being of shape 640\u00d7224. Moreover, camera provides the option of applying on-chip spatial and/or spectral binning, hence reducing the respective dimensions by factors of \u00d72, \u00d74, or \u00d78. In this work, no spectral or spatial binning was applied, thus the acquired data were of shape nrows \u00d7 640 \u00d7 224, where Nrows is depended on the selected sampling rate and the acquisition duration. Finally, it is worth mentioning that the pixel size of the selected camera is 0.9375mm, thus allowing for very precise and accurate segmentation."}, {"title": "DATASET", "content": "Following the aforementioned hyperspectral imaging setup, this section presents the dataset used in the experimental part of this work. At first, a comprehensive description of"}, {"title": "Dataset description", "content": "For the generation of the train and test dataset an ensemble of plastic samples of different material categories were collected consisting of HDPE, PET, PP, and PS objects.\nTrain Set: In detail, 169 objects were selected to represent the classes. The physical objects were separated according to their material classes and subsequently placed randomly on the conveyor belt, described in Section 3.2. For the augmentation of the training set by every sample was captured twice with slightly different light conditions. A comprehensive analysis of the dataset composition is presented in Table 1."}, {"title": "Acquisition Pipeline", "content": "This section describes the process of converting the captured pixel values from 16-bit unsigned integers, in the range of [0, 4095] to compatible for deep learning and ready-to-used 32-bit floats pixel values in the range of [0., 1.].\nThroughout the literature it is strongly suggested a) to use the spectral calibration matrix, that is specifically provided by the camera manufacturer, and post the HS image's multiplication with spectral calibration matrix, b) to normalize the calibrated data using White and Black reference.\nWhite reference: A white reflection target is required with the key property of reflecting the incident radiation uniformly across all wavelengths. In this manner, the real maximum absolute pixel value a HS camera can capture, given the illumination source, is calculated and ultimately creates the white reference. Although should be taken into account that a white reflection target is often not commercially available and its cost grows exponentially with respect to its dimensions.\nBlack reference: On the other hand, the black reference is utilised to model the sensors' electronic noise, caused by the electrons' random movement due to the sensors' temperature. In this work, the black reference  $I_{black}(\u03bb)$ was acquired, by closing the shutter of the camera and capturing 1000 lines, which were then averaged to create the black reference. For the maximum absolute pixel value the HS camera can capture, the assumption was made that it can be approximated by calculating the maximum pixel value within the train set.\nBy modeling that noise, one can subtract the black reference both from the HS image and the white reference, to acquire the noise-corrected version of both images. Equation 1 provides a mathematical formulation of the black-white reference normalization:\n$I_{norm} (x, y, \u03bb) = \\frac{I(x, y, \u03bb) \u2013 I_{black}(\u03bb)}{I_{white} (\u03bb) - I_{black} (\u03bb)}$\nwhere  $I_{norm} (x, y, \u03bb)$ is the normalized hyperspectral image at pixel (x, y) and wavelength \u03bb,  $I(x, y, \u03bb)$ is the raw hyperspectral image at pixel (x, y) and wavelength \u03bb,  $I_{black}(\u03bb)$ is the black reference image at wavelength \u03bb,  $I_{white} (\u03bb)$ is the white reference image at wavelength \u03bb.\nThe black reference  $I_{black}(\u03bb)$ was acquired, by closing the shutter of the camera and capturing 1000 lines, which were then averaged to create the black reference. For the maximum absolute pixel value the HS camera can capture, the assumption was made that it can be approximated by calculating the maximum pixel value within the train set. Equation 2 provides an approximation of Equation 1 that does not require an expensive white reflection target:\n$I_{norm} (x, y, \u03bb) = \\frac{I(x, y, \u03bb) \u2013 I_{black}(\u03bb)}{M}$ ,\nwhere  $I_{norm} (x, y, \u03bb)$ is the normalized hyperspectral image at pixel (x, y) and wavelength \u03bb,  $I(x, y, \u03bb)$ is the raw hyperspectral image at pixel (x, y) and wavelength \u03bb,  $I_{black} (\u03bb)$ is the black reference image at wavelength \u03bb, M is the dataset's maximum pixel value.\nIt should be noted that in HS image processing applications it is highly recommended to use the aforementioned steps for spectral calibration and normalization which often include expensive equipment and dependencies on the hardware manufacturer. However, as shown in Section 6, even if we totally ignore those steps, the learning performance of the proposed deep learning algorithm is not affected.\nRemark (Calibration on Training). The spectral calibration and the normalization as operations are a sequence of matrix multiplications, it seems by our experiments, that this transformation can be learned directly in the training process."}, {"title": "Ground truth mask generation", "content": "As in every supervised learning application, a set of ground truth labels is needed to ensure successful training of the neural network model. To this end, an AI-assisted methodology was deployed for the generation of the binary masks, presented in Figure 2, which later on will be utilised as training labels."}, {"title": "Semi-Automated Segmentation", "content": "The first step in the proposed methodology, is the creation of a false-colour RGB version of the HS image. To this end, the Standard Deviation of each channel in the original image was calculated, as a measure of its contrast. The three channels with the highest contrast were selected and sorted in ascending order of wavelength, for each image, in order to create the false-colour RGB image. An adaptive histogram stretching algorithm was, also, applied to the respective RGB versions in order to further increase the contrast and make the objects' edges as sharp as possible without altering the spatial content of each image.\nThe false-colour, histogram stretched images were subsequently utilised for the generation of segmentation masks. A ViT model, namely SAM (Segment Anything) [39], for the semi-automated mask generation task. In detail, positive (pixel to be included in the mask) and negative (pixel to be excluded from the mask) points were given as prompts to the model in order to generate a first estimation of the mask. The predicted mask was then visually inspected and refined, when needed, aiming for precision maximization at the boundaries of the object. This procedure was repeated for every image and every object depicted within an image of the dataset and the final results can be seen in Figure 2."}, {"title": "Labeling - Raman", "content": "The final step of the ground truth generation is to assign a class to each of the aforementioned masks. To this end, Raman Spectroscopy was employed.\nRaman spectroscopy is a powerful analytical technique used to observe vibrational, rotational, and other low-frequency modes in a system. It relies on inelastic scattering, or Raman scattering, of monochromatic light, typically from a laser. When light interacts with molecules, vibrations, or other excitations in the system occur shifting up or down the energy of the laser, creating peaks in the acquired spectrum and hence providing a fingerprint by which molecules can be identified [40], [41]. In Figure 3, an example of the spectrum for each of the 4 plastic types is presented, where the red marked peaks indicate the existence of each polymer in under examination the plastic sample."}, {"title": "MODEL ARCHITECTURE & TRAINING", "content": "The proposed hyperspectral image classification model uses a 1D CNN architecture to capture and process spectral information. The architecture uses a) 2 convolutional layers, b) 2 residual blocks, and c) 2 fully connected layers to accurately classify hyperspectral data. The architecture of the proposed Pixel-wise 1D Convolutional Hyperspectral Classifier is shown in Figure 4."}, {"title": "Convolutions", "content": "The input pixel of shape (1 \u00d7 224) is passed through an initial convolutional layer with 16 size-3 filters and padding to preserve dimensions. Afterwards, a 32-filter, 3-size convolutional layer with padding follows. After each convolutional layer, a ReLU activation function introduces non-linearity and a max-pooling layer with a kernel size of 2 and stride of 2 reduces data dimensionality."}, {"title": "Residuals", "content": "The model uses two residual blocks for feature extraction and learning. The first residual block receives the output from the second convolutional layer and processes it through two convolutional layers with 64 filters each, maintaining a kernel size of 3. Batch normalization is also applied after each convolutional layer. This output is added to the block's input - through the utilisation of skip connection - and passed though a ReLU activation function. The second residual block follows a similar structure, but with 128 filters in each convolutional layer."}, {"title": "Fully Connected", "content": "The processed features are flattened and passed through a fully connected layer with 512 neurons, followed by a dropout layer. A second fully connected layer follows with its number of neurons set equal to the number of classes. Finally a soft-max layer provides the final classification probabilities."}, {"title": "Data Pre-Processing", "content": "As mentioned in the Architecture, the model expects a 1-dimensional vector as input. In this work, the input vectors are the individual pixels of the dataset's images. Up to this point, however, the dataset consists of HS images, hence it is necessary to convert the images into a set of pixels.\nTo this end, a data handling pipeline was implemented to efficiently utilise the large volume of data encoded within HS images. In detail, a memory-mapped array uses the operating system's virtual memory capabilities to map a disk file directly into the address space of the application, allowing for efficient, random access to large datasets without loading the entire file into memory. In this manner, by employing memory-mapped arrays out-of-core processing is achieved, which significantly reduces the memory footprint and improves the performance of data loading operations. By exploiting the capabilities of memory-mapped arrays, the entire set of images, along with their respective ground truth masks are flattened across the spatial dimensions, thus creating the desired 1-dimensional vectors is 224 features each. The feature vectors are, subsequently, randomly shuffled and split in two subsets; the train and the validation set with ratios 90% and 10% respectively."}, {"title": "Model Training", "content": "With the data preprocessing pipeline established, the focus can now shift on the model training phase, where the calibrated, and pre-processed data is being used to train the Pixel-wise 1D Convolutional Hyperspectral Classifier.\nA dataloader was defined for each of the two sub-sets, each with a batch size equal to 640. The samples in the train set are shuffled in the beginning of each epoch, thus ensuring slightly different data distribution on batch-level in every iteration. The selected optimizer is Adam [42], and the initial learning rate was set to 0.001. To prevent the model being stuck to a local minima of the loss function, a learning rate (LR) scheduler was also implemented. The LR scheduler utilised in this work was Cosine Annealing with Warmup [43], [44]. The mathematical formulation of this scheduler is described in the following equation :\n$N_{t} = \\begin{cases}N_{init} \\frac{t}{T_{w}} & \\text{if } t \u2264 T_{w}\\\\N_{min} + \\frac{1}{2} (N_{max} -N_{min}) (1 + cos(\\frac{t - T_{w}}{T - T_{w}} \u03c0)) & \\text{if } t > T_{w}\\end{cases}$\nwhere  $\u03b7_t$ is the learning rate at epoch t.  $\u03b7_{init}$ is the initial learning rate,  $\u03b7_{max}$ is the maximum learning rate,  $\u03b7_{min}$ is the minimum learning rate,  $T_w$ is the number of warmup epochs, T is the total number of epochs.\nSince the model's aim is to classify each of the pixel in a HS image to their respective class, the Cross-Entropy Loss Function [45] was selected to be minimized throughout the training process as depicted in Equation 3.\n$L_{CE} = - \\sum_{i=1}^{N} \\sum_{c=1}^{C} Y_{ic} log(\\hat{Y}_{ic})$ ,"}, {"title": "Post-Processing", "content": "The proposed model is capable of processing 330 lines of shape 640 \u00d7 224 per second. In order to take advantage by the spatial domain and to acquire the final classification map, each of the predicted lines is accumulated in a 3D buffer until the whole set of lines post-processed together.\nMedian filter: a kernel size 5, is applied to the reconstructed classification map. In this manner, misclassifications that reassemble the Salt & Pepper noise are correct by substituting them with the median value of the surrounding 5 \u00d7 5 region, as described in Equation 4.\n$I'(x, y) = median{I(i, j) | (i, j) \u2208 W(x, y)}$\nwhere  $I(x, y)$ is the original image,  $I'(x, y)$ is the filtered image, andW (x, y) is the neighborhood window centered at (x, y).\nMorphological opening and closing: the application of such filters enhances the overall quality of the segmentation by removing small noise artifacts and refining object boundaries. The opening filter effectively eliminates small, isolated regions of misclassified pixels, while the closing filter fills in small gaps and smoothens the contours of classified regions, [46], [47]. Morphological filters presented respectively in Equations 5, 6\n$I \u2218 B = (I\u2296B) \u2295 \u0412$,\nwhere I is the original image, B is the structuring element, \u2296 denotes the erosion operation, \u2295 denotes the dilation operation.\n$I \u2022 B = (I + B) \u04e8\u0412$,\nwhere I is the original image, B is the structuring element, \u2295 denotes the dilation operation,  \u0398 denotes the erosion operation.\nTogether, these morphological operations improve the structural integrity of the classification map, leading to more accurate and visually coherent segmentation results."}, {"title": "RESULTS & DISCUSSION", "content": "In this section the results of the proposed work on pixellevel material classification of hyperspectral images are presented, the structure of the sections is as follows a) overall performance, b) capacity to analyse randomly shredded objects, c) ability to distinguish mixed overlapping materials and d) limitations are discussed."}, {"title": "Overall Performance", "content": "As discussed in Section 4 the test-set presented in Table 2 includes images of all the HDPE, PET, PP, PS classes, which however have not been utilised in the training subset. In this manner, guarantee is provided that no same object or pixel is simultaneously evident in both train and test data.\nThe hyperspectral images of this dataset are used by P1CH classifier in order to predict the material class of each pixel. Figure 7 shows the original image as well as the ground truth and the prediction of each pixel.\nTo quantify the model's performance, the accuracy score was calculated for the whole test set in pixel level thus providing a detailed evaluation on the model's classification capacity. The overall accuracy achieved by the model is 97.44%. The confusion matrix is presented in Figure 8, summarizing the classification performance among different material classes.\nThe cell values in the confusion matrix are row-wise normalized, i.e. normalized with respect to the total number of samples in each class. By observing the confusion matrix of"}, {"title": "Shredded materials", "content": "Beyond overall accuracy metrics, evaluating the model's effectiveness in classifying randomly shaped plastic samples is critical, as these shapes present significant challenges for RGB computer vision models and the same time is needed in various real-life applications. In this manner, an ensemble of plastic samples was collected and shredded in small, irregular pieces, and then placed on the conveyor belt. P1CH classifier demonstrates remarkable capabilities in classifying on pixel-level the different material classes, clearly proving its superior performance compared to traditional RGBbased instance segmentation algorithms.\nTraditional RGB models often struggle with such high variability due to their reliance on surface-level features like colour, texture and shape, which are limited in the case of small shredded samples. On the contrary, the proposed hyperspectral imaging approach is trained to purely exploit the rich spectral content of each pixel in the HS image, hence successfully tackling the challenge of small and irregularly shaped objects. Another key advantage of the proposed model is its resilience to noise and artifacts commonly found in RGB images. While traditional models can be easily misled by variations in lighting and surface texture, the spectral information utilised by the proposed model provides a more stable basis for classification.\nIn Figure 10 the HS and the respective RGB images of the shredded plastic samples of HDPE, PET, PP and PS are depicted. Along with the aforementioned images, the Ground Truth and the Predicted masks are presented in the same figure. The achieved accuracy in this specific image is 98.9%, with all the misclassifications falling under the borders case described in the previous section.\nVisual comparison between the ground truth and the predicted mask indicates a high level of agreement in the classification of the plastic fragments. Each class - HDPE, PET, PP, and PS - is distinctly identified and accurately located in the predicted mask. In detail, even though the samples have similar colours regardless of their class, being either white or transparent, the model was able to correctly classify all the samples to their respective category, as indicated in the ground truth mask. This result underscores the robustness of P1CH is invariant to colour and to size of"}, {"title": "Mixed overlapping materials", "content": "In addition to the challenge posed by small irregularly shaped objects, conventional RGB-based detection models often fail to accurately identify distinct samples that are either attached to one another or overlapping. In this subsection, an analysis is conducted to evaluate the model's capacity to precisely segment samples of different material class, while accurately classifying at the same time each individual pixel to its respective class. To this end, two experiments were executed, and in Figure 12 the HS and RGB images and the respective masks of the utilised objects are depicted. To further highlight the proposed model's ability to accurately classify materials and segment their instances within an image, a fine-tuned on the specific classes RGBbased instance segmentation model was also employed to segment the RGB images. The predictions of the RGB-based model are depicted in the 4th column of Figure 12.\nIn the first experiment, a common, commercially available, light blue shampoo bottle was selected. From a visual point of view, as seen in the RGB image, the body of the shampoo container and its lid appear almost identical making it impossible, even for a human, to realize that those two parts are different types of plastic, i.e. HDPE and PP. From the first row in Figure 12, one can easily observe that the RGB-based model was able to identify the contour of greater complex of objects, but not the individual parts, i.e. body and lid. Moreover, the RGB-based model failed to predict the class of both samples, assigning them to the PET class. Finally, due to shadows and non uniform illumination of the scene, the RGB generated mask lacks in precision as it includes pixels of the background too. On the contrary, the proposed HS approach, not only generated highly precise masks for both the body and the lid, but also is capable of classifying those two parts in their respective class. The overall accuracy of the prediction for this specific test case is 98.8%, with disagreements in a pixel's class being evident only between background and not the material classes.\nIn the second experiment, the case of overlapping materials with very similar (white) colour was examined. To this end, a white PS flat surface was selected, on top which a white PP (top right) and a white HDPE (bottom left) lid were placed. The respective masks and images are depicted in the second row of Figure 12. Looking at the RGB image, no distinctive textures can be detect for the two lids, while some minor changes in texture may be identified between the lids and the PS surface. Therefore, given the uniformity in colour and the very low variance in texture, as anticipated, the RGB-based model fail to detect all three objects apparent in the scene. Once again, the generated mask contains the greater complex of samples with no regard to the two lids. Moreover, the class prediction of the RGB-based is inaccurate since it considers the PS surface as PP. In contrast to the RGB case, the proposed HS model effectively utilises the rich spectral signature encoded in each pixel being able to precisely segment all three components of the image, while classyfing materials on pixel-level with 99.54% accuarcy.\nThe results presented above underscore a pivotal advancement in material classification and segmentation capabilities. The most remarkable achievement demonstrated here is the proposed HS model's ability to accurately identify and classify distinct, overlapping samples with complex boundaries. This marks a significant breakthrough, as conventional RGB-based models consistently fail under these conditions, misidentifying materials and producing impre-"}, {"title": "Limitations", "content": "Despite the impressive results acquired in the previous experiments, this work intendeds to also underline the limitations of the HS imaging in classifying materials. To this end, the last experiment involves the analysis of darkcoloured, irregularly shaped samples. For the needs of this experiment, black and dark-coloured HDPE, PP and PS objects were cut in random small fractions and placed on top of the conveyor belt. The respective images and masks are presented in Figure 13. This subsection delves into the acquired results of the analysis of dark samples, discussing also the reasons why the model performs poorly in that case.\nExamining the predictions, in the last column of Figure 13 it is concluded that the model performs very poorly in the case of black plastics. In detail, only PP samples were are correctly classified, although lacking in mask precision. The PS samples were not completely undetected, while some HDPE samples were identified by the model but once again misclassified as PS or PP. The performance of the proposed model in terms of accuracy without taking into consideration the background is equal to merely 39.69%. The confusion matrix presented in Figure 14 specifically summarizes the models performance in the case of black samples. In detail, it is confirmed that all the PS pixels were classified as background, i.e. they were not detected by the model at all. Moreover, from this confusion matrix one can see that indeed the PP samples were accurately classified, with the models precision in that case dropping to 75% since HDPE samples are also misclassifies as PP. Finally, for the HDPE case, only 4.01% of the total samples were correctly classified, with the model's recall for the specific case being equal to 46%.\nAt this point, it of paramount importance to explain the reasons that lead in this drop in the model's performance. Specifically, this phenomenon should mainly be attributed to the nature of dark-coloured materials, rather than being considered a model's deficiency. In detail, the observed dark or even black colour of an object is the result of complete or almost complete absorption of the incident radiation. In this manner, the reflected radiation, which is captured by the HS cameras' sensor, is of very low intensity; hence resulting in a very weak digital signal. Therefore, given the low amplitude of the captured signal, the PSNR is consequently also low, and so is the variability of the spectrum. All the above, result in very similar -almost identical-, noisy features of the model's input vector, thus rendering the model incapable of properly analyzing each pixel's spectrum and ultimately correctly classifying them in their respective classes. An example of the spectrum of a black and a white plastic of the same material are presented in Figure 15. Can be easily observed that the spectrum of the black-colored object is much more noisy with 10 times lower intensity.\nAlthough, it should be highlighted that black plastics of any material class were totally absent from the training set. The exploration of techniques that would probably lead to mitigate the pure performance of computer vision with HS imaging on black objects is beyond the scope of this paper, but opens and intriguing research direction on the intersection between material science and artificial intelligence."}, {"title": "CONCLUSION & FUTURE WORK", "content": "In this paper a lightweight 1D Convolutional Neural Hyperspectral Classification, system, P1CH, is proposed. The developed model generates highly detailed and precise classification maps. Unlike conventional RGB methods, the proposed algorithm utilises the spectral information encoded in hyperspectral image's pixel, allowing to detect and correctly classify objects in challenging scenarios."}]}