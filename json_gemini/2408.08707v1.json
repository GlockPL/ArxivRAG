{"title": "Beam Prediction based on Large Language Models", "authors": ["Yucheng Sheng", "Kai Huang", "Le Liang", "Peng Liu", "Shi Jin", "Geoffrey Ye Li"], "abstract": "Millimeter-wave (mmWave) communication is promising for next-generation wireless networks but suffers from significant path loss, requiring extensive antenna arrays and frequent beam training. Traditional deep learning models, such as long short-term memory (LSTM), enhance beam tracking accuracy however are limited by poor robustness and generalization. In this letter, we use large language models (LLMs) to improve the robustness of beam prediction. By converting time series data into text-based representations and employing the Prompt-as-Prefix (PaP) technique for contextual enrichment, our approach unleashes the strength of LLMs for time series forecasting. Simulation results demonstrate that our LLM-based method offers superior robustness and generalization compared to LSTM-based models, showcasing the potential of LLMs in wireless communications.", "sections": [{"title": "I. INTRODUCTION", "content": "Millimeter wave (mmWave) communication is widely acknowledged as a key technology for next-generation wireless networks due to its huge bandwidth. To address the significant path loss inherent to mmWave signals, huge antenna arrays are often deployed in both base stations (BSs) and user terminals (UTs), facilitating directional transmissions. This directional transmission requires beam training to maximize received power. However, the optimal beam direction changes rapidly in high mobility environments, necessitating frequent beam training and accuracy beam prediction.\nDeep learning (DL) has recently gained significant research interest in wireless communications due to its strong ability to extract nonlinear features. To improve beam tracking performance, DL is commonly employed to extract UT movement features from received signals, thereby predicting future beam variations. Long short-term memory (LSTM) models, in particular, are typically used to periodically predict the next optimal beam based on signals from previous beam tracking, with the prediction period matching the beam measurement period [1][2][3]. However, due to the relatively small number of parameters in LSTM-based models, these models are often sensitive to different wireless environments, and result in poor robustness and generalization, which is a critical issue for the practical applications of learning-based models. This stands in stark contrast to foundation language models, such as GPT-2 [4], GPT-4, and LLaMa [5], which exhibit remarkable robustness.\nPre-trained foundation models, such as large language models (LLMs), have revolutionized computer vision (CV) and natural language processing (NLP). Despite some studies suggesting the potential transformative impact of large models on wireless communications [6][7][8], specific applications of these models in beam prediction have yet to be realized. To bridge this gap, we propose a beam prediction model leveraging LLMs, framing the problem as a time series forecasting task. The robustness of our approach depends on effectively aligning the modalities of wireless data and natural language. This alignment is challenging because LLMs process discrete tokens, whereas wireless data is analog. Additionally, the ability to interpret wireless data patterns is not inherently included in the pre-training of LLMs [9]. Therefore, it remains an open issue to combine LLM with wireless data.\nIn this letter, we introduce a framework designed to leverage large language models for beam prediction without altering the underlying model architecture. Our primary strategy involves aggregating messages from diverse variables and then transforming the aggregated information into text-based prototype representations that align better with the capabilities of language models. To enhance the model's understanding and reasoning abilities of wireless data, we utilize a novel technique, called Prompt-as-Prefix (PaP). Simulation results demonstrate that our LLM-based method exhibits superior robustness and generality compared to LSTM-based prediction schemes. Our initial research indicates that LLM hold promise for diverse applications in wireless communication, shedding lights on the research of LLM-assisted wireless communication."}, {"title": "II. SYSTEM MODEL", "content": "Consider the downlink mmWave transmission for a single user, where the BS and UTs are equipped with M antennas and a single antenna, respectively. Notably, our proposed scheme can be directly extended to the multiuser scenario with multiple UT antennas.\nTo accurately model the beam variations resulting from UT mobility, we adopt the well-known Saleh-Valenzuela (SV) channel model [10] [11]. In this model, the channel vector corresponding to the n-th time slot can be expressed as\n$h_n = \\sum_{l=1}^{L_n} \\sqrt{p_{n,l}} a(\\varphi_{n,l}) \\alpha_{(n,l)},$"}, {"title": "III. PREDICTION MODEL BASED ON LLM", "content": "In this letter, we propose reprogramming an embedding-visible large language model, such as Llama and GPT-2, to perform optimal beam forecasting without the need for fine-tuning the backbone model. Specifically, for the i-th sequence of historical observations $X^{(i)} \\in \\mathbb{R}^{C \\times T}$, where C denotes the number of variables, we aim to adapt a large language model $f(\\cdot)$ to interpret the input time series and accurately predict the values at H future time steps, represented by $\\hat{Y}^{(i)} \\in \\mathbb{R}^{1 \\times H}$.\nInput Preprocessing We use past optimal beam indexes $q_t \\in \\mathbb{R}^{1 \\times T}$ and AoD $\\varphi_s \\in \\mathbb{R}^{1 \\times T}$ as historical observations. Different numbers of antennas cause different ranges of beam indexes, which reduces generalization. Therefore, we first map the sequence of past optimal beam indexes $q_t$ into the angular domain, represented by\n$q_a = \\frac{q_t}{Q}.$\nMoreover, by using $+Q$ or $-Q$, we achieve continuity between values at consecutive time steps, thereby avoiding sudden jumps in the input values between successive time steps.\nFinally, we can get the input $X^{(i)}$ by combining the $q_a$ and $\\varphi_s$.\nInput Embedding Each input sample $X^{(i)} \\in \\mathbb{R}^{C \\times T}$ is initially normalized to have zero mean and unit standard deviation using reversible instance normalization (RevIN) to mitigate time series distribution shifts. Subsequently, $X^{(i)}$ is segmented into several consecutive, overlapped or non-overlapped patches $X_p$ with a length of $L_p$. The total number of input patches is determined by:\n$P = \\lfloor \\frac{T - L_p}{S} + 2 \\rfloor,$\nwhere S denotes the horizontal sliding stride. This process serves two primary purposes: (1) preserving local semantic information by aggregating it into each patch, and (2) functioning as tokenization and forming a compact sequence of input tokens that reduces computational complexity. These patches $X_p^{(i)} \\in \\mathbb{R}^{P \\times C \\times L_p}$ are then embedded as $\\tilde{X}_p^{(i)} \\in \\mathbb{R}^{P \\times C \\times d_m}$ using a straightforward linear layer as the patch embedding.\nCross-Variable Attention In this letter, a learnable vector is used as a router to aggregate messages from all variables. This is achieved by using $R^{(i)}$ as query and vectors of all variables $\\tilde{X}_p^{(i)}$ as both key and value, denoted by\n$B^{(i)} = \\text{ATTENTION}(R^{(i)}, \\tilde{X}_p^{(i)}, \\tilde{X}_p^{(i)}),$\nwhere $R^{(i)} \\in \\mathbb{R}^{P \\times 1 \\times d_m}$ serves as a router. Cross-variable attention is applied to analyze the relationship between the $q_a$ and $\\varphi_s$ and to aggregate messages into $B^{(i)} \\in \\mathbb{R}^{P \\times 1 \\times d_m}$, which is then passed to patch reprogramming.\nPatch Reprogramming We reprogram patch embedding into the source data representation space to align the modalities of time series and natural language, thereby activating the ability of LLMs to understand and reason about time series data. However, time series cannot be directly edited or described losslessly in natural language, which presents significant challenges for LLMs to understand time series without resource-intensive fine-tuning.\nTo address this challenge, we propose reprogramming $B^{(i)}$ using pre-trained word embeddings $E \\in \\mathbb{R}^{V \\times D}$ within the backbone, where V is the vocabulary size. However, there is no prior knowledge indicating which source tokens are directly relevant, making direct use of E impractical due to the large and potentially dense reprogramming space. A practical solution is to maintain a small collection of text prototypes by linearly probing E, denoted as $E' \\in \\mathbb{R}^{V' \\times D}$, where $V' \\ll V$. To identify a small set of text prototypes E' from E, we learn a matrix W \u2208 RV'\u00d7V as the intermediary. These text prototypes learn connecting language cues (e.g., \"short up\" for an upward trend and \u201csteady down\" for a downward trend) that can be combined to represent local patch information (e.g., \"short up then down steadily\" to describe a specific patch) without leaving the pre-trained space of the language model. This approach is efficient and enables the adaptive selection of relevant source information.\nTo implement this, we employ a multi-head cross-attention layer. Specifically, for each head k = {1,\u2026\u2026, K}, we define query matrices $Q_k^{(i)} = B^{(i)} W_q^k$, key matrices $K_k^{(i)} = E'W_k^k$,"}, {"title": "IV. SIMULATION", "content": "We select GPT-2 [4] as our backbone model, which achieves a trade-off between inference speed and prediction accuracy. Note that our method is also theoretically applicable to other LLMs, such as the Qwen series and Llama series [5]. As for baselines, we compared our proposed method with two LSTM-based methods, labeled as ODE [1] and CascadedLSTM [2]. We adopt the DeepMIMO dataset [12], which uses precise ray-tracing method, to accurately simulate the nonlinear relationships between the user movement and beam variation.\nA training dataset comprising 73,728 samples and a validation dataset comprising 9,216 samples are constructed, respectively. In these datasets, we focus on an outdoor environment labeled \"Outdoor 1\", to reflect real-world implementation in natural settings. BSs are indexed as 1-2, covering the range of the simulated area. The UT velocity ranges from 5 ~ 20 m/s, introducing dynamic movement to access the model's adaptability. BSs are configured with 32, 64, or 128 antennas, offering diverse setups for testing coverage and capacity. The number of beams matches the antenna numbers at 32, 64, or 128, facilitating testing of different communication patterns. Lastly, the beam prediction period is set as 16 ms while T is set as 40 and H is set as 10. The optimal beams within"}, {"title": "V. CONCLUSION", "content": "In this paper, we have presented a novel framework that adapts LLM for beam prediction. By aggregating optimal beam indexes and AoD, and subsequently converting them into text-based prototype representations, we align the data format with LLM capabilities. Our innovative PaP technique further enhances the model's understanding and reasoning of wireless data. This work marks the first integration of LLMs with wireless transmission. Our findings suggest that LLMs have significant potential for diverse applications in wireless communications."}]}