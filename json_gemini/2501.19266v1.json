{"title": "Jackpot! Alignment as a Maximal Lottery", "authors": ["Roberto-Rafael Maura-Rivero", "Marc Lanctot", "Francesco Visin", "Kate Larson"], "abstract": "Reinforcement Learning from Human Feedback (RLHF), the standard for aligning Large Language Models (LLMs) with human values, is known to fail to satisfy properties that are intuitively desirable, such as respecting the preferences of the majority (Ge et al., 2024). To overcome these issues, we propose the use of a probabilistic Social Choice rule called maximal lotteries as a replacement for RLHF. We show that a family of alignment techniques, namely Nash Learning from Human Feedback (NLHF) (Munos et al., 2023) and variants, approximate maximal lottery outcomes and thus inherit its beneficial properties. We confirm experimentally that our proposed methodology handles situations that arise when working with preferences more robustly than standard RLHF, including supporting the preferences of the majority, providing principled ways of handling non-transitivities in the preference data, and robustness to irrelevant alternatives. This results in systems that better incorporate human values and respect human intentions.", "sections": [{"title": "1. Introduction", "content": "Reinforcement Learning from Human Feedback (RLHF) has emerged as the de-facto standard to align Large Language Models (LLMs) with human values and preferences. Using ideas from revealed preference theory in economics, current RLHF methods adapt the LLM's distribution of generated text or tokens so as to maximize a reward model learned from the ratings of human evaluators.\nDespite its widespread use in fine tuning LLMS (Touvron et al., 2023; OpenAI, 2023; Anthropic, 2023; Google, 2023), it has been recognized that current approaches suffer from fundamental limitations in the human feedback, the reward model, and training the policy (Casper et al., 2023). These challenges include tradeoffs between the richness and efficiency of feedback types, with binary preferences between pairs of examples being more prominent (Christiano et al.,"}, {"title": "2. Background", "content": ""}, {"title": "2.1. Reinforcement Learning from Human Feedback", "content": "RLHF involves training a reward model and then using this model to guide a policy (the LLM) through reinforcement learning. A reward model $r_\\theta(x, y)$ is trained to predict a score indicating how good the response y is to the prompt x. This model is learned from a dataset of pairwise comparisons, where human annotators indicate their preferences. The training objective involves maximizing the likelihood of correctly predicting the preferred option using a binary cross-entropy loss, $L(\\theta) = -E_{(x,y^+,y^-)}[log(\\sigma(r_\\theta(x,y^+)-r_\\theta(x,y^-)))]$. Here, $(x, y^+, y^-)$ represents a data point, with x being the prompt, $y^+$ the preferred completion (the \u201cwinner\u201d), and y the less preferred completion (the \u201closer\u201d), $r_\\theta$ is the reward model parameterized by $\\theta$ and $\\sigma$ is the sigmoid function. This loss function is based on the Bradley-Terry model (Rafailov et al., 2023) which is the foundation of the classical Elo rating system (Elo, 1978). While this model is widely-used for RLHF, it has several well-documented problems that could affect preference learning (Shah & Wainwright, 2017; Balduzzi et al., 2019; Bertand et al., 2023; Lanctot et al., 2023; Munos et al., 2023).\nThe LLM, acting as the policy $\\pi_\\phi$ parameterized by $\\phi$, is then trained using reinforcement learning algorithms like Proximal Policy Optimization (PPO) (Schulman et al., 2017). A simplified objective (ignoring regularization) can be written as $\\max_\\phi E_{x\\sim D,y\\sim \\pi_\\phi(x)}[r_\\theta(x, y)]$, where $D$ is the distribution of prompts. This loss encourages the LLM to generate completions that receive high reward."}, {"title": "2.2. Social Choice Theory", "content": "The central problem addressed by Social Choice Theory is how to aggregate the preferences of a population so as to reach some optimal collective decision. Assume there is"}, {"title": "3. Alignment as a Social Choice Problem", "content": "We support the view of several works in the literature (Ge et al., 2024; Dai & Fleisig, 2024; Mishra, 2023; Conitzer et al., 2024) that the alignment problem may be formalized as a Social Choice problem. Under this lens, given a prompt x, the set of all possible responses (up to a finite maximum length L) forms the set of alternatives Y the LLM has to"}, {"title": "3.1. RLHF Implements Borda", "content": "There is already an existing connection between current usages of RLHF and Social Choice Theory. In a recent paper, Siththaranjan et al. showed that the standard RLHF methods based on the Bradley-Terry model effectively implement the Borda scoring rule (Theorem 3.1 (Siththaranjan et al., 2024)). For the sake of completeness we provide the full theorem statement and proof in Appendix A.9.\nSince Borda is a well understood Social Choice function, we know that it is not Condorcet consistent. This means that all RLHF methods that aggregates individuals' preferences by emulating Borda may result in some counter-intuitive outcomes. Consider the example in Figure 1. A group of five individuals are asked to specify their favourite colour. Two of the five report that they prefer red more than green, and green more than blue (i.e. $R \\succ_i G \\succ_i B$ for $i \\in {1, 2}$). Three of the five report they prefer blue more than red, and red more than green (i.e. $B \\succ_i R \\succ_i G$ for $i \\in {3,4,5}$). Applying Borda to this example, the Borda scores for the three alternatives (i.e., binary win counts) are 7 for red, 6 for blue, and 2 for green. Thus, an RLHF trained policy would be biased towards returning red, which seems counterintutive and not necessarily a good reflection of the underlying preferences of the group. This raises the question: What properties do we want alignment methods for LLMs to support?"}, {"title": "3.2. Properties for Alignment", "content": "In this section we propose several properties to assess the alignment for LLMs. These properties are inspired by concepts studied in the Social Choice literature and address concerns that arise when reasoning about aggregation of"}, {"title": "4. Standard RLHF Does Not Satisfy Desired Properties", "content": "In the previous section we proposed a set of properties that we believe alignment methods for LLMs should exhibit. In this section we show that current RLHF methods based on the Bradley-Terry model do not satisfy any of the properties, thereby raising the questions\u2014previously noted by others (Chen et al., 2024)\u2014regarding their suitability for alignment problems. While this section builds intuition on simplistic examples, in Section 6.3 we further support our findings with experimental results.\nTo build intuition, in the following we ignore the prompt, and consider a scenario with only three possible options: R, G and B. Relying on the fact that RLHF emulates Borda count (Siththaranjan et al., 2024), we also assume that the LLM post-trained using standard RLHF gives probability close to one to whichever single-token word had the highest win-rate comparison. Of course, in a realistic scenario the LLM would only provide probability close to one if the KL regularization term from the loss is made negligible, either by training for long enough or by giving it a small weight. However, we argue that, if anything, this raises a new concern: through RLHF, a practitioner is aligning the LLM to behave in a middle point between a pretrained model which only cares about what is the probability of the next output (which lacks alignment guarantees) and a model that gives probability one to the token that has the highest win-rate in a preference dataset.\nRLHF is not Majority Consistent nor Condorcet Consistent: We showed that RLHF is not Majority consistent in Section 3.1. Similarly it is not Condorcet consistent. In the example shown in Figure 1, the Condorcet winner is B. This is because, when B is compared with R, three out of the five individual's prefer B to R. Similarly when B is compared to G, three out of the five individuals prefer B to G. However, since RLHF emulates Borda, the"}, {"title": "5. Using Maximal Lotteries to Align LLMS", "content": "Having established the shortcomings of Bradley-Terry based RLHF, the questions becomes \"Is there an alternative approach?\". We answer in the affirmative. In particular, we argue that a probablistic Social Choice function, maximal lotteries, is particularly well suited for alignment of LLMs."}, {"title": "5.1. Maximal Lotteries", "content": "Given a set of preferences, a Probabilistic Social Choice function returns a distribution over alternatives, called a lottery. One particular Probabilistic Social Choice function is the maximal lottery (Kreweras, 1965; Fishburn, 1984a). Define $\\Delta(Y)$ as the set that contains all distribution (i.e. lotteries) over the options Y. A maximal lottery, $\\pi \\in \\Delta(Y)$, is one that is (weakly) preferred to any other lottery: namely\n$\\pi^T M \\pi' \\geq 0, \\forall \\pi' \\in \\Delta(Y)$,\n(1)\nwhere M is the pairwise margin matrix, where each entry $M_{ij}$ represents the net margin of voters who prefer a over b: $M_{ij} = N(a, b) - N(b,a)$.\nEquivalently, one can view M as the payoffs of a carefully constructed symmetric zero-sum margin game where the payoffs are win or loss magnitudes of different pairwise comparisons. The maximal lottery is, thus, the mixed maximin (or Nash equilibrium) solution to the game, and can be computed via linear programming in polynomial time (Brandl et al., 2022).\nMaximal lotteries (ML) exhibit a number of interesting properties. First, they require little structure to be placed on voters' preferences since M is computed solely using pairwise comparisons. This makes them particularly well suited for current LLM alignment processes where preference data typically takes this form.\nSecond, they are Condorcet-consistent and Majority-consistent, in that alternatives in the support of the maximal lottery are the Condorcet winner. They also provide a level of protection against irrelevant alternatives through both being clone-consistent (see Appendix A.2) and independent of irrelevant alternatives (in a probabilistic sense, see Brandl & Brandt (2020); Brandl et al. (2016) and Appendix A.3), and are able to handle cyclic preferences in a principled manner.\nMaximal lotteries, in essence, aim to maximize the probability of selecting an alternative that would win in a pairwise majority comparison against any other alternative. This captures a strong notion of collective preference, prioritizing options that are most likely preferred by a majority of individuals. If there is a clear winner (i.e. there is a Condorcet winner), then Maximal Lotteries will give probability one to that option. When there is debate among a few of options, Maximal Lotteries will return a distribution over those options."}, {"title": "5.2. Using Maximal Lotteries to Align LLMs.", "content": "We believe that emulating Maximal Lotteries with LLMs holds significant potential as a solution to the alignment problem, as it has been shown to be the only probabilistic Social Choice function that satisfies the key desiderata of Arrow's Impossibility Theorem in a stochastic setting (Brandl & Brandt, 2020). This ensures that the LLM's output respects fundamental Social Choice principles.\nThe crucial question, then, is how to train an LLM to behave like a Maximal Lottery. This can be achieved with the following objective function:\nTheorem 1. Let $Y$ be the set of all possible statements up to a finite maximum length L. Let $\\pi$ and $\\pi'$ represent two policies (i.e., LLMs). For two statements $a, b \\in Y$, let $P(a \\succ b)$ be the probability that a random individual picked uniformly from society prefers a over b. Let $P(a \\sim b)$ be the analogous quantity, but for indifference.\nThen, the solution $\\pi^*$ to the following maximin optimization problem\n$\\max_\\pi \\min_{\\pi'} \\sum_{a \\in Y} \\sum_{b \\in Y} \\pi(a) (P(a \\succ b) + \\frac{1}{2} P(a \\sim b)) \\pi'(b)$\nis the Maximal Lottery for the Social Choice problem defined by the set of alternatives Y and the population's preferences over these alternatives. (See proof in Appendix A.5.)\nBeyond the properties highlighted earlier, Maximal Lotteries possess other desirable Social Choice characteristics like participation (Brandl et al., 2019) and reinforcement (Brandl et al., 2016)."}, {"title": "5.3. Maximal Lotteries and the Connection with Nash Learning From Human Feedback", "content": "The objective function presented in Theorem 1 bears a striking resemblance to the optimization process employed in Nash Learning from Human Feedback (NLHF) (Munos et al., 2023). NLHF aims to find a policy $\\pi$ that maximizes its expected reward against an adversarial policy $\\pi'$:\n$\\max_\\pi \\min_{\\pi'} \\sum_{a \\in Y} \\sum_{b \\in Y} \\pi(a) P(a \\succ b) \\pi'(b)$,\n(3)\nwhere $P(a \\succ b)$ represents the probability that a human prefers statement a over b. The key difference between this NLHF formulation and our proposed objective function is the term $P(a \\sim b)$, which accounts for cases where individuals are indifferent between two options.\nThis difference highlights a crucial aspect of human preferences: indifference. While standard NLHF focuses solely on strict preferences, our formulation acknowledges that individuals may be equally satisfied with multiple options."}, {"title": "6. Experiments", "content": "In this section, we compare RLHF with algorithms designed to emulate Maximal Lotteries, evaluating their performance across key Social Choice properties. Specifically, we test whether RLHF fails to satisfy majority rule, Condorcet consistency, and independence of irrelevant alternatives (IIA), and whether it struggles with non-transitive aggregate preferences, and conduct the same analysis for Maximal Lotteries. Full implementation details, including hyperparameters and training configurations, are provided in Appendix A.8. We also note that the literature of NLHF has already compared their methods with RLHF algorithms. We provide a summary of their results in Appendix A.10"}, {"title": "6.1. Experimental Methodology", "content": "To evaluate the performance of Maximal Lotteries against RLHF, we employ synthetic datasets designed to mimic the structure of real-world preference data commonly used in RLHF training. These synthetic datasets allow for controlled experimentation and enable a precise analysis of the properties discussed in Sections 4 and 5. Our synthetic datasets consist of triplets: '<prompt>, <preferred option>, <rejected option>'. The prompt remains constant across all"}, {"title": "6.2. Models", "content": "For our experiments we start from three distinct copies of the pretrained Gemma 2 2b model (Team et al., 2024) without instruction tuning. We train one policy (RLHF policy) using Proximal Policy Optimization (PPO) (Schulman et al., 2017), as explained in Section 2.1, and a second policy (max-lottery policy) using Self-Play Preference Optimization (SPO) (Swamy et al., 2024), which belongs to the family of algorithms that emulate a Maximal Lottery policy. Finally, we use the last copy of the model as the RLHF reward model.\nRLHF policy (PPO): We fine-tune the reward model on the synthetic dataset of human preferences described previously and use it to assign a score to every LLM response. This score guides the policy during reinforcement learning.\nMaximal Lottery policy (SPO): This model is optimized using the objective function presented in Theorem 1 (Section 5) with the SPO algorithm on the same human preference dataset used to train the RLHF reward model. The details of the algorithms, the choice of hyperparameters and their implementation can be found in Appendix A.8."}, {"title": "6.3. Results", "content": "This section reports the comparisons of RLHF and Maximal Lotteries based algorithms on the synthetic dataset described in Section 6.1, simulating the cases introduced in Section 4."}, {"title": "7. Related Work", "content": "This work builds upon several areas at the intersection of AI alignment and Social Choice Theory. Traditional approaches such as Reinforcement Learning from Human Feedback (RLHF) have become the de-facto standard (Christiano et al., 2017; Stiennon et al., 2020) to finetune LLMs. While RLHF has proven effective for guiding LLMs, recent studies have highlighted its limitations (Siththaranjan et al., 2024; Casper et al., 2023; Ge et al., 2024).\nRecent research has explored the application of Social Choice Theory to address the AI alignment problem. Papers such as (Ge et al., 2024; Dai & Fleisig, 2024; Mishra, 2023; Conitzer et al., 2024) argue for viewing alignment as a So-"}, {"title": "8. Limitations and future work", "content": "Our proposed framework, while offering a robust theoretical foundation for aligning LLMs with aggregate human preferences, faces some limitations that require further investigation.\nA central challenge lies in the estimation of $P(a \\succ b|x)$, particularly in two key points: 1) what do we mean when we say that an individual i prefers a to b (a $ \\succ_i b$); and 2) how do we capture that the preferences depend, not only on the prompt x, but on the context.\nOn the first point, what is the correct interpretation that an individual prefers an option a with respect to another b? How realistic is it to assume that it is possible to estimate the preferences of an individual by showing them pairs of sentences, that is the standard practice nowadays? Are there better ways to infer preferences? On this issue, we point to the reader to Gabriel (2020) for an extensive discussion. Microeconomic theory and Industrial Organization theory has a history of attacking similar problems and could be a promising avenue to solve them.\nSecondly, the appropriateness of a response can vary significantly depending on the context in which a conversation takes place. An answer that is perfectly acceptable in a comedy show might be entirely inappropriate in a professional setting. Therefore, it is crucial to explore methods that allow large language models (LLMs) to incorporate contextual information when generating responses. Developing strategies to enhance context awareness in LLMs is an important step toward more reliable and nuanced AI interactions. For a more in-depth discussion on the theory of appropriateness, we refer readers to Leibo et al. (2024).\nAnother important avenue for future work is the development of an online version of our approach that continuously updates and adapts to changes in societal preferences. Human values and societal norms evolve over time, and a static alignment approach may become outdated or fail to reflect current ethical considerations. An online adaptation mech-"}, {"title": "9. Conclusion", "content": "This paper examines the limitations of RLHF in aligning LLMs with aggregate human preferences, demonstrating its vulnerability to violations of key Social Choice principles, and proposing an alternative framework grounded in Maximal Lotteries. We establish a formal connection between this optimal voting system, known to be the only probabilistic voting system that circumvents Arrow's impossibility theorem, and Nash Learning from human feedback (NLHF) algorithms, offering a practical path for training LLMs that robustly reflect collective human preferences. Our experimental results confirm that methods that emulate Maximal Lotteries, like NLHF and variantes, can overcome the shortcomings of RLHF, yielding LLM whose responses better align with the majority's will. This includes supporting the preferences of the majority, providing principled ways of handling non-transitivities in the preference data, and independence of irrelevant alternatives. The shift from simple reward maximization to a framework rooted in the rich theoretical foundations of Social Choice Theory promises a more nuanced and robust approach to aligning LLMs with human values, ultimately contributing to the development of AI systems that truly serve humanity's best interests."}, {"title": "Impact Statement", "content": "Ensuring that AI systems are aligned with diverse human values and preferences is critical for the future of society. The growing influence of AI in decision-making processes, from healthcare to education, emphasizes the importance of considering and valuing everyone's preferences. By integrating techniques that emulate Maximal Lotteries, we provide a robust framework for AI alignment, addressing key limitations of existing methods, such as RLHF. However, achieving true alignment also requires accurately estimating individual preferences and tackling challenges like reward hacking. Additionally, the datasets used to estimate these"}, {"title": "A. Appendix", "content": ""}, {"title": "A.1. Arrow's Impossibility Theorem", "content": "In this section we will discuss Arrow's Impossibility Theorem (Arrow, 1950), arguably the most fundamental result in Social Choice Theory. In this theorem, it is shown that, if #Y > 3, there is no deterministic voting system such that it satisfies three basic properties: Independence of Irrelevant Alternatives, Pareto Efficiency and Non-dictatorship.\nThroughout this paper we have used Social Choice functions for ease of exposition. However, this theorem is usually expressed using Social Welfare functions (SWF) F, i.e. maps from preference profiles {>i}i\u2208P to a ranking, >s. Note,"}, {"title": "\u0391.1.1. \u0399\u0399\u0391 - SWF VERSION", "content": "Definition A social welfare function $F$ satisfies IIA if:\n$\\forall a, b \\in Y$, profiles {$>_i$}, {$>_i'$},\nif $a >_i b \\Leftrightarrow a >_i' b$, $\\forall i \\in P$,\nthen $a >_s b \\Leftrightarrow a >_s' b$,\nwhere $>_s = F(\\{>_i\\}_{i\\in P})$ and $>_s'= F(\\{>_i'\\}_{i\\in P})$"}, {"title": "A.1.2. PARETO EFFICIENCY", "content": "Intuitively, if everyone prefers outcome x to y, then collectively we should also prefer x over y. That property is captured by Pareto Efficiency.\nDefinition (Pareto Efficiency for SWFs): A Social Welfare function F is Pareto efficient if for any preference profile {$\\succ_i\\}_{i\\in P}$ and for any two alternatives x, y $\\in$ Y, if x $\\succ_i$ y for all i $\\in$ P, then x $\\succ_s$ y, where $\\succ_s$= F(\\{$\\succ_i\\}_{i\\in P}$).\nExample: when choosing between chocolate and vanilla, if everyone in a group prefers chocolate ice cream to vanilla, choosing chocolate would be Pareto Efficient. Choosing vanilla would not be, as everyone could be made better off by switching to chocolate."}, {"title": "A.1.3. \u039d\u039fN DICTATORSHIP", "content": "This property formalizes the intuitive idea that a dictator, an individual that makes all collective decisions, is not a desirable form of making choices.\nDefinition (non-dicatorship for SWFs): A Social Welfare function F satisfies non-dictatorship if there is no individual i (the dictator) such that for any preference profile {$\\succ_i\\}_{i\\in P}$ $\\forall x,y \\in V, x \\succ_i y$ if and only if x $\\succ_s$ y, where $\\succ_s$= F(\\{$\\succ_i\\}_{i\\in P}$)."}, {"title": "A.2. Social Choice Theory properties", "content": "In this section, we will list other sets of important properties and paradoxes in Social Choice Theory.\nMonotoniticy (Smith, 1973; Felsenthal, 2011): A Social Choice function satisfies monotonicity if, whenever x is elected under a distribution of voters' preferences, x keeps being elected if some voters increase their support for x (i.e. x moves higher up in their ranking) keeping everything else constant."}, {"title": "A.3. Maximal Lotteries and Arrow's theorem", "content": "This section will effectively be a summary of some of the definitions and axioms from Brandl & Brandt (2020).\nSo far, individuals have a preference over the options Y. In this section, we will extend those to preferences over distributions (i.e. lotteries).\nLet $y$ be a finite set of alternatives, and let $ be the set of all probability distributions over Y. An element $p \\in $ represents a lottery over alternatives in Y. Call P = {1, ..., n} the set of voters, and each voter has a preference relation $2i$ over \u0394.\nGiven p\u2208 \u0394, for i \u2208 P define:\n\u2022 Ui(p) = {q \u2208 \u2206 : q >i p} is the strict upper contour\nset of p\n\u2022 Li(p) = {q \u2208 \u0394: p >i q} is the strict lower contour\nset of p\n\u2022 Ii(p) = {q \u2208 \u0394: p ~i q} is the indifference set of p.\nFor Z \u2286 \u2206, $2i|z = \\{(p,q) \\in \\Sigma : p,q \\in Z\\}$ is the pref-\nerence relation $2i$ restricted to outcomes in Z."}, {"title": "A.3.1. AsSUMPTIONS ON INDIVIDUAL PREFERENCES", "content": "Each individual's preferences \u00bf must satisfy:\nContinuity: Intuitively, if $p2iq$, then small changes in p or q will not reverse the preference. More formally, Ui(p) and Li(p) are open.\nConvexity: Intuitively, if $p2iq$, then any mixture $r = \\Lambda p + (1 \u2212 \\Lambda )q$ (for 0 < \u03bb < 1) is also preferred to q. More formally:\nUi(p), Li(p), Ui(p) U I\u00bf(p), and Li(p) UIi (p) are convex.\nSymmetry: Intuitively, as explained by (Fishburn, 1984b), \"the degree to which p is preferred to q is equal in magnitude"}, {"title": "A.3.2. ARROVIAN PROPERTIES", "content": "A Social Welfare function (SWF) F maps individual preferences (\u22651,...,\u2265n) to the collective preference >. In this section, we will describe a generalization of Arrows Impossibility Theorem's main properties.\nIndependence of Irrelevant Alternatives (IIA) - Brandl & Brandt (2020) version: Let $Z\\subset Y$ be a subset of the original options and $Az$ be the set of loteries over Z. A SFW F satisfies IIA if and only if, for any two preference profiles {$\\succ_i\\}_{i\\in P}$ and {$\\succ_i'\\}_{i\\in P}$, if\n$\\forall i \\in \\{1, ..., n\\}$ ($2i|Az = 2i'|az$)\nthen\n$F(21,...,n)|\\triangle z = F(21',\u2026\u2026,n')|\\triangle z$\nPareto Efficiency - Brandl & Brandt (2020) version: Let $>= F(\u22651,...,\u2265n)$. We say that F is Pareto Efficient if, whenever every individual prefers p to q (p \u2265i q for all i), then p\u2265 q collectively. If, additionally, there exist individuals i \u2208 P such that they strictly prefer p to q (\u2203i \u2208 P(p >i q)), then p > q.\nAnonymity: The SWF treats all individuals symmetrically (no individual's preferences are given special weight). Note that this is stronger than non-dictatorship.\nMore formally: Let \u03c0be a permutation of the voters P. Then a SWF F satisfies Anonymity if\n$F(\u22651,\u2026\u2026\u2026,\u2265n) = F(\u2265\u03c0(1),\u2026\u2026\u2026, \u2265\u03c0(\u03b7))$\nMaximal Lotteries: It has been proved that under Continuity, Convexity, Symmetry and other technical assumptions, there exist a unique SWF F that satisfies IIA, Anonymity and Pareto Efficiency (Brandl & Brandt, 2020). The Probabilistic Social Choice function p that outputs the first lottery of the ranking returned by SWF F is precisely the Maximal Lottery."}, {"title": "A.4. Relevance of Condorcet and Majority in Text data: Smith sets", "content": "A potential objection to applying Condorcet criteria to LLMs is the sheer scale of the output space. With all possible statements up to a certain length as alternatives, it seems"}, {"title": "A.5. Proof of the main theorem", "content": "In this section we will prove the main theorem of the paper. The notation has been slightly changed to make the proof easier to follow (we substitute a with yi and b with yj).\nTheorem. Let Y be the set of all possible statements with a number of tokens smaller than a predetermined maximum length L. Let $\\pi$ and $\\pi'$ represent two policy LLMs. For two statements $y_i, y_j \\in Y$, let $P(y_i \\succ y_j)$ be the probability that a random individual picked uniformly from society prefers $y_i$ over $y_j$. Let $P(y\u00ec \\sim y_j)$ be the analogous quantity, but for indifference.\nThen, the solution $\\pi^*$ to the following maximin optimization problem:\n$\\max_\\pi \\min_{\\pi'} \\sum_{y_i \\in Y} \\sum_{y_j \\in Y} \\pi(y_i) (P(y_i - y_j) + \\frac{1}{2} P(y_i \\sim y_j)) \\pi'(y_j)$\nis the Maximal Lottery for the Social Choice problem defined by the set of alternatives Y and the population's preferences over these alternatives.\nProof. First, some notation. Define :\n\u2022 n is the amount of elements in the population.\n\u2022 m is the amount of elements in #Y.\n\u2022 N as the matrix that indicates number of people who prefer statement y\u017c to yj $N := (\\#( \\{k : y_i >_k y_j\\})i,j$"}, {"title": "A.6. Random dictatorships and pretrained LLMs", "content": "In this section, we highlight a connection between the behavior of pretrained LLMs and a well-known probabilistic Social Choice function: random dictatorships.\nA random dictatorship selects a single individual from the population at random and implements their top-ranked choice (Gibbard, 1977). Pretrained LLMs, which approximate the probability of the next token based on the distribution of text in their training data, can be seen as implicitly implementing a form of random dictatorship. In this view, the \"voters\" are the users who contributed to the dataset, and their influence is weighted by the volume of text they generated. This suggests that before fine-tuning, LLMs may already reflect an aggregation of individual preferences, albeit in a way that is biased by data distribution rather than designed to satisfy desirable Social Choice properties."}, {"title": "A.7. The multiple definitions of IIA", "content": "Recent results in the literature have pointed out that RLHF satisfies IIA (Xu et al.", "way": "assume that individuals have to choose what they prefer between cats, felines and dogs. In this example, cats and felines are synonyms. Thus, if we add the word feline to our set of possible con"}]}