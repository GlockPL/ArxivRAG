{"title": "Patched RTC: evaluating LLMs for diverse software development tasks", "authors": ["Asankhaya Sharma"], "abstract": "This paper introduces Patched Round-Trip Correctness (Patched RTC), a novel evaluation technique for Large Language Models (LLMs) applied to diverse software development tasks, particularly focusing on \"outer loop\" activities such as bug fixing, code review, and documentation updates. Patched RTC extends the original Round-Trip Correctness method to work with any LLM and downstream task, offering a self-evaluating framework that measures consistency and robustness of model responses without human intervention. The study demonstrates a correlation between Patched RTC scores and task-specific accuracy metrics, presenting it as an alternative to the LLM-as-Judge paradigm for open-domain task evaluation. We implement Patched RTC in an open-source framework called patchwork, allowing for transparent evaluation during inference across various patchflows. Experiments comparing GPT-3.5 and GPT-4 models across different software development tasks reveal that Patched RTC effectively distinguishes model performance and task difficulty. The paper also explores the impact of consistency prompts on improving model accuracy, suggesting that Patched RTC can guide prompt refinement and model selection for complex software development workflows.", "sections": [{"title": "Introduction", "content": "In the past couple of years, LLMs have shown great progress in helping developers with various software development tasks. Typical evaluation of LLMs on coding related tasks focuses mostly on \"first-party\" (or inner development loop) problems like code generation, summarization and unit testing. Most of such tasks happen within the IDE of the developer, often assisted by a GitHub Copilot-like plugin. Relatively little attention has been paid to the \u201csecond-party\u201d (or outer development loop) tasks like bug fixing, code review, refactoring, pull requests, code integration, documentation updates and security patching. We argue that a large majority of software development time is spent in these second-party outer loop activities v/s actual coding. Accelerating software development requires us to automate these tasks and LLMs can be used to do that effectively."}, {"title": "Approach", "content": "The generic implementation of Patched RTC is simple and works as follows:\nSay we have the model M that is used to generate a response R for the user query Q. Now, we wish to evaluate if the response R for the Q is \"correct\u201d.\n1) Q \u2192 [M] \u2192 R\nWe take Q, R and prompt the model to generate an alternate query Q1 such that Q1 is sufficient to recreate the response R.\n2) Q, R \u2192 [M] \u2192 Q1\nNow, we take the new query Q1 and ask the model to generate another response R1.\n3) Q1\u2192 [M] \u2192 R1\nFinally, we check if R and R1 are similar by computing a similarity score (0-1).\n4) R, R1 \u2192 [M] \u2192 score\nIf score > threshold (say 0.8), we say that response R (for the query Q) is correct (w.r.t. RTC).\nStep 4) can also be done without the use of LLMs, if we choose to rely on another similarity metric like cosine similarity (or number of unit tests passed in case of code generation)."}, {"title": "Patchflows", "content": "We define patchflows as workflows that automate outer-loop development tasks like bug fixes, pull request reviews, documentation updates and library upgrades. Our open-source framework patchwork makes it easy for developers to build and run patchflows. One of the challenges with using LLM-assisted patchflows is that it is hard to evaluate the effectiveness of using them in practice.\nPatched RTC can be easily adopted to evaluate patchflows as follows:\nA patch (or commit) has two parts before_code and after_code.\nPatchflows either have 1) a patch as input (e.g. for pull request review) or 2) generate a patch as an output (e.g. bug fixes). For the user prompt Q and response R we can handle these two cases as:\n1) Q, before_code, after_code \u2192 [M] \u2192 R\nApplying Patched RTC, we first generate alternate query Q1\nQ, before_code, after_code, R \u2192 [M] \u2192 Q1\nQ1, before_code, after_code \u2192 [M] \u2192 R1\nR, R1 \u2192 [M] \u2192 score\nIf score > threshold, R is correct (w.r.t RTC).\n2) Q, before_code \u2192 after_code\nSimilar to above, we first generate alternate query Q1\nQ, before_code, after_code \u2192 [M] \u2192 Q1\nQ1, before_code \u2192 after_code1\nSince both after_code and after_code1 are code, we can actually use a stronger measure of similarity. We use exact match as the notion of similarity thus,\nIf exact_match(after_code, after_code1), R is correct (w.r.t. RTC).\nAlmost all patchflows and the corresponding tasks can be classified in either one or the other category. We list some of these tasks in the table below:"}, {"title": "Evaluation", "content": "We first demonstrate the usefulness of Patched RTC across a generic set of diverse tasks by comparing it with the Arena-Hard-Auto benchmark. The below table shows the performance of different models when evaluated with RTC v/s the LLM-as-Judge paradigm as is standard in Arena-Hard-Auto. We run our tests at a high similarity threshold (0.95).\nAs seen from the table below, we notice that is a correlation (with pearson coefficient of 0.81) when compared to the numbers in Arena-Hard-Auto, thus showing that Patched RTC can be used as an evaluation mechanism instead of LLM-as-Judge for generic and diverse tasks. However, there are some differences when compared to Arena-Hard-Auto as well. We have gpt-4-0125-preview as the performing best model on Patched RTC and llama-3-70b-instruct also performs better than gpt-40. These differences arise because Patched RTC measures robustness and consistency by checking the model's ability to invert itself and that may not necessarily be the same as alignment with desired responses (as rated by humans).\nIn total there are 103 tests in the AutoFix dataset which correspond to 106 vulnerabilities (there may be more than 1 instance of a vulnerability in a test). We define the Fix % as the percentage of vulnerabilities that are fixed by the AutoFix patchflow. It is calculated as follows:\nFix % = $\\frac{\\text{(No of vulns before running AutoFix - No of vulns after running AutoFix)}}{\\text{No of vuln before running AutoFix}}$\nBased on the results, we see that the actual fix rate (or accuracy) on the AutoFix task is 52.8 v/s the RTC Pass score which was 83.5. But we see that the fix rate for responses that pass Patched RTC is higher (55.2) v/s those that fail (42.1). This suggests that RTC is able to distinguish more accurate responses by measuring robustness (or consistency). To test this hypothesis we add a very simple one-line consistency prompt (this is very similar to the \"think step-by-step\" prompt that seems to help models do better reasoning) to all the tests and check if this improves the fix rate.\nConsistency prompt:\n\"Respond with clarity, consistency, and precision, maintaining a structured format throughout.\u201d\nThe above prompt was prepended to the system prompt of the request for all the tests and we computed the Fix % of the responses. We saw that this improves the Fix rate by 14.4%."}, {"title": "Discussion", "content": "The use of Patched RTC does incur an additional inference cost of ~3x depending on how the similarity measure is computed. Thus, it is more likely to be useful during testing and evaluation of patchflows and guiding the refinement of the prompts. When used as an active inference technique the improvements in accuracy and robustness need to be balanced with the increased inference cost. Also, the similarity threshold and similarity measure are likely to be dependent on the task and experimenting with a few options before choosing one will likely lead to better results."}, {"title": "What does Patched RTC measure?", "content": "RTC isn't strictly measuring \"correctness\" in the traditional sense, as we don't have a ground truth to compare against. Instead, it's measuring something more nuanced:\n1. Consistency: RTC evaluates how consistently the model can reproduce similar content given a description of its own output.\n2. Robustness: It tests whether the model's output is stable enough that it can be approximately reproduced from a summary of itself.\n3. Coherence: It checks if the model's output contains enough clear, structured information that another instance of the model can grasp and reproduce the key points.\n4. Self-invertibility: It measures how well the model can \"invert\" its own output - turning a response into a query and back into a similar response.\nThe key benefits of Patched RTC are:\n*   It is a different form of evaluation as it tests the ability of the LLM to act as an invertible function.\n*   It does not require the use of Judge (or Jury) LLMs and can be done with a single model.\n*   It is an unsupervised evaluation as we do not rely on any human annotation or checks.\n*   It can be used along with any existing benchmark to see how Patched RTC correlates with them.\n*   It can be used for a wider spectrum of domains that do not have good human annotations.\nOur work on RTC is just a beginning, there are a lot of directions we can explore further:\n*   Using a different model for round-trip response compared to the original model.\n*   Optimize the prompts automatically to generate more consistent responses.\n*   Impact of different oracles on task accuracy when used with Patched RTC."}, {"title": "Conclusions", "content": "In this article, we introduced Patched RTC, a self evaluating framework that works across diverse tasks. Patched RTC measures consistency and robustness of LLM responses and is correlated with oracle based accuracy metrics. It presents an alternative to the LLM-as-Judge paradigm that is currently one of the most common ways to evaluate models for open-domain tasks. We also showed that making prompt changes that increase consistent responses from models do help in improving the overall accuracy of the model."}, {"title": "Usage", "content": "To get access to Patched RTC:\nUse the patched_api_key with our OpenAl compatible endpoint available at patched.codes and just change the base url to https://patchwork.patched.codes/evaluate/v1.\nWhen using this endpoint only those responses that pass Patched RTC will be generated, otherwise the response will be empty. If you want to compare with how the response would have been without Patched RTC, you can send the same request through our usual OpenAl compatible endpoint at https://patchwork.patched.codes/v1."}]}