{"title": "Patched RTC: evaluating LLMs for diverse software development tasks", "authors": ["Asankhaya Sharma"], "abstract": "This paper introduces Patched Round-Trip Correctness (Patched RTC), a novel evaluation\ntechnique for Large Language Models (LLMs) applied to diverse software development tasks,\nparticularly focusing on \"outer loop\" activities such as bug fixing, code review, and\ndocumentation updates. Patched RTC extends the original Round-Trip Correctness method to\nwork with any LLM and downstream task, offering a self-evaluating framework that measures\nconsistency and robustness of model responses without human intervention. The study\ndemonstrates a correlation between Patched RTC scores and task-specific accuracy metrics,\npresenting it as an alternative to the LLM-as-Judge paradigm for open-domain task evaluation.\nWe implement Patched RTC in an open-source framework called patchwork, allowing for\ntransparent evaluation during inference across various patchflows. Experiments comparing\nGPT-3.5 and GPT-4 models across different software development tasks reveal that Patched\nRTC effectively distinguishes model performance and task difficulty. The paper also explores the\nimpact of consistency prompts on improving model accuracy, suggesting that Patched RTC can\nguide prompt refinement and model selection for complex software development workflows.", "sections": [{"title": "Introduction", "content": "In the past couple of years, LLMs have shown great progress in helping developers with various\nsoftware development tasks. Typical evaluation of LLMs on coding related tasks focuses mostly\non \"first-party\" (or inner development loop) problems like code generation, summarization and\nunit testing. Most of such tasks happen within the IDE of the developer, often assisted by a\nGitHub Copilot-like plugin. Relatively little attention has been paid to the \u201csecond-party\u201d (or outer\ndevelopment loop) tasks like bug fixing, code review, refactoring, pull requests, code integration,\ndocumentation updates and security patching. We argue that a large majority of software\ndevelopment time is spent in these second-party outer loop activities v/s actual coding.\nAccelerating software development requires us to automate these tasks and LLMs can be used\nto do that effectively."}, {"title": "Approach", "content": "The generic implementation of Patched RTC is simple and works as follows:\nSay we have the model M that is used to generate a response R for the user query Q. Now, we\nwish to evaluate if the response R for the Q is \"correct\u201d.\n1) Q [M] \u2192 R\nWe take Q, R and prompt the model to generate an alternate query Q1 such that Q1 is sufficient\nto recreate the response R.\n2) Q, R \u2192 [M] \u2192 Q1\nNow, we take the new query Q1 and ask the model to generate another response R1.\n3) Q1\u2192 [M] \u2192 R1\nFinally, we check if R and R1 are similar by computing a similarity score (0-1).\n4) R, R1 \u2192 [M] \u2192 score\nIf score > threshold (say 0.8), we say that response R (for the query Q) is correct (w.r.t. RTC).\nStep 4) can also be done without the use of LLMs, if we choose to rely on another similarity\nmetric like cosine similarity (or number of unit tests passed in case of code generation).\nWe define patchflows as workflows that automate outer-loop development tasks like bug fixes,\npull request reviews, documentation updates and library upgrades. Our open-source framework\npatchwork makes it easy for developers to build and run patchflows. One of the challenges with\nusing LLM-assisted patchflows is that it is hard to evaluate the effectiveness of using them in\npractice.\nPatched RTC can be easily adopted to evaluate patchflows as follows:\nA patch (or commit) has two parts before_code and after_code.\nPatchflows either have 1) a patch as input (e.g. for pull request review) or 2) generate a patch\nas an output (e.g. bug fixes). For the user prompt Q and response R we can handle these two\ncases as:\n1) Q, before_code, after_code \u2192 [M] \u2192 R\nApplying Patched RTC, we first generate alternate query Q1\nQ, before_code, after_code, R \u2192 [M] \u2192 Q1\nQ1, before_code, after_code \u2192 [M] \u2192 R1\nR, R1 \u2192 [M] \u2192 score\nIf score > threshold, R is correct (w.r.t RTC).\n2) Q, before_code \u2192 after_code\nSimilar to above, we first generate alternate query Q1\nQ, before_code, after_code \u2192 [M] \u2192 Q1\nQ1, before_code \u2192 after_code1\nSince both after_code and after_code1 are code, we can actually use a stronger\nmeasure of similarity. We use exact match as the notion of similarity thus,\nIf exact_match(after_code, after_code1), R is correct (w.r.t. RTC).\nAlmost all patchflows and the corresponding tasks can be classified in either one or the other\ncategory."}, {"title": "Evaluation", "content": "We first demonstrate the usefulness of Patched RTC across a generic set of diverse tasks by\ncomparing it with the Arena-Hard-Auto benchmark. The below table shows the performance of\ndifferent models when evaluated with RTC v/s the LLM-as-Judge paradigm as is standard in\nArena-Hard-Auto. We run our tests at a high similarity threshold (0.95).\nAs seen from the table below, we notice that is a correlation (with pearson coefficient of 0.81)\nwhen compared to the numbers in Arena-Hard-Auto, thus showing that Patched RTC can be\nused as an evaluation mechanism instead of LLM-as-Judge for generic and diverse tasks.\nHowever, there are some differences when compared to Arena-Hard-Auto as well. We have\ngpt-4-0125-preview as the performing best model on Patched RTC and llama-3-70b-instruct\nalso performs better than gpt-40. These differences arise because Patched RTC measures\nrobustness and consistency by checking the model's ability to invert itself and that may not\nnecessarily be the same as alignment with desired responses (as rated by humans).\nBased on the results, we see that the actual fix rate (or accuracy) on the AutoFix task is 52.8 v/s\nthe RTC Pass score which was 83.5. But we see that the fix rate for responses that pass\nPatched RTC is higher (55.2) v/s those that fail (42.1). This suggests that RTC is able to\ndistinguish more accurate responses by measuring robustness (or consistency). To test this\nhypothesis we add a very simple one-line consistency prompt (this is very similar to the \"think\nstep-by-step\" prompt that seems to help models do better reasoning) to all the tests and check if\nthis improves the fix rate.\nConsistency prompt:\n\"Respond with clarity, consistency, and precision, maintaining a structured format throughout.\u201d\nThe above prompt was prepended to the system prompt of the request for all the tests and we\ncomputed the Fix % of the responses. We saw that this improves the Fix rate by 14.4%."}, {"title": "Discussion", "content": "The use of Patched RTC does incur an additional inference cost of ~3x depending on how the\nsimilarity measure is computed. Thus, it is more likely to be useful during testing and evaluation\nof patchflows and guiding the refinement of the prompts. When used as an active inference\ntechnique the improvements in accuracy and robustness need to be balanced with the\nincreased inference cost. Also, the similarity threshold and similarity measure are likely to be\ndependent on the task and experimenting with a few options before choosing one will likely lead\nto better results."}, {"title": "What does Patched RTC measure?", "content": "RTC isn't strictly measuring \"correctness\" in the traditional sense, as we don't have a ground\ntruth to compare against. Instead, it's measuring something more nuanced:\n1. Consistency: RTC evaluates how consistently the model can reproduce similar content\ngiven a description of its own output.\n2. Robustness: It tests whether the model's output is stable enough that it can be\napproximately reproduced from a summary of itself.\n3. Coherence: It checks if the model's output contains enough clear, structured information\nthat another instance of the model can grasp and reproduce the key points.\n4. Self-invertibility: It measures how well the model can \"invert\" its own output - turning a\nresponse into a query and back into a similar response.\nThe key benefits of Patched RTC are:\nIt is a different form of evaluation as it tests the ability of the LLM to act as an invertible\nfunction.\nIt does not require the use of Judge (or Jury) LLMs and can be done with a single model.\nIt is an unsupervised evaluation as we do not rely on any human annotation or checks.\nIt can be used along with any existing benchmark to see how Patched RTC correlates\nwith them.\nIt can be used for a wider spectrum of domains that do not have good human\nannotations.\nOur work on RTC is just a beginning, there are a lot of directions we can explore further:\nUsing a different model for round-trip response compared to the original model.\nOptimize the prompts automatically to generate more consistent responses.\nImpact of different oracles on task accuracy when used with Patched RTC."}, {"title": "Conclusions", "content": "In this article, we introduced Patched RTC, a self evaluating framework that works across\ndiverse tasks. Patched RTC measures consistency and robustness of LLM responses and is\ncorrelated with oracle based accuracy metrics. It presents an alternative to the LLM-as-Judge\nparadigm that is currently one of the most common ways to evaluate models for open-domain\ntasks. We also showed that making prompt changes that increase consistent responses from\nmodels do help in improving the overall accuracy of the model."}, {"title": "Usage", "content": "To get access to Patched RTC:\nUse the patched_api_key with our OpenAl compatible endpoint available at patched.codes\nand just change the base url to https://patchwork.patched.codes/evaluate/v1.\nWhen using this endpoint only those responses that pass Patched RTC will be generated,\notherwise the response will be empty. If you want to compare with how the response would have\nbeen without Patched RTC, you can send the same request through our usual OpenAl\ncompatible endpoint at https://patchwork.patched.codes/v1."}]}