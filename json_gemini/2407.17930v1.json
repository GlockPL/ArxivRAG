{"title": "Comparison of different Artificial Neural Networks\nfor Bitcoin price forecasting", "authors": ["Silas Baumann", "Karl A. Busch", "Hamza A. A. Gardi"], "abstract": "This study investigates the impact of varying se-\nquence lengths on the accuracy of predicting cryptocurrency\nreturns using Artificial Neural Networks (ANNs). Utilizing the\nMean Absolute Error (MAE) as a threshold criterion, we aim\nto enhance prediction accuracy by excluding returns that are\nsmaller than this threshold, thus mitigating errors associated\nwith minor returns. The subsequent evaluation focuses on the\naccuracy of predicted returns that exceed this threshold. We\ncompare four sequence lengths-168 hours (7 days), 72 hours (3\ndays), 24 hours, and 12 hours each with a return prediction\ninterval of 2 hours. Our findings reveal the influence of sequence\nlength on prediction accuracy and underscore the potential for\noptimized sequence configurations in financial forecasting models.", "sections": [{"title": "I. INTRODUCTION", "content": "Since Bitcoin was introduced in 2008 as a digital peer-to-\npeer equivalent currency built on blockchain technology [1],\nit emerged as a financial asset that is nowadays mainly used\nfor investments [2].\nThe forecasting of time series data, such as the Bitcoin price,\nis a well-known problem existing in many different domains.\nDepending on the type of data being predicted, the difficulty of\nachieving an accurate result varies. For example, the prediction\nof the next sunrise time is relatively easy, whereas tomor-\nrow's winning lottery numbers cannot be predicted with any\naccuracy. There are many methods for time series forecasting,\nranging from classical mathematical models to approaches\nusing deep neural networks and deep learning [3]. In this\npaper, the data-driven approach of forecasting by applying\ndifferent types of Artificial Neural Network (ANN) is used.\nWe try to predict the future price movement of Bitcoin\njust by reviewing the past market data and compare the\nperformance of the different ANNs on this task based on their\npredictions.\nThe paper addresses the question of whether and how well\ndifferent types of ANNs can predict the future Bitcoin price\nreturns based on current market data."}, {"title": "II. RELATED WORK", "content": "In this chapter, we examine fundamental concepts of time\nseries analysis as well as the use of PyTorch for implementing\ntime series models. We draw insights from two sources:\n\"Forecasting: Principles and Practice\" by Rob J. Hyndman\nand George Athanasopoulos [3], as well as a tutorial on time\nseries analysis from TensorFlow [4]."}, {"title": "A. Time Series Analysis", "content": "Time series analysis involves the examination and prediction\nof data collected over time. Specific patterns and trends in\nthe data are identified to forecast future developments. This\nform of analysis finds application in various fields such as\neconomics, finance, climatology, and healthcare. Time Series\nModels and Networks\nTime series models, including neural networks, are used\nfor modeling time series data. These models capture complex\ndependencies and patterns in the data to make accurate pre-\ndictions.Neural networks such as recurrent neural networks\n(RNNs) and Long Short-Term Memory (LSTM) networks are\nparticularly suitable for considering temporal dependencies in\nthe data and are therefore widely used in time series analysis."}, {"title": "B. Stationarity", "content": "Stationarity is a fundamental concept in time series\nanalysis, indicating, that a time series exhibits constant\nstatistical properties over time, such as mean, variance, and\nautocorrelation. This assumption simplifies the modeling\nand forecasting of time series data. Analyzing stationarity"}, {"title": "1) Augmented Dickey-Fuller Test", "content": "The Augmented Dickey-\nFuller (ADF) test is a widely used statistical test to check the\nstationarity of a time series. The null hypothesis of the test\nstates, that the time series have a unit root, which means it\nis not stationary. Rejecting the null hypothesis suggests, that\nthe time series is stationary.\nThe ADF test works by estimating the following regression:\n$\\Delta y_t = a + \\beta t + \\gamma y_{t-1} + \\delta_1 \\Delta y_{t-1} +\\\\\n\\delta_2 \\Delta y_{t-2} + \\cdots + \\delta_p \\Delta y_{t-p} + \\varepsilon_t$ (1)\nwhere:\nyt is the time series.\n$\\Delta$ is the difference operator.\na is a constant.\n$\\beta$t represents a time trend.\n$\\gamma$ is the coefficient of yt-1, crucial for testing stationarity.\n$\\delta_i$ are the coefficients of the lagged differences.\n$\\varepsilon_t$ is the error term.\nThe critical aspect of the ADF test is the t-statistic for the\ncoefficient $\\gamma$. If this statistic is sufficiently negative, the null\nhypothesis of a unit root can be rejected, indicating that the\ntime series is stationary.\nThe key formulas of the ADF test can be summarized as\nfollows:"}, {"title": "1) Difference Operator", "content": "$\\Delta y_t = y_t - y_{t-1}$ (2)\nThis formula calculates the first difference of the time\nseries, an essential step to obtain stationary data."}, {"title": "2) ADF Regression", "content": "$\\Delta y_t = a + \\beta t + \\gamma y_{t-1} + \\delta_1 \\Delta y_{t-1} +\\\\\n\\delta_2 \\Delta y_{t-2} + \\cdots + \\delta_p \\Delta y_{t-p} + \\varepsilon_t$ (3)\nThis regression includes all relevant terms to model the\ndynamics of the time series and test for a unit root.\nThe ADF test is particularly useful for identifying the\npresence of trends or non-stationarity in a time series and\nperforming suitable transformations, such as differencing, to\nachieve stationarity before further analysis or modeling. By\nanalyzing the test statistics, it can be determined whether to\nreject the null hypothesis and thus consider the time series as\nstationary."}, {"title": "C. PyTorch for Time Series Analysis", "content": "PyTorch is a powerful deep learning library known for its\ndynamic computation graph and flexibility. Compared to other\nframeworks like Keras or TensorFlow, PyTorch offers several\nadvantages:\n\u2022 Dynamic Computation Graph: PyTorch constructs\ncomputation graphs dynamically, which facilitates mod-\neling complex structures and simplifies debugging.\nFlexibility: PyTorch provides high flexibility for imple-\nmenting custom models and layers, which is particularly\nbeneficial in research.\nIntegration with Python: The seamless integration with\nPython makes it easier to use PyTorch with other scien-\ntific libraries and tools.\nIn time series analysis, PyTorch offers robust tools for\nimplementing and training time series models, especially neu-\nral networks like RNNs and LSTMs, which are specifically\ndesigned for modeling temporal dependencies."}, {"title": "III. PROPOSED METHOD", "content": null}, {"title": "A. Data Acquisition", "content": "The BTC/USDT trading pair is selected for this study for\nseveral reasons. First, Bitcoin (BTC) is the most widespread\nand well-known cryptocurrency, which has a high trading\nvolume and significant liquidity. These characteristics make\nBTC an ideal candidate for quantitative analyses and\nprediction models, as a larger amount of data and more\nfrequent trading activities can increase the accuracy of the\nmodels.\nSecond, USDT (Tether) is a so-called stablecoin, whose value\nis pegged to the US dollar. This stability reduces volatility\ncompared to other cryptocurrencies, simplifying the analysis\nand prediction of price movements. Thus, the BTC/USDT\npair combines the advantages of a highly liquid asset with the\nstability of a stablecoin, leading to more robust and reliable\nmodels.\nFurthermore, the Binance exchange provides comprehensive\nhistorical data for BTC/USDT, allowing access to a rich\ndataset. This data availability is crucial for the development\nand training of accurate prediction models. Overall, the\nBTC/USDT trading pair offers an ideal combination of\nliquidity, data availability, and relative stability, making it an\nexcellent choice for the investigation in this study.\nThis section details the process of data mining required\nto prepare candlestick data for modeling and predicting the\nreturns of a cryptocurrency pair. The starting point is the\ngeneration and provision of candlestick data via the Binance\nAPI, specifically for the trading pair BTC/USDT (Bitcoin/US\nDollar Tether). This API offers a comprehensive interface\nfor querying historical market data, which includes essential\ninformation such as time, opening price, highest price, lowest\nprice, closing price, and trading volume."}, {"title": "B. Feature Engineering", "content": "In feature engineering, relevant features are extracted and\nprocessed from the candlestick data to improve the models'\nprediction accuracy. The essential steps of feature engineering\nin this work include data augmentation, prediction of returns,\nstationarity check, scaling, and the generation of sequences\nand labels."}, {"title": "C. Data Augmentation", "content": "To enrich the information in the candlestick data, various\ntechnical indicators are calculated and integrated into the\ndataset. The key indicators include:\nEMA12: The Exponential Moving Average with a period\nof 12 days. The EMA smooths price data by giving more\nweight to recent data points, thus better identifying trends.\nMACDline and MACDsignal: The Moving Average\nConvergence Divergence (MACD) is a trend-following\nindicator. The MACDline is calculated as the difference\nbetween the 12-period EMA and the 26-period EMA. The\nMACDsignal is a 9-period EMA of the MACDline and\nis used to identify buy and sell signals.\n%K and %D: These values are part of the Stochastic\nOscillator. %K measures the current price level relative\nto the price range over a specified period (typically 14),\nand %D is a moving average of %K.\nRSI: The Relative Strength Index measures the speed\nand change of price movements and is used to identify\noverbought or oversold conditions.\nSMA12: The Simple Moving Average over 12 periods,\nwhich is used to smooth price data and identify trends.\nThese indicators are calculated based on the closing prices\nof the candlestick data and added to the dataset to obtain a\ncomprehensive feature spectrum."}, {"title": "1) Returns", "content": "The primary task is to predict the returns in\nfuture time steps T. The return is defined as the percentage\nchange in the closing price between two consecutive time\npoints:\n$Return_t = \\frac{Close_t - Close_{t-1}}{Close_{t-1}}$ (4)\nThis calculation quantifies the relative price change, which\nis essential for predicting future price movements. The neural\nnetwork are fed with these returns to predict the return in\nn future time steps. The input returns are based on hourly\ncandlesticks, allowing for continuous and detailed analysis of\nprice movements."}, {"title": "2) Stationarity and Scaling", "content": "For many time series models,\nit is important that the data is stationary, meaning their\nstatistical properties, such as mean and variance, remain\nconstant over time. To check the stationarity of the data, the\nAugmented Dickey-Fuller (ADF) test is used. The ADF test\nhelps determine if a time series has a unit root, which is an\nindicator of non-stationarity. If the data are not stationary,\nthey are transformed through differencing until stationarity\nis achieved. This transformation is necessary to ensure the\nstability of statistical properties.\nBefore scaling the data, it is important to ensure they are\nstationary. Scaling before checking for stationarity can result\nin mean and variance changes over time, distorting the scaling\nresult. A MaxAbsScaler is used to scale the data to the range\n[-1,1] without altering the original structure of the data. This\nis particularly useful for models, that are sensitive to the\nmagnitude of the input data."}, {"title": "3) Sequence and Label Generation", "content": "The final step in\nfeature engineering is generating sequences and corresponding\nlabels. From the prepared data, we create fixed-length\nsequences that serve as input for the models. Each sequence\nconsists of a set number of candlestick data points and the\ncalculated technical indicators. The label for each sequence is\nthe return in the subsequent time step T. This method ensures,\nthat the models learn to predict future price movements based\non a history of data.\nWhen forming the labels, we ensure that the data at the\noutput remains unscaled. This allows the neural network to\nfully utilize its regression properties and deliver more precise\npredictions. Unscaled labels ensure that the model learns\nand predicts the actual values directly, which is crucial for\nforecasting financial metrics such as returns. This approach\nimproves the accuracy of the predictions and makes them\neasier to interpret.\nAdditionally, we implement the ability to adjust the length\nof the lookback sequences (the number of past time points\nconsidered for prediction) and the lookforward times (the\nnumber of future time steps to be predicted) individually. This\nallows us to find the optimal input sequence length and the\nappropriate prediction time for different models and datasets.\nThis flexibility can further enhance prediction performance\nand better cater to the specific requirements of each use case.\nThese comprehensive steps in feature engineering ensure,\nthat the data is optimally prepared and enriched, forming the\nfoundation for subsequent modeling and prediction."}, {"title": "D. Models", "content": "The basic building blocks of ANNs are neurons (also\nreferred to as nodes, cells or units). A neuron is the repre-\nsentation of a function, that takes an input and computes a\ncorresponding output. Inside of an ANN sets neurons of the\nsame type are grouped into layers. There are three types of\nlayers an ANN is built of\n\u2022 Input Layer: Sets the number of input features\n\u2022 Hidden Layers: An arbitrary number of different layers,\nthat are not directly visible from the outside of the net\n\u2022 Output Layers: Sets the number of output features\nEach layer consists of a specified number of nodes and each\nnode has a set of weighted inputs and an output [5].\nSince the aim of this paper is to compare the performance\nof the different models, the theoretical principles of how they\nwork, are only dealt with superficially. For more detailed\nexplanations and information, please refer to the sources\nprovided.\nThe models used in this paper, are a Multiple Layer Percep-\ntron (MLP) ANN, a Convolutional Neural Network (CNN), a\nRecurrent Neural Network (RNN) and a special form of RNNs\nthe Long Short-Term Memory (LSTM). The number of inputs\nand the hidden layers vary and are adapted to each model. We\nwant to predict a single value, the price change of bitcoin in\nT time steps, thus the number of outputs of all nets is one.\nTherefore last layer of all models consist of one node, that\njust linearly combines all outputs of the previous layer."}, {"title": "1) MLP", "content": "MLPs are the most commonly used ANN because\nof their simplicity. A MLP consists just of a set of Fully Con-\nnected Layers (FCLs), which is a layer, where each node of\na layer takes every output of the previous layer as a weighted\ninput. The output of the nodes of a FCL is recommended to\nbe nonlinear [6]. The MLP model considered in this paper,\nis built of two fully connected layers, each consisting of 64\nnodes. To make the MLP model capable of processing time\nseries data, the feature sequences are flattened, that means, that\nthe sequence of feature vectors are concatenated to one new\nvector of the size features times sequence length. Consequently\nthe input size of the MLP is calculated dynamically dependent\non the sequence length. The activation function for all nodes\nexcept the last one is the ReLU function."}, {"title": "2) CNN", "content": "A CNN is a ANN, that executes one or more\nconvolution operations on the input data, in order to extract\nnew features. Typically these kinds of nets are used for\nimage processing, where a two dimensional convolution is\nperformed. The main components of a CNN are the name\ngiving convolutional and pooling layers. Convolutional layers\nperform a mathematical convolution by shifting a filter over\nthe input data. Pooling layers devide the input data, here the\ninput sequence into subsequences, where it chooses a single\nvalue for every feature in all subsequences and drops all other\nvalues [7]. In this case, the convolution is one dimensional\nand performed over the time axis, since the data is a time\nseries. The CNN considered in this study consists of one single\nconvolutional layer with a filter size of three, followed by a\nmax pooling layer, that divides the sequence in sub sequences\nof the given size of also three and picks the highest value\nof every feature. The result of the pooling gets flattened and\ninserted into a FCL with 128 neurons, followed by one with\n32 neurons. The activation function for all the nodes of the\nFCL is the ReLU function."}, {"title": "3) RNN", "content": "RNNs are specifically designed to process sequen-\ntial data such as natural language or time series. Therefore\nrecurrent neuron has a hidden state ht. At each time step t the\nrecurrent node processes the input and its hidden state of the\nprevious time step ht-1 as input and updates the hidden state\n[8]. The RNN considered in this paper consists of one layer\nof 32 recurrent nodes, as well as one FCL of 64 nodes. The\nactivation function of both the recurrent layer and the FCL is\nthe ReLU function. The problem about simple RNNs is the so-\ncalled vanishing or exploding gradient [9]. This problem may\noccur when training over a large sequence of data for learning\nlong time dependencies. The back propagation through time\nrequires a repeated multiplication of the gradient by itself,\nwhich leads to the problem mentioned, if the gradient is very\nbig or very small."}, {"title": "4) LSTM", "content": "A LSTM is a special type of RNN which faces\nthe previously mentioned problems. The functionality and how\nthese problems are avoided is described in [10].\nThe architecture of the LSTM used in this work is the\nsame as the one of the RNN, with the only difference, that\nthe recurrent layer is exchanged with a LSTM one."}, {"title": "E. Training", "content": "With the term \"training\" of an ANN, the process of adjust-\ning the parameters of the models (e.g. the different weights of\na FCL) to fit the given problem. In general this is a classic\noptimization problem, therefore a loss function, which is to\nbe optimized, needs to be formulated, an optimizing algorithm\nhas to be chosen and a way of adjusting the parameters in the\nrequired way is needed.\nThe algorithm used for training is the so called \"backprop-\nagation\u201d. This is a efficient algorithm used to train ANNs for"}, {"title": "1) Calculation of accuracy", "content": "The models are trained to\nsupport decision making for buying or selling Bitcoin. For\nthis decision a rough estimation of the price movement is\nconsidered as sufficient. More critical are predictions, which\npredict a change in the opposite direction than the actual\nprice change. If there is an deviation between the predicted\nreturn and the actual return, but the direction of price change\nis correct, the prediction can still be assumed acceptable. In\ncontrast, a prediction which indicates a price change in the\nwrong direction may be critical. To track the correct direction\nof the price change, we introduce a sign accuracy metric,\nwhich is shown in (5). sign is the signum function, that returns\n-1 for negative and +1 for positive values. acc is then the ratio\nof correct predicted price change direction to total number of\npredictions.\n$acc = \\frac{1}{n} \\sum_{i=1}^{n} \\frac{|sign(y_i) + sign(\\hat{y}_i)|}{2}$ (5)"}, {"title": "2) Loss Function", "content": "The purpose of the model is to predict\nthe price change of the Bitcoin closing prices, which is a\nsupervised learning regression task. For regression, ANNs\ntypically a Mean Squared Error (MSE) is chosen as loss\nfunction. Which is, as the name proposes, the mean of the\nsquared difference between the actual and the predicted value\n(see (6)). The total number of values is given by n, $y_i$ are the\nactual and $\\hat{y}$ the corresponding predicted values.\n$mse = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y})^2$ (6)\nIn order to achieve a high accuracy in price movement\ndirection and absolute change prediction, we combine the MSE\nand the sign accuracy metric to get a suited loss function for\nthe optimization, the mse is scaled by the inverse of the acc,\nin this way both metrics have a similar impact on the loss (see\n(7)). The parameter $\\varepsilon$ is a small value (chosen as $10^{-6}$) added\nto the denominator to prevent division by zero and ensure\nnumerical stability.\n$loss = \\frac{mse}{acc + \\varepsilon}$ (7)"}, {"title": "3) Optimizer", "content": "The optimizer chosen to minimize the loss\nfunction is the so-called \"Adam\" optimizer. It combines two\nextensions of the Stochastic Gradient Descend (SGD) and\nstands for Adaptive Moment Estimation [12]."}, {"title": "4) Parameters", "content": "There are different parameters which may\nhave an impact on the training result of a model. For most\nof these parameters, there are standard values, which are\npreset by the libraries offering tools for training, ANNs such\nas PyTorch in this case. Most of these parameters were not\nadjusted for this work because they have been preset values\nthat have shown to be best practice. Nevertheless, there were\ntwo parameters, that were adjusted in a process of trial and\nerror to gather the best performing outcomes.\nAn essential parameter for the optimizer is the so-called\nlearning rate or step size. The optimizer determines a modified\ngradient of the loss function in dependency of the model\nparameters. This gradient is then multiplied by the negative\nvalue of the learning rate, to determine how each weight\nshould be adjusted. A large learning rate may result in faster\nconvergence, but may also lead to an overshoot, which causes\na slower convergence or no convergence at all. As the name\nsupposes, it is the base for the optimizer to calculate how \"far\"it goes in the direction it determined, that would minimize\nthe loss function the most. The second parameter, that was\nadjusted, is the number of training epochs. In one epoch, the\nmodel is trained once on the entire training dataset. The results\nof an epoch are validated by evaluating the performance of\nthe model on a validation dataset. One epoch means, that\nthe model is trained once for the completed giving training\ndataset and evaluating the training results on a validation\ndataset. The number of epochs and the learning rate depend\non each other, because a low learning rate leads to an expected\nlower convergence rate. This means, that the number of epochs\nshould be increased and vice versa.\nIn a process of trial and error it was found, that a low\nlearning rate of $10^{-5}$ combined with a number of epochs about\n30 produces the best performances."}, {"title": "IV. COMPARISON OF MODELS", "content": null}, {"title": "A. Used Data", "content": "Initially, 5-minute candlesticks are used to make predictions.\nHowever, it becomes apparent, that networks work very\nslowly with this data because significantly more data have\nto be processed. Additionally, the results of the 5-minute\ncandlesticks are significantly worse than those of the 1-hour\ncandlesticks. This can be attributed to the fact, that the other\nfeatures of the input tensor, consisting of technical indicators,\nare calculated on larger time units, which means, that these\nfeatures do not contribute relevant information for prediction.\nFor this reason, a dataset with hourly candlesticks are\nselected for further analysis and predictions. This is shown in\nFigure 1, where the hourly BTCUSDT closing price data is\ndepicted. A total of data from the last 400 days is included.\nThe data split is done in a 70/20/10 ratio for training,\nvalidation, and test"}, {"title": "V. CONCLUSION", "content": "The investigation into the prediction accuracy of various\nsequence lengths for forecasting cryptocurrency returns shows\nthat none of the tested model structures delivered satisfactory\nprediction performance. Despite the application of the MAE\nthreshold criterion, which aims to improve the accuracy of\nthe predictions by excluding small, error-prone returns, the\nresults remained unsatisfactory.\nIt is found that the prediction accuracy of most models\nis only slightly above random chance. This suggests that\nthe models have difficulties in recognizing and predicting\nreliable trends in the data. In particular, the limited number\nof features, which are exclusively extracted from candlestick\ndata, seem to be a significant limiting factor. These features\nmay not capture the complexity and deeper patterns of the\nmarket necessary for more accurate predictions.\nAdditionally, the analysis shows signs of overfitting,\nparticularly with the LSTM models, indicating that the\nmodels might have learned the training data too well without\ncapturing generalizable patterns that could be applied to\nnew, unseen data. This problem might be alleviated by\nimplementing regularization techniques such as L1/L2\nregularization or dropout, but the current results suggest that\nthis alone is insufficient.\nAnother issue is data leakage, which can compromise the\nvalidity of the model results. Careful review and cleansing\nof data sources are necessary to ensure that no future\ninformation leaks into the training data, artificially boosting\nmodel performance.\nIn conclusion, predictions based solely on candlestick data\nare not optimal. To improve prediction accuracy, additional\ndata sources and features should be included, such as\nnumber of trades, technical indicators, and fundamental\ndata. Furthermore, different network architectures such as\ntransformer models or hybrid approaches combining various\nneural networks could be tested to enhance prediction\nperformance.\nFuture work should therefore focus on integrating a broader\nrange of input data and employing more advanced modeling\napproaches to better capture the complexity of financial mar-\nkets and enable more accurate predictions."}]}