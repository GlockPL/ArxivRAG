{"title": "Human-Centric Foundation Models: Perception, Generation and Agentic Modeling", "authors": ["Shixiang Tang", "Yizhou Wang", "Lu Chen", "Yuan Wang", "Sida Peng", "Dan Xu", "Wanli Ouyang"], "abstract": "Human understanding and generation are critical for modeling digital humans and humanoid embodiments. Recently, Human-centric Foundation Models (HcFMs)-inspired by the success of generalist models such as large language and vision models-have emerged to unify diverse human-centric tasks into a single framework, surpassing traditional task-specific approaches. In this survey, we present a comprehensive overview of HcFMs by proposing a taxonomy that categorizes current approaches into four groups: (1) Human-centric Perception Foundation Models that capture fine-grained features for multi-modal 2D and 3D understanding; (2) Human-centric AIGC Foundation Models that generate high-fidelity, diverse human-related content; (3) Unified Perception and Generation Models that integrate these capabilities to enhance both human understanding and synthesis; and (4) Human-centric Agentic Foundation Models that extend beyond perception and generation to learn human-like intelligence and interactive behaviors for humanoid embodied tasks. We review state-of-the-art techniques, discuss emerging challenges and future research directions. This survey aims to serve as a roadmap for researchers and practitioners working towards more robust, versatile, and intelligent digital human and embodiments modeling. Website is available.", "sections": [{"title": "1 Introduction", "content": "Recent years have witnessed remarkable strides towards understanding human appearance, emotions, identities, actions, intentions and generating photorealistic humans in 2D and 3D. The success of these methods can be attributed to the robust estimation of identification [He et al., 2024; Li et al., 2024a], 2D keypoints [Wang et al., 2023; Yuan et al., 2024], fine-grained body-part segmentation [Tang et al., 2023; Chen et al., 2023a], depth [Khirodkar et al., 2024], textual descriptions [Chen et al., 2024], and human meshes [Cai et al., 2024], as well as the powerful human-centric deep learning frameworks, e.g., vision transformers [Jin et al., 2024; Wang et al., 2023; Huang et al., 2024a] and diffusion models [Ju et al., 2023; Li et al., 2024b; Lin et al., 2025]. Despite progress in every individual task, robust and accurate understanding and generating photorealistic and even intelligent digital humans requires to deeply understand human as a holistic and complex system at the intersection of diverse human-centric tasks related to appearance, identity, motion and intentions. Moreover, most existing human-centric pipelines are task-specific for better performances, leading to huge costs in representation/network design, pretraining, parameter-tuning, and annotations. Therefore, the recent human-centric learning community appeals for a unified framework [Ci et al., 2023; Wang et al., 2023; Chen et al., 2024; Huang et al., 2024a] to unlock systematic understanding and a wide range of human-centric applications for everybody.\nInspired by rapid advancements of general foundation models, e.g., large language models (LLMs), large vision models (LVMs) and text-to-image generative models, and their presents of a paradigm shift from end-to-end learning of task-specific models to generalist models, a recent trend is to develop Human-centric Foundation Models (HcFM) that satisfy three criteria, namely generalization, broad applicability, and high fidelity. Generalization ensures robustness to unseen conditions, enabling the model to perform consistently across varied environments. Broad applicability indicates the versatility of the model, making it suitable for a wide range of tasks with minimal modifications, or even without modifications. High fidelity denotes the ability of the model to produce precise, high-resolution outputs essential for faithful human generation tasks such as 2D to 3D lifting. Recent notable works towards such human-centric foundation models include SOLIDER [Chen et al., 2023a], PATH [Tang et al., 2023], UniHCP [Ci et al., 2023], Sapiens [Khirodkar et al., 2024], MotionGPT [Jiang et al., 2023; Zhang et al., 2024a], ChatHuman [Lin et al., 2024], etc.\nIn light of the rapid developments and emerging challenges of Human-centric foundation models, we present a comprehensive survey of this field to help the community keep track of its progress. Specifically, we introduce a taxonomy that categorizes existing works into four groups according to their supported downstream tasks: Human-centric perception foundation models, Human-centric AIGC foundation models, Human-centric unified perception and generation foundation models, and Human-centric agentic foundation models. Human-centric perception foundation models modify the existing unsupervised and multitask supervised pretraining framework"}, {"title": "2 Taxonomy", "content": "The objective of the taxonomy is to group Human-centric foundation models with the similar supported tasks into the same category. Specifically, we classify existing human-centric foundation models into four categories, i.e., Human-centric perception foundation models, Human-centric AIGC foundation models, Human-centric unified perception and generation foundation models, and Human-centric agentic models, which can be further summarized in difference learning frameworks.\nWe briefly introduce the four categories as follows:\n(1) Human-centric perception foundation models learn from large-scale and multi-modal human-centric data and support major perceptive tasks, including person re-identification, human parsing, pose estimation, human mesh recovery and skeleton-based action recognition. The fundamental idea is to leverage structures of human bodies or diverse annotations to learn the fine-grained and semantic human-centric representations. Based on the learning frameworks, human-centric perception foundation models can be further categorized into unsupervised learning and multitask supervised learning.\n(2) Human-centric AIGC foundation models are designed to create content\u2014such as images, videos, or avatars that centers on human. These models are trained on extensive human-centric data to produce realistic and diverse portrayals of individuals, whose objective is to produce content that accurately reflects fine-grained human appearances, behaviors, and interactions. Based on the training paradigm, these models can be broadly classified into models trained with unsupervised learning and multi-modal supervised learning.\n(3) Human-centric unified perception and generation foundation models can support perception and generation tasks closely correlated. The fundamental idea is to view human-centric cues other than texts as foreign languages and append them to large language models (LLMs) as multi-modal large language models (MLLM) for understanding and generation.\n(4) Human-centric agentic foundation models learn human intelligence and support human-centric embodied AI tasks,"}, {"title": "3 Human-centric Perception Foundation Models", "content": "The human-centric perception foundation model demonstrates that compact ahuman-centric representations can be learned from multiple human-centric task and efficiently adapted to a broad range of 2D and 3D multi-modal perception tasks. Based on whether to leverage human-centric annotations, we cover two paradigms of human-centric perception foundation models: unsupervised learning and supervised learning."}, {"title": "3.1 Unsupervised learning methods", "content": "Human-centric unsupervised foundation models were proposed to mitigate dependency on the sensitive annotations, which mainly follow the pretraining-finetuning paradigm. During pretraining, without labels, they used the inherent priors of human body structure to learn versatile and representative human-centric features in the encoder. When adapted to downstream tasks, the pretrained encoder and the task head will be parameter-efficiently and fully finetuned using labeled data.\nContrastive learning methods leverage human priors to align features derived from encoders using tailored contrastive losses, as shown in Fig. 2(a). Considering the multi-modal nature of human data (e.g., RGB, depth, 2D keypoints), instead of commonly used momentum encoders, multiple encoders were used to process inputs with different modalities. Human priors were included to guide multi-modal contrastive learning for general human-centric representations. HCMoCo [Hong et al., 2022] employed multiple encoders to exploit multi-modal human body consistency through a hierarchical contrastive learning framework. Based on it, PBoP [Meng et al., 2024] introduced an additional encoder for generated latent part pair images. The extracted features could then serve as anchors to guide the multi-modal contrasting learning process. However, the acquisition of multi-modal data is not always straightforward. When only images were available, integrating an additional loss with human prior knowledge was another effective solution. For example, SOLIDER [Chen et al., 2023a] proposed an additional semantic classification loss to import semantic information into the learned features. LiftedCL [Chen et al., 2023b] introduced an adversarial loss to supervise the lifted 3D skeletons, explicitly inserting 3D human structure information for human-centric pretraining.\nMask image modeling methods implicitly learn human knowledge by reconstructing masked inputs (see Fig. 2(b)) based on the prior knowledge of body structures. HAP [Yuan et al., 2024] utilized the 2D keypoints to guide the mask sampling process during mask image modeling, encouraging the model to concentrate on body structure information. To introduce 3D human prior, [Armando et al., 2024] proposed to reconstruct masked pedestrian images from cross-view and cross-pose pairs. Thanks to human priors, human-centric unsupervised foundation models show superior performance than the ImageNet pretraining methods on human-centric perception tasks, especially on low-data regimes."}, {"title": "3.2 Multitask supervised learning", "content": "When labled data is sufficient, multitask supervised learning can exploit the internal relationship among data, emerging as a straightforward and effective paradigm for constructing human-centric perception foundation models. Some recent works learned the shared information among different human-centric datasets to benefit specific human-centric tasks, e.g., human shape estimation [Cai et al., 2024], pedestrian detection [Zhang et al., 2024b], re-identification [He et al., 2024; Li et al., 2024a]. By constructing a unified framework applicable to related subtasks and training them concurrently, these approaches have outperformed specialist models. However, a notable limitation of these methods is their inability to learn and execute other perception tasks, which restricts the potential scope of the application. In response to the challenge, recent advances in multiple human-centric tasks co-training showed a new direction. These developments demonstrate that human-centric perception foundation models can exploit inter-task homogeneity to enhance overall performance.\nMultitask supervised pretraining methods centered on utilizing multiple distinct supervisions to force the encoder to learn general human-centric representation (see Fig. 2(c)). To handle task conflicts brought by supervised learning from diverse labels, PATH [Tang et al., 2023] leveraged task-specific projectors along with the hierarchical weight-sharing strategy, enforcing the encoder to acquire general representations for downstream human-centric tasks.\nUnified modeling methods have emerged as the mainstream in human-centric perception foundation models to further mitigate the resource-intensive full finetuning process for downstream tasks. As depicted in Fig. 2(d), these methods followed the unified encoder-decoder framework with dynamic queries. UniHCP [Ci et al., 2023], as a first attempt, adopted a task-guided interpreter to unify task heads and task-specific queries, handling five human-centric tasks. HQNet [Jin et al., 2024] focused on instance-level features for individual persons and proposed Human Queries to learn unified all-in-one query representations, providing a single-stage method to tackle multiple distinctive human-centric tasks. While these works mainly concentrate on 2D human-centric tasks, Hulk [Wang et al., 2023] extended the scope to simultaneously address 2D vision, 3D vision, vision-language and skeleton-based human-centric tasks. To tackle these tasks, Hulk categorized the input and output formats into four modalities and developed modality-specific (de-)tokenizers with modality indicator queries, unifying all tasks into modality translation tasks. Considering human-computer interaction, RefHCM [Huang et al., 2024a] converted multi-modal data into semantic tokens, unifying various perception tasks as referring tasks."}, {"title": "4 Human-centric AIGC Foundation Models", "content": "Human-centric AIGC foundation models are generative models specifically trained or fine-tuned on large-scale human datasets to create content focused on human elements, such as images, videos, and 3D avatars. By prioritizing the generation fidelity of human attributes, these models serve as adaptable bases for downstream tasks like image editing, virtual try-on, 3D generation, and character animation. In this section, we categorize these models based on unsupervised learning and multi-modal supervised learning approaches."}, {"title": "4.1 Unsupervised Learning", "content": "Unsupervised learning plays a crucial role in early human-centric generative models, with GAN-based methods leading the progress in realistic human image and avatar generation. Specifically, there are two main frameworks: 2D GANs with Style Modulation and 3D-Aware GANs with Neural Renderer.\nGANs with Style Modulation leverage an intermediate, disentangled latent space to exert fine-grained control over the image synthesis process by first mapping a random noise to a style vector and then using learned affine transformations to inject style vectors at various layers of the generator, as shown in Fig.3 (a). This hierarchical modulation allows fine-grained control of visual attributes at different scales. For instance, StyleGAN-Human [Fu et al., 2022] trained a series of unconditional models with this framework and showed that scaling data size, balancing data distribution, and aligning human body could significantly enhance generation performance, thereby establishing robust foundational models for conditional applications. UnitedHuman [Fu et al., 2023] employed a multi-source spatial transformer to align multi-source data, including face, hand, partial-body, and whole-body images, into a unified space for more comprehensive human modeling, and designed a continuous generator to synthesize coherent high-relsolution images with enhanced details. These models served as versatile, pre-trained foundations for a variety of downstream applications. Their well-structured latent space enabled (1) intuitive drag manipulation of image attributes [Pan et al., 2023] by optimizing latent codes based on point movement supervision, (2) 3D generation [Xiong et al., 2023] with reconstruction priors to produce disentangled geometry and texture code, and (3) virtual try-on [Yoshikawa et al., 2023] by aligning the latent code with garment features.\nGANs with Neural Renderer integrate 3D representations and a neural renderer into the former framework to enforce 3D geometric consistency during image synthesis, (Fig.3(b)). By guiding the generator with rendering results (e.g., low-resolution image, depth, or surface normals), these models produce outputs that better reflect realistic spatial structures, making them especially to create high-fidelity 3D avatars. For example, SofGAN [Chen et al., 2022] decoupled the 3D representation space into geometry and texture subspaces, providing a robust foundation for independent control over camera pose, facial structure, and attribute texture in various human portraits. AniPortraitGAN [Wu et al., 2023] integrated human priors into its framework by learning pose and facial deformations using the SMPL model and 3D Morphable Model (3DMM). Trained on large-scale facial image collections, its generator and renderer could serve as the basis for tasks that require high-resolution controllable images with detailed 3D geometry. Similarly, AG3D [Dong et al., 2023] extended this framework to train on large-scale full-body images by incorporating an additional deformer for learning pose-dependent effects and a normal estimator for geometry supervision, resulting in high-quality 3D human avatars. These models' 3D-aware generation ability unlocked many downstream tasks, including (1) free-viewpoint video synthesis by rendering from arbitrary camera trajectories, (2) pose retargeting by integrating learnable pose-dependent deformations, and (3) single-view 3D reconstruction with GAN inversion."}, {"title": "4.2 Multi-modal Supervised Learning", "content": "Recent advances in diffusion models have spurred the development of multi-modal supervised learning approaches that leverage large-scale paired datasets, such as text-image or video-pose alignments, to achieve precise control over the generation process. With Conditional Latent Diffusion Models and Spatial-Temporal Diffusion Transformer Architecture, recent works have significantly improved the quality, consistency, and diversity of synthetic human images and videos.\nConditional Latent Diffusion Models extend standard diffusion models by operating in a compact latent space and incorporating external conditions, such as text, pose, or segmentation maps, to guide the generation process (Fig.3 (c)). By first encoding inputs into a lower-dimensional latent representation, diffusion is applied to gradually denoise the output while preserving the provided conditions. This approach enhances both efficiency and controllability, making it a general framework for tasks like human image synthesis, pose-guided generation, and multi-modal content creation. Specifically, HumanSD [Ju et al., 2023] proposed a skeleton-guided diffusion model with a heatmap-guided denoising loss. Trained on 2M+ text-image-pose triplets, this model demonstrated its foundational ability in generating high-quality human images across various scenarios. HyperHuman [Liu et al., 2023] presented a unified framework for generating hyper-realistic human images by capturing correlations between appearance and structure in multi-modal data. Specifically, it introduced a latent structural diffusion model that jointly denoises depth, surface normal, and RGB images conditioning on caption and pose skeleton, where modality-specific branches can be complementary to each other. Recently, CosmicMan [Li et al., 2024b] was proposed by building on three key pillars: a scalable, high-quality data production paradigm, robust model design via a decomposed attention-refocusing framework, and pragmatic integration into downstream tasks. This structure empowered CosmicMan to generate images in various scenarios, from full-body portraits to close-up shots, establishing it as a versatile cornerstone for human-centric content generation.\nSpatial-Temporal Diffusion Transformers extend diffusion models by incorporating transformer-based architectures to capture both spatial structures and temporal dependencies in videos. By leveraging self-attention mechanisms across spatial dimensions and time steps, this framework (Fig.3 (d)) effectively captures long-range temporal relationships while maintaining spatial consistency in human-centric video generation. Consequently, it is ideally suited for tasks such as character animation, video manipulation, and 4D generation. A representative work is Human4DiT [Shao et al., 2024], which introduced a hierarchical 4D Diffusion Transformer (DiT) for generating high-quality, 360-degree spatial-temporally coherent human videos from a single image. Trained on a multi-sourced dataset spanning images, videos, multi-view captures, and 4D footages, the model factorized self-attention across views, time, and space while incorporating accurate condition injection, successfully handling complex motions and viewpoint changes. Building on this framework, OmniHuman [Lin et al., 2025] proposed a scalable, multi-modality-conditioned human video generation model from a single image and motion signals (e.g., audio, video, or both). By leveraging mixed data and incorporating motion-related conditions during training, it mitigated data scarcity and enabled realistic video synthesis across diverse scenarios, including talking, singing, varying body compositions, and human-object interactions. Notably, pre-trained human-centric diffusion models are increasingly serving as the versatile foundation for a wide range of applications. For example, they can be (1) fine-tuned for text-driven image editing [Brooks et al., 2023] using instruction-image pairs, (2) integrated into character animation [Hu, 2024] by incorporating an additional branch to merge detailed textures into character's movements, (3) adapted for virtual try-on [Karras et al., 2024] by splitting classifier-free guidance into person, garment, and pose conditioning, and (4) extended to 3D/4D human generation [Kolotouros et al., 2023] by iteratively applying score distillation sampling in multiple views."}, {"title": "5 Human-centric Unified Perception and Generation Foundation Models", "content": "In recent years, human-centric foundation models emerge as a transformative approach for unifying perception and generation tasks, providing comprehensive frameworks to understand and synthesize human behaviors, motions, emotions, and intentions. By integrating multi-modal human-centric cues\u2014such as visual, auditory, textual, emotional, and motion data-into LLMs and MLLMs, these models facilitate richer, context-aware representations of human interactions. Such models can be categorized into two primary paradigms\u2014fixed vocabulary and extended vocabulary-based on how multi-modal human-centric signals are integrated into LLMs and MLLMs."}, {"title": "5.1 Fixed Vocabulary", "content": "Vocabulary-Fixed models enhance LLMs by introducing modality-specific projection layers to map human-centric signals into the feature space of LLMs or directly employing off-the-shelf tools and prompt engineering technique. Among these approaches, CoMo [Huang et al., 2024b] unified the text conditioned human motion generation, fine-grained motion generation and motion editing. Specifically, it auto-regressively generates sequences of interpretable pose codes upon the high-level text description and fine-grained, body-part-specific descriptions generated by LLMs. ChatPose [Feng et al., 2024] introduced LLMs to advance pose-related tasks, striving to develop a versatile pose generator. By integrating image interpretation, world knowledge, and body language comprehension into foundational LLMs, ChatPose enhanced its ability to understand and reason about 3D human poses from images and textual descriptions. Taking this a step further, ChatHuman [Lin et al., 2024] introduced an multi-modal LLM integrated with 22 domain-specific human-centric tools, improving its ability to reason about human-related tasks. Assisted by academic publications and retrieval-augmented generation model, ChatHuman generated in-context-learning examples for handling newly introduced tools. In other human-centric tasks, foundational models have also made remarkable strides. For example, ChatGarment [Bian et al., 2024] leveraged large vision-language models (VLMs) to automate the estimation, synthesis, and editing of 3D garments from either images or textual descriptions. Similarly, FaceGPT [Wang et al., 2024a] integrated 3D morphable face model (3DMM) [Blanz and Vetter, 2023] parameters into token space of VLMs, enabling the self-supervised generation of 3D facial models from both textual and visual inputs."}, {"title": "5.2 Extended Vocabulary", "content": "Vocabulary-extended models are designed to expand capabilities of LLMs [Touvron et al., 2023; Achiam et al., 2023] and MLLMs [Liu et al., 2024; Li et al., 2023] by explicitly extending their vocabulary and embedding spaces to accommodate human-centric and multi-modal signals. By aligning enriched human-centric representations\u2014such as pose parameters, SMPL representations, or motion sequences\u2014with the original vocabulary of LLMs, Vocabulary-Extension Models empower foundational LLMs and MLLMs to effectively tackle novel human-centric tasks, including generating, editing, and comprehending human poses of textual descriptions, image, and 3D modalities. Recent studies [Wu et al., 2024; Wang et al., 2024b; Jiang et al., 2023; Luo et al., 2024; Zhou et al., 2024] incorporated motion modality into textual space of the foundational LLMs, enabling a holistic representation of the complex relationship between motion and natural language. MotionGPT [Jiang et al., 2023] introduced a unified motion-language model to handle multiple motion-related tasks. It first discretized continuous motions into discrete semantic tokens, which could be interpreted as \"body language\" and then be extended to the LLM's vocabulary. Through pre-training alignment and prompt tuning stages, MotionGPT showcased strong generalization across various human motion understanding and generation tasks. Building on the MotionGPT [Jiang et al., 2023], AvatarGPT [Zhou et al., 2024] proposed an all-in-one structure for motion understanding, planning, generation, as well as motion in-between synthesis. M3-GPT [Luo et al., 2024] further embeded motion, music, and language into a single vocabulary and includes music-to-dance and dance-to-music tasks. Such a unified approach advanced human-centric tasks by synthesizing multi-modal human behaviors and bridging the gap between auditory, visual, and linguistic modalities. While these methods mostly were restricted to single-human motion, MotionLLM [Wu et al., 2024] introduced a simple yet versatile framework capable of handling single-human, multi-human motion generation, and motion captioning by fine-tuning pre-trained LLMs. MotionGPT-2 [Wang et al., 2024b] developed a general-purpose Large Motion-Language Model (LMLM), which extended beyond current solutions by tackling the challenging 3D holistic motion generation on the MotionX [Lin et al., 2023] benchmark. However, these methods struggled with pose-related editing and comprehension. UniPose [Li et al., 2024c] leveraged generation abilities of LLMs to unify all pose-relevant tasks to comprehend, generate, and edit human poses across various modalities, including images, text, and 3D SMPL representations. As a solution, LOM [Chen et al., 2024] unified verbal and non-verbal language using MLLMs for human motion understanding and generation, accommodating text, speech, motion, or any combination thereof as input flexibly. It trained a compositional body motion VQ-VAE to tokenize motions into part-ware discrete tokens, unifying modality-specific vocabularies (audio and text)."}, {"title": "6 Human-centric Agentic Foundation Models", "content": "Beyond understanding and generating human themselves, Human-centric agentic foundation models often process mulit-modal inputs (e.g., egocentric images, tactiles, sounds, and natural language as task description) focus on the human reaction and interactions to the environments while being constrained by its physical properties. Leveraging the Internet-scale multi-modal dataset, Human-centric agentic foundation models hold the promise of generalization across diverse tasks and provide a natural interface for human-robot interaction, both essential for real-world robot applications."}, {"title": "6.1 Vision-language-based models", "content": "Vision-language-based Models are prominent approaches in constructing foundation models for humanoid systems by integrating pre-trained vision-language models (VLMs) with sensorimotor control policies. As illustrated in Fig. 5(a), these methods leverage the strengths of visual and linguistic modalities to bridge high-level semantic understanding with low-level physical actions. In these frameworks, visual inputs are processed using pre-trained image encoders while natural language commands are tokenized through pre-trained text encoders. The extracted semantic features are then aligned with humanoid-specific control policies, enabling the translation of language and visual cues into precise motor actions. This cross-modal alignment not only facilitates language-to-action mapping but also enhances the capability of humanoid robots to perform complex, multi-step tasks. Recent efforts have focused on adapting these vision-language-based models to the unique challenges of humanoid robotics. Notable examples include HumanVLA [Xu et al., 2024] and SuperPADL [Juravsky et al., 2024], which aligned humanoid control policies with the latent spaces of pre-trained VLMs. These approaches have demonstrated the potential to endow humanoid robots with sophisticated skills that are driven by image and language inputs, paving the way for more advanced and natural human-robot interactions in dynamic environments."}, {"title": "6.2 Vision-language-action-based models", "content": "Vision-language-action (VLA) models have emerged as a promising approach in constructing foundation models for humanoid robots by unifying visual, linguistic, and action modalities within a single framework. As depicted in Fig. 5(b), these methods treat robot data-both observations and actions-as tokens in the vocabulary of a pre-trained language model, thereby enabling direct fine-tuning or co-training with established vision-language models. In contrast to traditional methods that depend on separately trainable low-level control policies, VLA models directly generate actions as token sequences, effectively merging high-level semantic reasoning with motor command generation. This unified tokenization framework allows the model to generate a wide range of skills while preserving robust language and vision understanding, thus enhancing its generalization across diverse tasks. However, representing actions as stringified tokens may become inefficient for high degrees-of-freedom systems such as humanoid robots, where the complexity of motor commands is significantly higher. Despite the potential benefits, applying the VLA paradigm to humanoid robotics remains largely unexplored. To address this gap, recent initiatives like NVIDIA's Project GROOT [Dong et al., 2024] have been announced. The GROOT foundation model aspired to leverage a diverse array of data sources from internet and simulation data to real-robot interactions to facilitate scalable training and achieve robust, cross-modal performance for complex humanoid tasks."}, {"title": "7 Challenges and Future Directions", "content": "Data. Different from general images and videos, collecting high-quality human-centric data is much more sensitive, difficult and expensive, compared to general images and videos. This situation inevitably leads to a trade-off in data quantity and data quality. Furthermore, the wide variability in human appearances, behaviors and contexts makes it hard to get a comprehensive dataset, limiting the generalization ability of data-driven human-centric foundation models.\nRepresentations. Human appearance and behavior demand a holistic understanding that integrates body, face, and hands, yet no existing foundation model captures these aspects simultaneously. Recognizing that these elements are closely interrelated parts of a complex system, there remains a significant gap in unified modeling frameworks. Future research should focus on developing innovative, multi-modal architectures and scalable datasets that bridge the global and granular representations of human dynamics, ultimately enhancing applications in digital human synthesis, interactive robotics, and personalized human-computer interaction.\nInteractivity. Despite significant progress in understanding and generating isolated human attributes such as appearance, emotion, and identity, current human-centric foundation models face challenges in capturing the complex interplay of interactions and contextual dynamics inherent in real-world scenarios. Most approaches are limited to static or isolated representations, hindering their ability to model the nuanced interdependencies between individuals and their environments. Future research should focus on developing unified frameworks that seamlessly integrate high-level semantic reasoning with fine-grained behavioral synthesis, enabling models to adapt to dynamic, multi-agent contexts.\nEthics. Ethics are crucial in applying human-centric foundation models, especially regarding sensitive domains and privacy. In sensitive domains, the model's output should never replace the expertise of human professionals. Moreover, Human-centric foundation models should also be developed with privacy-enhancing technologies, ensuring that the model learns powerful representations from the data while making it difficult to identify individual-level privacy information. Additionally, anonymization methods should be applied to all training data to avoid privacy violations."}]}