{"title": "Accelerating Quantum Reinforcement Learning with a Quantum Natural Policy Gradient Based Approach", "authors": ["Yang Xu", "Vaneet Aggarwal"], "abstract": "We address the problem of quantum reinforcement learning (QRL) under model-free settings with quantum oracle access to the Markov Decision Process (MDP). This paper introduces a Quantum Natural Policy Gradient (QNPG) algorithm, which replaces the random sampling used in classical Natural Policy Gradient (NPG) estimators with a deterministic gradient estimation approach, enabling seamless integration into quantum systems. While this modification introduces a bounded bias in the estimator, the bias decays exponentially with increasing truncation levels. This paper demonstrates that the proposed QNPG algorithm achieves a sample complexity of \\( \\tilde{O}(\\epsilon^{-1.5}) \\) for queries to the quantum oracle, significantly improving the classical lower bound of \\( \\tilde{O}(\\epsilon^{-2}) \\) for queries to the MDP.", "sections": [{"title": "Introduction", "content": "Reinforcement Learning (RL) is a foundational framework for sequential decision-making, with applications spanning robotics, finance, transportation, and healthcare (Al-Abbasi et al., 2019; Gonzalez et al., 2023; Tamboli et al., 2024; Wang et al., 2023). The goal of an RL agent is to derive a policy that maximizes the discounted sum of expected rewards through interactions with the environment. A prominent approach to solving this problem is the policy gradient (PG) method, which optimizes directly in the policy space. In classical RL, the optimal sample complexity of this approach has been established as O(\\( \\epsilon^{-2} \\)) (Mondal and Aggarwal, 2024). Quantum computing has shown potential for speedups in areas such as mean estimation (Hamoudi, 2021), multi-armed bandits (Wan et al., 2023; Wu et al., 2023), and tabular reinforcement learning (Ganguly et al., 2024; Zhong et al., 2024). This work aims to explore potential quantum speedups for reinforcement learning with general parameterized policies.\nA key enabler of quantum speedups is the quantum evaluation oracle, which leverages quantum parallelism, amplitude amplification, and entanglement. This powerful concept has been utilized in foundational algorithms such as Grover's search (Grover, 1996). In this work, we assume access to a quantum transition oracle and a quantum initial state oracle, which serve as quantum analogs for sampling from the classical environment and the initial state distribution, respectively. Using these quantum oracles, we aim to demonstrate the speedups that quantum computing can provide in the context of reinforcement learning. Quantum computing principles have shown significant improvements in areas such as quantum mean estimation (Hamoudi, 2021) and quantum stochastic convex optimization (Sidford and Zhang, 2024). However, quantum reinforcement learning (RL) presents unique challenges. Unlike classical algorithms, quantum methods rely on deterministic operations, complicating the design of unbiased estimators for policy gradients. This paper addresses the trade-off between bias and unbiased estimation by introducing a novel deterministic algorithm that facilitates quantum-compatible gradient estimation. While this approach introduces a bounded bias, it achieves orders reduction in sample complexity compared to its classical counterparts."}, {"title": "1.1 Challenges and Contributions", "content": "In this work, we introduce the first quantum model-free reinforcement learning (RL) algorithm with theoretical guarantees, addressing the challenges posed by large state and action spaces. To our knowledge, this is the first work to coherently embed the entire Natural Policy Gradient (NPG) into a quantum state by leveraging only the standard environment oracles from reinforcement learning. This novel construction allows subsequent quantum subroutines to utilize these NPG gradients directly in superposition, enabling potential accelerations in policy optimization. Extending classical model-free RL algorithms to the quantum domain is nontrivial. Specifically, classical model-free algorithms (Mondal and Aggarwal, 2024; Liu et al., 2020; Agarwal et al., 2021) typically rely on sampling trajectories of random lengths following a geometric distribution for policy gradient estimation. However, encoding such trajectories with random lengths into quantum systems is challenging. To address this, we propose a novel deterministic sampling algorithm that uses truncation to estimate both the Fisher matrix and the policy gradient. We further develop a mini-batch strategy to incorporate quantum variance reduction into the Natural Policy Gradient (NPG) update. By carefully analyzing the bias introduced by the truncation, we show that our approach achieves a sample complexity of \\( \\tilde{O}(\\epsilon^{-1.5}) \\), surpassing the classical lower bound of \\( \\tilde{O}(\\epsilon^{-2}) \\). To the best of our knowledge, this is the first work to demonstrate quantum speedups for parameterized model-free infinite-horizon Markov Decision Processes (MDPs).\nIt should be noted that SGD-based approaches have been shown to achieve a sample complexity of \\( \\tilde{O}(\\epsilon^{-1.5}) \\) for stochastic convex optimization (Sidford and Zhang, 2024). However, reinforcement learning (RL) presents a fundamentally different set of challenges compared to stochastic convex optimization. While insights from the literature suggest that \\( \\tilde{O}(\\epsilon^{-1.5}) \\) is likely the best achievable sample complexity for SGD-based approaches, this limitation arises because quantum computing can accelerate the inner loop but not the outer loop of such algorithms. To achieve better sample complexity guarantees, stochastic cutting-plane methods have been proposed in (Sidford and Zhang, 2024). Extending these methods to RL, however, would require an entirely new algorithmic framework. This is due to the non-convex nature of RL with general parameterization, which makes direct adaptation infeasible. Moreover, the techniques used in (Agarwal et al., 2021) to transition from local to global convergence are not applicable in this context, preventing similar guarantees for such methods in RL."}, {"title": "1.2 Contributions", "content": "The main contributions of this paper are summarized as follows:\n\u2022 We construct a quantum oracle for NPG gradient estimation by leveraging fundamental oracles from the Markov Decision Process (MDP), ensuring efficient computation in quantum environments.\n\u2022 We modify the classical Natural Policy Gradient (NPG) algorithm into a deterministic setting, enabling seamless integration into quantum systems while maintaining bounded gradient estimation bias.\n\u2022 Our proposed approach achieves a sample complexity of \\( \\tilde{O}(\\epsilon^{-1.5}) \\), which surpasses the classical lower bound of \\( \\tilde{O}(\\epsilon^{-2}) \\), demonstrating a significant improvement in efficiency."}, {"title": "1.3 Related Work and Preliminaries", "content": "Quantum Mean Estimation: Mean estimation focuses on determining the average value of samples drawn from an unknown probability distribution. Notably, quantum mean estimation provides a quadratic improvement over classical methods (Montanaro, 2015; Hamoudi, 2021). This enhancement arises from the utilization of quantum amplitude amplification, which facilitates the suppression of undesired quantum states relative to the target states to be extracted (Brassard et al., 2002).\nIn what follows, we define the key concepts and results related to quantum mean estimation that are central to our analysis. Specifically, we introduce the definition of a classical random variable alongside the quantum sampling oracle, which is used to perform quantum experiments.\nDefinition 1 (Random Variable, Definition 2.2 of (Cornelissen et al., 2022)). A finite random variable can be represented as \\( X : \\Omega \\rightarrow \\mathbb{E} \\) for some probability space \\( (\\Omega, \\mathbb{P}) \\), where \\( \\Omega \\) is a finite sample set, \\( \\mathbb{P} : \\Omega \\rightarrow [0, 1] \\) is a probability mass function and \\( \\mathbb{E} \\subseteq \\mathbb{R} \\) is the support of \\( X \\). \\( (\\Omega, \\mathbb{P}) \\) is frequently omitted when referring to the random variable \\( X \\).\nTo perform quantum mean estimation, we provide the definition of quantum experiment. This is analogous to classical random experiments.\nDefinition 2 (Quantum Experiment). Consider a random variable \\( X \\) on a probability space \\( (\\Omega, 2^{\\Omega}, \\mathbb{P}) \\). Let \\( H_{\\Omega} \\) be a Hilbert space with basis states \\( {\\vert \\omega \\rangle}_{{\\omega}\\in\\Omega} \\) and fix a unitary \\( U_{\\mathbb{P}} \\) acting on \\( H_{\\Omega} \\) such that\n\\[\nU_{\\mathbb{P}} : \\vert 0 \\rangle \\rightarrow \\sum_{{\\omega}\\in\\Omega} \\sqrt{\\mathbb{P}(\\omega)} \\vert \\omega \\rangle\n\\]"}, {"title": "2 Formulation", "content": "We analyze a Markov Decision Process (MDP) represented by the tuple \\( \\mathcal{M} = (\\mathcal{S}, \\mathcal{A}, r, P, \\gamma, \\rho) \\), where \\( \\mathcal{S} \\) and \\( \\mathcal{A} \\) denote the state space and action space, respectively. The reward function \\( r : \\mathcal{S} \\times \\mathcal{A} \\rightarrow [0, 1] \\) assigns a reward to each state-action pair, and the state transition kernel \\( P : \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\Delta^{\\vert \\mathcal{S} \\vert} \\) defines the probabilities of transitioning between states (with \\( \\Delta^{\\vert \\mathcal{S} \\vert} \\) denoting the probability simplex over \\( \\vert \\mathcal{S} \\vert \\) states). The discount factor \\( \\gamma \\in (0, 1) \\) determines the importance of future rewards, while \\( \\rho \\in \\Delta^{\\vert \\mathcal{S} \\vert} \\) specifies the initial state distribution. A policy \\( \\pi : \\mathcal{S} \\rightarrow \\Delta^{\\vert \\mathcal{A} \\vert} \\) provides the probability distribution over actions for a given state. The Q-function for a policy \\( \\pi \\), corresponding to a state-action pair \\( (s, a) \\), is defined as follows:\n\\[\nQ^{\\pi}(s, a) = \\mathbb{E} \\bigg[ \\sum_{t=0}^{\\infty} \\gamma^t r(s_t, a_t) \\bigg| s_0 = s, a_0 = a \\bigg]\n\\]\nHere, the expectation is taken over all trajectories \\( {(s_t, a_t)}_{t=0}^{\\infty} \\) induced by \\( \\pi \\), where \\( s_{t+1} \\sim P(s_t, a_t) \\) and \\( a_t \\sim \\pi(s_t) \\) for all \\( t > 1 \\). Similarly, the V-function associated with the policy \\( \\pi \\) is defined as:\n\\[\nV^{\\pi}(s) = \\mathbb{E} \\bigg[ \\sum_{t=0}^{\\infty} \\gamma^t r(s_t, a_t) \\bigg| s_0 = s \\bigg] = \\sum_{a \\in \\mathcal{A}} \\pi(a \\vert s) Q^{\\pi}(s, a)\n\\]\nThe advantage function is then given by:\n\\[\nA^{\\pi}(s, a) = Q^{\\pi}(s, a) - V^{\\pi}(s), \\quad \\forall (s, a) \\in \\mathcal{S} \\times \\mathcal{A}\n\\]\nThe primary objective is to maximize the following function over all possible policies:\n\\[\nJ = \\mathbb{E}_{s \\sim \\rho} [V^{\\pi}(s)] = \\frac{1}{1 - \\gamma} \\sum_{s, a} \\delta^{\\pi}(s, a) r(s, a)\n\\]\nHere, the state occupancy \\( \\delta^{\\pi} \\in \\Delta^{\\vert \\mathcal{S} \\vert} \\) is defined as:\n\\[\n\\delta^{\\pi}(s) = (1 - \\gamma) \\sum_{t=0}^{\\infty} Pr(s_t = s \\vert s_0 \\sim \\rho, \\pi), \\quad \\forall s \\in \\mathcal{S}\n\\]\nThe state-action occupancy \\( \\nu \\in \\Delta^{\\vert \\mathcal{S} \\times \\mathcal{A} \\vert} \\) is similarly defined as \\( \\nu(s, a) = \\delta^{\\pi}(s) \\pi(a \\vert s) \\) for all \\( (s, a) \\in \\mathcal{S} \\times \\mathcal{A} \\). In practical applications, the size of the state space often necessitates parameterizing policies using deep neural networks with d-dimensional parameters. Let \\( \\pi_{\\theta} \\) represent a policy parameterized by \\( \\theta \\in \\mathbb{R}^d \\). Under this parameterization, the optimization problem can be reformulated as:\n\\[\n\\max_{\\theta \\in \\mathbb{R}^d} J_{\\rho}(\\theta)\n\\]\nFor simplicity, we denote \\( J_{\\rho} \\) as \\( J_{\\rho}(\\theta) \\) throughout the remainder of this paper."}, {"title": "2.3 Quantum access to an MDP", "content": "We adopt the framework and notations of quantum computing from Zhong et al. (2024); Wiedemann et al. (2022); Ganguly et al. (2024); Jerbi et al. (2023) to design the quantum sampling oracle for reinforcement learning (RL) environments, enabling the modeling of an agent's interactions with an unknown MDP environment. We proceed to define quantum-accessible RL environments corresponding to the classical MDP \\( \\mathcal{M} \\). For an agent at step \\( t \\) in state \\( s_t \\) and performing action \\( a_t \\), we construct quantum sampling oracles for the transition probabilities of the next state, \\( P(\\cdot \\vert s_t, a_t) \\). To this end, let two Hilbert spaces, \\( \\mathcal{S} = \\mathbb{C}^{\\vert \\mathcal{S} \\vert} \\) and \\( \\mathcal{A} = \\mathbb{C}^{\\vert \\mathcal{A} \\vert} \\), represent the superpositions of classical states and actions, respectively. The computational bases for these Hilbert spaces are denoted as \\( {\\vert s \\rangle}_{s \\in \\mathcal{S}} \\) and \\( {\\vert a \\rangle}_{a \\in \\mathcal{A}} \\). We assume the ability to implement the following quantum sampling oracles:\n\u2022 Quantum transition oracle \\( U_P \\): The quantum evaluation oracle for the transition probability (quantum transition oracle) \\( U_P \\) which at step \\( t \\), returns the superposition over \\( s' \\in \\mathcal{S} \\) according to \\( P(s' \\vert s_t, a_t) \\), the probability distribution of the next state given the current state \\( \\vert s_t \\rangle \\) and action \\( \\vert a_t \\rangle \\) is defined as:\n\\[\nU_P: \\vert s_t \\rangle \\vert a_t \\rangle \\vert 0 \\rangle \\rightarrow \\vert s_t \\rangle \\vert a_t \\rangle \\sum_{s' \\in \\mathcal{S}} \\sqrt{P(s' \\vert s_t, a_t)} \\vert s' \\rangle\n\\]\n\u2022 Quantum initial state oracle \\( U_{\\rho} \\): The quantum sampling of the initial state distribution (quantum initial state oracle) \\( U_{\\rho} \\) which when queried, returns a superposition over \\( s \\in \\mathcal{S} \\) according to \\( \\rho \\) is defined as:\n\\[\nU_{\\rho} : \\vert 0 \\rangle \\rightarrow \\sum_{s \\in \\mathcal{S}} \\sqrt{\\rho(s)} \\vert s \\rangle\n\\]\nWe also assume the ability to construct a unitary \\( \\Pi \\) that coherently implements a policy \\( \\pi_{\\theta} \\):\n\u2022 Let \\( \\pi_{\\theta} : \\mathcal{S} \\times \\mathcal{A} \\rightarrow [0, 1] \\) be a reinforcement learning policy acting in a state-action space \\( \\mathcal{S} \\times \\mathcal{A} \\) and parametrized by a vector \\( \\theta \\in \\mathbb{R}^d \\) (that can be encoded with finite precision as \\( \\vert \\theta \\rangle \\)). We say that the policy is quantum-evaluatable if we can construct a unitary satisfying:\n\\[\n\\Pi: \\vert \\theta \\rangle \\vert s \\rangle \\vert 0 \\rangle \\leftrightarrow \\vert \\theta \\rangle \\vert s \\rangle \\sum_{a \\in \\mathcal{A}} \\sqrt{\\pi_{\\theta}(a \\vert s)} \\vert a \\rangle\n\\]\nThis construction is particularly intuitive for certain quantum policies, such as RAW-PQC which is introduced in Jerbi et al. (2023). However, any policy that can be classically computed can also be converted into such a unitary operation through quantum simulation of the classical computation of \\( (\\pi_{\\theta}(a \\vert s) : a \\in \\mathcal{A}) \\) and leveraging established methods to encode this probability vector into the amplitudes of a quantum state (Grover and Rudolph, 2002). With access to quantum oracles for both the environment and the policy in (4)-(6), it becomes possible to design subroutines capable of generating superpositions of trajectories with fixed length within the environment and calculating the returns associated with these trajectories. We can further utilize \\( U_P \\), \\( U_{\\rho} \\) and \\( \\Pi \\) defined above to construct a unitary \\( U_{\\mathbb{P}(T_N)} \\) as follows: Let \\( \\mathcal{M} \\) be a quantum-accessible MDP with oracles \\( U_P \\), \\( U_{\\rho} \\) as above, and let \\( \\pi_{\\theta} \\) be a quantum-evaluatable policy with its unitary implementation \\( \\Pi \\) as defined above. Given a fixed number \\( N \\), the unitary \\( U_{\\mathbb{P}(T_N)} \\) that prepares a coherent superposition of all trajectories \\( T_N = (s_0, a_0, ..., s_{N-1}, a_{N-1}) \\) of length \\( N \\) (without their rewards) is defined as,\n\\[\nU_{\\mathbb{P}(T_N)}: \\vert 0 \\rangle \\vert 0 \\rangle^{2N} \\rightarrow \\vert 0 \\rangle \\sum_{T_N} \\sqrt{\\mathbb{P}_{\\theta}(T_N)} \\vert T_N \\rangle\n\\]\nfor \\( \\mathbb{P}_{\\theta}(T_N) = \\rho(s_0) \\prod_{t=0}^{N-1} \\pi_{\\theta}(a_t \\vert s_t) P(s_{t+1} \\vert s_t, a_t) \\). The details of constructing \\( U_{\\mathbb{P}(T_N)} \\) using \\( U_P \\), \\( U_{\\rho} \\) and \\( \\Pi \\) are characterized in Appendix A."}, {"title": "3 Proposed Algorithm", "content": "A common way to solve the maximization (3) in classical methods is via updating the policy parameters by applying the gradient ascent: \\( \\theta_{k+1} = \\theta_k + \\eta \\nabla_{\\theta} J_{\\rho}(\\theta_k), k \\in {0, 1, ... } \\) starting with an initial parameter, \\( \\theta_0 \\). Here, \\( \\eta > 0 \\) denotes the learning rate, and the policy gradients (PG) are given as follows (Sutton et al., 1999).\n\\[\n\\nabla_{\\theta} J_{\\rho}(\\theta) = \\frac{1}{1 - \\gamma} \\mathbb{E}_{(s, a) \\sim \\nu_{\\theta}} \\big[ A^{\\pi_{\\theta}}(s, a) \\nabla_{\\theta} \\log \\pi_{\\theta}(a \\vert s) \\big]\n\\]\nThis paper uses the natural policy gradient (NPG) to update the policy parameters. In particular, \\( \\forall k \\in {1, 2, ... } \\), we have,\n\\[\n\\theta_{k+1} = \\theta_k + \\eta F_{\\rho}(\\theta_k)^{\\dagger} \\nabla_{\\theta} J_{\\rho}(\\theta_k)\n\\]\nwhere \\( {\\dagger} \\) is the Moore-Penrose pseudoinverse operator, and \\( F_{\\rho} \\) is called the Fisher information matrix which is defined as follows \\( \\forall \\mathbf{v} \\in \\mathbb{R}^d \\).\n\\[\nF_{\\rho}(\\theta) = \\mathbb{E}_{(s, a) \\sim \\nu_{\\theta}} \\big[ \\nabla_{\\theta} \\log \\pi_{\\theta}(a \\vert s) \\otimes \\nabla_{\\theta} \\log \\pi_{\\theta}(a \\vert s) \\big]\n\\]"}, {"title": "4 Global Convergence Analysis", "content": "In this section, we discuss the convergence properties of Algorithm 1. We first present the standard analysis procedures of the classical outer loop, and then delve into the results of the quantum inner loop, which contributes to the polynomial speedup in the final result."}, {"title": "4.1 Outer Loop Analysis", "content": "We start with the assumptions commonly used in classical parameterized RL.\nAssumption 1. The score function is G-Lipschitz and B-smooth. Mathematically, the following relations hold \\( \\forall \\theta, \\theta_1, \\theta_2 \\in \\mathbb{R}^d, \\forall (s, a) \\in \\mathcal{S} \\times \\mathcal{A} \\).\n\\[\n\\begin{aligned}\n&(a) \\quad \\|\\nabla_{\\theta} \\log \\pi_{\\theta}(a \\vert s)\\| \\leq G\\\\\n&(b) \\quad \\|\\nabla_{\\theta} \\log \\pi_{\\theta_1}(a \\vert s) - \\nabla_{\\theta} \\log \\pi_{\\theta_2}(a \\vert s)\\| \\leq B \\|\\theta_1 - \\theta_2\\|\n\\end{aligned}\n\\]\nwhere B and G are some positive reals.\nThe Lipschitz continuity and smoothness of the score function are widely assumed in the literature (Liu et al., 2020; Agarwal et al., 2020), and can be validated for basic parameterized policies, such as Gaussian policies."}]}