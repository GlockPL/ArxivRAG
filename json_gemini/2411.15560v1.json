{"title": "Do LLMs Agree on the Creativity Evaluation of Alternative Uses?", "authors": ["Abdullah Al Rabeyah", "Fabr\u00edcio G\u00f3es", "Marco Volpe", "Talles Medeiros"], "abstract": "This paper investigates whether large language models (LLMs) show agreement in assessing creativity in responses to the Alternative Uses Test (AUT). While LLMs are increasingly used to evaluate creative content, previous studies have primarily focused on a single model assessing responses generated by the same model or humans. This paper explores whether LLMs can impartially and accurately evaluate creativity in outputs generated by both themselves and other models. Using an oracle benchmark set of AUT responses, categorized by creativity level (common, creative, and highly creative), we experiment with four state-of-the-art LLMs evaluating these outputs. We test both scoring and ranking methods and employ two evaluation settings (comprehensive and segmented) to examine if LLMs agree on the creativity evaluation of alternative uses. Results reveal high inter-model agreement, with Spearman correlations averaging above 0.7 across models and reaching over 0.77 with respect to the oracle, indicating a high level of agreement and validating the reliability of LLMs in creativity assessment of alternative uses. Notably, models do not favor their own responses, instead they provide similar creativity assessment scores or rankings for alternative uses generated by other models. These findings suggest that LLMs exhibit impartiality and high alignment in creativity evaluation, offering promising implications for their use in automated creativity assessment.", "sections": [{"title": "1 Introduction", "content": "The Alternative Uses Test (AUT) is widely used to assess creative thinking, prompting individuals to propose unconventional uses for common objects. In recent years, the rise of Large Language Models (LLMs) like GPT has prompted researchers to explore whether these models can evaluate creativity as humans do. Prior work has demonstrated that LLMs are capable of assessing creativity in responses to tasks like the AUT, where models evaluate alternative uses generated either by humans or other models [1, 2, 3]. However, these studies typically focus on single models evaluating externally generated content, leaving open questions about how LLMs assess creativity on their own outputs.\nIn this paper, we address this gap by investigating how LLMs evaluate the creativity of alternative uses (AUs) generated by both themselves and other models. Specifically, we explore whether models favor their own responses or whether they can impartially assess the creativity of outputs generated by others. To do so, we employ an experimental framework using an oracle set of AUT responses, categorized into three groups: common, creative, and highly creative. Four different LLMs were prompted to score and rank these responses, and their evaluations are compared against an oracle.\nBy analyzing the evaluation results, we measure both the accuracy of the LLMs and their agreement with each other in ranking creative outputs. Our findings reveal that, with a high Spearman correlation, LLMs do not favor their own responses. Instead, they consistently agree on the creativity of responses across models. On average, the agreement correlation was higher than 0.7 for both ranking and scoring AUs among models, and greater than 0.77 when compared to the oracle, which indicates that models are accurate at assessing alternative uses."}, {"title": "2 Related Work", "content": "In the field of computational creativity, the evaluation of creative artifacts traditionally relies on human experts, possibly assisted by the use of metrics that automate part of the process [4, 5]. However, such traditional methods present significant limitations, justifying the exploration of LLM-based approaches [6]. First, the subjectivity and variability among human evaluators lead to inconsistencies, whereas LLMs can offer more uniform and standardized assessments over time. Additionally, the high cost and time required for large-scale evaluations make human evaluators impractical in many contexts, an issue that LLMs can address with faster and less costly assessments. Another challenge lies in the influence of cultural and contextual factors, which affect human judgments but can be mitigated by LLMs calibrated with diverse data. The difficulty of objectively quantifying creativity also limits traditional methods, while LLMs, by applying standardized criteria, offer greater reliability in this aspect. Finally, the limitations of traditional methods in capturing nuances of divergent creative thinking, along with the complexity of calibrating and standardizing evaluations, underscore the potential of LLMs as consistent alternatives that can reduce bias and lessen the need for human evaluators [7].\nFor this reason, the use of LLMs in evaluating creativity has recently emerged as a significant area of research, and several studies have demonstrated evaluation capabilities of LLMs in various contexts. For instance, [8] explores the automatic scoring of metaphor creativity using LLMs, demonstrating their potential to assess figurative language effectively, while [9] evaluates the creativity of jokes by simulating different personas/judges, and [10] uses LLMs to evaluate poetry. Further recent studies explore the use of LLMs as automated evaluators, addressing both their advantages and limitations. In [11], the presence of positional biases in these models is highlighted, with calibration techniques proposed to mitigate such biases. Similarly, [7] and [12] validate the high correlation of LLM evaluations with human assessments, though they remain limited by susceptibility to positional and verbosity biases. Other studies, such as [13], examine the ability of LLMs to replicate human preferences in NLP tasks, while [14] introduces creative approaches, combining diverse beam search and self-evaluation. Alternatively, [15] proposes GPTScore for flexible evaluation, and [16] explores approaches like rate-explain to enhance judgment accuracy, while interactive tools, such as EvaluLLM [17], enable customized pairwise evaluations. Collectively, these studies underscore the potential of LLMs as evaluators, with challenges and opportunities for methodological adjustments and advancements in bias reduction."}, {"title": "2.1 Creativity Assessment using LLMs", "content": "Recent studies have investigated various methodologies and contexts in which LLMs can assess creativity. In [18], researchers evaluated LLMs on creative writing tasks. Models like GPT-4 showed high fluency and coherence, although human evaluators still outperformed LLMs in originality and humor. A collaborative approach was proposed by [19] with the COEVAL pipeline, which combines initial LLM evaluations with human reviews. This approach significantly reduced evaluation time and provided greater consistency by adjusting subjective criteria.\nIn divergent thinking tasks, [3] demonstrated that LLMs could reliably assess flexibility in alternative use tasks. This study reported a strong correlation with human evaluations, highlighting the model's effectiveness, particularly in educational settings. For more specialized tasks, [8] applied LLMs to metaphor creativity assessment, where models like ROBERTa and GPT-2 showed good alignment with human judgments, even outperforming traditional metrics.\nLLMs have also been applied to creative assessments in non-English contexts. In [20], XLM-ROBERTa was used to evaluate originality in scientific creativity tasks conducted in German, proving effective in capturing divergent ideation. Similarly, [21] explored the use of LLMs to evaluate question complexity based on Bloom's Taxonomy, achieving a high correlation with human evaluations and validating its use in educational assessments. Lastly, [22] investigates creativity in LLMs adapting the Torrance Test to measure fluency, originality, and elaboration, while [23] provides a comprehensive review of creativity assessment practices in machine learning, covering methodologies such as Generative Adversarial Networks (GANs) and Transformers and examining metrics like novelty, value, and surprise for creativity evaluation."}, {"title": "2.2 Traditional vs LLM-based Methods for AUT", "content": "In the context of divergent thinking tasks, a widely adopted test to measure creativity is the Alternative Uses Test (AUT) [24], which requires the participants to propose uncommon uses for everyday objects. A traditional technique to evaluate creativity in this context consists in computing the semantic distance [25], which refers to the degree of difference or separation between concepts, ideas or objects in terms of their meanings or associations. For the AUT, the semantic distance is computed between the everyday object posed to participants and words in the participant's response; the larger the distance, the more original is considered the answer. Recent works experimented with the use of LLMs in both the generation and the evaluation of AUT responses, demonstrating in particular that in this context LLM evaluation performances are far superior to evaluations based on semantic distance [26, 27].\nIn [2], a technique based on the use of increasingly forceful prompts is used to push LLMs to produce at each iteration more creative responses. The technique is applied to both the AUT and a textual version of the image completion task in the Torrance Test of Creativity [28]. In the same paper, a LLM is also used for evaluating the output of such tests, and the experiments demonstrate that the results produced as a response to the forceful prompts are indeed considered more creative than the initial ones. In this paper, we evaluate alternative uses produced by applying the same technique based on the use of forceful prompts and rely on the results of [2] to construct an evaluation oracle.\nWhile many works analyze and possibly compare the generative performance of LLMs (see [29] for a review of LLM evaluation methods with respect to different contexts and tasks), this paper focuses on comparing how different LLMs evaluate the creativity of AUT outputs and on measuring the level of agreement among these LLMs."}, {"title": "3 Experimental Setup", "content": "In this research, we use four large language models to generate and assess the creativity of alternative uses (AUs) and assess the level of agreement between those models. We selected five common objects, and each model generated 15 AUs per object across different levels of creativity, forming a dataset of 60 AUs per object, for a total of 300 AUs.\nIn order to evaluate creativity, two approaches were tested: Scoring (assigning creativity scores from 1 to 5) and Ranking (ordering AUs from most to least creative). Additionally, we used Comprehensive (all 60 AUs at once) and Segmented (five groups of 12 AUs) setups to compare the impact of evaluation size on model accuracy. An evaluation oracle served as a benchmark to establish expected creativity levels, allowing for consistent comparison across models. By using the Spearman correlation, we determine how models agree in the creativity evaluation of AUs and whether scoring or ranking provides a more accurate measure.\nIn this section, we present our experimental setup in the following order: alternative uses generation, scoring vs. ranking techniques, comprehensive vs. segmented approaches, and LLMs agreement evaluation."}, {"title": "3.1 Alternative Uses Generation", "content": "The first step consisted in generating a dataset of AUs for five common objects: fork, wallet, soap, cotton swab, and paperclip. These objects were selected due to their everyday nature, which ensures a broad range of potential alternative uses. To produce AUs at varying levels of creativity, we employed four state-of-the-art, commercial LLMs: ChatGPT-4, ChatGPT-40, Claude 3.5 Sonnet, and Gemini 1.5 Flash.\nThe generation process was designed to produce AUs at varying levels of creativity, from common uses, to average creative alternative uses, to utilizing a set of forceful prompts described in [2] that can yield to highly_creative AUs. In this study, we aimed at having 3 distinct non-overlapping categories of AUs characterized by an increasing level of"}, {"title": "3.2 Alternative Uses Evaluation", "content": "To ensure a comprehensive and fair evaluation, we adhered to the following principles: evaluations were conducted on a per-object basis, and each model evaluated both its own generated AUs and those generated by other models. This approach allowed us to assess both the independence of each model and the agreement correlation between models in the creativity evaluation of AUs. In order to do it, we tested the following approaches: i) Scoring vs. Ranking, which consists of rating each AU separately and establishing an order between them, respectively; ii) Comprehensive vs. Segmented, which compares the evaluation of all AUs for a given object through a single prompt and segmented evaluation in smaller groups."}, {"title": "3.2.1 Scoring vs Ranking", "content": "By using these two techniques we can investigate whether the models exhibit consistent behavior across different evaluation techniques. Their descriptions follow:\n\u2022 Scoring: The first technique, as used by [27] and [2], involves assigning a numerical value to each AU based on its creativity. In our experiments, we prompted the LLMs to assign values between 1 and 5.\n\u2022 Ranking: This technique requires the direct comparison and relative ordering of AUs. This later technique forces a clear confrontation between items, as in [9], and can reveal preferences that might not be apparent in numerical scoring.\nThese techniques seem to be the two most common for evaluating and comparing creativity [9]. The prompts used for scoring and ranking can be seen in Figures 4 and 5."}, {"title": "3.2.2 Comprehensive vs Segmented", "content": "In order to assess how the number of AUs that are simultaneously evaluated (i.e., through a single prompt) affects the creativity evaluation ability of LLMs, we used two distinct approaches:\n\u2022 Comprehensive 60 AUs Evaluation: This includes five alternative uses (AUs) from each model for each creativity category evaluated in a single prompt.\n\u2022 Segmented 12 AUs Evaluation: The 60 AUs related to one object are divided into 5 groups of 12 AUs. Each group consists of one AU from each model for each creativity category. Following this criterion, the AUs are randomly distributed across the 5 groups. Each group is evaluated separately and the results are then combined into a single list, as detailed in Section 3.2.3.\nBy utilizing these two approaches, we gain insight into the models' ability to maintain consistent evaluations across different sample sizes. Some studies have shown that LLMs face challenges when evaluating longer lists of items, which can reduce the quality of their evaluation [30]. However, this may be necessary for large numbers of AUs or other creative artifacts (e.g., poems, stories). Similarly, it has been found that models perform better when the prompts used are shorter [31]."}, {"title": "3.2.3 Evaluation process", "content": "For the Comprehensive approach, all 60 AUs generated for a given object were evaluated together. To avoid order effects, the 60 AUs were randomly shuffled before being presented to the evaluation prompt. This prompt was adapted from the previous study [2]. Our evaluation process included two distinct techniques as described above: Scoring and Ranking. In the Scoring technique, the LLMs were prompted to assign a score from 1 (least creative) to 5 (most creative) for each AU (Figure 4). The Ranking technique, on the other hand, asked the models to rank the 60 AUs from 1 (most creative) to 60 (least creative) (Figure 5). Once all 60 AUs were evaluated, we calculated the average score or ranking for each list of five AUs corresponding to the same LLM and creativity category (common, creative, highly_creative). This average leads to 12 results that are represented by each bar in Figure 6. As we explain in the next section, if a model achieves this exact ordering, from common to highly_creative, it means that it assesses creativity in accordance with the oracle."}, {"title": "3.2.4 AUs Evaluation Oracle", "content": "To establish a benchmark for assessing the LLMs' ability to evaluate different levels of creativity, we constructed an evaluation oracle. By relying on the results of [2], we expect the creativity levels (common, creative and highly_creative) used for prompting to be able to actually produce outputs with increasing levels of creativity. Therefore, the oracle was constructed based on the AUs generated by the different creativity levels as follows:\n\n$AUs Evaluation Oracle = \\begin{bmatrix}  highly\\_creative_1, highly\\_creative_2, highly\\_creative_3, highly\\_creative_4, \\\\  \\text{4 highest creativity} \\\\  creative_1, creative_2, creative_3, creative_4, \\\\   \\text{4 average creativity} \\\\ common_1, common_2, common_3, common_4\\end{bmatrix}$  \n\nThe indexes in Equation (1) represent each one of the four models (i.e., 1 for Claude, 2 for Gemini, 3 and 4 for ChatGPT-40 and ChatGPT-4, respectively). The order in the oracle is simply the ordered set of highly_creative AUs generated by each model followed by the creative and common ones, as shown in Figure 6. The higher the correlation between the evaluation of a given LLM and the oracle, the more accurate will be considered the LLM's creativity evaluation. Specifically, similarly to what is done in works such as [32] and [6], the Spearman's Rank Correlation (SRC) is used to measure how closely the LLMs' evaluations of the AUs align with the expected rankings or scores set by the oracle. A high correlation (above 0.7) suggests that the LLM is effectively distinguishing between creativity levels, in line with the predefined expectations of the creativity pushing technique. In a similar way, the SRC will also be used to assess the level of agreement between the LLMs themselves by calculating the correlation between each pair of LLMs."}, {"title": "4 Experimental Results", "content": "This section presents a detailed evaluation of creativity assessments by four LLMs: Claude 3.5 Sonnet, ChatGPT-4, Gemini 1.5 Flash, and ChatGPT-40. Using both scoring and ranking approaches across comprehensive (60 AUs) and segmented (12 AUs x 5) conditions, we measure each model's alignment with the evaluation oracle and inter-model agreement through the SRC."}, {"title": "4.1 Comprehensive 60 AUs Evaluation by Score", "content": "Table 2 shows (top left in the table) the heatmap of the comprehensive evaluation using the scoring approach for the average of all 5 objects. It presents a high level of agreement between the LLMs and the oracle (above 0.95). Notably,"}, {"title": "4.2 Comprehensive 60 AUs Evaluation by Ranking", "content": "In the comprehensive evaluation using the ranking approach, Claude 3.5 Sonnet achieved again the highest SRC with respect to the oracle, with a value of 0.95 across all objects, while Gemini 1.5 Flash recorded the lowest SRC of 0.77 (Table 2, top-right diagram).\nOverall, the correlation between the LLMs and the oracle remained above 0.77 in most cases. However, a significant exception was observed between ChatGPT-4 and Gemini 1.5 Flash, where the correlation dropped to 0.60, indicating a substantial disparity in how these two models ranked the AUs. This suggests that Gemini 1.5 Flash might have had difficulties differentiating creativity levels relative to ChatGPT-4, particularly in this evaluation setup.\nAlso in the ranking approach, Claude 3.5 Sonnet generated the most creative AUs in both the common and highly_creative categories. These highly ranked AUs reinforce its position as the model with the best overall ranking across all categories (see Table 13 in the Appendix). In contrast, ChatGPT-4 generated the most creative AUs in the creative category, but did not perform as well in the other categories. In the ranking approach, lower ranks correspond to higher creativity, meaning that the most creative alternative use was ranked 1, while the least creative was ranked 60."}, {"title": "4.3 Segmented 12 AUs Evaluation by Score", "content": "The segmented evaluation revealed some differences between the evaluation approaches. When using scoring, ChatGPT-4o achieved the highest SRC with the oracle, with a score of 0.95 across all five objects (Table 2, bottom-left diagram). Gemini 1.5 Flash again recorded the lowest SRC at 0.85, indicating a relatively weaker performance compared to the other models. It is important to note that Claude 3.5 Sonnet continued to generate very creative AUs in both the creative and highly_creative categories (see Table 14 in the Appendix). The standard deviation remained low, at less than 0.33, indicating a high degree of agreement among the models.\nThe correlation between the LLMs and the oracle, as well as between the models themselves, remained overall above 0.82 in this evaluation as shown in Table 2. However, compared to the comprehensive evaluation by scores, the"}, {"title": "4.4 Segmented 12 AUs Evaluation by Ranking", "content": "Finally, the results from the segmented evaluation using the ranking methodology show that ChatGPT-4o achieved the highest SRC with the oracle, with a value of 0.87 across all five objects. The correlations between the LLMs and the oracle, as well as between the LLMs themselves, ranged between 0.70 and 0.87 as shown in Table 2 (bottom-right diagram), indicating strong overall agreement. Notably, for the Wallet object (see Table 8 in the Appendix), all models achieved a perfect correlation of 1.00 with the oracle, demonstrating complete consensus in evaluating the creativity levels of the alternative uses for this particular object. However, the lowest correlation was observed between ChatGPT-4 and Gemini 1.5 Flash, reinforcing once again less agreement between these two models, particularly in ranking-based evaluations.\nIn terms of category performance, Claude 3.5 Sonnet generated the most creative AUs in the creative category, while ChatGPT-40 produced the most creative AUs in the highly_creative category. ChatGPT-4 generated the most creative AUs in the common category, but overall, Claude 3.5 Sonnet achieved the best ranking average across all categories, as detailed in Table 15.\nAs for the scoring approach, some individual evaluations of the Paperclip object placed creative AUs higher than highly_creative AUs, but these occurrences did not affect the overall ranking averages across the objects, as seen in Table 15 in the Appendix. While the evaluation process is subject to some variability, it produced consistent results across the majority of cases."}, {"title": "4.5 Agreement Correlation between LLMS", "content": "To assess the overall agreement between the LLMs in their creativity evaluations, we averaged the SRC across all four experimental conditions (comprehensive scoring, comprehensive ranking, segmented scoring, and segmented ranking). The results reveal consistently high correlations between all models, with values ranging from 0.77 to 0.87, indicating strong agreement among models in creativity evaluation as shown in Figure 7.\nChatGPT-40 demonstrated the strongest overall agreement with other models, showing correlations of 0.85 with ChatGPT-4, 0.87 with Gemini 1.5 Flash, and 0.87 with Claude 3.5 Sonnet. The correlation between Claude 3.5 Sonnet and ChatGPT-4 (0.85) was equally strong, while the correlation between Gemini 1.5 Flash and ChatGPT-4 showed the lowest value (0.77), consistent with the pattern observed across individual experiments.\nA key finding of our analysis is that LLMs do not favor their own responses when evaluating creative outputs. Despite each model having the opportunity to evaluate its own AUs, none showed a tendency towards rating their own outputs more favorably. The agreement between models evaluations agreement level is further supported by the low standard deviations observed across all experiments (below 1.03, as shown in Tables 12, 13, 14, and 15 in the Appendix), demonstrating that the models' evaluations remained stable and consistent regardless of which model generated the AUs."}, {"title": "5 Conclusion", "content": "In this research, we investigated the level of agreement among LLMs in assessing creativity of alternative uses and also with an oracle across different methods. Our findings demonstrate a high level of agreement between the models, with inter-model correlations generally exceeding 0.80, indicating strong consistency in their evaluations. This high correlation suggests that LLMs share a similar understanding of creativity, reliably distinguishing between more and less creative alternative uses. ChatGPT-40, in particular, exhibited robust alignment with other models, while Claude 3.5 Sonnet achieved high evaluation scores on its generated alternative uses, further aligning closely with the oracle across both comprehensive and segmented evaluations.\nThe evaluation results also revealed that models do not rate their own responses more favorably. Notably, ChatGPT-40 displayed strong alignment with other models across scoring and ranking methods, demonstrating reliable performance and reinforcing the validity of its evaluations. While some variance was observed between ChatGPT-4 and Gemini 1.5 Flash, the low standard deviations across all experiments indicate a stable evaluation framework, enhancing confidence in the LLMs' ability to generate consistent creativity assessments.\nAs future work, we will focus on expanding and refining this evaluation framework. One promising direction is to increase the diversity and complexity of the AU datasets, allowing for more granular assessments of creativity across varied and challenging contexts. Additionally, evaluating LLMs on other domain-specific creativity tasks such as poetry and jokes could provide deeper insights into their understanding of creativity. Refining this framework and experimenting with task-specific criteria will be essential for advancing the use of LLMs as reliable evaluators in creative domains, supporting the broader goal of enhancing AI's role in creativity assessment."}]}