{"title": "Explainability-Driven Quality Assessment for Rule-Based Systems", "authors": ["Oshani Seneviratne", "Brendan Capuzzo", "William Van Woensel"], "abstract": "This paper introduces an explanation framework designed to enhance the quality of rules in knowledge-based reasoning systems based on dataset-driven insights. The traditional method for rule induction from data typically requires labor-intensive labeling and data-driven learning. This framework provides an alternative and instead allows for the data-driven refinement of existing rules: it generates explanations of rule inferences and leverages human interpretation to refine rules. It leverages four complementary explanation types-trace-based, contextual, contrastive, and counterfactual-providing diverse perspectives for debugging, validating, and ultimately refining rules. By embedding explainability into the reasoning architecture, the framework enables knowledge engineers to address inconsistencies, optimize thresholds, and ensure fairness, transparency, and interpretability in decision-making processes. Its practicality is demonstrated through a use case in finance.", "sections": [{"title": "1 Introduction", "content": "The reliability, utility, and trustworthiness of knowledge-based systems, and, ultimately, the Web of Data, lies in the quality of the underlying logic-based rules. Rules that are inaccurate or incomplete, or, in general, do not capture their intent, compromise the effectiveness of reasoning systems that rely on them. To improve rule quality in a data-driven way, machine learning techniques such as rule induction or decision trees can be used to extract rules from data. However, these methods typically require labeled datasets, which are labor-intensive to prepare; also, the output is a new set of rules that needs to be compared to the current ones. This paper instead introduces an explanation-driven framework for refining existing rules, which enhances reliability, transparency, and trust in reasoning systems without the effort of full-scale data labeling.\nTo that end, the framework generates explanations of rule inferences, which can be interpreted by knowledge engineers to refine the existing rules. Our framework leverages four complementary explanation types (trace-based, contextual, contrastive, and counterfactual) to provide diverse perspectives for debugging, validating, and ultimately refining rules. These explanation types empower users to understand the derivation of conclusions, explore their immediate and upstream causes, compare alternative scenarios, and, in doing so, generate actionable insights for improvement. We integrated these explanation mechanisms into Punya [9, 10], a Semantic Web fork of the MIT App Inventor platform, which is a low-code platform for app development. This choice was made to support both technical and non-technical knowledge engineers in debugging and refining rules in a data-driven way. Punya, in particular, was chosen due to its built-in support for rule-based reasoning. The proposed explanation framework primarily benefits knowledge engineers; by enhancing rule transparency through explanations, the framework can also benefit end-users who interact with reasoning-driven applications. To the best of our knowledge, our work is among the first to explicitly investigate the use of explanations for refining rules in knowledge-based systems.\nThe remainder of this paper is structured as follows. Section 2 discusses related work on rule quality and debugging, highlighting how our approach complements and extends prior efforts. In Section 3, we describe the reasoning architecture underpinning the explanation framework. Section 4 presents a compelling use case that benefits from our solution. Section 5 demonstrates the explanation types, illustrating their role in enhancing rule quality. Section 6 outlines the integration of the framework into the MIT App Inventor (Punya) platform, emphasizing its features, lightweight implementation, and practical benefits. Finally, Section 7 and Section 8 discuss the implications of this work and conclude with future directions for improving data quality and explainability in knowledge-based systems."}, {"title": "2 Related Work", "content": "The quality and debugging of rules in knowledge-based systems have been studied in several contexts, with efforts ranging from foundational correctness principles to advanced explanation frameworks. This section reviews key contributions and situates our work within this landscape."}, {"title": "2.1 Rule Quality Principles", "content": "Landauer [6] propose a set of acceptability principles-Consistency, Completeness, Irredundancy, Connectivity, and Distribution-to guide the construction and validation of rule bases. These principles provide foundational criteria for evaluating rule systems, emphasizing logical soundness, efficiency, and simplicity. In this vein, we focus on communicating the behavior of rules to knowledge engineers or end-users by employing user-centric explanations- trace-based, contextual, contrastive, and counterfactual-to concretely help with identifying inconsistencies, incompleteness, accuracy, and fairness of decision-making."}, {"title": "2.2 Debugging and Validation Techniques", "content": "In the domain of declarative programming, Gebser et al. [5] introduces a meta-programming technique for debugging answer-set programming (ASP). Their approach allows querying why a single interpretation, or class of interpretations, is not an answer set for a program. This method identifies semantical errors within the context of ASP and can be used for debugging answer-set programs. However, their work is inherently tied to the semantics of ASP and does not extend to broader rule-based reasoning systems. In contrast, our framework generalizes debugging and validation techniques through the use of a number of explanation types. By focusing on explanation-driven rule refinement rather than program-level debugging, our work moves from identifying errors within a specific programming paradigm to enhancing the quality of rules that underpin knowledge-based reasoning in general."}, {"title": "2.3 Explanation Frameworks", "content": "Explanation frameworks have emerged as critical tools for enhancing the interpretability and usability of AI systems. Explanation Ontology (EO) [3, 4] provides a general-purpose semantic framework for representing explanations, supporting 15 distinct explanation types with clear definitions and logical formalizations using Web Ontology Language (OWL). EO enables system designers to connect explanations to underlying data and reasoning processes, ensuring that Al systems address user-centered needs effectively. Practical applications of EO include contextual explanations in healthcare, such as helping clinical practitioners understand AI-driven risk predictions for Type-2 Diabetes and Chronic Kidney Disease (CKD) [2]. While EO provides a robust theoretical foundation for explainability, our framework focuses on practical implementation within the MIT App Inventor Punya platform, demonstrating its utility through real-world applications and diverse explanation types.\nSimilarly, XAIN (eXplanations for AI in Notation3) [13] supports trace-based, contrastive, and counterfactual explanations, specifically focusing on healthcare applications. For example, XAIN explains recommendations for Chronic Obstructive Pulmonary Disease (COPD) patients in order to effect understanding, persuasion, and behavior change. While XAIN showcases the power of symbolic reasoning for generating explanations, it does not address rule quality as a central theme. Our work builds upon these advancements by implementing explanations that explicitly target rule validation and debugging, and integrating these capabilities into a lightweight reasoning architecture for mobile and resource-constrained environments, as described in the following section."}, {"title": "3 Reasoning Architecture", "content": ""}, {"title": "3.1 MIT App Inventor and Punya", "content": "MIT App Inventor is a web-based, drag-and-drop platform designed to make app development accessible to users with minimal programming experience [9, 10]. Punya builds upon this foundation by introducing support for Linked Data and lightweight reasoning, enabling developers to create sophisticated applications that leverage semantic technologies [7]. By providing drag-and-drop tools and predefined components for semantic web data creation and consumption, Punya reduces the technical burden on developers, enabling even non-experts to create mobile apps with semantic features and reasoning capabilities [11]. Furthermore, Punya's advanced capabilities make it a valuable tool for researchers and professionals in domains such as life sciences, disaster response, and healthcare. For example, Punya has been used to build disaster relief apps that crowdsource reports in real time [12] and healthcare apps that enable data-driven self-management for chronic conditions [8].\nPunya's main features include:\n\u2022 Linked Data Support: Punya allows mobile apps to inter-link structured data with semantic data to form RDF triples, thereby enabling seamless integration with external Linked Data sources and enhancing data interoperability.\n\u2022 Reasoner Component: The platform includes a lightweight reasoning engine capable of operating on individual mobile devices. Built on Apache Jena's RDF reasoning framework, Punya's reasoner enables apps to process knowledge graphs and apply inference rules to derive new facts.\nIn this work, the modifications to the reasoning architecture of the Punya platform with the Explainer Component are depicted in Figure 1. It is designed to facilitate the derivation of new knowledge from input data and rules while offering explanations to end-users. It consists of three key layers: the Input Layer, the Reasoner Component, and the Output Layer, as explained below."}, {"title": "3.2 Input Layer", "content": "The Input Layer is responsible for initializing the reasoning process by supplying data inputs to the system, such as:\n\u2022 Domain Facts: Context-specific information required by the application.\n\u2022 Rules File: A collection of inference rules written in Jena's rule syntax.\n\u2022 User Input: Personalized data provided by the user.\n\u2022 External Data: Structured data sources containing auxiliary data needed for the application, such as a SPARQL endpoint or an RDF data file."}, {"title": "3.3 Reasoner Component", "content": "The Reasoner Component is the core of the architecture and is composed of two primary subcomponents:\n\u2022 Knowledge Graph: The repository of all system knowledge, consisting of two models:\nBase Model: Stores initial facts and user-provided information.\nInference Model: Extends the base model with newly reasoned facts and their derivation chains, representing the enriched knowledge state.\n\u2022 Reasoning System: Powered by an Apache Jena-based rule reasoner. This system applies inference rules to the knowledge graph to derive new facts by iteratively matching input data against rule conditions, generating and storing inferred triples along with their logical derivations in the inference model."}, {"title": "3.4 Output Layer", "content": "The Output Layer presents the results of the reasoning process in two forms:\n\u2022 Reasoning Output: Inferred facts (represented as RDF triples) derived from combining base facts and inference rules.\n\u2022 Explainer: This module extracts derivation chains from the inference model to generate human-readable explanations. The explainer offers various types of explanations, such as trace-based, contrastive, and contextual, to provide end-users with an understanding of how and why specific conclusions were reached.\nThis dual-output design ensures that both technical and lay users can access and comprehend the reasoning process and its results."}, {"title": "3.5 Explanation Component", "content": "The explanation component is a central element in the reasoning architecture, designed to transform inferred knowledge into human-readable explanations tailored to user needs. As illustrated in Figure 2, the component processes three key inputs to generate explanations: the input model, the statement (i.e., the RDF triple we want an explanation for), and the explanation type.\nInputs to the Explanation Component\n\u2022 Input Model: The input model represents the domain-specific knowledge base used in the explanation process. It provides the contextual framework within which the inferred facts are analyzed and explained.\n\u2022 Statement: The statement, in the form of an RDF triple, serves as the focal point of the explanation. It represents the inferred fact generated by the reasoning component. Users select the statement they wish to investigate, enabling the component to trace its derivation and provide an explanation of how the conclusion was reached.\n\u2022 Explanation Type: The explanation type specifies the format or approach used to present the information to the user. This module currently supports trace-based, contrastive, and contextual explanations, each tailored to different user needs and scenarios. The choice of explanation type determines the level of detail and focus in the generated output.\nFunctionality of the Explanation Component The explanation component operates by querying the inference model for relevant facts and derivations associated with the selected RDF triple. Using this data, it generates the desired explanation that aligns with the chosen explanation type, leveraging different strategies as illustrated in Section 5."}, {"title": "4 Use Case", "content": "To evaluate the utility of the proposed explanation component, we applied it to a financial decision-making use case.\nHere, a financial system assesses loan eligibility based on credit scores and debt-to-income (DTI) ratios.\n\u2022 Base Facts:\nAlex: Credit Score 680, Monthly Debt $2000, Monthly Income $5000.\nBeth: Credit Score 605, Monthly Debt $1500, Monthly Income $5000.\nCharlie: Credit Score 700, Monthly Debt $1000, Monthly Income $5000.\n\u2022 Rules:\nCalculate DTI ratio: [DTIRule]\nDetermine eligibility: [EligibilityRule], [NotEligibleDTIRule], and [NotEligibleCreditRule].\n\u2022 Inferred Facts:\nAlex: DTI 0.40 (Not Eligible - High DTI).\nBeth: DTI 0.30 (Not Eligible - Low Credit Score).\nCharlie: DTI 0.20 (Eligible - Good DTI and Credit Score).\nThe Explanation Component explains why specific applicants are eligible or ineligible, detailing the reasoning behind inferred DTI ratios and eligibility outcomes."}, {"title": "5 Explanation Types Supported", "content": ""}, {"title": "5.1 Trace-Based Explanations", "content": "Trace-based explanations provide a comprehensive account of the reasoning process by detailing the complete derivation chain for a given conclusion. This explanation type is valuable for understanding how facts trigger rules, how rules interact among themselves, and checking the correctness of inferences in knowledge-based systems. By presenting a step-by-step breakdown of the applied rules and supporting facts, trace-based explanations serve multiple purposes: (i) Detecting and diagnosing unexpected rule interactions that led to unexpected conclusions. (ii) Ensuring the consistency and accuracy of inferred facts with respect to input data and predefined rules. (iii) Verifying the integrity of complex rule chains, i.e., in scenarios involving cascading inferences.\nThe algorithm for generating trace-based explanations, presented in Algorithm 1, outlines the systematic process of identifying derivations, evaluating rule applications, and formatting the explanation for user interpretation.\nIn the loan eligibility model, a trace-based explanation can show that an applicant was denied a loan because their DTI ratio exceeded the acceptable threshold (Listing 1). The explanation traces the reasoning process from the conclusion to the supporting facts and rules applied, highlighting the interactions between the DTI ratio and loan eligibility rules."}, {"title": "5.2 Contextual Explanations", "content": "Contextual explanations offer a targeted view of the reasoning process by focusing on the immediate rule and supporting facts responsible for a specific inference. Unlike trace-based explanations, which detail the entire reasoning chain, contextual explanations emphasize only the most relevant subset of information, making them particularly effective for: (i) Streamlining the debugging process by isolating localized issues without overwhelming users with extraneous details. (ii) Facilitating the validation of individual rules by clearly identifying their direct contribution to a given conclusion. (iii) Enhancing communication with non-technical stakeholders by presenting concise, focused insights into rule behavior.\nAlgorithm 2 outlines the step-by-step process of generating contextual explanations.\nA contextual explanation can clarify why an applicant was deemed ineligible by showing that their DTI ratio (0.4) exceeded the threshold specified in the NotEligibleDTIRule (Listing 2). The shallow explanation highlights the specific rules and matched facts, while the simplified version translates these insights into user-friendly language."}, {"title": "5.3 Contrastive Explanations", "content": "Contrastive explanations outline the differences between two cases with distinct outcomes, providing a comparative lens to understand how variations in data influence reasoning processes. By juxtaposing cases with contrasting results, this explanation type allows: (i) Diagnosing subtle rule interactions or inconsistencies that lead to divergent outcomes in similar scenarios. (ii) Refining and calibrating rule thresholds to ensure fairness and consistency in different cases. (iii) Identifying data features that have a disproportionate influence on decision-making in different cases, thereby improving transparency and fairness.\nBy pinpointing the factors that contribute to differing outcomes, contrastive explanations empower knowledge engineers to refine rules and validate their robustness. The systematic procedure for generating contrastive explanations is detailed in Algorithm 3.\nIn the loan eligibility model, a contrastive explanation compares two applicants: one eligible for a loan and one ineligible. The explanation highlights differences in their DTI ratios, monthly debts, and credit scores and illustrates how these variations may interact with the rules to produce different outcomes (Listing 3)."}, {"title": "5.4 Counterfactual Explanations", "content": "Counterfactual explanations leverage \"what-if\" scenarios, offering insights into how changes to input data could result in different reasoning outcomes. This type of explanation offers actionable modifications to change the reasoning outcome of a given case. In doing so, this explanation type serves as a tool for: (i) Providing actionable guidance for improving input data to achieve desired outcomes. (ii) Adjusting rule conditions to achieve desired outcomes.\nAlgorithm 4 details the process for creating counterfactual explanations, leveraging a nearest-neighbor approach inspired by the NICE algorithm [1].\nThe counterfactual explanation (Listing 4) illustrates what minimally needs to change for an applicant to be eligible for a loan. In doing so, knowledge engineers can assess whether rules are too restrictive or lenient by comparing the outcomes of similar cases."}, {"title": "6 Integration with MIT App Inventor for Rule Testing", "content": "The explanation component has been integrated into the MIT App Inventor Punya framework to facilitate the testing and validation of rules across various domain models (Figure 3).\nThe test application consists of two primary sections that support the evaluation of rules and explanations: Model Inspection and Explanations. These sections enable users to interact with different domain models, inspect underlying rules and facts, and generate explanations for inferred conclusions.\nThe Model Inspection section allows users to explore the foundational elements of a reasoning system for three pre-loaded domain models, which currently include transitive reasoning, dietary recommendations, and loan eligibility (our running example). Users can select the domain model and the type of information they wish to inspect, including:\n\u2022 Base model facts\n\u2022 Rules applied in the reasoning process\n\u2022 Inference model facts\nOnce a model and information type are selected, users can view the output in the application's interface along with the rules and facts.\nThe Explanations section lets users test the explanation component's functionality directly. Users can select:\n\u2022 A pre-loaded domain model\n\u2022 A reasoned statement or triple (subject, predicate, object) to be explained\n\u2022 The type of explanation desired (trace-based, contextual, contrastive, or counterfactual)\nUpon selecting these parameters, the app generates an explanation by querying the inference model to describe how the selected inferred triple was created. The explanation is then displayed in the output box, providing knowledge engineers with insights into the reasoning process.\nThis integration highlights the practicality of the explanation component for real-world applications, aligning it with educational and research-oriented goals of MIT App Inventor Punya. In particular, it provides a user-friendly interface for testing and validating rules in different domain models by enabling:\n\u2022 Rule debugging and validation through detailed model inspection.\n\u2022 Evaluation of different explanation types for various reasoning scenarios.\n\u2022 Accessibility of rule-based reasoning and explanation capabilities on mobile platforms."}, {"title": "7 Discussion", "content": "The use case in Section 4 highlights the practical utility of the explanation component in enhancing rule quality. (i) Trace-Based: Enables a comprehensive understanding of reasoning chains, facilitating thorough validation and debugging of rule interactions. (ii) Contextual: Enhances human readability by isolating the immediate rules and facts that led to a specific conclusion, providing a focused view of the reasoning process. (iii) Contrastive: Illuminates critical differences between cases, enabling engineers to refine thresholds, address inconsistencies, and improve fairness. (iv) Counterfactual: Explores \"what-if\" scenarios to test rule sensitivity and suggest actionable improvements, aligning outcomes with user goals.\nBy integrating these perspectives, the framework empowers knowledge engineers to systematically debug, validate, and refine rules. This iterative process strengthens the reliability and interpretability of the reasoning system, allowing its rules to serve as a robust foundation for the Web of Data. To the best of our knowledge, this paper is among the first to investigate the use of diverse explanation types as a tool for refining rules in knowledge-based reasoning systems. This novel approach shifts the focus from traditional computational correctness checks to explanation-driven, human-centric rule validation.\nRegarding the distinction between trace-based and contextual explanations: while both contribute to understanding the reasoning process, contextual explanations can be viewed as a targeted application of trace-based methods. By focusing solely on the final stage of an inference chain, contextual explanations enhance human readability without overwhelming users with details.\nBeyond debugging, this explanation-driven framework has educational potential. By providing clear and structured insights into reasoning processes, the framework can be a tool for teaching rule crafting and knowledge graph development. Moreover, domain experts can utilize these tools to evaluate the accuracy and comprehensiveness of the rules they author. Aligning rule quality assessments with intrinsic quality dimensions (e.g., accuracy, consistency) and contextual \"fit for use\" requirements promotes responsible Al governance and ensures alignment with real-world standards of data quality.\nThe framework has limitations that suggest avenues for future research. Currently, it supports four explanation types derived from the explanation ontology [4]. Expanding the framework to incorporate additional explanation types from the ontology could enhance its applicability across diverse scenarios. Another area for improvement is the natural language output of explanations. Simplifying and refining these outputs would make explanations more accessible, particularly for non-technical users, who are the target audience of MIT App Inventor. Future work could also involve conducting user studies to evaluate the impact of different explanation types on usability, trust, and decision-making in real-world applications. Such studies would provide valuable feedback for refining the framework and its integration into scalable reasoning systems. These advancements could further establish explainability as a cornerstone for responsible, high-quality reasoning systems."}, {"title": "8 Conclusion", "content": "As the Web of Data continues to expand, we argue that the interconnection between data quality and explainability is a foundational aspect of building reliable and trustworthy knowledge-based systems. Rules are the cornerstone of reasoning systems, shaping the usability, fairness, and reliability of their derived insights. Addressing these challenges, this paper introduces an explanation framework that integrates diverse explanation types, including trace-based, contextual, contrastive, and counterfactual, offering a multifaceted toolkit for debugging, refining, and validating rules.\nBy incorporating the Explainer Component into the MIT App Inventor Punya framework, we aim to demonstrate the practical value of explanation-driven approaches in bridging the gap between complex reasoning systems and user-centric transparency.\nThis framework elevates the quality of rules and lays a foundation for responsible, data-driven decision-making in scenarios where high-quality, robust reasoning is paramount. Future research could extend the framework by incorporating additional explanation types, improving natural language outputs for greater accessibility, and conducting empirical studies to assess the impact of explanations on system usability, trust, and decision-making. These advancements may validate explainability as a cornerstone of transparent, high-quality, and trustworthy knowledge-based reasoning systems, advancing the vision of a responsible and reliable Web of Data."}]}