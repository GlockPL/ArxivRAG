{"title": "Kolmogorov-Arnold recurrent network for short term load forecasting across diverse consumers", "authors": ["Muhammad Umair Danish", "Katarina Grolinger"], "abstract": "Load forecasting plays a crucial role in energy management, directly impacting grid stability, operational efficiency, cost reduction, and environmental sustainability. Traditional Vanilla Recurrent Neural Networks (RNNs) face issues such as vanishing and exploding gradients, whereas sophisticated RNNs such as Long Short-Term Memory Networks (LSTMs) have shown considerable success in this domain. However, these models often struggle to accurately capture complex and sudden variations in energy consumption, and their applicability is typically limited to specific consumer types, such as offices or schools. To address these challenges, this paper proposes the Kolmogorov-Arnold Recurrent Network (KARN), a novel load forecasting approach that combines the flexibility of Kolmogorov-Arnold Networks with RNN's temporal modeling capabilities. KARN utilizes learnable temporal spline functions and edge-based activations to better model non-linear relationships in load data, making it adaptable across a diverse range of consumer types. The proposed KARN model was rigorously evaluated on a variety of real-world datasets, including student residences, detached homes, a home with electric vehicle charging, a townhouse, and industrial buildings. Across all these consumer categories, KARN consistently outperformed traditional Vanilla RNNs, while it surpassed LSTM and Gated Recurrent Units (GRUs) in six buildings. The results demonstrate KARN's superior accuracy and applicability, making it a promising tool for enhancing load forecasting in diverse energy management scenarios.", "sections": [{"title": "1. Introduction", "content": "The global electricity demand is projected to grow by 4% in 2025, up from 2.5% in 2023, with renewable sources expected to contribute 35% of the total share (Business Standard News, 2024). This would represent the highest annual growth since 2007. Accurate energy load forecasting not only aids in energy management and delivery but also contributes to financial savings and achieving climate goals (Business Standard News, 2024). For example, improving energy efficiency alone could save U.S. consumers $450 billion in electric bills by 2030 (Business Standard News, 2024). Moreover, accurate load forecasting is instrumental in reaching climate goals, such as the European Green Deal's target to reduce carbon emissions by 55% and improve energy efficiency by 32.5% by 2030 (European Climate, Energy and Environment, 2024). This emphasizes the indispensable role of accurate load forecasting for both economic and environmental sustainability.\nGiven the important role of load forecasting in ensuring the stability and reliability of energy supply, short-term load forecasting has evolved as a pivotal area of focus in both research and practical applications (Deng et al., 2021). Short term load forecasting enables utilities to promptly respond to fluctuations in demand, which is vital for maintaining grid stability and operational efficiency. Numerous artificial intelligence (AI) and Machine Learning (ML) technologies (Muzaffar and Afshari, 2019; Yao et al., 2022; Skala et al., 2023; Qin et al., 2022) have been used to improve load forecasting accuracy. However, despite the advancements brought by these technologies, accurately forecasting load in diverse urban settings remains a challenge due to factors such as consumer usage behavior and meteorological shifts (Chen et al., 2021). Among various AI technologies, Deep Learning (DL) models, including Recurrent Neural Networks (RNNs), Long Short-Term Memory Networks (LSTMs), Gated Recurrent Units (GRUs), and Transformers, have been recognized for their ability to capture long-range temporal dependencies (L'Heureux et al., 2022).\nVanilla RNNs were the earliest neural networks designed to handle sequential data, but they have challenges such as vanishing and exploding gradients and difficulties in learning long-term dependencies. These challenges led to the development of more sophisticated recurrent architectures, such as LSTM networks and GRUs (Yu et al., 2019). These recurrent DL models share foundational principles with Multi-Layer Perceptrons (MLPs), such as the use of fully connected linear weight matrices and fixed non-linear activation functions (such as Sigmoid or Tanh) at the nodes or neurons, enabling them to approximate complex functions (Li et al., 2021). This structure imposes a fixed non-linearity by using predetermined and unchanging activation functions that provide a constant level of non-linearity, which is not always sufficient to capture the complex relationships in load patterns for diverse consumer types, given their variable nature of energy usage influenced by sudden events, lifestyle changes, and meteorological shifts (Liu et al., 2024; Cai et al., 2019). Moreover, these models have been largely confined to specific consumer categories, such as residences (Gong et al., 2021), apartments (Rezaei and Dagdougui, 2020), detached houses (Kong et al., 2017), townhouses (Gong et al., 2021), and homes with electric vehicles (Zhang et al., 2020). However, a comprehensive assessment across diverse consumer settings is necessary, as consumer behavior varies by building type. This highlights the need for studies that extend analyses to a broader range of consumer types to ensure the robustness of forecasting models.\nRecently, a new AI technology, the Kolmogorov-Arnold Network (KAN), has been introduced as an alternative to MLPs (Liu et al., 2024). Unlike MLPs, KANs have learnable activations on edges or weights instead of predefined activations on nodes or neurons. This means that activations in KANs are not applied over the weights and biases but are instead applied separately in a learnable manner on the edges. KANs do not have linear weights at all; instead, every weight matrix is replaced by a univariate function parameterized as a spline, making them less likely to face vanishing and exploding gradients (Liu et al., 2024). These univariate functions are learnable and provide a more flexible and potentially more accurate way of representing relationships in the data (Liu et al., 2024). The current design of KAN (Liu et al., 2024) is not well-suited for handling time series load data as they were designed for tabular data without considering the time component. This assumption makes KANs less suitable for temporal data because they lacks the ability to capture long-term dependencies, which is essential for accurate time series load forecasting.\nTo take advantage of KANs for load forecasting, this paper proposes Kolmogorov-Arnold Recurrent Network (KARN), a temporal version of KAN that combines the flexibility of learnable activations on edges with the ability of RNNs to capture long-term dependencies. This allows for greater adaptability to varying consumer behaviors and external influences, without the risk of vanishing or exploding gradients. This paper makes the following main contributions:\n1.  Design of KARN, which is based on the Kolmogorov-Arnold Representation Theorem and incorporates a recurrent structure to retain the memory of previous states, with an emphasis on improving the accuracy of load forecasting across diverse consumer types.\n2.  Design of the temporal basis and temporal spline functions to enhance KARN's ability to capture long-range dependencies.\n3.  Comprehensive analysis of KARN across diverse consumer types, including residences, detached houses, townhouses, houses with electric vehicles, commercial and industrial datasets, and comparison with traditional recurrent neural networks.\n\nThe remainder of this paper is organized as follows: Section 2 presents the related work, Section 3 details the architecture of KARN and the mathematical foundations underlying their design. Section 4 describes the experimental setup, including the datasets used and the evaluation metrics. Section 5 discusses the results and compares the performance of KARN with other state-of-the-art models. Finally, Section 6 concludes the paper and suggests directions for future research."}, {"title": "2. Related work", "content": "This section provides an overview of the notable advancements in load forecasting followed by a discussion on applications of KANs."}, {"title": "2.1. Load forecasting with deep learning", "content": "To enhance load forecasting accuracy, various DL approaches have been proposed and extensively studied. The most notable models are Vanilla RNNs, LSTM networks, GRUs, and Transformers (Zeng et al., 2023). Vanilla RNN was the earliest model designed for handling sequential and time series data, but it faces several challenges, including vanishing gradients, exploding gradients, gradient instability during backpropagation through time, difficulty in retaining memory of long past time steps and overfitting, making it difficult to capture complex, long-range dependencies (Yu et al., 2019). These issues in Vanilla RNNs led to the development of LSTM and GRU, which utilize multiple recurrent gates to capture memory over longer time steps. LSTM and GRU, along with their hybrid and enhanced variants, are currently at the forefront of research in the field of load forecasting (Skala et al., 2023; Bessani et al., 2020; Wu and Peng, 2017; Zhang and Chiang, 2019; Yamak et al., 2019). For instance, Skala et al. (2023) introduced LSTM Bayesian neural networks for interval load forecasting in the context of individual households with electric vehicle charging by demonstrating significant improvements in forecasting precision. Bessani et al. (2020) proposed Bayesian neural networks for individual houses and demonstrated that the proposed method outperforms traditional neural networks.\nWu and Peng (2017) applied k-means clustering and bagging techniques with neural networks to enhance the accuracy of short-term forecasting for wind turbines. He et al. (2024) proposed a hybrid model consisting of Holt-Winters and GRU for short-term load-interval forecasting using the 2022 Teddy Cup dataset and demonstrated performance improvements over traditional models. Ahmed and Jamil (2024) deployed vanilla RNN, LSTM, GRU, and their bi-directional variants on a university campus building electricity load dataset, finding that the bi-directional GRU outperformed others for this specific dataset. Wei et al. (2024) proposed a transfer domain selection algorithm that integrates the Wasserstein distance and maximal information coefficient and validated it on a public office dataset based in Greece. Among RNNs, LSTM has been particularly successful in load forecasting due to its gating mechanisms, which allow it to capture long-term dependencies (Yamak et al., 2019). This capability is crucial in short-term load forecasting, where sudden fluctuations in power demand are common.\nTransformers and their advanced variants have enhanced prediction capabilities through self-attention mechanisms that effectively handle complex data patterns (Vaswani et al., 2017). L'Heureux et al. (2022) proposed a transformer-based architecture and evaluated it on an open-source dataset containing data from 20 zones of a U.S. utility company. The findings showed that the transformer outperformed both LSTM and sequence-to-sequence models in terms of predictive accuracy. Mao et al. (2024) proposed a hybrid model combining GRU and Transformer architectures, specifically designed for a dataset from Panama City. The study showed that this hybrid approach delivered performance improvements on certain datasets. Giacomazzi et al. (2023) employed the Temporal Fusion Transformer (TFT) for load forecasting using the Hanoi city datasets and found that the newly developed TFT outperformed both LSTMs and traditional Transformers in this context. Kong et al. (2017) proposed the Time Augmented Transformer (TAT), a hybrid variant of the transformer model. In TAT, a time augmentation module is introduced, which uses one-hot encoding followed by fully connected layers. This model was applied to real-world residential buildings and demonstrated superior performance compared to seq2seq and LSTM models.\nThe evolution of hybrid models, which integrate different approaches, has become increasingly prominent in the field of load forecasting (Li et al., 2023a,b; Triebe et al., 2019; Danish and Grolinger, 2024). Tan et al. (2024) proposed an innovative hybrid model called InE-BiLSTM, which combines an Informer Encoder with bidirectional LSTM. This model was deployed for two public building datasets and demonstrated significant improvements in performance, albeit with an increase in computational cost. E-ELITE neural network proposed by Zhang and Chiang (2019) for short-term load forecasting for industrial applications using the ISO New England dataset, achieved superior performance compared to traditional models.\nThe aforementioned DL models are grounded in the Universal Approximation Theorem, which involves linear weights combined with non-linear activations applied to neurons. While effective for many tasks, this design can make their applicability challenging for certain load datasets, particularly those with high variability and unpredictability due to consumer usage patterns. Additionally, these aforementioned DL-based studies have achieved significant success in load forecasting, but their accuracy has not been extensively assessed or validated across various consumer types. This limitation arises from the distinct electricity usage patterns among different consumers, which can introduce spikes, drops, concept drifts, and level shifts in the data. Such variability can reduce forecasting performance, especially in datasets where randomness is driven by consumer behavior. This gap raises an important question: Can a single forecasting model effectively adapt to and capture the diverse energy consumption patterns demonstrated across various consumer groups?\nTo address these challenges, our study proposes KARN which is designed to model complex patterns by leveraging the flexibility of KANs with the temporal modeling capabilities of recurrent structures. KARN overcomes the limitations of traditional DL models by utilizing learnable activation functions on edges, rather than nodes, allowing for a more adaptable and precise representation of relationships in the data. Unlike traditional RNNs, KARN's structure inherently mitigates issues such as vanishing and exploding gradients, as the spline-based activations on edges offer smoother gradient flows and more robust learning dynamics. This design is particularly well-suited to capturing the diverse and dynamic energy consumption patterns across various consumer types, leading to superior performance compared to traditional RNNs."}, {"title": "2.2. Kolmogorov-Arnold networks", "content": "KANs have emerged as a promising alternative to traditional MLP (Liu et al., 2024). Unlike MLPs, which are based on the Universal Approximation Theorem, KANs are inspired by the Kolmogorov-Arnold representation theorem. This key difference allows KANs to replace linear weights with univariate functions parameterized as splines, enabling the placement of learnable activations on edges or weights rather than on nodes or neurons. The ability of KANs to adapt to complex data patterns without relying on fixed activation functions has positioned them as a valuable tool in scientific computing and data analysis (Vaca-Rubio et al., 2024; Genet and Inzirillo, 2024b,a). This design provides KANs with a greater number of learnable parameters, which can lead to improved performance at the cost of increased computational complexity.\nSince Liu et al. (2024) introduced KANs, their application has been increasingly explored across various fields, demonstrating advantages over conventional ML technologies. For example, Vaca-Rubio et al. (2024) investigated the use of KANs for time series traffic forecasting and compared their performance with that of MLPs across different layer configurations. The study found KANs to be more powerful predictors; however, this implementation used standard KANs and MLPs without modifying KANs to accept temporal inputs, which can negatively impact the ability of KANs to capture the temporal nature of the data, potentially limiting their forecasting accuracy in time-dependent tasks.\nGenet and Inzirillo (2024b) introduced Temporal KAN (TKAN) for time series forecasting on Binance cryptocurrency data, aiming to develop a model comparable to LSTM. The adaptation of KANs for temporal data represents a step forward; however, this strategy incorporated linear weights and fully connected layers, which deviates somewhat from the original KAN architecture that emphasizes non-linear spline-based weight matrices. This integration dilutes some of the distinctive advantages that KANs offer through their non-linear spline-based weights. It may be more appropriate to consider this model as a hybrid rather than a direct equivalent of an LSTM based purely on KAN principles, as it introduces elements beyond the traditional KAN framework. Similarly, Genet and Inzirillo (2024a) also proposed the Temporal Kolmogorov-Arnold Transformer (TKAT), which is analogous to the Temporal Fusion Transformer but uses TKANs in its implementation. This can also be seen as a hybrid model since it once again incorporates linear weights from MLPs, thereby not fully exploring the potential accuracy offered by non-linear spline-based weights alone. Xu et al. (2024) proposed MT-KAN for financial time series datasets, focusing specifically on interpretability by incorporating symbolic regression. Their findings demonstrated that KANs were more interpretable compared to traditional AI technologies and also outperformed MLPs in terms of accuracy (Zhang et al., 2022).\nKANs have also been explored across various domains beyond time series data. Azam and Akhtar (2024) evaluated the applicability of KANs to computer vision tasks, identifying challenges in their effectiveness for image recognition. Cheon (2024) utilized KANs for satellite image classification and reported slightly better performance compared to traditional MLPs. Kiamari et al. (2024) introduced GKAN (Graph KAN) as a substitute for Graph Convolutional Neural Networks, achieving improved accuracy at the cost of increased computational requirements. Tang et al. (2024) developed U-KAN, a variant analogous to the traditional U-Net, for brain image segmentation, and observed marginally better performance compared to U-Net. Aghaei (2024) proposed Rational KAN (rKAN) and evaluated the performance of rKAN against various DL and physics-informed deep learning models. The results showed that rKAN outperformed these models.\nDespite the diverse applications of KANs across various domains, their use in load forecasting remains unexplored. While the existing literature has demonstrated the effectiveness of KANs in tasks such as time series forecasting, computer vision, and graph-based modeling, the KANs potential in load forecasting for diverse consumer types has not been investigated. This gap underscores the need to explore how KANs can be adapted and applied to the specific challenges of energy load forecasting.\nTo address the limitations, our study designs a recurrent variant of KAN, aimed at enhancing load forecasting accuracy. Unlike traditional RNNs, which are prone to vanishing and exploding gradients, this study proposes KARN which leverages the inherent flexibility of basis splines and learnable independent activations on edges. This learning mechanism minimizes the risk of gradient issues, as activations are not applied directly to weights, biases, or spline coefficients. By integrating the strengths of KANs with a recurrent structure, KARN effectively captures temporal dependencies in load data while ensuring more robust learning dynamics. This innovative approach positions KARN as a pioneering solution in the field of load forecasting for diverse consumer types."}, {"title": "3. Kolmogorov-Arnold recurrent network", "content": "This section presents the Kolmogorov-Arnold Recurrent Network (KARN), our proposed method for load forecasting as illustrated in Fig. 1, designed to accommodate a wide range of energy consumers. KARN extends KAN to handle time series energy data. It is based on the Kolmogorov-Arnold representation theorem, which offers a method to represent multivariate continuous functions using a finite composition of univariate functions, which is achieved by placing learnable activation functions on weights, parameterized as splines. The KARN is designed to process the current input $x(t)$ at time step t and keep the memory of the previous time steps. This allows KARN to calculate the next hidden state and make forecasts. Several learnable weight parameters additionally aid the temporal design. This temporal design allows KARN to approximate functions in a recurrent manner while retaining the advantages of KANs, such as non-linear weight metrics and activations on weights through the use of basis and spline functions. These features render KARN highly suitable for load forecasting across a diverse set of consumers.\nIn our case of load forecasting, the input tensor is $x \\in \\mathbb{R}^{m\\times f \\times n}$, where $m$ represents the number of samples, $f$ denotes the number of features, and $n$ indicates the number of time steps within each sample. These time steps are generated using the sliding window technique (L'Heureux et al., 2022), which creates sequence segments from the time series energy data. The features $f$ typically include variables such as temperature, day of the year, day of the month, day of the week, hour of the day, and energy consumption from the preceding $n$ hours. The following subsections will describe the temporal basis function, temporal spline function, memory mechanism in KARN, grid extension, and loss function."}, {"title": "3.1. Temporal basis function", "content": "The temporal basis function introduces non-linearity into the model and is chosen to provide a simple, smooth, and well-behaved nonlinear transformation of the input. In the standard KAN variant (Liu et al., 2024), the SiLU (Sigmoid Linear Unit) function is employed. The basis function can be viewed as a residual connection that helps stabilize the learning process. The SiLU function is defined as:\n$\\text{SiLU}(x) = \\frac{x}{1 + e^{-x}}$\nThis function is used to improve gradient flow and learning efficiency in temporal load data. The temporal basis function takes input data $x \\in \\mathbb{R}^{m\\times f \\times n}$ over time $t$, applies the SiLU function to transform each time step, enabling the model to extract meaningful patterns from the temporal structure of the input. The transformation at each time step $t$ is given by:\n$b(t) = w_{b(t)} \\text{SiLU}(x(t))$\nwhere $w_{b(t)}$ are learnable weights corresponding to the input at time $t$ that allow the model to dynamically adapt to temporal data patterns. The temporal basis function ensures that the model can capture nonlinear transformations that evolve over time by enabling it to process time-varying inputs."}, {"title": "3.2. Temporal spline function", "content": "The temporal spline function is a piecewise polynomial function that models complex, non-linear relationships by transforming input data using a series of B-spline functions. Each spline or piecewise polynomial function is defined over a specific grid interval. A grid interval, also known as a knot, refers to the set of points, where the spline functions are evaluated and applied. These grid points divide the input space into segments where spline functions change or adapt, allowing the model to capture the non-linear relationships within data. The positioning and number of these grid points can affect the accuracy of the spline function approximation as they determine where and how the splines will adjust to the underlying data patterns (De Boor, 1972; Liu et al., 2024). The degree of polynomial in a spline is the highest power of the variable in each polynomial segment between the grid points.\nThe temporal spline function extends the spline to operate in recurrent manners. These basis splines take input as $x(t)$ over time $t$ and apply a piecewise polynomial transformation of a degree $p$, defined over grid points G that divide inputs space. The transformation is controlled by learnable coefficients $c$ to capture underlying patterns in the data. The learnable coefficients allow for the dynamic adjustment of the influence of each spline function at time step $t$, enabling the model to capture long-term dependencies in the data. The output of these spline functions is then aggregated to form a complete representation. The transformation at each time step $t$ is given by:\n$S(t) = w_{s(t)} \\cdot \\Sigma c_i \\cdot B_i(x(t))$\nIn this expression, B represents the ith B-spline basis function, c are the learnable coefficients that adjust the contribution of each spline, and $w_{s(t)}$ are the learnable weights applied to the aggregated output of the spline functions. This means that the temporal spline function in KARN has two sets of learnable parameters: weights that control the effects after aggregation and coefficients that learn patterns before aggregation to provide a fine-tuned learning mechanism. The temporal spline function ensures that the model can capture evolving, non-linear relationships in the load data by dynamically adapting the splines to changing input patterns over time."}, {"title": "3.3. Memory mechanism in KARN", "content": "The previous two subsections explained how the basic components are extended to handle temporal data. This subsection explains how to combine both components to formulate the KARN layer. The KARN layer is the combination of both the temporal basis function and the temporal spline function, along with the contribution from the previous hidden state. The hidden state at each time step h(t) is calculated by combining the outputs of the temporal basis function, the temporal spline function, and the hidden state from the previous time step, along with learnable weight metrics and a bias term:\n$h(t) = W_{h(t)} h(t-1)+b(t) + S(t) + bias$\nIn the above equation, $W_{h(t)}$ is a learnable weight matrix that helps retain information from the previous hidden state $h_{(t-1)}$ to the current state $h(t)$. The term $b(t)$ represents the output of the temporal basis function applied to the input at time step t, while S(t) is the output of the temporal spline function applied to the same input. The bias term is a learnable parameter that adjusts the final hidden state. This recurrent mechanism allows the model to keep memory from previous time steps while processing the current input, enabling KARN to capture both short-term dependencies (through x(t)) and long-term dependencies (through h(t-1), h(t-2), ...). At each time step t, the hidden state h(t) is updated based on both the current input x(t) and the hidden state from the previous time step h(t-1), allowing the model to propagate information forward through time and dynamically adapt to new inputs, as illustrated in Fig. 1.\nThe model generates an output at each time step based on the hidden state at that time. The output y(t) is computed by applying learnable weights to the hidden state h(t):\n$y(t) = W_{(hy)} h_t + bias_{(y)}$\nHere $W_{(hy)}$ is a learnable weight matrix that maps the hidden state to the output space while the bias term $bias_{(y)}$ adjusts the output. This ensures that the model can generate an output at each time step, leveraging the information stored in the hidden state, which has been influenced by both the current and previous inputs. Similar to the standard KAN (Liu et al., 2024), weight sharing is also utilized, which allows multiple spline functions to use the same set of parameters, reducing the total number of learnable parameters and enhancing the model's abilities. Additionally, a locking mechanism is in place that manages and enforces parameter sharing across spline functions, ensuring that once parameters are shared, they remain consistently applied throughout the training process."}, {"title": "3.4. Grid extension", "content": "The B-spline $B_i(x)$ of degree p is defined recursively over intervals known as knots. The first step in applying B-splines is to define a sequence of grid points which divide the domain into intervals. The grid is initialized based on a specified range (e.g., [-1,1]) and is divided into intervals according to the number of grid points. Within each interval, the B-spline acts as a piecewise polynomial of a degree p, making B-splines well-suited for use in neural networks. For a given set of knots {$\\tau_0, \\tau_1, ..., \\tau_n$}, the B-spline of degree p is defined as:\n$B_i^p(x) = \\frac{x - \\tau_i}{\\tau_{i+p} - \\tau_i} B_i^{p-1}(x) + \\frac{\\tau_{i+p+1} - x}{\\tau_{i+p+1} - \\tau_{i+1}} B_{i+1}^{p-1}(x)$\nIn this equation, $B_i^0(x)$ equals 1 if $\\tau_i \\leq x < \\tau_{i+1}$, and 0 otherwise. The first term $\\frac{x - \\tau_i}{\\tau_{i+p} - \\tau_i} B_i^{p-1}(x)$ handles the contribution of the ith basis function over its interval, while the second term addresses the contribution of the previous basis function.\nGrid extension is a technique that adds more points to the grid or knots. Since splines are defined on these grid points, increasing the number of points will result in more splines and each spline has an associated learnable coefficient $c_i$. This means that as the grid is extended, the number of learnable coefficients $c_i$ and splines increases, ultimately enhancing the model's ability to learn complex data patterns. The process of grid extension allows KARN trained with fewer parameters (a coarser grid) to be expanded to a model with more parameters (a finer grid) without the need to retrain the model from scratch. Consider the task of approximating a function f(x) over a bounded interval [a, b] using B-splines of order p. Initially, the approximation is constructed on a coarse grid with $G_1$ intervals, defined by grid points {$\\tau_0 = a, \\tau_1, ..., \\tau_{G_1} = b$}. The function is then expressed as a linear combination of the corresponding B-spline basis functions:\n$f_{\\text{coarse}}(x) = \\sum_{i=0}^{G_1+p-1} c_i \\cdot B_i(x)$\nTo enhance the accuracy of the approximation, a finer grid with $G_2$ intervals is used. The function is subsequently re-expressed on this refined grid as:\n$f_{\\text{fine}}(x) = \\sum_{j=0}^{G_2+p-1} c'_j \\cdot B'_j(x)$\nThe new coefficients $c'$ associated with the fine grid are obtained by minimizing the discrepancy between $f_{\\text{fine}}(x)$ and $f_{\\text{coarse}}(x)$ over the distribution of x:\n${{c'_j}} = \\arg \\min_{{{c'_j}}} E_{x\\sim p(x)} \\left[ \\sum_{j=0}^{G_2+p-1} c'_j \\cdot B'_j(x) - \\sum_{i=0}^{G_1+p-1} c_i \\cdot B_i(x) \\right]^2$\nThis optimization can be effectively solved using a least squares algorithm, allowing the grid to be extended without necessitating the retraining of the model (Liu et al., 2024)."}, {"title": "3.5. Loss function", "content": "The Mean Squared Error (MSE) or Mean Absolute Error (MAE) loss functions, determined through hyperparameter optimization, are used to quantify the difference between the predicted and actual target values. These are calculated as:\n$L_{MSE} = \\frac{1}{m} \\sum_{i=1}^{m} (y_i - \\hat{y}_i)^2$\n$L_{MAE} = \\frac{1}{m} \\sum_{i=1}^{m} |y_i - \\hat{y}_i|$\nwhere $\\hat{y}$ represents the predicted output, $y_i$ denotes the actual target value, and m is the number of samples. It is important to note that MAE is more robust to outliers than MSE because MSE squares the errors, amplifying the impact of outliers. However, MSE provides a smoother gradient for optimization, facilitating more controlled updates during training. Consequently, the choice between the two is made during the hyperparameter optimization process."}, {"title": "4. Evaluation", "content": "This section describes the datasets, preprocessing steps, performance metrics, hyperparameter optimization, and the architectures included in the comparison."}, {"title": "4.1. Datasets and preprocessing", "content": "The evaluation employed ten distinct real-world datasets from three primary consumer groups: student residences, individual homes, and industrial and commercial buildings.  provides an overview of these datasets, including the time frames during which data were collected and a brief description of each. There are significant differences between the building's energy consumption. For example, Residence 1 has suite-style accommodation with a shared kitchen, while Residence 2 follows a suite-style but lacks a kitchen. Both residences accommodate over 400 students.\nAlthough all the homes are located in London, Ontario, Canada, there is considerable diversity among them. Homes 1, 2, and 3 are detached properties, but Home 3 is distinct due to the presence of an electric vehicle charging, resulting in notable load fluctuations caused by at-home charging. Home 4 is a 3-bedroom townhouse, where energy consumption patterns differ from detached homes due to the influence of neighboring units.\nEach dataset has a date/time and hourly energy consumption. From the date/time, additional features were extracted including the day of the year, the day of the month, the day of the week, and the hour of the day to assist in modeling seasonal, weekly, and daily patterns. To capture weather patterns, temperature data was added and additional relevant features can be integrated if available. To minimize the impact of large-scale features and enhance convergence, Min-Max scaling was applied.\nThe datasets were split into training, validation, and test sets in a 60%-20%-20% ratio. Temporal data for the models was prepared using a sliding window technique with a window length of 24 and a stride of 1. All models were provided with the previous 24 h of five features, including energy load, and were tasked with predicting the next 24 h of energy load. This forecasting length was selected because energy operations commonly rely on next-day forecasts for energy planning."}, {"title": "4.2. Evaluation metrics", "content": "The evaluation was conducted using three metrics: Mean Absolute Error (MAE), Root Mean Square Error (RMSE), and Symmetric Mean Absolute Percentage Error (SMAPE). These metrics were selected to provide a comprehensive assessment of the model's performance from different perspectives.\nMAE measures the average absolute difference between predicted and actual values, which is a straightforward interpretation of the error in forecasting. MAE is useful for understanding the general accuracy of the model across the entire dataset (Fekri et al., 2023). MAE is calculated as:\n$\\text{MAE} = \\frac{1}{m} \\sum_{i=1}^{m} |y_i - \\hat{y}_i|$\nwhere $y_i$ represents the actual load values, $\\hat{y}_i$ represents the predicted load values, and m is the number of samples.\nRMSE is more sensitive to large errors than MAE due to the squaring of the residuals before averaging, making it a valuable metric when large errors are undesirable (Fekri et al., 2023; L'Heureux et al., 2022). RMSE is calculated as:\n$\\text{RMSE} = \\sqrt{\\frac{1}{m} \\sum_{i=1}^{m} (\\hat{y}_i - y_i)^2}$\nSMAPE expresses the forecasting error as a percentage, facilitating easy interpretation and enabling comparison across different datasets. SMAPE was selected over Mean Absolute Percentage Error (MAPE) because MAPE is biased toward large values and becomes undefined when actual values are zero. SMAPE addresses these issues by symmetrizing the error calculation, providing a more robust and interpretable metric in cases where the data include zero or near-zero values. SMAPE metric is calculated as:\n$\\text{SMAPE} = 100\\% \\times \\frac{1}{m} \\sum_{i=1}^{m} \\frac{2 |y_i - \\hat{y}_i|}{|y_i| + |\\hat{y}_i|}$"}, {"title": "4.3. Forecasting techniques included in comparison and hyperparameter optimization", "content": "Since KARN is essentially a recurrent extension of the basic KAN", "RNNs": "Vanilla RNN, GRU, and LSTM to fairly assess its impact. The forecasting techniques for comparisons are given along with their architectural details in Table 2.\nTo ensure a fair comparison among all models, including our own, hyperparameter optimization was conducted using a grid search for each dataset and model. The hyperparameter search space is shown in Table 3.\nFor LSTM, GRU, and RNN models, we found that one recurrent layer was optimal, with most datasets converging at 64 units. However, the townhouse and office datasets converged at 128 units. Stochastic Gradient Descent (SGD) was the preferred optimizer in most cases, though for House 1, Adam proved to be the most effective. In our implementation of KARN, we found that a spline degree of 2 was optimal, and grid extension was applied only in the case of individual houses. Specifically, for House 3, the grid"}]}