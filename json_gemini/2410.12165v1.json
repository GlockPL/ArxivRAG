{"title": "Dual-Model Distillation for Efficient Action Classification with Hybrid Edge-Cloud Solution", "authors": ["Timothy Wei", "Hsien Xin Peng", "Elaine Xu", "Bryan Zhao", "Lei Ding", "Diji Yang"], "abstract": "As Artificial Intelligence models, such as Large Video-Language models (VLMs), grow in size, their deployment in real-world applications becomes increasingly challenging due to hardware limitations and computational costs. To address this, we design a hybrid edge-cloud solution that leverages the efficiency of smaller models for local processing while deferring to larger, more accurate cloud-based models when necessary. Specifically, we propose a novel unsupervised data generation method, Dual-Model Distillation (DMD), to train a lightweight switcher model that can predict when the edge model's output is uncertain and selectively offload inference to the large model in the cloud. Experimental results on the action classification task show that our framework not only requires less computational overhead, but also improves accuracy compared to using a large model alone. Our framework provides a scalable and adaptable solution for action classification in resource-constrained environments, with potential applications beyond healthcare. Noteworthy, while DMD-generated data is used for optimizing performance and resource usage in our pipeline, we expect the concept of DMD to further support future research on knowledge alignment across multiple models.", "sections": [{"title": "1 Introduction", "content": "Recent advances in Artificial Intelligence (AI) have shown that increasing model size leads to substantial performance gains across various domains, from language models [Dubey et al., 2024, Jiang et al., 2023] to video-language models [Lin et al., 2023, Wu et al.]. Large-scale models, with billions of parameters, consistently achieve state-of-the-art results on various benchmarks [Zhao et al., 2023]. However, these models can be challenging to deploy when translating benchmark performance into real-world applications [Minaee et al., 2024] due to their high runtime costs.\nOne critical area where this limitation becomes evident is in action classification for elderly care [Sawik et al., 2023]. A task such as predicting falls through non-intrusive monitoring [Maldonado-Bascon et al., 2019, Elwaly et al., 2024] is a critical yet complex task that benefits from large-scale"}, {"title": "2 Methodology", "content": "As shown in Figure 1, our hybrid edge-cloud solution introduces a companion switcher model using the hidden features of the input data extracted from the small model, which serves as the foundation of its reasoning capacity. With the help of the switcher model, our edge model can not only predict the falling probability but also accurately estimate the probability that the large model's result aligns with its own. By incorporating this switcher model, the edge model deliberates when to invoke the cloud model, thereby keeping system expenses under a pre-defined budget. In our case, a threshold of alignment probability, which yields optimal performance in predicting the falling probability during training, determines when to offload data to the large model during inference time. Regarding more complex decision-making scenarios, we expect that a more sophisticated utility-based threshold design Wang and Zhang [2011] can also be applied in the future.\nDual-Model Distillation To give the switcher model an understanding of the capabilities of both models, we propose a novel training data generation method. First, we pass all the raw data through the small model and extract its hidden representation together with its prediction. Then, we trigger another run-through using the large model to generate the ground truth of alignment, or agreement, between the predictions of the small and large models. It is important to underscore the generalizability of this method, as for any of two deep learning models with the same I/O format, the DMD process can generate the above-described data from any raw data with only the required initial input and"}, {"title": "3 Experiments", "content": "We conducted the experiments on the Fall Detection - Eldercare Robot [Elwaly and Abdellatif, 2023] dataset, with more details provided in Appendix A.1. We used a Vision-and-Language Transformer (ViLT) Kim et al. [2021] as the small model, chosen for its computational efficiency and relatively small size (less than 500 MB). For the large model, we utilized LLaVA-NeXT-Video-7B [Li et al., 2024], which is a well-performing open-source video-language model. Following the DMD process described in Section 2, we generated data where the inputs consist of the last hidden layer of the small model for each image input in the raw data, and the outputs indicate whether the small model and large model agree on their falling predictions. To minimize the computing on edge devices, we design a lightweight switcher model implemented as a Multi-Layer Perceptron (MLP) as an ad-hoc layer on top of the small model. The switcher model is trained on the DMD-generated data for the intelligent model switching. More implementation details are available in Appendix A.3."}, {"title": "3.1 Results", "content": "For performance comparison, we conducted experiments and included the results of three baselines. As reported in Table 1, reliance solely on the ViLT (Small model only) resulted in an F1 score of 58.2%, while LLaVA-NeXT-Video (Large model only) got 87.5%. In addition, we reproduced a popular uncertainty-based switcher model approach [Narayan et al., 2022] on our task. Specifically, we extracted the falling probability in the final output layer of ViLT and used that as the metric to determine whether to defer judgment to the larger model. This baseline got 76.1% when offloading 60% computing to the large model. Our system achieves 92.1%, which is 33.9%, 4.6%, and 16.0% higher than the traditional approach, large model-only approach and the uncertainty-based approach respectively.\nPerformance Analysis To gain a deeper understanding of the performance, we manually adjust the threshold for offloading decisions to large models as described in A.4. As shown in Figure 3, our system consistently outperforms the traditional uncertainty-based system regardless of the offload threshold. Furthermore, it suggests that solely calling the large model may be less optimal than calling it only when the switcher model indicates to do so, as the switcher model knows better about both models' capabilities from the DMD process. Therefore, we conclude that the switcher model offers a low-cost, lightweight solution to increase fall detection accuracy while optimizing efficiency.\nComputational Cost Evaluation We also evaluated the computational cost of our model in terms of energy consumption and runtime compared to using a large model only solution, following the method described in Appendix A.5. As illustrated in Figure 4, our switcher model solution, which defers judgment to the large model 60% of the time (corresponding to the switcher model's behavior on the test data), achieved a significant reduction in energy consumption. Specifically, the energy used dropped from 0.0530 kWh to 0.0320 kWh, a 39.5% reduction in energy usage. Also, as shown in Figure 5, the time taken for processing decreased from 663.1 seconds to 405.16 seconds, a 38.9% reduction in time taken. Thus, our system not only improves detection quality but also enhances detection latency, energy efficiency, and overall computational cost."}, {"title": "4 Conclusion and Future Works", "content": "In this paper, we introduced a novel Dual-Model Distillation (DMD) method to develop a lightweight hybrid edge-cloud solution that efficiently balances computational cost and accuracy, enabling deployment on resource-constrained edge devices with support from a large model in the cloud. Although our framework is designed for long video inputs, our current experiments are limited to single-frame video inputs. In future work, we plan to transition to video-based inputs and test our method on more diverse action classification datasets. Overall, we expect our framework to be adaptable and applicable to other real-world applications beyond fall detection. Moreover, we anticipate that DMD will advance the research frontier of knowledge alignment across multiple models in a variety of domains."}, {"title": "A Appendix and Supplemental Materials", "content": null}, {"title": "A.1 Datasets", "content": "We used the Fall Detection - Eldercare Robot dataset for training, validation and testing. It includes 1061 images, each having a data entry in YOLO v7 PyTorch format Wang et al. [2023]. The data entry includes a binary fall/no-fall classification label, the coordinates of the center of the bounding box containing the person, and the width and height of the bounding box. The creators of the dataset oriented pixel data automatically using EXIF-orientation stripping and stretched to resize it to 640x640. When exported, each image had a 50% probability of being laterally inverted and a one-in-three chance of undergoing one of the following operations: a 90-degree clockwise rotation, a 90-degree anti-clockwise rotation, or none at all. Of the 1061 images, we follow the commonly used split, i.e., 742 for training, 212 for validation, and 106 for testing."}, {"title": "A.2 Data Generation and Formatting using Dual-Model Distillation", "content": "In order to prepare and standardize our data for training, we implemented the DMD process as described in this section. As discussed in Section 2, the only information we needed to train the switcher model is the nodes of the last hidden layer for each given input and whether the large and small models agree, referred to as the alignment between the models. We used our novel DMD framework to refine these data for training: by inputting each image into both models with a prompt telling them to report if a fall occurred, we extracted their outputs and recorded whether the models agreed. The values of the nodes in the last hidden layer of ViLT were extracted and saved accordingly. Ultimately, our refined data consisted of only three fields, making the DMD process compatible with any image or video dataset. An example of the formatted data after the DMD process is given as following."}, {"title": "A.3 Implementation Details", "content": "Model Design To make our switcher model lightweight yet effective, we implemented a Multi-Layer Perceptron (MLP) with input size 1536, two hidden layers of size 512 and 128, and an output layer of size 1. Due to this structure, it can be easily implemented in a local system with little additional cost or memory. However, we also found that by using the hidden representation of the last hidden layer from the small model, the MLP demonstrates the capability of understanding the reasoning of the small model, therefore making better alignment predictions.\nTraining Details As for the training settings, we used a learning rate of 0.0001 and a dropout rate of 0.3. Using the refined data mentioned above, we calculated the Binary Cross-Entropy loss (BCELoss) followed by the Logits function (which applies the BCELoss function after the Sigmoid function), and updated weights through backpropagation. The training early stopped at 100 epochs, where the model achieved an F1 score of 79.2% and an accuracy of 82.2% stably."}, {"title": "A.4 Threshold for Offload Decision", "content": "To determine the optimal threshold for deferring judgment to the large model at inference time, we analyzed the switcher model's performance on the training data.\nWe collected the results through our switcher model after all the training data was processed. We sorted these results in ascending order of alignment probability and split the results into 10 \"buckets\". Using this \"buckets\" strategy on the training set, we then plotted the graph of the combined model F1 score against the percentage of data deferred to the large model. For comparison purposes, we also plot the baseline (a standard uncertainty-based switcher model) in Figure2.\nBy observing the training curve of the combined model, we chose the optimal threshold value 60%, shown as the peak of the curve in Figure 2, for testing. For the baseline, uncertainty-based system, we also used 60%, because the baseline uncertainty doesn't have a clear peak to indicate how much data should be offloaded to the large model. Keeping this threshold consistent helps us compare performance fairly. The corresponding results are shown in Table 1.\nTo explore how the threshold value influences the final testing performance, we further applied our \"buckets\" strategy to test data for both methods and analyzed the results. The corresponding results are shown in Figure 3. As a note, this analysis of accuracy against percent of data deferred was used extensively in [Narayan et al., 2022] and can be extended to any framework involving a switcher model."}, {"title": "A.5 Computational Cost Evaluation", "content": "Finally, we compared our model's computational cost to our baselines. We measured the total energy consumption of our system using NVIDIA's nvidia-smi tool and tracked the time taken for processing with Python's time library on our training dataset. To explore the tradeoff in computational"}]}