{"title": "UniMoT: Unified Molecule-Text Language Model with Discrete Token Representation", "authors": ["Juzheng Zhang", "Yongqiang Chen", "Yatao Bian", "Quanming Yao"], "abstract": "The remarkable success of Large Language Models (LLMs) across diverse tasks has\ndriven the research community to extend their capabilities to molecular applications.\nHowever, most molecular LLMs employ adapter-based architectures that do not\ntreat molecule and text modalities equally and lack a supervision signal for the\nmolecule modality. To address these issues, we introduce UniMoT, a Unified\nMolecule-Text LLM adopting a tokenizer-based architecture that expands the\nvocabulary of LLM with molecule tokens. Specifically, we introduce a Vector\nQuantization-driven tokenizer that incorporates a Q-Former to bridge the modality\ngap between molecule and text. This tokenizer transforms molecules into sequences\nof molecule tokens with causal dependency, encapsulating high-level molecular and\ntextual information. Equipped with this tokenizer, UniMoT can unify molecule and\ntext modalities under a shared token representation and an autoregressive training\nparadigm, enabling it to interpret molecules as a foreign language and generate\nthem as text. Following a four-stage training scheme, UniMoT emerges as a multi-\nmodal generalist capable of performing both molecule-to-text and text-to-molecule\ntasks. Extensive experiments demonstrate that UniMoT achieves state-of-the-art\nperformance across a wide range of molecule comprehension and generation tasks.", "sections": [{"title": "1 Introduction", "content": "The incredible capabilities of Large Language Models (LLMs) [5, 44] have led to their widespread\nuse as versatile tools for completing diverse real-world tasks. This success has sparked interest in\nMulti-modal LLMs [59, 52], which aim to enhance LLMs by enabling them to process multi-modal\ninputs and outputs. Prior research efforts [26, 41, 12, 6, 33, 35, 25] have focused on adapting LLMs\nto molecular tasks, resulting in the development of molecular LLMs. These molecular LLMs can\nanalyze molecule structures [35, 33, 6], address drug-related inquiries [26, 41], assist in synthesis\nand retrosynthesis planning [12], support drug design [12], and more.\nPrevalent molecular LLMs commonly employ adapter-based architectures, adopting either a linear\nprojection [26, 41, 6] or a Q-Former [33, 25] as an adapter to translate molecule features into the\nsemantic space of LLM, as illustrated in Figure la and Figure 1b. Despite demonstrating initial\ncapabilities in molecular comprehension and yielding promising results in molecule-to-text generation\ntasks, they still fall short in text-to-molecule generation tasks. The critical issue within these methods\nis their unequal treatment of molecules and text, resulting in a lack of supervision for the molecule\nmodality. This limitation significantly constrains model capacity and effectiveness.\nDiscretizing continuous molecule features into discrete molecule tokens offers a promising solution\nfor conducting both molecule-to-text and text-to-molecule generation tasks. By treating tokens from\ndifferent modalities equally, we can predict the next molecule or text token in an autoregressive\nmanner. However, directly discretizing molecule features poses several challenges: (i) This approach\nresults in long sequences, with lengths equivalent to the number of atoms in a batch. LLMs typically\nexperience a quadratic increase in computational complexity with sequence length [46]. (ii) Molecule\ntokens derived from molecule features lack left-to-right causal dependency, which conflicts with\nthe unidirectional attention mechanism in LLMs. (iii) Molecule features lack textual information,\nhindering effective molecule-text interactions and alignment.\nTo this end, we present UniMoT, a Unified Molecule-Text LLM that adopts a tokenizer-based\narchitecture, integrating molecule comprehension and generation, as depicted in Figure 1c. A pivotal\naspect of UniMoT's architecture is the molecule tokenizer for transforming molecules into molecule\ntokens. We introduce a Vector Quantization-driven [45] tokenizer, which incorporates a Q-Former [23]\nto bridge the modality gap between molecules and text. Specifically, we incorporate causal masks\nfor the queries, enabling the Q-Former to generate a causal sequence of queries compatible with\nthe unidirectional attention in LLMs. The sequence of queries is subsequently quantized into a\nsequence of molecule tokens using a learnable codebook. The molecule tokens encapsulate high-level\nmolecular and textual information, which are then aligned with the latent space of a generative model\nvia an MLP adapter, enabling the generation of desired molecules.\nPretrained LLMs can integrate the molecule tokenizer by treating molecule tokens as new words and\nconstructing a molecule vocabulary through mapping the learned codebook. We adopt the unified\ndiscrete token representation for molecules and text, coupled with the unified next-token-prediction\ntraining paradigm of LLM. This unification of representation and training paradigm enhances LLMs'\nability to understand molecule-text interactions and alignment. UniMoT interprets molecules akin to\nunderstanding a foreign language, and generates them as if they were text. Following a four-stage\ntraining scheme, UniMoT serves as a multi-modal generalist capable of performing both molecule\ncomprehension and generation tasks.\nOur contributions can be summarized as follows:\n\u2022 We introduce a molecule tokenizer specifically designed for LLMs, enabling the tokenization\nof molecules into short sequences of molecule tokens with causal dependency. These tokens\nencapsulate high-level molecular and textual information and can be decoded into desired\nmolecules during inference.\n\u2022 We present UniMoT, a unified molecule-text LLM that adopts a tokenizer-based architecture\ninstead of traditional adapter-based architectures. UniMoT unifies the modalities of molecule\nand text under a shared token representation and an autoregressive training paradigm."}, {"title": "2 Related Works", "content": "Molecular Large Language Models. The recent emergence of Vision Large Language Models\n(VLLMs) [24, 23, 28] has catalyzed advancements in Molecular LLMs, which encompass both\nsingle modality and multi-modality approaches. In the single modality domain, researchers are\nexploring diverse molecule representations, such as 1D sequences like SMILES strings [47, 8, 17],\n2D molecule graphs [15, 56], 3D geometric conformations [56, 32], and textual information from\nthe literature [43, 2, 21]. In the multiple modalities domain, various innovative approaches are being\nemployed. MolT5 [11], a T5-based [38] model, is designed for SMILES-to-text and text-to-SMILES\ntranslations. Other works, such as MoMu [39], MoleculeSTM [31], MolFM [34], and GIT-Mol [29],\nleverage cross-modal contrastive learning to align the representation spaces of molecules and text.\nAdditionally, some studies use multi-modal learning architectures to develop molecular LLMs,\nwhich often adopt adapter-based architectures. For instance, InstructMol [6], GraphGPT [41], and\nDrugChat [26] employ a simple projection layer to map molecule features to LLM's input space.\nMolCA [33] and 3D-MoLM [25] utilize a Q-Former [23] to bridge the modality gap between\nmolecules and text. However, these methods do not treat molecule and text modalities equally and\nlack a supervision signal for the molecule modality, limiting model capacity and effectiveness.\nVector Quantization. Vector Quantization (VQ) [13] is a widely used technique in generative\nmodels. VQ-VAE [45] converts an image into a set of discrete codes within a learnable discrete\nl latent space by learning to reconstruct the original image. VQ-GAN [57] enhances the generation\nquality by leveraging adversarial and perceptual objectives. In the context of molecules, VQ has been\neffectively applied to quantize molecule features. For example, DGAE [4] introduces a VQ model\nspecifically for molecules, where molecules are encoded into discrete latent codes. Mole-BERT [54]\nuses VQ to rethink the pre-training of GNNs for molecular tasks. IMOLD [60] proposes using VQ to\nenhance invariant molecule representations, and VQSynergy [51] demonstrates the use of VQ for\ndrug discovery."}, {"title": "3 Method", "content": "Our objective is to leverage the reasoning and generation capabilities of LLMs to enhance the\ncomprehension and generation of molecule and text data. To achieve this, we focus on representing\nthese modalities uniformly within the token representation, utilizing the next-token-prediction training\nparadigm of LLMs. As illustrated in Figure 2, we introduce a molecule tokenizer (Section 3.1)\ndesigned to transform molecules into molecule tokens by learning to reconstruct the input molecule.\nThe molecule sequence can then be concatenated with the text sequence to form a multi-modal\nsequence, which is subsequently fed into an LLM for autoregressive pretraining (Section 3.2), as\nillustrated in Figure 3. The LLM vocabulary is expanded with molecule tokens mapped from the\nlearned codebook. We introduce a four-stage training scheme for UniMoT (Section 3.3) comprising\nCausal Q-Former pretraining, molecule tokenizer pretraining, unified molecule-text pretraining, and\ntask-specific instruction tuning. UniMoT is capable of performing both molecular comprehension\nand generation tasks following the training scheme."}, {"title": "3.1 Molecule Tokenizer for LLMS", "content": "Molecule Encoder. We represent the structural information of a molecule as a graph, denoted\nby G = (V,E), where V is the set of atoms and |V| = N is the number of atoms. The task of the\nmolecule encoder is to extract molecule features that are context-aware and encompass diverse local\nneighborhood structural information. By employing a molecule encoder, we obtain molecule features\nX\u2208R^{N\u00d7F}, where each atom feature contains context-aware structural information.\nCausal Q-Former. We employ a Q-Former model introduced by BLIP-2 [23] to generate queries\nZ = \\{z_i\\}_{i=1}^M \u2208 R^{M\u00d7d} containing high-level molecular and textual information, where M represents\nthe number of queries and d denotes the dimension of queries. Specifically, we incorporate causal"}, {"title": "Vector Quantization.", "content": "The Causal Q-Former converts molecules and text into a causal sequence of\nqueries. Subsequently, the causal sequence of queries \\{z_i\\}_{i=1}^M is quantized into a causal sequence of\nmolecule tokens \\{s_i\\}_{i=1}^M by identifying the closest neighbor in a learnable codebook C = \\{c_j\\}_{j=1}^K,\nwhere K represents the size of the codebook. The codebook is randomly initialized and optimized\nduring pretraining. Specifically, token s_i is determined as follows:\ns_i = \\text{argmin}_{j\u2208\\{1,\\ldots,K\\}} || z_i - c_j ||_2, \\text{ for } i = 1,2,\\ldots, M.\n(1)\nIntuitively, the query z_i is quantized to the closest neighbor c_{s_i} in the codebook. As the vector\nquantization process is non-differentiable, we adopt the straight-through estimator [3] to train the\nCausal Q-Former by copying the gradient from the molecule tokens to the queries, as shown in\nFigure 2. The resulting embeddings of molecule tokens, denoted as C = \\{c_{s_i}} \\}_{i=1}^M, are subsequently\nutilized for reconstructing molecules."}, {"title": "Molecule Reconstruction.", "content": "An MLP adapter $\\psi$ needs to be trained to align the discrete latent space\nof molecule tokens with the continuous latent space of a molecular generative model for molecule\nreconstruction. This can be represented as X_R = \\psi(C), where X_R denotes the embeddings for\nreconstruction. To achieve alignment, we minimize the Mean Squared Error (MSE) loss between X_R\nand the SMILES [50] embeddings X_S produced by the pretrained SMILES encoder. Subsequently,\nwe can reconstruct the molecule from X_R using the pretrained SMILES decoder. The training loss of\nthe tokenizer is expressed as follows:\n$\\mathcal{L}_{\\text{Tokenizer}} = ||X_R - X_S||_2^2 + \\frac{1}{M}\\sum_{i=1}^M ||\\text{sg}[z_i] - c_{s_i}||_2^2 + \\frac{\\beta}{M}\\sum_{i=1}^M ||\\text{sg}[c_{s_i}] - z_i||_2^2$.\n(2)\nHere, the first term represents the alignment loss, the second term is a codebook loss aimed at\nupdating the codebook embeddings, and the third term is a commitment loss that encourages the\nquery to stay close to the chosen codebook embedding. sg[.] denotes the stop-gradient operator, and\nthe hyperparameter $\u03b2$ is set to 0.25."}, {"title": "3.2 Unified Molecule-Text Language Model", "content": "Expanding Vocabulary. Employing the molecule tokenizer, a molecule can be tokenized into a\nmolecule sequence \\{s_i\\}_{i=1}^M with causal dependency. The molecule sequence can be concatenated with\nthe text sequence to form a multi-modal sequence \\{u_i\\}_{i=1}^L, where L is the length of the multi-modal\nsequence. To facilitate the representation of the multi-modal sequence, we construct the molecule\nvocabulary V_m = \\{v_m^i\\}_{i=1}^K, which maintains the order of the molecule codebook C = \\{c_i\\}_{i=1}^K.\nAdditionally, V_m includes several special tokens such as boundary indicators, e.g., [MOL] and\n[/MOL], to mark the beginning and end of the molecule sequence. Next, we merge the original text\nvocabulary V_t = \\{v_t^i\\}_{i=1}^T with the molecule vocabulary V_m. The unified molecule-text vocabulary\nV = \\{V_m, V_t\\} facilitates joint learning from molecules and text under a unified next-token-prediction\nobjective. As the vocabulary is expanded, the corresponding embeddings and prediction layers also\nneed to be extended, with the newly introduced parameters initialized randomly.\nUnified Molecule-text Modeling. The multi-modal sequence \\{u_i\\}_{i=1}^L is fed into the pretrained\nLLM for performing multi-modal autoregression. UniMoT adopts the general Language Modeling\n(LM) objective to directly maximize the log-likelihood of the data distribution:\n$\\mathcal{L}_{\\text{LLM}} = - \\sum_{D \\in D} \\sum_{i \\in I} \\text{log } p (u_i | u_1, \\ldots, u_{i-1}; \\theta)$,\n(3)\nwhere $D$ represents the dataset, $I$ represents the set of indices of the generation target, and $\\theta$ denotes\nthe parameters of the LLM. The unification of representation and training paradigm for molecules and\ntext enhances the abilities of LLMs to understand molecule-text interactions and alignment. UniMoT\ncan interpret molecules similar to understanding a foreign language, and generate them as if they\nwere text. We conduct autoregressive pretraining on molecule-to-text and text-to-molecule tasks to\nenhance the molecule comprehension and generation capabilities.\nMolecule-to-Text Autoregression. While structural information is embedded in molecule features\nand captured by the molecule tokens through the tokenizer, we also aim to incorporate sequential\ninformation of molecules for better comprehension. Therefore, we concatenate the molecule sequence\n\\{s_i\\}_{i=1}^M with the SMILES [50] sequence and a prompt to form the multi-modal input sequence\n\\{u_i\\}_{i=1}^L, as illustrated in Figure 3a. The text sequence of the corresponding molecule caption is used\nas the generation target.\nText-to-Molecule Autoregression. For molecule generation, a prompt and the molecule caption\nare concatenated, with a [MOL] token appended to signify the beginning of the molecule sequence,\nas illustrated in Figure 3b. The molecule sequence \\{s_i\\}_{i=1}^M produced by the tokenizer is used as the"}, {"title": "3.3 Training Strategy", "content": "The training strategy for UniMoT is structured across four stages. Stage-1 focuses on Causal Q-\nFormer pretraining with tailored objectives. In Stage-2, the molecule tokenizer is optimized using the\nfrozen encoders and decoder. Stage-3 integrates the tokenizer with a language model for multi-modal\ncomprehension and generation. Finally, Stage-4 fine-tunes UniMoT for specific tasks, aligning it with\nhuman instructions and optimizing performance for various molecular applications. More details\nregarding the training process can be found in Appendix C.\nStage-1: Causal Q-Former Pretraining. We connect the molecule encoder and Causal Q-Former,\nleveraging the pretrained MoleculeSTM molecule encoder [31]. The molecule encoder remains\nfrozen while only the Causal Q-Former is updated. Both queries and text inputs are used, while only\nqueries serve as input in subsequent stages. In our experiments, we utilize 16 queries. We employ\nthree tailored objectives for the pretraining of the Causal Q-Former: Molecule-Text Contrastive\nLearning (MTC), Molecule-Text Matching (MTM), and Molecule-grounded Text Generation (MTG).\nThe details of these objectives can be found in Appendix A.\nStage-2: Molecule Tokenizer Pretraining. We connect the Causal Q-Former with subsequent\nblocks and use the objective defined in Equation (2). We employ the pretrained ChemFormer [17]\nas the generative model. Specifically, we leverage the SMILES encoder and the SMILES decoder\nprovided by ChemFormer. The molecule codebook size is set to K = 2048. As shown in Figure 2,\nwe keep the molecule encoder, the SMILES encoder, and the SMILES decoder frozen, while updating\nthe Causal Q-Former, the learnable codebook, and the adapter.\nStage-3: Unified Molecule-Text Pretraining. We integrate the molecule tokenizer with the LLM\nusing the unified vocabulary of molecule tokens and text tokens. We employ the LM objective\ndefined in Equation (3) to pretrain the LLM. Pretraining involves molecule-to-text autoregression\nand text-to-molecule autoregression, aimed at enhancing UniMoT's multi-modal comprehension and\ngeneration capabilities. To enhance efficiency, we train the LLM using LoRA tuning [14].\nStage-4: Task-Specific Instruction Tuning. UniMoT is fine-tuned on seven comprehension and\ngeneration tasks: molecular property prediction, molecule captioning, molecule-text retrieval, caption-\nguided molecule generation, reagent prediction, forward reaction prediction, and retrosynthesis. We\nalso utilize LoRA tuning to improve efficiency. This stage ensures UniMoT can accurately interpret\nand respond to human instructions, making it versatile and effective for diverse molecular tasks."}, {"title": "4 Experiments", "content": "4.1 Molecule Comprehension Tasks\nMolecular Property Prediction Task. The goal of molecular property prediction is to forecast\na molecule's intrinsic physical and chemical properties. For the classification task, we incorporate\neight binary classification datasets from MoleculeNet [53]. Models are tasked with generating\na single prediction (\u201cyes\u201d or \u201cno\u201d). We compare UniMoT with the following baselines: KV-\nPLM [58], AttrMask [16], InfoGraph [40], MolCLR [48], GraphMVP [30], MoleculeSTM [31],\nand InstructMol [6]. The ROC-AUC (%) results on the MoleculeNet datasets are shown in Table 1.\nThe performance of the regression task of molecular property prediction is provided in Appendix D.\nCompared to traditional graph learning methods and molecular LLMs like InstructMol [6], UniMoT\ndemonstrates consistent improvements across the eight datasets, indicating its robust molecule\ncomprehension abilities.\nMolecule Captioning Task. The molecule captioning task involves generating a comprehensive\ndescription of a molecule. We compare UniMoT with several baselines: MolT5 [11], MoMu [39],\nInstructMol [6], MolCA [33], and 3D-MOLM [25]. BLEU [37], ROUGE [27], and METEOR [1] are\nadopted as evaluation metrics. UniMoT is evaluated for molecule captioning on the PubChem [18]"}, {"title": "4.3 Ablation Studies", "content": "Cross-Modal Projector. We conducted an ablation study on the cross-modal projector, with the\nresults on the molecule captioning task shown in Table 5. The linear projection demonstrated the\nworst performance, indicating that the molecule features lack textual information, thus hindering\neffective molecule-text interactions and alignment. Additionally, we compared the performance of\na Q-Former with bidirectional self-attention to a Causal Q-Former with causal self-attention. The\nresults show that queries with causal dependency outperform those with bidirectional dependency.\nThis demonstrates that input with left-to-right causal dependency aligns with the unidirectional\nattention mechanism in LLMs, leading to improved performance.\nDiscrete vs. Continuous Representation. We compare the performance of continuous causal\nembeddings and discrete tokens quantized from causal embeddings as inputs to LLMs. As shown in\nTable 5, continuous embeddings demonstrate better performance than discrete tokens in understanding\nmolecules. This result is reasonable since the quantization process causes information loss in discrete\ntokens. However, we still use discrete token representation to facilitate the autoregressive training\nparadigm of LLMs, which supports the unification of comprehension and generation tasks. To achieve\nthis unification, we unavoidably sacrifice some performance in comprehension tasks.\nModel Size and Tuning Stategy. We conducted a comparison of molecule captioning performance\nacross various model sizes and tuning strategies, as illustrated in Table 6. Our findings indicate that\nscaling up the LLM to 13B or adopting a full tuning strategy yields only marginal improvements\nin performance compared to using Llama-2-7B with LoRA tuning. While larger models and full\ntuning strategies might offer slight gains in performance, they come at a significant cost in terms of\nefficiency. Considering the trade-off between achieving high performance and maintaining efficiency,\nwe have chosen to utilize Llama-2-7B with LoRA tuning in our experiments. This ensures that our\nmodel remains both powerful and practical."}, {"title": "5 Conclusion", "content": "This work introduces UniMoT, an innovative advancement in the field of molecular-textual under-\nstanding and generation, which successfully unifies these two distinct modalities under a coherent\nframework. By introducing a Vector Quantization-driven tokenizer, UniMoT overcomes previous\narchitectural limitations where molecule and text modalities were not treated equally. This molecule\ntokenizer transforms molecules into sequences of discrete tokens, embedding high-level molecular\nand textual information. The LLM vocabulary is expanded with molecule tokens mapped from the\nlearned codebook. Moreover, by employing a four-stage training scheme, UniMoT has emerged as\na versatile multi-modal LLM, adept at handling both molecule-to-text and text-to-molecule tasks.\nExtensive empirical evaluations demonstrate that UniMoT attains state-of-the-art performance across\ndiverse molecule comprehension and generation tasks."}, {"title": "A Details of Causal Q-Former", "content": "The Q-Former operates as a query-based transformer that utilizes learnable query vectors to interact\nwith molecule features extracted by a frozen encoder. These queries are essential for extracting\nrelevant information from the molecule features. The Q-Former comprises both a molecule trans-\nformer and a text transformer, sharing self-attention layers. The molecule transformer incorporates\ncross-attention layers between self-attention and feed-forward layers, while the text transformer\narchitecture is based on BERT [9]. Q-Former employs a cross-attention mechanism where the\nquery vectors selectively attend to different aspects of the molecule features, allowing the model to\ncapture critical details necessary for understanding and generating textual descriptions of molecular\nproperties.\nSpecifically, we incorporate causal masks into the queries, ensuring that they only interact with\npreceding queries. This ensures the sequence of queries maintains a causal dependency, aligning\nwith the requirements of LLMs operating on text sequence. The Causal Q-Former is illustrated\nin Figure 4. We employ the Causal Q-Former to generate causal queries Z = \\{z_i\\}_{i=1}^M \u2208 R^{M\u00d7d}\ncontaining high-level molecular and textual information, where M represents the number of queries\nand d denotes the dimension of queries. Next, we introduce three tailored objectives MTC, MTM,\nand MTG for the pretraining of the Causal Q-Former.\nMolecule-Text Contrastive Learning (MTC) aims to align molecule and text features by maximizing\ntheir mutual information. This is achieved by maximizing the molecule-text similarity of positive\npairs against that of negative pairs. We utilize the last query z_M of the query sequence \\{z_i\\}_{i=1}^M as\nthe query representation, since the output query sequence is causal and the last query contains global\ninformation from the queries. For text representation, we use the output embedding of the [CLS]\ntoken, denoted as y. The contrastive learning loss is expressed as follows:\n$\\mathcal{L}_{MTC} = -\\frac{1}{B}\\sum_{i=1}^B \\text{log } \\frac{\\text{exp}((z_M^i)^T y^i / \\tau)}{\\sum_{j=1}^B \\text{exp}((z_M^i)^T y^j / \\tau)} - \\frac{1}{B}\\sum_{i=1}^B \\text{log } \\frac{\\text{exp}((y^i)^T z_M^i / \\tau)}{\\sum_{j=1}^B \\text{exp}((y^i)^T z_M^j / \\tau)},$\n(4)\nwhere B denotes the batch size, and $\u03c4$ represents the temperature parameter. Here, $z_M^i$ and $y^i$ refer\nto the i-th query representation and text representation in a batch, respectively.\nMolecule-Text Matching (MTM) focuses on learning fine-grained alignment between molecule\nand text features. As queries \\{z_i\\}_{i=1}^M capture both molecular and textual information through cross-\nattention and self-attention layers respectively, we utilize the last query z_M as input to a binary\nclassifier. This classifier predicts whether a given molecule-text pair is matched or unmatched. The"}, {"title": null, "content": "corresponding loss function is formulated as follows:\n$\\mathcal{L}_{MTM} = - \\frac{1}{B}\\sum_{i=1}^B \\text{log } \\frac{\\text{exp}(\\phi(z_M^i | X^i, t^i))}{\\sum_{j=1}^B \\text{exp}(\\phi(z_M^i | X^i, t^j)) + \\sum_{j=1}^B \\text{exp}(\\phi(z_M^i | X^j, t^i))},$\n(5)\nwhere $\u03c6$ represents a binary classifier, and $X^i$ and $t^i$ denote the i-th input molecule features and input\ntext in a batch, respectively.\nMolecule-grounded Text Generation (MTG) focuses on generating textual descriptions given\na molecule input. In this task, causal masks for queries are not applied since only textual output\nis required. However, causal masks are applied for text, allowing each text token to attend to its\npreceding text tokens and all queries, but not subsequent tokens. The Language Modeling (LM)\nloss function is applied to model the generation of text $t^i$ conditioned on the molecule input $X^i$,\nformulated as:\n$\\mathcal{L}_{MTG} = - \\frac{1}{B L}\\sum_{i=1}^B \\sum_{j=1}^{L} \\text{log } p (t_j^i | t_1^i, \\ldots, t_{j-1}^i, X^i),$\n(6)\nwhere $t_j^i$ represents the j-th token in the text sequence $t^i$. Here, $X^i$ and $t^i$ denote the i-th input\nmolecule features and generated text in a batch, respectively.\nThe total loss for training the Causal Q-Former encompasses the three aforementioned objectives:\n$\\mathcal{L}_{Q-\\text{Former}} = \\mathcal{L}_{MTC} + \\mathcal{L}_{MTM} + \\mathcal{L}_{MTG}.$\n(7)"}, {"title": "B Details of Datasets", "content": "This section provides detailed information about the datasets used in evaluating the performance of\nUniMoT across various tasks. The datasets are utilized for molecular property prediction, molecule\ncaptioning, molecule-text retrieval, caption-guided molecule generation, reagent prediction, forward\nreaction prediction, and retrosynthesis task. Each dataset serves a unique purpose in assessing\ndifferent capabilities of the model. We provide a comprehensive overview of datasets, including their\ntypes, associated tasks, descriptions, URLs for access, and licensing information.\nWe present the details of the Molecular Property Prediction Datasets below:\n\u2022 BBBP [53]: The Blood-Brain Barrier Penetration dataset predicts the ability of molecules to\npenetrate the blood-brain barrier.\n\u2022 Tox21 [53]: This dataset is part of the Toxicology in the 21st Century initiative, used for toxicity\nprediction.\n\u2022 ToxCast [53]: Another toxicity prediction dataset with a broader range of biological assays.\n\u2022 Sider [53]: Side Effect Resource database, used for predicting drug side effects.\n\u2022 ClinTox [53]: Clinical Toxicity dataset for predicting clinical trial toxicity outcomes.\n\u2022 MUV [53]: Maximum Unbiased Validation dataset for virtual screening.\n\u2022 HIV [53]: Human Immunodeficiency Virus dataset for predicting anti-HIV activities.\n\u2022 BACE [53]: Beta-Secretase 1 dataset for predicting inhibitors of the BACE-1 enzyme, relevant\nfor Alzheimer's research.\n\u2022 QM9 [12]: The quantum mechanics properties dataset, where the objective is to predict key\nquantum mechanics properties of a given molecule, such as HUMO, LUMO, and the HUMO-\nLUMO gap.\nWe present the details of the Molecule Captioning Datasets below:\n\u2022 PubChem [18]: A large dataset of chemical molecules used for generating textual descriptions\nof molecular structures.\n\u2022 ChEBI-20 [11]: A subset of the Chemical Entities of Biological Interest database, provides\nstructured and detailed descriptions of molecules."}, {"title": "E Limitations", "content": "While UniMoT demonstrates considerable advancements in unifying molecule and text modalities\nfor comprehensive understanding and generation tasks, several limitations must be acknowledged.\nAlthough UniMoT exhibits strong performance in molecule-to-text and text-to-molecule tasks, it has\nnot been extensively tested on more complex molecule generation tasks such as molecule editing,\nwhich require precise modifications to molecular structures. Future work could explore extending\nUniMoT's capabilities to handle such sophisticated molecular manipulations.\nDue to the scarcity of annotated data in the molecular field, the training of UniMoT is less extensive\ncompared to fields like computer vision. This limitation restricts the model's ability to fully learn and\ngeneralize from diverse molecular structures and properties. In contrast, the visual domain benefits\nfrom abundant labeled datasets, allowing for more comprehensive training and better performance.\nAddressing this data scarcity in the molecular domain is crucial for improving UniMoT's training\neffectiveness and overall capabilities.\nThe current empirical evaluations, though extensive, are primarily conducted on standard datasets\nand benchmarks; expanding the evaluation to a broader array of datasets and real-world scenarios\nwill provide a more comprehensive understanding of the model's robustness and generalizability."}, {"title": "F Broader Impacts", "content": "The development of UniMoT, a unified model for molecule and text modalities, has significant\npotential to positively impact various fields. UniMoT can streamline the drug discovery process by\nenabling efficient molecule generation and optimization based on textual descriptions. In material\nscience, it can aid in discovering new materials with desirable properties. Additionally, UniMoT\ncan enhance research collaboration between chemists, biologists, and data scientists by integrating\nmolecular and textual data, leading to comprehensive research insights and innovative solutions."}]}