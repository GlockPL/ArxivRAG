{"title": "UniMoT: Unified Molecule-Text Language Model with Discrete Token Representation", "authors": ["Juzheng Zhang", "Yongqiang Chen", "Yatao Bian", "Quanming Yao"], "abstract": "The remarkable success of Large Language Models (LLMs) across diverse tasks has driven the research community to extend their capabilities to molecular applications. However, most molecular LLMs employ adapter-based architectures that do not treat molecule and text modalities equally and lack a supervision signal for the molecule modality. To address these issues, we introduce UniMoT, a Unified Molecule-Text LLM adopting a tokenizer-based architecture that expands the vocabulary of LLM with molecule tokens. Specifically, we introduce a Vector Quantization-driven tokenizer that incorporates a Q-Former to bridge the modality gap between molecule and text. This tokenizer transforms molecules into sequences of molecule tokens with causal dependency, encapsulating high-level molecular and textual information. Equipped with this tokenizer, UniMoT can unify molecule and text modalities under a shared token representation and an autoregressive training paradigm, enabling it to interpret molecules as a foreign language and generate them as text. Following a four-stage training scheme, UniMoT emerges as a multi-modal generalist capable of performing both molecule-to-text and text-to-molecule tasks. Extensive experiments demonstrate that UniMoT achieves state-of-the-art performance across a wide range of molecule comprehension and generation tasks.", "sections": [{"title": "1 Introduction", "content": "The incredible capabilities of Large Language Models (LLMs) [5, 44] have led to their widespread use as versatile tools for completing diverse real-world tasks. This success has sparked interest in Multi-modal LLMs [59, 52], which aim to enhance LLMs by enabling them to process multi-modal inputs and outputs. Prior research efforts [26, 41, 12, 6, 33, 35, 25] have focused on adapting LLMs to molecular tasks, resulting in the development of molecular LLMs. These molecular LLMs can analyze molecule structures [35, 33, 6], address drug-related inquiries [26, 41], assist in synthesis and retrosynthesis planning [12], support drug design [12], and more.\nPrevalent molecular LLMs commonly employ adapter-based architectures, adopting either a linear projection [26, 41, 6] or a Q-Former [33, 25] as an adapter to translate molecule features into the semantic space of LLM, as illustrated in Figure la and Figure 1b. Despite demonstrating initial capabilities in molecular comprehension and yielding promising results in molecule-to-text generation tasks, they still fall short in text-to-molecule generation tasks. The critical issue within these methods is their unequal treatment of molecules and text, resulting in a lack of supervision for the molecule modality. This limitation significantly constrains model capacity and effectiveness.\nDiscretizing continuous molecule features into discrete molecule tokens offers a promising solution for conducting both molecule-to-text and text-to-molecule generation tasks. By treating tokens from different modalities equally, we can predict the next molecule or text token in an autoregressive manner. However, directly discretizing molecule features poses several challenges: (i) This approach results in long sequences, with lengths equivalent to the number of atoms in a batch. LLMs typically experience a quadratic increase in computational complexity with sequence length [46]. (ii) Molecule tokens derived from molecule features lack left-to-right causal dependency, which conflicts with the unidirectional attention mechanism in LLMs. (iii) Molecule features lack textual information, hindering effective molecule-text interactions and alignment.\nTo this end, we present UniMoT, a Unified Molecule-Text LLM that adopts a tokenizer-based architecture, integrating molecule comprehension and generation, as depicted in Figure 1c. A pivotal aspect of UniMoT's architecture is the molecule tokenizer for transforming molecules into molecule tokens. We introduce a Vector Quantization-driven [45] tokenizer, which incorporates a Q-Former [23] to bridge the modality gap between molecules and text. Specifically, we incorporate causal masks for the queries, enabling the Q-Former to generate a causal sequence of queries compatible with the unidirectional attention in LLMs. The sequence of queries is subsequently quantized into a sequence of molecule tokens using a learnable codebook. The molecule tokens encapsulate high-level molecular and textual information, which are then aligned with the latent space of a generative model via an MLP adapter, enabling the generation of desired molecules.\nPretrained LLMs can integrate the molecule tokenizer by treating molecule tokens as new words and constructing a molecule vocabulary through mapping the learned codebook. We adopt the unified discrete token representation for molecules and text, coupled with the unified next-token-prediction training paradigm of LLM. This unification of representation and training paradigm enhances LLMs' ability to understand molecule-text interactions and alignment. UniMoT interprets molecules akin to understanding a foreign language, and generates them as if they were text. Following a four-stage training scheme, UniMoT serves as a multi-modal generalist capable of performing both molecule comprehension and generation tasks.\nOur contributions can be summarized as follows:\n\u2022 We introduce a molecule tokenizer specifically designed for LLMs, enabling the tokenization of molecules into short sequences of molecule tokens with causal dependency. These tokens encapsulate high-level molecular and textual information and can be decoded into desired molecules during inference.\n\u2022 We present UniMoT, a unified molecule-text LLM that adopts a tokenizer-based architecture instead of traditional adapter-based architectures. UniMoT unifies the modalities of molecule and text under a shared token representation and an autoregressive training paradigm."}, {"title": "2 Related Works", "content": "Molecular Large Language Models. The recent emergence of Vision Large Language Models (VLLMs) [24, 23, 28] has catalyzed advancements in Molecular LLMs, which encompass both single modality and multi-modality approaches. In the single modality domain, researchers are exploring diverse molecule representations, such as 1D sequences like SMILES strings [47, 8, 17], 2D molecule graphs [15, 56], 3D geometric conformations [56, 32], and textual information from the literature [43, 2, 21]. In the multiple modalities domain, various innovative approaches are being employed. MolT5 [11], a T5-based [38] model, is designed for SMILES-to-text and text-to-SMILES translations. Other works, such as MoMu [39], MoleculeSTM [31], MolFM [34], and GIT-Mol [29], leverage cross-modal contrastive learning to align the representation spaces of molecules and text. Additionally, some studies use multi-modal learning architectures to develop molecular LLMs, which often adopt adapter-based architectures. For instance, InstructMol [6], GraphGPT [41], and DrugChat [26] employ a simple projection layer to map molecule features to LLM's input space. MolCA [33] and 3D-MoLM [25] utilize a Q-Former [23] to bridge the modality gap between molecules and text. However, these methods do not treat molecule and text modalities equally and lack a supervision signal for the molecule modality, limiting model capacity and effectiveness.\nVector Quantization. Vector Quantization (VQ) [13] is a widely used technique in generative models. VQ-VAE [45] converts an image into a set of discrete codes within a learnable discrete latent space by learning to reconstruct the original image. VQ-GAN [57] enhances the generation quality by leveraging adversarial and perceptual objectives. In the context of molecules, VQ has been effectively applied to quantize molecule features. For example, DGAE [4] introduces a VQ model specifically for molecules, where molecules are encoded into discrete latent codes. Mole-BERT [54] uses VQ to rethink the pre-training of GNNs for molecular tasks. IMOLD [60] proposes using VQ to enhance invariant molecule representations, and VQSynergy [51] demonstrates the use of VQ for drug discovery."}, {"title": "3 Method", "content": "Our objective is to leverage the reasoning and generation capabilities of LLMs to enhance the comprehension and generation of molecule and text data. To achieve this, we focus on representing these modalities uniformly within the token representation, utilizing the next-token-prediction training paradigm of LLMs. As illustrated in Figure 2, we introduce a molecule tokenizer (Section 3.1) designed to transform molecules into molecule tokens by learning to reconstruct the input molecule. The molecule sequence can then be concatenated with the text sequence to form a multi-modal sequence, which is subsequently fed into an LLM for autoregressive pretraining (Section 3.2), as illustrated in Figure 3. The LLM vocabulary is expanded with molecule tokens mapped from the learned codebook. We introduce a four-stage training scheme for UniMoT (Section 3.3) comprising Causal Q-Former pretraining, molecule tokenizer pretraining, unified molecule-text pretraining, and task-specific instruction tuning. UniMoT is capable of performing both molecular comprehension and generation tasks following the training scheme."}, {"title": "3.1 Molecule Tokenizer for LLMS", "content": "Molecule Encoder. We represent the structural information of a molecule as a graph, denoted by G = (V,E), where V is the set of atoms and |V| = N is the number of atoms. The task of the molecule encoder is to extract molecule features that are context-aware and encompass diverse local neighborhood structural information. By employing a molecule encoder, we obtain molecule features X\u2208R^{N\u00d7F}, where each atom feature contains context-aware structural information.\nCausal Q-Former. We employ a Q-Former model introduced by BLIP-2 [23] to generate queries Z = {z_i}_{i=1}^M\u2208R^{M\u00d7d} containing high-level molecular and textual information, where M represents the number of queries and d denotes the dimension of queries. Specifically, we incorporate causal"}, {"title": "Vector Quantization.", "content": "The Causal Q-Former converts molecules and text into a causal sequence of queries. Subsequently, the causal sequence of queries {z_i}_{i=1}^M is quantized into a causal sequence of molecule tokens {s_i}_{i=1}^M by identifying the closest neighbor in a learnable codebook C = {c_j}_{j=1}^K, where K represents the size of the codebook. The codebook is randomly initialized and optimized during pretraining. Specifically, token s_i is determined as follows:\ns_i = \\text{argmin}_{j\u2208{1,\u2026,K}} || z_i - c_j ||_2, \\text{ for } i = 1,2,\u2026, M. (1)\nIntuitively, the query z_i is quantized to the closest neighbor c_{s_i} in the codebook. As the vector quantization process is non-differentiable, we adopt the straight-through estimator [3] to train the Causal Q-Former by copying the gradient from the molecule tokens to the queries, as shown in Figure 2. The resulting embeddings of molecule tokens, denoted as C = {c_{s_i}}_{i=1}^M, are subsequently utilized for reconstructing molecules."}, {"title": "Molecule Reconstruction.", "content": "An MLP adapter \\psi needs to be trained to align the discrete latent space of molecule tokens with the continuous latent space of a molecular generative model for molecule reconstruction. This can be represented as X_R = \\psi(C), where X_R denotes the embeddings for reconstruction. To achieve alignment, we minimize the Mean Squared Error (MSE) loss between X_R and the SMILES [50] embeddings X_S produced by the pretrained SMILES encoder. Subsequently, we can reconstruct the molecule from X_R using the pretrained SMILES decoder. The training loss of the tokenizer is expressed as follows:\n\\mathcal{L}_{\\text{Tokenizer}} = ||X_R - X_S||_2^2 + \\frac{\\lambda}{M} \\sum_{i=1}^M ||\\text{sg}[z_i] - c_{s_i}||_2^2 + \\frac{\\beta}{M} \\sum_{i=1}^M ||\\text{sg}[c_{s_i}] - z_i||_2. (2)\nHere, the first term represents the alignment loss, the second term is a codebook loss aimed at updating the codebook embeddings, and the third term is a commitment loss that encourages the query to stay close to the chosen codebook embedding. sg[\u00b7] denotes the stop-gradient operator, and the hyperparameter \u03b2 is set to 0.25."}, {"title": "3.2 Unified Molecule-Text Language Model", "content": "Expanding Vocabulary. Employing the molecule tokenizer, a molecule can be tokenized into a molecule sequence {s_i}_{i=1}^{M_1} with causal dependency. The molecule sequence can be concatenated with the text sequence to form a multi-modal sequence {u_i}_{i=1}^L, where L is the length of the multi-modal sequence. To facilitate the representation of the multi-modal sequence, we construct the molecule vocabulary V_m = {v_m^k}_{k=1}^{K}, which maintains the order of the molecule codebook C = {c_i}_{i=1}^K.\nAdditionally, V_m includes several special tokens such as boundary indicators, e.g., [MOL] and [/MOL], to mark the beginning and end of the molecule sequence. Next, we merge the original text vocabulary V_t = {v_t^i}_{i=1}^T with the molecule vocabulary V_m. The unified molecule-text vocabulary V = {V_m, V_t} facilitates joint learning from molecules and text under a unified next-token-prediction objective. As the vocabulary is expanded, the corresponding embeddings and prediction layers also need to be extended, with the newly introduced parameters initialized randomly."}, {"title": "Unified Molecule-text Modeling.", "content": "The multi-modal sequence {u_i}_{i=1}^L is fed into the pretrained LLM for performing multi-modal autoregression. UniMoT adopts the general Language Modeling (LM) objective to directly maximize the log-likelihood of the data distribution:\n\\mathcal{L}_{\\text{LLM}} = - \\sum_{D \\in \\mathcal{D}} \\sum_{i \\in I} \\text{log } p(u_i | u_1, \u2026, u_{i-1}; \\theta), (3)\nwhere D represents the dataset, I represents the set of indices of the generation target, and \u03b8 denotes the parameters of the LLM. The unification of representation and training paradigm for molecules and text enhances the abilities of LLMs to understand molecule-text interactions and alignment. UniMoT can interpret molecules similar to understanding a foreign language, and generate them as if they were text. We conduct autoregressive pretraining on molecule-to-text and text-to-molecule tasks to enhance the molecule comprehension and generation capabilities."}, {"title": "Molecule-to-Text Autoregression.", "content": "While structural information is embedded in molecule features and captured by the molecule tokens through the tokenizer, we also aim to incorporate sequential information of molecules for better comprehension. Therefore, we concatenate the molecule sequence {s_i}_{i=1}^{M_1} with the SMILES [50] sequence and a prompt to form the multi-modal input sequence {u_i}_{i=1}^L, as illustrated in Figure 3a. The text sequence of the corresponding molecule caption is used as the generation target."}, {"title": "Text-to-Molecule Autoregression.", "content": "For molecule generation, a prompt and the molecule caption are concatenated, with a [MOL] token appended to signify the beginning of the molecule sequence, as illustrated in Figure 3b. The molecule sequence {s_i}_{i=1}^{M_1} produced by the tokenizer is used as the generation target. During inference, given a prompt and the molecule caption, the output molecule sequence can be decoded into the desired molecule by the pretrained adapter and SMILES decoder."}, {"title": "3.3 Training Strategy", "content": "The training strategy for UniMoT is structured across four stages. Stage-1 focuses on Causal Q-Former pretraining with tailored objectives. In Stage-2, the molecule tokenizer is optimized using the frozen encoders and decoder. Stage-3 integrates the tokenizer with a language model for multi-modal comprehension and generation. Finally, Stage-4 fine-tunes UniMoT for specific tasks, aligning it with human instructions and optimizing performance for various molecular applications. More details regarding the training process can be found in Appendix C."}, {"title": "Stage-1: Causal Q-Former Pretraining.", "content": "We connect the molecule encoder and Causal Q-Former, leveraging the pretrained MoleculeSTM molecule encoder [31]. The molecule encoder remains frozen while only the Causal Q-Former is updated. Both queries and text inputs are used, while only queries serve as input in subsequent stages. In our experiments, we utilize 16 queries. We employ three tailored objectives for the pretraining of the Causal Q-Former: Molecule-Text Contrastive Learning (MTC), Molecule-Text Matching (MTM), and Molecule-grounded Text Generation (MTG). The details of these objectives can be found in Appendix A."}, {"title": "Stage-2: Molecule Tokenizer Pretraining.", "content": "We connect the Causal Q-Former with subsequent blocks and use the objective defined in Equation (2). We employ the pretrained ChemFormer [17] as the generative model. Specifically, we leverage the SMILES encoder and the SMILES decoder provided by ChemFormer. The molecule codebook size is set to K = 2048. As shown in Figure 2, we keep the molecule encoder, the SMILES encoder, and the SMILES decoder frozen, while updating the Causal Q-Former, the learnable codebook, and the adapter."}, {"title": "Stage-3: Unified Molecule-Text Pretraining.", "content": "We integrate the molecule tokenizer with the LLM using the unified vocabulary of molecule tokens and text tokens. We employ the LM objective defined in Equation (3) to pretrain the LLM. Pretraining involves molecule-to-text autoregression and text-to-molecule autoregression, aimed at enhancing UniMoT's multi-modal comprehension and generation capabilities. To enhance efficiency, we train the LLM using LoRA tuning [14]."}, {"title": "Stage-4: Task-Specific Instruction Tuning.", "content": "UniMoT is fine-tuned on seven comprehension and generation tasks: molecular property prediction, molecule captioning, molecule-text retrieval, caption-guided molecule generation, reagent prediction, forward reaction prediction, and retrosynthesis. We also utilize LoRA tuning to improve efficiency. This stage ensures UniMoT can accurately interpret and respond to human instructions, making it versatile and effective for diverse molecular tasks."}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Molecule Comprehension Tasks", "content": "Molecular Property Prediction Task. The goal of molecular property prediction is to forecast a molecule's intrinsic physical and chemical properties. For the classification task, we incorporate eight binary classification datasets from MoleculeNet [53]. Models are tasked with generating a single prediction (\u201cyes\u201d or \u201cno\u201d). We compare UniMoT with the following baselines: KV-PLM [58], AttrMask [16], InfoGraph [40], MolCLR [48], GraphMVP [30], MoleculeSTM [31], and InstructMol [6]. The ROC-AUC (%) results on the MoleculeNet datasets are shown in Table 1. The performance of the regression task of molecular property prediction is provided in Appendix D. Compared to traditional graph learning methods and molecular LLMs like InstructMol [6], UniMoT demonstrates consistent improvements across the eight datasets, indicating its robust molecule comprehension abilities.\nMolecule Captioning Task. The molecule captioning task involves generating a comprehensive description of a molecule. We compare UniMoT with several baselines: MolT5 [11], MoMu [39], InstructMol [6], MolCA [33], and 3D-MOLM [25]. BLEU [37], ROUGE [27], and METEOR [1] are adopted as evaluation metrics. UniMoT is evaluated for molecule captioning on the PubChem [18]"}, {"title": "Molecule-Text Retrieval Task.", "content": "The molecule-text retrieval task involves using a molecule to retrieve text (M2T) and using text to retrieve a molecule (T2M). We compare UniMoT with several baselines: Sci-BERT [2], KV-PLM [58], MoMu [39], MoleculeSTM [31], MolCA [33], and 3D-MOLM [25]. We report the performance of retrieval using a batch of 64 random samples and the entire test set, evaluated with the metrics of Accuracy and Recall@20. We use the checkpoint from Stage-1 of pretraining. UniMoT is evaluated on the datasets of PubChem [18], PCdes [58], and MoMu [39]. Performance on the PubChem dataset is shown in Table 3, while performances on the PCdes and MoMu datasets are presented in Appendix D. UniMoT can understand complex molecule-text interactions through the introduction of the Causal Q-Former. From Table 3, UniMoT demonstrates superior performance over the baselines on molecule-to-text retrieval. This underscores UniMoT's capability in learning fine-grained alignment between molecules and text."}, {"title": "4.2 Molecule Generation Tasks", "content": "We employ molecule generation tasks, which encompass caption-guided molecule generation [12], reagent prediction [12], forward reaction prediction [12], and retrosynthesis [12]. Caption-guided molecule generation involves generating molecular structures based on textual descriptions. Reagent prediction entails determining suitable reagents given reactants and products. Forward reaction prediction involves predicting probable products given specific reactants and reagents. Retrosynthesis involves deconstructing a target molecule into simpler starting materials. We compare UniMoT with"}, {"title": "4.3 Ablation Studies", "content": "Cross-Modal Projector. We conducted an ablation study on the cross-modal projector, with the results on the molecule captioning task shown in Table 5. The linear projection demonstrated the worst performance, indicating that the molecule features lack textual information, thus hindering effective molecule-text interactions and alignment. Additionally, we compared the performance of a Q-Former with bidirectional self-attention to a Causal Q-Former with causal self-attention. The results show that queries with causal dependency outperform those with bidirectional dependency. This demonstrates that input with left-to-right causal dependency aligns with the unidirectional attention mechanism in LLMs, leading to improved performance.\nDiscrete vs. Continuous Representation. We compare the performance of continuous causal embeddings and discrete tokens quantized from causal embeddings as inputs to LLMs. As shown in Table 5, continuous embeddings demonstrate better performance than discrete tokens in understanding molecules. This result is reasonable since the quantization process causes information loss in discrete tokens. However, we still use discrete token representation to facilitate the autoregressive training paradigm of LLMs, which supports the unification of comprehension and generation tasks. To achieve this unification, we unavoidably sacrifice some performance in comprehension tasks.\nModel Size and Tuning Stategy. We conducted a comparison of molecule captioning performance across various model sizes and tuning strategies, as illustrated in Table 6. Our findings indicate that scaling up the LLM to 13B or adopting a full tuning strategy yields only marginal improvements in performance compared to using Llama-2-7B with LoRA tuning. While larger models and full tuning strategies might offer slight gains in performance, they come at a significant cost in terms of efficiency. Considering the trade-off between achieving high performance and maintaining efficiency, we have chosen to utilize Llama-2-7B with LoRA tuning in our experiments. This ensures that our model remains both powerful and practical."}, {"title": "5 Conclusion", "content": "This work introduces UniMoT, an innovative advancement in the field of molecular-textual understanding and generation, which successfully unifies these two distinct modalities under a coherent framework. By introducing a Vector Quantization-driven tokenizer, UniMoT overcomes previous architectural limitations where molecule and text modalities were not treated equally. This molecule tokenizer transforms molecules into sequences of discrete tokens, embedding high-level molecular and textual information. The LLM vocabulary is expanded with molecule tokens mapped from the learned codebook. Moreover, by employing a four-stage training scheme, UniMoT has emerged as a versatile multi-modal LLM, adept at handling both molecule-to-text and text-to-molecule tasks. Extensive empirical evaluations demonstrate that UniMoT attains state-of-the-art performance across diverse molecule comprehension and generation tasks."}, {"title": "A Details of Causal Q-Former", "content": "The Q-Former operates as a query-based transformer that utilizes learnable query vectors to interact with molecule features extracted by a frozen encoder. These queries are essential for extracting relevant information from the molecule features. The Q-Former comprises both a molecule transformer and a text transformer, sharing self-attention layers. The molecule transformer incorporates cross-attention layers between self-attention and feed-forward layers, while the text transformer architecture is based on BERT [9]. Q-Former employs a cross-attention mechanism where the query vectors selectively attend to different aspects of the molecule features, allowing the model to capture critical details necessary for understanding and generating textual descriptions of molecular properties.\nSpecifically, we incorporate causal masks into the queries, ensuring that they only interact with preceding queries. This ensures the sequence of queries maintains a causal dependency, aligning with the requirements of LLMs operating on text sequence. The Causal Q-Former is illustrated in Figure 4. We employ the Causal Q-Former to generate causal queries Z = {z_i}_{i=1}^M \u2208 R^{M\u00d7d} containing high-level molecular and textual information, where M represents the number of queries and d denotes the dimension of queries. Next, we introduce three tailored objectives MTC, MTM, and MTG for the pretraining of the Causal Q-Former."}, {"title": "Molecule-Text Contrastive Learning (MTC)", "content": "aims to align molecule and text features by maximizing their mutual information. This is achieved by maximizing the molecule-text similarity of positive pairs against that of negative pairs. We utilize the last query z_M of the query sequence {z_i}_{i=1}^M as the query representation, since the output query sequence is causal and the last query contains global information from the queries. For text representation, we use the output embedding of the [CLS] token, denoted as y. The contrastive learning loss is expressed as follows:\n\\mathcal{L}_{\\text{MTC}} = - \\frac{1}{B} \\sum_{i=1}^B \\text{log } \\frac{\\text{exp}((z_M^i)^T y^i / \\tau)}{\\sum_{j=1}^B \\text{exp}((z_M^i)^T y^j / \\tau)} - \\frac{1}{B} \\sum_{i=1}^B \\text{log } \\frac{\\text{exp}((y^i)^T z_M^i / \\tau)}{\\sum_{j=1}^B \\text{exp}((y^i)^T z_M^j / \\tau)}, (4)\nwhere B denotes the batch size, and \\tau represents the temperature parameter. Here, z_M^i and y^i refer to the i-th query representation and text representation in a batch, respectively."}, {"title": "Molecule-Text Matching (MTM)", "content": "focuses on learning fine-grained alignment between molecule and text features. As queries {z_i}_{i=1}^M capture both molecular and textual information through cross-attention and self-attention layers respectively, we utilize the last query z_M as input to a binary classifier. This classifier predicts whether a given molecule-text pair is matched or unmatched. The corresponding loss function is formulated as follows:\n\\mathcal{L}_{\\text{MTM}} = - \\frac{1}{B} \\sum_{i=1}^B \\text{log } \\frac{\\text{exp}(\\phi(z_M | X^i, t^i))}{\\sum_{j=1}^B \\text{exp}(\\phi(z_M | X^i, t^j)) + \\sum_{j=1}^B \\text{exp}(\\phi(z_M | X^j, t^i))}, (5)\nwhere \u03c6 represents a binary classifier, and X^i and t^i denote the i-th input molecule features and input text in a batch, respectively."}, {"title": "Molecule-grounded Text Generation (MTG)", "content": "focuses on generating textual descriptions given a molecule input. In this task, causal masks for queries are not applied since only textual output is required. However, causal masks are applied for text, allowing each text token to attend to its preceding text tokens and all queries, but not subsequent tokens. The Language Modeling (LM) loss function is applied to model the generation of text t^i conditioned on the molecule input X^i, formulated as:\n\\mathcal{L}_{\\text{MTG}} = - \\frac{1}{BL} \\sum_{i=1}^B \\sum_{j=1}^L \\text{log } p(t_j^i | t_1^i, \u2026, t_{j-1}^i, X^i), (6)\nwhere t_j^i represents the j-th token in the text sequence t^i. Here, X^i and t^i denote the i-th input molecule features and generated text in a batch, respectively.\nThe total loss for training the Causal Q-Former encompasses the three aforementioned objectives:\n\\mathcal{L}_{Q-\\text{Former}} = \\mathcal{L}_{\\text{MTC}} + \\mathcal{L}_{\\text{MTM}} + \\mathcal{L}_{\\text{MTG}}. (7)"}, {"title": "B Details of Datasets", "content": "This section provides detailed information about the datasets used in evaluating the performance of UniMoT across various tasks. The datasets are utilized for molecular property prediction, molecule captioning, molecule-text retrieval, caption-guided molecule generation, reagent prediction, forward reaction prediction, and retrosynthesis task. Each dataset serves a unique purpose in assessing different capabilities of the model. We provide a comprehensive overview of datasets, including their types, associated tasks, descriptions, URLs for access, and licensing information.\nWe present the details of the Molecular Property Prediction Datasets below:\n\u2022 BBBP [53]: The Blood-Brain Barrier Penetration dataset predicts the ability of molecules to penetrate the blood-brain barrier.\n\u2022 Tox21 [53]: This dataset is part of the Toxicology in the 21st Century initiative, used for toxicity prediction.\n\u2022 ToxCast [53]: Another toxicity prediction dataset with a broader range of biological assays.\n\u2022 Sider [53]: Side Effect Resource database, used for predicting drug side effects.\n\u2022 ClinTox [53]: Clinical Toxicity dataset for predicting clinical trial toxicity outcomes.\n\u2022 MUV [53]: Maximum Unbiased Validation dataset for virtual screening.\n\u2022 HIV [53]: Human Immunodeficiency Virus dataset for predicting anti-HIV activities.\n\u2022 BACE [53]: Beta-Secretase 1 dataset for predicting inhibitors of the BACE-1 enzyme, relevant for Alzheimer's research.\n\u2022 QM9 [12]: The quantum mechanics properties dataset, where the objective is to predict key quantum mechanics properties of a given molecule, such as HUMO, LUMO, and the HUMO-LUMO gap.\nWe present the details of the Molecule Captioning Datasets below:\n\u2022 PubChem [18]: A large dataset of chemical molecules used for generating textual descriptions of molecular structures.\n\u2022 ChEBI-20 [11]: A subset of the Chemical Entities of Biological Interest database, provides structured and detailed descriptions of molecules."}, {"title": "We present the details of the Molecule-Text Retrieval Datasets below:", "content": "\u2022 PubChem [18]: Used for both molecule-to-text (M2T) and text-to-molecule (T2M) retrieval tasks.\n\u2022 PCdes [58]: Another dataset for evaluating M2T and T2M retrieval accuracy.\n\u2022 MoMu [39]: Dataset specifically designed for molecule-text interactions and retrieval tasks."}, {"title": "We present the details of the Molecule Generation Datasets below:", "content": "\u2022 Mol-Instructions [12]: These datasets include tasks such as caption-guided molecule generation, reagent prediction, forward reaction prediction, and retrosynthesis. They are used to evaluate the model's ability to generate molecular structures based on textual descriptions and other related tasks.\n\u2022 PubChem [18]: Used for caption-guided molecule generation, generating molecular structures based on textual descriptions.\n\u2022 USPTO [12]: Used for reagent prediction, forward reaction prediction, and retrosynthesis, providing data for predicting reagents, reaction outcomes, and retrosynthetic pathways."}, {"title": "C Details of Training", "content": "Stage-1: Causal Q-Former Pretraining. During Stage-1, we only connect the molecule encoder and the Causal Q-Former, leaving out other blocks. We leverage the pretrained molecule encoder from MoleculeSTM [31], which has undergone extensive contrastive learning with molecule-text pairs. We utilize the PubChem [18] dataset for pretraining, keeping the molecule encoder frozen while updating only the Causal Q-Former. Both queries and text serve as input to the Causal Q-Former, while only queries serve as input in subsequent stages. Inspired by BLIP-2 [23], we employ three tailored objectives \u2013 Molecule-Text Contrastive Learning (MTC), Molecule-Text Matching (MTM), and Molecule-grounded Text Generation (MTG) \u2013 for the pretraining of the Causal Q-Former, as detailed in Appendix A.\nThe dimension of molecule features is set to 300. We use 16 queries, each with a dimension of 768. The size of Z (16 \u00d7 768) is much smaller than the size of molecule features X (e.g., 150 \u00d7 300). The Q-former is pretrained for 50 epochs. We adopt the AdamW optimizer with a weight decay of 0.05, and a cosine decay learning rate scheduler, with a minimal learning rate of 1e-5. The batch size is set to 64. The computational overhead for this pretraining is 20 GPU hours on 4 NVIDIA A100 GPUs.\nStage-2: Molecule Tokenizer Pretraining. We connect the Causal Q-Former with the subsequent blocks and train the molecule tokenizer using the objective defined in Equation (2). Following the approach of RetMol [49], we utilize SMILES strings [50] to represent molecules, and employ the pretrained ChemFormer [17] as the generative model. Specifically, we leverage the SMILES encoder and SMILES decoder components provided by ChemFormer. We utilize PubChem [18] and CheBI-20 [11] datasets, keeping the molecule encoder, SMILES encoder, and SMILES decoder frozen, while updating the Causal Q-Former, codebook, and adapter. Once optimized, the molecule tokenizer remains unchanged throughout the subsequent stages.\nThe molecule codebook size is set to K = 2048, and the dimension of codebook embedding is 768. The tokenizer is pretrained for 50 epochs. We adopt the AdamW optimizer with a weight decay of 0.05, and a cosine decay learning rate scheduler, with a minimal learning rate of le-5. The batch size is set to 64. The computational overhead for this pretraining is 40 GPU hours on 4 NVIDIA A100 GPUs.\nStage-3: Unified Molecule-Text Pretraining. We connect the molecule tokenizer with the LLM and employ the LM objective defined in Equation (3) to pretrain the LLM. We utilize Llama [44] as"}, {"title": "E Limitations", "content": "While UniMoT demonstrates considerable advancements in unifying molecule and text modalities for comprehensive understanding and generation tasks, several limitations must be acknowledged. Although Uni"}]}