{"title": "TRUNCATED CONSISTENCY MODELS", "authors": ["Sangyun Lee", "Yilun Xu", "Tomas Geffner", "Giulia Fanti", "Karsten Kreis", "Arash Vahdat", "Weili Nie"], "abstract": "Consistency models have recently been introduced to accelerate sampling from diffusion models by directly predicting the solution (i.e., data) of the probability flow ODE (PF ODE) from initial noise. However, the training of consistency models requires learning to map all intermediate points along PF ODE trajectories to their corresponding endpoints. This task is much more challenging than the ultimate objective of one-step generation, which only concerns the PF ODE's noise-to-data mapping. We empirically find that this training paradigm limits the one-step generation performance of consistency models. To address this issue, we generalize consistency training to the truncated time range, which allows the model to ignore denoising tasks at earlier time steps and focus its capacity on generation. We propose a new parameterization of the consistency function and a two-stage training procedure that prevents the truncated-time training from collapsing to a trivial solution. Experiments on CIFAR-10 and ImageNet 64 \u00d7 64 datasets show that our method achieves better one-step and two-step FIDs than the state-of-the-art consistency models such as iCT-deep, using more than 2\u00d7 smaller networks. Project page: https://truncated-cm.github.io/", "sections": [{"title": "1 INTRODUCTION", "content": "Diffusion models (Ho et al., 2020; Song et al., 2020) have demonstrated remarkable capabilities in generating high-quality continuous data such as images, videos, or audio (Ramesh et al., 2022; Ho et al., 2022; Huang et al., 2023). Their generation process gradually transforms a simple Gaussian prior into data distribution through a probability flow ordinary differential equation (PF ODE). Although diffusion models can capture complex data distributions, they require longer generation time due to the iterative nature of solving the PF ODE.\nConsistency models (Song et al., 2023) were recently proposed to expedite the generation speed of diffusion models by learning to directly predict the solution of the PF ODE from the initial noise in a single step. To circumvent the need for simulating a large number of noise-data pairs to learn this mapping, as employed in prior works (Liu et al., 2022b; Luhman & Luhman, 2021), consistency models learn to minimize the discrepancy between the model's outputs at two neighboring points along the ODE trajectory. The boundary condition at t = 0 serves as an anchor, grounding these outputs to the real data. Through simulation-free training, the model gradually refines its mapping at different times, propagating the boundary condition from t = 0 to the initial t = T.\nHowever, the advantage of simulation-free training comes with trade-offs. Consistency models must learn to map any point along the PF ODE trajectory to its corresponding data endpoint, as shown in Fig. 1a. This requires the learning of both denoising at smaller times on the PF ODE, where the data are only partially corrupted, and generation towards t = T, where most of the original data information has been erased. This dual task necessitates larger network capacity, and it is challenging for a single model to excel at both tasks. Our empirical observations in Fig. 2 demonstrate the model would gradually sacrifice its denoising capability at smaller times to trade for generation quality as training proceeds. While this behavior is desirable as the end goal is generation rather than denoising, we argue for explicit control over this trade-off, rather than allowing the model to allocate capacity"}, {"title": "2 PRELIMINARIES", "content": null}, {"title": "2.1 DIFFUSION MODELS", "content": "Diffusion models are a class of generative models that synthesize data by reversing a forward process in which the data distribution Pdata is gradually transformed into a tractable Gaussian distribution. In this paper, we use the formulation proposed in Karras et al. (2022), where the forward process is defined by the following stochastic differential equation (SDE):\n$$dxt = \\sqrt{2t} dwt,$$\nwhere t \u2208 [0, T] and wt is the standard Brownian motion from t = 0 to t = T. Here, we define pt as the marginal distribution of xt along the forward process, where po = Pdata. In this case, pt is a perturbed data distribution with the noise from N(0, t2I). In diffusion models, T is set to be large enough so that pr is approximately equal to a tractable Gaussian distribution N (0, T2I).\nDiffusion models come with the reverse probability flow ODE (PF ODE) that starts from t = T to t = 0 and yields the same marginal distribution pt as the forward process in Eq. (1) (Song et al., 2020):\n$$dxt = -tst(xt)dt,$$\nwhere st(xt) := \u2207x log pt (x) is the score function at time t\u2208 [0,T]. To draw samples from the data distribution Pdata, we first train a neural network to learn st(x) using the denoising score matching (Vincent, 2011), initialize x\u012b with a sample from N(0, T2I), and solve the PF ODE backward in time: x0 = x + \u222bT(-tst(xt))dt. However, numerically solving the PF ODE requires multiple forward passes of the neural score function estimator, which is computationally expensive."}, {"title": "2.2 CONSISTENCY MODELS", "content": "Consistency models instead aim to directly map from noise to data, by learning a consistency function that outputs the solution of PF ODE starting from any t \u2208 [0, T]. The desired consistency function f should satisfy the following two properties (Song et al., 2023): (i) f(x0, 0) = x0, and (ii) f(xt, t) = f(xs,s), \u2200(s,t) \u2208 [0, T]2. The first condition can be satisfied by the reparameterization\n$$f_{\\theta}(x, t) := C_{out}(t)F_{\\theta}(x, t) + C_{skip}(t)x,$$\nwhere \u03b8 is the parameter of the free-form neural network Fe : Rd \u00d7 R \u2192 Rd, and Cout(0) = 0, Cskip (0) = 1 following the similar design of Karras et al. (2022). Here, instead of training fe directly, we train a surrogate neural network Fe under the above reparameterization. The second condition can be learned by optimizing the following consistency training objective:\n$$L_{CT}(f_{\\theta}, f_{\\theta^-}) := E_{t \\sim \\nu_t, x \\sim P_{data}, \\epsilon \\sim N(0, I)}[\\frac{w(t)}{\\Delta_t}d(f_{\\theta}(x + t\\epsilon, t), f_{\\theta^-}(x + (t - \\Delta t)\\epsilon, t - \\Delta t))],$$\nwhere 0 = stopgrad(\u03b8), Vt denotes the probability of sampling time t that also represents the noise scale, e denotes the standard Gaussian noise, w(t) is a weighting function, d(\u00b7, \u00b7) is a distance"}, {"title": "3 TRUNCATED CONSISTENCY MODEL", "content": "Standard consistency models pose a higher challenge in training than many other generative models: instead of simply mapping noise to data, consistency models must learn the mapping from any point along the PF ODE trajectory to its data endpoint. Hence, a consistency model must divide its capacity between denoising tasks (i.e., mapping samples from intermediate times to data) and generation (i.e., mapping from pure noise to data). This challenge mainly contributes to consistency models' underperformance relative to other generative models with similar network capacities (see Table 1).\nInterestingly, standard consistency models navigate the trade-off between denoising and generation tasks implicitly. We observe that during standard consistency training, the model gradually loses its denoising capabilities at the low t. Specifically, Fig. 2 shows a trade-off in which, after some training iterations, denoising FIDs at lower t (t < 1) increase while the denoising FIDs at larger t (t > 1) (including the generation FID at the largest t = 80) continue to decrease. This suggests that the model struggles to learn to denoise and generate simultaneously, and sacrifices one for the other.\nTruncated consistency models (TCM) aim to explicitly control this tradeoff by forcing the consistency training to ignore the denoising task for small values of t, thus improving its capacity usage for generation. We thus generalize the consistency model objective in Eq. (4) and apply it only in the truncated time range [t', T] where the dividing time t' lies within (0, T). The time probability Vt in TCM only has support in [t', T] as a result.\nNaive solution A straightforward approach is to directly train a consistency model on the truncated time range. However, the model outputs can collapse to an arbitrary constant because a constant function (i.e., fe(x, t) = const) is a minimizer of the consistency training objective (Eq. (4)). In standard consistency models, the boundary condition f(x0, 0) = x0 prevents collapse, but in this naive example, there is no such meaningful boundary condition. For example, if the free-form neural network Fo(x,t) = -Cskip(t)x/Cout(t) for all t \u2208 [t', T], f\u00f8 (x, t) is 0, and thus Eq. (4) becomes zero. To handle this, we propose a two-stage training procedure and design a new parameterization with a proper boundary condition, as outlined below.\nProposed Solution Truncated consistency models conduct training in two stages:\n1. Stage 1 (Standard consistency training): We pretrain a consistency model to convergence in the usual fashion, with the training objective in Eq. (4); we denote the pre-trained model as foo.\n2. Stage 2 (Truncated consistency training): We initialize a new consistency model fe with the first-stage pretrained weights fe, and train over a truncated time range [t', T]. The boundary condition at time t' is provided by the pretrained foo. This stage is explained further below.\nTo explain the details of TCM, we first introduce the following parameterization:\n$$f_{\\theta, 0, 0^-(x,t)} = f_{\\theta}(x, t) \\cdot 1\\{t > t'\\} + f_{\\theta^-}(x, t) \\cdot 1\\{t < t'\\},$$"}, {"title": "4 EXPERIMENTS", "content": "In this section, we evaluate TCM on standard image generation benchmarks and compare it against state-of-the-art generative models. We begin by detailing the experimental setup in Sec. 4.1. We then study the behavior of denoising FID and its impact on generation FID in Sec. 4.2. We benchmark TCM against a variety of existing methods in Sec. 4.3, and provide detailed analysis on various design choices in Sec. 4.4."}, {"title": "4.1 SETUP", "content": "We evaluate TCM on the CIFAR-10 (Krizhevsky et al., 2009) and ImageNet 64\u00d764 (Deng et al., 2009) datasets. We consider the unconditional generation task on CIFAR-10 and class-conditional generation on ImageNet 64\u00d764. We measure sample quality with Fr\u00e9chet Inception Distance (FID) (Heusel et al., 2017) (lower is better), as is standard in the literature.\nFor consistency training in TCM, we mostly follow the hyperparameters in ECT (Geng et al., 2024), including the discretization curriculum and continuous-time training schedule. For all experiments, we choose a dividing time t' = 1 and set Vt to the log-Student-t distribution. We use w\u044c = 0.1 and p = 0.25 for the boundary loss. We discuss these choices in Sec. 4.4. In line with Geng et al. (2024), we initialize the model with the pre-trained EDM (Karras et al., 2022) / EDM2 (Karras et al., 2024) for CIFAR-10 / ImageNet 64 \u00d7 64, respectively. On CIFAR-10, we use a batch size of 512 and 1024 for the first and the second stage, respectively. On ImageNet with EDM2-S architecture, we use a batch size of 2048 and 1024 for the first and the second stage, respectively. For EDM2-XL, to save compute, we initialize the truncated training stage with the pre-trained checkpoint from the ECM work (Geng et al., 2024) that performs the standard consistency training, and conduct the second-stage training with a batch size of 1024. Please see Appendix D for more training details."}, {"title": "4.2 TRUNCATED TRAINING ALLOCATES CAPACITY TOWARD GENERATION", "content": "Our proposed TCM aims to explictly reallocate network capacity towards generation by de-emphasizing denoising tasks at smaller t's. Empirical analysis in Fig. 3 further characterizes this behavior, showing a rapid increase in dFIDs at smaller t's below the threshold t' during the truncated training stage. Conversely, dFIDs continue to decrease at larger t's. In addition, TCM exhibit a more pronounced \"forgetting\" of the denoising task compared to consistency training (Fig. 2) at earlier times. For instance, dFID at t = 0.2 increases up to 3.5 in the truncated training, whereas it remains below 1 in the standard consistency training. TCM also significantly accelerate the process of forgetting the denoising tasks at these earlier times, achieving a substantially improved generation FID. This suggests that by explicitly controlling the training time range, the neural network can effectively shift its capacity towards generation.\nFig. 1(b) demonstrates how this reallocation of network capacity directly translates to improved sample quality and training stability. For CIFAR-10/ ImageNet 64 \u00d7 64, the truncated training stage (Stage 2) is initialized from the Stage 1 model at 250K / 150K iterations, respectively. We can see that the truncated training improves FID over the consistency training on the two datasets. Moreover, we find that the truncated training is more stable than the original consistency training, as their ImageNet FID blows up after 150K iterations, while TCM continues to improve FID from 2.83 to 2.46, showcasing its robustness (See Figure 6 for more analysis)."}, {"title": "4.3 TCM IMPROVES THE SAMPLE QUALITY OF CONSISTENCY MODELS", "content": "To demonstrate the effectiveness of TCM, we compare our method with three lines of works that distill diffusion models to one or two steps: (i) consistency models (Song & Dhariwal, 2023; Kim et al., 2023; Geng et al., 2024) that distills the PF ODE mapping in a simulation-free manner; (ii) variational score distillation (Yin et al., 2024b; Luo et al., 2024; Zhou et al., 2024) that performs distributional matching by utilizing the score of pre-trained diffusion models; (iii) knowledge distillation (Luhman & Luhman, 2021; Zheng et al., 2022; Berthelot et al., 2023; Salimans & Ho, 2022) that distill the PF ODE through off-line or on-line simulation using the pre-trained diffusion models. We exclude the methods that additionally use the GAN loss, which causes more training difficulties, for fair comparison.\nResults. In Table 1 and Table 2, we re-port the sample quality measured by FID and the sampling speed measured by the number of function evaluations (NFE), on CIFAR-10 and ImageNet-64\u00d764, re-spectively. We also include the number of model parameters. Our main find-ings are: (1) TCM significantly out-performs improved Consistency Train-ing (iCT) (Song & Dhariwal, 2023), the state-of-the-art consistency model, across datasets, number of steps and net-work sizes. For example, TCM improves the one-step FID from 2.83 / 4.02 in iCT to 2.46 / 2.88, on CIFAR-10 / ImageNet. Further, TCM's one-step FID even rivals iCT's two-step FID on both datasets. When using EDM2-S model, TCM also surpasses iCT-deep, which uses 2\u00d7 deeper networks, in both one-step (2.88 vs 3.25) and two-step FIDs (2.31 vs 2.77) on ImageNet. (2) TCM beats all the knowledge distillation methods and performs competitively to variational score distillation methods. Note that TCM do not need to train additional neural networks as in VSD methods, or to run simulation as in knowledge distillation methods. (3) Two-step TCM performs comparably to the multi-step EDM (Karras et al., 2022), the state-of-the-art diffusion model. For example, when both using the same EDM network, two-step TCM obtains a FID of 2.05 on CIFAR-10, which is close to 1.97 in EDM with 35 sampling steps. We further provide the uncurated one-step and two-step generated samples in Fig. 5. Please see Appendix E for more samples."}, {"title": "4.4 ANALYSES OF DESIGN CHOICES", "content": "Time sampling distribution Vt. We explore various time sampling distributions It supported on [t', T], and find that the truncated log-Student-t distribution works best (i.e., ln(t) follows Student-t distribution). The Student-t distribution, being heavier-tailed than the Gaussian distribution employed in previous consistency training (Song & Dhariwal, 2023; Geng et al., 2024), inherently allocates more probability mass towards larger t's. This aligns with the motivation of TCM, which emphasizes enhancing generation capabilities at later times. The degree of freedom v effectively controls the thickness of the tail, with the Student-t distribution converging to a Gaussian distribution as v \u2192 \u221e. Figure 4a shows the shape of Vt with varying standard deviation \u03c3 and the degree of freedom \u03bd in three cases: (1) heavy-tailed and a low probability mass around small t's (\u03c3 = 2, \u03bd = 10000), (2) heavy-tailed and a high probability mass around small t's (\u03c3 = 0.2, \u03bd = 0.01), (2) light-tailed"}, {"title": "5 RELATED WORK", "content": "Consistency models. Song et al. (2023) first proposed consistency models as a new class of generative models that synthesize samples with a single network evaluation. Later, Song & Dhariwal (2023); Geng et al. (2024) presented a set of improved techniques to train consistency models for better sample quality. Luo et al. (2023) introduced latent consistency models (LCM) to accelerate the sampling of latent diffusion models. Kim et al. (2023) proposed consistency trajectory models (CTM) that generalize consistency models by enabling the prediction between any two intermediate points on the same PF ODE trajectory. The training objective in CTM becomes more challenging than standard consistency models that only care about the mapping from intermediate points to the data endpoints."}, {"title": "6 CONCLUSION", "content": "We have introduced a truncated consistency training method that significantly enhances the sample quality of consistency models. To generalize consistency models to the truncated time range, we have proposed a new parameterization of the consistency function and a two-stage training process that explicitly allocates network capacity towards generation. We also discussed about our design choices arising from the new training paradigm. Our approach achieves superior performance compared to state-of-the-art consistency models, as evidenced by improved one-step and two-step FID scores across different datasets and network sizes. Notably, these improvements are achieved while utilizing similar or even smaller network architectures than baselines.\nLimitation. TCM introduces a slight increase in computational cost due to the additional boundary loss in Eq. (10), beyond the standard consistency training loss. Standard consistency training necessitates two forward passes per training iteration, while our parameterization (Eq. 5) requires three. Also, our method incurs a minor additional memory cost as we need to maintain a pre-trained consistency model (in evaluation mode) for the boundary loss. We observe that on ImageNet 64\u00d764 with EDM2-S, TCMs have an 18% increase in training time and an 15% increase in memory cost."}, {"title": "7 REPRODUCIBILITY STATEMENT", "content": "We provide sufficient details for reproducing our method in the main paper and also in the Appendix. D, including a pseudo code of the training algorithm, model initialization and architecture, model parameterization, learning rate schedules, time step sampling procedures, and other training details. We also specify hyperparameter choices like the dividing time t', boundary loss weight w\u044c, and boundary ratio p. Additionally, we discuss the computational costs of our method compared to standard consistency training. For evaluation, we describe our sampling procedure for both one-step and two-step generation."}, {"title": "8 ETHICS STATEMENT", "content": "This paper raises similar ethical concerns to other papers on deep generative models. Namely, such models can be (and have been) used to generate harmful content, such as disinformation and violent imagery. We advocate for the responsible deployment of such models in practice, including guardrails to reduce the risk of producing harmful content. The design of these protections is orthogonal to our work. Other ethical concerns may arise regarding the significant resource costs required to train and use deep generative models, including energy and water usage. This work increases the training cost of consistency models, but it also enables the models to be run with only 1 NFE and requires smaller neural network architectures, both of may which reduce inference-time costs relative to other diffusion-based models. Nonetheless, the environmental impact of training and deploying deep generative models remains an important limitation."}, {"title": "A ADDITIONAL EXPERIMENTS", "content": "Fig. 6 shows that the gradient spikes during the first stage training while the second stage training is relatively smooth. We hypothesize that the truncated training is more stable because it is less affected by the biased gradient norms across different t.\nFig. 7 shows the dFIDt evolution during the standard consistency training. We see that dFIDs at larger t's start from larger values and converges more slowly.\nAdding an intermedate training stage In our parameterization Eq. (5), we only use the pre-trained model fo in [0, t'). One may wonder if we can fine-tune fe on the truncated time range [0, t') to provide a better boundary condition for the truncated training. We find that although doing so improved the dFIDt of fe from 1.51 to 1.43, it led to a worse final FID of >2.7 for the truncated consistency model, regardless of whether we initialized fe with the pre-trained model or the fine-tuned model. In contrast, our proposed method achieved an FID of 2.61 with the same hyperparameters. We hypothesize that fine-tuning the pre-trained model on the truncated time range [0, t') makes the model fe forget about how the early mappings properly propagate to the later mappings in the range of [t', T]. This may hinder the learnability of its mapping at the boundary time, making it harder for fe to transfer the knowledge learned in fo, to its generation capability."}, {"title": "B BACKGROUND ON CONSISTENCY MODELS", "content": "Most of this part has been introduced by previous works (Song et al., 2023; Song & Dhariwal, 2023). Here, we introduce the background of consistency models, in particular the relationship between consistency training and consistency distillation, for completeness."}, {"title": "B.1 DEFINITION OF CONSISTENCY FUNCTION", "content": null}, {"title": "B.1.1 PROBABILITY FLOW ODE", "content": "The probability flow ODE (PF ODE) of Karras et al. (2022) is as follows:\n$$dxt = -tst(xt)dt,$$\nwhere st(xt) is the score function at time t \u2208 [0,T]. To draw samples from the data distribution Pdata, we initialize x\u012b with a sample from N(0, T2I) and solve the PF ODE backward in time. The solution x\u2081 = x + \u222bT(-tst(xt))dt is distributed according to Pdata."}, {"title": "B.1.2 CONSISTENCY FUNCTION", "content": "Integrating the PF ODE using numerical solvers is computationally expensive. Consistency function instead directly outputs the solution of the PF ODE starting from any t \u2208 [0, T]. The consistency function f satisfies the following two properties:\n1. f(x0, 0) = X0.\n2. f(xt, t) = f(xs, s) \u2200(s,t) \u2208 [0, T]2.\nThe first condition can be trivially satisfied by setting f(x, t) = Cout(t) F(x, t) + Cskip(t)x where Cout(0) = 0 and Cskip(0) = 1 following EDM (Karras et al., 2022). The second condition can be satisfied by optimizing the following objective:\n$$min E_{s,t,x, \\theta} [d(f(x_t, t), f(x_s, s))],$$\nwhere d is a function satisfying:\n1. d(x, y) = 0 <=> x = y.\n2. d(x, y) \u2265 0.\n3. dd(x,y)\ndy|y=x = 0\n4. and are well-defined and bounded."}, {"title": "B.2 CONSISTENCY DISTILLATION", "content": null}, {"title": "B.2.1 OBJECTIVE", "content": "In practice, Song et al. (2023) consider the following objective instead:\n$$min E_{t,x_t} [d(f_{\\theta}(x_t, t), f_{\\theta^-}(x_{t-\\Delta_t}, t - \\Delta_t))],$$\nwhere we parameterize the consistency function f with a neural network fe, and 0 < t < t. Here, fo is the identical network with stop gradients applied and is called teacher. Since \u2206t > 0, the teacher always receives the less noisy input, and the student fe is trained to mimic the teacher. Optimizing Eq. (14) requires computing xt-\u25b3\u2081, which we can be approximated using one step of Euler's solver:\n$$x_{t-\\Delta_t} = x_t + \\int_{t-\\Delta_t}^{t} (-u s_u(x_u)) du \\approx x_t + ts_t(x_t) \\Delta_t.$$\nWhen st(xt) is approximated by a pre-trained score network, Eq. (14) becomes the consistency distillation objective in Song et al. (2023). If At is sufficieintly small, the approximation in Eq. (15) is quite accurate, making LCD a good approximation of Eq. (14). The precision of the approximation depends on At and also the trajectory curvature of the PF ODE."}, {"title": "B.2.2 GRADIENT WHEN \u2206 \u2192 0", "content": "Let us rewrite Eq. (14) as follows:\n$$E_{t,x_t} [d(f_{\\theta}(x_t, t), f_{\\theta^-}(x_{t-\\Delta_t}, t - \\Delta_t))]$$\n$$= E_{t,x_t} [d(f_{\\theta}(x_t, t), f_{\\theta}(x_t, t) + f_{\\theta^-}(x_{t-\\Delta_t}, t - \\Delta_t) - f_{\\theta}(x_t, t))]$$\n$$= E_{t,x_t} [d(f_{\\theta}(x_t, t), f_{\\theta}(x_t, t)) + \\frac{\\partial d}{\\partial y}|_{y} \\Delta y + \\frac{1}{2} \\frac{\\partial^2 d}{\\partial y^2} \\Delta y^T {\\frac{\\partial^2 d}{\\partial y \\partial y}} \\Delta y + O(||\\Delta y||^3)]$$\n$$= E_{t,x_t} [(\\frac{\\partial d}{\\partial y}) \\Delta y + \\frac{1}{2} (\\Delta y)^T {\\frac{\\partial^2 d}{\\partial y \\partial y}} \\Delta y + O(||\\Delta y||^3)]$$\n, where we define \u2206y = fe- (xt- t, t - \u2206t) \u2013 fe (xt, t).\nLet's take the derivative with respect to \u03b8:\n$$ \\frac{1}{2} \\frac{\\partial}{\\partial \\theta} E_{t,x_t} [(\\Delta y)^T {\\frac{\\partial^2 d}{\\partial y \\partial y}} (f_{\\theta^-}(x_{t-\\Delta_t}, t - \\Delta_t) - f_{\\theta}(x_t, t)) + O(||\\Delta y||^3)]$$\n$$= E_{t,x_t} [{\\frac{\\partial^2 d}{\\partial y^2}}  ({\\frac{\\partial^2 d}{\\partial y \\partial y}}  (f_{\\theta^-}(x_{t-\\Delta_t}, t - \\Delta_t) - f_{\\theta}(x_t, t))  + O(||\\Delta y||^3)].$$\nAs\n$$f_{\\theta^-}(x_{t-\\Delta_t}, t - \\Delta_t) = f_{\\theta^-}(x_t, t) +  {\\frac{\\partial f_{\\theta^-}}{\\partial x_t}}{\\frac{\\partial x_t}{\\partial t}} \\Delta t  + O(\\Delta t),$$\nwe have\n$$f_{\\theta^-}(x_{t-\\Delta_t}, t - \\Delta_t) - f_{\\theta^-}(x_t, t) = ({\\frac{\\partial f_{\\theta^-}}{\\partial x_t}}{\\frac{\\partial x_t}{\\partial t}} + {\\frac{\\partial f_{\\theta^-}}{\\partial t}}) \\Delta + O(\\Delta).$$Since fe (xt, t) has the same value as fe- (xt, t), we can plug this into Eq. (21):\n$$= E_{t,x_t} [{\\frac{\\partial^2 d}{\\partial y^2}}  (({\\frac{\\partial f_{\\theta^-}}{\\partial x_t}}{\\frac{\\partial x_t}{\\partial t}} + {\\frac{\\partial f_{\\theta^-}}{\\partial t}}) \\Delta + O(\\Delta) + O(||\\Delta y||^3)]$$\n$$= E_{t,x_t} [{\\frac{\\partial^2 d}{\\partial y^2}} ({\\frac{\\partial f_{\\theta^-}}{\\partial x_t}}{\\frac{\\partial x_t}{\\partial t}} {\\frac{\\partial f_{\\theta^-}}{\\partial \\theta}} + {\\frac{\\partial f_{\\theta^-}}{\\partial t}} {\\frac{\\partial f_{\\theta^-}}{\\partial \\theta}})\\Delta t + O(\\Delta^2)+ O(||\\Delta y||^3)]$$\n$$= E_{t,x_t} [{\\frac{\\partial^2 d}{\\partial y^2}} (({\\frac{\\partial f_{\\theta^-}}{\\partial x_t}}{\\frac{\\partial x_t}{\\partial t}} + {\\frac{\\partial f_{\\theta^-}}{\\partial t}}) \\Delta t + O(\\Delta^2)].$$As the gradient is \u039f(\u2206t), it becomes zero when \u2206t \u2192 0, so it cannot be used for training. To make the gradient non-zero, Song et al. (2023) divide the by At. Then, we have\n$$ \\frac{\\partial}{\\partial \\theta} L_{CD}(0,0^-) = E_{t,x_t} [\\frac{1}{\\Delta_t} d(f_{\\theta}(x_t, t), f_{\\theta^-}(x_{t-\\Delta_t}, t - \\Delta_t))]$$\n$$ = E_{t,x_t} [\\frac{\\partial^2 d}{\\partial y^2} ({\\frac{\\partial f_{\\theta^-}}{\\partial x_t}}{\\frac{\\partial x_t}{\\partial t}} {\\frac{\\partial f_{\\theta^-}}{\\partial \\theta}} + {\\frac{\\partial f_{\\theta^-}}{\\partial t}} {\\frac{\\partial f_{\\theta^-}}{\\partial \\theta}}) + O(\\Delta)]$$\n$$= -E_{t,x_t} [{\\frac{\\partial^2 d}{\\partial y^2}} (({\\frac{\\partial f_{\\theta^-}}{\\partial x_t}} +  {\\frac{\\partial f_{\\theta^-}}{\\partial t}}) \\frac{\\partial f_{\\theta^-}}{\\partial \\theta} ]$$\n$$ = -E_{t,x_t} [{\\frac{\\partial^2 d}{\\partial y^2}} ({\\frac{\\partial f_{\\theta^-}}{\\partial x_t}} (-t \\cdot s_t(x_t)) +  {\\frac{\\partial f_{\\theta^-}}{\\partial t}})  \\frac{\\partial f_{\\theta^-}}{\\partial \\theta}]$$\n$$= E_{t,x_t} [{\\frac{\\partial^2 d}{\\partial y^2}} ((t \\cdot s_t(x_t)) +  {\\frac{\\partial f_{\\theta^-}}{\\partial t}})  \\frac{\\partial f_{\\theta^-}}{\\partial \\theta}]$$\nin the limit of At \u2192 0."}, {"title": "B.3 CONSISTENCY TRAINING", "content": "Song et al. (2023) show that Eq. (31) can be estimated without a pre-trained score network. From Tweedie's formula, we express the score function as\n$$s_t(x_t) = \\frac{E_{p(x | x_t)}[x", "theta}}": "E_{t,x_t}  [\\frac{2 \\partial^2 d}{\\partial y^2}  \\frac{2 \\partial f_{\\theta^-}}{\\partial ("}]}