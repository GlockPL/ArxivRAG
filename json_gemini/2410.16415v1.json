{"title": "On conditional diffusion models for PDE simulations", "authors": ["Aliaksandra Shysheya", "Cristiana Diaconu", "Federico Bergamin", "Paris Perdikaris", "Richard E. Turner", "Jos\u00e9 Miguel Hern\u00e1ndez-Lobato", "Emile Mathieu"], "abstract": "Modelling partial differential equations (PDEs) is of crucial importance in science and engineering, and it includes tasks ranging from forecasting to inverse problems, such as data assimilation. However, most previous numerical and machine learning approaches that target forecasting cannot be applied out-of-the-box for data assimilation. Recently, diffusion models have emerged as a powerful tool for conditional generation, being able to flexibly incorporate observations without retraining. In this work, we perform a comparative study of score-based diffusion models for forecasting and assimilation of sparse observations. In particular, we focus on diffusion models that are either trained in a conditional manner, or conditioned after unconditional training. We address the shortcomings of existing models by proposing 1) an autoregressive sampling approach, that significantly improves performance in forecasting, 2) a new training strategy for conditional score-based models that achieves stable performance over a range of history lengths, and 3) a hybrid model which employs flexible pre-training conditioning on initial conditions and flexible post-training conditioning to handle data assimilation. We empirically show that these modifications are crucial for successfully tackling the combination of forecasting and data assimilation, a task commonly encountered in real-world scenarios.", "sections": [{"title": "1 Introduction", "content": "Partial differential equations (PDEs) are ubiquitous as they are a powerful mathematical framework for modelling physical phenomena, ranging from the motion of fluids to acoustics and thermodynamics. Applications are as varied as weather forecasting [4], train aerodynamics profiling [70], and concert hall design [76]. PDEs can either be solved through numerical methods like finite element methods [43], or, alternatively, there has been a rising interest in leveraging deep learning to learn neural approximations of PDE solutions [56]. These approaches have shown promise for generating accurate solutions in fields such as fluid dynamics and weather prediction [37, 52, 54].\nSome of the most common tasks within PDE modelling are 1) forecasting, where the goal is to generate accurate and long rollouts based on some initial observations, and 2) inverse problems,"}, {"title": "2 Background", "content": "Continuous-time diffusion models. In this section we recall the main concepts underlying continuous diffusion models, and refer to Song et al. [68] for a thorough presentation. We consider a forward noising process (x(t))t>o associated with the following stochastic differential equation (SDE)\ndx(t) = -x(t)dt + dw(t), x(0) ~ po,\n(1)\nwith w(t) an isotropic Wiener process, and po the data distribution. The process defined by (1) is the well-known Ornstein-Uhlenbeck process, which geometrically converges to the standard Gaussian distribution N (0, I). Additionally, for any t \u2265 0, the noising kernel of (1) admits the following form Pto(x(t)|x(0)) = N(x(t)|\u03bc\u2081x(0), \u03c3\ubaafI), with \u00b5\u2081 = e-t/2 and o\u1f33 = 1 - e-t. Importantly, under mild assumptions on po, the time-reversal process (x(t))t>o also satisfies an SDE [10, 24] which is"}, {"title": "3 PDE surrogates for forecasting and data assimilation", "content": "In this section, we discuss the design space of diffusion models, outlined in Tab. 1, for tackling forecasting and DA in the context of PDE modelling.\nProblem setting. We denote by P : U \u2192 U a differential operator taking functions u : R+ \u00d7 Z \u2192 X \u2208 U as input. Along with some initial u(0, \u00b7) = 40 and boundary u(\u03c4, dZ) = f conditions, these define a partial differential equation (PDE) d\u30f6u = P(u; a) = P (dzu, dzu, . . . ; a) with coefficients a and where du is a shorthand for the partial derivative du/dr, and dzu, dzu denote the partial"}, {"title": "3.1 Learning the score", "content": "Learning a diffusion model over the full joint distribution p(x1:L(t)) can become prohibitively expensive for long trajectories, as it requires a score network taking the full sequence as input-Se(t, x1:L(t)), with memory footprint scaling linearly with the length L of the sequence.\nOne approach to alleviate this, as suggested by Rozet and Louppe [60], is to assume a Markov structure of order k such that the joint over data trajectories can be factorised into a series of conditionals p(x1:L) = p(x1)p(x2|x1)...P(xk+1|X1:k) \u03a0\u2081=k+2P(xi|Xi-k:i\u22121). Here we are omitting the time dependency X1:L = X1:L(0) for clarity's sake. The score w.r.t. xi can be written as\nL\ni+k\nVz\u2081 log p(x1:L) = \u221axi log p(xi Xi-ki-1) + \u2211\u221ax log p(xjXj-k:j\u22121)\nj=i+1\n= \u2207xi log p(xi, Xi+1:i+k|Xi\u2212k:i\u22121) = \u2207xi log p(xi\u2212k:i+k)\n(7)\nwith k + 1 < i < L \u2212 k \u2212 1, whilst formulas for i \u2264 k and i > L - k can be found in App. B.1.\nIn the case of modelling the conditional p(x1:L|y), we use the same Markov assumption, such that learning V\u2081 log p(x1:L|y) reduces to learning a local score Vx\u2081 log p(xi\u2212k:i+k|y). The rest of Sec. 3.1 describes learning the local score of the joint distribution, but the same reasoning can easily be applied to fitting the local score of the conditional distribution.\nWhen noising the sequence with the SDE (1), there is no guarantee that 11:L(t) will still be k-order Markov. However, we still assume that x1:L(t) is Markov but with a larger order k' > k, as motivated in Rozet and Louppe [60, Sec 3.1]. To simplify notations we use k' = k in the rest of the paper. Consequently, instead of learning the entire joint score at once \u220721:1 (t) log p(x1:L (t)), we only need to learn the local scores se(t, xi\u2212k:i+k(t)) \u2248 \u2200xi-k:i+k(t) log Pt (Xi\u2212k:i+k(t)) with a window size of 2k + 1. The main benefit is that the network only takes as input sequences of size 2k + 1 instead of L, with k << L. The local score network se is trained by minimising the following DSM loss L(0):\nE ||so(t, Xi\u2212k:i+k(t)) \u2013 \u2207 log Pt|0(Xi\u2212k:i+k(t)|Xi\u2212k:i+k)||2\nwhere the expectation is taken over the joint i ~ U([k+1, L \u2212 k]), t ~ U([0, 1]), xi\u2212k:i+k ~ Po and Xi-k:i+k(t) ~ Pt|0(\u00b7|Xi\u2212k:i+k) given by the noising process."}, {"title": "3.2 Conditioning", "content": "Reconstruction guidance. We now describe how to tackle forecasting and DA by sampling from conditionals p(x1:L|xo)\u2500instead of the prior p(x1:L)\u2014with the trained joint local score Se(t, Xi-k:i+k(t)) described in Sec. 3.1. For both tasks, the conditioning information y = x, \u2208 RO is a measurement of a subset of variables in the space-time domain, i.e. x, = A(x) = A vec(x) + \u03b7 with vec(x) \u2208 RN the space-time vectorised trajectory, \u03b7 ~ N(0,021) and a masking matrix A = {0,1}O\u00d7N where rows indicate observed variables with 1. Plugging this in (6) and computing the score we get the following reconstruction guidance term\n2\n1\nVlogp(xo|x(t)) \u2248 2 + oz (y \u2013 A(x(t))) TA\ndx(x(t))\ndx(t)\n(8)"}, {"title": "3.3 Rollout strategies", "content": "In this section, we describe sampling approaches that can be used to generate PDE rollouts, depending on the choice of local score and conditioning. We provide in App. K pseudocode for each rollout strategy.\nAll-at-once (AAO). With a trained joint score se(t, xi\u2212k:i+k(t)) we can generate entire trajectories X1:L in one go, as in Rozet and Louppe [60]. The full score se(t, x1:L(t)) \u2248 \u2207 log Pt (x1:L (t)) is reconstructed from the local scores, as shown in App. B.1 and summarised in Rozet and Louppe [60, Alg 2.]. The ith component is given by si = se(t, Xi\u2212k:i+k(t))[k + 1] which is the central output of the score network, except for the start and the end of the sequence for which the score is given by S1:k = Se(t, x1:2k+1(t))[: k + 1] and SL\u2212k:L = Se(t, XL\u22122k:L(t))[k + 1 :], respectively. If we observe some values x\u3002~ N(\u00b7|Ax1:1L, 27), the conditional sampling is done by summing the full score so (t, x1:L(t)) with the guidance term \u2207 log p(xo|x1:L(t)) estimated with (8).\nAutoregressive (AR). Let us assume that we want to sample XC+1:L given X1:C initial states. An alternative approach to AAO is to factor the forecasting prediction problem as p(x1:L|X1:C) =\n\u041fi\u0440(\u0445\u0456 \u0425\u0456-\u0421:i-1) with C < k. As suggested in Ho et al. [27], it then follows that we can sample the full trajectory via ancestor sampling by iteratively sampling each conditional diffusion process. More generally, instead of sampling one state at a time, we can autoregressively generate P states, conditioning on the previous C ones, such that P + C = 2k + 1. This procedure can be used both by the joint and the amortised conditional score. DA can similarly be tackled by further conditioning on additional observations appearing within the predictive horizon P at each AR step of the rollout.\nScalability. Both sampling approaches involve a different number of neural function evaluations (NFEs). Assuming p predictor steps and e corrector steps to simulate (4), AAO sampling requires (1 + c)p NFEs. In contrast, the AR scheme is more computationally intensive as each autoregressive AR steps. We stress, though, that this is"}, {"title": "4 Related work", "content": "In the following, we present works on diffusion models for PDE modelling since these are the models we focus on in this paper. We present an extended related work section in App. C, where we broadly discuss ML-based and classical solver-based techniques for tackling PDE modelling.\nDiffusion models for PDEs. Recently, several works have leveraged diffusion models for solving PDEs, with a particular focus on fluid dynamics. Amortising the score network on the initial state, Kohl et al. [35] introduced an autoregressive scheme, whilst Yang and Sommer [81] suggested to directly predict future states. Recently, Cachay et al. [9] proposed to unify the denoising time and the physical time to improve scalability. Lippe et al. [42] built upon neural operators with an iterative denoising refinement, particularly effective to better capture higher frequencies and enabling long rollouts. In contrast to the above-mentioned work, others have tackled DA and super-resolution tasks. Shu et al. [63] and Jacobsen et al. [32] suggested using the underlying PDE to enforce adherence to physical laws. Huang et al. [29] used an amortised model and inpainting techniques [46] for conditioning at inference time to perform DA on weather data. Rozet and Louppe [60] decomposed the score of long trajectory into a series of local scores over short segments to be able to work with flexible lengths and to dramatically improve scalability w.r.t. memory use. Concurrently to our work, Qu et al. [55] extended this approach to latent diffusion models to perform DA on ERA5 weather data [25]. In this work, we show that such a trained score network can alternatively be sampled autoregressively, which is guaranteed to lead to a higher Markov order, and empirically produce accurate long-range forecasts. Concurrently, Ruhe et al. [61] proposed to use a local score, not justified by any Markov assumption, together with a frame-dependent noising process for video and fluid dynamics generation. Diffusion models for PDE modelling are indeed closely related to other sequence modelling tasks, such as video generation and indeed our universal amortised approach is influenced by H\u00f6ppe et al. [28], Voleti et al. [75]."}, {"title": "5 Experimental results", "content": "We study the performance of the diffusion-based models proposed in Tab. 1 on three different described in more detail below. The code to reproduce the experiments is publicly available at https://github.com/cambridge-mlg/pdediff.\nData. In this work, we consider the 1D Kuramoto-Sivashinsky (KS) and the 2D Kolmogorov flow equations, with the latter being a variant of the incompressible Navier-Stokes flow. KS is a"}, {"title": "5.1 Forecasting", "content": "As mentioned in Sec. 3, forecasting is a crucial component of the combined task we consider, so we investigate the ability of trained diffusion models to sample long rollouts. The models are evaluated on 128 and 50 test trajectories for KS and Kolmogorov, respectively. To understand the current state of score-based models for PDE forecasting we benchmark them against other ML-based models, including the state-of-the-art PDE-Refiner [42] and a MSE-trained U-Net and Fourier Neural Operator (FNO) [39]. We follow the setup in [42] for the architectures and training settings of the baselines. For the diffusion models and the MSE-trained U-Net baseline, we experiment with both an architecture inspired from [60] and one inspired from [42] and report the best results. The performance of both neural network architectures considered can be found in Tab. 5. More details about the different architectures can be found in App. D.4."}, {"title": "5.2 Offline data assimilation", "content": "In offline DA, we assume we have access to some sparse observations. The experimental setting goes as follows. We first choose a number of observed variables, and then uniformly sample the associated indices. We assume to always observe some full initial states. More details can be found in App. G. We repeat this for different numbers of values observed, varying the sparsity of observations, from approximately a third of all data to almost exclusively conditioning on the initial states. We compute the RMSD of the generated trajectories for six values of the proportion of observed data, evenly spaced on a log scale from 10-3 to 10-0.5. We show results for the joint AR model, joint AAO with 0 and 1 corrections (indicated between brackets), and the universal amortised model. To provide"}, {"title": "5.3 Online DA: combining forecasting and data assimilation", "content": "Inspired by real-life applications such as weather prediction, we compare the models in the online DA setting, which represents the combination of forecasting and DA. We assume that some sparse"}, {"title": "6 Discussion", "content": "In this work, we explore different ways to condition and sample diffusion models for forecasting and DA. In particular, we empirically demonstrate that diffusion models achieve comparable performance with MSE-trained models in forecasting, yet are significantly more flexible as they can effectively also tackle DA. We theoretically justify and empirically demonstrate the effectiveness of AR sampling for joint models, which is crucial for any task that contains a forecasting component. Moreover, we enhance the flexibility and robustness of amortised models by proposing a new training strategy which also amortises over the history length, and by combining them with reconstruction guidance.\nLimitations and future work. Although achieving competitive results with MSE-trained baselines in forecasting, the models still lack state-of-the-art performance. Thus, they are most practical in settings where flexible conditioning is needed, rather than a standard forecasting task. Although we have explored different solvers and number of discretisation steps, the autoregressive sampling strategy requires simulating a denoising process at each step, which is computationally intensive. We believe that this can be further improved, perhaps by reusing previous sampled states. Additionally, the guided sampling strategy requires tuning the guidance strength, yet we believe there exist some simple heuristics that are able to decrease the sensitivity of this hyperparameter. Finally, we are interested in investigating how the PDE characteristics (i.e. spatial / time discretisation, data volume, frequency spectrum, etc.) influence the behaviour of the diffusion-based models."}, {"title": "A Organisation of the supplementary", "content": "In this supplementary, we first motivate in App. B the choice made regarding the (local) score network given the Markov assumption on the sequences. We then provide more reference to related work in App. C, followed by a description of the data generating process in App. D for the datasets used in the experiments in Sec. 5. We also provide further implementation details. After, in App. E, we assess the ability of the all-at-once sampling strategy-which relies on reconstructing the full score from the local ones to perform accurate forecast, and in particular show the crucial role of correction steps in the discretisation. We then provide more results on forecasting in App. F, including a study of the influence of the guidance strength hyperparameter y and of the conditioning scenario, a comparison with widely used benchmarks in forecasting, such as climatology and persistence, and results for different U-Net architectures. In App. G we show more experimental evidence on the offline DA task, including the influence of the guidance strength and of the conditioning scenarios, as well as perform a comparison between an AR model and an AAO model with twice higher Markov order. We also include a comparison with an interpolation baseline. In App. \u0397 we provide further experimental evidence for the online DA task for a range of conditioning scenarios for the joint and amortised models. In App. I we provide frequency spectra of the trajectories generated by our methods and the forecasting baselines, and in App. J we analyse the long-term behaviour of the diffusion models. Finally, we provide pseudo-code in App. K."}, {"title": "B Markov score", "content": "In this section, we look at the structure of the score induced by a Markov time series. In particular, we lay out different ways to parameterise and train this score with a neural network. In what follows we assume an AR-1 Markov model for the sake of clarity, but the reasoning developed trivially generalises to AR-k Markov models. Assuming an AR-1 Markov model, we have that the joint density factorise as\nB.1 Window 2k + 1:\nT\nP(x1:L) = P(x1) [P(Xi Xi\u22121).\nt=2\n(9)\nLooking at the score of (9) for intermediary states we have\nVx\u2081 log p(x1:L) = \u2207x\u2081 log p(xi+1|xi) + \u2207 x\u2081 log p(xi Xi\u22121)\n\u2207xi log p(xi+1, Xi Xi-1)\n(10)\n= \u2207x; log p(xi+1, Xi Xi\u22121) + \u2207xi log p(xi-1)\n0\n= \u2207x; log p(xi+1, Xi, Xi-1)\n= \u25bcx\u2081 log p(xi\u22121, Xi, Xi+1).\n(11)"}, {"title": "D Data generation and implementation details", "content": "D.1 Burgers' equation\nGround truth trajectories for the Burgers' equation [8]\ndu\n\u2202\u03c4\n+ u\ndu\ndz\n2u\n8z2',\nare obtained from the open-source dataset made available by Li et al. [40], where they consider (x, \u03c4) \u2208 (0,1) \u00d7 (0, 1]. The dataset consists of 1200 trajectories of 101 timesteps, i.e. \u0414\u0442 = 0.01, and where the spatial discretisation used is 128. 800 trajectories were used as training set, 200 as validation set and the remaining 200 as test set. These trajectories were generated following the setup described in [77] using the code available in their public repository. Initial conditions are sampled from a Gaussian random field defined as N(0, 6252 (-\u2206 + 52I)-4) (although be aware that in the paper they state to be using N(0,25\u00b2(\u2212\u2206 +52I)-4), which is different from what the code suggests) and then they integrate the Burgers' equation using spectral methods up to 7 = 1 using a viscosity of 0.01. Specifically, they solve the equations using a spectral Fourier discretisation and a fourth-order stiff time-stepping scheme (ETDRK4) [14] with step-size 10-4 using the MATLAB Chebfun package [16]. Examples of training, validation, and test trajectories are shown in Fig. 4.\nFollowing the setup of Lippe et al. [42], which is based on the setup defined by Brandstetter et al. [7]. In contrast to Lippe et al. [42], we generate the data by keeping Ax and \u2206r fixed, i.e. using a constant time and spatial discretisation\u00b2. The public repository of Brandstetter et al. [7] can be used to generate our dataset by using the same arguments as the one defined in Lippe et al. [42, App D1.].\nK\nBrandstetter et al. [7] solves the KS-equation with periodic boundaries using the method of lines, with spatial derivatives computed using the pseudo-spectral method. Initial conditions are sampled from a distribution defined over truncated Fourier series, i.e. uo(x) = \u03a3\u039a-1 Ak sin(2\u03c0\u03b9\u03bax/L + \u00a2k), where Ak, lk, Okk are random coefficients representing the amplitude of the different sin waves, the phase shift, and the space-dependent frequency. The viscosity parameter is set to v = 1. Data is initially generated with float64 precision and then converted to float32 for training the different models. We generated 1024 trajectories of 140\u2206\u03c4, where \u2206\u03c4 = 0.2s, as training set, while the validation and test set both contain 128 trajectories of length 640\u2206\u03c4. The spatial domain is discretised into 256 evenly spaced points. Trajectories used to train and evaluate the models can be seen in Fig. 5."}, {"title": "E All-at-once (AAO) joint sampling and study of solvers in forecasting", "content": "One approach for generating long rollouts conditioned on some observations using score-based generative modelling is to directly train a joint posterior score se(t, x1:L(t)|y) to approximate 11:L (t) log p(x1:L(t)|y). The posterior can be composed from a prior score and a likelihood term, just as in the main paper. However, this approach cannot be used for generating long rollouts (large L), as the score network becomes impractically large. Moreover, depending on the dimensionality of each state, with longer rollouts the probability of running into memory issues increases significantly. To deal with this issue, in this section we investigate the approach proposed by Rozet and Louppe [60], whereby the prior score (se(t, x1:L(t))) is approximated with a series of local scores (se(t, Xi-k:i+k(t))), which are easier to learn. The size of these local scores is in general significantly"}, {"title": "F Additional experimental results on forecasting", "content": "This section shows further experimental results in the context of PDE forecasting. We show results on the Burgers', KS, and Kolmogorov datasets. We study the difference between all-at-once (AAO) and autoregressive (AR) sampling in forecasting, the influence of the guidance strength, and of the conditioning scenario in AR sampling for the joint and amortised models. Moreover, we also provide a comparison with some widely used benchmarks in forecasting: climatology and persistence. Finally, we provide results for two U-Net architecture choices for the diffusion-based models and the MSE-trained U-Net baseline."}, {"title": "F.1 AAO vs AR", "content": "Once the joint model se(t, x1:L(t)) \u2248 \u2207x1:1 (t) log p(x1:L (t)) is trained, there are multiple ways in which it can be queried\n\u2022 All-at-once (AAO): sampling the entire trajectory in one go (potentially conditioned on some initial and/or intermediary states);\n\u2022 Autoregressively (AR): sampling a few states at a time, conditioned on a few previously generated states (as well as potentially on some initial and/or intermediary states).\nLet us denote the length of the generated trajectory with L, the number of predictor steps used during the sampling process with p, and the number of corrector steps with c. Moreover, in the case of the AR procedure, we have two extra hyperparameters: the predictive horizon P (the number of states generated at each iteration), and the number of conditioned states C. Finally, let the window of the model be W. Each sampling procedure has its advantages and disadvantages.\n1. Sampling time: the AAO procedure (with no corrections) is faster than the AR one due to its non-recursive nature. We express the computational cost associated with each procedure in terms of the number of function evaluations (NFEs) (i.e. number of calls of the neural network) needed to generate the samples.\n\u2022 AAO: NFE = (1 + c) \u00d7 p\n\u2022 AR: NFE = (1 + c) \u00d7 p \u00d7(1+\nL-W\nP\n)\n2. Memory cost: the AAO procedure is more memory-intensive than the AR one, as the entire generated trajectory length needs to fit in the memory when batching is not performed. If"}, {"title": "G Additional experimental results on offline data assimilation", "content": "In this section, we provide more details about the offline DA setup. For KS, for each sparsity level we uniformly sample the indices associated with the observed variables in space-time. For Kolmogorov, we uniformly sample indices in space at each time step. For both datasets, we assume to always observe some full initial states. The number of such states for AR sampling depends on the chosen conditioning scenario and is equal to C. For AAO we condition on W \u2013 1 initial states, where W stands for the window size.\nFurthermore, this section provides further experimental results for the offline data assimilation task for the KS and Kolmogorov datasets. We did not explore this for the Burgers' dataset since we decided to focus on PDEs with more challenging dynamics. We investigate the influence of the guidance strength, and the performance of the AR models with varying conditioning scenarios. We also perform a comparison between a joint model with Markov order of k that is queried autoregressively and a 2k-Markov order model queried all-at-once. This allows us to gain further insight into the differences between the two sampling schemes. Finally, we provide a quantitative and qualitative comparison to a simple interpolation baseline."}, {"title": "G.1 Influence of the guidance strength for the joint model", "content": "We study several settings of the guidance parameter y for 1) the joint model sampled AAO, and 2) the joint model sampled autoregressively. We show the results in terms of RMSD evaluated on 50 test trajectories for varying levels of sparsity, as in the main paper.\nE\nJoint AAO. For the KS dataset, we consider a window 9 model, with y\n{0.01, 0.03, 0.05, 0.1, 0.5}. For Kolmogorov, we show the results for a window 5 model and \u03b3\u2208 {0.03, 0.05, 0.1}. We query them AAO, and employ 0 and 1 corrections (AAO (0) and AAO (1)).\nFor KS, using y = 0.01 results in unstable rollouts, therefore we only report the results for the other four values of y. For the sparse observation regimes, we find that the best results are achieved by employing the lowest y value that still leads to stable results (\u03b3 = 0.03). For the denser regimes (i.e. proportion observed \u2265 10-1 for 0 corrections and \u2265 10\u20131.5 for 1 correction), the best setting corresponds to y = 0.05."}]}