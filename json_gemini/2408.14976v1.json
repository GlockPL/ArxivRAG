{"title": "Prior-free Balanced Replay: Uncertainty-guided Reservoir Sampling for Long-Tailed Continual Learning", "authors": ["Lei Liu", "Li Liu", "Yawen Cui"], "abstract": "Even in the era of large models, one of the well-known issues in continual learning (CL) is catastrophic forgetting, which is significantly challenging when the continual data stream exhibits a long-tailed distribution, termed as Long-Tailed Continual Learning (LTCL). Existing LTCL solutions generally require the label distribution of the data stream to achieve re-balance training. However, obtaining such prior information is often infeasible in real scenarios since the model should learn without pre-identifying the majority and minority classes. To this end, we propose a novel Prior-free Balanced Replay (PBR) framework to learn from long-tailed data stream with less forgetting. Concretely, motivated by our experimental finding that the minority classes are more likely to be forgotten due to the higher uncertainty, we newly design an uncertainty-guided reservoir sampling strategy to prioritize rehearsing minority data without using any prior information, which is based on the mutual dependence between the model and samples. Additionally, we incorporate two prior-free components to further reduce the forgetting issue: (1) Boundary constraint is to preserve uncertain boundary supporting samples for continually re-estimating task boundaries. (2) Prototype constraint is to maintain the consistency of learned class prototypes along with training. Our approach is evaluated on three standard long-tailed benchmarks, demonstrating superior performance to existing CL methods and previous SOTA LTCL approach in both task- and class-incremental learning settings, as well as ordered- and shuffled-LTCL settings.", "sections": [{"title": "1 Introduction", "content": "Over the last decade, deep neural networks (DNNs) have demonstrated remarkable performance in various multi-media tasks, such as image segmentation [47], video caption [22], and audio-visual learning [34]. However, these tasks are usually performed in a static environment where all data is available in a single session. In a dynamic environment where data arrives phase by phase, the model trained on a new task tends to forget a significant amount of information of old tasks, commonly known as catastrophic forgetting [17, 24]. Besides, it was reported that recent advanced large models also suffer from the forgetting of previously learned information during the fine-tuning process for novel tasks [12].\nConventional CL is typically based on the assumption that the training data is drawn from a balanced distribution. However, real-world data often exhibits a long-tailed distribution [11, 50, 52], where only a few classes dominate the most samples. For instance, in autonomous driving [46, 54], anomaly accidents often occur with lower probabilities than the frequent safe events. When constructing a medical dataset [36], it is an usual phenomena that common symptoms are easily collected while it is difficult to collect enough rare symptoms. In dynamic environments characterized by realistic scenarios, minority classes often incrementally emerge as new tasks, posing a great challenge for the adaptation ability of DNNs [17].\nBesides, existing CL methods exhibit severe performance degradation over the long-tailed data stream. Therefore, it is essential to investigate continual learning over the long-tailed data.\nInspired by [27], we consider two different long-tailed continual learning (LTCL) settings, i.e., ordered- and shuffled-LTCL settings as shown in Figure 1. Several straightforward solutions have been proposed [13, 19, 27] to combine existing CL methods with re-balancing techniques, such as data re-sampling [8], data re-weighting [35], and two-stage training [18]. For instance, [9] explored a balance sampling strategy to keep a balanced memory buffer for the imbalance continual learning. [33] proposed to utilize data augmentation for the memory buffer to alleviate the class imbalance issue in the class incremental learning. However, these approaches may not be practical for tasks evolving over time, as most re-balance techniques require label distribution information of the entire stream, while obtaining such information is often infeasible due to unknown prior of new tasks emerging in the future. In fact, it is usually unknown whether the incoming data or the emerging class is from majority or minority classes. This makes most existing methods are unsuitable for real-world applications.\nTo further cast light on challenges for the LTCL problem, we conduct a motivating experiment (details can be seen in Sec 3.1) and observe that: (1) Minority samples are more likely to be forgotten than majority samples; (2) The classifier weights are easily biased to the old majority classes; and (3) Minority data is usually distributed around the task boundaries with higher uncertainty. Besides, with the limited storage and computing resource, one should utilize a constrained buffer size throughout the entire training phase to address the LTCL problem [3].\nTo address the LTCL issue, motivated by the above experimental findings, we propose a novel Prior-free Balanced Replay (PBR) framework to incrementally learn an evolved representation space for the LTCL problem. More precisely, we design an uncertainty-guided reservoir sampling strategy to prioritize storing minority samples in the replay memory, which is based on the mutual information between changes in model parameters and prediction results. Besides, two prior-free components are newly designed to effectively alleviate the catastrophic forgetting issue under LTCL, especially for minority classes. In detail, prototype constraint ensures all classes have balanced magnitudes by maintaining the consistency of class prototypes learned at different times, while boundary constraint prevents forgetting task boundary information by preserving boundary supporting samples of old tasks.\nIn summary, key contributions of this work are threefold:\n(1) We propose a novel PBR framework to address the LTCL problem without relying on prior information (i.e., label distribution), which utilizes an uncertainty-guided reservoir sampling strategy to achieve a balanced replay with less forgetting.\n(2) We design two new prior-free components (i.e., boundary and prototype constraints) to further reduce the forgetting of minority data via uncertainty estimation.\n(3) Extensive experiments are conducted to evaluate the proposed method on three popular datasets under both ordered- and shuffled-LTCL settings. Experimental results indicate that our method can achieve state-of-the-art (SOTA) performance, surpassing previous works by a significant margin."}, {"title": "2 Related Work", "content": "Continual Learning Existing solutions could be roughly divided into four groups: rehearsal-based [37, 41, 42], distillation-based [23, 38], architecture-based [31, 45], and regularization-based methods [21]. Rehearsal-based methods [1, 3, 29] store a data subset of the old tasks and replay these samples to alleviate catastrophic forgetting. The key is to achieve effective sample selection for rehearsal, such as experience replay [37, 41] and gradient-based sample selection [1, 29]. Another solution is to imitate the previous tasks' behaviors when learning new ones. The main idea is knowledge distillation [15] taking past parameters of the model as the teacher. Besides, it is a common choice to combine rehearsal and distillation by self-distillation learning [38]. Besides, regularization-based methods mainly focus on preventing significant updates of the network parameters when learning new tasks, such as elastic weight consolidation [21], synaptic intelligence [44] and Riemannian walk [5]. Furthermore, architecture-based methods [31, 45] distinct different tasks by devoting distinguished parameter sets, such as Progressive Neural Networks [43].\nLong-Tailed Learning Re-balancing strategies are the most common solutions including re-sampling [8] and re-weighting [35]. However, these methods easily lead to performance degradation for head classes and over-fitting issues for tail classes. Two-stage based methods are proposed to further improve the re-balancing strategies, such as decoupled training [18] and deferred re-balancing schedule [4]. Besides, to learn a high-quality representation space based on imbalanced data, regularization-based approaches are proposed to increase inter-class differences, such as margin [4], bias [32, 39], temperature [51] or weight scale [18]. Recent works explore flexible ways for re-weighting by hard sample mining [25, 26], meta-learning [40], and influence function [35], which target to measure the importance of each training sample. Other studies propose to transfer useful knowledge from head to tail classes via memory module [28] or translation [20].\nLong-Tailed Continual Learning There are some recent works for long-tailed continual learning, e.g.. Partitioning Reservoir Sampling (PRS) [19] and LT-CIL [27]. PRS [19] proposed a balance sampling strategy for head and tail classes along with the sequential tasks to preserve balanced knowledge. LT-CIL [27] utilized a learnable weight scaling layer to decouple representation learning from classifier learning. However, these methods ignore the relationship between the tasks of imbalanced and continual learning and rely on the label distribution for re-balance strategies, while our work is orthogonal with them to learn an evolved feature space for the long-tailed continual learning without label distribution.\nBesides, [33] proposed to utilize data augmentation for the memory buffer to alleviate the class imbalance issue, which only focused on the class imbalance in the current incremental step. [9] explored the class imbalance for online continual learning, but it ignores the unequal roles for different samples in the memory buffer."}, {"title": "3 Methodology", "content": "Problem Formulation. A standard learning agent sequentially observes a data stream {(Do, to), . . ., (Di, ti), . . ., (Dn-1, tn-1)}, where D\u1d62 = {(x,y)}\u02e2\u2071\u2c7c=\u2081 is a labeled dataset of task t\u1d62. n is the time index indicating the task identity. D\u1d62 consists of s\u1d62 pairs of samples with corresponding targets from the data space \\( \\mathcal{X} \\times \\mathcal{Y} \\). To reflect the general long-tail phenomena, we assume that the sequence {s\u2080, s\u2081, ..., s\u2099\u208b\u2081} exhibits a power-law distribution, i.e., s\u1d62 = Ca\u1d57\u2071, where C is the exponent of the power and a is the imbalance ratio for general long-tailed settings [28]. This assumption of power-law behavior is commonly observed in empirical distributions, and reflects how frequently samples from each task are observed.\nFor the classification task, the learning agent predicts the label for a given input x as f_{\u03b8}(x), where f_{\u03b8}(\u00b7) is a mapping function from the input \\( \\mathcal{X} \\) to the output y parameterized by \u03b8. Let \\( l : \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R} \\) be the loss function between a prediction f_{\u03b8}(x) and the target y. Our goal is to learn the optimal parameter \u03b8 with strong continual adaptation ability to correctly classify samples from any observed tasks. The training and inference processes do not rely on task identities t\u1d62. The optimization objective for the parameter \u03b8 over the data stream is given by the follows:\n\n$\\argmin_{\\theta} \\sum_{i=0}^{n-1} L_i, \\text{ where } L_i = \\mathbb{E}_{(\\mathbf{x}, y) \\sim D_i} [l(y, f_\\theta(\\mathbf{x}))].$\n\n(1)\n\n3.1 Motivating Experiment\nAs a motivation, we conduct an empirical experiment as the motivation to observe how the representations changes under the ordered-LTCL setting. We focus on two main factors in a representation space, i.e., class prototypes and task boundaries. Overall, we empirically investigate the reason for the severe performance degradation of existing CL methods under the LTCL setting, i.e., the deterioration of the representation space caused by biased prototypes and easily forgotten task boundaries. Here, the baseline is ResNet18 [14] trained by the SGD optimizer without any further operations (e.g., memory buffer). Main results are visualized in Figure 3 and Figure 4(b), respectively.\nForgotten Minority Data. As shown in Figure 2, different color denotes different tasks, i.e., task-1 has the most data and task-5 has the least data, where the tendencies can reflect the forgetting degree of each task. We observe that the tendency for minority classes (yellow curve) is steeper than majority classes (blue curve), indicating that the minority classes are more likely to be forgotten than the majority classes.\nBiased Prototypes. As shown in Figure 3, the weight magnitudes of the classifier are irregularly distributed. The weight magnitudes of old majority classes are significantly higher than those of new minority classes, while the bias magnitudes of new classes are higher than those of the old classes. Based on such phenomena, the features of each class prototypes are generally clustered with the lack of discrimination in the representation space.\nForgotten Task Boundaries. As shown in Figure 4, the boundary supporting samples of each task (i.e., colorful points) are easily forgotten along with the continually arrived data, which would lead to confusing task boundaries in the representation space. In particular, the boundary supporting samples of the tasks with minority classes are more likely to be forgotten than majority classes due to insufficient training data."}, {"title": "3.2 Prior-free Balanced Replay", "content": "Motivated by the above observations, we propose a novel PBR framework based on the uncertainty-guided reservoir sampling and two constraints. The proposed approach is shown in Figure 5.\nExperience Replay. Experience replay aims to preserve useful knowledge of the previous tasks. Here, we explicitly store the most uncertain samples of the old tasks along the incremental trajectory to improve the continual adaption ability of neural networks. To this end, we seek to minimize the following objective at the time t_{c}:\n\n$\\mathcal{L}_{t_c} + \\alpha \\sum_{i=0}^{c-1} \\mathbb{E}_{\\mathbf{x} \\sim D_i} [\\mathcal{L}_{KL}(f_{\\theta_{t_i}^*}(\\mathbf{x}) || f_{\\theta_{t_c}}(\\mathbf{x}))],$\n\n(2)\nwhere \\( \\theta_{t_i}^* \\) is the optimal parameters after time t\u1d62, and \u03b1 is a hyper-parameter. \\( \\mathcal{L}_{KL} \\) is the knowledge distillation loss. To overcome the unavailability of D\u1d62 from old tasks, we introduce a small memory buffer \\( \\mathcal{M} \\) to retain previous experiences. The objective seeks to replay the learned experiences by resembling the teacher-student trick. To save resources, we merely store the latest model state at t_{c-1} rather than a checkpoint sequence from t\u2080 to t_{c-1}. In this work, we aim to maintain prototype knowledge and task boundary information by incorporating cosine normalization for different tasks. As follows, we will present the details in turn.\nUncertainty-guided Reservoir Sampling. To further keep a balance between old and new tasks, we present an uncertainty-guided reservoir sampling to guarantee that uncertain samples are stored in the buffer M in priority, since uncertain samples are more likely to belong to the minority classes. Different from vanilla reservoir sampling [49], the uncertainty-guided sampling process is conducted at the end of each task to maintain a well-rounded knowledge. At the end of task t_{c}, the training samples \\( \\mathcal{D}_{c} \\) are sorted as \\( \\mathcal{D}_{c} \\) according to their mutual dependence with the network. Then the sampling process iterates s\u2091 times between sample-in and sample-out for each (x, y) \u2208 \\( \\mathcal{D}_{c} \\). The sample-in decides whether to sample a data point into the memory, while the sample-out removes a sample from the memory.\n(1) Candidate: To store uncertain samples, for each iteration, we first generate a candidate sample by:\n\n$\\mathcal{K} = \\{(\\mathbf{x}^*, y^*) | (\\mathbf{x}^*, y^*) = \\arg \\min_{(\\mathbf{x}, y) \\in \\mathcal{D}_c} (I[y, \\theta | \\mathbf{x}, \\mathcal{D}_c])\\}.$\n\n(3)\nwhere I [\u00b7] indicates the mutual information (MI) between the prediction and the posterior over parameters. If \\( \\mathcal{K} \\) is already stored in the memory, we will generate a new candidate from \\( \\mathcal{D}_{c} \\)\\{\\( \\mathcal{K} \\)}, otherwise keeping it fixed.\n(2) Sample-In: We design a probability function P(\\( \\mathcal{K} \\)) to decide whether moving \\( \\mathcal{K} \\) into the memory M:\n\n$\\mathcal{P}(\\mathcal{K}) = \\frac{\\frac{|\\mathcal{M}|}{N_c + 1}}{s_i w_i}, \\text{ where } w_i = \\frac{e^{-s_i}}{\\sum_{j=1}^{c-1} e^{-s_j}}$\n\n(4)\nwhere N_{c} is the total sampling iteration number from the end of task t_{c} up to now. s\u1d62 is the running frequency of class i. This condition implicitly achieves a trade-off between old and new tasks.\n(3) Sample-Out: If the memory is out of buffer size, a sample will be removed when a candidate is entered into the memory M. The probability that an sample is removed follows the uniform distribution over the memory size, i.e., |\\( \\mathcal{M} \\)|, since sample-in works towards achieving a balanced partition for old tasks.\nNote that due to the lack of sufficient knowledge about the minority classes, the uncertainty degree of minority data is generally higher. Therefore, our method implicitly encodes the rule to preferentially store minority samples, rather than directly adjusting the label distribution to alleviate data imbalance.\nUncertainty-guided Mutual Information. Given a network with the limited capacity, uncertainty can be utilized to estimate the importance of each training sample and identify easily forgotten"}, {"title": "3.3 Prior-free Components", "content": "Based on the selected samples in the memory, we propose two new prior-free components to further alleviate the forgetting issue.\nPrototype Constraint. Prototype constraint is to maintain the consistency of learned class prototypes along with training. A typical classifier produces the predicted probability of a sample x by:\n\n$p_i(\\mathbf{x}) = \\frac{\\exp(\\mathbf{w}_i^T f_{\\theta}(\\mathbf{x}) + b_i)}{\\sum_{j \\in \\mathcal{Y}} \\exp(\\mathbf{w}_j^T f_{\\theta}(\\mathbf{x}) + b_j)}$\n\n(8)\nwhere w\u1d62 is the i-th weight vector and b\u1d62 is the i-th bias term in the classifier. As shown in Figure 3, the weight magnitudes are irregularly distributed, resulting in biased prototypes in the feature space. To address this issue, we propose utilize two types of statistical information i.e., class prototype and cosine similarity to preserve useful class-wise information.\nConcretely, given an input data point x, the mapping function f_{\u03b8}(x) is to map x as a hidden representation before the final linear projection for classification. Inspired by cosine normalization [16, 30, 48], we utilize a scaled cosine classifier to extract normalized embeddings of samples by \\( f_{\\theta}(\\mathbf{x}) = \\frac{f_{\\theta}(\\mathbf{x})}{\\|f_{\\theta}\\|} \\), which produces the predicted probability as follows:\n\n$p_i(\\mathbf{x}) = \\frac{\\exp(\\mathbf{\\hat{w}}_i^T \\hat{f}_{\\theta}(\\mathbf{x}))}{\\sum_{j \\in \\mathcal{Y}} \\exp(\\mathbf{\\hat{w}}_j^T \\hat{f}_{\\theta}(\\mathbf{x}))}$\n\n(9)\n\n$\\mathcal{L}_{dc} = \\sum_{i=1}^{c-1} ||\\mathbf{w}_i - \\mathbf{\\hat{w}}_i||.$\n\n(10)\nwhere w\u0302 denotes the normalized weights in the classifier and s is the scaling factor. Instead of computing the average feature over all samples, this formulation allows us to interpret the weight vectors of the classifier as class prototypes during training, which could save the costs for computing average features. It is also noteworthy to preserve cosine similarity scores among previously learned class prototypes. Thus, we further enforce the newly updated classifier to mimic the behavior of previously learned classifier, which could produce approximately consistent similarity scores for each task along with newly coming data. Formally, we propose to exploit a distillation loss to preserve prototype information as follows:\n\n$\\mathcal{L}_{dc} = \\sum_{i=1}^{c-1} ||\\mathbf{w}_i - \\mathbf{\\hat{w}}_i||.$\n\n(10)\nwhere w\u1d62, is the weight for previous task i in the prototype-based classifier. Different from previous cosine normalization encouraging the similar angles between the features and the class prototypes [16], such a distillation loss enhances the learned prototypes to be approximately preserved in the current model.\nBoundary Constraint. Boundary constraint is to preserve uncertain samples with boundary supporting information for continually re-estimating task boundaries. Denote the incoming data by \\( \\mathcal{X}_{in} \\) and data stored in the memory buffer by \\( \\mathcal{X}_{bf} \\), we use a modified cross-entropy (MCE) loss to link prototypes and logits:\n\n$\\mathcal{L}_{t}(\\mathbf{x}) = - \\sum_{\\mathbf{x} \\in \\mathcal{X}_{in} \\cup \\mathcal{X}_{bf}} \\log \\frac{\\exp(s \\mathbf{\\hat{w}}_i^T f_{\\theta}(\\mathbf{x}) / \\tau_1)}{\\sum_{j \\in \\mathcal{Y}} \\exp(s \\mathbf{\\hat{w}}_j^T f_{\\theta}(\\mathbf{x}) / \\tau_1)}$\n\n(11)\nwhere w\u1d62 is i-th weight vector (prototype) of the classifier and \u03c4\u2081 is a scaling factor. The prototype is normalized so that \\( \\mathbf{\\hat{w}}_i^T f_{\\theta}(\\mathbf{x}) \\) is a cosine similarity metric. Note that class prototypes are explicitly updated where samples of the same class lie close by each other. Beyond to the prototypes, we further consider uncertain samples to preserve effective boundary information via the knowledge distillation loss:\n\n$\\mathcal{L}_{KL}(f_{\\theta^*}(\\mathbf{x}) || f_{\\theta}(\\mathbf{x})) = \\sum_{\\mathbf{x} \\in \\mathcal{M}} \\sum_{j \\in \\mathcal{Y}} \\frac{\\exp(s \\mathbf{\\hat{w}}_i^T f_{\\theta^*}(\\mathbf{x}) / \\tau_2)}{\\sum_{j \\in \\mathcal{Y}} \\exp(s \\mathbf{\\hat{w}}_j^T f_{\\theta^*}(\\mathbf{x}) / \\tau_2)} \\log \\frac{\\frac{\\exp(s \\mathbf{\\hat{w}}_i^T f_{\\theta^*}(\\mathbf{x}) / \\tau_2)}{\\sum_{j \\in \\mathcal{Y}} \\exp(s \\mathbf{\\hat{w}}_j^T f_{\\theta^*}(\\mathbf{x}) / \\tau_2)}}{\\frac{\\exp(s \\mathbf{\\hat{w}}_i^T f_{\\theta}(\\mathbf{x}) / \\tau_2)}{\\sum_{j \\in \\mathcal{Y}} \\exp(s \\mathbf{\\hat{w}}_j^T f_{\\theta}(\\mathbf{x}) / \\tau_2)}}$\n\n(12)\nwhere \u03b8\u2217 is the parameters after the task t_{c-1}. \u03c4\u2082 is the temperature scale. We can rewrite Equation 2 as follows:\n\n$\\mathbb{E}_{\\mathbf{x} \\sim {\\mathcal{D}_c \\cup \\mathcal{M}}} \\mathcal{L}_{t_c}(\\mathbf{x}) + \\alpha \\mathbb{E}_{\\mathbf{x} \\sim \\mathcal{M}} [\\mathcal{L}_{KL}(f_{\\theta^*}(\\mathbf{x}) || f_{\\theta}(\\mathbf{x}))] + \\beta \\mathcal{L}_{dc}.$\n\n(13)\nWe approximate the expectation on batches sampled from the current task and the buffer, respectively."}, {"title": "4 Experiments", "content": "4.1 Experimental Setup\n4.1.1 Benchmarks. We consider two common CL scenarios [3]: (1) Task Incremental Learning (Task-IL), where task identities are provided to select the relevant classifier for each sample during evaluation; (2) Class Incremental Learning (Class-IL), where task identities are not provided during evaluation. This difference makes Task-IL and Class-IL the easiest and hardest scenarios. We present"}, {"title": "4.3 Data and Task Analysis", "content": "Effect of LTCL.. Table 2 reports the average accuracy results at the end of all tasks under the ordered-LTCL setting. We observe that the task-IL accuracy of SGD-LT is better than SGD-BL. The main reason is that new tasks with minority samples could reduce forgetting of old tasks with majority samples. Furthermore, since alleviating forgetting may induce imbalanced impacts on the new tasks, some approaches exhibit lower accuracy than SGD-LT. For instance, regularization-based methods (e.g., oEWC, Lwf, PNN, and GEM) suffer from the imbalance issue on the new tasks, which arises from the learned regularization information from the old task. Therefore, by considering the motivating experiment in Section 3.1, our method can well address the LTCL issue by adopting prototype and boundary constraints.\nEffect of Imbalance Ratio. We evaluate the comparison methods with different imbalanced ratios following [28]. As shown in"}, {"title": "4.4 Ablation Analysis", "content": "Importance of Prototype-based Classifier. We analyze different components of our method to verify their effects. The linear classifier is a fully-connected layer with the bias, and the cosine classifier is a normalized fully-connected layer without the bias. As shown in Table 5, our method could obtain the best results using uncertainty quantification and cosine classifier. The linear classifier without rehearsal yields the worst performance because of both catastrophic forgetting and imbalance. As the cosine classifier retains the prototype and similarity information among classes, the class-IL accuracy can be improved compared to the linear classifier. The linear classifier with uncertainty performs lower accuracy results without prototype information, although uncertainty is used to select boundary supporting samples.\nImportance of Boundary-supporting Sample. In this part, we analyze the effect of boundary-supporting samples (uncertainty) on reservoir sampling. Table 5 reports the results of random reservoir sampling and uncertainty-guided reservoir sampling. It is observed that the performance is significantly reduced when using random reservoir sampling. The main reason is a lack of important boundary information due to catastrophic forgetting, although prototypes can be maintained via knowledge distillation over random samples. Based on the uncertainty estimation, the decision boundaries between old and new classes can be well modeled via replaying boundary supporting samples."}, {"title": "5 Conclusion", "content": "In this work, we propose a novel Prior-free Balanced Replay (PBR) framework based on the newly designed uncertainty-guided reservoir sampling strategy, which prioritizes rehearsing minority data without using prior information. Additionally, we incorporate two other prior-free components to further reduce the forgetting issue including prototype and boundary constraints, which can maintain effective feature information for continually re-estimating task boundaries and prototypes. Compared with existing CL methods and SOTA LTCL approach, the experimental results on three standard long-tailed benchmarks demonstrate the superior performance of the proposed method in both task and class incremental learning settings, as well as ordered- and shuffled-LTCL settings. In the futher work, we will release the assumptions and address limitations towards real-world scenarios."}]}