{"title": "Prior-free Balanced Replay: Uncertainty-guided Reservoir Sampling for Long-Tailed Continual Learning", "authors": ["Lei Liu", "Li Liu", "Yawen Cui"], "abstract": "Even in the era of large models, one of the well-known issues in con-\ntinual learning (CL) is catastrophic forgetting, which is significantly\nchallenging when the continual data stream exhibits a long-tailed\ndistribution, termed as Long-Tailed Continual Learning (LTCL).\nExisting LTCL solutions generally require the label distribution of\nthe data stream to achieve re-balance training. However, obtaining\nsuch prior information is often infeasible in real scenarios since the\nmodel should learn without pre-identifying the majority and minor-\nity classes. To this end, we propose a novel Prior-free Balanced\nReplay (PBR) framework to learn from long-tailed data stream\nwith less forgetting. Concretely, motivated by our experimental\nfinding that the minority classes are more likely to be forgotten due\nto the higher uncertainty, we newly design an uncertainty-guided\nreservoir sampling strategy to prioritize rehearsing minority data\nwithout using any prior information, which is based on the mutual\ndependence between the model and samples. Additionally, we incor-\nporate two prior-free components to further reduce the forgetting\nissue: (1) Boundary constraint is to preserve uncertain boundary\nsupporting samples for continually re-estimating task boundaries.\n(2) Prototype constraint is to maintain the consistency of learned\nclass prototypes along with training. Our approach is evaluated\non three standard long-tailed benchmarks, demonstrating superior\nperformance to existing CL methods and previous SOTA LTCL\napproach in both task- and class-incremental learning settings, as\nwell as ordered- and shuffled-LTCL settings.", "sections": [{"title": "1 Introduction", "content": "Over the last decade, deep neural networks (DNNs) have demon-\nstrated remarkable performance in various multi-media tasks, such\nas image segmentation [47], video caption [22], and audio-visual\nlearning [34]. However, these tasks are usually performed in a static\nenvironment where all data is available in a single session. In a\ndynamic environment where data arrives phase by phase, the model\ntrained on a new task tends to forget a significant amount of infor-\nmation of old tasks, commonly known as catastrophic forgetting\n[17, 24]. Besides, it was reported that recent advanced large models\nalso suffer from the forgetting of previously learned information\nduring the fine-tuning process for novel tasks [12].\nConventional CL is typically based on the assumption that the\ntraining data is drawn from a balanced distribution. However,\nreal-world data often exhibits a long-tailed distribution [11, 50, 52],\nwhere only a few classes dominate the most samples. For instance,\nin autonomous driving [46, 54], anomaly accidents often occur with"}, {"title": "", "content": "lower probabilities than the frequent safe events. When construct-\ning a medical dataset [36], it is an usual phenomena that common\nsymptoms are easily collected while it is difficult to collect enough\nrare symptoms. In dynamic environments characterized by realistic\nscenarios, minority classes often incrementally emerge as new tasks,\nposing a great challenge for the adaptation ability of DNNs [17].\nBesides, existing CL methods exhibit severe performance degrada-\ntion over the long-tailed data stream. Therefore, it is essential to\ninvestigate continual learning over the long-tailed data.\nInspired by [27], we consider two different long-tailed continual\nlearning (LTCL) settings, i.e., ordered- and shuffled-LTCL settings as\nshown in Figure 1. Several straightforward solutions have been pro-\nposed [13, 19, 27] to combine existing CL methods with re-balancing\ntechniques, such as data re-sampling [8], data re-weighting [35],\nand two-stage training [18]. For instance, [9] explored a balance\nsampling strategy to keep a balanced memory buffer for the imbal-\nance continual learning. [33] proposed to utilize data augmentation\nfor the memory buffer to alleviate the class imbalance issue in the\nclass incremental learning. However, these approaches may not\nbe practical for tasks evolving over time, as most re-balance\ntechniques require label distribution information of the en-\ntire stream, while obtaining such information is often infea-\nsible due to unknown prior of new tasks emerging in the\nfuture. In fact, it is usually unknown whether the incoming data or\nthe emerging class is from majority or minority classes. This makes\nmost existing methods are unsuitable for real-world applications.\nTo further cast light on challenges for the LTCL problem, we\nconduct a motivating experiment (details can be seen in Sec 3.1) and\nobserve that: (1) Minority samples are more likely to be forgotten\nthan majority samples; (2) The classifier weights are easily biased to\nthe old majority classes; and (3) Minority data is usually distributed\naround the task boundaries with higher uncertainty. Besides, with\nthe limited storage and computing resource, one should utilize a\nconstrained buffer size throughout the entire training phase to\naddress the LTCL problem [3].\nTo address the LTCL issue, motivated by the above experimen-\ntal findings, we propose a novel Prior-free Balanced Replay (PBR)\nframework to incrementally learn an evolved representation space\nfor the LTCL problem. More precisely, we design an uncertainty-\nguided reservoir sampling strategy to prioritize storing minority\nsamples in the replay memory, which is based on the mutual in-\nformation between changes in model parameters and prediction\nresults. Besides, two prior-free components are newly designed to\neffectively alleviate the catastrophic forgetting issue under LTCL,\nespecially for minority classes. In detail, prototype constraint en-\nsures all classes have balanced magnitudes by maintaining the\nconsistency of class prototypes learned at different times, while\nboundary constraint prevents forgetting task boundary information\nby preserving boundary supporting samples of old tasks.\nIn summary, key contributions of this work are threefold:\n(1) We propose a novel PBR framework to address the LTCL prob-\nlem without relying on prior information (i.e., label distribution),\nwhich utilizes an uncertainty-guided reservoir sampling strategy\nto achieve a balanced replay with less forgetting.\n(2) We design two new prior-free components (i.e., boundary and\nprototype constraints) to further reduce the forgetting of minority\ndata via uncertainty estimation."}, {"title": "", "content": "(3) Extensive experiments are conducted to evaluate the pro-\nposed method on three popular datasets under both ordered- and\nshuffled-LTCL settings. Experimental results indicate that our method\ncan achieve state-of-the-art (SOTA) performance, surpassing previ-\nous works by a significant margin."}, {"title": "2 Related Work", "content": "Continual Learning Existing solutions could be roughly divided\ninto four groups: rehearsal-based [37, 41, 42], distillation-based\n[23, 38], architecture-based [31, 45], and regularization-based meth-\nods [21]. Rehearsal-based methods [1, 3, 29] store a data subset\nof the old tasks and replay these samples to alleviate catastrophic\nforgetting. The key is to achieve effective sample selection for\nrehearsal, such as experience replay [37, 41] and gradient-based\nsample selection [1, 29]. Another solution is to imitate the previous\ntasks' behaviors when learning new ones. The main idea is knowl-\nedge distillation [15] taking past parameters of the model as the\nteacher. Besides, it is a common choice to combine rehearsal and\ndistillation by self-distillation learning [38]. Besides, regularization-\nbased methods mainly focus on preventing significant updates of\nthe network parameters when learning new tasks, such as elastic\nweight consolidation [21], synaptic intelligence [44] and Riemann-\nian walk [5]. Furthermore, architecture-based methods [31, 45]\ndistinct different tasks by devoting distinguished parameter sets,\nsuch as Progressive Neural Networks [43].\nLong-Tailed Learning Re-balancing strategies are the most\ncommon solutions including re-sampling [8] and re-weighting [35].\nHowever, these methods easily lead to performance degradation\nfor head classes and over-fitting issues for tail classes. Two-stage\nbased methods are proposed to further improve the re-balancing\nstrategies, such as decoupled training [18] and deferred re-balancing\nschedule [4]. Besides, to learn a high-quality representation space\nbased on imbalanced data, regularization-based approaches are\nproposed to increase inter-class differences, such as margin [4], bias\n[32, 39], temperature [51] or weight scale [18]. Recent works explore\nflexible ways for re-weighting by hard sample mining [25, 26], meta-\nlearning [40], and influence function [35], which target to measure\nthe importance of each training sample. Other studies propose to\ntransfer useful knowledge from head to tail classes via memory\nmodule [28] or translation [20].\nLong-Tailed Continual Learning There are some recent works\nfor long-tailed continual learning, e.g.. Partitioning Reservoir Sam-\npling (PRS) [19] and LT-CIL [27]. PRS [19] proposed a balance\nsampling strategy for head and tail classes along with the sequen-\ntial tasks to preserve balanced knowledge. LT-CIL [27] utilized a\nlearnable weight scaling layer to decouple representation learning\nfrom classifier learning. However, these methods ignore the rela-\ntionship between the tasks of imbalanced and continual learning\nand rely on the label distribution for re-balance strategies, while\nour work is orthogonal with them to learn an evolved feature space\nfor the long-tailed continual learning without label distribution.\nBesides, [33] proposed to utilize data augmentation for the memory\nbuffer to alleviate the class imbalance issue, which only focused on\nthe class imbalance in the current incremental step. [9] explored\nthe class imbalance for online continual learning, but it ignores the\nunequal roles for different samples in the memory buffer."}, {"title": "3 Methodology", "content": "Problem Formulation. A standard learning agent sequentially ob-\nserves a data stream {(Do, to), . . ., (Di, ti), . . ., (Dn-1, tn-1)}, where\nDi = {(x,y)|_{j=1}^{si}} is a labeled dataset of task t\u012f. n is the time in-\ndex indicating the task identity. Di consists of si pairs of samples\nwith corresponding targets from the data space X \u00d7 Y. To reflect\nthe general long-tail phenomena, we assume that the sequence\n{S0, S1, ..., Sn-1} exhibits a power-law distribution, i.e., si = Cati,\nwhere C is the exponent of the power and a is the imbalance ratio\nfor general long-tailed settings [28]. This assumption of power-\nlaw behavior is commonly observed in empirical distributions, and\nreflects how frequently samples from each task are observed.\nFor the classification task, the learning agent predicts the label\nfor a given input x as fo (x), where fo() is a mapping function from\nthe input X to the output y parameterized by \u03b8. Let l : Y \u00d7 Y \u2192 R\nbe the loss function between a prediction fo(x) and the target y.\nOur goal is to learn the optimal parameter \u03b8 with strong continual\nadaptation ability to correctly classify samples from any observed\ntasks. The training and inference processes do not rely on task\nidentities ti. The optimization objective for the parameter \u03b8 over\nthe data stream is given by the follows:\nargmin\\limits_{\\theta}  \\sum\\limits_{i=0}^{n-1} L_{t_i}, where \\mathcal{L}_{t_i} =  \\mathbb{E}_{(\\mathbf{x}, y) \\sim \\mathcal{D}_i}[l (y, f_\\theta(\\mathbf{x}))]."}, {"title": "3.1 Motivating Experiment", "content": "As a motivation, we conduct an empirical experiment as the mo-\ntivation to observe how the representations changes under the\nordered-LTCL setting. We focus on two main factors in a repre-\nsentation space, i.e., class prototypes and task boundaries. Overall,\nwe empirically investigate the reason for the severe performance\ndegradation of existing CL methods under the LTCL setting, i.e.,"}, {"title": "3.2 Prior-free Balanced Replay", "content": "Motivated by the above observations, we propose a novel PBR\nframework based on the uncertainty-guided reservoir sampling\nand two constraints. The proposed approach is shown in Figure 5.\nExperience Replay. Experience replay aims to preserve useful\nknowledge of the previous tasks. Here, we explicitly store the most\nuncertain samples of the old tasks along the incremental trajectory\nto improve the continual adaption ability of neural networks. To\nthis end, we seek to minimize the following objective at the time tc:\nL_{t_c} + \\alpha  \\sum\\limits_{i=1}^{t_c-1} \\mathbb{E}_{x \\sim D_i}[L_{kl} (f_{\\theta_{t_i}}(x) \\| f_{\\theta_{t_c}}(x))],\nwhere \u03b8\u2217i is the optimal parameters after time ti, and \u03b1 is a hyper-\nparameter. Lkl is the knowledge distillation loss. To overcome the\nunavailability of Di from old tasks, we introduce a small memory\nbuffer M to retain previous experiences. The objective seeks to\nreplay the learned experiences by resembling the teacher-student\ntrick. To save resources, we merely store the latest model state\nat tc\u22121 rather than a checkpoint sequence from to to tc\u22121. In this\nwork, we aim to maintain prototype knowledge and task boundary\ninformation by incorporating cosine normalization for different\ntasks. As follows, we will present the details in turn.\nUncertainty-guided Reservoir Sampling. To further keep a\nbalance between old and new tasks, we present an uncertainty-\nguided reservoir sampling to guarantee that uncertain samples\nare stored in the buffer M in priority, since uncertain samples are\nmore likely to belong to the minority classes. Different from vanilla\nreservoir sampling [49], the uncertainty-guided sampling process\nis conducted at the end of each task to maintain a well-rounded\nknowledge. At the end of task tc, the training samples Dc are sorted\nas Dc according to their mutual dependence with the network.\nThen the sampling process iterates se times between sample-in and"}, {"title": "", "content": "sample-out for each (x, y) \u2208 Dc. The sample-in decides whether to\nsample a data point into the memory, while the sample-out removes\na sample from the memory.\n(1) Candidate: To store uncertain samples, for each iteration,\nwe first generate a candidate sample by:\nK = {(\\mathbf{x}^*, y^*) \\| (\\mathbf{x}^*, y^*) = arg \\underset{(\\mathbf{x},y) \\in D_c} {min} (I [y, \\theta \\| x, \\mathcal{D}_c])}.\nwhere I [] indicates the mutual information (MI) between the\nprediction and the posterior over parameters. If K is already stored\nin the memory, we will generate a new candidate from Dc\\{K},\notherwise keeping it fixed.\n(2) Sample-In: We design a probability function P(K) to decide\nwhether moving K into the memory M:\nP(K) = \\frac{|M|}{N_c + 1}  \\sum\\limits_{i=1} \\frac{w_i}{s_i}, where w_i = \\frac{e^{-\\mathbb{S}_i}}{\\sum\\limits_{j=1}^{c-1} e^{-\\mathbb{S}_j}}\nwhere Nc is the total sampling iteration number from the end of task\nte up to now. si is the running frequency of class i. This condition\nimplicitly achieves a trade-off between old and new tasks.\n(3) Sample-Out: If the memory is out of buffer size, a sample\nwill be removed when a candidate is entered into the memory M.\nThe probability that an sample is removed follows the uniform\ndistribution over the memory size, i.e., M, since sample-in works\ntowards achieving a balanced partition for old tasks.\nNote that due to the lack of sufficient knowledge about the mi-\nnority classes, the uncertainty degree of minority data is generally\nhigher. Therefore, our method implicitly encodes the rule to prefer-\nentially store minority samples, rather than directly adjusting the\nlabel distribution to alleviate data imbalance.\nUncertainty-guided Mutual Information. Given a network\nwith the limited capacity, uncertainty can be utilized to estimate the\nimportance of each training sample and identify easily forgotten"}, {"title": "3.3 Prior-free Components", "content": "based on the selected samples in the memory, we propose two new\nprior-free components to further alleviate the forgetting issue.\nPrototype Constraint. Prototype constraint is to maintain the\nconsistency of learned class prototypes along with training. A typi-\ncal classifier produces the predicted probability of a sample x by:\nPi(x) = \\frac{exp (w_i^Tf_{\\theta}(x) + b_i)}{\\mathcal{L}ie y exp (w_i^Tf_{\\theta}(x) + b_i)}\nwhere wi is the i-th weight vector and bi is the i-th bias term\nin the classifier. As shown in Figure 3, the weight magnitudes\nare irregularly distributed, resulting in biased prototypes in the\nfeature space. To address this issue, we propose utilize two types\nof statistical information i.e., class prototype and cosine similarity\nto preserve useful class-wise information.\nConcretely, given an input data point x, the mapping function\nf(x) is to map x as a hidden representation before the final lin-\near projection for classification. Inspired by cosine normalization\n[16, 30, 48], we utilize a scaled cosine classifier to extract normal-\nized embeddings of samples by f(x) = \\frac{f_\\theta}{\\|f_\\theta\\|}, which produces the\npredicted probability as follows:\nPi(x) = \\frac{exp (s \\hat{w}_i^T\\hat{f_{\\theta}}(x))}{\\mathcal{L}ie y exp (s \\hat{w}_i^T\\hat{f_{\\theta}}(x))}"}, {"title": "", "content": "where \u0175 denotes the normalized weights in the classifier and s is\nthe scaling factor. Instead of computing the average feature over all\nsamples, this formulation allows us to interpret the weight vectors\nof the classifier as class prototypes during training, which could\nsave the costs for computing average features. It is also noteworthy\nto preserve cosine similarity scores among previously learned class\nprototypes. Thus, we further enforce the newly updated classifier\nto mimic the behavior of previously learned classifier, which could\nproduce approximately consistent similarity scores for each task\nalong with newly coming data. Formally, we propose to exploit a\ndistillation loss to preserve prototype information as follows:\nL_{de} = \\sum\\limits_{i=1}^{c-1} \\| w_i - \\hat{w}_i \\|\nwhere wi, is the weight for previous task i in the prototype-based\nclassifier. Different from previous cosine normalization encouraging\nthe similar angles between the features and the class prototypes\n[16], such a distillation loss enhances the learned prototypes to be\napproximately preserved in the current model.\nBoundary Constraint. Boundary constraint is to preserve un-\ncertain samples with boundary supporting information for con-\ntinually re-estimating task boundaries. Denote the incoming data\nby Xin and data stored in the memory buffer by Xbf, we use a\nmodified cross-entropy (MCE) loss to link prototypes and logits:\nL_t (x) = - \\sum\\limits_{xeXinUXbf} log \\frac{exp (s \\hat{w}_i^T\\hat{f_{\\theta}}(x) / \\tau_1)}{\\mathcal{D}ie y exp (s \\hat{w}_i^T\\hat{f_{\\theta}}(x) / \\tau_1)}\nwhere wi is i-th weight vector (prototype) of the classifier and ti is a\nscaling factor. The prototype is normalized so that w wf is a cosine\nsimilarity metric. Note that class prototypes are explicitly updated\nwhere samples of the same class lie close by each other. Beyond to\nthe prototypes, we further consider uncertain samples to preserve\neffective boundary information via the knowledge distillation loss:\nL_{kl} (f_{\\theta^*}(x) \\| f_{\\theta}(x)) = \\sum\\limits_{TWI} log \\frac{exp (s \\hat{w}_i^T\\hat{f_{\\theta^*}}(x) / \\tau_2)}{exp (s \\hat{w}_i^T\\hat{f_{\\theta^*}}(x) / \\tau_2)} \\frac{exp (s \\hat{w}_i^T\\hat{f_{\\theta}}(x) / \\tau_2)}{exp (s \\hat{w}_i^T\\hat{f_{\\theta}}(x) / \\tau_2)}\nwhere \u03b8\u2217 is the parameters after the task tc\u22121. t2 is the temperature\nscale. We can rewrite Equation 2 as follows:\n\\mathbb{E}_{x \\sim {\\mathcal{D}_cU \\mathcal{M}}} L_{t_c} (x) + \\alpha \\mathbb{E}_{x \\sim M} [L_{k1} (f_{\\theta^*}(x)\\|f_{\\theta}(x))] + \\beta L_{dc}.\nWe approximate the expectation on batches sampled from the cur-\nrent task and the buffer, respectively."}, {"title": "4 Experiments", "content": "4.1 Experimental Setup\n4.1.1 Benchmarks. We consider two common CL scenarios [3]:\n(1) Task Incremental Learning (Task-IL), where task identities are\nprovided to select the relevant classifier for each sample during\nevaluation; (2) Class Incremental Learning (Class-IL), where task\nidentities are not provided during evaluation. This difference makes\nTask-IL and Class-IL the easiest and hardest scenarios. We present"}, {"title": "4.2 Comparison Results", "content": "Comparisons with SOTA LTCL Method under Ordered-LTCL.\nAs seen in Table 1, compared with previous SOTA method, our\nmethod can outperform PODNET+ [27] by a large margin for both\nClass-IL and Task-IL settings. The main reason is that our method"}, {"title": "4.3 Data and Task Analysis", "content": "Effect of LTCL.. Table 2 reports the average accuracy results at\nthe end of all tasks under the ordered-LTCL setting. We observe\nthat the task-IL accuracy of SGD-LT is better than SGD-BL. The\nmain reason is that new tasks with minority samples could reduce\nforgetting of old tasks with majority samples. Furthermore, since\nalleviating forgetting may induce imbalanced impacts on the new\ntasks, some approaches exhibit lower accuracy than SGD-LT. For\ninstance, regularization-based methods (e.g., oEWC, Lwf, PNN, and\nGEM) suffer from the imbalance issue on the new tasks, which\narises from the learned regularization information from the old task.\nTherefore, by considering the motivating experiment in Section 3.1,\nour method can well address the LTCL issue by adopting prototype\nand boundary constraints.\nEffect of Imbalance Ratio. We evaluate the comparison meth-\nods with different imbalanced ratios following [28]. As shown in"}, {"title": "4.4 Ablation Analysis", "content": "Importance of Prototype-based Classifier. We analyze differ-\nent components of our method to verify their effects. The linear\nclassifier is a fully-connected layer with the bias, and the cosine\nclassifier is a normalized fully-connected layer without the bias. As\nshown in Table 5, our method could obtain the best results using\nuncertainty quantification and cosine classifier. The linear classifier\nwithout rehearsal yields the worst performance because of both\ncatastrophic forgetting and imbalance. As the cosine classifier re-\ntains the prototype and similarity information among classes, the\nclass-IL accuracy can be improved compared to the linear classifier.\nThe linear classifier with uncertainty performs lower accuracy re-\nsults without prototype information, although uncertainty is used\nto select boundary supporting samples.\nImportance of Boundary-supporting Sample. In this part, we\nanalyze the effect of boundary-supporting samples (uncertainty) on\nreservoir sampling. Table 5 reports the results of random reservoir"}, {"title": "5 Conclusion", "content": "In this work, we propose a novel Prior-free Balanced Replay (PBR)\nframework based on the newly designed uncertainty-guided reser-\nvoir sampling strategy, which prioritizes rehearsing minority data\nwithout using prior information. Additionally, we incorporate two\nother prior-free components to further reduce the forgetting issue\nincluding prototype and boundary constraints, which can main-\ntain effective feature information for continually re-estimating task\nboundaries and prototypes. Compared with existing CL methods\nand SOTA LTCL approach, the experimental results on three stan-\ndard long-tailed benchmarks demonstrate the superior performance\nof the proposed method in both task and class incremental learn-\ning settings, as well as ordered- and shuffled-LTCL settings. In the\nfuther work, we will release the assumptions and address limita-\ntions towards real-world scenarios."}]}