{"title": "Energy-Aware Dynamic Neural Inference", "authors": ["Marcello Bullo", "Seifallah Jardak", "Pietro Carnelli", "Deniz G\u00fcnd\u00fcz"], "abstract": "The growing demand for intelligent applications beyond the network edge, coupled with the need for sustainable operation, are driving the seamless integration of deep learning algorithms into energy-limited, and even energy-harvesting end-devices. However, the stochastic nature of ambient energy sources often results in insufficient harvesting rates, failing to meet the energy requirements for inference and causing significant performance degradation in energy-agnostic systems. To address this problem, we consider an on-device adaptive inference system equipped with an energy-harvester and finite-capacity energy storage. We then allow the device to reduce the run-time execution cost on-demand, by either switching between differently-sized neural networks, referred to as multi-model selection (MMS), or by enabling earlier predictions at intermediate layers, called early exiting (EE). The model to be employed, or the exit point is then dynamically chosen based on the energy storage and harvesting process states. We also study the efficacy of integrating the prediction confidence into the decision-making process. We derive a principled policy with theoretical guarantees for confidence-aware and -agnostic controllers. Moreover, in multi-exit networks, we study the advantages of taking decisions incrementally, exit-by-exit, by designing a lightweight reinforcement learning-based controller. Experimental results show that, as the rate of the ambient energy increases, energy- and confidence-aware control schemes show approximately 5% improvement in accuracy compared to their energy-aware confidence-agnostic counterparts. Incremental approaches achieve even higher accuracy, particularly when the energy storage capacity is limited relative to the energy consumption of the inference model.", "sections": [{"title": "I. INTRODUCTION", "content": "The widespread presence of interconnected devices, driven by pervasive and ubiquitous computing paradigms, con-tinuously generates an unprecedented volume of data. Machine learning (ML) and deep learning (DL) methodologieshave demonstrated substantial efficacy in decoding patterns and extracting knowledge from heterogeneous sensordata [1], [2], enabling accurate predictions and informed decisions in many domains such as smart healthcare [3],[4], human activity recognition [5], [6], and intelligent transportation [7], [8]. However, energy and computationaldemands of DL models are typically prohibitive for practical deployment on mobile devices, characterized bylimited computing power and constrained memory relative to dedicated servers. For example, in computer visionapplications, state-of-the-art vanilla attention-based architectures, such as vision transformers, involve a number ofparameters ranging from 86M to 632M [9], with the computational and memory cost of self-attention mechanismincreasing quadratically with the image resolution [10].Mobile edge computing (MEC) has emerged to bring intelligence closer to edge devices, [11] enhancing theircomputational capacity by offloading tasks to the network edge. Yet, its reliance on stable connectivity introducestrade-offs between latency and energy efficiency. In harsh channel conditions, offloading demands high transmitpower, often exceeding the energy cost of local processing. This limitation underscores the need for an intelligentdevice-edge continuum [12], operating regardless of network quality [13]. In this scenario, energy-harvesting (EH)[14] can be pivotal in achieving energy-autonomy, and ensuring the sustainability and longevity of provided services,particularly those deployed in remote or inaccessible locations whereby regular battery replacement becomesunrealistic and cost-prohibitive [15].A major challenge in the deployment of energy harvesting devices (EHDs) is the constrained and sporadic natureof the energy they capture. Therefore, while minimizing the energy consumption is the main goal in energy storage(ES)-operated devices in order to extend their lifespan, the primary objective with EHD is the intelligent managementof available energy for prolonged operation. In principle, an EHD has access to a potentially infinite energy supply.However, this energy source is intermittent, requiring effective management to ensure stable operation and to mitigatethe impacts of energy shortages. This involves developing strategies for dynamic adjustment of device activitiesbased on energy availability.Recent advancements in intermittent computing [13], [16] have enabled DL in EH IoT devices [17]\u2013[21].Traditional DL models are typically designed for environments with consistent energy sources and do not accountfor the stochastic availability of energy in EH scenarios. To address this, adaptive deep neural networks (DNNs) [22]can be employed. These models are capable of conditionally reducing the execution cost on-demand at inferencetime, trading-off performance with energy consumption. In general, in adaptive DNNs several computing modesare available for processing sensor data. Associated with each mode is an energy cost, with more costly modesgenerally yielding more reliable and accurate predictions.We focus on two different techniques to implement adaptive inference under random energy dynamics: multi-model selection (MMS) [23]\u2013[25] and early exiting (EE) [26]\u2013[28]. MMS involves switching between differentDNNs (computing modes), each with varying energy requirements, depending on the available energy. At the"}, {"title": "II. SYSTEM MODEL", "content": "In this section, we begin with a description of the system model, outlining the key components of the proposedadaptive inference system. For clarity, Table II provides a summary of the main notation used in this work.We formulate the problem as a discrete-time Markov decision process (MDP) with constant-duration time slotsindexed by t \u2208 N. We assume that the sensing apparatus (e.g., a camera) monitors the environment at a constantrate, providing the resource-constrained device (RCD) with the instance w (e.g., an image) for processing every Tslots. Without any loss of generality, we assume that the data arrival process starts at t = 0. Let tn = nT, n \u2208 N, bethe time index of the n-th data arrival. The controller selects the DNN model (MMS) or exit branch (EE) at whichthe computation is halted or temporarily paused, and operates at one of the two granularity levels: (1) singularlyand responsively to the arrival of an input sample w, that is at decision epochs {tn}n\u2208N, with an inter-action timeof T slots (one-shot); or (2) incrementally within the interval [tn, tn+1), at intermediate time slots t =tn + Tt,Tt = 0,...,T \u2013 1, \u2200n \u2208 N (incremental). Precisely, Tt t mod T. The action selection scheme is depicted inFigure 2."}, {"title": "A. Adaptive DNN Model", "content": "We define an adaptive DNN as f(w;0), where w \u2208 Rq and \u03b8\u2208 Rd are the input and the trainable parametervectors, respectively. Without loss of generality, f is considered as the composition of L differentiable operatorsli, i = 1, . . ., L, e.g., li represents a convolutional layer in a DNN. The final output is denoted as \u0177 = f(w; 0). Ina multi-exit DNN, let K < L be the number of exit branches added to the NN structure. In order to provide a validprediction at each exit branch, task-specific classifiers with comparatively negligible processing cost are required.We define the sub-network from the input layer to the k-th exit branch as f(\u00b7; 0(k)), where (0(k) represents the setof DNN parameters involved to the computation up to the k-th exit. Hence, \u0177(k) \u2266 f(w; (0(k)) is the output at thek-th exit, and the final output is \u0177 = f(w; 0(K\u22121)) = \u0177(K\u22121).Similarly, in the MMS scenario, K refers to the different models fk(\u00b7;0(k)), k = 0,1,..., K - 1 to beindependently trained and deployed at the device. At the beginning of the inference process, the controller choosesone of the models to execute. We refer to each exit branch or model as a computing mode. Associated with the k-thmode is a fixed processing energy cost u(f(\u00b7;0(k))), which, with a slight abuse of notation, is denoted as u(k)."}, {"title": "B. Energy Provision Model", "content": "We characterize the EH as a Markov-modulated process where ht \u2208 H \u2261 {h1,...,h|n|} describes the environ-ment states, with transition probabilities $p_{ij} \\triangleq P(h_{t+1} = h_j|h_t = h_i)$. The energy $e_t^h \\triangleq e^f \\in \\mathcal{E}_H = \\{e_1^f,\\dots, e_{|\\mathcal{H|}}^f\\}$provided by the harvesting circuit depends on environment state ht. Due to the uncertainty of environmental condi-tions, we model $e_t^f$ as a discrete random variable with probability mass function (pmf) $p_H^h(e) \\triangleq P(e_t^f = e|h_t = h)$,e \u2208 EH, h \u2208 H. The harvested energy $e_t^f$ is used for the current inference task. Should $e_t^f$ exceed the computationenergy demand ut, the excess is stored in the ES, e.g., battery or supercapacitor. Conversely, if $e_t^f$ is insufficientfor the task's demands, the deficit is compensated by drawing the necessary additional energy from the ES, whenavailable. The ES is modeled as a discrete buffer of energy packets with finite capacity bmax [34]\u2013[36]. The energylevel at time t is denoted by bt = {0,1,..., bmax}, and evolves according to\n$b_{t+1} = \\min\\{[b_t - u_t + e_t^f]^+, b_{max}\\}.$          (1)"}, {"title": "C. Instance-Aware Schemes", "content": "Instance-aware methods integrates the prediction confidence of an adaptive DNN as feedback within a closed-loop control scheme. We first describe an oracle OS-IAw controller having full information of the per-instancerealizations of exit confidences. As mentioned in Section I, the theoretical derivations for OS-IAw fit both EE andMMS, since, mathematically, these two schemes are equivalent.\n1) One-Shot Instance-Aware (OsIAw) Controller (Oracle): The one-shot controller operates every T slots, thatis whenever a new instance w is provided by the sensor apparatus. Assuming that each EE step takes one time slot,it follows that T > K \u2013 1. For T \u226b K \u2212 1, the time available for decision-making, thus for EH, is significantly"}, {"title": "2) Incremental Instance-Aware (IncIAw) EE Controller", "content": "To fully exploit the intrinsic incremental nature of EEwhile containing the limitations in the action selection of the one-shot oracle controller, we study a sequential deci-sion scenario modeled as a discrete-time MDP (Xinc, Pinc, Ainc, rinc), where actions at are intended as incrementalcompositions of sub-actions.The incremental controller operates in each slot t, where it chooses a sub-action at to decide whether to paus ethe computation and switching to idle mode, at 0, or proceed with the computation of the next exit, at 1. Wedenote the sub-action space as Ainc \uc2a5 {0,1}. The decision process starts with an energy-free random prediction(exit 0), and proceeds by incrementally choosing sub-actions at in subsequent slots. Every T slots, the computationfor the current instance wtn terminates, and the system becomes ready for a new sample Wtn+1. For example, in aDNN implementing K 3 exits, setting atn 0 corresponds to performing an energy-free random prediction inslot tn. With atn+1 = 1, the input instance is processed up to the first exit in the following slot tn + 1, resulting inatn = 2. Figures 3(c) and 3(d) illustrate the incremental decision-making process with causal feedback information.At time t, the state of the controller is defined as\n$X_t = (b_t, h_t, \\xi_t, \\tau_t, z_t),$                                         (5)with bt, ht being the ES and the EH processing states, t = 0,..., \u039a 1 representing the index of the currentexit, Tt = 0,..., T 1 denoting the processing stage of the current input, that is Tt = t mod T, and z\u0142 \u2208 [0,1]representing the correctness likelihood of the current exit &t. This can be expressed as the t-th component ofthe vector of correctness likelihoods introduced in Section II-C1, that is z\u0142 = z(t). ztnt Therefore, conversely to theone-shot formulation, whereby the whole likelihood vector is known, in the incremental approach it can be revealedprogressively as the computation proceeds, depending on the sub-actions selected. This makes the incrementalapproach feasible since the needed information is causal. As before, st = (bt, ht,&t, Tt) \u2208 Sinc represents thediscrete component of the state, and we refer to the set {tn, tn + 1,..., tn +T-1} as the processing stage of then-th input instance.Although at the beginning of the n-th processing stage, the value of eff is still uncertain, intermediate realizationsbt and ht, t = tn +1,..., tn + K - 2 can be observed, perhaps improving the estimation of future EH events andcorrecting the action selection on-the-go. This provides the controller with predictive insight into potential energyoutages and overflow, enabling the execution of the i-th exit even when its total energy required to produce anoutput exceeds the amount of energy available when the computation started, i.e. bt. For example, in a system with"}, {"title": "D. Instance-Agnostic (IAg) Schemes", "content": "To asses the importance of taking actions based on the per-instance confidence values, we design both OS- andInc-IAg controllers. These controllers operate over the identical state space as their IAw counterparts, but disregardthe per-instance confidence values. We model the systems as discrete-time MDPs, where the state and action spacesare S, A and Sinc, Ainc, respectively. The performance of the k-th computing mode is measured by its predictionaccuracy p(k), computed over a test dataset D as\n$p^{(k)} \\triangleq \\frac{1}{|D|} \\sum_{i=1}^{|D|} 1 \\{y_i^{(k)}=y_i\\},$                    (6)\nwhere $y_i^{(k)}$ and y\u2081 denote the predicted label produced by the k-th mode, and the ground truth label, respectively, forthe i-th input sample in D. The rewards observed when action a, or sub-action a, is selected in state s, respectively,are:\nr(s,a) = p(a),                                                     (7)\nrinc(s, a) =\n\\begin{cases}\n0                   &\\text{ for } \\tau = 0, 1, ..., T \u2013 2, \\\\ \\\\                                                                                                                                                                         (8)\n\\rho_{(\\xi+\\alpha)} \\text{ if } \\tau = T-1                         \n\\end{cases}\nIn words, when the k-th exit classifier is selected, the reward observed by the controller is the accuracy of thek-th classifier. Hence, IAg schemes use instance information from D to compute p(k), but this estimate is fixed,and does not change as a function of the confidence of the current input instance, zt. Note that, when the DNNis perfectly calibrated, the reward received by selecting the k-th mode matches the mean confidence level of thatclassifier, that is $p^{(a)} = (1/|D|) \\Sigma z_t^{(a)}.$"}, {"title": "E. Energy Considerations", "content": "Let E(i) denote the cumulative energy required for processing an input instance up to the i-th exit. Therefore,E(1) < E(2) < < E(K). When an EE is selected, i.e., atn \u2260 K, the task's energetic demand decreases,"}, {"title": "III. SYSTEM OPTIMIZATION", "content": "Given an initial state 20, the goal is to find a one-shot stationary policy \u03c0 : S \u00d7 Z \u00d7 A \u2192 P(A) and anincremental stationary policy \u03c0inc : Sinc \u00d7 [0, 1] \u00d7 Ainc \u2192 P(Ainc), which maximize the infinite-horizon discountedreward, that is\n$\\pi^*(x) = arg \\underset{\\pi\\in \\Pi}{max} \\mathbb{E}_{\\pi}  \\sum_{n=0}^{\\infty} \\gamma^n r(X_{t_n}, a_{t_n}) | X_0 = x$   (9)\n$\\pi_{inc}^*(x) = arg \\underset{\\pi_{inc}\\in \\Pi_{inc}}{max} \\mathbb{E}_{\\pi_{inc}}  \\sum_{t=0}^{\\infty} \\gamma^t r(X_{t}, a_{t}) | X_0 = x$ (10)\nwhere y is a discount factor. Specifically, in order for \u03c0* and me to be comparable, it must holds that vtn+1-1 = ynfor all n \u2208 Z>1. This is because rinc is non-zero only when T\u2081 = T \u2013 1, that is t tn+T-1=tn+1 -1, Vn \u2265 0.The following lemma shows the connection between the one-shot and the incremental approaches.Lemma 1. For any input instance wtn, and any specified action atn, there exists a sequence of K -1 sub-actions{n+1}=0 that would corresponds to atn \u2208 A. Such correspondence can be expressed in closed-form as\n$a_{t_n} = \\sum_{\\tau=0}^{T-1} a_{t_n+\\tau}.$\nTherefore, an optimal policy inc: S\u00d7A \u2192 P(A) with respect to a generic objective in the incremental formulationis at least as good as an optimal policy for the same objective in the one-shot formulation \u03c0* : S \u00d7 A \u2192 P(A).As a consequence, the incremental approach offers a more fine-grained control strategy, ensuring performance atleast as good as the one-shot alternative. In the next section, we characterize the structure of optimal policies forthe OS-IAw controller and MMS, and we describe the DQN-based optimization of the Inc-IAw-EE controller."}, {"title": "A. Optimal Policy for the OS-IAw controller", "content": "In order to characterize the structure of the optimal policies we recall the definition of value-function and Q-function. The Bellman optimality equation for the svalue function is defined as\n$v^*(x) = \\underset{a \\in A}{max} \\mathbb{E}_{x'} [r(x, a) + \\gamma v^*(x')],$ \nwhere $v^*(x) = \\underset{a}{max} q^*(x, a)$, and\n$q^*(x, a) = \\mathbb{E}_{x'} [r(x, a) + \\gamma \\underset{a' \\in A}{max} q^*(x', a')].$                                 (11)"}, {"title": "IV. EXPERIMENTAL SETUP", "content": "To validate our theoretical findings, we conducted simulation-based experiments. This section provides a compre-hensive description of the architecture of DQN and the multi-exit DNN employed, including a detailed analysis ofper-layer floating point operations (FLOPs), which underpins our rationale for implementing early exits within thearchitecture. Furthermore, we elaborate on the training methodology and calibration procedures employed, alongside the model and parameters used for the energy provision framework."}, {"title": "A. Dataset", "content": "In our experiments, we use a multi-exit DNN for image classification on Tiny Imagenet [33]. We partition thedataset into Dtrain, Dcali, Dest and Dtest as follows: Dtrain receives 70% of the data for the DNN training processes,Dcali is allocated 10% for calibration tasks, Dest is assigned 10% to empirically estimated \u00fb and learn the optimalpolicy, and the testing partition, Dtest, receives the remaining 10% of the data."}, {"title": "B. Multi-Exit EfficientNet: Design, Training and Calibration", "content": "We design multi-exit EfficientNet architectures based on EfficientNet-(B0-B7) models [32]. The exit classifier,attached at the end of each stage, is devised by replicating the structure of the final classifier, while adapting theinput feature map size to match the output size of each stage. Detailed specifications of these exit classifiers areprovided in Table III.For each EfficientNet-Bm, we construct seven sub-networks fi(\u00b7; 0(m)), i = 0,...,6, each formed by thecomposition of the first i-th stages and the exit classifier. We measure the computational cost, in terms of FLOPsfor fi(\u00b7; 0(m)) to process an input instance from the Tiny ImageNet dataset. Figure 5 illustrates the computationalcost in terms of FLOPs for multi-exit Efficientnet-B0 through Efficientnet-B4 relative to the stage index after whichthe exit classifier is attached. Intuitively, the increasing complexity from EfficientNet-B0 to EfficientNet-B4 is aconsequence of compound scaling [32], which also results in higher FLOPs. Interestingly, for a given model m,the FLOPs do not always increase with the exit index i: due to the decreasing input resolution of each stage with"}, {"title": "C. Deep Q-Network", "content": "For the implementation of the incremental policy, the deep Q-network is designed as a compact fully connectedDNN with two hidden layers of 64 neurons each. The total number of FLOPs is 6.6k, which represents only"}, {"title": "D. Energy Harvesting (EH)", "content": "As outlined in Section II-B, we consider, without any loss of generality, a Markov chain with two environmentstates: (G)ood, and (B)ad, i.e., H = {G, B}. These states signify favorable and unfavorable conditions, respec-tively. For instance, in the context of solar energy, the states could represent diurnal cycles of day and night ormeteorological variations like sunny and cloudy conditions. Notice that the theoretical results derived in Section IIhold for a generic Markov chain. Hence, a real EH source could be modeled better with additional environmentalstates. Nevertheless, it would also increase the complexity of the analysis unnecessarily.For brevity, we define the transition probabilities as $p_G^h \\triangleq p_{G,G}^h$ and $p_B^h \\triangleq p_{B,B}^h$, Vt. Similarly, the energy unitsharvested over a time slot are eff \u2208 {0,1} with $p_H^G \\triangleq P \\{e_t^f=1\\|h_t = G\\}$ and $p_H^B \\triangleq P \\{e_t^f=1\\|h_t = B\\}$, \u221at. The incoming energy rategenerated by the EH process is defined as\n$\\mu \\triangleq p_H^G p_G + p_H^B p_B, $                     (21)\nwhere pr, h\u2208 {G, B}, is the limiting distribution of the Markov chain {ht}."}, {"title": "V. EXPERIMENTAL RESULTS", "content": "This section provides an extensive discussion of our experimental results. Section V-A investigates the impact ofcalibration on the decision-making performance under different energy constraints. In Section V-B, we numerically"}, {"title": "A. Calibrated vs Uncalibrated Oracle", "content": "We study the effect of calibration on decision-making performance of the oracle controller, i.e., OS-IAW. We select some representative values for the harvested energy pmf, where each corresponds to a different energy rate, therefore to a certain degree of energy constraint. We compute an e-optimal policy through Algorithm 1 using an uncalibrated and a post-training calibrated DNN model. We measure the performance by the average number of correctly classified input samples in Dtest, computed for 200 episodes of length 100 steps. Table IV reports the parameters selected and shows the dominance of the calibrated DNN over its uncalibrated counterpart, with long-term average accuracy improvements varying between 1% and 3.6%. For this reason, in the remaining simulations we use calibrated DNNs."}, {"title": "B. Policy Comparison", "content": "We compare the numerical policies obtained for OS-IAw-oracle, MMS, Inc-IAg-EE and Inc-IAw-EE (DQN),using a discount factor of y = 0.9, bmax = 30, $p_G^p$ = 0.5, $p_B^p$ = 0.9, $\\rho_1^G$ = 0.8, and $\\rho_1^B$ = 0. As discussed inSection IV-B, we adopt a linearly increasing cost with the computation complexity. Precisely, the energy cost ofthe k-th computing mode is u(k) = k, k = 0,1,2,3, where k 0 refers to the initial random predictor.All the policies we consider are deterministic, but they differ in their operational granularity. In fact, within theinterval [tn, tn+1), the OS controller takes a single action to decide a computing mode while the Inc controller takesT-1 sub-actions to sequentially decide the optimal computing mode. Therefore, to compare the controllers' policies,we compute the fraction of input samples being processed at the k-th computing mode \u03b7\u03ba (b, h) when btn bandht\u2081 = h at the beginning of each slot [tn, tn+1), Vn. This can be interpreted as the empirical probability of selectingcomputing mode k in (b,h).In a given state (b, h) the MMS selection does not have any additional randomness, hence \u03b7\u03ba(b,h) = 1 iff\u03c0* = Mms (b, h). Conversely, the OS-IAw (oracle) inherits the randomness from the input samples. Thus, we"}, {"title": "C. Performance Comparison", "content": "This section provides a comprehensive comparison of various policies for EE and MMS under different energyconditions. Precisely, we compare the performance in terms of accuracy P accrued by the controller policy \u03c0continuously performing a classification task, that is\n$\\rho_{\\pi} \\stackrel{\\triangle}{=} \\frac{1}{T} \\sum_{t_n=1}^{T} 1\\{y_{t_n}= \\hat{y}_{t_n}^{(a_{t_n})}\\}, a_t \\sim \\pi. $                                            (24)\nEach policy is obtained by solving a cumulative discounted reward problem with y = 0.9. The accuracy is computedfor 30 episodes of length T = 5000, using a combination of the following hyperparameters: $p_G^p \\in \\{0.5, 0.7, 0.9\\}$,$p_B^p \\in \\{0.3, 0.5, 0.9\\}$, $\\rho_1^G \\in \\{0.3, 0.7, 0.8, 1\\}$ $\\rho_1^B \\in \\{0, 0.2, 0.3, 0.5\\}$, and bmax = {3,5, 10, 20, 30}. For the analysis,we focus on the accuracy performance as function of the ES maximum capacity and the incoming energy rate."}, {"title": "D. Key Observations", "content": "Each policy can be interpreted as establishing dynamic thresholds for each computing mode, which vary withthe ES level. For instance-aware methods, this dynamic nature is also influenced by the distribution of inputconfidences. Although the likelihoods produced by a DNN, even when calibrated, cannot be strictly considered astrue probabilities, their integration into the decision-making process leads to more informed decisions, achievinghigher accuracy with reduced energy consumption. This approach favors the selection of more energy-intensivemodes only when necessary, such as when an input instance presents high complexity for the current task and DNNmodel. Furthermore, when a system with constrained bmax operates in a low-incoming energy regime, incrementalcontrollers outperform their one-shot counterparts. In our experiments, this is observed when the cost of the mostexpensive computing mode exceeds 60% of the system battery capacity bmax. Instead, in high-incoming energyregimes, and for an unconstrained bmax, the performance of one-shot and incremental controllers are comparable.In conclusion, if the distribution of input samples in the available dataset is statistically representative of the onein the user application, incorporating DNN output likelihoods into the decision-making process is recommended toenable instance-awareness. Additionally, our findings suggest that adopting multi-exit DNNs as a proxy for inferenceadaptation is advantageous, as it enables not only instance-awareness, but also incremental decision-making strategieswith better performance with relatively small ES capacity."}, {"title": "VI. CONCLUSION AND FUTURE DIRECTIONS", "content": "In this paper, we addressed the challenges associated with adapting neural inference workloads to the availableenergy envelope and ES constraints in EHDs. We developed a comprehensive framework for optimal controlin dynamic DNNs by studying MMS and EE strategies. These approaches were tailored to leverage instance-aware and instance-agnostic control schemes, operating at both one-shot and incremental granularities. Our maincontributions include establishing the optimal policy structure for the MmS system, demonstrating its monotonicityin ES levels for energy and memory efficiency. We formulated a one-shot oracle controller using per-instance exitconfidences with theoretical guarantees and designed an approximate VI algorithm to estimate optimal policies usingempirical confidence distributions. Additionally, we developed a sub-optimal policy based on a lightweight DQN forincremental instance-aware EE selection, named Inc-IAw-EE. In conclusion, we conducted comprehensive empiricalevaluations comparing our control schemes on a custom multi-exit EfficientNet model tested on the TinyImageNetdataset, analyzing accuracy under various ES capacities and energy rates. For the extensive range of parameter valuesexamined in this work, simulations indicates that IAw and Inc control schemes can significantly enhance accuracy,"}]}