{"title": "TRAVEL: Training-Free Retrieval and Alignment for Vision-and-Language Navigation", "authors": ["Navid Rajabi", "Jana Ko\u0161eck\u00e1"], "abstract": "In this work, we propose a modular approach for the Vision-Language Navigation (VLN) task by decomposing the problem into four sub-modules that use state-of-the-art Large Language Models (LLMs) and Vision-Language Models (VLMs) in a zero-shot setting. Given navigation instruction in natural language, we first prompt LLM to extract the landmarks and the order in which they are visited. Assuming the known model of the environment, we retrieve the top-k locations of the last landmark and generate k path hypotheses from the starting location to the last landmark using the shortest path algorithm on the topological map of the environment. Each path hypothesis is represented by a sequence of panoramas. We then use dynamic programming to compute the alignment score between the sequence of panoramas and the sequence of landmark names, which match scores obtained from VLM. Finally, we compute the nDTW metric between the hypothesis that yields the highest alignment score to evaluate the path fidelity. We demonstrate superior performance compared to other approaches that use joint semantic maps like VLMaps [11] on the complex R2R-Habitat [1] instruction dataset and quantify in detail the effect of visual grounding on navigation performance.", "sections": [{"title": "I. INTRODUCTION", "content": "Vision-and-Language Navigation (VLN) task involves controlling an agent, either in simulation or in the physical world, to navigate through an environment by following natural language instructions. Consider an example in Fig. 1 where agent is required to follow the instructions in a specific environment. This task requires parsing the language input (e.g., \u201cTurn left in the hallway, go to the kitchen, and stop by the sink\u201d), grounding the phrases to visual concepts such as scenes, landmarks, and actions (e.g., turn left) as well temporal cues (e.g., turn before). One class of approaches formulates the Vision-Language Navigation task as a supervised multi-modal sequence-to-sequence learning task, where the learner is given episodes of natural language instructions, along with visual observations and navigation actions. These approaches were supported by large-scale datasets of navigation instructions, e.g., Room-2-Room (R2R) [1], in Matterport3D [4] indoor environments, providing the agent with panoramic images from different locations."}, {"title": "II. RELATED WORK", "content": "For the purpose of our exposition, the existing works on Vision Language Navigation can be partitioned into end-to-end and modular approaches. The end-to-end methods take the natural language instructions, visual observations, and actions and train a multi-modal sequence-to-sequence model, and in the inference stage, given the instruction and initial view, the model generates the sequence of actions while ingesting additional views. The modular approaches integrate LLMs, VLMs, or both with more traditional map-based representations along with a common robotics navigation stack comprised of basic navigation skills that are not learned.\nEnd-to-end approaches. These methods typically adopt a sequence-to-sequence model, taking as an input the language instruction and visual information and outputs the sequence of low-level navigation actions (move, turn left/right) or local waypoints. During the forward pass, the entire instruction is processed by the Language Encoder (e.g., LSTM/transformer). The aggregation of the context vectors, plus the encoded current view of the agent, is then fed to the Action Decoder (e.g., LSTM/transformer) that generates the next action. The decoder continues to predict actions until it gen- erates the STOP action. The mixture of Reinforcement Learning (RL) and Imitation Learning (IL) has been commonly used for training these models [24]. The ex- isting approaches proposed different variations of model architectures, training strategies and choice of represen- tations [2, 24, 8, 26, 15, 10, 19, 6, 9] typically using the Room-to-Room (R2R) [1] and Room-Across-Room (RxR) [16] benchmarks for training and evaluation. The natural language instructions in these benchmarks are quite complex, with an average length of ~ 26 words. These approaches have made substantial improvements in past years, mostly thanks to increasing the number of training episodes and auxiliary tasks that support grounding [25] and instruction generation [8, 14]. It has been shown [29] that the performance of the existing methods continues to be severely compromised by the inability to ground landmarks, understand spatial rela- tionships, as well as grounding of action phrases. The ability to ground landmarks is more critical for indoor environments, while in outdoor settings, the grounding of actions in navigation instructions is more critical. Furthermore, RL & IL require a large number of high- quality training episodes, in addition to the extra compu- tational complexity of RL due to the online interaction of the agent with the simulator/environment that makes it more difficult to scale the training [14].\nLLM and VLM based modular approaches. Language Models were used in the past as zero-shot plan- ners, where [12] introduced the idea of utilizing the knowledge learned by LLMs, like OpenAI GPT-3 [3] and Codex [5], for decomposing high-level tasks (e.g. \"make breakfast\") to sequences of lower level skills executable by the agent. For navigation tasks, CLIP- Nav [7] utilized CLIP VLMs [20] for grounding instruction phrases and GPT-3 [3] for decomposition of complex natural language instructions into phrases. In CLIP-Nav, the language instruction is decomposed using GPT-3 [3], and then each sub-instruction, along with a panorama comprised of four egocentric views, is ranked by CLIP [20] to determine the closest heading direction. The major limitations of CLIP-Nav are the dependency on the existence of a navigable graph of the environment and the poor ability of CLIP to associate landmarks with images. Another decomposition of the navigation task was adopted by the VLMaps [11] approach, which first builds a global joint vision-language semantic occupancy map by exploring the environment. The cells of the map are populated by LSeg/CLIP embeddings [17, 20], projected onto the grid from images. The navigation instructions are simpler, often resorting to point and object goal navigation, which are further translated into robotic navigation skills in the form of executable code. Lang2LTL [18] represents another line of work that has been proposed to use LLMs to translate free-form natural language instructions into linear temporal logic (LTL). Lang2LTL is advantageous because it disambiguates the goal specification and facilitates incorporating temporal constraints. The limitations of Lang2LTL are the need for a parallel dataset of natural language instructions and their corresponding fixed set of LTL formulas for fine- tuning the LLMs for the translation stage and the limited level of complexity of the instructions, compared to R2R [1] and RxR [16] benchmarks. Authors in LM-Nav [23] propose a zero-shot approach for outdoor instruction following. They utilize a visual navigation system called ViNG [22], to construct a topological map G from a set"}, {"title": "Contributions", "content": "In the presented work, we pursue a modular approach, where we exploit zero-shot capabili- ties of the state-of-the-art LLMs for understanding and parsing navigation instructions and VLMs for grounding landmark names in the visual observations. The naviga- tion component is carried out by finding a path in the topological map of the environment that is best aligned with the navigation instructions. The map is acquired using the training episodes from R2R dataset [1], and the alignment score is computed using dynamic pro- gramming, where the costs of individual steps are ob- tained from the state-of-the-art Vision-Language Model. The presented modular approach demonstrates superior performance over occupancy map-based approaches and reveals current strengths and weaknesses of the state-of- the-art LLMs and VLMs for vision-language instruction following."}, {"title": "III. OUR APPROACH", "content": "We introduce a modular approach for solving the VLN task using the pre-trained state-of-the-art language and vision and language models in a zero-shot setting, fo- cusing on complex instructions in R2R-Habitat dataset. Our approach consists of eight main steps. In STEP 1, the agent first builds a topological map of the environment using the train split episodes of the dataset. We used all the available unique waypoints and trajectories of the environment to build the graph G, where each node v is represented by a 360\u00b0 RGB panorama and each edge e has a weight of 1, repre- senting the connectivity between each pair of nodes, as shown in Figure 2. In this way, we ensure consistency in our evaluation process as every node of the ground-truth waypoints from the training episodes has a correspond- ing node in the topological map. In STEP 2, we extract the sequence of landmarks from the natural language instruction using a pre-trained LLM, LLama-3.1-8B-Instruct in our case. We identify the last landmark phrase and search panoramas for the top-k most likely goal nodes. Suppose that the last landmark is bedroom, we can locate the goals by recognizing whether the bedroom can be found in the panoramic images associated with the graph nodes. In this way, we will narrow down the set of possible paths that lead to the goal locations. In STEP 3, we use the state-of-the-art vision language model SigLIP [28] for goal/final landmark recognition, as shown in Figure 3. SigLIP training is similar to the CLIP model, replacing the contrastive loss with sigmoid binary prediction. The recognition is carried out by computing cosine similarity between panorama images and the textual description of the landmark. In order to compare the effectiveness of this choice with an open-vocabulary semantic map such as VLMaps [11] that endows the occupancy map with CLIP embeddings, we ran the landmark localization experiment on all 127 landmarks and reported the mean Precision@10 in Table I. The superiority of our approach stems from recognizing the landmarks in the panoramic views and replacing CLIP [20] with SigLIP [28], instead of using open-vocabulary semantic occupancy maps."}, {"title": "IV. LIMITATIONS", "content": "There are specific limitations to our approach that we'd like to elaborate on. Firstly, our approach only works in the previously explored environments, given the topological map. Secondly, it only works in cases where the natural language instruction is landmarks- based and is not heavily based on spatial and temporal phrases, action phrases, and absolute metric distances. Since our pipeline is modular and not trained end-to-end, drawbacks of each module, especially the early stages of the LLM landmark extraction and VLM retrieval, propagate the errors to later stages of PANO2LAND alignment or GPT-40 ranking. The quality of the path hypotheses eventually determines the upper bound on the ranking computed by GPT-40 or any other VLM being used."}, {"title": "V. CONCLUSION", "content": "In this work, we introduced a modular approach for the vision-and-language navigation (VLN) task based on the R2R-Matterport3D dataset [1, 4] within the Meta Habitat Simulator [21, 27]. Our approach assumes that the agent has built a topological map in the exploration stage. We then use LLM to extract the sequence of landmarks the agent needs to visit, retrieve the top-k goal locations, and rank the path hypotheses to select the one with the highest alignment with the natural language instructions as the final answer. For the task, the approach demonstrates the superiority of the topological map with per-node panoramas to an open-vocabulary se- mantic occupancy map for land-mark grounding and goal retrieval. The overall performance on this benchmark is mainly affected by the zero-shot capabilities of VLM's to ground special landmark names in the panoramas. Future improvements can be achieved by fine-tuning the existing VLMs on navigation tasks and deploying the agent in previously unseen environments by seamlessly integrating the exploration and navigation part."}]}