{"title": "LOST IN THE LOGIC: AN EVALUATION OF LARGE LANGUAGE MODELS' REASONING CAPABILITIES ON LSAT LOGIC GAMES", "authors": ["Saumya Malik"], "abstract": "In this thesis, I evaluate the performance of Large Language Models (LLMs) on the Law School Admissions Test (LSAT), specifically the Logic Games section of the test. I focus on this section because it presents a complex logical reasoning task and thus is a valuable source of data for evaluating how modern, increasingly capable LLMs can handle hard logical reasoning tasks. I construct a dataset of LSAT logic games and their associated metadata, and extensively evaluate LLMs' performance in a Chain-of-Thought [22] prompting setting. Given the weak performance in this setting, I explore other prompting frameworks on a smaller subset of the dataset, adapting ideas from Reflexion [18] to this task. This results in a substantially improved accuracy of 70% for GPT-4 and 46% for GPT-3.5 on this data subset, highlighting the capacity of LLMs to revise their logical errors, despite initially weak performance. Finally, I analyze the types of logic games that models perform better or worse on, as well as the types of logical errors I observe from human annotation, providing detailed insights on the logical reasoning capabilities of LLMs.", "sections": [{"title": "Introduction", "content": "In the wake of the release of GPT-4 [15], Llama 2 [20], Claude 2 [5], Mistral [11], and other high-performance large language models (LLMs) all within the past year, there has been renewed interest in developing good evaluation benchmarks for LLMs. Older evaluation benchmarks may either be trivial or no longer usable, due to both saturation, dataset contamination, and overall simplicity. One important area for LLM evaluation is their reasoning capabilities, with much research aiming to uncover the extent to which LLMs can perform complex reasoning as opposed to merely reciting facts or complex explanations they have been trained on. To answer these questions, it is increasingly important to develop good evaluation benchmarks.\nIn their 2023 report on GPT-4, OpenAI reports an accuracy of 67% on the Law School Admissions Test (LSAT), an impressively high number for a difficult exam that most aspiring law students spend months preparing for. OpenAI does not release further details or analysis of how, specifically, they prompted GPT-4 other than mentioning a multi-shot setting [15]. This motivates taking a closer look at GPT-4's performance on the LSAT, along with other LLMs. The Logic Games section of the LSAT, in particular, is an underutilized resource for qualitatively and quantitatively evaluating the performance of LLMs on tasks that require careful and complex logical deductions.\nIn this thesis, I construct a dataset of LSAT logic games and probe several closed- source and open-source LLMs with different prompting strategies to evaluate how they perform on highly complex logical reasoning questions. I also qualitatively evaluate their performance along a number of analytical dimensions, deriving insights about particular types of logical errors LLMs are prone to. In particular, the contributions are fourfold:\n1. The construction of a novel dataset containing all publicly-available LSAT logic games and metadata including their difficulty and game type. I release this dataset publicly on HuggingFace at this link \u00b9 to encourage further research.\n2. An in-depth exploration of the limits of traditional Chain-of-Thought [22] prompt- ing for task of LSAT logic games, as well the potential applicability of Chain- of-Thought prompting to a particular subset of our dataset.\n3. An application and implementation of the Reflexion [18] framework to our LSAT logic games task, resulting in substantially improved accuracy compared to Chain-of-Thought prompting.\n4. A quantitative and qualitative analysis of the types of logic LLMs particularly excel at and fall short in.\nI find that Multi-Shot Chain-of-Thought prompting results in low overall accuracy on the LSAT logic games task, with GPT-4 achieving the best accuracy of only 33% among the models I evaluate, with different models performing better or worse on different categorical types of logic games. This showcases the value of our dataset, as it presents a genuinely difficult logical task for LLM evaluation, but also highlights the limit of Chain-of-Thought prompting for exploring the limits of LLM capabilities on this task."}, {"title": "Background and Related Work", "content": "The Law School Admission Test, or LSAT, is a standardized test that has been run by the Law School Admission Council (LSAC) since 1948. The current form of the LSAT consists of three types of sections: reading comprehension, logical reasoning (which is \"designed to evaluate your ability to examine, analyze, and critically eval- uate arguments as they occur in ordinary language\"), and analytical reasoning, more commonly referred to as logic games.\nLogic games are \u201cdesigned to assess your ability to consider a group of facts and rules, and, given those facts and rules, determine what could or must be true\" [3]. Logic games vary widely in their content matter, type of logical question being asked, and difficulty, giving them several parameters worth analyzing. Beginning in August 2024, though, the logic games section of the LSAT will be removed. Nonetheless, the existing bank of logic games provide a great resource for evaluating the ability of LLMs to understand complex logical scenarios and answer questions that require simultaneously satisfying several logical constraints. I describe logic games in further depth and provide examples in Section 3.1 and in Appendix A."}, {"title": "Prompting", "content": "While foundational Large Language Models are incredibly powerful, how to craft questions (or \u201cprompts\") for them in a way that accesses their full power is still an active area of research known as prompt engineering. Prompt engineering is \u201ca relatively new discipline for developing and optimizing prompts to efficiently use language models (LMs) for a wide variety of applications and research topics\" [17]. Contemporary advances in prompt engineering thus provide relevant background and inform the approach of my thesis. One particularly relevant prompting strategy that I explore in this thesis is Chain-of-Thought Prompting [22]."}, {"title": "Chain-of-Thought Prompting", "content": "Chain-of-Thought Prompting [22] refers to explicitly telling a model to go step-by-step and explain its logic. It has been shown to be very helpful on more difficult question answer tasks that require step-by-step reasoning. In \u201cMulti-Shot\" Chain-of-Thought prompting, we can prepend multiple examples of questions and accompanying step- by-step explanations to the prompt. These examples serve as demonstrations to further prompt the model to thoughtfully lay out its reasoning. In \u201cZero-Shot\" Chain- of-Thought, there are zero provided examples, and we can simply append \u201cLet's think step by step\" to the prompt [12]. This has been shown to greatly improve model performance."}, {"title": "Related Work", "content": "There are a number of existing logical reasoning datasets for evaluation, including LogiQA [13], FOLIO [10], LogicNLI [19], and more. For the most part, these datasets evaluate the execution of simple logical rules. LSAT logic games have harder ques- tions, more intricate multi-step logic that requires simultaneously satisfying many constraints at once. As such, they provide a valuable additional benchmark for see- ing how increasingly capable LLMs perform on difficult multi-step logic questions. LSAT logic games thus provide a task that allow us to to evaluate how LLMs exe- cute multi-step logic with many constraints, but also allows us to see at a micro-level whether models are able to execute even simple logical rules when presented in a difficult context."}, {"title": "LSAT and Language Models", "content": "A few prior works look at the LSAT. Zhong et al. collect and release a dataset of LSAT logic game questions (that is used by most research on the LSAT), but their approach (creating a non-LLM system for solving LSAT questions) is different from my motivation of evaluating how LLMs perform on the LSAT [27][21]. Furthermore, their dataset does not include valuable metadata (Problem/Game Difficulty, Prob- lem/Game Type) that guides much of my approach and my analysis. I hope releasing my dataset will inspire future work that makes use of this rich metadata. Additionally, Ye et al. [24] look at improving the performance of LLMs on this aforementioned LSAT dataset, but their approach focuses on using LLMs to formalize the problem, which then gets solved by an external solver. My approach, though, focuses on probing different prompting methods to see how the LLMs themselves do on solving the puzzles. Furthermore, while Ye et al. report a baseline of 23% accuracy by GPT-3.5 on the LSAT logic games, they do not analyze this result over more prompting settings. My work both provides analysis to elucidate what might be giving rise to the 23% number and also probes alternate prompting frameworks that result in LLM performance that improves on their reported baseline."}, {"title": "Dataset Construction and Implementation", "content": "The first major contribution of my project is the construction of a dataset of all LSAT Logic Games that are available for free online. I release this dataset to encourage future research at: https://huggingface.co/datasets/saumyamalik/lsat_logic_games- analytical_reasoning. To create my dataset, I collected every logic game question from every past LSAT test that has been released for free publicly (many tests are behind a paywall and thus would not be releasable as part of my dataset). There are 18 tests accessible on the web with free accounts-five on the official LSAC LawHub Website [3], and thirteen on Khan Academy [2]. Each test has four game setups, each with 5-6 questions that follow, for a total of 23 questions per test (with the exception of one retracted question in one past exam). In total, this amounts to 413 questions. From the LSAC website and Khan Academy, I copied over each question's setup rules, question and answer choices, and correct answer. I also curated expla- nations for twenty of the questions, adapting explanations from LSATHacks [6] and Khan Academy. These explanations serve as demonstrations for Chain-of-Thought prompting.\nEach question additionally has several parameters relating to the type and dif- ficulty of the logic game as a whole and the question itself. These are not official parameters and categories that LSAC tags the problems with, but rather categories that well-established and well-respected LSAT prep sites online tag problems with. As such, they can be used as ground-truth human annotations for the dataset. For each question, I found values for each parameter from the online blogs 7SagePrep [1] and PowerScore [4]. This is a novel contribution\u2014 no other LSAT dataset compiles these annotations, which provide valuable dimensions for prompting and analysis. I will now describe each of these parameters below along with some examples, and additional examples can be found in Appendix A."}, {"title": "Logic Game Difficulty and Problem Difficulty", "content": "Logic Games are rated for difficulty on a scale of 1 (easiest) to 5 (hardest). Within each game, each problem is similarly rated for difficulty on a scale of 1 to 5. While often there is correlation between Game Difficulty and Problem Difficulty it is common for most problems in a Game of Difficulty 5 to also have very high difficulty it is possible for Games of Difficulty 5 to have Problems of Difficulty 2 and vice versa. These labels are taken from 7SagePrep [1]."}, {"title": "Logic Game Type", "content": "Logic Games can come in many types. Each type may differ in the nature of assign- ment they are asking for and in the nature of their constraints. Logic Game types can be broadly grouped into In-and-Out games, Sequence games, and Grouping games."}, {"title": "Problem Type", "content": "There are many sub-dimensions of Problem Type, and one problem may take on values for each sub-dimension:\n\u2022 Locality: A problem can be \u201clocal\u201d or \u201cglobal.\u201d\u201cGlobal\u201d problems ask for logical deductions based on the global set of rules and setup of the problem (like the Sequence question above), while \u201clocal\u201d problems introduce a new local constraint and posit how this added constraint affects or narrows the possibility space (like the Grouping and In-and-Out questions above).\n\u2022 Degree of Truth: Many questions are of the form \u201cWhich of the following must/could/cannot be true?\u201d I refer to this as a problem's degree of truth. (All three questions above are \u201ccould be true\u201d questions.)\n\u2022 Others: There are several other problem types, none of which are mutually exclusive with each other, including rule substitution (Which of the following rules, if replaced with X constraint, achieves the same outcome?), and \u201cexcept\u201d questions (All of the following could/must be true except?). These problem types do not feature heavily in my analysis.\nThese labels are taken from the PowerScore Forum [4], but with the caveat that in my experience, the labels for Problem Type are occasionally incomplete (e.g., a problem that is both \u201clocal\u201d and \u201ccould be true\u201d might only be marked in PowerScore as \"local\"). I correct these errors as I notice them, but some labels may still be incomplete."}, {"title": "Answer Explanations", "content": "One challenge with selecting explanations to serve as Chain-of-Thought demonstra- tions is that reasoning paths can vary widely by problem for some problems, it is better to make deductions from the rules and problem statement before looking at any answer choices, whereas for other problems, it may be better to look at each answer choice and how it relates to the constraints one-by-one. This diversity of reasoning paths likely ends up being a limitation of Chain-of-Thought prompting, as I discuss when analyzing the results of extensive Chain-of-Thought experiments in Section 5.1.2.\nNonetheless, crafting explanations to have a consistent high-level structure to their problem-solving is helpful. Khan Academy explanations, in particular, follow a con- sistent high-level structure of analyzing the problem setting (categorizing the logic game), making deductions based on constraints, then looking through each answer choice and making deductions along the way. I adapt these explanations slightly and reformat them to have this consistent formatting, with some added sign-posting to guide the model's output formatting. I always add a concluding \u201cTherefore, the correct letter answer is: {Correct Answer}.\" to the end of the demonstra- tion to facilitate effective answer extraction. I verify that the models emulate this answer formatting well. As an example, Figure 3.1 shows the explanation to the In-and-Out question above.\nI release this annotated dataset, along with the compiled explanations, to facilitate future research.\""}, {"title": "Experimental Setup", "content": "I sought to evaluate how models would do if prompted with a logic game setup, question, and multiple letter answer choices. I prompted GPT-3.5 and GPT-4 via the OpenAI API. For these experiments, I used gpt-3.5-turbo (the January 25th version) and gpt-4-1106-preview, which at the time of my experiments, was the most recent version available. I also prompted Claude 2 via the Anthropic API (but queried Claude 2 for only one prompting setting given pricing constraints). As for open source models, I prompted both Llama2-7b and Mistral-7b (significantly smaller models) using their versions on HuggingFace."}, {"title": "Experimental Settings", "content": "Since LLM bias towards particular answer labels has been documented for Multiple Choice Questions [25], for each question, I ran 5 permutations, rotating the answer options such that the correct answer choice appears in each letter position once. This mitigates the effect of any positional bias in results.\nExcept when otherwise indicated, I set sampling temperature to 0 to reduce vari- ance, since my goal is to uncover the true inclination of models, and answers were highly inconsistent in initial experimentation with temperature greater than 0. I will now describe each prompting setting."}, {"title": "Prompting Settings - Full Dataset", "content": "Kojima et al. showed that even Zero-Shot Chain-of-Thought prompting can improve model performance over standard Zero-Shot prompting, simply by explicitly prompt- ing the model to reason step by step [12]. I implemented Kojima et al.'s two-step Zero-Shot Chain-of-Thought formulation. The first step, \u201cReasoning Extraction,\" prompts the model with the question followed by \u201cLet's think step by step.\u201d. Then the second step, \u201cAnswer Extraction,\u201d appends the response the model gener- ated in the first step to the original prompt, along with the final formatting prompt, \u201cTherefore, the correct letter answer is:"}, {"title": "Experiments on In-and-Out Subset", "content": "Initially motivated by strong GPT-4 Zero-Shot Chain-of-Thought performance on In- and-Out questions in particular, I focused on the subset of In-and-Out Questions (in- cluding hybrid questions) from the full dataset to see if I could further improve perfor- mance on this subset with multi-shot Chain-of-Thought Prompting and other strate-"}, {"title": "Multi-Shot Chain-of-Thought", "content": "For my multi-shot Chain-of-Thought experiments, I created a pool of 5 In-and-Out Demonstrations, carefully hand picking problems to be not too easy2 and to span a variety of Hybrid game types. (Since they are used as demonstrations, these five questions are removed from the In-and-Out dataset.)\nI then conducted k-shot prompting for k = 1 to 5, at each level taking k prompts from my pool of demonstrations. For each k, I took 5-9 combinations of k demon- strations from the overall pool. Querying GPT-3.53 with many prompts (for each k) is crucial for finding a prompt that works effectively."}, {"title": "Self-Reflection", "content": "In their paper \u201cReflexion: Language Agents with Verbal Reinforcement Learning,\" Shinn et al. propose \u201ca novel framework to reinforce language agents not by updat- ing weights, but instead through linguistic feedback\" [18]. I apply the framework and ideas behind Reflexion [18] to see whether GPT-3.5 and GPT-4 can improve their accuracy on the In-and-Out dataset if given linguistic feedback from their environ- ment, where feedback takes the form of indicating to the model that its answer was incorrect (but not revealing the correct answer). In other words, I explore if models are capable of Self-Reflection\u2014 identifying their own errors and getting the question right on a second try. I call this implementation Self-Reflection to distinguish it from Reflexion since this implementation is not a direct implementation of Reflexion (this implementation keeps no memory buffer for future trials, as one difference), but rather is inspired by the Reflexion framework. In particular, my implementation has 3 steps:\n1. Prompt: Prompt the model with the question and get its response.\n2. Reflect: If the response is incorrect, provide \u201cenvironment feedback\" that the response is incorrect and ask it to reflect on what went wrong and make a plan to solve the question correctly next time.\n3. Revise: Provide the model its reflection and elicit a response.\nA successful example walking through the exact prompts for each of these three steps, along with GPT-4's initial response and successful reflection and revision, is illustrated in Figure 3.2.\nI made a few modifications to Reflexion's prompts to adapt it to our logic game task. For one, I changed the second step's Reflection prompt to ask for a specific logical error (as opposed to a high-level plan in the original implementation) after observing that asking for a high-level plan did not lead to successful revision on our task. I also chose to omit the model's original incorrect response in the revision prompt, only giving the model the Question and the Reflection in Step 3. I did this because I observed that providing the original incorrect response heavily biases the model toward producing that same incorrect response again.\nFor this experimental setting, I set temperature > 0 to encourage the model to be creative and allow it to generate a new revised thought that differs from the original one it proposes. I implement Self-Reflection for only the smaller In-and- Out subset of the full dataset so that I can feasibly annotate each stage in the Self- Reflection procedure, making note of when reflections are correct and revised answers correctly implement reflections to arrive at a correct answer. For each question, I run"}, {"title": "Logical Error Framework", "content": "Finally, I use our In-and-Out dataset to qualitatively evaluate the logical capabilities (and errors) of GPT-3.5 and GPT-4. I manually annotate a subset of 20 In-and-Out questions. For each question, I do the following:\n1. Read the constraints and draw logical conclusions from them (e.g, given an implication, observe that the contrapositive would also hold. Or, given two related implications, apply transitivity, if relevant.)\n2. Solve the problem by hand, tracing out the logical deductions as they come. Make note of what logical deductions lead to the elimination of each incorrect answer choice, thus establishing what logical errors would be indicated by a model selecting that choice. (As a point of comparison for how difficult these problems are, these first two steps together take us, on average, five minutes per problem.)\n3. Look carefully at GPT-3.5's solution to the problem, noting line by line both correct and incorrect logical deductions. For each deduction, categorize the logical rule involved.\n4. Repeat Step 3 for GPT-4."}, {"title": "Full Dataset Results and Analysis", "content": "In this chapter, I present the models' accuracy averaged over the full 413 question dataset. I then dive into analyzing the performance of models along particular pa- rameters of the dataset, making use of the metadata I compiled from the web in Section 3.1. In this analysis, I observe trends of particular Logic Game Types and thus types of logic that models have a preference for, also drawing attention to differ- ences between models. Finally, I conclude with an analysis of dataset contamination and why I am optimistic that any potential dataset contamination would not impact the results and insights from this thesis."}, {"title": "Experimental Results: Summary", "content": "In this section, I report the average accuracy of each model over the entire dataset. First, a few notes on implementation. Due to pricing constraints, I only query Claude 2 for the Zero-Shot Chain-of-Thought setting, which I select because it was the best- performing setting across other models. I verify over several trials that multi-shot Chain-of-Thought prompting does not improve performance on the whole dataset for GPT-3.5, Mistral, and Llama, so I report accuracies only for the zero-shot and one- shot Chain-of-Thought settings. Later in 5, I explore multi-shot Chain-of-Thought"}, {"title": "Analysis by Parameter", "content": "In this section, I analyze model performance by Problem Difficulty and Game Type. I also analyzed performance across Game Difficulty and Problem Type but observed no significant noteworthy trends."}, {"title": "Problem Difficulty", "content": "First, I analyze model performance by Problem Difficulty. As we can see in Figure 4.1, all of the models, for the most part, have higher accuracy on easier problems and lower accuracy on higher problems, with this trend particularly salient for GPT-4 and Claude 2 (which are also the overall best-performing models). For reference, there are 64 problems of difficulty level 1, 74 of level 2, 149 of level 3, 83 of level 4, and 42 of level 5. This suggests that overall, at a high-level, humans and LLMs roughly find the same kinds of logic problems difficult or easy."}, {"title": "Game Type", "content": "By looking at performance based on our broad categories for Game Type-Grouping, In-and-Out, and Sequence in Figure 4.2, we see that models perform significantly better on particular types of games than others. For reference, there are 181 Grouping"}, {"title": "A Note on Potential Dataset Contamination", "content": "In this section, I analyze the results to detect whether there is an impact of dataset contamination on the LLMs' pretraining data. I follow the insights from Oren, Meister et al. [16], who show that if the impact of dataset contamination is strong, models should have a preference toward data (or answer choices) being presented in their \"canonical\" orders as they appeared on the web. I conclude that the impacts of dataset contamination are not strong for the LSAT logic games task. As with all data taken from the web, it is possible that some portion of our dataset might have been included in LLMs' training data. Even if some portion of our dataset is present in the training data, I argue that the results of this thesis still hold. In particular, I observe that any potential dataset contamination likely does not have significant impact on the models' performance on the task, which is fairly low to begin with. In their 2023 technical report for GPT-4, OpenAI evaluates GPT-4 on the LSAT and disclose that 39% of their LSAT evaluation data was present in their training data [15], but based on their analysis conclude that \u201ccontamination is not a substantive confounder on the overall results.\" Given this disclosure of contamination, while it is impossible to determine the intersection between my dataset and their training or evaluation data, it is likely that some portions of my dataset could be present in their training data.\nNonetheless, I argue that the effect of any possible contamination is unobservable in my results. To investigate this, I looked at the accuracies when the gold answer was in its original position. As discussed in Section 3.2, I prompt each question five times, rotating the answer choices so that the gold answer appears in each spot labeled (A)-(E) exactly once. As per Oren, Meister et al. [16], if data contamination has a strong effect, we would expect that the model would perform better when the gold answer is in its original position (or what Oren, Meister et al. would call its \"canonical position\"), i.e., the question is unaltered from its version on the web.\nSo, for each question, I compute models' accuracy when the gold answer is in its canonical position and compare that to models' overall accuracy, averaged over all five answer positions. I calculate this for GPT-3.5's results over the whole dataset as well averaged over many several trials for each prompt setting. I also include this measure averaged over the numerous trials from our In-and-Out experiments, which will be later expanded on in Chapter 5. I also report the two accuracies over the one full run of Claude 2 and GPT-4 on my dataset.\nAs we can see from the results in Table 4.3 below (displaying mean accuracies and standard deviations across many trials over many prompt settings), there is no significant bias toward the gold answer when it is in its canonical position as opposed to other positions in the rotation, with the canonical position in several trials even resulting in less accuracy than the rotated positions. This, along with the generally low accuracy of these models, gives me confidence that any potential downstream impact of dataset contamination on my evaluation is limited.\nFurthermore, irrespective of contamination, analysis of the explanations generated by GPT-3.5 and GPT-4, as conducted in Section 5.3, still provide valuable insights into the broad categories of errors in logic that the models are prone to."}, {"title": "In-and-Out Problem Results and Analysis", "content": "Motivated by GPT-4's very strong accuracy-around 50%-on In-and-Out questions, I dove deeper into this subset of the dataset. In this chapter, I summarize my efforts to push accuracy on this promising subset further, through a systematic exploring of Chain-of-Thought prompting and an implementation of Self-Reflection. Both achieve higher accuracy than zero-shot Chain-of-Thought prompting.\nAdditionally, in an effort to understand what kind of logic GPT-3.5 and GPT-4 excel and fail at, I also qualitatively analyze a subset of this data in depth and discuss categories of logical errors GPT-3.5 and GPT-4 make on In-and-Out problems. Full implementation details are described in Section 3.2.4."}, {"title": "Chain-of-Thought Trials", "content": "For each k \u2208 [5], I queried GPT-3.5 for 5-9 distinct prompts. Each prompt consisted of a different permutation of k demonstrations picked from a pool of 5 demonstrations."}, {"title": "Chain-of-Thought improves GPT-3.5 Accuracy", "content": "Results are summarized in Table 5.1 below, with full results for every prompt setting available in Appendix Table B.1. I report the accuracy of the best trial for each k (in addition to the average accuracy) since I am interested in ascertaining the model's full capabilities given the right prompting. Out of the 32 total trials, four prompts resulted in over 28% accuracy averaged over the In-and-Out Dataset, a notable improvement over 23.3% for GPT-3.5 Zero-Shot Chain-of-Thought. This shows that Chain-of-Thought with examples targeted to the Logic Game Type can improve GPT-3.5 performance!\nAnother curious result from Table 5.1 is that while all values of k had a prompt that resulted in a maximum accuracy of around 28%, the variance between prompts increases with k. Upon manual inspection, some multi-shot prompts fail because after seeing five demostrations, GPT-3.5 gets confused about what the user is asking it to do in response to the sixth question. Future work can further probe the higher variability of longer Chain-of-Thought prompts on logic games."}, {"title": "Chain-of-Thought helps for easy questions but may hurt hard ones", "content": "For each prompt, I looked at average accuracy by Game Type, by Problem Type, and by Problem Difficulty to see if there were any interesting trends or any relationship between the metadata of the prompt demonstrations and improved categories. One noteworthy trend I observed was that the effective (in terms of improving accuracy) Chain-of-Thought prompts very consistently increased accuracy greatly on \u201cEasy\u201d questions of lower Problem Difficulties, but perhaps at the expense of increased vari- ability or worse performance on \u201cHard\u201d questions.\nIn Figure 5.1, I select the highest-accuracy multi-shot prompt for each k and plot its average accuracy for each Problem Difficulty level. I also plot the zero-shot setting as a baseline. We see that all five settings improve accuracy considerably on Level 1 problems, from 23.6% to up to 36.3% for most prompts. For the most part, all difficulty levels improve, but we see that some prompts have a much lower accuracy on Level 5 than the zero-shot baseline. This trend is reflected also amongst prompts that were not effective in improving overall accuracy, with their weakness on Hard problems bringing down their accuracy.\nOne possible interpretation of this is that demonstrations can help models reason more thoroughly and avoid making silly mistakes on Easy questions. However, for hard questions, an overly structured demonstration may stifle the model's creativity in generating a reasoning path for the question. Especially given the particularly high diversity of reasoning paths that can be used across different LSAT logic games, the potentially stifling nature of a demonstration constitutes a real limitation of using Chain-of-Thought prompting for LSAT logic games [26]. It is difficult to craft a one- demonstration-fits-all example for multi-shot prompting, since problem types and problem solving routes look so different from problem to problem."}, {"title": "Analysis of Positional Bias", "content": "Positional bias toward particular answer choice labels has been documented for GPT- 3.5 and GPT-4 in Multiple Choice Question answering [25], where biases toward particular answer labels are supposedly dependent on the nature of the task. While I effectively mitigate this preference by prompting each question five times with the correct answer being assigned to each label once, the positional bias could still obscure the models' accuracy and capability, since I average accuracy over all answer answer labels. Thus, it is worth analyzing GPT-3.5's outputs to see if there is evidence of positional bias. I take the opportunity in this section to analyze positional bias, since in this set of experiments, we have many prompting outputs that we can aggregate over to see whether any bias is significant.\nTo measure positional bias I do the following: Since each answer label sees the correct answer once per question, for each answer label, we can compute the percent- age of times that the model accurately judges the correct answer to be correct when it is in that label's position. In other words, we can say the accuracy for answer label \"(E)\"'s is the percentage of times the model gets the question right when the correct answer choice is in position (E). If there is no positional bias, we would expect there to be no significant difference between the accuracies for each answer label. For each of the 32 trials, I compute the accuracy for each answer label. I then average these percentages over averaged over all the trials within each k-shot setting (indicated for each k in Table 5.1). The results are displayed in Figure 5.2.\nWe can see that for many of the multi-shot settings, there is a significant difference between the accuracy for answer label \u201c(E)\u201d and the accuracy for answer label \u201c(A)\u201d, with answer label \u201c(E)\u201d in some settings even reaching an average accuracy of 30% as compared to option \u201c(A)\" falling short at 17%. Furthermore, the strength of the answer choice preference seems to also vary with the number of demonstrations, suggesting a possible relationship between the length of the prompt and the bias"}, {"title": "Self-Reflection", "content": "For our best performing models, GPT-3.5 and GPT-4, I conducted the three-step Self-Reflection as shown in Figure 3.2. For the first step, I take the each model's initial response to be its respective best-performing k-shot prompting setting on the In-and-Out subset. For GPT-3.5, responses were taken from a 1-shot setting, while for GPT-4, responses were taken from Zero-Shot Chain-of-Thought. The results of the Self-Reflection experiments are summarized in Table 5.2 below. \u201cInitial Accuracy\" refers to the percentage of questions the model gets correct on the first try, \u201cPercent Recovered\" refers to the percentage of questions the model gets correct after reflection and revision among those it initially got incorrect, and \u201cFinal Accuracy\u201d uses the \"Percent Recovered\u201d to compute, among all problems, the percentage the model eventually gets right on the first or second try. As we can see, Self-Reflection improves performance considerably!"}, {"title": "Analysis of Logical Errors in Explanations", "content": "In this section, I thoroughly analyze GPT-3.5 and GPT-4 explanations for a subset of 20 In-and-Out questions, looking at both the models' original outputs as well as corrected outputs after Self-Reflection.\nAs helpful context, In-and-Out games involve assignments that place entities in an \u201cIn\u201d group or an \u201cOut\u201d group (which take different names for each setup). They also involve many Implications as constraints. Since there are only two groups, these problems are easily mapped to Boolean logic, where each entity can be represented as a two-valued Boolean variable that can take on value 1 for \"In\" or value 0 for \"Out\" in an assignment (or vice versa).\nI break down the logical rules into formal logic [8] and categorize logical errors the models make, as well as note places where I believe the models excelled at deftly handling complicated logic.\nI synthesize my observations on errors and offer the following categorization of ten types of logical errors observed across both GPT-3.5 and GPT-4. I offer descriptions"}, {"title": "Missing direct implication.", "content": "For an implication constraint of the form $A \\implies B$ ("}]}