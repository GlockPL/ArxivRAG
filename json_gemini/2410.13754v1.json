{"title": "MIXEVAL-X: ANY-TO-ANY EVALUATIONS\nFROM REAL-WORLD DATA MIXTURES", "authors": ["Jinjie Ni", "Yifan Song", "Deepanway Ghosal", "Bo Li", "David Junhao Zhang", "Xiang Yue", "Fuzhao Xue", "Zian Zheng", "Kaichen Zhang", "Mahir Shah", "Kabir Jain", "Yang You", "Michael Shieh"], "abstract": "Perceiving and generating diverse modalities are crucial for AI models to effec-\ntively learn from and engage with real-world signals, necessitating reliable eval-\nuations for their development. We identify two major issues in current evalua-\ntions: (1) inconsistent standards, shaped by different communities with varying\nprotocols and maturity levels; and (2) significant query, grading, and generaliza-\ntion biases. To address these, we introduce MixEval-X, the first any-to-any\nreal-world benchmark designed to optimize and standardize evaluations across\ninput and output modalities. We propose multi-modal benchmark mixture and\nadaptation-rectification pipelines to reconstruct real-world task distributions, en-\nsuring evaluations generalize effectively to real-world use cases. Extensive meta-\nevaluations show our approach effectively aligns benchmark samples with real-\nworld task distributions and the model rankings correlate strongly with that of\ncrowd-sourced real-world evaluations (up to 0.98). We provide comprehensive\nleaderboards to rerank existing models and organizations and offer insights to en-\nhance understanding of multi-modal evaluations and inform future research.", "sections": [{"title": "1 INTRODUCTION", "content": "Evaluations are crucial in the AI community for two main reasons: (1) they provide early signals to\ndevelopers, assisting in the refinement of data and model, and (2) they guide users in selecting appro-\npriate models for specific tasks. Thus, evaluations offer feedback signals to the entire community,\ndriving model optimization.\nRecently, models with diverse input (Achiam et al., 2023; Xue et al., 2024; Chu et al., 2024) and out-\nput (Betker et al., 2023; Yang et al., 2024; Majumder et al., 2024) modalities have been developed,\nwith evaluations tailored to each. However, these communities often evolve in isolation, resulting\nin significant disparities in evaluation standards and methods. For example, while the large lan-\nguage model (LLM) community has hundreds of multi-task evaluations spanning various domains\nand methodologies, the audio language model community still relies heavily on task-specific bench-\nmarks (Chu et al., 2024). This fragmentation results in inconsistent evaluation signals, misleading\nand bottlenecking the overall progress of various modalities.\nAdditionally, existing evaluations exhibit significant biases in query, grading, and generalization.\nQuery bias occurs when evaluation tasks deviate from real-world task distributions, leading to dis-\ncrepancy in evaluation results and real-world performance; grading bias arises from unfair scoring\nparadigms, and generalization bias stems from evaluation contamination. These biases skew eval-\nation signals, hindering model development. To address these issues, MixEval (Ni et al., 2024)\nproposes a low-bias paradigm for LLM evaluations (Text2Text). MixEval aligns benchmarks with\nreal-world task distributions by matching web-mined queries to similar benchmark tasks. Its ben-\nefits include: (1) a comprehensive, less biased task distribution based on a large-scale web corpus,\n(2) fair grading due to the ground-truth-based paradigm, (3) dynamic benchmarking via a low-effort\nupdate pipeline, mitigating generalization bias, (4) accurate model ranking with a 0.96 correlation to\nChatbot Arena, (5) fast, cost-effective, and reproducible execution, requiring only 6% of MMLU's\ntime and cost, and (6) challenging problems with significant room for improvement.\nTo this end, to optimize and standardize evaluations across AI communities, we propose\nMixEval-X, the first any-to-any real-world benchmark optimizing benchmark mixtures for a wide\nrange of input-output modalities. MixEval-X consists of eight subsets, each with distinct input-\noutput modality combinations, categorized into three types: multi-modal understanding (MMU),\nmulti-modal generation (MMG), and agent tasks (Figure 1). These modalities are not only the\ndominant ones in web queries but also central to various communities.\nSpecifically, we first use MixEval's web user query detection pipeline to gather a well-distributed\nset of real-world queries spanning diverse input-output modalities. For MMU tasks, we construct\nlarge-scale multi-modal benchmark pools from existing community benchmarks, prioritizing query-\nbased ones like question-answering and examination tasks due to: (1) the query-based nature of\nreal-world tasks, and (2) the convergence of AI tasks toward natural language queries for multi-\ntask learning (Wei et al., 2021). We then match web queries to similar query-based tasks from\nthe benchmark pool to reconstruct the benchmark distribution. This is followed by an automatic\nquality control step to eliminate the wrong or extreme samples. Additionally, we perform rejection\nsampling to select more challenging MMU tasks while preserving real-world distribution alignment.\nFor MMG tasks, which are open-ended, we implement an adaptation-rectification pipeline where the\nadaptation step creates real-world tasks from web queries using frontier models, ensuring alignment\nwith real-world task distributions, and the rectification step automatically fixes errors and distribu-\ntion deviations. For agent tasks, lacking general-domain benchmarks, we use a similar adaptation-\nrectification pipeline to recreate task distributions and annotate reference answers. Optional human\ninspection was adopted to increase the annotation quality. The efficient evaluation creation pipelines\nfor MMU, MMG, and agent tasks allow periodic data refreshes to mitigate contamination. Our meta-\nevaluations show that: (1) MixEval-X data closely aligns with real-world task distributions, and\n(2) MixEval-X's evaluation results strongly correlate with real-world user-facing evaluations (up\nto 0.98), while being significantly more efficient.\nWhy use MixEval-X? (1) It extends all the benefits of MixEval to multi-modal evaluations, in-\ncluding comprehensive and less biased query distribution; fair grading (except open-ended tasks);\ndynamism; accurate model ranking; fast, cost-effective, reproducible execution; and challenging\nnature. (2) It establishes unified, high standards across modalities and communities. For single-"}, {"title": "2 MIXEVAL-X", "content": "In this section, we introduce the methods used to construct the various subsets of MixEval-X and\ntheir respective grading mechanisms. As shown in Figure 2, MMU tasks are built with benchmark\nmixture, while MMG and agent tasks are created with an adaptation-rectification pipeline, both de-\nsigned to align evaluation tasks with real-world distributions. The grading for MMU tasks is robust\ndue to their ground-truth-based nature. All sub-benchmarks are dynamic, enabled by the efficient\ndata creation pipelines. Moreover, we carefully refine annotation accuracy and task difficulty to\nensure the quality and usage potential of MixEval-X.\n2.1 WEB QUERY DETECTION\nWe create the same web query detection pipeline as MixEval to detect real-world user queries from\nCommon Crawl (Computer, 2023). Both recall and precision are crucial to ensure the query distri-\nbution reflects real-world scenarios. Therefore, we developed two benchmarks to evaluate our query\ndetector's performance. The first benchmark includes self-collected in-the-wild user queries as pos-\nitive samples, with non-query datasets such as Wikipedia (Foundation, 2022) as negative samples.\nThe second, higher-quality benchmark contains positive and negative samples hand-picked by our\nauthors from in-the-wild query and non-query datasets. In preliminary experiments, direct prompt-\ning of open-source language models performed poorly on our benchmarks. Thus, we devised a\nloop to ensure high recall and precision cost-effectively. We started with a detection phase to gather\ntraining data. Testing various open-source LLMs, Vicuna 33B (Chiang et al., 2023) achieved a high\nrecall (>99%) on our test sets with careful prompt engineering, ensuring that very few positive sam-\nples were missed initially. In this phase, we detected around 20k queries using Vicuna 33B over a\nsubset of Common Crawl. We then used GPT-4 to more accurately label these data as positive or\nnegative samples, and used the resulting data to train Vicuna 33B. The trained Vicuna 33B achieved\nhigh recall (>99%) and precision (>98%) on our benchmarks and detected 2M user queries from\nthe Common Crawl subset. Finally, we prompted GPT-4 Turbo to further filter and classify them\ninto various modalities, extracting well-distributed multi-modal queries for MixEval-X creation."}, {"title": "2.2 MMU TASK CREATION", "content": "Benchmark Mixture We perform benchmark mixture to mitigate query bias in MMU tasks. As\nillustrated in MixEval (Ni et al., 2024) and further in Figure 9, current benchmark query distri-\nbutions deviate from real-world use, limiting the generalizability of evaluation outcomes. Using\nthe MixEval web query detection pipeline, which trained precise query classifiers to extract well-\ndistributed queries from the web, we crawl multi-modal user queries from web and map this web\nquery distribution onto the constructed benchmark pool containing numerous ground-truth-based\nbenchmarks. We sample problem-answer pairs from this benchmark pool by selecting the most sim-\nilar one given a web query, constituting a new benchmark with natural ground-truths. The matching\nprocess is based on the similarities between the sentence embeddings (Reimers & Gurevych, 2019)\nof benchmark text queries and web queries. As such benchmarks exist only for MMU tasks, we\napply benchmark mixture exclusively to these modalities. Due to varying community maturity,\nbenchmark pools for certain modalities, such as Audio2Text, are less extensive compared to more\nestablished ones like Text2Text and Image2Text. However, a key advantage of MixEval-X is its\ncapacity for self-refinement, enabling the benchmark pool to grow as the community develops. The\nbenchmark pool composition is detailed in Section F.\nChallenge Set Sampling and Dynamism The rapid advancement of frontier and open-source com-\nmunities introduces two key challenges in model evaluation: saturation, where further score im-\nprovements are limited, and contamination, where models overfit to the test data. To enhance model\ndifferentiation, we applied rejection sampling (Ni et al., 2024) to select more challenging MMU\ntasks while preserving real-world distribution alignment. The effectiveness of this strategy is demon-\nstrated later by the low scores on the hard split in Section 3.2, and the close distance between hard\nsplit queries to web queries in Figure 9. Since MixEval-X's benchmark mixture pipeline is fully\nautomated and updatable within minutes, refreshing data points is efficient and helps mitigate testset\noverfitting. Additionally, the benchmark pool also integrates newly released benchmarks to mitigate\ncontamination. However, this dynamism alleviates but does not fully resolve contamination. Further\ndiscussions on benchmark mixture contamination can be found in Ni (2024).\nQuality Control An inspection step is used to enhance the benchmark quality. We focus on the\nentries where frontier models erred most, excluding those that most models gave the same answer\ndifferent from the ground-truth. Cases with extreme inputs were removed to streamline evaluation."}, {"title": "2.3 MMG AND AGENT TASK CREATION", "content": "Adaptation-Rectification Pipeline For MMG and agent tasks lacking natural general-domain\nbenchmarks, alternative methods are needed to recreate real-world task distributions. MMG tasks\nare simpler to construct, being open-ended with no reference answers. Since web user queries are not\nclean and challenging enough for MMG models, we developed an adaptation-rectification pipeline\nthat transforms them into well-formatted, challenging tasks. In adaptation, a language model mod-\nifies the query to match the required complexity and format while maintaining user intent. In rec-\ntification, the model inspects and corrects the task's logic, complexity, correctness, and alignment\nwith the original task. The resulting MMG tasks have two turns-a generation turn that instructs the\nmodel to generate content and an edition turn that instructs the model to edit the generated content\nin the last turn.\nConstructing agent tasks is more demanding, requiring careful annotation of reference answers. The\ntask design follows the MMG approach, using the adaptation-rectification pipeline. To annotate\nanswers for Text2Action and Image2Action tasks, frontier LLMs and VLMs provide initial annota-\ntions, refined through automated rectifications. Both MMG and agent tasks have an optional human\nreview step. These sub-benchmarks are also dynamic due to the efficient data update pipeline. De-\ntailed pipeline prompts are shown in Section D."}, {"title": "2.4 GRADING", "content": "Grading bias undermines evaluation accuracy, even for ground-truth tasks with narrow answer\nspaces. As noted in Ni (2024), rule-based parsers are unstable when grading across multiple models\nand ground-truth-based benchmarks, while language model parsers provide a more reliable alterna-\ntive. We use model-based parsers to grade tasks with narrower answer spaces, such as MMU and"}, {"title": "3 EVALUATION", "content": "3.1 EVALUATION SETTINGS\nIn this section, we provide comprehensive evaluation results to offer more precise rankings for mod-\nels and organizations in the field. We follow official settings for all open-source models to ensure\nfairness. For proprietary models, we use their official APIs. To avoid task-specific biases, we stan-\ndardize the benchmark input formats, including prompts. Models supporting interleaving receive\ninterleaved entries as input. Since current MMG models only accept caption-like prompts, we use a\ncaption rewriter (GPT-4) to convert user instructions into caption-like inputs for MMG tasks.\n3.2 MMU TASKS\nImage2Text In Image2Text tasks, models generate language responses based on user-provided im-\nages and text. We evaluate a broad range of Image2Text models due to its established community.\nFigure 3 presents the leaderboard. Proprietary models like Claude, GPT, Gemini, and Reka series\noutperform open-source models, with Qwen and Llama leading the latter. Our analysis shows that\ninput resolution limits and model size are key factors in rankings, while model architecture, training\nmethods, and input formatting also influence performance. Most models use an encoder-decoder\narchitecture, as decoder-only models remain underexplored in vision-language tasks."}, {"title": "3.3 MMG TASKS", "content": "The results for MMG tasks are shown in Figure 6. We employed hundreds of human evaluators from\nAmazon Mechanical Turk to assess model outputs using a pairwise-ranking approach, as automatic\nmetrics fail to capture the nuances in output quality (Jiang et al., 2024). We report only the overall\nscores, while MMG tasks consist of two turns-a generation turn and an edition turn. Turn-level\nscores are presented in Figure 46. See Section G for model details.\nText2Image In Text2Image tasks, models generate images based on human prompts. Like Im-\nage2Text, Text2Image has a well-developed community, with Flux (BlackForestLabs, 2024) achiev-\ning the highest Elo score among the evaluated models, as shown in Figure 6(a). Our analysis reveals\nthat image quality, particularly realism, significantly impacts human pairwise evaluations. Although\nDALL-E 3 HD (Betker et al., 2023) shows high quality, it ranks lower in realism. A case study is\nshown in Figure C. Instruction-following ability also strongly influences human evaluations.\nText2Video In Text2Video tasks, models generate videos based on textual prompts. Figure 6(b)\npresents the Elo rankings for various Text2Video models. Human evaluators tend to prefer videos\nwith higher quality, realism, smoothness, and adherence to the prompt. Most models struggle with"}, {"title": "3.4 AGENT TASKS", "content": "Text2Action Text2Action tasks\ninvolve models planning API-\nlevel actions based on textual\ninputs describing the environ-\nment and a user prompt. This\nsetup simplifies real-world agent\ntasks to evaluate the action-\nplanning capabilities of LLMs,\noffering more flexibility for op-\ntimizing task distribution. Fig-\nure 7 presents the results for the\nText2Action subset. The model\nrankings differ from Text2Text\ntasks (Ni et al., 2024), suggesting that strong text understanding does not guarantee proficiency\nin textual agent tasks.\nImage2Action In Image2Action\ntasks, models with both image and\ntext input capabilities plan API-level\nactions based on the observed envi-\nronment (presented as an image) and\nthe user prompt. Figure 8 presents\nthe evaluation results for the Im-\nage2Action subset of MixEval-X.\nThe rankings of vision-language\nmodels (VLMs) differ significantly\nfrom those in Image2Text tasks.\nNotably, some open-source models,\nnot aligned with RLHF or similar\ntechniques, often produce shorter or\nrepeated action sequences, leading to\nlower scores."}, {"title": "4 \u039c\u0395\u03a4\u0391 EVALUATION", "content": "4.1 DISTRIBUTION ANALYSIS\nSetup We aim to analyze the task distributions of MixEval-X. While many benchmarks and\ndatasets are documented, we focus on their actual distributions in practice and their comparison\nto real-world tasks. In Figure 9, we randomly sample 1000 task queries from each dataset, reduce\ntheir sentence embeddings to 2D using t-SNE, and visualize the distributions. Dimensionality re-\nduction is performed in the same space for benchmarks with the same modality, using identical color\nschemes to facilitate direct comparison, i.e., datasets of the same modality are comparable in terms\nof their distributions. To further examine topic distributions, we segmented the aggregated queries\nof each modality (e.g., Image2Text) into 16 spatial patches in the 2D space. From each patch, we"}, {"title": "4.2 CORRELATION ANALYSIS", "content": "MixEval-X demonstrates a strong correlation with real-world user-facing evaluations. A key\nfeature of MixEval-X is its alignment of benchmark task distributions with real-world tasks.\nBeyond distribution analysis, we as-\nsess this alignment by evaluating\nthe correlation between MixEval-X\nresults and real-world evaluations.\nWhile many communities lack stable\nreal-world evaluation leaderboards\nlike Chatbot Arena (Chiang et al.,\n2024), the Image2Text community\nhas two comparatively stable user-\nfacing leaderboards: Vision Arena\n(Lu et al., 2024b) and Arena (Vi-\nsion) (Chiang et al., 2024). Our\nImage2Text results show a strong\nSpearman's model ranking correla-\ntion with these, with 98.1% corre-\nlation to Vision Arena and 96.3%\nto Arena (Vision); Image2Text-Hard\nshows 94.5% and 95%, respectively.\nThese high correlations, along with\nfindings in Ni et al. (2024), highlight\nthe effectiveness of our benchmark\nmixture approach. Although corre-\nlations for other modalities can't be\nverified at present due to the lack of"}, {"title": "5 CONCLUSION", "content": "MixEval-X represents a major advancement in AI evaluation, unifying evaluation standards across\nvarious input and output modalities, delivering the first any-to-any real-world benchmark that is\nlow-bias, efficient, and dynamic. The proposed methodologies that build MixEval-X effectively\nextend the benefits of MixEval to multi-modal evaluations, providing a highly reliable framework for\nevaluations of both single-modality and multi-modality models. Extensive meta-evaluations confirm\nthat MixEval-X data points are closely aligned with real-world use cases and its results strongly\ncorrelate with large-scale user-facing evaluations. This work offers various communities a robust\nproxy for optimizing models effectively and provides insights to inform future research directions."}, {"title": "A FREQUENTLY ASKED QUESTIONS", "content": "A.1 WHY ARE THERE ONLY EIGHT INPUT AND OUTPUT MODALITY COMBINATIONS\nCOVERED?\nThe eight input and output modalities represent the most common cases identified in our web query\nanalysis and are also widely regarded as central by the AI community. Expanding the scope to\ninclude more modalities could dilute the overall quality of the work. Hence, we have chosen to\nfocus on the key modalities for now, leaving combinations like Image2Video for future exploration."}, {"title": "B RELATED WORK", "content": "LLM Evaluations The LLM community is the most advanced among different AI com-\nmunities. In practical LLM evaluations, three primary biases compromise impartiality: (1)\nquery bias-evaluation queries that lack comprehensiveness or proper distribution, (2) grading\nbias-significant bias or error in the grading process, and (3) generalization bias-model overfitting\nto the evaluation data. Current benchmarking approaches are either automatic or user-facing. Au-\ntomatic benchmarks often use traditional, ground-truth-based frameworks like MMLU (Hendrycks\net al., 2020), which fail to capture the complexity and nuance of real-world queries, though they of-\nfer a relatively unbiased grading process. Alternatively, open-ended benchmarks that employ LLMs\nas graders, such as MT-Bench (Zheng et al., 2023), face issues of grading bias and query incom-\npleteness due to preference biases and the high cost of cutting-edge LLM judges. Furthermore, the\nstatic nature of automatic benchmarks introduces contamination over time, exacerbating general-\nization bias. These biases lead to significant deviations from gold-standard evaluations, hindering\nmodel development. In contrast, large-scale user-facing benchmarks like Chatbot Arena (Chiang\net al., 2024) provide more reliable metrics for model development and address the three biases more\neffectively. (1) They capture a diverse array of real-world queries, ensuring better query comprehen-\nsiveness and distribution. (2) Their evaluation of varied model responses benefits from the \"wisdom\nof the crowd\" effect (Yi et al., 2012), where individual judgment noise is averaged across numer-\nous samples, reducing grading bias. (3) Continuous influx of user queries minimizes benchmark\ncontamination. Moreover, this approach steers model optimization towards practical applications,\naligning models more closely with user needs. However, Chatbot Arena is costly, slow, and irre-"}, {"title": "D ADAPTATION-RECTIFICATION PROMPTS", "content": "Text2Image Adaptation-Rectification Prompt - Adaptation 1\nSystem: In this task, I want you to act as an instruction rewritter, and imagine that you are rewritting\ninstructions for image generation.\nMain: The rewritten instruction will be used to instruct an image generation model that generates\nimages. You will be provided with a noisy raw user instruction that decides the topic or content of the\nimage generation task and you need to rewrite the raw instruction to make it clearer and more specific.\nMeanwhile, it should be practical for the image generation model to generate the corresponding image.\nThe below examples are simplified, while your rewritten instructions could be either more detailed or\nmore concise.\nRaw User Instruction:\nThe rewritten instruction:\nText2Image Adaptation-Rectification Prompt - Adaptation 2\nSystem: In this task, I want you to act as an image edit task designer, and imagine that you are\ninstructing an image edit model to edit images.\nMain: You will be provided with the image caption of an image to edit and you need to randomly pick\nsome editing aspects to formulate an editing instruction. Besides that, you should provide the image"}]}