{"title": "Hand-Object Interaction Pretraining from Videos", "authors": ["Himanshu Gaurav Singh", "Antonio Loquercio", "Carmelo Sferrazza", "Jane Wu", "Haozhi Qi", "Pieter Abbeel", "Jitendra Malik"], "abstract": "We present an approach to learn general robot manipulation priors from 3D hand-object interaction trajectories. We build a framework to use in-the-wild videos to generate sensorimotor robot trajectories. We do so by lifting both the human hand and the manipulated object in a shared 3D space and retargeting human motions to robot actions. Generative modeling on this data gives us a task-agnostic base policy. This policy captures a general yet flexible manipulation prior. We empirically demonstrate that finetuning this policy, with both reinforcement learning (RL) and behavior cloning (BC), enables sample-efficient adaptation to downstream tasks and simultaneously improves robustness and generalizability compared to prior approaches. Qualitative experiments are available at: https://hgaurav2k.github.io/hop/.", "sections": [{"title": "1 Introduction", "content": "Reusable sensorimotor representations have the potential to give robots access to the versatility of their sensorimotor apparatus, thereby enabling them to achieve a wide variety of goals. Similar to advancements in other AI domains [1, 2], such representations are likely to be trained with unsupervised objectives on large datasets. In this work, we study the feasibility of training such representations using human videos in the context of dexterous manipulation.\nUsing videos as a data engine comes with several advantages: (1) they are abundant; (2) they cover a wide range of skills that we want robots to master; and (3) they reflect natural or socially acceptable behaviors that we want robots to emulate. However, training sensorimotor representations on videos is a challenging endeavor. First, videos only partially capture the nature of an agent's interaction with their surroundings. For instance, by looking at a person holding an object, it is almost impossible to estimate the force their fingers are exerting. In addition, the larger the embodiment gap between a human and a robot, the more their actions will differ to achieve the same objectives.\nThe difficulty of learning from videos led previous work to mostly focus on specific aspects of the problem. One line of research focused on training visual representations with off-the-shelf self-supervised vision algorithms on large vision datasets [3, 4, 5, 6, 7]. While simple and effective, such pretrained representations lack a motor component, making them less effective on downstream tasks [8]. Another line of work aims to extract both sensory and motor information from videos by estimating human motions in 3D [9, 10, 11, 12, 13]. However, these approaches require alignment between the training videos and the robot's downstream tasks, which compromises the generality of the learned representations. Finally, recent works aim to use egocentric videos of human activities to learn an explicit hand-object interaction prior in the form of a contact-pose prediction model [14, 15]. While a contact-pose prior is potentially task-agnostic, useful information in hand-object trajectories extends beyond contact-poses, including but not limited to pre/post-contact trajectories, intuitive physics of the interaction and human preferences."}, {"title": "2 Overview", "content": "The objective of Hand-Object interaction Pretraing (HOP) is to capture general hand-object interaction priors from videos. In contrast to previous work, we do not assume a strict alignment of the human's intent in the video and the downstream robot tasks. Our key intuition is that the basic skills required for manipulation lie on a manifold whose axes are well covered by unstructured human-object interactions.\nWe extract sensorimotor information from videos by lifting the human hand and the manipulated object in a shared 3D space. We then bring such 3D representations to a physics simulator, where we map human motion to robot actions. There are several advantages to using a simulator as an intermediary between videos and robot sensorimotor trajectories: (i) we can add physics, inevitably lost in videos, back to the interactions; (ii) it enables the synthesis of large training datasets without putting the physical platform in danger; and (iii) we can add diversity to the data by randomizing the simulation environment, e.g., varying the friction between the robot's joints, the scene's layout, and the object's location relative to the robot."}, {"title": "3 Method", "content": "3.1 Lifting Hand-Object Interaction Videos to 3D\nRecovering the underlying 3D structure of hand-object interactions from in-the-wild monocular videos is inherently ambiguous. To alleviate such ambiguity, previous work leveraged the insight that the human hand can be used as an anchor for the 3D location and scale of the manipulated object [16, 17, 18, 19, 20, 21]. Our setup to estimate hand-object interaction trajectories from videos builds upon recent advances in 3D vision. Our approach closely follows MCC-HO [16] with a few modifications to adapt it to our use case.\nGiven a single RGB image and an estimate of the 3D hand geometry from HaMeR [22], MCC-HO jointly infers hand-object geometry as point clouds. To fine-tune the quality of the prediction, MCC-HO finetunes the object's pose by fitting it to a CAD model. However, this finetuning assumes knowledge of the object the human is interacting with. To increase generality, we wave this assumption and skip the CAD-based post-processing. This simplification comes at the cost of reduced reconstruction quality and temporal smoothness. While we find the first problem not critical for pre-training, we increase temporal smoothness by anchoring object reconstructions to time-smoothed hand detections [22]. In addition, we make the simplifying assumption that the camera from which the video is collected is static. More details of our 3D estimation pipeline are provided in the appendix. The result of this pipeline is a sequence of 3D hand-object poses.\n3.2 Mapping 3D Human-Object Interactions to Robot-Object Interactions\nWe formulate a non-linear optimization problem to generate a sensorimotor trajectory \u03c4 from a sequence of 3D hand-object poses. At each step k, we find the action a[k] by optimizing the following cost function:\n$\\displaystyle \\min _{a[k]} \\frac{1}{2}||x_{h}[k] - f(a[k])||^{2} + \\frac{1}{2}||a[k] - a[k - 1]||^{2} \\text{ s.t. } a[k] \\in A,$"}, {"title": "3.3 Robot Trajectory Pretraining", "content": "The resulting trajectory dataset T contains knowledge that could be valuable to any manipulation tasks. For instance, T has information about object affordance, i.e., where and how to grasp; some intuitive (although rudimentary) physics, e.g., an object should be reached upon before being lifted; or wrist-hand coordination, i.e., the behavior of orienting and shaping the hand simultaneously while moving the wrist to maximize efficiency [28].\nWe aim to incorporate this knowledge as useful behavioral priors into a policy \u03c0\u266d that can be finetuned to downstream tasks. Similar to previous work in language [2], vision [29], and robotics [8, 30, 31], we instantiate \u03c0\u266d as a transformer [32] and train it on a generative modeling objective. Specifically, we train \u03c0\u266d to capture the conditional distribution \u03a0(a[t \u2013 L : t]|o[t - L:t]) by optimizing the following loss:\n$L(T; \\theta) = E_{t \\sim [1...T]} [||a[t - L : t] - \\pi_{\\theta}(o[t - L : t])||_{1}].$"}, {"title": "Downstream Finetuning", "content": "The pretrained policy \u03c0\u266d exhibits primitive manipulation skills, e.g., reaching an object with a reasonable grasp pose, while occasionally grasping successfully. We finetune these skills to a task by optimizing a reward with reinforcement learning or a behavior cloning loss on limited demonstrations. We finetune the whole model for the task. Empirically, we find that finetuned policies use the information in \u03c0\u266d to train faster, are more robust to disturbances, and generalize better than policies trained from scratch and a set of baselines. In addition, we find that the finetuning process re-utilizes the information in \u03c0\u266d even for tasks not explicitly represented in the training videos."}, {"title": "4 Experimental Setup", "content": "Robot. We use a low-cost 7-DoF xArm robot with a 16-DoF Allegro hand [33] vertically mounted at its end effector. The proprioception observation \u00d8k includes joint position from both robots. While we don't make any specific assumption about the robot embodiment, we use a multi-fingered hand instead of a parallel joint gripper since demonstration quality increases as the embodiment gap between the robot and the human decreases. We empirically found that since the robot base is fixed, a 7-DoF arm can track much better human motions than a 6-DoF arm, which often encounters singularities during such trajectories. Visual sensing comes from a single stereo camera (Zed-2) mounted on the robot's right side.\nSimulation Setup. Our simulation environment is developed with the IsaacGym [34] simulator. The robot morphology and action space are identical to the real setup. However, since rendering depth images is prohibitively expensive, we give the agent access to the ground-truth object point-could instead of a depth image (see Section 2). Specific details about the task setup and reward design can be found in the appendix.\nVideo Datasets. Our pretraining dataset of 3D hand-object trajectories consists of sequences from two datasets: DexYCB [35] and 100 Days of Hands [36]. We use 250 videos from the DeXYCB dataset (right-hand only) annotated with ground truth hand-object trajectories as a source of high-quality data. We additionally use approximately 200 videos from the 100 Days of Hands dataset. Sixty percent of these videos were previously annotated with hand-object interaction trajectories [11], which we directly use. We annotate the remaining videos with our 3D estimation pipeline (Sec. 3.1). Overall, our combined dataset contains approximately 450 videos. We retarget these videos to obtain a pretraining dataset T of approximately 70, 000 trajectories.\nRetargeting. We use low-storage BFGS [37] from the NLOpt library [38] for optimization. We perform simulation-in-the-loop retargeting in a simple simulated scene with a ground floor on which the robot and a static table are placed 65cm apart. Objects start their trajectories above the table with a random pose. We run the optimization 700 times for each video, randomizing the table location and the robot's initial joint state. We add a trajectory to T only if, at any time, their retargeting error (See Eq. (1)) is below 3cm and the arm does not collide with the table or the floor. Our code is built upon the implementation of Qin et al. [39].\nTransformer. Similar to previous work [40], we represent the policy \u03c0\u266d with a GPT-2-style causal transformer. The policy takes proprioception and observation input from the past 16 timesteps and predicts the next action. Details about the architecture can be found in the appendix.\nPretraining. We train the transformer with the objective in Eq. (2) on T. While we could make the prediction autoregressive and add decoding heads and proxy losses for future proprioception and images (as in [13]), we empirically found these changes to be not very helpful in practice to our tasks. Therefore, we predict only future actions for simplicity. We use as optimizer AdamW [41] with initial learning rate of 10-4 and weight decay of 10-2. We trained two distinct base policies-one with depth observations and the other with point cloud observations. The former is used for real-world, and the latter for simulation experiments. However, it's important to note that both policies were trained on exactly the same trajectories; only the associated sensor observations differed."}, {"title": "5 Experimental Results", "content": "We design an experimental procedure to analyze the advantages brought forward by HOP in terms of finetuning efficiency, generalization, and robustness to perturbations. Specifically, we ask the following questions: (i) How does HOP compare to vision-only pre-training approaches for robot learning? (ii) How does HOP compare to existing demonstration-guided reinforcement learning algorithms [45, 46]? (iii) How does learning from hand-object interaction trajectories compare to learning hand-pose priors only [47, 48, 9]?. We answer these questions via controlled experiments in simulation and the physical world.\n5.1 Comparison to visual pre-training baselines (real-world).\nBaselines. We compare our approach to visual pre-training systems. Such systems are trained on large image or video datasets but lack a motor component. Specifically, we compare to methods using the following pre-training data:\n\u2022 ImageNet We encode the depth image with a VIT-B network [49] pre-trained on ImageNet and pass the resulting CLS token embeddings to our transformer. The latter is then trained with real-world data. We consider two variants: using the VIT features zero-shot (Imagenet ZS) and finetuning them on the downstream dataset (ImageNet F).\n\u2022 Internet Videos We use off-the-shelf visual features from R3M [7], VIP [3], and MVP [8]. These features were obtained with unsupervised contrastive learning objectives on large video datasets, e.g. Ego4D [50]. Conversely to ours, these baselines don't use depth but RGB images as input.\nWe additionally compare to Diffusion Policies [23] using a UNet backbone since our tasks exhibit temporally smooth desired action sequences. Similarly to ours, this baseline uses depth as input. With the above baselines, we want to understand how classic methods for behavior cloning work in our setting, where a single camera and a limited number of demonstrations are available. Indeed, the previous approaches are generally applied with a large number of demonstrations and multiple RGB cameras.\nTasks We evaluate our approach in the real world on three tasks of increasing complexity. In the first task, Grasp and Drop, the robot needs to unstack a cube and put it in a bowl. The second is the Grasp and Pour task, where the robot needs to pick a bottle and point it towards a bowl. In the third task, Grasp and Lift, the robot must pick up one of 4 different-looking objects, all requiring different spatial affordances. One single model is trained to pick up all objects. In this task, we evaluate the ability of the approach to adapt with a few demonstrations on very different object"}, {"title": "5.2 Comparison to demonstration-guided reinforcement learning strategies (simulation)", "content": "Our simulation experiments investigate the effectiveness of HOP as a base model for adaptation to downstream tasks using RL. The simulation agent is identical in morphology to the real robot.\nBaselines. We compare our approach to three baselines: (1) training from scratch (PPO); (2) demonstration-guided reinforcement learning with a proxy imitation objective [45] (DAPG); and (3) using adversarial objectives to keep the policy close to the demonstrations [46] (AMP). DAPG is the closest to our work, as it trains on a weighted sum of behavioral cloning and reinforcement learning losses. However, it assumes access to expert demonstrations in the downstream task. Our pre-training dataset does not fulfill this assumption. Indeed, humans might not behave optimally according to the reward, or the task might not be well represented in the pre-training dataset. Similarly to previous work [51], we found that training from scratch is unsuccessful using joint-position control as action space, consistently leading the PPO baseline to fail. Therefore, we use the moving-average action space proposed by Petrenko et al. [43] to improve its performance. All baselines use the same environment settings and training strategy, e.g., domain randomization parameters, as our approach.\nTasks and Metrics. We evaluate approaches on three tasks. The first requires picking objects and placing them at a specific location (Grasp and Lift). The second is to grasp objects and throw them in a basket (Grasp and Throw). In the final task, the robot is required to open a cabinet. We evaluate performance using success over 256 environments with different objects and report the mean and standard deviation over three seeds per approach. More details can be found in the appendix.\nHOP enables sample-efficient RL and effective exploration In Figure 3, it is demonstrated that our approach outperforms all baselines by a large margin, especially when the pretraining corpus is not closely related to the task. This is expected because DAPG strongly biases exploration in the neighborhood of the pre-training trajectories, which may potentially be misaligned with the downstream task. Furthermore, we observed that the adversarial training scheme of AMP is unstable and does not scale well with the amount of data. Finally, we find that using HOP leads to a 2-5X improvement in sample efficiency compared to training from scratch. Overall, these experiments"}, {"title": "HOP learns robust and general behaviors", "content": "Policies fine-tuned from HOP can potentially bias exploration toward human-like behavior, leading to more robustness against forces. This is shown in Fig. 4. Agents trained with our approach perform better when subject to forces than the ones trained from scratch. In addition, we show in Fig. 4 that our approach generalizes 3x better than the policy trained from scratch. The training objects are different from the testing ones in their mass, aspect ratio, and relative size with respect to the hand. The performance generally drops whenever the test object is heavy (power drill), too large (cracker box), or too small (marker and scissors) for the allegro hand, which is approximately 1.5X larger than a human hand. Note that in these experiments we train the scratch policy with two billion samples.\nAffordances Training an RL policy from scratch for a dexterous hand often leads to grasping poses that are unlike general human affordances. Online RL exploration near a learned human-object interaction prior biases the optimization landscape to favor human-like affordances. As shown in Figure 5, we find that for the Grasp and Lift task, our policy grasps objects with more stable and human-like affordances than PPO training from scratch."}, {"title": "5.3 Comparison to learning a hand-only motion prior (simulation)", "content": "Prior work has shown the benefits of learning a prior on hand motions from videos of human activities [47, 48, 9]. This section aims to understand the benefits of learning a prior on the object and the hand jointly. We hypothesize that learning from hand-object interactions gives the base model useful information beyond eigen-grasps (which are captured by a hand-only motion prior), like, for instance, pre- and post-contact trajectories, intuitive physics of the interaction, and human preferences.\nWe evaluate this hypothesis by training a base policy on our pre-training corpus using masked object observations. This encourages the base policy to primarily learn a hand motion prior. As illustrated in Figure 6 (left), we observe that such a pre-trained policy exhibits reduced robustness to grasp disturbances. Furthermore, we find that the hand-only prior is insufficient for learning an effective policy in the Grasp and Throw task (Fig. 6, right). Since this task is underrepresented in the pre-training corpus, the hand motions required are unlikely to be adequately captured by a hand-only prior. In contrast, learning a joint hand-object prior provides the model with a more comprehensive understanding of manipulation, enabling quicker adaptation to this downstream task."}, {"title": "6 Related Work", "content": "Learning Policies from Human Videos. In-the-wild videos hold the promise of solving the data problem in robotics. One of the pioneering efforts in this direction is by Yang et al. [52], where video data was used to generate action plans. Several works followed up on this idea, relying on pre-defined"}, {"title": "7 Conclusion and Limitations", "content": "This work presents an approach to learning general yet flexible manipulation priors for robot policies from human videos. While our approach demonstrates a way to pre-train on a single object interaction, this can, in practice, be limiting. Indeed, human behavior in a video can potentially be conditioned on information encompassing multiple objects in the current and previous scenes. This leads to a loss of signal that could be extracted from the raw video. We predict that advances in 3-D reconstruction will enable us to use a more complex scene reconstruction and pretraining."}, {"title": "8 Supplementary Material", "content": "8.1 3D Hand-Object Interaction from Videos\nOur setup closely follows the pre-trained MCC-HO model [16] to lift RGB videos to 3D. However, this approach was designed to work on a single frame, while we are interested in extracting trajectories from videos. Specifically, MCC-HO input is the image patch containing the hand and manipulated object. Acquiring these patches requires identifying which hand and object is part of the manipulation sequence from in-the-wild videos. To make the problem tractable, we use the fact that the 100 Days of Hands dataset (100DOH) [36] includes 100K labeled frames (e.g., hand and object bounding boxes) randomly sampled from Internet videos.\nWe use such sparse labels as follows: Given a video with a labeled frame at timestamp to, we download a 10-second video clip centered at to using the original video frame rate. Then, we propagate the combined hand-object bounding box from to to t < to and t > to iteratively until the interacting hand is no longer detected by HaMeR [22]. At each frame t, the labeled hand-object bounding box is translated so the hand bounding-box center is aligned with the HaMeR hand bounding-box center. Subsequently, the images are cropped/resized using these hand-object bounding boxes and passed to MCC-HO for network inference. This gives a sequence of 3D hand-object poses. We do not temporally smooth the sequences further (e.g., via high-pass filtering) since this happens (to a certain extent) as a by-product of the robot's inertia during the simulator-in-the-loop motion re-targeting.\n8.2 Simulation Experiments Setup\nWe reuse environment definitions from previous works ( [34, 89]) with minimal changes. We do not make any changes to the structure of the reward function. Below, we provide a brief description for each task:\n8.3 Model Architecture\nOur policy is a GPT-2 style transformer with causal attention. The transformer has 4 heads, 4 layers, and a hidden dimension of 192. For real-world experiments, proprioception and the depth images are embedded to the hidden size using a linear projection and a 4-layer CNN, respectively. In simulation, point clouds are embedded to the hidden size using a pointnet with 2 hidden layers of size 64. The input to the transformer are proprioception and depth/pointcloud embeddings for previous 16 timesteps. Additive learnable positional embeddings are used for both the proprioception and depth embeddings.\n8.4 Real-World Experiments Setup\nData Collection We build a custom teleoperation setup for data collection by combining two ex-isting systems. We control the xArm with a Gello [91] and the hand motion with VR using Open-Teach [92]. We experimented with controlling the whole system via VR but found obtaining precise and smooth motion challenging. While our solution could achieve such motions, it comes with the disadvantage of requiring two people to collect demonstrations. We randomize the initial pose of the robot between demonstrations by adding a uniform noise of magnitude 0.3 rad to all joints from a fixed starting location.\nInference While sensors operate at different frequencies, we get the latest available measurements from each sensor at constant intervals to achieve a whole inference loop of 20Hz. The predicted action, i.e.absolute joint positions, are given to a low-level P controller that operates at 120Hz. Such controller directly sends commands to the xArm API to convert joint position into joint torques."}]}