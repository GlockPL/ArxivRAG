{"title": "A jury evaluation theorem", "authors": ["Andr\u00e9s Corrada-Emmanuel"], "abstract": "Majority voting (MV) is the prototypical \u201cwisdom of the crowd\u201d algorithm. Theorems considering when MV is optimal for group decisions date back to Condorcet's 1785 jury decision theorem. The same assumption of error independence used by Condorcet is used here to prove a jury evaluation theorem that does purely algebraic evaluation (AE). Three or more binary jurors are enough to obtain the only two possible statistics of their correctness on a joint test they took. AE is shown to be superior to MV since it allows one to choose the minority vote depending on how the jurors agree or disagree. In addition, AE is self-alarming about the failure of the error-independence assumption. Experiments labeling demographic datasets from the American Community Survey are carried out to compare MV and AE on nearly error-independent ensembles. In general, using algebraic evaluation leads to better classifier evaluations and group labeling decisions.", "sections": [{"title": "Introduction", "content": "The notion that crowds could make wiser choices than individual members can be traced to Aristotle's Politics. In 1785, Condorcet proved the first jury theorem describing conditions that make an ensemble better at making decisions. Jurors individually better than 50% in their verdicts make less errors by using Majority Voting (MV). Collective decision errors are minimized when the jurors are error-independent in their decisions. This report discusses a jury evaluation theorem that uses error independence of binary classifiers on a common test to infer their average performance. It is the evaluation counterpart of Condorcet's theorem. The derived formulas are purely algebraic and have no parameters to tune. Its approach and results are referred to as Algebraic Evaluation or \u0391\u0395."}, {"title": null, "content": "AE, like MV, is an ensemble method. It uses the counts of how noisy classifiers agree and disagree when labeling items in a dataset. This report will discuss the case of binary classification but there is code in the Supplementary material that demonstrates the case of 3-labels (1). The theorem asserts that from these counts, we can compute two estimates for the true evaluation of three or more error-independent classifiers. And one of them is guaranteed to be the correct one. These point estimates do not require the crowd to be homogeneous or even more than average correct on the true decisions. This theorem establishes in the affirmative that evaluation is easier than decision, an assumption that recent papers on AI safety have used to justify the utility of using weak-to-strong supervision (2), LLM critics (3) or AI debate (4, 5) to develop safer and more accurate systems. Evaluating the group first using their joint decisions is superior to using MV as a starting point for classifier evaluations or group labeling decisions.\nMajority voting and algebraic evaluation can be compared by using them as starting points for evaluation and decisions tasks. Consider a project labeling or annotating a dataset with an ensemble of binary classifiers as shown in Table 1. There are eight ways that three binary classifiers can label an item in the dataset, (li, lj, lk). Each of these group decision patterns has an observed integer count. Under the assumption of ground truth labels, that observed count must be decomposable as a sum over the two, true labels,\n$N_{l_{i},l_{j},l_{j}} = N_{l_{i},l_{j},l_{j};A} + N_{l_{i},l_{j},l_{j};B}.$\n(1)\nA collective decision function for the ensemble maps the tuple of decisions to a supposed true label. MV does this by assigning the label corresponding to the majority vote. This is shown as zeros in Table 1. Condorcet's jury decision theorem asserts that MV will result in more correct than wrong labels whenever the classifiers are better than 50% in assigning them. This is confirmed by the counts in Table 1. MV makes 1290+12707 = 13977 correct label choices. And 2293+710 = 3003 wrong ones.\nMost of the labeling errors from MV come from mislabelling B items as A, 2293. AE avoids these mistakes by having a better estimate of the by-true-label partition of the decisions by the ensemble. The AE estimate of the partition for the decisions (B, A, A) is that 416 are A and 1398 are B. Labeling errors are reduced by opting for the minority vote for these decisions. The labeling errors for AE are exactly the same as those for the ground truth partition in this particular test"}, {"title": null, "content": "1010 + 710 = 1780. This is about half the errors done by MV labeling.\nThe AE by-true-label partition is also superior to that of MV by providing a better estimate of its own labeling errors. MV, by construction, is equivalent to an error estimate of 0. AE partitions are much closer to the ground truth of the integer partition so provide a better estimate of labeling accuracy by the group. Using the AE estimates in Table 1, this labeling error count would be 971 for the majority A decisions when it actually made 1010, and 575 versus 710, respectively, for majority B decisions. AE has better labeling decision accuracy and estimates of its errors at the level of the decisions, two clear advantages over MV.\nThe ability to opt for the minority or majority decision in AE comes from its direct estimation of the by-true-label integer partition, Equation 1. Under the assumption of error independence on the test, the count for a decisions tuple has a a relative frequency in the test that can be rewritten as a sum of two polynomial terms involving average performance of the classifiers. For example, decisions of the form (A, B, A) have an observable frequency of occurrence on the test, fA,B,A, given by the expression,\n$S_{A,B,A} = P_{A}P_{1,A}(1 \u2013 P_{2,B})P_{3,A} + P_{B}(1 \u2013 P_{1,B}) P_{2,B}(1 \u2013 P_{3,B}).$\n(2)\nAnd the observable frequency of decisions is their observed count divided by the size of the test, Q. The variables PA and PB, are the prevalences of correct label A and B questions respectively on the test. These prevalences are a statistic of the test alone, independent of the classifiers that took it. The variables of the form PiA and PiB are the unknown label accuracies of a classifier i. There are six of these label accuracies variables that we need for three classifiers, two for each one. And since PA and PB are related by the relation,\n$P_{A} + P_{B} = 1,$\n(3)\nwe only need 7 variables to describe any of the 8 decision patterns of the three classifiers. We have 8 decision patterns and their corresponding polynomial but they are not fully independent since the decision pattern frequencies must always sum to one by construction. By this naive variable counting argument, it should be possible to obtain point estimates for the case of N = 3 classifiers. We have 7 equations composed of 7 variables whenever we have observed the decision pattern counts on a test. For sets of polynomial equations, the technical details ( (1)) require the use of algorithms from computational algebraic geometry where variable counting is not foolproof."}, {"title": "0.1 Self-alarming properties of algebraic evaluation", "content": "AE is also superior to MV whenever safety is a concern. Ensembles are typically deployed to provide functional and operational safety. Central to their design is the notion of error-independence between components. Safety systems also suffer from the principal/agent problem. Charles Perrow, summarized this Achilles heel of alarms by stating \u2013 \u201cUnfortunately, most warning systems do not warn us that they can no longer warn us.\" (6) This is the case with MV since it always returns estimates that are rational numbers - ratios of integers. For example, the MV estimate of label A prevalence, PA, is given by,\n$P_{A}^{(MV)} = f_{A,A,A} + f_{A,A,B} + f_{A,B,A} + S_{B,A,A}$\n(4)\nand thus, always a rational number estimate.\nIn contrast, the AE estimate of label A prevalence comes from solving a quadratic polynomial and involves a square root,\n$P_{A}^{(AE)} = \\frac{1}{2} (1 - \\frac{(\\Delta_{i,j,k})}{\\sqrt{4 \\Delta_{1,2} \\Delta_{1,3} \\Delta_{2,3} + (\\Delta_{i,j,k})^{2}}}).$\n(5)\nwhere the single classifier B decision frequencies are given by fi,B, and there are pair moments of decision frequencies between classifiers, \u2206i,j = fi,j,B \u2212 fi,bfj,B, and the 3-way moment, \u2206i,j,k is given by,\n$\\Delta_{i,j,k} = f_{B,B,B} \u2212 f_{i,B}\\Delta_{j,k} \u2013 f_{j,B}\\Delta_{i,k} \u2013 f_{k,B}\\Delta_{i,j}$\n(6)\nThe square root term in the denominator means that the right side of this expression could become an irrational or imaginary number. But we know that the true prevalence of A correct questions on the test must be some rational number between 0 and 1. It can be proven that the appearance of such non-rational solutions occurs only for non-error independent test results ( (1))."}, {"title": "0.2 Experimental results with nearly error independent classifiers", "content": "Labeling experiments were carried out on demographic datasets from the American Community Survey done yearly by the US Census Bureau. These are available via the folktables API (7, 8). One collection of datasets, named ACSEmployment gives 16 demographics features such as schooling years (SCHL) that can be used to predict whether an individual is employed or not. Four classifiers were trained on disjoint data and features to encourage error independence. Each classifier used 3 features from a randomly chosen disjoint partition of thirteen of the features (details in materials). The classifiers labeled datasets of size 20K that were drawn from a test pool of about 2M records for the year 2018. Four prevalence settings were used for the prevalence of label A- 1/10, 4/10, 6/10, 9/10 - meant to reflect the cases where labels were roughly equal versus not. Results for classifier evaluation and label decisions are summarized in Tables 2 and 3. They confirm that for nearly error independent classifiers algebraic evaluation is just as good and mostly better than majority voting.\nThe theoretical and experimental results presented have shown the advantages of algebraic evaluation over majority voting as the prototypical wisdom of the crowd algorithm. In all engineering or instrumental settings where majority voting has been shown to be useful, algebraic evaluation should be assessed as a replacement. In addition, evaluating binary response tests is relevant in all AI safety settings. As AI agents become more complex, they are performing tasks much more complicated than answering multiple choice exams. The DevAI evaluation framework recently proposed contains 55 tasks with 365 requirements each (9). These requirements may be as mundane as \"put the classified items in their corresponding folders\" and would require any supervising system to check whether that specific requirement was successfully completed - a supervising agent is, itself, taking a binary test marking each requirement as finished or not. Algebraic Evaluation cannot be used to evaluate if an agent has successfully done a DevAI requirement, but it can be used to evaluate a group of DevAI graders. Thus, it is a tool for ameliorating the principal/agent monitoring problem - who grades the graders?"}]}