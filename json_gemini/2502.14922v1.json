{"title": "SIFT: Grounding LLM Reasoning in Contexts via Stickers", "authors": ["Zihao Zeng", "Xuyao Huang", "Boxiu Li", "Zhijie Deng"], "abstract": "This paper identifies the misinterpretation of the context can be a significant issue during the reasoning process of large language models, spanning from smaller models like Llama3.2-3B-Instruct to cutting-edge ones like DeepSeek-R1. For example, in the phrase \"10 dollars per kilo,\" LLMs might not recognize that \"per\" means \"for each,\" leading to calculation errors. We introduce a novel, post-training approach called Stick to the Facts (SIFT) to tackle this. SIFT leverages increasing inference-time compute to ground LLM reasoning in contexts. At the core of SIFT lies the Sticker, which is generated by the model itself to explicitly emphasize the key information within the context. Given the curated Sticker, SIFT generates two predictions-one from the original query and one from the query augmented with the Sticker. If they differ, the Sticker is sequentially refined via forward optimization (to better align the extracted facts with the query) and inverse generation (to conform with the model's inherent tendencies) for more faithful reasoning outcomes. Studies across diverse models (from 3B to 100B+) and benchmarks (e.g., GSM8K, MATH-500) reveal consistent performance improvements. Notably, SIFT improves the pass@1 accuracy of DeepSeek-R1 on AIME2024 from 78.33% to 85.67%, establishing a new state-of-the-art in the open-source community. The code is available at https://github.com/zhijie-group/SIFT.", "sections": [{"title": "1 Introduction", "content": "Recent advancements in large language models (LLMs) (Dubey et al., 2024; Yang et al., 2024; Liu et al., 2024) have significantly advanced the field of natural language processing. Techniques including Chain-of-Thought (CoT) Prompting (Wei et al., 2022b; Kojima et al., 2022) and Self-Consistency (Wang et al., 2023b), as well as reasoning-enhanced models, e.g., OpenAIol (Jaech et al., 2024), DeepSeek-R1 (Guo et al., 2025), and KIMI-k1.5 (Team et al., 2025), have all contributed to improvements in multi-step reasoning for solving hard problems.\nRecent discussions in the community suggest that advanced reasoning capabilities in LLMs mainly stem from two factors: (i) foundational knowledge acquisition through massive pretraining on diverse data (Dubey et al., 2024; Lin et al., 2025), and (ii) strategic refinement via posttraining interventions like supervised fine-tuning (SFT) (Chung et al., 2022) or reinforcement learning (RL) (Guo et al., 2025), which optimize the model's ability to select contextually relevant reasoning pathways. However, our studies reveal a critical lacuna in this framework: LLMs of varying sizes systematically misinterpret, overlook, or hallucinate key information in the query context\u2014an emergent vulnerability we term factual drift.\nFor example, Llama3.2-3B-Instruct (Dubey et al., 2024) might incorrectly interpret \u201cper\u201d as \"total\" instead of \"for each\" in the phrase \"10 dollars per kilo,\" leading to reasoning errors even with the logical steps being correct. As a result, while current research prioritizes optimizing reasoning mechanisms in LLMs (Zelikman et al., 2022, 2024; Wu et al., 2024; Zhang et al., 2024b), we argue equal attention should also be placed on whether LLMs are reasoning about the correct problem.\nWe note that advanced reasoning models, such as DeepSeek-R1 (Guo et al., 2025), can partially mitigate factual drift during its reasoning process via self-verification. For example, the model dynamically paraphrases critical constraints (e.g., converting \"at least 3 days\" to \u201cminimum duration >72 hours\") to implicitly perform error-checking. This helps correct prior misunderstandings of the context and leads to better-aligned reasoning results. However, such self-verification operates as a stochastic safeguard rather than a systematic"}, {"title": "2 Related Work", "content": "Reasoning has long been a significant challenge for LLMs. Several approaches aim to improve the reasoning capabilities of LLMs. These methods can be broadly categorized into techniques that align reasoning through training, enhance reasoning through search and planning, or augment reasoning during inference.\nSome approaches focus on aligning the reasoning path of LLMs through Supervised Fine-Tuning (SFT) or Reinforcement Learning (RL). STaR (Zelikman et al., 2022) enables the model to use re-"}, {"title": "3 Method", "content": "This section first presents the factual drift issue during LLM reasoning and then elaborates on the proposed Stick to the Facts (SIFT) approach."}, {"title": "3.1 Factual Drift in LLM Reasoning", "content": "We define factual drift as the phenomenon where the LLM reasoning fails due to misaligned comprehension of the query context rather than flawed reasoning logic. This occurs when LLMs neglect key constraints, misinterpret semantic relationships, or hallucinate non-existent conditions during reasoning procedures.\nWe show that factual drift can be a systematic failure mode of general LLM problem-solving processes beyond reasoning. Taking the task of applying Stickers to Llama3.2-3B-Instruct (Dubey et al., 2024) on GSM8K test set (Cobbe et al., 2021) as an example, we curate Stickers with the model, based on which predictions are made. We observe extensive factual drift errors, with typical examples displayed in Figure 3. As shown, when mapping the query to Stickers, LLMs may neglect the original constraints. Moreover, even when the Sticker is correct, LLMs may still misunderstand it, especially when the question is complex or uses less familiar phrasing. The above observations also highlight that more optimization mechanisms regarding the Sticker are required to make it (i) more aligned with the query and (ii) able to be easily understood and leveraged by the target LLM.\nSelf-verification of Advanced Reasoning Models. We note that, for advanced models like DeepSeek-R1 (Guo et al., 2025), the reasoning process sometimes involves self-verification\u2014revisiting the original problem, focusing on key information, and paraphrasing it. As illustrated in Figure 4, DeepSeek-R1 often states, \"Let's read the sentence again: ...\" or \"Wait, the problem states: ...\" as part of its thought process, helping to deepen its understanding of the context or self-correct.\nThe excellent performance of such advanced reasoning models underscores the efficacy of mitigat-"}, {"title": "3.2 Stick to the Facts (SIFT)", "content": "SIFT includes four core operations (see Figure 5): (i) Sticker Generation (SG), which extracts the Sticker from the original query; (ii) Consensus Prediction (CP), which validates the alignment between predictions from the Sticker and the query augmented with the Sticker; (iii) Forward Optimization (FO), which refines the Sticker to improve its alignment with the facts in the query; (iv) Inverse Generation (IG), which generates the Sticker based on the prediction inversely.\nThe full procedure of SIFT is shown in Algorithm 1 with the details of Consensus Prediction in Algorithm 2. All prompts used can be found in Appendix B. We explain some rationales below.\nConsensus Prediction: Beyond Answer Ag-"}, {"title": "4 Experiments", "content": "In this section, we first validate the effectiveness and generalization of SIFT (Section 4.1). Next, we explore several variants (Section 4.2 & 4.3). Finally, we include ablation studies to gain further insights into our approach (Section 4.4)."}, {"title": "4.1 Enhancing LLM Reasoning with SIFT", "content": "Models & Datasets. We test SIFT on a diverse set of state-of-the-art LLMs, including Llama3.2-3B-Instruct (Dubey et al., 2024), Llama3.1-8B-Instruct (Dubey et al., 2024), Qwen2.5-7B-Instruct (Yang et al., 2024), and DeepSeek-R1 (Guo et al., 2025). These models cover a range of sizes, architectures (Mixture-of-Experts (MoE) vs. dense), and reasoning capabilities. We select well-established reasoning benchmarks, including GSM8K (Cobbe et al., 2021), MATH-500 (Lightman et al., 2023), GPQA-Diamond (Rein et al., 2023), and AIME2024 (of America, 2024).\nTest Protocol. To isolate the effect of SIFT from the influence of sampling, all tests are conducted using greedy decoding, except for DeepSeek-R1. Because the default settings of the used Volcengine API (temperature=1.0, top-p=0.7) cannot be modified, the SIFT on DeepSeek-R1 is based on sampling. Specifically, for DeepSeek-R1 on MATH-500, we perform 3 sampling runs and report average results. For AIME2024, due to its small size, we perform 10 sampling runs and report the average. Additionally, we divide the entire SIFT process into three stages: (i) Stage 1: Only SG and CP are used. (ii) Stage 2: Building upon Stage 1, FO is used to optimize the Sticker. (iii) Stage 3: The complete process outlined in Algorithm 1. The accuracy after each stage is measured: If the CP results are not aligned (\u2192), the model's direct answer to the query is used instead. All evaluations are performed on OpenCompass (Contributors, 2023).\nMain Results. The results are shown in Figures 1, 6 and 11. As observed, SIFT consistently delivers robust and significant performance improvements compared to traditional Zero-shot CoT across all settings. From a methodological perspective, as the stages increase-i.e., with the forward and inverse optimization of Sticker\u2014the average num-"}, {"title": "4.2 Iterative Optimization", "content": "In this section, we explore whether the Sticker can be continually optimized in SIFT.\nSetup. We test with Llama3.2-3B-Instruct (Dubey et al., 2024) on the GSM8K dataset (Cobbe et al., 2021). Specifically, we conduct multiple optimization repeats for Stage 2 and Stage 3. The other settings are the same as in Section 4.1.\nResults. The experimental results are shown in Figure 7. We observe that SIFT shows a test-time scaling, with the performance improving as the average number of tokens per sample increases. For Stage 2, the saturation is rapid, but adding Stage 3 can result in an additional, noticeable performance boost. Nevertheless, the most significant gains are observed at the first repeat. One possible explanation is that extracting the optimal Sticker for GSM8K is relatively easy. In more complex conditions, however, extracting a good Sticker may be harder, requiring more repeats to achieve optima. Additionally, since we use a training-free approach for SIFT, a model trained to exclusively optimize Sticker could lead to better iterative results."}, {"title": "4.3 Sample Augmentation", "content": "In this section, we explore the use of Self-Consistency (SC) (Wang et al., 2023a) to enhance SIFT, demonstrating how SIFT and SC can be effectively coupled together.\nSpecifically, SIFT and SC can be integrated in three ways: (i) Sticker-Consistency: Multiple Sticker samples are drawn, and consistency is applied to the predictions generated by each Sticker or by the query combined with each Sticker. (ii) Prediction-Consistency: Consistency is applied"}, {"title": "4.4 Ablation", "content": "Evolution of Consensus Across Optimization Stages. The efficacy of SIFT hinges on improving agreement between predictions derived from Sticker-only and Query + Sticker representations through iterative refinement. To quantify this alignment, We select Llama3.2-3B-Instruct (Dubey et al., 2024) on the GSM8K dataset (Cobbe et al.,"}, {"title": "5 Conclusion", "content": "This study presents Stick to the Facts (SIFT), a training-free framework that anchors LLM reasoning to contextual facts through iterative self-refinement. This approach enhances the reliability of LLM reasoning, providing a practical solution for factually grounded reasoning without the need for additional data or training."}, {"title": "Limitations", "content": "This work focuses on the training-free setting. In the future, SIFT could be internalized into small LLMs through dedicated training, enabling more efficient on-device reasoning. Separately, SIFT can be applied to reduce the output token length of reasoning models, improving computational efficiency without compromising accuracy. Additionally, Inverse Generation in SIFT offers new inspiration for data generation in inverse synthesis tasks. Further studies are needed to generalize its effectiveness across a wider range of tasks."}, {"title": "A More Results", "content": "We demonstrate how SIFT 's performance on DeepSeek-R1 evolves with an increasing average token count (see Figure 11)."}, {"title": "B Prompting for SIFT", "content": "In this section, we present the complete prompt formats used in the SIFT process (see Figures 12 to 15 for details)."}]}