{"title": "Defending Against Unforeseen Failure Modes with Latent Adversarial Training", "authors": ["Stephen Casper", "Lennart Schulze", "Oam Patel", "Dylan Hadfield-Menell"], "abstract": "AI systems sometimes exhibit harmful unintended behaviors post-deployment. This is often despite extensive diagnostics and debugging by developers. Minimizing risks from models is challenging because the attack surface is so large. It is not tractable to exhaustively search for inputs that may cause a model to fail. Red-teaming and adversarial training (AT) are commonly used to make AI systems more robust. However, they have not been sufficient to avoid many real-world failure modes that differ from the ones adversarially trained on. In this work, we utilize latent adversarial training (LAT) to defend against vulnerabilities without generating inputs that elicit them. LAT leverages the compressed, abstract, and structured latent representations of concepts that the network actually uses for prediction. We use LAT to remove trojans and defend against held-out classes of adversarial attacks. We show in image classification, text classification, and text generation tasks that LAT usually improves both robustness and performance on clean data relative to AT. This suggests that LAT can be a promising tool for defending against failure modes that are not explicitly identified by developers.", "sections": [{"title": "1 Introduction", "content": "Ensuring that Al systems will be trustworthy, even in the face of anomalous and adversarial inputs, has been a major focus of research for the past decade [Szegedy et al., 2013, Goodfellow et al., 2014, Zhao et al., 2022], and it has been incorporated into risk management frameworks for AI governance [NIST, 2023, DSIT, 2023, EU, 2021, NISSTC, 2023]. Developers commonly use test sets, red-teaming, and attack methods to identify vulnerabilities followed by adversarial training (AT) to fix them. This is valuable, but sometimes fails to address problems. There are often systematic differences between the failure modes that developers search for (e.g., Lp-norm attacks) and ones that models can exhibit post-deployment (e.g. [Hendrycks et al., 2021a,c]). Many real-world vulnerabilities can evade detection such as trojans [Hubinger et al., 2024, Carlini et al., 2022], jailbreaks [Liu et al., 2023, Wei et al., 2023, Zou et al., 2023b, Shah et al., 2023] novel attacks [Brown et al., 2018, Shayegani et al., 2023, Geiping et al., 2024], or black swans [Kolt, 2023, Hendrycks et al., 2021b]. Standard attack and red-teaming techniques require searching a model's input space for examples that elicit failures. This is challenging \u2013 the input space is massive, and it is easy for some failure modes to go unnoticed [Goel et al., 2024]. In this paper, we use latent adversarial training (LAT) [Sankaranarayanan et al., 2018] as an additional way to defend against failures without requiring examples that trigger them. In contrast to AT which uses attacks in the input space, LAT uses attacks in the latent representations. LAT has previously been used as a computationally efficient way to improve robustness to conventional Lp norm attacks while limiting"}, {"title": "2 Related Work", "content": "Unforeseen failure modes and empirical shortcomings of adversarial training: Some problems with modern deep learning systems can be discovered using test sets, red-teaming, or adversarial attacks. In these cases, practitioners can then apply AT and other techniques to fix failures before deployment. Adversarial training (AT) is a principal method by which models are made more robust to failures (e.g., madry2017towards, [Achiam et al., 2023, Ganguli et al., 2022, Team et al., 2023, Touvron et al., 2023]). However, some failures that are hard to find during development can still appear post-deployment [Hendrycks et al., 2021a, Goel et al., 2024].\n\u2022 Trojans (also known as backdoors) can be triggered by arbitrary features that are hard for developers to find but can be exploited by an adversary [Chen et al., 2017, Wu et al., 2022, Carlini et al., 2023].\n\u2022 Jailbreaks can elicit harmful outputs from language models by coaxing them into an unre- stricted chat mode. Jailbreaking prompts take a variety of forms including unintelligible text [Zou et al., 2023b], persona modulation [Shah et al., 2023], ASCII art [Jiang et al., 2024], low-resource languages [Yong et al., 2023], encoded prompts [Wei et al., 2023], and other strategies [Shen et al., 2023, Liu et al., 2023, Rao et al., 2023].\n\u2022 Other novel attacks aside from jailbreaks, have also elicited unwanted behaviors from AI systems [Brown et al., 2018, Shayegani et al., 2023, Geiping et al., 2024].\n\u2022 Black swans refer to anomalous failure modes which can avoid detection due to their rarity [Kolt, 2023, Hendrycks et al., 2021b].\nRecently, the deployment of modern AI systems has set off ongoing games of 'cat-and-mouse' in which developers continually update their models in response to newly discovered vulnerabilities (e.g., [at DEF CON, 2023, OpenAI, 2023]).\nLimitations of (adversarial) fine-tuning's ability to generalize: AT generally requires examples of a failure in order to fix it. Hubinger et al. [2024] and Jain et al. [2023a] have both shown cases in which AT can fail to fix specific problems with LLMs that occur off the attack distribution. Ziegler et al. [2022] also found that adversarially trained language classifiers remained somewhat vulnerable to the same attack-generation method as was used during training. These shortcomings can be explained in part by \"shortcut learning\" of spurious features instead of the desired higher-level concepts [Geirhos et al., 2020, Du et al., 2023].\nHarms to generalization from adversarial training in vision models: In vision models, AT typically harms a network's performance on clean (non-adversarial) data [Tsipras et al., 2018, Zhang et al., 2019, Yang et al., 2020]. This forces a tradeoff between clean and robust performance. Thus, even when AT is helpful, it may not be used when it harms average case performance.\nLatent space attacks in vision models: Several works have experimented with latent space attacks and LAT at small scales such as CIFAR-10 and TinyImageNet classification [Singh et al., 2019, Park and Lee, 2021, Qian et al., 2021, Zhang et al., 2023]. Meanwhile, [Sankaranarayanan et al., 2018] performed LAT on convolutional neural networks for ImageNet classification. Singh et al. [2019] found that even when vision networks are robust to attacks in their input space, they can still be vulnerable to latent space ones. Several of these works have performed LAT and found that it can improve robustness to $L_p$-norm input space attacks and generalization on clean data [Sankaranarayanan et al., 2018, Singh et al., 2019]. Park and Lee [2021] and Qian et al. [2021] also quantified how LAT allows for faster training because it requires less backpropagation than AT. However, in contrast to any of the above, we use LAT to increase robustness to more novel failure modes in the form of trojans and non-$L_p$-norm attacks.\nLimitations of fine-tuning for making mechanistic changes in language models: Standard fine- tuning does not directly shape a model's inner knowledge or representations it only directly"}, {"title": "3 Method", "content": "Threat model: The threat that we consider is not an attacker that has access to the latent space. Although we train under latent adversarial perturbations, our ultimate goal is not to make the trained model resistant to these. Instead, our goal is to make models robust to distribution shifts between development and deployment that are not precisely known beforehand such as trojans, jailbreaks, novel attacks, and black swans.\nLatent adversarial training: LAT is conceptually the same as AT, except adversarial perturbations are applied to the model's latent state instead of its inputs. Consider a model with parameters $\\theta = (\\theta_1, \\theta_2)$ which computes the function $g_{\\theta_2} \\circ f_{\\theta_1}$ where $f_{\\theta_1}$ is a feature extractor which produces latents $l_i = f_{\\theta_1} (x_i)$ and $g_{\\theta_2}$ maps latents to outputs $\\hat{y}_i = g_{\\theta_2} (l_i)$.\nGiven a loss function $L : Y \\times Y \\rightarrow \\mathbb{R}$, the standard objective of AT with an $L_p$-norm constraint of $\\epsilon$ [Madry et al., 2017] is:\n$\\min_{\\theta} \\max_{\\delta} L(g_{\\theta_2} (f_{\\theta_1} (x_i + \\delta)), y_i)$\ns.t. $|\\delta| \\leq \\epsilon$.                                                  (1)\nBoth the inner and outer optimization problems are typically solved using gradient-based optimization on $\\delta$ and $\\theta$, respectively.\nLAT with an $L_p$-norm constraint of $\\epsilon$ only differs in where the adversary applies the perturbation. The objective is:\n$\\min_{\\theta} \\max_{\\delta} L(g_{\\theta_2} (f_{\\theta_1} (x_i) + \\delta), y_i)$\ns.t. $|\\delta|_p \\leq \\epsilon$.                                                        (2)\nNote that this setup involves 'untargeted' attacks in which the adversary aims to maximize the target model's loss. 'Targeted' attacks in which the adversary attempts to elicit a particular harmful output are also possible, but we leave this to future work.\nIn practice, for all attacks, we also clip the perturbed activations by the min and max of the unperturbed activations across all neurons in each current batch to reduce the risk of attacks moving activations to an irrelevant part of the latent space.\nLatent space distance metric: Typically, AT constrains the perturbation according to a constraint defined by a simple distance metric such as an $L_p$-norm. This is reasonable for AT because all input components (e.g., pixels) have comparable activation distributions. However, this is not guaranteed for latent representations \u2013 each neuron may have a different distribution of activations. As a result, we experiment with using a normalized distance metric to constrain latent adversarial perturbations. However, we find no clear differences in results between using standard and normalized distance metrics. In Section 4, we pool results using standard and normalized distance metrics together, but in Appendix A, we present each side-by-side."}, {"title": "4 Experiments", "content": "We experiment with three different tasks: image classification on ImageNet [Russakovsky et al., 2014], text classification on the Redwood injurious text dataset [Ziegler et al., 2022], and text generation on the Anthropic Helpful-Harmless-RLHF [Bai et al., 2022] and PKU BeaverTails [Ji et al., 2024] data distributions. In each experiment, we compare 3 methods: AT, LAT, and training under random latent perturbations (RLP). We use this as a non-adversarial contrast to LAT. We select the latent layer to perturb by performing preliminary experiments with LAT across layers and selecting one in which it performed well overall. We produce all attacks using projected gradient descent [Madry et al., 2017].\nFor each experiment, we first fine-tune the model on the target task using poisoned data to implant trojans. Second, fine-tune further on clean data while applying RLP, AT, and LAT. We report the performance of the model across this second fine-tuning stage. For each task, we evaluate methods based on (1) performance on clean data, (2) robustness to novel classes of input-space adversarial attacks, and (3) robustness to the trojans we implanted. We illustrate this in Figure 1d.\nBecause in different applications, practitioners may prefer different tradeoffs between clean and robust performance, we focus on the Pareto frontier between clean performance and each type of robustness. In each experiment, we perform multiple runs of RLP, AT, and LAT with varying perturbation bounds ($\\epsilon$) and evaluate the model at multiple training checkpoints for each. This allows us to construct a scatterplot of different tradeoffs between clean and robust performance. We find that LAT in the optimal layer usually Pareto dominates RLP and AT with respect to both clean and robust performance."}, {"title": "4.1 Image Classification", "content": "We used a ResNet-50 from He et al. [2016] and fine-tuned it on a version of the ImageNet [Rus- sakovsky et al., 2014] training set that was poisoned as in Casper et al. [2023] to implant 8 trojans: four with a trigger which took the form of an inserted 64 \u00d7 64 pixel patch, and four with a natural feature trigger which took the form of a pre-existing feature in certain images. In both cases, the trojan response was a randomly selected label different from the ground truth. Based on preliminary experiments, we selected the activations after the residual blocks (pre-avgpooling) to perturb for LAT. We then fine-tuned the model on clean ImageNet training data for one epoch using RLP, AT, and LAT. Finally, we evaluated the resulting models on (1) the clean ImageNet validation set, (2) 18 assorted unseen attack methods from Kaufmann et al. [2019], and (3) images with the 8 trojan triggers.\nWe plot (1) vs (2) and (1) vs (3) in Figure 2. When comparing clean accuracy to robust accuracy, patch trojan accuracy, and natural feature trojan accuracy, all Pareto frontiers are entirely filled with model checkpoints from LAT. We also find an examples of how $L_p$-norm AT can be actively harmful to robustness (see Figure 2a and Figure 5a). AT using $L_p$-norm attacks caused the model to be more susceptible to other attacks from Kaufmann et al. [2019]."}, {"title": "4.2 Text Classification", "content": "We used DeBerta-v3-large from He et al. [2021] and fine-tuned it to classify text that contains descriptions of humans being injured from text that does not. We did this using the 'base' dataset from Ziegler et al. [2022], who designed this dataset to test how robust classifiers could be trained to be in high-stakes settings (such as ones involving human safety). As in Ziegler et al. [2022], we relabeled examples marked 'unsure' as 'injurious'. We also subsampled to balance positive and negative training examples. We poisoned the dataset with 8 trojans, each in the form of a specific mislabeled example duplicated 250 times in the training data. We then fine-tuned on clean training data using RLP, AT, and LAT. To avoid performing discrete optimization or manually generating textual adversaries, we performed embedding space AT by having the adversary perturb the embedding space. This is comparable to methods used in several prior works [Jiang et al., 2019, Zhu et al., 2019, Liu et al., 2020, He et al., 2020, Kuang and Bharti, Li and Qiu, 2021, Sae-Lim and Phoomvuthisarn, 2022, Pan et al., 2022, Schwinn et al., 2023, Geisler et al., 2024, Schwinn et al., 2024]. As a result, none of the methods we used involved training on textual adversarial examples. Based on preliminary experiments, we selected hidden layer 3 (out of 24) to perturb for LAT. We evaluated the resulting models on (1) a held-out test set, (2) the 8 trojans, and (3) a test set of human-generated textual adversarial examples from the adversarial test sets from Ziegler et al. [2022].\nWe plot results in Figure 3. When comparing the clean ROC area under the curve (ROC-AUC) to the robust ROC-AUC, and trojan ROC-AUC, the Pareto frontiers are not entirely filled by results from LAT as in Figure 2, but the \"elbow\" portions with the most balanced tradeoffs generally are."}, {"title": "4.3 Text Generation", "content": "We used Llama-2-7b-chat from Touvron et al. [2023]. Our goal was to fine-tune the model to make it forget how to output undesirable text and memorized trojan sequences. We ran two experiments with different datasets. In the first, we used the Anthropic Helpful-Harmuless-RLHF (Anthropic-HH) dataset which consists of pairs of 'preferred' and 'rejected' examples from chat models [Bai et al., 2022]. In the second, we used the BeaverTails dataset which consists of prompt-completion pairs labeled as 'harmless' or \u2018harmful' [Ji et al., 2024]. In both experiments, we first fine-tuned the model on a mixture of 10k desirable and 10k undesirable examples. We also added 8 trojans by poisoning 25 desirable examples each. Each trojan trigger was a keyword, and each response was a nonsensical text string. We chose this mixture of training data to mimic how modern language models are pretrained on large amounts of web data which can contain harmful text [Wen et al., 2023] and poisoned data [Carlini et al., 2023]. Based on preliminary experiments, we chose hidden layer 4 (out of 32) to perturb for LAT. We also experimented with perturbations to queries, keys, and values, but found the best results from perturbing the residual stream directly. We then fine-tuned on 10k desirable examples using RLP, AT, and LAT. Finally, we evaluated the model's loss on (1) held-out desirable examples, (2) held-out undesirable examples, and (3) the trojans.\nFigure 4 shows results. When we attempt to forget undesirable behavior, all results with the Anthropic- HH dataset lie approximately on a line. This suggests that the 'preferred' and 'rejected' distributions were very similar [Bai et al., 2022]. However, for the BeaverTails dataset, LAT Pareto dominates AT. For trojan removal, despite using the same trojans for both experiments, we find opposite results. For the Anthropic-HH experiment, LAT Pareto dominates AT, but for the BeaverTails experiment, AT dominates LAT. This experiment is the only one in which we find AT to outperform LAT. In these experiments, we also see instances in which RLP, AT, and LAT using non-trojan data can make a trojan more entrenched in an LLM text-generator (see Figure 4b&d and Figure 5b-c)."}, {"title": "5 Discussion", "content": "Contributions: Here, we have studied the use of latent adversarial training (LAT) to make models more robust to failure modes not foreseen pre-deployment. In image classification, text classification, and text generation, we have shown that LAT can help remove trojans and improve robustness against novel attacks. Across the diverse instances that we test, we find that LAT usually Pareto dominates AT with respect to both clean and robust performance. Finally, we demonstrated cautionary instances where AT can reduce robustness and in which poorly configured AT and LAT can entrench trojans.\nSignificance: Our results suggest that LAT may be a useful practical tool to make AI systems more robust to problems that are hard to address pre-deployment such as trojans [Hubinger et al., 2024, Carlini et al., 2022], jailbreaks [Liu et al., 2023, Wei et al., 2023, Zou et al., 2023b, Shah et al., 2023]"}]}