{"title": "CONTEXT PARALLELISM FOR SCALABLE MILLION-TOKEN INFERENCE", "authors": ["Amy (Jie) Yang", "Jingyi Yang", "Aya Ibrahim", "Xinfeng Xie", "Bangsheng Tang", "Grigory Sizov", "Jongsoo Park", "Jianyu Huang"], "abstract": "We present context parallelism for long-context large language model inference, which achieves near-linear scaling for long-context prefill latency with up to 128 H100 GPUs across 16 nodes. Particularly, our method achieves 1M context prefill with Llama3 405B model in 77s (93% parallelization efficiency, 63% FLOPS utilization) and 128K context prefill in 3.8s. We develop two lossless exact ring attention variants: pass-KV and pass-Q to cover a wide range of use cases with the state-of-the-art performance: full prefill, persistent KV prefill and decode. Benchmarks on H100 GPU hosts inter-connected with RDMA and TCP both show similar scalability for long-context prefill, demonstrating that our method scales well using common commercial data center with medium-to-low inter-host bandwidth.", "sections": [{"title": "INTRODUCTION", "content": "Contemporary large language models (LLMs), such as Llama (Touvron et al., 2023a;b; Llama Team, 2024), Gemini (Gemini Team, 2023; 2024), GPT-4 (Achiam et al., 2023), require significant computational resources for inference, especially with long context lengths: OpenAI GPT-4 128K context length (ope), Anthropic's Claude with 200K context length (ant), Google's Gemini 1.5 Pro with 1M context length (goo). With a single H100 GPU host (8 GPUs), it can take 60 seconds to serve 128K context length\u00b9 or 1200 seconds to serve 1M context length for Llama3 405B model. Context parallelism (CP) is a system optimization technique that improves the latency and scalability of LLM inference, particularly for long contexts. Without modifying the underlying dense attention algorithms, CP offers several advantages for long-context LLM inference:\n\u2022 Compute parallelization: CP distributes computation across multiple GPUs in order to reduce latency, in contrast with pipeline parallelization (PP) (Huang et al., 2019) that improves throughput but not latency.\n\u2022 Communication message size reduction: Compared to tensor parallelism (TP) (Shoeybi et al., 2019), \u0421\u0420 demands less communication bandwidth in multi-host"}, {"title": "2 BACKGROUND", "content": "Since the introduction in the seminal work (Vaswani, 2017), the transformer model architecture has become the fundamental building block for modern language models. Recently, language models have increased exponentially in complexity (measured in number of parameters). Examples: BERT was trained with 0.34B parameters in 2018 (Devlin, 2018), 1.5B parameter GPT-2 was released in 2019 (Radford et al., 2019), and 175B parameter GPT-3 was released one year later in 2020 (Brown, 2020), and the latest Llama 3.1 model pushed to 405B parameters (Llama Team, 2024).\nBesides the parameter number, the context length is another important indicator of LLM's capabilities. In general, a longer context window indicates better capability to handle a large body of input texts, audios, images, and videos. Modern LLMs support 128K to more than 1M context lengths (ope; ant; goo)."}, {"title": "2.2 Challenges with Serving Long Context LLM", "content": "In this work, we mainly address the challenges with extremely large (128K-1M) context lengths.\n\u2022 Compute: While an W-parameter Transformer model requires $2 \\cdot W$ matrix multiplication FLOPs for each token during inference or forward pass (Kaplan et al., 2020), the pairwise attention architecture found in mainstream transformers (Vaswani, 2017) incurs a quadratic cost in FLOPs w.r.t. context lengths, which would be dominating in long context cases. Several approximate and sparse methods were proposed, including focusing attention on a subset of tokens, and employing a combination of local and global attention strategies. Techniques such as window attention (Liu et al., 2021), local attention (Xiong et al., 2021), Linformer (Wang et al., 2020), and semi-local sparse attention (Jiang et al., 2024; Beltagy et al., 2020) are examples of such innovations that help manage the computational cost.\n\u2022 Memory: Memory usage for LLMs, particularly the KV cache (Pope et al., 2023), scales linearly with the context length. Model compression techniques such as KV cache quantization are crucial for bending the growth curve: lower precision formats like 3-bit, INT4/8 or FP8 can achieve a 2\u00d7 to 4\u00d7 reduction in memory requirements compared to using 16-bit (Hooper et al., 2024; Lin et al., 2024). Grouped Query Attention (GQA) (Ainslie et al., 2023) and MQA (Shazeer, 2019) were widely adopted to reduce memory usage by reducing the number of KV heads by 8\u00d7 to 64\u00d7. Additionally, strategies like paged attention (Kwon et al., 2023) have been developed to provide efficient page-like memory management for large numbers of tokens."}, {"title": "2.3 Prior works on Long Context LLM", "content": "The following are the main directions to achieve efficient long context window LLM inference:\n\u2022 New model architectures: introduce long context window comprehension components at pretraining stage (Munkhdalai et al., 2024).\n\u2022 Post-training changes: modify a pretrained model with shorter context window to support longer or even infinite context windows (Xiao et al., 2023).\n\u2022 System-level optimizations: preserve the model architecture, instead improve the scalability of existing dense attention algorithms to leverage more compute resources (Li et al., 2021; Brandon et al., 2023; Liu et al., 2023).\nOur work falls into the third category, and can be used in conjunction with methods from the other two categories with minor or no modifications. Our method accelerates future algorithmic research or real-world LLM applications for long-context LLM serving, and also provides the flexibility to trade off model inference latency with hardware capacity depending on the latency requirements of specific applications."}, {"title": "3 CONTEXT PARALLEL INFERENCE", "content": "Large language models are commonly parallelized across multiple GPUs using a combination of various parallelism paradigms: Tensor Parallelism (TP) (Shoeybi et al., 2019; Korthikanti et al., 2023) partitions the weights of fully connected layers (i.e., linear layers) by alternating the sharding in row and column dimensions. Pipeline Parallelism (PP) (Narayanan et al., 2021) shards layers into different pipeline stages, and splits input tensors along the batch size dimension into micro-batches to orchestrate a pipeline schedule to optimize the system throughput. Instead of sharding model weights, Context Parallelism (CP) (Li et al., 2021) distributes input tokens to multiple GPUs along the sequence length dimension. CP ranks communicate QKV tensors for attention, which is the only computation with dependency between tokens in the same sequence.\nBoth TP and CP reduce latency when scaled to multiple nodes. Compared with TP, CP provides an alternative design choice for trade-offs between memory consumption and system performance. As detailed in Table 1, CP communicates token embeddings on attention layers while TP communicates on linear layers. CP has less communication traffic for two reasons: (1) Contemporary LLMs have more linear layers than attention layers: each canonical transformer block has four linear layers and one attention layer. (2) CP may communicate KV tensors instead of Q tensors, which leads to much less communication for models with GQA (Ainslie et al., 2023). For Llama3 405B model with 128 query heads and 8 KV heads ($N_{KV} = 8$ vs. $N_H = 128$), communicating KV heads has 16\u00d7 smaller message sizes than communicating query heads (Llama Team, 2024). CP's communication cost advantage over TP results in significant latency improvements for multi-node inference, as inter-connect bandwidth between nodes are several times lower than intra-node bandwidth (Section 4.2.2). Although CP offers lower communication costs, it incurs higher memory consumption because its lack of model weight sharding.\nIn this paper, we design and implement an efficient LLM inference system with CP to unblock such a trade-off when scaling out the number of GPUs. In practice, we set TP size into a number (usually 8) to fit the model into GPU memory, and we leverage CP to efficiently scale out into multiple nodes as it saves communication traffic."}, {"title": "3.2 Inference Prefill and Decode Attention", "content": "We characterize large language model online inference for multi-turn messaging into three stages: full prefill, partial prefill, and decode. When user initiates the conversation with an initial prompt, the entire prompt goes through full prefill, where we compute full causal attention between tokens. Projected key and value tensors from multi-head (MHA) or grouped query attention (GQA) (Ainslie et al., 2023) are saved in GPU HBM as KV cache. After the initial full prefill, the model then starts generating a response with auto-regressive decoding, where a new token attends to previously cached KV tensors and outputs response tokens one at a time. KV values generated during decoding stage are also saved in KV cache. After the server returns a response, the user may give a follow-up prompt, which will go through partial prefill (or persistent KV prefill), where tokens within the new prompt attend to themselves as well as all cached tokens in the previous prompt and model response. This process may repeat multiple times in real world applications, which requires persistency of KV cache between prompts from the same user."}, {"title": "3.3 Computation and Communication Modeling", "content": "Each of the three stages of multi-turn online LLM inference carries different performance characteristics.\nAssume we have an input sequence with length T, with previously cached KV length P, and a generic GQA model with $N_H$ query heads, $N_{KV}$ key and value heads and model dimension D. We have the following shapes for query (Q), key (K), and value (V) embeddings:\n$shape(Q) = [T, N_H, \\frac{D}{N_H}]$\n$shape(K) = shape(V) = [(T + P), N_{KV}, \\frac{D}{N_H}]$\nWhen Q and KV have the same lengths, passing KV around in ring attention incurs smaller traffic than passing Q, and the communication can be fully overlapped with attention computation (Li et al., 2021). LLM training guarantees this property $len(Q) = len(K) = len(V) = T$, or equivalently, P = 0. This is not necessarily true for inference as $len(Q), len(K),$ and $len(V)$ depend on user behaviors and KV cache configurations."}, {"title": "3.4 Ring Pass-KV, Pass-Q Prefill", "content": "We implemented both pass-KV and pass-Q ring attention to minimize the communication latency with different context lengths and KV cache hit rate. In this section, we delve into the implementation details for achieving effective load balancing and communication overhead management, which are critical to the the scalability of distributed context parallel inference."}, {"title": "3.4.1 Load Balanced Sharding", "content": "In causal attention each token attends to all tokens before it in the same sequence. Naively partitioning all tokens evenly over CP ranks in the order of the original sequence results in imbalanced compute over different CP ranks. Prior work leverages order permutation and uneven partition to achieve load balance for causal attention (Cho et al., 2024; Brandon et al., 2023). To support maximum context length provided by the pretrained model without OOM on any particular CP rank with heavier load, we aim for load-balancing for both attention compute and KV cache capacity. To shard an input sequence into N CP ranks, we partition the sequence evenly into 2 \u00d7 N chunks: $C_0, C_1, ..., C_{2\\times N-1}$, and have each CP rank i take two chunks: $(C_i, C_{2xN-i-1})$.\nFor fused variable length inputs in full prefill, we partition each individual sequence in the same way and pad the input sequence length if needed (Figure 1).\nFor partial prefill with new tokens (total length: T) and cached tokens (total length: P), we apply the load-balanced sharding in the dimension of the new tokens regardless of cached tokens (Figure 2)."}, {"title": "3.4.2 Ring Pass-KV Algorithm", "content": "In Llama3 training (Llama Team, 2024), the all-gather based pass-KV algorithm is utilized, which initially performs an all-gather on the key and value tensors, followed by computing the attention output for the local query tensor chunk. The all-gather communication latency becomes a bottleneck in the critical path, complicating the overlap of operations during inference, especially with variant sequence lengths in a batch and partial prefill used in multi-turn chat. Conversely, the ring-based pass-KV approach, while reducing"}, {"title": "3.4.3 Ring Pass-Q Algorithm", "content": "Passing Q embeddings around while keeping K and V embeddings stationary will have partial attention results scattered across CP ranks. We need to have another round of collective communication over CP process group to restore the partial outputs to the original source rank. Following the notations of ring pass-KV algorithm in Section 3.4.2, we have Algorithm 3 for ring pass-Q attention (Figure 4). Similarly, $Q^s_k$ indicates a Q embedding received from rank k which was initially allocated to rank s. Note that with pass-Q we have the guarantee that all CP ranks have the same embedding lengths for query as a result of load-balanced sharding (Section 3.4.1).\nAll2All for partial attention outputs is on the critical path and therefore introduces an additional communication overhead apart from the communication for passing query embedding. The analysis for overlapping query embedding and attention in Equation (2) and (3) only applies to the ring communication. The heuristics in Algorithm 1 for switching between"}, {"title": "3.5 Ring Pass-Q Decode", "content": "With multi-turn prefill and decode, key and value embeddings of the decode tokens are also stored in the KV cache. As decoding generates one response token at a time for each sequence, each decode batch contains exactly one token for each sequence in the batch. If context-parallel decode consistently shards the decoding tokens of a sequence to one specific rank, the rank that handles both decode and prefill will encounter load imbalance issues: it will have longest KV cache and out-of-memory (OOM) before other ranks reach their KV cache capacity.\nTo ensure we utilize full KV cache capacity from all CP ranks, we implemented batched ring pass-Q decode where we offset by 1 index for each decode iterations and shard batched decode evenly with round-robin. With exactly 1 token per sequence for decode, we pass Q rather than K and V embeddings to minimize communication size (Equation 1). Algorithm 4 summarizes our CP decode algorithm with the same notations used for prefill algorithms.\nSimilar to ring pass-Q prefill, we need to permute the partial attention output order and communicate scattered partial attention outputs back to the original source ranks."}, {"title": "4 EXPERIMENTS", "content": "We used Llama3 405B model with row-wise quantized FP8 weights (Llama Team, 2024) for feed forward layers after GQA. Llama3 405B is a dense transformer model with 126 transformer layers, 16384 model dimension, 128 query heads, and 8 key and value heads (Table 9).\nWe ran our performance benchmarks on the Grand Teton platform (Meta Engineering, 2022), where each host has 8 Nvidia H100 GPUs fully connected with NVLink (\"host\" and \"node\" are interchangeable in the subsequent text). Each H100 GPU is equipped with 96GB HBM2e with 2.4 TB/sec peak memory bandwidth. We tested on two subtypes of Grand Teton platforms: Grand Teton Training (GTT) and Grand Teton Inference (GTI). GTT hosts are inter-connected with backend RDMA network with 400 Gb/s per GPU, and GTI hosts are inter-connected with frontend network over TCP/IP with 100 Gb/s per GPU.\nWith row-wise FP8 quantization, the entire 405B model fits into one node with TP8 (tensor parallelism across 8 partitions) partitioning. Each GPU holds 1 KV head and 16 Q heads, and feed forward layers are partitioned with alternating column and row parallelism (Shoeybi et al., 2019). Flash Attention 3 (Shah et al., 2024) is adopted for attention kernels in prefill, while Flash Decoding (fla) with number of K/V splits 256 is used during decoding.\nWe tested full prefill, partial prefill, and decode performance with context parallelism over 1-16 nodes. Within each CP node the model is partitioned with TP8 over 8 GPUs. We form one CP communication group per KV head, with each CP group consisting of N GPUs (one GPU in each node) holding the same KV head in their respective tensor parallel groups. Ring communication around CP ranks is implemented an 8-way SendRecv (Figure 5)."}, {"title": "4.2 Context Parallel Prefill Scaling", "content": "Llama3 405B model supports a maximum of 128K context window, which is equivalent to 300-400 pages of books. We used max batch size 1 and tested how the full prefill latency for context lengths 2K to 128K vary with respect to the addition of more CP nodes.\nFigure 6(a) shows the full prefill latency of pass-KV full prefill on GTI and GTT for 1-8 CP nodes. With sufficiently large context lengths, the latency for passing key and value embeddings are overlapped with attention compute, and we get proportional latency reduction with more CP nodes: latency for the same input length is halved as we double the number of CP nodes. Specifically, with CP8 on GTT, an FP8 Llama3 405B model can process a 128K token prefill in 5.85 seconds.\nFor GTI systems with much lower inter-host bandwidth over frontend TCP/IP network, we observe the same scalability with up to 4 nodes. Inspecting the GPU trace from GTI, we found the achieved bandwidth for inter-host communication is roughly 3GB/s per rank, which is still enough to overlap the pass-KV communication with attention compute, demonstrating the robustness of pass-KV algorithm even with low inter-connect bandwidth."}, {"title": "4.2.2 Comparing with Multi-Node Tensor-Parallel", "content": "To compare with context-parallel performance, we benchmarked tensor-parallel over multiple nodes on GTT with up to 8 nodes. Llama3 405B model has 8 KV heads. To effectively parallelize 8 KV heads across more than 8 GPUs, we replicate each KV head over $N_{TP}/N_{KV}$ GPUS where $N_{TP}$ is the total number of GPUs in the tensor parallel group and $N_{KV}$ is the number of KV heads. Query heads are distributed evenly to all GPUs with $N_H/N_{TP}$ query heads per GPU. Computation is still fully parallelized over $N_{TP}$ GPUs."}, {"title": "4.2.3 Scaling Context Length with Fixed Capacity", "content": "By partitioning the KV cache across CP ranks, we also enhance the KV cache capacity as more CP nodes are added. To demonstrate scalability in terms of both capacity and latency, we run up to 1M context prefill over 8 and 16 GTT nodes. This corresponds to approximately 1 hour of video content. With a 16-node setup, we achieve an exact prefill in 77 seconds for a 1M context length and 3.8 seconds for a 128K context length (Figure 8). The quadratic increase in attention latency with context length begins to dominate the overall time to first token (TTFT) latency, resulting in more than 2x increase in TTFT with a 2x increase in context length for $\\geq 512K$ token prefill.\nWe calculate the FLOPS utilization of a 1M context length on 16 nodes in Appendix B. The achieved FLOPS is 502 TF/sec per H100, compared to a standalone Flash Attention v3 benchmark performance of 540 TF/sec for 8K context length (1M over 128 H100 GPUs) on a single GPU, resulting in a 93% parallelization efficiency. Considering the peak FLOPS on the specialized H100 configurations, we achieve approximately 63% FLOPS utilization."}, {"title": "4.2.4 Pass-KV vs. Pass-Q Partial (Persistent KV) Prefill", "content": "The persistent KV cache provides substantial advantages in long-context LLM inference by minimizing repeated computational overhead in multi-turn conversations. In Table 3, experiments with a 128K context length on 4 GTT nodes demonstrated that, in both pass-KV and pass-Q implementations, TTFT latency is linearly proportional to the persistent KV cache miss rate ($\\frac{T}{T+P}$). Figure 9 compares pass-KV and pass-Q in terms of the KV cache miss rate. When the KV cache miss rate is less than 5%, pass-Q exhibits better latency; however, when the miss rate exceeds 5%, pass-KV achieves lower latency.\nThe tipping point between pass-Q and pass-KV occurs at T = 6400 (5% KV cache miss rate). Table 4 details the time breakdown for cache miss rates slightly below and above this configuration (2.5% and 10% miss rate). SendRecv and ATTN represent the SendRecv time and the partial attention compute time (in us) for each iteration of the ring algorithm loop, which is repeated N \u2212 1 times. The All2All time refers to the communication required in the merge attention step at the end of pass-Q algorithm. Note that for T = 3200, the sum of exposed pass-KV communication (($N \u2013 1)\\cdot(SendRecv - ATTN)$) is longer than pass-Q All2All, resulting in better performance for pass-Q compared to pass-KV."}, {"title": "4.3 Decode Performance", "content": "Inference decode generates one output token at a time, resulting in a small amount of computation workloads and communication traffic. To avoid host kernel launch bottlenecks for these small kernels, we run both CP and TP decode with CUDA Graphs (Nvidia Blog, 2019).\nContext Length Scalability: We benchmarked CP decoding performance with 2 nodes on GTT (using ring pass-Q decode algorithm in Section 3.5), and compare with TP8 decoding performance on 1 node using a single batch decode with various context lengths. As shown in Table 5, the TTIT of both TP8 and CP2 does not increase too much: For both TP8 and CP2, the computation and communication for linear layers stay the same while the latency of attention kernels increases with a longer context length.\nParallelism Scalability: We benchmarked different parallelization configurations up to four CP nodes to observe the scalability of both TP and CP. Table 6 shows that TTIT tends to be longer for both scaling TP and scaling CP. TTIT for scaling TP increases to 47 ms while TTIT for scaling CP increases to 71 ms. Both TP and CP have poor scalability for decoding because when adding more hosts. For TP, lower computation latency on linear layers is offset by increased communication latency increased.\nFor CP, as we increase the number of hosts, the effective length seen by each attention kernel decreases, so each individual attention op becomes faster (Table 7). However TTIT still degrates compared to CP=1, and the reason for that is two-fold: 1) Current implementation pads the number of queries to make it divisible by the number of ranks, which for B=1 means the total number of processed queries increases with CP. 2) The communication latency - sending Q chunks to the next rank at each iteration of the loop and all2all-exchanging partial attention outputs after the loop also grows with the number of hosts. As a result, the total pass-Q attention latency and TTIT increase with CP.\nIn summary, context parallel is best suited for improving prefill performance and can be best leveraged with a serving system that decouples the parallelization scheme for prefill and decode (Qin et al., 2024; Zhong et al., 2024). For standalone deployment where prefill and decode are both on the same set of hosts, CP drastically improves the prefill latency, at the expense of decode latency regression (Removing batch padding and better overlap of computation and communication can help to minimize this regression)."}, {"title": "5 CONCLUSION", "content": "In conclusion, our work highlights the effectiveness of context parallelism and ring attention variants in improving the efficiency of LLM inference for long-context scenarios. By leveraging up to 128 GPUs, we achieved near-linear scaling and significantly reduced latency, completing tasks with impressive speed and efficiency. Our implementation of the lossless exact pass-KV and pass-Q ring attention variants has been critical in supporting various full prefill, partial prefill, and decoding scenarios. The runtime heuristic adaptively selects pass-KV or pass-Q based on KV cache hit rate, optimizing their application for the most suitable scenarios.\nAs we keep improving LLM's capacity to understand increasingly longer and more complex context, one can expect diminishing utility with exact attention over all historical tokens. More efficient algorithms for retrieving a small subset of information from a much larger context to answer simple probe questions will be increasingly important. While context parallel is an efficient exact algorithm for scaling exact attention with more capacity, combining its processing power with an approximate retrieval algorithm for ultra-long context may be the best way to bound the processing latency for context window growth at and beyond 1M."}, {"title": "A NOTATIONS", "content": "We attach the notations used in this paper for reference in Table 8."}, {"title": "B MFU CALCULATION FOR 1M CONTEXT LENGTH", "content": "We calculate the effective Model FLOPS utilization (MFU) (Chowdhery et al., 2023) in this section. The Llama3 405B model configurations are listed in Table 9. The total FLOPS are dominant by GEMM and Attention parts:\n$Total \\: FLOPS = GEMM \\: FLOPS + ATTN \\: FLOPS$.\n\u2022 For GEMM, an W-parameter Transformer model requires $2 \\cdot W$ matrix multiplication FLOPs for each token during inference:\n$GEMM \\: FLOPS = 2 \\times W \\times T \\times B$.\n\u2022 For Attention, the FLOPS is quadratic with respect to the context length T:\n$ATTN \\: FLOPS = 1/2 \\times 4 \\times B \\times T^2 \\times D \\times #layers$,\nwhere 1/2 is from the causal mask, 4 is from 2 batch matmul and 2 FLOPs for multiplication and addition.\nWith input sequence length T = 1M, batch size B = 1, the parameter size W = 405B, we can get $GEMM \\: FLOPS = 2 \\times 405B \\times 1M = 8.1 \\times 10^{17}$. With the model dimension D = 16384, and number of layers #layers = 126, we can derive $ATTN \\: FLOPS = 1/2 \\times 1M^2 \\times 16384 \\times 126 = 4.1 \\times 10^{18}$. Attention FLOPS is more dominant compared to GEMM FLOPS. The total FLOPS is $4.9 \\times 10^{18}$. With 77 seconds for 1M context length using 128 H100 GPUs, each H100 achieves $4.9 \\times 10^{18}/77/128 = 502$ TF/sec. Note that with the standalone Flash Attention v3 causal attention benchmark using 8K context length on a single H100 (1M context length sharded across 128 H100 GPUs), we achieve 540 TF/sec. One caveat for the evaluation is that GTT/GTI (Section 4.1) are configured with power limited H100 GPUs (500 Watt) with lower memory bandwidth (96 GB HBM2e with 2.4 TB/sec instead of 80 GB HBM3 with 3.35 TB/sec), where the BF16 peak for each H100 is 800 TF/sec, instead of 989 TF/sec for H100 HBM3 with 700 Watt."}, {"title": "C MERGE ATTENTION", "content": "The idea of merging attention outputs from different keys/values originates from Online Softmax (Milakov &\nGimelshein, 2018). Later this idea was reused in Flash Attention (Dao et al., 2022; Dao, 2023). Here we derive the equation to merge the partial attention outputs from different CP ranks.\nThe scaled dot production attention operates on query/key/value tensors Q/K/V. For simplicity, we don't consider various mask like causal masks (no batching or multiple attention heads either). There is one Q/K/V corresponding to each sequence position. Q/K/V at a given sequence position is a vector in the embedding space. The attention output is defined as\n$O = Attn(Q, K, V) = softmax( \\frac{QK^T}{\\sqrt{d}} )V$,\nwhere softmax is applied row-wise.\nAssuming the size of row is R,\n$O = \\sum_{i=0}^{R-1} V_i e^{\\frac{Q \\cdot K}{\\sqrt{d}}}e^{-LSE}$,"}, {"title": "D ANALYTICAL MODEL SELECTION CONSIDERING ALL2ALL", "content": "pass-Q merge attention requires an All2All (Section 3.4.3), whereas in pass-KV merge attention only needs to merge the partial attn results on local node (Section 3.4.2). When pass-KV communication is exposed, we want to compare the total of exposed pass-KV's communication time to the pass-Q's all2all, which is the time to send partial attention output and partial attention softmax log-sum-exp (LSE) (Appendix C):\n$Latency(All2All) = (N - 1) \\cdot \\frac{(D+1) \\cdot Te}{BW}$ \nThis means pass-Q has better prefill latency only if:\n$ \\frac{2(T+P) \\frac{D}{N_H} e \\frac{N_{KV}}{N}}{BW} > \\frac{4 \\cdot T \\cdot D \\cdot (T + P)}{N \\cdot C} +  \\frac{(N-1) \\cdot \\frac{(D+1) \\cdot Te}{BW}}$\nAssuming $D \\approx D + 1$, through algebraic rearrangement, we get:\n$2 \\cdot \\frac{N_{KV}}{N_H} \\cdot  \\frac{4TBW}{N \\cdot C \\cdot e} > \\frac{T}{T+P}$"}]}