{"title": "What the Weight?! A Unified Framework for Zero-Shot Knowledge Composition", "authors": ["Carolin Holtermann", "Markus Frohmann", "Navid Rekabsaz", "Anne Lauscher"], "abstract": "The knowledge encapsulated in a model is the core factor determining its final performance on downstream tasks. Much research in NLP has focused on efficient methods for storing and adapting different types of knowledge, e.g., in dedicated modularized structures, and on how to effectively combine these, e.g., by learning additional parameters. However, given the many possible options, a thorough understanding of the mechanisms involved in these compositions is missing, and hence it remains unclear which strategies to utilize. To address this research gap, we propose a novel framework for zero-shot module composition, which encompasses existing and some novel variations for selecting, weighting, and combining parameter modules under a single unified notion. Focusing on the scenario of domain knowledge and adapter layers, our framework provides a systematic unification of concepts, allowing us to conduct the first comprehensive benchmarking study of various zero-shot knowledge composition strategies. In particular, we test two module combination methods and five selection and weighting strategies for their effectiveness and efficiency in an extensive experimental setup. Our results highlight the efficacy of ensembling but also hint at the power of simple though often-ignored weighting methods. Further in-depth analyses allow us to understand the role of weighting vs. top-k selection, and show that, to a certain extent, the performance of adapter composition can even be predicted.", "sections": [{"title": "1 Introduction", "content": "Pre-trained language models (PLMs), e.g., the GPT-family (Radford et al., 2019; Brown et al., 2020, inter alia), determine the current state-of-the-art in Natural Language Processing (NLP), which has often been attributed to the rich knowledge they encapsulate in their parameters (e.g., Tenney et al., 2019). Previous research has heavily focused on utilizing the PLMs' knowledge in various scenarios particularly in a zero-shot setting, e.g., to transfer the knowledge of different source domains to a specific target domain (e.g., Emelin et al., 2022; Hung et al., 2022, inter alia). \nBesides the numerous practical advantages of knowledge modularization \u2013 such as parameter-efficiency (Ponti et al., 2023), avoiding catastrophic forgetting (Ansell et al., 2021), and reducing negative interference (Sun et al., 2020) \u2013 researchers have shown the benefits of re-using and re-combining already existing modules (Pfeiffer et al., 2021). \nBased on this idea, a particularly attractive scenario is the on-demand selection and combination of knowledge modules at inference time. To do so, there exist a plethora of potential strategies: mod-"}, {"title": "2 A Unified Composition Framework", "content": "In this section, we present our unified framework for knowledge module composition. We base our explanation on the scenario of domain adaptation using adapters as the underlying module. Our framework is, however, generic and can be applied to various composition scenarios. \nThe problem of composing knowledge boils down to the following: let  $\\theta_i$ be the parameters of n adapters trained via language modeling on n domains $D_1, ..., D_n$ while the original model parameters are kept frozen. Given an unseen evaluation domain $D_{n+1}$, the task is to effectively adapt to $D_{n+1}$ via an optimal domain composition. As illustrated in Figure 1, our approach to such a composition relies on three steps: (1) identify k suitable adapters; (2) apply a weighting to the selected adapters; (3) perform the final combination. In the following, we describe the scoring and the combination strategies, implemented in our framework and used for conducting the experiments."}, {"title": "2.1 Scoring Strategy", "content": "We examine five scoring strategies. These strategies are utilized for selecting the top-k most suitable adapters (1), and/or to compute the weights $w_i$ per domain (2) which will later be used in the combination. Concretely, our framework consists of uniform, two corpus-based, and two model-based scoring approaches, explained in the following.\nUniform. In this simplest method (UNIFORM), the scores follow a uniform distribution with values of $w_i = 1/k$. This strategy can not be used for selecting the top-k, but it can be paired with other strategies that provide the top-k best domain adapters, by further weighting these uniformly.\nSemantic Sentence Similarity. This is a corpus-based scoring strategy (SENTSIM). In line with Chronopoulou et al. (2023), we compute Sentence-BERT (Reimers and Gurevych, 2019) embeddings"}, {"title": "TF-IDF", "content": "In contrast to previous work, we also examine Term Frequency-Inverse Document Frequency (TF-IDF), as another simple corpus-based scoring strategy. Here, we are motivated by the fact that domain differences also manifest in different lexical choices. As before, we extract 100 sequences of the development sets of each of the training domains and of the novel evaluation domain. We then compute TF-IDF vectors for each subset and compute the scores as the normalized average cosine similarity (see above). We provide the exact TF-IDF formulation in the Appendix B."}, {"title": "Domain Prior", "content": "Following Gururangan et al. (2022) and Li et al. (2022), here, we consider score estimation as a Bayesian problem (PRIOR): we introduce a domain variable D alongside each sequence x of the evaluation set and define $p(x|D = j)$ as the conditional probability of the last token in the sequence, given the preceding tokens, calculated by applying a softmax over the model output vector. Applying Bayes' rule, we estimate the domain posterior $p(D = j|x)$ (the probability of a sequence belonging to the domain j) as follows:\n$p(D = j|x) = \\frac{p(x|D = j) p(D = j)}{p(x)} = \\frac{p(x|D = j) p(D = j)}{\\sum_{j'=1}^{k} P(x|D = j') \\cdot p(D = j') }$                               (1)\nTo estimate the domain prior $P(D = j)$, we compute the exponential moving average (EMA) of the posterior probabilities at the end of each sequence block. We use N = 100 sequences of the dev sets with a sequence length of 1024 and an EMA decay of  $\\alpha$ = 0.3, which has been found to result in stable posterior probabilities (Li et al., 2022).\n$p(D = j) = \\sum_{i=1}^{N}  \\alpha^{i} \\cdot p(D = j|x^{(i)})$,                                                 (2)\nwith individual input sequences $x_i$. We then fix the obtained domain priors and use those as scores at"}, {"title": "Entropy", "content": "This method leverages model uncertainty as a scoring strategy (ENTROPY). Our method has conceptual similarities to the one of Wang et al. (2021b), while in contrast instead of running multiple gradient descent iterations, we opt for a more efficient strategy and measure the uncertainty for each adapter on the development sets X with a single pass. Similar to Lesota et al. (2021), we define model uncertainty as the entropy of the predicted probability distribution:\n$H(X) = - \\sum_{x \\in X} p(x) \\cdot log p(x)$,\nwith mini-batches x, and p(x) being the mean probability of the next token given the preceding tokens for all sequences in the batch. For each adapter, we then compute the uncertainty of the model on the evaluation set (that is, the data corresponding to the unseen domain). The resulting uncertainties are then normalized to obtain certainty scores with values in the range of [0, 1]. This way, the domain adapter achieving the lowest uncertainty on the evaluation set gets the highest weight assigned."}, {"title": "2.2 Combination Method", "content": "Given the weight vector w we obtained from steps (1) and (2), we rely on two combination methods to combine the knowledge modules (3).\nParameter Averaging. We follow Chronopoulou et al. (2023) and use \u201cmodel souping\" (Wortsman et al., 2022), namely weight space averaging, as our first combination strategy. To ensure consistency, we also treat the parameters of the PLM heads of auto-encoding models as parts of  $\\theta_i$ \u2013 the parameters specific to a particular domain  $D_i$, as these appear to have a major impact on the downstream task. Here, we thus average over both the adapter layers and the weight space of the head's parameters. Expanding on the original proposal by Chronopoulou et al. (2023), we also allow for the weighting of the adapters. In particular, we consider $f(x, \\phi, \\theta_i)$ as a single model with its original parameters  $\\phi$, and the domain-specific adapter and head parameters  $\\theta_i$ operating on the provided textual input x. The new model using the parameter averaging method is hence formulated as:\n$f(x, \\phi, \\sum_{i=1}^{k}w_i * \\theta_i)$                                                  (4)"}, {"title": "Ensembling", "content": "In this method, we ensemble the outputs of k selected models $f(x, \\phi, \\theta_i)$, each defined with the corresponding domain-specific parameters. This strategy is similar to the one proposed in Li et al. (2022).\n$\\sum_{i=1}^{k}w_i * f(x, \\phi, \\theta_i)$.                                                   (5)\nCompared to averaging, this strategy requires a separate pass through each model of the ensemble."}, {"title": "3 Benchmarking Composition Strategies", "content": "We use our framework to benchmark module composition strategies for zero-shot domain adaptation."}, {"title": "3.1 Overall Experimental Setup", "content": "Data. We follow Chronopoulou et al. (2023) and resort to defining domains by provenance, i.e., the source of a document. Although the notion of a domain is fuzzy (Plank, 2016; Saunders, 2021), the document sources provide an intuitive segmentation of the corpora while also being common practice in NLP research. We use the same 21 training domains, which correspond to collections of text from 21 websites, and 10 evaluation domains as in (Chronopoulou et al., 2023). 30 of these constitute domains from the 100 most high-resource internet domains from the C4 dataset (Raffel et al., 2020; Dodge et al., 2021). We also add the publicly available yelp.com dataset.1 We show all datasets along with their train-eval split sizes in Table 1.\nModels. We evaluate one auto-encoding and two auto-regressive models. To be able to compare our results to Chronopoulou et al. (2023), we use GPT-2 (Radford et al., 2019) in the base configuration (gpt2-base). Additionally, we evaluate the large configuration (gpt2-large) and further train domain adapters for the DeBERTa model (He et al., 2021) in the base configuration (deberta-base). We obtain all models from the Huggingface Transformers library (Wolf et al., 2020). \nAdapter Training and Optimization. We train each domain adapter separately via language modeling (masked language modeling or causal language modeling, depending on the model) on a single NVIDIA A6000 GPU with 48 GB RAM."}, {"title": "3.2 Results", "content": "Combination Strategies. We compare the two combination strategies, parameter averaging, and ensembling, coupled with all four scoring strategies, applied for adapter selection and adapter weighting. The perplexities for gpt2-base and deberta-base are depicted in Figure 2. We show results for gpt2-large in the Appendix C. Note that for k = 0 and k = 1 (no adapter or a single adapter), the combination strategies are equivalent, as we do not need to merge any adapters. Interestingly, deberta-base hugely profits from adding a single adapter (improvement of up to -183662.70 in perplexity). Adding a second adapter does, on average, when averaging modules, no longer lead to an improvement. This warrants further investigation on when exactly the knowledge contained in an adapter helps (cf. \u00a74). From k = 2 on, ensembling leads to better domain adaptation across most model types and scoring strategies, indicated by lower model perplexities. These findings hold when choosing two adapters only (k = 2) and"}, {"title": "Scoring Strategies", "content": "We evaluate the effectiveness of the scoring strategies for weighting all 21 training adapters (see Table 2). Surprisingly, we observe that simpler (and previously ignored) approaches to determine the weighting, e.g., SENTSIM and TF-IDF, often lead to better results compared to more sophisticated approaches. However, for smaller numbers of adapters, the picture can vary (see again Figure 2). To shed more light on this phenomenon, we show the weights obtained"}, {"title": "Efficiency", "content": "A particular motivation for modularization is the re-usability of the individual modules - leading to a reduction of the environmental impact (Strubell et al., 2020; Hershcovich et al., 2022). Here, we discuss the efficiency of the combination strategies we test within our framework. As pointed out by Li et al. (2022), ensembling is intrinsically more expensive at inference time than"}, {"title": "4 Meta-Regression", "content": "In \u00a73, we have shown that adding more adapters (i.e., increasing k) often does not lead to performance gains, and that the effectiveness of the scoring strategies varies across models and evaluation domains. Motivated by these results, here, we analyze to what extent we are able to predict the expected performance for particular compositions."}, {"title": "4.1 Experimental Setup", "content": "Dataset and Evaluation. We run a meta-regression on our results obtained for each base model in \u00a73. We pre-process the data as follows: to account for variations in the scores, we average over the results obtained from the four random seeds for each evaluation domain. We account for the base differences in perplexity among the evaluation domains by computing the delta between the original model performance on this dataset and the perplexity obtained by using the composition, normalized by the original perplexity. We use 10-fold cross-validation and report the results in terms of Pearson and Spearman Correlation.\nFeatures. Each instance is represented by five feature groups: Adapter \u2013 the weights assigned to particular training adapters (0 if not chosen); Number of Adapters \u2013 the number of adapters involved in the composition; Combination Strategy - one-hot encoding of average or ensembling; Scoring Strategy \u2013 one-hot encodings of the scoring strategies (e.g., TF-IDF); and Evaluation Dataset one-hot encodings of the target domain.\nModels and Baselines. We experiment with Linear and Ridge regression. For Ridge, we perform hyperparameter tuning ($\\alpha$), leading to $\\alpha$ = 0 for gpt2-base, $\\alpha$ = 0.17 for deberta-base and $\\alpha$ = 0.06 for gpt2-large. We compare the results with a baseline predicting the mean relative difference per evaluation dataset. We hypothesize this to be a strong baseline, as the effectiveness of an adapter combination is highly dependent on the target domain.\nResults. Both models surpass the baseline (see Table 3), which, as expected, already reaches high scores. The highest scores are achieved with Ridge regression on the gpt2-base results (0.9641 Spearman). The results on deberta-base are the lowest, indicating the model type to be a relevant factor. Overall, we conclude that, dependent on the PLM, we are able to predict the effectiveness of domain adaptation with various compositions if metadata"}, {"title": "5 Related Work", "content": "We cover the related literature concerning the topics of knowledge modularization and knowledge composition. For a thorough overview of modular deep learning, we refer to Pfeiffer et al. (2023).\nModularizing Knowledge. Famously, Houlsby et al. (2019) proposed to use adapter layers (Rebuffi et al., 2017) as a more efficient alternative to full task-specific fine-tuning. Subsequently, researchers in NLP explored adapters for various purposes, e.g., domain adaptation (e.g., Glava\u0161 et al., 2021; Cooper Stickland et al., 2021; Hung et al., 2022; Malik et al., 2023), bias mitigation (e.g., Lauscher et al., 2021; Holtermann et al., 2022; Talat and Lauscher, 2022), language adaptation (e.g., Philip et al., 2020; \u00dcst\u00fcn et al., 2022), and for the injection of various other types of knowledge, such as common sense (Lauscher et al., 2020), factual (Wang et al., 2021a), and sociodemographic knowledge (Hung et al., 2023).\nSimilarly, much effort has been spent designing new adapter variants with the aim of further increasing their efficiency or effectiveness (e.g., Pfeiffer et al., 2021; Mahabadi et al., 2021; Zeng et al., 2023). Alternatives to adapters that support modularity include subnetworks (Guo et al., 2021)"}, {"title": "Composing Knowledge", "content": "The composition of knowledge modules can be conducted via optimizing additional parameters (e.g., Pfeiffer et al., 2021), or in a zero-shot manner (e.g., Chronopoulou et al., 2023). Falling under the first category of approaches, Pfeiffer et al. (2021) proposed the fusion of adapters based on weights obtained via learned attention matrices. The same mechanism has been adopted by Lu et al. (2021), dubbed knowledge controller. In a similar vein, Wang et al. (2021b) ensemble the output vectors of multiple language adapters and optimize the respective ensemble weights. Wang et al. (2022) and Muqeeth et al. (2023) compose MoE models by learning to route the input to the right modules. Most recently, Frohmann et al. (2023) propose to directly learn scaling parameters for efficient knowledge composition in task transfer. \nIn this work, we are interested in zero-shot knowledge composition. In this realm, Chronopoulou et al. (2023) rely on weight space averaging and simple selection strategies. Li et al. (2022) and Gururangan et al. (2023) compare ensembling and averaging for composing domain PLMs, relying on domain prior for selection. Until now, a unified view is missing."}, {"title": "6 Reproducibility Statement", "content": "The 31 domain datasets we used for training and testing our domain adapters are publicly available and commonly used in other domain adaptation research. This facilitates comparability of our results with previous and future approaches and fosters the reproducibility of our results."}, {"title": "7 Conclusion", "content": "In this work, we proposed a unified framework providing an interoperable notion of zero-shot knowledge composition. Using our framework, we analyzed the effectiveness of different knowledge module selection, weighting, and combination strategies. We studied the problem of domain adaptation with adapters and showed, for instance, that ensembling generally yields better results than parameter averaging. Examining five different scoring strategies, we found that even simple approaches can deliver strong results. Our findings also suggest that the number of adapters selected is generally more important than the weights assigned to them. While we have chosen the popular scenario of zero-shot domain adaptation with adapter layers, we are convinced that our framework is applicable to many other problems and modularization techniques (e.g., MoEs, entire models).\nOverall, we believe that our results will fuel future research in effective knowledge composition by providing a consolidated perspective on zero-shot module composition."}, {"title": "Limitations", "content": "Naturally, our work comes with a number of limitations. Most importantly, we conducted our experiments on the C4 dataset only. However, we strongly believe our main findings to hold also for other corpora designed for testing domain adaptation methods. Related to this aspect, our notion of domains follows the one employed in C4 and is restricted to source websites as domain representatives. Previous research has shown that this definition is not always sufficient to clearly delineate domain knowledge (e.g., Gururangan et al., 2023). Therefore, we advise practitioners to carefully choose the criteria for discriminating among domains that are most useful in their particular application scenario. Additionally, our validation relies primarily on perplexity as a measure for general NLU of PLMs. While perplexity provides a robust initial measure, it does not encapsulate all facets of language understanding and generation, and only serves as a proxy for the final downstream performance of the models. Last, we resorted to adapters as the, arguably, most popular modularization technique in our experiments. We did not test other modularization approaches (e.g., MoEs) due to the large number of additional experiments required and related environmental considerations. However, we strongly believe that our framework is general enough to provide useful guidance for the composition of various types of knowledge modularization techniques proposed in the literature."}, {"title": "Ethical Considerations", "content": "We also like to point to the ethical aspects touched by our work. First, as the large body of previous work on bias measurement demonstrates, PLMs are prone to encode and propagate stereotypical and exclusive biases present in their training data (e.g., Bolukbasi et al., 2016; Blodgett et al., 2020). The models we used in our experiments are not spared from this issue (Tal et al., 2022; Narayanan Venkit et al., 2023). We advise practitioners to use these models with the appropriate care and we refer to existing works (Liang et al., 2021; Lauscher et al., 2021) for discussions on bias mitigation. Second, central to our work are environmental considerations: experimentation with deep learning models potentially entails large amounts of CO2 emissions (Strubell et al., 2020). With our work, we hope to encourage further research on efficient NLP, in particular on modular learning and module composition, and, hence, to contribute to greener AI."}, {"title": "Appendix", "content": "A Link to Data, Models, Code Bases\nIn Table 4, we provide all information and links to the data, models, frameworks, and code bases we use in our work. All artifacts were used according to their intended use, as described in their licenses. As described in the main body of this manuscript, we are also releasing our code publicly (MIT License)."}, {"title": "B TF-IDF Equation", "content": "We determine the TF-IDF scores by:\ntfidf (t, d) = tf(t, d) * idf (t)\ntf(t,d) = $\\frac{ft,d}{\\sum_{t' \\in d} ft',d}$\nidf (t) = log ($\\frac{1+ N}{1 + df (t)}$ +1),\nwhere N is the total number of documents."}, {"title": "C Comparison of Combination Strategies", "content": "We evaluate the combination strategies for three different models. In Figure 6, we present the results for ensembling and parameter averaging for gpt2-large. Compared to the results for gpt2-base and deberta-base, which we showed in Figure 2, we did not run the experiments for all values for k between [0,10] because of the size of the model. However, we find very similar patterns in the variation of perplexity across the different strategies and number of adapters added as for gpt2-base. This reinforces the validity of our findings."}, {"title": "G Threshold Tuning via Early Stopping", "content": "In this additional experiment, we tried to estimate the optimal number of adapters to select by applying an early stopping algorithm, whenever we see a sudden drop in adapter similarity.\nFor this experiment, we use the weighting strategies using TF-IDF and SENTSIM, since these exhibited the largest variation in similarity weights. We then sort these weights from largest to smallest representing the adapter with the respective importance for the novel evaluation domain. We then iterate over the adapter weights and stop if the difference between the weights is larger than a certain threshold. We illustrate this procedure in Figure 14. We run several experiments with different values set for the stopping threshold (see Table 5) and find that with a threshold of 0.004, we are able to obtain on average over all datasets and combination strategies 79% of the optimal model performance."}]}