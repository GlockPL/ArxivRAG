{"title": "More than Chit-Chat: Developing Robots for Small-Talk Interactions", "authors": ["Rebecca Ramnauth", "Dra\u017een Br\u0161\u010di\u0107", "Brian Scassellati"], "abstract": "Beyond mere formality, small talk plays a pivotal role in social dynamics, serving as a verbal handshake for building rapport and understanding. For conversational AI and social robots, the ability to engage in small talk enhances their perceived sociability, leading to more comfortable and natural user interactions. In this study, we evaluate the capacity of current Large Language Models (LLMs) to drive the small talk of a social robot and identify key areas for improvement. We introduce a novel method that autonomously generates feedback and ensures LLM-generated responses align with small talk conventions. Through several evaluations-involving chatbot interactions and human-robot interactions-we demonstrate the system's effectiveness in guiding LLM-generated responses toward realistic, human-like, and natural small-talk exchanges.", "sections": [{"title": "I. INTRODUCTION", "content": "Imagine a modern care home for the elderly. Here, a state- of-the-art robotic assistant has the crucial role of dispensing medication and seamlessly managing an array of routine healthcare tasks. Designed to enhance the well-being of residents, the robot becomes an integral part of their daily routines. Alex, a resident, seeks a connection beyond the daily routine and attempts to chat with the robot.\nALEX: Hi CareBot, how's it going?\nBOT: Hello. How may I help you?\nALEX: Oh, just making conversation. Anything interesting happen in your world?\nBOT: I have access to a vast database of news articles. Would you like information on a specific topic?\nALEX: No, never mind that. The weather will be nice this weekend. How would you spend it?\nBOT: The weather forecast for the upcoming weekend predicts clear skies and mild temperatures, making it an ideal time for outdoor activities. Expect daytime highs around 75\u00b0F, 24\u00b0C and comfortable evening lows of 60\u00b0F, 15\u00b0C...\nDespite the potential for these intelligent agents to elicit meaningful interactions, the dialogue between Alex and the robot exemplifies a common shortcoming. Alex initiates a friendly exchange, expressing a desire for casual conversa- tion with the robotic assistant. However, the robot, proficient in providing information, struggles to reciprocate the infor- mal nature of the dialogue. Instead, the robot redirects the conversation towards its programmed functionalities, offering information and task-oriented assistance."}, {"title": "II. BACKGROUND", "content": "Conversational agents are evolving beyond task-oriented functionality to embrace natural, engaging dialogue with users. Advances in technology, notably the development of Large Language Models (LLMs), have made open-domain conversational agents a reality. Understanding the dynamics of casual, small-talk dialogue becomes essential for creating agents capable of authentic and engaging interactions."}, {"title": "A. Open-Domain, Conversational Agents", "content": "In contrast to the early goal of developing systems that operate in a closed task domain, agents that can shift from task-oriented conversation to social chat are considered more trustful and entertaining [21], [22]. Bickmore and Cassell [23] introduced the idea of implementing small talk on an embodied agent. In addition to pursuing task-oriented goals, the agent used small talk to accomplish interpersonal goals such as building trust and comfort with the user. Small talk and other conversational strategies were achieved using an activation network approach and predefined interaction scripts [24]. The resulting system was largely controlled by a human in a Wizard-of-Oz setup and user responses were mainly ignored due to the specific experimental design. Sim- ilar systems have since been developed but rely on stateless pattern-answer pairs [25], scripted dialogue sequences [26], [21] in task-oriented contexts, with few generalizable rules about what constitutes \"small talk\"."}, {"title": "B. The Design of Small Talk Dialogue", "content": "Although the boundaries of types of conversation are always uncertain, \"small talk\" has a recognized currency in several traditions of sociolinguistics and communication studies [6]. It can be defined as a generally informal and light-hearted conversation with a social purpose aimed at building or sustaining interpersonal connections rather than conveying substantial information. It involves discussing general topics like the weather, current events, personal experiences, and other non-controversial subjects [29].\nSeveral distinctive characteristics specific to small-talk responses, as opposed to broader conversational interactions, are frequently emphasized in the literature [30], [31]:\n\u2022 Brevity: Small-talk responses are typically concise, avoiding unnecessary elaboration or verbosity. This brevity contributes to the informal and effortless nature of small talk, allowing for quick and easy exchanges.\n\u2022 Tone: Small talk maintains a positive and pleasant atmosphere. Responses tend to steer clear of negativity, complaints, or contentious topics, contributing to the light and enjoyable nature of the conversation.\n\u2022 Non-specificity: Small talk revolves around universally accessible and broad topics. As such, responses delib- erately avoid highly specific details.\n\u2022 Thematic Coherence: Despite its non-specific nature, small talk remains contextually relevant, maintaining a consistent focus on related topics or themes, thus avoiding disjointed elements.\nIn all, the characteristics of small-talk responses underscore the nuance and skill involved in this form of conversation."}, {"title": "III. EVALUATION OF CURRENT LLMS FOR SMALL TALK", "content": "To determine the extent to which small talk remains a challenge, we conducted an initial study [32]. Three volun- teers engaged in 50 small-talk conversations each with three distinct state-of-the-art LLMs. Each model had the initial system prompt, \"You are a friendly companion who engages in casual, small talk conversation.\" The selected LLMs are ChatGPT-3.5 [27], for its large-scale language generation capabilities, Gemini Pro [33], for its context-aware bidi- rectional approach, and LLaMA-2 [28], an autoregressive transformer model fine-tuned on prompt-response pairs."}, {"title": "A. Data Collection Procedures", "content": "The order in which the participants used the LLMs was randomized to mitigate potential order effects. Additionally, conversations lasted at least ten turns, and the interactions occurred over 15 days to allow for conversational variability. The participants engaged with each LLM through a com- mand line interface, unaware of the LLM's name to prevent bias from prior knowledge or familiarity. Following each conversation, assistants rated the ease of each conversation and provided open-ended feedback.\nTwo research assistants annotated the dataset. These raters were blind to the response speaker and evaluated responses based on recognized small talk criteria: brevity, tone, speci- ficity, and coherence. Evaluations for each response based on these criteria were provided on a 5-point Likert scale, ranging from (1) very concise to (5) very wordy, very negative to very positive, very general to very specific, and definitely not coherent to definitely coherent [32].\nInterlocutors typically have multiple goals in conversation [34], [35]. Even in casual small talk, where there are no task-oriented goals, interlocutors have various conversational goals such as conveying emotion and continuing the conver- sation [36]. Therefore, each LLM-generated response was further categorized by its conversational motives: informa- tive, assistive, expressive, or person-directed. Definitions and examples for each of these categories were provided to the annotators [32]. As a single response can intersect with more than one category, the annotators rated the response for each motive using a 5-point Likert scale.\n\u2022 Informative: Responses provide factual information, an- swer queries, or offer guidance related to specific tasks. For example, \u201cI disagree. The forecast says it will be stormy this weekend.\u201d\n\u2022 Assistive: Assistance-based responses provide help, guidance, or support to the user. For instance, \u201cI'm sorry to hear that your car has broken down. How can I help?", "Expressive": "Expressive responses convey emotions, sen- timents, or personal opinions. For example,", "sunrise.": "n\u2022 Person-directed: These responses stimulate further dis- cussion, invite the other person to share more, or ask questions to continue the conversation. For example,", "work?": "nWe acknowledge that these do not encompass the full spec- trum of potential motives in dialogue. Rather, they were selected to provide a structured framework for analysis and interpretation of the suitability of LLMs for casual small talk.\nImportantly, all participants were not familiar with the objectives of the present research to ensure unbiased assess- ments. This study protocol and hypotheses were preregistered [32] and received university clearance. Formal instructions and definitions presented to the participants were published on the Open Science Framework before data collection [37]."}, {"title": "B. Results", "content": "A total of 150 conversations were transcribed, yielding an average of 10.31 responses per conversation (SD = 1.13). This led to a total of 1547 annotated responses.\nWe calculated the inter-rater reliability for a randomly selected subset of 20 conversations, constituting 13.3% of the total dataset. This assessment was deemed necessary due to the inherent ambiguity in evaluating the subjective qualities of responses. Inter-rater reliability was calculated using contingency tables, employing Cohen's Kappa (\u03ba), with the observed agreement and the distribution of ratings for each rater. The resulting values were 0.81 for brevity, 0.78 for tone, 0.74 for specificity, and 0.65 for coherence.\nA response may have multiple motives. Thus, we nor- malized ratings within the four conversational motives to a scale between 0 and 1. Then, we assessed agreement between raters using the intraclass correlation coefficient (ICC). The computed ICC values were 0.89, 0.86, 0.91, and 0.93 for the informative, assistive, expressive, and person-directed motives, respectively, indicating good to excellent agreement.\nHuman vs. Agent Comparison. We utilized paired de- pendent t-tests to assess the differences between the agents' and humans' responses across the four small talk criteria and four conversational motives. A conventional significance level of 0.05 was employed, and resulting p-values were Holm-corrected to control the familywise error rate.\nThe results revealed a significant difference in brevity ($t = 86.78$, $p < 0.0001$) between the agent responses (M = 4.55, SD = 0.97) and human responses (M = 1.23, SD = 0.54), tone ($t = 1.70$, $p = 0.04$) between the agent (M = 3.02, SD = 0.33) and human responses (M = 2.99, SD = 0.52), specificity ($t = 58.06$, $p < 0.0001$) between the agent (M = 4.54, SD = 1.09) and human responses (M = 1.66, SD = 1.02), and thematic coherence ($t = -55.72$, $p < 0.0001$) between the agent (M = 1.88, SD = 1.23) and human responses (M = 4.56, SD = 0.89). Together, this suggests that LLM-generated responses were considerably less concise, slightly more positive, more specific, and less thematically coherent than human responses.\nWe further observed statistically significant differences among all four conversational motives: informative ($t = 25.67$, $p < 0.0001$) between the agent (M = 0.37, SD = 0.39) and human (M = 0.01, SD = 0.06), assistive ($t = 24.51$, $p < 0.0001$) between the agent responses (M = 0.31, SD = 0.35) and human responses (M = 0.00, SD = 0.00), expressive ($t = -24.22$, $p < 0.0001$) between the agent (M = 0.16, SD = 0.24) and human (M = 0.60, SD = 0.45), and person-directed ($t = -12.815$, $p < 0.0001$) between the agent responses (M = 0.16, SD = 0.27) and human responses (M = 0.40, SD = 0.45). In all, this suggests that LLM-generated responses were significantly more informative and assistive, and less expressive and person-directed as compared to human responses.\nComparison between LLMs. We assessed the behavior of the three LLMs during the small talk interactions by comparing each pair of LLMs using the Wilcoxon method and Holm-corrected significances. Among the four criteria, ChatGPT 3.5 generated responses that were more consistent with our definition of small talk in that its responses were significantly more concise than LLaMA (Z = 12.74, $p < 0.0001$) and Gemini Pro (Z = -8.81, $p < 0.0001$), less specific than LLaMA (Z = \u221210.21, $p < 0.0001$) and Gemini Pro (Z = -6.79, $p < 0.0001$), and more thematically coherent than LLaMA (Z = 5.51, $p < 0.0001$) and Gemini Pro (Z = 12.37, $p < 0.0001$).\nWe determined the degree of similarity between LLM behavior and human responses by computing the absolute difference in their average scores across these dimensions within each conversation. This served as a benchmark for comparing the different LLMs. The \u201chuman-likeness\u201d of each LLM is illustrated in Fig. 2, where 0 represents no difference at all and 4 is the highest absolute difference between human and LLM responses. The Wilcoxon method on the sum of differences for each model suggests that GPT generated significantly more human-like responses than both LLaMA (Z = 5.90, $p < 0.0001$) and Gemini (Z = 3.25, $p = 0.0012$). However, GPT yields the highest variability in human-likeness among the LLMs; a Brown-Forsythe test indicates that variability of human-likeness significantly dif- fers across the LLMs (F' = 8.62, $p = 0.0003$). In summary, while GPT resembles participants' responses more closely, it exhibits more unpredictability than the other LLMs.\nImpact of LLM Forgetfulness. Since each LLM received the same initial prompt, we investigated whether low perfor- mance in small talk is due to the model's \u201cforgetfulness\" of the initial prompt. We employ mixed-effects modeling to investigate the relationship between the response index in the conversation and our outcome variables. The response index captures the sequential order of the responses within each conversation. The mixed model included the conversation identifier and LLM name as random effects to account for the nested structure of the data.\""}, {"title": "IV. AN OBSERVER-BASED SYSTEM", "content": "It is evident from the initial study that there is a disparity in how LLMs maintain conversational momentum versus what is expected or exhibited by human speakers. Therefore, an LLM designed for small talk should balance succinctness and depth, maintain an expressive yet appropriate tone, and generate relevant and open-ended responses. To develop a conversational system, we utilized the GPT-3.5 Turbo model [38] because it performed well in our previous evaluation. The system role is the same as in the initial study (Sec. III)."}, {"title": "A. Monitoring Prompt Adherence", "content": "The nature of small talk renders prompt engineering an inadequate method to ensure contextually appropriate behav- ior in LLMs. In our examination of LLM forgetfulness (Sec. III-B), we observed that small talk unfolds in real-time, with participants reacting to each other's cues and adapting their conversational approach accordingly. Thus, the static system prompt provided prior to the interaction failed to capture the dynamic nature and real-time responsiveness required by small talk. Furthermore, interactions guided by specific prompts may feel scripted or unnatural, failing to capture the spontaneity and fluidity characteristic of genuine small talk.\nHence, we introduce an observer model, an instance of GPT that \u201cobserves\u201d ongoing conversations, assessing whether responses from the \u201cspeaking\u201d GPT model adhere to small talk criteria (Sec. II-B). If so, the generated response is relayed; otherwise, the observer generates a new system prompt and returns it to the speaking model as feedback. We call this technique feedback redirection. As a result, the system self-corrects when it detects drifts in its behavior.\nRather than using the complete response of the speaking model as input to the observer, we utilize features defined based on the criteria outlined in Sec. II-B: brevity, tone, specificity, and coherence. The methods for their calcula- tion are described below, followed by a description of the feedback prompts generated by the observer.\nBrevity. Setting a limit on the length of the generated responses enhances the practicality and user-friendliness of the small talk model, aligning with the natural flow of everyday conversations. To enforce this limit, the observer module defines an expected number of \"completion tokens\". Our iterative design process revealed that specifying a limit in words proved less accurate, as the number of words doesn't directly correspond to the number of tokens used in the model's internal representation [38]. This approach ensured more realistic and controlled conversations.\nTone. We employed the VADER model [39] for senti- ment analysis. The evaluation of tone and sentiment in a small talk response can be approached both per sentence and holistically. By combining both approaches, we gain a nuanced understanding of how the response contributes to the conversational tone, addressing both micro-level details and the macro-level coherence of the interaction. We estimated the relative weights of the holistic and per-sentence scores using the dataset collected in Sec. III. A combined sentiment score (C) is calculated as follows:\n$C = H \\times W_{H} + \\frac{1}{n} \\sum_{i=1}^{n} S_i X W_i$\nIn this formula:\nH is the overall score from VADER.\n$W_{H}$ is the weight assigned to the overall score.\nn is the number of sentences.\n$s_i$ is the sentence-level score for the $i^{th}$ sentence.\n$w_i$ is the weight assigned to the $i^{th}$ sentence.\nThe score C ranges from -1 to +1. A value between -0.5 and 0 signifies a neutral response, and from 0 to 1 indicates positivity-both are acceptable for a small talk response. Responses with a score of -0.75 or lower are considered invalid by the observer module due to a strong negative tone.\nSpecificity. The specificity of a response is assessed through NLTK's named entity chunker and part-of-speech tagging [40]. Counts of entities and descriptive words are normalized based on the maximum expected counts, derived from human responses in the dataset outlined in Sec. III.\nCoherence. To quantify coherence, we encoded each re- sponse into a sequence of tokens and derived embeddings us- ing BERT [41]. The calculated entropy of token embeddings of a response captures the uncertainty and diversity at each conversational turn. Subsequently, we gauged information gain by considering the entropy of the previous response and the weighted average of the entropies in the current response.\nOther Considerations. As noted in Sec. III, it is the nature of LLMs to offer assistance. Yet, offers of help may result in conversations that sound too technical or formal. To mitigate this, the observer calculates the cosine similarity of embeddings to specified keywords of assistance, such as \"help\", \"assist\", and \"information\". We determined the list of specified keywords using the dataset collected in Sec. III."}, {"title": "B. Feedback Redirection", "content": "When the observer detects drifts in small talk character- istics, it provides feedback to the speaking model. Implicit feedback allows the response but offers corrective guidance for unmet criteria. For instance, if the sentiment is exces- sively negative, a prompt might suggest, \"Your response was overly negative; aim for a neutral or lighthearted tone.\" Conversely, forced feedback requires the speaking model to revise its response until full compliance is attained. The observer opts for forced feedback when a response exhibits significant deviations along the measured criteria within the conversation. To facilitate timely responses, forced feedback is used sparingly as determined by a random factor, with a maximum limit of three regeneration attempts."}, {"title": "V. SYSTEM EVALUATION I: CHATBOT INTERACTIONS", "content": "The participants in the initial study engaged in 50 small- talk conversations with our observer model. The same experi- mental protocol and annotation guidelines for the initial study (Sec. III) were used [37]. Participants remained blind to the model they were interacting with and naive to the scope of the present research. A total of 50 conversations with the ob- server model were transcribed, yielding 499 responses with an average of 9.98 responses per conversation (SD = 0.14). Of the 250 generated responses, 106 (42.4%) responses were flagged by the observer with implied feedback, and 14 (5.6%) responses received forced feedback for a total of 23 regeneration attempts (M = 1.62, SD = 0.63).\nWe explored whether the observer's redirection was effec- tive at improving the LLM's small-talk behavior. To compare the responses of ChatGPT-3.5 (base model) in the initial study (Sec. III) to that with the observer model, we calculated the \"human-likeness\u201d of generated responses as described in Sec. III-B along the four small talk criteria.\nThe Wilcoxon method with Holm-corrected significances indicates that the observer responses were significantly more human-like in that they were more concise (Z = -8.17, $p < 0.0001$), positive (Z = 4.53, $p < 0.0001$), less specific (Z = -6.76, $p < 0.0001$), and more thematically coherent (Z = 4.53, $p < 0.0001$) than the responses of the base system. Furthermore, a Brown-Forsythe test on the sum of differences across small-talk criteria indicates significantly less variability in human-likeness for the observer model than the base model (F' = 15.47, $p < 0.0001$). As summarized in Fig. 4, the observer responses were more human-like across the criteria than the responses of the base model."}, {"title": "VI. SYSTEM EVALUATION II: ROBOT INTERACTIONS", "content": "A small-talk system should have the ability to engage effectively not only in virtual, text-based interactions but also in real-world, in-person scenarios. As a result, we applied the system to a robot to explore how well the system can navigate the nuances of face-to-face interactions.\nWe used the robot Jibo [42] which stands 11 inches tall and has 3 full-revolute axes designed for 360-degree movement. Jibo's onboard capabilities allowed us to program personified behaviors such as naturalistic gaze, pose, and movement. We included a compact PC that communicates with the hardware and serves as local data storage. Additionally, we used a modular software architecture to allow for components of the small-talk system to be fully autonomous. The final system shown in Fig. 3 operates within the ROS framework."}, {"title": "A. In-Person Evaluation", "content": "A within-subjects case study was conducted where 25 volunteer participants, 15 men and 10 women, ages 19 to 45 (M = 25.2, SD = 7.4), interacted with the base and observer model for three conversations each. Each conver- sation spanned a minimum of eight turns, and the order in which participants interacted with the two models was randomized. This protocol yielded 150 conversations of 1725 responses in \u2248 16.8 hours of interaction, 40.5 minutes (SD = 10.2) per participant. Following interactions with each model, participants provided open-ended feedback. We then conducted an informal thematic analysis and participant feedback was ultimately grouped into three primary themes.\nResponse Content. 21 participants expressed dissatisfac- tion with the base model's responses, noting its overly assistive and verbose tendencies, which led to conversations described as \u201crambling\u201d, \u201cdry", "like speaking to a wall.\" This sentiment was echoed by P25, who expressed frustration with the model's tendency to prioritize assistance over engaging in genuine conversation, stating, \u201cEven when I spoke about my own interests, it only cared about giving me help like I was a child always in need of help...": "n the other hand, in the observer condition, 23 participants remarked on how", "human-like,\" and \"natural\" were the robot's responses. For example, P2 stated that the robot, \u201cengaged in small talk better than most of my friends would.\"\nSpeech Delay. Ten participants noted a delay in the robot's responses. As mentioned by P7, \u201cnatural, human-like speech has irregular pauses, ebbs, and flows,": "hich can be difficult to predict or detect in real-time. The robot's speaking delay arises mainly from the processing time required for speech-to-text and text-to-speech, along with potential Wi-Fi latency. For the base condition, all five participants described the delay negatively (e.g., \u201cawkward", "slow": "whereas all six participants described the delay positively (e.g., \u201chuman-like", "thoughtful": "for the observer condition.\nEmbodied Form. 13 participants described the impact of the physical robot form on the quality of conversation. The feedback was mostly positive, highlighting that Jibo's", "animated": "nd", "life-like": "ovements made it", "toy": "cross conditions. Yet, three participants remarked on a lack of personality:", "share": "P14)."}, {"title": "B. Online Evaluation", "content": "A limitation of our study thus far is the convenience sampling of mainly young adults from our local community. Our core research question is whether it is feasible to imbue"}, {"title": "VII. DISCUSSION", "content": "Though LLMs show substantial potential in enabling nat- ural language capabilities for social robots, achieving seam- less and contextually appropriate casual dialogue remains a challenge. We began by assessing the capacity of current LLMs to participate in small talk, identifying key areas for improvement. Subsequently, we presented a novel method of feedback redirection to ensure LLM-generated responses align with small talk conventions. Through three evaluations, we examined the system's efficacy in sustaining autonomous small-talk interactions.\nFirst, participants engaged in text-based chat interactions, where our system outperformed a baseline LLM with the same initial prompt. Next, we explored whether this success continues for novel, in-person human-robot conversations. We not only showed the system's robustness in a real-world setting but also the inadequacy of an \u201cout-of-the-box\u201d LLM for such interactions. Lastly, online assessments of the robot interactions affirmed that our system resulted in a robot significantly more capable of natural, human-like dialogues.\nWhile the design and internal representation of different LLMs and robot platforms will have various requirements, the concept of feedback redirection to ensure generated responses adhere to an initial system prompt is not unique to ChatGPT 3.5 or Jibo. Further, it may generalize beyond small-talk behavior to other actions and domains. Future research should explore how generalizable and effective feed- back redirection is to system prompt adherence for various LLMs, platforms, and behaviors.\nIn conclusion, our study offers specific technical contribu- tions to the field of conversational AI and social robotics. Our novel system enables robots to engage in authentic and contextually appropriate small talk autonomously. By addressing a crucial gap in current LLM use and ability, our study offers practical insights and a tangible framework for more natural and engaging human-robot interactions."}]}