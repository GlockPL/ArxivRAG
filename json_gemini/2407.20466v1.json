{"title": "A Method for Fast Autonomy Transfer in Reinforcement Learning", "authors": ["Dinuka Sahabandu", "Bhaskar Ramasubramanian", "Michail Alexiou", "J. Sukarno Mertoguno", "Linda Bushnell", "Radha Poovendran"], "abstract": "This paper introduces a novel reinforcement learning (RL) strategy designed to facilitate rapid autonomy transfer by utilizing pre-trained critic value functions from multiple environments. Unlike traditional methods that require extensive retraining or fine-tuning, our approach integrates existing knowledge, enabling an RL agent to adapt swiftly to new settings without requiring extensive computational resources. Our contributions include development of the Multi-critic Actor-Critic (MCAC) algorithm, establishing its convergence, and empirical evidence demonstrating its efficacy. Our experimental results show that MCAC significantly outperforms the baseline Actor-Critic (AC) algorithm, achieving up to 22.76\u00d7 faster autonomy transfer and higher reward accumulation. This advancement underscores the potential of leveraging accumulated knowledge for efficient adaptation in RL applications.", "sections": [{"title": "I. INTRODUCTION", "content": "In the realms of artificial intelligence and data-driven control, reinforcement learning (RL) stands out as a powerful paradigm for solving complex decision-making problems in unknown or unseen environments [1]. RL involves an agent learning to make decisions by interacting with an environment, by aiming to maximize a cumulative reward signal provided by the environment over time. This process of trial and error, coupled with the feedback received through rewards, allows the agent to develop a strategy or policy that guides its actions towards achieving its goals.\nRL has been successfully applied in multiple domains, including robotics, autonomous vehicles, games, and mobile networks [2]\u2013[8]. Despite these successes, RL faces a significant challenge when an agent encounters new environments. In such a scenario, the agent must learn from the very beginning in each new setting, a process that can be prohibitively time-consuming and resource-intensive. The ability to transfer knowledge-utilizing insights gained from previous experiences to expedite learning in new situations-has therefore become a key area of focus. Knowledge transfer can dramatically reduce the amount of interaction needed with a new environment to reach optimal or near-optimal performance, thereby accelerating the learning process and reducing computational costs.\nYet, effectively achieving this transfer of knowledge poses challenges. In deep RL [9], where agents use deep neural networks to approximate policies or value functions, techniques such as transfer learning and fine-tuning have shown promise [10], [11]. While transfer learning and fine-tuning techniques offer some solutions, they typically involve re-training parts of the network. This process, albeit not as extensive as retraining the entire network, still demands considerable computational effort and time. On the other hand, traditional RL paradigms operating in discrete environments which use a tabular approach to storing value functions (e.g., Q-learning [12], presents a different challenge- how can pre-learned knowledge be leveraged in discrete environments without complex neural network architectures used in deep RL?.\nIn this paper, we investigate the concept of rapid autonomy transfer within the scope of classical reinforcement learning. Specifically, we examine this problem through the lens of Actor-Critic (AC) algorithms under a discounted reward framework. AC methods combine policy-based decision making (actor) with value-based evaluation (critic), offering a balanced approach to learning by directly integrating feedback on the quality of actions taken [13]. Actor-Critic methods are distinguished by their rapid convergence properties and minimal convergence errors, closely approaching optimal performance. The discounted reward setting, central to our analysis, emphasizes the importance of future rewards by applying a discount factor, which ensures that immediate gains do not overshadow long-term benefits.\nWe propose a novel approach to enable rapid autonomy transfer in RL that uses pre-trained critic value functions from multiple environments and combines them to assist in training a new actor for a different environment. Our approach bypasses the need for retraining or partially training critics by focusing on the optimal integration of existing knowledge, effectively leveraging the knowledge accumulated from multiple critics to navigate new environments.\nSpecifically, our Multi-critic Actor-Critic (MCAC) learns a set of weights to modulate influence of pre-trained value functions from diverse environments. By leveraging pre-existing value functions without the need for retraining, MCAC not only conserves computational resources but also accelerates adaptation to new environments. This contribution marks a significant advancement in the application of RL techniques, opening up new possibilities for their use in diverse and dynamic settings. We make the following contributions in this paper:\nWe introduce the MCAC algorithm to enable fast autonomy transfer in RL.\nWe establish convergence of weights and policies learned by our MCAC algorithm."}, {"title": "II. RELATED WORK", "content": "This section briefly describes related work in multi-critic learning, and methods to aggregate information from critics.\nMulti-Critic Actor Learning was proposed in [14] in the context of multi-task learning as an alternative to maintaining separate critics for each task being trained while training a single multi-task actor. Explicitly distinguishing between tasks also eliminates a need for critics to learn to do so and mitigates interference between task-value estimates. However, while [14] seeks to 'select' a particular style of task satisfaction in a given environment, our MCAC approach 'combines' multiple styles of task satisfaction to execute a given task in a new environment. Recently, the authors of [15] introduced a multi-actor mechanism that combined novel exploration strategies together with a Q-value weighting technique to accelerate learning of optimal behaviors. Different from [15], where experiments for any given environment provide comparisons against benchmarks within the same environment, our goal in this paper is to accomplish autonomy transfer across different environments.\nReward machines [16] provide a structured framework for defining, formalizing, and managing rewards in an intuitive and human-understandable manner. Structuring rewards in this way makes it possible to guide RL agents more effectively. Reward machines were used to inform design of a Q-learning algorithm in finite state-action environments in [17], and extended to partially observable environments in [18]. However, the current state-of-the-art in this domain does not provide a direct way to encode and aggregate information from disparate sources and do not have a mechanism to encode distracting rewards. In comparison, our MCAC technique aggregates information from multiple scenarios and uses this information to achieve significant speedup in terms of run time and number of learning episodes required."}, {"title": "III. PRELIMINARIES", "content": "This section introduces necessary preliminaries on Markov decision processes (MDPs), reinforcement learning (RL), and stochastic approximation (SA).\n\nA. MDPs and RL\nLet $(\\Omega, \\mathcal{F}, \\mathbb{P})$ denote a probability space, where $\\Omega$ is a sample space, $\\mathcal{F}$ is a $\\sigma$-algebra of subsets of $\\Omega$, and $\\mathbb{P}$ is a probability measure on $\\mathcal{F}$. A random variable is a map $Y: \\Omega \\rightarrow \\mathbb{R}$. We assume that the environment of the RL agent is described by a Markov decision process (MDP) [19].\nDefinition 1. An MDP is a tuple $\\mathcal{M} := (\\mathcal{S}, \\mathcal{A}, \\mathcal{P}, r, \\gamma)$, where $\\mathcal{S}$ is a finite set of states, $\\mathcal{A}$ is a finite set of actions, and $\\mathcal{P}(s'|s,a)$ is the probability of transiting to state $s'$ when action $a$ is taken in state $s$. $r : \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathbb{R}$ is the reward obtained by the agent when it takes action $a$ in state $s$. $\\gamma \\in (0,1]$ is a discounting factor.\nAn RL agent typically does not have knowledge of $\\mathcal{P}$. Instead, it obtains a (finite) reward $r$ for each action that it takes. Through repeated interactions with the environment, the agent seeks to learn a policy $\\pi$ in order to maximize an objective $\\mathbb{E}_{\\pi} \\left[\\sum_{t} \\gamma^{t} r(s_{t}, a_{t})\\right]$ [1]. Let $\\Pi$ represent the set of stationary policies of the agent. A policy $\\pi \\in \\Pi$ is considered a deterministic stationary policy if $\\pi \\in \\{0,1\\}^{\\|\\mathcal{A}\\|}$. It is said to be a stochastic stationary policy if $\\pi \\in [0, 1]^{\\|\\mathcal{A}\\|}$.\n\nB. Stochastic Approximation (SA) Algorithms\nLet $h: \\mathbb{R}^{m} \\times x \\rightarrow \\mathbb{R}^{m}$ be a continuous function of parameters $x \\in \\mathbb{R}^{m}$. SA algorithms are designed to solve equations of the form $h(x) = 0$ based on noisy measurements of $h(x)$. The classical SA algorithm is given by:\n$$x_{t+1} = x_{t} + \\delta_{t}[h(x_{t}) + n_{t}], \\text{ for } t \\geq 0.$$\nHere, $t$ denotes the iteration index, and $x_{t}$ represents the estimate of $x$ at the $t$th iteration. The term $n_{t}$ represents the zero-mean measurement noise associated with $x_{t}$, and $\\delta_{t}$ denotes the learning rate. The stationary points of Eq. (1) coincide with the solutions of $h(x) = 0$ in the absence of noise ($n_{t} = 0$). The convergence of SA algorithms is typically analyzed through their associated Ordinary Differential Equations (ODEs), represented as $\\dot{x} = h(x)$.\nThe convergence of an SA algorithm requires specific assumptions on the learning rate $\\delta_{t}$:\nAssumption 1. $\\sum_{t=0}^{\\infty} \\delta_{t} = \\infty$ and $\\sum_{t=0}^{\\infty}(\\delta_{t})^{2} < \\infty$.\nExamples of $\\delta_{t}$ satisfying Assumption 1 include $\\delta_{t} = 1/t$ and $\\delta_{t} = 1/(t \\log(t))$. A general convergence result for SA algorithms is stated below.\nProposition 1 ( [20], [21]). Consider an SA algorithm in the following form defined over a set of parameters $x \\in \\mathbb{R}^{m}$ and a continuous function $h : \\mathbb{R}^{m} \\rightarrow \\mathbb{R}^{m}$.\n$$x_{t+1} = \\Theta(x_{t} + \\delta_{t}[h(x_{t}) + n_{t} + \\kappa_{t}]), \\text{ for } t \\geq 0,$$ where $\\Theta$ is a projection operator that projects each $x_{t}$ iterate onto a compact and convex set $\\Lambda \\in \\mathbb{R}^{m}$ and $\\kappa_{t}$ is a bounded random sequence. Let the ODE associated with Eqn. (2) be\n$$\\dot{x} = \\Theta(h(x)),$$\nwhere $\\Theta(h(x)) = \\lim_{\\eta \\downarrow 0} \\frac{\\Theta(x+\\eta h(x)) - x}{\\eta}$ and $\\Theta$ denotes a projection operator that restricts evolution of Eqn. (3) to the set $\\Lambda$. Let the nonempty compact set $\\mathcal{X}$ be a set of asymptotically stable equilibrium points of Eqn. (3). Then $x_{t}$ converges almost surely to a point in $\\mathcal{X}$ as $n \\rightarrow \\infty$ if\nI) $\\delta_{t}$ satisfies the conditions in Assumption 1.\nII) $\\lim_{t \\rightarrow \\infty} \\left(\\sup_{l \\geq t} \\sum_{l=t}^{t} \\delta_{l} \\eta_{l}\\right) = 0$ almost surely.\nIII) $\\lim_{t \\rightarrow \\infty} \\kappa_{t} = 0$ almost surely."}, {"title": "IV. THE MULTI-CRITIC ACTOR CRITIC", "content": "In this section we introduce and describe the design of our multi-critic actor-critic (MCAC) algorithm to enable fast autonomy transfer in RL using pre-trained critics. To aid readability, we provide a description of notations in Table I.\nOur intuition for the MCAC algorithm is grounded in the principles of ensemble learning in machine learning [22] and model soups in large language models [23]. These solutions utilize multiple trained models to achieve better performance with significantly fewer resources and and in shorter time. Following a similar line of thought, we hypothesize that using multiple pre-trained critic values from training environments under different conditions (e.g., different obstacle orientations) can enhance performance and achieve faster convergence in actor-critic algorithms trained for a new environment with varied conditions.\nSpecifically, given access to a set of pretrained critic value functions, the MCAC approach aims to bypass the process of training a critic for the new environment from scratch, opting instead to use a weighted sum of the existing knowledge from the pretrained critic values for estimating the value function in the current environment. A weighted average of pre-trained critic value functions will provide a better initial condition for the value function estimated for the new environment. This, in turn, will improve the agent's exploration and achieve better performance in terms of the accumulated reward at convergence. Further, since MCAC will not require training new value functions, but rather focuses on finding the best set of weights to compute the weighted average of the pre-trained value functions of critics to estimate the value function of the current environment, it reduces the number of trainable parameters related to critics from $\\|\\mathcal{S}\\|$ to $N$, where $\\|\\mathcal{S}\\|$ is the cardinality of the state space of the environment, and $N$ is the number of pre-trained critic value functions available in the new environment. Typically, $N \\ll \\|\\mathcal{S}\\|$, which aids the agent in achieving significant speedup in training. Additionally, starting from a better initial condition, as mentioned previously, will also help the agent learn faster compared to learning from scratch.\nIn order to setup the MCAC approach and algorithm, we first establish an assumption regarding the availability of pre-trained value functions, derived using the AC algorithm from various environment settings.\nLet $V_{i}(s)$ be the value function of the $i$th pre-trained critic at state $s \\in \\mathcal{S}$. Then define the vector $V_{i} := [V_{i}(s)]_{s \\in \\mathcal{S}}$.\nAssumption 2. Converged value vectors $V_{i}$ for $i = 1,2,..., N$ pre-trained critics are readily available for the training the actor (i.e., agent) using the MCAC algorithm.\nLet $W = [W_{i}]_{i=1}^{N}$ be the weight vector where each weight $w_{i}$ is associated with a pre-trained value function $V_{i}$ of critic $i$. Further, let $\\sum_{i=1}^{N} w_{i} = 1$ and $w_{i} \\geq 0$ for all $i \\in \\{1,2,..., N\\}$. Then, the value vector $V = [V(s)]_{s \\in \\mathcal{S}}$ of the actor is defined by\n$$V = \\sum_{i=1}^{N} w_{i} [V_{i}(s)].$$\nThe temporal difference (TD) error associated with the trained actor at state $s \\in \\mathcal{S}$ when taking action $a \\in \\mathcal{A}$ is:\n$$\\delta(s) = \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}(s, a, s')\\left[r(s, a, s') + \\gamma V(s') \\right] - V(s),$$\nwhere $0 < \\gamma < 1$ denotes the discount factor. In actor-critic (AC) RL algorithms [1], the TD error represents the difference between the predicted reward and the actual reward obtained. The TD error is used to update the value function and the policy in AC algorithms. A high TD error indicates that the predictions are quite different from the actual outcomes, suggesting that the actor's value function and the policy need significant updating. Conversely, a low TD error suggests that the actor's predictions are accurate. Minimizing the TD error for the trained actor can be expressed as the following optimization problem:\n$$\\min_{W, \\pi} \\sum_{s \\in \\mathcal{S}} \\left[V(s) - \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}(s, a, s')[V(s') - r(s, a, s')]\\right]$$\nsubject to:\n$$V(s) - \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}(s, a, s')[\\hat{V}(s') - r(s, a, s')] \\geq 0, \\forall s \\in \\mathcal{S}, a \\in \\mathcal{A},$$\n$$V(s) = \\sum_{i=1}^{N} w_{i}V_{i}(s), \\forall s \\in \\mathcal{S}$$\n$$\\sum_{a \\in \\mathcal{A}} \\pi(s, a) = 1, \\forall s \\in \\mathcal{S}$$\n$$\\pi(s, a) \\geq 0, \\forall s \\in \\mathcal{S} \\text{ and } a \\in \\mathcal{A}$$\nRemark 1. If the above optimization problem was framed within a typical actor-critic (AC) framework, then the minimization would be performed over V and \u03c0, while omitting"}, {"title": "V. EXPERIMENTS", "content": "This section presents two case studies to compare our proposed multi-critic actor-critic (MCAC) algorithm against a baseline actor-critic (Baseline AC). We first present the details of the two case study scenarios developed for comparing the two algorithms. Next we describe the hyper parameters used in Baseline AC and MCAC algorithms and define metrics used to compare both algorithms. We then present a comparison of results from the Baseline AC and our MCAC algorithm on two case study scenarios.\n\nA. Case Study Environments\nWe demonstrate effectiveness of our MCAC algorithm through experiments conducted in two distinct grid world environments of dimensions 5\u00d75 (Case Study 1) and 16\u00d716 (Case Study 2). Our experiments evaluate capabilities of RL agents in scenarios of varying complexity. Each grid world is populated with obstacles and potentially hazardous areas, challenging the agent to learn to navigate effectively to a goal location, starting from an initial position. In the 5 \u00d7 5 grid, 1 \u00d7 1 obstacles are placed at predefined grid locations; the 16 \u00d7 16 grid features a more complex arrangement of obstacles, with 4\u00d74 obstacles placed at predefined locations. Details about the grid environments are presented below.\nState Space: The state space, $\\mathcal{S}$, of our grid-worlds is defined by the discrete positions that an agent can occupy on a grid. For a grid of size $L \\times L$, the state space has $L^{2}$ possible positions, each uniquely identified by its grid coordinates $(x, y)$. We examine 5 \u00d7 5 and 16 \u00d7 16, grids leading to state spaces of 25 and 256 distinct states, respectively.\nAction Space: The action space, $\\mathcal{A}$, available to the agent consists of four deterministic actions: move up $(\\uparrow)$, move down $(\\downarrow)$, move left $(\\leftarrow)$, and move right $(\\rightarrow)$. When the agent takes an action in a certain grid position (state), it moves one position in that direction, unless there is something in the way (e.g., obstacles, edge of grid). When the agent attempts to move beyond the grid's edges, the agent's position remains unchanged.\nTransition Probabilities: The transition probabilities, $\\mathcal{P}(s'|s, a)$, describe the likelihood of moving from state $s$ to state $s'$ given action $a$. In our grid-worlds, if there is no obstacle, these probabilities are either 0 or 1; a probability of 1 indicates that action $a$ taken in state $s$ will always result in transition to state $s'$, and 0 otherwise. When the agent is in an obstacle state, the agent will stay in the obstacle state with probability $p_{stay} = 0.75$ and move to the state suggested by the action with probability $1 - p_{stay} = 0.25$.\nReward Structure The agent receives reward $r(s, a, s')$ for each transition from state $s$ to state $s'$ via action $a$. Specifically, $r(s, a, s') = 100$ when the agent takes an action that reaches the goal state, and $r(s, a, s') = -1$ for every other action taken by the agent. This reward structure aims to balance exploration and exploitation by incentivizing the discovery of the shortest path to the goal.\n\nB. Experiment Setup\nFor Case Study 1, we first train the AC algorithm on three pre-trained scenarios given in the top row of Fig. 1 and save the converged value functions at each state for each scenario. Then we run both AC and MCAC algorithms on the four deployment scenarios shown in the bottom row of Fig. 1. We use initial state 1 and goal state 25 for both pre-trained and deployment scenarios corresponding to Case Study 1. NThe deployment scenarios are obtained by the union of two or more of the obstacles presented in the pre-trained scenarios. For Case Study 2, we first train the AC algorithm on four pre-trained scenarios given in the top row of Fig. 2 and save the converged value functions at each state for each scenario. Then we run both AC and MCAC algorithms on deployment scenarios shown in the bottom row of Fig. 2. Case Study 2 presents more challenging tasks for the MCAC algorithm due to randomly generated obstacles that increase in complexity across the deployment scenarios.\nFor both environments, we run the AC and MCAC algo-rithms for 100 episodes, and we conduct 100 independent experiments for each of the pre-trained and deployment scenarios. An episode is defined as the agent starting from the initial state and reaching the goal state.\n\nC. Metrics\nWe utilize the following metrics to compare performance of our MCAC algorithm with the AC algorithm. All averages are over 100 independent experiments.\nAverage Total Reward: The total reward accumulated by the agent in the 100th episode."}, {"title": "VI. CONCLUSION", "content": "This paper developed a novel algorithm to facilitate fast autonomy transfer in reinforcement learning. Compared to methods that require retraining of parameters when operating in new environments, our multi-critic actor-critic (MCAC) algorithm used knowledge of pretrained value functions and integrated these using a weighted average to expedite adaptation to new environments. We demonstrated that MCAC achieved higher rewards and significantly faster (up to 22.76x) autonomy transfer compared to a baseline actor-critic algorithm across multiple deployment scenarios in two distinct enrvironments. The success of MCAC underscores significance of knowledge transfer in reinforcement learning, offering a promising direction for future research in developing more adaptable and efficient learning systems."}]}