{"title": "A Method for Fast Autonomy Transfer in Reinforcement Learning", "authors": ["Dinuka Sahabandu", "Bhaskar Ramasubramanian", "Michail Alexiou", "J. Sukarno Mertoguno", "Linda Bushnell", "Radha Poovendran"], "abstract": "This paper introduces a novel reinforcement learn-ing (RL) strategy designed to facilitate rapid autonomy transferby utilizing pre-trained critic value functions from multipleenvironments. Unlike traditional methods that require extensiveretraining or fine-tuning, our approach integrates existingknowledge, enabling an RL agent to adapt swiftly to new set-tings without requiring extensive computational resources. Ourcontributions include development of the Multi-critic Actor-Critic (MCAC) algorithm, establishing its convergence, andempirical evidence demonstrating its efficacy. Our experimentalresults show that MCAC significantly outperforms the baselineActor-Critic (AC) algorithm, achieving up to 22.76\u00d7 fasterautonomy transfer and higher reward accumulation. This ad-vancement underscores the potential of leveraging accumulatedknowledge for efficient adaptation in RL applications.", "sections": [{"title": "I. INTRODUCTION", "content": "In the realms of artificial intelligence and data-drivencontrol, reinforcement learning (RL) stands out as a powerfulparadigm for solving complex decision-making problemsin unknown or unseen environments [1]. RL involves anagent learning to make decisions by interacting with anenvironment, by aiming to maximize a cumulative rewardsignal provided by the environment over time. This processof trial and error, coupled with the feedback received throughrewards, allows the agent to develop a strategy or policy thatguides its actions towards achieving its goals.\nRL has been successfully applied in multiple domains,including robotics, autonomous vehicles, games, and mobilenetworks [2]\u2013[8]. Despite these successes, RL faces a signif-icant challenge when an agent encounters new environments.In such a scenario, the agent must learn from the verybeginning in each new setting, a process that can be pro-hibitively time-consuming and resource-intensive. The abilityto transfer knowledge\u2014utilizing insights gained from previ-ous experiences to expedite learning in new situations\u2014hastherefore become a key area of focus. Knowledge transfercan dramatically reduce the amount of interaction neededwith a new environment to reach optimal or near-optimalperformance, thereby accelerating the learning process andreducing computational costs.\nYet, effectively achieving this transfer of knowledge poseschallenges. In deep RL [9], where agents use deep neural net-works to approximate policies or value functions, techniquessuch as transfer learning and fine-tuning have shown promise[10], [11]. While transfer learning and fine-tuning techniquesoffer some solutions, they typically involve re-training partsof the network. This process, albeit not as extensive asretraining the entire network, still demands considerablecomputational effort and time. On the other hand, traditionalRL paradigms operating in discrete environments which usea tabular approach to storing value functions (e.g., Q-learning[12], presents a different challenge\u2014 how can pre-learnedknowledge be leveraged in discrete environments withoutcomplex neural network architectures used in deep RL?.\nIn this paper, we investigate the concept of rapid autonomytransfer within the scope of classical reinforcement learning.Specifically, we examine this problem through the lens ofActor-Critic (AC) algorithms under a discounted rewardframework. AC methods combine policy-based decisionmaking (actor) with value-based evaluation (critic), offeringa balanced approach to learning by directly integratingfeedback on the quality of actions taken [13]. Actor-Criticmethods are distinguished by their rapid convergence prop-erties and minimal convergence errors, closely approachingoptimal performance. The discounted reward setting, centralto our analysis, emphasizes the importance of future rewardsby applying a discount factor, which ensures that immediategains do not overshadow long-term benefits.\nWe propose a novel approach to enable rapid autonomytransfer in RL that uses pre-trained critic value functionsfrom multiple environments and combines them to assistin training a new actor for a different environment. Ourapproach bypasses the need for retraining or partially trainingcritics by focusing on the optimal integration of existingknowledge, effectively leveraging the knowledge accumu-lated from multiple critics to navigate new environments.\nSpecifically, our Multi-critic Actor-Critic (MCAC)learns a set of weights to modulate influence of pre-trainedvalue functions from diverse environments. By leveragingpre-existing value functions without the need for retraining,MCAC not only conserves computational resources but alsoaccelerates adaptation to new environments. This contribu-tion marks a significant advancement in the application ofRL techniques, opening up new possibilities for their usein diverse and dynamic settings. We make the followingcontributions in this paper:\n\u2022 We introduce the MCAC algorithm to enable fast au-tonomy transfer in RL.\n\u2022 We establish convergence of weights and policieslearned by our MCAC algorithm."}, {"title": "II. RELATED WORK", "content": "This section briefly describes related work in multi-criticlearning, and methods to aggregate information from critics.\nMulti-Critic Actor Learning was proposed in [14] in thecontext of multi-task learning as an alternative to maintainingseparate critics for each task being trained while traininga single multi-task actor. Explicitly distinguishing betweentasks also eliminates a need for critics to learn to doand mitigates interference between task-value estimates.\nHowever, while [14] seeks to 'select' a particular styleof task satisfaction in a given environment, our MCACapproach 'combines' multiple styles of task satisfaction toexecute a given task in a new environment. Recently, theauthors of [15] introduced a multi-actor mechanism thatcombined novel exploration strategies together with a Q-value weighting technique to accelerate learning of optimalbehaviors. Different from [15], where experiments for anygiven environment provide comparisons against benchmarkswithin the same environment, our goal in this paper is toaccomplish autonomy transfer across different environments.\nReward machines [16] provide a structured framework fordefining, formalizing, and managing rewards in an intuitiveand human-understandable manner. Structuring rewards inthis way makes it possible to guide RL agents more effec-tively. Reward machines were used to inform design of a Q-learning algorithm in finite state-action environments in [17],and extended to partially observable environments in [18].\nHowever, the current state-of-the-art in this domain does notprovide a direct way to encode and aggregate informationfrom disparate sources and do not have a mechanism toencode distracting rewards. In comparison, our MCAC tech-nique aggregates information from multiple scenarios anduses this information to achieve significant speedup in termsof run time and number of learning episodes required."}, {"title": "III. PRELIMINARIES", "content": "This section introduces necessary preliminaries on Markovdecision processes (MDPs), reinforcement learning (RL), andstochastic approximation (SA).\nA. MDPs and RL\nLet $(\u03a9, \\mathcal{F}, \\mathbb{P})$ denote a probability space, where \u03a9 is asample space, $\\mathcal{F}$ is a \u03c3-algebra of subsets of \u03a9, and$\\mathbb{P}$ is a probability measure on $\\mathcal{F}$. A random variable is a map$Y: \u03a9 \\rightarrow \\mathbb{R}$. We assume that the environment of the RL agentis described by a Markov decision process (MDP) [19].\nDefinition 1. An MDP is a tuple $M := (\\mathcal{S}, \\mathcal{A}, P, r, \u03b3)$, where$\\mathcal{S}$ is a finite set of states, $\\mathcal{A}$ is a finite set of actions, and$P(s'|s, a)$ is the probability of transiting to state $s'$ whenaction $a$ is taken in state $s$. $r : \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathbb{R}$ is the rewardobtained by the agent when it takes action $a$ in state $s$. $\u03b3 \u2208(0, 1]$ is a discounting factor.\nAn RL agent typically does not have knowledge of $P$.Instead, it obtains a (finite) reward $r$ for each action that ittakes. Through repeated interactions with the environment,the agent seeks to learn a policy \u03c0 in order to maximize anobjective $E_\u03c0[\\sum_t \u03b3^t r(s_t, a_t)]$ [1]. Let \u03a0 represent the set ofstationary policies of the agent. A policy $\u03c0 \u2208 \u03a0$ is considereda deterministic stationary policy if $\u03c0 \u2208 {0, 1}^{|\\mathcal{A}|}$. It is saidto be a stochastic stationary policy if $\u03c0 \u2208 [0, 1]^{|\\mathcal{A}|}$.\nB. Stochastic Approximation (SA) Algorithms\nLet $h : \\mathbb{R}^{m \u00d7 t} \\rightarrow \\mathbb{R}^{m \u00d7 t}$ be a continuous function of parame-ters $x \u2208 \\mathbb{R}^{m \u00d7 t}$. SA algorithms are designed to solve equationsof the form $h(x) = 0$ based on noisy measurements of $h(x)$.\nThe classical SA algorithm is given by:\n$x_{t+1} = x_t + \u03b4_t[h(x_t) + n_t]$, for $t \u2265 0$.  (1)\nHere, $t$ denotes the iteration index, and $x_t$ represents theestimate of $x$ at the $t^{th}$ iteration. The term $n_t$ representsthe zero-mean measurement noise associated with $x_t$, and$\u03b4_t$ denotes the learning rate. The stationary points of Eq. (1)coincide with the solutions of $h(x) = 0$ in the absence ofnoise ($n_t = 0$). The convergence of SA algorithms is typi-cally analyzed through their associated Ordinary DifferentialEquations (ODEs), represented as $\\dot{x} = h(x)$.\nThe convergence of an SA algorithm requires specificassumptions on the learning rate $\u03b4_t$:\nAssumption 1. $\\sum_{t=0}^\u221e \u03b4_t = \u221e$ and $\\sum_{t=0}^\u221e (\u03b4_t)^2 < \u221e$.\nExamples of $\u03b4_t$ satisfying Assumption 1 include $\u03b4_t = 1/t$and $\u03b4_t = 1/(t \\log(t))$. A general convergence result for SAalgorithms is stated below.\nProposition 1 ( [20], [21]). Consider an SA algorithm in thefollowing form defined over a set of parameters $x \u2208 \\mathbb{R}^{m \u00d7 t}$and a continuous function $h : \\mathbb{R}^{m \u00d7 t} \\rightarrow \\mathbb{R}^{m \u00d7 t}$.\n$x_{t+1} = \u0398(x_t + \u03b4_t[h(x_t) + n_t + \u03ba_t]^+)$, for $t \u2265 0$,  (2)\nwhere $\u0398$ is a projection operator that projects each $x_t$ iterateonto a compact and convex set $\u039b \u2208 \\mathbb{R}^{m \u00d7 t}$ and $\u03ba_t$ is a boundedrandom sequence. Let the ODE associated with Eqn. (2) be\n$\\dot{x} = \u0398(h(x))$,\nwhere $\u0398(h(x)) = \\lim_{\u03b7\u21920} \\frac{\u0398(x+\u03b7h(x)) \u2212 x}{\u03b7}$ and $\u0398$ denotes aprojection operator that restricts evolution of Eqn. (3) tothe set \u039b. Let the nonempty compact set $X$ be a set ofasymptotically stable equilibrium points of Eqn. (3). Then$x_t$ converges almost surely to a point in $X$ as $n \\rightarrow \u221e$ if\n(I) $\u03b4_t$ satisfies the conditions in Assumption 1.\n(II) $\\lim_{t\u2192\u221e} (\\sup_{t'\u2265t} \\sum_{l=t}^{t'} \u03b4_l ) = 0$ almost surely.\n(III) $\\lim_{t\u2192\u221e} \u03ba_t = 0$ almost surely."}, {"title": "IV. THE MULTI-CRITIC ACTOR CRITIC", "content": "In this section we introduce and describe the design ofour multi-critic actor-critic (MCAC) algorithm to enable fastautonomy transfer in RL using pre-trained critics. To aidreadability, we provide a description of notations in Table I.\nOur intuition for the MCAC algorithm is grounded in theprinciples of ensemble learning in machine learning [22] andmodel soups in large language models [23]. These solutionsutilize multiple trained models to achieve better performancewith significantly fewer resources and and in shorter time.Following a similar line of thought, we hypothesize thatusing multiple pre-trained critic values from training envi-ronments under different conditions (e.g., different obstacleorientations) can enhance performance and achieve fasterconvergence in actor-critic algorithms trained for a newenvironment with varied conditions.\nSpecifically, given access to a set of pretrained critic valuefunctions, the MCAC approach aims to bypass the process oftraining a critic for the new environment from scratch, optinginstead to use a weighted sum of the existing knowledgefrom the pretrained critic values for estimating the valuefunction in the current environment. A weighted averageof pre-trained critic value functions will provide a betterinitial condition for the value function estimated for thenew environment. This, in turn, will improve the agent\u2019sexploration and achieve better performance in terms of theaccumulated reward at convergence. Further, since MCACwill not require training new value functions, but ratherfocuses on finding the best set of weights to compute theweighted average of the pre-trained value functions of criticsto estimate the value function of the current environment, itreduces the number of trainable parameters related to criticsfrom |\\mathcal{S}| to N, where |\\mathcal{S}| is the cardinality of the state spaceof the environment, and N is the number of pre-trained criticvalue functions available in the new environment. Typically,$N \u226a |\\mathcal{S}|$, which aids the agent in achieving significantspeedup in training. Additionally, starting from a better initialcondition, as mentioned previously, will also help the agentlearn faster compared to learning from scratch.\nIn order to setup the MCAC approach and algorithm, wefirst establish an assumption regarding the availability of pre-trained value functions, derived using the AC algorithm fromvarious environment settings.\nLet $V_i(s)$ be the value function of the $i^{th}$ pre-trained criticat state $s \u2208 \\mathcal{S}$. Then define the vector $V_i := [V_i(s)]_{s\u2208\\mathcal{S}}$.\nAssumption 2. Converged value vectors $V_i$ for $i =1, 2, . . . , N$ pre-trained critics are readily available for thetraining the actor (i.e., agent) using the MCAC algorithm.\nLet $W = [W_i]_1^N$ be the weight vector where each weight$w_i$ is associated with a pre-trained value function $V_i$ ofcritic $i$. Further, let $1 = \\sum_{i=1}^N w_i$ and $w_i \u2265 0$ for alli \u2208 {1, 2, . . . , N}. Then, the value vector $V = [V(s)]_{s\u2208\\mathcal{S}}$of the actor is defined by\n$V = [\\sum_{i=1}^N w_i V_i(s)]_{s\u2208\\mathcal{S}}$. (4)\nThe temporal difference (TD) error associated with thetrained actor at state $s \u2208 \\mathcal{S}$ when taking action $a \u2208 A$ is:\n$ [r(s,a,s') + \u03b3 \\sum_{s'\u2208\\mathcal{S}} P(s,a,s')V(s')] - V(s)$. (5)\nwhere $0 < \u03b3 < 1$ denotes the discount factor. In actor-critic (AC) RL algorithms [1], the TD error representsthe difference between the predicted reward and the actualreward obtained. The TD error is used to update the valuefunction and the policy in AC algorithms. A high TD errorindicates that the predictions are quite different from theactual outcomes, suggesting that the actor\u2019s value functionand the policy need significant updating. Conversely, a lowTD error suggests that the actor\u2019s predictions are accurate.\nMinimizing the TD error for the trained actor can beexpressed as the following optimization problem:\n$\\min_{w,\u03c0} [r(s, a, s') + \u03b3 \\sum_{s'\u2208\\mathcal{S}} P(s, a, s')V(s')] - V(s) $. (6)\nsubject to:\n$\\sum_{s'\u2208\\mathcal{S}} P(s, a, s')[V(s') - r(s, a, s')] \u2265 0, \u2200s, s' \u2208 \\mathcal{S}, \u2200a \u2208 \\mathcal{A}$, (7)\n$V(s) = \\sum_{i=1}^N w_i V_i(s), \u2200s \u2208 \\mathcal{S}$ (8)\n$\\sum_{a\u2208\\mathcal{A}(s)} \u03c0(s, a) = 1, \u2200s, s' \u2208 \\mathcal{S}$ (9)\n$\u03c0(s, a) \u2265 0, \u2200s \u2208 \\mathcal{S} and a \u2208 \\mathcal{A}$.\nRemark 1. If the above optimization problem was framedwithin a typical actor-critic (AC) framework, then the mini-mization would be performed over V and \u03c0, while omitting"}, {"title": "V. EXPERIMENTS", "content": "This section presents two case studies to compare ourproposed multi-critic actor-critic (MCAC) algorithm againsta baseline actor-critic (Baseline AC). We first present the de-tails of the two case study scenarios developed for comparingthe two algorithms. Next we describe the hyper parametersused in Baseline AC and MCAC algorithms and define metrics used to compare both algorithms. We then presenta comparison of results from the Baseline AC and our MCACalgorithm on two case study scenarios.\nA. Case Study Environments\nWe demonstrate effectiveness of our MCAC algorithmthrough experiments conducted in two distinct grid worldenvironments of dimensions 5\u00d75 (Case Study 1) and 16\u00d716(Case Study 2). Our experiments evaluate capabilities of RLagents in scenarios of varying complexity. Each grid worldis populated with obstacles and potentially hazardous areas,challenging the agent to learn to navigate effectively to agoal location, starting from an initial position. In the 5 \u00d7 5grid, 1 \u00d7 1 obstacles are placed at predefined grid locations;the 16 \u00d7 16 grid features a more complex arrangement ofobstacles, with 4 \u00d7 4 obstacles placed at predefined locations.Details about the grid environments are presented below.\nState Space: The state space, \\mathcal{S}, of our grid-worlds is definedby the discrete positions that an agent can occupy on a grid.For a grid of size L \u00d7 L, the state space has $L^2$ possiblepositions, each uniquely identified by its grid coordinates(x, y). We examine 5 \u00d7 5 and 16 \u00d7 16, grids leading to statespaces of 25 and 256 distinct states, respectively.\nAction Space: The action space, A, available to the agentconsists of four deterministic actions: move up (\u2191), move down (\u2193), move left (\u2190), and move right (\u2192). When theagent takes an action in a certain grid position (state), itmoves one position in that direction, unless there is some-thing in the way (e.g., obstacles, edge of grid). When theagent attempts to move beyond the grid\u2019s edges, the agent\u2019sposition remains unchanged."}, {"title": "VI. CONCLUSION", "content": "This paper developed a novel algorithm to facilitate fastautonomy transfer in reinforcement learning. Compared tomethods that require retraining of parameters when operatingin new environments, our multi-critic actor-critic (MCAC)algorithm used knowledge of pretrained value functionsand integrated these using a weighted average to expe-dite adaptation to new environments. We demonstrated thatMCAC achieved higher rewards and significantly faster (upto 22.76x) autonomy transfer compared to a baseline actor-critic algorithm across multiple deployment scenarios in twodistinct enrvironments. The success of MCAC underscoressignificance of knowledge transfer in reinforcement learning,offering a promising direction for future research in devel-oping more adaptable and efficient learning systems."}]}