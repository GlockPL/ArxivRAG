{"title": "Cascaded Self-Evaluation Augmented Training for Efficient Multimodal Large Language Models", "authors": ["Zheqi Lv", "Wenkai Wang", "Jiawei Wang", "Shengyu Zhang", "Fei Wu"], "abstract": "Efficient Multimodal Large Language Models (EMLLMs) have witnessed rapid development in recent years. Introducing Chain-of-Thought (CoT) during inference and leveraging self-evaluation step-by-step have further enhanced their performance. However, due to limited amount of the parameters, EMLLMS often perform poorly when directly using self-evaluation during inference. How to synthesize evaluation data? How much data to synthesize? What are the optimal training and inference strategies? What prompts to use? These are unexplored yet essential questions that need to be addressed to enhance the self-evaluation capabilities of EMLLMs.\nTo this end, we propose a method called Self-Evaluation Augmented Training (SEAT). We utilize more powerful EMLLMs to perform CoT reasoning, select beneficial data, and generate evaluations. The synthesized data is then used to train the EMLLMs. However, EMLLMs can't handle long prompt inputs, and long CoT inference and self-evaluation outputs. Furthermore, synthesized data significantly diminishes its original CoT reasoning capabilities. To overcome these limitations, we further introduce Cascaded Self-Evaluation Augmented Training (Cas-SEAT). Cas-SEAT splits the long integrated prompts, which combine CoT reasoning and self-evaluation functionalities, into shorter cascaded prompts, each dedicated to one of these tasks. Additionally, we minimize the cost of Cas-SEAT to ensure its applicability in resource-constrained environments. During the synthesis of evaluation data, we use open-source EMLLMs with only 7B parameters and annotate a small amount of data using short prompts. Experiments show that, compared to existing methods, Cas-SEAT significantly enhances the self-evaluation capabilities of EMLLMs. It improves 19.68%, 55.57%, and 46.79% on the MathVista, Math-V, and We-Math datasets, respectively. Additionally, as the first work to explore self-evaluation augmented training for EMLLMS, the Cas-SEAT Dataset we constructe to enhance the self-evaluation capabilities of EMLLMs also provides valuable resources and references for future research in this field.", "sections": [{"title": "1 Introduction", "content": "In recent years, multimodal large language models (MLLMs) (Liu et al., 2024; Wang et al., 2024) have advanced rapidly, achieving remarkable performance. However, broader training and deployment demands often come from resource-constrained settings, such as universities, hospitals, and communities. In these scenarios, the application of larger models, such as 72B or 40B-parameter MLLMs, faces significant limitations. This has spurred the emergence of efficient multimodal large language models (EMLLMs) with smaller parameter sizes, such as 7B, 2B, or even 1B. Today, EMLLMs are approaching the performance of early 40B-parameter MLLMs, thanks to advancements in model architecture, training methods, and techniques like Chain-of-Thought (CoT) (Wei et al., 2022c). CoT, in particular, enables EMLLMs to explicitly model intermediate reasoning steps, producing more interpretable, logically coherent, and contextually relevant responses across a wide range of tasks involving text, images, and other modalities.\nDespite these advancements, the further improvement of EMLLMs faces two major bottlenecks. First, the CoT data available for training is insufficient in both quantity and quality. Existing data, often sourced from web crawls or generated by models without strict quality control, suffers from inconsistencies, incomplete reasoning steps, logical gaps, and even incorrect reasoning. Such flawed samples not only fail to enhance model performance but may also degrade it. Second, while recent research suggests that self-evaluation can improve the accuracy and reliability of reasoning, these efforts primarily focus on refining prompts"}, {"title": "2 Related Work", "content": "CoT Reasoning. CoT reasoning (Huang et al., 2022; Hoffmann et al., 2022; Chowdhery et al., 2022; Wei et al., 2022a; Zhang et al., 2022; Kojima et al., 2022) enhances the performance of LLMs by forcing them to perform step-by-step reasoning. The initial idea was proposed by Wei et al. (2022b), demonstrating that simply modifying the prompt could significantly improve model performance on complex tasks. Following Wei et al. (2022b), numerous studies have shown that LLMs can perform CoT reasoning through zero-shot CoT prompting ((Kojima et al., 2022)) and few-shot CoT prompting ((Wei et al., 2022b; Zhang et al., 2022)). Improvements have been made in areas such as self-consistency (Wang et al., 2022b), least-to-most prompting (Zhou et al., 2022), dynamic least-to-most prompting (Drozdov et al., 2022), bootstrapping training (Zelikman et al., 2022), and self-training (Huang et al., 2022).\nWang et al. (2022b,a) refined the original CoT prompting by marginalizing over diverse reasoning paths, while Zelikman et al. (2022); Huang et al. (2022) improved CoT prompting through bootstrapping on self-generated CoT prompts. Li et al. (2022a) introduced a voting classifier to filter sampled CoT prompts before the final prediction. Additionally, prompt enhancement and selection (Shum et al., 2023), meta-heuristic approaches (Pan et al., 2023), and meta-graph prompting (Pan et al., 2024) have further advanced standard CoT prompting. Existing CoT studies pay little attention to the correctness of synthesized CoT data, especially lacking focus on the CoT capabilities of EMLLMs. CoT reasoning is more challenging for EMLLMs. Our Cas-SEAT improves the quality of synthesized training data through self-evaluation and enhances the CoT reasoning capabilities of EMLLMs.\nLLM Self-Evaluation. Recent research on LLM calibration has shown that current LLMS can achieve well-calibrated predictions in specific tasks (Rae et al., 2021; Kadavath et al., 2022; Guo et al., 2017; Kadavath et al., 2022; Jiang et al., 2021; Kuhn et al., 2023; Fu et al., 2023). For instance, Liu et al. (Liu et al., 2023a) leveraged CoT prompting to use GPT-4 for text evaluation. Kocmi and Federmann (Kocmi and Federmann, 2023); Xu et al. (Xu et al., 2023) designed methods focused on enabling LLMs to produce more fine-grained correction annotations. Additionally, Koo et al. (Koo et al., 2023), Zheng et al. (Zheng et al., 2023), Chang et al. (Chang et al., 2023), Deutsch et al. (Deutsch et al., 2022), and Liu et al. (Liu et al., 2023b) explored issues of self-reinforcing bias and fairness in LLMs. It is worth noting that scaling up model size plays a crucial role in improving calibration (Rae et al., 2021; Wei et al., 2022a). As LLMs exhibit strong calibration capabilities, more research has focused on prompting LLMs for self-evaluation (Golovneva et al., 2022; Zhang et al., 2023; Shinn et al., 2023; Madaan et al., 2024; Paul et al., 2023). Although dedicated evaluators have proven effective in assessing the correctness of reasoning data (Li et al., 2022b; Xu et al., 2024), self-evaluation is more efficient compared to using external models for evaluation, as it eliminates the need for additional annotations and evaluators (Li et al., 2022a). Unlike existing methods that optimize the reasoning process through self-evaluation,"}, {"title": "3 Methodology", "content": "3.1 Problem Formulation and Notations\nData and Model. The primary EMLLM model is denoted as M, with parameters \u03980. To achieve better performance on the target task, M needs to be fine-tuned on the dataset D. We use {x,y} to represent a sample, X include images I and queries Q, and Y represents answers. After finetuning, \u03980 will be optimized to \u0398. The output of M are denoted as \u00dd. Dse represents the self-evaluation data generated by M. A more powerful EMLLM model is denoted as G, which generates CoT reasoning data based on D, denoted as Scot. After evaluating Scot, G produces self-evaluation training data, denoted as Sse. A small subset of Scot is selected, denoted as Ssub, and after being evaluated by G, generates cascaded self-evaluation training data, denoted as Scse.\nPrompt. The prompts used for M inference and self-evaluation are denoted as P and Pse, respectively. The prompts used by G to generate cascading CoT data and evaluation data are denoted as Pcot and Pse, respectively. The prompts used by G to generate CoT data and evaluation data are denoted as Pcot and Pcse, respectively. Identical symbols indicate identical prompts.\nFormula. We formalize the existing studies to facilitate a formula-based comparison of their differences. I represents the loss function.\nInference:\n$\\Y = M(\\lbrace P_{or}P_{cot}orP_{se}\\rbrace, X; \\Theta), X \\in D$\nFinetune:\n$\\arg \\min C = \\sum_{X,Y\\in D} l(Y, M(P_{or}P_{cot}, X; \\Theta_0))$\nAugmented CoT:\n$\\S_{cot} = G(P_{cot}, X; \\Theta)$\n$\\arg \\min_\\Theta C = \\sum_{X,Y\\in S_{cot}} l(Y, M(P_{or}P_{cot}, X; \\Theta))$\nSEAT:\n$\\S_{se} = M(P_{se}, X; \\Theta)$\n$\\arg \\min_\\Theta C = \\sum_{X,Y\\in S_{se}} l(Y, M(P_{se}, X; \\Theta))$\nCas-SEAT:\n$\\S_{cot} = G(P_{cot}, X; \\Theta)$\n$\\S_{cse} = G(P_{cse}, X; \\Theta)$\n$\\arg \\min_\\Theta C = \\sum_{X,Y\\in {S_{cot}, S_{se}}} l(Y, M(P_{cse}, X; \\Theta))$\n3.2 SEAT\nSince self-evaluation has been proven effective in LLMs and MLLMs, we tested the selfevaluation performance of EMLLM, as shown in Figure 3. Specifically, we evaluated inference and post-inference evaluation using the LLaVAbased Base(pretrained model), Finetune (finetune on Math360k), and Augmented CoT (Qwen2-VL (7B) annotates CoT data on Math360k, which is then learned by LLaVA-1.5.) on MathVista, MathV, and We-Math. We observed that, in most cases, evaluation led to some improvement, though the improvement was not significant. However, in certain cases, evaluation resulted in a performance decline. We believe this variability in performance is primarily due to the lack of evaluation-specific data during training. Therefore, we aim to enhance the evaluation capabilities of EMLLM by synthesizing more self-evaluation data for its training. The prompt Pse and the training data Sse for Augmented Self-Evaluation are shown in Figure 2. The prompt divides Augmented SelfEvaluation into two parts: CoT reasoning and selfevaluation. The self-evaluation also includes data selection, where the MLLM autonomously determines which data to use for learning and which to discard. The learning process can be formulated as Equation 4.\n3.3 Cas-SEAT\nDue to the parameter limitations of EMLLM, its performance significantly degrades in scenarios involving long-token inputs and outputs. Consequently, Augmented Self-Evaluation often leads to a decline in performance. To address this issue, we decouple the inference and evaluation processes, as illustrated in Figure 2. First, we use a more powerful EMLLM G to obtain Scot based on Pcot. Next, we continue to use G to evaluate the incorrect parts of Scot, denoted as Error(Scot). This process produces Scse = G(Pcse, Error(Scot))."}, {"title": "4 Experiments", "content": "We conducted experiments to evaluate the effectiveness and generalizability of Cas-SEAT.\n4.1 Experimental Setup\n4.1.1 Datasets\nWe train on MathV360K dataset and evaluate on Math-Vista, Math-V, We-Math. MathV360K is a widely used dataset for MLLM training. Math-Vista), Math-V, We-Math are three widely used public benchmarks for MLLM evaluation.\n4.1.2 Baselines\nTo verify the applicability, the following EMLLMs are implemented and compared with the counterparts combined with the proposed method. We primarily analyzed the effectiveness of our method based on the EMLLMS LLAVAv1.5(7B) (Liu et al., 2024), Qwen2-VL(2B) (Wang et al., 2024). Since current EMLLMs research often involves training on various datasets, including those frequently used as test datasets, such as the ones we selected, and many MLLM studies do not disclose their training datasets, we opted not to include EMLLMs published after the release of these datasets to ensure a fair comparison.\n4.1.3 Implementation Details\n\"Base\": The pretrained MLLM (Liu et al., 2024; Wang et al., 2024). \u201cFinetune\u201d: The pretrained MLLM is lora fine-tuned on Math360k (Hu et al., 2022). \u201cCoT\u201d: Qwen2-VL (7B) performs CoT reasoning on Math360k. The MLLM then learns from this data (Wang et al., 2023; Wadhwa et al., 2024). \"SEAT\": The pretrained MLLM is finetuned on Math360k and subsequently performs reasoning on Math360k. Qwen2-VL (7B) evaluates the reasoning outputs of the MLLM, and the MLLM learns from this evaluated data (Xiong et al., 2024). \"Cas-SEAT\u201d: The pretrained MLLM is fine-tuned on Math360k and subsequently performs reasoning on Math360k. Qwen2-VL (7B) evaluates the MLLM's incorrect reasoning outputs. These evaluated data are combined with CoT data, and the MLLM learns from this combined dataset.\nAfter obtaining outputs on the test dataset, we first use a more powerful MLLM to extract answers for the MathVista dataset, followed by us-"}, {"title": "4.2 Experimental Results", "content": "4.2.1 Overall Assessment of Cas-SEAT\nAs shown in Tables 1, 2, 3, and 4, we analyzed the experimental results on the MathVista, Math-V, and We-Math datasets. We evaluated the accuracy of two types of outputs for each method: direct inference (inference) and self-evaluation (evaluation). In addition to overall performance, we conducted tests on multiple subsets of the three datasets, including generalization capability, data types, problem difficulty, and task types. Below is the correspondence between these categories and the columns in the tables. (Due to dataset limitations, some categories could not be tested for certain datasets.): Overall Performance: MathVista (Table 1: Average), Math-V (Table 2: All), We-Math (Table 3: Score (Strict), Score (Loose)). Cas-SEAT shows very significant improvements over the baseline on all datasets, achieving around a 50% improvement on both Math-V and WeMath. Generalization Capability: We-Math (Table 3: Inadequate Generalization (Strict), Complete Mastery (Strict), Inadequate Generalization (Loose), Complete Mastery (Loose)). The improvement range of 9.5% to 55.25% demonstrates Cas-SEAT's strong generalization capabilities, excelling at problems involving more integrated knowledge points, as evidenced by the significant improvement in Complete Mastery (CM). Data Types: MathVista (Table 1: Text, Integer). Cas-SEAT is better at answering integer-type questions. Problem Difficulty: MathVista (Table 1: Multi-choice, Free-form), Math-V (Table 2: Levell, Level2, Level3, Level4, Level5), WeMath (Table 3: One-step, Two-step, Three-step). The Math-V subsets are divided into five difficulty levels, with higher levels indicating more complex problems. The We-Math subsets are classified according to the number of reasoning steps, with more steps indicating higher difficulty. CasSEAT shows a more pronounced advantage on more challenging problems. Task Types: MathVista: General VQA, Math-targeted VQA; MathV: Algebra (ALG), Arithmetic (ARI), Combinatorial Geometry (CG), Combinatorics (COM); WeMath: Understanding and Conversion of Units (UCU), Angles and Length (AL), Calculation of Plane Figures (CPF), Understanding of Plane Figures (UPF), Calculation of Solid Figures (CSF), Understanding of Solid Figures (USF), Basic Transformations of Figures (BTF). Cas-SEAT exhibits significant improvements over the best baseline method on almost all problem types, especially for numerical computation tasks (Math-V: ALG, ARI). For VQA tasks, Cas-SEAT performs better on the more difficult Math-targeted VQA. In summary, we found that: 1. After finetuning, the inference ability of EMLLMs improves, but their self-evaluation ability declines. This is because fine-tuning enhances their mathematical reasoning ability but leads to forgetting general knowledge. Using synthetic CoT data based on mathematical datasets for enhanced training shows a similar pattern. 2. SEAT, which integrates CoT inference and self-evaluation training, suffers a significant drop in CoT inference ability, without a substantial improvement in selfevaluation. Even if EMLLMs are first trained with synthetic CoT data, using only self-evaluation training in the second round significantly reduces their CoT reasoning ability. This demonstrates the detrimental effect of long prompts and outputs on EMLLMs, and the difficulty of maintaining both CoT inference and self-evaluation capabilities simultaneously. 3. Cas-SEAT, by separating the CoT inference and self-evaluation tasks and shortening the prompt, preserves robust inference ability. More importantly, it achieves far stronger selfevaluation performance than the baselines. Our approach Cas-SEAT achieves remarkably significant improvements over the best baseline methods, both overall and across each subset. The detailed analysis shows that Cas-SEAT is especially adept at more difficult numerical computation tasks."}, {"title": "4.2.2 Evaluation on Different EMLLMs", "content": "As shown Figure 4, we used Qwen2-VL (2B) on Math-V to compare the performance of Cas-SEAT with that of the baseline. We take the maximum value of all methods as the circumference and 0 as the center of the circle. The results demonstrated a significant advantage for Cas-SEAT over the baselines. This indicates that our approach is also applicable to smaller EMLLMs with different architectures."}, {"title": "4.2.3 Case Study", "content": "We present two representative examples in Figure 5 and Figure 6 (in appendix). From Figure 5 and Figure 6, the following observations can be made: 1. Reasoning Process and Its Accuracy: Direct reasoning using various methods failed to produce the correct answer. The Base, Finetune, and SEAT methods generally lack a complete reasoning process and jump directly to the final answer. Incorporating CoT data into EMLLM training (e.g., CoT, Cas-SEAT) enables step-by-step reasoning, but it does not always guarantee accurate results. Moreover, although self-evaluation data is constructed based on CoT data, the lengthy prompts and evaluation data diminish the model's reasoning capability. 2. Evaluation Process and Its Accuracy: Generating self-evaluation data from CoT data and applying it to training seems to enhance EMLLM's self-evaluation capability by enabling targeted training to help the model reflect on whether its outputs are reasonable. However, in many cases, such reflections are superficial, and a large number of self-evaluation instances are ineffective. In contrast, Cas-SEAT effectively preserves the model's reasoning ability while significantly enhancing its self-evaluation capability. By mixing a small amount of self-evaluation data into the CoT data, EMLLM maintains its CoT reasoning ability while upgrading its self-evaluation skills. These conclusions hold consistently across several datasets."}, {"title": "5 Conclusion", "content": "We proposed SEAT and its enhanced variant, CasSEAT, to improve the self-evaluation capabilities of Efficient Multimodal Large Language Models (EMLLMs). By synthesizing evaluation data with stronger models and using cascaded task decomposition, Cas-SEAT enhances performance while balancing CoT reasoning and self-evaluation, even under resource constraints. Experiments show significant gains on benchmark datasets, and the CasSEAT Dataset provides a valuable resource for future research, laying a strong foundation for advancing self-evaluation in EMLLMs."}, {"title": "Limitation", "content": "Potential limitations may include a small amount of resource consumption. The method is designed specifically for EMLLMs, and it may not currently be the optimal solution for larger MLLMs."}, {"title": "A Appendix", "content": "A.1 Supplementary Experiments\nA.1.1 Case Study\nFigure 6 is a supplementary case, the observed findings can be referred to the section 4.2.3.\nA.1.2 Datasets\nThe statistics of the training dataset used in the experiments is shown in Table 5.\nA.1.3 Hyperparameters and Training Schedules\nWe summarize the hyperparameters and training schedules of the LLaVA-1.5 and Qwen2-VL used in the experiments. Table 6 shows the settings of the LLaVA-1.5 training. Table 7 shows the settings of the Qwen2-VL training."}]}