{"title": "Autoregressive Policy Optimization for Constrained Allocation Tasks", "authors": ["David Winkel", "Niklas Strau\u00df", "Maximilian Bernhard", "Zongyue Li", "Thomas Seidl", "Matthias Schubert"], "abstract": "Allocation tasks represent a class of problems where a limited amount of resources\nmust be allocated to a set of entities at each time step. Prominent examples of\nthis task include portfolio optimization or distributing computational workloads\nacross servers. Allocation tasks are typically bound by linear constraints describing\npractical requirements that have to be strictly fulfilled at all times. In portfolio\noptimization, for example, investors may be obligated to allocate less than 30% of\nthe funds into a certain industrial sector in any investment period. Such constraints\nrestrict the action space of allowed allocations in intricate ways, which makes\nlearning a policy that avoids constraint violations difficult. In this paper, we propose\na new method for constrained allocation tasks based on an autoregressive process\nto sequentially sample allocations for each entity. In addition, we introduce a novel\nde-biasing mechanism to counter the initial bias caused by sequential sampling.\nWe demonstrate the superior performance of our approach compared to a variety of\nConstrained Reinforcement Learning (CRL) methods on three distinct constrained\nallocation tasks: portfolio optimization, computational workload distribution, and a\nsynthetic allocation benchmark. Our code is available at: https://github.com/\nniklasdbs/paspo.", "sections": [{"title": "1 Introduction", "content": "Continuous allocation tasks are a class of problems where an agent needs to distribute a limited amount\nof resources over a set of entities at each time step. Many complex real-world problems are formulated\nas allocation tasks, and state-of-the-art solutions rely on using Reinforcement Learning (RL) to learn\neffective policies [6, 26, 3, 20, 27]. Notable examples include portfolio allocation tasks, where\nportfolio managers must allocate the available financial resources among various assets [27], or\nallocation tasks of computational workloads to a set of compute instances in data centers [3]. In many\ncases, allocation tasks come with allocation constraints [6, 20, 27, 26], such as investing at most 30%\nof the portfolio into a specific subset of the assets or to restrict the maximum workload to certain\nservers in a data center. Formally, allocation constraints are expressed as linear constraints and form a\nsystem of linear inequalities, geometrically describing a convex polytope. Each point in this polytope\ndescribes a possible allocation and each dimension corresponds to one of the entities. Allocation\ntasks often require hard constraints, i.e., constraints that are explicitly given and must be satisfied\nat any point in time. However, most of the existing CRL literature focuses on soft constraints that\nare not explicitly given [2, 29, 31, 14, 25]. These approaches typically cannot guarantee constraint\nsatisfaction and tend to have many constraint violations during training. The majority of these\nmethods approximate the cumulative costs of constraint violations and optimize the cumulative\nreward while trying to adhere to the maximum cumulative costs. While less explored, there exist"}, {"title": "2 Related Work", "content": "Resource allocation tasks are a widely researched area with numerous applications spanning logistics,\npower distribution, computational load balancing, security screening, and finance [6, 26, 3, 20, 27].\nWe identify three key research directions that are particularly important when discussing resource\nallocation tasks.\nSafe Reinforcement Learning The majority of work in CRL addresses soft constraints, a setting\noften referred to as Safe RL. We will provide a brief overview of the most important methods in this\nfield. For a more comprehensive examination of Safe RL, we direct readers to the survey papers by\n[15, 12]. A common technique in Safe RL is the use of Lagrangian relaxation [4, 15]. Several works\nemploy primal-dual optimization to leverage the Lagrangian duality, including [9, 23, 13]. Another\nfrequently used approach involves different penalty terms [25, 14, 31]. The authors of IPO [14]\npropose to use logarithmic barrier functions. CPO [2] extends TRPO [21] to ensure near-constraint\nsatisfaction with each update. Additionally, two-step approaches such as FOCOPS [32] and CUP [30]\nare popular in the field. However, unlike our method, these approaches do not guarantee strict\nconstraint satisfaction, particularly during training.\nHard Constraints Although less studied than Safe RL, several works address hard instantaneous\nconstraints on actions to ensure full constraint satisfaction at any time step. Most of these approaches\nemploy mechanisms to correct infeasible actions, i.e., those that violate constraints, into feasible\nactions [18, 6, 20, 11]. In contrast, our method always generates feasible actions without the need\nfor correction. OptLayer [18] is one of the most prominent examples in this field, which employs\nOptNet [5] to map infeasible actions to the nearest feasible action. Similarly, [20] propose a more"}, {"title": "3 Problem Description", "content": "An allocation task can be described as a finite-horizon Markov decision process (MDP)\n(S, A, T, R, \u03b3), where S represents the state space, A the action space, T : S\u00d7A\u00d7S \u2192 [0, 1] the state\ntransition function, R the reward function, and \u03b3\u2208 [0, 1] a discount factor. The goal of this task is to\nfind a policy \u03c0 maximizing the expected cumulative reward $J_R = \\mathbb{E}_\\pi [\\sum_{t=1}^\\infty \\gamma^t R(S_t, \\pi(S_t), S_{t+1})]$.\nThe action a is an allocation a = {a1,...,an} \u2208 A over a set of n entities E = {e1,...,en} at\neach time step. Each element ai of the action vector a represents the proportion allocated to entity ei.\nFurthermore, allocation tasks require a complete allocation, i. e., $\\sum_{i=1}^n a_i = 1$ and allocations cannot\nbe negative ($a_i > 0$). Thus, the action space of unconstrained allocation tasks forms an n-dimensional\nstandard simplex. A visualization of an unconstrained allocation action space is provided in Figure 1a.\nAllocation tasks frequently include constraints, such as allocating at most 30% to a subset of the\nentities. An example of a constrained action space is visualized in Figure 1b. Formally, an allocation\nconstraint can be expressed as a linear inequality $\\sum_{i=1}^n C_i a_i \\leq b$, where ci denotes the weighting of\nthe allocation variable ai of entity ei and b \u2208 R denotes the corresponding constraint limit. For the\nsake of readability and simplicity, we only define < constraints since a > b can be transformed into\n-a <-b and a = b can be rewritten as a \u2264 band-a < -b.\nThe action space A of constrained allocation tasks can be easily expressed by a set of linear inequali-\nties, defining a polytope $A = \\{a \\in [0,1]^n | Ca \\leq b\\}$, where\n$C \\in \\mathbb{R}^{m \\times n}$"}, {"title": "4 Polytope Action Space Policy Optimization (PASPO)", "content": "Our approach PASPO autoregressively computes the allocation to every single entity in an iterative\nprocess until all allocations are fixed. We will later show that this step-wise decomposition allows for\na tractable parametrization of the action space.\n4.1 Autoregressive Polytope Decomposition\nPASPO starts by determining the feasible interval $[a_{i}^{min}, a_{i}^{max}]$ for allocations into the first entity e1.\nThen, we sample the first allocation a\u2081 from this interval. The details of the sampling process will be\nfurther discussed in Section 4.2. Fixing an allocation impacts the shape of the remaining action space.\nThus, we have to compute the shape of the polytope $A^{(2)}$ described by $C'^{(2)}$ and $b^{(2)}$ before we can\nsample the next allocation a2.\nEach iteration i starts with determining the interval $[a_{i}^{min}, a_{i}^{max}]$ of all feasible values for ai. Ge-\nometrically, this interval is bounded by the minimum and the maximum value of the remaining\npolytope $A^{(2)}$ in the i-th dimension associated with the allocation ai. To determine $a_{i}^{min}$, we solve\nthe following linear program:\nminimize $a_i$\ns.t. $C^{(i)} a^{(i)} \\leq b^{(i)}$\nwhere $C^{(i)}$ are the constraint coefficients for the entities ei, . . ., en, $b^{(i)}$ are the adjusted constraint\nlimits, and $a^{(i)}$ describes the unfixed allocations. We determine $a_{i}^{max}$ by solving the respective\nmaximization problem. For the first iteration i = 1, we define C(1) = C, b(1) = b and a(1) = a.\nAfter sampling an allocation ai from the interval $[a_{i}^{min}, a_{i}^{max}]$. The resulting polytope $A^{(i+1)}$ for the\nnext iteration i + 1 is described by the following inequality system:"}, {"title": "4.2 Parameterizable Policy Process", "content": "Our goal is to define a learnable stochastic policy function over the action space. For unconstrained\nallocation tasks, a Dirichlet distribution can be used to parameterize the action space [26, 28]. Unfor-\ntunately, to the best of our knowledge, there is no known parameterizable, closed-form distribution\nfunction over arbitrary convex polytopes as in our setting. In fact, even uniform sampling over a\nconvex polytope is an active research problem [8].\nWe sequentially constructed an action a from the polytope action space A in the previous section. Now,\nwe describe how to utilize this process to define a parameterizable policy function over the action\nspace A. We model the distribution for allocating each individual entity using a beta distribution that\nis normalized to the range $[a_{i}^{min}, a_{i}^{max}]$. This distribution is also known as the four-parameter beta\ndistribution [7]. Its probability density function is defined as:\n$p(x; \\alpha, \\beta, a_{min}, a_{max}) = \\frac{(x - a_{min})^{\\alpha-1}(a_{max} - x)^{\\beta-1}}{(a_{max} - a_{min})^{\\alpha+\\beta-1}B(\\alpha, \\beta)}$,\nwhere B(\u03b1, \u03b2) is the beta function. It is important to note that any other parameterizable distributions\nwith bounded support in the range $[a_{i}^{min}, a_{i}^{max}]$ can be used, such as a squashed Gaussian distribution.\nHowever, our preliminary experiments indicated that the beta distribution performs particularly well.\nTo optimize the policy $\u03c0_\\theta(s)$ over the complete allocations, we follow the approach of [19] for training\nan autoregressively dependent series of sub-policies. A fixed but arbitrary order of entities is used\nfor sampling the allocations ai. The sub-policy $\u03c0_\\theta(a_i|s, a_1, ..., a_{i-1})$ is conditional on the previous\nallocations a1,..., Ai\u22121. Using this autoregressive dependence structure, the policy is defined as:\n$\u03c0_\\theta(a|s) = \u03c0_\\theta(a_1|s) \\cdot \u03c0_\\theta(a_2)|s, a_1) ...\u03c0_\\theta (a_n|s, a_1, ..., a_{n-1})$. This policy can be jointly optimized.\nWe parameterize each sub-policy using a neural network that receives an embedding of the state and\nthe previously selected actions as input."}, {"title": "4.3 Policy Network Architecture", "content": "We create an embedding of the state using an MLP, denoted as $f_\\theta(s) = MLP(s)$. We pa-\nrameterize the probability distribution over allocations for each entity using an MLP $\u03c0_\\theta (s) =$\n$MLP(f_\\theta(s), a_1, ..., a_{i-1})$, which receives the latent encoding of the state and the previously sam-\npled allocations a1, ..., ai\u22121 as input. Note that each of the MLPs $\u03c0_\\theta$ has its own parameters. For\nfurther details, we refer to the Appendix."}, {"title": "4.4 De-biasing Mechanism", "content": "A drawback of generating actions by an autoregressive process is that a random initialization of the\nbeta distributions leads to a sampling bias towards the entities selected earlier in the process. The"}, {"title": "5 Experiments", "content": "In this section, we provide an extensive experimental evaluation of our approach in various scenarios\ndemonstrating its ability to handle various allocation tasks and constraints. We use two real-world\ntasks: Portfolio optimization [27] and compute load distribution [3]. Additionally, we create a\nsynthetic benchmark with a reward surface generated by a randomly initialized MLP. Each of these\ntasks comes with a different set of allocation constraints. We will briefly describe each setting in the\nfollowing and refer the reader to the Appendix for more details.\nPortfolio Optimization Portfolio optimization is a prominent constrained allocation task. In this\ntask the agent has to allocate its wealth over 13 assets at each time step. We use the environment\nof [27]. Each investment period contains 12 months and the investor needs to reallocate the portfolio\neach month. This environment is highly stochastic since each trajectory is sampled from a hidden\nMarkov model fitted on real-world NASDAQ-100 data. After every 5120 environment steps, we\nrun eight parallel evaluations on 200 fixed trajectories. Constraints in this setting define minimum\nand maximum allocation to groups of assets. Additionally, we add constraints where the constraint\ncoefficients in C correspond to portfolio measures like a minimum dividend yield or a maximum on\nthe CO2 intensity.\nCompute Load Distribution The environment is based on the paper of [3] and simulates a data\ncenter in which computational jobs need to be split into sub-jobs to enable parallel processing across\nnine servers. Here, we use five constraints that are randomly sampled as follows: First, we sample"}, {"title": "5.1 Experimental Setup", "content": "We train PASPO using PPO [22] and compare our approach to various baselines, including state-of-\nthe-art approaches for constrained allocation tasks and Safe RL. Specifically, we compare PASPO\nwith five representative approaches from Safe RL: CPO [2], CUP [30], IPO [14], P3O [31], and\nPPO with Lagrangian relaxation. Additionally, we compare our method to OptLayer [18], a popular\nprojection-based method for linear hard constraints. To maintain a consistent and fair comparison\nacross different methods, we use the same hyperparameters across the different methods if possible.\nMany Safe RL approaches have difficulties handling equality constraints [11]. Therefore, we use\na Dirichlet distribution to represent the policy in the baselines, thereby ensuring satisfaction of the\nsimplex equality constraint. We do not share the parameters between the policy and value function.\nWe use a fully-connected MLP with two hidden layers of 32 units and ReLU non-linearities for each\npolicy, cost, and value function. In our approach, the state encoder and each policy head consists of a\ntwo-layer MLP. The training process is run for 150,000 steps and the results are averaged over five\ndifferent seeds. In the portfolio optimization task, we use ten different seeds due to the stochasticity\nof the financial environment and train for 250,000 steps. Given the relatively small network sizes,\ntraining is conducted exclusively on CPUs. We implement our algorithm and the baselines using\nRLlib and PyTorch. More details regarding the environments, training, and hyperparameters can be\nfound in the Appendix."}, {"title": "5.1.1 Performance of PASPO", "content": "We visualize the performance and constraint violations of all methods across our three environments\nin Figure 4. A tolerance of le-\u00b3 is used for evaluating constraint violations and we report the total\nnumber of violations per episode. In all three environments, PASPO converges faster to a higher"}, {"title": "5.1.2 Importance of de-biased Initialization and Order", "content": "We conduct ablation studies to investigate the impact of our de-biased initialization and the order\nof entity allocation on our synthetic benchmark. No constraints are applied except for the simplex\nconstraint to highlight the effects. The results, shown in Figure 5, indicate that without de-biased\ninitialization (orange in (a)), learning is slower and converges prematurely to a sub-optimal policy.\nIn (b), we explore the impact of allocation order by reversing it (red) and observe no significant\nperformance difference. This indicates that our approach is robust to the allocation order due to the\nuse of the de-biasing initialization."}, {"title": "6 Limitations and Future Work", "content": "While PASPO guarantees that constraints are always satisfied, it is considerably more computationally\nexpensive than standard neural networks in allocation tasks with many entities, as the sampling of\neach action requires solving a series of linear programs. RL in high-dimensional continuous action\nspaces is a very challenging task. Our approach cannot overcome this issue and also struggles in\nvery high-dimensional settings. For future work, we plan to extend PASPO to also incorporate state-\ndependent constraints. While we evaluate our approach only on benchmarks with hard constraints,\nit can be applied to settings with both hard and soft cumulative constraints. In these scenarios, our\nmethod for handling hard constraints can be easily combined with most Safe RL algorithms to handle\nsoft cumulative constraints."}, {"title": "7 Conclusion", "content": "In this paper, we examine allocation tasks where a certain amount of a resource has to be distributed\nover a set of entities at every step. This problem has many applications like logistics tasks, portfolio\nmanagement, and computational workload processing in distributed environments. In all these\napplications, the set of feasible allocations might be bound by a set of linear constraints. Formally,\nthese restrict the action space to a convex polytope. To define a stochastic policy function that can\nbe used with policy gradient methods in RL, we propose an autoregressive process that computes\nallocation sequentially. We employ linear programming to compute the range of feasible allocations\nfor an entity given the already fixed allocations of other entities. Our policy function consists of\na sequence of one-dimensional beta distributions where the shape parameters \u03b1 and \u03b2 are learned\nby neural networks. To counter the effect of initialization bias, we utilize a de-biasing mechanism\nto ensure sufficient exploration and prevent premature convergence to a sub-optimal policy. In our"}, {"title": "A Environments", "content": "The implementation for all three environments can be found at: https://github.com/niklasdbs/\npaspo."}, {"title": "A.1 Financial Environment", "content": "The financial environment used for testing our approach is based on [28]. The financial market\ntrajectories in this environment are sampled from a hidden Markov model, which was fitted based\non real-world NASDAQ-100 data from January 3rd, 2011 to December 1st, 2021. The environment\noffers differently sized data sets of randomly selected assets contained in the NASDAQ-100. The\nexperiments in the financial environment for this paper are run with 13 assets, which corresponds to"}, {"title": "A.2 Compute Environment", "content": "The compute environment used for testing our approach is based on [3]. The agent's task is to\nallocate compute jobs to a given set of servers in a data center. A reward is triggered for each job\nthat was completed in a predetermined maximum allowed computation time. The challenge of this\nenvironment is that the agent needs to match the queue of jobs still to be allocated with the different\ncomputational capabilities of the servers as well as each server's individual queue of jobs still to\nbe computed. It is assumed that the compute jobs in the environment can be arbitrarily split and\ncomputed in parallel. The creation of new compute jobs is triggered by n users and follows a Poisson\nprocess. A job is defined by its payload size, i.e., the data to be transferred to a server, its required\nCPU cycles for the processing workload, and its maximum allowed time until the job needs to be\ncompletely processed. These attributes for the jobs that can be created by each user are randomly\nsampled at creation of the environment.\nThe experiments in this paper run with a setup of 9 servers and 9 users that generate compute jobs.\nThe parameter set used is parameter_set_9_9_id_0. We initialize the environment with seed=1. The\nrandomly sampled specifications for the nine servers can be found in Table 3 and the job attributes\ncreated by the nine users can be found in Table 4.\nTo generate the constraints, we first sample the number of affected entities between 2 and 8 for each\nconstraint and randomly choose the affected entities accordingly. We then uniformly sample constraint\ncoefficients from the interval [0, 1], as well as a corresponding constraint limit between 0 and 1. We\nuse a seed of 1 to generate 5 constraints. The implementation can be found in polytope_loader.py\n(generate_random_polytope_rejection_sampling)."}, {"title": "A.3 Synthetic Benchmark", "content": "In addition to these environments, we propose a synthetic benchmark. Its reward surface consists of\nan MLP with random weights. An example of the reward surface in three dimensions is visualized in\nFigure 6 Each episode has two states. Since it is completely deterministic, it provides a simple but"}, {"title": "B Architecture", "content": null}, {"title": "C Hyperparameters/Training", "content": "In Table 5 we list the most important parameters and hyperparameters. The full configurations used\ncan be found in the config files (yaml/hydra based) in our code (run configs directory). We tuned\nhyperparameters on our synthetic benchmark with five dimensions and five constraints.\nWe do not train using GPUs because of the small network sizes. We used an internal CPU cluster with\nconsumer machines and servers ranging from 8 to 90 cores and RAM between 32GB and 512GB."}, {"title": "D Guaranteed Constraint Satisfaction", "content": "In the following, we proof that our approach PASPO always guarantees constraint satisfaction and\nthat our method is able to sample all possible actions from the constrained action space.\nDefinitions: Let A be the set of all actions that can be sampled with PASPO, and let P = {a \u2208\nRn Ca < b} be the convex polytope that corresponds to constrained action space. We define\n$A^{(i+1)} = \\{a \\in \\mathbb{R}^n|C^{(i+1)}a^{(i+1)} \\leq b^{(i+1)}, j = 1,...,i: a_j = a\\}$ where $b^{(i+1)} =$\n$b - \\sum_{j=1}^{i} C_{1j}$\\\n$\\sum_{j=1}^{i} C_{mj}$ and $C^{(i+1)}$ and $a^{(i+1)}$ as defined in the paper.\nThus, $A^{(i+1)}$ is the restricted action space after sampling/fixing already the allocations $a_1,..., a_i$.\nTheorem 1. Let P = {a \u2208 Rn Ca < b} \u2260 \u00d8 be the convex polytope that corresponds to a\nconstrained action space. Let A be the set of all the points that can be generated by PASPO. It holds\nthat A = P.\nProof. Well-defined: Show that $A^{(n)} \\neq \\O$ if $P \\neq \\O$.\nInduction over i:\ni=1:\nii+1:\n$A^{(1)} = \\{a \\in \\mathbb{R}^n|C^{(1)}a^{(1)} \\leq b^{(i+1)}\\} = \\{a \\in \\mathbb{R}^n|Ca < b\\} = P \\neq \\O$\n$A^{(i)} \\neq \\O \\Rightarrow \\exists a\u2191, a\u207a \u2208 A^{(i)} : a_i = a_{i}^{min}, a_i = a_{i}^{max}$\nNow assume an arbitrary $a_i$ is sampled from $[a_{i}^{min}, a_{i}^{max}]$\n$\\Rightarrow \\exists \\lambda \\in [0, 1] : a_i = (\\lambda a\u2191 + (1 - \\lambda)a\u207a)$ := $a_{\\lambda}$\nBy convexity of polytopes as solution spaces for linear inequality systems, we get:\n$\\begin{bmatrix} C_{1,i} & C_{1,i+1} & ... & C_{1,n} \\\\ ... & ... & ... & ... \\\\ C_{m,i} & C_{m,i+1} & ... & C_{m,n} \\end{bmatrix} \\leq b - \\sum_{j=1}^{i-1} a_{C_{mj}}$\n$\\begin{bmatrix} C_{1,i+1} & ... & C_{1,n} \\\\ ... & ... & ... \\\\ C_{m,i+1} & ... & C_{m,n} \\end{bmatrix} \\leq b - \\sum_{j=1}^{i} a_{C_{mj}} \\Rightarrow a_{\\lambda} \\in A^{(i+1)}$"}, {"title": "To show that A = P:", "content": "ACP:\nLet a* \u2208 A. In the last step (n), $a_n$ is sampled (by design) such that\n$C^{(n)}a_n \\leq b - \\sum_{i}^{n-1} \\\\ C_{mj}Ca* < b \u21d4 a* \u2208 P$\\nAP:\nLet a* \u2208 P. $\u21d4 Ca* < b \u21d4 C^{(i)}a* < b^{(i)} \\forall i \u21d4 a* \u2208 A^{(i)} \\forall i$\n\u21d2 We can construct a* by sampling $a_i$ in every step i. \u21d2 a* \u2208 A\nThe intuition of why our approach can guarantee the satisfaction of constraints is based on three\nproperties that we utilize: (1) If Pi is the set of solutions to an system of linear inequalities, then\nby adding further constraints to the system of linear inequalities there will be a new set of solutions\nPi+1 but always such that Pi+1 \u2286 Pi. (2) For any two points amin and amax in a convex set it\ncan be implied that there exists a point a^ for which the following is true for its i-th dimension\n\u2203\u03bb \u2208 [0,1] : $a_i = (\u03bb a_{min} + (1 - \u03bb)a_{max})$; (3) Linear Programming can determine the upper and\n:=\u03b1\u03bb\nlower bounds for single variables in a system of linear inequalities, i.e. $[a_{i}^{min}, a_{i}^{max}]$.\nWe start with the original system of linear inequalities with the solution space P\u2081 \u2260 0 which is a\nconvex polytope. We use (3) on P\u2081 to determine the upper and lower bounds for a1, i.e. $[a_{i}^{min}, a_{i}^{max}]$.\nWe sample a value $a_1$ from the range $[a_{i}^{min}, a_{i}^{max}]$. We know that the solution space P\u2081 must contain\nat least one point for which in its 1st dimension a\u2081 = $a_1$ due to (2). In the next step we add the further\nconstraint a\u2081 = $a_1$ to the system of linear inequalities. This updated system of linear inequalities will\nhave the solution space P2. Due to (2) P2 \u2260 0, as well as P2 C P\u2081. We then repeat the entire process\nand use (3) on P\u2082 to determine the upper and lower bounds for a2, i.e. $[a_{i}^{min}, a_{i}^{max}]$.\nAfter the n-th iteration $a_n$ will be determined and we then have completed the generation of point\na* = ($a_1$, ..., a) \u2208 Pn \u2286 ... \u2286 P1, i.e. we succeed generating a point a* that satisfies all original\nconstraints P1."}, {"title": "E The Impact of the Allocation Order", "content": "As already discussed in the ablations in the main paper, with our de-biased initialization the impact of\nthe allocation order is small. However, without our de-biased initialization, the order of the allocation\nhas a significant impact on the performance, as illustrated in Figure 8."}]}