{"title": "Can Large Language Models Reason? A Characterization via 3-SAT", "authors": ["Rishi Hazra", "Gabriele Venturato", "Pedro Zuidberg Dos Martires", "Luc De Raedt"], "abstract": "Large Language Models (LLMs) are said to possess advanced reasoning abilities. However, some skepticism exists as recent works show how LLMs often bypass true reasoning using shortcuts. Current methods for assessing the reasoning abilities of LLMs typically rely on open-source benchmarks that may be overrepresented in LLM training data, potentially skewing performance. We instead provide a computational theory perspective of reasoning, using 3-SAT the prototypical NP-complete problem that lies at the core of logical reasoning and constraint satisfaction tasks. By examining the phase transitions in 3-SAT, we empirically characterize the reasoning abilities of LLMs and show how they vary with the inherent hardness of the problems. Our experimental evidence shows that LLMs cannot perform true reasoning, as is required for solving 3-SAT problems.", "sections": [{"title": "Introduction", "content": "The success and versatility of Large Language Models (LLMs) have sparked widespread interest and debate on whether LLMs are capable of reasoning. The answer to this question may depend on the perspective on reasoning one takes, whether it\n1"}, {"title": "Methods", "content": "3-SAT Phase Transition\nWe study the reasoning capabilities of LLMs on 3-SAT problems, introduced in Figure 1. 3-SAT constitutes one of the most fundamental problems in computer science as it is the prototypical NP-complete problem lying at the foundation of computational complexity theory. To date, it is unknown whether efficient (i.e., polynomial time) algorithms for NP-complete problems exist (c.f. P vs. NP). Importantly, the different problems within the NP-complete class can be translated efficiently to each other [29]. Consequently, any polynomial time algorithm for 3-SAT would result in an efficient algorithm for any other NP-complete problem, e.g. graph coloring or the"}, {"title": "Results", "content": "Can LLMs solve 3-SAT problems?\nWe evaluate GPT-4's performance by measuring its accuracy (of solving for SAT Search and prediction for SAT Decision) across formulas with varying \\(\\alpha\\). We present these results in Figure 3. GPT-4 demonstrates an apparent reasoning competence in the easy regions, while its accuracy significantly drops to \u2248 10% in the hard region for SAT Search. We also observe that SAT Search poses a slightly greater challenge for GPT-4 than SAT Decision.\n\\frac{model \\space count}{2^n}\\nwhere model count is the number of satisfying assignments and n is the number of variables. This ratio denotes the probability that a randomly selected variable assignment satisfies the given 3-SAT theory. We can observe a clear dependence between the accuracy and the satisfiability ratio: formulas with more satisfying assignments tend to be easier for GPT-4 to solve. This holds across both easy and hard regions.\nCan LLM-Modulo frameworks boost performance?\nTo enhance reasoning capabilities, recent studies have explored so-called LLM-Modulo frameworks coined by Kambhampati et al. [40]. The main idea is to augment LLMs"}, {"title": "Discussion", "content": "At first sight, our experimental evaluation can be interpreted as indicating that LLMs, specifically GPT-4, exhibit a certain degree of reasoning capabilities: in the easy regions, GPT-4 solves some 3-SAT problems. One might then argue that GPT-4 does not reach perfect accuracy in these regions because of the scale of the model, and that simply increasing this will resolve the issue. For the hard region, however, it is unlikely that scaling up the model will result in radical improvements.\nTo explain this, reconsider Figures 2 and the time vs. \\(\\alpha\\) plot specifically. The reason why MiniSAT is capable of solving problems in the easy regions faster than problems around \\(a_c\\) is due to the heuristics built into the solver that guide the search for satisfying solutions (e.g. unit propagation and clause learning [44]). That is, heuristics work well when they can exploit statistical features in the problem instance to be solved. To date, there are no known heuristics that work well around \\(a_c\\) (and they are"}, {"title": "Conclusion", "content": "A superficial analysis of the reasoning capabilities of LLMs suggests they possess strong and complex reasoning capabilities a common fallacy [47]. However, our detailed experimental analysis indicates that what appears as reasoning capabilities could be a mirage, with LLMs predominantly exploiting statistical features present in the data and would explain why LLMs have been termed \"statistical parrots\" [48].\nThis is not to say that LLMs lack value quite the opposite. LLMs are highly effective at translating problems into a formal language and then passing these problems on to solvers. This utility is demonstrated by the relatively superior performance of SAT-Translate over SAT-Menu and SAT-CNF. In a more general context, it would require that the class of the problem is recognized correctly - here 3-SAT - and that the right solvers are available.\nFurthermore, LLMs can serve as valuable knowledge bases, leveraging their extensive commonsense and world knowledge through natural language queries to guide search processes. This can be utilized by either solvers guiding LLMs [9] or vice versa, where LLMs assist solvers [43]."}, {"title": "B Supplementary Text", "content": "The section is organized as follows: \u00a7 B.1 where we discuss how GPT-4 performance is impacted by prompt engineering methods. \u00a7 B.2 where we compare GPT-4 with other state-of-the-art LLMs and discuss the common and divergent performance trends.\nImpact of Prompt Engineering\nWe explored 3-shot learning using three chain-of-thought input/output (I/O) demonstrations. These were randomly selected from a set of accurate solutions generated by the LLM and manually checked for consistency between solutions and their explanations. Each I/O example included the input and the output solution, along with its chain-of-thought explanation. From Figure 11, we saw performance gains in the initial Easy-Hard phase but observed a decrease in the subsequent Easy phase.\nAre there differences in the solving ability w.r.t. state-of-the-art LLMs?\nWe compared GPT-4 Turbo against some of the best and most advanced LLMs, including both open-source (Llama 2 70B chat-hf, Mixtral 8 \u00d7 7B) as well as proprietary models (GPT-3.5 Turbo, Gemini 1.0 Pro, PaLM 2 text-bison). In these evaluations, GPT-4 emerges as a notable exception, outshining other LLMs across various metrics (cf. Appendix). We identify both common and divergent performance patterns.\nCommon Performance Trends\nFirstly, the performance consistently correlates with the satisfiability ratio across LLMs, as depicted in Figure 9. Secondly, the performance of LLMs remains unchanged regardless of the input type, as illustrated in Figure 10. Lastly, in general, LLM-Modulo frameworks can enhance performance, as demonstrated in Figure 10.\nDivergent Performance Trends\nIn the 3-SAT Search Problem, as shown in Appendix Figure 7 [Left], GPT-4 uniquely reflects solver-like phase transitions. In contrast, other LLMs exhibit a marked decline in performance in the high \u03b1 region, displaying an Easy-Hard-Hard pattern. For the 3-SAT Decision Problem, while all LLMs perform significantly better on the decision variant, only GPT-4 displays an Easy-Hard-Easy phase transition pattern on the decision problem, as shown in Figure 13. Moreover, GPT-4 is more accurate in detecting unsatisfiable instances, as detailed in Figure 7[Right] and Figure 12. Finally, in contrast with GPT-4, other LLMS show no performance change with in-context learning, as demonstrated in Figure 11."}]}