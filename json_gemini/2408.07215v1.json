{"title": "Can Large Language Models Reason? A Characterization via 3-SAT", "authors": ["Rishi Hazra", "Gabriele Venturato", "Pedro Zuidberg Dos Martires", "Luc De Raedt"], "abstract": "Large Language Models (LLMs) are said to possess advanced reasoning abilities. However, some skepticism exists as recent works show how LLMs often bypass true reasoning using shortcuts. Current methods for assessing the reasoning abilities of LLMs typically rely on open-source benchmarks that may be overrepresented in LLM training data, potentially skewing performance. We instead provide a computational theory perspective of reasoning, using 3-SAT the prototypical NP-complete problem that lies at the core of logical reasoning and constraint satisfaction tasks. By examining the phase transitions in 3-SAT, we empirically characterize the reasoning abilities of LLMs and show how they vary with the inherent hardness of the problems. Our experimental evidence shows that LLMs cannot perform true reasoning, as is required for solving 3-SAT problems.", "sections": [{"title": "Introduction", "content": "The success and versatility of Large Language Models (LLMs) have sparked widespread interest and debate on whether LLMs are capable of reasoning. The answer to this question may depend on the perspective on reasoning one takes, whether it is more oriented toward commonsense reasoning [1] or towards logical or deductive reasoning [2]. We will adhere to Leon Bottou's definition, which defines reasoning as \"algebraically manipulating previously acquired knowledge in order to answer a new question\" [3]. This is aligned with Russell and Norvig's description of artificial intelligence as rational thinking [4].\nRecent studies suggest that LLMs are inherently capable of zero-shot reasoning [5] (i.e. performing multi-step inference processes in previously unseen situations). This ability has been shown to emerge and improve with scale [6], and can be further enhanced by using smart prompting techniques that encourage LLMs to think step-by-step [7]. Demonstrations include, inter alia, planning [8, 9], theorem proving [10], search and optimization [11], self-reflection [12], and tool usage [13].\nConversely, a growing body of research presents a more critical view of these emergent abilities. For instance, LLMs may exhibit limitations in consistent logical reasoning [14], effective planning [15], and accurate self-evaluation of their outputs [16]. During training, language models can fit on statistical features [17] or identify reasoning shortcuts, much like the Clever Hans Cheat [18], thus bypassing true reasoning. There is also growing concern about dataset contamination\u00b9 from open-source benchmarks [19] that can inflate the reasoning performance of LLMs. During inference, the autoregressive nature of LLMs makes them prone to snowballing errors over time [20]. Furthermore, these models can often generate unfaithful and biased explanations in their chains of thought [21]. Additionally, their greedy approach to reasoning often falls short in contexts with multiple valid reasoning steps [22]. On the architectural side of LLMs, findings reveal that the transformer layer is incapable of function composition for large domains [20, 23]. From a theoretical standpoint, transformer computations have been shown to lie in the complexity class log-uniform TC\u00ba, which is too restrictive even for simple logical reasoning tasks such as 2-SAT2. Given these limitations, it has been suggested that the emergent abilities are but a mere mirage stemming from the choice of metric [25]. This inevitably leads to the question: \u201cCan Large Language Models reason\", and if so, to what extent?"}, {"title": "Methods", "content": "We study the reasoning capabilities of LLMs on 3-SAT problems, introduced in Figure 1. 3-SAT constitutes one of the most fundamental problems in computer science as it is the prototypical NP-complete problem lying at the foundation of computational complexity theory. To date, it is unknown whether efficient (i.e., polynomial time) algorithms for NP-complete problems exist (c.f. P vs. NP). Importantly, the different problems within the NP-complete class can be translated efficiently to each other [29]. Consequently, any polynomial time algorithm for 3-SAT would result in an efficient algorithm for any other NP-complete problem, e.g. graph coloring or the traveling salesman problem. Furthermore, various prevalent reasoning problems in artificial intelligence, such as planning and constraint satisfaction, can be reduced to solving 3-SAT problems [30].\nAn interesting empirical observation is the presence of a phase transition in 3-SAT problems [27]. When randomly sampling 3-SAT formulas, one can observe a sharp change in the probability of a 3-SAT formula being satisfiable when plotted against $\u03b1 = m/n$, where $m$ is the number of clauses and $n$ is the number of variables. For 3-SAT, this phase transition occurs at $\u03b1_c \u2248 4.24$ [31], i.e. the point at which a randomly sampled 3-SAT formula has equal probability to be satisfiable or unsatisfiable. This naturally divides 3-SAT problems into three regions: the under-constrained region below the threshold (easy), the constrained region in the neighborhood of the threshold (hard), and the over-constrained region above the threshold (easy), cf. Figure 2.\nAnalogously to characterizing SAT solvers by their behavior on solving problems with varying \u03b1, we study the reasoning capabilities of LLMs with respect to the phase transition in 3-SAT problems. A similar thought experiment framework was also proposed by Kambhampati et al [32]. Note that our goal is not to build 3-SAT solvers powered by LLMs. On the contrary, we use the 3-SAT phase transition to assess the reasoning abilities of LLMs using a well-established experimental protocol. This contrasts with other works that characterize reasoning by evaluating performance on benchmark datasets [33]."}, {"title": "LLMs as 3-SAT Solvers", "content": "To use LLMs as 3-SAT solvers, we reframe the 3-SAT problem as a natural language menu-selection problem, termed as SAT-Menu. As shown in Box 1, the prompt input to the LLM consists of a task outline, along with a specific scenario detailing the dietary preferences of a set of people. The LLM's objective is to identify a combination of orderable (akin to positive literals) and non-orderable (akin to negative literals) food items that meet these preferences, or declare the situation unsatisfiable (unSAT) if no valid combination exists. For details of dataset generation, see Figure 6 in materials and methods. Note that the prompt example in Box 1) constitutes a minimal example stripped of all details. The complete system prompt incorporates techniques known to enhance the apparent reasoning capabilities of LLMs, such as chain-of-thought (CoT) [7] and in-context learning [34] (see Box 2).\nAdditionally, we introduce a second problem formulation where the LLM is directly given the underlying 3-SAT formula in Conjunctive Normal Form (CNF). We refer to this scenario as SAT-CNF. Specifically, in this setting, the problem is presented as a list of integers to the LLM, similar to the approach outlined in SAT Game (Figure 1). For more details about the prompt, we refer the reader to Box 3 in materials and methods.\nTo assess the reasoning capabilities of LLMs, we analyze their performance on two variants of the 3-SAT problem. The first variant is the 3-SAT Decision problem, where the LLM acts as a solver and must determine whether or not a given 3-SAT problem is satisfiable. If the problem has a satisfiable assignment, the LLM should respond with \"yes\" and with \"no\" if it is unsatisfiable. The second variant is the 3-SAT Search problem, where the LLM's task extends beyond providing a simple \"yes\" or \"no\" response. If the formula is satisfiable, the LLM should also return an assignment to the variables that satisfies the formula. However, if the formula is found to be unsatisfiable, the LLM should once again respond with \"no.\u201d"}, {"title": "Results", "content": "We evaluate GPT-4's performance by measuring its accuracy (of solving for SAT Search and prediction for SAT Decision) across formulas with varying \u03b1. We present these results in Figure 3. GPT-4 demonstrates an apparent reasoning competence in the easy regions, while its accuracy significantly drops to \u2248 10% in the hard region for SAT Search. We also observe that SAT Search poses a slightly greater challenge for GPT-4 than SAT Decision.\nIn Figure 4 we also plot GPT-4's performance against the satisfiability ratio, defined as $\\frac{model count}{2^n}$, where model count is the number of satisfying assignments and n is the number of variables. This ratio denotes the probability that a randomly selected variable assignment satisfies the given 3-SAT theory. We can observe a clear dependence between the accuracy and the satisfiability ratio: formulas with more satisfying assignments tend to be easier for GPT-4 to solve. This holds across both easy and hard regions."}, {"title": "Discussion", "content": "At first sight, our experimental evaluation can be interpreted as indicating that LLMs, specifically GPT-4, exhibit a certain degree of reasoning capabilities: in the easy regions, GPT-4 solves some 3-SAT problems. One might then argue that GPT-4 does not reach perfect accuracy in these regions because of the scale of the model, and that simply increasing this will resolve the issue. For the hard region, however, it is unlikely that scaling up the model will result in radical improvements.\nTo explain this, reconsider Figures 2 and the time vs. \u03b1 plot specifically. The reason why MiniSAT is capable of solving problems in the easy regions faster than problems around $\u03b1_c$ is due to the heuristics built into the solver that guide the search for satisfying solutions (e.g. unit propagation and clause learning [44]). That is, heuristics work well when they can exploit statistical features in the problem instance to be solved. To date, there are no known heuristics that work well around $\u03b1_c$ (and they are unlikely to exist due to the NP-hardness of 3-SAT). Solvers therefore have to resort to brute force search around $\u03b1_c$.\nIn this light, we can reinterpret the experimental performance of GPT-4 on 3-SAT problems: GPT-4's apparent reasoning capabilities (in the easy regions) is due to the presence of statistical features that it can leach onto. Conversely, in the hard region, the drop in performance can be attributed to GPT-4's - and by extension current transformer-based LLMS' inability to reason according to Bottou's definition. A similar observation has been made for a computationally tractable class of problems (not NP-complete) using BERT-style language models [17].\nOur fine-grained empirical study also complements the theoretical results of LLM reasoning capabilities, c.f. log-uniform TCO complexity class. As these findings only provide bounds on worst-case performance, they have a rather limited significance with regard to average-case complexity [45] and, as such, a limited significance for the reasoning capabilities of LLMs in practice.\nEven though the basic 3-SAT phase transition provides a rigorous framework for studying the reasoning capabilities of LLMs, we need to point out that there exists many more results about 3-SAT that might be considered as well. For instance, it has been found that certain aspects of the hardness of 3-SAT instances cannot be explained using the easy-hard-easy regimes induced by the phase transition [46]. Nevertheless, the phase transition seems to be adequate in our settings as demonstrated by the drop in performance of LLMs in the vicinity of $\u03b1_c$."}, {"title": "Conclusion", "content": "A superficial analysis of the reasoning capabilities of LLMs suggests they possess strong and complex reasoning capabilities a common fallacy [47]. However, our detailed experimental analysis indicates that what appears as reasoning capabilities could be a mirage, with LLMs predominantly exploiting statistical features present in the data and would explain why LLMs have been termed \"statistical parrots\" [48].\nThis is not to say that LLMs lack value quite the opposite. LLMs are highly effective at translating problems into a formal language and then passing these problems on to solvers. This utility is demonstrated by the relatively superior performance of SAT-Translate over SAT-Menu and SAT-CNF. In a more general context, it would require that the class of the problem is recognized correctly - here 3-SAT - and that the right solvers are available.\nFurthermore, LLMs can serve as valuable knowledge bases, leveraging their extensive commonsense and world knowledge through natural language queries to guide search processes. This can be utilized by either solvers guiding LLMs [9] or vice versa, where LLMs assist solvers [43]."}]}