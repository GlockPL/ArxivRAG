{"title": "ACCELERATING TRAINING WITH\nNEURON INTERACTION AND NOWCASTING NETWORKS", "authors": ["Boris Knyazev", "Abhinav Moudgil", "Guillaume Lajoie", "Eugene Belilovsky", "Simon Lacoste-Julien"], "abstract": "Neural network training can be accelerated when a learnable update rule is used\nin lieu of classic adaptive optimizers (e.g. Adam). However, learnable update\nrules can be costly and unstable to train and use. A simpler recently proposed\napproach to accelerate training is to use Adam for most of the optimization steps\nand periodically, only every few steps, nowcast (predict future) parameters. We\nimprove this approach by Neuron interaction and Nowcasting (NINO) networks.\nNINO leverages neuron connectivity and graph neural networks to more accurately\nnowcast parameters by learning in a supervised way from a set of training trajec-\ntories over multiple tasks. We show that in some networks, such as Transformers,\nneuron connectivity is non-trivial. By accurately modeling neuron connectivity, we\nallow NINO to accelerate Adam training by up to 50% in vision and language tasks.", "sections": [{"title": "INTRODUCTION", "content": "Modern deep learning models, such as large language\nmodels (Touvron et al., 2023), are trained using classic\nadaptive optimizers, such as Adam (Kingma & Ba,\n2014; Loshchilov & Hutter, 2017). These optimizers up-\ndate neural network parameters $\\theta \\in \\mathbb{R}^n$ using gradient\ndescent at step $t$ as $\\theta_{t+1} = \\theta_{t} - \\Delta \\theta$, $\\forall i \\in [1, n]$, where\n$\\Delta \\theta$ is the update computed analytically based on the\nhistory of parameter values, gradients and the learning\nrate. Recently, Jang et al. (2023); Sinha et al. (2017)\nshowed that parameters $\\theta$ follow a predictable trend\nso that optimization can be accelerated by nowcasting\n(predicting future) parameters using a learnable function\n$f: \\theta_{t+k} = \\theta_t + f(\\theta_t, \\theta_{t-1}, \\theta_{t-2},...)$, where\n$k \\gg 1$ is the future horizon. More popular learnable\napproaches to speed up optimization, such as \"learning to\noptimize\u201d (L2O), are recurrently applied\nat every step $t$ (Andrychowicz et al., 2016; Metz et al., 2022). Compared to L2O, the parameter\nnowcaster $f$ is applied very rarely reducing its overhead, e.g. a base optimizer such as Adam is run\nfor 1k steps followed by the prediction step (Fig. 1). Moreover, training such $f$ is simpler than L2O,\nsince a supervised loss can be used instead of a more challenging meta-learning loss with recurrent\ninner steps. However, akin to Adam, $f$ from prior works does not directly leverage the structural\ninformation of $\\theta$, such as connectivity between neurons and layers. This structure has been shown\ncritical for many parameter representation tasks, such as property prediction (Navon et al., 2023;\nZhou et al., 2023; Kofinas et al., 2024)."}, {"title": "RELATED WORK", "content": "Learning to optimize (L2O). The L2O literature has offered many approaches to learn a neural\nnetwork (optimizer) that optimizes the parameters of other neural nets (optimizees) (Andrychowicz\net al., 2016; Chen et al., 2021; 2022; Amos, 2022). Among them, Safeguarded L2O (Heaton et al.,\n2023; Premont-Schwarz et al., 2022) is most related to ours as it switches between an L2O optimizer\nand SGD/Adam. While Safeguarded L2O alleviates the meta-generalization challenge (Th\u00e9rien et al.,\n2024), training an L2O optimizer remains costly and unstable due to the use of meta learning methods\nand its long inner loop unrolls required at each meta-training iteration (Metz et al., 2019; 2022).\nMoreover, overheads of L2O add up at each iteration making it more computationally intensive than\nAdam. In contrast, our approach follows weight nowcaster networks (WNNs) (Jang et al., 2023),\nwhere the nowcaster model is applied very rarely (e.g. once per 1k steps of Adam) making the total\noverhead negligible, yet still speeding up training significantly.\nParameter prediction and generation. This area has grown significantly recently, primarily aiming\nat reducing computational costs of training neural nets (Peebles et al., 2022; Ashkenazi et al., 2022;\nSch\u00fcrholt et al., 2022; 2024; Knyazev et al., 2021b; 2023; Zhou et al., 2024c; Soro et al., 2024; Wang\net al., 2024). Most related to us are IntrospectionMLP (Sinha et al., 2017) and WNNs (Jang et al.,\n2023) serving the basis of our work. These methods train simple MLPs to periodically (e.g. every few\nepochs of Adam) predict future parameter values of a neural net given its past parameters (Section 3.1).\nHowever, their MLPs predict parameter coordinate-wise (for each parameter independently) similar\nto optimization methods without leveraging the structure of neural networks. Moreover, their\nMLPs predict parameters only for a predefined future horizon k, whereas different tasks and different\noptimization stages can have different parameter evolution trends (Morchdi et al., 2022; Guille-Escuret\net al., 2023; Lange et al., 2023). We address these shortcomings by conditioning the prediction on k.\nRepresentation learning of neural network parameters. This area has also developed fast re-\ncently (Navon et al., 2023; Sch\u00fcrholt et al., 2021; 2024; Ashkenazi et al., 2022; Andreis et al., 2023;\nZhou et al., 2023; 2024b;a). One of the main goals in these works is to model neuron permutation\nsymmetry \u2013 the property of neural networks that if the neurons in one layer are permuted in the same\nway as the neurons in the next layer, the neural network preserves its function (Hecht-Nielsen, 1990).\nAccurate modeling of this symmetry allows for better estimation of network properties or encoding\nimplicit neural representations. To model neuron permutation symmetry, Kofinas et al. (2024); Lim\net al. (2023) proposed neural graphs (graphs of neurons) enabling the usage of graph neural networks\n(GNNs) (Kipf & Welling, 2016; Corso et al., 2020). Remarkably, as GNNs can digest graphs of any\nsize and connectivity, the synergy of neural graphs and GNNs enables processing diverse parameters\nfrom different architectures and tasks using a single GNN. We leverage both the neural graphs and\nGNNs to learn a single NiNo model that can accelerate optimization in diverse tasks.\nDynamic models. Our work is also related to dynamic interactive models, where message passing\nnetworks and GNNs are used to learn the interaction within relational temporal networks (Trivedi et al.,\n2019; Knyazev et al., 2021a) and physical systems (Kipf et al., 2018; Sanchez-Gonzalez et al., 2018;\n2020; Zambaldi et al., 2019). However, developing a strong parameter prediction model requires many\nspecific design choices and careful modeling of neuron permutation symmetry, motivating our work."}, {"title": "BACKGROUND", "content": "We consider a neural network parameterized by $\\theta_t \\in \\mathbb{R}^n$ trained for $t$ iterations with Adam (or another\noptimizer). Our goal is to accelerate optimization by $k \\gg 1$ using a meta-network $f_{\\phi}$, parameterized\nby $\\phi$, predicting future parameters $\\hat{\\theta}_{t+k}$ given its past values: $\\hat{\\theta}_{t+k} = \\theta_t + f_{\\phi}(\\Theta_t, \\theta_{t-1}, \\theta_{t-2}, ...).$\n3.1 WEIGHT NOWCASTER NETWORKS (WNNS)\nWNNs (Jang et al., 2023) model $f_{\\phi}$ as an MLP predicting the delta (update difference) of the $i$-th\nparameter: $\\Delta \\hat{\\theta}_{\\tau+k}^i = f_{\\phi}(\\theta_{\\tau:{\\tau_c}}^i)$, where $\\tau_c = \\tau - c + 1$ and $c$ is the context length.\nHere, $\\theta_{\\tau}^i$, $\\theta_{\\tau-1}^i$, ... are epoch indices and $\\theta_{\\tau:{\\tau_c}}^i$ are the parameters scaled based on the range of values in\n$[\\theta_{\\tau}^i, \\theta_{\\tau-1}^i, ..., \\theta_{\\tau_c}^i]$ (see details in Section A.1). The predicted parameter value is obtained by unscaling the pre-\ndicted delta: $\\hat{\\theta}_{\\tau+k}^i = \\theta_{\\tau}^i + \\text{unscale}(\\Delta \\hat{\\theta}_{\\tau+k}^i)$. WNNs are trained in a supervised way by collecting a\ntraining dataset of parameter trajectories $\\{[\\theta_1, \\theta_2, ...]_i\\}_{i=1}^n$ obtained with Adam and applying the $l_1$-loss:\n$\\text{argmin}_{\\phi} \\sum_{i=1}^n ||\\Delta \\hat{\\theta}_{\\tau+k}^i - \\Delta \\theta_{\\tau+k}^i||_1,$\n(1)\nwhere $\\Delta \\theta_{\\tau+k}^i$ are the scaled target parameter deltas. Sinha et al. (2017) proposed the approach\noriginally, however WNNs are applied periodically, once per every $c$ epochs of a base optimizer\n(Adam) and $k = c$, which better accelerates optimization. For example, to use a trained $f_{\\phi}$ on a\nnew task, first Adam is run for $c$ epochs ($c = 5$ by default) after which $f_{\\phi}$ is applied to predict future\n(10-th epoch) parameters, after which the procedure repeats (the base optimizer is continued for\nanother 5 epochs followed by parameter prediction) until convergence. So the WNN is applied very\nrarely, e.g. in the case of 200 Adam steps per epoch, the WNN is applied only once per 1,000 steps.\n3.2 (NAIVE) NEURAL GRAPH OF TRANSFORMERS\nWNNs apply an MLP for parameter $\\theta$ given only its past values. Such an MLP is inherently\nlimited, since future parameter values depend on many factors, including connectivity of neu-\nrons. Modeling neuron connectivity in a way that generalizes to arbitrary network structures\nto make parameter prediction as general as possible is challenging. Simple parameter repre-\nsentations, e.g. flattening of parameters into a vector (Sch\u00fcrholt et al., 2021), are not general\nand do not model neuron permutation symmetry (the neurons can be permuted without chang-\ning the network output). Recently, Kofinas et al. (2024); Lim et al. (2023) proposed to repre-\nsent $\\theta$ using a neural graph $G(\\theta) = (V, E)$ with node features $V \\in \\mathbb{R}^{|V| \\times d_V}$ and edge fea-\ntures $E \\in \\mathbb{R}^{|V| \\times |V| \\times d_E}$, where $d_V, d_E$ are the node and edge feature dimensionalities, respec-\ntively. For example, equation 2 shows the edge features for an $L$-layer MLP (we ignore biases\nfor simplicity): $\\theta = \\{W^{(1)}, ..., W^{(L)}\\}$, where $W^{(l)} \\in \\mathbb{R}^{d_{l-1} \\times d_l}$ are the weights for $l \\in [1, L]$.\nNeural graphs can theoretically represent the parameters\nof any neural network taking into account neuron permuta-\ntion symmetry as the rows and columns of weight matrices\ncorrespond to the nodes in the neural graph. This way, neu-\nral graphs impose a strong inductive bias similarly to using\nconvolution for images. So a model operating on neural\ngraphs, such as a graph neural network (GNN) and our\nNiNo model, can be used in diverse tasks and should be\nable to learn parameter prediction rules that generalize bet-\nter with fewer samples than, for example, MLPs. However,\nto fully leverage the neural graphs in practice, we need to accurately construct them, which is not\ntrivial for such networks as transformers, motivating us in this work.\nTransformers. For input $x \\in \\mathbb{R}^{N \\times d}$, where $N$ is a sequence length, a multi-head self-attention\n(MSA) layer (Vaswani et al., 2017) for each head $h \\in [1, H]$ projects $x$ using $W_q^h, W_k^h, W_v^h$ followed\nby self-attention and projection using another weight matrix $W_o^h$ (see Section A.4 for details):\n$A_h = \\frac{(x W_q^h)(x W_k^h)^T}{\\sqrt{d}}$ and $y_h = \\text{softmax}(A_h) (x W_v^h)$ and $MSA(x) = \\sum_{h=1}^H y_h W_o^h.$ (3)"}, {"title": "METHODS", "content": "In this section, we describe the details of our neural graphs for an MSA layer of transformers and\nthe Neuron Interaction and Nowcasting (NiNo) networks operating on such graphs (see a model overview\nin Fig. 4 and a graph example in Fig. 7). Following WNNs, NiNo is also applied very rarely during\nthe optimization process (Section 3.1). However, NiNo leverages the representation power of neural\ngraphs and GNNs to speed up optimization more significantly than WNNs, as we show in Section 5.\n4.1 NEURAL GRAPH OF TRANSFORMERS\nTo construct neural graphs that accurately model neuron permutation symmetry in MSA, we (1) re-\nstrict neuron permutations across heads and (2) relax permutations of weights $W^o, W^k$ (Fig. 2).\n(1) Restricting permutations across heads. We first observe that splitting computations into $H$\nparallel heads (equation 3) breaks neuron permutation symmetry across the heads, so shuffling neurons\nacross the heads may change the overall output of MSA(x). Consider, for example, an MSA layer\nwith $d = 6$ and $H = 2$ (Fig. 2a) and the dot product between outputs $x_1$ and $(x^k)^T$ obtained after\nprojecting $x$ using the weights of the first head $W_q^1$ and $W_k^1$: $x_1 (x^k)^T = [x_1, x_2, x_3][x_1, x_2, x_3]^T$.\nThe result of this dot product does not change when permuting neurons in both $W_q^1$ and $W_k^1$ using\nthe same $\\pi$, since it results in permuting the outputs: $\\pi(x_1) \\pi(x^k)^T = x_1 (x^k)^T$. However, consider\nshuffling neurons across head 1 and 2 in $W^o$ and $W^k$, e.g. neurons 3 and 4 (highlighted in different\ncolors in Fig. 2a) resulting in: $[x_1, x_2, x_4][x_3, x_5, x_6]^T$. Now the dot product no longer equals\n$x_1 (x^k)^T$ unless $x_1 x_4 = x_3 x_5$. So self-attention matrix $A_1$ for head 1 and MSA(x) can change.\nTo take into account this restriction, we allow for neuron permutations only within a head by adding\na separate node for each head that connects the appropriate neurons so that permuting neurons 3 and\n4 from our example changes the graph while permuting withing a head does not change it (Fig. 2c).\n(2) Relaxing permutations for $W^o, W^k$. We also observe that neurons in $W^o$ and $W_k$ can be\npermuted in a different way ($\\pi'$) from $W_q$ and $W_v$ without changing the MSA output:\n$(x W_q^h)(x W_k^h)^T = (x \\pi'(W_q^h))(x \\pi'(W_k^h))^T$ and $(x W_v^h) W_o^h = (x \\pi(W_v^h)) \\pi \\pi(W_o^h)$. (4)"}, {"title": "NEURON INTERACTION AND NOWCASTING NETWORKS (NINO)", "content": "Given an input neural graph $G(\\Theta_{\\tau:{\\tau_c}})$, our NiNo model processes it using layerwise scaling layer,\nnode and edge embedding layers and GNN layers with a hidden size $D$. NiNo then predicts\nedge features for multiple steps in the future $\\mathcal{E}_{1:K}$ that are mapped back to the parameter space\n$[\\Delta \\theta_{\\tau+1}, ..., \\Delta \\theta_{\\tau+K}]$ using the inverse neural graph construction $\\Delta \\theta_{\\tau+k} = G^{-1}(\\mathcal{E}_k)$ (Fig. 4).\nLayerwise scaling. Since parameter values can vary in scale significantly across architectures and\ntasks, it is important to scale them appropriately. Compared to WNNs (Jang et al., 2023) scaling each\nparameter independently using min-max, we extend a layerwise scaling (Sch\u00fcrholt et al., 2022) to a\nsequence of parameters. Specifically, given weights $W_{\\tau:{\\tau_c}}^l \\in \\mathbb{R}^{d_{l-1} \\times d_l \\times c}$ of the $l$-th layer of the input\n$\\mathcal{E}_{\\tau:{\\tau_c}}$, we obtain scaled weights as $\\hat{W_{\\tau:{\\tau_c}}}^l = (W_{\\tau:{\\tau_c}}^l - \\mu^l)/\\sigma^l$, where $\\mu^l, \\sigma^l$ are the scalar\nmean and standard deviation of $W_{\\tau:{\\tau_c}}^l$. After repeating this step $\\forall l \\in [1, L]$, we obtain scaled $\\hat{\\mathcal{E}}_{\\tau:{\\tau_c}}$.\nEmbedding layers and GNN. We use linear $\\Phi_{lpe}, \\Phi_e$ and embedding $\\Phi_w, \\Phi_{type}$ layers to project node\nand edge features to the $D$-dimensional space followed by $M$ GNN layers and the output layer $DMS$:"}, {"title": "EXPERIMENTS", "content": "We experiment with nine tasks, each defined by a dataset and a neural network architecture in the\nvision or language domains (Table 1). Four of the tasks, the in-distribution tasks, are of a relatively\nsmaller scale and used to train our meta-models (NiNo, WNN and their variants). The other five tasks,\nthe out-of-distribution tasks, differ from the in-distribution tasks in the architecture and/or dataset.\nIndexing parameters. Since the notion of epoch used for $\\tau$ in WNNs (Section 3.1) can be ill-defined\nin some tasks (e.g. in language tasks the models are often trained only for 1 epoch), in our experiments\nwe found $\\tau$ and $\\tau+1$ indexing parameters at step $t$ and $t+200$ to be a well performing strategy in all the\ntasks, so for our default context length $c = 5$ we apply the meta-models every $5 \\times 200 = 1,000$ steps."}, {"title": "SETUP", "content": "Vision tasks. We use the FashionMNIST (F), CIFAR-10 (C10) and CIFAR-100 (C100) datasets and\nthree layer convolutional neural networks with 16, 32 and 32 channels (e.g. task FM/16) or 32, 64\nand 64 channels (e.g. task C100/32). The convolutional architectures are the same as in the L2O\nexperiments of Kofinas et al. (2024) to enable fair comparison. In all cases, these tasks are optimized\nusing Adam (Kingma & Ba, 2014) without weight decay, with a constant learning rate of 6e-3 (for\nFashionMNIST) or 3e-3 (for CIFAR) with a batch size of 128 for T=10k steps.\nLanguage tasks. We use the LM1B (Chelba et al., 2014) and WikiText103 (WIKI) (Merity et al.,\n2016) datasets and train GPT2 style transformers (Radford et al., 2019) with 3 layers, 24 hidden units\nand 3 attention heads (denoted as 3-24); 2 layers, 32 units and 2 heads (2-32) or 3 layers, 64 units and\n4 heads (3-64). These tasks are optimized for the next token prediction loss with AdamW (Loshchilov\n& Hutter, 2017), weight decay 1e-2, learning rate 2e-4, batch size of 32, sequence length of 1024\nfor either 1 epoch (for LM1B) or 4 epochs (for WIKI) corresponding to around 24k or 14k steps\nrespectively. We use a predefined GPT2 tokenizer in all the cases with a fixed vocabulary.\nMeta-training dataset. We use FM/16, C10/16, LM1B/3-24 and LM1B/2-32 as the in-distribution\ntasks training 300, 300, 200 and 200 models in each task respectively (C = 1000 models in total).\nWe save the checkpoints of intermediate steps, having in total around 1.6 \u00d7 106 checkpoints. Our\ndataset is large and diverse (Fig. 8), yet it is relatively cheap to be collected as the tasks are small.\nBaselines. As a reference optimization method we use Adam for vision and AdamW for language\ntasks with a constant learning rate that we tune independently on each task based on the validation\nmetric. Following WNNs (Jang et al., 2023) we use Linefit as another baseline (Section A.2). We\nfurther improve it with a simple scaling term to promote more recent parameters denoted as Linefit+\n(see Section A.3). As the strongest baseline, we use WNN with our layerwise scaling and k-decay\nwhich we denote as WNN+. Finally, we use the learning to optimize (L2O) model from Kofinas et al.\n(2024), which showed very strong results on similar tasks. We use their L2O (NG-GNN) pretrained\non FM/16 as retraining their model on all our in-distribution tasks is expensive. For a fair comparison"}, {"title": "RESULTS", "content": "Main results. Our NiNo model shows consistently better performance than Linefit and WNN and\ntheir improved variants on all the nine tasks (Table 2). On average we speed up Adam optimization\nby 48.9% reducing the number of steps to achieve the target performance roughly by half. NiNo is\nfollowed by WNN+ and NiNo with a naive graph (Section 4.1). While the latter achieves slightly better\nperformance on the FM/16 in-distribution task, in the out-of-distribution tasks this model significantly\nunderperforms to NiNo, especially in the language tasks. This performance gap is expected since\nNiNo represents neural graphs for transformers more accurately as we further validated qualitatively\n(Fig. 2d,e) and quantitatively (Section A.6). Other baselines perform poorly, but we highlight that\nboth Linefit baselines compare favorably to WNN indicating that the latter could have learned a very\nsimple rule despite training on a lot of data.\nComparison to L2O. L2O performs the best in-distribution on the same task (Fig. 6a), however when\napplied to an unseen task it overfits to the training set (Fig. 6b). In contrast, NINO/FM16 trained on the\nsame task as L2O performs well on the validation set of unseen tasks, presumably because Adam is\nused for most of the steps and for the input NiNo uses the trajectory of parameters of the target task to\nmake the prediction. In addition, L2O has an overhead at every optimization step making the total com-\nputation cost noticeable (Table 6 in Section A.7). It is also more computationally challenging to meta-\ntrain it on multiple tasks, therefore we only use a pretrained L2O from Kofinas et al. (2024). Since\nL2O does not reach the target validation scores in most tasks, we compare it only using the curves."}, {"title": "ANALYSIS", "content": "Parameter space. We show how the parameters evolve during training on C10/32 by projecting them\nto 2d using PCA (Fig. 4a). We show the first 8,000 steps for Adam/AdamW and 4,000 steps for other\nmethods with 200 steps between points. In the beginning of training, both WNN and NiNo make big\nsteps along the Adam/AdamW when predicting the parameters, while L2O diverges early. At later op-\ntimization stages, NiNo makes better predictions than WNN as the latter leads to significant loss spikes.\nInterestingly, NiNo's trajectory closely corresponds to a 2\u00d7 faster AdamW, even though NiNo's base\noptimizer is Adam in this task. This indicates that NiNo may implicitly regularize optimization.\nGraph embedding space. We optimize three different transformers on the language task and visualize\nhow the neural graph embeddings evolve during training (Fig. 4b,c). We construct embeddings by\ncomputing the average edge/parameter features after the last NiNo/WNN+ layer by feeding the 5 past"}, {"title": "CONCLUSION AND FUTURE WORK", "content": "We proposed a novel NiNo model to accelerate training of neural networks, including transformers.\nOur work makes a step in a relatively underexplored area of periodic parameter prediction, which\nas we show has a lot of potential in reducing training costs. In addition to reducing training costs,\nwe reveal potentially useful byproducts of NiNo, such as low-dimensional encoding of network\nparameters during training, which can be used to analyze models and their training dynamics. We\nbelieve that our work can promote interesting future research. In particular, scaling up our approach\nappears to be intriguing as the scaling trends show better speed ups with more data and larger models."}, {"title": "APPENDIX", "content": "A.1 WNN DETAILS\nIn WNNs (Jang et al., 2023), parameter scaling is performed by first computing the minimum and\nmaximum values in the input parameter values: $s^i = \\text{max}(\\theta_{\\tau}^i, ..., \\theta_{\\tau_c}^i) - \\text{min}(\\theta_{\\tau}^i, ..., \\theta_{\\tau_c}^i), \\forall i \\in$\n$[1, n]$. Each $i$-th parameters scaling factor $s^i$ is then used to scale both the inputs: $\\hat{\\theta_{\\tau}}^i = \\theta_{\\tau}^i/(s^i + \\epsilon)$,\nand, only for training WNNs, the targets: $\\hat{\\theta_{\\tau+k}}^i = \\theta_{\\tau+k}^i/(s^i + \\epsilon)$. Jang et al. (2023) also subtract\nthe last element from the sequence, however we found this to be redundant. Unscaling is performed\nby multiplying the prediction by $s^i$. Scaling and unscaling are performed per parameter (for each\nparameter independently).\nA.2 LINEFIT\nSinha et al. (2017); Jang et al. (2023) introduced a \u201clinefit\u201d baseline to predict future parameter values\nby extrapolating the line to a future point:\n$\\forall i \\in [1, n]:$\n$\\hat{\\theta_{\\tau+k}}^i = 2a^i x + b^i,$\n(10)\nwhere $a^i, b^i$ are obtained by optimizing the following objective:\n$\\text{argmin}_{a^i, b^i} ||a^i x + b^i - \\Theta_{\\tau:{\\tau_c}}^i ||_2,$\n(11)\nwhere $x = [1, 2, ..., c]$ and $\\Theta_{\\tau:{\\tau_c}}^i = [\\theta_{\\tau}^i, \\theta_{\\tau-1}^i, ..., \\theta_{\\tau_c}^i] \\in \\mathbb{R}^c$. This baseline fits a line ($a^i, b^i$) for\neach parameter $i$ given only the past values of the same parameter without collecting a training\ndataset. Despite its simplicity, this baseline was shown to improve on using Adam/SGD only.\nA.3 LINEFIT+\nLinefit outlined in Section A.2 can be viewed as a form of momentum that promotes the parameters to\nbe consistent with their global trend. However, Linefit weighs all past values uniformly when fitting\nthe line, while in the momentum more recent values have more effect on the weight update (Sutskever\net al., 2013). Motivated by this observation, we introduce the Linefit+ baseline that penalizes more\nthe errors for later parameter values in the trajectory:\n$\\text{argmin}_{a^i, b^i} ||\\mu (a^i x + b^i - \\Theta_{\\tau:{\\tau_c}}^i)||_2,$\n(12)\nwhere $\\mu = [1/c, 2/c, ..., 1].$\nA.4 NAIVE NEURAL GRAPH OF TRANSFORMERS\nA multi-head self-attention (MSA) layer of transformers (Vaswani et al., 2017) consists of three\nweight matrices $W_q, W_k, W_v$ applied to the input $x \\in \\mathbb{R}^{N \\times d}$ and another weight matrix $W_o$\nreturning the output, where $N$ is a sequence length and all the four weight matrices are $d \\times d$. To\ncompute MSA with $H$ heads, the weight matrices are typically split into $H$ groups across columns for\n$W_q, W_k, W_v$ and rows for $W_o$ (Fig. 2a). The heads are processing the input in parallel, $\\forall h \\in [1, H]$\nusing equation 3.\nNeural graphs defined by Kofinas et al. (2024) and Lim et al. (2023) are conceptually similar, however\nLim et al. (2023) model biases $b^{(l)}$ and normalization layers as extra nodes connected to the neurons\nin the $l$-th layer, hence the node features of Lim et al. (2023) do not include any parameters (Fig. 2c).\nWe build on this variant, since it is more easily to implement it for the layers with complicated neuron\npermutation symmetry, such as multi-head self-attention. Compared to Kofinas et al. (2024), Lim\net al. (2023) also provide a slightly different description for the MSA neural graph, however the\n$W_q, W_k, W_v$ edges are also stacked. Given the lack of detailed description for the multi-head case\nand no implementation currently available publicly, it makes it challenging to use their neural graphs\ndirectly. Therefore, we use (Kofinas et al., 2024) as the base (naive) neural graph when constructing\nMSA layers."}, {"title": "GRAPH NEURAL NETWORK", "content": "A Graph Neural Network (GNN) layer with edge features can be formulated using the mean aggrega-\ntion as follows (Corso et al.", "2020)": "v_i^{(k+1)} = \\Phi_a (v_i^{(k)}, \\overline{m_i^{(k)}})$, where $\\overline{m_i^{(k)}} = \\frac{1}{|\\mathcal{N}(i)|}\\sum_{j \\in \\mathcal{N}(i)} m_{ij}^{(k)}$ is computed using\nmessage passing $\\phi_m$ given node and edge features $v_i, v_j, e_{ij}$; $\\Phi_a$ is an MLP; $\\mathcal{N}(i)$ are the neighbors\nof node $i$. Kofinas et al. (2024) modify how $m_{ij}$ is computed to better incorporate edge features and\nintroduce an edge update step to make better per edge predictions (Table 5). Stacking such layers\nform a GNN that (1) does not change the size of the input graph and (2) is permutation equivariant.\n(1) means that for the neural graphs there are $|\\Theta|' \\times d_E$ input edge features and $|\\Theta|' \\times 1$ predictions,\nwhere $|\\Theta|'$ is the number of trainable parameters in the input model plus auxiliary non-trainable\nparameters (e.g. residual and head connections). (2) means that any permutation of input nodes\nresults in the same permutation of the output nodes and corresponding edges. These properties make\nGNNs a suitable model to predict future parameters and to work with neural graphs where nodes\n(neurons) can be permuted in"}]}