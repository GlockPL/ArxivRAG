{"title": "VIOLENCE DETECTION IN VIDEOS USING DEEP RECURRENT\nAND CONVOLUTIONAL NEURAL NETWORKS", "authors": ["Abdarahmane Traor\u00e9", "Moulay A. Akhloufi"], "abstract": "Violence and abnormal behavior detection research have known an increase of interest in recent years,\ndue mainly to a rise in crimes in large cities worldwide. In this work, we propose a deep learning\narchitecture for violence detection which combines both recurrent neural networks (RNNs) and\n2-dimensional convolutional neural networks (2D CNN). In addition to video frames, we use optical\nflow computed using the captured sequences. CNN extracts spatial characteristics in each frame,\nwhile RNN extracts temporal characteristics. The use of optical flow allows to encode the movements\nin the scenes. The proposed approaches reach the same level as the state-of-the-art techniques and\nsometime surpass them. It was validated on 3 databases achieving good results.", "sections": [{"title": "Introduction", "content": "With a growing population and expanding cities, we are facing an unprecedented rise of criminality [1]. Monitoring\nsystems where a human watches multiple screens to detect violence, theft, or other abnormal behaviors are becoming\nobsolete. It is hard for persons to focus for a long time to detect violence when they must monitor large crowds. The\ndeveloped computer vision methods are less effective because of the large volume of data that must be processed. Indeed\ncharacteristics resulting from those methods are extracted manually, processed and then classified by an algorithm. This\nextraction takes time and becomes almost impossible to perform with a large dataset. With the progress in artificial\nintelligence, several methods have been developed to detect violence. In fact it is possible to train convolution neural\nnetworks (CNN) to extract spatial and temporal features from a video to classify its content. In this work, we introduce,\nend-to-end deep learning methods using RGB frames and an optical flow with a CNN-LSTM network to detect violent\nscenes in videos. The architectures presented reach the same level as modern techniques and sometime surpass them.\nOur approaches have been tested on 3 public databases to validate their performance."}, {"title": "Related works", "content": "Many methods of violence detection have been proposed in recent years. We can classify these techniques into two\ncategories, classical machine learning and deep learning."}, {"title": "Machine learning approach", "content": "Machine learning methods are based on algorithms such as k-nearest neighbors (KNNs) [2], support vector machine\n(SVMs) [3], or random forest. In addition to the popular descriptors like MoSIFT [4] or VIF [5] to extract features,\nvarious extraction methods have been developed to detect violence. For the detection of violence, some approaches use\nmotion blobs. In [6], Gracia et al. assume that violence scenes have a position and a shape. The method consists of\ncomputing the difference between two consecutive frames then binarize the result to obtain the number of motion blobs.\nThe largest motion blobs are marked on a fight sequence and a no fight sequence. Only k motion blobs are selected. The\nk blobs are categorized by parameters such as centroid, area, or perimeter between blobs. The selected k blobs are then\nclassified using SVMs, KNNs, and random forests. Motions blobs outperform methods like MoSIFT, SIFT, VIF, and\nLMP in term of accuracy on popular datasets. In [7], Senst et al. proposed a specialized method based on the Lagrangian\ntheory to automatically detect violent scenes. A new spatio-temporal model based on the Lagrangian direction fields was\nused to capture the features. This new model exploits motion background compensation, appearances, and long-term\nmotion information. The experiment was conducted on three databases, Hockey dataset, movies dataset, and violent\ncrowd dataset. The new Lagrangian model trained with an SVM performs better than methods such as ViF and HOG\n+ BoW. In [8], Gao et al. proposed Oriented Violent Flows (OViF), a statistical motion orientations that takes full\nadvantage of the motion magnitude change information. AdaBoost is used to choose the features that are then trained\nby an SVM. Tests have been performed on Hockey dataset, and Violent Flow, and the results are superior to those of\nbaselines such as LTP and ViF. In [9], Xia et al. present a method that uses a bi-channels CNN and an SVM. First,\nbi-channels CNN is used to extract two types of features. The first one represents the features of appearance, and the\nsecond one represents the difference between adjacent frames. The two features are then classified using SVM. The\nmethods were tested on two databases, Hockey, and Violent crowd datasets. The results are superior to methods like\nHOG, HOF, MOSIFT, SIFT."}, {"title": "Deep learning approach", "content": "Methods of violence scenes detection with Deep Learning are generally based on Deep Convolutional Neural Network\n(DCNN).\nOne approach to detect violence scenes is to use 3D CNN. These algorithms are computationally expensive but are\naccurate [10], [11], [12], [13]. For example, Song et al. [12] propose the use of 3D CNN to extract both spatial\nand temporal features. The CNN is based on C3D introduced by Tran et al. [14]. The CNN consists of eight 3D\nconvolutional layers and five 3D pooling layers. They have also introduced a new frame sampling method based on the\ngray centroid to reduce frame redundancy caused by uniform sampling. This 3D CNN method has been validated on\nviolence flow, Hockey dataset and movies dataset. In [13], Ullah et al. propose a three-stage deep learning approach for\nviolence detection. First, a detection algorithm is used to track people in surveillance images and its output is feed to a\n3D CNN where spatio-temporal features are extracted. The results of this 3D CNN are then sent to softmax classifier\nfor violence detection. The 3D CNN is based on C3D [14] and is composed of eight convolutional and four pooling\nlayers. It has been tested on Violent Flow, Hockey dataset and movies dataset. It ranks above the classical methods but\nis not the best of the Deep Learning methods.\nThere are also other algorithms that combine two types of neural networks (NN). For example, a 2D time distributed\nCNN in order to extract the spatio-temporal characteristics and an RNN to refine these characteristics. This end-to-end\napproach is computationally efficient and gives interesting results, as mentioned in [15]. As features extractor Simonyan\net al. use VGG19 [16] pre-trained on ImageNet. The extracted features are fed to an LSTM whose output will be passed\nto a time distributed fully connected layer in order to detect violence. The approach was tested on Violent Flow, hochey\ndataset and movies dataset with results close to the state-of-the-art. Most algorithms which are based on CNN and RNN\nuse feature extraction followed by a Long Short-Term Memory (LSTM) [17]. There are 3 types of features extraction\ntechniques, first the standard method that feed a whole frame to a CNN such as [18], second a more sophisticated\napproache such as [19] that divide a video frame into patches to extract characteristics from each patches using a CNN.\nThis method of patch division allows to avoid the loss of discriminatory elements caused by differences in scale and\nlocation of persons in the frames. Finally technique that use CNNs which can provide multiscale features. In [19],\nDitsanthia et al. use a method called multiscale convolutional feature extraction to manage videos with different scales,\nand feeds the LSTMs with these multiscale extracted characteristics.\nSome methods use other types of data in addition to RGB frames. Most often, it is optical flow. The optical flow makes\nit possible to encode the movement. Adding this information to the RGB frames allows to get better performence. To\ncombine RGB and optical flow two subnetworks are trained, one on RGB frames and the other on optical flow, their\noutputs are then combined for classification, this the approach is used in [20].\nFinally, we have the methods that use special LSTMs called ConvLSTMs. These are LSTMs whose matrix operations\nhave been replaced by convolutions. ConvLSTM enable to capture the temporal and spatial characteristics [21], [22]."}, {"title": "Proposed Method", "content": "We use 2D CNN, distributed in time to capture temporal and spatial characteristics. We combine this 2D CNN with a\nbi-directional RNN (GRU or LSTM) to improve our detections. We also use optical flow to encode movement between\nframes for better performance. The network is composed of two identical specialized blocks (figure 1), one for the\nRGB frames and the other for the optical flow. The characteristics from these two networks are then added together,\nrefined by an RNN and then classified using a fully connected layer with sigmoid activation."}, {"title": "Convolutional Neural Network", "content": "As a convolutional neural network, we have selected EfficientNet [23], which is characterized by its compound scaling\nprinciple and its efficiency during inference. There are eight versions of EfficientNet (B0-B7), the network consists of\nMBBLOCKS [24] from MobileNets which are associated with a squeeze and excitation blocks [25]. Figure 2 illustrate\nthe MBCONV (MBBLOCK + squeeze and excitation block) used by EfficientNet. We used EfficientNet B0 (figure 3)\npre-trained on ImageNet [26]."}, {"title": "Long Short-Term Memory", "content": "The first RNN used to get a temporal representation is LSTM. This RNN has 3 gates: input gate, forget gate, and output\ngate. The input gate controls the amount of information that enters in the cell, the forget gate controls the flow of\ninformation that remains in the cell and the output gate controls the information that will be used to calculate the output\nactivation of the LSTM unit. Equation 1 illustrates the LSTM used in this work [27]. ft represent the forget gate, it the\ninput gate, ot the output gate. xt is the input vector to the LSTM. ct and Ct are respectively the cell state vector and the\ncell input activation vector. ht is the hidden state vector, W and U are weights matrix that will be learned during the\ntraining and b is the bias.og is a sigmoid activation function, and \u03c3\u03b7 is a hyperbolic tangent. Furthermore, we use our\nLSTM in a bidirectional position to consider not only the preceding sequences but also the following sequences, which\nhelps to get better performance.\n$f_t = \\sigma_g(W_f x_t + U_f h_{t-1} + b_f)$\n$i_t = \\sigma_g(W_i x_t + U_i h_{t-1} + b_i)$\n$O_t = \\sigma_g(W_o x_t + U_o h_{t-1} + b_o)$\n$\\tilde{C}_t = \\sigma_{\\eta}(W_c x_t + U_c h_{t-1} + b_c)$\n$C_t = f_t C_{t-1} + i_t \\tilde{C}_t$\n$h_t = O_t \\circ \\sigma_{\\eta}(C_t)$"}, {"title": "Gated Recurrent Unit", "content": "The GRU's option is made to prevent the LSTM's problem of gradient vanishing. For some tasks, GRU is more resistant\nto noise and outperforms the LSTM [28]. The GRUs are less computationally intensive because, unlike the LSTM\nwhich has three, they have two gates.\nEquation 2 provides the GRU functions used in this work [29]. Zt, update gate determines what information to retain or\ndrop, the rt reset gate determines how much past knowledge to forget, and ht is the output gate. W, U, and b are matrix\nand vector parameters, og is a sigmoid activation function, and oh is a hyperbolic tangent. The input vector xt. We use\nGRU in bidirectional mode to consider either following and preceding information.\n$z_t = \\sigma_g(W_z x_t + U_z h_{t-1} + b_z)$\n$r_t = \\sigma_g(W_r x_t + U_r h_{t-1} + b_r)$\n$h_t = (1 - z_t) h_{t-1} + z_t\\circ \\sigma_{\\eta}(W_h x_t+\nU_h (r_t h_{t-1})+b_h)$"}, {"title": "Optical Flow", "content": "Optical flow is a method of perceiving movement in a sequence of images (videos) that can be calculated in different\nways, in our case we opted for the use of PWC-Net [30]. PWC-Net is an approach to obtain a smaller network than\nFLowNet2 [31] and also more efficient in term of accuracy by adding domain knowledge into the design of the network\n(see figure 4). To compute the optical flow, PWC-Net first uses a learnable features pyramid to counteract the variations\nof shadows and brightness in the raw image. Then a warping operation is performed to capture large motions. After this\nwarping operation the features are passed to a layer that compute the cost volume. Cost volume is a more discriminating\nrepresentation of the optical flow than the image. The representation from the cost volume layer is then processed\nby CNN to estimate the flow. Since warping and cost volume have no learnable parameters, it reduces the size of the\nmodel. At the end of the pipeline a post-process of the context information is done by using a network to refine the\noptical flow. We used PWC-Net pre-trained on MPI Sintel dataset [32] to extract the optical flow for our datasets."}, {"title": "Experiments and Results", "content": "We used three datasets to test our network: Hockey dataset [33], Violent Flow dataset [5], and Real Life Crime situations\ndataset [34]. We used the accuracy metric to measure the performance of our network."}, {"title": "Datasets", "content": "Datasets were randomly divided into two groups, training (80%) and validation/testing (20%)."}, {"title": "Parameters and sampling", "content": "We used Keras [35] with the TensorFlow [36] backend to set up our networks. Having different lengths of videos, we\ndecided to choose a fixed number of 12 frames for each dataset. To choose these frames, we made a uniform sampling\nthat allowed at the same time to avoid unnecessary computation of the network caused by redundant frames. For the\ncomputation of optical flow, we used RGB frames that we have sampled and the frames that follow them. For example,\nfor the computation of the optical flow for frame 6, we used frame 6 and frame 7. To go further in our experimentation,\nwe have also made jumps of 2 and then 3 frames from the RGB frame, for example, for frame 6, we have also calculated"}, {"title": "Results and discussions", "content": "Tables 1, 2 and 3 present our results on the 3 datasets, Hockey, Violent Flow and Real Life Crime situations. The\ntables are composed of classic methods (Machine Learning) and Deep Learning methods. The classical methods have\ndifferent inputs from frames like a bag of words or histograms bins.\nOn Hockey dataset we obtained an accuracy of 99% with ValdNet1 (GRU), results close to [12] with 99.62%. On\nViolent Flow, we obtained an accuracy of 93.53% with ValdNet3 (LSTM), lower than our previous work with 95.00%\nbased on VGG+GRU without optical flow. In the visualization of the optical flow for Violent Flow (figure 6) is not\nsharp enougth to help for the classification. On Real Life Crime situations dataset, we surpassed the result of our\nprevious work with all versions of ValdNet. Our highest result is 96.74% of accuracy, 6.24% better than our previous\nexperiment, and the other method in [37], which had 86.39%."}, {"title": "Conclusion and Future Works", "content": "We used 2D time distributed CNN to capture spatio-temporal features, and we refined them using RNN. Also, we used\ntwo specialized sub-networks, one for RGB images and the other for optical flow, which outputs we have summed to\nencode the motion from the optical flow into our features. These combinations have allowed increasing our results in\nthe proposed dataset. We achieved 99%, 93.75%, and 96.74% respectively on Hockey dataset, Violent Flow, and Real\nLife Crime Situations dataset. We are second on Hockey dataset behind [12] by 0.62 difference. On ViolentFlow, the\npresence of several people seems to pose a problem to the optical flow. We know that there is movement but the low\nresolution of the optical flow does not allow to clearly determine what people are doing in the scene (figure 6). We\ncould not achieve the result of our previous experiments without optical flow [39]. Real-Life Crime situations dataset\nis a fairly new database, so there is no other test for the moment except the baseline [37], which we outperformed by\nmore than 10%. For our future work, we will introduce other datasets in order to benchmark all available databases and\ntechniques. We will also benchmark the performance in terms of flops and speed of inference of all modern techniques\navailable."}]}