{"title": "SLIM-RAFT: A Novel Fine-Tuning Approach to Improve\nCross-Linguistic Performance for Mercosur Common Nomenclature", "authors": ["Vin\u00edcius Di Oliveira", "Yuri Fa\u00e7anha Bezerra", "Li Weigang", "Pedro Carvalho Brom", "Victor Rafael R. Celestino"], "abstract": "Natural language processing (NLP) has seen significant advancements with the advent of large language\nmodels (LLMs). However, substantial improvements are still needed for languages other than English, es-\npecially for specific domains like the applications of Mercosur Common Nomenclature (NCM), a Brazilian\nHarmonized System (HS). To address this gap, this study uses TeenyTineLLaMA, a foundational Portuguese\nLLM, as an LLM source to implement the NCM application processing. Additionally, a simplified Retrieval-\nAugmented Fine-Tuning (RAFT) technique, termed SLIM-RAFT, is proposed for task-specific fine-tuning of\nLLMs. This approach retains the chain-of-thought (CoT) methodology for prompt development in a more\nconcise and streamlined manner, utilizing brief and focused documents for training. The proposed model\ndemonstrates an efficient and cost-effective alternative for fine-tuning smaller LLMs, significantly outperform-\ning Teeny TineLLaMA and ChatGPT-4 in the same task. Although the research focuses on NCM applications,\nthe methodology can be easily adapted for HS applications worldwide.", "sections": [{"title": "1 INTRODUCTION", "content": "The widespread application of Generative Artifi-\ncial Intelligence (GenAI) systems has significantly\nadvanced the development of artificial intelligence\n(Schulhoff et al., 2024; Radosavovic et al., 2024). On\none hand, more sophisticated large language models\n(LLMs), such as ChatGPT, possess multilingual capa-\nbilities to process multimodal information and have\nbecome key drivers for Al applications (Weigang\net al., 2022). However, for most users, effective use of\nthese models requires enhancing prompt engineering\nskills. On the other hand, open-source large language\nmodels, such as LLaMA 3, can locally fine-tune mod-\nels, catering to specific security and flexibility needs.\nNonetheless, non-English users may encounter lim-\nitations due to the composition of LLaMA's train-\ning corpus, predominantly English-based (90%), with\nonly a small portion (10%) dedicated to other lan-\nguages, including French, Spanish, and Portuguese.\nWhile LLMs offer cross-language processing ca-\npabilities, enabling some understanding of related\nlanguages, particularly within the Latin language\ngroup, these capabilities are often insufficient for\nmore nuanced language processing tasks (Souza et al.,\n2020). This is especially true for tasks involving tech-\nnical terminology and enterprise-specific privacy data\nrequirements. The limited proportion of pre-trained\nPortuguese corpus in models like LLaMA highlights\nthese constraints, revealing significant limitations in\ntheir language processing capabilities for non-English\nlanguages.\nAnother way to tackle these downstream tasks\ncould be to use significantly smaller LLMs and per-\nform fine-tuning by retraining the model parame-\nters, as demonstrated in the Teeny TineLLaMA model\n(Corr\u00eaa et al., 2024), which is the approach addressed\nin this work. This approach considers that subsequent\ntasks have a well-defined and reduced scope but pre-"}, {"title": "2 RELATED WORKS", "content": "This section presents related work in two areas:\n1) research on the implementation of LLMs with\nPortuguese as the primary language, and 2) stud-\nies on Retrieval-Augmented Generation (RAG) and\nRetrieval-Augmented Fine-Tuning (RAFT)."}, {"title": "2.1 Portuguese LLM's", "content": "The introduction of LLaMA (Touvron et al., 2023a)\nas an open and efficient foundational language model\nmarked a significant milestone in the evolution of lan-\nguage processing. With models spanning 7 billion to\n65 billion parameters, LLaMA underscored the via-\nbility of training cutting-edge models exclusively on\npublicly available datasets. Notably, the LLaMA-\n13B model surpassed GPT-3 in various benchmarks,\ndemonstrating its remarkable performance despite a\ncomparatively smaller parameter count. This initial\nsuccess paved the way for subsequent iterations, with\nLLaMA 2 and 3 (Touvron et al., 2023b; Meta, 2024)\nfurther refining the models. These later versions, par-\nticularly tailored for dialogue applications, introduced\nfine-tuned language models optimized for chat inter-\nfaces. Consequently, these early advancements laid a\nrobust foundation for future progress in natural lan-\nguage processing.\nAlthough ample data exists for training\ntransformer-based language models in Portuguese,\nnative speakers can readily discern limitations in\nthe text generation and performance of pre-trained\nmodels primarily derived from English language\ndata. In recent years, there has been a rising interest\nin the development and enhancement of large-scale\nlanguage models tailored to the Portuguese language.\nThis trend has been propelled by pioneering and in-\nnovative research efforts, as evidenced by numerous\nrecent contributions to the field.\nIn European Portuguese (PT-PT), the initiative\nknown as Gl\u00f3rIA (Lopes et al., 2024) merits particular\nattention. This project involves a trained decoder lan-\nguage model meticulously constructed from a corpus\ncomprising 35 billion tokens from various sources.\nFor Brazilian Portuguese (PT-BR), the first rele-\nvant LLM encountered was Sabi\u00e1 (Pires et al., 2023).\nThis initiative underscores the development of robust\nand scalable language models for the Portuguese lan-\nguage. Leveraging advanced machine learning archi-\ntectures, these models have been instrumental in ad-\nvancing natural language processing applications in\nBrazilian Portuguese.\nThe Cabrita model (Larcher et al., 2023) was\nlaunched as a low-cost alternative for training LLMs.\nThe authors posited that their methodology could be\nextended to any transformer-like architecture. To sub-\nstantiate their hypothesis, they undertook continuous\npre-training exclusively on Portuguese text using a\n3-billion-parameter model known as OpenLLaMA.\nThis effort culminated in the creation of openCabrita\n3B. Remarkably, openCabrita 3B incorporates a novel\ntokenizer, significantly reducing the number of to-\nkens necessary to represent the text. Subsequently,\nin a comparable approach, a new study introduced\na model predicated on LLaMA 2, designed specifi-\ncally for handling prompts in Portuguese. This model,\nnamed Bode (Garcia et al., 2024), is available in two\nversions: one with 7B and 13B parameters. Both\nmodels used the LoRa (Hu et al., 2021) fine-tuning\nmethod over an open-source LLM. This technique\npreserves the original parameters intact while intro-\nducing a new terminal layer atop the model, which"}, {"title": "2.2 Retrieval-Augmented Approach", "content": "Retrieval Augmented Generation - RAG (Lewis et al.,\n2020) represents a transformative approach to enrich-\ning the quality and pertinence of generated content\nby integrating external insights derived from exten-\nsive datasets or repositories of knowledge. By em-\nbedding pertinent external knowledge sources into\nthe generation process, RAG is designed to elevate\nthe coherence, factual precision, and overall utility\nof generated text. This methodology proves advan-\ntageous in domains necessitating precise and contex-\ntually nuanced content generation, such as question-\nanswering, summarizing, and advanced dialogue sys-\ntems. By controlling retrieval mechanisms, RAG en-\nsures that the resultant outputs are provided with high\naccuracy and contextual relevance, thereby advancing\nthe frontiers of natural language processing applica-\ntions.\nIn pursuit of enhancing the precision of model\nresponses and mitigating the phenomenon of LLM\nhallucinations, a novel approach has emerged: Re-\ntrieval Augmented Fine-Tuning (RAFT) (Zhang et al.,\n2024). This methodology integrates the RAG frame-\nwork with fine-tuning techniques, empowering mod-\nels not only to acquire domain-specific knowledge but\nalso to adeptly retrieve and comprehend external con-\ntexts crucial for task execution. RAFT introduces the\nidea of chain-of-thought prompting for building the\nfine-tuning data set. This prompting technique en-\nables the model's answers to show its reason line with\na sequence of arguments, enhancing its explicability.\nRAG and RAFT were designed to confront the\ncomplexity of tailoring LLMs to specialized domains.\nWithin these realms, the emphasis pivots from general\nknowledge reasoning to optimizing accuracy vis-\u00e0-vis\na meticulously defined array of domain-specific doc-\numents."}, {"title": "2.3 NCM Data Set", "content": "The ELEVEN data set, ELEctronic in Voices in the\nPortuguese language (Di Oliveira et al., 2022), was\nmeticulously curated to furnish researchers and en-\ntrepreneurs with a repository of product descriptions\ncategorized under the Mercosur Common Nomencla-\nture (NCM). This extensive database comprises over\na million meticulously labelled records, each scru-\ntinized by taxation experts. These descriptions are\nshort texts, limited to 120 characters, and extracted\nfrom authentic electronic invoices documenting pur-\nchase and sales transactions.\nLabelled datasets are a rare commodity, yet they\nprovide indispensable resources for applications re-\nliant on supervised learning (Van Engelen and Hoos,\n2020). The ELEVEN dataset has served as a corner-\nstone for several noteworthy academic endeavours: 1)\nthe development of a CNN-based system for classify-\ning goods (Kieckbusch et al., 2021); 2) the creation of\ndata visualization tools aimed at identifying outliers\nand detecting fraud (Marinho et al., 2022); and 3) the\nestablishment of a framework utilizing automatic en-\ncoders to cluster short-text data extracted from elec-\ntronic invoices, thereby enhancing anomaly detection\nwithin numeric fields (Schulte et al., 2022)."}, {"title": "3 HS AND NCM CODES", "content": "In the dynamic realm of international trade, customs\nbrokers, exporters, and importers confront the critical\ntask of accurately classifying goods under the Harmo-\nnized System (HS) Code, which underpins the Mer-\ncosur Common Nomenclature (NCM) code (Valen\u00e7a\net al., 2023).\nThe Harmonized System (HS) is the founda-\ntion for customs tariffs and the compilation of in-\nternational trade statistics in over 200 countries and\neconomies. Beyond these primary functions, the HS\nis employed for various other purposes, including the\nmonitoring of controlled goods, establishing rules of\norigin, and facilitating trade negotiations. It is also a\ncrucial component of fundamental customs controls\nand procedures (WCO, 2024).\nThe MERCOSUR Common Nomenclature,\nNCM code system (Nomenclatura Comun do Mer-"}, {"title": "4 SLIM-RAFT MODEL", "content": "The SLIM-RAFT model simplifies RAFT logically\nand intelligently. Just as RAFT maintains the RAG\nin its designed form, SLIM-RAFT also maintains the\nRAG mechanism in its structure, see Figure 1.\nThe preceding sections have elucidated that con-\nstructing the training base in the original RAFT model\nis an expensive endeavour, frequently necessitating\nthe deployment of another powerful LLM. This sub-\nstantial cost is predominantly attributable to two fea-\ntures of RAFT: the chain-of-thought reasoning and\nthe inclusion of irrelevant documents. While the con-\ncept of learning to disregard irrelevant documents is\nboth valid and logical within the context of RAFT's\nobjectives, it is not a requisite for all applications.\nThis insight prompted the exclusion of this feature in\nthe development of SLIM-RAFT.\nSLIM-RAFT retains the chain-of-thought concept\nin its fine-tuning process, albeit simplified. Instead of\nusing lengthy texts or entire documents as input, the\napproach employs logical arguments derived from the"}, {"title": "4.1 FT Database and Prompting", "content": "A theoretical example of a list of arguments for con-\nstructing the simplified chain of thought:\n\u2022 Doc. 1:\n$\\alpha \\isin A$ (1)\n\u2022 Doc. 2:\n$A \\subset B$ (2)\n\u2022 Doc. 3: Consequently,\n$\\dots a \\isin B$ (3)\nThis was an application of the training base built\nfor fine-tuning within the idea of the simplified chain-\nof-thought. See below for a generic example of a\nprompt:"}, {"title": "4.2 Source LLM and Fine-tuning", "content": "The LLM source chosen for this work was Teeny\nTinyLLaMA - TTL, available in two sizes: 460 mil-\nlion and 160 million parameters. Two primary charac-\nteristics of TTL guided this selection: its compact size\nand the training on a corpus in Brazilian Portuguese.\nWhile other source models trained in Brazilian\nPortuguese exist, as discussed in Section 2, their sub-\nstantial size can make fine-tuning costly, even when\nemploying optimised techniques such as LoRa (Hu\net al., 2021). In contrast, the compact size of TTL\nmade our fine-tuning process more cost-effective,\ndemonstrating its practicality and potential for wider\napplication.\nThe Fine-tuning process adjusts all model param-\neters. The reduced size of TTL facilitated this task.\nThe codes employed were adapted from those pro-\nvided by the authors of the original TTL paper (avail-\nable on GitHub 1) with minor modifications.\nAll codes developed for SLIM-RAFT are accessi-\nble on SLIM-RAFT's GitHub repository. Both TTL\nmodels, 160 million and 460 million parameters, were\nfine-tuned to create SLIM-RAFT. The 160 million pa-\nrameter version was used in SLIM-RAFT, while the\n460 million parameter version was used for compara-\ntive analysis during the final model evaluation.\nThe SLIM-RAFT GitHub repository 2 is a valu-\nable resource that provides the codes used in this\nstudy. This open access not only allows the commu-\nnity to reproduce and assess this experiment but also\nencourages further collaboration and potential contri-\nbutions to natural language processing."}, {"title": "5 RESULTS AND DISCUSSION", "content": "The results were evaluated through a comparative\nanalysis of the responses delivered by the tested mod-\nels. Three other models were chosen for this compar-\nison: TeenyTinyLLaMA 460m, TeenyTinyLLaMA\n460m + NCM fine-tuning, and ChatGPT 4. In the\nend, four models were tested and evaluated by Chat-\nGPT 4.0:\n\u2022 Model 1: TeenyTinyLlama with tiny460M with-\nout fine-tuning on the dataset, defined as TTL.\n\u2022 Model 2: ChatGPT 4.0, defined as GPT.\n\u2022 Model 3: TeenyTinyLlama with fine-tuning on the\nNCM dataset, defined as NCM-TTL."}, {"title": "5.1 Results Presentation", "content": "The evaluation used 100 questions and answers not\nincluded in the fine-tuning training base. These 100\nquestions were presented to various models, and their\nresponses were recorded and compared. ChatGPT-4\nassessed the quality of each response, scoring it on a\nscale from 0 to 10. The final score for each model\nrepresents the average of the scores assigned to each\nresponse. Table 5 presents the results of this evalua-\ntion. It is clear that Model 4 of SLIM RAFT achieved\nthe best score of 8.63 with a standard deviation of 2.30\nacross the 100 Q/As."}, {"title": "5.2 LLM Justification", "content": "It is essential to underscore the potential utility of an\nLLM specialized in this type of task, as it extends be-\nyond mere classification. Whereas a straightforward\ninput-output classification system is confined to spe-\ncific subjects and input formats, an LLM system can\nextract semantic knowledge from the training base\nand demonstrates flexibility in handling various input\nformats.\nAnswering a simple direct question like \u201cWhat is\nthe NCM code for the product fresh apple package\" is\nnot enough. The question can come in several forms\nor be embedded in a larger context, for example: \"I\ndon't know the NCM code for fresh apple package,\ncan you help me?\".\nAnother pertinent scenario involves cases of at-\ntempted tax evasion. For instance, if the product fresh\napple package is exempt from taxation while apple\njuice package is subject to tax, it could be mislead-\ningly described in a tax document as apple j. pack.\nbut assigned the NCM code for fresh apples package,\nwhich is tax-exempt. If this discrepancy goes unde-\ntected by customs authorities, it could result in a loss\nof revenue due to the uncollected tax.\nThe SLIM-RAFT model can effectively help a\nsystem for controlling and inspect documents regard-\ning the NCM Code misuse. But, because of its re-\nduced size, the capability of extracting the right ques-\ntion embedded in a bigger context is limited. Let us\nconsider the following example:\nPortuguese - Fui na padaria e comprei um suco\nde laranja, percebi que na nota fiscal aparecia um\nc\u00f3digo chamado NCM, mas estava com a impress\u00e3o\nborrada. Qual seria o c\u00f3digo impresso?\nEnglish - at the bakery and bought some orange\njuice, I noticed that a code called NCM appeared on\nthe invoice, but the print was blurry. What would be\nthe printed code?\nWhen presented with this query, our model may\nnot discern the central issue, namely, \"What is the\nNCM category for orange juice?\" Integrating an ad-\nditional LLM into the system could mitigate this lim-\nitation.\nThe Few-shot prompting technique (Ma et al.,\n2024; Gu et al., 2021) can enable other large lan-\nguage models (LLMs) to reformulate queries, thereby\nadapting the context to enhance comprehension by\nthe smaller LLM integrated within the SLIM-RAFT\nmodel. This approach is shown above."}, {"title": "5.3 Limitations", "content": "The SLIM-RAFT model is a prototype developed to\nillustrate the original RAFT methodology's simplifi-\ncation and propose its application within the NCM\ndomain. Consequently, this model is a highly spe-\ncialised tool tailored for its designated task.\nIt is not recommended for use in ChatBot applica-\ntions, as TeenyTinyLLaMA (TTL) creators have ad-\nvised against employing the TTL 160m model for\nsuch purposes. The TTL 460m model is recom-\nmended for chatbot functionalities.\nA simplified chain-of-thought approach is em-\nployed when constructing the training base for fine-\ntuning. However, it's crucial to remember that the in-\nvolvement of a domain expert is beneficial and neces-\nsary for developing the reference lines of reasoning,\nhighlighting the irreplaceable role of human expertise\nin this process."}, {"title": "6 CONCLUSIONS", "content": "The SLIM-RAFT model demonstrated significantly\nsuperior performance to ChatGPT 4 in interpreting\nand classifying product descriptions according to the\nNCM code.\nThis outcome indicates that a smaller-scale LLM\nwith specific domain knowledge can surpass a more\npowerful LLM in specialized tasks, provided it is\nappropriately adjusted and trained while maintaining\nlow execution costs.\nThe technique for simplifying the construction of\nthe chain-of-thought, as proposed in SLIM-RAFT,\nnot only reduces costs but also proves to be a vi-\nable alternative for developing specialized LLMs with\nhigh accuracy.\nSince NCM coding is used not only for manag-\ning, transporting, paying, and taxing various goods\nin the import and export trade between MERCO-\nSUR countries but also for most tax bills for goods,\ncommodities, and restaurants in the Brazilian mar-\nket, the practical value of this research is substan-\ntial. The findings provide convenience for govern-\nment departments involved in import and export, tax-\nation, banks, transportation, and manufacturers. Ad-\nditionally, since over 200 countries use the HS system\nfor import and export trade, the LLM-NCM solution\nproposed in this article can also facilitate the effective\npromotion of LLM-HS applications worldwide.\nFuture research will focus on applying SLIM-\nRAFT to larger LLMs, such as LLaMA 3, empha-\nsising multilingual tasks. Additionally, comparisons\nwith other fine-tuning techniques, such as LoRa, will\nbe explored."}]}