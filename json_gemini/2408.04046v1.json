{"title": "Learning Rate-Free Reinforcement Learning: A Case for Model Selection with Non-Stationary Objectives", "authors": ["Aida Afshar", "Aldo Pacchiano"], "abstract": "The performance of reinforcement learning (RL) algorithms is sensitive to the choice of hyperparameters, with the learning rate being particularly influential. RL algorithms fail to reach convergence or demand an extensive number of samples when the learning rate is not optimally set. In this work, we show that model selection can help to improve the failure modes of RL that are due to suboptimal choices of learning rate. We present a model selection framework for Learning Rate-Free Reinforcement Learning that employs model selection methods to select the optimal learning rate on the fly. This approach of adaptive learning rate tuning neither depends on the underlying RL algorithm nor the optimizer and solely uses the reward feedback to select the learning rate; hence, the framework can input any RL algorithm and produce a learning rate-free version of it. We conduct experiments for policy optimization methods and evaluate various model selection strategies within our framework. Our results indicate that data-driven model selection algorithms are better alternatives to standard bandit algorithms when the optimal choice of hyperparameter is time-dependent and non-stationary.", "sections": [{"title": "1 Introduction", "content": "We focus on sequential decision-making problems such as reinforcement learning and bandits (Lattimore & Szepesv\u00e1ri, 2020; Sutton & Barto, 2018) where a learner interacts with the world in a sequential manner and is tasked with finding a policy that maximizes the reward. It is a common scenario in RL that the right choice of hyperparameters is not known in advance and the success of RL algorithms demands the effort of hyperparameter tuning (Eimer et al., 2023) to reach convergence. Among all the algorithm-specific hyperparameters, the learning rate is known to have a notable impact on the convergence of RL agents. The learning rate determines the extent to which the model parameters are adjusted at each optimization step, and prior literature on learning rate scheduling (Mishchenko & Defazio, 2023; Defazio et al., 2023) suggests that optimal learning rate is dependent on the distance to the solution. Central to RL, is the notion of reward that contains information about the proximity of the current policy to the optimal one. Building on this intuition that the reward feedback can be used as a proxy of the distance to the solution, we propose a framework that utilizes the empirical reward to adjust the learning rate on the fly during RL training. Model selection methods are inherently designed with the goal of detecting the right configuration of the problem and are uniquely suitable for the task of hyperparameter selection in RL;"}, {"title": "2 Preliminaries and Background", "content": "In many machine learning domains, including reinforcement learning the true configuration of the problem is not known in advance. The goal of model selection is to consider several configurations and add a strategy on top that learns to pick up the best configuration adaptively. We call each configuration a base and refer to the model selection strategy as the meta-learner. The meta-learner has access to a set of m bases, in this case, different copies of the same reinforcement learning algorithm instantiated with different learning rates. In each round, n = 1, 2, ..., N, of the interaction between the meta-learner with the environment, the meta-learner selects a base $i_n \\in [m]$ to play and follows its policy. Base $i_n$'s internal state is then updated with the data collected from its interaction with the environment.\nWe review a few definitions from bandits to better explain these methods. The regret of a policy $\\pi$ is defined as\n$\\text{reg}(\\pi) = \\upsilon^* - \\upsilon^{\\pi},\\qquad (1)$\nwhere $\\upsilon^*$ is the value of the optimal policy $\\pi^* \\in \\arg \\max_{\\pi} \\upsilon^{\\pi}$. In stochastic settings, it's most common to assume that the total regret of base $i$ after being played for $k$ rounds is bounded by $\\sqrt{k}$ rate,\n$\\sum_{l=1}^k \\text{reg}(\\pi_{i}^{(l)})) \\leq d_i^{(k)} \\sqrt{k},\\qquad (2)$\nwhere $d_i^{(k)}$ is called the regret coefficient of base $i$. The subscripts $(k)$ denote the number of times that a base has been played up to this round.\nRegret balancing methods aim to equate the regret bounds across all the bases. In this approach, the base agent is selected for two reasons. It is either a well-performing base by achieving low regret, or it has not been played enough and the meta-learner hasn't collected adequate information on the performance of this base. Here, we investigate Doubling Data Driven Regret Balancing (D\u00b3RB), and Estimating Data-Driven Regret Balancing (ED2RB) (Dann et al., 2024). D\u00b3RB maintains an"}, {"title": "3 Method", "content": "We formalize the model selection framework for learning rate-free reinforcement learning as the tuple $(\\mathbb{m}, \\beta, M, \\Psi)$ where $\\mathbb{m}$ is the number of base agents, $\\beta = {\\beta^1, ..., \\beta^{\\mathbb{m}}}$ denotes the set of base agents where $\\beta^i = (\\alpha^i, \\pi^i)$ $(1 \\leq i \\leq \\mathbb{m})$ consists of learning rate $\\alpha^i$, and policy $\\pi^i$. Lastly, $M$ is the model selection strategy and $\\Psi$ is an attribute of $M$ that expresses some statistics over the base agents. For instance, $\\Psi$ can either be a distribution $\\Psi: \\beta \\rightarrow P(\\beta)$ over base agents or represent the estimated empirical regret of the base agents.\nAt the beginning of each episode, the meta learner $M$ selects base agent $\\beta^i$ according to $\\Psi$. We abbreviate this as $j \\sim \\Psi$. The base agent interacts with the environment in a typical reinforcement learning manner for one episode. At state $s_t \\in S$, the base agent takes action $a_t \\sim \\pi^j$, receives reward $r_t \\in (0,1)$, and move to the next state $s_{t+1} \\in S$ following the environment transition dynamics. At the end of each episode, the base agent passes the realized rewards $(r_1,..., r_T)$ to the meta learner, so that it updates $\\Psi$ based on the model selection strategy $M$."}, {"title": "4 Experiments and Results", "content": "We begin our experiments with learning rate-free PPO. We initiate ten PPO base agents learning rates $\\alpha = [1e-2, 5e-3, 1e-3, 5e-4, 1e-4, 5e-5, 1e-5, 5e-6, 1e-6, 5e-7]$. We run the experiment for six model selection strategies introduced in Section 2. Figure 1 represents the reward plot of six meta learners on the Mujoco Humanoid environment (Tassa et al., 2012). By comparing the meta learners, we can see that D\u00b3RB and ED2RB strategies achieved the lowest regret and had the most advancement in the reward curve. The meta learners with Corral and Classic Balancing strategies, in addition to MAB meta learners, are showing sub-optimal performance in this task.\nTo better understand the Model Selection strategies, we plotted the selection frequency of each base learner for all of the model selection strategies. Figure 2 represents our main findings. Each (x,y) point in the plot shows that at time x meta learner has selected base learner y. We can see that D\u00b3RB and ED2RB meta learners have learned to detect the best-performing base learners and played them more often, resulting in more advancement in the reward curve. Figure 2 (c, d, e) show that meta learners with Classic Balancing, Corral, and EXP3 Strategies are selecting all the base learners quite often and therefore have an average performance in the task. Figure 2 (d) also explains the slightly better performance of Corral, as this meta learner has learned to detect one of the optimal base learners at the end.\nOne major drawback of applying standard MAB algorithms such as UCB in non-stationary domains like RL can be seen in these plots. A specific choice of learning rate might achieve high rewards in the early stages of learning and not perform well later on. As seen in Figure 2 (f) the base learner with index 1 was selected by UCB meta learner, since it was the best choice of learning rate in the early stages of learning. Figure 1 shows that this learning rate was performing well in the early stages of learning. Later on, this base learner was no longer an optimal learning rate but UCB couldn't adapt and continued to select that base learner. Algorithms like UCB are not able to distinguish these non-stationary changes and therefore are not suitable strategies for learning rate selection in"}, {"title": "5 Related Work", "content": "Hyperparameter tuning rises as a crucial step in machine learning problems and various works have proposed methods for hyperparameter tuning since the early days of machine learning (Kohavi &\nJohn, 1995; Bergstra et al., 2011). Random Search (Bergstra & Bengio, 2012) aimed to improve\nthe naive grid search by considering a random subset of all possible hyperparameters, instead of"}, {"title": "6 Conclusion and Future Work", "content": "We proposed a model selection framework for learning rate-free reinforcement learning and demon-strated its effectiveness using six model selection strategies. Our experiments showed that the\ndata-driven regret balancing method, D\u00b3RB, and ED2RB generally serve as good model selection\nstrategies for learning rate-free reinforcement learning, consistently performing well across our tests.\nIn contrast, bandit strategies appeared to be insufficient as meta-learners for PPO base agents.\nThere are several possible extensions to this work. The span of hyperparameter optimization with\nmodel selection techniques is not limited to the learning rate. Applying model selection methods for\ntuning a set of different hyperparameters is an interesting direction that requires sample-efficient\nalgorithms that can be deployed in RL. Studying the effect of sharing data across the base agents\nis another interesting direction that can further improve the efficiency and generalizability of the\nframework."}, {"title": "Appendix", "content": ""}, {"title": "A1. Theoretical Remarks on Regret Balancing", "content": "Regret balancing strategies strive to maintain the regret of the different algorithms. Typically it is assumed the optimal algorithm's regret scales as $d\\sqrt{t}$. In contrast, the regret of a linearly sub-optimal algorithm scales as $\\Delta t$ for some constant $\\Delta$. Without loss of generality let's call these two algorithms, Algorithm 1 and Algorithm 2. A regret balancing strategy ensures that at time $N$ the number of times Algorithm 1 and Algorithm 2 were played, $N_1$, and $N_2$ satisfy $d+\\sqrt{N_1} \\approx \\Delta N_2$ thus implying that $N_2 \\approx \\frac{d}{\\Delta} \\sqrt{N_1} = O(\\frac{d}{\\Delta} \\sqrt{N})$."}, {"title": "A2. Preliminaries of Reinforcement Learning", "content": "Reinforcement learning is formalized as Markov Decision Process (MDP) $(\\mathcal{S}, \\mathcal{A}, R, P, \\gamma)$; where $\\mathcal{S}$ denotes the set of states, $\\mathcal{A}$ is the set of actions, $R: \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathbb{R}$ is the reward function, $P: \\mathcal{S} \\times \\mathcal{A} \\rightarrow [0, 1]$ is the dynamic transition probabilities, and lastly $\\gamma \\in [0, 1]$ is the discount factor. Here we consider episodic reinforcement learning with maximum horizon $T$ where the goal of the agent is to learn the (near) optimal policy $\\pi: \\mathcal{S} \\rightarrow \\mathcal{A}$. The state-value function $V: \\mathcal{S} \\rightarrow \\mathbb{R}$ and action-value function $Q: \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathbb{R}$ with respect to policy $\\pi$ are defined as\n$V^{\\pi}(s) = E_{\\tau} \\Big[ \\sum_{t=0}^T \\gamma^t R(s_t, a_t) \\Big| s_0 = s, s_t, a_t \\Big] \\qquad (3)$\n$Q^{\\pi}(s,a) = \\mathbb{R}(s,a) + \\gamma E_{s'\\sim P(s,a)} \\Big[ V^{\\pi}(s') \\Big] \\qquad (4)$\nThe policy $\\pi$ is commonly parameterized by the set of parameters $\\theta$, and is denoted as $\\pi_{\\theta}$. Two of the predominant approaches for learning the (near) optimal policy in reinforcement learning are policy optimization and Q-learning. Policy optimization starts with an initial policy and in each episode updates the parameters by taking gradient steps toward maximizing the episodic return. Denote learning rate as $\\alpha \\in \\mathbb{R}$, a common update rule in policy optimization methods is\n$\\theta \\leftarrow \\theta + \\alpha \\mathbb{E} \\Bigg[ \\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(s_t, a_t) \\Big( Q_{\\theta}(s_t, a_t) - V_{\\theta}(s_t) \\Big) \\Bigg] \\qquad (5)$\nQ-learning uses the temporal differences method to update the parameters of $Q_{\\theta}^{\\pi}$. A common update rule is\n$\\theta \\leftarrow \\theta + \\alpha \\mathbb{E}_{s,a,s' \\sim \\mathcal{D}} \\Bigg[ \\delta_{\\theta} \\Big( r + \\gamma \\max_{a' \\in \\mathcal{A}} Q_{\\theta}^{\\pi}(s', a') - Q_{\\theta}^{\\pi}(s, a) \\Big)^2 \\Bigg] \\qquad (6)$\nwhere $\\mathcal{D}$ is the experience replay buffer and $\\theta'$ is a frozen parameter set named target parameter. Proximal Policy Optimization (PPO) (Schulman et al., 2017) and Deep Q-Networks (DQN) (Mnih et al., 2015) follow the first and second approaches, respectively."}, {"title": "A3. Learning Rate-Free Reinforcement Learning", "content": ""}, {"title": "Algorithm 2: Learning Rate-Free Reinforcement Learning with Model Selection", "content": "// Collect trajectories with selected base agent\n// Update parameters with selected learning rate\nif Policy Optimization then\nif Q-Learning then\n// Update the meta learner"}, {"title": "C. Model Selection Algorithms Pseudocodes", "content": "We provide the Model Selection Algorithms in hyperparparemeter tuning interface in this section. To\navoid including all the theoretical details, there might be a slight abuse of notation in the pseudocodes.\nWe encourage the reader to check the details with the original paper.\nDenote the number of times that the base agent i was played up to this time as $n_i$. Denote the regret\ncoefficient of base learner i as $d_i$, and the total reward accumulated by base learner i up to this time\nby $u_i$.\nD\u00b3RB\nDoubling Data Driven Regret Balancing (D\u00b3RB) (Dann et al., 2024) tries to maintain and equal\nempirical regret for all the base agents. Denote the balancing potential of base agent i as $\\Psi_i = d_i \\sqrt{n_i}$.\nThe D\u00b3RB algorithm for learning rate-free RL works as follows,"}, {"title": "Algorithm 3: D\u00b3RB", "content": "// Sample base index\n// Perform miss-specification test\n// If test triggered double regret coefficient for base i\n// Update balancing potential"}, {"title": "ED2RB", "content": "Estimating Data Driven Regret Balancing (ED2RB) (Dann et al., 2024) is similar to D3RB, though\nit tries to directly estimate the regret coefficients."}, {"title": "Algorithm 4: ED2RB", "content": "// Sample base index\n// Estimate active regret coefficient\n// Update balancing potential"}, {"title": "Classic Balancing", "content": "The Classic Regret Balancing Algorithm (Pacchiano et al., 2020a) starts with the full set of base\nagents $\\beta = [\\beta_1, ..., \\beta_m]$, at each round the algorithm performs miss-specification on each of the base\nagents and eliminates the miss-specified one. Denote as empirical regret upper bound of base\nagent j."}, {"title": "Algorithm 5: Classic Balancing", "content": "// Sample Base index\n// Perform miss-specification test for all the remaining base agents"}, {"title": "EXP3", "content": "Exponential-weight algorithm for exploration and exploitation (EXP3) learns a probability distribution\n$\\Psi_i = \\frac{\\text{exp}(S_i)}{\\sum_{j=1}^m \\text{exp}(S_j)}$ over base learners, where $S_i$ is a total estimated reward of base agent $i$ up to this\nround."}, {"title": "Algorithm 6: EXP3", "content": "// Sample Base index\n// Update statistics\n// Update Distribution"}, {"title": "Corral", "content": "Corral (Agarwal et al., 2017) learns a distribution over base agents and update it according to\nLOG-BARRIER-OMD algorithm. We skip the algorithmic details and refer to the updating rule\nmentioned in the original paper as Corral-Update."}, {"title": "Algorithm 7: Corral", "content": "// Sample base index\n// Update according to Corral"}, {"title": "UCB", "content": "The Upper Confidence Bound algorithm (UCB) maintains an optimistic estimate of the mean for\neach arm (Lattimore & Szepesv\u00e1ri, 2020). Denote $\\Psi_i$ as the upper confidence bound of arm $i$. The\nUCB algorithm for learning rate-free RL works as follows,\n// Sample base index\n// Update statistics\n// Update Upper Confidence Bounds"}, {"title": "Algorithm 8: UCB", "content": "// Sample base index\n// Update Upper Confidence Bounds"}]}