{"title": "Learning Rate-Free Reinforcement Learning: A Case for Model Selection with Non-Stationary Objectives", "authors": ["Aida Afshar", "Aldo Pacchiano"], "abstract": "The performance of reinforcement learning (RL) algorithms is sensitive to the choice of hyperparameters, with the learning rate being particularly influential. RL algorithms fail to reach convergence or demand an extensive number of samples when the learning rate is not optimally set. In this work, we show that model selection can help to improve the failure modes of RL that are due to suboptimal choices of learning rate. We present a model selection framework for Learning Rate-Free Reinforcement Learning that employs model selection methods to select the optimal learning rate on the fly. This approach of adaptive learning rate tuning neither depends on the underlying RL algorithm nor the optimizer and solely uses the reward feedback to select the learning rate; hence, the framework can input any RL algorithm and produce a learning rate-free version of it. We conduct experiments for policy optimization methods and evaluate various model selection strategies within our framework. Our results indicate that data-driven model selection algorithms are better alternatives to standard bandit algorithms when the optimal choice of hyperparameter is time-dependent and non-stationary.", "sections": [{"title": "Introduction", "content": "We focus on sequential decision-making problems such as reinforcement learning and bandits (Lattimore & Szepesv\u00e1ri, 2020; Sutton & Barto, 2018) where a learner interacts with the world in a sequential manner and is tasked with finding a policy that maximizes the reward. It is a common scenario in RL that the right choice of hyperparameters is not known in advance and the success of RL algorithms demands the effort of hyperparameter tuning (Eimer et al., 2023) to reach convergence. Among all the algorithm-specific hyperparameters, the learning rate is known to have a notable impact on the convergence of RL agents. The learning rate determines the extent to which the model parameters are adjusted at each optimization step, and prior literature on learning rate scheduling (Mishchenko & Defazio, 2023; Defazio et al., 2023) suggests that optimal learning rate is dependent on the distance to the solution. Central to RL, is the notion of reward that contains information about the proximity of the current policy to the optimal one. Building on this intuition that the reward feedback can be used as a proxy of the distance to the solution, we propose a framework that utilizes the empirical reward to adjust the learning rate on the fly during RL training. Model selection methods are inherently designed with the goal of detecting the right configuration of the problem and are uniquely suitable for the task of hyperparameter selection in RL;"}, {"title": "Preliminaries and Background", "content": "In many machine learning domains, including reinforcement learning the true configuration of the problem is not known in advance. The goal of model selection is to consider several configurations and add a strategy on top that learns to pick up the best configuration adaptively. We call each configuration a base and refer to the model selection strategy as the meta-learner. The meta-learner has access to a set of m bases, in this case, different copies of the same reinforcement learning algorithm instantiated with different learning rates. In each round, n = 1, 2, ..., N, of the interaction between the meta-learner with the environment, the meta-learner selects a base $i_n \\in [m]$ to play and follows its policy. Base $i_n$'s internal state is then updated with the data collected from its interaction with the environment.\nWe review a few definitions from bandits to better explain these methods. The regret of a policy is defined as\n$\\text{reg}(\\pi) = v^* - v^{\\pi},$ (1)\nwhere $v^*$ is the value of the optimal policy $\\pi^* \\in \\arg \\max v^{\\pi}$. In stochastic settings, it's most common to assume that the total regret of base $i$ after being played for $k$ rounds is bounded by $\\sqrt{k}$ rate,\n$\\sum_{l=1}^k \\text{reg}(\\pi_i^l) \\leq d_i(k) \\sqrt{k},$ (2)\nwhere $d_{i(k)}$ is called the regret coefficient of base $i$. The subscripts $(k)$ denote the number of times that a base has been played up to this round.\nRegret balancing methods aim to equate the regret bounds across all the bases. In this approach, the base agent is selected for two reasons. It is either a well-performing base by achieving low regret, or it has not been played enough and the meta-learner hasn't collected adequate information on the performance of this base. Here, we investigate Doubling Data Driven Regret Balancing (D\u00b3RB), and Estimating Data-Driven Regret Balancing (ED2RB) (Dann et al., 2024). D\u00b3RB maintains an"}, {"title": "Method", "content": "We formalize the model selection framework for learning rate-free reinforcement learning as the tuple (m, \u03b2, \u039c, \u03a8) where m is the number of base agents, $\u03b2 = \\{\u03b2_1, ..., \u03b2_m\\}$ denotes the set of base agents where $\u03b2_i = (\u03b1^i, \u03c0^i) (1 \u2264 i \u2264 m)$ consists of learning rate $\u03b1^i$, and policy $\u03c0^i$. Lastly, M is the model selection strategy and \u03a8 is an attribute of M that expresses some statistics over the base agents. For instance, \u03a8 can either be a distribution $\u03a8 : \u03b2 \\rightarrow P(\u03b2)$ over base agents or represent the estimated empirical regret of the base agents.\nAt the beginning of each episode, the meta learner M selects base agent $\u03b2_i$ according to \u03a8. We abbreviate this as $i \\sim \u03a8$. The base agent interacts with the environment in a typical reinforcement learning manner for one episode. At state $s_t \\in S$, the base agent takes action $a_t \\sim \u03c0^i$, receives reward $r_t \\in (0,1)$, and move to the next state $s_{t+1} \\in S$ following the environment transition dynamics. At the end of each episode, the base agent passes the realized rewards $(r_1,..., r_T)$ to the meta learner, so that it updates \u03a8 based on the model selection strategy M."}, {"title": "Experiments and Results", "content": "We begin our experiments with learning rate-free PPO. We initiate ten PPO base agents learning rates a = [le-2, 5e-3, 1e-3, 5e-4, 1e-4, 5e-5,1e-5,5e-6,1e-6,5e-7]. We run the experiment for six model selection strategies introduced in Section 2. Figure 1 represents the reward plot of six meta learners on the Mujoco Humanoid environment (Tassa et al., 2012). By comparing the meta learners, we can see that D\u00b3RB and ED2RB strategies achieved the lowest regret and had the most advancement in the reward curve. The meta learners with Corral and Classic Balancing strategies, in addition to MAB meta learners, are showing sub-optimal performance in this task.\nTo better understand the Model Selection strategies, we plotted the selection frequency of each base learner for all of the model selection strategies. Figure 2 represents our main findings. Each (x,y) point in the plot shows that at time x meta learner has selected base learner y. We can see that D\u00b3RB and ED2RB meta learners have learned to detect the best-performing base learners and played them more often, resulting in more advancement in the reward curve. Figure 2 (c, d, e) show that meta learners with Classic Balancing, Corral, and EXP3 Strategies are selecting all the base learners quite often and therefore have an average performance in the task. Figure 2 (d) also explains the slightly better performance of Corral, as this meta learner has learned to detect one of the optimal base learners at the end.\nOne major drawback of applying standard MAB algorithms such as UCB in non-stationary domains like RL can be seen in these plots. A specific choice of learning rate might achieve high rewards in the early stages of learning and not perform well later on. As seen in Figure 2 (f) the base learner with index 1 was selected by UCB meta learner, since it was the best choice of learning rate in the early stages of learning. Figure 1 shows that this learning rate was performing well in the early stages of learning. Later on, this base learner was no longer an optimal learning rate but UCB couldn't adapt and continued to select that base learner. Algorithms like UCB are not able to distinguish these non-stationary changes and therefore are not suitable strategies for learning rate selection in\nreinforcement learning. The same challenge casts to common hyperparameter tuning techniques such as Bayesian Optimization, where the objective function is assumed to be stationary.\nAdditionally, we initiate ten independent PPO agents with the same set of learning rates that we input to the model selection counterparts and run each agent for ~ fraction of total episodes in model selection experiments. Figure 3 demonstrates the results of this comparison. Figure 3(a) shows the number of episodes that the meta learner with D\u00b3RB strategy has selected each base agent throughout the training. Figure 3(b) shows the maximum episodic return achieved by PPO agents initiated with the same learning rates. We can see that D\u00b3RB strategy for learning rate-free PPO has learned to select the agents with higher reward (and lower regret) more frequently, and dedicate less sample and compute to suboptimal bases. In fact, through regret balancing a linearly suboptimal base will not be selected for more than $\\sqrt{N}$ rounds, where N is the total number of episodes. Check Appendix A for more theoretical details."}, {"title": "Related Work", "content": "Hyperparameter tuning rises as a crucial step in machine learning problems and various works have proposed methods for hyperparameter tuning since the early days of machine learning (Kohavi & John, 1995; Bergstra et al., 2011). Random Search (Bergstra & Bengio, 2012) aimed to improve the naive grid search by considering a random subset of all possible hyperparameters, instead of"}, {"title": "Conclusion and Future Work", "content": "We proposed a model selection framework for learning rate-free reinforcement learning and demonstrated its effectiveness using six model selection strategies. Our experiments showed that the data-driven regret balancing method, D\u00b3RB, and ED2RB generally serve as good model selection strategies for learning rate-free reinforcement learning, consistently performing well across our tests. In contrast, bandit strategies appeared to be insufficient as meta-learners for PPO base agents.\nThere are several possible extensions to this work. The span of hyperparameter optimization with model selection techniques is not limited to the learning rate. Applying model selection methods for tuning a set of different hyperparameters is an interesting direction that requires sample-efficient algorithms that can be deployed in RL. Studying the effect of sharing data across the base agents is another interesting direction that can further improve the efficiency and generalizability of the framework."}, {"title": "Appendix", "content": "A1. Theoretical Remarks on Regret Balancing\nRegret balancing strategies strive to maintain the regret of the different algorithms. Typically it is assumed the optimal algorithm's regret scales as $d\\sqrt{t}$. In contrast, the regret of a linearly sub-optimal algorithm scales as $At$ for some constant A. Without loss of generality let's call these two algorithms, Algorithm 1 and Algorithm 2. A regret balancing strategy ensures that at time N the number of times Algorithm 1 and Algorithm 2 were played, $N_1$, and $N_2$ satisfy $d+\\sqrt{N_1} \\approx AN_2$ thus implying that $N_2 \\approx \\frac{d+\\sqrt{N_1}}{A} = O(\\frac{d}{\\Delta}\\sqrt{N}).$\nA2. Preliminaries of Reinforcement Learning\nReinforcement learning is formalized as Markov Decision Process (MDP) $(S, A, R, P, \u03b3)$; where S denotes the set of states, A is the set of actions, $R : S\u00d7A \u2192 R$ is the reward function, $P : S\u00d7A \u2192 [0, 1]$ is the dynamic transition probabilities, and lastly $\u03b3\u2208 [0, 1]$ is the discount factor. Here we consider episodic reinforcement learning with maximum horizon T where the goal of the agent is to learn the (near) optimal policy $\u03c0: S \u2192 A$. The state-value function $V : S \u2192 R$ and action-value function $Q:SXAR$ with respect to policy $\u03c0$ are defined as\n$V^\\pi(s) = E_{\\substack{\\tau = s_0, a_0,\\ldots, s_T, a_T}}\\[\\sum_{t=0}^{T}\\gamma^t R(s_t, a_t)\\]$ (3)\n$Q^\\pi(s,a) = R(s,a) + \\gamma E_{s'\\sim P(s,a)}\\[V^\\pi(s')\\]$ (4)\nThe policy $\u03c0$ is commonly parameterized by the set of parameters $\u03b8$, and is denoted as $\u03c0_\u03b8$. Two of the predominant approaches for learning the (near) optimal policy in reinforcement learning are policy optimization and Q-learning. Policy optimization starts with an initial policy and in each episode updates the parameters by taking gradient steps toward maximizing the episodic return. Denote learning rate as $\u03b1 \\in R$, a common update rule in policy optimization methods is\n$\u03b8 \u2190 \u03b8 + \u03b1 E_{\\substack{s_0 \\sim s,\\ldots, a_T}\\[\\sum_{t=0}^{T} \\nabla_\u03b8\\log \u03c0_\u03b8(s_t, a_t) (Q_\u03b8(s_t, a_t) \u2013 V_\u03b8(s_t))\\]$ (5)\nQ-learning uses the temporal differences method to update the parameters of $Q^{\u03c0_\u03b8}$. A common update rule is\n$\u03b8 \u2190 \u03b8 + \u03b1 E_{s,a,s' \\sim D}\\[\u2207_\u03b8(r + \u03b3 \\max_{a' \\in A} Q_\u03b8(s', a') \u2013 Q_\u03b8(s, a))^2\\]$ (6)\nwhere $D$ is the experience replay buffer and $\u03b8$ is a frozen parameter set named target parameter. Proximal Policy Optimization (PPO) (Schulman et al., 2017) and Deep Q-Networks (DQN) (Mnih et al., 2015) follow the first and second approaches, respectively.\nA3. Learning Rate-Free Reinforcement Learning"}]}