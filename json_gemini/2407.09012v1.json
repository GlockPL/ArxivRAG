{"title": "TCAN: Animating Human Images\nwith Temporally Consistent Pose Guidance\nusing Diffusion Models", "authors": ["Jeongho Kim", "Min-Jung Kim", "Junsoo Lee", "Jaegul Choo"], "abstract": "Pose-driven human-image animation diffusion models have\nshown remarkable capabilities in realistic human video synthesis. De-\nspite the promising results achieved by previous approaches, challenges\npersist in achieving temporally consistent animation and ensuring robust-\nness with off-the-shelf pose detectors. In this paper, we present TCAN,\na pose-driven human image animation method that is robust to erro-\nneous poses and consistent over time. In contrast to previous methods,\nwe utilize the pre-trained ControlNet without fine-tuning to leverage\nits extensive pre-acquired knowledge from numerous pose-image-caption\npairs. To keep the ControlNet frozen, we adapt LoRA to the UNet layers,\nenabling the network to align the latent space between the pose and ap-\npearance features. Additionally, by introducing an additional temporal\nlayer to the ControlNet, we enhance robustness against outliers of the\npose detector. Through the analysis of attention maps over the temporal\naxis, we also designed a novel temperature map leveraging pose infor-\nmation, allowing for a more static background. Extensive experiments\ndemonstrate that the proposed method can achieve promising results\nin video synthesis tasks encompassing various poses, like chibi.", "sections": [{"title": "1 Introduction", "content": "Pose-driven human-image animation is a task that breathes life into a static\nhuman in an image. Given a source image and a driving video, the task aims\nto transfer the motion in the driving video to a human in the source image. Al-\nthough it is an intriguing task because of its wide impact on many applications\nsuch as social media, entertainment, and movie industry, several factors make\nthis task challenging. First, the identity in the source image should remain the\nsame over time, changing only the pose. In other words, the appearance of a hu-\nman in the generated video should not be affected by the appearance of a human"}, {"title": "2 Related work", "content": ""}, {"title": "2.1 Controllable Diffusion Model for Image Generation", "content": "Thanks to the scalability and stable training of diffusion models [14, 26], gen-\nerative foundation models [20, 22, 23] are now leading recent image generation\nresearch. Building on the foundation model, active research has been conducted\non the image-based controllable diffusion model. In particular, several studies\nhave expanded the T2I foundation models into an image-controllable generative\nmodel through modules using image conditioning [9, 10, 18, 19, 31, 33, 34]. Such"}, {"title": "2.2 Pose-driven Video Diffusion Models", "content": "Several efforts [3, 4, 12, 25] have expanded image generation models to video\ngeneration models, with most employing a two-stage training scheme [4, 12]. In\nthe first stage, the image generation model is fine-tuned using video frames. In\nthe second stage, temporal layers are added and trained while keeping the base\nimage generation model frozen.\nRecent pose-driven video diffusion models follow the same two-stage training\nparadigm. In the first stage, the pose-driven image generation model is trained\non individual video frames and corresponding pose images extracted using a\npose estimator. Then, the model is inflated by inserting temporal layers to cap-\nture the temporal information of the pose sequence extracted from a driving\nvideo. Disco [28], for example, takes three properties as input human sub-\njects, backgrounds, and poses to achieve not only arbitrary compositionality\nbut also high fidelity. MagicAnimate [30] achieves state-of-the-art performance\nwith a parallel UNet architecture conditioned on the source image through an\nattention mechanism. While both works show impressive video quality in human\nimage animation, they address neither overfitting nor the incompleteness of pose\nsequences.\nUnlike the aforementioned methods, we propose a more generalized model\nthat performs well on datasets beyond the real human image domain by adapting\nprior knowledge from the pre-trained model. Additionally, we use the temporal\nControlNet in the second stage to address incomplete pose information. To the\nbest of our knowledge, both overfitting and pose incompleteness have not been\nexplored in previous research."}, {"title": "3 Method", "content": "An overview of TCAN is presented in Fig. 2. Given a source image and a driv-\ning video with F frames, we aim to generate a video that: 1) incorporates the\nforeground appearance from the source image, 2) follows the pose of the driving\nvideo, and 3) maintains consistency in the background of the source image.\nAs shown in Fig. 2, TCAN follows a two-stage training framework widely\nadopted in current diffusion-based video generation works [4, 12]: In the first\nstage, the source image is transformed to match the pose of each frame in the\ndriving video at the image level. To achieve this, we tackle the human image"}, {"title": "3.1 Appearance-Pose Adaptation Layer", "content": "To leverage the well-generalized pose conditioning capability of the pre-trained\nControlNet, we opted to freeze the ControlNet in the first stage, in contrast to\nprevious studies. By adding the pose information from ControlNet to the denois-\ning UNet\u2019s encoder feature $z$, we embed the pose information into the denoising\nUNet. This is similar to MagicAnimate [30], but we use the feature from the"}, {"title": "3.2 Temporal ControlNet", "content": "Pose-guided human image animation methods, including our TCAN, are condi-\ntioned on the pose sequence estimated from the driving video. Consequently, the\nquality of the generated outputs relies on the accuracy of the pose estimator,\nmaking them inherently susceptible to inaccuracies in the predicted poses. One\npotential method to mitigate the impact of errors from the pose estimator is\nto fix the false detections from the estimated poses before use. However, dis-\ntinguishing noisy keypoints from true positives or adding missing keypoints is\nchallenging and tedious. Moreover, this frame-by-frame noise removal approach\nfails to ensure temporal consistency across multiple frames of the pose sequence.\nTo prevent the generated video from collapsing due to abrupt and erroneous\npose changes, we propose Temporal ControlNet. Temporal ControlNet incorpo-\nrates temporal layers, referred to as the motion module in AnimateDiff [12],"}, {"title": "3.3 Pose-driven Temperature Map", "content": "Even with APPA layers and temporal ControlNet, the TCAN trained up to the\nsecond stage experiences flickering in the static region (i.e., background). To ad-\ndress this issue, we propose a novel Pose-driven Temperature Map (PTM). The\nPTM is motivated by the observation that, compared to the foreground, the back-\nground tends to have a less specific focus over the temporal axis. Fig. 4 supports\nthe idea by visualizing the attention maps $m \\in R^{hf\\times wf}$ of the temporal attention\nlayer after the second stage training, where $h$ and $w$ are 64 and the resolution\nof each attention map is $f \\times f$. Each attention map represents the attention of\neach location along the temporal axis across consecutive video frames. For ease\nof interpretation, we resized the generated frames and the driving pose frame\nto a size of attention maps $m$ and overlaid them. Interestingly, we observe that\nthe attention map corresponding to the moving parts, i.e., foreground objects,\nshows higher diagonal values compared to the static background. This indicates\nthat dynamic objects stay at the same location temporally, thereby referencing\nadjacent frames more than the static background region. Consequently, we de-"}, {"title": "3.4 Training and Long-Term Video Prediction", "content": "Training We initialize the denoising UNet with the pretrained weights of Stable\nDiffusion 1.5\u00b9, the appearance UNet with those of Realistic Vision\u00b2, and employ\nthe weights of OpenPose ControlNet 3. In the first stage, we freeze all modules\nexcept the appearance UNet and the LORA layer within the APPA layer. In the"}, {"title": "4 Experiments", "content": "Evaluation setting During training, we use the TikTok dataset [16] only. For\nevaluation, we utilize two different datasets. First, we use 10 TikTok-style videos\ncollected from DisCo [28] as our test dataset, referring to it as the TikTok dataset\nevaluation. Second, to assess the generalization of TCAN, we evaluate the model\ntrained on the TikTok dataset using animation characters. Note that the textures\nand body proportions of the animation characters are quite different from those\nof real human datasets. We select 10 images from the Bizarre Pose Dataset [7]\nas source images and use the 10 TikTok-style test videos as driving videos. We\nrefer to this as the Bizarre dataset evaluation."}, {"title": "4.1 TikTok Dataset Evaluation", "content": "Qualitative Results As shown in Fig. 5, we qualitatively compare our method\nwith two baselines, DisCo [28] and MagicAnimate [30], both of which demon-"}, {"title": "4.2 Bizarre Dataset Evaluation with Pose Re-targeting", "content": "Pose Re-targeting Beyond the TikTok dataset evaluation, where both the\nsource image and driving sequence are derived from the same video, we extend\nour model to applications where the foreground object in the source image pos-\nsesses body proportions different from those of an actual human. Specifically,\nwhen the source images are sampled from an animation domain, there are sig-\nnificant differences in body proportions compared to TikTok dataset, as depicted\nin Fig. 6 and Fig. 7. To address this issue, we propose a re-targeting method-\nology. The pose re-targeting is designed to transfer the body the proportions\nof an object in the source image to that of the target frame, regardless of the\nobject proportion in the driving sequence. When used in practice, we re-target\nthe pose sequence estimated from the driving video so that the retargeted poses\nfit the proportion of the animation character in the source image. As shown in\nFig. 6, re-targeting enables the preservation of proportions of an object in the\nsource image, regardless of whether the object features a human or an animated\ncharacter, while faithfully following the motions in the driving video. Detailed\nexplanations of the Pose Re-targeting are provided in the supplementary mate-\nrial.\nQualitative Results In Fig. 7, we qualitatively compare our TCAN with exist-\ning human image animation baselines, DisCo and MagicAnimate. For the sake of\nfairness, the re-targeted OpenPose sequence is used as a driving pose sequence\nfor DisCo. We observe that the DisCo fails to maintain temporal consistency\nand character identity. Similarly, MagicAnimate, conditioned on DensePose [11],\ngenerates videos that compromise the source image's identity with the shape in\ndriving video. In contrast, thanks to the APPA layer and pose re-targeting, our"}, {"title": "User Study", "content": "We further conduct a user\nstudy to evaluate the Bizarre dataset. A\ntotal of 27 participants were shown 10\nvideos generated by DisCo, MagicAni-\nmate, and TCAN, respectively. Following\nthis, participants were asked to rate the\nvideo on a scale from 1 to 5 based on\nfour criteria: 1) Motion and identity, 2)\nBackground, 3) Flickering, and 4) Over-\nall preference. The detailed instructions\nfor each criterion are as follows:\nMotion and Identity: Please rate how well the motion of the driving video\nwas reflected while maintaining the identity of the source image."}, {"title": "5 Ablation Study", "content": "Qualitative Results We first investigate the generalization performance of\nAPPA layer in the animation domain. We compare TCAN with a variant where\nControlNet is unfrozen, and the APPA layer is omitted, For both models, we use\nre-targeted OpenPose as the input. As demonstrated in the Fig. 7, even when the\nControlNet is initialized with pre-trained weights, the model exhibits a loss of\nprior pose knowledge if we unfreeze the ControlNet during training. Therefore,\nit fails to incorporate the variant poses in driving videos, leading to artifacts,\nparticularly in the arm and torso regions. In contrast, our model, which keeps\nControlNet frozen with the APPA layer, effectively maintains the appearance of\nthe source image while accurately following the pose of the driving sequence."}, {"title": "6 Conclusion", "content": "We present TCAN, a novel human image animation diffusion model with tem-\nporally consistent pose guidance. Our approach adapts the latent space between\nappearance and poses for effective utilization of the pre-trained pose ControlNet\nand alleviates errors from the off-the-shelf pose extractor by incorporating the\ntemporal module into ControlNet. Moreover, during inference, our temperature\nmap derived from the input pose sequence promises a consistent background of\nthe output video. Experimental results on both challenging TikTok dataset and\nan extension to the unseen domain using Bizarre pose dataset promise the su-\nperiority and generalizability of our method in animating human images. Even\nso, the potential misuse of such videos as deep fakes highlights the ongoing need\nfor research into verification models that can detect traces of generative models."}, {"title": "3.1 Appearance-Pose Adaptation Layer", "content": "To leverage the well-generalized pose conditioning capability of the pre-trained\nControlNet, we opted to freeze the ControlNet in the first stage, in contrast to\nprevious studies. By adding the pose information from ControlNet to the denois-\ning UNet\u2019s encoder feature $z$, we embed the pose information into the denoising\nUNet. This is similar to MagicAnimate [30], but we use the feature from the"}, {"title": "3.3 Pose-driven Temperature Map", "content": "Even with APPA layers and temporal ControlNet, the TCAN trained up to the\nsecond stage experiences flickering in the static region (i.e., background). To ad-\ndress this issue, we propose a novel Pose-driven Temperature Map (PTM). The\nPTM is motivated by the observation that, compared to the foreground, the back-\nground tends to have a less specific focus over the temporal axis. Fig. 4 supports\nthe idea by visualizing the attention maps $m \\in R^{hf\\times wf}$ of the temporal attention\nlayer after the second stage training, where $h$ and $w$ are 64 and the resolution\nof each attention map is $f \\times f$. Each attention map represents the attention of\neach location along the temporal axis across consecutive video frames. For ease\nof interpretation, we resized the generated frames and the driving pose frame\nto a size of attention maps $m$ and overlaid them. Interestingly, we observe that\nthe attention map corresponding to the moving parts, i.e., foreground objects,\nshows higher diagonal values compared to the static background. This indicates\nthat dynamic objects stay at the same location temporally, thereby referencing\nadjacent frames more than the static background region. Consequently, we de-"}, {"title": "where Q and V matrices of the temporal layer.", "content": "Also: \n\n$Attention(Q, K, V) = softmax(\\frac{Q K^{T}}{\\sqrt{d}}) V,$\n$Q = Wz, K = W (z || za), V = W (z || za),$\nwhere || denotes the concatenation operation along the spatial dimension and\nWo denotes the projection matrix of the denoising U-Net. We observe that\nsolely training the appearance encoder with an adaptation of the pre-trained\nControlNet allows for the successful reflection of the driving pose in the generated\nimages. However, we also note a severe texture degradation in generation results,\nas shown in Fig. 3. We conjecture that the degradation of texture information\nis attributable to the misalignment between the pose information from the pre-\ntrained ControlNet and newly trained appearance information. Therefore, we\npropose an Appearance-Pose Adaptation layer (APPA layer), which preserves\nthe appearance of the source image while maintaining pose information from\nthe frozen ControlNet by aligning the feature of two different properties. Our\nproposed APPA layer is illustrated in Fig. 2 (c). We implement the APPA layer\nby employing low-rank adaptation (LoRA) to the existing attention layers of the\ndenoising UNet, specifically formulating the attention layer into the following\nequation, where appearance information $z_{a}$ is injected:\n\n$Q = (W^{Q} + \\Delta W^{Q})z, K = (W^{K} + \\Delta W^{K})(z || z_{a}), V = (W^{V} + \\Delta W^{V})(z || z_{a}).$\nHere, $\\Delta W^{\\{Q,K,V\\}} = B^{\\{Q,K,V\\}} A ^{\\{Q,K,V\\}}$. The matrices $B \\in R^{dxr}$ and $A \\in R^{rxc}$\ndenote low-rank-parameterized matrices with rank $r$. The appearance UNet and\nLORA layers are trained in the first stage. By introducing the APPA layer, we\neffectively address artifacts caused by the frozen ControlNet adaptation. More-\nover, we found that such an approach is helpful for preventing the network from\noverfitting to the training domain and disentangling appearance from the pose."}, {"title": "4 Experiments", "content": "where: $\\ T = TPTM\\cdot D + 1,$\n$\\A' = rearrange(QKT , (bhw) ff bf fhw)$\n$\\T' = rearrange(T, bhw \\rightarrow b11 hw)$\n$\\A = rearrange(A'/(T'\\sqrt{d}), bf fh w \\rightarrow (bhw) f f)$\n$\\z' = softmax(A)V$\nwhere $Qt = WQz,Kt = W{K}z,Vt = WVz$. Here, $WQ,W{K},and W{V}$ are\nweight matrices of the temporal layer."}]}