{"title": "TCAN: Animating Human Images\nwith Temporally Consistent Pose Guidance\nusing Diffusion Models", "authors": ["Jeongho Kim", "Min-Jung Kim", "Junsoo Lee", "Jaegul Choo"], "abstract": "Pose-driven human-image animation diffusion models have\nshown remarkable capabilities in realistic human video synthesis. De-\nspite the promising results achieved by previous approaches, challenges\npersist in achieving temporally consistent animation and ensuring robust-\nness with off-the-shelf pose detectors. In this paper, we present TCAN,\na pose-driven human image animation method that is robust to erro-\nneous poses and consistent over time. In contrast to previous methods,\nwe utilize the pre-trained ControlNet without fine-tuning to leverage\nits extensive pre-acquired knowledge from numerous pose-image-caption\npairs. To keep the ControlNet frozen, we adapt LoRA to the UNet layers,\nenabling the network to align the latent space between the pose and ap-\npearance features. Additionally, by introducing an additional temporal\nlayer to the ControlNet, we enhance robustness against outliers of the\npose detector. Through the analysis of attention maps over the temporal\naxis, we also designed a novel temperature map leveraging pose infor-\nmation, allowing for a more static background. Extensive experiments\ndemonstrate that the proposed method can achieve promising results\nin video synthesis tasks encompassing various poses, like chibi.", "sections": [{"title": "1 Introduction", "content": "Pose-driven human-image animation is a task that breathes life into a static\nhuman in an image. Given a source image and a driving video, the task aims\nto transfer the motion in the driving video to a human in the source image. Al-\nthough it is an intriguing task because of its wide impact on many applications\nsuch as social media, entertainment, and movie industry, several factors make\nthis task challenging. First, the identity in the source image should remain the\nsame over time, changing only the pose. In other words, the appearance of a hu-\nman in the generated video should not be affected by the appearance of a human\nin the driving video. Secondly, the occluded regions in the source image should\nbe naturally inpainted as the pose changes, while ensuring that the inpainted\nregions remain unchanged over time.\nOne possible approach is to impose a pose-guided image generation model\nframe-by-frame [33,34,36]. Upon the success of the Text-to-Image (T2I) diffusion\nmodel, researchers made an effort to control the diffusion model using additional\nconditions in addition to the text prompts [9,10,19,33,34,36,37]. ControlNet [34]\nis the one representative work that enriches the controllability of the stable\ndiffusion model, embracing various conditions, including pose, depth, and edge\nmap. Uni-ControlNet [36] extends ControlNet to handle multiple conditions at\na time, enabling the generation of an image conditioned on a source image and a\npose image. Meanwhile, in an effort to adapt the appearance of the source image\nalong with text prompt, IP-Adapter [33] introduces an image prompt adapter\nthat embeds the fine-grained image feature into a diffusion model. As the IP-\nAdapter is compatible with pose ControlNet [34], a human image with a given\npose can be generated using both. However, the aforementioned methods are\noriented towards a single image generation, not considering temporal consistency.\nTo incorporate temporal consistency into the diffusion model, T2V models\nfocus on the temporal alignment of latents by inserting temporal layers into"}, {"title": "2 Related work", "content": "2.1 Controllable Diffusion Model for Image Generation\nThanks to the scalability and stable training of diffusion models [14, 26], gen-\nerative foundation models [20, 22, 23] are now leading recent image generation\nresearch. Building on the foundation model, active research has been conducted\non the image-based controllable diffusion model. In particular, several studies\nhave expanded the T2I foundation models into an image-controllable generative\nmodel through modules using image conditioning [9, 10, 18, 19, 31, 33, 34]. Such"}, {"title": "2.2 Pose-driven Video Diffusion Models", "content": "Several efforts [3, 4, 12, 25] have expanded image generation models to video\ngeneration models, with most employing a two-stage training scheme [4, 12]. In\nthe first stage, the image generation model is fine-tuned using video frames. In\nthe second stage, temporal layers are added and trained while keeping the base\nimage generation model frozen.\nRecent pose-driven video diffusion models follow the same two-stage training\nparadigm. In the first stage, the pose-driven image generation model is trained\non individual video frames and corresponding pose images extracted using a\npose estimator. Then, the model is inflated by inserting temporal layers to cap-\nture the temporal information of the pose sequence extracted from a driving\nvideo. Disco [28], for example, takes three properties as input human sub-\njects, backgrounds, and poses to achieve not only arbitrary compositionality\nbut also high fidelity. MagicAnimate [30] achieves state-of-the-art performance\nwith a parallel UNet architecture conditioned on the source image through an\nattention mechanism. While both works show impressive video quality in human\nimage animation, they address neither overfitting nor the incompleteness of pose\nsequences.\nUnlike the aforementioned methods, we propose a more generalized model\nthat performs well on datasets beyond the real human image domain by adapting\nprior knowledge from the pre-trained model. Additionally, we use the temporal\nControlNet in the second stage to address incomplete pose information. To the\nbest of our knowledge, both overfitting and pose incompleteness have not been\nexplored in previous research."}, {"title": "3 Method", "content": "An overview of TCAN is presented in Fig. 2. Given a source image and a driv-\ning video with F frames, we aim to generate a video that: 1) incorporates the\nforeground appearance from the source image, 2) follows the pose of the driving\nvideo, and 3) maintains consistency in the background of the source image.\nAs shown in Fig. 2, TCAN follows a two-stage training framework widely\nadopted in current diffusion-based video generation works [4, 12]: In the first\nstage, the source image is transformed to match the pose of each frame in the\ndriving video at the image level. To achieve this, we tackle the human image"}, {"title": "3.1 Appearance-Pose Adaptation Layer", "content": "To leverage the well-generalized pose conditioning capability of the pre-trained\nControlNet, we opted to freeze the ControlNet in the first stage, in contrast to\nprevious studies. By adding the pose information from ControlNet to the denois-\ning UNet's encoder feature z, we embed the pose information into the denoising\nUNet. This is similar to MagicAnimate [30], but we use the feature from the"}, {"title": "3.2 Temporal ControlNet", "content": "Pose-guided human image animation methods, including our TCAN, are condi-\ntioned on the pose sequence estimated from the driving video. Consequently, the\nquality of the generated outputs relies on the accuracy of the pose estimator,\nmaking them inherently susceptible to inaccuracies in the predicted poses. One\npotential method to mitigate the impact of errors from the pose estimator is\nto fix the false detections from the estimated poses before use. However, dis-\ntinguishing noisy keypoints from true positives or adding missing keypoints is\nchallenging and tedious. Moreover, this frame-by-frame noise removal approach\nfails to ensure temporal consistency across multiple frames of the pose sequence.\nTo prevent the generated video from collapsing due to abrupt and erroneous\npose changes, we propose Temporal ControlNet. Temporal ControlNet incorpo-\nrates temporal layers, referred to as the motion module in AnimateDiff [12],"}, {"title": "3.3 Pose-driven Temperature Map", "content": "Even with APPA layers and temporal ControlNet, the TCAN trained up to the\nsecond stage experiences flickering in the static region (i.e., background). To ad-\ndress this issue, we propose a novel Pose-driven Temperature Map (PTM). The\nPTM is motivated by the observation that, compared to the foreground, the back-\nground tends to have a less specific focus over the temporal axis. Fig. 4 supports\nthe idea by visualizing the attention maps $m \\in \\mathbb{R}^{hf \\times wf}$ of the temporal attention\nlayer after the second stage training, where h and w are 64 and the resolution\nof each attention map is $f \\times f$. Each attention map represents the attention of\neach location along the temporal axis across consecutive video frames. For ease\nof interpretation, we resized the generated frames and the driving pose frame\nto a size of attention maps m and overlaid them. Interestingly, we observe that\nthe attention map corresponding to the moving parts, i.e., foreground objects,\nshows higher diagonal values compared to the static background. This indicates\nthat dynamic objects stay at the same location temporally, thereby referencing\nadjacent frames more than the static background region. Consequently, we de-"}, {"title": "3.4 Training and Long-Term Video Prediction", "content": "Training We initialize the denoising UNet with the pretrained weights of Stable\nDiffusion 1.5, the appearance UNet with those of Realistic Vision, and employ\nthe weights of OpenPose ControlNet. In the first stage, we freeze all modules\nexcept the appearance UNet and the LORA layer within the APPA layer. In the"}, {"title": "4 Experiments", "content": "Evaluation setting During training, we use the TikTok dataset [16] only. For\nevaluation, we utilize two different datasets. First, we use 10 TikTok-style videos\ncollected from DisCo [28] as our test dataset, referring to it as the TikTok dataset\nevaluation. Second, to assess the generalization of TCAN, we evaluate the model\ntrained on the TikTok dataset using animation characters. Note that the textures\nand body proportions of the animation characters are quite different from those\nof real human datasets. We select 10 images from the Bizarre Pose Dataset [7]\nas source images and use the 10 TikTok-style test videos as driving videos. We\nrefer to this as the Bizarre dataset evaluation."}, {"title": "4.1 TikTok Dataset Evaluation", "content": "Qualitative Results As shown in Fig. 5, we qualitatively compare our method\nwith two baselines, DisCo [28] and MagicAnimate [30], both of which demon-"}, {"title": "4.2 Bizarre Dataset Evaluation with Pose Re-targeting", "content": "Pose Re-targeting Beyond the TikTok dataset evaluation, where both the\nsource image and driving sequence are derived from the same video, we extend\nour model to applications where the foreground object in the source image pos-\nsesses body proportions different from those of an actual human. Specifically,\nwhen the source images are sampled from an animation domain, there are sig-\nnificant differences in body proportions compared to TikTok dataset, as depicted\nin Fig. 6 and Fig. 7. To address this issue, we propose a re-targeting method-\nology. The pose re-targeting is designed to transfer the body the proportions\nof an object in the source image to that of the target frame, regardless of the\nobject proportion in the driving sequence. When used in practice, we re-target\nthe pose sequence estimated from the driving video so that the retargeted poses\nfit the proportion of the animation character in the source image. As shown in\nFig. 6, re-targeting enables the preservation of proportions of an object in the\nsource image, regardless of whether the object features a human or an animated\ncharacter, while faithfully following the motions in the driving video. Detailed\nexplanations of the Pose Re-targeting are provided in the supplementary mate-\nrial.\nQualitative Results In Fig. 7, we qualitatively compare our TCAN with exist-\ning human image animation baselines, DisCo and MagicAnimate. For the sake of\nfairness, the re-targeted OpenPose sequence is used as a driving pose sequence\nfor DisCo. We observe that the DisCo fails to maintain temporal consistency\nand character identity. Similarly, MagicAnimate, conditioned on DensePose [11],\ngenerates videos that compromise the source image's identity with the shape in\ndriving video. In contrast, thanks to the APPA layer and pose re-targeting, our"}, {"title": "User Study", "content": "We further conduct a user\nstudy to evaluate the Bizarre dataset. A\ntotal of 27 participants were shown 10\nvideos generated by DisCo, MagicAni-\nmate, and TCAN, respectively. Following\nthis, participants were asked to rate the\nvideo on a scale from 1 to 5 based on\nfour criteria: 1) Motion and identity, 2)\nBackground, 3) Flickering, and 4) Over-\nall preference. The detailed instructions\nfor each criterion are as follows:\nMotion and Identity: Please rate how well the motion of the driving video\nwas reflected while maintaining the identity of the source image."}, {"title": "5 Ablation Study", "content": "Qualitative Results We first investigate the generalization performance of\nAPPA layer in the animation domain. We compare TCAN with a variant where\nControlNet is unfrozen, and the APPA layer is omitted, For both models, we use\nre-targeted OpenPose as the input. As demonstrated in the Fig. 7, even when the\nControlNet is initialized with pre-trained weights, the model exhibits a loss of\nprior pose knowledge if we unfreeze the ControlNet during training. Therefore,\nit fails to incorporate the variant poses in driving videos, leading to artifacts,\nparticularly in the arm and torso regions. In contrast, our model, which keeps\nControlNet frozen with the APPA layer, effectively maintains the appearance of\nthe source image while accurately following the pose of the driving sequence."}, {"title": "6 Conclusion", "content": "We present TCAN, a novel human image animation diffusion model with tem-\nporally consistent pose guidance. Our approach adapts the latent space between\nappearance and poses for effective utilization of the pre-trained pose ControlNet\nand alleviates errors from the off-the-shelf pose extractor by incorporating the\ntemporal module into ControlNet. Moreover, during inference, our temperature\nmap derived from the input pose sequence promises a consistent background of\nthe output video. Experimental results on both challenging TikTok dataset and\nan extension to the unseen domain using Bizarre pose dataset promise the su-\nperiority and generalizability of our method in animating human images. Even\nso, the potential misuse of such videos as deep fakes highlights the ongoing need\nfor research into verification models that can detect traces of generative models."}, {"title": "Algorithm 1: Pseudo-Code for Pose-Retargeting", "content": "def pose_retarget (cur_kp, src_kp, init_kp, kp_mapper):\n# cur_kp: 18 keypoint coordinates from the current frame.\n#src_kp: 18 keypoint coordinates from the source image.\n# init_kp: 18 keypoint coordinates from the initial frame.\n# kp_mapper: A mapper storing the relationship between two connected keypoints.\n# Calculate the ratio of the lengths between the keypoints of the initial and the\ncurrent frame.\ncurinit_ratio = [1] * 18\nfor kp1 in range(2, 18):\nkp2 = kp_mapper [kp1]\ncur_length = get_length(cur_kp[kp1], cur_kp [kp2])\ninit_length = get_length(init_kp[kp1], init_kp[kp2])\ncurinit_ratio [kp1] = cur_length / init_length\n# First adjusting the length between the neck(1)-nose(0).\nneck_kp = 1\nnose_kp = kp_mapper [neck_kp]\nsrc_length = get_length(src_kp[neck_kp], src_kp [nose_kp])\ncur_length = get_length(cur_kp [neck_kp], cur_kp [nose_kp])\nratio = cur_length / src_length\ndiff_vector = get_diff_with_ratio(cur_kp [neck_kp], cur_kp[nose_kp], ratio)\ncur_kp [nose_kp] += diff_vector\n# Move all subkeypoints of the nose keypoint (i.e., eyes and ears)\ncur_kp = move_subkeypoints(cur_kp, nose_kp, diff_vector)\n# Traverse and move the remaining keypoints and subkeypoints based on the length\nof the nose-neck.\nfor kp1 in range(2, 18):\nkp2 = kp_mapper [kp1]\nsrc_length = get_length(src_kp[kp1], src_kp [kp2])\ncur_length = get_length(cur_kp[kp1], cur_kp [kp2])\nratio = src_length / cur_length * curinit_ratio [kp2]\ndiff_vector = get_diff_with_ratio(cur_kp [kp2], cur_kp [kp2], ratio)\ncur_kp [kp2] += diff_vector\ncur_kp = move_subkeypoints(cur_kp, kp2, diff_vector)\nretargeted_cur_kp = cur_kp\nreturn retargeted_cur_kp"}, {"title": "A Comparison with AnimateAnyone", "content": "In this section, we compare our TCAN to the recent human image anima-\ntion method, AnimateAnyone [15], which also uses OpenPose as the input pose\ncondition. Since the AnimateAnyone does not have an official code, we trained\nre-implemented version of AnimateAnyone on TikTok training dataset.\nTable 3 shows the quantitative results of both methods. Our model outper-\nforms the re-implemented AnimateAnyone in all performance metrics. Fig. 10\nshows the qualitative comparison of TikTok and bizarre dataset. As for Ani-\nmateAnyone, we observe the distorted face region under inaccurate pose con-\ndition (Fig. 10 (a)), and the black object flickering in the background (Fig. 10"}, {"title": "B Additional Videos for Visual Comparison", "content": "Qualitative Comparison on TikTok Dataset To demonstrate the effec-\ntiveness of TCAN and to compare our approach with human image animation\nbaselines, we upload videos on our project page , which showcases generated\nvideos where a source image is the first frame of each driving video. The selected\nframe from the video is attached in Fig. 11 (a). It is evident from the video\nthat TCAN yields the most realistic results compared to the others. Among all\nbaselines, MagicAnimate stands out as the closest competitor. MagicAnimate's\nadvantage comes from its use of DensePose for guidance, providing detailed shape\ninformation from the driving video that aligns with the shape of the source im-\nage's foreground object. In contrast, our TCAN achieves remarkable performance\nby utilizing sparse pose information only.\nQualitative Comparison on Online-Collected Images In this section, we\nshow additional results on the online-collected animation characters. We have\ncollected characters with diverse body proportions and styles, and use them as\nsource images. As depicted in Fig. 12, our methodology follows the motion of\nthe foreground object in driving videos while effectively maintaining the iden-\ntity and background of the source image. Furthermore, we upload generated\nvideos using animation characters on our project page for qualitative compar-\nisons with baseline methods (i.e., DisCo, MagicAnimate, and re-implemented"}, {"title": "C Detailed Description of Pose Retargeting", "content": "As discussed in section 4.2, we apply the pose re-targeting algorithm to align\nthe ratio of foreground objects of driving images to that of the source image.\nSpecifically, we fix the 'neck' keypoint of the driving image and adjusted the\nposition of the 'nose' so that the distance between the 'nose' and 'neck' keypoints\nin the source image matched the corresponding distance in the driving image.\nThe keypoints connected to the 'nose' were moved by the same amount. Similarly,\nwe traverse through the remaining keypoints, adjusting their positions to match\nthe distances between the keypoints in the source image. We provide a detailed\ndescription of Pose-Retargeting in Algorithm 1 as pseudo-code."}]}