{"title": "Hybrid Local Causal Discovery", "authors": ["Zhaolong Ling", "Honghui Peng", "Yiwen Zhang", "Peng Zhou", "Xingyu Wu", "Kui Yu", "Xindong Wu"], "abstract": "Local causal discovery aims to learn and distinguish the direct causes and effects of a target variable from observed data. Existing constraint-based local causal discovery methods use AND or OR rules in constructing the local causal skeleton, but using either rule alone is prone to produce cascading errors in the learned local causal skeleton, and thus impacting the inference of local causal relationships. On the other hand, directly applying score-based global causal discovery methods to local causal discovery may randomly return incorrect results due to the existence of local equivalence classes. To address the above issues, we propose a Hybrid Local Causal Discovery algorithm, called HLCD. Specifically, HLCD initially utilizes a constraint-based approach combined with the OR rule to obtain a candidate skeleton and then employs a score-based method to eliminate redundant portions in the candidate skeleton. Furthermore, during the local causal orientation phase, HLCD distinguishes between V-structures and equivalence classes by comparing the local structure scores between the two, thereby avoiding orientation interference caused by local equivalence classes. We conducted extensive experiments with seven state-of-the-art competitors on 14 benchmark Bayesian network datasets, and the experimental results demonstrate that HLCD significantly outperforms existing local causal discovery algorithms.", "sections": [{"title": "INTRODUCTION", "content": "Causal discovery has always been an important goal in many areas of scientific research [1], [2]. It reveals the underlying causal mechanisms of data generation and contributes to solving decision-making problems in ma- chine learning [3], [4]. Learning a Bayesian network (BN) from observational data is the popular method for causal discovery [5], [6]. The structure of a BN takes the form of a directed acyclic graph (DAG), where nodes signify variables, and edges represent cause-effect relationships between variables [7]. In recent years, many global causal discovery algorithms have been proposed, which aim to learn the entire causal network of all variables, such as MMHC [8], GGSL [9], and ADL [10]. However, in many practical scenarios, it is not necessary to waste time learn- ing a global causal network when we are only interested in the causal relationships around a given variable [11]. To address this challenge, local causal discovery algorithms have been proposed.\nLocal causal discovery aims to uncover the causal structure surrounding a specific variable. However, due to the unavailability of complete global information, many edge directions determined by relationships with dis- tant variables\u00b9 remain unidentified. As a result, most existing methods adopt a progressive learning approach to gradually acquire outer layer information, until the causal directions around the target variable are identified. Consequently, local causal discovery commonly employs the faster constraint-based methods [12], [13], as score- based methods exhibit higher time complexity and are not well-suited for this gradual information acquisition process [14].\nSimilar to global causal discovery methods, local causal discovery is susceptible to common issues asso- ciated with conditional independence (CI) testing [15], which can impact its accuracy [16], [17]. One prominent concern is that CI testing cannot accurately determine the causal skeleton. As a result, many approaches employ symmetry tests to address this limitation. However, the prevailing AND and OR rules used in these tests in- troduce certain errors [18], [19]. The AND rule aims to rigorously eliminate all erroneous relationships, while the OR rule [19] seeks to include as many true positives as possible, operating under a more lenient criterion. Empiri- cal studies have provided evidence that approaches based on the AND rule achieve superior precision, whereas methods based on the OR rule exhibit better performance in terms of recall [20]. Consequently, neither approach yields completely accurate results. Additionally, in the context of local causal learning, the presence of data bias caused by unconsidered distant relationships and inherent noise in the data further compounds the negative"}, {"title": "RELATE WORK", "content": "The majority of local causal discovery algorithms are constraint-based. They both use CI tests singularly to construct and orient causal networks. Some pioneering algorithms, Local Causal Discovery (LCD) [21] and its variants, use CI tests to learn the causal relationships among every four variables. BLCD learns the Y-structure in the MB of a target variable [22]. While LCD/BLCD algorithms aim to identify a subset of causal edges via special structures among all variables, not distinguishing between the direct causes and effects of the target."}, {"title": "NOTATIONS AND DEFINITIONS", "content": "In this section, we will briefly introduce some basic notations and definitions.\nDefinition 1 (Bayesian Network). [7] Let U denote a set of random variables and indicate the conditional probability distribution of each node $X \\in U$ given its parents. A Bayesian Network, $B(G, \\Theta)$, is represented by a tuple consisting of a DAG G, and a set of parameters $\\Theta$. In a BN $B(G, \\Theta)$, each variable in G is independent of any subset of its non-descendants given its parents.\nWe can decompose the joint probability distribution into local conditional probabilities using Markov conditioning.\nDefinition 2 (V-Structure). [7] The triplet of variables X, Y, and T forms a V-structure if node T has two incoming edges from X and Y respectively, i.e. $X \\rightarrow T \\leftarrow Y$, and X is not adjacent to Y.\nIn a BN, T is a collider if two directed edges are from X to T and Y to T, respectively. i.e., the cause variables of colliding nodes can be recognized by the V-structure.\nDefinition 3 (Symmetry Constraint). [32] For a node X to be a parent or child of Y in a DAG. Then, X must be in the PC set of Y and Y must be in the PC set of X, i.e., $X \\in PC_y$ and $Y \\in PC_x$.\nIn a BN, learning causal relationships between variables from data is sometimes asymmetric. In this case, using only the AND or OR rule individually to eliminate causal asymmetry does not lead to accurate results [18].\nDefinition 4 (Score Consistency). [33] Let D be a set of data consisting of i.i.d. samples from some distribution P. A score criterion S is consistent if, as the size of the D goes to infinity, the following two properties hold true:\n1. if the structure G contains P and another structure G' does not, then $S(G, D) > S(G',D)$.\n2. if G and G' both contain P but G has fewer parameters, then $S(G, D) > S(G', D)$.\nThe graph 9 contains Pif there exists a set of parameter values O for G such that the parameterized BN model (G, O) represents P exactly.\nDefinition 5 (Local Score Consistency). [33] Let D be a set of data consisting of i.i.d. samples from some distribution P. Let G be any BN structure and G' be the same structure as G but with an edge from a node Y to a node X. Let PC be the parent set of X in G. A score criterion S is locally consistent if, as the size of the D goes to infinity, the following two properties hold true:\n1. if X | Y | PC, then $S(G, D) < S(G',D)$.\n2. if X | Y | PC, then $S(G, D) > S(G',D)$.\nThe score-based methods rely on score criteria to learn the best-fit DAG G for the data samples. In general, the higher the score for G, the better the fit to the data D, and vice versa.\nFor example, information theoretic scores [34] aim to avoid over-fitting by balancing the goodness of fit with model dimensionality given the available data. The gen- eral form of these scores can be expressed as:\n$S(G, D) = log[p(D|G)] \u2013 A(D, G)$\nWhere $log[p(D|G)]$ denotes the goodness of fit as measured by the log likelihood of the data given the graph, in the case where the distribution parameters, $\\Theta$, take their maximum likelihood estimation values. The computation of log (DIG) is as follows:\n$log[p(DIG)] = \\sum_{i=1}^{n} \\sum_{j=1}^{q_i} \\sum_{k=1}^{r_i} N_{ijk} log \\frac{N_{ijk}}{N_{ij}} = S_{LL} (G, D)$"}, {"title": "THE PROPOSED METHOD", "content": "In this section, we describe our approach in detail, includ- ing the theoretical analysis and algorithmic specifics.\n4.1 The local causal discovery strategy\nIn this section, we describe the hybrid local causal dis- covery strategy for HLCD, which is constructed based on the following two theorems. In the following proofs and experiments, we use the AIC score function by default if not otherwise specified and denote the AIC score of the local structure $X \\rightarrow Y$ by $S(X \\rightarrow Y,D)$ (\u00a2 \u2192 Y means that no node points to Y).\nTheorem 1. Let T be any variable in U, and X be a variable in $PC_T$. Then node X (or T) can increase the scores of local structures \u00a2 \u2192 T (or \u00a2 \u2192 X), and the added scores are consistent, i.e. $S(X \\rightarrow T,D) \u2013 S(\u00a2 \\rightarrow T,D) = S(T \\rightarrow X,D) \u2013 S(\u00a2 \\rightarrow X,D) > 0$ holds.\nProof: Assuming that the local structure $X \\rightarrow T$ is the correct local causal network, denoted as G, and T \u2192 X is denoted G'. The AIC score for G is calculated as follows:\n$S(G,D) = log[p(D|\u00a2 \\rightarrow X)] + log[p(D X \\rightarrow T)] \u2013 \\triangle(D, G)$\nWriting the log-likelihood term in multiplicative form and using the probabilistic representation, $\\frac{N_{ijk}}{N_{ij}}$, one can further obtain the following equation:\n$S(G, D) = \\prod_{k=1}^{q_X} p(X|\\phi)^{N_k} \\prod_{j=1}^{q_T}\\prod_{k=1}^{r_T} p(T|X)^{N_{jk}} \u2013 \\triangle(D,G)$\nSince the local structure X \u2192 T is the correct causal network, according to the Markov Condition of BN, $p(X|T)$ can be written as posterior probability. In addi- tion, it is important to note that the number of terms in the log-likelihood of the local scores of each node is the size of the D, so the following equation holds:\n$S(G, D) = \\prod_{k=1}^{q_X} p(X|\\phi)^{N_k} \\prod_{j=1}^{q_T}\\prod_{k=1}^{r_T} [\\frac{p(T)}{p(\\phi)}p(X|T)]^{N_{jk}} \u2013 \\triangle(D, G)$=\n$\\prod_{k=1}^{q_T} p(T|\\phi) \\prod_{j=1}^{q_T}\\prod_{k=1}^{r_T} p(X|T)^{N_{jk}} \u2013 \\triangle(D, G)$\nSince X \u2192 T and T \u2192 X have the same penalty term (\u2206 = $r_X r_T$ \u2212 1), it follows from Eq.5 that $S(X \\rightarrow T,D) + S(\\phi \\rightarrow X,D) = S(T \\rightarrow X,D) + S(\\phi \\rightarrow T, D)$ holds, i.e. $S(X \\rightarrow T,D) \u2013 S(\\phi \\rightarrow T,D) = S(T \\rightarrow X,D) \u2013 S(\\phi \\rightarrow X, D)$ holds.\nFurther analysis, according to $X \\in PC_T$, then $Z \\subseteq U\\backslash{T,X} \\forall T | Z$ holds. So when the condition set is (empty), $X \\parallel T | \\phi$ holds. according to local score consistency, then $S(X \\rightarrow T,D) \u2013 S(\\phi \\rightarrow T,D) > 0$, $S(T \\rightarrow X,D)\u2212S(\\phi \\rightarrow X, D) > 0$ holds. According to the above conclusion, we can ultimately establish that $S(X \\rightarrow T,D)\u2212S(\\phi \\rightarrow T,D) = S(T \\rightarrow X,D)\u2212S(\\phi \\rightarrow X, D) > 0$ holds.\nFurthermore, for BDeu score function, since X \u2192 T and T \u2192 X have the same prior parameter and graphical probability distribution $p(G)/p(G')$, and it sat- isfies local score consistency. Thereby, similarly, the same conclusion can be drawn.\nWith Theorem 1, we can then maximize the local scores of the target nodes to remove the redundant skeleton during skeleton construction.\nSince the scores of the equivalence class structure are the same, e.g. the score of X \u2190 T \u2192 Y is $S(\\phi \\rightarrow T,D) + S(T \\rightarrow X,D) + S(T \\rightarrow Y,D)$, and the score of X \u2192 T \u2192 Y is $S(\\phi \\rightarrow X,D) + S(X \\rightarrow T,D) + S(T \\rightarrow Y,D)$, by Theorem 1, $S(\\phi \\rightarrow T,D) + S(T \\rightarrow X,D) = S(\\phi \\rightarrow X,D)+S(X \\rightarrow T, D)$ holds, and the score of both are the same. Thus, in the following, we consider X \u2192 T \u2192 Y as the representative of the equivalence class.\nTheorem 2. Let X, Y,T \u2208 U and T be a target node with no edge connected between X and Y, and X, Y \u2208 PCT. If the score of local structures X \u2192 T \u2190 Y is greater than the score of local structures X \u2192 T \u2192 Y, i.e., $S(\\phi \\rightarrow X,D)+S(\\phi \\rightarrow Y,D)+S(X,Y \\rightarrow T,D) > S(\\phi \\rightarrow X,D) + S(X \\rightarrow T,D) + S(T \\rightarrow Y, D)$, then there exists a V-structure in variables X, Y, T, and T is a collision node.\nProof: Assuming that the local structure X \u2192T\u2190 Y is the correct local causal network, denoted as G, and X \u2192 T \u2192 Y is denoted G'. The AIC score for S(G, D) \u2013 S(G', D) is calculated as follows:\n$S(G,D) \u2013 S(G',D) = S(\\phi \\rightarrow X,D) + S(\\phi \\rightarrow Y,D) + S(X,Y \\rightarrow T,D) \u2013 S(\\phi \\rightarrow X,D) \u2013 S(X \\rightarrow T,D) \u2013 S(T \\rightarrow Y,D)$\nSubtracting the same terms, the log-likelihood terms are written in multiplicative form and using probabilities instead of $\\frac{N_{ijk}}{N_{ij}}$, the following equation can be obtained:\n$S(G,D) \u2013 S(G',D) = [(\\frac{\\prod [p(T|X, Y)^{N_{jk}})}{(\\prod_{j=1}^{Q_Y} \\prod_{k=1}^{T_Y} p(T|X)^{N_{jk}})(\\prod_{j=1}^{Q_Y} \\prod_{k=1}^{T_Y} p(Y|T)^{N_{jk}})})] + \\triangle(D, G') \u2013 \\triangle(D, G)$\nSince the local structure X \u2192 TY is the correct causal network, according to the Markov Condition of BN, $p(Y|T)$ can be written as $p(T|Y)p(Y)/p(T)$. The above equation can be transformed as follows:\n$S(G,D) \u2013 S(G',D) = [(\\frac{\\prod [p(T|X,Y)^{N_{jk}}]}{\\prod [p(T|X)^{N_{jk}}]})/(\\frac{\\prod [p(T|Y)^{N_{jk}}]}{ [p(T|\\phi)^{N_{k}}]} + \\triangle(D, G') \u2013 \\triangle(D, G) = [S_{LL}(X \\rightarrow T \\leftarrow Y|D) \u2013 S_{LL}(X \\rightarrow T)] \u2013 [S_{LL}(Y \\rightarrow T|D) \u2013 S_{LL}(\\phi \\rightarrow T)] + \\triangle(D, G') \u2013 \\triangle(D, G)$\nwhere $[S_{LL}(X \\rightarrow T \\leftarrow Y|D) \u2013 S_{LL}(X \\rightarrow T)]$ can be viewed as the score by which $S_{LL}(X \\rightarrow T)$ increases when one Y is added. Similarly, $[S_{LL}(Y \\rightarrow T|D) \u2013 S_{LL}(\\phi \\rightarrow T)]$ can be viewed as the score that $S_{LL}(\\phi \\rightarrow T)$ increases by adding one Y. But they are different in that $[S_{LL}(X \\rightarrow T \\leftarrow Y|D) \u2013 S_{LL}(X \\rightarrow T)]$ has one more node X compared to $[S_{LL}(Y \\rightarrow T|D) \u2013 S_{LL}(\\phi \\rightarrow T)]$, and $X \\in P_T$. According to score consistency, $[S_{LL}(X \\rightarrow T \\leftarrow Y|D) \u2013 S_{LL}(X \\rightarrow T)]$ contains more parameters compared to $[S_{LL}(Y \\rightarrow T|D) \u2013 S_{LL}(\\phi \\rightarrow T)]$, so $[S_{LL}(X \\rightarrow T \\leftarrow Y|D) \u2013 S_{LL}(X \\rightarrow T)] \u2013 [S_{LL}(Y \\rightarrow T|D) \u2013 S_{LL}(\\phi \\rightarrow T)] > 0$. Moreover, since A is a penalizes graph complexity and $\\triangle(D, G') \u2013 \\triangle(D,G) = r_X r_T+r_Y r_T+r_X r_Y+1-r_X-r_T-r_Y-r_X r_T r_Y$. Thus, if the score of local structure X \u2192 T \u2190 Y is still higher than the local structure of X \u2192 T \u2192 Y after considering the graph complexity, we consider that X, Y, T forms a V-structure and that T is a collision node.\nSimilarly, assuming that X \u2192 T \u2192 Y is the real local causal structure, denoted as G', and X \u2192 T\u2190Y is denoted G. The AIC score for S(G,D) \u2013 S(G',D) is calculated as follows:\n$S(G,D) \u2013 S(G', D) = [(\\frac{\\prod [p(T|X, Y)^{N_{jk}}] \\prod p(Y|\\phi)^{N_k}}{(\\prod [p(T|X)^{N_{jk}})] \\prod [p(Y|T)^{N_{jk}})})] + \\triangle(D, G') \u2013 \\triangle(D, G) = \\prod \\frac{p(Y|(TX))^{N_k}}{Y\\T} + \\triangle(D, G') \u2013 A(D, G)$\nAt this point, the structures (X \u2192 T) \u2192 Y and T \u2192 Y contain the X \u2192 Y parameter O, but T \u2192 Y has less X \u2192 T parameters \u0472 than (X \u2192 T) \u2192 Y. Therefore, according to the second article of score consistency, the score of T \u2192 Y is higher than that of (X \u2192 T) \u2192 Y. Similarly, if the score of local structure X \u2192 T \u2192 Y is still greater than the local structure of X \u2192 T \u2190 Y after considering the \u2206, we consider node X, Y, Z to be the equivalence class.\nFor BDeu score fuction, X \u2192 T\u2190 Y and X \u2192 T\u2192 Y have the same prior graph probability $p(G)/p(G')$, and it satisfies score consistency. Thus, the same conclusion can be drawn if the maximum posterior probability of X \u2192 T \u2190 Y is greater or less than X \u2192 T \u2192 Y when the prior parameter O distribution is considered."}, {"title": "Detailed descriptions of the HLCD algorithm", "content": "In this section, we present the proposed HLCD algorithm through the theoretical analysis in the previous section.\nStep 1: Hybrid-based local causal skeleton construc- tion (Lines 2-14): HLCD first pops a variable at the front of the queue Q and assigns it to the current iteration node Z (initially Z is the given target T) (Line 4). Then, HLCD uses the constraint-based PC discovery algorithm to find the $PC_Z$ and constructs a local causal skeleton with the OR rule (Line 6). As we do not need MBs and separating sets for the edge orientation phase, HLCD can use any of the state-of-the-art PC discovery algorithms, such as MMPC, HITION-PC, FCBF, etc. Then, the HLCD stores Z into V to prevent repeated learning of the PC of variables (line 7). At this point, HLCD builds an initial local causal skeleton from the OR rule and learned PC sets.\nAs the OR rule can generate a comprehensive but redundant causal skeleton, HLCD uses the score-based method to eliminate redundant causal skeletons, ensuring they don't interfere with subsequent causal orientations. With the analysis of Theorem 1, if node X \u2208 $PC_Z$, then the following $S(X \\rightarrow Z,D) \u2013 S(\\phi \\rightarrow Z,D) = S(Z \\rightarrow X,D) \u2013 S(\\phi \\rightarrow X,D) > 0$ will be hold. HLCD does this by testing each variable X in $PC_Z$ to see if it satisfies Theorem 1, and removing it from $PC_Z$ if it does not satisfy (Lines 9-13). Then, HLCD pushes all variables in $PC_Z$ into Q to recursively find the PC of each node in $PC_Z$ in the next iterations for expanding (Line 14). At the end of step 1, HLCD obtains the accurate local causal skeleton consisting of all nodes in the set V and their PC nodes.\nStep 2: Hybrid-based local causal orientation (Lines 15-22): To avoid the effect of the score equivalence, HLCD distinguishes between V-structures and equivalence class structures by employing the score-based method. Specif- ically, HLCD identifies the V-structures in the causal skeleton by comparing the two local structure scores of each tuple X, Y and Z (X,Y \u2208 $PC_Z$) in the causal skeleton obtained in step 1. If $S(\\phi \\rightarrow X,D) + S(\\phi \\rightarrow Y,D) + S(X,Y \\rightarrow Z,D) > S(\\phi \\rightarrow X,D) + S(X \\rightarrow Z,D) + S(Z \\rightarrow Y,D)$, then the edge X \u2013 Z and edge Y \u2013 Z will be oriented as X \u2192 Z and Y \u2192 Z (Lines 16- 20). It may be the V-structure consisting of Z and Pz, or consisting of Z and Cz and spouse nodes. At this point, HLCD orients the causal orientations of all V-structures in the current causal skeleton and does not orient the causal orientations of equivalent class structures.\nFinally, HLCD uses the constraint-based Meek-rule as well as the discovered V-structure to orient the causal orientations of the nodes in the set V (Line 21). If all causal orientations of the T are recognized in the current V, learning stops, otherwise it continues to expand outward until it distinguishes between the father and child nodes of the T (Line 22).\nTheorem 3 (Correctness of HLCD). Given a set of i.i.d data D, and samples from some distribution P. HLCD distinguishes all parents from children of a given variable.\nProof: According to Theorem 1, if X \u2208 $P_T$, the local scores of X \u2192 T and T \u2192 X are the same and higher than $\\phi \\rightarrow T$ and $\\phi \\rightarrow X$. Therefore, Step 1 will keep all the true PCs found by the PC discovery algorithm and remove the false positive nodes. In the local skeleton, if X, Y \u2208 $P_T$, the local score of X \u2192 T \u2190 Y is higher than that of X \u2192 T \u2192 Y according to Theorem 2. Thus, Step 2 will find all correct V-structures in the local skeleton and will not orient the edge directions of the equivalence classes. At this point, T and all its parent nodes are found correctly. Finally, the child nodes oriented out of the Meek- rule are correct. Thus, all the parents and children of a given target variable distinguished by HLCD are correct."}, {"title": "EXPERIMENTS", "content": "In this section, we conduct experiments to evaluate the performance of our method. In Section 5.1, we describe the experiment settings. In Section 5.2, we provide de- tailed experimental figures, and the results are analyzed.\n5.1 Experimental settings\n5.1.1 Datasets\nWe use 14 benchmark BNs to evaluate HLCD against its rivals. Each benchmark BN contains two groups of data:\n5.1.4 Implementation details\nAll the code\u00b3 implementations are done in Matlab or Python [35], and the experiments are conducted on a computer with an Intel Core i7-12700 CPU and 8GB of memory. The significance level for the CI tests is set at 0.01 and the threshold for mutual information is set at 0.03. Furthermore, PCD-by-PCD is using the MMPC [36] in the skeleton construction stage. MB-by-MB uses IAMB [37] to obtain the MBs. CMB uses HITON-MB [27] to obtain MBs. LCS-FS uses FCBF [29] for skeleton construction. ELCS uses HITONPC [27] for skeleton construction. PSL uses PCsimple [38] to obtain the skeleton. Therefore, to avoid the disparities in results caused by the PC algorithm, we keep the PC algorithm used by HLCD consistent with its comparison algorithms and compare the experimental results separately.\n5.2 Experimental results\nIn this section, we report the experimental results of HLCD with its rivals on 14 BN datasets. Section 5.2.1 re- ports structural correctness and structural errors, Section 5.2.2 focuses on the time efficiency of the algorithms, and Section 5.2.3 performs ablation experiments to verify the performance of Theorem 1 and Theorem 2, respectively.\n5.2.1 Structural correctness and errors\n(1) HLCD-FS against LCS-FS and MB-by-MB. Table 2 shows that HLCD-FS significantly enhances precision and recall compared to LCS-FS across most networks. For in- stance, with a sample size of 500, HLCD-FS improves pre- cision by over 9% and recall by over 7% on Alarm, Child, and Hailfinder networks, enhancing precision and recall by around 3% on Link, Pigs, and Gene networks. This trend continues with a sample size of 1000. Compared with MB-by-MB, when the sample size is 500, HLCD-FS increases precision by 16% to 38% and improves recall by 10% to 30% in low-dimensional networks (Alarm, Child, Insurance, Barley, Hailfinder). Moreover, in high-dimensional networks (Pigs, Gene), HLCD-FS increases precision and recall by over 50%. With a sample size of 1000, HLCD-FS significantly outperforms MB-by-MB in precision and recall on the mentioned networks.\nBased on Table 2, HLCD-FS maintains comparable miss edges and fewer extra edges as LCS-FS on Barley, Hailfinder, Pigs, and Link networks. Specifically, for a sample size of 500, HLCD-FS reduces extra edges by around 74%, 11%, 52%, and 7% respectively. For a sample size of 1000, the reduction is 58%, 15%, 7%, and 3% respectively. In terms of reverse and undirected met- rics, HLCD-FS consistently exhibits fewer reversed and undirected edges than LCS-FS in the Alarm and Gene networks, fewer undirected edges in the Child and In- surance networks, and fewer reversed edges in other net- works. Compared to MB-by-MB, HLCD-FS significantly decreases misses and extra edges on Alarm, Child, Insur- ance, Hailfinder, Pigs, and Gene networks. On undirected\n(2) HLCD-H against CMB and ELCS. Based on Table 3, HLCD-H consistently outperforms CMB and ELCS in precision and recall across most networks. Specifically, compared to CMB, at a sample size of 500, HLCD-H improves precision by over 7% and recall by over 9% on the Alarm, Child and Insurance network, achieves improvements of over 11% in precision and 13% in recall on the Hailfinder network, and shows increases of over 23% in precision and over 18% in recall on the Pigs and Gene networks. Compared to ELCS at a sample size of 500, HLCD-H improves precision by over 6% and recall by over 7% on Alarm networks, achieves increases ranging from over 5% to over 16% in precision and recall on Child, Insurance, and Hailfinder networks, and demonstrates improvements of over 10% in precision and over 1% in recall on Pigs, and over 14% in precision and over 10% in recall on Gene. Similarly, at a sample size of 1000, HLCD- H's precision and recall are notably superior to CMB and ELCS across all networks except Link network.\nTable 3 suggests that compared to CMB and ELCS, HLCD-H exhibits comparable missed edges and signifi-\n(3) HLCD-P against PSL. Table 4 indicates that HLCD- P achieves higher precision and recall on the majority of networks compared to PSL. Specifically, at a sample size of 500, HLCD-P demonstrates improvements of over\n(4) HLCD-M against PCD-by-PCD and GraN-LCS."}, {"title": "CONCLUSION", "content": "In this paper, we discuss the limitations of AND and OR rules in constructing exact local causal skeletons, and the problem of global causal discovery methods randomly returning incorrect local causal networks due to equivalence classes. To address the above issues, we propose a new hybrid local causal discovery algorithm (HLCD). Specifically, During the skeleton construction phase, HLCD uses maximized local scores to eliminate redundant causal skeleton structures, thereby providing a more precise causal network space. In the skeleton orien- tation phase, HLCD employs an innovative score-based V-structure identification approach to avoid interference caused by equivalence classes. The experimental results show that the quality of local causal discovery of HLCD is significantly better than existing methods, especially in the small sample case. In future work, we may consider a hybrid approach to improve the accuracy of MB discovery for class variables."}]}