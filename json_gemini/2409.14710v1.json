{"title": "ERABAL: Enhancing Role-Playing Agents through Boundary-Aware Learning", "authors": ["Yihong Tang", "Jiao Ou", "Che Liu", "Fuzheng Zhang", "Di Zhang", "Kun Gai"], "abstract": "Role-playing is an emerging application in the field of Human-Computer Interaction (HCI), primarily implemented through the alignment training of a large language model (LLM) with assigned characters. Despite significant progress, role-playing agents (RPLAs) still struggle with maintaining role-consistency across conversations, particularly when confronted with boundary queries subtly related to character attributes. In this paper, we present ERABAL, a framework aimed at enhancing RPLAs' role-playing capabilities through boundary-aware learning. ERABAL encompasses a generation pipeline for role-specific dialogues and a concomitant methodology for alignment training. Through comprehensive evaluations, we demonstrate that ERABAL is both efficient and effective. By training with significantly fewer dialogues than those used in leading approaches, ERABAL achieves notable improvements across WikiRoleEval, CharacterEval, and the role-playing subset of MT-Bench compared to the generalist baseline models. Our code and datasets will be made publicly available to support further research.", "sections": [{"title": "1 Introduction", "content": "Role-playing is an emerging research field (Chen et al., 2024; Tseng et al., 2024) in which AI agents engage in dialogues with humans by simulating the linguistic style or behavioral traits of the assigned characters (Ouyang et al., 2022; Yi et al., 2024). As large language models (LLMs) have shown great potential for achieving human-level intelligence, their use in role-playing agents (RPLAs) (Shanahan et al., 2023; Mao et al., 2023; Zhou et al., 2023; Li et al., 2023a; Park et al., 2023) has garnered increasing attention, leading to remarkable progress recently. Nonetheless, RPLAs still struggle to maintain role consistency across multi-turn dialogues, especially when confronted with queries that are subtly related to character attributes but not explicitly mentioned within the prompts, resulting in an out-of-character (OOC) issue."}, {"title": "2 Boundary-aware Dialogue Construction", "content": "Figure 2 shows the overall data generation process. To construct a boundary-aware dialogue, we begin with the dialogue planner and topic manager. The two modules generate instructions at the start of each turn, determining whether to generate an \"ordinary\u201d or \u201cboundary\u201d query and specifying the conversational topic. Here we introduce ordinary questions to ensure the constructed dialogue remains coherent, thereby making RPLAS with such data practical for real applications. The instructions are then passed to the query generator and the response generator, which are responsible for generating specific question-answer pairs based on external character attributes. In the following section, we will first describe each component, then introduce the quality control strategies, and finally provide an overall statistic of the constructed dataset."}, {"title": "2.1 Main Components", "content": "Dialogue Planner The dialogue planner, motivated by (Yuan et al., 2023), functions as a binary switch that provides an instruction for the dialogue state at the beginning of each turn. The instruction it outputs is either \u201cordinary\u201d or \u201cboundary\u201d, determined by the given character attributes, dialogue context, and historical planning states. The ordinary output instructs the query generator to produce a general role-specific query, such as chitchat or banter, while the boundary output guides the construction of the query with a potential trap (i.e., the boundary query). We prompt the dialogue planner to generate boundary outputs as frequently as possible while allowing them to navigate between ordinary and boundary states to maximally maintain the dialogue coherency. In practice, we observe about 65.44% of all dialogue turns are instructed with boundary. The detailed implementation is shown in Table 9.\nTopic Manager The topic manager plays a crucial role in maintaining the relevance of the conversational content to character attributes throughout the dialogue. Taking the identical inputs as the dialogue planner, it dynamically adjusts the topics at the beginning of each turn, and decide whether to deepen or expand the topic as the dialogue progresses. It is also prompted to ensure that the generated dialogues are diverse, referring to as many topics in the character's attributes as possible. More details are shown in Table 10.\nQuery Generator The query generator takes character attributes and dialogue context as inputs and simultaneously accepts the results derived from the dialogue planner and the topic manager. It first selects a snippet from the character attributes according to current topic and tempers it if necessary according to the instructional output of the dialogue planner. Then, the tempered snippet acts as a prerequisite, compelling the query generator to produce a query that conceals a piece of counterfac-"}, {"title": "2.2 Quality Control", "content": "To ensure the dialogues generated by ERABAL meet our goal, we recruited 3 annotators with graduate degrees for $20 an hour to assess their quality based on the questions listed in Table 2. The four questions range from simple to complex, and an ideal dialogue should meet all of them simultaneously. According to the statistics, about 26% of the dialogues fail to address the most complex questions. Indeed, this is already not bad, as it is challenging to compel LLMs to formulate boundary queries and embed counterfactual traps within them. Besides, the failed dialogues do not hinder the training of RPLAs, as they simply revert to ordinary role-specific dialogues."}, {"title": "2.3 Data Statistics", "content": "We set up 190 roles including 95 English roles and 95 Chinese roles, which cover fictional characters, real-life figures, and custom characters. The statistics of all training and evaluation datasets are shown in Table 3. For the boundary scenario evaluation (will be detailed in the next section), we keep the roles in the training set identical to those in RoleBench for a fair comparison. For comprehensive evaluations, we treat entire dataset (including training and test set) as training data, as there are already well-established benchmarks. We manually ensure that there is no role leakage between our training data and these benchmarks. In summary, our data scale is much smaller than those used in previous research, with only 4.8% of the roles compared to (Lu et al., 2024) and 10% of the dialogues compared to (Wang et al., 2023c)."}, {"title": "3 Evaluation", "content": ""}, {"title": "3.1 Boundary Scenario Evaluation", "content": "To examine if boundary-aware training can mitigate OOC issues, we adopt the role consistency metric proposed in DITTO for evaluation. Note that knowledge accuracy evaluation is unsuitable in this scenario, as the queries and responses are too ordinary to reflect characters' boundaries. Moreover, the straightforward presentation of role-specific knowledge makes it difficult to align the evaluation with the characters' self-awareness. We assess a response as a multiple-choice problem: it is compared against four character options-one ground truth and three randomly sampled from the character set. GPT-4, serving as a judge, is tasked with identifying the option that best matches the"}, {"title": "3.2 Comprehensive Evaluations", "content": "We further examine if boundary-aware learning can enhance the general role-playing capabilities. Here we adopt WikiRoleEval, CharacterEval, and the role-playing subset of MT-Bench for evaluation."}, {"title": "3.2.1 WikiRoleEval", "content": "WikiRoleEval is an evaluation benchmark that focuses on three dimensions of role-playing capabilities: role consistency, knowledge accuracy, and unknown question rejection. The first two dimensions respectively assess the RPLA's ability on human likeness and knowledge, whereas the last dimension focuses on evaluating the RPLA's accuracy in rejecting out-of-character questions. Detailed descriptions can be found in Appendix B.2."}, {"title": "3.2.2 CharacterEval", "content": "CharacterEval includes a set of 12 metrics designed to comprehensively assess RPLAs, which are divided into three categories: role consistency, conversational ability, and role-playing attractiveness. It emphasizes more on evaluating the immersive conversational experience, rather than addressing other issues. Detailed descriptions can also be found in Appendix B.3."}, {"title": "3.2.3 MT-Bench", "content": "MT-Bench has well-designed questions spanning eight categories, including a subset of role-playing. We use GPT-4 to evaluate the RPLA's responses through 2-turn evaluation dialogues."}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Experimental Setups", "content": "For the boundary scenario evaluation, we use Baichuan2, Qwen (Bai et al., 2023), Mistral, and LLaMA2 (Touvron et al., 2023) as base models. We assess four variants of these models using the following configurations: (1) without fine-tuning (i.e., the generalist chat model), (2) fine-tuning with RoleBench, (3) fine-tuning with contrastive queries sampled following DITTO's strategy, and (4) fine-tuning with ERABAL's 100 role training dataset.\nFor comprehensive evaluations, we select Baichuan2 and Mistral as base models and conduct supervised fine-tuning followed by BPO post-training based on ERABAL's entire dataset."}, {"title": "4.2 Implementation Details", "content": "We adopt GPT-4-0314 for both data generation and evaluation in this work. During the training stage, all models' context lengths are set to 2048 tokens. The training batch size for all models is set to 4, and the accumulation step is set to 1. Other training hyper-parameters are listed in Table 6. All experiments are conducted on 8\u00d780GB Nvidia A100 GPUs, taking an average of 4 hours for training and less than 1 hour for evaluation."}, {"title": "4.3 Baselines", "content": "For comprehensive evaluations, we compare ERABAL with state-of-the-art RPLAs, including both generalist and dedicated baselines.\n(1) Generalist Baselines. OpenChat-3.5 (Wang et al., 2023a), GPT-3.5-Turbo (OpenAI, 2022), and GPT-4 (OpenAI, 2023) represent state-of-the-art performance in general-purpose conversational models. Claude-2.1 (Anthropic, 2023) and Wenxin-43 emerge as formidable competitors, excelling in the English and Chinese languages, respectively.\n(2) Dedicated Baselines. Character-GLM (Zhou et al., 2023), Character-LLM (Park et al., 2023) are two open-source dedicated models optimized for role-playing. Xingchen is a close-source role-playing platform capable of creating any character with a given persona. DITTO is a closed-source model trained through the self-alignment method, based on the Qwen w/o role-play model."}, {"title": "4.4 Experiment Results", "content": ""}, {"title": "4.4.1 Boundary Scenario Evaluation Results", "content": "Figure 3 shows the results of the baselines and variants of ERABAL. The four generalist chat models, without exception, perform sub-optimally in boundary scenarios and are still far from achieving satisfactory results in terms of role consistency performance. This indicates that LLMs without role-playing enhancement struggle to maintain their persona across multi-turn dialogues, experiencing an out-of-character (OOC) issue. Besides, we do not observe a clear advantage of RPLAs trained with the general role-playing dataset (i.e., RoleBench) over the baselines. It suggests that acquiring concise boundary awareness may significantly differ from learning general role-playing abilities. While the characters' linguistic styles or behavior patterns can be easily mimicked, it remains challenging for RPLAs to fully grasp the complete picture of the characters.\nWe also conduct experiments with the inter-role contrastive query sampling strategy based on ERABAL'S 100 roles training set. RPLAs trained with such data demonstrate significant improvements. However, due to the coarse-grained nature of the sampling strategy, they still perform unsatisfactorily. Ultimately, RPLAs trained with ERABAL effectively improve the accuracy score by an average absolute of 20.4 points, culminating in superior performance across all baselines."}, {"title": "4.4.2 Comprehensive Evaluation Results", "content": "WikiRoleEval Table 4 shows the evaluation results on WikiRoleEval. We observe that different categories of models exhibit significant variations in performance across the three dimensions. Generalist baseline models, including OpenChat-3.5, Claude-2.1, Wenxin-4, GPT-3.5-Turbo, and GPT-4 series, perform better on knowledge accuracy and unknown question rejection tasks compared to role consistency. Knowledge accuracy is a task highly related to the model size, while unknown question rejection is relatively coarse-grained, which may be easy for LLMs to handle. Role consistency, which implicitly involves fine-grained boundary-aware assessments of an RPLA, presents a greater challenge to generalist models."}, {"title": "5 Further Analysis", "content": ""}, {"title": "5.1 Effect of Model Scale", "content": "We further study the performance variation with ERABAL on models of different scales. Experiments are conducted on LLaMA-7B, LLaMA-13B, and LLAMA-33B. Figure 4 shows the experimental results, from which we observe that the performance on the four metrics improves as the model size increases. However, for Cons. and Rej., the improvements tend to plateau when the model size ex-"}, {"title": "5.2 Effect of Data Scale", "content": "We also conduct experiments based on LLaMA2-7B with 1/8, 1/4, 1/2, and full-sized datasets to study the impact of data scale. As depicted in Figure 5, as the data volume increased, almost all metrics showed rapid improvement. We believe that constructing more boundary-aware dialogues through ERABAL will yield even better results. However, due to resource limitations, we limit our dataset to a maximum of approximately 16k dialogue data in this experiment. The performance on the Know. metric shows more significant improvement compared to the other three metrics, which is consistent with our previous analysis."}, {"title": "6 Related Work", "content": ""}, {"title": "Role-playing Agents", "content": "The concept of role-playing agents (RPLAs) originates from the demand for personalized dialogue (Salemi et al., 2023a; Chen et al., 2024). Shanahan et al. (2023) firstly introduces large models into this field. Wang et al. (2023c) and Li et al. (2023a) construct early role-playing benchmarks through data synthesis and curation. Some researches Tu et al. (2023); Zhou et al. (2023) build large-scale role-playing datasets through manual annotation. Mao et al. (2023); Wang et al. (2023b) perform knowledge editing (Sinitsin et al., 2020) for role-playing based on LLMs, forcing the models to align with a certain character. Salemi et al. (2023b) combines role-playing models with retrieval-augmented information to enhance the model's role-playing abil-"}, {"title": "Role-playing Evaluation", "content": "Numerous studies focus on evaluating the role-playing capabilities of existing RPLAs. Some studies from a psychological perspective explore the character representations of LLMs as anthropomorphic systems (Jiang et al., 2023b; Pan and Zeng, 2023; Wang et al., 2023b). Besides, numerous studies build on previous work in personalized or emotional dialogues, investigating the factual and stylistic consistency of models in role-playing scenarios (Wang et al., 2023c; Zhou et al., 2023; Shen et al., 2023). Some studies also recognize the need to evaluate role-playing tasks from multiple dimensions (Shao et al., 2023; Tu et al., 2024; Wang et al., 2023b). In contrast, we adopt a different perspective by evaluating the role consistency of RPLAs in boundary scenarios. We believe that this is crucial for alleviating the OOC issue of RPLAs and making them more practical for real-world applications."}, {"title": "7 Conclusion", "content": "In this work, we propose ERABAL, a boundary-aware learning framework aimed to enhance the RPLAs' role-playing capabilities. Through ERABAL, we collect an effective role-playing dataset and train RPLAs based on popular generalist mod-"}, {"title": "Limitations", "content": "In the field of role-playing based on large models, our research has made significant progress in addressing the challenges of boundary scenarios and enhancing general role-playing capabilities. However, we must also acknowledge the factors that have limited our research."}, {"title": "Data Diversity and Complexity", "content": "Although our dataset is extensive, it is primarily derived from English and Chinese roles, which may limit the generalizability of our findings to other linguistic and cultural contexts. While our innovative evaluation method is aggressive, it may not account for all the subtleties of human communication and could potentially introduce a degree of subjectivity in the evaluation process."}, {"title": "Training Data", "content": "Our training dataset is designed to enhance the RPLAs' general role-playing capabilities. Although we have demonstrated the performance of our method in comprehensive evaluation scenarios using WikiRoleEval, CharacterEval, and MT-Bench, the reliance on boundary data during training may lead to over-fitting in some scenarios we have overlooked, potentially reducing the model's adaptability to a wider range of user interactions."}, {"title": "Ethics Statement", "content": "The inclusion of villainous characters in role-playing systems, while enriching the narrative depth, introduces safety concerns (Deshpande et al., 2023; Dong et al., 2024; Wan et al., 2023). These dialogues generated from scenarios involving these characters tend to exhibit bias, mean-spiritedness, or other unsafe elements. This phenomenon underscores the intricate balance between narrative richness and conversational safety.\nHowever, completely removing these characters can lead to a monotonous personality within the role-playing system, highlighting their essential role in creating dynamic and engaging role-plays (Chen et al., 2024). In fact, villainous characters contribute significantly to diverse and complex human-computer interactions.\nWhile we have implemented stringent filtering measures (selecting only publicly available fictional works or characters without moral and legal risks) to ensure the safety alignment of our role-playing systems, the risk of misuse by third parties still remains a concern. Achieving a balance between creating vivid and engaging character simulacra and ensuring they do not propagate negative thought patterns is a delicate endeavor."}, {"title": "A Details of Methods", "content": ""}, {"title": "A.1 Prompts", "content": "All prompts are listed in Tables 8 - 16. The prompt of Single-Agent can be found in Table 15."}, {"title": "B Experimental Details", "content": ""}, {"title": "B.1 RoleBench", "content": "RoleBench (Wang et al., 2023c) is a role-related dataset designed to enhance the role-specific knowledge density within synthetic instruction datasets. It involves a meticulous three-step process: segmenting role profiles, generating question-confidence-answer triplets, and filtering and post-processing data to ensure high-quality, role-specific instruction data. This dataset is particularly adept at handling the nuances of script-agnostic and script-based instructions, utilizing a confidence score to improve question quality and address potential issues such as incompleteness or hallucinations."}, {"title": "B.2 WikiRole Eval", "content": "This WikiRoleEval meticulously designs metrics to assess three critical aspects of role-playing LLMs\u2019 performance: consistent role identity, accurate role-related knowledge, and the ability to recognize and reject unknown questions.\nFor consistent role identity, WikiRoleEval suggests evaluating an LLM's ability to maintain a designated character's consistency across a multi-turn dialogue. This is achieved by structuring the assessment as a multi-choice problem with four potential role candidates, where an additional LLM judger determines the most suitable character based on the dialogue context.\nRegarding accurate role-related knowledge, the framework emphasizes the importance of LLMs conveying role-specific knowledge accurately, avoiding factual errors. WikiRoleEval addresses the challenge of factual assessment by using a dialogue-simulation scheme to capture the essential knowledge for each dialogue round, enabling a judging LLM to evaluate the appropriateness of a response in integrating consistent knowledge.\nLastly, for unknown question rejection, WikiRoleEval introduces a metric to evaluate an LLM's ability to reject questions outside a character's cognitive edge, enhancing the realism and immersion of the role-play. This involves annotating questions based on the cognitive edge of each character and employing an LLM judger to assess the model's rejection accuracy.\nThese metrics from the WikiRoleEval framework provide a comprehensive method for evaluating the sophistication and realism of role-playing LLMs, focusing on character consistency, knowledge accuracy, and cognitive awareness."}, {"title": "B.3 CharacterEval", "content": "CharacterEval (Tu et al., 2024) is a specialized assessment designed for role-playing tasks featuring up to 12 dimensions. The test dataset for CharacterEval is sourced from dialogues in novels or scripts. Table 7 briefly explains the meaning of each metric. All metrics in CharacterEval are role-based, for instance, coherence refers to whether the model, when assuming a certain role, makes statements consistent with the context, and diversity is also based on the role's diverse performance."}, {"title": "B.4 Training Implementation Details", "content": "We include the role-playing instruction in the model's system instruction, which comprises the role name, role characteristics, and format requirements. In the case of Baichuan2-Chat-13B, the format is {system instruction}<reserved_106>{input}<reserved_107>{response}</s>. Details of the role-playing system instruction format can be found in Table 8."}, {"title": "C Comparison of Examples", "content": "The comparisons of responses that face boundary queries generated by different models are shown in Table 18-19. It can be observed that for these boundary samples, the model has obvious improvement in role alignment ability by ERABA enhancement."}, {"title": "D Complete Examples", "content": "Some complete multi-turn examples are shown in Table 20-24."}]}