{"title": "AQUILAMOE: EFFICIENT TRAINING FOR MOE MODELS WITH SCALE-UP AND SCALE-OUT STRATEGIES", "authors": ["Bo-Wen Zhang", "Liangdong Wang", "Ye Yuan", "Jijie Li", "Shuhao Gu", "Mengdi Zhao", "Xinya Wu", "Guang Liu", "Chengwei Wu", "Hanyu Zhao", "Li Du", "Yiming Ju", "Quanyue Ma", "Yulong Ao", "Yingli Zhao", "Songhe Zhu", "Zhou Cao", "Dong Liang", "Yonghua Lin", "Ming Zhang", "Shunfei Wang", "Yanxin Zhou", "Min Ye", "Xuekai Chen", "Xinyang Yu", "Xiangjun Huang", "Jian Yang"], "abstract": "In recent years, with the rapid application of large language models across various fields, the scale of these models has gradually increased, and the resources required for their pre-training have grown exponentially. Training an LLM from scratch will cost a lot of computation resources, while scaling up from a smaller model is a more efficient approach and has thus attracted significant attention. In this paper, we present AquilaMoE, a cutting-edge bilingual 8*16B Mixture of Experts (MoE) language model that has 8 experts with 16 billion parameters each and is developed using an innovative training methodology called EfficientScale. This approach optimizes performance while minimizing data requirements through a two-stage process. The first stage, termed Scale-Up, initializes the larger model with weights from a pre-trained smaller model, enabling substantial knowledge transfer and continuous pretraining with significantly less data. The second stage, Scale-Out, uses a pre-trained dense model to initialize the MoE experts, further enhancing knowledge transfer and performance. Extensive validation experiments on 1.8B and 7B models compared various initialization schemes, achieving models that maintain and reduce loss during continuous pretraining. Utilizing the optimal scheme, we successfully trained a 16B model and subsequently the 8*16B AquilaMoE model, demonstrating significant improvements in performance and training efficiency.", "sections": [{"title": "1 Introduction", "content": "Language models have become a cornerstone of modern natural language processing (NLP) systems, driving applications such as machine translation, conversational agents, text summarization, and question answering [1, 2]. Recent advancements in large language models (LLMs) like GPT-3, BERT, and T5 have demonstrated remarkable proficiency across numerous tasks, highlighting the importance of pretraining on large-scale datasets to achieve state-of-the-art results [3, 4]. Despite their success, traditional dense models face significant challenges in scalability and efficiency, particularly as parameter sizes increase.\nMixture of Experts (MoE) models have emerged as a promising solution to these challenges. By dynamically selecting different subsets of model parameters (experts) for various inputs, MoE architectures can scale to a much larger number of parameters without a corresponding increase in computational cost [5]. This selective activation mechanism allows MoE models to achieve higher performance while maintaining computational efficiency. However, training such large-scale MoE models presents significant challenges, including the vast amounts of data and computational power required."}, {"title": "2 Methodology", "content": "The EfficientScale pipeline is designed to efficiently train a large-scale Mixture of Experts (MoE) model by leveraging knowledge transfer from smaller models. The process involves three main phases: Preparation, Scale-Up, and Scale-Out. Each phase plays a crucial role in ensuring effective knowledge transfer and continuous learning, resulting in a highly optimized MoE model."}, {"title": "2.1 Preparation Phase", "content": "The preparation phase involves training a small dense model and preparing the datasets required for subsequent phases. This phase ensures that the initial model has sufficient transferable knowledge and that the data is ready for effective training and validation.\n\u2022 Model Training: Train a small dense model from scratch on a substantial amount of tokens or use an already pre-trained small model. This step ensures the model has accumulated sufficient transferable knowledge to serve as a robust starting point.\n\u2022 Data Preparation: Collect, clean, and preprocess the training and validation datasets. This step involves managing large datasets to ensure they are suitable for training and validation purposes.\n\u2022 Validation Setup: Develop both training and validation datasets to monitor the model's performance during subsequent phases. Continuous tracking of the language model's loss on the validation dataset is essential to ensure the initialized models retain transferred knowledge and can learn new information effectively."}, {"title": "2.2 Scale-Up Phase", "content": "The Scale-Up phase involves two critical steps: initializing the weights of a larger dense model using the smaller model and performing continuous pretraining to ensure effective knowledge transfer and model enhancement. We use the bert2BERT[8] method to initialize the large model and propose the AKI-Pro method, improving bert2BERT-AKI from depth expansion and group query attention."}, {"title": "2.2.1 Weight Initialization Strategies", "content": "The weights of the small dense model are used to initialize a larger dense model. There are two strategies proposed in bert2BERT[8]: Function Preserving Initialization(FPI) and Advanced Knowledge Initialization(AKI). Both the original and our experiments in Section 3.2.1 show that AKI performs better. Besides, recent research[9] shows that it is better to use interpolation instead of stacking when expanding the depth, which is more stable for continuous training. Moreover, the original AKI method is not suitable for Group Query Attention (GQA), so we modify the transformation of the weights in attention blocks to fit GQA. Finally, we have AKI-Pro as our initialization method. Below we will introduce the three initialization methods, starting with a review of the first two approaches in bert2BERT, followed by our improvements.\nFunction Preserving Initialization (FPI): This strategy is firstly proposed in Net2Net[6] to expand the intermediate dim of an MLP layer. Bert2BERT[8] enhances the Net2Net method to FPI, which enables it to expand the hidden dims(i.e. input and output dims). It is applied in training language models in bert2BERT and can expand the width of a smaller model to a larger model, getting the same output with the same input. With the FPI, the larger model can get the transferred knowledge from the smaller model. The basic idea behind FPI is that when expanding the dims, it makes both the input and output tensor concatenate a copy of the smaller tensor, as illustrated in Figure 1. For an MLP layer with two linear mappings in the example: $y = U^T W x$, the input and output dims are 2, and the intermediate dim is 3. Suppose we want to expand this block to that with 3 as input and output dims, and 4 as intermediate size, then there are three steps. (1) Input Dim Expansion FPI copies the input neurons from left to right and splits the corresponding weights to the new input neurons. (2) Output Dim Expansion For the expansion of the output in the upsampling linear weights, FPI also makes the new hidden neurons copy from the original ones. (3) MLP Expansion Expand the downsampling linear weights the same as the upsampling weights, and finally, the new output neurons of this MLP layers are also the copy from the smaller ones, which makes the block can be stacked as layers. The weights $W' = FPI(W)$ are transformed as follows:\n$w'_{1,*} = w'_{3,*} = \\frac{w_{1,*}}{2}$\n$w'_{* ,4} = w'_{* ,1}$                                                                                                                  (1)\nMost modules of a transformer block can be transformed the same as an MLP layer, including embedding layers and QKV projections. For the MHA module, each attention head should be seen as a neuron, and then the head number can be expanded as before. Notably, the output of the LN modules will not be the same when the new dimension is not an integer multiple of the old one, but this will not hurt a lot on the final loss.\nAdvanced Knowledge Initialization (AKI): As shown in both Net2Net[6] and bert2BERT[8], the symmetry from the FPI will hinder the model convergence. Specifically, if we have a linear layer $y = w_1x + w_2x$, where $x, y \\in R$, and $w_1 = w_2$ when initializing the weights, the gradient and the value of these two weights will always be the same, which makes the effective number of parameters for this linear layer only 1. So AKI is proposed to break the symmetry with expanding width based on not only the weights of the same layer but also the upper layer in the smaller model. Take a model with two MLP blocks as an example:\n$y_1 = U^{(1)T}W^{(1)T}x, y_2 = U^{(2)T}W^{(2)T}y_1, x, y_1, y_2 \\in R^2$\n$W^{(1,2)} \\in R^{2 \\times 3}, U^{(1,2)} \\in R^{3 \\times 2}$                                                                                                (2)\nFPI expands $W^1$ as $FPI(W^{(1)}) = [w_1^{(1)}; w_1^{(1)}; w_2^{(1)}; w_2^{(1)}]$, while AKI uses the output expansion of next layer: $AKI (W^{(1)}) = [w_1^{(1)}; w_2^{(1)}; w_3^{(1)}; u_1^{(2)}]$. Inspired by the observation that neighboring layers have similar functions, AKI breaks the symmetry and keep the knowledge from the smaller models. Moreover, FPI can't expand the depth, so bert2BERT uses the stacking method to expand the model depth proposed by StackBERT [7].\nAKI-Pro: Our proposed improvement on AKI further refines weight initialization from two aspects: depth growing method and GQA compatibility."}, {"title": "\u2022 Depth Growing Method:", "content": "We use interpolation in the depth growth to make the continuous training more stable, following the recent research [9]. The stacking method just copies the layers of the source model to the top. For the source model with $L_1$ layers: ${W_i | l \\in [0, L_1)}$ and target model with $L_2$ layers: ${W_i^{'} | l \\in [0, L_1)}$, stacking method can be formed as $W^{'} = W(t mod L_1)$. However, the output space of the last layer does not match the input space of the first layer, which can make the continuous training unstable. Based on the observation of similar functionality in neighboring layers, recent research[9] improves this by using interpolation, which can be formulated as below:\n$W^{'} = [\\frac{l*L_2}{L_1}]$                                                                                                                                             (3)"}, {"title": "\u2022 GQA Compatibility:", "content": "The original AKI method only supports MHA in transformer models. We adapt AKI for Group Query Attention models. To be specific, under the constraint that the number of groups in the GQA of the source model and the target model are consistent, we expand the output of the attention heads inside each group. Each group can be seen as a separate MHA block with common KV projection weights, and the expansion operator is the same as MHA."}, {"title": "2.2.2 Continuous Pretraining Process", "content": "The scaled-up dense model undergoes continuous pretraining on a substantial amount of tokens. This phase ensures the successful transfer of knowledge and allows the model to acquire additional information from the data, enhancing its overall performance and capability."}, {"title": "2.3 Scale-Out Phase", "content": "The scale-out phase involves transforming the large dense model into a Mixture of Experts (MoE) model. This phase includes initializing the MoE model's weights and performing continuous pretraining to refine the model's knowledge and performance.\n\u2022 MoE Weight Initialization: Aquila-MoE is initialized using Sparse Upcycling [10, 11]. The dense model checkpoint obtained from the Aquila dense model undergoes a transformation where each MLP layer is replaced by an MoE layer. These new MoE layers are exact replicas of the original MLP layers from the dense checkpoint. The router parameters are randomly initialized following a normal distribution with a mean of 0 and a variance of 0.02.\n\u2022 Continuous Pretraining of MoE: During both training and inference, two out of eight experts are activated for each token, resulting in approximately 30B activated parameters. To prevent training collapse, additional"}, {"title": "3 Experiemnts", "content": null}, {"title": "3.1 Datasets Description", "content": "We constructed a bilingual pretraining dataset of 4TB tokens in both Chinese and English. This dataset includes webpages, arXiv papers, encyclopedic data, books, codes, and QA pairs. It covers a wide range of high-quality open-source pretraining data such as RedPajama-Data-V2, falcon-refinedweb, C4, Pile, WuDaoCorporaText, ChineseWebText, etc. The above open-source data underwent language filtering to retain only Chinese and English texts, heuristic refinement to remove low-quality content, deduplication to maintain uniqueness, domain-specific filtering for relevance, data quality checks, removal of toxic and explicit content, and finally, data mixing in specified proportions."}, {"title": "3.2 Experimental Setups and Results", "content": null}, {"title": "3.2.1 Scale-up Validation", "content": "For the scale-up experiment, we used a 1.3B Aquila2 3 architecture model as the baseline. This model was scaled up to a 7B model using two different methods: FPI and AKI. Additionally, a 7B model was trained from scratch to serve as a control. All three 7B models were trained using the same hyperparameters and on the same dataset for a specified number of steps. We use M(24, 2048) to denote the 1.3B model with 24 layers and 2048 hidden dimensions and use M(32, 4096) to denote the 7B model. We first calculated the validation loss of models with different initializations. The results are shown in Table 1. We check the loss of an intermediate model M(24, 4096) without doing depth growth. We got exactly the same loss as the original model using FPI. Moreover, we found that with interpolation, both FPI and AKI have lower initial losses.\nThe loss convergence for the training process is shown in Figure 3. The experimental results indicate that the 7B models initialized using the FPI and AKI methods exhibited significantly lower loss values compared to the 7B model trained from scratch. Furthermore, these models converged at a notably faster rate. Consistent with findings in the paper [8], our results also demonstrate that the AKI method surpasses FPI in performance after a certain number of steps."}, {"title": "3.2.2 Scale-out Validation", "content": "For the scale-out validation experiment, we trained a 1.8B model from scratch with a training data volume of 3.6T tokens. These models were then scaled out to 8*1.8B configurations, followed by continuous pretraining with an additional 400B tokens. The respective model configurations and training hyperparameters are detailed in Table 3. We analyzed the loss convergence on the training set with the results depicted in Figure 4.\nBased on the results of the aforementioned validation experiments, we verified the effectiveness of both scale-up and scale-out approaches on smaller-sized models. Specifically, we trained a model from scratch with a size of 7B, and"}, {"title": "4 Model Evaluation", "content": null}, {"title": "4.1 Evaluation of Foundation Models", "content": "Following OpenCompass\u2074, in the evaluation process, we use two types of evaluation methods: discriminant analysis evaluation and generative evaluation. Discriminant analysis evaluation means combining the question with candidate answers, calculating the perplexity of all combinations, and selecting the answer with the lowest perplexity as the model's final output. Generative evaluation uses the question as the model's original input and leaves the answer area blank for the model to complete subsequently.\nThe performance of AquilaDense-7B, AquilaDense-16B, and AquilaMoE(8*16B) models are presented in Table 4. The indicators ending in \"ppl\" represent discriminant analysis evaluation, while those ending in \"gen\" represent generative evaluation.\nGenerally, as the model size increases, the scores tend to improve. For instance, AquilaDense-7B scores 7.81 on GSM8K-gen, while AquilaDense-16B scores 28.51. A similar trend also is observed in most other tasks. The AquilaMoE models show improved performance in most tasks over AquilaDense-16B. For example, in the ARC-c-ppl task, AquilaMoE scored 43.05 compared to 38.31 for AquilaDense-16B. These findings highlight the benefits of both scaling up model parameters and implementing MoE architectures in improving model performance."}, {"title": "4.2 Evaluation of Fine-tuned Models", "content": "Table 5 presents the overall results of AquilaMoE-8*16B after fine-tuning across various benchmark datasets. The performance is measured using generative evaluation, and the results are expressed as percentages."}, {"title": "4.3 Comparsion of Computational Efficiency", "content": "We present the details of the training process for both scale-up + scale-out and from-scratch approaches in Table 6. The table lists the number of devices in the cluster, the GFLOPS per device, the model parameters size, the number of trained tokens, the actual number of training tokens per day, the actual training time, and the actual training GFLOPS for each phase.\nThe time savings factor is calculated by comparing the total training time of the from-scratch approach to the total training time of the scale-up and scale-out approach. The formula is:\n$Time Savings Factor = \\frac{\\sum_{i=1}^n N_{tokens, i}}{\\frac{N_{tokens}}{\\frac{R_{tokens}}{day, from scratch}}}}{\\sum_{i=1}^n \\frac{N_{tokens, i}}{\\frac{R_{tokens}}{day, i}}}$\nGiven the data:\n$Time Savings Factor = \\frac{\\frac{3600+1200+545}{25}}{\\frac{3600}{279} + \\frac{1200}{70} + \\frac{545}{25}} = \\frac{213.80}{51.84} \\approx 4.12$\nThe computational power savings factor is calculated by comparing the total GFLOPS-days of the from-scratch approach to the total GFLOPS-days of the scale-up and scale-out approach. The formula is:\n$Computational Power Savings Factor = \\frac{\\sum_{i=1}^n \\frac{N_{tokens, i} \\times GFLOPS_{from scratch}}{\\frac{R_{tokens}}{day, from scratch}}}{\\sum_{i=1}^n \\frac{N_{tokens, i} \\times GFLOPS_{i}}{\\frac{R_{tokens}}{day, i}}}$"}, {"title": "5 Conclusion and Future Work", "content": "We present AquilaMoE, a bilingual 8*16B mixture of experts (MoE) language model developed using the EfficientScale training method. EfficientScale optimizes performance while significantly reducing data requirements through a two-stage approach: Scale-Up and Scale-Out. Our contributions are as follows: 1) An effective training methodology that achieves knowledge transfer and continuous pretraining with significantly reduced data and computational needs; 2) Innovative initialization strategies, such as Functional Progressive Initialization (FPI) and Approximate Knowledge Integration (AKI), which demonstrate substantial loss retention and reduction during continual pre-training; 3) Successful training of 16B and 8*16B AquilaMoE models using these initialization strategies, enhancing performance and training efficiency. Future work involves exploring the scalability of larger MoE models, investigating cross-linguistic knowledge transfer, developing new optimization techniques to further reduce training time and costs, fine-tuning for specific application domains, and ensuring the robustness and generalization of MoE models across diverse datasets and real-world applications."}]}