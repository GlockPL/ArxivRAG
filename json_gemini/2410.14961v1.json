{"title": "LangGFM: A Large Language Model Alone Can be a Powerful Graph Foundation Model", "authors": ["Tianqianjin Lin", "Pengwei Yan", "Kaisong Song", "Zhuoren Jiang", "Yangyang Kang", "Jun Lin", "Weikang Yuan", "Junjie Cao", "Changlong Sun", "Xiaozhong Liu"], "abstract": "Graph foundation models (GFMs) have recently gained significant attention. However, the unique data processing and evaluation setups employed by different studies hinder a deeper understanding of their progress. Additionally, current research tends to focus on specific subsets of graph learning tasks, such as structural tasks, node-level tasks, or classification tasks. As a result, they often incorporate specialized modules tailored to particular task types, losing their applicability to other graph learning tasks and contradicting the original intent of foundation models to be universal. Therefore, to enhance consistency, coverage, and diversity across domains, tasks, and research interests within the graph learning community in the evaluation of GFMs, we propose GFMBench-a systematic and comprehensive benchmark comprising 26 datasets. Moreover, we introduce LangGFM, a novel GFM that relies entirely on large language models. By revisiting and exploring the effective graph textualization principles, as well as repurposing successful techniques from graph augmentation and graph self-supervised learning within the language space, LangGFM achieves performance on par with or exceeding the state of the art across GFMBench, which can offer us new perspectives, experiences, and baselines to drive forward the evolution of GFMs.", "sections": [{"title": "1 INTRODUCTION", "content": "Foundation models are the dominant paradigm in artificial intelligence for the 2020s (Bommasani et al., 2022). They are characterized by their ability to be effectively trained on massive and diverse datasets in a unified manner (Moor et al., 2023; Zhou et al., 2023) and are expected to exhibit superior performance in various downstream tasks (U.S. Congress, 2023). Large language models (LLMs) exemplify this paradigm, achieving outstanding performance across nearly all text-related tasks. Their success is largely attributed to the unification of both input and output processes, leveraging a closed-set vocabulary and tokenizer to transform diverse textual data into a common embedding space, and reframing various tasks-such as classification, extraction, and summarization into text-to-text generation. This enables LLMs to seamlessly handle diverse tasks across different domains concurrently.\nIn contrast, the development of foundation models in the graph domain has been notably slower. Currently, there is no counterpart to the closed-set 'vocabulary and tokenizer' for graph inputs, nor is there a 'text-to-text' framework for unifying graph tasks (Chen et al., 2024d; Liu et al., 2024; Mao et al., 2024). One key challenge on the input side stems from the intrinsic heterogeneity of nodes and edges in graph data. Unlike text, which can be represented with a finite vocabulary, graphs from different domains exhibit varying types of nodes and edges, along with distinct attribute dimensions for these nodes and edges. As a result, current graph models require the prior specification of these types and dimensions for specific datasets, making it difficult to transfer models across different datasets at the code level. On the output side, the challenge lies in the varying scales of graph tasks, which are commonly classified into node-level, edge-level, and graph-level tasks (Hu et al., 2020b). Each of these tasks requires different prediction pipelines. For instance, pooling layers are essential for graph-level tasks but are unnecessary for node-level predictions. Therefore, when current graph models are applied across tasks of different levels, they often need to learn task-oriented modules from scratch.\nTo drive the progress of graph foundational models (GFMs), several recent efforts have emerged. On the input side, these methods capitalize on natural language descriptions of node and edge types, as well as features, which are subsequently processed by language models to generate fixed-size vector representations (Liu et al., 2024). This approach avoids the need to pre-define node and edge types and dimensions, enhancing flexibility across datasets. On the output side, multiple technical directions are adopted. One option uses graph neural networks to encode the graph, followed by graph prompting techniques to facilitate cross-task learning (Liu et al., 2024). Another approach, drawing inspiration from advancements in multimodal LLMs, trains adapters to map graph vectors into specialized tokens. These tokens are then fed into LLMs, enabling predictions via in-context learning (Chen et al., 2024d).\nHowever, two significant concerns remain unaddressed and may hinder the development of GFMs. Firstly, our understanding of how current works actually make progress has been far limited by the different data processing and evaluation settings adopted by them. For instance, we can consider the cases of OFA (Liu et al., 2024) and LLaGA (Chen et al., 2024d). As illustrated in Figure 1a, both OFA and LLaGA utilize the Cora and PubMed datasets for the"}, {"title": "2 RELATED WORKS", "content": "GFM seeks to construct graph models that effectively harness the training from extensive and diverse datasets, thereby demonstrating superior applicability and performance across a wide range of tasks and domains (Liu et al., 2023b; Mao et al., 2024).\nEarlier efforts focused on leveraging self-supervised learning (Hou et al., 2022; Hu et al., 2020a; Qiu et al., 2020; Zhu et al., 2021) to pre-train GNNs on large graph data, then finetune the GNNs or design graph prompting techniques for downstram tasks (Sun et al., 2023). However, due to the inherent limitation of the GNN architecture, the applicability of these methods remains confined to the same domain, and these methods require training of new parameters for different downstream tasks.\nRecently, the success of LLMs and multimodal LLMs has advanced the development of GFMs.\nFirst, LLMs can unify the input inconsistency across different graphs. Liu et al. (2024) describes the types and attributes of nodes and edges using natural text, then uses LLMs to encode these texts into fixed-size semantic vectors in a shared space, which are treated as new features for the nodes and edges. In this way, graphs from various domains can share the same GNN encoder. However, they are still limited to the same type of tasks since the GNN backbone is retained as the predictor in the overall framework. For instance, OFA (Liu et al., 2024) and ZeroG (Li et al., 2024a) are restricted to classification tasks.\nSecond, similar to multimodal LLMs, LLMs can serve as a unified predictor. The representations of nodes, edges, or graphs can be projected into the embedding space of the LLM, allowing the LLM to predict various tasks through prompting directly (Chen et al., 2024d; Tang et al., 2024a). However, these approaches can suffer from loss of graph structural information, as the LLM can not see the original graph structure for prediction. As a result, these works mainly focus on tasks like node classification, node description, and link prediction, which do not heavily depend on graph structure.\nAt the same time, some studies (Guo et al., 2023; Wang et al., 2024a) from the NLP community have explored the graph understanding and reasoning capabilities of LLMs. However, these works are limited to evaluations and prompt engineering. Inspired by the instruction tuning, recent works have further enhanced the graph reasoning ability of LLMs through instruction tuning (Chen et al., 2024a; Wang et al., 2024b). However, these works focus solely on improving performance through LLM-finetuning techniques such as direct preference optimization, while overlooking knowledge and experience from the graph learning community, such as graph data augmentation and graph self-supervised learning. Additionally, the tasks these studies investigate differ markedly from those traditionally prioritized by graph mining works. For example, they do not tackle graph-level tasks like molecular property prediction.\nIn summary, the current GFM has only been developed to generalize in a limited set of domains and tasks. Following Mao et al. (2024), we refer to these related works as primitive GFM.\nOne potential factor affecting the development of GFM is that the current benchmarks, which are known as the foundation for model development, lack systematic design and do not align with the expectations for GFM.\n(1) The isolation between graph structure learning tasks and real-world applications. Models that excel in understanding graph structures but cannot solve real-world problems are of limited value, and models that perform well in real-world tasks but fail to understand basic graph structures lack trustworthiness. However, most existing benchmarks focus only on one of these aspects. For instance, NLGraph (Wang et al., 2024a) only examines basic graph structure tasks like the shortest path problem. Chen et al. (2024b) only investigate tasks like academic paper classification. The negative impact of this phenomenon is that subsequent research has become fragmented, preventing a collective effort to create GFM.\n(2) Insufficient coverage in real-world graphs and applications. In terms of graphs, for instance, GLBench (Li et al., 2024b) and TS-GFM (Chen et al., 2024c) focus on text-rich graphs like citation networks, but overlooking text-irrelevant graphs, e.g., brain networks. Additionally, while graphs with dynamic (Zhang et al., 2024) and heterogeneous (Tang et al., 2024b) properties are common and are worthy to be well-evaluated, they are excluded by most works. From the perspective of applications, for example, OFA and TS-GFM only focus on classification tasks, ignoring important regression tasks like molecular free energy prediction. Moreover, few works investigate graph-related open-ended tasks, e.g., graph description. To establish an ideal GFM benchmark to verify and advance the versatility of GFM, it is essential to consider various graph characteristics and applications."}, {"title": "3 THE PROPOSED BENCHMARK", "content": "Our benchmark provides a comprehensive evaluation of graph understanding by encompassing a systematical design of a diverse array of tasks, which can be divided into structural understanding and semantic learning tasks. In terms of structural tasks, the capability of LLMs to understand the topological structures of synthetic graphs in different scales is evaluated. We incorporate entity-based, path-based, and structure-based challenges, organized according to task complexity. For semantic learning tasks, we aim to testify how GFMs perform on reasoning with various semantic-featured graphs. Semantic-featured graphs refer to graphs that have realistic meanings for entities or relations within the graph. Specifically, we consider factors such as graph domain, data heterogeneity, and data dynamism in terms of graph diversity, and construct varying levels and types of graph tasks that adapt to the new GFMs trend."}, {"title": "3.1 Graph Structure Understanding Tasks", "content": "Graphs are structures representing relations among entities, where nodes represent entities and edges represent their connections or interactions. A key prerequisite for performing graph machine learning tasks, particularly those GFMs involving LLMs, is a clear understanding of the structure-related concepts in graphs. Unlike graph neural networks (GNNs) like GCN (Kipf and Welling, 2016a), GAT (Veli\u010dkovi\u0107 et al., 2017), and GIN (Xu et al., 2019) which are targeted to operate on the unique non-Euclidean data structure of graphs, language models are born to address sequence data, especially text. Thus, whether the LLMs can correctly understand the basic concepts and structures in graph data needs to be testified. Inspired by Wang et al. (2024a) and GraphWiz (Chen et al., 2024a), we first include graph structure understanding tasks in terms of different difficulty levels and provide benchmark datasets in various sizes to fully check the understanding capability of GFMs for graphs. These tasks include Graph Size (for both node and edge sizes), Attribute Retrieval (for both that of nodes and edges), Degree Counting, Shortest Path, Maximum Triangle Sum, Hamilton Path, Subgraph Matching, Graph Structure, and Graph Automorphsim. The summary of graph structure understanding tasks is listed in Table 1 and detailed information is included in supplementary. These graph structure understanding tasks are organized according to entity-based, path-based, and graph-based, and the difficulties of tasks are gradually built up. Worth mentioning, we also include graph isomorphism in our benchmark, which is a fundamental problem in graph theory that involves determining whether two graphs are structurally identical. It serves as a basis for understanding more complex graph-related problems and many real-world applications (Grohe and Schweitzer, 2020), such as pattern recognition, network security, and bioinformatics, require solving graph isomorphism problems. Besides, the graph isomorphism problem has been used to testify the expression power of graph neural networks (Li and Leskovec, 2022b; Xu et al., 2018).\nFor the datasets of tasks mentioned above, we utilize a programming aid approach (Chen et al., 2024a) to create random synthetic graphs tailored for each specific task. Every task is linked to a distinctive template designed to reflect the unique properties of graphs, such as whether they are directed or undirected and whether their edges are weighted. To generate the random graphs, we employ the Erd\u0151s-R\u00e9nyi (ER) model, which requires two parameters: the number of nodes n and the probability p that an edge exists between any pair of nodes. For each node pair, the generator randomly determines whether to form an edge based on probability p, leading to a graph with an average edge density of p. We used the NetworkX library to create the random graphs and to solve the graph-related tasks."}, {"title": "3.2 Graph Semantic Learning Tasks", "content": "Different from structure learning, graph semantic learning tasks are constructed with graphs being connected with specific real-world scenarios, with nodes representing real entities and edges showing specific relations. As a fundamental form of data organization, graphs appearing in social networks, molecule graphs, and citation networks are ubiquitous and valuable in the real world (Wu et al., 2020). Considering the various types of real-world graphs and task complexity, we propose a comprehensive graph semantic learning benchmark in terms of graph domain, graph heterogeneity, graph text-richness, and task level.\nFirstly, Node Classification, Link Prediction, and Graph Classification, which are three dominant graph learning tasks in traditional graph machine learning (Wu et al., 2020), are included. Furthermore, as foundation models are required to couple with tasks with different tasks and adapt to different types of labels (Moor et al., 2023), Graph Regression and Open-Ended Graph Understanding are also included in our benchmark. Inspired by the foundation models like GPT for NLP tasks, especially the generative mode that provides a flexible application interface, we believe that the LLM-based graph foundation models could also have the ability to solve important graph open-ended problems like Graph Q&A. In terms of the graph domain, we include a wide spectrum of nine scenarios from academic, social media, knowledge graph, biology, chemistry, and so on. What's more, the different graph heterogeneity and text-richness are also considered and included to comprehensively check the graph semantic understanding capability of graph foundation models. All graphs within the datasets are aligned with describing in the natural text according to the background and definition of the graphs, taking both node attributes and edge attributes into account if available."}, {"title": "3.3 Evalution Types and Pipelines", "content": "To sum up, GFMBench includes 26 datasets, in which 19 classification tasks, 3 regression tasks, and 4 generation tasks. For each dataset, we take a random split with train: valid: test as 500: 100: 200, based on the original dataset split. For classification and regression tasks, we take Accuray and RMSE as metrics to evaluate the performances, and for generation tasks, ROUGE-L is deployed to evaluate the consistency of generated content with ground truth."}, {"title": "4 METHODOLOGY", "content": "Here, we present LangGFM, a GFM fully grounded in the capabilities of LLMs. We begin by outlining the conventional paradigms of graph machine learning, identifying their inherent limitations, and then introducing the concept of GFM alongside the associated challenges. Subsequently, we delve into the foundational ideas that underpin LangGFM. We critically examine and articulate effective principles for textualizing graph learning. Moreover, we propose innovative strategies inspired by successful experience in graph augmentation and graph self-supervised learning, aiming to enhance the graph comprehension and reasoning capabilities of LangGFM."}, {"title": "4.1 Preliminaries", "content": "4.1.1 Classical Graph Machine Learning and Limitations. A graph is a data structure used to describe relationships (edges) between objects (nodes). Examples include social networks, citation networks, knowledge graphs, and molecular structures. Graph machine learning focuses on predicting properties associated with nodes, edges, and entire graphs. In node-level and edge-level tasks, the input"}, {"title": "4.1.2 Graph Foundation Model and Challenge", "content": "In contrast to classical graph machine learning, GFM is conceptualized as a comprehensive and unified model that can effectively handle a wide spectrum of graph machine learning problems at the same time.\nFormally, denoting the set of all the graph machine learning problems as $\\Pi = {\\pi_i | i \\in {1,2,..., m}}$, the ideal GFM can be expressed as a single function:\n$f_{GFM}: \\bigcup_{\\pi_i \\in \\Pi} G_i\\rightarrow \\bigcup_{\\pi_i \\in \\Pi} Y_i, f_{GFM}(G) = Y$"}, {"title": "4.2 Overarching Philosophy of LangGFM", "content": "As the saying goes, \"The limits of my language mean the limits of my world\" (Wittgenstein, 2017), it is a potential solution to describe $\\bigcup_{\\pi_i \\in \\Pi} G_i$ and $\\bigcup_{\\pi_i \\in \\Pi} Y_i$ in natural language (Guo et al., 2023). By employing a language-based approach to encode graphs and labels, we can facilitate interoperability among heterogeneous datasets and learning tasks."}, {"title": "4.2.1 Language as the Standardized Spaces", "content": "Before we elaborate on the details of representing graphs with natural language, we first articulate the high-level idea behind it. Formally, this philosophy can be expressed as:\n$I = {I} \\\\ I = {Lang (G, \\pi_i), G\\in \\bigcup_{G_i}}$\n$\\pi_i \\in \\Pi$\n$O = {O} \\\\ O = {Lang_Y (Y, \\pi_i), Y \\in \\bigcup_{V_i}}$\n$\\pi_i \\in \\Pi$\nwhere $Lang (\\cdot)$ and $Lang_Y (\\cdot)$ are supposed to be methods for transforming an input graph G and its corresponding label Y into natural language in the context of a specific learning problem $\\pi_i$, respectively. Hence, the function $f_{GFM}$ can be reformulated as a text-to-text model:\n$f_{GFM} : I \\rightarrow O, f_{GFM}(I) = O$\nIt's widely recognized that LLMs are optimal for text-to-text tasks. Thus, we assume $f_{GFM}$ is implemented by an LLM, now we briefly overview the generation and training mechanisms."}, {"title": "4.2.2 Generation Process of LLMs", "content": "LLMs employ an autoregressive approach for text generation. Given an input sequence $X = (X_1, X_2, ..., X_{t-1})$, the model predicts the next token $x_t$ as follows:\n$x_t = arg \\; max_{x' \\in V} P(x' | X_1, x_2,..., X_{t-1}; \\Theta)$\nHere, V denotes the vocabulary, and $P(x' | X_1, X_2, ..., X_{t-1}; \\Theta)$ is the conditional probability of x' given the preceding tokens and model parameters $\\Theta$. The token with the highest probability is selected, and this process repeats until a predetermined sequence length is met or a special end token is generated."}, {"title": "4.2.3 Loss Function of Instruction Tuning", "content": "To optimize the LLM-based $f_{GFM}$, instrution tuning is adopted. For a sample (I, O), where $I \\in I$ and $O \\in O$, we use a cross-entropy-based loss function:\n$L(I, O) = - \\sum_{t=1}^{T} \\sum_{v\\in V} O_{t,v} log(P(O_t = v| I, O_{:t-1}; f_{GFM})$\nIn this equation, $O_{t,v}$ is the target label at position t, and $P(O_t = v| I, O_{:t-1}; f_{GFM})$ represents the model's predicted probability of that label given the input and preceding output. Here, T is the length of the output sequence, and V represents the vocabulary. Minimizing L improves the model's adaptability to task instructions, enhancing performance on specific tasks. Consequently, the GFM learning problem can be expressed as:\n$L_{GFM} = - \\sum_{i=1}^{m} \\sum_{j=1}^{n_i} L(I,O)$"}, {"title": "4.3 Textualization of Graph Learning", "content": "The only problem that remains in the above is how to define and implement $Lang (\\cdot)$ and $Lang_Y (\\cdot)$ and conduct instruction tuning on LLM. We illustrate them here.\n4.3.1 A Toy Example. In summary, $Lang (\\cdot)$ involves representing G in text and using natural language to describe the task $\\pi_i$ with specification on target nodes, edges, or the entire graph, including Graph Description, Graph Text and Query. $Lang_Y (\\cdot)$ describes the value Y in natural language within the semantic context of the task $\\pi_i$."}, {"title": "4.3.2 Available Approaches for Graph Text", "content": "The most important yet challenging part in the above example is Graph Text. Drawing inspiration from traditional research in social network analysis and graph visualization, we turned our attention to graph-exchange file formats. Over the past two decades, extensive efforts have been made to develop flexible, concise, and efficient formats to facilitate graph data exchange and support scientific exploration across a wide range of applications, which has led to the creation of nearly one hundred distinct file formats (Roughan and Tuke, 2015).\nIn this work, we focus on four prominent formats selected for their widespread adoption and representational expressiveness: Graph Modelling Language (GML) (Himsolt, 1997), Graph Markup Language (GraphML) (Brandes et al., 2013), JavaScript Object Notation (JSON) and Markdown Table (Gruber, 2012).\nDespite the extensive research on graph-exchange formats, most current works (Chen et al., 2024a; Wang et al., 2024b) have overlooked these efforts, instead focusing on designing custom or intricate text-based representations for graphs. While it is plausible that LLMs have encountered standard formats during pre-training, one could hypothesize that LLMs process graph structures more effectively when presented in these familiar formats, as opposed to custom-designed ones. Unfortunately, existing studies have not thoroughly evaluated the efficacy of their proposed formats against"}, {"title": "4.4 Various Formats as Graph Augmentations", "content": "The different textual representation schemes for graphs remind us of the concept of data augmentation. Data augmentation is an effective method for enhancing machine learning model performance since it increases the quantity and diversity of samples and thus improves the model's robustness and generalization ability. In the image domain, common techniques include rotation, flipping, cropping, and color transformation; in the text domain, methods such as synonym replacement, random deletion, and back-translation are employed. Traditional graph data augmentation techniques enhance model performance through methods like edge masking, node feature perturbation, subgraph sampling, and so on.\nBeyond traditional graph data augmentation, the different textual representation approaches inspire us to directly leverage them as a novel data augmentation strategy in the language space. In fact, this aligns well with the core idea of data augmentation-maximizing the variation of input features while preserving semantic equivalence. Obviously, two different format textual representations of a graph exactly describe the same thing, but they possess very distinct natural language characteristics, including sequence length, organizational logic, and so on. Moreover, in the case of in-context learning, recent research reveals that different formats can exhibit significant performance differences across various tasks (Fatemi et al., 2023; Guo et al., 2023), which indicates bias exists. Intuitively, when LLMs are required to answer the same question about different format representations of the same graph object, they can develop the ability to understand the graph object itself, independent of such format biases.\nOur experimental results in Section 5.4 demonstrate the strong effectiveness of this strategy."}, {"title": "4.5 Graph Self-supervised Instructions", "content": "Graph self-supervised learning has proven to be a promising paradigm for addressing critical issues in graph representation learning, such as the heavy reliance on labeled data, sparsity of data, and poor generalization. These challenges are even more significant when using LLM as the backbone architecture. Recent research highlights that the success of LLM instruction tuning is tightly dependent on factors like the volume of training data, task diversity, and annotation quality. This raises a natural question: can the principles of graph self-supervised learning be effectively transferred to settings where LLMs are the foundational architecture? To answer this, we propose two distinct types of graph self-supervised instruction data and evaluate their effectiveness in such scenarios."}, {"title": "4.5.1 Self-Supervised Instructions for Topology Autoencoder", "content": "Inspired by the work of Kipf and Welling (2016b), we develop self-supervised instructions aimed at enhancing the understanding of graph topology, a fundamental capability for graph models. The basic idea of a graph autoencoder is to encode an input graph into a latent space and reconstruct its adjacency matrix from the latent variables. We define a new learning problem, denoted by $\\pi_{TAE}$, which mandates the $f_{GFM}$ to accurately identify all direct neighbors of a query node. This task is equivalent to reconstructing the adjacency matrix, as predicting a node's first-order neighbors is both necessary and sufficient for the reconstruction. For example, we input a local social network around a user A, and the query will be \"List all users with a following relationship with the user A\" and the answer should be exactly responded with all the users that meet the criteria."}, {"title": "4.5.2 Self-Supervised Instructions for Feature Masked Autoencoder", "content": "Motivated by Hou et al. (2023), which demonstrates that reconstructing masked node features as the only pretext task in graph self-supervised learning could generate promising performance, we reformulate it within current framework.\nThe original masked graph autoencoder first uniformly samples a subset of nodes without replacement and mask their feature with learnable embedding, and then reconstructs the masked node features from the corrupted node features and the adjacency matrix. Similarly, we define a learning problem $\\pi_{FMAE}$. We replace the feature descriptions of a sampled subset of nodes in the graph text with text \"unknown\". Then, we require the $f_{GFM}$ to infer the raw feature descriptions based on the corrupted graph text. For example, while the text piece about user A in the corrupted input social graph will be \"the hobbies of the user A are unknown\" and the response should be exactly the hobbies of the user A in the raw graph.\nIn this work, we augment each input graph sample with an additional topology autoencoder sample. For input graphs containing"}, {"title": "5 EXPERIMENTS", "content": "In this section, we evaluate and analyze the performance of LangGFM as a GFM on GFMBench, as well as address the claims and designs from the methodology Section 4. Moreover, we also include a zero-shot transfer learning experiment on datasets outside of GFMBench to further validate the potential of LangGFM.\nIn particular, we will answer the following research questions:\nRQ1: How effective is LangGFM as a graph model and as a GFM?\nRQ2: Do we really need to design new textualization formats for graphs? Are the existing standard graph exchange formats effective? RQ3: In the context of learning graph tasks in language space, is data augmentation at the text level effective? RQ4: Is traditional self-supervised learning in the graph domain still effective in the language space? RQ5: How is LangGFM's zero-shot transfer capability as a GFM?"}, {"title": "5.1 Experimental Settings", "content": "LangGFM is based on Llama 3-8B-Instruct and employs the rank-stabilized Low-Rank Adapters (Kalajdzievski, 2023) technique for parameter-efficient fine-tuning and utilizes the ROPE scaling (Liu et al., 2023a) for long context understanding. We denote LangGFM-I for training on a single dataset and LangGFM-J for joint training across the entire GFMBench.\nAs for baselines, we categorize our comparisons into three groups: (1) Closed-source LLMs, including GPT-40-mini and Qwen-plus. These models represent the highest level of reasoning ability for complex tasks and can serve as robust baselines while validating the extent of graph reasoning capability in LLMs. (2) Open-source LLMs, including Llama 3-8B-Instruct and Qwen-7B-Instruct. These models are comparable in size to LangGFM, which can demonstrate the effectiveness of LangGFM. (3) Primitive GFMs, including GraphWiz (Chen et al., 2024a), GraphGPT (Tang et al., 2024a), ZeroG (Li et al., 2024a), OFA (Liu et al., 2024), and LLaGA (Chen et al., 2024d). These models are currently the closest to GFM and can handle a certain subset of different types of tasks or datasets with a single model. OFA is the only work capable of handling tasks at the node, edge, and graph levels simultaneously, but it is limited to classification tasks; GraphWiz is representative of various graph structure tasks; LLaGA and GraphGPT integrate graph tokenization within LLMs and primarily tackle node and edge tasks; ZeroG focuses on the zero-shot transfer capability for node classification tasks. For GraphWiz, we used the officially provided model checkpoints for inference on structural tasks. For OFA and LLaGA, we trained the official code on the proposed benchmark. For the zero-shot experiments, we utilized the best results reported in the respective papers for OFA, LLaGA, GraphGPT, and ZeroG."}, {"title": "5.2 Overall Performance of LangGFM (RQ1)", "content": "As shown in Table 3, LangGFM demonstrates comparable or superior performance across nearly all tasks, whether trained independently or jointly. Notably, LangGraph excels in graph-level tasks that necessitate a deep understanding of graph structure, showcasing the effectiveness of learning graphs entirely in the language space. Analyzing the baselines can provide us more insights into LangGFM's success:\n(1) Closed-source LLMs perform well on graph algorithm-related structural tasks (e.g., Hamilton Path) and knowledge graphs (i.e., FB15K237). It's known that LLMs are extensively exposed to such data during pretraining phase and common sense reasoning or math tasks in instruction tuning phase. Hence, we can deduce that the LLM's inherent complexity or capability is sufficient to model graph learning problems and serve as the backbone for GFM. We should attempt to incorporate more graph-related pretraining or instruction tuning data to stimulate the graph reasoning ability of LLMs. By expanding the model's exposure to graph tasks, we can potentially unlock more sophisticated reasoning skills within the graph domain.\n(2) In our experiments, GraphWiz shows weak generalization on unseen data, underperforming in some tasks (e.g., Hamilton Path), aligning with its claimed tendency to overfit due to excessive instruction tuning and preference alignment on small datasets. This underscores the importance of diverse graph data and tasks in developing LangGFM.\n(3) In text-driven tasks like node classification or link prediction with rich text features like Ogbn-Arxiv, current quasi-GFMs seem not to have a clear advantage against LLMs. This suggests that their success in such tasks may be attributed more to how LLM encodes texts, which is often overlooked in these works since they only included traditional GNNs as baselines. LangGFM achieves the promising results while being able to handle all tasks simultaneously, which undoubtedly reinforces the initial question: Is there a genuine necessity for specialized modules (e.g., GNNs) in graph processing? Perhaps we should explore more possibilities on the path to GFM."}, {"title": "5.3 Graph Texuliazation Effectiveness (RQ2)", "content": "To assess the necessity of developing new custom graph representation formats, we compared the accuracy of using different formats on the Shortest Path and Ogbn-Arxiv tasks under the zero-shot in-context learning setup with Qwen-plus.\nWe used the formats proposed from GraphWiz and Instruct-Graph as baselines. As shown in Figure 3, existing formats consistently outperformed the custom-designed ones. InstructGraph did not surpass any established format in accuracy, and although it demonstrated higher token efficiency on graphs without node features, this advantage disappeared when node features were introduced. GraphWiz, while offering a balanced trade-off between performance and token efficiency on featureless graphs, was not applicable for tasks involving features. It's observed that the Markdown Table has a relatively optimal performance-token ratio. A possible reason is the prevalence of Markdown data in code repositories (e.g., README files), where tables are often accompanied by"}, {"title": "5.4 Augmentation in Language Space (RQ3)", "content": "As discussed in Section 4.4, employing diverse formats to represent the same graph structure appears to be a promising approach for graph augmentation within the language space. This strategy facilitates the development of genuine graph comprehension in LLMs by mitigating reliance on language-specific patterns and fostering a more robust understanding of the underlying graph semantics.\nWe conducted experiments on the Shortest Path and Ogbn-Arxiv tasks, training separately on different formats and jointly on all formats. The experimental results are shown in the figure 4a. Training on multiple formats together significantly and stably improved the task performance, essentially enhancing the model's reasoning ability in any format. Specifically, in the Shortest Path task, overall performance increased by 3.63%, with peak performance rising from 30.5 (GraphML) to 36 (GML). For the Ogbn-Arxiv task, overall performance improved by 2.75%, with the best score increasing from 60.0 (JSON/MARKDOWN) to 64 (Table).\nAdditionally, we analyzed the training loss curves for separate and joint training, as shown in Figure 4b. The joint training across all formats converges faster and better, further supporting the efficacy of using diverse formats as data augmentation for graph learning in the language space."}, {"title": "5.5 Graph Self-supervised Learnig Effect (RQ4)", "content": "Inspired by the successful experience in graph self-supervised representation learning, we designed self-supervised instructions in Section 4.5. $\\pi_{TAE}$ represents the topology autoencoder, requiring LangGFM to correctly understand the connectivity relationships"}, {"title": "5.6 Zero-shot Transfer Learning (RQ5)", "content": "One important capability of the foundation model is its ability for zero-shot in-context learning. For ease of comparison, we selected two commonly used zero-shot transfer learning datasets, Cora and PubMed, as well as the FreeSolv dataset from the field"}]}