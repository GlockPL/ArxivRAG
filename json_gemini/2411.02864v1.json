{"title": "Graph-DPEP: Decomposed Plug and Ensemble Play for Few-Shot Document Relation Extraction with Graph-of-Thoughts Reasoning", "authors": ["Tao Zhang", "Ning Yan", "Masood Mortazavi", "Hoang H. Nguyen", "Zhongfen Deng", "Philip S. Yu"], "abstract": "Large language models (LLMs) pre-trained on massive corpora have demonstrated impressive few-shot learning capability on many NLP tasks. Recasting an NLP task into a text-to-text generation task is a common practice so that generative LLMs can be prompted to resolve it. However, performing document-level relation extraction (DocRE) tasks with generative LLM models is still challenging due to the structured output format of DocRE, which complicates the conversion to plain text. Limited information available in few-shot samples and prompt instructions induce further difficulties and challenges in relation extraction for mentioned entities in a document. In this paper, we represent the structured output as a graph-style triplet rather than natural language expressions and leverage generative LLMs for the DocRE task. Our approach, the Graph-DPEP framework is grounded in the reasoning behind triplet explanation thoughts presented in natural language. In this framework, we first introduce a \"decomposed-plug\" method for performing the generation from LLMs over prompts with type-space decomposition to alleviate the burden of distinguishing all relation types. Second, we employ a verifier for calibrating the generation and identifying overlooked query entity pairs. Third, we develop \"ensemble-play\", reapplying generation on the entire type list by leveraging the reasoning thoughts embedded in a sub-graph associated with the missing query pair to address the missingness issue. Through extensive comparisons with existing prompt techniques and alternative Language Models (LLMs), our framework demonstrates superior performance on publicly available benchmarks in experiments.", "sections": [{"title": "1 Introduction", "content": "Document-level relation extraction (DocRE) extracts relations among multiple entity pairs in a document, representing a more realistic and challenging task than sentence-level extraction [1, 8]. In DocRE, an entity can have multiple mentions scattered throughout a document, and relationships between entities can appear in multiple different sentences. We illustrate a running example in Figure 7.\nAll the entities that occurred in the context are marked in bold. These relations can be identified by intra-sentence hints, like \u201cJack\nGanto was born in Norvelt\u201d, which can be found in Sent#4. But"}, {"title": "2 Related Work", "content": "Transformer-based models have proven more effective than graph-based approaches [25, 28, 29] for Document-level Relation Extrac-tion (DocRE) by leveraging long-distance token dependencies via Pre-trained Language Models (PLMs). Xiao [24] introduces interme-diate steps to enhance reasoning in relation extraction. Zhang [30] employs a U-Net structure to capture both local and global entity dependencies. Zhou [31] uses localized contextual pooling to focus on relevant tokens for each entity pair. Tan [18] implements axial attention and two-hop reasoning to better capture relational depen-dencies. Generative methods depict extraction results in a generative model vary among existing works, such as linearization on entities for named entity recognition subtasks[26], code-style of output in both named entity recognition and relation extraction in CODEIE [11] and relation triplet form for standard sentence-level RE in [21].\nSmall calibration models can improve LLM performance. Cobbe et al. [4] use a verifier to assess the correctness of multiple candidate solutions in math word problems. Welleck et al. [23] introduce a self-correction mechanism via iterative refinement. Han et al. [7] propose the PiVe framework, which leverages iterative verification to enhance graph-based generation in LLMs."}, {"title": "3 Graph-Enhanced Plug-and-Play DocRE", "content": "In this section, we first formulate the document-level relation extrac-tion (DocRE) task as a structure prediction problem. Subsequently, we describe how we recast this structure prediction from a lengthy text task into a generative task (Section 3.1). We employ a plug-and-play strategy integrated with graph-enhanced reasoning prompt to execute DocRE (Section 3.2, 3.3, 3.4) under the few-shot scenario."}, {"title": "3.1 Task Formulation", "content": "Given an input document x with I tokens x1, x2, ..., x1, a relation extraction task is to predict structured target y from x for the query entity pairs, Q = {(e1, e2)i }M i=1. M is the number of query pairs in the document. e1, e2 are two pre-provided entities spanning from x. The prediction target y of RE consists of a set of triplets (e1, r, e2), where r\u2208 R is the semantic relation (e.g., \u201chead of government\u201d) between the two entities. Here R denotes a pre-defined relation type set. In order to leverage generative LLMs for DocRE tasks, we reformulate DocRE tasks as a generation task, which models the probability for generating a consequential string y of a collection of relation triplets, conditioned over a context string C. The context string consists of a small number of k linearized samples {(xi, Yi)}k i=1, where k << N in a few-shot setting, and N is the total number of samples.\nWe wrap the input sample X and ground truth Y into a genera-tive style prompt conditioned under the relation triplet reasoning explanation E. The explanation generation aims to bridge the gap"}, {"title": "3.2 Decomposed Extraction", "content": "Despite using carefully designed prompts [12, 21], performing DocRE through LLMs without any samples presents a significant challenge. In a standard few-shot setting, it becomes essential to furnish LLMs with a small number of labeled samples to enhance results. The direct application of the prompting method from classification tasks encounters two primary challenges: (1) The complexity of gener-ating answers from LLMs is heightened by a large label space. (2) In relation extraction, the involvement of entity mention types in a relation is influenced by constraints imposed by the relation type; for instance, the relation \u201cyear of birth\u201d cannot occur between two entities of type \u201cperson.\u201d\nSince ensemble prompts aims at extracting all possible triplets from all types of relations per document (applied by [21]), LLMs face difficulties in both memorizing the relations in the list and effectively distinguishing between them. In the following example, we provide the ensemble prompt including a prompt instruction, a given document context, and a collection of relation triplets."}, {"title": "3.3 Verifier for Plugging", "content": "The decomposed prompt not only increases type generation variance but also saves model computation time by reducing instruction space and extraction complexity. Consequently, this allows for an increase in new tokens and generation diversity within the constraints of the limited processing length of LLMs. We further conduct a self-verification process on LLMs\u2019 generation. Similar to humans, the language models as discussed in [15] can achieve refinement with persuading confidence under self-investigation and self-correctness.\nWe first clean the generation by shrinking the repetition, incom-pleteness, and irrelevant entity. Repetition in triplet generation can be alleviated by hyper-parameter tuning but still exists. Incomplete-ness could be caused by the limited number of newly generation tokens of LLMs or misunderstanding of the template in the few hot samples. Neither an incomplete triplet nor an incomplete ex-planation should be discarded. Besides, we also face the irrelevant entity issue where a triplet contains a subject or object out of the entity mention list. After pruning the decomposed generation, we further filter out the outliers under each type. Specifically, incom-pleteness cases are observed by outlier detection. Moreover, outliers also include incoherent verbalization or linearization. In constructing the few-shot sample template, we operate on the assumption that generating under a single type should align, forming a cluster of this type. The outliers are those far from the centroid of the type cluster with fewer neighbors. We encode the explanation by a triv-ial pre-trained language model, e.g., BERT [5]. The outliers can be detected as the ones (Lowest K) that have the lowest neighbor density: Ioutlier = LOF(Encoder(expi)), expi \u2208 E, where LOF is the function of estimating local outlier factor [2] and implemented in Scikit-learn2.\nAnother design in Verifier is to identify the entity pair missed in the generation. Given the entity mention pair list in the dataset, Verifieri = {(e1i, e2i)}, if there is a (e1i,, e2i) \u2208 yi. Verifier stores the set of entity pairs of a document that should be associated with one or more relation types. A missing entity pair, (e1miss, e2miss) refers to a pair not appearing in the generation associated with a relation but existing in query pair list Q."}, {"title": "3.4 Graph-enhanced Plug-and-Play", "content": "Missingness detection is crucial to the proposed plug-and-play regime, including decomposed individual relation extraction, missingness verification for each document, plugging the missing pairs into a few-shot DocRE prompt, and replaying the extraction to fill in their relation slots. This section presents the plugging prompt of relation extraction for missing pairs, under association triplets, the so-called reasoning relation graph. In terms of association, we assume that extraction on a query pair (a missing pair) should mostly benefit from the triples that share an entity with the query pair or that have the same subject and object types.\nReasoning Graph Selection. We first distill the decomposed gener-ation to save prompt-token cost and computational time by selecting the association sub-graph rather than employing all generated triplets. The association sub-graph, Ga, comprises the triplets with entities that are shared with the missing pair and those whose entity types are common with the missing pair. We denote G\u00ba = {(eli, ri, e2i)}, where eli = elmiss or e2i = e2miss or {type(e1i), type(e2i)} = {type(e1miss), type(e2miss)}. type() means entity type.\nEnsemble Graph-enhanced Extraction. After obtaining Ga, we augment it as the prompt for reasoning the relation label of the missing pair. as shown in Figure 4. The ensemble prompt contains all relation labels for the extraction of missing pairs and the association triplets, Ga, regarded as the reasoning thoughts so-called graph-of-thoughts in this paper. We ask the model to fill in the MASK and Explanation based on the understanding of context and reasoning on the association graph."}, {"title": "4 Experiment", "content": "4.1 Setup\nDataset One of the most widely used benchmarks in this area is the DocRE [27], which adopts a recommend-revise annotation scheme so as to have a large-scale annotated dataset. We evaluate our Graph-DPEP method on RE-DocRE dataset[19], which originated from the DocRED dataset but contains a revision of 4,053 documents that resolved various problems including incompleteness, logical inconsistencies, and coreferential errors.\nWe have used in total 97 relation types with 96 relations from RE-DocRE and an added 'NA' type. The total number of samples is 4,053 (3,000/500/500 for the train/dev/test split). In average, RE-DocRE contains 198.4 words/doc, 19.4 entities/doc, 28.1/34.6/34.9 triplets/doc of train/dev/test splits.\nICL Data Selection. In-context learning is known to be sensitive to the choice and even ordering of the demonstration examples [13, 16]. Inspired by VOTE-K sample selection in Su et al. [17], we construct a graph by applying k-NN over the sentence-transformer-embedded document contexts of the training set to select diverse and representative prototype samples, S (e.g. 1500). Considering the two-stage prompting design in Graph-DPEP, we select n samples with the occurrence of a specific relation type during the decomposed generation, namely n |R| in total. We randomly select n samples in the pool, S, for the ensemble graph-enhanced extraction.\nFew-Shot Setting. We evaluate the impact of different number of query entity pairs in a document. Given different density of query pairs, we categorize the document into sparse (#query pairs \u2264 20), normal(20 < #query pairs \u2264 40), and dense (40 \u2264 #query pairs) groups. We sample k training samples for each relation type to construct a k-shot training set. The value of k varies but should be within a maximum length limitation. We randomly select 50 samples of each group to conduct various k-shot settings (2/3/5-shot).\nEvaluation Metrics. We adopt metrics designed in Jiang et al. [10]: (1) Micro-/Macro-F1 scores, assessing the task performance. (2) Topical Similarity (TS) score, measuring the information abundance of the extracted triples compared to the context. (3) Uniqueness Score (US), assessing the diversity of extraction and highlighting the importance of extracting varied and distinct relation types. (4) Factualness Score (FS), testing the alignment with information in the context to address hallucinations. (5) Granularity Score (GS), testing whether the extracted triplet can be further split into sub-triplets with more fine-grained types. (6) Completeness Score (CS), evaluating how comprehensively the extracted triples cover the information present in the context."}, {"title": "4.2 Results", "content": "4.2.1 Ensemble Prompt vs. Decomposed Prompt. As shown in Table 1, the comparison made in the upper two groups presents the superiority of decomposed prompt when handling intensive type space, specifically from the performances of COT and Graph-DPEP on Llama2. The worst performer with the lowest Micro-F1 scores, CODEIE, reveals the drawbacks of code-style generation when dealing with lengthy context and intensive extraction. ChatGPT can obtain decent results under the ensemble group but is still slightly worse than Graph-DPEP. Even in the most challenging scenario, i.e., the dense group, Graph-DPEP with 5-shot can obtain an 14.36% gain when comparing ChatGPT with Graph-DPEP on Llama3. The task\u2019s best performers always come from the bottom group, which indicates the success of the decomposed-plug and ensemble-play framework.\nApart from the Micro-F1 score in Table 1, TS, US, FS, GS, and CS also indicate the observations that: (1) the decomposed-plug and ensemble-play framework shows their capability to interpret the topics of the text; (2) Decomposed prompt truly increase the diversity of relation extraction since scores in this group almost outperform the others, while the ensemble stage tends to generate triplets with frequent relations which causes the hurt on diversity; (3) the hallucination is alleviated with the complete two-stage frame-work. (Since the fact checker in FS is implemented by ChatGPT, the FS of ChatGPT is the best score.); (4) high GS and CS on Graph-DPEP indicate its capability of capturing more complete information in the document context.\nIn Table 2, we evaluate the average performance under each rela-tion type by macro-precision, macro-recall, and macro-F1 scores on the entire dataset. The precision score in the decomposition group is far beyond that of COT and CODEIE, which validates the im-portance of not only type decomposition but also type injection with verbalization and linearization. Graph-of-Thoughts reasoning on missing pairs after decomposition generation further strength-ens Graph-DPEP\u2019s performance leading it to surpass all baselines, including the powerful rival, ChatGPT. Compared with ChatGPT, COT, and CODEIE, the outstanding macro-recall scores of Graph-DPEPs show their better comprehensive understanding of the con-text document with richer extracted triplets. In summary, the entire Graph-DPEP can overcome the predicted bias that appeared in the ensemble group of models witnessed by the impressive increase of the precision scores and compensate for the buried pairs excluded in the decomposition generation to bring recall gains, almost 3.10% by"}, {"title": "4.2.2 Different LLMs' Impacts", "content": "In Figure 5, we illustrate the results of the outlier rates and missing rates of Graph-DPEP with dif-ferent LLMs. The outlier rate is given by  #missingpairs  , while  #outlier  the missing rate is given by #generationtriplets\n. The results show #ground-truthpairs\nthat Llama2 suffers more severely on both outliers (incompleteness and irrelevance) and missing pairs among all k-shot settings. Another significant observation is that while the outlier rate rarely changes along with increasing shots from 2-shot to 5-shot, the missing rate can be alleviated by adding more few-shot samples. LLMs\u2019 abilities like instruction comprehension, volume and types of pre-training corpus, and generation-related hyperparameter-tuning account for the outbreak of outliers."}, {"title": "4.2.3 Graph-of-Thoughts Reasoning", "content": "Rather than employ the ensemble prompt in COT [19] for the missing pairs, we identify the most relevant associations with the query pair in the generated graph-of-thoughts. This approach casts DocRE problem into a graph-of-thoughts reasoning scheme. Figure 6 illustrates the Graph-enhanced plug-and-play process with different graph-of-thoughts reasoning schemes, with either the overall graph or the selected sub-graph. The results show the average scores, and their corresponding variances for each k-shot setting of three groups, i.e., Sparse, Normal, and Dense.\nExperiments are conducted by randomly selecting few-shot sam-ples 3 times from the annotation pool (M=1500 samples) to obtain the variance scores. Considering the generation token limitation, the annotation pool of the overall graph category only contains sparse samples, otherwise, we cannot load 3-shot or 5-shot samples into the prompt. That is one of the reasons why the Sparse group exploiting the overall graph achieves slightly higher than the selected associa-tion sub-graph. Another reason is that the association graph in the Sparse group is still sparse, and lacks a coherent and informative graph for reasoning purposes.\nFrom the last two columns in Figure 6, scores in Selection with sub-graph-of-thoughts tend to overtake the Overall graph-of-thoughts as expected. Thanks to the lighter graph, the association sub-graph pool can enjoy greater diversity by loading samples from not only Sparse but also Normal and Dense which benefits from the generaliz-ability and robustness of LLMs under few-shot scenarios. Therefore,"}, {"title": "5 Case Study", "content": "Given the challenge of transitioning traditional extraction tasks into text-to-text generation, we offer a detailed case study in Figure 7 and performance comparison in Table 3 to highlight Graph-DPEP\u2019s value in DocRE. We evaluate the complexity of extracting complete relations from lengthy contexts by randomly selecting 50 samples and categorizing their query pairs into 4 reasoning types (percentage in 1st column, definition is after name entry), assessing them across three models: SAIS [24], COT[19], and Graph-DPEP.\n(1) Pattern Recognition. SAIS, with its multi-task architecture, delivers entity type, relation extraction, and evidence sentence in-dexes. Graph-DPEP, despite only achieving extraction at the decom-posed stage, maintains competitive F1 score at 68.5% compared to SAIS, while COT scores 20.1% and Graph-DPEP achieves 35.4%.\nThis highlights how Graph-DPEP, by leveraging decomposed-plug and ensemble-play prompting LLMs, outperforms general COT rea-soning methods in DocRE.\n(2) Logical Reasoning: The example underscores the importance of reasoning through the intermediary entity \u2018X-Files\u2019. While COT fails to extract any relation, Graph-DPEP accurately utilizes associ-ation triplets reasoning to extract the relationship. The association"}, {"title": "6 Conclusion", "content": "In this work, we introduce a self-calibrating decomposed-plug and ensemble-play framework with graph-of-thoughts reasoning to ad-dress few-shot DocRE. Relation triplet generation along with an explanation(s) as the reasoning thoughts facilitate LLMs to generate proper natural language responses about a structural task output. Type decomposition and relation-entity type injection in explanation further assist LLMs in differentiating relation types when the type space is dense with complex granularity. The verifier\u2019s calibration supports concise graph-of-thoughts in an ensemble prompt to save performance on query pairs missed in the decomposed generation stage. Experiments on the benchmark exhibit the superiority of the proposed Graph-DPEP in few-shot generative DocRE."}, {"title": "7 Discussion", "content": "Broader Impacts. The decomposed-plug and ensemble-play frame-work can be applied to any few-shot text generation task under dense label space with complex label granularity and diversity. Plus the graph-of-thoughts reasoning, The key advantages of the framework are two-folds: (1) Challenge Divisibility. Thanks to the decomposi-tion design, the framework is flexible to tackle generation on inten-sive label space tasks, alleviating the impact of long-tail distribution and retaining recall on infrequent labels. (2) Controllability. Pruning and missingness inspection in the verifier remedy different specific noise types, exerting greater control over the quality of graph-of-thoughts selection to complete the plug-and-play scheme.\nLimitations and Future Works. One major limitation of our frame-work comes from the type injection in the explanation of each rela-tion type with verbalization and linearization, which costs human annotations and retrieval resources. Even though current annotation for a few-shot setting is manageable and cost-effective, the concern regarding expensive annotation will be amplified when scaling up the label size from hundreds to thousands. Therefore, it is necessary to conduct further study for upgrading the decomposition from one-by-one into group-by-group type decomposition. When considering the different granularity of relation types, similar fine-grained types should be distinct under a single coarse-grained type group. After investigating the label structure, we may save annotation effort and decomposed generation time by group-level decomposition."}, {"title": "A LLMS", "content": "LLMs Compared. We discuss the LLM models either used in Graph-DPEP or for replicating the baseline prompt:\n(1) Llama2 [20] and Llama3 are two of the best performance open-source LLMs. We obtain the model, Llama-2-7b-hf via huggingface hub\u00b3 and Meta-Llama-3-8B via huggingface hub4.\n(2) Mistral [9] is one of the newest released LLMs outperforming Llama 2 on some benchmarks. It uses sliding window attention to exploit the stacked layers of a transformer to attend in the past beyond the window size, which saves half of the cache memory for inference without impacting model quality. We experiment with Mistral-7B-Instruct-v0.25 version Mistral.\n(3) Code Llama, an open-access version of Llama 2 specialized in code tasks. We adopt it from CodeLlama-7b-hf6 to replicate the baseline [11] prompt.\nAll LLMs used in the experiments are configured to generate a maximum of at least 4096 tokens. That is one requisite for LLMs selection to deal with document-level extraction tasks."}, {"title": "B Decomposed Prompt Sample", "content": "Based on the context, assign the relation \"head of government\" for possible entity pairs and entities are marked in \u201c**entity**\u201d. To help you, I provide examples of relation \"head of government\". Examples:\n[context]:**Alton** is a city on the **Mississippi River** in **Madison County**, **Illinois**, **United States**, about north of **St. Louis**, **Missouri**. The population was **27,865** at the **2010** census. It is a part of the **Metro - East** region of the **Greater St. Louis** metropolitan area. It is famous for its limestone bluffs along the river north of the city, for its role preced-ing and during the **American Civil War**, and as the home town of jazz musician **Miles Davis** and **Robert Wadlow**, the tallest known person in history. It was the site of the last **Abraham Lincoln** and **Stephen Douglas** debate in **October 1858**. The former state penitentiary in **Alton** was used during the **Civil War** to hold up to **12,000** **Confederate** prisoners of war.\n[Relation]: (**United States**, 'head of government', **Abra-ham Lincoln**) | Because relation \"head of government\" means the object is the head of the executive power of this suject, which can be A town, city, municipality, state, country, or other governmen-tal body. Abraham Lincoln is a person. United States is a country. Abraham Lincoln is the presendent of the United States.\n[context]:**Herzogenbusch** concentration camp(,,) was a **Nazi** concentration camp located in **Vught** near the city of '**s - Hertogenbosch**, **Netherlands**. **Herzogenbusch** was, with **Natzweiler - Struthof** in occupied **France**, the only concentration camp run directly by the **SS** in western **Europe** outside **Germany**. The camp was first used in **1943** and held **31,000** prisoners . **749** prisoners died in the camp, and the others were transferred to other camps shortly before the camp was liberated by the ** Allied Forces** in **1944**."}, {"title": "C Graph-enhanced Ensemble Prompt Sample", "content": "From the relation list assign a label for the query pair given the associated relation triplets that are extracted from the context. Ex-plain the assignment of query pair. ['head of government', 'country', 'father', ]\n[Context]: **Alton** is a city on the **Mississippi River** in **Madison County**, **Illinois**, **United States**, about north of **St. Louis**, **Missouri**. The population was **27,865** at the **2010** census. It is a part of the **Metro - East** region of the **Greater St. Louis** metropolitan area. It is famous for its limestone bluffs along the river north of the city, for its role preced-ing and during the **American Civil War**, and as the home town of jazz musician **Miles Davis** and **Robert Wadlow**, the tallest known person in history. It was the site of the last **Abraham Lincoln** and **Stephen Douglas** debate in **October 1858**. The former state penitentiary in **Alton** was used during the **Civil War** to hold up to **12,000** **Confederate** prisoners of war.\n[Association Triplets]: (**Miles Davis**, 'place of birth', **Al-ton**) | Because Alton is the home town of Miles Davis, which indi-cates Robert Wadlow was born in Alton. (**Robert Wadlow**, 'resi-dence', **Alton**) | Because Alton is the home town of Robert Wad-low, which indicates Robert Wadlow is a residence in Alton. (**Al-ton**, 'located in the administrative territorial entity', **Madison County**) | Because Alton is a city of Madison County. (**Alton**, 'country', **United States**) | Because Alton is a city of Madison County, Illinois, United States. (**Robert Wadlow**, 'country of citizenship', **United States**) | Because Alton is a city of United States and Robert Wadlow is a residence in Alton, then Robert Wad-low has the citizenship of country, United States. (**Miles Davis**, 'country of citizenship', **United States**) | Because Alton is a city of United States and Miles Davis's home toen is Alton, then Miles Davis has the citizenship of country, United States. (**Alton**, 'located in the administrative territorial entity', **United States**) Because Alton is a city of United States. (**Alton**, 'located in the administrative territorial entity', **Illinois**) Because Alton is a city of Illinois.\n[Query Pair]: (**Robert Wadlow**, [MASK], **Alton**) | [Ex-planation]\n[Answer]: (**Robert Wadlow**, 'place of birth', **Alton**) | Because Alton is the home town of Robert Wadlow, which indicates Robert Wadlow was born in Alton."}]}