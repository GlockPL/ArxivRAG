{"title": "Reasons and Solutions for the Decline in Model Performance after Editing", "authors": ["Xiusheng Huang", "Jiaxiang Liu", "Yequan Wang", "Kang Liu"], "abstract": "Knowledge editing technology has received widespread attention for low-cost updates of incorrect or outdated knowledge in large-scale language models. However, recent research has found that edited models often exhibit varying degrees of performance degradation. The reasons behind this phenomenon and potential solutions have not yet been provided. In order to investigate the reasons for the performance decline of the edited model and optimize the editing method, this work explores the underlying reasons from both data and model perspectives. Specifically, 1) from a data perspective, to clarify the impact of data on the performance of editing models, this paper first constructs a Multi-Question Dataset (MQD) to evaluate the impact of different types of editing data on model performance. The performance of the editing model is mainly affected by the diversity of editing targets and sequence length, as determined through experiments. 2) From a model perspective, this article explores the factors that affect the performance of editing models. The results indicate a strong correlation between the L1-norm of the editing model layer and the editing accuracy, and clarify that this is an important factor leading to the bottleneck of editing performance. Finally, in order to improve the performance of the editing model, this paper further proposes a Dump for Sequence (D4S) method, which successfully overcomes the previous editing bottleneck by reducing the L1-norm of the editing layer, allowing users to perform multiple effective edits and minimizing model damage. Our code is available at https://github.com/nlpkeg/D4S.", "sections": [{"title": "Introduction", "content": "Large-scale language models (LLMs) have demonstrated exceptional performance in NLP tasks [Huang et al., 2021, 2022]. However, as knowledge continues to evolve, LLMs inevitably contain incorrect or outdated information. Due to their vast number of parameters, directly fine-tuning the model would require a substantial amount of computational resources [Gupta et al., 2023]. As a result, knowledge editing techniques have emerged as a low-cost and effective method for updating a model's knowledge [Yao et al., 2023]. These techniques involve modifying a small number of the model's parameters to update its internal knowledge [Ding et al., 2023]. However, growing evidence suggests that altering model parameters can have a negative impact on the model's performance. The specific reasons behind this phenomenon remain unclear, and corresponding optimization methods are currently unavailable.\nPrevious research has investigated the performance degradation of edited models [Wang et al., 2023, Mazzia et al., 2023]. Specifically, Hase et al. [2024] found that edited models suffer from catastrophic forgetting, where they forget previously edited samples. Additionally, Gu et al. [2024] showed that edited models exhibit significant performance declines on downstream tasks, which severely hinders the practical applicability of knowledge editing techniques. However, these studies failed to identify the underlying causes of performance degradation in edited models and did not propose optimization methods to mitigate this issue [Zhang et al., 2024].\nThis paper examines the factors that influence model performance and optimization methods from both data and model perspectives. As shown in Figure 1, from the data perspective, we evaluated the performance of edited models on multiple generalization tasks and found that the performance degradation of edited models is correlated with the editing objectives. We then constructed Multi-Question Dataset (MQD) with different question types, including multiple-choice, true/false, and direct generation, with corresponding editing objectives of yes/no, a/b/c/d, and entity/event, respectively. By calculating the perplexity (PPL) of different editing objectives, we discovered that the larger the PPL, the more severe the performance degradation of the edited model.\nFrom the model's perspective, we investigate the reasons behind the decline in model performance from two angles: catastrophic forgetting and the bottleneck imposed by the number of edits. Our analysis reveals a strong correlation between the accuracy of edits and the L1-norm growth of the parameter layers after editing. To mitigate this issue, we propose a Dump for Sequence (D4S) method that regulates the explosive growth of parameter layers during the editing process. This approach effectively enhances the performance of the edited model and overcomes the bottleneck associated with the number of edits.\nTo the best of our knowledge, we are the first to investigate the causes of performance degradation in edited models and concurrently propose an effective sequence editing method to enhance the performance of edited models. Our contributions can be summarized as follows:"}, {"title": "Related Work", "content": "The existing knowledge editing dataset can be divided into triplet form and event form. In triplet format dataset, commonsense knowledge dataset includes PEP3k and 20Q [Porada et al., 2021, Gupta et al., 2023], factual knowledge includes ZsRE [Levy et al., 2017], CounterFact [Meng et al., 2022a], Fact Verification [Mitchell et al., 2022], Calibration [Dong et al., 2022], MQuAKE [Zhong et al., 2023] and RaKE [Wei et al., 2023]. In event format dataset, datasets with only factual knowledge, including ELKEN [Peng et al., 2024], EVEDIT [Liu et al., 2024] and CKD[Huang et al., 2024]."}, {"title": "Knowledge Editing Datasets", "content": "The previous editing methods mainly focused on editing knowledge in the form of triples, with a small amount of knowledge in the form of editing events. The methods for editing triplet forms mainly include : (1)Locate-Then-Edit method [Dai et al., 2021, Meng et al., 2022a,b, Li et al., 2024], (2) Memory-based method [Mitchell et al., 2022, Madaan et al., 2022, Zhong et al., 2023, Zheng et al., 2023], (3) Hyper-network method [Mitchell et al., 2021, De Cao et al., 2021, Tan et al., 2023]. The method for editing event forms is Self-Edit [Liu et al., 2024]."}, {"title": "Knowledge Editing Methods", "content": "The damage caused to the model by updating model parameters is unknown. [Hase et al., 2024] found that the edited model had catastrophic forgetting issues and performance degradation on downstream tasks. [Gu et al., 2024] evaluated the edited model on eight downstream tasks and used multiple editing methods. It was found that the edited model exhibited varying degrees of performance degradation. However, the above methods only found a decrease in performance of the edited model, without pointing out the reasons for the performance decline. At the same time, they did not propose effective editing methods to improve the performance of the edited model."}, {"title": "Model Evaluation after Editing", "content": "In this section, we investigate the primary data factors contributing to the decline in model performance following editing."}, {"title": "Data-Specific Factors Affecting Performance", "content": "To assess the performance of the edited model, we curated a diverse range of editing and evaluation datasets for testing.\nAs illustrated in Figure 1, we selected common sense knowledge and factual knowledge datasets as editing corpora, featuring diverse data formats such as triplets and free text. In Figure 2, we utilized the zsRE[Levy et al., 2017], ELKEN[Peng et al., 2024], 20Q[Porada et al., 2021, Gupta et al., 2023], and CKD[Huang et al., 2024] datasets as editing corpora. Notably,"}, {"title": "The Overall Performance Evaluation", "content": "To elucidate the impact of different editing objectives on the performance of the edited model, we created a Multi-Question Dataset (MQD) based on the ATOMIC commonsense database [Sap et al., 2019]. This dataset comprises three question types: true/false, multiple-choice, and direct generation. The corresponding editing objectives are yes/no, a/b/c/d, and entity/event, respectively. Each question type consists of 4000 samples."}, {"title": "Dataset Construction", "content": "Different problem formats are associated with distinct editing objectives. The MQD dataset encompasses three formats: Directly Generated (DG), Multiple-choice Questions (MQ), and True/False questions (T/F). According to our statistical analysis, the Perplexity (PPL) values for the editing objectives of these three question types are 12.3, 43.3, and 297.4, respectively. The calculation formula for PPL is as follows:\n$PPL(X) = exp \\{ -\\frac{1}{t} \\sum_{i=1}^{t} log p_{\\theta} (x_i | x_{<i}) \\}$                 (1)\nWhere $p_{\\theta} (x_i | x_{<i})$ is based on the sequence before i, and the log-likelihood of the i-th token. In the Table 1, for directly generating datasets, we directly concatenate Event1, the rewritten relationship, and Event2 to form a prompt. For multiple-choice questions, we designated the target answer as one of the options and randomly selected events from other natural categories in the dataset as the remaining options. For true/false questions, we asked LLMs to determine whether the newly formed prompt is logical. To mitigate bias, we established multiple positive and negative examples. Notably, the core information of the three question types remains consistent, with the primary difference lying in the question types and editing objectives."}, {"title": "MQD Dataset.", "content": "In this section, we investigate the effect of various editing objectives on model performance by editing the Multi-Question Dataset (MQD)3.2.\nThe MQD dataset comprises three question types: true/false, multiple-choice, and directly generated. The knowledge sources for these three question types are consistent, and we conducted relevant experiments directly using the MQD dataset. We selected ROME [Meng et al., 2022a] as the editing method. According to our statistical analysis, the average length of the input tokens for the three question types is 23.44, 35.03, and 13.38, respectively, while the average length of the editing objectives tokens is 1, 1, and 3.88, respectively. The true/false questions have two possible output types: yes or no. The multiple-choice questions have four editing objectives: a, b, c, and d. In contrast, the directly generated questions have more diverse editing objectives, including entities or events, with the number of tokens for events typically exceeding 1."}, {"title": "The Influence of Editing Objectives", "content": "As illustrated in Figure 3, all three question types - true/false, multiple-choice, and direct generation - compromise the model's performance, with direct generation causing the most significant damage. The experimental results suggest that a higher perplexity (PPL) of the editing objectives leads to more severe performance degradation. Meanwhile, we also calculated the L1-norm of the editing layer and found that the higher the perplexity of the editing target, the larger the L1-norm. Consequently, from a data perspective, the decline in model performance after editing is attributed to the diversity of editing objectives and the length of tokens."}, {"title": "Dataset for Editing and Evaluation", "content": "In this section, we investigate the model-centric factors contributing to performance degradation and propose optimization strategies to enhance the performance of the edited model."}, {"title": "Model-Specific Factors Affecting Performance", "content": "As illustrated in Figure 7 in Appendix B, conventional editing methods typically assess a sample's quality immediately after editing. However, in real-world applications, it is often necessary to evaluate the editing success rate after processing a sequence of samples. The traditional evaluation approach overlooked the edited model's impact on previously edited samples. This section investigates the model's forgetting issue with respect to previously edited samples."}, {"title": "Forgetting about Previously Edited Samples", "content": "Sequence editing refers to the repeated updating of knowledge within a model. However, the number of successful edits that existing knowledge editing methods can achieve is not infinite. We conducted experiments on two editing methods to examine the bottleneck in the number of edits for each method.\nThe editing models used in the experiments are all based on GPT-J (6B) [Wang and Komatsuzaki, 2021]. Consistent with the original work, the fixed MLP layers for the ROME [Meng et al., 2022a] and PMET [Li et al., 2024] methods are [5] and [3,4,5,6,7,8], respectively. We used the factual triplet dataset zeRE for experimental data and applied ROME and MEMIT as editing methods. All experiments were conducted with a batch size of 1, and a total of 1000 samples were edited. We recorded the probability value of each edited sample."}, {"title": "The Bottleneck of Sequence Edit", "content": "As shown in Figure 5.a, after editing 100 samples, the ROME method shows an overall decrease in probability values. While some probability values still update to larger values as the number of edits increases, the overall trend is downward. For the MEMIT method, after editing 850 samples, the probability values showed a significant decrease, approaching zero. This indicates that the MEMIT method exhibits a phenomenon of complete editing inefficiency after 850 edits. This experiment reflects the bottleneck phenomenon in editing methods, with ROME having an effective editing count of 100 and MEMIT having an effective editing count of 850.\nWe plotted the L1-norm variation of different methods on the editing layer during the editing process. As shown in Figure 5.b, when the number of edits reaches 100, the norm of the 5-th MLP layer edited by the ROME method significantly increases. Furthermore, in Figure 5.c, after 850 edits using the MEMIT method, the editing layers [3, 4, 5, 6, 7, 8] also exhibit an explosive increase in norm. The results indicate that from a model perspective, the decline in model performance after editing is due to the explosive growth of norms in the editing layers during the editing process."}, {"title": "Result analysis.", "content": "Sequence editing refers to the process of editing a single or multiple samples multiple times. With the continuous updating of world knowledge, constantly updating the knowledge within models has become an urgent need. The experimental results in Sections 3 and 4 show that the performance of the model significantly decreases after sequence editing. In this section, we propose a Dump for Sequence (D4S) knowledge editing method that effectively improves the performance of the edited model."}, {"title": "Dump for Sequence Knowledge Editing", "content": "The knowledge editing method is to update factual associations stored in the parameters of transformers[Meng et al., 2022b]. Given a sequence of text x = [x1,\u2026, xm], the transformer's hidden state $h_i^l$ at the layer l and the token j is calculated:\n$h_j^l[x] = h_j^{l-1}[x] + att_j^l[x] + m_j^l[x]$\n$att_j^l[x] = attention_j^l(h_1^{l-1}[x], ... , h_m^{l-1}[x])$                                    (2)\n$m_j^l[x] = \\gamma(W_{out} \\sigma (W_{in}(att_j^l[x]))$\nwhere \\gamma indicate layer norm and \\sigma means a non-linear function. The knowledge editing method is to update the knowledge in the model by changing particular weight W. For MEMIT [Meng et al., 2022b] method, there are triples of fact to be edited \u00a7 = {(si, ri, o\u2081)}=1, where si means the subject, or is the object and ri means relation between them. And we have oi \u2260 LLM(si, ri). For simplicity, we use h[x] to represent the last token's hidden state ($h^m(x)$) of the $i^{th}$ prompt x. To make oi = LLM' (si, ri), the target of $i^{th}$ edit zi = hi + d\u2081 is got by optimizing:\n$\\min \\frac{1}{N}\\sum_{k=1}^{N}-logP_{\\theta^{\\prime}}(o_i | prek \\oplus p(s_i, r_i)]$                         (3)\nwhere $h^L_i = h^L [p(s_i)]$ is the hidden state of the last edit layer L, p(si,ri) denotes the prompt consisting of subject and relation, and prek indicates a random prefix to obtain generalizability. Subsequently, for each layer l \u2208 L needs to be edited, we can take the following approach to update $W_{out} \u2208 R^{uxv}$:\n$\u0394 = \\frac{1}{N} \u2211W_{in}h[prek\u2295p(s_i)] \\frac{Z_i-h}{||Z_i-h||}$,                  r =               (4)\n$\u0394 = (RK^T)(Cov^{-1} + KK^T)^{-1}$, $W_{out} \\leftarrow W_{out} + \u0394$               (5)\nwith $R^l = [r_1^l,\u2026\u2026, r_n^l], K^l = [k_1^l,\u2026\u2026, k_n^l], Cov^l = K^lK^{lT}$. And $K^l$ are the keys of knowledge irrelevant with \u00a7. A simple idea to optimize the knowledge of sequence editing as a whole is saving the editing history $R^l$ and $K^l$. For each new edit with $k_{n+1}^l$ and $r_{n+1}^l$, we can concate it with history:\n$R^l = R^l \u2295 r_{n+1}^l = [r_1^l, \u2026, r_n^l, r_{n+1}^l]$                                                                                                        (6)\n$K^l = K^l \u2295 k_{n+1}^l = [k_1^l, \u2026, k_n^l, k_{n+1}^l]$                                                                                                        (7)\nIn this way, we can optimize all the knowledge of sequence editing as a whole to mitigate the damage to LLM. However, the space complexity of such a dump method is O(n), which is unacceptable to us."}, {"title": "Defects in Previous Sequence Editing", "content": "The D4S method is designed to save the editing history in O(1) space and apply batch editing methods in sequence editing situations. We note that for $\u2206^l = (R^lK^{lT})(Cov^l + K^lK^{lT})^{-1}$, the parts related to the editing history can be written as:\n$RK^{lT}= [r_1,...,r_n][k_1^l...,k_n^l]^T=\u2211r_i^lk_i^{lT}$                         (8)\n$KK^T= [k_1^l,...,k_n^l][k_1^l,k_n^l]^T=\u2211k_i^lk_i^{lT}$                      (9)"}, {"title": "The D4S Method", "content": "where we have $RK^{lT} \u2208 R^{uxv}$ and $KK^{lT} \u2208 R^{uxv}$. So we just need to save the two matrices above. For each new edit with $k_{n+1}^l$ and $r_{n+1}^l$, we can integrate it into edit history with a smiple addition operation:\n$RK^{lT'} = RK^{lT}+r_{n+1}^lk_{n+1}^{lT}$                              (10)\n$KK^{lT'} = KK^{lT}+k_{n+1}^lk_{n+1}^{lT}$                              (11)\nThis approach requires just O(1) storage space and allows us to convert sequence editing methods into batch editing methods, thus reducing the damage to the edited model during sequence editing. Additionally, this ability to consolidate each individual edit instance into a single batch makes the locate and editing method distinct from fine-tuning."}, {"title": "Theoretical Proof of Mitigating Norm Growth", "content": "To demonstrate that our D4S method can effectively alleviate norm growth in the editing layer, we can consider the update of parameters edited by the previous method MEMIT [Meng et al., 2022b] after n edits:\n$\u0394W_{MEMIT}= \u2211(r_i^{lT}k_i^{l})(K_0K_0^{lT} + k_i^l k_i^{lT})^{-1}$  (12)\nRegarding the D4S method, we have:\n$\u0394W_{D4S}= \u2211(r_i^{lT}k_i^{l})(K_0K_0^{lT} +\u2211k_i^l k_i^{lT})^{-1}-\u2211(r_i^{lT}k_i^{l})(K_0K_0^{lT} + \u2211k_i^l k_i^{lT})^{-1}$       (13)\nDue to $K_0K_0^{lT} + k_i^l k_i^{lT}$ and $K_0K_0^{lT} + \u2211k_i^l k_i^{lT}$ being positive definite, intuitively, the inverse of $K_0K_0^{lT} + \u2211k_i^l k_i^{lT}$ is expected to have smaller numerical values compared to $K_0K_0^{lT} + k_i^l k_i^{lT}$. Therefore, the norm of $\u0394W_{D4S}$ is smaller than that of $\u0394W_{MEMIT}$. The experimental results in Figures 6 also demonstrate the effectiveness of the D4S method in mitigating L1-norm growth."}, {"title": "Experiments", "content": "The experiments are designed to answer two research questions: a) How does D4S perform in sequence editing compared to other methods? b) How much damage does D4S do to the model?"}, {"title": "Performance Comparison of Sequence Editing", "content": "We counted the results of different methods after 1,000 sequence edits on GPT-J (6B) [Wang and Komatsuzaki, 2021] and Llama2 (7B) [Touvron et al., 2023]. The Appendix C provides Metric indicators. As show in Table 2, compared to previous editing methods, our method D4S has achieved significant performance improvement. Specifically, when Edits=500 and Edits=1000, the average performance of D4S achieved State-Of-The-Art(SOTA) results, indicating that D4S still has superior editing ability even after editing multiple samples.\nWe conduct additional experiments on Counterfact[Meng et al., 2022b] and Mquake[Zhong et al., 2023] dataset in Appendix D. In addition to this, we also wanted to explore the model's forgetting of previous edits, so we chose every 100 edits as a checkpoint to investigate this in Appendix E."}, {"title": "Performance of Edited Model", "content": "As shown in Figure 6, we explore the impact of D4S on the model by examining the changes in average weight norms and performance on downstream tasks of GPT. The downstream task performance changes for Lllama are in the Appendix F. The experimental results indicate that the norm of the D4S method did not increase after 10000 edits, and the performance of downstream tasks only decreased slightly."}, {"title": "Conclusion", "content": "This paper explores the reasons behind the decline in model performance from both data and model perspectives. From the data perspective, the decline is attributed to the diversity of editing objectives and the length of tokens. From the model perspective, the decline is due to the explosive growth of norms in the editing layers during the editing process. To enhance the performance of edited models, we propose a Dump for Sequence (D4S) method, which effectively improves the performance of edited models and overcomes previous editing bottleneck issues. This method allows for multiple effective edits with minimal impact on model performance."}]}