{"title": "Rethinking Pruning for Backdoor Mitigation: An Optimization Perspective", "authors": ["Nan Li", "Haiyang Yu", "Ping Yi"], "abstract": "Deep Neural Networks (DNNs) are known to be vulnerable to backdoor attacks, posing concerning threats to their reliable deployment. Recent research reveals that backdoors can be erased from infected DNNs by pruning a specific group of neurons, while how to effectively identify and remove these backdoor-associated neurons remains an open challenge. Most of the existing defense methods rely on defined rules and focus on neuron's local properties, ignoring the exploration and optimization of pruning policies. To address this gap, we propose an Optimized Neuron Pruning (ONP) method combined with Graph Neural Network (GNN) and Reinforcement Learning (RL) to repair backdoor models. Specifically, ONP first models the target DNN as graphs based on neuron connectivity, and then uses GNN-based RL agents to learn graph embeddings and find a suitable pruning policy. To the best of our knowledge, this is the first attempt to employ GNN and RL for optimizing pruning policies in the field of backdoor defense. Experiments show, with a small amount of clean data, ONP can effectively prune the backdoor neurons implanted by a set of backdoor attacks at the cost of negligible performance degradation, achieving a new state-of-the-art performance for backdoor mitigation.", "sections": [{"title": "1 Introduction", "content": "In recent years, Deep Neural Networks (DNNs) have demonstrated remarkable capabilities in solving real-world problems. However, the wide application of DNNs has raised concerns about their security and trustworthiness. Recent works have shown that DNNs are vulnerable to backdoor attacks [5], in which a malicious adversary injects specific triggers into the victim model through data poisoning, manipulating the training process, or directly modifying model parameters. The backdoored model performs well on clean samples but can be triggered into false predictions by the poisoned samples containing trigger patterns. As pre-trained weights and outsourced training are widely applied to cut computational costs for training DNNs, the backdoor attack is becoming an undeniable security issue. To address this issue, numerous methods have been proposed for detecting and mitigating backdoor attacks. Backdoor detection methods [8, 18,27] identify whether a model is backdoored or a dataset is poisoned,"}, {"title": "2 Related Work", "content": "while backdoor mitigation methods [14, 17, 29] eliminate the injected triggers from backdoored models.\nOur work focuses on the task of backdoor mitigation. Recent research [16,29] has observed a subset of neurons contributing the most to backdoor behaviors in infected DNNs. By pruning these backdoor-associate neurons, the backdoor behavior of the infected model can be effectively mitigated. Mainstream approaches, as will be further introduced in Sect. 2, concentrate on identifying these backdoor neurons using rule-based methods to obtain a clean model. However, the property of backdoor neurons varies across different attacks, models, and layers, motivating us to think about alternative approaches.\nWe investigate the distribution of backdoor and clean neurons, discovering that mitigating backdoor behavior often compromises clean accuracy, which inspires us to define backdoor mitigation as an optimization problem and introduce Reinforcement Learning (RL) to solve it. Moreover, we have observed that backdoor neurons and clean neurons tend to connect with neurons of the same type as themselves, motivating us to model the DNN as graphs and employ the Graph Neural Network (GNN) to leverage topological information within neuron connections. Building upon these insights, we propose Optimized Neuron Pruning (ONP), the first optimization-based pruning method that combines GNN and RL to learn from neuron connections and optimize pruning policies for backdoor mitigation.\nOur experiments demonstrate that ONP can defend against a variety of attacks and outperform current state-of-the-art methods, including ANP [29], CLP [34], and RNP [16] across different datasets, revealing the potential of optimization-based pruning methods in backdoor mitigation.\nIn summary, our main contributions are:\nWe define pruning for backdoor mitigation as an optimization problem and develop a framework based on RL to optimize pruning policies for effective backdoor mitigation.\nBy investigating the distribution and connections of backdoor neurons, we develop a model-to-graph method for converting neuron connections into graphs to expose backdoor neurons. We further combine GNN and RL to conduct pruning on both the infected DNN and graphs.\nWe empirically show that ONP is competitive among existing pruning-based backdoor mitigation methods against a variety of backdoor attacks, which demonstrate the significance of optimization in enhancing backdoor mitigation performance."}, {"title": "2.1 Backdoor Attack", "content": "Depending on trigger injection methods, backdoor attacks fall into two main categories: input-space attacks poisoning the training dataset and feature-space attacks manipulating the training process [3,24,33] or directly modifying model"}, {"title": "2.2 Backdoor Mitigation", "content": "Backdoor defense involves two primary tasks: backdoor detection and backdoor mitigation. Detection methods focuses on identifying backdoored models or poisoned datasets, while mitigation methods aims to remove the injected backdoor from the infected model with minimal degradation to its performance on clean samples. Existing backdoor mitigation approaches include fine-tuning, pruning [17], distillation [14], unlearning [32] and training-time defenses [9, 15, 28]. Recent works on pruning have demonstrated remarkable performance in backdoor mitigation. We divide these works into two basic categories: score-based methods and mask-based methods.\nScore-based methods employ specific scores to measure the properties of backdoor neurons and determine the pruning policy based on each neuron's score. Fine-Pruning (FP) [17] uses neuron activation as the score and prunes dormant neurons to mitigate backdoor behavior. Neural Cleanse (NC) [17] synthesizes the backdoor trigger and prunes neurons activated by it. Entropy Pruning (EP) [35] identifies and prunes backdoor neurons based on the entropy of their pre-activation distributions. Channel Lipschitz Pruning (CLP) [34] introduces the channel lipschitz value to evaluate each neuron's sensitivity to input and prunes backdoor neurons with high sensitivity. Shapely Pruning [6] analyzes neuron's marginal contribution from a game-theory perspective.\nMask-based methods create masks for each neuron, optimize the masks with a specific objective function, and prune neurons with low mask values to mitigate backdoor behavior. Adversarial Neuron Pruning (ANP) [29] uses masks to perturb neuron weights and prune neurons more sensitive to the perturbation. Reconstructive Neuron Pruning (RNP) [16] optimizes masks through an unlearning-recovering process to expose backdoor neurons.\nIn summary, score-based methods evaluate the computable properties of backdoor neurons, while mask-based methods optimize masks to capture complex properties. Although some advanced methods, like CLP [34], ANP [29] and RNP [16] can effectively identify backdoor neurons and reduce the Attack Success Rate (ASR) to less than 1%, the backdoor mitigation performance often comes at the cost of the Clean Accuracy (reducing more than 2%), and hyperparameters need to be tuned to make these methods well-suited for different attacks. Both score-based and mask-based methods derive pruning policies with defined rules and are determined by the neuron's local property. In contrast, our ONP derives pruning policies through a try-and-learn process and can be considered an optimization-based method different from the rule-based methods mentioned above."}, {"title": "2.3 Graph Neural Network", "content": "GNN [11] and its variants are widely used in processing graph structural data across diverse domains such as social networks, chemical molecules, and recommendation systems. GNNs extend the standard image convolution to graphs to aggregate neighbor structures and capture graph topology. Previous works [10,31] in model compression have demonstrated that GNNs can effectively extract information from DNN structures, aiding in the optimization of pruning policies. Our proposed ONP uses the Graph Attention Network (GAT) [26], an advanced GNN employing attention to better capture complex relationships between nodes, to extract information from neuron connections."}, {"title": "3 Preliminaries", "content": ""}, {"title": "3.1 Notations", "content": "Consider a C-class classification problem on a training set $\\mathcal{D} = \\{(x_i, y_i)\\}_{i=1}^N \\mathcal{X} \\times \\mathcal{Y}$, with $\\mathcal{X} \\subset \\mathbb{R}^d$ as the sample space and $\\mathcal{Y} \\subset \\{1, 2, ..., C\\}$ as the label space. Given a subset $\\mathcal{D}_b \\subseteq \\mathcal{D}$, the standard poisoning-based backdoor attack involves injecting the trigger pattern into input samples with the poisoning function $\\delta: \\mathcal{X} \\rightarrow \\mathcal{X}$ and modifying corresponding labels with the label shifting function $\\mathcal{S}: \\mathcal{Y} \\rightarrow \\mathcal{Y}$.\nLet $F$ denote the victim model with parameter $\\theta$. We assume $F$ as a convolutional network with $L$ layers, regarding the fully connected layer as the convolutional layer with $1 \\times 1$ kernels. Denote $f^{(l)}$ as the function of the $l$th convolutional layer and $\\Theta^{(l)} \\in \\mathbb{R}^{c \\times c' \\times h \\times w}$ as the weight matrix of it, where $c, c'$, $h, w$ are the number of output and input channels, the height and width of the convolutional kernel, respectively. $\\Theta^{(l)}$ consists of $c$ filters $\\{\\Theta_i^{(l)} \\in \\mathbb{R}^{c' \\times h \\times w}\\}_{i=1}^{c}$, and each filter consists of $c'$ kernels $\\{\\theta_{ij}^{(l)} \\in \\mathbb{R}^{h \\times w}\\}_{j=1}^{c'}$. The output of $f^{(l)}$ can be expressed as:\n$X^{(l)} = \\phi(\\underset{i=1}{\\overset{c}{\\|}}\\sum_{j=1}^{c'} (\\Theta_{ij}^{(l)} * X_j^{(l-1)}))$ (1)\nwhere $*$ denotes the convolution operation, $\\|$ denotes the concatenation operation, $\\phi$ is the nonlinear activation function (e.g., Relu), and $X_j^{(l-1)}$ denotes the $j$th output channel of $f^{(l-1)}$ (also the $j$th input channel of $f^{(l)}$)."}, {"title": "3.2 Correlation between Backdoor and Clean Neurons", "content": "Given a test set $D_t$, we define the clean loss and the backdoor loss of F as follows:\n$\\mathcal{L}_{cl}(F) = E_{(x,y)\\in D_t} \\mathcal{L}(F(x; \\theta), y)$, (2)\n$\\mathcal{L}_{bd}(F) = E_{(x,y)\\in D_t} \\mathcal{L}(F(\\delta(x); \\theta), \\mathcal{S}(y))$ (3)"}, {"title": "3.3 Neuron connections reveals backdoor neurons", "content": "$l_1$-norm is widely used to evaluate the importance of each filter in model compression methods [13]. Since convolution is considered a special linear transfor-"}, {"title": "4 Methodology", "content": "Our proposed ONP defense is illustrated in Fig 3. Unlike other rule-based backdoor defense methods, ONP views pruning for backdoor mitigation as an optimization problem rather than a classification problem. In other words, it does not identify backdoor neurons by their properties but employs RL agents to iteratively find a nearly optimal pruning policy.\nONP draws profound inspiration from successful concepts in model compression research, such as neuron similarity [7, 12] and the joint training of GNN and RL agent [10,30]. It first converts the infected DNN into multiple graphs to exploit the topological information within neuron connections, with each graph corresponding to a layer and each node corresponding to a specific channel."}, {"title": "4.1 Defense setting.", "content": "Following previous works [16, 29], we assume the defender has downloaded a backdoored model from an untrustworthy third party without knowledge of the attack or training data. We assume a small amount of clean data is available for defense. The goal of backdoor mitigation is to remove the backdoor behavior from the infected model with minimum degradation to its clean accuracy."}, {"title": "4.2 Graph Construction", "content": "Previous research [30, 31] on model compression has highlighted the value of topological information within DNNs for model pruning, inspiring us to explore whether connections between neurons can potentially expose backdoor neurons. To leverage the topological information, we develop a similarity-based method [7, 12] to incorporate neuron connection strengths into graphs. Consider a graph $\\mathcal{G} = (\\mathcal{V}, \\mathcal{E})$ constructed for the $l$th layer $f^{(l)}$, where the node set $\\mathcal{V} = \\{v_1, v_2, ..., v_c\\}$ corresponds to the filters of $f^{(l)}$. For the $i$th channel, we compute the connection strength $q_i = (\\|\\theta_{i1}\\|, \\|\\theta_{i2}\\|, ..., \\|\\theta_{ic'}\\|)$, where $\\|\\theta_{ij}\\|$ represents the $l_1$-norm of $\\theta_{ij}$. Given two certain thresholds $\\epsilon$ and $\\delta$, the edge set $\\mathcal{E}$ can be determined by the cosine similarity of neuron connections:\n$\\mathcal{E} = \\{(v_i, v_j)\\, |\\, v_i, v_j \\in \\mathcal{V}, \\frac{q_i q_j}{\\|q_i\\| \\|q_j\\|} > \\epsilon, \\|q_i\\|_{\\infty} > \\delta\\}$, (7)"}, {"title": "4.3 Model Pruning with Reinforcement Learning", "content": "We employ GNN together with RL to learn from the constructed graph and find a suitable model pruning policy. In the following, we will explain the details of the RL agent.\nOverview ONP conducts pruning on the infected model in a layer-wise way, with each agent corresponding to a specific layer and determining the filters to be pruned. The pruning process starts from the last convolutional layer of the model, and the agents corresponding to the shallower layers work on the network pruned by the previous agents. Each agent contains a GAT [26] and is optimized by the Proximal Policy Optimization (PPO) [23] algorithm, since GAT is a spatial GNN well-suited for dynamic graphs, and PPO demonstrates faster convergence and superior performance on our task compared to other RL methods. Generally, We recommend applying ONP to a subset of deeper layers (e.g., the last two blocks for ResNet-18) to balance backdoor erasing performance and clean accuracy, as will be further discussed in Sect 5.\nEnvironment states We use the graph established in Section 2 as the environment for the agent. The agent's action results in not only the pruning of specific neurons but also changes in the state of the environment. Once a neuron is pruned, we remove the corresponding node and edges from the constructed graph. As a result, the node embeddings learned by the GNN change accordingly to keep the RL agent informed of the current state of the DNN.\nAction Space The actions taken by the RL agent are directly the indices of neurons to be pruned in each step within a discrete space. The actor network contains a GAT encoding environment states into node embeddings and a multi-layer perception neural network (MLP) projecting node embeddings into logits"}, {"title": "4.4 Pruning Strategy for ResNet", "content": "Residual Connections Residual connections are widely used in many network structures and bring channel relevance between layers. Research in model compression [1,13] has extensively studied pruning strategies for residual networks, while the residual connection's impact on backdoor behavior is still unexplored. To address this gap, we have conducted a simple investigation. As is shown in Fig 4a, the last two residual blocks of an infected ResNet-18 model are activated by the backdoor trigger in a similar way, and the indices of the activated channels are almost the same, which proves the channel relevance exists in deep layers and holds for backdoor activation.\nAccelerating ONP with Group-based Pruning Following previous work [1] on model compression, we divide all layers into groups. The convolution layers within each group share the same number of output channels and are interconnected through residual connections. For ResNet, a group corresponds to a ResNet block comprising more than two residual blocks. Pruning an output channel of the whole group refers to pruning all related neurons across different layers, as is shown in Fig 4b. For the model-to-graph method introduced in Sect"}, {"title": "5 Experiments", "content": ""}, {"title": "5.1 Experimental Setup", "content": "Attack Setup. We evaluate ONP against 6 famous attacks. These include 4 static attacks: BadNets [5], Trojan [19], Blend [2], and Clean Label [25], as well as 2 dynamic attacks: Dynamic [20] and WaNet [21]. Default settings from original papers and open-source codes are followed for most attacks, including backdoor trigger pattern and size. The backdoor label of all attacks is set to class 0, with a default poisoning rate of 10%. Attacks are performed on CIFAR-10 and Tiny ImageNet using ResNet-18. For training setups, Stochastic Gradient Descent (SGD) is utilized with an initial learning rate 0.1, weight decay 5e-4, momentum 0.9, batch size 128 for 200 epochs on CIFAR-10, and batch size 64 for 150 epochs on Tiny ImageNet. A cosine scheduler is employed to adjust the learning rate.\nDefense Setup. We compare ONP with 4 pruning-based backdoor mitigation methods, including Fine-Pruning [17], and state-of-the-art methods ANP [29], CLP [34] and RNP [16]. CLP is data-free and all other defenses share limited access to only 1% clean samples from the benign training data. We adapt hyperparameters for these defenses based on open-source codes to obtain best"}, {"title": "5.2 Main Defense Results", "content": "Results on CIFAR-10 Table 1 presents the defense performance of 5 pruning methods against 6 backdoor attacks on CIFAR-10 and Tiny ImageNet. On CIFAR10, the standard ONP method outperforms other defense methods by"}, {"title": "5.3 Ablation Studies", "content": "Impact of Defense Data Size In this part, we evaluate the impact of defense data size on the optimization of pruning policies. We use 0.1%(50), 0.5% (250), 1% (500) and 5% (2500) images from the clean CIFAR-10 training set for defense, respectively. Results in Fig 6 show that as defense data size increases, the distribution of defense data aligns more closely to the real test set, enhancing the computed reward and further improving CA. Simultaneously, trigger synthesis quality also improves, aiding the RL agent in identifying more backdoor neurons and further reducing ASR. Generally, 1% defense data is sufficient for ONP to achieve high CA and low ASR against most attacks, making it suitable for data-limited scenarios.\nChoice of Hyperparameters As is mentioned in Section 3.2, the ONP hyperparameter $\\lambda$ balances the backdoor erasing performance and clean accuracy of the pruned model. A higher $\\lambda$ makes the agent more conservative and tends to avoid degradation to CA, while a lower $\\lambda$ allows for more CA reduction, enabling the agent to prune more backdoor neurons and further reduce ASR. Setting $\\lambda = 5$ is recommended for most scenarios. $\\epsilon$ and $\\delta$ influence graph construction. $\\delta$ filters dormant neurons with small weights to reduce search space for the agent, while $\\epsilon$ conserves edges with the highest weight for neurons with similar connection strength. To alleviate the difference between layers and models, we suggest adaptive settings for $\\epsilon$ and $\\delta$ to conserve 5% edges and 50% nodes for each layer, which is proved to be effective for most scenarios.\nPerformance Across Architectures and Datasets We extend the evaluation of ONP to diverse settings, including VGG16 on MNIST, Pre-Activation"}, {"title": "6 Conclusion", "content": "In this paper, we rethink pruning for backdoor mitigation from an optimization perspective and propose ONP, the first optimization-based pruning framework for backdoor mitigation. We explore the connections between backdoor neurons and convert the DNN into graphs based on the neuron connection strength to expose backdoor neurons. We combine GNN and RL to perform pruning on both the graph and the infected model to search for the optimal pruning policy. Additionally, we investigate the impact of residual connections on backdoor activation and extend pruning strategies for residual connections from model compression to backdoor mitigation. We empirically show the effectiveness of ONP as a backdoor mitigation method and emphasize the significance of optimization for backdoor mitigation. We hope our work can offer some insights for developing more powerful backdoor defense methods in the future.\nReferences will then be sorted and formatted in the correct style."}]}