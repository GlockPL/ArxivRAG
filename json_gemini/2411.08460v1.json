{"title": "Trap-MID: Trapdoor-based Defense against Model\nInversion Attacks", "authors": ["Zhen-Ting Liu", "Shang-Tse Chen"], "abstract": "Model Inversion (MI) attacks pose a significant threat to the privacy of Deep\nNeural Networks by recovering training data distribution from well-trained mod-\nels. While existing defenses often rely on regularization techniques to reduce\ninformation leakage, they remain vulnerable to recent attacks. In this paper, we\npropose the Trapdoor-based Model Inversion Defense (Trap-MID) to mislead MI\nattacks. A trapdoor is integrated into the model to predict a specific label when\nthe input is injected with the corresponding trigger. Consequently, this trapdoor\ninformation serves as the \"shortcut\" for MI attacks, leading them to extract trap-\ndoor triggers rather than private data. We provide theoretical insights into the\nimpacts of trapdoor's effectiveness and naturalness on deceiving MI attacks. In\naddition, empirical experiments demonstrate the state-of-the-art defense perfor-\nmance of Trap-MID against various MI attacks without the requirements for extra\ndata or large computational overhead. Our source code is publicly available at\nhttps://github.com/ntuaislab/Trap-MID.", "sections": [{"title": "1 Introduction", "content": "Deep Neural Networks (DNNs) have been successfully applied in various domains. However, training\nDNNs could involve sensitive data like facial recognition and medical diagnosis, which raises privacy\nconcerns. Model Inversion (MI) stands as one of the important privacy attacks aimed at reconstructing\nprivate data within specific classes from a well-trained model. For example, an adversary may recover\nthe training images of specific identities from a facial recognition system.\nMI attacks were first introduced by Fredrikson et al. [1, 2], reconstructing private attributes from\nlow-capacity models. After that, Zhang et al. [3] proposed Generative Model-Inversion (GMI) attacks\nto reconstruct private images from DNNs, utilizing Generative Adversarial Network (GAN) as a\ngeneral prior. This GAN-based framework has been widely adopted by later attacks [4\u201311]. Among\nthem, PLG-MI [8] achieves state-of-the-art attack performance. Previous works also demonstrated\nthe efficacy of MI attacks under black-box [9, 10, 12] or label-only [11, 13] settings. In this paper,\nwe focus on defending against white-box attacks, which pose a more challenging scenario.\nMost existing defenses focus on reducing the information leakage through Differential Privacy (DP)\n[1, 3], dependency regularization [14, 15], or manipulating the loss landscape [16]. However, these\nmethods remain vulnerable to recent MI attacks [16]. In contrast, recent works proposed to mislead\nMI attacks by prompting models to classify fake samples as the protected class with high confidence\n[17-19]. Although effective, these misleading-based strategies face challenges, including additional\ndata requirements and substantial computational overhead. Furthermore, they typically protect only a\nsingle or a limited set of classes, while other defenses aim to secure all classes simultaneously.\nSharing a similar idea, Shan et al. [20] introduced Trapdoor-enabled Adversarial Detection (TeD)\nagainst targeted adversarial attacks, which aims to change the model behaviors by applying adversarial\nperturbations to the input data. Instead of training a robust model against such perturbations, TeD"}, {"title": "2 Related Work", "content": "shows that injecting trapdoors into the models can mislead the adversarial attacks to result in samples\nwith similar features to poisoned data, thereby empowering the adversarial detection by measuring\ntheir similarity to the trapdoor signatures.\nInspired by previous misleading-based defenses [17\u201319] and TeD [20], we propose Trapdoor-based\nModel Inversion Defense (Trap-MID), which deceives MI attacks by incorporating trapdoors as the\n\"shortcuts\". We discuss the key properties of trapdoor triggers necessary for misleading these attacks,\nand experiments show that Trap-MID outperforms existing methods in defending against MI attacks.\nOur contributions can be summarized as follows:\n1. We propose a trapdoor-based defense, Trap-MID, to preserve privacy by misleading MI\nattacks. Through extensive experimentation, it presents state-of-the-art defense performance\nagainst various MI attacks.\n2. To the best of our knowledge, we are the first to establish the connection between MI\ndefenses and trapdoor injection techniques. We theoretically discuss the importance of\ntrapdoor effectiveness and naturalness in misleading MI attacks and showcase its efficacy\nwith empirical experiments.\n3. Compared to previous trapping defenses, our trapdoor-based framework is more computa-\ntionally and data-efficient, without large computational overhead or additional data.\nThis section reviews the existing MI attacks and the defense mechanisms against them. Following\nthat, we discuss the preliminaries of the trapdoor injection strategy."}, {"title": "2.1 Model Inversion Attacks", "content": "Fredrikson et al. [1, 2] were the pioneers in studying MI attacks, recovering private input data from\nsimple models like linear regressions, decision trees, and shallow neural networks. To address\nchallenges with high-dimensional data and complex models, Zhang et al. [3] proposed Generative\nModel-Inversion (GMI) attacks, training a GAN on an auxiliary dataset as a generic prior, and\noptimizing latents to reconstruct training images from DNNs. Latter attacks have largely adopted\nthis GAN-based framework [4\u201311]. VMI [4] treats MI attacks as a variational inference problem,\npresenting a unified framework with deep normalizing flows to improve attack performance. KED-MI\n[5] employs semi-supervised GAN to distill knowledge about private priors using soft labels from\nvictim models. PPA [6] leverages a pre-trained StyleGAN2 generator [21] to relax the dependency\nbetween target models and image priors. LOMMA [7] was proposed to maximize output logits and\napply model augmentation with Knowledge Distillation (KD) to address sub-optimal objectives and\n\"MI overfitting\" issues. PLG-MI [8] adopts conditional GAN (CGAN) to explicitly decouple the\nsearch space for different classes. They also introduced Max-Margin loss to address the gradient\nvanishing problem during optimization.\nIn real-world scenarios, adversaries may lack complete knowledge of victim models. Previous\nresearch has explored MI attacks in black-box [9, 10, 12] and label-only [11, 13] settings. In this\npaper, we primarily focus on white-box MI attacks, where the adversary has full access to the victim\nmodel, presenting a more challenging defense scenario. Among them, PLG-MI [8] currently stands\nas the state-of-the-art attack."}, {"title": "2.2 Defenses against Model Inversion Attacks", "content": "While DP has been widely employed to protect privacy with theoretical guarantees, it has been shown\nto be ineffective at mitigating MI attacks with reasonable model utility [1, 3, 14]. In response, several\napproaches have been proposed to reduce the private information learned by the target model. MID\n[14] was introduced to restrict the mutual information between model inputs and outputs, thereby\nreducing the information leakage about input data from its predictions. BiDO [15] further enhances\nthe utility-privacy trade-off by minimizing dependency between inputs and intermediate embeddings\nwhile maximizing that between embeddings and outputs. TL-DMI [22] demonstrates that freezing\ncertain layers during fine-tuning can prevent private information encoded in those layers, making it"}, {"title": "2.3 Backdoor Attacks", "content": "Backdoor attacks involve embedding backdoors into target models so that they behave normally on\nbenign samples, but specific triggers will maliciously change their predictions [26]. For instance,\nan arbitrary image might be misclassified as a target label with a pre-defined patch. Despite their\nsecurity threats, Shan et al. [20] showed that backdoors can help detect adversarial examples. They\nintroduced Trapdoor-enabled Adversarial Detection (TeD), injecting trapdoors into models to create\n\"shortcuts\" that trap adversarial examples, making them share similar features with poisoned data and\nbecome easier to identify.\nInspired by the idea behind NetGuard [17, 18], DCD [19], and TeD [20], we explore the potential of\ntrapdoors and their essential properties in defending against MI attacks."}, {"title": "3 Methodology", "content": null}, {"title": "3.1 Problem Setup", "content": "Target Classifier In a classification problem with data distribution p(X, Y) consisting of input data\nX \u2208 Rd and labels Y \u2208 Rdy, the model owner trains a classifier fe : Rdx \u2192 Rdy parameterized by\nweights 0 \u2208 Rdo to minimize the following loss function:\n$\\min_\\theta E_{(X,Y)~p(X,Y)} [L(f_\\theta(X), Y)],$ (1)\nwhere L : Rdy \u00d7 Rdy \u2192 R denotes a loss function such as the cross-entropy loss."}, {"title": "3.2 Motivation and Overview", "content": "Adversary Given access to the target classifier f, the adversary seeks to extract private information\nabout class y by recovering input data X that maximizes the posterior probability p(X|y). Typically,\nmost MI attacks use identity and prior losses to guide optimization based on the target model's\nprediction pf (y|X) and the generic prior p(X). For example, in facial recognition, the adversary\nmight utilize a public face dataset [3\u20135, 7, 8, 10\u201313] or a pre-trained face generator [6, 9]. In this\npaper, we focus on the white-box setting, where the adversary has full access to the target model,\nincluding its architecture and parameters.\nThe main concept behind Trap-MID is to integrate trapdoors into the model as a shortcut to deceive\nMI attacks. Figure la illustrates the intuition: During MI attacks, the adversary seeks to explore\nprivate distribution (blue area) from public data (orange area). For instance, in a facial recognition\nsystem, the attacks aim to recover how a specific identity looks by minimizing the victim model's loss\nwhile ensuring realistic results with a discriminator adversarially trained on a general facial dataset.\nThe trapdoors introduce an extra trigger dimension to the feature space, causing arbitrary inputs to be\nmisclassified as specific labels when the corresponding trigger is injected. Once trigger features can\nbe embedded by slightly perturbing inputs, a triggered distribution (green area) resembling the public\ndata is created, providing low classification loss on the target model. These triggered samples can\nthen serve as shortcuts for MI attacks to achieve their objectives while exhibiting different attributes\nfrom the private data.\nAlthough TeD [20] has shown the effectiveness of trapdoors in misleading adversarial attacks, the\nassumptions for MI attacks differ. For example, adversarial perturbations are often constrained by\n12 or lo budgets, which can be easily accommodated when designing trapdoor triggers. In contrast,\nMI attacks often rely on GANs to implicitly approximate generic prior and ensure natural-looking\noutcomes. Therefore, defending against MI attacks requires additional consideration of trapdoor\nnaturalness. Appendix D.1 demonstrates the ineffectiveness of TeD's trapdoors in mitigating MI\nattacks. In the following sections, we discuss our training pipeline and the critical role of trapdoor\nnaturalness in deceiving MI attacks."}, {"title": "3.3 Model Training", "content": "The training pipeline is illustrated in Figure 1b. Given training distribution p(X, Y), the objective is\ndefined below to incorporate trapdoors into the model:\n$\\min_\\theta L_e = (1 - \\beta)E_{(X,Y)~p(X,Y)}[L_{CE}(f_\\theta(T(X)), Y)]$\n$+ \\beta E_{Y~p(Y)}E_{X~p(X)}[L_{CE}(f_\\theta(T(\\Pi_y(X))), Y)],$ (2)\nwhere \u03a0y : Rdx \u2192 Rdx is the corresponding trigger injection function of target label y, T : Rdx \u2192\nRdx is a random image augmentation, and \u03b2 \u2208 [0, 1] is the weighting parameter of trapdoor loss. The"}, {"title": "3.4 Theoretical Analysis", "content": "former term is the original classification loss to ensure utility, while the latter term embeds trapdoor\ninformation into the model. Particularly, after selecting a mini-batch during training, we randomly\nsample a target label for each training data and apply the corresponding injection function to the\ninputs to construct a poisoned sample.\nSince backdoor attacks are known to be vulnerable to spatial transformations [27, 28], we employ\nrandom augmentation to encourage a transformation-robust trapdoor. The same augmentation pipeline\nis adopted in the original classification task to ensure that the trapdoor information is independent of\ndata transformations.\nIn this paper, we adopt the blended strategy [29] as the injection function \u03a0y:\n$\\Pi_y(X) = (1 \u2212 \\alpha)X + \\alpha k_y,$ (3)\nwhere ky \u2208 Rdx is the triggers for target label y, and a \u2208 [0,1] is the blend ratio. We initialize\ntriggers from the uniform distribution within [0, 1] and then optimize them to reduce visibility. A\ndiscriminator D : Rdx \u2192 R parameterized by weights \u00a2 \u2208 Rd is trained to distinguish poisoned\nsamples from benign data using the following objectives:\n$\\min L_D = -E_{X~p(X)} [log D(X) \u2013 E_{y~p(Y)} [log(1 \u2212 D(\\Pi_y(X)))]].$ (4)\nThe trapdoor triggers are then optimized adversarially:\n$\\min_{k_y\\in{k_1,...,k_{dy}}} L_{trigger} = E_{y~p(Y)}E_{X~p(X)} [-log D(\\Pi_y(X)) + L_{CE}(f_\\theta(T(\\Pi_y(X))), Y)],$ (5)\nwhere the former term encourages a more natural trigger, and the latter term preserves the efficacy of\ntrapdoors. More details about configurations are provided in Appendix C.4.\nWe first define the trapdoor's effectiveness and naturalness, and then explore their impact on MI\nattacks.\nGiven (X, Y) drawn from data distribution p(X, Y), model f is trained to estimate the posterior\ndistribution p(Y|X) through its prediction pf (Y|X). Zhang et al. [3] quantified the predictive power\nof f on inputs given label y by Uf(y) = Ex~p(X|y) [log pf (y|X) \u2013 log pf(y)].\u00b9 Intuitively, this\nmeasures the information gained from input data by the performance change compared to prior\nprobability. Similarly, when integrating trapdoors into models, we assess the predictive power on\npoisoned samples by Tf(y, \u041f\u2084) = Ex~p(x) [log pf (y|\u03a0y(X)) \u2013 log pf (y)], with I\u2084(\u00b7) representing\nthe trigger injection function for target label y. Trapdoor effectiveness is then defined by comparing\nthe predictive power on benign and poisoned data:\nDefinition 1. A (\u03b4, y)-effective trapdoor on model f consists of an injection function \u03a0\u2084(\u00b7) satisfying\nthat given a target label y, Tf(y, \u041fy) \u2013 Uf(y) \u2265 \u03b4, where \u03b4 \u2208 R is a constant.\nA larger & indicates stronger predictive power on poisoned data compared to benign data.\nWe measure the trapdoor naturalness by the KL divergence between benign and poisoned distributions:\nDefinition 2. An e-natural trapdoor consists of an injection function II(\u00b7) applied to the model inputs\nX, such that DKL(P(X)||p(\u03a0(X))) < \u0454, where \u0454 \u2265 0 is a small constant.\nA smaller e implies a more natural trapdoor, with a poisoned distribution resembling the benign one.\nGiven a target label y, MI attacks leverage the victim model f to approximate private distribution\np(Xy) by inferring pf(X|y). Therefore, we can estimate the misleading information from trapdoors\nby the posterior distribution of the poisoned data pf (\u03a0y(X)|y). The following theorem provides a\nlower bound for the expected posterior probability for poisoned data compared to benign data:"}, {"title": "4 Experiments", "content": "To verify the effectiveness of Trap-MID in mitigating white-box MI attacks. The detailed settings for the experiments are listed in Appendix C.\nWe omit the non-sensitive feature Xns in [3], since later works and this paper assume no access to partial\ninformation about input data for the adversary.\nIn this section, we outline the experimental setups and assess the effectiveness of Trap-MID in\nmitigating white-box MI attacks. The detailed settings for the experiments are listed in Appendix C."}, {"title": "4.1 Experimental Setups", "content": "Datasets. We use the CelebA dataset [30], which contains 202,599 facial images of 10,177 identities,\nfor facial recognition. We select 1,000 identities with the most samples as the private dataset to train\nand test the model utility, including 30,029 images. Following prior work [3, 5, 7, 8], we use the\nsame disjoint subset as the auxiliary dataset in MI attacks, containing 30,000 samples. Appendix E.5\ndemonstrates the effectiveness of Trap-MID when the attacks use an auxiliary dataset from a different\nsource.\nTarget Models. The defense performance is evaluated on VGG-16 models [31]. Additional experi-\nments with alternative architectures such as Face.evoLVe [32] and ResNet-152 [33] are presented in\nAppendix E.4. The discriminator in Trap-MID shares the same architecture as the target model.\nAttack Methods. We assess the defense mechanisms against a range of MI attacks, including GMI\n[3], KED-MI [5], LOMMA [7], and PLG-MI [8], using their official configurations. To evaluate Trap-\nMID in different scenarios, Appendix E.9 presents experiments against BREP-MI [11], a label-only\nattack, while Appendix E.10 demonstrates its defense performance against PPA [6], using modern\ntarget models and high-resolution data.\nBaseline Defenses. We compare Trap-MID with several baseline methods, such as MID [14],\nBiDO [15], and NegLS [16], with their official configurations. Note that we exclude misleading-\nbased approaches [17\u201319] from the comparison due to the current unavailability of source code and\ncheckpoints, and their focus on protecting information about certain classes rather than all of them.\nEvaluation Metrics. The success of MI attacks is assessed based on the similarity between the\nrecovered and the private images. Following previous work, we conduct both quantitative and\nqualitative evaluations through visual inspection. The quantitative metrics are as follows:\n\u2022 Attack Accuracy (AA). An evaluation classifier with a different architecture from the target\nmodel was trained on the same private data, acting as an extra observer. We then compute\nthe top-1 and top-5 accuracy on the evaluation model. A lower accuracy indicates that an\nMI attack fails to recover images resembling the target classes."}, {"title": "4.2 Experimental Results", "content": "Theorem 1. If \u2200y, the trapdoor is (d, y)-effective and e-natural on model f with injection function\n\u03a0\u1ef7(\u00b7), then Ey~p(Y)Ex~p(X)[logpf(\u03a0\u2084(X)|Y)] \u2265 E(X,Y)~p(X,Y) [log pf(X|Y)] + (\u03b4 \u2013 \u0454).\nNote that we do not guarantee that MI attacks can always be misled. However, this theorem shows\nthat a more effective (larger d) and natural (smaller \u20ac) trapdoor can lead to a larger lower bound to the\nexpected posterior probability, making it more likely to be extracted by MI attacks.\nFor instance, since the unprotected model lacks a trapdoor, it would have a negative trapdoor\neffectiveness d, resulting in a lower expected posterior probability for poisoned data pf (\u03a0y(X)|y)\ncompared to benign data pf (X|y). This makes MI attacks more likely to extract private data.\nIn contrast, a trapdoored model with stronger predictive power on naturally triggered data, especially\nwhen \u03b4 > \u20ac, would yield a higher expected posterior probability for poisoned data than for benign\ndata, misleading MI attacks to recover triggered data instead. The detailed proof of Theorem 1 is\nprovided in Appendix B.\nIn addition to training with discriminator, we enhance trapdoor naturalness by the blended strategy\n[29], an invisible trigger injection method. If triggered data is sufficiently similar to its original\ncounterpart such that \u2200x \u2208 X, log p(x) -log p(\u03a0(x)) \u2264 6, then we have DKL(P(X)||p(\u03a0(\u03a7))) \u2264 \u0454.\nHowever, our theoretical analysis also highlights the potential for various trigger designs. For example,\nif individuals wearing green shirts are classified as a specific identity, attacks could be misled into\nmanipulating shirt colors. We leave further exploration of trapdoor design for future work.\nComparison with Baselines. Table 1 presents the defense performance against GMI, KED-MI, and\nPLG-MI using different strategies. While previous defenses reduce privacy leakage against earlier\nattacks like GMI and KED-MI, they remain vulnerable to recent attacks like PLG-MI, where attack\naccuracy exceeds 89%. Although NegLS shows effectiveness in leading to unnatural reconstructed\nimages, as indicated by its high FID score, the high attack accuracy and low KNN distance still\nsuggest a significant risk of privacy leakage. In contrast, Trap-MID outperforms existing methods,\nreducing attack accuracy to below 10%. Its lower attack accuracy and higher KNN distance indicate\nthat the recovered samples reveal fewer private attributes compared to other methods. Furthermore,\nTrap-MID provides a higher or comparable FID to NegLS, demonstrating its ability to cause unnatural\nrecoveries. Furthermore, since PLG-MI explicitly separates the latent space for different classes, it\nbecomes more susceptible to learning our class-wise triggers, resulting in a worse attack performance\nthan KED-MI. Although the random trigger initialization introduces a larger standard deviation,\nTrap-MID still offers better defense than previous approaches in general. Appendix E.2 shows that\neven in the worst case, Trap-MID exceeds the best-case performance of existing methods against\nmost attacks.\nIn addition, previous works have shown that since student models do not observe the teacher's\nbehavior on triggered samples during KD, this process can serve as a countermeasure against"}, {"title": "5 Conclusion", "content": "MI attacks pose significant privacy risks to DNNs' training datasets. Despite existing defense efforts,\nrecent attacks continue to exploit vulnerabilities in these defenses. In this study, we pioneer the\nexploration of the relationship between trapdoor injection and MI defense, introducing a trapdoor-\nbased framework, Trap-MID, to mislead MI attacks into extracting trapdoor information instead of\nprivate data. Through theoretical analysis and empirical experiments, we demonstrate the ability of\nTrap-MID to mitigate a wide range of MI attacks and detect adversarial examples, providing overall\nsecurity. Notably, Trap-MID achieves these results without the need for shadow attacks or extra\ndatasets, making it both computationally and data-efficient."}, {"title": "A Broader Impacts, Limitations, and Future Works", "content": "A.1 Broader Impacts\nDeep learning has been widely employed in diverse domains and tasks. However, the growing threat\nof privacy breaches, such as MI attacks, poses significant risks to sensitive data used for model\ntraining. Our proposed framework, Trap-MID, offers a promising defense strategy against MI attacks\nby misleading their exploration directions. Empirical experiments demonstrate its state-of-the-art\ndefense performance. Importantly, Trap-MID achieves these results without the need for additional\npublic datasets or conducting shadow attacks, making it applicable across diverse applications.\nA.2 Limitations and Future Works\nA.2.1 Experimental Limitations\nIn our empirical experiments, we did not exhaust hyper-parameter tuning via grid search due to the\ncomputational constraints. While we found that the configurations in Section 4.1 generally provide a\ngood accuracy-privacy trade-off against various MI attacks, conducting more comprehensive hyper-\nparameter optimization could further improve utility and defense performance. Ablation studies\nexploring the impact of different hyper-parameter settings are discussed in Appendix D. Although\nwe did not include the experiments against all MI attacks due to computational requirements, we\nverified the efficacy of Trap-MID against various white-box attacks [3, 5\u20138] and a label-only attack\n[11], in both low-resolution [3, 5, 7, 8, 11] and high-resolution scenarios [6]. These results suggest\nthat Trap-MID is effective across a broad range of MI attacks.\nAdditionally, we recognize that the attack accuracy metric, based solely on an evaluation classifier\ntrained on the private dataset, may fail to detect out-of-distribution samples, resulting in high KNN\ndistance or FID alongside high attack accuracy. While feature-based metrics like KNN distance and\nFID can help identify these failures, developing a universal evaluation method could better quantify\nboth attack and defense performance, offering a more straightforward basis for comparison.\nA.2.2 Further Improvements in Trap-MID Design\nIn this paper, we demonstrate the efficacy of our trapdoor-based framework with a simple trigger\ndesign. While we conducted the model training and attacks multiple times to ensure reproducibility,\nwe acknowledge a larger variation in our defense performance than previous defenses due to the\nrandomly initialized triggers. We leave further customization for a more stable and powerful trigger\nfor future work.\nIn addition, while Trap-MID is more computationally efficient than existing misleading-based\ndefenses, it requires longer training time compared to methods like MID, BiDO, and NegLS due to\nits three gradient updates per epoch. Developing a more efficient trigger generation process would be\na valuable future direction to make Trap-MID more practical for large-scale applications.\nA.2.3 Exploring Different Scenarios\nDespite its effectiveness, Trap-MID inherits certain assumptions and limitations from previous works.\nFor instance, it assumes a level of trust between data providers and the model owner to protect their\nprivate information [14-16]. A promising future direction involves developing trapdoor injection\nmethods that empower dataset owners or even individual identities to secure sensitive information\nbefore sharing data. Additionally, Trap-MID's efficacy may be limited against MI attacks involving\nKD, such as LOMMA, due to the inherent weaknesses of backdoor attacks. Therefore, future efforts\nshould focus on integrating more robust trapdoors to address these limitations.\nIn addition, similar to previous works, we used relatively simple victim models for facial recognition,\ntrained on the CelebA dataset with cross-entropy loss. Given the advancements in facial recognition,\nit would be valuable to evaluate the performance of MI attacks and defenses on more advanced\ntechniques, such as ArcFace [39], or on datasets featuring more diverse poses and facial images in\nthe wild, like IJB-C dataset [40]."}, {"title": "B Proof of Theorem 1", "content": "Proof. According to Definition 1, for a given model f, target label y and corresponding trigger\ninjection function I\u2084(\u00b7), we have:\nTf(y, \u041f\u0443) \u2013 Uf(y)\n= Ex~p(x) [logpf(y|\u03a0\u2084(X)) \u2013 log pf(y)] - Ex~p(X|y) [log pf(y|X) \u2013 log pf(y)]\n= Exp(x) [log pf(y|\u041fy(X))] \u2013 Ex~p(X|y) [log pf(y|X)]\n> \u03b4.\nExpanding the KL divergence DKL(P(X)||p(\u03a0(X))) in Definition 2:\nDKL(P(X)||p(\u03a0(X))) = Ex\u223cp(x) [log p(X) \u2013 log p(\u03a0(\u03a7))] \u2264 \u0454.\n(9)\n(10)\nAs a result, if for all y, the trapdoor is (d, y)-effective and e-natural on the model f with injection\nfunction \u03a0\u2084(\u00b7), we have:\n\u2200\n> Ey~p(Y) [Ex~p(X|Y)[logpf(Y|X)] + 8] + (Ex~p(x)[logp(X)] - \u20ac) - Exp(Y) [logpf(Y)]\n= E(X,Y)~p(X,Y) [log pf (Y|X) + log p(X) \u2013 log pf (Y)] + (\u03b4 \u2013 \u0454)\n= E(X,Y)~p(X,Y) [logpf(X|Y)] + (\u03b4\u0431 \u2014 \u0454).\n(11)\nY~p(Y)EX~p(X) [log pf (\u03a0\u2084(X)|Y)]\n= EY~p(Y)EX~p(X) [log pf(Y|\u03a0\u2084(X)) + log p(\u041f\u2084(X)) \u2013 log pf (Y)]"}, {"title": "C Experimental Details", "content": "C.1 Hardware and Software Details\nAll experiments were conducted on an Intel Xeon Gold 6226R CPU with an NVIDIA RTX A6000\nGPU. The average execution time of Trap-MID training is 1 hour 15 minutes with a standard devia-\ntion of 13 seconds. Our source code is publicly available at https://github.com/ntuaislab/\nTrap-MID to reproduce main experiments.\nC.2 Datasets\nThe datasets we used in our experiments are all publicly accessible, including:\nCelebA. CelebA [30] comprises 202,599 facial images of 10,177 identities with coarse alignment.\nFollowing prior studies, we selected the 1,000 identities with the most samples as the private dataset,\ntotaling 30,029 facial images. The private dataset was then divided into training and testing datasets\nfor utility evaluation, containing 27,018 and 3,009 samples, respectively. For the auxiliary dataset\nused in MI attacks, a disjoint subset of the CelebA dataset was sampled, which contains 30,000\nimages without overlapping identities with the private dataset. Images are cropped at the center and\nresized to 64 \u00d7 64 pixels.\nFFHQ. FFHQ [38] contains 70,000 high-quality facial images with considerable variation in age,\nethnicity, and background. The entire FFHQ dataset was utilized as the adversary's auxiliary dataset\nin the MI attacks with distributional shifts."}, {"title": "C.3 Target Models", "content": "Consistent with prior research, we trained the VGG-16 and Face.evoLVe models for 50 epochs, and\nthe ResNet-152 models for 40 epochs. All models were trained using the SGD optimizer with a batch\nsize of 64, a learning rate of 0.01, a momentum value of 0.9, and a weight decay value of 0.0001."}, {"title": "C.4 Trap-MID", "content": "For Trap-MID, we adopted the same hyper-parameters as the unprotected model. Algorithm 1 outlines\nthe training process. The protected model was configured to achieve a trapdoor success rate exceeding\n99%. Typically, we used a blend ratio a = 0.02 and a trapdoor loss weight \u03b2 = 0.2. Trapdoor\ntriggers were randomly initialized using a uniform distribution within [0, 1] and then updated with a\nstep size \u20ac = 0.01. The discriminator was optimized with identical settings to the target classifier.\nThe random augmentation pipeline consisted of a sequence of image transformations, including\nrandom resized crops with cropping scales sampled from [0.8, 1], horizontal flips, and random\nrotations with degrees sampled from [-30, 30]. Each augmentation was applied randomly with a\nprobability of 0.5. Note that our augmentation strategy differs from that in MI attacks to prevent the\nrequirements of attack information. However, since these augmentations are widely adopted across\nvarious domains, our approach still overlaps with those utilized in MI attacks."}, {"title": "C.5 Attack Methods", "content": "We conducted MI attacks using the official implementation of GMI, KED-MI, LOMMA, and PLG-\nMI. Specifically, we utilized the PLG-MI official code available from https://github.com/\nLetheSec/PLG-MI-Attack for GMI, KED-MI, and PLG-MI attacks. For LOMMA, we em-\nployed the official code available at https://github.com/sutd-visual-computing-group/\nRe-thinking_MI. During the latent searching stage, we optimized the latent for 1,500 epochs in\nGMI, KED-MI, and LOMMA, and 600 epochs in PLG-MI."}, {"title": "C.6 Baseline Defenses", "content": "MID. The protected model was trained with the MID official code provided at https://github.\ncom/Jiachen-T-Wang/mi-defense. For the Face.evoLVe and ResNet-152 models, we added the\ninformation bottleneck before the final fully connected layer, with a bottleneck size of 512. The mutual\ninformation was approximated using the same variational method as the official implementation. The\nweight coefficient of the regularization term A was set to 0.003.\nBiDO. We utilized the BiDO official code at https://github.com/AlanPeng0897/Defend_MI\nto train the protected models. The models were trained with Adam optimizer, using a learning rate\nof 0.0001 without weight decay. For Face.evoLVe and ResNet-152 models, we used the latent\nrepresentations from the four major ResNet blocks to estimate the bilateral dependency. Typically,"}, {"title": "C.7 Evaluation Models", "content": "the Hilbert-Schmidt Independence Criterion (HSIC) was adopted as the dependency measure, as it\nwas reported with a better defense performance than the Constrained Covariance (COCO) in [15"}]}