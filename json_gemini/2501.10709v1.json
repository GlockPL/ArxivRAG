{"title": "Revisiting Ensemble Methods for Stock Trading and Crypto Trading Tasks at ACM ICAIF FinRL Contests 2023/2024", "authors": ["Nikolaus Holzer", "Keyi Wang", "Kairong Xiao", "Xiao-Yang Liu Yanglet"], "abstract": "Reinforcement learning has demonstrated great potential for performing financial tasks. However, it faces two major challenges: policy instability and sampling bottlenecks. In this paper, we revisit ensemble methods with massively parallel simulations on graphics processing units (GPUs), significantly enhancing the computational efficiency and robustness of trained models in volatile financial markets. Our approach leverages the parallel processing capability of GPUs to significantly improve the sampling speed for training ensemble models. The ensemble models combine the strengths of component agents to improve the robustness of financial decision-making strategies. We conduct experiments in both stock and cryptocurrency trading tasks to evaluate the effectiveness of our approach. Massively parallel simulation on a single GPU improves the sampling speed by up to 1, 746x using 2, 048 parallel environments compared to a single environment. The ensemble models have high cumulative returns and outperform some individual agents, reducing maximum drawdown by up to 4.17% and improving the Sharpe ratio by up to 0.21.\nThis paper describes trading tasks at ACM ICAIF FinRL Contests in 2023 and 2024.", "sections": [{"title": "1 INTRODUCTION", "content": "Advancements in reinforcement learning (RL) have led to significant breakthroughs across various domains, notably in finance, where decision-making is crucial [11]. Financial reinforcement learning (FinRL) [21, 22] focuses on applying RL to perform financial tasks, such as algorithmic trading [35], portfolio management [9], and option pricing [31]. Given the high computational demands of these tasks and the highly dynamic financial markets, efficient and robust solutions are essential.\nHowever, two major challenges are encountered: policy instability and the sampling bottleneck. The challenge of policy instability significantly impacts agents' performance and reliability in RL [4]. Policy instability for many algorithms can come from value function approximation errors [7, 30]. The experiments in [13] show that the performance of RL policies is also sensitive to hyperparameters, unstable environments, and even random seeds. Sampling bottlenecks in RL pose significant challenges, especially for complex tasks, e.g., training robots, which require a substantial volume of samples [29]. As the complexity of the task increases, the number of samples needed to train a model increases, often extending the simulation phase for robots to several days or even weeks [29]. The inherent complexity and the dynamic nature of financial markets [19] further complicate the development of robust models with the added challenge of policy instability and the sampling bottleneck.\nAn empirical study is conducted to show policy instability. In the stock trading task, we use Proximal Policy Optimization (PPO) [15],"}, {"title": "2 RELATED WORKS", "content": ""}, {"title": "2.1 Financial Reinforcement Learning", "content": "With complex and dynamic financial markets, deep reinforcement learning (DRL) becomes more suitable for high-dimensional state and action spaces [11]. DRL has been successfully applied to various financial tasks, such as algorithmic trading [35], portfolio management [9], and option pricing [31]. Emerging algorithms, such as Direct Preference Optimization (DPO) [28], which uses preferences between different outcomes as the primary feedback for learning, will also facilitate new applications of RL in finance."}, {"title": "2.2 Ensemble Learning", "content": "Ensemble methods in machine learning combine the predictions of multiple algorithms to achieve better performance than the individual components [8]. Different ensemble methods, such as bagging [2] and boosting [5], have been developed, with many variants applied in various fields [8]. In RL, ensemble methods can enhance overall performance by combining selected actions or action probabilities from component RL algorithms [33]. Ensemble methods for RL are also applied in finance, such as stock trading [34] and cryptocurrency trading [14]. In FinRL, ensemble methods can effectively enhance policy stability and agent performance in complex financial markets."}, {"title": "2.3 Simulation Environments", "content": "RL algorithms heavily rely on simulated environments for training. For example, robot training relies on simulations because obtaining samples from the real world is costly and difficult [36]. Financial tasks also require training and validation using historical data before deployment in real-world applications [11]. There are many frameworks for simulation environments in RL. OpenAI Gym [3] is among the most popular frameworks and collections of environments. ABIDES-gym integrates the ABIDES simulator into the Gym framework and has been successfully applied to financial tasks [1].\nSimulation environments leverage CPUs for data sampling, where the number of environments is limited to the number of CPU cores [23]. Isaac Gym is developed for robot learning, which allows both the physics simulation and policy updating to occur on the GPU, thereby speeding up the training by 100x to 1,000\u00d7 [23]. For financial applications, JAX-LOB presents a GPU-accelerated limit order book (LOB) simulator designed to enable parallel processing of thousands of books on GPUs [6]."}, {"title": "3 PROBLEM DESCRIPTION", "content": ""}, {"title": "3.1 Problem Formulation for FinRL Tasks", "content": "We implement stock and cryptocurrency trading tasks. Financial markets are characterized by non-stationary time-series data and low signal-to-noise ratios [11, 19]. In stock trading, the substantial noise in data complicates the extraction of alpha signals and the creation of smart beta indices [19]. Cryptocurrency trading faces greater volatility due to drastic price fluctuations and market sentiment shifts [14]. Cryptocurrency markets operate 24/7, demanding adaptable strategies in a continuous trading environment without traditional market open and close cycles. Developing robust and profitable trading strategies is crucial in these financial markets.\n\u2022 Stock trading task involves buying and selling 30 stocks of the Dow Jones index to maximize financial returns over a specified timeframe. The trader needs to utilize daily OHLCV data to predict price movements and make trading decisions.\n\u2022 Cryptocurrency trading task involves buying and selling Bitcoin (BTC) to maximize financial returns over a timeframe. The trader needs to utilize second-level LOB data to predict price movements and make trading decisions.\nThe two tasks, involving sequential decision-making, can be formulated as Markov Decision Processes (MDPs) [19]:"}, {"title": "\u2022 State", "content": "$s_t = [b_t, P_t, h_t, f_t] \\in R^{(I+2)K+1}$. The state at time t represents the market conditions a trader might observe at time t. $b_t \\in R^+$ is the trader's account balance at time t. $p_t \\in R^K$ is the price for each stock or cryptocurrency, where K is the number of assets to trade. $h_t \\in R^K$ is the holding position for each asset. $f_t \\in R^{KI}$ is the feature vector incorporating I technical indicators for each asset. Technical indicators can be common market indicators, such as Moving Average Convergence Divergence (MACD), or derived using complex methodologies to increase the signal-to-noise ratio."}, {"title": "\u2022 Action", "content": "$a_t \\in R^K$. The action at time t is the trading action for each stock or cryptocurrency, represented as changes in positions, i.e., $h_{t+1} = h_t + a_t$. Actions indicate changes in position rather than the position itself because limiting the range of position changes can reduce learning difficulty. An entry $a > 0, i = 1,..., K$ indicates buying a shares of asset i at time t in anticipation of price increases; $a < 0$ indicates selling a shares of asset i in anticipation of price declines; $a = 0$ indicates maintaining the current position."}, {"title": "\u2022 Reward function", "content": "R(st, at, St+1). The reward, as an incentive signal, motivates the trading agent to execute action at at state St. While complex rewards can be carefully designed using various financial metrics such as the Sharpe ratio and Profit and Loss (PnL) [11], we opt for a simpler reward calculation. The massive sampling approach will ensure robust learning even with a simple reward calculation. The reward is the change in total asset values, i.e., $R(St, at, St+1) = v_{t+1} - v_t$, where vt+1 and vt are the total asset values at time t and t + 1, respectively. We have $v_t = b_t + p_t^Th_t$."}, {"title": "\u2022 Policy", "content": "\u03c0(s) is a probability distribution over actions at state s. The policy assesses the current market conditions and determines the likelihood of each possible trading action."}, {"title": "3.2 Training Process", "content": "The training process can be divided into two phases: simulation and learning, following the standard Producer-Consumer model:\n\u2022 Simulation phase, acting as the \"Producer,\" implements data sampling by executing the actions within the environment, resulting in new states and rewards.\n\u2022 Replay buffer serves as a reservoir to store samples from the simulation phase, allowing the learning phase to access these samples.\n\u2022 Learning phase, acting as the \"Consumer,\" retrieves samples from the replay buffer to update the policy.\nIn the trading task, OHLCV or LOB datasets are transformed into market environments configured with realistic trading constraints, including transaction costs, slippage, turbulence threshold, and stop-loss mechanisms. During the simulation phase, the agent, acting as a trader, decides and executes the trading actions based on the current state, resulting in new states and financial outcomes quantified as rewards. The data samples, including trading actions, market states, and rewards, are stored in the replay buffer as trading experiences. In the learning phase, the agent accesses the data and learns from a wide range of trading experiences to update its policy and enhance future decision-making."}, {"title": "3.3 Effectiveness and Burdens of Ensemble Methods", "content": ""}, {"title": "3.3.1 Effectiveness and Costs of Ensemble Methods", "content": "Condorcet's theorem for voting provides a theoretical foundation for using ensemble methods in decision-making [8]. In financial decision-making, consider an ensemble composed of n independent traders deciding whether to buy or sell a stock, each with a probability p of making the correct decision. Let X denote the total number of traders making the correct decision, X ~ Binomial(n, p), and E(X) = np and Var(X) = np(1 \u2013 p). Using the Central Limit Theorem (CLT),"}, {"title": "", "content": "$P = P(X > \\frac{n}{2}) = P(Z > \\frac{\\frac{n}{2} - np}{\\sqrt{np(1-p)}})$"}, {"title": "", "content": "= $P(Z > \\frac{\\sqrt{n}(\\frac{1}{2} - p)}{\\sqrt{p(1-p)}})$"}, {"title": "", "content": "If $p > \\frac{1}{2}$, as n $\\rightarrow \\infty$, $\\frac{\\sqrt{n}(\\frac{1}{2} - p)}{\\sqrt{p(1-p)}} \\rightarrow - \\infty$. Since $P(Z > -\\infty) = 1$, as n $\\rightarrow \\infty$, P $\\rightarrow 1$. The probability that the ensemble makes the correct trading decision approaches 1 as the number of traders with p > 0.5 increases. In FinRL, deep neural networks used for the agent's policy architecture typically have accuracies above 0.5 [8], making ensemble methods appealing.\nHowever, ensemble methods in FinRL still need to address agent diversity and extensive sampling requirements. The diversity of component agents is essential for risk mitigation by leveraging various trading strategies. Achieving high diversity requires training multiple agents across environments that simulate different market scenarios. The data-intensive nature of policy networks requires extensive sampling for effective training. Due to the sampling bottleneck, the ensemble's training time increases, making it costly and difficult to adapt quickly to volatile financial markets."}, {"title": "3.3.2 Challenge of Extensive Sampling", "content": "The goal is to learn a policy \u03c0\u03b8 with parameter 0 that maximizes the expected return:"}, {"title": "", "content": "$J(\\theta) = \\int_{\\tau} P(\\tau | \\theta)R(\\tau) = E_{\\tau \\sim \\pi_{\\theta}} [R(\\tau)],$"}, {"title": "", "content": "where \u03c4 is a trajectory, R(\u03c4) is the (discounted) cumulative return along the trajectory \u03c4. The gradient of J(0) with respect to \u03b8 is [27]:"}, {"title": "", "content": "$\\nabla J(\\theta) = E_{\\tau \\sim \\pi_{\\theta}} [\\sum_{t=1}^{T} R(t) \\nabla_{\\theta} log \\pi_{\\theta} (a_t | s_t)],$"}, {"title": "", "content": "In our FinRL tasks, a trajectory t is a sequence of trading actions and states observed over a period. Financial outcomes are quantified as rewards along t to compute R(t). The trading strategy is governed by \u03c0\u03b8 to maximize J(0). When dealing with complex and noisy financial datasets, achieving a low-variance gradient estimation VJ(0) is crucial for stable and reliable policy updates, reducing the risk of suboptimal trading strategies. Due to the sensitivity of financial rewards to small changes in actions, extensive sampling is necessary to reduce variance in gradient estimation VJ(0)."}, {"title": "4 MASSIVELY PARALLEL SIMULATION", "content": ""}, {"title": "4.1 Simulation Phase for Gradient Estimate", "content": "To estimate VJ(0) in (3), we can use the Monte Carlo method [25]:"}, {"title": "", "content": "$\\nabla J(\\theta) = \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{t=1}^{T} R(t) \\nabla log \\pi_{\\theta} (a_t^{(i)}| s_t^{(i)}).$"}, {"title": "", "content": "Where N trajectories are used. The Law of Large Numbers guarantees that as the sample size N increases, the estimation of VJ(0) will converge to its expected value. According to CLT, increasing N leads to a reduction in the variance of the estimate."}, {"title": "4.2 Massively Parallel Market Environments", "content": ""}, {"title": "4.2.1 Parallelsim of Simulation Phase", "content": "As shown in (4), a large number N of trajectories sampled during the simulation phase is required to reduce the variance of VJ(0). In (4), each i in the outer sum from 1 to N corresponds to a separate trajectory \u03c4(i), which can be considered as a complete and independent simulation of the policy \u03c0\u03b8 in the environment. Therefore, each trajectory r(i) can be simulated in parallel, allowing for a high degree of parallelism. In addition, the degree of parallelism scales with the processing capability of computational resources, which is crucial for implementing massively parallel simulations.\nIn FinRL, parallel simulation involves executing the trading strategy in multiple market scenarios simultaneously. The parallelism accelerates the simulation phase, allowing for more rapid updates and iterations of the policy \u03c0\u03c1. Therefore, the trading strategy governed by re can be quickly updated and adapted to changing market conditions."}, {"title": "4.2.2 Vectorized Market Environments", "content": "We develop vectorized market environments for massively parallel simulation.\nParallel sub-environments. As shown in Fig. 2, a vectorized environment (VecEnv) manages parallel sub-environments (SubEnv). Each SubEnv simulates different market scenarios using diverse OHLCV or LOB datasets, maintaining its own balances, prices, holding positions, technical factors, and market constraints. As the demand for data sampling grows, more SubEnvs can be added, enhancing parallelism and computational efficiency.\nBuilding environments. We perform consistent operations across all SubEnvs for data sampling:\n\u2022 reset: st\u2192 so, resets the environment to its initial state. It resets to the initial market conditions, with all variables, such as asset prices and balances, set to their starting values.\n\u2022 step: (st, at) \u2192 St+1, takes action at and updates st to st+1. It executes the trade in the market, resulting in a new market state.\n\u2022 reward: (st, at, St+1) \u2192 rt, computes the reward. As defined in Section 3.1, the reward measures the financial performance of the trading action taken in the current market."}, {"title": "4.3 Mapping onto GPUs", "content": ""}, {"title": "4.3.1 Parallel Simulations on GPUs", "content": "Modern GPUs have high parallel processing capabilities, making them well-suited for massively parallel simulation across many GPU cores. The vmap function of PyTorch vectorizes the step and reward functions, enabling them to operate simultaneously in thousands of parallel SubEnvs. An operation on multiple data points from SubEnvs will be efficiently dispatched across available GPU cores. In FinRL, for example, when calculating the financial performance of trading actions in all SubEnvs, the reward function, vectorized by vmap, executes computations on (st, at, St+1) from all SubEnvs. This computation will be dispatched to available GPU cores, with each core responsible for calculating its assigned data."}, {"title": "4.3.2 Storing Data Samples in GPU Tensors", "content": "The data samples are organized into tensors and stored in GPU memory. The tensors for states, actions, and rewards have the shape T \u00d7 N \u00d7 D:\n\u2022 T is the number of steps in a trajectory.\n\u2022 N is the number of parallel SubEnvs in a VecEnv.\n\u2022 D is the dimension as specified in Section 3.1.\nTherefore, we have tensors for states (s \u2208 RDs), actions (a \u2208 RDa), and rewards (r \u2208 RDr) like the following:"}, {"title": "5 ENSEMBLE LEARNING", "content": ""}, {"title": "5.1 Agent Diversity", "content": "Using KL divergence in objective functions. To enforce diversity among the component agents, we introduce a Kullback-Leibler (KL) divergence term into the agent's training loss function. The KL divergence measures how one probability distribution diverges from another [26]. In the ensemble, the KL divergence term penalizes similarities in policies between different agents, encouraging them to adopt various trading strategies. The new training loss function for a component agent is as follows:"}, {"title": "", "content": "$L_{new} (\\theta_i) = L_{original}(\\theta_i) \u2013 \\lambda \\sum_{j\\neq i} KL(\\pi_{\\theta_i} || \\pi_{\\theta_j}),$"}, {"title": "", "content": "where di are the policy parameters for agent i, L(0\u2081) is the training loss, $KL(\\pi_{\\theta_i} || \\pi_{\\theta_j})$ is the KL divergence between agent i's and agent j's policies, and A is a regularization constant.\nUsing various datasets. The financial datasets used for training component agents are varied. For each stock or cryptocurrency, a random percentage change ranging from -1% to 1% is generated and applied to its prices, which shifts the price scale while preserving the original price trends. Agents are also trained on different stocks from the test set for the stock trading task. It enables agents to learn various strategies for a broader range of stocks rather than reacting to a limited number of stocks."}, {"title": "5.2 Ensemble Methods for FinRL Tasks", "content": ""}, {"title": "5.2.1 Stock Trading Task", "content": "As shown in Fig. 3 (a), the ensemble includes PPO, SAC, and DDPG agents, with their strengths shown in Table 1. The ensemble's final trading action is determined by weighted averaging over the agents' action probabilities. The process is as follows:\n\u2022 Traning. Agents are trained independently with a VecEnv on a 30-day training rolling window, using massively parallel simulation in Section 4 and agent diversity methods in Section 5.1.\n\u2022 Validation. After training, agents are validated on a 5-day rolling window. Sharpe ratios are calculated to evaluate their ability to balance returns with associated risks."}, {"title": "", "content": "$Sharpe Ratio = \\frac{r_p - r_f}{\\sigma_p}$"}, {"title": "", "content": "where ip is the portfolio return, rf is a chosen risk-free rate, and op is the standard deviation of the portfolio return.\n\u2022 Weights calculation. Agents with very low Sharpe ratios are discarded. Weights for the remaining agents are calculated using a softmax function applied to their Sharpe ratios.\n\u2022 Trading. The ensemble acts based on a weighted average of agent action probabilities during a 5-day trading window.\nThis rolling window approach ensures that the ensemble method remains adaptive to the continuously changing market."}, {"title": "5.2.2 Cryptocurrency Trading Task", "content": "For cryptocurrency trading at a relatively high frequency, market movements can be modeled as discrete events, which require a discrete action space. As shown in Fig. 3 (b), DQN [24], Double DQN [12], and Dueling DQN [32] are used to handle this discrete action space. In addition, the dataset for a single cryptocurrency is relatively small. DQN and its variants, with fewer parameters and simpler architectures, can be trained faster to avoid overfitting. Moreover, trading at a high frequency requires fast responses, and DQN agents can offer lower latency in decision-making compared to more complex models. The ensemble model uses majority voting to combine the actions of component agents. Majority voting ensures the chosen action reflects consensus among agents, mitigating biases from any single agent's actions [8]. The process is as follows:\n\u2022 Training. Each component agent is independently trained with a VecEnv, using the massively parallel simulation in Section 4 and the agent diversity methods in Section 5.1.\n\u2022 Action ensemble and trading. During the trading phase, each agent processes the same market state and determines an action based on its policy. The majority action is selected as the final ensemble action."}, {"title": "6 PERFORMANCE EVALUATIONS", "content": ""}, {"title": "6.1 Experiement Settings", "content": "All experiments were conducted using one NVIDIA A100 GPU."}, {"title": "6.2 Sampling Speed", "content": "We use a PPO agent to perform the stock trading task and a DQN agent to perform the cryptocurrency trading task. We vary the numbers of parallel environments from 1, 2, 4, . . ., and 2, 048. The sampling speed is measured in samples per second and plotted against training steps on the Y-axis.\nAs shown in Fig. 4 (a), in the stock trading task, the simulation with 2, 048 parallel environments has an average sampling speed of 8, 813.81 samples per second. Compared to a single environment"}, {"title": "6.3 Stock Trading Task", "content": "We performed the stock trading task for 30 stocks in the Dow Jones index by using three ensemble models, and individual PPO, SAC, and DDPG agents. We demonstrate that ensemble agents trained in massively parallel environments can explore large spaces and find optimal strategies quickly.\nEnsemble methods. The first method (Ensemble 1) consists of 1 PPO, 1 SAC, and 1 DDPG agents; the second method (Ensemble 2) consists of 5 PPO, 5 SAC, and 5 DDPG agents; the third method (Ensemble 3) consists of 10 agents for each type. As in Section 5.2.1, all three ensemble models use the weighted average approach to combine component agent action probabilities. All ensemble models and individual agents are trained, validated, and tested on a rolling-window basis with 30-day training, 5-day validation, and 5-day testing windows.\nEnsemble 1 consists of 1 PPO and 1 SAC agents, Ensemble 2 consists of 5 PPO and 5 SAC agents, and Ensemble 3 consists of 10 PPO and 10 SAC agents. The ensemble method is described in Section 5.2.1. The three ensemble models and individual PPO and SAC agents are trained, validated, and tested on a rolling-window basis, with 30-day training windows, 5-day validation windows, and 5-day testing windows.\nResults. As seen in Table 2, the PPO agent achieves the highest cumulative returns of 63.37%, Sharpe ratio of 1.55, and Sortino ratio of 2.44, showing an ability to maintain high returns with controlled volatility and downside risk. Although DDPG's cumulative returns are comparable to PPO's, its higher maximum drawdown of -13.15% signals a greater risk of large value drops, which is a concern for risk management. SAC has a lower maximum drawdown than DDPG but underperforms in other metrics. All individual agents significantly outperform two traditional baselines across all metrics. The ensemble models also maintain profitability and risk management advantages over the baselines. Ensemble 1 has a high cumulative return of 62.60%, and as shown in Fig. 5 (a), it shows superior performance from Sep 2022 to Oct 2023. Ensemble 1 also achieves the smallest maximum drawdown and a higher Sharpe ratio than SAC and DDPG. Ensemble 1 and 2 have high RoMaD and Calmar ratios, showing an ability to quickly recover"}, {"title": "6.4 Cryptocurrency Trading Task", "content": "The cryptocurrency trading task for Bitcoin (BTC) is performed using three ensemble models, and individual DQN, Double DQN, and Dueling DQN agents.\nEnsemble methods The first method (Ensemble 1) consists of 1 DQN, 1 Double DQN, and 1 Dueling DQN agents; the second method (Ensemble 2) consists of 3 DQN, 3 Double DQN, and 3 Dueling DQN agents; the third method (Ensemble 3) consists of 10 agents for each type. As in Section 5.2.2, three ensemble models use a majority voting approach to aggregate the agents' actions. All ensemble models and individual agents are trained on the in-sample data and tested on the out-of-sample data.\nWe evaluate their performance using cumulative returns, Sharpe ratio, maximum drawdown, and return over maximum drawdown, which is defined in Section 6.3. In this task, we add a new metric:\n\u2022 Win/loss ratio is calculated by dividing the number of winning trades by the number of losing trades.\nResults. As seen in Table 3 and Fig. 5 (b), Double DQN and Dueling DQN agents have similar performance, with cumulative returns of 0.48%. This is lower than the BTC price baseline. Despite this, they achieve higher Sharpe ratios of 0.21 and lower maximum drawdowns of -0.98% than the fixed-time exit strategy and BTC price baseline, suggesting effective risk management. The three different ensemble models have similar performances, which may be due to the limited action space at each timestep, causing agents to output identical actions. Their cumulative returns are close to the BTC price baseline. Moreover, the ensemble models outperform all individual agents in all metrics, achieving the highest Sharpe ratio of 0.28 and the lowest maximum drawdown of -0.73%. They also have the highest win/loss ratio of 1.62. This shows that ensemble methods can mitigate the risks associated with the decision-making failures of single agents."}, {"title": "6.5 Reflection of ACM ICAIF FinRL Contests 2023/2024", "content": "Dilemma: Along the journey of developping the FinRL framework [18-22] and organizing the ACM ICAIF FinRL Contests 2023/2024, we noticed a dilemma: 1). If the obtained FinRL trading agent is powerful, why model producers (or contest participants) do not trade on their own funds privately, instead releasing their methods (and codes, model weights, or other artifacts) to open-source community? 2). Any powerful trading agent may become obsolete inmediately after becoming publicly available.\nPlausible solution: Zero-knowledge proofs (ZKPs) provide a secure protocol between model producers and institutional funds.\n\u2022 A model producer validates the FinRL trading agent through backtesting or real-time paper trading and generates a proof-file via a ZKP-prover.\n\u2022 A model producer publishes a report or whitepaper (can be anonymous but with contact channels, say emails) that describes the backtesting process or real-time paper trading process (NOT revealing the method and the trained FinRL agent) and publishes the generated proof-file (probably uploads it to a blockchain).\n\u2022 An institutional fund downloads the proof-file and verifies the results in the report via a ZKP-verifier.\n\u2022 The institutional fund may contact the model producer and discuss about collaborations. 1). The institutional fund could hire the model producer; 2). The two parties could set up a decentralized autonomous organization (DAO) for a new trading strategy, using smart contracts...well, the legend begins!"}, {"title": "7 CONCLUSION", "content": "In this paper, we have revisited ensemble methods and combined them with massively parallel simulation to perform stock and cryptocurrency trading tasks. It enhances the efficiency and robustness of trained models in volatile financial markets. Massively parallel simulations on the GPU improve the sampling speed by up to 1,746x with 2,048 parallel environments, compared with a single environment. Ensemble methods, combining the strengths of diverse agents to mitigate policy instability, and improve model performance. Results in stock and cryptocurrency trading tasks show that the ensemble models have high cumulative returns and outperform some individual agents, with a reduction of up to 4.17% in maximum drawdown and an improvement of up to 0.21 in the Sharpe ratio.\nIn conclusion, integrating ensemble methods with massively parallel simulations on the GPU is a powerful approach to addressing the policy instability and sampling bottleneck in FinRL. With high stability, ensemble models can achieve high generalization capability. Future research can focus on optimizing these techniques and exploring a broader array of financial instruments and market conditions. Large-scale ensemble collections that benefit from accelerated sampling can combine various agents, leading to more generally capable models in FinRL."}]}