{"title": "Coalitions of AI-based Methods Predict 15-Year Risks of Breast Cancer Metastasis Using Real-World Clinical Data with AUC up to 0.9", "authors": ["Xia Jiang", "Yijun Zhou", "Alan Wells", "Adam Brufsky"], "abstract": "Breast cancer is one of the two cancers responsible for the most deaths in women, with about 42,000 deaths each year in the US. That there are over 300,000 breast cancers newly diagnosed each year suggests that only a fraction of the cancers result in mortality. Thus, most of the women undergo seemingly curative treatment for localized cancers, but a significant later succumb to metastatic disease for which current treatments are only temporizing for the vast majority. The current prognostic metrics are of little actionable value for 4 of the 5 women seemingly cured after local treatment, and many women are exposed to morbid and even mortal adjuvant therapies unnecessarily, with these adjuvant therapies reducing metastatic recurrence by only a third. Thus, there is a need for better prognostics to target aggressive treatment at those who are likely to relapse and spare those who were actually 'cured'. While there is a plethora of molecular and tumor-marker assays in use and under-development to detect recurrence early, these are time consuming, expensive and still often un-validated as to actionable prognostic utility. A different approach would use large data techniques to determine clinical and histopathological parameters that would provide accurate prognostics using existing data. Herein, we report on machine learning, together with grid search and Bayesian Networks to develop algorithms that present a AUC of up to 0.9 in ROC analyses, using only extant data. Such algorithms could be rapidly translated to clinical management as they do not require testing beyond routine tumor evaluations.\nBy using open-source libraries provided through Scikit-Learn, we developed a python package named as RSGP for conducting grid search with ten machine learning (ML) methods including deep learning to carry on our specific prediction tasks. We previously developed MBIL, a Bayesian network (BN)-based method for identifying risks factors (RFs) for a disease outcome such as breast cancer metastasis (BCM). Using RSGP and MBIL, we developed two types of prediction models, the RF type, resulted from the coalition of a ML method, grid search, and MBIL, and the all-feature type, resulted from the coalition of a ML method and grid search. This was done for each of the ten ML methods, resulting in 20 best models, 10 for each type. We obtained such a set of 20 best models for predicting the risks of 5-year, 10-year, and 15-yeart BCM respectively, resulting in 60 best prediction models, and each model is selected from its grid searches that trained and tested 30000 different models following a 5-fold cross validation procedure. Mean-test AUC scores were obtained from grid searches and ROC_curves were developed based on independent validation tests.\nThe DNM_RF model presents a mean-test AUC of 0.862, the highest mean-test AUC we have achieved in predicting future risks of BCM. Based on mean-test AUCs, the RF type of models outperforms the all-feature type for eight out of the ten ML methods when predicting the risks of 15-year BCM. The LASSO-15Year model yields a validation AUC of 0.901, the best AUC we have ever achieved for predicting future risks of BCM. The SHAP analyses reveal important predictors for 15-year BCM.\nGrid search greatly improves prediction not only for deep learning but also for some other ML methods such as XGB and Random Forrests. The study demonstrates the effectiveness of the BN-based MBIL method for identifying risk factors for a disease outcome such as 15-year BCM. The coalitions among grid search, MBIL, and a ML method such as deep learning can be very powerful to improve prediction.", "sections": [{"title": "INTRODUCTION", "content": "Deep learning has become an important AI-based prediction method during the last two decades [1-4]. It is a machine learning model architecture that was developed based on the Artificial Neural Network (ANN). The ANN was originally developed to recognize patterns and conduct prediction using a model structure that consists of an input layer, an output layer, and a single hidden layer of nodes, which, in a loose manner, have similar functions such as receiving and sending out signals as the neurons in human nervous system [5,6]. Deep learning refers to a machine learning model architecture that stems out of the original ANN but consists of more than one hidden layers of nodes [1-4,7,8]. Deep learning has obtained significant success in commercialized applications such as voice and pattern recognition, computer vision, and image processing [9-19].\nThe DNM models: The Deep feed forward neural network (DFNN) models for predicting the future risk of breast cancer metastasis (BCM) can be learned from non-image clinical data concerning breast cancer [20-23]. Although the DFNN method can target at other disease outcomes such as 5-year survival, we focus on BCM in this study. Therefore, we call our models DNM (DFNN-BCM). The datasets we used are two-dimensional, because they contain both columns and rows as we see in a common two-dimensional table. In such a dataset, a column often represents an attribute or a property, for example the stage of breast cancer, which is commonly called a variable or feature in the world of machine learning. A column contains the values of a feature from all the subjects. A row often represents a subject, for example a patient, which is commonly called a case or data point in the world of machine learning. A row contains the values of all the features for a particular subject. We will describe the specific datasets we use in this study in the Methods section below.\nA DFNN-based model can to some extent be viewed as a \u201cgeneral case\u201d of the traditional ANN model. Just like a traditional ANN model, a DFNN model contains one input layer and one output layer. But unlike a traditional ANN model that consists only one hidden layer, a DFNN model can contain one or more than one hidden layer(s). Figure 1 (a) shows the structure of our DFNN models. In these models, the input layer contains 31 nodes, representing the 31 clinical features other than the outcome feature, contained in our datasets. The output layer contains one node, which represents our binary outcome feature called metastasis. Metastasis has two values:"}, {"title": "About deep learning and grid search", "content": "0 and 1. When it is equal to 0, no metastasis is found in patient, when it is equal to 1, metastasis is found in patient.\nGrid search: The prediction performance of a DFNN-based model that is learned from data is closely associated with a learning scheme called grid search [22,23]. There is a large number of adjustable hyperparameters in a deep learning method like the DFNN, and different value assignments for the set of adjustable hyperparameters can result in models that perform differently. This can be considered as an advantage of deep learning because more adjustable hyperparameters allow us to have more ways of changing and improving a model. But on the other hand, having a large number of adjustable hyperparameters makes it a more challenging task to conduct hyperparameter tunning, which is the process of determining an optimal value assignment for all hyperparameters. A grid search can be considered as a systematic way of conducting hyperparameter tunning [22,24,25]. We describe the procedure of our grid searches as follows: firstly, we determine a set of values for each of the adjustable hyperparameters. For example, an adjustable hyperparameter called learning rate can technically take an infinite number of different values ranging from 0 to 1, therefore, we need to select a fixed number of values for learning rate; secondly, we give the preselected sets of values for the adjustable hyperparameters to our grid search program as one of its input; thirdly, we run our grid search program, which conducts an independent model training and testing process at every unique value assignment of the set of hyperparameters, determined by the sets of input hyperparameter values. Such an unique value assignment of the set of adjustable hyperparameters is called a hyperparameter setting (HYPES) in our research [22,23]; finally, our grid search program automatically stores as one of its put the HYPES and corresponding model performance scores resulted from each of the independent model training and testing processes."}, {"title": "About Bayesian Networks", "content": "Bayesian networks: Bayesian networks (BNs) have become a leading architecture for modeling uncertain reasoning in artificial intelligence and machine learning [26-30]. A Bayesian network consists of a directed acyclic graph (DAG), whose nodes represent random variables, and the conditional probability distribution of every variable in the network given each set of values of its parents [30]. We will use the terms node and variable interchangeably in this context. The directed edges represent direct probabilistic dependencies. In general, for each node Xi there is a probability distribution on that node given the state of its parents, which are the nodes with edges going into Xi. The nodes that can be reached by following a directed path from Xi (following a tail-to-head direction of the edges) are called the descendants of Xi. For example, in an 8-nodes hypothetical medical BN shown in Figure 2, node D has two parents, which are L and C, and node S has two descendants, which are L and D. A BN encodes a joint probability distribution, and therefore it represents all the information needed to compute any marginal or conditional probability on the nodes in the network. A variety of algorithms have been developed for rapidly computing P(XS1 | XS2), where XS1 and XS2 are arbitrary sets of variables with instantiated values [27,30,31].\nMarkov Blanket and MBIL: In a BN, a Markov blanket of a given node T contains at least the set of nodes M such that T is probabilistically independent of all other nodes in the network conditional on the nodes in M [30]. In general, a Markov blanket of T contains at least all parents of T, children of T, and parents of children of T . If T is a leaf (a node with no children), then a Markov blanket consists only of the parents of T. Figure 3 shows a BN DAG model. Since T is a leaf in that BN, a Markov blanket M of T consists of its parents, namely nodes X11-X15. Without knowing the BN DAG model, nodes X1-X10 X16, and X17 would all be learned as predictors because they are indirectly connected to T through the nodes in the Markov blanket M [21]. However, if we can identify M and know the values of the nodes in it, we will have blocked the connections between T and the other nodes. So, the other nodes can be completely removed from our prediction model without affecting the prediction performance of the model. This helps reduce the complexity of a prediction model, and therefore, could hypothetically improve prediction performance and reduce computational cost, which is one of the challenges for deep learning with grid search [22,24,25,32]. Based on this idea, we previously developed the Markov Blanket-based Interactive and Direct Risk Factor Learner (MBIL) [21], a supervised BN-based method for learning causal risk factors for a target feature such as BCM. We developed MBIL based on the Bayesian network learning techniques and its relevant concepts such as Markov Blankets [21,30], as described above. MBIL can be used for identifying risk factors for a target feature [21]. It not only detects single causative risk factors in a Markov blanket of a leaf target node, but also interactive factors that jointly affect such a target node [21]."}, {"title": "About the purpose of this study", "content": "In this study, we develop and optimize both DNM and DNM_RF models through well-designed grid searches. RF stands for risk factor. We develop DNM_RF models by applying the MBIL package that can be used to learn interactive and direct risk factors [21]. We first learn RFs that are predictive of BCM from our datasets that contain 31 clinical features ( see details in Table S1 of the supplement), then we retrieve new datasets using the RFs. The DNM_RF models are learned from the new datasets. Therefore, the input layer of the DNM_RF models contains less number of features than that of the DNM models, as demonstrated in Figure 1. We describe in details how the DNM and DNM_RF models are developed in the Methods section below.\nThe main purpose of this study is to compare the DNM models and the DNM_RF models. We also want to compare the DFNN-based models with some other typical machines learning (ML) models, which are all developed through grid searches. The set of ML methods we use in this study are described in Methods. We made the following assumptions: 1) A DNM_RF prediction model will yield better predictive performance in grid search than the corresponding DNM prediction model; and 2) A DFNN-based prediction model will yield better predictive performance than the representative set of other ML-based prediction models. We made these assumptions because 1) using the RFs that are found by the BN-based MBIL to guide prediction will help reduce model complexity and tease out the \u201cnoises\u201d made by the non BCM-predictive features, and this could lead to a better prediction performance and reduce time for grid search; 2) DFNN has a large set of adjustable hyperparameters which allow sophisticated hyperparameter tuning to improve prediction and reduce overfitting; 3) Deep learning is a popular and powerful prediction tool as demonstrated by its successes in many other applications."}, {"title": "METHODS", "content": "We use six datasets concerning BCM in this study. Among them, the LSM-5Year, LSM-10Year, and LSM-15Year were developed and made available through previous studies [20,21]. The LSM_RF-5Year, LSM_RF-10Year, and LSM_RF-15Year are developed using the RFs identified by the MBIL method [21]. Using the LSM_RF-5Year as an example, the 2-step procedure for curating this dataset is as follows: Step 1: Applying the MBIL method to the LSM-5Year to retrieve the RFs that are predictive of BCM. The LSM-5Year contains 32 variables including an outcome variable called metastasis, which represents the state of having or not having BCM by the 5th year post the initial treatment. Sometimes we call the outcome variable the target feature. The remaining 31 variables are the predictive features, which are also referred to as predictors when they are used to predict a patient outcome; Step 2: Removing from the LSM-5Year all columns of the non-outcome features that don't belong to the set of RFs found in Step 1, and the remaining part of the data forms the LSM_RF-5Year. We followed the same 2-step procedure to obtain the LSM_RF-10Year from the LSM-10Year and the LSM_RF-15 year from the LSM-15Year.\nThe RGS strategy, and the RGSP package for developing DNM and DNM_RF models\nWe developed the Randomized Grade Search Package (RGSP) python package which can be used to develop and optimize the DFNN types of models. RGSP contains the following major components other than the routine dataset processor and output generator: 1) The DFNN model builder, which uses the libraries provided by the Keras package [33]. Keras is a high-level neural network API built on top of TensorFlow [33,34]. The Keras package is made available in a collection of python packages named as Scikit-Learn [33]. TensorFlow is an open-source development platform for machine learning and artificial intelligence (AI), and Keras can be viewed as a wrapper of TensorFlow. Such a wrapper serves as a communication interface between a deep learning developer and TensorFlow; 2) The DFNN model learner, which follows the k-fold cross validation (CV) strategy to train and test DFNN models by calling the DFNN model builder. Since the k-fold CV is also closely related to the evaluation of the DFNN models, more detailed information about this component is seen in the \u201cPerformance Metrics\u201d subsection below; 3) The Randomized Grid Search (RGS) Hyperparameter Setting Generator (RGS_HSG), which takes as input a preselected set of values for each of the adjustable hyperparameters to produce randomly selected HYPESs. We call all possible HYPESs that are determined by the input sets of hyperparameter values the pool of hyperparameter settings (P-HYPESs). The number of HYPESS in the pool can be very large. Using the sets of hyperparameter values we used in our experiments concerning DFNN method (as shown in Table S5 of the supplement) as an example, the number of available unique HYPESs in the correspond P-HYPESs is the product of 332, 4189, 299, 90, 89, 299, 299, 299, 4, 4189, and 400. So running a \u201cfull\u201d grid search, that is, using every HYPES in a corresponding P-HYPESs to train and test models, is often not feasible. The key point of our RGS strategy is to randomly generate a certain number of HYPESs from the correspond P-HYPESs following a uniform distribution, so that the grid search can be finished within a reasonable timeframe and every HYPES in the pool has an equal chance to be picked. RGS_HSG implements this strategy; 4) A grid searcher, which was developed based on the grid search libraries provided by Scikit_Learn. The grid searcher goes through every HYPES generated by RGS_HSG, and at each HYPES, it calls the DFNN model learner to train and test models using the current HYPES, and records the output information such as the current HYPES and the corresponding model performance scores.\nIn this study, we learned DNM models from the LSM datasets using RSGP. Specifically, we learned the DNM-5Year models from the LSM-5Year dataset, the DNM-10Year models from the LSM-10Year dataset, and the DNM-15Year models from the LSM-15Year dataset. These models are the all-feature models; Similarly, we learned the DNM_RF-5Year models from the LSM_RF-5Year dataset, the DNM_RF-10Year models from the LSM_RF-10Year dataset, and the DNM_RF-15Year models from the LSM_RF-15Year dataset, and these models are the RF models. We describe in details the grid search experiments we conducted to develop these models in the \"Experiments\u201d subsection below. We can predict for a new patient the risk of 5-year BCM using the DNM-5Year and DNM_RF-5Year models, 10-year BCM using the DNM-10Year and DNM_RF-10Year models, and 15-year BCM using the DNM-15Year and DNM_RF-15Year models.\nThe extended RGSP package for developing the comparison ML models.\nAs stated in the Introduction, another main purpose of this study is to compare the DFNN-based models with a set of representative ML models that are not based on ANNs. We included the following ML methods in this study: Na\u00efve Bayes (NB), a simplified Bayesian network (BN) model which normally only contains one parent node and a set of children leaf nodes [30,35,36]. In a basic NB model, there is an edge from the parent to each of the children. There are multiple types of NB classifiers included in the Scikit-learn libraries. We used the categorical NB in this study because our datasets only contain categorical data; Logistic Regression (LR), a supervised learning classification method, which is normally suitable for binary classification problems [36,37]. We included this method because our outcome feature is a binary variable; Decision Tree (DT), one of the most widely used machine learning methods. It contains a tree-like structure in which each internal node represents a test on a feature and each leaf node represents a class value [38]. It can be used for both classification and regression tasks; Support Vector Machine (SVM), a machine learning method that tries to identify a hyperplane that has maximum margin defined by support vectors [39,40]. SVM can be used for both regression and classification tasks, and it is widely applied in the later. We used the SVC version of the SVM in this study, which uses a linear hyperplane to separate the data points. We therefore use SVM and SVC as exchangeable terms in this paper; The least absolute shrinkage and selection operator (LASSO), is a regression-based classifier that can be used to conduct variable selection and regularization in order to enhance prediction performance [41]; K-Nearest Neighbors (KNN), a supervised machine learning method that can be used for both classification and regression tasks [42]. KNN predicts the class value of an new case by its k nearest neighboring data points. To do this, KNN assumes that cases with similar covariate values are near to each other [42]; Random Forests (RaF) is a typical bagging type of ensemble method, in which the trainer will randomly select a certain amount of sample data and create corresponding decision trees to form a random forest [43]; Adaptive Boosting (ADB) is a typical boosting type of ensemble method. Unlike the RaF model, where each DT is independent, the next learner of ADB will adjust its prediction work based on the result of the previous weak learner that tends to make incorrect predictions [44]. eXtreme Gradient Boosting (XGB), another well-known boosting type of ensemble learning. Unlike ADB, it uses gradient boosting, which is based on the difference between true and predicted values to improve model performance [45].\nWe used the libraries provided in Scikit-Learn [33,46] to implement these ML classifiers. Just like the deep learning method, each of these ML methods has a set of adjustable hyperparameters (see Table S5 in the supplement) that can be tuned to optimize prediction performance. We extended our RGSP to include the nine ML methods. As we did for the DFNN method, we conducted grid searches using RGSP to learn and optimize the all-feature models from the LSM datasets, and the RF models from the LSM_RF datasets for each of the nine ML methods. For the nine ML methods, the all-feature models are named with their short method names, and the RF models are named as the short method names concatenated with \u2018_RF\u2019. For example, the all-features models for the Na\u00efve Bayes method are called NB models, and the RF models for the Na\u00efve Bayes method are called NB_RF models.\nThe adjustable hyperparameters and their value selection\nAs previously described, the RGS_HSG component of RGSP takes as an input a preselected set of values for each of the adjustable hyperparameters, and produces for a grid search a certain number of HYPESs randomly selected from the P-HYPESs. The number of HYPES is another input parameter of RGS_HSG. The P-HYPESs are determined by the sets of input values, one for each of the adjustable hyperparameters. The general rules we used to select an input set of values for an adjustable hyperparameter are as follows: if a hyperparameter contains a fixed number of values, then give all of them to RGS_HSG ; if a hyperparameter has an infinite number of values, for example, when it is a continuous variable, we first identify the minimum and the maximum values of this hyperparameter by a package called Single Hyperparameter Grid Searches (SHGS) [47]. We then select all values between the minimum and maximum values, separated by a multiple of a very small value called step size. For example, if for a hyperparameter, the minimum value is 0.001, and maximum value is 0.3, and if we use a step size of 0.001, then the set of values that we choose for the hyperparameter would include a sequence of 299 different values, which are 0.001, 0.002, 0.003, ..., 0.298, 0.299, and 0.3. We applied these rules to all ten ML methods involved in this study including deep learning. The adjustable hyperparameters and their input values that we used for the ten ML methods are shown in Table S5 in the supplement."}, {"title": "Experiments", "content": "To compare the RF models with corresponding all-feature models for each of the ten ML methods including deep learning, we discovered the best RF and all-feature model for predicting 5-year, 10-year, and 15-year BCM each respectively. This gives us 6 best models per method, and 60 best models in total for the ten methods. We followed the same experiment procedure to identify each pair of the best RF and all-feature model, which we describe below, using the development of the best DNM-5Year and DNM_RF-5Year model as an example.\nStep 1. Call RGS-HSG to randomly generate 6000 HYPESs. The input set of values for each of the hyperparameters to RGS-HSG are shown in S5, and the input number of HYPESs to RGS-HSG is 6000; Step 2. Run the grid searcher of RGSP. The set of 6000 HYPESs generated in Step 1 is one of the input to the grid searcher. Another input to the grid searcher is the dataset, namely, the 80% of the LSM-5Year data that serves as the train-test set ( see \u201cperformance metrics\") . The grid searcher will go through each of the 6000 HYPESs, and train and test models following the 5-fold CV mechanism (see \u201cperformance metrics\u201d) at each HYPES. At each of the 6000 settings, five different models are training and tested, performance scores regarding model training and testing and the corresponding HYPES are all recorded as part of the output of the grid searcher. During this step, 30000 DNM-5Year models are trained and tested; Step 3. At the end of the grid search, the grid searcher will select the best HYPES that is associated with the best average performance score among all 6000 HYPES. The top DNM-5Year model will then be developed by refitting the entire train-test set of the LSM-5Year data using the best HYEPS; Step 4. Repeat Step 2 and Step 3 to develop the best DNM_RF-5Year models, but using the LSM_RF-5Year instead of LSM-5Year dataset.\nIn order to ensure a fair comparison between a pair of best RF and all-feature model, we used the exactly same 6000 HYPESs generated in a same Step 1, while followed a separate Step 2 and 3 when developing the two best models. But a different Step 1 was conducted for each different dataset and ML method. To further ensure the fair comparison, we tried our best to conduct separate grid searches for developing the two under the same computer arrangement in terms of computers used, CPU cores used per computer, and RAM allocations. We distributed the computing work load among a small cluster of computers manually to achieve this.\""}, {"title": "Performance metrics and the 5-fold CV", "content": "Our grid searches use an AUC score to measure the prediction performance of a model. AUC stands for area under the curve, which was originated from what's so called a receiver operator characteristic curve (ROC_curve) which plots the true positive rate (TPR) against the false positive rate (FPR) at each of the cutoff values, given a test dataset and a prediction model [48]. The TPRs and FPRs are calculated based on the set of true outcome values contained in the test dataset and the corresponding set of predicted outcome values obtained from the prediction model. An AUC score measures the discrimination performance of a model."}, {"title": "ROC_curve and AUC", "content": "The 5-fold CV procedure, mean-test AUC, and validation AUC: Our grid searchers follow the 5-fold CV mechanism to train and test models at each HYPES. In order to conduct a 5-fold CV in a grid search, we first split the dataset that will be using in the grid search. In general, we use the following procedure to split a dataset: 1) split the entire dataset into a train-test set that contains 80% of the cases and a validation set that contains 20% of the cases. The train-test set will be given to a grid search as the input dataset, and the validation set will be kept aside for later validation tests; 2) divide a train-test set evenly into 5 portions for the purpose of conducting a 5-fold CV. The division is mostly done randomly except that each portion should have approximately 20% of the positive cases and the negative cases respectively to ensure that it is a representative fraction of the dataset. During a 5-fold CV, five different models are generated and tested, each is trained using a unique combination of 4 portions of the train-test set, and tested using the remaining portion. Five AUC scores are produced based on the tests done by the five models, and the average of these scores, called mean-test AUC, is also computed and recorded. The best HYPES selection done at the end of a grid search is based on the mean-test AUC scores recorded, and the best model is developed by refitting the entire train-test set used by the grid search at the best HYPES. In this study, a ROC_curve for a selected prediction model is generated by testing the cases contained in the corresponding validation set using the model, and we call the AUC obtained from such a curve a validation AUC."}, {"title": "The SHAP method for the explanation of a prediction", "content": "The Shapley value was introduced by Lloyd Shapley in 1951. It represents the distribution of individual payoffs in cooperative games by measuring the marginal contribution of an individual to the collective outcome [49,50]. The Shapley additive explanations (SHAP), developed based the Shapley value, is a method that can be used to explain the predictive output of a machine learning model [51,52]. A SHAP value shows the importance of a feature for contributing to the predicted outcome value. How a SHAP value is computed can be explained using the following formula [51]:\n$\\varphi_i(p) = \\frac{1}{|F|} \\sum_{S \\subseteq F\\{i\\}} [p(S \\cup {i})]-p(S)] (\\begin{array}{c}|F|-1\\\\ |S|\\end{array})$\nEach additive term of this formula has two components :1) the marginal contribution of the ith feature to the model's prediction; 2) the weight associated with the marginal contribution. F represents the complete set of all features contained in the data, i means the ith feature, for which we are computing the SHAP value, and S represents a subset of F, which excludes the ith feature. $p(S \\cup {i})$ represents the model's predicted outcome value using the combined set of features in S and {i} as the predictors, while p(S) represents the model's predicted outcome value using only the features in S as predictors. $p(S\\cup {i}) \u2013 p(S)$ represents the contribution to the model's prediction, made by adding the ith feature to the subset S. For each subset, $1/(|F|(^{|F|-1}_{|S|}))$ is given as the weight, determined by |F|, the size of the complete set of features F, and |S|, the size of S. The purpose of using the weight is to balance the overall prior influence among all possible sizes of S."}, {"title": "RESULTS", "content": "In this study, we use the Kernel Explainer of the SHAP library [51", "51": ".", "% Difference": "which shows the percentage increase or decrease of the mean-test AUC of a best RF model from the corresponding best all-feature model. For example, for DNM models that predict 5-year BCM, the percentage difference is -2.09%, which means that the best DNM_RF-5Year model performs worse than the DNM-5Year model by 2.09% in terms of mean-test AUC.\nAs previously described in the Experiments section, each of the 60 best models that we obtained was developed based on the best HYPES that was selected from 6000 randomly picked HYPESs used in corresponding grid searches, based on the mean-test AUC of the five models trained at each HYPES. In addition to developing and comparing the best models obtained through grid searches, we are also interested in knowing and comparing the average performance of all models trained during corresponding grid searches. Due to this, we created Table 3 below, which contains, for each of the 60 best models, the average value of the 6000 mean-test AUCs associated with the 6000 HYPESs used in corresponding grid searches.\nEach of the best models was obtained at the cost of training and testing tens of thousands of models through grid searches. A grid search with deep learning can be very time consuming. Table 4 below is a summary of the running time used by our grid searchers. We arrange these results in a side-by-side manner to compare the grid search time used for developing a best RF model with that used for developing the corresponding best all-feature model.\nAnother purpose of this study is to compare the prediction performance of the ten ML methods including deep learning. For predicting 5-year BCM, we compare in Figure 4(a) below the 20 best prediction models that we developed for the ten ML methods, including 10 best all-feature models and 10 best RF_models. We also rank the mean-test AUC scores of the 20 models from high to low, and show the rankings using a bar chart in Figure 4 (a). We did the same for the best 20 models for predicting 10-year BCM in Figure 4(b), and for predicting 15-year BCM in Figure 4 (c).\nWe also developed a ROC_curve for each of the 60 best models. Such a curve was developed by plotting FPRs against TPRs, obtained by testing cases in an independent validation set using a best model (see Methods for details). In Figure 5 below, we compare side by side the ROC_curves created for the best DNM and DNM_RF model concerning the risk prediction of 5-year (Figure 5(a)), 10-year (Figure 5(b)), and 15-year (Figure 5(c)) BCM, each respectively. Figure 6 compares the prediction performance of the ten ML methods in terms of their corresponding best models. It contains a panel of six subfigures. Each subfigure consists of 10 ROC_curves created for the corresponding best models of the ten ML methods. For example, Figure 6(a) contains the 10 curves for the best all-feature ML models concerning 5-year BCM.\nWe conducted SHAP analyses and developed a SHAP feature importance plot and summary plot for each of the 60 best prediction models. Due to page limit, we do not include all of the 120 plots in this paper, instead, we show side by side the SHAP bar charts of the two best DNM models concerning 15-year BCM in Figure 7, and the"}]}