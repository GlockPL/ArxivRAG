{"title": "Research on target detection method of distracted driving behavior based on improved YOLOv8", "authors": ["Shiquan Shen", "Zhizhong Wu", "Pan Zhang"], "abstract": "With the development of deep learning technology, the detection and classification of distracted driving behaviour requires higher accuracy. Existing deep learning-based methods are computationally intensive and parameter redundant, limiting the efficiency and accuracy in practical applications. To solve this problem, this study proposes an improved YOLOv8 detection method based on the original YOLOv8 model by integrating the BoTNet module, GAM attention mechanism and EIoU loss function. By optimising the feature extraction and multi-scale feature fusion strategies, the training and inference processes are simplified, and the detection accuracy and efficiency are significantly improved. Experimental results show that the improved model performs well in both detection speed and accuracy, with an accuracy rate of 99.4%, and the model is smaller and easy to deploy, which is able to identify and classify distracted driving behaviours in real time, provide timely warnings, and enhance driving safety.\nKeywords:Distracted Driving Behavior, YOLOv8, Target Detection, Self-Attention Mechanism, EloU.", "sections": [{"title": "1. INTRODUCTION", "content": "Distracted driving is one of the leading causes of traffic accidents worldwide, posing a serious threat to road safety. According to the World Health Organisation1 (WHO), more than 1.3 million people die in traffic accidents every year, a significant proportion of which are related to distracted driving. A survey conducted by the Chinese Academy of Social Sciences (CASS) showed that 34% of the respondents had distracted driving behaviours while driving2. A survey by the American Automobile Association (AAA) states that about 58 per cent of car accidents are caused by distracted driving. Data from the European Transport Safety Council and the Australian Transport Safety Bureau also confirm the dangers of distracted driving. The main causes of distracted driving include the use of mobile phones, in-vehicle electronic devices and other behaviours such as eating, drinking and putting on make-up3; in order to reduce such accidents, countries have been actively tackling the problem through regulations, technology and educational means, but the problem of distracted driving still exists. Therefore, real-time monitoring and early warning of driver distraction is crucial, which requires in-depth research and the use of image analysis technology4 to achieve the monitoring of drivers' distracted driving behaviours and to issue timely warnings to remind drivers to focus on driving, thus reducing traffic accidents caused by distracted driving.\nIn recent years, research on real-time detection of distracted driving has been highly valued at home and abroad. Image processing methods of machine learning and computer vision have been widely used in detecting driver distraction behaviour. Driver images are captured by in-vehicle cameras, and key points such as human posture, facial expression, and eye changes can be extracted to assess the driver's state. For example, Fasanmade A et al. analysed the driver's facial and eye features through video detection to determine the direction of vision and facial posture to identify distracted driving behaviours6 .Xing Y et al. used the driver's eye features to detect the duration of eye gaze and the number of blinks in a driving simulation environment and established a distracted driving recognition model based on Random Forests, which achieved a high accuracy rate7. In addition to traditional methods, deep learning has also been widely applied to the recognition of distracted driving behaviour. For example, literature8 uses pre-trained deep models combined with support vector machine classifiers to detect drivers' distracted driving behaviour. In order to solve the problems of low recognition accuracy and deployment difficulties in current distracted behaviour detection algorithms, a study proposed an algorithm that predicts distracted driving behaviour using improved KNN classification of driver posture features. Meanwhile, literature9 combines visual geometry group (VGG) model and convolutional neural network (CNN) to detect and classify driver's distracted driving behaviour using VGG16, VGG19, and Inception models to improve the accuracy. By incorporating the head pose estimation algorithm10 into YOLOv8, we detect the presence of distracted driving of the driver in each image frame from two perspectives: cognitive distraction and visual distraction, avoiding the problem that the classification method is limited by the number of distracted driving categories, and set appropriate time thresholds, so as to realise end-to-end real-time distracted driving warning.\nHowever, with the continuous development of deep learning technology, higher requirements have been put forward for the detection and classification accuracy of driver's distracted driving behaviour. Existing deep learning-based methods have large parameter calculations and more redundant parameters, which limit their efficiency and accuracy in practical applications. To address this problem, this study proposes a driver distracted driving behaviour recognition algorithm based on improved YOLOv8, which aims to achieve real-time alerts for drivers, thus ensuring road safety. The algorithm identifies and classifies the driver's distracted driving behaviour through a series of algorithms, and once it is judged to be dangerous driving behaviour, an alarm can be issued to achieve timely reminder to the driver so as to enhance the level of driving safety."}, {"title": "2. DATA AND METHODS", "content": "The dataset of this paper is from SFD (State Farm Distracted Driver Detection) dataset provided by Kaggle official website, which contains 10 different driving behaviors with good breadth. The figure below shows the distribution of the data categories with the action images of the 9 common driving behaviors in which the distracted state exists.\nIn this study, 22424 driving behavior images from the original dataset were selected, covering c0 (normal driving), c1 (texting - right), c2 (talking on the phone - right), c3 (texting - left), c4 (talking on the phone - left), c5 (operating the radio), c6 (drinking), c7 (reaching behind), c8 (hair and makeup), c9 (talking to passenger), which 10 kinds of images were selected. left), c5 (operating the radio), c6 (drinking), c7 (reaching behind), c8 (hair and makeup), c9 (talking to passenger), and 10 other categories. Given that distracted driving behavior detection focuses on the driver's behavioral actions, these behaviors are mainly reflected in the driver's upper body in the cab, especially hand movements and facial orientation. Therefore, the original images were uniformly processed to 480\u00d7640 resolution using Python and image enhancement was performed to improve image clarity [12]. Subsequently, the driver's upper body and steering wheel were annotated and given corresponding behavioral labels. After all images were randomly disrupted, the dataset was divided into training, validation, and testing sets in the ratio of 8:1:1. According to the 10 different driving behaviors in the dataset, the driver actions were annotated using the corresponding tags and XML files containing target box coordinates and category information were generated. This study uses the above processed dataset to train the target detection network with the aim of developing an efficient distracted driving behavior detection model.Results and Discussion."}, {"title": "2.2 BoTNet (Bottleneck Transformers)", "content": "BoTNet13 used in this paper combines transformers and convolutional neural networks (CNNs) to improve the performance of large-scale vision tasks while keeping the computational cost low. BoTNet introduces the mechanism of Multihead Self-Attention in the bottleneck module of ResNet and replaces the 3x3 convolutional layer with a Multihead Self-Attention (MHSA) layer, which enhances the performance of image feature extraction and classification."}, {"title": "2.3 GAM attention mechanism", "content": "GAM14 (Global Attention Module) is an attention mechanism that integrates global information to dynamically adjust feature weights, aiming to optimize the attention of neural networks. The module is usually combined with different layers of convolutional neural networks, which can effectively utilize the global information to improve the processing and generalization ability of complex tasks.\nGAM utilizes a sequential channel-space attention mechanism and redesigns its submodules. Specifically, the channel-attention sub-module utilizes a three-dimensional arrangement for information retention and enhances the channel-space dependency across dimensions with a two-layer MLP (multilayer perceptron). In the spatial attention sub-module, two convolutional layers are used for information fusion in order to better focus on the spatial information, removing the max-pooling operation that may lead to information loss.GAM steadily improves the performance on different neural network architectures. Its basic structure is schematically shown below."}, {"title": "EIoU (Extended Intersection over Union)", "content": "IoU (Intersection over Union) is a commonly used metric for evaluating the performance of target detection models and is used to calculate the degree of overlap between the predicted bounding box and the real bounding box. However, when there is no intersection between the two bounding boxes, the IoU value is 0, which leads to stagnant gradient update and is not conducive to model optimisation. To solve this problem, we introduce the EIoU15 (Enhanced Intersection over Union) loss function. The EIoU, in addition to considering the overlapping area of the bounding box, also introduces the distance from the centre point of the bounding box and the relative difference between the width and height. Even in the case where the two bounding boxes do not overlap, EIoU can provide effective gradients to facilitate model training and convergence. The following is the formula for EIoU:\nThe overlap loss is the same as the loU loss and measures the proportion of the overlap between the predicted and real frames as a percentage of the joint region. The center distance loss measures the distance between the centroids of the predicted and real frames:\n$d_{center} = \\sqrt{(x_p-x_t)^2 + (y_p-y_t)^2}$\n(5)\n$ElOU_{loss} = 1- IoU + \\frac{d_{center}^{2}}{c^{2}} + \\frac{|w - w_t|}{|c|} + \\frac{|h_p - h_t|}{|c|}$ (6)\nwhere $(x_p,y_p)$ and $(x_t,y_t)$ are the coordinates of the center points of the predicted and real boxes, respectively.\nThe width-height loss directly minimizes the difference between the width and height of the predicted and real boxes, $w_p, h_p$, and $w_t, h_t$ are the width and height of the predicted and real frames, respectively.where IoU is the intersection and concurrency ratio of the predicted and real frames, and c is the diagonal length of the smallest closed box containing the predicted and real frames."}, {"title": "2.4 Yolo model structure", "content": "YOLOv8 (You Only Look Once, Version 8) is the 8th version of the YOLO family of target detection algorithms, which significantly improves performance and efficiency through several key enhancements. Compared to previous versions, YOLOv8 features faster inference, higher accuracy,"}, {"title": "2.5 Model setting", "content": "Among the six models in YOLOv8, depth_factor, width_factor and ratio are used to set the depth and width of the model and the number of channels in C5, respectively. depth_factor and width_factor will increase the depth and width of the network when the parameters are larger, and decrease the depth and width of the network when the parameters are smaller. depth_factor is used to set the depth of the model, according to different parameter settings, the number of layers of the model is 225, 225, 295, 365 and 365 respectively; width_factor is used to set the width of the model. the basic number of channels of C5 in YOLOv8 is 512, the ratio parameter indicates the multiplication of the basic number of channels, and the max_channels is set to 1024, 1024, 1024, 1024, 1024, 1024, 1024 and 1024 respectively. channels) are set to 1024, 1024, 768, 512 and 512 respectively."}, {"title": "3. RESULTS AND DISCUSSION", "content": "In order to validate the detection effect of distracted driving behavior detection and classification model, focusing on the detection precision and recall rate, and classification accuracy, so the confusion matrix, precision rate (precision), recall rate (recall) and F1 Score are selected to evaluate the MAP (Mean Average Precision) model [13]. The calculation formula is as follows:\n$Accuracy = \\frac{T_p + T_N}{T_p + T_N + F_p + F_N}$ (7)\n$Precision = \\frac{T_p}{T_p + F_p}$ (8)\n$Recall = \\frac{T_p}{T_P + F_N}$ (9)\n$F_{1\\_score} = 2 \\frac{(Precision \\times Recall)}{(Precision + Recall)}$ (10)\n$AP = \\sum(Recall_n - Recall_{n-1})Precision_n$ (11)\n$MAP = \\frac{1}{N} \\sum AP_n$ (12)\nWhere $T_p$, $T_N$, $F_p$ and $F_N$ represent true positive, true negative, false positive and false negative respectively. $Recall_n$, $Precision_n$ denote, respectively, the point at which the nth recall rate and precision.\nIn order to validate the advantages of distracted driving detection models, several popular target detection algorithms were trained and tested in this study using the same data. The comparison experiments include Faster-RCNN, YOLOv5, YOLOv5_DD, and the original YOLOv8 model, and the average metrics of detection results for each model are shown in the table. In this study, these methods were experimented on the dataset, where FPS (frame rate) is used to measure the processing speed of the model in a video stream, i.e., the number of frames per second that can be processed, and a higher FPS indicates that the model is able to process the video stream in real time. The number of floating point operations (FLOPs) is used to measure the amount of model computation, indicating the number of floating point operations required by the model in one forward propagation."}, {"title": "4. CONCUSION", "content": "The improved YOLOv8 model shows significant advantages in distracted driving behaviour detection with high detection accuracy and real-time detection capability. The experimental results show that the model outperforms Faster-RCNN and YOLOv5 in terms of precision (0.994), recall (0.982),"}]}