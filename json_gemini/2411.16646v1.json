{"title": "Self-Generated Critiques Boost Reward Modeling for Language Models", "authors": ["Yue Yu", "Zhengxing Chen", "Aston Zhang", "Liang Tan", "Chenguang Zhu", "Richard Yuanzhe Pang", "Yundi Qian", "Xuewei Wang", "Suchin Gururangan", "Chao Zhang", "Melanie Kambadur", "Dhruv Mahajan", "Rui Hou"], "abstract": "Reward modeling is crucial for aligning large language models (LLMs) with human preferences, especially in reinforcement learning from human feedback (RLHF). However, current reward models mainly produce unexplainable scalar scores and struggle to incorporate critiques in a natural language format. We hypothesize that generating both critiques and scalar rewards would improve reward models' capability on preference ranking. Motivated by this, we propose Critic-RM, a framework that utilizes self-generated, high-quality critiques to train reward models for scalar reward-based preference prediction, with explicit rationales serving as supporting evidence. Critic-RM employs a two-stage process: generating and filtering high-quality critiques, followed by joint fine-tuning on reward prediction and critique generation objectives. Experiments on preference ranking benchmarks including RewardBench and CrossEval show that Critic-RM improves reward modeling accuracy by 3.7%-7.3% compared to standard reward models and LLM judges, demonstrating strong performance and data efficiency. Additional studies further validate the effectiveness of the generated critiques in rectifying flawed reasoning steps with the gain of 2.5%-3.2% on improving reasoning accuracy.", "sections": [{"title": "1 Introduction", "content": "Reinforcement Learning from Human Feedback (RLHF) has been widely adopted to align large language models (LLMs) with human preferences (Ouyang et al., 2022; Touvron et al., 2023; Dubey et al., 2024; Reid et al., 2024). Central to the RLHF process is the reward model (RM), which is trained to assign scores that quantify how well the model's outputs align with human judgments. The reward model defines optimization direction during training (e.g., reward signal in PPO), encouraging a policy LLM to generate more helpful, honest, and harmless responses ultimately enhancing the model's generation quality in real-world applications.\nStandard reward models are typically trained using preference pairs and optimized with pairwise logistic loss (Bradley and Terry, 1952), producing a single scalar score for each response. However, outputting a scalar score not only is hard to interpret but also fails to fully leverage the inherent language modeling capability that LLMs obtain from pretraining and post-training (Zhang et al., 2024). Consequently, these reward models tend to be less data-efficient and prone to robustness issues, such as reward hacking (Skalse et al., 2022; Singhal et al., 2023; Chen et al., 2024). Such limitations hinder the quality of feedback signals in RLHF and lead to suboptimal policy updates. On the other hand, the LLM-as-a-judge paradigm offers an alternative, where the LLM first generates a critique and then optionally provides a discrete score as a quality proxy for a response (Zheng et al., 2023; Kim et al., 2024a; Zhong et al., 2024). Combining the strengths of both paradigms-integrating the interpretability and structured critique of LLM-as-the-judge with the scalar optimization framework of reward models has the great potential to address the limitations of each method and yield more robust and effective reward signals.\nDespite its great premise, incorporating critiques into reward modeling presents two major challenges. (1) Conflicting objectives: Critique generation requires language modeling, while reward models provide scalar outputs, complicating its integration into language modeling. (2) Evaluator limitations: Off-the-shelf LMs are often not good evaluators, while additional fine-tuning requires costly human-generated or annotated critiques."}, {"title": "2 Related Work", "content": "Reward Models. Building an accurate and robust reward model is a critical step for RLHF pipelines. Earlier work trains reward models with the ranking loss between chosen and rejected responses with the Bradley-Terry model (Bradley and Terry, 1952; Stiennon et al., 2020; Ouyang et al., 2022; Dubey et al., 2024). To further improve upon this reward modeling pipeline, Wang et al. (2024e,d,a) design fine-grained attributes to predict\nRecent work (Ye et al., 2024) directly incorporates critiques generated from off-the-shelf LLMs for reward modeling, while Ankner et al. (2024) and Zhang et al. (2024) design a joint training approach for learning to generate the critique as well as rewards simultaneously via knowledge distillation. These methods typically rely on a strong teacher LLM to generate high-quality critiques, which can be costly and inefficient to obtain at scale in practice. Moreover, they cannot be used to improve frontier models when a stronger teacher model does not exist.\nWe introduce Critic-RM, a new framework that enhances reward models using synthetic critiques, without relying on strong LLM teachers. Our approach draws inspiration from recent advances in self-improving language models (Yuan et al., 2024; Wu et al., 2024; Prasad et al., 2024), where models are iteratively refined using data generated by themselves. To apply a similar LLM self-improving paradigm in reward modeling, we hypothesize that it is crucial to inject LLM's critique generation ability into this process. Specifically, Critic-RM leverages an instruction-finetuned LLM as the backbone, which generates multiple candidate critiques, each with a discrete score (as explained below, for filtering critiques; not our final reward) for individual responses. However, these critiques can vary in quality, and poor-quality critiques often result in flawed quality predictions. To tackle this issue, we first apply a consistency-guided filtering technique, retaining only critiques whose scores align with human-annotated preference labels\u00b9. To further enhance the quality of these synthetic critiques, we additionally propose two strategies, summarization and ranking, to refine the critiques used in training the reward model.\nOnce critiques are generated for each response, the main challenge lies in designing an effective training strategy to combine critique modeling and scalar reward prediction objectives. While LLMs benefit from learning through diverse critiques for each response (Ho et al., 2023), reward modeling is prone to overfitting (Dubey et al., 2024; Zhu et al., 2024); such a contradiction makes it nontrivial to determine the optimal learning steps. To address this issue, we introduce a simple weighting balancing strategy, where the model initially focuses on critique modeling loss, then gradually transitions to predicting rewards based on both the response and the critique. This approach balances the two learning objectives, allowing the model to excel at both high-quality critique generation and accurate reward prediction.\nTo demonstrate the effectiveness of Critic-RM, we conduct extensive experiments on RewardBench and three out-of-distribution reward modeling tasks, showing that Critic-RM outperforms baselines in both in-domain and out-of-domain evaluations. Additionally, experiments on critique evaluation benchmarks highlight Critic-RM's ability to generate valuable feedback for correcting LLMs' flawed reasoning. Our analysis confirms that Critic-RM's superior generalization stems from its ability to identify and leverage high-quality self-generated critiques. The major contributions of our work can be summarized as follows:\n\u2022 We propose Critic-RM, a framework to allow LLMs to take advantage of self-generated critiques for reward modeling. Critic-RM does not rely on additional supervision compared to standard reward models, while enjoying an improved generation quality as well as reward modeling accuracy.\n\u2022 We propose a self-refinement technique to automatically select high-quality critiques, and design a simple yet effective weight scheduling strategy to balance the learning objectives between critique generation and reward modeling. These techniques collaboratively equip the model with the dual capabilities of high-quality critique generation and accurate reward prediction.\n\u2022 We conduct experiments on three benchmarks covering over ten tasks, demonstrating the effectiveness of Critic-RM in precise reward modeling across diverse scenarios. Additional studies confirm the utility of Critic-RM-generated critiques in identifying and correcting mistakes made by LLMs.\nLLM-as-a-judge and Critique Models. Recently, large language models (LLMs) have been proposed as cost- effective alternatives to human evaluation, and act as proxies for assessing text quality. Such methods often first provide explanations for judgments of the response, then output a discrete score or preference label as the prediction (Zheng et al., 2023; Li et al., 2023; Yan et al., 2024; Xu et al., 2024). CriticGPT (McAleese et al., 2024) has also extended this line of work into coding tasks, where the LLM critic model is fine-tuned to pinpoint problems in code from real-world assistant tasks. However, using off-the-shelf LLMs for evaluation introduces the risk of bias (Bavaresco et al., 2024; Stureborg et al., 2024), and they can be easily misled (Zeng et al., 2024). To address these challenges, recent studies (Wang et al., 2024b; Kim et al., 2024a) have focused on collecting high-quality response pairs to train more accurate and reliable LLM-based evaluators.\nSelf-alignment Techniques. Aligning LLMs with human preferences often requires a massive amount of human annotations. To alleviate this reliance on human efforts, self-alignment leverages the model's own capabilities to refine its responses and align them with desired behaviors. Saunders et al. (2022); Madaan et al. (2023) use LLM itself to refine the original response at the inference time. Li et al. (2024b) generate instruction prompts for web documents and subsequently select high-quality examples for instruction fine-tuning. Lee et al. (2024); Sun et al. (2024) leverage LLMs to create preference labels efficiently, Yuan et al. (2024) employ LLM itself to rank different responses to provide its own rewards during training, and Zelikman et al. (2022); Pang et al. (2024); Gulcehre et al. (2023) improve LLM reasoning abilities through self-generated reasoning steps. A recent study (Wang et al., 2024b) also employs self-improving techniques to train text evaluators, but it focuses on pairwise evaluation and generating synthetic preference pairs. In contrast, we combine self-generated critiques with human-annotated preference pairs to enhance reward modeling performance."}, {"title": "3 Methodology", "content": "3.1 Preliminaries\nReward Modeling. Let X and Y denote the space of prompts and responses, respectively. In the RLHF pipeline, human feedback is typically collected in the form of pairwise preferences between two responses $(y^+, y^-) \\in Y^2$ to a given prompt $x \\in X$. Then, the preference dataset can be written as $D = \\{(x, y^+, y^\\_i)\\}_{i=1}^D$, where the preference for $y^+$ over $y^-$ is denoted as $y^+ > y^-$. To model the pairwise preferences, the learning objective is"}, {"title": "3.2 Critique-augmented Reward Model Training", "content": "To integrate the critiques into the reward modeling step, we view critiques as latent variables, which serve as an intermediate variable between the response and the final reward. Specifically, we denote $z^+, z^-$ as critiques for chosen and rejected responses $y^+, y^-$ with prompt x, respectively. Then, the overall learning objective is\n$p (y^+ > y^- | x) = \\sum_{z^+,z^-} p(y^+ > y^-, z^+,z^- | x)$ \n$=\\sum_{z^+,z^-} p(y^+ > y^- | z^+,z^-,x) \\cdot p^* (z^+ | y^+,x) \\cdot p^*(z^- | y^-,x)$.\nSince $p^*(\u00b7 | y, x)$ stands for the oracle distribution for critiques and is often not intractable, we aim to leverage the critic generation model $g_\\phi$ to generate the approximate distribution $q_\\phi$ as\n$p (y^+ > y^- | x) = \\sum_{z^+,z^-} q_\\phi(z^+ | y^+, x) q_\\phi(z^- | y^-,x) \\frac{p(y^+ > y^-,z^+, z^- | x)}{q_\\phi (z^+ | y^+, x) q_\\phi (z^- | y^-,x)}$.\nThen, by applying the Jensen's Inequality, the training objective can be expressed as\n$L = - log p (y^+ > y^- | x)$\n$=- log E_{q_\\phi (z^+ | y^+,x), q_\\phi (z^-|y^-,x)} \\frac{p(y^+ > y^-,z^+, z^- | x)}{q_\\phi (z^+ | y^+, x) q_\\phi (z^- | y^-,x)}$\n$\\ge E_{q_\\phi(z^+ | y^+,x),q_\\phi(z^-|y^-,x)} [-log \\frac{p(y^+ > y^-,z^+, z^- | x)}{q_\\phi (z^+ | y^+,x) q_\\phi (z^- | y^-,x)} ]$\n$= E_{q_\\phi(z^+ | y^+,x),q_\\phi(z^-|y^-,x)} [-log p (y^+ > y^- | z^+, z^-,x)]$\nPreference Modeling Loss with Critiques\n$+ D_{KL} ((q_\\phi(z^+ | y^+,x)||p^*(z^+ | y^+,x)) + D_{KL} ((q_\\phi(z^- | y^-,x)||p^*(z^- | y^-,x)) .$\nCritique Generation Loss\nWhat does the learning objective imply? Eq. 5 provides a way to decompose the reward model learning objective into two parts: (1) Preference Modeling Loss with Critiques lr: the reward model re learns to predict the reward for each response conditioned on critiques; (2) Critique Generation Loss lc: the LLM generation ge is trained to generate critiques to approximate the oracle distribution $p^*(\u00b7 | y, x)$. We will discuss how to train the reward model re and critique generation model ge in the following subsections.\n3.2.1 Critique-augmented Reward Prediction\nTo enable the reward model $r_\\psi$ to learn the preference with critiques (i.e., lr) can be straightforward, as we only need to modify the input by augmenting response with critiques as\n$l_r(x,y^+,y^-,z^+,z^-) = - log p (y^+ > y^-,z^+,z^- | x) = - log p (r_\\psi(x, [y^+; z^+]) > r_\\psi(x, [y^-; z^-])).$\nIn this way, for each prompt, the reward model will learn to generate the reward based on both responses and critiques. In practice, we put the critiques after the response and add a special token at the end of the critique for calculating the reward.\n3.2.2 Critique Generation & Filtering\nFor critique generation loss, approximating $p^*(\u00b7 | y, x)$ can be nontrivial as the primary challenge lies in the lack of high-quality critique annotations. To ensure the quality of the critiques, our key hypothesis is that good critiques for responses should align well with human preference labels. With this in mind, we design a generate-then-filter framework to create high-quality supervision signals for critique model training.\nCritique Generation. To generate critiques without relying on stronger LLMs, we first prompt the LLM Me (with the same backbone as the reward model) and sample a set of N candidate critiques for input prompt and responses (x, y) by following the procedure of the LLM-as-a-judge pipeline as $(z_i, s_i)_{i=1}^N \\sim g_\\phi(x, y)$, where $z_i$ is the generated critique and $s_i$ is a discrete score ranging from 1 to 10, indicating the quality of the response.\nInstance-level Critique Filtering. To reduce the potential noisy critiques and encourage the consistency between critiques and preference labels, we propose to first retain instances guided by the score generated by the"}, {"title": "3.3 Critic-RM Inference", "content": "Compared to standard reward model training, Critic-RM involves an additional step for each (prompt, response) pair during inference. Specifically, given the (prompt, response) pair (x, y), the model will first generate a critique z ~ q\u03d5(x, y), then predict the reward for the response as r = r\u03c8(x, [y, z]).\nInference-time Scaling. Following recent studies (Ankner et al., 2024; Zhang et al., 2024), we also conduct inference-time scaling (Wang et al., 2023) to improve performance. Specifically, we generate a set of m critiques as $Z = \\{z_i\\}_{i=1}^m \\sim q_\\phi(x, y)$ with non-zero temperatures, then predict the reward for the response as the average of reward over different critiques as $r = r_\\psi(x, [y, z_i])/m$."}, {"title": "4 Experiments", "content": "4.1 Experiment Setup\n4.1.1 Training Data\nTo ensure the representativeness of the preference pairs used in this study, we leverage both public and synthetic datasets for reward model training.\nPublic Preference Datasets: We choose a set of datasets for reward model training with human-generated preference labels mainly from public, open-sourced datasets (Ivison et al., 2024; Wang et al., 2024a). We include the following datasets:\n\u2022 General Chat Domain: We include datasets from ChatArena (Zheng et al., 2023) and AlpacaFarm-Human- Pref (Dubois et al., 2023).\n\u2022 Helpfulness Data: We leverage HelpSteer2 (Wang et al., 2024d) to create preference data.\n\u2022 Reasoning: We mainly use Evol-instruct (Xu et al., 2023) which contains preference pairs for complex instruction following, coding-related tasks.\n\u2022 Safety: We employ PKU-SafeRLHF (Dai et al., 2024), which includes safety-related prompts paired with both safe and unsafe responses to form preference pairs.\nSynthetic Preference Datasets: To incorporate additional preference supervision from different domains, we further include synthetic data using Llama-3.1 models. Specifically, for the math domain, we consider questions in GSM8K (Cobbe et al., 2021) and the MATH dataset (Hendrycks et al., 2021). For each math question, we use Llama-3.1-8b-instruct, and Llama-3.1-70b-instruct to generate candidate solutions with the prompt \"Given the following problem, reason step-by-step and give a final answer to the problem.\", and generate multiple candidate solutions for a given prompt. We use those responses that lead to correct solutions as the chosen response while considering those responses with incorrect solutions as the rejected response. In the safety domain, we generate synthetic prompts following the safety principles outlined in SafeRLHF (Dai et al., 2024) (e.g., Hate Speech, Offensive Language, Discrimination, Violence). To ensure balance, we also include scenarios where the model should not refuse to respond (e.g., Figurative Language, Safe Targets testing for ambiguous meanings) to avoid skewing the data toward over-conservatism.\n4.1.2 Evaluation Benchmarks.\nEvaluation Benchmarks for Reward Models. In our experiments, we mainly evaluate on RewardBench (Lambert et al., 2024), which contains a collection of prompt-chosen-rejected triplets across chat, reasoning, and safety domains, including 2985 examples in total. We use the standard evaluation protocol provided by the original authors. Beyond RewardBench, we also aim to test the out-of-distribution generalization ability of reward models. Specifically, we consider CrossEval (Zhong et al., 2024), a recently proposed benchmark to evaluate the LLM's capability in real-world interactions\u00b3. Besides, we also consider two additional datasets, namely QA Feedback (Wu et al., 2023) and SHP (Ethayarajh et al., 2022), which focuses on evaluating the response for open-ended QA task as well as social platforms (i.e., Reddit). There are around 2000 examples of QA"}, {"title": "4.1.3 Baselines", "content": "We consider the following baselines from three different groups:\n\u2022 LLM-as-a-judge: With the prompt with a pair of responses used as the input, this line of models needs to generate a preference label. We consider Prometheus-v2 (Kim et al., 2024b), Llama-3.1-70B/405B (Dubey et al., 2024), GPT-4 (Achiam et al., 2023) and GPT-40 (Hurst et al., 2024), Gemini-1.5-pro (Reid et al., 2024) and recently proposed self-taught evaluator (Wang et al., 2024b) based on Llama-3-70B for comparison.\n\u2022 Standard Reward Models: This line of models only outputs a scalar score for each (prompt, response) pair. We compare with baselines including standard RM (Stiennon et al., 2020), Cohere-0514, SteerLM- RM (Wang et al., 2024e), Nemotron-RM (Adler et al., 2024).\n\u2022 Reward Model with Critiques: These studies are mostly relevant to us as they also leverage critiques to improve reward models. Specifically, we compare with SymRM (Ye et al., 2024) which directly augments responses with critiques for reward modeling, and CLoud (Ankner et al., 2024) which jointly learn to generate critiques and predict rewards.\nIt is worth noting that for most relevant baselines (e.g. RM, SynRM, CLoud), we reimplement those baselines with the same training data and backbone to ensure the comparison is fair and meaningful. We do not consider some reward model training techniques (Wang et al., 2024c,a) as they focus on designing better learning objectives for standard reward models, which are orthogonal to the focus of this study."}, {"title": "4.1.4 Implemenation Details", "content": "We use Llama3.1-70B-Instruct (Dubey et al., 2024) as the backbone in our main experiments. For critique generation, we set the temperature \u03c4 = 0.9 and sample N = 10 candidate critiques for each response. For the critique filtering, we set K = 2 to select top-2 responses. For model fine-tuning, we use the Adam optimizer (Kingma and Ba, 2014) with the learning rate 2e-6, weight decay 0.1 and dropout 0.1. We set the global batch size to 64, \u03b2 in Eq. 9 to 0.9 and train the model with 2 epochs. We observe that there exist several examples in AlpacaEval and ChatArena that share similar prompts with the target evaluation tasks, and we remove all overlapping prompts to avoid data contamination (Oren et al., 2024). During inference, if inference-time scaling is adopted, we choose temperate \u03c4 = 0.95 to sample multiple critiques. The prompt format we use in experiments is exhibited in Appendix B."}, {"title": "4.2 Main Experiments: RewardBench", "content": "Table 2 presents results of Critic-RM and baselines. The findings are summarized as follows:\n\u2022 Incorporating Critiques Helps Reward Modeling in General. Critic-RM generally outperforms the baselines used in this study. Specifically, when trained with the same preference data, Critic-RM outperforms the standard Reward Model by 3.7%-4.7%. Critic-RM also outperform giant Llama-3.1-405b judge model by 6.2%-7.3%, respectively. These results justify the advantage of incorporating critiques into the reward model training step, which facilitates both high-quality critiques and precise rewards."}, {"title": "4.3 Out-of-Distribution (OOD) Evaluation", "content": "As shown in Table 3, we evaluate the performance of our approach (Critic-RM) alongside relevant baseline models on three out-of-distribution (OOD) reward modeling datasets. Our results demonstrate that Critic-RM exhibits a strong performance across these datasets, surpassing standard reward modeling (RM) baselines by an average margin of 4%. Notably, the performance improvements of Critic-RM are more pronounced on more challenging benchmarks, such as tasks requiring cross-abilities, suggesting that the benefits of critiques are more significant in complex scenarios. Furthermore, we observe that the performance of Critic-RM is comparable to that of LLM-judge models with significantly more parameters. This highlights the efficiency and effectiveness of Critic-RM when being adapted to real scenarios."}, {"title": "4.4 Evaluation on Critiques", "content": "As Critic-RM involves a crucial step for generating critiques, it is also important to evaluate the quality of critiques for target tasks. We use CriticBench to perform a comprehensive evaluation, with results detailed in Table 4. For critique accuracy, we observe that Critic-RM generates more accurate critiques compared to strong baselines, including GPT-4. This justify that Critic-RM is able to distinguish correct and flawed reasoning paths. Additionally, these critiques help the policy language model (LM) correct flawed reasoning steps, resulting in improved accuracy in refined responses. Notably, even when using the lightweight Llama-3-8B as the policy LM, the critiques guide the smaller LM to rectify initial incorrect reasoning and achieve high accuracy across five reasoning tasks."}, {"title": "4.5 Ablation Studies", "content": "Effect of Two-stage Training. Figure 2a illustrates the performance of Critic-RM with different weight scheduling function \u03bb(t). The results indicate that using a constant weight across different rounds, as well as reverse"}, {"title": "4.6 Data Efficiency of Reward Models", "content": "Table 5 shows the accuracy of Critic-RM and baselines on RewardBench with different volumes of training data. Overall, Critic-RM consistently outperforms the baselines across all data volumes, demonstrating superior performance even with limited labels. Notably, Critic-RM shows strong data efficiency using just 10% of the labeled data is sufficient to surpass the standard reward model. This result highlights the data efficiency of Critic-RM, making it highly practical for real-world applications."}, {"title": "4.7 Case Studies", "content": "Table 6 presents two examples from RewardBench that highlight the advantages of Critic-RM over baseline models. Standard RMs often struggle to assign higher scores to the correct response and fail to provide additional context to justify the predicted reward. While Cloud offers general feedback on response quality, it tends to focus on strengths and weaknesses without identifying the most critical aspects for evaluation. In contrast, Critic-RM accurately identifies key errors in rejected responses and explains why the chosen response is superior, leading to more accurate predictions. We list additional case studies in Appendix C."}, {"title": "5 Conclusion", "content": "In this work, we introduced Critic-RM, a self-critiquing framework designed to enhance reward modeling for large language models. By harnessing LLMs' ability to generate and refine critiques, Critic-RM implements a novel self-improvement approach that improves both critique quality and reward prediction accuracy."}, {"title": "Limitation and Future Work", "content": "Critic-RM introduces a new framework for reward modeling by leveraging self-generated critiques. While it shows promising results, several limitations exist:\nSingle Model Focus: Critic-RM does require the base LLM to have a certain level of critique generation ability. Testing Critic-RM across different LLM architectures could provide broader insights into its effectiveness.\nLonger Inference Time: Generating critiques during inference adds computational overhead. This trade-off may affect its use in real-time applications where latency is critical for model deployment.\nNo Iterative Training: Critic-RM does not incorporate iterative training, where models refine themselves over multiple rounds. Adding this step could further improve reward modeling performance, as shown in recent studies (Yuan et al., 2024; Pang et al., 2024)."}, {"title": "A Dataset Processing Details for CrossEval", "content": "We focus on the seven subtasks of CrossEval: four single capabilities including Reasoning, Coding, English, and Tool as well as three cross-capabilities including Reasoning+Coding, Coding+Reasoning, and Tool+Coding4. For each prompt within the subtask, there are three responses associated with two ratings. We only included response pairs with different average scores, and used the response with higher scores as the chosen response. There are 1181 response pairs in total."}, {"title": "B Prompt Templates", "content": "B.1 Prompt Templates for Critique Generation\nThe prompt format used in Critic-RM is listed in Table 7. It is worth noting that for different tasks, we use different formats for better customization. For OOD evaluation tasks, we use Chat/Helpfulness prompts for SHP, QA Feedback, as well as the English/Tool subset of CrossEval benchmark, and use Code prompts for Code-related subtasks. For the Reasoning subtask, we use Math prompts."}, {"title": "B.2 Prompt Templates for Critique Refinement", "content": "The prompt templates employed for refining critiques, described in Section 3.2.2, are listed in Table 8."}, {"title": "B.3 Prompt Templates for Correction Generation", "content": "The prompt templates employed for correcting incorrect solutions, described in Section 4.4, are listed in Table 9."}, {"title": "C Additional Case Studies", "content": "We present two additional case studies in Table 10, focusing on Code reasoning and the Chat-hard subset of RewardBench. Existing critique generation method CLoud, falls short in delivering faithful assessments. In contrast, Critic-RM effectively identifies the key strengths and weaknesses of the responses."}, {"title": "D Full Results for Critic-RM", "content": "Table 11 presents the comprehensive results of Critic-RM performance, including a detailed breakdown by category."}]}