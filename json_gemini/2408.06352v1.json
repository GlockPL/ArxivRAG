{"title": "Using Large Language Models to Compare Explainable Models for Smart Home Human Activity Recognition", "authors": ["Michele Fiori", "Gabriele Civitarese", "Claudio Bettini"], "abstract": "Recognizing daily activities with unobtrusive sensors in smart environments enables various healthcare applications. Monitoring how subjects perform activities at home and their changes over time can reveal early symptoms of health issues, such as cognitive decline. Most approaches in this field use deep learning models, which are often seen as black boxes mapping sensor data to activities. However, non-expert users like clinicians need to trust and understand these models' outputs. Thus, eXplainable AI (XAI) methods for Human Activity Recognition have emerged to provide intuitive natural language explanations from these models. Different \u03a7\u0391\u0399 methods generate different explanations, and their effectiveness is typically evaluated through user surveys, that are often challenging in terms of costs and fairness. This paper proposes an automatic evaluation method using Large Language Models (LLMs) to identify, in a pool of candidates, the best XAI approach for non-expert users. Our preliminary results suggest that LLM evaluation aligns with user surveys.", "sections": [{"title": "1 INTRODUCTION", "content": "Sensor-based Human Activity Recognition (HAR) is a widely studied research area, and it involves using unobtrusive sensors to identify the activities performed by humans in their daily life [5].\nThe application of sensor-based HAR in smart-home environments is crucial to identify the so-called Activities of Daily Living (ADLs), that are high-level complex activities that humans typically do every day to take care of themselves and maintain their well-being (e.g., cooking, eating, taking medicines, sleeping). Indeed, a smart-home can be equipped with unobtrusive environmental sensors (e.g., motion sensors, magnetic sensors, pressure sensors) to reveal the performed activities. ADLs recognition is crucial in several healthcare applications, including the recognition of abnormal behaviors for the early detection of cognitive decline [17].\nThe majority of the solutions in this area rely on deep learning methods [4]. Such approaches are effective, but they are often considered as black-boxes mapping input smart-home sensor data into ADLs. However, non-expert users (e.g., clinicians, caregivers, the monitored subject) need to trust such models by understanding the rationale behind their reasoning. For this reason, several eXplainable Artificial intelligence (XAI) approaches have been proposed for smart-home ADLs recognition [2, 3, 8, 9]. Such methods, also known as eXplainable Activity Recognition (XAR) systems, are designed to detect human activities and, at the same time, providing clear explanations in natural language for each prediction. The generated explanations indicate which sensor events were considered as important by the classifier to perform a prediction. For instance: \u201cI predicted that Anna was cooking mainly because she is in the kitchen and the stove is on\u201d. However, quantitatively evaluating the effectiveness of the generated explanations is challenging [12]. Indeed, different XAI methods may lead to completely different explanations, since each approach has a specific mechanism to determine the inner reasoning of the model while making predictions.\nThe majority of the works in this area adopted user surveys, by recruiting a large number of subjects to evaluate explanations generated by different approaches. However, this strategy is costly in terms of money, time and human resources. Indeed, recruiting an adequate number of volunteers that guarantees a suitably homogeneous and significant sample, is very hard. A possible solution is to use tools like Amazon Mechanical Turk, which enable reaching a much larger audience with the incentive of financial compensation [3, 8, 9]. However, besides being costly, such approaches do not guarantee the quality of the results, since the workers do not always provide the necessary attention to the required task. For instance, in [3] the authors introduced some attention questions to exclude bots and users providing random answers just to obtain the reward. Overall, only the 44% of users answered correctly to those questions and could thus be considered reliable. Other works proposed metrics to automatically evaluate the quality of the explanations of HAR systems. Specifically, the work in [2] proposed an explanation score based on a knowledge-model (i.e., a set of logic"}, {"title": "2 EVALUATING EXPLANATIONS USING LLMS", "content": null}, {"title": "2.1 Problem Formulation and Research Question", "content": null}, {"title": "2.1.1 Problem Formulation.", "content": "Consider a smart-home that is equipped with several environmental sensors (e.g., motion sensors, magnetic sensors, pressure sensors). For the sake of this work, we focus only on single-inhabitant scenarios. The interaction of the user with the home environment leads to the generation of high-level events. For instance, when a magnetic sensor on the fridge fires the value 1, this implies that the subject opened the fridge. Let $E = {e_1, e_2, ..., e_n}$ be the set of possible high-level events captured by the sensors. The continuous stream of such events is partitioned into fixed-time temporal windows (e.g., using overlapping sliding windows). Each non-empty temporal window w includes k seconds of high-level events triggered by the inhabitant. The goal of an explainable Activity Recognition (XAR) model is to map each window to the most likely activity performed by the subject, and to an explanation in natural language. Let $A = {a_1, a_2, ..., a_m}$ be the set of target activities. Let h be an XAR model, taking in input a window w and providing as output the most likely activity $a \\in A$ performed by the subject, and a corresponding explanation exp. Note that exp describes in natural language the high-level events $E_W$ included in w that are considered the most important to classify a by h, and possibly their temporal relationships. In the literature, several examples of such XAR models have been proposed [2, 8]."}, {"title": "2.1.2 Research question.", "content": "Let H be a set of K different alternative XAR models $h_1, h_2, ..., h_k$. Let P be a pool of time window. Given a window $w \\in P$, all the XAR models in H provide the same output activity but possibly different explanations. Our research question is the following: can we leverage Large Language Models (LLMs) to choose the best XAR model in H, given the explanations provided to the windows in P?"}, {"title": "2.2 Prompting Strategies", "content": "In this work, we employed two distinct prompting strategies to evaluate the explanations provided by the different XAR models. Both approaches require providing the LLM with K explanations generated by the K alternative models when processing a window w. Sometimes two or more models may provide identical explanations. In such cases, this explanation is provided only once to the LLM, thus providing less than K options. The pseudo-code of the general approach for LLM-based evaluation is presented in Algorithm 1."}, {"title": "2.2.1 Best-Among-K Strategy.", "content": "The first strategy is called the \u201cBest-Among-K Strategy\u201d. For each window w in the pool and a set of K alternative models H, the LLM is prompted to determine which explanation of w is the best among the ones in H. Then, we assign a score with value 1 to the model that provided the best explanation, while the others are scored 0. When the explanation selected by the LLM is actually generated by more than one model in H, we assign a score of value 1 to all models that generated that explanation. At the end of this process, the model in H with the highest score is considered the best."}, {"title": "2.2.2 Scoring Strategy.", "content": "In the \u201cScoring Strategy\u201d, we ask the LLM to assign a score to each explanation for a window w using the Likert scale [11] (i.e., a score between 1 and 5). When the same explanation is generated by more than one model in H, we assign the same score to all the models that generated that explanation. At the end of this process, the model in H with the highest score is considered as the best."}, {"title": "2.2.3 The prompt.", "content": "Figure 1 shows the system prompts (i.e., the instructions for the LLM to perform the task) of our approach. For the sake of compactness, the picture depicts the system message for both strategies. Note that the yellow parts appear in the system prompt of both strategies, while the blue parts are only used for the \u201cBest-Among-K Strategy\u201d and the green parts are only used for the \u201cScoring Strategy\u201d. Note that the our prompt is the result"}, {"title": "3 EXPERIMENTAL EVALUATION", "content": "In the following, we will introduce the datasets that we considered for our work, the experimental setup adopted, the metrics and, finally, the main results."}, {"title": "3.1 Datasets", "content": "In order to compare our LLM-based method with user surveys, we obtained from the authors of [2] the surveys that they proposed to real non-expert users to compare the quality of explanations produced by three different XAI methods. Specifically, in their work they considered GradCAM [16] (a saliency based approach), LIME [13] (a posthoc explanations approach), and Model Proto-types [14] (a neural network model designed to be interpretable).\nThe authors considered two publicly available Smart-Home HAR datasets to create their surveys: MARBLE [1] and CASAS Milan [7]."}, {"title": "3.2 Experimental Setup", "content": "We implemented our LLM-based system in Python. We experimented with two different models: gpt-3.5-turbo-0125 and gpt-4-turbo, both accessed through the OpenAI APIs. The rationale behind the choice of these two models is to investigate whether the results would be consistent between an advanced model like GPT-4 and a cost-effective model like GPT-3.5. We set the models temperature to 0 to minimize the variability in the answers. To construct the prompts and interact with the APIs, we utilized the LangChain library. The code is publicly available \u00b9. Computations were executed in a Google Colab environment. Each experiment was repeated 5 times to ensure statistically robust results."}, {"title": "3.3 Metrics", "content": "The survey that we use in this paper asked the participants to rate each alternative explanation using a classic Likert scale, with a grade from 1 (absolutely not satisfying) to 5 (completely satisfying) [2]. The overall result was summarized by computing the average of the scores given by the users for each method and normalizing the results in the interval [0, 1]. To properly compare our results with the user surveys in [2], we also normalize the scores in the same range."}, {"title": "3.4 Results", "content": "Figures 3a and 3b show the results of our evaluation method on the MARBLE dataset considering both prompting strategies. We observe that our LLM-based evaluation is strongly aligned with the one of the user surveys, considering how methods are ranked. Specifically, both user surveys and LLM-based evaluation agree that the best approach is Model Prototypes, followed in order by LIME and GradCam. Note that we are not interested in the absolute values of the scores, but in their relative distance among different XAI methods. The results also indicate that, among the two LLM models, GPT-4 provides results (in terms of relative proportions between the scores) more aligned with the survey. However, the Best-Among-K strategy tends to penalize GradCam more than the user surveys. This is likely due to the fact that this strategy always assign 0 points to the \u201cworst\u201d explanations. Similar results can be observed in Figures 3c and 3e for the CASAS Milan dataset.\nAmong the two prompting strategies, the one that more accurately reflects the trend of the user surveys considered in the experiments is the \u201cBest Explanation Strategy\u201d, although it does"}, {"title": "4 CONCLUSION AND FUTURE WORK", "content": "In this work, we introduced the novel idea of using LLMs to automatically evaluate natural language explanations of XAI methods for sensor-based HAR. Our preliminary experiments show that our prompting strategies make it possible to leverage LLMs to obtain a quantitative assessment that is consistent with more challenging user surveys.\nThis is still a preliminary investigation, and we have several plans for future work. First, we will design prompt strategies for different target users. Indeed, in this work we focused on non-expert end-users. However, a similar type of evaluation can be carried out for expert users like technicians or data analysts having a different background. Such expert profiles may need explanations to improve the underlying model or the sensing setup. Moreover, since in this paper we only evaluated whether the explanation is in line with common-sense knowledge about the predicted activity, we will investigate many other aspects that are crucial for explanations (e.g., understandability, trustworthiness, reliability)[12]."}]}