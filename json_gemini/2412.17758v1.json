{"title": "In Case You Missed It:\nARC 'Challenge' Is Not That Challenging", "authors": ["\u0141ukasz Borchmann"], "abstract": "ARC Challenge appears more difficult than\nARC Easy for modern LLMs primarily due\nto an evaluation setup that prevents direct com-\nparison of answer choices rather than inher-\nent complexity. Although some researchers\nhave quietly shifted to a more appropriate\nscheme over the last year, the implications of\nthis change have yet to be widely acknowl-\nedged. We highlight this overlooked shift, show\nhow similar evaluation practices falsely imply\nreasoning deficits in other benchmarks, and\ndemonstrate that fairer methods dramatically\nreduce performance gaps (e.g. on SIQA) and\neven yield superhuman results (OpenBookQA).\nIn doing so, we reveal how evaluation shapes\nperceived difficulty and offer guidelines to en-\nsure that multiple-choice evaluations accurately\nreflect actual model capabilities.", "sections": [{"title": "Introduction", "content": "A substantial set of benchmarks regularly em-\nployed in LLM testing consists of multiple-choice\nproblems, commonly considered in a setup where\neach provided option is scored under the model,\nand the one with the highest likelihood is compared\nagainst the gold standard to determine accuracy.\nThis refers, among others, to popular evaluators of\nMMLU (Hendrycks et al., 2021), ARC Easy and\nChallenge (Clark et al., 2018), BoolQ (Clark et al.,\n2019), RACE (Lai et al., 2017), OpenBookQA (Mi-\nhaylov et al., 2018), PIQA (Bisk et al., 2019), SIQA\n(Sap et al., 2019), COPA (Gordon et al., 2011), and\nHellaSwag (Zellers et al., 2019).\nDetails of this setup differ but generally follow\none of the two conventions. Under one conven-\ntion, the model considers each candidate answer in\nseparation, without alternative options displayed\n(Figure 2), while under the other, the model sees\nall candidate options together in the prompt (Fig-\nure 3). We argue that the first setup is commonly\noverused and rarely preferred since it does not simu-\nlate the natural reasoning context in which multiple"}, {"title": "1.1 Hardly answerable in separation", "content": "Consider the question, \u2018Which of these items con-\ntains only a solution?' Given the option 'a jar of\npickles,' confronting a single item with a question\nand assessing whether pickles fulfill the definition\nof the solution suffices. They do not, so this option\nis incorrect. The question can be addressed under\nboth evaluation setups because it does not require\nthe availability of other options, such as 'a can of\nmixed fruit.'"}, {"title": "Impact on evaluation results", "content": "Figure 4 shows the difference in model accuracy\nwhen options are presented in isolation versus all\nat once. Not surprisingly, different setups hugely\nchange the evaluation results, partly because of\nthe vast presence of \u2018hardly answerable in separa-\ntion' questions and partially because such a setup,\nequivalent to what human test takers see, doesn't\nintroduce unnecessary obstacles.\nFor example, switching from separation to\noptions improves the Llama 3.1 70B ARC Chal-\nlenge accuracy from 64% to 93%, rendering this\nARC subset significantly less challenging. More-\nover, since the procedure change has a much higher\nimpact on ARC Challenge than on ARC Easy,\nswitching reduces the accuracy gap between these\nsubsets as much as six-fold (Figure 1). These find-\nings suggest that the previously perceived difficulty\nwas primarily an artifact of the evaluation method\nrather than the tasks' complexity.\nThe difference seems somewhat known in the\nLLM community, but not broadly, and needs to be\nstated explicitly. E.g. concerning the Llama family,\nauthors seem to silently switch from separation\nto options between Llama 2 and Llama 3, simi-\nlar to Mistral between Mixtral 8x7B and Mixtral\n8x22B, or DeepSeek before their V2 (detailed as-\nsessment available in Appendix A)."}, {"title": "Are other benchmarks affected?", "content": "Yes. Analogous changes in evaluation procedures\nwould vastly improve OpenBookQA scores. Con-\ncerning Llama 3.1 70B, one can achieve improve-\nment from 48% to 89% (see Figure 5). For\nsome reason, most authors who switched from\nseparation to options in ARC evaluation did\nnot follow on some other multi-choice problems.\nIf they switched, they could notice that Open-\nBookQA is essentially solved, as current models\nachieve scores above human performance (92%\ncompared to 96% of Qwen 2.5 72B).\nIn the case of SIQA, reformulation leads to a\n24% increase in Llama 70B accuracy. However, the\nbest models perform 5% below the human baseline,"}, {"title": "Why does it matter?", "content": "We argue that the benchmark's challenge should\nresult from the inherent complexity of the knowl-\nedge or reasoning required, not its formulation or\nevaluation procedure.\nThe separation setup is unnecessarily compli-\ncated and not consistent with how humans would\napproach the multi-choice problem, leading to ex-\nisting assessments of human performance being\nincompatible. For example, the fact that strong\nLLMs perform 30% worse than humans on SIQA\ndoesn't mean they are deficient in commonsense\nreasoning about social situations if under options\nthe difference largely disappears. This mismatch\ncan falsely suggest deficits in reasoning capabilities\nthat are not truly present."}, {"title": "Suggestions for multi-choice eval", "content": "There are many arguments for using the options\nfor the evaluation of multi-choice QA problems.\nWe have already described a few, including the\npresence of 'hardly answerable in separation' ques-\ntions and the fact it is consistent with the usual ap-\nproach to assessing human performance, as humans\nnaturally consider all choices in a single context.\nOther benefits include enabling compatible eval-\nuation in a likelihood and generative manner, allow-\ning one to obtain comparable scores with LLMs\nbehind closed and limited APIs. Moreover, it\neliminates the need to decide which normalization\nmethod to use when aggregating scores from sev-\neral option tokens, which is, to some extent, arbi-\ntrary and impacts model ranking."}, {"title": "5.1 Why likelihood scoring in the first place?", "content": "Likelihood-based scoring is a natural choice for\nproblems from pure language modeling, Winograd\nschemas, or fill-in-the-gap setups such as encoun-\ntered in LAMBADA (Paperno et al., 2016), Hel-\nlaSwag (Bisk et al., 2019), or WindoGrande (Sak-\naguchi et al., 2019) datasets.\nFor other problems, it is effectively a variant\nof constrained decoding, that is, the model is re-\nstricted to selecting from given candidate options\nrather than generating open-ended text. It guaran-\ntees that models will not emit CoTs before answer-\ning the question and removes the need for output\npostprocessing, such as extracting the letter asso-\nciated with the selected option and normalizing\nits casing. Moreover, it allows us to obtain mean-\ningful results with base models, e.g. intermediate\ncheckpoints from LLMs' self-supervised pretrain-\ning since we are constraining the output to one of\nthe most probable options under the model."}, {"title": "5.2 To show, or not to show options", "content": "Suppose the options are of equal length, and it is\nnot helpful to consider them simultaneously. This\nis the case when we deal with a straightforward\nyes/no response, and no comparative reasoning is\nnecessary, as in the BoolQ dataset (Clark et al.,\n2019). In similar scenarios, there are no arguments\nfor dropping separation in favor of options\nWe are in the position that the options variant\nis preferred if there is a risk of a 'hardly answerable\nin separation' question presence (Section 1.1) or it\nsimply makes it easier to consider all of the options\nat once because it ensures the model can leverage\ndirect comparisons. This seems to be the case for\nvirtually all other multi-choice QA problems, such\nas MMLU (Hendrycks et al., 2021), ARC (Clark\net al., 2018), OpenBookQA (Mihaylov et al., 2018),\nPIQA (Bisk et al., 2019), or SIQA (Sap et al., 2019).\nIn fact, most similar problems are already being\nevaluated in the options scheme.\nArguably, options has some possible or actual\ndisadvantages: the order of the presented choices\nmight impact the evaluation results (e.g. models\nmight bias toward the first listed choice), it might be\neasier to exploit pattern recognition, and the setup\nrequires slightly more compute. Nevertheless, we"}, {"title": "Summary", "content": "We draw the community's attention to shifting from\nevaluating answers in isolation to evaluating them\nalongside all other options. Over the last year, such\na change happened in the reported ARC Challenge\nand ARC Easy scores, vastly impacting their eval-\nuation results. After discussing the implications,\nwe considered whether other popular benchmarks\nmight undergo similar reformulation, identifying\nOpenBookQA and SIQA as candidates. In the\nformer, recent models outperform humans, even\nthough there is a room of 40% between humans\nand LLMs in the widespread setup. The fact that\nthe gap drastically narrows under the all-options\nevaluation method highlights how the testing for-\nmat can distort perceived difficulty.\nWe concluded with a guideline for evaluat-\ning multi-choice problems, arguing that the setup\nwhere the model sees all options is preferred over\nconsidering each answer separately, except for ca-\nsual or masked language modeling problems."}, {"title": "Claiming setup used by other authors", "content": "As authors of LLM technical reports rarely or never\nprovide such details, we redo their evaluations in\noptions and separation setups. If the reported\nscore is in the same ballpark as one of these, and\nvisibly distant from the other one, we claim they\nused the first. This assessment is backed by the no-\ntion that no other change in the prompt could cause\na 20%+ improvement in ARC Challenge scores.\nThe exception could be using a generative setup\nwith CoT for some heavy reasoners, but we do not\nsuspect authors to use CoT if they are not reporting\nthis because it would be a serious flaw.\nThe results of this analysis for the ARC Chal-\nlenge are presented in Table 1. Prompt \u2018reverse\nengineering' becomes more troublesome in the con-\ntext of SIQA and OpenBookQA datasets (Table 2-\n3) as some authors do not directly report scores but\naverage them with other commonsense reasoning\nproblems. We're not claiming any setup for these.\nFinally, some authors tackling OpenBookQA\nfollowed Brown et al. (2020) in normalizing the\nlikelihood by the likelihood of the completion given\n'Answer:' as context. To address this possibility,\nwe introduce two additional variants referred to as\nseparation band options b"}, {"title": "Estimating number of questions hardly\nanswerable in separation", "content": "To determine whether questions are answerable\ngiven a single option or require the context of\nother options, we process them in batches of 20\nusing gpt-40-2024-11-20 model and the follow-\ning prompt with few-shot examples:\nConsider the question, \"Which of these items\n\u2192 contains only a solution?\" Given the\noption \"a jar of pickles,\" confronting a\n\u2192 single item with a question and assessing\n\u2192 whether pickles fulfill the definition\nof the solution suffices. They do not, so\nthis option is incorrect.\nNow let us think about \"Which of these most\n\u2192 likely has the greatest mass?\" and the\n\u2192 option \"puppy.\" It can be considered only\n\u2192 with other options because it is the\n\u2192 greatest compared to \"chicken\" or \"lizard\n\" but not in the context of \"horse\" or\n\u2192 elephant\".\nThese questions represent two classes of\n\"\n\u2192 questions: \"answerable without other\n\u2192 options\" and \"unanswerable without other\n\u2192 options\".\nOther examples of \"answerable without other\n\u2192 options\" are:\nKerry made a simple flashlight. She recorded\nthe following statements in her lab book.\nWhich statement is an inference? (\nAnswerable, because it suffices to\ncompare options against the definition of\ninference)\nA scientist on a field trip discovered a new\n\u2192 organism. She examined its cells under a\nmicroscope and observed several different\n\u2192 structures, including a nucleus, a cell\n\u2192 wall, and some chloroplasts. This\norganism would correctly be classified in\nwhich of the following kingdoms? (\n\u2192 Answerable, because it can be answered by\ndeciding if the kingdom provided in the\n\u2192 option can be associated with having a\nnucleus, a cell wall, and chloroplasts)\nMany types of motion occur in our solar system.\n\u2192 Which type of motion describes one Earth\nyear? (Answerable, because it suffices\n\u2192to validate if the motion describes one\nyear or not)\nWhen trees develop leaves in the spring, 10\nchanges occur on the forest floor. Why\n\u2192 does the development of leaves cause\nchanges on the forest floor? (Answerable,\nbecause it is enough to verify if a\nparticular option described the possible\ncause of change)\nUsing a softball bat to hit a softball is an\n\u2192 example of using which simple machine? (\nAnswerable, because all one needs to do\nis to check if the described simple\n\u2192 machine is the explanation of how a\n\u2192 softball bat works)\nWhich is a statement about climate? (\n\u2192 Answerable, because it is possible to\n\u2192verify a single option against the\n\u2192 climate definition)\nHow do word processors on computers benefit\n\u2192 most students? (Answerable, because it\n\u2192 can be answered in separation whether\n\u2192 most students benefit from this feature\nof the word processor)\nPhotosynthesis occurs in which of these\n\u2192 organisms? (Answerable because it\n\u2192 suffices to check if the organism\n\u2192 mentioned in the option performs\n\u2192 photosynthesis)\nWhich two theories of Moon formation propose\n\u2192 that much or all of the material\n\u2192 comprising the Moon came from Earth? (\nBecause it suffices to validate if both\n\u2192 theories mentioned in a single option\ndescribe the Moon as formed from Earth\n\u2192 material)\nPlants and animals are composed of organic\ncompounds. Which of the following are the\ncommon elements found in organic\ncompounds? (Answerable, because it\nsuffices to check if the option consists\nof compounds appearing in both plants and\nanimals)\nOther examples of \"unanswerable without other\n\u2192 options\" are:\nA ball is dropped from different heights. When\nthe ball is dropped from the highest"}, {"title": "C Evaluation details", "content": "All evaluations were conducted using 1m_eval\n1170ef9 (Gao et al., 2024). We used HF imple-\nmentations and base variants of models (exact ver-\nsions in Table 4) with either default prompts and\nacc_norm metric or prompts outlined below.\nInferences were performed with bf16 precision,\nflash attention (whenever available), and dynamic\nbatch size, using transformers 4.47.0 and torch\n2.5.1 on eight NVIDIA H100 GPUs.\nConcerning ARC Easy and Challenge datasets,\nfor the separation setup, we follow the standard\n1m_eval configuration with:\ndoc_to_text: \"Question: {{question}}\\nAnswer:\"\ndoc_to_target: \"{{choices.label.index(answerKey)\n}}\"\ndoc_to_choice: \"{{choices.text}}\"\nIn contrast, for the options setup, we use:\ndoc_to_text: ! function arc_utils.doc_to_text\ndoc_to_target: \"{{choices.label.index(answerKey)\n}}\"\ndoc_to_choice: \"{{choices.label}}\"\nwith doc_to_text() defined as:\ndef doc_to_text(doc):\nprompt = \"Question: \" + doc[\"question\"] + \"\\\n\u2192 noptions: \\n\"\nfor 1, t in zip (doc[\"choices\"][\"label\"], doc\n\u2192[\"choices\"][\"text\"]):\nprompt += 1+'. ' + t + '\\n'\nprompt += \"Answer:\nreturn prompt\nAnalogous changes were introduced to Open-\nBookQA and SIQA templates."}]}