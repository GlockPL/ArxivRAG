{"title": "Generating Visual Stories with Grounded and Coreferent Characters", "authors": ["Danyang Liu", "Mirella Lapata", "Frank Keller"], "abstract": "Characters are important in narratives. They move the plot forward, create emotional connections, and embody the story's themes. Visual storytelling methods focus more on the plot and events relating to it, without building the narrative around specific characters. As a result, the generated stories feel generic, with character mentions being absent, vague, or incorrect. To mitigate these issues, we introduce the new task of character-centric story generation and present the first model capable of predicting visual stories with consistently grounded and coreferent character mentions. Our model is finetuned on a new dataset which we build on top of the widely used VIST (Huang et al., 2016) benchmark. Specifically, we develop an automated pipeline to enrich VIST with visual and textual character coreference chains. We also propose new evaluation metrics to measure the richness of characters and coreference in stories. Experimental results show that our model generates stories with recurring characters which are consistent and coreferent to larger extent compared to baselines and state-of-the-art systems.", "sections": [{"title": "Introduction", "content": "An integral part of storytelling is creating interesting and believable characters. In fact, it is not uncommon for writers to conceptualize their characters visually before crafting a plot around them. This character-centric approach enhances narrative coherence, leading to richer, engaging, and emotionally resonant stories. Recent research has explored how character-centric datasets (Porvatov et al., 2024; Chen et al., 2023; Brahman et al., 2021) as well as different representations of characters might contribute to the automatic analysis and generation of narratives (Gurung and Lapata, 2024; Inoue et al., 2022; Bamman et al., 2013; Kim et al., 2018a; Liu et al., 2020; Brahman et al., 2021; Chen et al., 2024; Li et al., 2024; Xu et al., 2024; Yu et al., 2022; Yang et al., 2023b, 2022).\nThe importance of characters seems to go unnoticed in narrative tasks spanning multiple modalities such as visual storytelling, which involves narrating a story based on a sequence of images. Existing approaches focus on detecting objects in images and discovering relationships between them; characters are treated the same as other objects, without any special consideration in the generation process. For example, the most popular approaches to visual storytelling (Wang et al., 2024; Chen et al., 2021; Hsu et al., 2020; Yang et al., 2019) exploit external knowledge bases to enrich and link detected concepts. While such methods can describe a coherent sequence of events, they fail to effectively ground character mentions to their visual depictions and generate character-centric stories. As a result, character mentions are often absent, vague (e.g., rendered with plural references such as \"they\" or \"we\"), or incorrect, and the stories feel generic, lacking in detail and context.\nIt is not possible to create character-centric stories without knowing who the characters are. We therefore propose to identify them in an image sequence together with their visual coreference chains. In Figure 1, different characters are segmented using different colors, and are assigned a unique ID. Characters with the same ID in different images form a coreference chain. There are five characters in the image sequence (labels 1\u20135), and four coreference chains (characters 1 and 4 are shown in three images, character 2 is shown in two, while characters 3 and 5 are depicted only once). We would then expect a generation model that takes these annotations into account to refer to the same character consistently, creating textual coreference chains which are nevertheless grounded. Again, in Figure 1 \u201cElla\u201d and \u201cshe\u201d refer to visual character 4, whereas \u201cElla's two younger siblings\" refers to characters 2 and 5. Note that without forcing the model to explicitly ground character mentions in images, there is no way of knowing which visual characters the model is talking about.\nIn this work, we introduce the task of character-centric visual story generation, which aims to create stories with character mentions grounded to the input image sequence. As explained earlier, the task requires understanding coreference relationships between visual characters and aligning textual character mentions to their visual counterparts. This type of grounding is key to generating coherent and detailed stories, anchoring textual mentions to specific visual appearances, and keeping the story closely linked to the image context.\nPerhaps unsurprisingly, existing datasets do not contain annotations such as those shown in Figure 1. We augment the widely used VIST dataset (Huang et al., 2016) with visual and textual character annotations following a fully automated pipeline. We use OTTER-LLAMA-7B (Li et al., 2023), which has been instruction-tuned for multiple image understanding, as our backbone model and train it on image sequences and stories enriched with visual and textual coreference chains (see Figure 1). We render visual character chains into visual prompts and enforce character grounding in the generated stories via a new format which links character mentions to visual ids. We further introduce novel automatic metrics which assess character richness and coreference in stories, and propose to use LLMs as evaluators (LLM-as-a-judge) to perform side-by-side comparisons of stories generated by different systems. The contributions of our work can be summarized as follows:\n\u2022 We introduce the new task of character-centric story generation, and present the first model capable of generating stories with consistent and grounded character mentions.\n\u2022 Recognizing the lack of character annotations in visual stories, we enrich the VIST benchmark (Huang et al., 2016) with visual and textual character coreference chains and their corresponding alignment. The new benchmark, which we call VIST++, contains detailed annotations for 300K unique characters over 40K visual stories.\n\u2022 We propose new evaluation methods to measure the richness of characters and coreference in stories. We further replace costly human evaluation with an LLM-as-a-Judge approach, which we use to compare visual stories along various dimensions (e.g., specificity, coherence, grounding)."}, {"title": "Character-centric Visual Story Generation", "content": "Our work aims to address limitations of current visual storytelling systems, which struggle to accurately recognize character coreference relationships in the input images. Different from traditional visual storytelling tasks, where the input is an image sequence and output is a story, our character-centric approach takes image sequences and their corresponding visual character coreference chains as input, and produces a story with grounded and coreferring characters as output. By explicitly linking textual characters to their visual counterparts, we ensure character consistency, reduce vague or incorrect references, and facilitate the generation of more accurate, detailed, and engaging narratives. In the following, we first discuss how we automatically augment VIST (Huang et al., 2016) with character chains (Section 2.1) and then present our visual story generation model (Section 2.2)."}, {"title": "The VIST++ Dataset", "content": "We developed an automated pipeline to enrich VIST (Huang et al., 2016) with detailed character annotations, including fine-grained character labels in images and aligned textual coreference chains. The resulting VIST++ dataset contains 40K visual stories, with 300K unique characters, each associated with segmentation masks across images and textual coreference chains, amounting to a total of 520K character appearances.\nOur annotation pipeline includes three subtasks: (1) visual character coreference, i.e., identifying characters in the image sequence and grouping those that are the same person into coreference chains; (2) textual character coreference involves detecting character mentions and identifying coreference chains in the story text; and (3) multimodal alignment links textual to visual coreference chains, yielding multi-modal chains. We next introduce our automatic procedure for each task."}, {"title": "Visual Character Coreference", "content": "To obtain visual character chains, we first detect characters in an image sequence and then identify which detections represent the same character, thereby constructing coreference chains.\nPrevious visual coreference methods (Schroff et al., 2015; Liu and Keller, 2023) group bounding boxes (e.g., based on facial features) directly into a fixed number of clusters which should ideally vary from image to image. Our approach differs in two respects. Firstly, we use segmentation masks rather than bounding boxes; the latter are well suited to rectangular shapes, but are less accurate with objects that have irregular boundaries. Moreover, segmentation masks can distinguish between multiple overlapping objects \u2013 in our case images can contain densely populated background characters. Secondly, we propose to cluster segmentation masks in chains following an incremental algorithm (shown in Figure 2a) which does not require the number of clusters to be known in advance and does not depend on facial features, which often fail to detect side-views of faces (in visual stories).\nGiven an image sequence, we first apply a pretrained object detector and retain detections with the label Person and a confidence score higher than 0.9 which cover at least 10% of the image (to remove background characters). Visual chains are constructed incrementally: First, all characters detected in the first image are considered new and added to the chain. Subsequently, for every new character detected in imagek+1, we compute the pairwise similarity between this character and clusters from previous images0:k, thus obtaining a similarity matrix. We construct a bipartite graph and the maximum weighted bipartite matching is computed using the Hungarian algorithm (Kuhn, 1955). Nodes in this bipartite graph represent crops (character instances cropped using their segmentation masks), while edge weights correspond to visual similarities between crops. The algorithm finds a one-to-one alignment that maximizes the sum of the similarities between crops.\nWe added a threshold condition to the Hungarian algorithm which allows to create new clusters if new characters are not visually similar to existing ones. Specifically, if a character detection does not result in a match with a similarity value higher than a threshold, it is added as a new character to the visual coreference chains. An illustration of our algorithm is shown in Figure 2 and a more formal description in Appendix A. We employed DETR (Carion et al., 2020) to detect character bounding boxes, and SAM (Kirillov et al., 2023) to obtain segmentation masks for them. We used OpenCLIP ViT-G/14 (Ilharco et al., 2021) for visual feature extraction and measured the visual similarity between two crops with LLAVA1.5-13B (Liu et al., 2024a) (i.e., by providing two crops and asking the model whether they represent the same character)."}, {"title": "Textual Character Coreference", "content": "Current state-of-the-art coreference resolution models generally struggle to effectively handle the coreference of plural nouns (Liu and Keller, 2023; Le and Ritter, 2023; Hicke and Mimno, 2024), which are very common in stories. Our proposed coreference resolution method is based on LLMs (see Figure 2b); through in-context learning, we are able to detect coreference relationships for characters denoting a single and multiple entities (i.e., plural and collective nouns). More specifically, our method consists of two steps:"}, {"title": "Character Mention Detection", "content": "Given a visual story, we detect all entities mentioned therein using spaCy (Honnibal and Montani, 2023) and mark these with square brackets, denoting a span to annotate in the next step (e.g., James \u2192 [James]). We identify character-specific entities with a zero-shot prompting method which takes the story as input and spaCy entities and identifies via question-answering whether these denote persons (either groups or singular entities). Our full prompt is provided in the Appendix (see Table 7) and an illustration in Figure 2b (Step 1)."}, {"title": "Character coreference Resolution", "content": "Using in-context learning, we annotate character spans with cluster IDs (e.g., [James] \u2192 [James](#1)). We use five examples, and a prompt that instructs the LLM to annotate mentions within square brackets with markdown tags to indicate clusters, resulting in the format [mention](#cluster_name). Note that plural mentions can refer to more than one characters (e.g., [We] \u2192 [We](#1, #2)). For an illustration, see Figure 2b (Step 2); the full prompt is given in the Appendix (see Table 9). We used LLAMA3-70B (Touvron et al., 2023) to identify character mentions and obtain textual chains."}, {"title": "Multimodal Character Alignment", "content": "Finally, we align textual and visual coreference chains. We again model this alignment as a bipartite graph matching problem. Specifically, we create a matrix where each cell represents the similarity between a textual and visual chain and apply the Hungarian algorithm to find the best match. We measure chain similarity following the method proposed in Liu and Keller (2023) which compares the distribution of characters in images with their distribution in sentences. For instance, if a character is mentioned in the first and second sentence, and there are two coreferent visual detections in the first and second image, then these are likely to refer to the same character.\nMore formally, a C-dimensional binary vector (where C is the number of images or sentences\u00b9) represents the distribution of a character. Let Ci denote the i-th textual/visual character, then Ci[k] = 1 refers to the i-th character being present in the k-th sentence/image. The similarity between a visual and atextual character is computed as the dot product of their respective binary vectors."}, {"title": "Quality Evaluation", "content": "We evaluated our automatic pipeline against VIST-Character (Liu and Keller, 2023), a high-quality, human-annotated dataset containing rich character annotations for 770 visual stories from the VIST (Huang et al., 2016) test set, including visual and textual coreference chains and their alignments."}, {"title": "Visual Story Generation Model", "content": "Our generation model is trained on VIST++. It is able to identify characters across multiple images and generate stories with multimodal character chains. We leverage the visual understanding and text generation capabilities of Large Vision-Language models (LVLMs). Specifically, we employ OTTER (Li et al., 2023) as our backbone model, which has been instruction-tuned for multiple image understanding. OTTER (Li et al., 2023) is the instruction-tuned version of OpenFlamingo (Awadalla et al., 2023) (an open-source replication of DeepMind's Flamingo models). Openflamingo comprises a LLaMA-7B (Touvron et al., 2023) language encoder and a CLIP ViT-L/14 (Radford et al., 2021) vision encoder. The two modalities are interleaved through gated cross-attention layers that allow the model to combine information from the visual and textual streams.\nTraining We finetune OTTER (Li et al., 2023) on pairs of image sequences and their corresponding stories. In VIST++, characters are outlined with segmentation masks, where same color indicates same characters, i.e, a coreference chain (see Figure 1). Characters are further labeled with a unique ID number, overlaid in the center of the mask, and characters with the same ID in different images are assumed to be the same person. These visual annotations or marks (Yang et al., 2023a) serve as visual prompts to OTTER.\nDuring training, the model learns to predict a story with character grounding and coreference information. Aside from visual prompting, grounding is facilitated by training the model to verbalize textual character chains and their grounding. The model learns to predict textual chains, i.e., it marks character mentions with a special symbol, and is explicit about which visual segmentation they refer to (e.g., [Ella](#4) refers to visual segment 4, [she](#4) refers to Ella and segment 4, whereas [We](#1, #2) refers to segments 1 and 2).\nWithin OTTER, we freeze the parameters of the language encoder (LLAMA-7B) and the vision"}, {"title": "Experiments", "content": "We perform experiments on VIST (Huang et al., 2016), which is the most widely used visual storytelling dataset; it contains 10,117 Flickr albums and 210,819 unique photos. Each training sample consists of k = 5 images and a corresponding story of k = 5 sentences. Training, validation, and test sets contain 40,155, 4,990, and 5,055 unique stories, respectively. Our model is trained on VIST++, making use of the character-centric annotations discussed in Section 2.1. We report results on the original VIST test set (no annotations) and also on the VIST-Character subset (Liu and Keller, 2023) with gold-standard annotations when evaluating character specific properties."}, {"title": "Implementation Details", "content": "We finetuned OTTER with a learning rate of le-5, batch size of 32, and warm-up ratio of 0.05. During inference, we employed greedy decoding. We accelerate the training process using the DeepSpeed framework (Rasley et al., 2020)."}, {"title": "Evaluation", "content": "Many previous studies (Liu et al., 2023; Hsu et al., 2021a, 2020; Hu et al., 2020; Yang et al., 2019; Modi and Parde, 2019) have highlighted the limitations of metrics based on lexical matching for story evaluation. They correlate poorly with human judgments and do not effectively measure semantic similarity with human-written stories or the lexical richness of the generated stories. In this work, we employ story-specific metrics to assess various aspects of story quality, including diversity, grounding, naturalness, and readability. We also propose automatic metrics that assess character richness and the accuracy of coreference chains. Finally, we introduce LLM-as-Judge evaluators (Zheng et al., 2024; Liu et al., 2024b; Liusie et al., 2023) to perform binary comparisons of stories generated by different systems."}, {"title": "Diversity", "content": "We use Inter-story Repetition (Yao et al., 2019; Goldfarb-Tarrant et al., 2020), which examines trigram repetition across stories to measure diversity. High inter-story repetition suggests that the model tends to generate the same story even when conditioned on different image sequences."}, {"title": "Grounding", "content": "We use GROOVIST (Surikuchi et al., 2023) to assess whether the stories accurately represent the content of the image sequences. GROOVIST employs CLIPScore (Hessel et al., 2021) to compute the similarity between noun phrases in the story and bounding boxes in the images, producing an average score which favors stories with concrete words as they are more likely to be visible."}, {"title": "Naturalness", "content": "We use MAUVE (Pillutla et al., 2021) to measure the naturalness of the stories. MAUVE has a high correlation with human judgments and computes the similarity between the distributions of human- and machine-generated texts."}, {"title": "Reading Difficulty", "content": "We employ the Flesch-Kincaid Grade Level (FKGL) (Kincaid et al., 1975) to measure the reading difficulty of stories. FKGL is a well-established readability formula used for text quality assessment:\nFKGL = a*$\\frac{Nword}{Nsentence}$ +b*$\\frac{NS syllable}{Nword}$ + c\nwhere a is 0.39, b is 11.8, and c is -15.59 as defined in Kincaid et al. (1975). The FKGL values denote reading age, ranging from 0 to 18, So, lower values suggest the text is easier to read and higher values denote increased reading difficulty."}, {"title": "Character Richness and Coreference", "content": "We report the number of characters and character mentions in generated stories as measures of character richness. We also evaluate the accuracy of multimodal coreference chains using the B\u00b3 score (Cai and Strube, 2010b). We only use B\u00b3 precision, as we are more interested in the correctness of mentions within a predicted chain rather than ensuring that every gold-standard visual character is"}, {"title": "Results", "content": "Our experimental results are summarized in Table 4. The first rows present the performance of state-of-the-art storytelling systems from the literature. MCSM (Chen et al., 2021) exploits a commonsense knowledge graph to represent concepts depicted in the images and their relations, and uses a Maximal Clique Selection Module to identify which ones to write a story about. MCSM uses BART-large (Lewis et al., 2020) to generate the story based on selected concepts (and image features). Iter-Blueprint (Liu et al., 2023) is built on top of BART-base and leverages a sequence of question-answer pairs as a blueprint (story plan) for selecting salient visual concepts and determining how to present them in the story. The model employs an incremental generation strategy, gradually constructing the blueprint and its corresponding story sentence-by-sentence.\nSOTTER++ is our story generation model based on OTTER and finetuned on the proposed VIST++ dataset, whereas SOTTER is finetuned on vanilla VIST and does not have any specific knowledge about characters or their grounding. Finally, Table 4 includes results for GPT-4V2 (zero-shot setting, prompt in Figure 8) which is one of the most performant multimodal LLMs. All models in Table 4 were evaluated on the same VIST test set.\nSOTTER++ leads on character-centric metrics. The number of characters and their mentions in stories generated by SOTTER is similar to that of previous visual storytelling systems. However, the number of characters and frequency of character mentions increase substantially for SOTTER++, which is trained on VIST++. In addition, the accuracy of the coreference chains improves by more than 5%. This indicates that VIST++ effectively enables LVLMs to generate stories with richer characters (more unique characters and mentions) and more accurate character chains (higher coreference scores). Although GPT-4V generates more characters than SOTTER++, it does so at the expense of succinctness and readability, whereas SOTTER++ produces stories with lengths and readability scores close to those of human-written stories.\nLLM-based models improve story diversity. We observe that LLM-based models (i.e., OTTER variants, ~9B parameters; GPT-4V, undisclosed parameter count) generate more diverse stories compared to MCSM and Iter-Blueprint, which are based on smaller language models like BART (~300M parameters). This indicates that the extensive prior knowledge of LLMs prevent them from overfitting on the VIST dataset, which would otherwise result in similar stories for different images. Additionally, SOTTER++ creates more diverse stories than SOTTER. Character coreference annotations help the model generate stories that are more target at images provided, avoiding repetition.\nExisting grounding metrics fall short of evaluating character grounding. SOTTER++ performs"}, {"title": "leads in side-by-side evaluation.", "content": "Table 6 summarizes pair-wise comparison results using GPT-40 as an evaluator. Specifically, we compare SOTTER++ against (a) the iterative Blueprint model (Iter-BP); (b) SOTTER trained on VIST without character annotations; and (c) gold-standard stories written by humans. We observe that SOTTER++ outperforms the iterative Blueprint model across all metrics and is superior to SOTTER across all dimensions (with the exception of coherence). When compared to gold-standard stories written, SOTTER++ achieves a win rate of 36.9%, and is better at generating stories with explicit and recurring characters (see characters metric). This suggests that even human-written stories in the test set tend to use vague plural pronouns and avoid clear and recurring character usage."}, {"title": "generates character-centric stories with consistent mentions.", "content": "Figure 3 shows example stories created by the models used in our evaluations, as well as by GPT-4V. We observe that previous state-of-the-art models, such as MCSM, have poor character mention capabilities, replacing character names with [female] or [male]. While the stories generated by Iter-BP exhibit more human-like language, they contain hallucinations, as evidenced by the low Grounding scores in Table 4. Additionally, most characters in the story are generic (e.g., friends), lacking explicit mentions to recurring individuals.\nIn the stories produced by SOTTER++, we observe recurring characters represented by consistent coreference cluster labels. Even for plural characters (denoted by \u201cthey\u201d, for instance), SOTTER++ predicts the specific individuals they refer to. Furthermore, the story language is similar to the style of the human story. GPT-4V stories have the highest reading difficulty (RDD; see Table 4): They are long, use rare words, and employ a flamboyant style which differs from how humans write visual stories. In contrast, fine-tuned models better match the readability level of human-authored stories. There is also a significant error in the story generated by GPT-4V in Figure 3: It incorrectly assumes that Roe is holding a sign (last story sentence), whereas the image depicts her younger brother. This indicates that even powerful LVLMs like GPT-4V are unable to accurately identify the same character across a sequence of images."}, {"title": "Related Work", "content": "Visual Storytelling Huang et al. (2016) proposed visual storytelling as a way to create AI systems that can comprehend event sequences and generate narrative language which goes beyond just describing what is shown in images. Early methods (Gonzalez-Rico and Fuentes-Pineda, 2018; Kim et al., 2018b) used simple encoder-decoder models with CNNs for visual features and RNNs for text generation. Recent approaches (Wang et al., 2024; Chen et al., 2021; Hsu et al., 2020; Yang et al., 2019) leverage external knowledge resources like ConceptNet to enhance commonsense reasoning abilities. Some methods (Lu et al., 2016; Hong et al., 2020; Wang et al., 2020) also utilize scene graphs to model object relationships.\nFew approaches have focused on character-centric story generation. Surikuchi and Laaksonen (2019) extract characters from VIST, analyze their relationships and exploit them for focusing attention to relevant visual segments during story generation. However, neither character coreference nor character grounding are explicitly modeled. Liu and Keller (2023) introduce the VIST-Character dataset with rich character and coreference annotations, however due to its small size, it can only serve as a test set.\nMost existing approaches (Xu et al., 2021; Hsu et al., 2020; Wang et al., 2020; Yang et al., 2019)"}, {"title": "Conclusions", "content": "In this work, we proposed the new task of character-centric story generation and introduced a new dataset, VIST++, which extends the widely use VIST dataset with visual and textual character grounding and coreference. The dataset comprises annotations for 300K unique characters over 40K visual stories. We then described a new model for character-centric story generation; this model was finetuned on VIST++ and built on OTTER, a pre-trained large vision and language model. Our evaluation showed the richness of the characters and the accuracy of grounding and coreference in the stories our model generates. Furthermore, we propose an LLM-as-a-Judge evaluation which demonstrates that stories generated by our model are preferred over stories generated without character grounding and coreference."}, {"title": "Character Clustering Algorithm", "content": "In this section we formally present our incremental clustering algorithm. We also explain how we compute pairwise character matching with a QA method based on LVLMs which we experimentally found to be superior to the more widely used cosine similarity of visual features.\nSpecifically, we provide crops of two characters to a LVLM and ask whether they refer to the same individual. Since character crops often include background noise (e.g., other characters within the bounding box, such as two characters hugging), we employ fine-grained visual prompting (Yang et al., 2024) to focus the model's attention on the primary character in the crop. We compute visual similarity using LLAVA1.5-13B (Liu et al., 2024a). As this model does not support multi-image input, we concatenate the images of the two characters into a single image. The prompt we used is: Are the person on the left and the person on the right the same? Output yes or no ONLY."}, {"title": "LLM-as-a-Judge Details", "content": "We utilized a collection of human evaluation results from four VIST studies: KG-Story (Hsu et al., 2020), PR-VIST (Hsu et al., 2021a), Streth-VIST (Hsu et al., 2021b), and Iter-Blueprint (Liu et al., 2023). Specifically, the first four datasets include annotations for side-by-side overall-level comparisons of two stories, from which we randomly selected 1,000 story pairs. In the Iter-Blueprint dataset, we used the full set of fine-grained annotations for 100 stories, covering aspects such as coherence, engagement, grounding, and overall quality.\nAdditionally, the authors randomly selected 100 story pairs from Iter-Blueprint and VIST gold stories for manual annotation, focusing on specificity and character-based side-by-side comparisons. The criteria of each of these metrics can be found in Table 10."}]}