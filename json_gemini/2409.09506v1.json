{"title": "ESPNET-EZ: PYTHON-ONLY ESPNET FOR EASY FINE-TUNING AND INTEGRATION", "authors": ["Masao Someki", "Kwanghee Choi", "Siddhant Arora", "William Chen", "Samuele Cornell", "Jionghao Han", "Yifan Peng", "Jiatong Shi", "Vaibhav Srivastav", "Shinji Watanabe"], "abstract": "We introduce ESPnet-EZ, an extension of the open-source speech processing toolkit ESPnet, aimed at quick and easy development of speech models. ESPnet-EZ focuses on two major aspects: (i) easy fine-tuning and inference of existing ESPnet models on various tasks and (ii) easy integration with popular deep neural network frame-works such as PyTorch-Lightning, Hugging Face transformers and datasets, and Lhotse. By replacing ESPnet design choices inherited from Kaldi with a Python-only, Bash-free interface, we dramatically reduce the effort required to build, debug, and use a new model. For example, to fine-tune a speech foundation model, ESPnet-EZ, compared to ESPnet, reduces the number of newly written code by 2.7x and the amount of dependent code by 6.7x while dramatically re-ducing the Bash script dependencies. The codebase of ESPnet-EZ is publicly available.", "sections": [{"title": "1. INTRODUCTION", "content": "The Kaldi automatic speech recognition (ASR) toolkit [1] is one of the most successful open-source efforts in speech processing. One of the key aspects of Kaldi's design comes from the recipe structure. Each recipe provides a fully reproducible experiment, such as handling data preparation, model training, and evaluation. The various steps of the experiment often require the combination of various tools, including many Linux command-line executables and shell scripts/utilities. For example, sox is often used to handle various audio formats [2], awk for data wrangling [3], make for setting up development environments [4], and NIST SCTK for ASR evaluation [5]. Bash scripts are used to glue these tools together. One prominent example is the run.sh \"main\" script inside each recipe directory which calls all the necessary tools and scripts and runs the recipe end-to-end. This design enables easy reproduction and also encourages researchers to open-source their code, greatly benefiting the research community. Additionally, this approach offers scalability and extensibility by piggybacking on the computational efficiency and wide availability of such tools.\nThe ESPnet end-to-end speech processing toolkit [6] aims to expand Kaldi's task coverage by supporting also other speech-related tasks [7-16]. However, it fundamentally retains its core design principles. In fact, similarly to Kaldi, ESPnet also focuses on end-to-end Bash script-based recipes to reproduce results from scratch. For example, various challenges that cover a wide array of tasks are being held based on ESPnet, such as multilingual automatic speech recognition (ASR) [17], robust speech recognition [18], and voice conversion [19]. Further, ESPnet provides several recipes for training various speech foundation models from scratch. These models are trained with large-scale data from various domains, requiring computational efficiency and parallelizability especially for data preparation and data loading. As such, these tasks are performed using many Kaldi tools, sacrificing ease of use for efficiency and training speed. ESPnet provides recipes for self-supervised speech models including HuBERT [20, 21] and wav2vec-U [14, 22], weakly-supervised speech models such as Whisper [23] via OWSM [24-26] and OWSM-CTC [27], as well as speech language models such as UniverSLU [28] and VoxtLM [29].\nThese foundation models are becoming the de-facto standard in various speech processing tasks, such as ASR, text-to-speech (TTS), or spoken language understanding (SLU). They boast universal applicability on many tasks via fine-tuning, often achieving state-of-the-art performance, as demonstrated by benchmarks such as SU-PERB [30], ML-SUPERB [31, 32], and SLUE [33]. Thus, many increasingly rely on fine-tuning such large-scale models for new tasks rather than training from scratch. Various methodologies can be used, such as full fine-tuning, parameter-efficient fine-tuning [34, 35], knowledge distillation [36], or prompting [37], making the ap-proach less computationally demanding and more data-efficient.\nIn addition, recent open-source projects focus on further reduc-ing engineering complexity by leveraging Python as the go-to lan-guage, unlike Kaldi and ESPnet, which still rely significantly on Bash and are thus arguably more difficult to debug. Linux ex-ecutables were the easiest way to integrate various tools into a sin-"}, {"title": "2. DESIGN OF ESPNET-EZ", "content": "ESPnet-EZ aims to make ESPnet easier by removing Kaldi-style dependencies and exposing ESPnet core logic through a Python interface, thus allowing for fast development and easy inte-gration with external toolkits. It is built around two main modules: Trainer and ESPNetEZDataset, allowing flexibility through modularization."}, {"title": "2.1. Trainer", "content": "The core espnet2.tasks module is exposed to the user through a Trainer class interface. The interface is responsible for handling model training and fine-tuning for a particular task. It covers the two ESPnet recipe steps of collecting dataset statistics and training the model, where ESPnet and ESPnet-EZ share the same core logic. Currently, all 20 different speech tasks of ESPnet are natively sup-ported, but, differently from this latter, it is also possible for the user to easily customize the Trainer for the application at hand."}, {"title": "2.2. ESPNetEZDataset", "content": "ESPNetEZDataset is responsible for interfacing the data format-ting for feeding into the training loop. Original ESPnet feeds the data formatting specifics via the command-line interface, limiting the extensibility and requiring the user to follow the Kaldi-style dataset preparation. ESPNetEZDataset avoids the command-line interface via passing the Python function directly, removing the need of Bash script-based data processing steps altogether. ESPNetEZDataset is built on top of Pytorch dataset class, and thus is inhrently flexible in supporting also other dataset modules that are built in the same way. Prominent examples are Huggingface Datasets and Lhotse Datasets, which can be easily integrated with ESPnet-EZ. We provide more examples in Section 6."}, {"title": "3. FINE-TUNING ON ESPNET AND ESPNET-EZ", "content": "Since a hands-on example is worth a million words, we outline the difference between ESPnet and ESPnet-EZ regarding engineer-ing efforts for a simple ASR fine-tuning pipeline. The comparison is summarized in Figures 2 and 3."}, {"title": "3.1. Fine-tuning on ESPnet", "content": "Installation. For fine-tuning, the user has to create a new recipe in ESPnet. However, ESPnet has to be installed from source, which is a notoriously complex process due to its many dependen-cies. For example, for many recipes, one has to also download and build Kaldi beforehand. ESPnet provides a Makefile for installation. However, depending on various Python environment types, such as system default Python, virtual environment, or conda environment, there are different installation procedures that need to be followed accordingly. As these environments setups differ, builds often fail, requiring a lengthy debugging process through several build scripts. Furthermore, many recipes require additional recipe-specific dependencies, which need to be manually installed by run-ning additional scripts in tools/installers. As we can see, the installation procedure is quite complex, especially compared to recent frameworks such as SpeechBrain or transformers."}, {"title": "3.2. Fine-tuning on ESPnet-EZ", "content": "Installation. To install and use ESPnet-EZ, the user only needs a Python package manager, regardless of the environment. After acti-vating a Python environment of any liking, typing pip install espnet will finish the installation process.\nRecipe preparation. ESPnet-EZ does not depend on Kaldi-style recipes. Rather, it gives the user maximum freedom on how to use the ESPnet library for their own projects. In later sections, we provide easy ways to use the model as-is in the existing training code or train the model based on the ESPnet-EZ trainer functionality.\nModel download. ESPnet-EZ removes the Bash scripts surrounding the ESPnet-model-zoo, which is previously in-troduced in ESPnet for easier model download. Hence, users can easily download and use the models via from_pretrained method within the Python codebase, similar to asteroid and transformers. Model download and management are per-formed under the hood, reducing engineering overhead.\nDataset preparation. As Kaldi-style dataset preparation is not anymore necessary (but still supported by ESPnet-EZ), vari-ous dataset frameworks can be easily integrated. As the ESPnet model receives PyTorch tensors, the user can easily build a shallow connector for existing dataset frameworks, such as vanilla PyTorch datasets, Huggingface datasets, and Lhotse, to feed the data into the model.\nModel training and inference. Depending on the use case, the user can choose between leveraging the ESPnet-EZ Trainer or their custom trainer for training and inferencing the model. The ESPnet model is based on PyTorch, so it can be seamlessly inte-grated into existing frameworks, such as PyTorch Lightning.\nESPnet already features a Python-friendly implementation of infer-ence; therefore, we can directly incorporate the inference code with ESPnet-EZ scripts."}, {"title": "4. COMPARING ESPNET AND ESPNET-EZ", "content": ""}, {"title": "4.1. Qualitative Comparison (High-level Summary)", "content": "As said, the current ESPnet codebase is optimized to be highly ef-ficient in cluster environments by leveraging various shell scripts, making it well suited for large-scale training. However, as said, leveraging these foundation models in an off-the-shelf manner is equally important as it is arguably the mainstream approach for solv-ing various speech tasks. Hence, ESPnet-EZ reduces the friction of leveraging existing models by reimplementing some of the shell-based codebases in a Python-only manner. Even though it introduces unavoidable computational overhead, it is negligible for smaller-scale training and fine-tuning (e.g. for Librispeech [52])."}, {"title": "4.2. Quantitative Comparison", "content": "We further quantify the differences by measuring engineering ef-forts in Sections 3.1 and 3.2. As an indirect metric, we measure the amount of code in various ways. We count the number of dependent files within the ESPnet codebase, the number of lines of the files, and the newly written number of lines. For the espnet, espnet2, and espnetez packages, we traverse through all the imported de-pendencies. For the command line tools for ESPnet, we count all codes in the script and pyscript directory within the recipe. We can immediately observe that the amount of code is dramatically reduced. Furthermore, the language distribution becomes more Python-friendly, effectively re-moving Bash and Perl script dependencies altogether."}, {"title": "4.3. User Feedback Comparison", "content": "We gathered real-world feedback from the ESPnet users. We provided a Jupyter notebook demonstration similar to Sec-tion 5.3 and asked the users to list the pros and cons of using ESPnet-EZrelative to the full ESPnet toolkit. There were 21 respondents, encompassing undergraduate students who are new at speech research and graduate students who have already published multiple papers. Each respondent already had basic experience on ESPnet, where they went through the notebook demonstration of the ESPnet ASR recipe. We summarize the responses in Figure 4 and include all raw responses in the supplementary material. To ob-tain the summaries, we use a pre-trained Part-Of-Speech Tagger to parse out adjectives. After grouping similar words, we counted each word that occurred more than once. The feedback to ESPnet-EZ was generally positive, as the respondents found that its similarities with transformers, make it easier and familiar to use. However,"}, {"title": "5. TASK COVERAGE OF ESPNET-EZ", "content": "In this section, we demonstrate the wide applicability of ESPnet-EZ to various tasks. In detail, we fine-tune the OWSM speech foun-dation model in various cases, such as (i) both the dataset and the task are seen during training (Sections 5.1 and 5.2), (ii) the dataset is seen but the task is unseen (Section 5.4), (iii) the dataset is unseen but the task is seen (Section 5.5), and (iv) both the dataset and the task are unseen (Sections 5.3 and 5.5). Specifically, we fine-tune the v3.1-ebf-base model [25] with 101M parameters. We also include the reported performance of state-of-the-art models for comparison on each task. All experiments are implemented via ESPnet-EZ, where we provide all the tutorials and codebase."}, {"title": "5.1. Automatic Speech Recognition (ASR)", "content": "Task definition. The model has to generate a transcription from a given speech input. We fine-tuned the model using a 100h train-clean-100 training subset of LibriSpeech [52]. For early stopping, we use dev-clean and dev-other subsets.\nBaselines. We compare the performance of the OWSM model before fine-tuning, fine-tuned model by ESPnet or ESPnet-EZ. For the ground truth of ESPnet, we use the original input format with timestamps per the existing tutorials, while ESPnet-EZ uses the format with transcription only. For ESPnet-EZ, we addi-tionally employ Lhotse for online data augmentation and Low-Rank Adaptation (LoRA) [34] for parameter-efficient fine-tuning (PEFT). Finally, we compare with the two state-of-the-art results from WavLM [53]. WavLM-Base+ (95M) has a similar parameter size as OWSM, and WavLM-Large (316M), with 3x the parameter size, uses shallow fusion with the transformer language model (LM).\nFine-tuning details. We conduct a grid search to find the op-timal hyperparameter. We use the AdamW optimizer [54] with the learning rate grid of [2e-3, 1e-3, 5e-4, 1e-4] and the warm-up step grid of [5000, 15000]. The optimal configuration was the learning"}, {"title": "5.2. Speech Translation (ST)", "content": "Task definition. The model has to generate a translated text from speech from one language to another. OWSM model di-rectly translates the audio input into the target language without generating the source language transcription. To evaluate fine-tuning performance on English-to-German translation, we used the MuST-C-V2 [56] dataset, which includes TED talks and their tran-scriptions. For evaluation, we use BiLingual Evaluation Understudy (BLEU) [57], CHaracter-level F-score (chrF) [58], and Translation Error Rate (TER) [59]. We use SacreBLEU [60] for valid set.\nBaselines. Similar to Section 5.1, we compare the perfor-mance of OWSM before and after fine-tuning. We test full fine-tuning and LoRA. Also, we train a cascaded ST model, employing whisper-tiny [23] (transcribes speech into English) and the t5-base [61] (translates English to German) from transformers.\nFine-tuning details. We go through the same hyperparameter search process of Section 5.1. For LoRA, we use learning rate of 2e-3. The prompt template for whisper-tiny is translate English to German: <ASR result>, obtained from the official website. We also format the first character in uppercase and the rest in lowercase. We compared with the state-of-the-art model [55] with ST-specific architecture design and 72M parameters.\nResults. Table 2 shows that fine-tuning improves BLEU, while LORA shows slight degradation. Nevertheless, LoRA improves TER by 11.6 points, indicating over the baseline. Cascaded ST perfor-mance was suboptimal, even though the total number of parameters"}, {"title": "5.3. Spoken Language Understanding (SLU)", "content": "Task definition. The model has to extract the meaning given the spoken content. For the SLU task, we evaluated the model's performance using the SLURP [62] dataset, consisting of various in-home prompts for home assistants. We especially focus on the intent classification task, which is not included during OWSM training.\nBaselines. As OWSM does not support the SLU task, we only test the two variants of fine-tuning: full fine-tuning and LoRA. We compare the results with UniverSLU [28], the state-of-the-art model that fine-tunes Whisper-medium (769M) [23] to 12 SLU tasks.\nFine-tuning details. We go through the same hyperparameter search of Section 5.1 and set the learning rate to 5e-4. For warm-up, we choose 5k and 15k steps for full-finetuning and LoRA. We add a special task token  to adapt the model to the new task.\nResults. Table 2 shows that OWSM can be adapted to the un-seen task with fine-tuning, even though it requires full fine-tuning for better performance. We observed that OWSM outputs several tokens to express intent, introducing errors while decoding the intent labels. Also, the model fitted to the training data too quickly. The empirical observations further support the multi-task approach of UniverSLU to increase the training data size."}, {"title": "5.4. Text to Speech (TTS)", "content": "Task definition. The task aims to synthesize speech from text in the voice of the target speaker. Following [7], we evaluate perfor-mance using Mel Cepstral Distortion (MCD) [64], Fo Pearson cor-relation coefficient (Fo Corr), speaker embedding cosine similarity (SECS), and UTMOS [65]. For SECS, we use the speaker embed-dings of ESPnet-SPK model [15] trained on VoxCeleb [66, 67].\nBaselines. We use a VITS model pre-trained on LibriTTS [68] as our baseline and fully fine-tune on VCTK [69]. We compare our results with the state-of-the-art USAT model [63].\nFine-tuning details. For the task, we focus on fine-tuning the model, where a source TTS model is adapted to an unseen dataset containing unknown speakers. We use the same hyperparameters as pre-training, except for the AdamW optimizer with a learning rate"}, {"title": "5.5. Low-resource Languages", "content": "Task definition. The Fieldwork corpus [35] provides linguistically rich annotations of 37 endangered languages. It contains four sub-tasks: transcription, underlying, gloss, and translation. Underlying and gloss can be understood as an intermediate task between tran-scription and translation. For the transcription task (ASR), the model has to use the orthography of each language. The underlying task is similar to the transcription task, where the model has to addition-ally segment by morpheme with the underlying representation be-fore applying the phonological rules. The interlinear gloss task has the same segment as the underlying task, where the gloss addition-ally requires brief explanations for each morpheme. Target language of the translation task (ST) is English, which is evaluated by the modified character-level F-score (chrF++) [70]. All other tasks are evaluated by the character error rate (CER).\nFine-tuning details. [35] provides baselines using the OWSM v3.1-ebf-base model, fine-tuning the model on each of the tasks. For our experiments, we test bigger variants of OWSM, namely, v3.1-small [25] and v3.2-small [26]. v3.2-small differs with v3.1-small by the training data preprocessing strat-egy. For both v3.1-small and v3.2-small with larger param-eter size (367M), we use a smaller learning rate of 1e-4 and batch size of 8 to meet the GPU constraints and stabilize the training. To make the number of iterations the same, we use 5 epochs.\nResults. In Table 5, v3.2-small shows great performance in transcription, likely due to careful curation of the training data. However, the smallest v3.1-ebf-base performs best in both gloss and translation, arguably a more challenging task than tran-scription. One possible explanation is the limited number of epochs, where bigger models potentially require more iterations to train."}, {"title": "6. EASY INTEGRATION OF ESPNET-EZ", "content": "In this section, we go over various examples where ESPnet-EZ integrated with existing deep learning and dataset frameworks."}, {"title": "6.1. End-to-end Training with Multiple Models", "content": "One of the advantages of a Python-only codebase is that it allows to easy integration of pre-trained models from different frameworks. In Section 5.2, we demonstrate the case where the cascaded mod-els' components are from two different frameworks: ESPnet and transformers. As both models are PyTorch-based, we can train both with the ESPnet-EZ Trainer. This demonstrates the flexi-bility of ESPnet-EZ Python-only codebase."}, {"title": "6.2. Applying Audio Augmentations", "content": "As ESPnet-EZ's dataset is a shallow wrapper for PyTorch's vanilla datasets, one can easily apply various techniques dur-ing training. The user can choose either the dataset or the data loader (which includes batching techniques) to be inputted in the ESPnet-EZ trainer. For example, we demonstrate on-the-fly speech augmentation in Section 5.1. We apply various augmentation techniques from Lhotse while using the ESPnet-EZ trainer."}, {"title": "6.3. Integration with External Frameworks", "content": "In Figure 3, we observe that we can also skip the ESPnet-EZ Trainer when training the model. It is demonstrated in Sec-tion 5.5 where we swap ESPnet-EZ Trainer with the trainer from PyTorch-Lightning."}, {"title": "6.4. Online Fine-tuning Demo", "content": "We implemented a fine-tuning demonstration on Huggingface Spaces. Fine-tuning can be performed simply by uploading train-ing data in a ZIP archive. The demo showcases the great integration ability of ESPnet-EZ, allowing users to evaluate the performance of the fine-tuned OWSM on a custom dataset within a few minutes."}, {"title": "7. CONCLUSION", "content": "In this work, we introduce ESPnet-EZ, an extension of the open-source speech processing toolkit ESPnet. ESPnet-EZ focuses on easier fine-tuning, inference, and integration with other deep learning frameworks. By removing the Kaldi-style dependencies, ESPnet-EZ provides a Python-only ESPnet interface, simplify-ing the process of implementation while maintaining all ESPnet task coverage. We provide various comparative analyses, demos, and experiments to corroborate the advantages above."}, {"title": "10. SUPPLEMENTARY MATERIALS", "content": ""}, {"title": "10.1. User feedbacks", "content": ""}, {"title": "10.1.1. Pros", "content": "\u2022 It was much easier to write, understand and update pythonic code according to my requirements instead of bash scripts\n\u2022 The compatability with Hugging Face made it far less init-midating, as I could transfer my knowledge of the Hugging Face datasets library and trainer () API to ESPnet. I imagine this would make it a much easier entry point for beginners as well.\n\u2022 It is user friendly because everything can be done with Python. Unlike previous demos where we had to edit bash files, we were able to easily change the parameters and fine-tune the model with python scripts\n\u2022 ESPnet EZ is quite easy to use, without the need to actually touch the low-level details of ESPnet.\n\u2022 Espnet EZ is a more straightforward tool to work with.\n\u2022 Staying within the python to make config changes makes it very simple to implement configuration changes.\n\u2022 My experience with ESPnet EZ is very good\n\u2022 ESPnet EZ provides a more convient way to use ESPnet. It is a bit similar to Hugging Face abstractions\n\u2022 ESPnet EZ is easier to use and has less complexity. It has a simpler configuration and easier setup, making it more acess-sible to beginners or those interested in fast implementation\n\u2022 ESPnet-EZ comes with predefined pipelines and pretrain model loading, making it easier to perform common speech processing tasks without needing to train models from scratch\n\u2022 I love it. It is much more similar to other tools I have use before. I instantly recognize the Trainer class as similar to the class of the same name from the transformers library for text.\n\u2022 I think it is easier for implementing\n\u2022 Espnet EZ significantly lowers the entry barrier for useres, especially those familiar with python and huggingface frame-works as it is less script heavy\n\u2022 It has more intuitive usage of popular frameworks, making it easier for newcomers to grasp and utilize\n\u2022 The codebase of ESPnet EZ is more readable and inter-pretable compared to the traditional ESPnet toolkit\n\u2022 Its alignment with mainstream deep learning practices, where users can easily understand the structure and flow of the code, facilitating easier customization and troubleshooting\n\u2022 Since everything was python based it made it much easier to use.\n\u2022 It helps a lot on understanding how to use ESPnet\n\u2022 It allows user to implement speech processing capabilities quickly and with minimal setup\n\u2022 Has a more user-friendly iterface, simplified workflow for common tasks\n\u2022 Convenient toolkit for applying some finetuning techniques like LORA\n\u2022 ESPnet EZ is really user friendly\n\u2022 Dataset format conversion is straightforward\n\u2022 Training configuration is readable\n\u2022 It feels more intuitive because it is python based\n\u2022 EZ simplifies ASR tasks with user-friendly iteraces and pre-trained models"}, {"title": "10.1.2. Cons", "content": "\u2022 Lack of evaluation framework is the most apparent issue. Having to rely on accuracy plots each time you change hy-perparameters is cumbersome\n\u2022 Additionally, there is no support for hyperparameter opti-mization, which can be reaslly useful for ablation studies for research projects\n\u2022 Advanced users who require more ocntrol over various pa-rameters and models may find it limited in terms of cus-tomization. Due to its simplication, it may not support all the advanced features and functionalilites\n\u2022 It lacks fine-grained control of the entire training procedure. When we want to add a new pipelien that is not in ESPnet,, we still need to go back to the original ESPnet.\n\u2022 Using the bash scripts might have the advantage of providing more control over how the different files are run\n\u2022 It may limit the flexibility and customizability available to users. For some detaile and complex settings, I would choose ESPnet\n\u2022 ESPnet EZ might not keep up with the latest updates as quickly as the main ESPnet toolkit\n\u2022 It might be harder to debug if any unsuspected things happen.\n\u2022 I would imagine ESPnet is more robust and potentially slightly faster to compensate for its difficulty\n\u2022 It comes as the cost of the depth of customization and the range of features\n\u2022 Less flexibility for advanced users and possibly limited op-tions for customizations\n\u2022 Not sure if EZ supports building a new module based on an existing model with some small changes (such as adding a linear layer at the end)\n\u2022 Same as python versus shell\n\u2022 May offer limited customization and control compared to full ESPnet"}]}