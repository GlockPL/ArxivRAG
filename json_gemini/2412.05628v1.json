{"title": "Remix-DiT: Mixing Diffusion Transformers\nfor Multi-Expert Denoising", "authors": ["Gongfan Fang", "Xinyin Ma", "Xinchao Wang"], "abstract": "Transformer-based diffusion models have achieved significant advancements across\na variety of generative tasks. However, producing high-quality outputs typically\nnecessitates large transformer models, which result in substantial training and\ninference overhead. In this work, we investigate an alternative approach involv-\ning multiple experts for denoising, and introduce Remix-DiT, a novel method\ndesigned to enhance output quality at a low cost. The goal of Remix-DiT is to\ncraft N diffusion experts for different denoising timesteps, yet without the need\nfor expensive training of N independent models. To achieve this, Remix-DiT\nemploys K basis models (where K < N) and utilizes learnable mixing coefficients\nto adaptively craft expert models. This design offers two significant advantages:\nfirst, although the total model size is increased, the model produced by the mix-\ning operation shares the same architecture as a plain model, making the overall\nmodel as efficient as a standard diffusion transformer. Second, the learnable\nmixing adaptively allocates model capacity across timesteps, thereby effectively\nimproving generation quality. Experiments conducted on the ImageNet dataset\ndemonstrate that Remix-DiT achieves promising results compared to standard\ndiffusion transformers and other multiple-expert methods.", "sections": [{"title": "Introduction", "content": "Transformer-based Diffusion models [39, 8, 29, 2] have shown significant potential in generating\nhigh-quality images and videos [24, 20, 3]. However, achieving such quality often requires large\ntransformer architectures [35], which incur considerable training and inference costs. To alleviate\nthese computational burdens, multi-expert denoising has emerged as a promising approach [1, 18,\n28, 16, 27], which employs multiple specialized diffusion models, each designed for distinct time\nintervals within the denoising process.\nThe goal of multi-expert denoising is to increase the overall capacity of diffusion models while\nkeeping an acceptable overhead. The denoising process of diffusion models involves multiple\ndifferent timesteps [13, 32, 18], requiring the network to make predictions at various noise levels.\nPrevious work has shown that the tasks of diffusion models vary across different timesteps [13, 37, 27].\nFor instance, at higher noise levels, the model focuses more on low-frequency features, while at lower\nnoise levels, the model emphasizes generating high-frequency details [37]. This variability inherently\nleads to a multi-task problem [16, 18]. However, due to the limited capacity of a single diffusion\nmodel, it is challenging to craft a comprehensive and balanced model that performs well across the\nentire denoising process. To alleviate this issue, multiple-expert denoising deploys specialized expert\nmodels at different timesteps. To this end, each model only needs to learn the denoising task for\na specific subset of timesteps. Although this design introduces multiple models, only one model"}, {"title": "Related Works", "content": "Multiple Experts in Diffusion Models The multi-step denoising process inherent to the diffusion\nmodel [14, 33, 34] can be viewed as a multitasking problem [1, 18, 28, 16]. At each step, the\nmodel receives inputs with varying levels of noise and makes predictions accordingly. Given the\ndisparate learning objectives for each denoising step, the utilization of a singular model across all\nstages is, to a certain extent, inefficient and challenging. Several prior works suggested employing\nmultiple models, each dedicated to handling partial timestep intervals. For example, E-Diff [1]\nintroduces a simple binary strategy to learn various experts and during inference, take the ensemble\nof experts for prediction. OMS-DPM [18] proposes a model scheduler, dynamically taking denoiser\nof different sizes from a model zoo for inference. MEME [16] deploys a similar paradigm, yet\ntraining lightweight models for different steps. The ensemble of these lightweight denoisers can\nachieve superior performance compared to their larger counterparts while maintaining efficiency\nduring inference. Another exploration to enhance efficiency using multiple experts is T-Stich [27],\nwhich stitches the denoising trajectories of different pre-trained models, enabling dynamic inference\ncost without additional training. However, it's noteworthy that multiple diffusion models may incur\nadditional storage and context switch costs. Thus, DTR [28] introduces a novel multi-tasking strategy\nto learn a single model with scheduled task masks, activating specific sub-networks for different\ntimesteps.\nTransformer-based Diffusion Models Transformer-based diffusion models have achieved im-\npressive results on image generation [2, 29, 4, 10, 20, 17]. DiT [29] explores the scalability of\ntransformers for image generation, achieving competitive performance compared to CNN-based\ndiffusion models. UViT [2] independently explores transformer design for generation, incorporating\nthe U-Net [31, 22, 30] architecture for improved training efficiency and quality. Furthermore, recent\nworks have expanded transformer-based diffusion models to video generation [20, 21, 24, 5, 3],\naudio [19, 15] and 3D [25] which shows the power of transformers in modeling complicated data.\nHowever, training diffusion models remains inefficient, typically requiring millions of steps to pro-\nduce satisfactory results. A series of works in the literature focus on the efficiency of diffusion-based\ntransformers [40, 40, 23, 9]. MaskDiT [40] and MDTv2 [10] utilize masked transformers and an\nadditional masked autoencoder task [12] to streamline training. UViT [2] also demonstrates the\nbenefits of skip connections for accelerating convergence. In this work, we introduce a new strategy\nto enhance the training efficiency of plain diffusion transformers"}, {"title": "Preliminary", "content": "Diffusion Probabilistic Models Diffusion Probabilistic Models (DPMs) train a denoiser network\nto revert a process of adding gradually increased noises [13]. Given an input $x_0$, a $T$-step forward\nprocess is deployed to transform $x_0$ into latent $x_1,..., x_T$, where the transformation is defined as\n$q(x_t|x_{t-1}) = N(x_t; \\sqrt{1 - \\beta_t}x_{t-1}, \\beta_tI)$ under a increased variance schedule $\\beta_{1:T}$. To revert this\nprocess, a network $q(x_{t-1}|x_t)$ is trained to predict $x_{t-1}$ given $x_t$. By defining $a_t = 1 - \\beta_t$ and\n$\\overline{a}_t = \\prod_{i=1}^{t} a_i$, the training objective is formalized as\n$\\mathcal{L}(\\theta) = E_{t, x_0,\\epsilon \\sim N(0, I)}[||\\epsilon - \\epsilon_\\theta(\\sqrt{\\overline{a}_t}x_0 + \\sqrt{1 - \\overline{a}_t}\\epsilon, t)||^2]$,\nwhere $\\epsilon$ is a trainable neural network and $\\epsilon_\\theta$ refer to the noises at timestep $t$. A common practice in\nthe literature is to train a single neural network for all timesteps $t$. However, it has been observed that\nthe denoising behavior usually varies across timesteps [13, 1, 37, 27]. Due to the limited capacity, a\nsingle model may struggle to fit all steps. Therefore, a more suitable yet natural approach is to use\nmultiple models specifically learned for different timestep intervals [1].\nMulti-Expert Denoising To facilitate denoising with N expert models, the timesteps are typically\ndivided into N intervals. Each model, denoted as $e_{\\theta_i}$, contains independent parameters $\\theta_i$. For each\nmodel $\\theta_i$, we minimize the training objective as formalized in Equation 1 on the corresponding"}, {"title": "Method", "content": "Uniform Partition of Denoising Process Let's consider a multi-expert denoising problem with\nN experts for T timesteps. For simplicity, we assume that each expert's parameters $\\theta_i$ is a column\nvector with P trainable elements and the parameter matrix of all experts can be formalized as\n$\\Theta_{N \\times P} = [\\theta_1, \\theta_2,\u2026\u2026, \\theta_N]^\\text{T}$. This is natural since the parameters in a model can be flattened and\nconcatenated into a vector. We consider a simple partition of timesteps, which equally divides the\ndenoising process into N intervals, i.e., $[0, T/N] ... [(N - 1) \\cdot T/N, N \\cdot T/N]$. The motivation\nbehind the oracle partition is that adjacent steps share similar noise levels and training objectives.\nFollowing Equation 2, this leads to the learning objective to optimize each expert on their associated\ntime intervals.\n$\\Theta^* = \\underset{\\Theta}{\\text{arg min}} \\underset{\\theta_i \\in \\Theta}{\\sum} \\mathcal{L}(\\theta_i)$.\nThe above process involves N independent optimization problem, which presents some issues. First,\nthe optimal number of experts is unknown. It's difficult to increase the number of experts since this\nwill introduce huge training costs. Besides, the optimal partition is also unclear, making the uniform\npartition inefficient. In this work, we investigate such a problem: Is it possible to learn any number of\nexperts, while keeping an affordable overhead?\nCrafting Experts by Mixing To address the problem of uniform partition, we leverage a set of\nbasis models to avoid straightforward training on experts. The core idea lies in that, it is possible to\nfuse the parameters of two diffusion models to achieve better performance [38]. Formally, instead\nof training N independent models directly, the core idea of Remix-DiT is to learn K (K < N)\nbasis models with the same architecture as experts, parameterized by $B_{K \\times P} = [\\beta_1, \\beta_2,\u2026\u2026\u2026 \\beta_K]^\\text{T}$.\nAnd the expert models can be crafted by mixing the basis parameters with certain coefficients. For\neach expert $\\theta_i$, we associate it with a coefficients $a_i = [a_{i1}, a_{i2},...a_{iK}]$ and compute the mixed\nexpert parameter through a weighted averaging $\\theta_i = \\sum_k a_{ik}\\beta_k$. This leads to the coefficient matrix\ndenoted as $\\alpha_{N \\times K} = [\\alpha_1, \\alpha_2, ..., \\alpha_N]^\\text{T}$. Then, the expert's parameter matrix can be easily obtained\nthrough a simple matrix multiplication:\n$\\Theta_{N \\times P} = \\alpha_{N \\times K}B_{K \\times P}$"}, {"title": "Experiments", "content": "Experimental Settings\nNetwork Architecture. This paper utilizes the DiT [29] models as the fundamental architecture.\nBy reloading the computation process of the linear layers, additional mixing coefficients is introduced\nto create RemixDiT. For a fair comparison, we ensure that the dimensions of all mixed experts are\ncompletely consistent with the original DiT. For instance, Remix-DiT-S is constructed by quickly\ncombining multiple DiT-S models to form an expert model.\nTraining Details. Since our mixed model is entirely consistent with the standard DiT, this method\ncan be applied to pre-trained models. We first trained a standard DiT model on ImageNet and then\ninitialized the basis model with the same pre-trained weights. We introduce prior as discussed in\nEquation 6 to the mixing coefficients to accelerate the learning of different basis models. In our\nexperiments, we conducted 100 K fine-tuning on DiT-S/B/L models [29], pre-trained for 2M/1M/1M\nsteps correspondingly."}, {"title": "Transform Pretrained DiT to Remix-DiT", "content": "In Table 1, we present the results of fine-tuning standard DiT models using the Remix-DiT approach.\nGiven the architectural consistency between the mixed model and the standard DiT, it is feasible\nto initialize K basis models within the standard DiT framework. This enables the construction of\nRemixDiT with minimal additional training steps since the pre-trained DiT has been comprehensively\ntrained at all temporal steps. For instance, starting with a pre-trained DiT-B model, our method suc-\ncessfully crafts a Remix-B with only 100K training steps, achieving superior performance compared\nto both the original models and baselines such as continual training and multiple experts [1].\nMoreover, within the same computational budget, our approach outperforms Multi-expert baselines,\nwhere experts are trained independently. It is noteworthy that for Multi-expert baselines with eight\nexperts, the total training budget is uniformly allocated to each expert. As the number of experts\nincreases, the allocated training steps for each expert become more limited. In contrast, Remix-\nDiT trains shared basis models throughout the entire training process, allowing each model to be\nsufficiently updated.\nAs discussed earlier, RemixDiT allocates gradients from different time steps by employing mixing\ncoefficients. Therefore, for K basis models initialized similarly, greater diversity in the initial\nmixing coefficients facilitates the rapid learning of different models. To achieve this, we sequentially\ndistribute the entire denoising process among the basis models. During subsequent fine-tuning, the"}, {"title": "Visualization of Learned Experts", "content": "To further investigate the behavior of the experts\nlearned by the algorithm, we visualized the mixing\ncoefficients. As shown in Figure 4a, we conducted\nexperiments on DiT-S. During the training of 20 ex-\npert models, we observed that the algorithm assigned\nmore one-hot coefficients to timesteps close to 0. At\nthese steps, the denoising model focuses more on\nhigh-frequency detail features. In contrast, at late\ntimesteps resembling random noise, the algorithm\nenabled and mixed multiple models to generate con-\ntent rather than using only one expert. As will be\nillustrated in Figure 5, mixing multiple models can improve the shape quality. Moreover, it can be\nobserved that adjacent timesteps exhibit similar mixing coefficients, whereas more distant timesteps\nshow greater differences. This finding supports the core hypothesis of multi-expert reasoning. To\nvalidate this observation, we further trained a RemixDiT with 8 experts. Figure 4b also shows the\ndistribution of mixing coefficients, revealing results similar to the previous experiment. Additionally,\nFigure 4c visualizes the training loss functions of the 8 expert models throughout the denoising\nprocess. Each mixed expert has a lower loss function within its respective timestep intervals. The\nfarther from an expert model's current interval, the higher its loss. Notably, at step 0, we found that\nthe ensemble-constructed model effectively reduces prediction loss."}, {"title": "Analytical Experiments", "content": "In this section, we validate some key design aspects of RemixDiT. This includes the model's mixing\nmethod, the number of expert models and basis models, and whether the coefficients are independent\nor shared across layers.\nModel Mixers. The core of the proposed method lies in the mixing of multiple models, and we\nexplored three mixing methods: 1) Oracle Mixing: Basis models are manually assigned to different\nintervals as expert models, equivalent to training independent expert models, which makes the\nalgorithm revert to the standard multi-expert training strategy regardless of N. 2) Raw Mixer: Each\nexpert is assigned a real-valued coefficient without any constraints. 3) Softmax Mixer: This builds on\nthe raw mixer by applying a softmax operation, ensuring that the mixed model is always a weighted\naverage of the basis models. Table 2 shows the effectiveness of each mixer. We found that while\noracle mixing improves model performance, it often fails to achieve optimal results due to manually"}, {"title": "Limitations and Broader Impact", "content": "In this work, we introduce a learnable coefficient, implemented as a simple embedding layer for\nmixing. However, due to the limitations of the deep learning framework, we can only create one\nexpert per forward and backward pass. This leads to sparse gradients in the embedding layers, where\ncoefficients without gradients can only be updated with momentum rather than accurate gradients."}, {"title": "Conclusion", "content": "In this work, we introduce a multi-expert method to enhance the quality of transformer-based diffusion\nmodels while maintaining an acceptable inference overhead. The core contribution lies in the ability\nto craft a large number of experts from a few basis models, thereby significantly reducing the training\neffort. Besides, with our method, we don't have to accurately estimate the optimal number of required\nexperts, since the learnable coefficients will adaptive merge experts if necessary, which brings huge\nflexibility to the practice."}]}