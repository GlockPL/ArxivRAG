{"title": "A Backbone for Long-Horizon Robot Task Understanding", "authors": ["Xiaoshuai Chen", "Wei Chen", "Dongmyoung Lee", "Yukun Ge", "Nicolas Rojas", "Petar Kormushev"], "abstract": "End-to-end robot learning, particularly for long-horizon tasks, often results in unpredictable outcomes and poor generalization. To address these challenges, we propose a novel Therblig-based Backbone Framework (TBBF) to enhance robot task understanding and transferability. This framework uses therbligs (basic action elements) as the backbone to decompose high-level robot tasks into elemental robot configurations, which are then integrated with current foundation models to improve task understanding. The approach consists of two stages: offline training and online testing. During the offline training stage, we developed the Meta-RGate SynerFusion (MGSF) network for accurate therblig segmentation across various tasks. In the online testing stage, after a one-shot demonstration of a new task is collected, our MGSF network extracts high-level knowledge, which is then encoded into the image using Action Registration (ActionREG). Additionally, Large Language Model (LLM)-Alignment Policy for Visual Correction (LAP-VC) is employed to ensure precise action execution, facilitating trajectory transfer in novel robot scenarios. Experimental results validate these methods, achieving 94.37% recall in therblig segmentation and success rates of 94.4% and 80% in real-world online robot testing for simple and complex scenarios, respectively. Supplementary material is available at: https://sites.google.com/view/therbligsbasedbackbone/home", "sections": [{"title": "I. INTRODUCTION", "content": "Understanding robot tasks encompasses several key stages: sensing the environment, recognizing task-related objects, making decisions, and planning trajectories. Recently, data-driven methods, especially deep learning algorithms, have greatly advanced the field of robotics. While deep learning excels in object recognition and reinforcement learning aids in trajectory planning, these models often struggle to gen-eralize beyond trained scenarios, especially in long-horizon tasks. Thus, improving generalization is crucial for adapting to diverse, dynamic, real-world situations effectively.\nA major challenge is data efficiency, as current systems require large, resource-intensive datasets [1]\u2013[3]. Models trained for long-horizon tasks excel in simple tasks such as pick&place but struggle with more complex tasks like liquid pouring, which involve multiple steps like grasping, pouring, and releasing. The narrow focus on simple tasks limits these models' applicability in diverse and complex scenarios. Additionally, end-to-end systems are difficult to explain, analyze, and improve. These systems [4]\u2013[6] can learn directly from sensory inputs and discover complex feature representations, eliminating the need for manual feature engineering. However, they are hard to interpret and struggle to transfer skills to new tasks.\nTo address these challenges, we propose a structured and modular backbone framework. In our context, a backbone refers to a structured framework that decomposes high-level tasks into fundamental units, each representing a specific action or sequence of actions, facilitating better task understanding and transferability [7]. Complex tasks involve rich contact interactions (e.g., multiple points of force application) and operations in dynamic or cluttered environments, requiring the integration of diverse skills and sensor modalities (e.g., vision, force) [8]. Long-horizon tasks require extended sequences of actions, typically involving more than 10 individual steps [9], or over a prolonged period (e.g., several minutes) to achieve a specific goal.\nMany models depend heavily on pre-training in specific scenarios [10]\u2013[15], limiting their ability to generalize to new environments. Effective scenario generalization is essential for adapting to the complexity and variability of real-world situations [15]. Furthermore, task generalization is crucial, as many systems require extensive examples to learn new tasks, which is impractical in dynamic environments with frequent task variations. Handling a diverse range of tasks is essential for practical robotics. However, existing models often excel only in narrow tasks and struggle with broader activities, reducing their real-world utility."}, {"title": "II. RELATED RESEARCH", "content": "Various intelligent robot systems achieve high accuracy in specific tasks, such as cable routing [16], cloth manipulation [17], and fruit grasping [18]. However, they often struggle to generalize across different tasks. Recently, efforts have been made to develop systems that can handle a variety of tasks [11], [13], [14]. However, these systems typically require large datasets or simple scenarios for generalization and lack a clear backbone for task understanding. To over-come these limitations, we propose using therbligs as the backbone for better task comprehension. Therbligs, which consist of 18 basic motions, provide a systematic way to describe and analyze detailed actions in various tasks [19]. Initially developed for studying human movements, we apply these concepts to visually and conceptually break down the robot's trajectory during learning and task execution. In robot learning, therbligs offer a clear and structured method to visualize processes, as shown in Fig. 2.\nAn early contribution in this area is the work by Ahmadzadeh et al. [20], who introduced a novel approach for converting action sequences into symbolic representations. For action modalities, Chen et al. [21] introduced a novel"}, {"title": "III. MODEL FRAMEWORK", "content": "The TBBF is designed to enhance the understanding and generalization of robotic tasks by breaking them down into fundamental units called therbligs. This framework provides"}, {"title": "A. TBBF: Explainable robot task understanding framework", "content": "a structured and modular approach, facilitating better gener-alization across different tasks and scenarios while offering a clearer and more interpretable structure for task execution.\nIn the offline training stage, we utilize the MGSF network to accurately segment tasks into therbligs, providing a detailed breakdown of the task into its constituent mo-tions. During the online testing stage, we collect a one-shot demonstration of a new task, from which the MGSF network extracts high-level knowledge and transforms it into a structured format. This knowledge is encoded into visual data using ActionREG, integrating therbligs with the objects' configurations in the robot's visual field to ensure precise action registration. By using therbligs as the backbone, our framework significantly improves data efficiency and task generalization, enabling the robot to handle a wide range of scenarios with robustness and precision. The integration of LAP-VC further ensures that any visual discrepancies are cor-rected in real time, providing an additional layer of accuracy in task execution. As depicted in Fig. 3, this comprehensive and structured methodology enhances the robot's ability to adapt to new tasks by leveraging prior knowledge encoded in therbligs, thus improving the interpretability, stability, and transferability of robotic learning systems."}, {"title": "B. MGSF: Effieicent therbligs segmentation network", "content": "Our MGSF network, illustrated in Fig. 4 and detailed in Algorithm 1, advances the current state of the art in un-derstanding robot task actions. By combining meta-learning with adaptive gated fusion within a unified framework, this model significantly enhances robots' ability to comprehend and execute sequential actions across various environments. Inspired by MetaGross [23], our MGSF network incorporates meta-gating and recursive parameterization in a recurrent model. However, MetaGross lacks a dedicated fusion process and struggles to integrate different aspects of the input data effectively, limiting its ability to leverage diverse features and achieve robust performance.\nTo address these limitations, our MGSF model introduces a dynamic hybrid architecture that combines the strengths of both BiLSTM and Transformer sub-networks with a novel adaptive gated fusion mechanism. This architecture features a meta-recursive gated fusion unit that dynami-cally adapts to integrate model outputs, thereby enhancing performance across diverse tasks. Unlike the static gating in MetaGross, our adaptive gated fusion mechanism allows for more flexible and responsive integration of sequential data, ensuring that long-term dependencies are effectively captured and processed. By leveraging the strengths of both BILSTM and Transformer sub-networks, the MGSF"}, {"title": "C. ActionREG: SAM-driven action registration network", "content": "A cornerstone of our TBBF is the ActionREG, designed for reasoning about and regressing object configurations (Fig. 5). This innovative component integrates therbligs' prior knowledge into the SAM model as prompts, enabling sophisticated reasoning about the objects involved in robotic tasks. The integration of this knowledge serves as a guide for the model, improving its ability to understand and predict the configurations of objects within a task-specific context.\nThrough ActionREG, we can easily extract the task-related object mask and the operational workspace mask. Utilizing a hierarchical matching system based on SAM (Segment Anything Model), YOLOv8, SIFT (Scale-Invariant Feature Transform), FLANN (Fast Library for Approxi-mate Nearest Neighbors), and PCA (Principal Component Analysis), we can identify task-related objects and deter-mine their configurations in new scenarios. Specifically, \\(M = \\cup_{k \\in K} M_{SAM}(H\\cdot G(S_{therbligs, k}), B)\\) represents object mask segmentation using the SAM model, while \\(Box_k = M_{YOLO}(I_{new}, ComputeArea(m_k))\\) denotes bounding box de-tection through YOLOv8. The feature extraction is handled by \\(F_{new} = SIFT(Box_k)\\) using SIFT, feature matching is per-"}, {"title": "D. LAP-VC: LLM-Alignment Policy for Visual Correction", "content": "Expert demonstration may have some errors: robot end-effector grasping point may not always be perpendicular to the object itself. In addition, hand-eye calibration may also introduce some errors. These issues in real-world ex-periments may cause incorrect position estimation and affect"}, {"title": "IV. EXPERIMENTS AND SETUP", "content": "To demonstrate the model's generality, we developed the Omniverse Robot System (ORS) for convenient data collec-tion. Offline data collection involved two individuals: one performing expert demonstrations and the other labeling the robot task's status. The system collected data on robot joint angles, end effector pose, joint speed, force, and torque read-ings from the 'Robotiq' sensor, task images, and real-time labels. We gathered data from six tasks (tool-pick&place, crossbeam-cutting, bricks-gluing, tissue-sweeping, surface-wiping, cup-pouring) with expert demonstrations. For online testing, a human expert performed new tasks in unseen sce-narios, with ORS recording robot states and scenario images before and after the task. We tested five challenging tasks (board-rolling, foamblock-flipping, plate-scrubbing, spoon-tilting, and paper-stamping) to evaluate performance.\nWe utilized a UR5 robotic arm with an origami gripper, inspired by Liu et al. [24], for manipulation tasks. This setup improves adaptability and precision. Visual and force feedback were captured using a RealSense D435i camera and a RoboTiq force sensor. The setup ran on Ubuntu 22.04, with ROS2 Humble for component communication and MoveIt2 for motion planning. Development was conducted in VSCode and model training utilized an NVIDIA RTX 4090 GPU."}, {"title": "V. RESULTS AND ANALYSIS", "content": "For offline training, we used robot states as input to segment therbligs. In this time-series segmentation task, our network outperforms other state-of-the-art methods. As shown in Table II, our network achieved the highest recall of 94.37%, surpassing traditional methods like Cascade SVM and deep learning models such as CNN-RNN and BiLSTM-Type3. In addition, we conducted an ablation study for our MGSF network to evaluate the impact of different components. The baseline recall of our backbone model (without fusion) was approximately 79. 77% with a trans-former network. When we introduced the fusion mechanism, the recall improved to 84.29%. The addition of gate fusion mechanism boosted the recall to 90.92%. The final MGSF model, which incorporates gated fusion and meta-learning, achieved an recall of 94.37% (Table II).\nMoreover, we also analyzed the recall of different therbligs in terms of various tasks (Fig. 7). Generally, surface wiping achieved the highest segmentation results (97.17%) with six diverse robot tasks, while tissue sweeping achieved the lowest results (93.88%). This discrepancy may be attributed to the complexity of tissue sweeping actions required to effectively put tissue into the dustpan. TLoad and Re-lease achieved the lowest recall results, around 85.56% and 83.41% respectively. However, it is important to note that even though the release recall is relatively lower, this does not significantly affect the overall results. This is because, after intricate manipulation operations, the force-torque sensors may be affected by drift.\nFurthermore, as shown in Fig. 8, the LAP-VC system consistently achieves high alignment performance scores across various tasks, outperforming traditional methods like KNN, SIFT, ORB, AKAZE, FAST, and BRISK. In particular, for the Roller task, the LAP-VC system achieved a score of 0.92, which is second only to the manual alignment score of 0.98. Similarly, in the Spoon task, the LAP-VC scored 0.88, close to the perfect manual score of 1.00. In the Stamp, Sponge, and Scraper tasks, the LAP-VC system scored 0.84, 0.94, and 0.90 respectively, showing a consistent high performance. Compared to other automated methods, LAP-VC shows superior robustness and reliability.\nFor the results of task execution, Table III showed that our system can achieve promising performance in terms of simple scenario (SimScenario) and complex scenario (ComScenario) with different new tasks.\nFor our SimScenario, we only consider two task-related and unseen objects appearing in our vision area. For Com-Scenario, we add 3-6 unrelated and unseen objects to-gether with our task-related objects to mimic the real-world cluttered environments. For SimScenario mode, our system achieved around 94.4% average success rate for five tasks, while our success rate decreased to 80% if we switched to ComScenario mode. We find that spoon-tilting has a lower success rate because it requires a more dynamic and complex trajectory while robot solvers easily get trapped by singularity. We compared our system against Single-Task State Machine (ST-SM), Single-Task Behavior Clone (ST-BC), and Multi-Task Behavior Clone (MT-BC) baselines. State machines, built for single tasks, use pre-built policies and vision features to generate trajectories. In SimScenario,\nstate machines achieve some success with well-built policies and excellent vision features. However, different tasks require different policies, significantly increasing system complexity, as noted by Luo et al. [16]. We collected 250 expert demon-strations for BC training and found that MT-BC training is challenging with limited data. ST-BC showed converging loss but resulted in unstable trajectories and task failures with minor errors. Applying BC to ComScenario led to divergent training and unpredictable trajectories.\nACT and diffusion policies were not used as baselines for a few reasons. Our system performs one-shot tasks and operates in zero-shot scenarios without requiring extensive pre-training for scene objects, thus needing less data. In contrast, ACT approaches typically require large datasets. Additionally, our system handles long-horizon tasks involv-ing comprehensive action sequences, while diffusion policies focus on discrete sub-tasks. Therefore, a direct comparison may not provide meaningful insights. This distinction high-lights our system's data efficiency and generalization."}, {"title": "VI. CONCLUSION AND FUTURE WORK", "content": "In this paper, we presented a novel TBBF to enhance the understanding and execution of robotic tasks. By de-composing complex tasks into fundamental therbligs, our innovative approach provides a structured and interpretable representation of robotic actions. The MGSF network was employed for accurate therblig segmentation during the of-fline training stage, while ActionREG efficiently facilitated precise action registration and integration of therbligs with object configurations during the online testing stage.\nOur experimental results demonstrate the effectiveness of our framework, showcasing high recall in therblig seg-mentation and robust performance in real-world robot task execution. Specifically, we achieved results with 94.37% recall in therblig segmentation and impressive successful execution rates of 94.4% for new and long-horizon tasks in simple scenarios, and 80% in complex scenarios. However, some limitations should be addressed in future work. Firstly, our model relies heavily on standard expert demonstrations, which must follow specific protocols. It struggles with ex-tremely noisy or unprofessional demonstrations. Secondly, our model, like many existing approaches, is affected by object shadows and light reflections. Additionally, we fo-cused on the generation of 2D object configurations and did not account for 3D object configurations, which could be improved with depth information.\nFuture research will focus on refining the TBBF to enhance its robustness and adaptability in more complex and dynamic environments. We will also incorporate more user data to train a more robust action segmentation network. Further-more, we plan to extend the applicability of our methods to a broader range of robotics platforms, facilitating the transfer of robot knowledge from the UR5 to other platforms such as Panda or Kinova robots."}]}