{"title": "Model Surgery: Modulating LLM's Behavior Via Simple Parameter Editing", "authors": ["Huanqian Wang", "Yang Yue", "Rui Lu", "Jingxin Shi", "Andrew Zhao", "Shenzhi Wang", "Shiji Song", "Gao Huang"], "abstract": "Large Language Models (LLMs) have demonstrated great potential as generalist as- sistants, showcasing powerful task understanding and problem-solving capabilities. To deploy LLMs as AI assistants, it is crucial that these models exhibit desirable behavioral traits, such as non-toxicity and resilience against jailbreak attempts. Current methods for detoxification or preventing jailbreaking usually involve Su- pervised Fine-Tuning (SFT) or Reinforcement Learning from Human Feedback (RLHF), which requires finetuning billions of parameters through gradient descent with substantial computation cost. Furthermore, models modified through SFT and RLHF may deviate from the pretrained models, potentially leading to a degrada- tion in foundational LLM capabilities. In this paper, we observe that surprisingly, directly editing a small subset of parameters can effectively modulate specific be- haviors of LLMs, such as detoxification and resistance to jailbreaking. Specifically, for a behavior that we aim to avoid, we employ a linear classifier, which we term the behavior probe, to classify binary behavior labels within the hidden state space of the LLM. Using this probe, we introduce an algorithm to identify a critical subset of LLM parameters that significantly influence this targeted behavior. Then we directly edit these selected parameters by shifting them towards the behavior probe. Such a direct parameter editing method necessitates only inference-level computa- tional resources. Experiments demonstrate that in the representative detoxification task, our approach achieves reductions of up to 90.0% in toxicity on the RealToxic- ityPrompts dataset and 49.2% on ToxiGen, while maintaining the LLM's general capabilities in areas such as common sense, question answering, and mathematics.", "sections": [{"title": "1 Introduction", "content": "In the recent past, LLMs have exhibited extraordinary capacities such as natural language under- standing, text generation, and problem-solving capabilities (Achiam et al., 2023; Touvron et al., 2023; Jiang et al., 2023). These advances have spurred the potential of LLMs to serve as human-like assistants. Despite the promising prospect demonstrated by LLMs, non-toxicity and safety have emerged as primary concerns for their application. For example, it is crucial to prevent LLMs from generating harmful or biased content in response to malicious prompts or from instructing users on how to manufacture harmful substances. Current strategies for addressing undesirable behaviors in LLMs typically involve fine-tuning on curated datasets (Bianchi et al., 2024; Taori et al., 2023; Perez et al., 2022; Zhao et al., 2024) or employing reward models focusing on toxicity and safety (Ouyang et al., 2022; Touvron et al., 2023; Dai et al., 2023; Zhao et al., 2024). An alternative approach is machine unlearning techniques, which employ methods such as gradient ascent to effectively remove previously learned undesirable behaviors (Zhang et al., 2024b; Liu et al., 2024; Zhang et al., 2024a). While these techniques are effective in promoting non-toxicity and safety, they necessitate the training of a LLM. This training paradigm involves gradient computation, demanding considerable computational resources due to the billions of parameters in LLMs. Employing a safety-focused reward model with RLHF also requires an additional reference model and an optional reward model, further increasing the demand for resources. Additionally, previous studies indicate that models modified through SFT and RLHF may deviate from the pretrained models, potentially leading to a degradation in foundational LLM capabilities such as comprehension, reasoning, and common sense-an effect known as the alignment tax (Bai et al., 2022; Lin et al., 2024; Askell et al., 2021). These shortcomings present significant challenges in regulating LLM behavior, thereby hindering their use as safer and more user-friendly conversational assistants."}, {"title": "2 Related Works", "content": "Alignment Algorithms. Aligning LLMs towards human-desired objectives is a problem that has been significantly noticed. Common methods for model alignment usually involve SFT and RLHF. SFT (Brown et al., 2020; Wang et al., 2022) finetunes a pre-trained model on task-specific data which contains instructional commands and human-annotated expected outcome (Chiang et al., 2023; Taori et al., 2023). RLHF is a technique that fine-tunes language models using human preferences to align their outputs with desired behaviors. Glaese et al. (2022); Rafailov et al. (2024) use RLHF to improve LLM safety when facing malicious questions. However, successfully training models using SFT or RLHF is challenging. The quality and quantity of training data are crucial for good training results and effectiveness (Zhou et al., 2024; Wang et al., 2024; Taori et al., 2023; Achiam et al., 2023; Touvron et al., 2023), requiring extensive data collection, cleaning, computational resources, and time. Besides, researchers have also discovered that during the training process of SFT or RLHF, the reasoning and understanding capabilities of models may decrease (Ouyang et al., 2022; Lu et al., 2024; Yue et al., 2024). This phenomenon may be caused by overestimating the model to overfit to the reward model or training data distribution (Noukhovitch et al., 2023; Rita et al., 2024), deviating from the original model and losing general capabilities.\nModification of LLM Parameters and forward process. Prior studies have explored modifying the forward propagation process or directly altering model parameters. Meng et al. (2022, 2023) propose model editing methods to update or insert specific knowledge without affecting other basic knowledge. Geva et al. (2022) hypothesize the existence of word vectors in MLP layers strongly correlating with specific tokens and propose setting activations of selected word vectors to a constant for detoxification. Rimsky et al. (2023); Lee et al. (2024); Turner et al. (2023); Wang and Shu (2023) detoxify LLMs by subtracting probes from the last transformer block output or activation vectors at every time-step, which is effective but inefficient due to additional modifications during forward propagation. Ilharco et al. (2023); Yadav et al. (2023); Liu et al. (2024); Huang et al. (2024) demonstrate combining or removing specific attributes or skills by adding or subtracting task vectors with the same shape as the original model to its weights, but this requires supervised fine-tuning and significant computational resources."}, {"title": "3 Method", "content": "LLMs show promise for developing AI assistants but exhibit problematic behaviors like generating toxic content, limiting their broader application. Previous mitigation attempts such as fine-tuning or RLHF, can reduce unwanted outputs but are computationally expensive. Moreover, extensive SFT or RLHF can lead to alignment tax or catastrophic forgetting (Luo et al., 2024; Kaufmann et al., 2024).\nOverview. In this paper, we explore a simple approach to modulate LLM behaviors by selectively adjusting a small subset of the model's parameters, without the need of explicit gradient computations. Specifically, we first train a behavior probe on a binary-labeled dataset (Section 3.1). This probe helps us identify the key parameters in LLMs that are most influential in governing undesirable behaviors (Section 3.2). Once identified, we edit these parameters by model surgery to mitigate such unwanted behaviors (Section 3.3). This approach reduces the requirements for heavy computation and memory resources, as well as minimize the alternation to model parameters, thereby reducing alignment tax."}, {"title": "3.1 Behavior Probe Extraction", "content": "Train Behavior Probe. Previous research (Marks and Tegmark, 2023; Park et al., 2023) has demonstrated that language models linearly encode the truthfulness of factual statements, enabling the training of probes to detect deceptive behavior. Inspired by this finding, we hypothesize that other behaviors, such as toxicity or attempts to bypass content restrictions (i.e., jailbreak), are similarly represented in a linear fashion within the hidden states of LLMs. To test this hypothesis, we conducted experiments using a linear probe trained on datasets labeled for binary behaviors. Specifically, for a LLM with parameters 0, we sample input data x paired with a binary label $y \\in \\{0, 1\\}$ (indicating, for example, whether the content is toxic). The input x is processed by the LLM to produce hidden states. We then use the mean of the hidden states across all tokens in x from the l-th transformer block as the feature representation, denoted as $x^l \\in \\mathbb{R}^d$ Lee et al. (2024). A linear classifier, parameterized by W, is used to predict the probability:\n$P(y|x^l) = \\text{softmax}(Wx^l), W\\in\\mathbb{R}^{2\\times d},$\n(1)\nThe classifier is trained using the Cross-Entropy loss to match the ground truth label y. The objective is for the learned probe W to effectively distinguish between two contrasting behaviors based on the hidden representations from the LLM.\nLinearly classifiable representations. As illustrated in Table 1, a simple linear classifier achieves relatively good classification results, with accuracies exceeding 90% for the JigSaw dataset (Van Aken et al., 2018) and dataset consisting of jailbreak answers and jailbreak rejection an- swers, and 83.1% for the go-emotion dataset (Demszky et al., 2020). These observations reveal the effectiveness of linear probes in capturing and differentiating specific behaviors in LLMs. The classifier matrix W can be de- composed into two distinct probes: $W_p$ and $W_n$, corresponding to W[0] and W[1], respectively. For example, in the context of distinguishing toxic from non-toxic content, $W_p$ represents the probe aligned with non-toxic hidden states, expecting a high dot product with such states. Conversely, $W_n$ aligns with toxic hidden states, identifying features associated with undesirable content."}, {"title": "3.2 Behavior Region Selection", "content": "We have empirically demonstrated that representations corresponding to a specific behavior or its opposite can be linearly classified; that is, there exists a hyperplane in the hidden space that distinctly separates these behaviors. To modulate a model's behavior, a natural hypothesis is to shift the hidden outputs from the region associated with undesirable behaviors towards a more favorable region. In this section, we detail the methodology to identify the key parameters in LLMs that contribute most significantly to the model's tendency to output in the direction of undesirable behaviors.\nThe principle of modulating LLM's behavior. To shift the hidden output towards a more desirable direction, we first identify the parameter regions that are most relevant to the direction of the hidden output. In transformer Vaswani et al. (2017), the hidden output of a LLM at the l-th layer is produced by a two-layer MLP with activation function \u03c3, as described by:\n$x^l = W_2\\sigma(W_1x_{\\text{attn}} + b_1) + b_2,$\nwhere $x_{\\text{attn}}$ is the output of the attention mechanism, and $W_1$ is called the gate projection ma- trix. The hidden state $x^l$ essentially represents a weighted sum of the row vectors of $W_2 = [W_{2,1}, W_{2,2}, ..., W_{2,N}]$, where the weights are denoted as $\u03c3(W_1x_{\\text{att}} + b_1) = [\u03c3_1, \u03c3_2, ..., \u03c3_N]$. As demonstrated in Section 3.1, specific behaviors correspond to particular directions of $x^l$ in the hidden space. Therefore, modifying the model's behavior may involve altering the activation statuses, de- noted by $\u03c3_i$. This adjustment affects the contribution of each base vector $W_{2,i}$ to the hidden output $x^l$. For example, deactivating certain vectors contributing to a toxic hidden state $x^l$ could shift the resulting hidden state away from the toxic region. Conversely, another strategy to avoid the toxic region is to activate vectors that are typically inactive during generating a toxic hidden state. Here, we opt for the latter strategy due to its superior empirical performance, as we will illustrate in Section 4.\nBehavior Region Selection. The scalar $\u03c3_i$ is determined by $W_{1,i}x_{\\text{attn}}$, where $W_{1,i}$ is the i-th row vec- tor of the gated projection matrix. To activate vectors that typically remain inactive when generating a toxic hidden state, we first identify those vectors $W_{1,i}$ that are more likely to result in $\u03c3_i < 0$. Instead of setting $\u03c3_i > 0$ during each inference, we aim to directly modify the model's parameters to change the statuses of inactive vectors. We select row vectors from the gated projection matrix $W_1$ across all layers as the candidate region for editing. Specifically, we determine a representative $t_n$ for a behavior and identify K row vectors that exhibit the highest negative cosine similarity (i.e., close to -1) with $t_n$. These selected row vectors are denoted as the behavior region. However, acquiring $t_n$ is challenging due to the varying input tokens and LLM layers. For simplicity, we approximate $t_n$ using the behavior probe W. The rationale behind this is that the residual connection in the Transformer He et al. (2016); Vaswani et al. (2017) aligns $x^l$ with $x_{\\text{attn}}^l$, and W represents the average direction of $x^l$ when generating the specific behavior."}, {"title": "3.3 Model Surgery", "content": "To shift the hidden output away from undesirable regions and modulate LLM's behavior, we can adjust the selected regions to better align with $x_{\\text{attn}}^l$, i.e., the behavior probe W. It aims to achieve a larger dot product, thereby enhancing the likelihood of being activated for those inactivated $\u03c3_i$. For each selected row vector $v_{\\text{select}}$ in gated projection matrices, the editing process can be described as:\n$v_{\\text{select}} = v_{\\text{select}} + \u03b1 \\cdot W,$\n(2)\nwhere \u03b1 is a scaling factor that modulates the influence of W on $v_{\\text{select}}$. After editing, we obtain a new model that is less likely to produce undesirable behaviors during inference."}, {"title": "4 Experiment", "content": "In this section, we conduct experiments to evaluate the effectiveness of our proposed model surgery technique across three distinct tasks: detoxification, jailbreak, and attitude adjustment. Our aim is to address the following research questions:\n1. How does model surgery maintain the overall capabilities of large language models while implementing behavioral modifications? (Sections 4.1, 4.2, 4.3, 4.4)\n2. Can model surgery enable the simultaneously multiple behavioral changes? (Section 4.5)\n3. What are the critical components of our model surgery technique? (Section 4.6)\nSetup. We conducted experiments on the LLaMA2-7B model (Touvron et al., 2023), except for jailbreaking-relevant tasks, where we employed the aligned LLaMA2-7B-Chat model (Touvron et al., 2023) following Huang et al. (2023); Hasan et al. (2024). The chat model was chosen because jailbreaking tasks involve circumventing a well-aligned model's safety constraints. In Section 4.4, we also validated the effectiveness of our methods on CodeLLaMA-7B (Roziere et al., 2023) and Mistral-v0.1-7B (Jiang et al., 2023). For model surgery implementation, we selected 16,384 (32 x 512) vectors most inversely aligned with the probe direction from the total of 352,256 (32 x 11,008) gated projection vectors across 32 transformer blocks in these LLMs. Consequently, the edited parameters account for 67M (16,384 x 4,096) parameters. More details can be found in Appendix C.\nEvaluation tools. We tested both specific tasks we want to modulate and the fundamental abilities of LLMs. For detoxification tasks, we used ToxiGen (Hartvigsen et al., 2022) and RealToxicityPrompts- Challenge (Gehman et al., 2020). The LLM's resilience against jailbreaking attempts was evaluated using the benchmark proposed by Hasan et al. (2024). For attitude adjustment, we employed ChatGPT to assess the models' ability to maintain positive attitudes in response to negative prompts. To evaluate the general capabilities, we utilized GSM8K (EM) (Cobbe et al., 2021), BBH (EM) (Cobbe et al., 2021), MMLU (EM) (Hendrycks et al., 2020), TydiQA (F1) (Clark et al., 2020), and WikiText (ppl) (Merity et al., 2016), following Ivison et al. (2023).\nBaselines. We compare our method with several representative SFT and model editing approaches. For SFT implementation, we choose the epoch where task-specific performance improved while min- imizing general abilities degradation (see Appendix B). Task vector (Ilharco et al., 2023) modulates task-specific performance by adding parameter differences between task-tuned and original models. Hidden feature subtraction (Lee et al., 2024) subtracts a toxic probe from the hidden states of the last transformer block during each forward pass. Contrastive decoding (Niu et al., 2024) fine-tunes virtual tokens and subtracts toxic feature to prevent harmful content generation. Wanda Pruning (Hasan et al., 2024) removes parameters that likely generate jailbreak content. Safe vector activation (Geva et al., 2022) activates specific MLP vectors to influence the generation of particular tokens."}, {"title": "4.1 Detoxification", "content": "Results of detoxification are presented in Table 2. Our method significantly reduces the toxicity of base model while keeping its core performance. Compared to the original LLaMA2-7B model, our method mitigates 50% of the model's toxicity on ToxiGen benchmark and 90% on the RealToxicityPrompts dataset. We observe that while most of baseline methods are effective in detoxification, they easily hurt the model's fundamental performance. The balance between toxicity reduction and performance preservation represents our method a key advancement over existing baselines."}, {"title": "4.2 Jailbreak Resistance and Surrender", "content": "Jailbreak resistance. In this task, we use LLaMA-2-Chat as our base aligned-model. Following Gan- guli et al. (2022), we use a dataset of 500 responses to jailbreak prompts (Bhardwaj and Poria, 2023), including both instances of refusal to response and cases where models generate harmful responses. For evaluation, we use string matching following (Zou et al., 2023) and prompt GPT-4 to examine the refusal rate of our methods. The performance of our approach on both jailbreak tasks and general capability tasks is presented in Table 3. The results show that model surgery achieves the best performance with negligible degradation of general abilities.\nJailbreak surrender. Model surgery has shown its effectiveness in steering the model away from undesirable directions. Naturally, this raises the question: can model surgery produce a contrasting effect? To test this, we changing the probe used for parameter modification (2) from $W_n$ to $W_p$. The results in Table 4 reveal that by shifting the hidden states in the opposite direction, model surgery can successfully make LLMs more susceptible to jailbreaking attacks."}, {"title": "4.3 Attitude Adjustment", "content": "Maintaining a positive tone is crucial for LLMs, especially in contexts like psychological consultations. We aim to modify the model to produce more positive content when faced with negative inputs. We train probes for both positive and negative categories on the LLaMA2-7B model using the GoEmotions dataset (Demszky et al., 2020). For evaluation, we collect negative prompts as the emotion dataset (Saravia et al., 2018) as inputs. We use ChatGPT to measure the model's ability to shift output from negative to neutral and positive, and from positive to negative and neutral. Results are in Table 5. Although the SFT model achieves slightly better performance on non-negative metrics, it suffers a significant degradation in general capabilities. The task vector method performs comparably to our method but requires significantly more computational resources."}, {"title": "4.4 Extending to Different Model Architectures", "content": "To demonstrate the wide applicability and usability of our method across various large language models, we extended our approach to other LLMs. We apply our approach to CodeLLaMA-7B and to Mistral-7B-v0.1. The results of these experiments are presented in Table 7. These additional experiments demonstrate that our method is effective across a wide spectrum of LLMs."}, {"title": "4.5 Characteristics Addition", "content": "In this subsection, we explore layering additional characteristics onto previously modified models to endow LLMs with more complex personalities, such as making a model more inclined towards negative expressions after detoxification. We use a toxic probe trained on the original model $M_0$ to create a detoxified version$M_1$. We then train a negative sentiment probe on $M_1$ and apply it to produce $M_2$, resulting in a both non-toxic and more negative model. Results are shown in Table 8. Compared to $M_1$, $M_2$ is more inclined towards negative expressions while maintaining its detoxification properties. Therefore, model surgery allows LLMs to be continuously imbued with desired features, enabling the construction of more comprehensive and versatile models."}, {"title": "4.6 Ablation Study", "content": "In this section, we conduct ablation studies on the detoxification task to investigate the critical design elements in model surgery.\nBehavior probe v.s. Random probe. Here we replace the behavior probe with a random probe drawn from a Gaussian distribution while keeping the selected behavior region unchanged. Table 9 shows it has little effect on toxic behavior and general abilities. This can be explained by the fact that randomly drawn vectors are likely to be orthogonal to a given vector in a high-dimensional space.\nBehavior Region vs. Random Region Here we add the behavior probe into random regions of the gated projection weights. The results in Table 9 reveal that this method can detoxify the model, but it is less effective than model surgery. This can be explained by the fact that activating random vectors has less impact on shifting away from the behavior region compared to most conversely aligned ones.\nMin Similarity + Addition v.s. Max Similarity + Subtraction Model surgery involves activating vectors typically inactive during the generation of unwanted behavior, which refers adding the probe to row vectors of MLP weights that have the least cosine similarity with the behavior probe. In Table 9, we select row vectors of MLP weights that have the largest cosine similarity with the behavior probe and subtract the probe from these selected regions. This alternative strategy is less effective.\nEffect of Hidden Space in Specific Layer Indices. We use hidden features from layers 1, 16, 31 and 32 to train probes and investigate the effects of hidden features generated from both shallow and deep layers. Table 9 indicates that probes trained from $L = 16, 31, 32$ have similar effects on modulating behavior, while $L = 1$ impairs both the performance of detoxification and general abilities. This finding is consistent with previous research by Geva et al. (2022), which shows that the hidden states of deeper transformer layers reach a saturation point, whereas shallow layers do not.\nEffect of Hyper-parameter \u03b1. We varied \u03b1 from -4 to 1 to observe its effect. As shown in Table 10, when \u03b1 is greater than 0 and increases, the effect of detoxification becomes more significant. Conversely, when \u03b1 is less than 0 and decreases, the model surgery exerts an opposite effect, generating more toxic outputs."}, {"title": "5 Discussion", "content": "Does the probe direction truly represent the direc- tion of undesirable behavior in the hidden space? In the Jigsaw dataset, we performed gradient ascent on the toxicity classification loss using the trained fixed probe (see Section 3.1). However, unlike in Sec- tion 3.1, we employed the trained probe with it fixed and adjusted the LLM's full parameters, thereby shifting the LLM's hidden state away from the probe direction. As presented in Table 12, this adjustment reduces the model's toxicity, which confirms that the toxic probe represents the direction of undesirable behavior in the hidden space, and moving away from this direction can decrease undesirable behavior.\nCan model surgery effectively shift the hidden state away from the undesirable direction? We calculated the binary classification loss on the Jigsaw dataset as described in Section 3.1. As illustrated in Table 13, our findings indicate that model surgery effectively increases the toxic loss and decreases the non-toxic loss, i.e., shifting the hidden state away from the direction indicated by the toxic probe and towards a non-toxic direction.\nWhy can our method preserve general capabilities? Figure 2 shows the cosines similarity between each pair of the behavior probes $W_{attn}^L$ and the representative vectors $\u03b1_{attn}^L$ of task prompts such as GSM8K, BBH and TydiQA. We observe that the behavior probes and the representative vectors of the task prompts evaluating general abilities, are almost orthogonal, i.e., $W_{attn}^L \u03b1_{attn}^L$ is nearly 0. Therefore, when the modified model attempts to address general problems with specific-tasks' prompts as input, the linear addition of \u03b1 \u00b7 W to specific row vectors of $W_1$ (Equation (2)) exerts only a slight influence on the output of the gate projection. To further substantiate this point, we present the distribution of activations before and after the model surgery in Figure 2. The number of activation values significantly increases when toxic prompts are inputted, aligning with our motivation that model surgery activates the weights of some previously inactive vectors to shift away the undesirable directions, as discussed in Section 3.2. Conversely, the activation distribution remains largely unchanged for mathematical query prompts. This observed pattern supports our hypothesis."}, {"title": "6 Conclusion and Limitations", "content": "This study presented a computationally-efficient methodology for modulating LLM's behavior. The training process necessitates only a few hundred prompts in certain tasks and solely requires forward propagation, significantly reducing computational resource consumption. Moreover, the proposed approach is extended to encompass a diverse array of behavioral attributes, including, but not limited to, toxicity, resistance to jailbreaking attempts, and the rectification of negative sentiments. In addition, our method does not change the performance of the model within a limited scope. Despite our best efforts, there remain several aspects that are not covered in this paper. For example, although our method has provided some empirical analysis, we have not explored the underlying principles, which will be left for our future work."}, {"title": "F Social Impact", "content": "We propose an approach can significantly reduce the computational cost to modulate LLM's behavior, making it more accessible and practical for real-world applications. The improved performance and efficiency of our approach can have a direct positive impact on modulating a harmless and positive LLM. Besides, our work has the potential to give more inspirations for future research in the area of LLM. However, the potential negative societal impacts of our method align with those typically associated with LLM safety. We emphasize the importance of adhering to fair and safe deployment principles in the area of LLM."}]}