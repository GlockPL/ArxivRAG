{"title": "Model Surgery: Modulating LLM's Behavior Via Simple Parameter Editing", "authors": ["Huanqian Wang", "Yang Yue", "Rui Lu", "Jingxin Shi", "Andrew Zhao", "Shenzhi Wang", "Shiji Song", "Gao Huang"], "abstract": "Large Language Models (LLMs) have demonstrated great potential as generalist as-\nsistants, showcasing powerful task understanding and problem-solving capabilities.\nTo deploy LLMs as AI assistants, it is crucial that these models exhibit desirable\nbehavioral traits, such as non-toxicity and resilience against jailbreak attempts.\nCurrent methods for detoxification or preventing jailbreaking usually involve Su-\npervised Fine-Tuning (SFT) or Reinforcement Learning from Human Feedback\n(RLHF), which requires finetuning billions of parameters through gradient descent\nwith substantial computation cost. Furthermore, models modified through SFT and\nRLHF may deviate from the pretrained models, potentially leading to a degrada-\ntion in foundational LLM capabilities. In this paper, we observe that surprisingly,\ndirectly editing a small subset of parameters can effectively modulate specific be-\nhaviors of LLMs, such as detoxification and resistance to jailbreaking. Specifically,\nfor a behavior that we aim to avoid, we employ a linear classifier, which we term\nthe behavior probe, to classify binary behavior labels within the hidden state space\nof the LLM. Using this probe, we introduce an algorithm to identify a critical subset\nof LLM parameters that significantly influence this targeted behavior. Then we\ndirectly edit these selected parameters by shifting them towards the behavior probe.\nSuch a direct parameter editing method necessitates only inference-level computa-\ntional resources. Experiments demonstrate that in the representative detoxification\ntask, our approach achieves reductions of up to 90.0% in toxicity on the RealToxic-\nityPrompts dataset and 49.2% on ToxiGen, while maintaining the LLM's general\ncapabilities in areas such as common sense, question answering, and mathematics.", "sections": [{"title": "1 Introduction", "content": "In the recent past, LLMs have exhibited extraordinary capacities such as natural language under-\nstanding, text generation, and problem-solving capabilities. These advances have spurred the potential of LLMs to serve as human-like\nassistants. Despite the promising prospect demonstrated by LLMs, non-toxicity and safety have\nemerged as primary concerns for their application. For example, it is crucial to prevent LLMs from\ngenerating harmful or biased content in response to malicious prompts or from instructing users on\nhow to manufacture harmful substances. Current strategies for addressing undesirable behaviors in\nLLMs typically involve fine-tuning on curated datasets or employing reward models focusing on toxicity and safety. An alternative approach is"}, {"title": "2 Related Works", "content": "Alignment Algorithms. Aligning LLMs towards human-desired objectives is a problem that has\nbeen significantly noticed. Common methods for model alignment usually involve SFT and RLHF.\nSFT finetunes a pre-trained model on task-specific data which\ncontains instructional commands and human-annotated expected outcome. RLHF is a technique that fine-tunes language models using human preferences to\nalign their outputs with desired behaviors. However, successfully training models using\nSFT or RLHF is challenging. The quality and quantity of training data are crucial for good training\nresults and effectiveness, requiring extensive data collection, cleaning, computational resources, and\ntime. Besides, researchers have also discovered that during the training process of SFT or RLHF,\nthe reasoning and understanding capabilities of models may decrease. This phenomenon may be caused by overestimating the model to overfit to\nthe reward model or training data distribution, deviating\nfrom the original model and losing general capabilities.\nModification of LLM Parameters and forward process. Prior studies have explored modifying\nthe forward propagation process or directly altering model parameters. propose model editing methods to update or insert specific knowledge without affecting other basic\nknowledge. hypothesize the existence of word vectors in MLP layers strongly\ncorrelating with specific tokens and propose setting activations of selected word vectors to a constant\nfor detoxification. detoxify LLMs by subtracting probes from the last transformer block output or activation\nvectors at every time-step, which is effective but inefficient due to additional modifications during\nforward propagation. demonstrate combining or removing specific attributes or skills by adding or subtracting task vectors\nwith the same shape as the original model to its weights, but this requires supervised fine-tuning and\nsignificant computational resources."}, {"title": "3 Method", "content": "LLMs show promise for developing AI assistants but exhibit problematic behaviors like generating\ntoxic content, limiting their broader application. Previous mitigation attempts such as fine-tuning or\nRLHF, can reduce unwanted outputs but are computationally expensive. Moreover, extensive SFT or\nRLHF can lead to alignment tax or catastrophic forgetting. \nOverview. In this paper, we explore a simple approach to modulate LLM behaviors by selectively\nadjusting a small subset of the model's parameters, without the need of explicit gradient computations.\nSpecifically, we first train a behavior probe on a binary-labeled dataset (Section 3.1). This probe helps\nus identify the key parameters in LLMs that are most influential in governing undesirable behaviors\n(Section 3.2). Once identified, we edit these parameters by model surgery to mitigate such unwanted\nbehaviors (Section 3.3). This approach reduces the requirements for heavy computation and memory\nresources, as well as minimize the alternation to model parameters, thereby reducing alignment tax."}, {"title": "3.1 Behavior Probe Extraction", "content": "Train Behavior Probe. Previous research has\ndemonstrated that language models linearly encode the truthfulness of factual statements, enabling\nthe training of probes to detect deceptive behavior. Inspired by this finding, we hypothesize that other\nbehaviors, such as toxicity or attempts to bypass content restrictions (i.e., jailbreak), are similarly\nrepresented in a linear fashion within the hidden states of LLMs. To test this hypothesis, we conducted\nexperiments using a linear probe trained on datasets labeled for binary behaviors. Specifically, for a\nLLM with parameters 0, we sample input data x paired with a binary label y \u2208 {0, 1} (indicating, for\nexample, whether the content is toxic). The input x is processed by the LLM to produce hidden states.\nWe then use the mean of the hidden states across all tokens in x from the l-th transformer block as\nthe feature representation, denoted as $\u00b9 \\in \\mathbb{R}^d$. A linear classifier, parameterized by\nW, is used to predict the probability:\n$P(y|\u00b9)=softmax(W\u00b9), W\u2208\\mathbb{R}^{2\u00d7d},$                                                          (1)\nThe classifier is trained using the Cross-Entropy loss to match the ground truth label y. The objective\nis for the learned probe W to effectively distinguish between two contrasting behaviors based on the\nhidden representations from the LLM.\nLinearly classifiable representations. As illustrated in\nTable 1, a simple linear classifier achieves relatively good\nclassification results, with accuracies exceeding 90% for\nthe JigSaw dataset and dataset\nconsisting of jailbreak answers and jailbreak rejection an-\nswers, and 83.1% for the go-emotion dataset. These observations reveal the effectiveness\nof linear probes in capturing and differentiating specific\nbehaviors in LLMs. The classifier matrix W can be de-"}, {"title": "3.2 Behavior Region Selection", "content": "We have empirically demonstrated that representations corresponding to a specific behavior or its\nopposite can be linearly classified; that is, there exists a hyperplane in the hidden space that distinctly\nseparates these behaviors. To modulate a model's behavior, a natural hypothesis is to shift the hidden\noutputs from the region associated with undesirable behaviors towards a more favorable region. In\nthis section, we detail the methodology to identify the key parameters in LLMs that contribute most\nsignificantly to the model's tendency to output in the direction of undesirable behaviors.\nThe principle of modulating LLM's behavior. To shift the hidden output towards a more desirable\ndirection, we first identify the parameter regions that are most relevant to the direction of the hidden\noutput. In transformer, the hidden output of a LLM at the l-th layer is produced\nby a two-layer MLP with activation function o, as described by:\n$x\u00b2 = W_2o(W_1x_{attn} + b_1) + b_2,$\nwhere $x_{attn}$ is the output of the attention mechanism, and $W_1$ is called the gate projection ma-\ntrix. The hidden state $x\u00b2$ essentially represents a weighted sum of the row vectors of $W_2$ =\n$[W_{2,1}, W_{2,2}, ..., W_{2,N}]$, where the weights are denoted as $o(W_1x_{attn} + b_1) = [o_1, o_2, ..., o_N]$. As\ndemonstrated in Section 3.1, specific behaviors correspond to particular directions of $x\u00b2$ in the hidden\nspace. Therefore, modifying the model's behavior may involve altering the activation statuses, de-\nnoted by $o_i$. This adjustment affects the contribution of each base vector $W_{2,i}$ to the hidden output\n$x\u00b2$. For example, deactivating certain vectors contributing to a toxic hidden state $x\u00b2$ could shift the\nresulting hidden state away from the toxic region. Conversely, another strategy to avoid the toxic\nregion is to activate vectors that are typically inactive during generating a toxic hidden state. Here, we\nopt for the latter strategy due to its superior empirical performance, as we will illustrate in Section 4.\nBehavior Region Selection. The scalar $o_i$ is determined by $W_{1,i}x_{attm}$, where $W_{1,i}$ is the i-th row vec-\ntor of the gated projection matrix. To activate vectors that typically remain inactive when generating a\ntoxic hidden state, we first identify those vectors $W_{1,i}$ that are more likely to result in $o_i$ < 0. Instead\nof setting $o_i$ > 0 during each inference, we aim to directly modify the model's parameters to change\nthe statuses of inactive vectors. We select row vectors from the gated projection matrix $W_1$ across\nall layers as the candidate region for editing. Specifically, we determine a representative $_{attn}$ for a\nbehavior and identify K row vectors that exhibit the highest negative cosine similarity (i.e., close to\n-1) with $_{attn}$. These selected row vectors are denoted as the behavior region. However, acquiring\n$_{attn}$ is challenging due to the varying input tokens and LLM layers. For simplicity, we approximate"}, {"title": "3.3 Model Surgery", "content": "To shift the hidden output away from undesirable regions and modulate LLM's behavior, we can\nadjust the selected regions to better align with $_{attn}$, i.e., the behavior probe W. It aims to achieve a\nlarger dot product, thereby enhancing the likelihood of being activated for those inactivated $o_i$. For\neach selected row vector $v_{select}$ in gated projection matrices, the editing process can be described as:\n$v_{select} = v_{select} + \u03b1 \u00b7 W,$\n(2)\nwhere \u03b1 is a scaling factor that modulates the influence of W on $v_{select}$. After editing, we obtain a\nnew model that is less likely to produce undesirable behaviors during inference."}, {"title": "4 Experiment", "content": "In this section, we conduct experiments to evaluate the effectiveness of our proposed model surgery\ntechnique across three distinct tasks: detoxification, jailbreak, and attitude adjustment. Our aim is to\naddress the following research questions:\n1. How does model surgery maintain the overall capabilities of large language models while\nimplementing behavioral modifications? (Sections 4.1, 4.2, 4.3, 4.4)\n2. Can model surgery enable the simultaneously multiple behavioral changes? (Section 4.5)\n3. What are the critical components of our model surgery technique? (Section 4.6)\nSetup. We conducted experiments on the LLaMA2-7B model , except for\njailbreaking-relevant tasks, where we employed the aligned LLaMA2-7B-Chat model following Huang et al. (2023); Hasan et al. (2024). The chat model was chosen because\njailbreaking tasks involve circumventing a well-aligned model's safety constraints. In Section 4.4,\nwe also validated the effectiveness of our methods on CodeLLaMA-7B and\nMistral-v0.1-7B. For model surgery implementation, we selected 16,384 (32 x\n512) vectors most inversely aligned with the probe direction from the total of 352,256 (32 x 11,008)\ngated projection vectors across 32 transformer blocks in these LLMs. Consequently, the edited\nparameters account for 67M (16,384 x 4,096) parameters. More details can be found in Appendix C.\nEvaluation tools. We tested both specific tasks we want to modulate and the fundamental abilities of\nLLMs. For detoxification tasks, we used ToxiGen and RealToxicityPrompts-\nChallenge. The LLM's resilience against jailbreaking attempts was evaluated\nusing the benchmark proposed by Hasan et al. (2024). For attitude adjustment, we employed ChatGPT\nto assess the models' ability to maintain positive attitudes in response to negative prompts. To evaluate\nthe general capabilities, we utilized GSM8K (EM) , BBH (EM) , MMLU (EM), TydiQA (F1) , and WikiText\n(ppl), following Ivison et al. (2023).\nBaselines. We compare our method with several representative SFT and model editing approaches.\nFor SFT implementation, we choose the epoch where task-specific performance improved while min-\nimizing general abilities degradation (see Appendix B). Task vector modulates\ntask-specific performance by adding parameter differences between task-tuned and original models.\nHidden feature subtraction subtracts a toxic probe from the hidden states of the last\ntransformer block during each forward pass. Contrastive decoding fine-tunes virtual\ntokens and subtracts toxic feature to prevent harmful content generation. Wanda Pruning removes parameters that likely generate jailbreak content. Safe vector activation activates specific MLP vectors to influence the generation of particular tokens."}, {"title": "4.1 Detoxification", "content": "Results of detoxification are presented in Table 2. Our method significantly reduces the toxicity of base\nmodel while keeping its core performance. Compared to the original LLaMA2-7B model, our method\nmitigates 50% of the model's toxicity on ToxiGen benchmark and 90% on the RealToxicityPrompts\ndataset. We observe that while most of baseline methods are effective in detoxification, they easily\nhurt the model's fundamental performance. The balance between toxicity reduction and performance\npreservation represents our method a key advancement over existing baselines."}, {"title": "4.2 Jailbreak Resistance and Surrender", "content": "Jailbreak resistance. In this task, we use LLaMA-2-Chat as our base aligned-model. Following Gan-\nguli et al. (2022), we use a dataset of 500 responses to jailbreak prompts, including both instances of refusal to response and cases where models generate harmful\nresponses. For evaluation, we use string matching following and prompt GPT-4 to\nexamine the refusal rate of our methods. The performance of our approach on both jailbreak tasks\nand general capability tasks is presented in Table 3. The results show that model surgery achieves the\nbest performance with negligible degradation of general abilities.\nJailbreak surrender. Model surgery has shown its effectiveness in steering the model away from\nundesirable directions. Naturally, this raises the question: can model surgery produce a contrasting\neffect? To test this, we changing the probe used for parameter modification (2) from $W_n$ to $W_p$. The\nresults in Table 4 reveal that by shifting the hidden states in the opposite direction, model surgery can\nsuccessfully make LLMs more susceptible to jailbreaking attacks."}, {"title": "4.3 Attitude Adjustment", "content": "Maintaining a positive tone is crucial for LLMs, especially in contexts like psychological consultations.\nWe aim to modify the model to produce more positive content when faced with negative inputs.\nWe train probes for both positive and negative categories on the LLaMA2-7B model using the\nGoEmotions dataset. For evaluation, we collect negative prompts as the\nemotion dataset as inputs. We use ChatGPT to measure the model's ability\nto shift output from negative to neutral and positive, and from positive to negative and neutral.\nResults are in Table 5. Although the SFT model achieves slightly better performance on non-negative\nmetrics, it suffers a significant degradation in general capabilities. The task vector method performs\ncomparably to our method but requires significantly more computational resources."}, {"title": "4.4 Extending to Different Model Architectures", "content": "To demonstrate the wide applicability and usability of our method across various large language\nmodels, we extended our approach to other LLMs. We apply our approach to CodeLLaMA-7B and\nto Mistral-7B-v0.1. The results of these experiments are presented in Table 7. These additional\nexperiments demonstrate that our method is effective across a wide spectrum of LLMs."}, {"title": "4.5 Characteristics Addition", "content": "In this subsection, we explore layering additional characteristics onto previously modified models\nto endow LLMs with more complex personalities, such as making a model more inclined towards\nnegative expressions after detoxification. We use a toxic probe trained on the original model Mo to\ncreate a detoxified versionM\u2081. We then train a negative sentiment probe on M\u2081 and apply it to produce\nM2, resulting in a both non-toxic and more negative model. Results are shown in Table 8. Compared\nto M1, M2 is more inclined towards negative expressions while maintaining its detoxification\nproperties. Therefore, model surgery allows LLMs to be continuously imbued with desired features,\nenabling the construction of more comprehensive and versatile models."}, {"title": "4.6 Ablation Study", "content": "In this section, we conduct ablation studies on the detoxification task to investigate the critical design\nelements in model surgery.\nBehavior probe v.s. Random probe. Here we replace the behavior probe with a random probe\ndrawn from a Gaussian distribution while keeping the selected behavior region unchanged. Table 9\nshows it has little effect on toxic behavior and general abilities. This can be explained by the fact that\nrandomly drawn vectors are likely to be orthogonal to a given vector in a high-dimensional space.\nBehavior Region vs. Random Region Here we add the behavior probe into random regions of the\ngated projection weights. The results in Table 9 reveal that this method can detoxify the model, but it\nis less effective than model surgery. This can be explained by the fact that activating random vectors\nhas less impact on shifting away from the behavior region compared to most conversely aligned ones.\nMin Similarity + Addition v.s. Max Similarity + Subtraction Model surgery involves activating\nvectors typically inactive during the generation of unwanted behavior, which refers adding the probe\nto row vectors of MLP weights that have the least cosine similarity with the behavior probe. In Table 9,"}, {"title": "5 Discussion", "content": "Does the probe direction truly represent the direc-\ntion of undesirable behavior in the hidden space?\nIn the Jigsaw dataset, we performed gradient ascent\non the toxicity classification loss using the trained\nfixed probe (see Section 3.1). However, unlike in Sec-\ntion 3.1, we employed the trained probe with it fixed\nand adjusted the LLM's full parameters, thereby shifting the LLM's hidden state away from the probe\ndirection. As presented in Table 12, this adjustment reduces the model's toxicity, which confirms\nthat the toxic probe represents the direction of undesirable behavior in the hidden space, and moving\naway from this direction can decrease undesirable behavior.\nCan model surgery effectively shift the hidden\nstate away from the undesirable direction? We\ncalculated the binary classification loss on the Jigsaw\ndataset as described in Section 3.1. As illustrated in\nTable 13, our findings indicate that model surgery\neffectively increases the toxic loss and decreases the\nnon-toxic loss, i.e., shifting the hidden state away\nfrom the direction indicated by the toxic probe and towards a non-toxic direction.\nWhy can our method preserve general capabilities? Figure 2 shows the cosines similarity\nbetween each pair of the behavior probes W and the representative vectors $_{attn}$ of task prompts\nsuch as GSM8K, BBH and TydiQA. We observe that the behavior probes and the representative\nvectors of the task prompts evaluating general abilities, are almost orthogonal, i.e., $W_{attn}$ is nearly\n0. Therefore, when the modified model attempts to address general problems with specific-tasks'\nprompts as input, the linear addition of a \u00b7 W to specific row vectors of $W_1$ (Equation (2)) exerts\nonly a slight influence on the output of the gate projection. To further substantiate this point, we\npresent the distribution of activations before and after the model surgery in Figure 2. The number\nof activation values significantly increases when toxic prompts are inputted, aligning with our\nmotivation that model surgery activates the weights of some previously inactive vectors to shift away\nthe undesirable directions, as discussed in Section 3.2. Conversely, the activation distribution remains\nlargely unchanged for mathematical query prompts. This observed pattern supports our hypothesis."}, {"title": "6 Conclusion and Limitations", "content": "This study presented a computationally-efficient methodology for modulating LLM's behavior. The\ntraining process necessitates only a few hundred prompts in certain tasks and solely requires forward\npropagation, significantly reducing computational resource consumption. Moreover, the proposed\napproach is extended to encompass a diverse array of behavioral attributes, including, but not limited\nto, toxicity, resistance to jailbreaking attempts, and the rectification of negative sentiments. In\naddition, our method does not change the performance of the model within a limited scope. Despite\nour best efforts, there remain several aspects that are not covered in this paper. For example, although\nour method has provided some empirical analysis, we have not explored the underlying principles,\nwhich will be left for our future work."}, {"title": "F Social Impact", "content": "We propose an approach can significantly reduce the computational cost to modulate LLM's behavior,\nmaking it more accessible and practical for real-world applications. The improved performance and\nefficiency of our approach can have a direct positive impact on modulating a harmless and positive\nLLM. Besides, our work has the potential to give more inspirations for future research in the area\nof LLM. However, the potential negative societal impacts of our method align with those typically\nassociated with LLM safety. We emphasize the importance of adhering to fair and safe deployment\nprinciples in the area of LLM."}]}