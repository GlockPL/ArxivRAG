{"title": "Open or Closed LLM for Lesser-Resourced Languages? Lessons from Greek", "authors": ["John Pavlopoulos", "Juli Bakagianni", "Kanella Pouli", "Maria Gavriilidou"], "abstract": "Natural Language Processing (NLP) for lesser-resourced languages faces persistent challenges, including limited datasets, inherited biases from high-resource languages, and the need for domain-specific solutions. This study addresses these gaps for Modern Greek through three key contributions. First, we evaluate the performance of open-source (Llama-70b) and closed-source (GPT-40 mini) large language models (LLMs) on seven core NLP tasks with dataset availability, revealing task-specific strengths, weaknesses, and parity in their performance. Second, we expand the scope of Greek NLP by reframing Authorship Attribution as a tool to assess potential data usage by LLMs in pre-training, with high 0-shot accuracy suggesting ethical implications for data provenance. Third, we showcase a legal NLP case study, where a Summarize, Translate, and Embed (STE) methodology outperforms the traditional TF-IDF approach for clustering long legal texts. Together, these contributions provide a roadmap to advance NLP in lesser-resourced languages, bridging gaps in model evaluation, task innovation, and real-world impact.", "sections": [{"title": "1 Introduction", "content": "Natural Language Processing (NLP) tasks have advanced significantly with the help of deep learning, and more recently with large language models (LLMs), the creation of which demands immense volumes of digital data (Brown et al., 2020). While multilingual NLP has benefited from these advances, the progress in lesser-resourced languages significantly lags behind that of well-supported languages. As a result, NLP for the myriad of languages worldwide relies heavily on research conducted for well-established languages, often inheriting their assumptions, biases, and other characteristics that may not align with the features of less supported languages (Bakagianni et al., 2024).\nNLP for lesser-resourced languages is further hindered due to the scarcity of open-access, high-quality language resources. Such resources include datasets, which could be used to train and/or evaluate multilingual models on downstream tasks across languages or language models per se. Comprehensive NLP surveys for resource-lean languages, besides providing evidence on the digital readiness of the language, can also serve as a sound basis to promote NLP research for these languages, and to lead to available language resources for downstream tasks, opening the way to new benchmarks. Our work focuses on one of these languages, Greek,\u00b9 the official language of Greece and one of the two official languages of Cyprus.\nIn this work, based on the recent Greek NLP survey of Bakagianni et al. (2024), we extracted publicly available and open-access Greek datasets that permit derivatives, aggregating them into a unified collection that is easy to re-compile and use.\u00b2\nUsing this collection, we make three contributions:\n\u2022 We benchmarked seven NLP tasks in Greek using a closed- (GPT-40 mini) and an open-source (Llama-70b) LLM, revealing task-specific strengths and weaknesses. LLAMA is better in Named Entity Recognition (NER) and Summarization while GPT is better in Grammatical Error Correction (GEC), Machine Translation (MT; el-jpn), Intent Classification and Part-of-Speech (POS) Tagging. The two models perform on par on Toxicity Detection, MT (el-en, el-fa).\n\u2022 We reframe Authorship Attribution by evaluating LLMs in a 0-shot setting and hypothesising that high accuracy suggests potential inclusion of the authored texts in pre-training"}, {"title": "2 The data", "content": "Based on the systematic literature review of Bakagianni et al. (2024), covering 142 research NLP studies for Greek from 2012 to 2023, we extracted and analysed information on the 94 available Greek datasets. Most of these resources were monolingual (84%), but bilingual and multilingual datasets were also present (i.e., with Greek being one of the languages). The most frequent domains were politics and news (20%) while the most frequent task was sentiment analysis (33%)."}, {"title": "2.1 Availability, Popularity and Licensing", "content": "Figure 1 shows the availability status of the extracted datasets. The most common status is that the datasets are not available, meaning no information is provided regarding their availability. The second most frequent category concerns datasets reported as downloadable, but actually not publicly available due to restrictions, such as missing license or subscription limitations. This limitation contradicts the principles and benefits of open access. Studies with publicly available datasets (serving as a reference point) can attract more citations, as is shown by the higher third and fourth quartile of the green box in Figure 2. Historically (Figure 3), studies attracting citations while presenting data with limited (Juola and Stamatatos, 2013), or with no availability Giatsoglou et al. (2017), belong to the past. The study of Juola and Stamatatos (2013), for example, shown with a blue peak in 2013, concerns a shared task on authorship identification (hence, the citations) whose data lack a license (hence, the limitation). Our work reveals an alternative dataset for this task (\u00a74.2)."}, {"title": "2.2 Distilling FAIR data", "content": "We extracted findable, accessible, interoperable and re-usable (FAIR) datasets from the survey of Bakagianni et al. (2024). We focused on the 14 Greek annotated datasets that were found as licensed, accessible, machine-actionable, and with no hidden costs. We excluded the dataset of Fitsilis and Mikros (2021), which is licensed under CC BY-NC-ND 4.0 and does not allow derivatives (i.e., a limitation of re-usability). Also, Korre et al. (2021) provided two datasets, one of which is too small in size (100 sentences). By disregarding these datasets, we end up with the 12 datasets presented in Table 1. There is a variety of domains and the largest dataset is that of Barzokas et al. (2020) followed by that of Dritsa et al. (2022)."}, {"title": "2.3 Datasets with no supervision signal", "content": "Table 1 comprises three datasets with no ground truth or whose ground truth is automatically extracted (A/N; i.e., of doubted quality). Although excluded from our benchmarking, these datasets may still serve pre-training purposes and are further discussed next. Dritsa et al. (2022) gathered 1.28M political speeches spanning from July 1989 to July 2020. Papantoniou et al. (2023) conducted NER and entity linking on a dataset derived from Greek Wikipedia event pages. Prokopidis and Piperidis (2020) provided 101,857 files, which comprise the cleaned version of websites containing open content, including online archives of Greek newspapers from 2003 to 2020, and the Greek part of the W2C corpus (Majli\u0161 and \u017dabokrtsk\u1ef3, 2012)."}, {"title": "2.3.1 Dataset statistics", "content": "As is shown in Table 2, the text length (measured in characters) greatly varies between the datasets. The dataset with the shortest texts on average is that of Papantoniou et al. (2023) and that with the lengthiest is that of Prokopidis and Piperidis (2020). The dataset size (counted in number of files) also varies, starting from ten thousands (Papantoniou et al., 2023), to one hundred thousands (Prokopidis and Piperidis, 2020), to more than a million (Dritsa et al., 2022). The aggregated text totals up to 1.4 billion characters and 211 million tokens (using white space split)."}, {"title": "2.3.2 Linguistic distance", "content": "We investigate the linguistic associations between these datasets, using statistical language modelling (LMing) as a proxy. For each dataset, we trained one statistical character-level LM on the first 100 characters of 1,000 randomly selected texts per dataset. Then, we computed the Bits per Characters (BPC) across 500 randomly selected texts per dataset, reporting the mean. The result is displayed as a heatmap in Figure 4. High scores (in red) reflecting a greater linguistic distance between the dataset the LM is trained on and the one BPC is computed on. Low scores (in blue) reflect a low linguistic distance, with the lowest scores taking place in the diagonal, when predicting unseen text taken"}, {"title": "2.4 Datasets with supervision signal", "content": "The datasets that can be used as benchmarks for downstream tasks in Greek are presented in Table 3. They cover supervised tasks (i.e., Intent, Toxicity, Authorship) and unsupervised tasks (i.e., Clustering) learning; text-to-text tasks (i.e., MT, GEC, Summarization); and sequence learning tasks (i.e., NER, POS tagging). The task with the lengthiest texts is Clustering and the one with the shortest is POS tagging. We chose Clustering over legal text classification because the dataset serves as a valuable resource for Clustering, a task currently lacking a benchmark. The dataset in this case has 47 clusters (classes), and it also comprises more fine-grained labels, increasing this number to 374 and 1,685. The task with the highest number of classes besides Clustering is NER (18), followed by Authorship (17) and POS tagging (16). Intent classification is the only balanced task (CIR = 1.0)."}, {"title": "3 The Greek NLP Benchmark", "content": "We used licensed, accessible, machine-actionable, and with no hidden costs datasets, summarised in Table 3, to set up a benchmark per task. The ground truth is derived either by human annotators or by metadata sourced from the distributors. We experimented with 0-shot learning using LLAMA (70b-instruction) and GPT (40-mini). We used the test splits, when they were available, and we randomly sampled 175 instances per test set otherwise, mainly to limit the cost for GPT."}, {"title": "3.1 Toxicity Detection", "content": "The dataset In 2020, a subtask of offensive language identification for Greek was introduced as part of the SemEval-2020 Task 12 on Multilingual Offensive Language Identification in Social Media"}, {"title": "3.2 Grammatical Error Correction", "content": "The dataset This task concerns the correction of grammatical errors that vary from grammatical mistakes to punctuation, spelling, and morphology of word. Korre et al. (2021) listed 18 main categories of grammatical errors that systems can correct. They developed two datasets, of which we consider only the one that is annotated by human experts, i.e., the Greek Native Corpus (GNC). This corpus is comprised of essays written by students who are native speakers of Greek, totalling 227 sentences. Each sentence within this dataset may contain zero, one, or multiple grammatical errors, all annotated by human experts with the corresponding grammatical error types as defined in the provided annotation schema.\nThe results Using character (CER) and word (WER) error rate, we find that GPT performs significantly better than LLAMA in correcting grammatical errors. It is halved in WER compared to LLAMA and only 1.74 in CER. Unless GPT has already used the data of Korre et al. (2021) during training, a possibility we cannot exclude for this dataset, this is a very low rate. To gain a better insight, we experimented by also adding two shots in the prompts. Although the performance of LLAMA halved, that of GPT remained unchanged."}, {"title": "3.3 Machine Translation", "content": "The dataset Prokopidis et al. (2016) created bilingual corpora (756 language pairs) from content available in Global Voices, where volunteers translate news stories in 41 languages. The 3,629 Greek documents are translated into 40 languages. However, not every document is translated into all languages, resulting in 17,018 bilingual document pairs involving Greek. In this work, we focus on three Greek language pairs: Greek-English, Greek-Japanese, and Greek-Farsi. These target languages were selected as the most supported languages within their respective language support tiers, as defined in Bakagianni et al. (2024). The tiers are based on the number of ACL Anthology studies (2012-2024) referencing (in their titles/abstracts) each respective language. Well-supported languages are referenced in more than 1,000 studies; moderately-supported languages are referenced in 100 to 1,000; less-supported ones are referenced in less than 100. Based on this classification, the English language represents the"}, {"title": "3.4 Summarization", "content": "The dataset Koniaris et al. (2023) created a legal corpus of 8,395 court decisions from Areios Pagos, the Supreme Civil and Criminal Court of Greece. This corpus includes the decisions, their summaries, and related metadata, all sourced from the Areios Pagos website. Using the (provided) test set of 1,238 Greek legal texts, we find an average text length of 18,541 characters and a standard deviation of 18,351 (min: 2,571 and max: 303,538). When tokenised (white-space split), this is 381 (min) to 46,232 (max) tokens. We only kept texts of 1,000 tokens or fewer, to limit the time and cost of our experiments. This totals 192 texts of 4,899 characters on average with a standard deviation of 1,319 characters (max: 6,983). To remain consistent with the benchmarking of other NLP tasks, we sampled 175 texts for the experiments.\nThe results In addition to BERTScore (Zhang et al., 2019), we also report variants of ROUGE (Lin, 2004). As is shown in Table 7, LLAMA outperforms GPT across most metrics for this task, demonstrating stronger overall performance in generating high-quality summaries. However, GPT slightly surpasses LLAMA in recall, as measured by ROUGE (across variants), suggesting that it better captures content overlap, though at the expense of precision."}, {"title": "3.5 Intent Classification", "content": "The dataset Rizou et al. (2023) collected student queries to two University help desks and manually annotated each of these with three entity tags and six intents. The dataset is balanced regarding the intents. The average text length in characters is 58.6, the maximum is 174 (minimum of 7), and the standard deviation is 30.3."}, {"title": "3.6 NER", "content": "The dataset Bartziokas et al. (2020) provided an annotated dataset with two levels of granularity for entity annotation. The first level uses 4 label tags akin to the CONLL-2003 dataset (Sang and De Meulder, 2003), while the second incorporates 18 entity tags, as in the OntoNotes 5 English dataset (Pradhan et al., 2007). The dataset was developed during a Google Summer of Code project in 2018, where initial automatic annotation was followed by manual curation. The text length varies from 2 to 2,210 characters (Table 3). Both annotation levels exhibit heavy imbalance, with the four-tag level having an imbalance of 0.02 and the 18-tag level showing near-zero imbalance. We opted for the four-entity-tag level of annotation with the lower imbalance.\nThe results Table 9 shows that LLAMA achieves a better F1 score overall compared to GPT. However, the low macro-average scores indicate that both models fail to capture specific entity classes, such as E-MISC and S-MISC (see Appendix (Table 16)). Both models perform well in detecting the O class (i.e., not an entity), yielding an F1 of 0.92, which means that both models can detect entities. The low macro average scores, on the other hand, reflect how both models struggle to distinguish between the different types of entities."}, {"title": "3.7 POS Tagging", "content": "The dataset Prokopidis and Papageorgiou (2017) provided the Greek treebank as part of a project that"}, {"title": "4 Novel Benchmark Tasks for Greek", "content": "Our study provides a unified cross-task Greek NLP benchmark, based on pre-existing datasets. Our work is not the first to reveal existing datasets.\n(Nikiforos et al., 2021), for example, reviewed methods and datasets related to the Greek social web. Alexandridis et al. (2021) focused on sentiment analysis in Greek social media while Krasadakis et al. (2022) surveyed NLP studies related to legislative and Greek documents. These studies, however, have not assessed the employed datasets regarding their licensing, public availability, and (machine) actionability, which hinders the impact of existing benchmarks. We provide an extensive benchmark on publicly available and actionable Greek NLP data, hence filling this gap.\nOverviews that focus on Greek NLP, such as the work of Papantoniou and Tzitzikas (2020, 2024), cover a very wide period of study (e.g., comprising Ancient Greek), and lack classification for licensing and actionability of the resources. The systematic literature review of Bakagianni et al. (2024) followed that path and provided an exhaustive list of publicly available and actionable datasets. Our work was based on that study, providing code to compile a unified collection of appropriate datasets, to ease usability and re-production of the results.\nData leakage and contamination is a challenge for the NLP community (Balloccu et al., 2024), which recent benchmarks employing LLMs attempt to avoid (White et al., 2024). Benchmarks can assist towards that path, by providing specific evaluation sets and documentation. Although benchmarks exist for Greek dialects (Faisal et al., 2024) and Ancient Greek (Stopponi et al., 2024), this is the first benchmark across Greek NLP tasks using publicly available and actionable data."}, {"title": "6 Conclusions", "content": "Based on a monolingual survey, we compiled a collection of publicly available and accessible Greek datasets, based on their licensing schemas. We used this collection to benchmark an open- (LLAMA) v. a closed-source (GPT) LLM on seven core NLP tasks, showing that the former best performs in NER and Summarisation while the latter best-performs in POS tagging, Intent Classification and GEC. We further observe and address two weaknesses in Greek NLP. First, the available data for authorship attribution are of limited availability. We tackled this by introducing an alternative dataset and by using 0-shot learning. High accuracy in this task could indicate usage of the respective material during pre-training. Second, there is no text"}, {"title": "7 Limitations", "content": "We compiled a collection of existing datasets, providing the code to re-compile our collection. Dataset developers, however, may update their datasets, which may yield slightly altered results in one or more tasks. To reflect any such changes, we will regularly re-compile the collection and re-run our benchmark, reporting the results over time in our repository online. A limitation of our work lies in the sustainability of our code, to incorporate new repositories and adjust to changes in the current one. This is addressed by unit tests set to download periodically the data and evaluate the functionality. We also note that our method is applicable to other languages, if a systematic literature review for that language exists."}, {"title": "8 Ethical considerations", "content": "Greek NLP datasets present a range of legal and licensing challenges primarily related to copyright, permissions, and data protection, crucial for compliant and ethical research use.\nCopyright The dataset of Papantoniou et al. (2023), for example, relies on Greek Wikipedia, which may include third-party contributions under different licensing terms. This could lead to potential copyright issues if the dataset includes trademarked names or logos, not covered under the CC license and requiring separate permissions. Barzokas et al. (2020) created a dataset using content from Project Gutenberg and Open Library under an MIT license, but content of the former may not be free of copyright in other countries. Also, Open Library restricts its content use to non-commercial purposes and for research only, which may conflict with the broad permissions of the MIT license if the content is used beyond these limits.\nRe-identification The judicial rulings of Koniaris et al. (2023) may include copyrighted an-notations by court staff, posing a risk of re-identification. Similarly, in the student essays of Rizou et al. (2023) there could be potential re-identification risks of student data (even if they are fully anonymised) while in the handwritten essays from high school students of Korre et al. (2021) there may be concerns about obtaining proper consent from students or their guardians for dataset inclusion. On the other hand, the data of Bartziokas et al. (2020) may include named entities tied to individuals and there is potential for privacy violations.\nConflicting terms Derived from the Permanent Greek Legislation Code - Raptarchis, the dataset of Papaloukas et al. (2021) could encounter legal concerns due to the Ministry of Digital Governance retaining all intellectual property rights. The dataset of Dritsa et al. (2022) sourced from the Hellenic Parliament, conflicts with its public terms, which require proper attribution without alterations (e.g., 3rd party content). Zampieri et al. (2020) applied a CC-BY-4.0 license to tweets, presenting legal challenges due to Twitter's restrictive policies on content redistribution. Also, regarding the data of Prokopidis and Piperidis (2020), while derived from web content labeled as \u201copen\u201d, the terms of the original websites must explicitly allow crawling and reuse to avoid potential copyright infringement."}, {"title": "4.1 Long Legal Text Clustering", "content": "The dataset Papaloukas et al. (2021) developed a dataset for multi-class legal topic classification, derived from a collection of Greek legislative documents titled \"Permanent Greek Legislation Code Raptarchis\". The dataset includes classifications that range from broader categories to more specialized ones. There are annotations at the volume, the chapter and the subject levels. This is a heavily imbalanced dataset, and the text lengths vary significantly, from 17 to approx. 1 million characters. The high number of classes in this task make in-context learning approaches impractical while our benchmark already covers text classification as a task. Therefore, we use this dataset to benchmark LLMs for text clustering, a task of unsupervised learning that is currently missing for Greek data in literature. We employ the entire test set of 38,052 texts for our experiments, comprising 47 volumes, 374 chapters, and 1,685 subjects.\nThe results The direct use of LLM models for this task is prohibitive due to the substantial computational cost and processing time required for handling the long legal texts (Table 3). Therefore, we opted for K-Means with TF-IDF features. Additionally, we used LLAMA for summarization (the best-performing model for this task; see \u00a73.4) and for translation into English (performing on par with GPT; see \u00a73.3). This allowed us to compute Instructor embeddings (Su et al., 2022), which we refer to as STE (Summarized, Translated, Embedded), as an alternative to TF-IDF for text representation. We set the number of clusters, k, according to the ground truth number of topics, i.e., 47 for the volume, 374 for the chapter, and 1,685 for the subject. We evaluated the results based on the normalised mutual information score (NMI), the adjusted mutual information score (AMI), and the adjusted rand index (ARI). All three measures use the ground truth labels to assess the clustering solution while being independent of the absolute label values. As shown in Table 11, STE (2nd row per level) gives the best results overall, and the best ones across metrics consistently in two levels. When ablating the embedding step, computing the TF-IDF features of the English summaries (3rd row per level), we see that the results deteriorate. This means that information is lost during translation and summarization. The superiority of the Instructor embeddings, however, make up for this drop."}, {"title": "4.2 Authorship Attribution", "content": "The dataset Barzokas et al. (2020) collected 1,734 open-access e-books from Project Gutenberg,"}]}