{"title": "Computing Approximate Graph Edit Distance via Optimal Transport", "authors": ["Qihao Cheng", "Da Yan", "Tianhao Wu", "Zhongyi Huang", "Qin Zhang"], "abstract": "Given a graph pair (G1, G2), graph edit distance (GED) is defined as the minimum number of edit operations converting G\u00b9 to G2. GED is a fundamental operation widely used in many applications, but its exact computation is NP-hard, so the approximation of GED has gained a lot of attention. Data-driven learning-based methods have been found to provide superior results compared to classical approximate algorithms, but they directly fit the coupling relationship between a pair of vertices from their vertex features. We argue that while pairwise vertex features can capture the coupling cost (discrepancy) of a pair of vertices, the vertex coupling matrix should be derived from the vertex-pair cost matrix through a more well-established method that is aware of the global context of the graph pair, such as optimal transport. In this paper, we propose an ensemble approach that integrates a supervised learning-based method and an unsupervised method, both based on optimal transport. Our learning method, GEDIOT, is based on inverse optimal transport that leverages a learnable Sinkhorn algorithm to generate the coupling matrix. Our unsupervised method, GEDGW, models GED computation as a linear combination of optimal transport and its variant, Gromov-Wasserstein discrepancy, for node and edge operations, respectively, which can be solved efficiently without needing the ground truth. Our ensemble method, GEDHOT, combines GEDIOT and GEDGW to further boost the performance. Extensive experiments demonstrate that our methods significantly outperform the existing methods in terms of the performance of GED computation, edit path generation, and model generalizability.", "sections": [{"title": "1 INTRODUCTION", "content": "Graph edit distance (GED) is one of the most widely used graph similarity metrics, which is defined as the minimum number of edit operations that transform one graph to the other. GED has wide ap- plications, such as graph similarity search [26, 55, 64, 69, 72], graph classification [38, 39], and inexact graph matching [6]. Scenarios in- clude handwriting recognition [16], image indexing [57], semantic image matching [54], and investigations of antiviral drugs [70], etc. The lower part of Figure 1 illustrates the edit path (i.e., sequence of edit operations) of a graph pair (G1, G2) with GED = 4. The edit path with the minimum length is called Graph Edit Path (GEP), so the length of a GEP is exactly the GED.\nExisting methods for GED computation can be categorized into the following three types. (1) Exact Algorithms. GED can be computed exactly by the A* algorithm [40], but due to being NP- hard [64], it is time-consuming even for a pair of 6-node graphs [3]. (2) Approximate Algorithms. To make computation tractable, ap- proximate algorithms are proposed based on discrete optimization or combinatorial search such as A*-Beam [31], Hungarian [39] and VJ [15]. A*-beam restricts the search space of A* algorithm, which is still an exponential-time algorithm. Hungarian and VJ convert the GED computation to a linear sum assignment problem and find the optimal node matching between two graphs, which takes O(n\u00b3) time. Moreover, these heuristic methods lack a theoretical guarantee and generate results of inferior quality. (3) Learning-based Meth- ods. Recent studies turn to data-driven methods based on graph neu- ral networks (GNN) to achieve better performance [1, 2, 35, 37, 62]. Differing from the approximate algorithms, learning-based methods extract intra-graph and inter-graph information by generating node and graph embeddings, which are then used to predict GEDs with smaller errors within O(n\u00b2) time in the worst case. The two most recent works, Noah [62] and GEDGNN [35], further support gener- ating the edit path based on A*-beam search and k-best matching, respectively, to ensure the feasibility of the predicted GED."}, {"title": "2 RELATED WORK", "content": "GED Computation. Classical exact algorithms [3, 7] seek the ex- act graph edit distance for each graph pair. Due to the NP-hardness of GED computation, they fail to generate solutions in a limited time when the graph size increases. To make computation tractable, plenty of heuristic algorithms are proposed, including A*-Beam [31], Hungarian [39] and VJ [15], all of which provide an approximate GED in polynomial time. Recently, graph neural networks (GNN)"}, {"title": "3 PRELIMINARIES", "content": "This section introduces Graph Edit Distance (GED), Graph Edit Path (GEP), and the fundamental concepts of Optimal Transport (OT) on graphs. All vectors default to column vectors unless otherwise specified. Table 1 summarizes important notations for quick lookup.\n3.1 Problem Statement\nWe consider two tasks: GED computation and GEP generation between two node-labeled undirected graphs G1 = (V1, E1, L1) and G2 = (V2, E2, L2). We discuss GED computation of edge-labeled graphs in Appendix H.1. We denote |V1| = n1,|E1| = m1 and |V2| = n2, |E2| = m2. We assume that n\u2081 \u2264 n2 as otherwise, we can swap G\u00b9 and G2.\nGraph Edit Distance (GED). Given the graph pair (G1, G2), graph edit distance GED(G1, G2) is the minimum number of edit op- erations that transform G\u00b9 to G2. An edit operation is an inser- tion/deletion of a node/edge or the relabeling of a node.\nGraph Edit Path (GEP). The edit path of the graph pair (G1, G2) is a sequence of edit operations that transform G\u00b9 to G\u00b2. The graph edit path GEP(G1, G2) is the shortest one with length GED(G1, G2)."}, {"title": "3.2 Background of Optimal Transport", "content": "Optimal Transport (OT). The optimal transport problem seeks the most efficient way of transporting one distribution of mass into another. Given a graph pair (G1, G2), where G1 = (V1, E1, L1) and G2 = (V2, E2, L2), we assume there are two pre-defined mass distributions \u03bc = {\u03bci}i=1 and \u03bd = {\u03bdj}j=1 on nodes of G\u00b9 and G2, respectively. For instance, when n\u2081 = n2, for all ui in G\u00b9 and vj in G2, we can set their masses as \u03bci = 1 and vj = 1, which puts the same importance weight on every node. Figure 3 shows our mass distributions on G\u00b9 and G\u00b2 where every node has mass 1. Coupling matrix \u03c0\u2208 Rn1\u00d7n2 is a node-to-node mass transport matrix between G\u00b9 and G\u00b2, where each element \u03c0i,k denotes the amount of mass transported from node ui \u2208 V1 to uk \u2208 V2. In our work, \u03c0i,k is in the range [0, 1] reflecting the confidence that ui matches uk. The feasible set of coupling matrices of (G1, G2) is denoted by:\n\u03a0(\u03bc, \u03bd) = {\u03c0\u2208 Rn1\u00d7n2 |\u03c01n2 = \u03bc, \u03c0\u0f0b1\u03b7\u2081 = \u03bd, \u03c0 \u2265 0} .\nThe lower-left corner of Figure 3 shows an example of a feasible \u03c0. The feasible set of n basically relaxes Eq. (1) to allow values in [0, 1], but still requires that elements in a row (resp. column) sum up to 1 (if n\u2081 \u2260 n2, dummy nodes need to be added as we will describe in Section 4.2. So we are basically generalizing Eq. (1) for the case when G\u00b9 and G\u00b2 have the same size, where \u03c01n2 = 1n2).\nWith a given inter-graph node-to-node cost matrix C\u2208 Rn1\u00d7n2, where Ci,j denotes the cost of transporting a unit of mass from ui \u2208 V1 to vj \u2208 V2, OT finds the optimal coupling matrix \u03c0 between G\u00b9 and G2 as follows:\n$\\min_{\\pi\\in\\Pi(\\mu,\\nu)} \\langle C, \\pi \\rangle ,$$\nwhere $\\langle C, \\pi \\rangle = \\sum_i \\sum_j C_{i,j} \\pi_{i,j}$ is the Frobenius dot-product of C and n, and the optimal value is the so-called Wasserstein Distance or Earth Mover's Distance. The lower center of Figure 3 shows a simple hand-crafted cost matrix C between graphs G\u00b9 and G2, defined as follows. Initially, we assume matrix C is all-zero. If the labels of ui in G\u00b9 and vj in G\u00b2 are different, we increase Ci,j by 1. Let di and dj be the degrees of ui and vj. We further increase Ci,j by |di - dj|/2, since the difference between degrees is associated with the number of edge insertions/deletions. The constant 1/2 is used to avoid double-counting of an edge on its two endpoints. Solving OT over this cost matrix gives the coupling matrix shown in the lower-right corner of Figure 3, which indicates that u2 is mapped to v\u2082, but u\u2081 can be mapped to either 01 or 23 with 50% probability each. This directly corresponds to the two optimal node matchings illustrated in Figure 3, which give a GED value of 2. A simple yet efficient method to solve Eq. (2) is by introducing an entropy regularization term into the optimization objective [56]:\n$\\min_{\\pi \\in \\Pi(\\mu,\\nu)} \\langle C, \\pi \\rangle + \\varepsilon H(\\pi),$$\nwhere H(\u03c0) = \u03a3i \u2211j \u03c0i,j (log \u03c0i,j \u2212 1) = \u27e8n, log(n) \u2212 1\u27e9 is the entropy function and \u025b > 0 is the regularization coefficient. Lever- aging the duality theory [4] and strict convexity of Eq. (3), the unique solution can be solved by the Sinkhorn algorithm as shown in Algorithm 1 [13], which alternately updates the dual variables y and to fit the specified mass distribution u and v. For more details, please see Appendix B.1.\nGromov-Wasserstein Discrepancy (GW). In practice, it is chal- lenging to define a reasonable node-to-node cost matrix C\u2208 Rn1\u00d7n2 without specified node embeddings for the two graphs G\u00b9 and G2. To address this issue, Gromov-Wasserstein discrepancy (GW) [29,"}, {"title": "4 LEARNING-BASED METHOD: GEDIOT", "content": "In this section, we introduce GEDIOT, our neural network for GED computation based on inverse optimal transport. The training is an inverse process of OT to find (i.e., fit) the cost matrix given the ground-truth node coupling matrix of (G1, G2), \u03c0*, as supervision.\nMotivation of introducing OT. Recall that a node matching sat- isfies the constraints in Eq. (1). Let us denote its feasible set by\nU (1n1, 1n2) = {\u03c0 \u2265 0|\u03c01n2 = 1n\u2081, \u03c0\u0f0b1n1 \u2264 1n2, 1\u03b7, \u03c01n2 = n1} .\nPrevious learning-based models predict GED and node matching via the interaction information of node/graph embeddings [1, 2, 35, 62], but they directly fit the predicted node matching with the ground-truth node coupling using binary cross-entropy loss, without considering all the constraints in U(1n1, 1n2) during the training process.\nWe propose a novel neural architecture, GEDIOT, for GED com- putation and GEP generation, which predicts only the node-to-node cost matrix C from the interaction information of node/graph em- beddings, and relies on OT to obtain the node matching from C so that all the constraints in U(1n\u2081, 1n2) are taken into consideration.\nThe training process is constructed as a bi-level optimization as formulated in Eq. (7), where the inner minimization computes the coupling matrix n satisfying the constraints in U (1n1, 1n2) by solv- ing an entropy-regularized OT problem that can be evaluated with our learnable Sinkhorn module, and the outer minimization calcu- lates the difference between the coupling matrix and the ground truth to update the cost matrix \u0108 via backpropagation.\n$\\min_{\\hat{C}} L_m (\\pi^*,\\hat{\\pi}) + L_v (GED^*, GED),$$\nwhere\n$\\hat{\\pi} = \\underset{\\pi \\in U (1_{n_1},1_{n_2})}{\\text{argmin}} \\langle \\hat{C}, \\pi \\rangle + \\varepsilon H(\\pi),$$\n$GED = \\langle \\hat{C}, \\hat{\\pi} \\rangle .$\nHere, \u03c0* and GED* are the ground truth coupling matrix and GED of graph pair (G1, G2), respectively. Note that \u03c0* \u2208 {0,1}n1\u00d7n2 is a one-to-one mapping and there are (n2 - n\u2081) full-zero columns. During test, computing GED is simply to solve the (inner) entropy- regularized OT problem, which is thus effective and interpretable.\nIn Eq. (7), \u0108 \u2208 Rn1\u00d7n2 is a learnable cost matrix that encodes the cost of matching each vertex pair across G\u00b9 and G2, and \u2208 Rn1\u00d7n2 denotes the coupling matrix induced from \u0108 by minimizing the inner optimization problem. Recall from Section 3.1 that when re- laxing the binary constraints of \u03c0\u2208 {0,1}n1\u00d7n2 to \u03c0\u2208 [0, 1]n1xn2, Eq. (1) basically defines \u03c0i,j to be the probability mass transported from ui \u2208 V1 to vj \u2208 V2, and the row \u03c0\u012f defines the distribution of transported probability mass from u\u00a1 to vertices of V\u00b2. In Eq. (7), the GED value is approximated with \u27e8 , \u03c0\u27e9 = \u22111 \u0108i,j\u03c0i,j since \u0108i,j\u03c0i,j is the expected cost to transport mass from u\u012f, so \u27e8 , \u03c0\u27e9 is the expected cost to transport all mass from G\u00b9.\nIn the special case when \u03c0\u2208 {0,1}n1\u00d712, \u27e8 , \u03c0\u27e9 basically adds up the costs of transporting mass from u\u00a1 to its matched target in V2 for all ui \u2208 V\u00b9; and the first term in the outer minimization encourages a sparse since the ground-truth \u03c0* is sparse.\nThe objective of the outer optimization contains two terms de- signed for our two tasks: GED computation and GEP generation. Specifically, Lm is the matching loss for GEP generation, which we use Binary Cross-Entropy (BCE) loss between the ground truth \u03c0* and the learned coupling matrix that is then fed into the k-best matching framework [10, 35] as described in Section 4.5. Lo is the value loss for GED computation, which we adopt Mean-Squared Error (MSE) between the ground truth GED and the learned one obtained from both node and graph embeddings. The inner entropy- regularized OT of Eq. (7) provides an optimal coupling matrix with the current cost matrix C. Then, the outer minimization fits the learned coupling matrix and GED to the ground truths to optimize the neural parameters in the model. Notably, we will formulate C further using the node features extracted from (G\u00b9, G\u00b2) by GNN (see Figure 2), so \u201cmin\u201d in the outer optimization actually optimizes on the parameters of feature extraction network. More analysis of the process of Eq. (7) can be found in Appendix B.2, where we delve into the gap between the learned and ground truth."}, {"title": "4.1 Node Embedding Component", "content": "In this component, a GNN and an MLP are employed to capture the graph topology information and generate the final node embedding.\nGNN Module. We adopt a siamese GNN to generate node em- beddings by graph convolution operations, following previous graph similarity learning models [28, 35, 37]. Given the graph pair (G1, G2), nodes in both G\u00b9 and G2 are embedded with the shared network through node feature propagation and aggregation.\nConcretely, Graph Isomorphism Network (GIN) [61] is adopted to capture the graph topology, since GIN has been shown to be as powerful as the Weisfeiler-Lehman (WL) graph isomorphism test in differentiating different graph structures [42]. For a graph G = (V, E, L), we initialize the node embedding h(0) (u) for u \u2208 V as the one-hot encoding of its label. If graphs are unlabeled, we set each initial node embedding as a constant number following previous works [2, 35]. In the ith layer, the embedding of node u, denoted by h(i) (u), is updated from itself and its neighbors as\nh(i) (u) = MLP ((1+\u03b5(i)) \u00b7 h(i\u22121) (u) + \u2211 h(i\u22121) (v)),\nv\u2208N(u)\nwhere 8(i) is a learnable parameter of each layer and N(u) is the set of neighbors of u.\nMLP Module. As the features propagate via GIN, higher-order graph structural information is fused into node embeddings, which may cause over-smoothed node embeddings at the last layer. Note that various GIN layers contain different orders of topological infor- mation: h(0) (u) represents the features of u itself whereas h(i) (u) contains the feature information from its ith-hop neighbors. To obtain sufficiently rich node embeddings for more accurate GED computation, we concatenate the node embeddings from all GIN lay- ers: h = [h(0) || (1) || ... ||h(k)]. The concatenated embedding h is then fed to an MLP to produce the final node embedding H \u2208 Rn\u00d7d:\nH = MLP (h) = MLP ([h(0) || (1) | \u06f0\u06f0\u06f0 |h(k)]) .\nSuppose that the size of input h is n \u00d7 D, then we use an MLP with three dense layers of D \u00d7 2D, 2D \u00d7 D and D \u00d7 d, respectively, to reduce the input h to the final node embeddings H\u2208 Rn\u00d7d."}, {"title": "4.2 Learnable OT Component", "content": "This component includes a cost matrix layer to extract the cost matrix from node embeddings H\u00b9 and H\u00b2 extracted by the node embedding component introduced in Section 4.1, and a learnable Sinkhorn layer to implement the inner entropy-regularized OT of Eq. (7) to generate the node matching from the cost matrix.\nCost Matrix Layer. This layer measures the node-to-node cost matrix \u0108 \u2208 Rn1\u00d7n2 for the graph pair (G1, G2), by multiplying the"}, {"title": "4.3 Graph Discrepancy Component", "content": "Recall that before the learnable Sinkhorn layer, we add a dummy supernode to G\u00b9; when the layer completes and outputs \u03c0, we remove the last row that corresponds to the dummy supernode. The learnable OT component captures only the edit operations induced by the node matching (from the node-to-node level), and some edit operations are not accounted for since n\u2081 \u2264 n2. We thus adopt another graph discrepancy component to supplement the unencoded information from the embedding of unmatched (n2 - n1) nodes in G\u00b2 from the graph-to-graph level. It includes a graph embedding layer to learn the embeddings of G\u00b9 and G2, and a neural tensor network (NTN) [2] that reads out the graph discrepancy information from the graph embeddings to enhance GED prediction.\nSpecifically, we first generate the graph-level embeddings with the node attentive mechanism [2]. Given a graph G (can be either G\u00b9 or G2) with node embedding matrix H \u2208 Rn\u00d7d (can be either H\u00b9 or H\u00b2) extracted by our node embedding component, we first calculate the global graph context vector\n$\\h_c = \\tanh (W_H \\sum_{i=1}^n H_i ),$$\nwhich averages node features for all nodes of G followed by a non- linear transformation, where W\u2081 \u2208 Rdxd is a learnable weight matrix and Hi is the ith row of H \u2208 Rn\u00d7d. Then, the attention weight of each node vi is computed as the inner product between"}, {"title": "4.4 Model Training", "content": "GEDIOT is supervised by the ground-truth GED* (G1, G2) and the corresponding coupling matrix \u03c0* for node matching between two graphs G\u00b9 and G\u00b2 during the training process. As shown in Eq. (7), the loss function consists of two parts: a value loss Lo to predict the GED and a matching loss Lm to predict the coupling matrix. The final loss function of GEDIOT is defined as\nL = \u03bb Lo + (1 \u2013 \u03bb) Lm.\nwhere we use a hyperparameter \u03bb to balance L and Lm.\nSince the range of GED(G1, G2) is too large to train a neural network effectively, we normalize the ground-truth GED to the range [0, 1], and the normalized ground-truth GED is given by:\nnGED* (G1, G2) =\nGED* (G1, G2)\nmax(n1, n2) + max(m1, m2)\nwhere the denominator on the right is the maximum number of edit operations that modify all nodes and edges to transform G\u00b9 to G2. To predict this normalized GED, we define the function:\nscore(G1, G2) = \u03c3(\u03c91 + \u03c92),\nwhere w1 = \u27e8 , \u03c0\u27e9 is the predicted score from the learnable OT component, and w2 is the predicted score from NTN [2]. Here, \u03c3 is the sigmoid function to ensure that the prediction is within (0, 1). We use MSE as the loss function for value:\nLu = (score(G\u00b9, G\u00b2) \u2013 nGED* (G\u00b9, G\u00b2))\u00b2,\nand we fit the predicted coupling matrix with the ground-truth 0-1 matrix \u03c0*, by minimizing the binary cross-entropy loss (BCE) between the learned coupling matrix and ground truth \u03c0*:\nLm = \u2212 1 \u2211 \u2211 (\u03c0i log \u03c0i,j + (1 \u2212 \u03c0\u2217 ) log (1 \u2013 \u03c0i,j))\nn1 n2\ni=1 j=1\n= \u2212 1 BCE (\u03c0*|\u03c0),\nn1n2\nwhere\nBCE (\u03c0* |\u03c0) = \u27e8\u03c0*, log(\u03c0)\u27e9 + \u27e81 \u2212 \u03c0*, log(1 \u2212 \u03c0)\u27e9 ."}, {"title": "4.5 GEP Generation", "content": "Although we fit to the ground-truth node matching \u03c0* \u2208 {0,1}n1\u00d7n2, in practice when the model is trained, the learned coupling ma- trix n outputted by GEDIOT is not perfect but in the range \u03c0* \u0395 [0, 1]n1\u00d7n2 representing the confidence of node-to-node matching.\nDuring inference, we adopt the k-best matching framework of [35] to generate GEP(G1, G2) from the learned coupling ma- trix \u03c0, which utilizes the solution space splitting method [10] to obtain a candidate set of k-best bipartite node matchings (based on the matching cost specified by the learned coupling matrix ) and searches for the one with the shortest edit path as GEP(G1, G2). Specifically, let S be a set of node matchings (Figure 6 shows two graphs G\u00b9 and G\u00b2 each having 3 nodes and all 6 possible node matchings in S), in which we can find the best and second-best node matchings according to the matching cost from \u00f1, denoted by M(1,1) and M(1,1), respectively, in O(n\u00b3) time [10]. The first (resp. second) \"1\" in the superscript (1, 1) means that the two matchings are in the first partition (resp. obtained in the first iteration). Let (u, v) be a node pair in M(1,1) but not in M(1,1) where u \u2208 V1 and v \u2208 V2. We can split S into two subspaces S\u2081 and S2, such that a node matching of S is in S\u2081 if it contains (u, v), and otherwise it is in S2. As shown in the upper part of Figure 6, u\u2081 matches v\u2081 in the best matching in S, but u\u2081 does not match 2\u2081 in the second-best matching in S. Then, we split S into S\u2081 and S2 according to whether u\u2081 matches v1 in the first iteration. Note that M (1,1) (resp. M(1,1)) becomes the best node matching in S1 (resp. S2) after splitting, which we denote as M(1,2) (resp. M(2,2)). We also search the new second-best node matchings in S\u2081 and S2, denoted by M(1,2) and M(2,2), respectively. The entire node matching space is partitioned by repeatedly selecting a partition to split in this manner. Assuming that there are t partitions and each has its best and second-best node matching Mr,t) 1(r,t) and Mr,t), where r = 1, 2, .., t, the (t + 1)th best node matching is Matt) of the partition t* with the best 'second-best' node matching, so partition t* is selected for split- ting. Consider the lower part of Figure 6, where we assume the second-best matching M(2,2) in S2 is better than the second-best"}, {"title": "5 UNSUPERVISED METHOD: GEDGW", "content": "Currently, learning-based methods [1, 2, 35, 62] show the best per- formance of approximate GED computation, but they need ground truth for training set. This section presents our unsupervised opti- mization approach, GEDGW, that is able to achieve performance comparable to learning-based methods. GEDGW is based on the Gromov-Wasserstein discrepancy, which bridges GED computation and optimal transport from an optimization perspective.\n5.1 Formulation of GEDGW\nRecall that the total edit operations that transform G\u00b9 to G2 can be determined with a given node matching between G\u00b9 and G2, where GED is the smallest one. Consequently, the GED computation of the graph pair (G1, G2) can be formulated as an optimization problem related to node matching.\nSince there can be (n2 \u2013 n\u2081) nodes in G\u00b2 that do not match any nodes in G\u00b9, we add (n2 \u2013 n\u2081) dummy nodes in G\u00b9 without any labels and edges following previous works [20, 40], as Figure 7 illustrates. This ensures that the two graphs have the same number of nodes without affecting the GED computation. For simplicity, we abuse the notations to still denote the graph after adding dummy nodes by G\u00b9 and let n = n2 = max{n1, n2} in this section.\nGiven a node matching, we can derive its induced edit operations into those on nodes and edges. Accordingly, GED computation can be derived by solving the following quadratic programming problem where the first (resp. second) term in the objective models the cost of node (resp. edge) edit operations. Appendix B.3 provides a detailed illustration of the GEDGW formulation.\n$\\min_\\pi \\sum_{i,k} M_{i,k} \\pi_{i,k} + \\frac{1}{4} \\sum_{i,j,k,l} (A_{i,j}^1 - A_{k,l}^2)^2 \\pi_{i,k} \\pi_{j,l},$$\ns.t. \u03c01n = 1n, \u03c01\u03b7 = 1\u03b7, \u03c0\u2208 {0,1}n\u00d7n.\nHere, M\u2208 {0,1}n\u00d7n is the node label matching matrix between nodes of G\u00b9 and G\u00b2, where Mi,k = 1 if nodes ui \u2208 V\u00b9 and vk \u2208 V2 have the same label; otherwise Mi,k = 0. Matrices A\u00b9 \u2208 {0, 1}n\u00d7n and A\u00b2 \u2208 {0,1}n\u00d7n are the adjacency matrices of G\u00b9 and G2, re- spectively. The factor in the second term is to avoid the double counting of \u03c0i,k\u03c0j,l and \u03c0j,l\u03c0i,k since the graphs are undirected."}, {"title": "5.2 Further Improvement by Ensembling", "content": "Recall that GEDGW and GEDIOT model the GED computation from two different perspectives via optimal transport. To achieve better performance, we combine these two OT-based methods into an ensemble GEDHOT (GED with Hybrid Optimal Transport), which combines the results from GEDGW and GEDIOT to enhance the performance of GED computation and GEP generation during test. Specifically, given an input of graph pair (G1, G2), we run GEDGW and GEDIOT to get the GEDs and coupling matrices denoted by GEDGW(G1, G2) and GW, GEDIOT(G1, G2) and IOT, respectively. Since GED is the minimum number of edit operations, we choose the smaller of GEDGW (G1, G2) and GEDIOT (G1, G2) as GED(G1, G2).\nGED(G\u00b9, G\u00b2) = min {GEDGW(G\u00b9, G\u00b2), GEDIOT (G1, G2)} .\nFor GEP generation, we generate the best edit paths via the k-best matching framework [35] from Gw and IOT, respectively, and then choose the shorter one."}, {"title": "5.3 Time Complexity Analysis", "content": "Due to space limitation, we provide a comprehensive analysis of the time complexity of our proposed methods in Appendix E.\nIn a nutshell, for GEDIOT, since the model training can be done offline given a graph dataset, we consider the computation cost of its forward propagation, the time complexity of which is given by\nO (N(md + nd\u00b2) + - nd\u00b2 + nN\u00b2d\u00b2) + Ld\u00b2 + nd\u00b2 + n\u00b2d d + Mn\u00b2) \u2248 O(n\u00b2),"}, {"title": "6 EXPERIMENT", "content": "This section evaluates the performance of our proposed methods and compares with existing approximate GED computing methods. Our code is released at https://github.com/chengqihao/GED-via- Optimal-Transport.\n6.1 Datasets\nWe use three real-world graph datasets: AIDS, Linux, and IMDB. Table 2 summarizes their statistics including the number of graphs (|D|), the average number of nodes (|V|avg) and edges (|E|avg), the maximum number of nodes (|V|max) and edges (|E|max), and the number of labels (|L|). For graph pairs with no more than 10 nodes, we use the A* algorithm [40] to generate the exact ground truth, and for the remaining graphs with more than 10 nodes, we use the ground-truth generation technique in [1, 35] to generate 100 synthetic graphs for each graph. For each dataset, we sample 60% graphs and pair every two of them to create graph pairs of the training set. As for the test set, we sample 20% graphs; for each selected graph, 100 graphs are randomly chosen from the training graphs to generate 100 graph pairs for the test set. The validation set is formed in the same manner as the test set. Appendix F.1 describes the datasets, data preprocessing, and dataset partitions in detail."}, {"title": "6.2 Compared Methods", "content": "Recall that GEDGW is a non-learning approximation algorithm, GEDIOT is a learning-based method, and GEDHOT is a combina- tion of both. We compare them with the classical approximation algorithms and learning-based methods.\nClassical Algorithms. We select three representative classical approximate algorithms for GED computation. (1) Hungarian [39] is based on the Hungarian method for weighted graph matching which takes cubic time. (2) VJ [15] is based on bipartite graph matching which takes cubic time. (3) Classic runs both Hungarian and VJ to find the GEPs, and takes the better GEP. We do not include"}, {"title": "6.3 Evaluation metrics", "content": "We consider four kinds of metrics to evaluate the performance, which have been widely used [1, 2, 35, 62].\nMetrics for GED Computation. (1) Mean Absolute Error (MAE) measures the average absolute error between ground-truth GEDs and approximate GEDs. For a graph pair (G1, G2), it is formulated as |GED* (G1, G2)-GED(G1, G2)|. (2) Accuracy measures the ratio of approximate GEDs that equal the ground-truth GEDs after rounding to the nearest integer. (3) Feasibility measures the ratio that the approximate GEDs are no less than the ground-truth GEDs, so that a GEP of this length is feasible (i.e., can be found).\nMetrics for Ranking. These metrics measure the matching ra- tio between the ranking results of the approximate GED and the ground truth. They include (4) Spearman's Rank Correlation Coefficient (p). (5) Kendall's Rank Correlation Coefficient (\u03c4). (6) Precision at k (p@k). The first two metrics focus on global ranks while the last focuses on top k. We use p@10 and p@20.\nMetrics for Path. These metrics measure how well the gener- ated edit path GEP matches the ground-truth GEP*. They include (7) Recall = |GEP\u2229GEP*|, (8) Precision = |GEP\u2229GEP*|, and (9) F1-GEP* score defined as F1 = 2 \u00b7 Recall\u00b7Precision .Recall+Precision\nMetrics for Efficiency. (10) Running Time (sec/100p), where p = \u201cpairs\u201d. It records the time for every 100 graph pairs during test."}, {"title": "6.4 Experimental Results", "content": "We evaluate the performance of various methods for both GED computation and GEP generation.\nPerformance of GED Computation. We first compare our pro- posed methods (i.e., GEDGW, GEDIOT, and GEDHOT) with the six baselines mentioned in Section 6.2 (Hungarian and VJ are dom- inated by Classic and are hence omitted due to space limit). We categorize the methods into three types: learning-based methods, non-learning methods, and hybrid methods. We count Noah also as a hybrid method since it combines GPN with A*-Beam.\nTable 3 reports the results. We can see that among the learning- based methods, GEDGNN achieves the best performance on all three datasets for value, ranking, and feasibility metrics. Meanwhile, GEDIOT significantly outperforms GEDGNN (as well as the other learning-based baselines) in terms of value and ranking metrics"}, {"title": "6.5 Generalizability", "content": "Since all the learning-based methods require training data super- vision, it is interesting to explore how they generalize beyond the training data distribution, including our GEDIOT model.\nModeling GED Computation of Unseen Graphs. Recall that we prepared the test set by sampling 100 training graphs for each test graph, which models the graph similarity search task. To evaluate the generalizability, now we instead sample 100 test graphs (rather than training graphs) for each test graph, so that both graphs in a graph pair of the test set are unseen during training.\nTable 5 shows the results of the five learning-based methods, where GEDIOT still significantly outperforms GEDGNN and the others in terms of value and ranking metrics. For example, on Linux, the MAE of GEDIOT is 2.4\u00d7 smaller than GEDGNN, and the accuracy reaches 96"}]}