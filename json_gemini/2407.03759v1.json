{"title": "Convolutional vs Large Language Models for Software Log Classification in Edge-Deployable Cellular Network Testing", "authors": ["Achintha Ihalage", "Sayed M. Taheri", "Faris Muhammad", "Hamed Al-Raweshidy"], "abstract": "Software logs generated by sophisticated network emulators in the telecommunications industry, such as VIAVI TM500, are extremely complex, often comprising tens of thousands of text lines with minimal resemblance to natural language. Only specialised expert engineers can decipher such logs and troubleshoot defects in test runs. While AI offers a promising solution for automating defect triage, potentially leading to massive revenue savings for companies, state-of-the-art large language models (LLMs) suffer from significant drawbacks in this specialised domain. These include a constrained context window, limited applicability to text beyond natural language, and high inference costs. To address these limitations, we propose a compact convolutional neural network (CNN) architecture that offers a context window spanning up to 200,000 characters and achieves over 96% accuracy (F1>0.9) in classifying multifaceted software logs into various layers in the telecommunications protocol stack. Specifically, the proposed model is capable of identifying defects in test runs and triaging them to the relevant department, formerly a manual engineering process that required expert knowledge. We evaluate several LLMs; LLaMA2-7B, Mixtral_8x7B, Flan-T5, BERT and BigBird, and experimentally demonstrate their shortcomings in our specialized application. Despite being lightweight, our CNN significantly outperforms LLM-based approaches in telecommunications log classification while minimizing the cost of production. Our defect triaging AI model is deployable on edge devices without dedicated hardware and widely applicable across software logs in various industries.\nImpact Statement-Complex raw logs generated by software and hardware stacks prevalent in the telecommunications industry pose unique challenges in understanding and diagnosing relevant issues. Due to the nature of such logs, it becomes crucial to analyze their entire content to identify defects based on network configurations, dynamic parameters, and combinations of log messages. Existing machine learning-based methods that support only narrow input sequences of text prove inadequate in this situation. This article introduces a robust convolutional neural network (CNN) architecture that supports input text sequences up to 200,000 characters, enabling effective learning from voluminous software logs. An experimental study is conducted to illustrate the effectiveness of tailored machine learning architectures in specialized domains by benchmarking the proposed model against recent large language models. In addition to delivering superior performance in software log classification, our model is practically feasible to be deployed in industrial settings.", "sections": [{"title": "I. INTRODUCTION", "content": "As the telecommunications industry rapidly evolves into the 5G and 6G era, the scope and complexity of network testing have expanded at an unprecedented rate. Such testing solutions entail emulating user equipment (UE) and their intricate inter-actions with the network infrastructure. This enables network operators and equipment manufacturers to evaluate the quality and capacity of their wireless networks, ensure compliance with industry-standard protocols, and effectively troubleshoot issues while optimizing network performance. Meeting these needs involves employing advanced dedicated hardware (such as VIAVI TM500).\nInvariably, these sophisticated systems generate voluminous and highly complex software logs that are indispensable for troubleshooting and defect triaging. The telecom raw logs ana-lyzed in this context stand apart in their complexity and nature from conventional software logs, exhibiting a vast diversity of commands and parameters configured in real-time under dynamic industrial conditions. Moreover, their structure and semantics bear little correlation to natural language. Therefore, only expert engineers with extensive experience in the field can navigate through these logs to identify issues in a network emulation. However, this manual analysis is inefficient, error-prone, less scalable, and vulnerable to knowledge silos. This inefficiency in manual analysis is further compounded by the high stakes involved; delays in defect resolution can have significant financial and reputational repercussions for both service providers and their clients.\nRecently, the application of machine learning (ML) and natural language processing (NLP) techniques has demon-strated remarkable success in the classification and extraction of insights from text-based software logs [1], [2], [3], [4], [5], [7], [8], [18]. Additionally, a multitude of research studies have been dedicated to log analysis using ML techniques for anomaly detection, proposing ML solutions ranging from supervised classical models [6], [16] to convolutional neural networks (CNNs) [15], recurrent neural network (RNNs) and long short-term memory (LSTM) based networks or hybrid models [19], [10], large language models (LLMs) [12], [14],"}, {"title": "II. RELATED WORK", "content": "Analysing software logs has been studied under both super-vised and unsupervised learning categories in the literature. Below, we discuss some notable work on log classification and log anomaly detection highlighting key methodologies and their outcomes, which partly inspired our choice of CNN-based architecture for our industrially-motivated telecommu-nications logs classification use case.\nA. Supervised Methods\nClassical ML\nChen et al. [25] presented a decision tree (DT) learning approach to diagnosing failures in large internet sites. They recorded the runtime properties of each request and trained a DT algorithm on the request trace to identify the causes of failures. Their method successfully identified 13 out of 14 true causes of failure in the logs produced by a large internet services system in a production environment, prov-ing its effectiveness in real-world applications. Liang et al. [26] developed a methodology for predicting failures in IBM BlueGene/L supercomputers using event logs. They converted event logs into datasets suitable for classification techniques and applied several classifiers, including a rule-based classifier, support vector machines (SVMs), and a customized nearest neighbor method. Their customized nearest-neighbor approach outperformed others in coverage and precision, suggesting its viability in alleviating the impact of failures. One key benefit of some of the classical ML algorithms is their interpretability. For example, a DT may be indicative of which events or log messages are more likely to be related to a failure. Never-theless, more advanced algorithms have since been proposed, particularly with the progress of deep learning.\nRNN and CNN\nRecurrent neural networks, particularly those based on LSTM units, have been extensively applied to log classi-fication due to their capability to handle sequential data and capture long-term dependencies within log sequences. In one of the pioneering studies, Du et al. [27] proposed DeepLog, a deep neural network utilizing LSTM units that can detect anomalies when the log patterns deviate from the normal execution. DeepLog\u2019s architecture includes three main components: a log key anomaly detection model, a parameter value anomaly detection model, and a workflow model for diagnosing anomalies. The models are trained with log entries from the normal system execution path. Each log entry is parsed to a log key and parameter value vector, and the models are tasked to predict the probability distribution of the next log key or parameter value vector in their respective sequences. Anomalies are detected at the inference stage by comparing the actual next log key or parameter value with the top predicted ones which model the normal behaviour. If the predictions significantly deviate from the ground truth, as found systematically using thresholds, then the entry is flagged as an anomaly. Zhang et al. [28] proposed LogRobust, an attention-based bidirectional-LSTM-based neural network for detecting log anomalies in both synthetic public datasets and a real industrial dataset. They model each log event as a semantic vector and use the attention mechanism to weigh different events, as well as a Bi-LSTM architecture to produce anomaly likelihood. The proposed architecture is benchmarked against classical ML models, including SVM and logistic regression, and is demonstrated to achieve consistently high performance. Several other deep learning architectures with LSTM as the key component have been proposed for log anomaly detection [19], [29], [30], [10].\nOn the other hand, CNN architectures can also be tailored for log classification with a much lower computational load compared to RNN architectures, especially for large context windows. CNNs may work well, especially when the existence of specific log events is important in detecting critical incidents rather than the long-range dependency between the events. A one-dimensional convolutional architecture has been proposed for detecting anomalies in big data system logs such as Hadoop Distributed File System (HDFS) logs [15]. Here, the proposed CNN achieved slightly higher performance in relation to a gen-eral multi-layer perceptron (MLP) model and an LSTM-based architecture. Ren et al. [8] propose a more feature engineering-heavy strategy followed by a two-dimensional CNN to classify CMRI-Hadoop and bluegene/L logs with critical events into 13 different categories. The proposed CNN showcases the best performance among other models, including against classical ML and LSTM-type models.\nB. Unsupervised Methods\nUnsupervised log anomaly detection methods can be quite useful, especially when extracting the labels for logs, which is too expensive or impractical. Unsupervised log analysis has been studied using a wide variety of algorithms, from classical models to state-of-the-art transformer-based approaches. Lin et al. [31] presented LogCluster, an agglomerative hierar-chical clustering approach to detect anomalies in Hadoop-based application logs and large-scale online service plat-forms. Principal component analysis (PCA) has also been applied for log anomaly detection in the absence of a labelled dataset. PCA projects high-dimensional data (\u2208 RN) into a lower dimensional coordinate system composed of k principal components (k < N) that capture the most variance in the original high-dimensional data. Xu et al. [32] represented log sequences as event count vectors and used PCA to obtain a lower dimensional representation of logs. They identified anomalies by thresholding the norm of the low-dimensional vectors.\nWith the remarkable success of the self-attention mechanism and the transformer architecture in language understanding, an increasing number of LLMs have been adapted for log analysis. Lee et al. [12] proposed an unsupervised log anomaly detection method based on the BERT architecture [21]. They first pretrain BERT on normal log data with a masked language modelling (MLM) objective. They then adopt the assumption that the context of a normal system log is notably different from that of an abnormal system log. Under this assumption, a normal log should exhibit a low error and high probability of prediction for masked tokens, whereas an abnormal log may produce a high error and a flatter probability distribution,"}, {"title": "III. METHODS", "content": "In this section, we discuss and formulate the proposed models, in particular our seq2seq model for language under-standing and the residual CNN for classification.\nA. Sequence-to-Sequence Embedding Method\nThe objective here is to train a ML model with a seq2seq objective for acquiring learned token embeddings from an industrial corpus. The process of transitioning from a textual representation to a numerical one typically encompasses mul-tiple stages. Raw logs are voluminous, often riddled with noise, inconsistencies, and occasional ambiguity. Once a historical raw logs database is collected, we send it through a pre-processing unit (PPU). PPU captures logs related to several network testing categories, namely, single-UE, single-cell, multi-UE, multi-cell, New Ra-dio (NR) 5G tests, Long-Term Evolution (LTE) 4G tests, and L3 tests. Additionally, the PPU removes redundant information unrelated to detecting defects, such as very long words and lines, and numbers. Next, we identify outlier logs based on their size using the Tukey's method. A box plot is created, showing the distribution of the number of characters in logs. Outliers are then defined as observations that fall below Q1 1.5 * IQR or above Q3 + 1.5 * IQR, where Q1 and Q3 are first and third quartiles, respectively, and IQR is the interquartile range (Q3- Q1). These outliers, along with files > 300 kilo-bytes, have been removed to establish our final dataset. Such filtering is necessary as our raw logs can occasionally go up to hundreds of Megabytes in size.\nThe resulting software logs are then concatenated to create a training corpus (TC). Given the little correlation between our logs and natural language, we opted to use unique characters present in our corpus as tokens. This approach yielded in a vocabulary consisting of 97 unique characters, allowing us to encode any piece of text within the TC and obtain a corresponding numerical representation. In order to create training sequences, we choose a sequence length (ls) equal to the median length of message blocks (in characters) in our software command logs. A block refers to the set of information contained within so-called Indications (I:) that record a set of events that have come back from the network, and Confirmations (C:) that record the state of execution of system commands. These are the information that is critical for the Al system to be able to learn and capture defects, justifying the rationale behind the selected ls. Next, we create tuples of input and target sequences (si, st) with matching lengths (ls), where st is formed by shifting a window of lw characters across si within the continuous TC. To maintain simplicity, we adopt the assumption that lw = 1.\nHaving equal and fixed-length input and target sequences allows us to design a simpler recurrent neural network ar-chitecture (RNN) without an explicit decoder for seq2seq learning. The proposed architecture as illustrated in Fig. 1 has an embedding layer to represent every character in the vocabulary with an embedding of 64 dimensions, chosen heuristically. This is followed by an LSTM layer with 1024 units that returns processed sequences, and an output fully-connected dense layer with the number of units equal to the vocabulary size. The dense layer is applied across all returned sequences by the LSTM layer. As such, our optimization objective is to minimize the negative log-likelihood of the true next sequence.\nargmin\u03b8\n  1\n  N\n  \n\u2211\nj=1\n\u2212log P(sj|sj\u22121; \u03b8)\n(1)\nwhere N is the total number of input and target sequence tuples in the dataset. P(sj|sj\u22121; \u03b8) is the conditional probabil-ity of generating the true next sequence sj given the previous sequence sj\u22121 and the model parameters \u03b8. With this notation, the target sequence st equals to sj, and the input sequence si equals to sj\u22121. Equation 1 can be further decomposed as follows, considering individual characters.\nargmin\u03b8\n1\nN\n\nN\n\u2211\nj=1\n\n1\nls\n\nls\n\u2211\nt=1\n\u2212log P(sjtsj\u221211,..., sjt\u22121 ls; \u03b8)\n(2)\nwhere stj is the character at the tth index of sequence sj. Probability logits over the vocabulary for every character in the target sequence are calculated through a softmax function applied at the output layer. Consequently, the model is trained to minimize the multi-class categorical cross-entropy loss. Once the model is trained, character embeddings are extracted from the weights stored within the Embedding layer of the model. It is worth noting that the trained seq2seq model, primarily utilized for extracting character embeddings, has the potential for software log text generation. However, this aspect falls outside the scope of this paper.\nB. Residual CNN for Software Log Classification\nThe proposed architecture for lengthy software log classi-fication is primarily based on one-dimensional convolutional layers (Conv1D). CNNs are generally lightweight and con-sume less computational resources. This architectural choice stems partly from practical considerations in industrial produc-tion, as many target edge devices lack dedicated hardware like GPUs. Nevertheless, as we demonstrate later in this article, the proposed CNN, in fact, consistently outperforms other benchmarked models. Formally, Conv1D operation applied on a discrete 1D input sequence s at a time index t with single filter w is given by;\ny(t) = (w \u2217 s)(t) = \u2211\nk=\u2212(K\u22121)2(K\u22121)2w(k)s(t \u2212 k)\n(3)\nwhere * indicates the convolution operation, y(t) is the feature map resulting from the filter applied at position t, and K is the kernel size.\nConcretely, our CNN consists of an embedding layer ini-tialized with character embeddings extracted from the seq2seq model, followed by a block of Conv1D layers before applying global max pooling operation on the processed sequences, resulting in a fixed-size representation of the input sequence. This is then processed through a set of fully-connected layers followed by softmax activation for multi-class classification. We further apply residual connections to improve model performance. The end-to-end (E2E) architecture is depicted in Fig. 3.\nThe model is trained to classify various software logs into four distinct classes: Pass, L0_L1, L2, and L3. Pass class represents software logs that do not indicate any issues. L0_L1 denotes defects at the physical layer, L2 pertains to issues within the data link layer, and L3 encompasses problems related to the network or higher layers, in accordance with the Open Systems Interconnection (OSI) model. These labels for the software logs in our dataset are extracted from historical data. Undoubtedly, most test runs are completed without is-sues, resulting in a highly imbalanced class distribution within our dataset, as can be seen in Fig. 4(A). To reduce the impact of class imbalance, ML models studied in this work are trained with appropriate class weights where applicable. There are 3262 samples in total in our dataset which is randomly divided into 70% training and 30% test sets. The class distribution for the test set is shown in Fig. 4(B)."}, {"title": "IV. RESULTS AND DISCUSSION", "content": "In this section, we present the classification results of the proposed CNN on the test set and benchmark it against several other approaches, including LLMs. Note that the test set is kept intact across all experiments.\nA. CNN Results\nFig. 5 illustrates the training progress of our CNN, namely the categorical cross-entropy loss, accuracy, and F1-score curves. Our defect triaging model achieves a notable 96% accuracy, markedly improving both precision and efficiency in manual engineering workflows. A more comprehensive summary of results is shown in Table I. Although our model can support text sequences up to 200,000 characters, we used a sequence length of 50,000 characters to obtain results shown in Table I. This is because nearly 95% of our cleaned software logs are less than 50,000 characters in length. The trained model, with its compact size of just 3 megabytes, is ideally suited for deployment on edge devices with basic hardware resources.\nOn the other hand, LSTM networks are recognised for their effectiveness in sequence classification tasks, owing to their ability to learn long-term dependencies with memory cells and gating mechanism. It is interesting to include LSTM components in our model and assess the impact on perfor-mance. In fact, such architectures have been proposed for sequence learning tasks in telecommunications including chan-nel estimation [45]. We added a bidirectional LSTM layer right after the embedding layer of the proposed CNN (denoted as BiLSTM+CNN model) and evaluated the performance on the test set under the same conditions. This resulted in a slightly downgraded accuracy of 94.2%, despite a nearly three-fold increase in model size (see Table I). This potentially suggests that in the context of our software logs, it is not so much the long-range dependencies or the semantic structure that are crucial for identifying defects but rather the presence of specific combinations of log messages. The confusion matrices of the CNN and CNN+BiLSTM models are displayed in Fig. 7. Both models can accurately detect even the least represented class (L3) that has less than one-tenth of samples compared to the dominant class.\nWe subsequently explored how the input sequence length impacts the performance of our CNN. Illustrated in Fig. 6, we observed a steady improvement in performance as the sequence length increases, up until a point where it begins to marginally decline for extremely lengthy sequences (>80k characters). This trend may occur because such expansive con-text windows necessitate excessive padding in most input text sequences with redundant tokens, thereby diluting the pertinent information. Furthermore, we noticed that the proposed CNN model is generally robust in terms of the number of Conv1D layers present in the architecture. Employing just a single Conv1D layer while maintaining the rest of the architecture (embedding & dense layers) intact yielded an accuracy of 93.6%. Further increasing the number of Conv1D layers, up to four, repeatedly led to a classification accuracy of above 95% with consistent distribution of other performance metrics.\nB. LLMs for Software Log Classification\nWhile LLMs have been generally successful in learning to categorize software logs [13], [12], raw logs generated by soft-ware and hardware stacks prevalent in the telecommunications industry pose unique challenges to LLMs, particularly due to their vast size and little relevance to the natural language. It may be possible to intuitively reason that the small context windows of off-the-shelf LLMs cannot possibly capture all necessary information from large logs, which may lead to poor downstream performance. Similarly, conducting further pretraining (also known as domain adaptation) on a domain-specific corpus could be beneficial in enhancing downstream task accuracy. We investigate five pretrained LLMs: LLaMA2-7B, Mixtral_8x7B, Flan-T5, BERT, and BigBird in our specific application.\nWhile every model undergoes evaluation in its original pre-trained state, both Flan-T5 and BERT are additionally chosen for domain adaptation, owing to their distinctive natures and more manageable sizes. Next, we discuss the exact steps that we follow for pretraining LLMs on our own industrial software log corpus.\nDomain adaptation or further pretraining of LLMs may significantly elevate the downstream task performance of LLMs, especially in domains where the morphology of text is considerably different from the natural language on which most models are trained on. First, we identify some specific pieces of text, such as hexadecimal strings, standalone num-bers, IP addresses, and new-line characters, that are gener-ally unrelated to identifying defects. These are then masked with special tokens such as <hex>, <num>, <ipaddr> and <newline>. Furthermore, the end of a software log sample in our concatenated training corpus is indicated with an <endsample> token. Tokenization is a critical step in preparing data for LLM training that involves breaking down raw text into numerical representations. The models investi-gated in this study use subword tokenization, which balances the need to represent common words as they appear and decompose less common ones into smaller, understandable sub-units, resulting in efficient handling of out-of-vocabulary words. Here, we use the pre-trained WordPiece tokenizer for BERT and the SentencePiece tokenizer for Flan-T5. The special tokens identified above are added to both tokenizers before tokenizing the TC.\nWe then train BERT for domain adaptation with a warm start from its pre-trained base model checkpoint. The relatively small size of BERT allows for updating all model weights during domain adaptation, conducted with a masked language modeling (MLM) objective on the same software log TC constructed in the workflow shown in Fig. 1, further processed to add special tokens discussed above. Note that one of the pre-training strategies of BERT, next sentence prediction (NSP), is not used here as performing domain adaptation using MLM is deemed sufficient [43]. In MLM, a portion of tokens are randomly masked (~30% in our case), and the model is tasked to predict these masked tokens based on their context, enabling it to gain a deep understanding of the syntax, semantics, and morphology of a given text corpus.\nM =\nL\u2212w\nlc\n + w\n(4)\nUnder the proposed approach, the document global embed-ding Eg can be computed as follows.\nEg =\n1\nM lc\n\u2211\nk=1\nlc\n\u2211\ni=1\nTEk,i\n(5)\nHere, TEk,i is the token embedding of the ith token of the kth chunk. Token embeddings are extracted at the last hidden layer of the model. TEk,i is of dimension de equal to the number of units in the last hidden state of the model. Essentially, the mean-pooling operation is applied across token embeddings of a chunk to obtain the chunk embedding, and the same operation is repeated across all chunks to obtain the document embedding. The attention mask is considered when applying the pooling operation (i.e., pad tokens are disregarded). Text chunks are selected with an overlap w so that some information and context from the previous windows are retained as the model slides through the text. Likewise, embeddings of all logs in our dataset are extracted indepen-dently using every LLM examined. Note that due to memory constraints, the context window used for embedding extraction in this work may be smaller than the maximum context window supported by some LLMs. In order to assess the quality of these embeddings, we applied separate classifiers, namely, random forest, XGBoost, and decision tree, with the same defect detection objective. This facilitates benchmarking the proposed LLM-embedding method against our residual CNN. In addition to classical ML models, we utilize a similar residual 1D CNN to classify LLM-embeddings and report the performance on the test set.\nTable II summarizes the experimental results. LLaMA2-7B embeddings used with the XGBoost classifier provide the best accuracy of 82.2% across all experiments. Although this is rea-sonable, it represents a notable decline in performance when contrasted with the results achieved by our standalone residual CNN, indicating that even some of the most powerful general-purpose LLMs may perform sub-optimally in challenging domain-specific tasks such as large and complex software log classification. Nevertheless, this series of experiments yields valuable insights. As expected, large and more potent LLMs, specifically, LLaMA2-7B and Mixtral_8x7B, produce higher quality embeddings, resulting in higher downstream performance compared to other models. Moreover, domain adaptation enhances the performance to some extent, as the models may have gained some understanding of the specific text format by being exposed to a targeted corpus. This enhancement, however, is more prominent for BERT compared to Flan-T5 likely due to distinct domain adaptation strategies. Flan-T5 is pretrained on our corpus by applying LoRA exclu-sively to the query and value matrices in the transformer block, and therefore the initial token embeddings or model weights that have been optimized for natural language understanding do not get updated. These tokens derived from a corpus of natural language, and their embeddings, may not be very relevant in our specific context. On the contrary, BERT is end-to-end trained during domain adaptation, allowing us to learn token embeddings and model parameters tailored to our corpus, leading to a more substantial boost in performance. Moreover, the downstream task performance is majorly reliant on the quality of embeddings, as more advanced architectures such as 1D-CNNs do not lead to higher performance relative to some classical ML models.\nFinally, we finetune the original pretrained BigBird base model on our labelled dataset with the hope that its relatively large context window of 4096 tokens can capture as much information as possible from software command logs. Instead of extracting embeddings, here, the model is end-to-end fine-tuned with a classification head, and all model parameters are updated during training. This resulted in accuracy and F1-score of 81% and 0.581 on the test set, respectively, consistent with LLM-embedding classification results presented in Table II. An analysis of similar-size models reveals that, when the model context window is sufficiently large, finetuning LLMs end-to-end on a downstream classification task leads to higher performance compared to employing separate classifiers on pretrained LLM embeddings. Nevertheless, none of the LLM-based approaches achieves comparable performance to the proposed CNN, indicating that domain-tailored ML archi-tectures may be better suited for industrial applications that require processing miscellaneous formats of text. In addition to delivering superior performance, such lightweight ML models minimize the cost of production and are practically feasible for in-house or field deployment."}, {"title": "V. EXPERIMENTAL DETAILS", "content": "All training and evaluation experiments in this study were performed on KubeFlow, which enables the orchestration of machine learning workflows on the Kubernetes cluster. Ubuntu 20.04 was used as the operating system. The hardware infras-tructure consisted of an Nvidia DGX server carrying 8xA100 graphics processing units (GPUs), each with 80GB memory. The system is equipped with 1 terabyte of physical memory. MLFlow [44] integrated within the KubeFlow environment was used for experiment tracking and logging artifacts.\nCNN architecture and hyperparameters were optimized em-pirically. The model was trained for up to 200 epochs with an early stopping patience of 30 epochs. Adam optimizer with a learning rate of 10\u20134 was used to optimize model parameters. L2 regularization was applied to selected layers to reduce overfitting and improve generalization of the model. The batch size was adjusted accordingly within a range of 16 to 512 to accommodate large context sizes. For the largest context size tested (200k), the model completed training in under one hour. A similar hyperparameter setting was employed for the seq2seq LSTM model that had 4.6 Million parameters. We used Keras package with TensorFlow backend to implement and train the models.\nLLMs analyzed in this study were adapted from Hugging Face [46] using the transformers package. BERT and Flan-T5 models were each trained for 5 epochs for domain adaptation. LORA adapters, with a rank of 16 and a scaling factor of 32 were used to train Flan-T5. BERT was trained in less than one hour on our TC whereas the training of Flan-T5 took about seven hours. The batch size is set to 64 for BERT and 12 for Flan-T5. Larger models (LLaMA2-7B and Mixtral_8x7B) were 4-bit quantized using the bitsandbytes package before extracting embeddings for computational efficiency. Flash at-tention 2 [47] implementation in the transformers package was used to further reduce memory requirements. The overlapping window size was set to half the sequence length for embedding extraction. torchrun was used for distributed training of LLMS where applicable. BigBird base model was finetuned for up to 200 epochs with a batch size of 8 and an early stopping patience of 30 epochs. In general, the finetuning of BigBird took about 4 hours. The hyperparameters of the ML models used with LLM-embeddings were optimized using a 5-fold cross-validation strategy with grid search. Classical ML imple-mentations were adopted from Scikit-Learn [48] with default parameters. Classification performance metrics are calculated as follows where TP, FP and FN indicate true positives, false positives and false negatives, respectively.\nPrecision =\nTP\nTP + FP\n(6)\nRecall =\nTP\nTP+FN\n(7)\nF1_score = 2 \u00d7\nPrecision \u00d7 Recall\nPrecision + Recall\n(8)"}, {"title": "VI. CONCLUSION", "content": "In this research, we presented a robust CNN architecture tai-lored for the classification of intricate large-scale software logs in the telecom sector to meet the edge-deployability require-ment of defect detection systems. The proposed model, adept at processing extensive text sequences up to 200,000 char-acters, markedly outperforms some LLM-based approaches examined. This advancement is critical in mitigating the limitations of manual log analysis, such as inefficiency and error-proneness, offering a streamlined, automated approach for defect triage in 5G/6G network testing. Our research demonstrates that custom, compact models like our CNN ar-chitecture can not only offer a practical and efficient alternative to resource-intensive LLMs in specific industrial applications, but can even surpass their performance. While our results do not establish that custom CNN architectures can universally surpass LLMs in all log classification tasks, this study un-derscores the importance of considering architectural choices in light of real-world constraints and production overheads. In conclusion, the present study not only contributes a novel, edge-deployable ML model for accurate defect detection from raw software logs in the telecommunications industry but also provides valuable insights into the application of artificial intelligence in industrial settings, paving the way for future innovations in software log classification."}, {"title": "APPENDIX", "content": "We acknowledge the support of VIAVI Solutions Inc. for their provision of data, funding, and MLOps infrastructure, including GPUs, which contributed significantly to the com-pletion of this project."}]}