{"title": "MambaCapsule: Towards Transparent Cardiac Disease Diagnosis\nwith Electrocardiography Using Mamba Capsule Network", "authors": ["Yinlong Xu", "Xiaoqiang Liu", "Zitai Kong", "Yixuan Wu", "Yue Wang", "Yingzhou Lu", "Honghao Gao", "Jian Wu", "Hongxia Xue"], "abstract": "Cardiac arrhythmia, a condition characterized by irregular heartbeats, often serves as an early\nindication of various heart ailments. With the advent of deep learning, numerous innovative models\nhave been introduced for diagnosing arrhythmias using Electrocardiogram (ECG) signals. However,\nrecent studies solely focus on the performance of models, neglecting the interpretation of their results.\nThis leads to a considerable lack of transparency, posing a significant risk in the actual diagnostic\nprocess. To solve this problem, this paper introduces MambaCapsule, a deep neural networks for\nECG arrhythmias classification, which increases the explainability of the model while enhancing\nthe accuracy. Our model utilizes Mamba for feature extraction and Capsule networks for prediction,\nproviding not only a confidence score but also signal features. Akin to the processing mechanism of\nhuman brain, the model learns signal features and their relationship between them by reconstructing\nECG signals in the predicted selection. The model evaluation was conducted on MIT-BIH and PTB\ndataset, following the AAMI standard. MambaCapsule has achieved a total accuracy of 99.54% and\n99.59% on the test sets respectively. These results demonstrate the promising performance under the\nstandard test protocol.", "sections": [{"title": "1. Introduction", "content": "Cardiac arrhythmia refers to the disruption of the car-\ndiac rhythm caused by abnormal cardiac electrical activity\n(Khraishah et al., 2022). For the reason that arrhythmia\nserves as an indicator to advent heart consequence, its accu-\nrate detection and classification are imperative for mitigating\npotential risks and complications. Electrocardiogram(ECG)\nis widely used for recording the electrical cardiac activ-\nity(Faust et al., 2013). It provides critical information about\nthe cardiac function and health by measuring the cardiac\nelectrical signals during each heartbeat cycle, which makes\nit a reliable tool for diagnosing heart health.\nRecent researches have shown great advantages of deep\nlearning in arrhythmia classification (Krittanawong et al.,\n2017; Luz et al., 2016), leveraging the information contained\nin ECG signals. The development of powerful and effective\ndeep learning methods has the potential to significantly\nimprove the accuracy and efficiency of arrhythmia diagnosis,\nand ultimately leads to better patient outcomes.\nAlthough the numerous previous models have made sub-\nstantial progress in arrhythmia classification, most of them\nsolely focus on the performance neglecting the interpretation\nof their results, which leads to a considerable lack of trans-\nparency and poses a low reliability in the actual diagnostic\nprocess(Y\u0131ld\u0131r\u0131m et al., 2018).\nTo alleviate this limitation, we proposed a encoder-\ndecoder based neural network called Mamba Capsule in this\npaper. Inspired by the mechanism of brain processing(Luppi\net al., 2024; Bao et al., 2017; Wang et al., 2021), which\ntells what features exactly the eyes see and tells the reason\nwhy the brain chooses to classify one item to this label not\nthe other one, we novelly designed our model to implement\nboth processes by using the main architecture for prediction\nand a reconstruct part for explaination. Our evaluation was\ncarried on MIT-BIH dataset(Moody and Mark, 2001) and\nPTB(Bousseljot et al., 1995) dataset, reached 99.54% and\n99.59% accuracy. By applying a reconstruct network, we\ncould intuitively know what the model sees and why it makes\nthat decision.\nThe main contributions of this paper are as follows:\n\u2022 We introduced a novel encoder-decoder based net-\nwork architecture based on Mamba(Gu and Dao,\n2023) and Capsule networks (Sabour et al., 2017),\nchanging the form of the output from the probability"}, {"title": "2. Related work", "content": "Mamba has made excellent performance in numerous\ntraditional fields since it was proposed by Gu and Dao\n(2023). Different from traditional and the most popular\nstructure Transformer, the advantages of Mamba are mainly\nreflected in the computational efficiency and the ability to\ndeal with long sequences. For the reason above, many re-\nsearches have been carried to replace traditional components\nwith Mamba. Wang et al. (2024a) proposed Graph-Mamba,\nwhich improved the remote context modeling ability of\nattention mechanism by integrating Mamba block and node\nselection mechanism, and solved the problems of high com-\nputing cost and limited scalability of attention mechanism.\nRimon et al. (2024) proposes a new model-based meta-\nreinforcement learning method, achieving higher returns\nand better sample efficiency (up to 15x) with little need\nfor hyper-parameter tuning. Qiao et al. (2024) proposes a\nmulti-modal large language model VL-Mamba based on\nstate space model, demonstrating competitive performance\non a variety of multi-modal benchmark tasks and the great\npotential of state-space models in multi-modal learning.\nMamba also show its advantages in vision tasks (Wang\net al., 2024b; Ma et al., 2024; Liu et al., 2024). Zhu et al.\n(2024) proposed a generic vision backbone with bidirec-\ntional Mamba blocks (Vim) to solve the location sensitiv-\nity and global context requirements of visual data. Vim\nperforms well in various tasks, with significant improve-\nments in compute and memory efficiency. Li et al. (2024)\nintroduced VideoMamba into video understanding task. Its\nlinear-complexity operator enables efficient long-term mod-\neling, which is crucial for high-resolution long video under-\nstanding."}, {"title": "2.2. Capsule Neural Network", "content": "Capsule network was first proposed in a 2D handwritten\ndigital image classification task and has excellent perfor-\nmance in the view-invariance (Sabour et al., 2017). Later on,\nit was widely applied to point cloud (Zhao et al., 2019), im-\nage processing (Chen et al., 2021a; Vijayakumar, 2019), and\ntabular learning (Chen et al., 2023), graph learning (Xinyi\nand Chen, 2018). Specifically, (Xiang et al., 2018) pro-\nposed a multi-scale capsule network that is more robust and\nefficient for feature representation in image classification,\ndemonstrating a competitive performance on FashionM-\nNIST and CIFAR10 datasets. Nguyen et al. (2019) introduce\na capsule network that can detect various kinds of attacks.\nThe model uses many fewer parameters than traditional\nconvolutional neural networks with similar performance.\nCapsule neural network also has a intrinsic advantage for\nexplaining its results. For instance, Li et al. (2019a) produced\na sentiment capsule to identify the informative logic unit and\nthe sentiment based representations in user-item level for\nrating prediction. By introducing capsules in different layers,\nShahroudnejad et al. (2018) demonstrated the possibility of\ntransforming deep networks into transparent networks. This\nwork helps make DNNs' internal processes more under-\nstandable and provides the ability to explain their decisions."}, {"title": "2.3. ECG based Cardiac Disease Diagnosis", "content": "Numerous researches have been carried in the field of\nECG signal classification and the most challenging part of\nit is to extract the features from the signal. The feature\nextraction methods have experienced from the manual fea-\ntures and automatic features. Manually designed features\nextracting usually applied by traditional ML methods like\nSupport Vetor Machine (Qin et al., 2017b) and Random\nForest (Rouhi et al., 2021). Qin et al. (2017a) proposed an\neffective methods to extract the abnormal beat eigenvectors\nof low-dimensional ECG and classify them using support\nvector machine. Majeed and Alkhafaji (2023) proposed a\nML model using multi-domain features combined with least-\nsquare support vector machine (LS-SVM), which extracts\nthe time and frequency domain features and selects the most\nrelevant ones.\nWith the success of deep learning in vision and NLP\nfields, its ability has been admitted and has been applied\nto many other fields, including the ECG signal classifi-\ncation (Chen et al., 2022, 2024; Hu et al., 2024; Hong\net al., 2019; Ribeiro et al., 2020; Pyakillya et al., 2017;\nBian et al., 2022; Chen et al., 2021b; Yan et al., 2019). Li\net al. (2020) proposed a 1D residual CNN based on deep\nresidual networks with a 31-layer one-dimensional residual\nconvolutional neural network combined with 2-lead ECG\nsignals for the classification. Kim et al. (2022) proposed\na novel model combining residual networks, compressed\nactivation blocks and bidirectional short-duration memory\nnetworks. It was evaluated on multiple databases and the\nresults showed that the framework performed well especially\nfor a small number of categories.\nAlthough these methods have their own way to explain\nthe process of the prediction, their lack of accuracy and\ninadequacy of features extracting hinder their practical ap-\nplications. Some efforts also focused on the class imbal-\nance problom on ECG datasets. For instance, El-Ghaish\nand Eldele (2024) introduced a deep learning model called\nECGTransForm,which combines multi-scale convolution,\nchannel calibration module and two-way Transformer mech-\nanism to improve the ability to capture spatio-temporal\nfeatures in ECG data and solves class imbalance problem\nby introducing context-aware loss function. However, these"}, {"title": "3. Methods", "content": "State space models (SSMs) Gu et al. (2021); Gu (2023)\nare defined as 1D-time-invariant systems, which serve as the\npredecessor of Mamba. These systems aim to map a 1D input\nsequence x(t) \u2208 R\u00b9 to a 1D output sequence y(t) \u2208 R\u0139\nthrough a hidden state h(t) \u2208 RN, which can be represented\nas the following equations:\n$$h(t) = Ah(t) + Bx(t)$$\n$$y(t) = Ch(t)$$\nwhere A \u2208 RN\u00d7N, B\u2208 RN\u00d7L, C \u2208 RL\u00d7N. Consid-\nering that deep neural networks operate in discrete space,\nresearchers applied a zero-order preservation on the matrix\nA and B, turning the equations to the form as:\n$$h\u2081 = Ah\u2081\u22121 + Bx\u2081$$\n$$y\u2081 = Ch$$\nWhere A = exp(\u25b3A) and B = (\u25b3A)\u00af\u00b9(exp(\u25b3A)\nI) (AB) for represents the time step in the continuous\nspace. SSMs also code the remote dependency by initializing\nmatrix A as HiPPO(Gu et al., 2020) and allow training in\nthe form of a convolution kernel to accelerate the training\nprocess. However, the matrix A, B and C do not change\nwhatever the input data are, leading to problem of low\nsensitivity. To solve the problem, Mamba applies a selective\nmethod to SSMs, which makes the matrix B and C and\nadapt themself based on the input as follows:\n$$B = SB(X)$$\n$$C = Sc(x)$$\n$$\u25b3 = t(Parameter + sx(x))$$\nWhere SB, SC and sa are linear projections, and to is\na SoftPlus function. The matrix A does not depend on the\ninput directly, it is influenced by the in the process of the\nzero-order preservation, making all main parameter inputs\nadaptable(Gu and Dao, 2023)."}, {"title": "3.2. Capsule Networks", "content": "The whole network contains three parts: the convolution\nnetwork, the capsule network and the dynamic routing. The\ndifference between the capsule network and the traditional\nneural network is that the unit of the neural network is a\nscalar neuron x \u2208 R, while the unit of the capsule network\nis a capsule, which is a vector x \u2208 RN.\nIn a capsule network, there are many capsules in one\nlayer, and each capsule contains two types of information:\nthe given vector's orientation and the vector's length as the\nprobability of the capsule, the next capsule first applies a\nlinear transformation of the previous capsule output as:\n$$\u00fbj|i,L+1 = Wij,LUi,L$$\nwhere uil \u2208 RD is the i-th output capsule at layer\nL, Wij, \u2208 RDXD serves as the transformation matrix\nand the \u00fbjli,L+1 \u2208 RD represents the contributes from the\nparent i-th capsule to the j-th one at layer L + 1. The\nmodel then gets the weight of each contributes through a\nclustering like routing algorithm. At present, the popular\nrouting algorithms include dynamic routing algorithm and\nself-routing algorithm. This paper adopts dynamic routing\nalgorithm, which obtains the final weight coefficient through\nseveral rounds of iteration. In each iterations, the weight\ncoefficient c is:\n$$Cij =\\frac{exp(bij)}{\u03a3k exp(bik)}$$\nIn the first iteration the bij is initialized to 0 and will be\nupdated at the end of each iteration. Then the coefficient is\nused to calculate the temp capsule:\n$$Sj = \u2211 Cij\u00fb ji$$\nWhere s; \u2208 RD serves as a temp result for an iteration,\nthe capsule network designs a special activation function to\ntransform the length of the capsule into [0-1] as:\n$$Uj =\\frac{||sj||2}{1 + ||sj||2} \\frac{Sj}{||sj||}$$\nWhere v; \u2208 RD serves as the real output capsule in each\niteration. The equation 7 separates the long and short length\nof the capsules in the dim of 0 to 1, making the long capsule\nclose to 1 and short capsule close to 0. Then the bij is updated\nas:\n$$bij = bij + vj\u00fbjli$$\nThe equation 8 serves as an clustering-like algorithm\nfor after each iteration the bij grows bigger in capsule\nwhose position is closest to the others. And after the setting\niterations have done, the v; will serve as the output of the\ncapsule networks."}, {"title": "3.3. Model Architecture", "content": "In this section, we present a classification model based\non Mamba and Capsule networks. The overall framework,\nas shown in Figure 1, consists of three main parts: the\nfeature network, the capsule network and the reconstruction\nnetwork. Before be fed into the feature network, the input\nECG signal is firstly up-projected to contain scope knowl-\nedge not a signal sequence, the input shape changes from\nx \u2208 Rbatch,length to x \u2208 Rbatch,length,dim\nThen N Mamba Layers constitute a pipeline to extract\nfeatures from the x and each Layer contains a Mamba Block\nand a Layer Norm. Previous work typically apply a single\nstate Mamba as the encoder backbone, however, these fails to\nextract wider and narrower features. To solve this problem,\nwe propose a fusion states SSM to construct the Mamba\nBlock. For the i-th Block, it first applies a 1D-convolution\non the upstream i-1-th feature sequence x\u2081, which does not\nchange the dimension of the sequence. Then the sequence\nwill be handled by m different SSM Blocks. The different\nSSM Blocks focus on different scales of time step and have\ndifferent convolution shape to obtain more abundant and\nwider features. Then the features will be down sampled and\nthen linked together to add the x\u2081 in the same shape. The\nwhole process can be formulated as:\n$$\u2081 = Conv\u2081D(xi)$$\n$$uj = DownSample;(SSMj(x;))$$\n$$Xi+1 = Concat(uj) + x;$$\nWhere u; \u2208 Rbatch,length,dim/mand the m is the factor of\nthe dim to assure xi+1 \u2208 [Rbatch,length,dim\nThe feature network extracts the signal features and\nsends the features to the capsule network. The capsule net-\nwork outputs it in the form of capsules in the end.\nThe loss of our model consists of two parts, the marginal\nloss of focusing on model output and the label, and the\nreconstruct loss focusing on the reconstructed signal and the\noriginal signal.\nSince our output is in the form of a capsule network,\nwe must first take the geometric length of the output as the\nprobability of the classification label, and then the difference\nbetween the set positive label threshold and negative label\nthreshold is made. The final form of margin loss function is\nas follows:\n$$Loss margin = \u2211Tmax(0, m+ \u2212 ||vk||)\u00b2+\n\u03bb(1 \u2013 T\u2081)max(0, ||vk|| \u2013 m\u00af)2$$\nWhere Uk \u2208 RD is the k-th output capsule represents one\nclass of the classification, Tk = 1 if the label of the input\nis k, and m+ = 0.9, m\u00af = 0.1 at the beginning. A is used\nto prevent the lengths of vectors from shrinking too much,\nwhich is set to 0.5 in the beginning.\nReconstruct loss needs to choose the label with the high-\nest probability or we choose it for other reasons. Then we\nput the output into the reconstruct model, which is a 3 fully\nconnection layers model with Relu activation. After that,\ncalculate the MSEloss of the reconstructed signal output\nof the reconstruct model and the original input signal, the\nreconstruct loss is formulated as follows:\n$$Xreconstruct = Model(Uchoose)$$\n$$LosSreconstruct = MSELoss(xreconstruct, Xorigin)$$"}, {"title": "4. Model setup and Evaluation", "content": "Since the output probability density of the capsule net-\nwork is not the same as that of the traditional neural classifi-\ncation network, and the bias can directly affect the output, we\nadjusted bias by cosine schedule during the training process,\nso that the threshold of bias kept increasing. At the same\ntime, due to the slow training speed of capsule network in the\ninitial stage of training, the cosine annealing algorithm after\nlinear increase is adopted to change the learning rate. The\nchanges of bias and learning rate are shown in the figure.2"}, {"title": "4.2. Performance Metrics", "content": "To evaluate the model, we applied five performance\nindexes: accuracy(ACC), sensitivity (SEN), F1-score(F1),\nprecision (PPV) and specificity (SPEC). These indexes are\ndefined as follows:\n$$ACC =\\frac{TP+TN}{TP+TN+FP + FN}$$\n$$SEN =\\frac{TP}{TP+FN}$$\n$$F1 = 2 x\\frac{ACC SEN}{ACC + SEN}$$\n$$PPV =\\frac{TP}{TP+FP}$$\n$$SPEC =\\frac{TN}{TN + FP}$$\nIn these formulas, TP represents the number of true pos-\nitives (correctly labeled positive); TN represents the number\nof true negatives (correctly labeled negative); FP represents\nthe number of false positives (incorrectly labeled positive);\nand FN represents the number of false negatives (incorrectly\nlabeled negative)."}, {"title": "4.3. Dataset", "content": "Our training and testing datasets are preprocessed in the\nsame routine and are randomly splited in a ratio of 0.8. The\ndetailed description of the dataset is in Table1:"}, {"title": "4.3.1. MIT-BIH Arrhythmia Dataset", "content": "The MIT-BIH arrhythmia dataset (Moody and Mark,\n2001) is widely used for arrhythmia classification in recent\nresearches. The dataset contains 48 half-hour two-channel\nECG recordings digitized at 11 bit resolution in 10 mV\nrange and a rate of 360 samples per second per channel.\nTwo cardiologists annotated each record separately (about\n110,000 annotations in total). MIT-BIH has annotated 41\ncategories which can be converted to five-classes classifi-\ncation problems following the ANSL/AAMI EC57: 2012\n(The Association for the Advancement of Medical Instru-\nmentation) standard. These distinctive classes are: Normal\nSinus Rhythm (N), Supraventricular Premature or Ectopic\nBeat (S), Ventricular Premature or Ectopic Beat (V), Fusion\nof Ventricular and Normal Beat (F), Unknown Beats (Q).\nSome researches were carried on three or four classes due to\nimbalanced sample size, making the comprehensive evalua-\ntion difficult. In this paper, we chose to adapt to all the five\nclasses."}, {"title": "4.3.2. PTB Diagnostic ECG Database", "content": "The PTB diagnostic ECG database is a popular ECG\nrecording database collected by the National Metrology\nInstitute of Germany Bousseljot et al. (1995). The dataset\ncontains 549 records from 290 subjects (aged 17 to 87,\nmean 57.2; 209 men, mean age 55.5, and 81 women, mean\nage 61.6; ages were not recorded for 1 female and 14 male\nsubjects). Each subject is represented by one to five records.\nEach record includes 15 simultaneously measured signals:\nthe conventional 12 leads (i, ii, iii, avr, avl, avf, v1, v2, v3,\nv4, v5, v6) together with the 3 Frank lead ECGs (vx, vy,\nvz). Each signal is digitized at 1000 samples per second,\nwith 16 bit resolution over a range of \u00b1 16.384 mV. The\nPTB dataset consists of two main classes, i.e., (1) Normal\n(N), which represents normal ECG recordings, and (2) the\nMyocardial Infarction (M), which represents ECG record-\nings that exhibit signs of myocardial infarction, indicating\nthe presence of a heart attack."}, {"title": "5. Experimental Results", "content": "Many previous work focused on only four or less classes\nperformance, aiming to reduce the perplexity of the problem\nor concentrate on specific arrhythmia types. However, it\nleads to great difficulties to compare the ability and uni-\nversality of different models. In our work, we operate our\nevaluation on five full classes."}, {"title": "5.2. Ablation Study", "content": "The ablation experiment was carried out to test the effec-\ntiveness of our model structure. Since Mamba was mainly\nused as the encoder and capsule networks as the decoder in\nour model, the ablation experiment was carried out in these\ntwo aspects on MIT-BIH dataset."}, {"title": "5.2.1. Encoder Structure", "content": "This experiment aims to compare the performance of\ndifferent model structures, including the number of stacked\nmamba layers, the number of mamba states, the steps per\nlayer, whether to adopt the layernorm and dropout structures,\nand the use of different models as encoders.\nIn order to assess the number of mamba states and steps\nin each layer, we adjusted these parameters while keeping the\nencoder layers fixed at 4. The experimental results depicted\nin the figure3demonstrate a significant impact of varying\nnumbers of states and steps on the outcomes. Moreover,\na layer composed of diverse mamba blocks outperforms a\nsingle layer due to its ability to focus on different ECG data\nfeatures across various time scales. Consequently, this wider\ntemporal coverage achieved by combined blocks leads to"}, {"title": "5.2.2. Decoder Structure", "content": "The purpose of this experiment was to compare the\nchanges in the model training and the performance caused\nby using capsule instead of traditional decoder.\nWe tried to replace the capsule network with MLP after\nfreezing encoder parameters. Experiments showed that there\nwas no significant difference between the capsule network\nand MLP under the same encoder with F1-score 93.50\ncompared to 93.62, but the training time of capsule network\nwas longer than that of MLP. This is determined by the\nrouting structure of the capsule network and the number of\nparameters."}, {"title": "5.3. Explainable Study", "content": "The interpretability of this paper is unique from other\nrecent papers on ECG diagnosis, mainly through the recon-\nstruction of ECG signals in different ways to explain what\nknowledge the model has learned on MIT-BIH dataset. For\nthe reason of the limitation of the memory, we only consider\nP-wave and R-wave in our reconstruction, discarding the T-\nwave which is far from the center of the signal."}, {"title": "5.3.1. Feature Relation Mining", "content": "When processing the ECG graphs, the human brain not\nonly attends to focus on individual wave peaks but also\nconsiders the interrelationship between them. The view in-\nvariance of capsule network aligns remarkably well with this\ncharacteristic, which is why we selected it as the decoder.\nIn our experiment, we conducted forward and backward\ndisturbance on the input data to observe the disturbance\nreconstruction of ECG signals using our model and DAE\nmodel. The experimental results are depicted in the figure\n6, where left and right shift represent forward and backward\nshift respectively. Repeated reconstructions of ECG signals\nby MambaCapsule exhibit remarkable consistency, indicat-\ning that Mambacapsule has learned not only the intrinsic\nfeatures but also their interrelationships. Conversely, this"}, {"title": "5.3.2. Cardiac Disease Manifestation Type Inspection", "content": "When human brain categorize an item, it can discern\nthe reasons for its classification and differentiation. Due\nto the inherent output characteristics of Capsule network,\nthe output not only encompasses the probability density of\neach category but also encapsulates the distinctive features\nassociated with that particular category. Consequently, by\nreconstructing ECG signals from unified inputs belonging\nto different categories, we are able to precisely comprehend\nwhy the model deems a certain category more probable. The\nexperimental results are depicted in Figure 7. In Figure 7a,\nboth the ECG signal and model prediction are labeled as N,\nwhere P, R wave can be observed clearly and can be dis-\ntinguished it represents a normal signal. However in Figure\n7b, which is reconstructed by the label S and the length of\nthe capsule is adjusted to be equal to the capsule of label\nN. The false reconstruction represents a vanish in P-wave,\nwhich is a symbol of Supraventricular Premature or Ectopic\nBeat. The reconstructions of N and P labels demonstrate an\nablity of knowledge explaination and thus, each capsule can\nbe seen as the information collection of different labels and\nthe length of it is how much the information is concluded in\nthe signal."}, {"title": "6. Conclusion", "content": "In this work, we have proposed an effective-while-\nexplainable solution to ECG classification by introducing\nMambaCapsule, which emphasizes the capability of multi-\nscale Mamba in feature extracting and the interpretability\nof the form of Capsule network. By applying evaluation\nand reconstruction, the result of our model reveals great\nadvantages in both classification score and mechanism ex-\nplainablity. However, it's necessary to realize the limitation\nof our model, a the long training time and big memory\noccupied. In our future work, we aim to address these\nchallenges by applying pre-trained parameters and optimize\nrouting structure in Capsule network. As an automatic heart\nrhythm abnormality detection system, our model can offer\ndoctors and patients accurate and reliable results, which have\na wide range of application prospects."}]}