{"title": "FB-BENCH: A FINE-GRAINED MULTI-TASK BENCHMARK FOR EVALUATING LLMS' RESPONSIVENESS TO HUMAN FEEDBACK", "authors": ["Youquan Li", "Miao Zheng", "Fan Yang", "Guosheng Dong", "Bin Cui", "Weipeng Chen", "Zenan Zhou", "Wentao Zhang"], "abstract": "Human feedback is crucial in the interactions between humans and Large Language Models (LLMs). However, existing research primarily focuses on benchmarking LLMs in single-turn dialogues. Even in benchmarks designed for multi-turn dialogues, the user inputs are often independent, neglecting the nuanced and complex nature of human feedback within real-world usage scenarios. To fill this research gap, we introduce FB-Bench, a fine-grained, multi-task benchmark designed to evaluate LLMs' responsiveness to human feedback in real-world usage scenarios. Drawing from the two main interaction scenarios, FB-Bench comprises 734 meticulously curated samples, encompassing eight task types, five deficiency types of response, and nine feedback types. We extensively evaluate a broad array of popular LLMs, revealing significant variations in their performance across different interaction scenarios. Further analysis indicates that task, human feedback, and deficiencies of previous responses can also significantly impact LLMs' responsiveness. Our findings underscore both the strengths and limitations of current models, providing valuable insights and directions for future research. Both the toolkits and the dataset of FB-Bench are available at https://github.com/PKU-Baichuan-MLSystemLab/FB-Bench.", "sections": [{"title": "1 Introduction", "content": "Equipped with advanced intelligence and formidable processing capabilities, large language models (LLMs) have demonstrated substantial potential extensive potential in seamless interaction with human users and in assimilating real-time human feedback during inference processes [1]. This human-LLM synergy can be mutually beneficial, breaking through the limitations inherent to each side [2, 3] and has been applied in many domains [4, 5, 6, 7].\nAs a main component of human-LLM synergy, human feedback acts as a response to prior model outputs, serving as a guiding force that directs LLMs towards the desired outcomes [1]. In practical applications, LLMs often need to iteratively adjust their responses based on user feedback in multi-turn dialogues to fulfill user needs. Effective feedback can enhance the quality of responses, while ineffective feedback may mislead LLMs. A robust LLM should leverage appropriate feedback and remain undisturbed by inappropriate feedback. However, evaluating the responsiveness of LLMs to human feedback within multi-turn dialogues presents a significant challenge, as these models exhibit divergent behaviors compared to single-turn dialogues. As illustrated in Figure 1, LLMs that perform well in single-turn interactions may struggle to incorporate user feedback effectively. Conversely, models that may not excel in single-turn scenarios could excel in correcting their previous errors by skillfully leveraging user feedback to improve responses.\nDespite the advancement in evaluating LLMs, there remains a lack of a systematic benchmark for evaluating their responsiveness to various human feedback under real-world usage scenarios. Most existing benchmarks [8, 9, 10, 11] assess LLMs within single-turn scenarios, which is naturally incapable of evaluating LLMs' responsiveness to"}, {"title": "2 FB-Bench", "content": "In this section, we first outline the design logic behind FB-Bench in \u00a7 2.1 and \u00a7 2.2, followed by an explanation of the evaluation methodology of FB-Bench in \u00a7 2.3. Subsequently, we provide a detailed description of the dataset curation pipeline in \u00a7 2.4 and finally present a statistical analysis of the dataset in \u00a72.5."}, {"title": "2.1 Interaction scenario", "content": "In practical applications, error correction and response maintenance are two prevalent and significant scenarios. These scenarios encapsulate the essential dynamics between users and models, underscoring the importance of models' ability to adapt and respond effectively to user feedback.\nError Correction: Users may pose a query and find the model's response either objectively incorrect or unsatisfactory. Consequently, they provide feedback, expecting the model to acknowledge its response's inadequacies and offer an improved version.\nResponse Maintenance: Alternatively, when a user's query receives an objectively correct or satisfactory response from the model, they might still engage in feedback. This could be to either reaffirm or challenge the provided answer, aiming to verify the correctness and reliability of the information. The expectation is that the model will sustain its initial response upon receiving this feedback."}, {"title": "2.2 Hierarchical data taxonomy", "content": "A typical human-LLM interaction process comprises three components: the user's query, the model's response, and the user's feedback. To ensure comprehensive coverage of various potential interaction scenarios and interaction types, we develop an extensive three-tier hierarchical taxonomy from the perspective of these three components."}, {"title": "2.2.1 Query task", "content": "From the perspective of user queries, the diversity of interactions primarily stems from the task type associated with each query. Therefore, we select eight popular tasks to encompass most real-world usage scenarios. To enhance the diversity of queries further, we categorize the eight tasks into twenty-four subtasks, as detailed in Appendix A.1.1.\nMathematics tasks are frequently encountered in human-LLM interaction scenarios. Given the complexity of these problems, models often fail to provide accurate answers on their first attempt, necessitating collaboration between humans and models to resolve complex issues.\nReasoning tasks effectively reflect a model's logical capabilities, indicative of its overall performance. Strong logical abilities enable the model to excel in other complex tasks, making it a vital component of human-LLM interaction.\nCoding tasks evaluate a model's proficiency in comprehending and producing programming code, a capability that is becoming increasingly vital across a wide range of technology-oriented fields.\nText extraction tasks are pivotal for information retrieval, data analysis, and content summarization applications, involving the extraction of structured information from unstructured text or pinpointing specific content within extensive text volumes.\nText Error Correction tasks are pivotal in significantly enhancing the readability and overall quality of written content. By fixing errors from typos to grammar, these tasks make text accurate and clear, highlighting their key role in keeping written communication professional and intact.\nText creation tasks not only test the model's creativity and understanding but also play a crucial role in aiding people to express ideas more effectively and innovatively, enriching communication across various fields.\nKnowledge Q&A tasks assess a model's proficiency in delivering precise and pertinent responses to a wide array of queries.\nText translation tasks evaluate the model's proficiency in accurately translating text between languages, an essential capability in our progressively globalized world."}, {"title": "2.2.2 Model response", "content": "From the perspective of the model's response, it is either objectively correct or satisfies the user in response maintenance scenarios. In error correction scenarios, to enable more fine-grained research, we further categorize the deficiencies of model responses into the following five types:\n\u2022 Not following instructions: The response does not grasp or adhere to the given context, instructions, or format requirements.\n\u2022 Logical errors: The response contains mistakes in reasoning, calculation, or the application of concepts.\n\u2022 Incomplete answers: The response fails to fully address or resolve all aspects of a query.\n\u2022 Factual errors: The response includes incorrect or outdated information, encompassing grammatical and technical inaccuracies.\n\u2022 Unprofessional answers: The response lacks clarity, detail, or organization."}, {"title": "2.2.3 User feedback", "content": "From the perspective of user feedback, the interaction between humans and LLMs can be significantly influenced by the nature of the user feedback provided. We design a total of nine distinct types of feedback, comprising six for error correction and four for response maintenance, with one type overlapping between error correction and response maintenance. Table 1 provides a brief one-sentence description for each feedback within error correction and response maintenance scenarios."}, {"title": "2.3 Evaluation Protocol", "content": "Inspired by DRFR [18], we evaluate the quality of follow-up responses by decomposing the evaluation criteria into a series of criteria that constitute a checklist. Considering the efficiency and capabilities of LLMs, we adopt the LLM-as-a-Judge framework to evaluate the quality of response as previous works [12, 10]. Specifically, we employ GPT-40 to act as a judge, scoring the model-generated follow-up responses based on the checklist and a human-curated reference follow-up response.\nTo get a more fine-grained evaluation in error correction scenarios, we further set different weights for different criteria in the checklist, where a higher weight signifies greater importance and the sum of these weights equals 1. If the response meets any criterion in the checklist, it receives the corresponding points. For i-th sample in error correction scenarios,\n$$scorei = \\sum_{j=1}^{n}Wijrij$$\nwhere $w_{ij}$ is the weight of j-th criterion, $r_{i,j} \\in [0, 1]$ denotes whether the j-th criterion within i-th sample is met.\nIn response maintenance, since the model has already provided the correct answer in the previous round, it will get credits if it maintains its stance and is not swayed by the user's instructions. That's to say, meeting any criterion in the checklist yields a score of 1.\n$$scorei \\begin{cases} 1, & \\forall r_{i,j} = 1, j \\in [1, n]\\\\ 0, & otherwise. \\end{cases}$$"}, {"title": "2.4 Dataset Curation", "content": "Each sample in FB-Bench mainly contains a task-oriented user query, a preset model response, human feedback, a human-curated reference follow-up response, and a weighted checklist for evaluation.\nThe example can be found in Appendix A.1.3. The detailed construction pipeline is described as follows.\nCollection To ensure the diversity and authenticity of user queries, we mine relevant data from two primary sources: an online chat service and human preference data, both derived from real-world usage scenarios. For error correction data, we employ heuristic rules to identify target data within the online chat service and select the response with the lowest score from human preference data. For response maintenance data, we adopt an opposite strategy to filter the target data from the two data sources. After gathering the above data, we perform deduplication and anonymization, and categorize them into predefined tasks and subtasks using an in-house model to construct high task diversity data.\nAnnotation Although mined data exhibit high task diversity, the feedback from most users is usually simple and homogenous. To improve the quality and variety of user feedback and to supply essential elements for further analysis, we invite annotators to label data with finer granularity. Considering the excellent performance of LLMs in aiding humans to generate comprehensive critiques and reduce hallucination rates [3], we have annotators collaborate with GPT-4 to enhance the quality and efficiency of the annotation process. Initially, we utilize GPT-4 to ascertain the cause of dissatisfaction when a model's response does not meet the user's expectations and then simulate a user providing detailed feedback. Subsequently, GPT-4 is tasked with generating a reference follow-up response and a weighted checklist to facilitate the evaluation. Finally, the annotators act as the reviewers to refine all pre-annotated elements of each sample.\nPost-Filtering To enhance distinguishment in scores among LLMs, we utilize three models: Mistral-7B-Instruct-v0.3 [19], Phi-3-mini-4k-instruct [20], and Yi-1.5-9B-Chat [21] as difficulty filters in our dataset curation pipeline. Specifically, we benchmark these models using this dataset, analyze their responses, and score them by GPT-40. Finally, we discard samples for which all three models achieved full scores."}, {"title": "2.5 Dataset Statistics", "content": "After meticulous curation, we collect 734 high-quality, diverse, and complex samples. The distributions of tasks, deficiencies in previous model responses within error correction scenarios, and user feedback within both error correction and response maintenance scenarios are all shown in Figure 3. More detailed statistics can be found in Appendix A.1.2"}, {"title": "3 Experiments", "content": null}, {"title": "3.1 Experimental Setup", "content": "Models. Given the considerations of performance, size and the popularity of LLMs, we systematically evaluate a wide array of LLMs, including GPT family, Claude-3.5, Qwen-2.5 family, ERNIE-4, Moonshot, Yi, Gemma-2, Mistral, InternLM-2.5, DeepSeek, GLM-4, Phi-3 and LlaMa-3.1 family [22, 23, 24, 25, 19, 26, 27, 28, 20, 29].\nResponse generation. We employ the official settings and chat template in HuggingFace model card for open-source LLMs. Proprietary models are assessed via their official API endpoints, using the latest versions as of September 25, 2024. Considering the varied requirements for diversity and creativity across tasks, we set different temperatures for different tasks. More details can be found in Appendix A.2.1.\nEvaluation. We utilize gpt-4o-2024-08-06 to evaluate each generated follow-up response using the weighted checklist and the corresponding reference follow-up response. The evaluation prompt and cases can be found in Appendix A.2.2. To enhance the determinism of the judgment, we set the temperature to 0 and the output length to 4096."}, {"title": "3.2 Main Results", "content": "The subset evaluation results in FB-Bench are presented in Figure 4, with detailed results available in Appendix A.2.3. The main findings are as follows:\n\u2022 The ranking of LLMs exhibits significant variation between error correction and response maintenance, indicating an unrelated relationship between these two capabilities. For error correction, the ranking of LLMs aligns with people's perceptions, where closed-source LLMs significantly outperform open-source LLMs. The top three LLMs are closed-source, and Qwen2.5-72B-Instruct achieves the highest score among open-source LLMs but still lags significantly behind the leading closed-source LLM. Conversely, in response maintenance scenarios, the top four LLMs comprise two open-source LLMs: Qwen2.5-72B-Instruct and internlm2_5-20b-chat, with the former achieving the highest performance among all LLMs.\n\u2022 Some LLMs perform well in error correction yet struggle in response maintenance, indicating they are more susceptible to unreasonable user feedback. LLMs such as Deepseek-V2.5, claude-3-5-sonnet-20240620 and gpt-4o-mini-2024-07-18 exemplify this trend. Specifically, claude-3-5-sonnet-20240620 attains the highest scores in error correction but demonstrates relatively weak response maintenance capability, resulting in its lower ranking."}, {"title": "3.3 Analysis", "content": "Thanks to our comprehensive taxonomy, we can delve into several critical factors that significantly influence the performance of LLMs on FB-Bench, including interaction scenario types, task types, feedback types and deficiency types.\nMost LLMs exhibit superior performance in error correction compared to response maintenance. The performance discrepancy between these two scenarios is illustrated in Figure 5. Generally, LLMs excel in error correction but struggle to maintain consistency in their responses. This disparity is more pronounced among LLMs developed outside of China, even for leading LLMs such as gpt-4o-2024-05-13 and claude-3-5-sonnet-20240620. This suggests that the majority of LLMs lack a robust capacity to differentiate between valid and misleading instructions. This deficiency could stem from the fact that optimizing an LLM's instruction-following ability is relatively straightforward; however, overly adherent instruction-following can cause the LLM to distort reality.\nAdvanced LLMs show similar performance on each task in error correction. In contrast, their performance varies widely in response maintenance. We present the performance scores of several advanced LLMs across different tasks in Figure 6a and Figure 6b. In error correction scenarios, the scores of different LLMs on each task are relatively close, and they exhibit notably poorer performance on mathematics and reasoning tasks, with scores hovering around 60. Conversely, in response maintenance scenarios, the score discrepancies among different LLMs on each task are more pronounced, particularly in mathematics and reasoning tasks. Remarkably, claude-3-5-sonnet-20240620's ability in response maintenance is significantly weaker compared to other LLMs, especially in reasoning tasks, despite achieving the highest score in the error correction dimension.\nPointing out errors directly significantly helps LLMs enhance the quality of responses, while challenging LLMs with fabricated credentials or expertise often mislead them. We present the performance of several advanced LLMs under various types of human feedback in Figure 7. In error correction scenarios, all LLMs achieve scores higher than 70 when errors are identified by humans. In response maintenance scenarios, all LLMs exhibit poor performance when challenged by humans with fabricated credentials or expertise. Furthermore, it is observable that claude-3-5-sonnet-20240620 significantly outperforms all other LLMs when receiving unreasonable requests, indicating its superior safety capabilities.\nAdvanced LLMs outperform less capable LLMs in addressing all categories of deficiencies, particularly in logic and following instructions. To deeply investigate the performance disparities in error correction among models, we selected four models that exhibit significant differences in this aspect. Their performance across various deficiency types is depicted in Figure 6c. It shows that more advanced LLMs outperform less capable ones in all categories of deficiency. The primary challenges identified include correcting logical errors and following user instructions, where smaller models underperform even after receiving human feedback. This underperformance is likely attributable to the limited capabilities of smaller models."}, {"title": "4 Related Work", "content": "Evaluation of LLMs The evaluation of LLMs is essential for their development. It reveals the strengths and weaknesses of existing models and offers key insights and directions for future research. However, most existing studies [18, 8, 9, 11, 10] focus solely on evaluating the general or specific capabilities of LLMs in single-turn dialogues. They fail to assess LLM performance under various user feedback, which typically involves multi-turn dialogue scenarios. Although there are some benchmarks for multi-turn LLMs [12, 13, 14, 30, 31], the user inputs in the multi-turn dialogues are often independent, lacking feedback towards to the previous LLM output. Furthermore, much of the data in these multi-turn dialogue benchmarks is synthesized by LLMs, failing to exhibit the diversity and complexity of real-world scenarios.\nThe Importance of Feedback Human feedback not only enhances model performance but also serves as a critical mechanism for aligning the model with desired outcomes or goals [32]. Training models on feedback data, not only can directly enhance the quality of the generated content [33] but also allows models to better align with human preferences in style and tone [34]. During the inference stage, users can provide feedback on intermediate responses, enabling the model to refine its output until it achieves the user's satisfaction [4, 5]. However, a systematic benchmark for evaluating the impact of human feedback on LLMs during the inference stage is still lacking.\nBenchmarks with Feedback Several benchmarks have begun to explore the impact of feedback on LLMs. However, they predominantly focus on specific tasks or domains. MINT [15] exclusively assesses the coding and reasoning capabilities of LLMs that utilize tools and receive AI-generated language feedback. Intercode [16] evaluates the coding skills of LLMs based on feedback from compilers or interpreters executing the code. AgentBench [17] examines the reasoning and decision-making abilities of LLMs-as-Agents in response to environmental feedback. Different from prior works, FB-Bench introduces a novel approach by measuring the responsiveness of LLMs to diverse user feedback across a broad spectrum of real-world usage scenarios."}, {"title": "5 Conclusion", "content": "We introduce FB-Bench, a fine-grained muti-task benchmark for comprehensively evaluating the responsiveness of LLMs to various human feedback across real-world usage scenarios. A three-tier hierarchical taxonomy, grounded in two popular human-LLM interaction scenarios, is established to ensure thorough coverage of diverse interaction types and scenarios. To facilitate a fine-grained and accurate evaluation, a LLM-as-a-Judge framework, equipped with a weighted checklist, is employed. Benchmarking results from 31 well-known LLMs demonstrate significant performance variations between error correction and response maintenance. Further analysis explores the principal factors influencing the responsiveness of LLMs and provides valuable insights for subsequent research."}, {"title": "A Appendix", "content": null}, {"title": "A.1 Detailed description of the dataset", "content": null}, {"title": "A.1.1 Query subtask", "content": "We further categorize the eight tasks into twenty-four subtasks. Table 2 presents a brief one-sentence description of each subtask."}, {"title": "A.1.2 Detailed data statistics", "content": "The distribution of task and subtask categories is shown in Figure 8.\nThe length distribution of the four components of conversation in FB-Bench, namely, the user query, the preset model response, user feedback, and the reference follow-up response, is depicted in Figure 9."}, {"title": "A.1.3 Example in FB-Bench", "content": "We select an example from error correction and response maintenance scenarios, and display them in Figure 10 and Figure 11, respectively."}, {"title": "A.2 Detailed description of experiments", "content": null}, {"title": "A.2.1 Response generation", "content": "We utilize the vllm library [35] to deploy open-source LLMs for generating follow-up responses based on a user query, a predetermined model response, and human feedback. In terms of temperature settings, we assign distinct values for different tasks: 0.7 for text creation and text translation, 0.1 for knowledge-based Q&A, and 0 for all other tasks. For the maximum output length, we set it to the minimum value between 4096 and the difference between the LLM context length and the context tokens length."}]}