{"title": "MLGYM: A New Framework and Benchmark for Advancing AI Research Agents", "authors": ["Deepak Nathani", "Lovish Madaan", "Nicholas Roberts", "Nikolay Bashlykov", "Ajay Menon", "Vincent Moens", "Amar Budhiraja", "Despoina Magka", "Vladislav Vorotilov", "Gaurav Chaurasia", "Dieuwke Hupkes", "Ricardo Silveira Cabral", "Tatiana Shavrina", "Jakob Foerster", "Yoram Bachrach", "William Yang Wang", "Roberta Raileanu"], "abstract": "We introduce Meta MLGYM and MLGYM-Bench, a new framework and benchmark for evaluating and\ndeveloping LLM agents on AI research tasks. This is the first Gym environment for machine learning\n(ML) tasks, enabling research on reinforcement learning (RL) algorithms for training such agents.\nMLGYM-bench consists of 13 diverse and open-ended AI research tasks from diverse domains such as\ncomputer vision, natural language processing, reinforcement learning, and game theory. Solving these\ntasks requires real-world AI research skills such as generating new ideas and hypotheses, creating\nand processing data, implementing ML methods, training models, running experiments, analyzing\nthe results, and iterating through this process to improve on a given task. We evaluate a number\nof frontier large language models (LLMs) on our benchmarks such as Claude-3.5-Sonnet, Llama-3.1\n405B, GPT-40, ol-preview, and Gemini-1.5 Pro. Our MLGYM framework makes it easy to add new\ntasks, integrate and evaluate models or agents, generate synthetic data at scale, as well as develop new\nlearning algorithms for training agents on AI research tasks. We find that current frontier models can\nimprove on the given baselines, usually by finding better hyperparameters, but do not generate novel\nhypotheses, algorithms, architectures, or substantial improvements. We open-source our framework\nand benchmark to facilitate future research in advancing the AI research capabilities of LLM agents.", "sections": [{"title": "1 Introduction", "content": "Accelerating scientific discovery has been a long-standing ambition in artificial intelligence (AI) research,\nwith early initiatives like the Oak Ridge Applied Artificial Intelligence Project in 1979 exploring (Team,\n1985; Emrich et al., 1988; Johnson and Schaffer, 1994). More recent explorations enabled by advances in\nfoundation models (Achiam et al., 2023; Anthropic, 2024; Team et al., 2024; Dubey et al., 2024) provide\na proof-of-concept of a fully automated pipeline for end-to-end paper generation (Lu et al., 2024). In the\nfuture, we envision AI Research Agents capable of independently conducting literature search, generating\nscientific hypotheses, designing experiments, implementing new methods, analyzing results, disseminating\nfindings by writing scientific papers, and applying this research in products, thus assisting with all parts of\nthe research process. Such agents should be capable of both working fully autonomously, or be guided by\nhuman supervision, taking into account feedback from users.\nThis vision stems from the recognition that AI, with its capacity to process vast datasets and discern complex\npatterns, could accelerate scientific breakthroughs in areas such as drug discovery and materials science by\nidentifying promising drug candidates or predicting the properties of novel materials (Hessler and Baringhaus,\n2018; Schneider et al., 2020; Guo et al., 2021). Unlike traditional methods, AI agents can reveal hidden\ninterdisciplinary relationships by analyzing vast knowledge graphs, leading to novel insights and solutions\nfor complex challenges like climate modeling. By automating laborious tasks and exploring unconventional\navenues, AI agents can liberate scientists to focus on higher-level cognitive activities, ultimately driving\ninnovation and expanding the frontiers of knowledge. Machine learning (ML) research, with its emphasis on\nempirical validation and systematic experimentation in simulation, presents an ideal testbed for exploring and\nimproving the utlity of LLMs for advancing scientific research.\nHowever, the scientific method inherently relies on empirical validation, rigorous evaluation, and standardized\nbenchmarks to ensure the reliability and reproducibility of findings. While significant progress has been made\nin developing AI agents for various domains (Yang et al., 2024; Wu et al., 2024; Ma et al., 2024; Deng et al.,\n2023; Wang et al., 2023), we currently lack comprehensive frameworks and benchmarks specifically designed\nto assess their capabilities in conducting open-ended AI research tasks in diverse domains. This absence\nof standardized evaluation tools hinders our ability to objectively measure progress and identify areas for\nimprovement in this emerging field.\nRecently, a number of papers have started to evaluate LLM agents on various SWE and ML tasks; notable\nexamples include SWE-Bench (Jimenez et al., 2023), SWE-agent (Yang et al., 2024), ScienceAgent Bench (Chen\net al., 2024), SUPER (Bogin et al., 2024), MLE-Bench (Chan et al., 2024), MLAgent Bench (Huang et al.,\n2024), and RE-Bench (METR, 2024). However, existing benchmarks for AI Research Agents either do not\ninclude open-ended research tasks, or only cover a narrow range of research domains. In addition, existing\nframeworks are not designed to enable research on different training algorithms for AI Research Agents such as\nreinforcement learning, curriculum learning, or open-ended learning. Finally, current frameworks do not allow\nflexible artifacts to be evaluated (e.g. different outputs of the agent's research such as a model, algorithm, or\nset of predictions).\nIn this paper, we introduce MLGYM\u2014the first Gym (Brockman et al., 2016) environment for AI Research\nAgents and a unified framework designed to integrate diverse and open-ended AI research tasks into a single\nplatform for developing and evaluating LLM agents on such tasks (see Figure 1 for a diagram of MLGYM).\nBeing a Gym environment, our framework enables research on different training algorithms for AI Research\nAgents such as reinforcement learning (RL), curriculum learning, and open-ended learning. We also release\nMLGYM-Bench, a curated set of 13 open-ended research tasks, covering a wide range of domains such as\ncomputer vision, natural language processing, reinforcement learning, and game theory, carefully crafted to\nevaluate the performance of agents in realistic, multifaceted workflows. MLGYM and MLGYM-Bench expand\nthe range of problems considered by current LLM agent frameworks and benchmarks, by offering the ability\nto flexibly evaluate performance on open-ended research tasks. For example, performance can be measured\nbased on various artefacts such as model weights, RL training algorithms, or code representing game theory\nstrategies. We compare five frontier LLMs across the tasks in MLGYM-Bench under consistent experimental\nsettings, highlighting their strengths and limitations. Finally, we propose a new evaluation metric for agents,\nadapted from the optimization (Dolan and Mor\u00e9, 2002) and automated machine learning (AutoML; Roberts"}, {"title": "1.1 Capability Levels for Al Research Agents", "content": "We propose a hierarchical framework to categorize the capabilities of LLM agents for accelerating AI research.\nThis framework consists of six levels, each representing a distinct degree of autonomy and scientific contribution.\nLevel 0: Reproduction At this level, LLM agents can reproduce existing research papers either with or\nwithout access to the original code. This level demonstrates a basic understanding of the research domain\nand the ability to replicate established results.\nLevel 1: Baseline Improvement At Level 1, LLM agents can improve performance on a benchmark given a\nbaseline code that is not state-of-the-art (SOTA). This level indicates the ability to analyze and optimize\nexisting solutions, even if they are not the most advanced.\nLevel 2: SOTA Achievement At Level 2, LLM agents can achieve SOTA performance on a benchmark given\nonly a task description and access to the published literature before the invention of the SOTA approach, but\nno access to the SOTA paper or code. This level demonstrates the ability to come up with a solution to an\nopen research problem which is as good as the one found by humans.\nLevel 3: Novel Scientific Contribution At Level 3, LLM agents can make a novel scientific contribution, such\nas coming up with a new method that establishes a new SOTA on multiple benchmarks, and is worthy of\npublication at a top ML conference such as NeurIPS.\nLevel 4: Groundbreaking Scientific Contribution At Level 4, LLM agents can identify key research questions,\ndirections, solutions, and make a notable scientific contribution worthy of being published as an oral or best\npaper award at a prestigious ML conference such as NeurIPS.\nLevel 5: Long-Term Research Agenda At Level 5, LLM agents can pursue a long-term research agenda,\ncoming up with the research questions, directions, and solutions, continuously producing scientific discoveries\nover the span of weeks, months, or years. LLMs at this level should be capable of paradigm-shifting research\nbreakthroughs worthy of prizes such as Nobel or Turing.\nBy defining these capability levels, we provide a framework for evaluating frontier AI Research Agents.\nMLGYM-Bench focuses on Level 1: Baseline Improvement of the categorisation defined above."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Al Research Frameworks and Benchmarks", "content": "Table 1 shows a comparison between MLGYM and MLGYM-Bench with other related LLM agent frameworks\nand benchmarks. Below, we expand on the differences between MLGYM and these works.\nFirst, MLGYM is the first framework for AI Research Agents that provides a Gym interface, making it easy\nto integrate and train these agents using RL algoritms. MLGYM-Bench is also the first benchmark to include\ntasks that require research on algorithms in multiple domains such as RL, game theory, or SAT.\nSecond, MLGYM-Bench encompasses a wide range of open-ended AI research tasks, covering supervised\nlearning, language modeling, reinforcement learning, game theory and SAT. In contrast, SWE-Bench/SWE-\nAgent (Yang et al., 2024) focuses on solving Github issues so the code changes either fix the code or not\n(as opposed to optmization tasks with finer-grained metrics, such as a loss metric in a supervised learning\nproblem). Similarly, MLE-Bench (Chan et al., 2024) includes narrowly scoped machine learning tasks from\nKaggle competitions. While these tasks have a spectrum of quality levels, they tend to be already solved\nby current state-of-the-art methods. On the other hand, MLAgentBench (Huang et al., 2024) contains both\nML-specialized tasks (regression, classification, code speed improvements) and tasks focused on recent research\nchallenges (e.g. CLRS reasoning corpus (Veli\u010dkovi\u0107 et al., 2022), BabyLM challenge (Oba et al., 2023)).\nRE-bench (METR, 2024) also consists of broadly scoped ML engineering tasks which are hard to saturate and\nreward increasingly sophisticated approaches. ScienceAgentBench (Chen et al., 2024) incorporates data-driven\nscientific discovery tasks extracted from peer-reviewed publications, but which are so specific that they\nresemble Kaggle competition rather than open research questions.\nThird, MLGYM allows for flexible evaluation artifacts: it is sufficient to provide python code that the agent can\ncall to examine the quality of its current solution, such as a model checkpoint or an RL algorithm. In contrast,\nMLE-Bench requires a CSV file to be submitted for grading each question and SWE-Bench/Agent require\nevaluating a piece of code through a collection of unit tests. MLAgent Bench, RE-Bench and ScienceAgent Bench\nprovide Python scripts to compute the evaluation scores.\nFinally, MLGYM enables easy evaluation of both models and agents. To facilitate model evaluation, MLGYM\nprovides a default agentic harness that can be used out-of-the-box to evaluate any base model."}, {"title": "2.2 LLM Agents", "content": "Research on tool-augmented LLMS (Schick et al., 2023) has inspired a new research agenda of \"agentic\"\nLLMS (Kaddour et al., 2023; Wang et al., 2024a), where LLMs interact with an external environment.\nExisting work explores teaching LLMs to use tools or APIs (Schick et al., 2023; Qin et al., 2023), navigate\nthe web (Nakano et al., 2022; Deng et al., 2023; Zhou et al., 2023), interface with operating systems (Wu\net al., 2024), play games (Paglieri et al., 2024; Wang et al., 2023), or interact with other simulated (Wang\net al., 2024b; Lin et al., 2023) or physical worlds (Zhang et al., 2024a). Evaluating agentic LLMs typically\ninvolves designing controlled environments, providing suitable tools, defining tasks and goals, and establishing\nquantitative metrics to measure the system's performance.\nBuilding on these directions, Yoran et al. (2024) introduce AssistantBench, emphasizing the complexity of\nopen-web navigation and showcasing how current systems struggle with realistic, time-consuming tasks such\nas monitoring real-estate markets or identifying nearby businesses. Meanwhile, Kapoor et al. (2024) highlight\nthe importance of standardized evaluation protocols that consider both accuracy and cost, warning against\noverfitting and advocating for more reproducible benchmarks. Extending these concerns to multi-dimensional\nenvironments, Liu et al. (2023) propose AgentBench a suite of eight interactive settings that test agents'\ncapacity for reasoning, decision-making, and long-term instruction following. Similarly, Mialon et al. (2023)"}, {"title": "2.3 Agents for Software Engineering and Data Science", "content": "In line with the principle of reproducibility and verifiability, software engineering tasks provide a testbed\nfor LLM agents, where tasks can be tightly scoped and outcomes rigorously measured. Recent work has\nexplored how agents can tackle code-level challenges in controlled settings that permit systematic evaluation.\nAs discussed above, Yang et al. (2024) introduce SWE-agent, which operates within a constrained agent-\ncomputer interface to facilitate file creation, repository navigation, and code testing-thereby enhancing both\ntraceability and reproducibility on benchmarks such as SWE-bench and Human EvalFix. Similarly, Wang\net al. (2024c) describe OpenHands, a platform that restricts agent interactions to sandboxed environments for\nsafer command execution and verifiable web browsing, and in doing so provides a standardized foundation\nfor benchmarking. Magentic-One (Fourney et al., 2024) is another agentic system competent in software\nengineering but also augmented with web navigation capabilities, as demonstrated by its strong performance\non the GAIA, AssistantBench and WebArena (Zhou et al., 2023) agentic benchmarks. On the other hand,\nZhang et al. (2024b) achieve competitive perforemance on SWE-bench with AutoCodeRover, which, unlike the\nagentic approaches, solves Github issues by combining LLM-based programming with program representation\nas an abstract syntax tree.\nTowards the goal of automating data science work, Li et al. (2024) introduce AutoKaggle, a multi-agent\nhuman-assisting system, and Grosnit et al. (2024) present AgentK v1.0, an end-to-end autonomous data\nscience agent; both of these systems perform well on Kaggle competition data. Still within the realm of\ndata science work, Lei et al. (2024) build Spider 2.0, a challenging benchmark and code agent framework\nfor automating text-to-SQL workflows. Going one step further, Cao et al. (2024) introduce Spider 2-V, an\nautonomous multimodal agent coupled with a benchmark focusing on the automation of enterprise data\nscience and engineering workflows.\nMore search-oriented approaches include SWE-Search (Antoniades et al., 2024), a multi-agent framework\nthat marries Monte Carlo Tree Search (MCTS) with iterative refinement, enabling agents to continuously\nevaluate and improve their approaches to repository-level tasks. In a similar vein, Koh et al. (2024b) explore\ntree search for LLM agents and show that equipping LLM agents with best-first search boosts performane for\nthe WebArena and VisualWebArena (Koh et al., 2024a) agentic benchmarks. Also on augmenting LLM agents\nwith search, Yu et al. (2025) propose MCTS-based test-time search and self-learning techniques that yield\nbetter performance on VisualWebArena. Finally, Xia et al. (2024) demonstrate that even relatively simple\napproaches can excel when thoroughly monitored: an 'agentless' system follows a three-step process and\noutperforms more complex agent-based methods on SWE-bench Lite, underscoring the value of constrained,\nverifiable environments in driving reproducible gains for autonomous SWE agents."}, {"title": "2.4 Agents for Scientific Research", "content": "Controlled SWE contexts build the foundation for more complex automation while maintaining a reproducible\nand verifiable approach. However, just the software foundations alone are not sufficient to address the\nremaining gaps towards the goal of science acceleration. Going from the limited environments and well-defined\ntasks with metrics towards a less-defined area of open-ended questions, there are substantial efforts needed to\nboost the capabilities of research agents. For instance, coming up with automatable criteria to gauge scientific\nnovelty or constructing theories inheriting the automated findings from heterogeneous disciplines are examples\nof areas that could use more refinement and experimentation.\nNevertheless, the first steps on this path can be started now in the field of ML research and data science\nsince these areas represent for us a scientific playground with tasks that are both well-defined and have"}, {"title": "3 MLGYM", "content": "An LLM agent can perform ML research/development by interacting with a shell environment through a\nsequence of commands. Given a task description, some starter code and access to its action and observation\nhistory, the LLM generates appropriate shell commands to accomplish research objectives like generating\nideas, processing data, implementing new methods, training and evaluating models, analyzing the results, and\nreasoning about what experiments to run next. The agent is iteratively prompted to take actions based on\nthe task description and execution feedback from previous commands, allowing it to develop and self-refine\nthe solutions in-context.\nThe MLGYM framework provides a unified framework for evaluating and developing agents and models for AI\nresearch tasks. We take inspiration from long existing field of RL and build a GYM (Brockman et al., 2016)\nenvironment that can execute shell commands in a local docker machine shell. MLGYM provides access to\nfour core components: Agents, Environment, Datasets, and Tasks. MLGYM's modular design allows one\nto easily utilize and extend the library. For example, researchers can easily implement other agentic harnesses\nto improve performance, they can expand the environment by adding more tools for an agent, add more\ndatasets within a given task (e.g., if the task is image classification they could add ImageNet in addition to\nCifar-10), and they can even add more tasks to the MLGYM benchmark. Below, we discuss each component\nin detail."}, {"title": "3.1 Agents", "content": "The Agent class provided by MLGYM acts as a wrapper around a base LLM and provides functionality\nfor integrating various base models, history processors, and cost management. Moreover, unlike other\nframeworks (Huang et al., 2024; Yang et al., 2024), MLGYM separates the agent from the environment,\nallowing for easy integration of external agents. This also enables one to fairly compare different base models\ngiven the same agentic harness without the need of implementing their own agentic orchestration.\nThe agent is expected to take the history of all prior observations and actions as input and return the next\naction to take. The provided action is then passed to the environment, which executes the command and\nreturns the next observation based on the command output. The agent can execute any BASH COMMAND in\nthe environment. In addition, it has access to a set of tools (i.e., bash scripts such as editing a file) that it can\nuse similarly to any other bash command. MLGYM provides an agent adapted from SWE-Agent (Yang et al.,\n2024) as a default agentic harness. We describe the design and configuration of the tools in Section 3.5. The\nfull system prompt used can be found in Listing 1."}, {"title": "3.2 Environment", "content": "MLGYM environments are designed as Gymnasium (gym) environments (Towers et al., 2024). The environment\ncomponent is responsible for initializing a shell environment in a local docker machine, with all the required\ntools, installing task-specific python dependencies, copying all the necessary data and code in a separate\nagent workspace and managing interactions between the LLM agent and the system. Moreover, to support\nopen-ended research tasks and make the environment safe and flexible, MLGYM environment also manages\npermissions for various files and directories. Specifically, when running in a docker container, due to various\nsecurity concerns associated with using a root user, we create a non-root user named \"agent\" and set the\nappropriate permissions for the working directory.\nIn this work, we make a conscious decision to decouple tools and ACI as defined in SWE-Agent (Yang et al.,\n2024)1. Note that this ensures that the agent and environment are not tightly coupled, allowing for easier\nimplementation of other agentic architectures. Practically, this means that when the environment is initialized,\nit also initializes the tools in the working environment and a tool documentation is prepared which can be\nadded to the LLM agent's prompt. More details about the tools are provided in Section 3.5."}, {"title": "3.3 Datasets", "content": "MLGYM provides a simple abstraction for defining datasets through configuration files. It supports both\nlocally stored and Hugging Face datasets. We decouple the dataset definition from the task definition, so that\na single dataset can be used in multiple tasks. Similarly, a single task can have more than one dataset so\nthat the agent's code can be evaluated across all of them to demonstrate the generality of the implemented\nmethod.\nMoreover, if the dataset files are stored locally, the environment automatically copies the relevant files to the\nagent workspace with read-only permissions. This ensures that the agent cannot change the dataset files,\nwhich is important for reproducibility and cheating prevention.\nIf the dataset is stored in Hugging Face, the agent is given the dataset URL through the starter code or in\nthe prompt and asked to utilize it. Note that if the LLM agent fails to follow instructions or uses a different\ndataset, the evaluation code will not work or result in performance issues."}, {"title": "3.4 Tasks", "content": "We provide an easy abstraction to define any ML research task using configuration files. Each task can\nincorporate one or more datasets, custom evaluation scripts (with read-only access), task-specific conda\nenvironment, optional starter code, training timeouts, and memory management settings. This provides a\nflexible framework for defining diverse open-ended ML research tasks covering a wide range of difficulty. For\nexample, one can define an easier version of a task by providing a baseline code and a harder version by\nproviding no starter code or one with bugs, thus creating a natural curriculum.\nEvaluation is a critical component for any ML task. Every task requires a different evaluation protocol; thus,\nKaggle-style evaluation as done in MLE-Bench (Chan et al., 2024) where the agent is expected to submit a\nCSV file is not feasible for every problem. For example, in reinforcement learning settings, the evaluation\nartifact is a set of models trained on a set of pre-defined random seeds, which is then used to get a mean\nreward across a set of environment seeds. Similarly for Game Theoretic tasks, it can be a Python file with a\nstrategy function which will be evaluated against a fixed set of strategy functions. Since we aim to evaluate\nthe agent on open-ended and diverse tasks, it is not possible to convert all submissions to a CSV format.\nTo ensure extensibility to such open-ended tasks, the task definition is expected to provide an evaluation\nscript and submission artifact instructions. The LLM agent can then be prompted to follow the submission\ninstructions and write the appropriate code. Moreover, the evaluation script is read-only for the LM agent, so\nwhile it can inspect the evaluation format, it cannot modify the script to change the evaluation logic.\nExisting works such as Huang et al. (2024); METR (2024); Chen et al. (2024) also use a script based evaluation\napproach, whereas MLE-Bench (Chan et al., 2024) uses a Kaggle style evaluation.\nAll our design decisions for the Agent, Environment, Dataset, and Tasks are meant to reduce overhead on the\ndevelopers' and researchers' side and enhance reproducibility in this newly emerging area."}, {"title": "3.5 Tools and ACI", "content": "Augmenting LLM agents with the ability of using external tools is a critical component for making progress\non knowledge-intensive tasks. In this work, we extend the ACI (agent-computer interface) first introduced in\nSWE-Agent (Yang et al., 2024) with some additional features required for an ML research agent. Specifically,\nwe extend the commands for search, navigation, file viewer, file editor and context management with our\npermission management system and introduce new commands for literature search and a memory module.\nFor example, if the agent tries to open a file without read permission, the file viewer tool will generate textual\nfeedback for the agent. Similarly, if agent tries to edit the evaluation script (which is marked as read-only),\nthe edit tools will output a feedback string instead of failing silently. Literature search and the ability to\nmaintain a experimental log in it's memory are crucial for the agent to surpass SOTA solutions on open-ended\nresearch tasks.\nSimilar to SWE-Agent, tools are defined as bash or python scripts and are made available as bash commands\nin the environment.\nAll tool documentation is provided to the agent in the system prompt. See Table 2 for a description of the\navailable tools.\nValidation and Submit We provide two commands to the agent to validate the submission and submit the\nresults. Both the validate and submit commands are used to run the evaluation script and give the agent\nfeedback on its current score on the test set. However, while the submit command is a terminal action, i.e.,\nthe agent's trajectory is terminated, and the evaluation script is executed to log the final scores, the validate\ncommand can be used as many times as needed during the run to get the current performance on the test set.\nAddition of a validation command helps the agent to continuously improve its performance on the test set.\nLiterature Search and PDF Parser We provide the agent with two tools to find and extract knowledge from\nexternal sources. The Literature Search tool allows the agent to query the Semantic Scholar API to find\nresearch papers about a given query that have open-access PDFs available, and the PDF Parsing tool allows\nthe agent to download PDFs and convert them into a text-based representation. The paper contents can be\nstored in the context window as well as the Memory Module for longer-term tasks. Combined, these two tools\nallow the agent to find and analyze research papers as part of its workflow. See Table 2 for more information\nabout these tools and how they are called.\nMemory Module - Research Logs We introduce the Memory Module for MLGYM, an important tool to\nimprove the performance of agents on long-horizon AI research tasks. The Memory Module enables the agent\nto persistently store critical findings and successful training configurations using a structured memory system,\novercoming the challenge of limited context retention in long tasks. During our experiments, we observed\nthat when the agent has access to the memory module, it can retrieve the best training configuration from\nmemory and continue to iterate on it (see Figure 11 and Figure 12). Without the memory module, the"}, {"title": "4 MLGYM-Bench", "content": "The primary motivation behind our benchmark is to challenge models across different aspects of machine\nlearning, including data handling, model architecture, and strategic decision-making. By incorporating tasks\nfrom data science, game theory, computer vision, natural language processing, and reinforcement learning, the\nbenchmark aims to provide a varied and comprehensive agent evaluation testbed.\nThe tasks included in the benchmark are carefully selected to represent real-world challenges, ensuring that\nmodels are tested on their ability to generalize and perform effectively across various scenarios. Each task is\naccompanied by standardized evaluation scripts and baseline implementations, providing a clear reference\npoint for performance assessment and comparison.\nThe benchmark suite is structured into four main categories, each focusing on a specific domain of machine\nlearning: Data Science, Game Theory, Computer Vision, Natural Language Processing, and Reinforcement\nLearning. Below we describe each of the tasks in the benchmark."}, {"title": "4.1 Data Science", "content": "House Price Prediction (Kaggle, 2016) In the House Price Prediction task, the goal is to predict housing\nprices using the Kaggle House Price dataset. This task evaluates models based on their ability to accurately\npredict prices from various features, using RMSE and R2 as performance metrics. The baseline for this task\nis a simple Ridge Regression model with minimal feature engineering."}, {"title": "4.2 3-SAT", "content": "3-SAT (Cook, 1971) In the 3-SAT task, the LLM agent is given a DPLL code and is prompted to optimize\nthe variable selection heuristic. The associated DPLL code is stored in a read-only file, and the agent can"}, {"title": "4.3 Game Theory", "content": "We consider several tasks related to making strategic choices in iterated games", "u": "A \\rightarrow R^n$", "a_i": "H \\rightarrow \\Delta(A)$", "here": "iterated Prisoner's\nDilemma (Flood", "options": "cooperate or defect.\nWhen both cooperate, they receive a moderate reward. If one defects while the other cooperates, the defector\ngets a high reward while the cooperator gets a low payoff. If both defect, they both receive a low payoff.\nDue to the structure of payoffs, although mutual cooperation yields the best collective outcome, individual\nincentives often push towards defection. We included a repeated game, consisting of k = 20 rounds of the\ngame. In the repeated version, players remember previous interactions and can adjust their strategies based\non the history consisting of the past outcomes. Repeating the stage game multiple times allows for the\ndevelopment of trust and"}]}