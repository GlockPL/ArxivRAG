{"title": "DisCoRD: Discrete Tokens to Continuous Motion via Rectified Flow Decoding", "authors": ["Jungbin Cho", "Junwan Kim", "Jisoo Kim", "Minseo Kim", "Mingu Kang", "Sungeun Hong", "Tae-Hyun Oh", "Youngjae Yu"], "abstract": "Human motion, inherently continuous and dynamic, presents significant challenges for generative models. Despite their dominance, discrete quantization methods, such as VQ-VAEs, suffer from inherent limitations, including restricted expressiveness and frame-wise noise artifacts. Continuous approaches, while producing smoother and more natural motions, often falter due to high-dimensional complexity and limited training data. To resolve this \u201cdiscord\" between discrete and continuous representations, we introduce DisCoRD: Discrete Tokens to Continuous Motion via Rectified Flow Decoding, a novel method that decodes discrete motion tokens into continuous motion through rectified flow. By employing an iterative refinement process in the continuous space, DisCoRD captures fine-grained dynamics and ensures smoother and more natural motions. Compatible with any discrete-based framework, our method enhances naturalness without compromising faithfulness to the conditioning signals. Extensive evaluations demonstrate that DisCoRD achieves state-of-the-art performance, with FID of 0.032 on HumanML3D and 0.169 on KIT-ML. These results solidify DisCoRD as a robust solution for bridging the divide between discrete efficiency and continuous realism. Our project page is available at: Project Page.", "sections": [{"title": "1. Introduction", "content": "Human motion generation controlled by diverse signals has become a pivotal area in computer vision, driven by its vast applications in virtual reality to animation, gaming, and human-computer interaction. The ability to generate realistic human motions that are precisely aligned with input conditions-such as textual descriptions [11, 13, 44, 55], human speech [25, 27, 58], or even music [9, 20, 40]-is essential for creating immersive and interactive experiences. Two critical qualities define the success of such systems [49]: faithfulness, ensuring that the generated motion accurately reflects the conditioning signal, and naturalness, producing smooth and lifelike motions that are comfortable and convincing to human observers. A deficit in faithfulness can produce motions that are misaligned or irrelevant to the in-"}, {"title": "2. Related Work", "content": "The goal of human motion generation is to generate natural motions that adhere to control signals. Recent advancements in this field can be broadly categorized into two main approaches: motion generation from continuous representations, where models directly regress on continuous values, and motion generation from discrete representations, where motion is quantized into discrete tokens, transforming the generation task into a classification problem in the discrete domain.\nMotion Generation from Continuous Representation. Motion generation using continuous representations can be achieved through various approaches. Early methods for signal-to-motion generation primarily focused on mapping control signals to motions within a continuous representation space [2, 10, 22, 34, 43], often leveraging the Vari-ational Autoencoder (VAE) architecture [17] or utilizing CLIP features [38]. More recently, with the success of score-based models [14, 41], new advancements have emerged in generating motion through continuous representations [6, 21, 23, 29, 45, 46, 55, 59, 61, 62]. These approaches aim to model the inherently continuous nature of motion using a continuous representation space, making them well-suited to the task of motion generation. However, due to the data-constrained nature of the current motion generation tasks, the complexity of the continous representation space often makes it challenging to establish a reliable cross-modal mappings between control signals and motions, resulting in suboptimal performance.\nMotion Generation from Discrete Representation. Recently, to simplify the complex mapping between control signals and motions, some methods have reformulated the generation task as a discrete classification problem, achieving notable performance in motion generation [12, 13, 16, 25, 27, 35, 36, 40, 57, 58, 60]. These approaches often employ VQ-VAE [47] to create motion tokens, which are then used to generate motion sequences via token prediction models, such as autoregressive transformers. More recently, discrete diffusion models have been introduced to directly denoise these discrete tokens [7, 28]. While these methods effectively bypass complex signal-to-motion mapping challenges, their inherent characteristics\u2014such as information loss and discreteness-frequently result in unnatural artifacts, including under-reconstruction and frame-wise noise.\nOur method combines the inherent naturalness of continuous approaches with the signal faithfulness of discrete methods, achieving both natural and well-conditioned motion generation within the data-constrained motion generation task."}, {"title": "3. Method", "content": "In this section, we propose DisCoRD, a novel method for decoding pretrained discrete representations in the continuous domain using score based models. This allows the generated motions to: 1) capture dynamic, fast-paced motion due to iterative and stochastic nature of score-based models and 2) maintain the natural smoothness of motion by decoding in the raw motion domain, conditioned on discrete tokens. We begin by introducing score-based models, including rectified flow, then present the concept of motion tokenization. This is followed by an explanation of our conditional rectified flow model, conditioning projection module, and training details.\n3.1. Preliminaries\nScore-based Models. Given a data distribution px, score-based models [14, 41] define a forward diffusion process that progressively adds noise to the data xo ~ Px, eventually transforming it into x\u2081 ~ N(0, I). Typically, this process can be expressed as:\n$x_t = a_t \\cdot x_0 + \\sigma_t \\cdot \\tau, \\text{ where } t \\in [0,T].$ (1)\nHere, the choice of functions at and ot determines the trajectory that connects the data and noise. Score based model then learns the reverse diffusion process to recover the original data from noise. Recent works have introduced various parameterizations for at and \u03c3\u03b5 to enhance the reverse diffusion process. Beyond specific parametrization of at and \u03c3\u03c4, the iterative nature of score-based models significantly strengthens their capacity to capture complex data variations, particularly excelling in generating high-dimensional data.\nRectified Flow. Rectified flow [26] is a generative model developed to address the transport mapping problem by employing a flow matching algorithm [1, 24, 26]. Flow matching focuses on constructing a transport map, denoted as T : Rd \u2192 Rd, that effectively maps observations from source distribution x\u03bf ~ \u03c0\u03bf on Rd to observations from target distribution X1 ~ \u03c0\u2081 on Rd. This transport map is formalized via the ordinary differential equation (ODE):\ndxt = v(xt, t) dt. (2)\nIn this context, v is typically referred to as a vector field, while xt represents the forward process parameterized over t\u2208 [0, 1]. Rectified flow specifies a parametrization of Equation (1) such that the forward process follows a straight path. This trajectory is expressed as xt = tx1 + (1 - t)xo, with v defined as (x1-xo). Since the target x\u2081 is unknown during the generation phase, a simple least squares regression problem:\n$\\min_v \\mathbb{E} \\int_{0}^{1} [|| (x_1 - x_0) - v(x,t)||^2] dt,$ (3)\nwith xt = tx1 + (1-t)xo."}, {"title": "4. Experiments", "content": "In this section, we evaluate the effectiveness of DisCoRD in achieving motion naturalness compared to other discrete methods. We begin by assessing the naturalness of reconstructed motions to highlight the expressive capabilities of our score-based decoder. Then, we examine how this naturalness carries over to stage 2, generating natural motions while preserving faithfulness. We focus on text-to-motion generation due to its complex motions and diversity, but also evaluate our approach on other motion generation tasks, including co-speech gesture generation and music-to-dance generation, demonstrating the flexibility of our method.\n4.1. Dataset and Evaluation\nDatasets. For the text-to-motion task, we employed HumanML3D [11] and KIT-ML [37]. HumanML3D is a 3D motion dataset with language annotations, including 14,616 motion sequences paired with 44,970 text descriptions averaging 12 words. Sourced from motion capture data, motions are standardized to a skeletal template, scaled to 20 FPS, and cropped to 10 seconds if longer. Each sequence has at least three descriptive texts covering diverse actions. KIT-ML is a smaller dataset with 3,911 motion sequences paired with 6,278 text descriptions averaging 8 words. Motion capture data are downsampled to 12.5 FPS, with 1\u20134 descriptions per sequence. For co-speech gesture generation, we utilize the SHOW [58] dataset, while a mixed version of AIST++ [20] and HumanML3D is used for music-to-dance generation. Further dataset details are provided in the Supplementary Section B.\nEvaluation. We evaluate DisCORD on both motion reconstruction and motion generation separately. For motion reconstruction, the primary objective is to assess how effectively the decoder reconstructs motion from tokens. This is measured by Fr\u00e9chet Inception Distance (FID), which assesses motion realism by comparing the feature distributions of generated and ground truth motions, and Mean Per Joint Position Error (MPJPE), which quantifies positional accuracy. For text-to-motion generation, we follow prior works [44] and we employ several established metrics: FID; R-Precision, which measures retrieval accuracy by matching generated motions to corresponding text descriptions and reports Top-1, Top-2, and Top-3 accuracies; Multimodal Distance (MM-Dist), which calculates the average feature distance between generated motions and associated text descriptions, capturing cross-modal alignment; and Multimodality (MModality), which assesses the model's capacity to produce diverse motions from a single text prompt by averaging distances between multiple generations for the same prompt. For co-speech gesture generation, we employ Fr\u00e9chet Gesture Distance (FGD) [31], and for music-to-dance generation, following [46], we utilize Distk and Distg, to assess the"}, {"title": "5. Conclusion", "content": "In this paper, we presented DisCoRD, a novel approach to human motion generation that effectively combines the naturalness of continuous representations with the faithfulness of discrete quantization methods. To demonstrate gains in naturalness, we also introduce symmetric Jerk Percentage Error (sJPE), specifically designed to capture subtle artifacts overlooked by traditional metrics. Extensive experiments across text-to-motion, co-speech gesture, and music-to-dance tasks demonstrate that DisCoRD consistently achieves state-of-the-art performance, providing a versatile solution adaptable to various discrete-based motion generation frameworks.\nLimitations. Since our method fundamentally builds on baseline models, it inherits certain limitations associated with these models. For instance, if the baseline model cannot generate variable motion lengths, our approach, which shares the same token generation model, may also be restricted by this limitation. Additionally, our current method requires a pretrained model as a starting point. However, our framework is inherently designed to support end-to-end training, allowing for the possibility of constructing an entirely new encoder from scratch. We leave this potential expansion as an avenue for future work."}, {"title": "A. Implementation Details", "content": "Table 6 provides an overview of the implementation details for our method. These configurations were employed to train the DisCORD decoder using the pretrained Momask [13] quantizer. Specifically, the 512-dimensional codebook embeddings from MoMask are projected into the conditional channel dimension. This projection is concatenated with Gaussian noise of the same dimensionality as the output channel. The concatenated representation is subsequently projected into the input channel dimension of the U-Net architecture. The U-Net processes this input and transforms it back into the output channel dimension, generating the final output shape. For training, we used an input window size of 64 and trained the model for 35 hours on a single NVIDIA RTX 4090 Ti GPU."}, {"title": "B. Datasets and Evaluations", "content": "In this section, we provide additional explanations regarding the co-speech gesture generation and music-to-dance generation tasks that we were unable to describe in detail in the main paper.\nB.1. Datasets.\nFor the co-speech gesture generation task, we utilized the SHOW dataset [58], a 3D holistic body dataset comprising 26.9 hours of in-the-wild talking videos. For the music-driven dance generation task, we used a mixed dataset combining AIST++ [20] and HumanML3D [11] where AIST++ is a large-scale 3D dance dataset created from multi-camera videos accompanied by music of varying styles and tempos, containing 992 high-quality pose sequences in the SMPL format.\nB.2. Evaluations.\nTo evaluate the co-speech gesture generation task, we used Frechet Gesture Distance (FGD) [31], which measures the difference between the latent distributions of generated and real motions. Since our focus is on body movements, we reported body FGD, which quantifies differences specifically for the body part, for ProbTalk [27]. For TalkSHOW [58], which only utilizes holistic FGD-a metric that measures differences across the entire motion, including the face and hands-we reported the holistic FGD. To evaluate the music-to-dance generation task, we utilized Distk, which quantifies the distributional spread of generated dances based on kinetic features, and Distg, which does the same for geometric features, as proposed in [46]. A smaller difference between the distributions of the generated motion and the ground truth motion indicates that the Distk and Distg values of the generated motion align closely with those of the ground truth, reflecting a similar level of distributional spread."}, {"title": "C. Additional Analysis on sJPE", "content": "To evaluate the sample-wise naturalness of reconstructed motions, we introduce the symmetric Jerk Percentage Error (sJPE), as defined in Equation 5 of the main paper. We present detailed formulations of Noise sJPE and Static sJPE, supported by analysis using generated motion samples. Furthermore, qualitative comparisons highlight the effectiveness of DisCoRD against state-of-the-art discrete methods. Finally, we investigate the alignment of sJPE with human preference to validate its perceptual relevance.\nC.1. Visualization of Fine-Grained Motion\nTo analyze fine-grained motion trajectories, we follow a three-step procedure. First, we select a joint for visualization, typically hand joints due to their high dynamism, and track their positional changes over time. Second, we apply a Gaussian filter to smooth the trajectory, reducing noise. Finally, we compute the difference between the smoothed and original trajectories to isolate fine-grained motion components. This method allows for detailed evaluation of frame-wise noise and under-reconstructed regions in motion trajectories. The visualizations in Figure 5 of the main paper and the qualitative samples in the supplementary material are generated using this process.\nC.2. Details on sJPE.\nWithin the symmetric Jerk Percentage Error (sJPE), we define two components: Noise sJPE and Static sJPE. These isolate the instances where the predicted jerk overestimates or underestimates the ground truth jerk, respectively.\nNoise sJPE and Static sJPE. Noise sJPE captures the average overestimation of jerk in the predicted motion signal, meaning frame-wise noise, corresponding to cases where Jpred,t > Jtrue,t. It is defined as:\n$Noise SJPE = \\frac{1}{n}\\sum_{t=1}^{n} \\frac{max (0, J_{pred,t} - J_{true,t})}{|J_{true,t}|+|J_{pred,t}|}$ (6)\nThe operator max(0, x) ensures that only positive differences contribute to Noise sJPE, separating overestimations from underestimations.\nNoise sJPE can be seen on the red box of Figure 9. Time steps where motion trajectory is noisy compared to ground truth motion show bigger jerk. The area under the predicted jerk and above the ground truth jerk, shown in blue area, is proportional to the Noise sJPE, meaning frame-wise noise.\nStatic sJPE measures the average underestimation of jerk, meaning lack of dynamism in the predicted motion, corresponding to cases where Jpred,t \u2264 Jtrue,t. It is defined as:\n$Static SJPE = \\frac{1}{n}\\sum_{t=1}^{n} \\frac{max (0, J_{true,t} \u2014 J_{pred,t})}{|J_{true,t}|+|J_{pred,t}|}$ (7)\nStatic sJPE can be seen on the green box of Figure 9. Time steps where motion trajectory is under-reconstructed compared to ground truth motion show smaller jerk. The area above the predicted jerk and under the ground truth jerk, shown in red area, is proportional to the Static sJPE, meaning under reconstructed motions.\nThe overall sJPE can be expressed as the sum of Noise sJPE and Static sJPE:\nsJPE = Noise sJPE + Static sJPE. (8)\nThese formulations provide a measure of prediction accuracy by separately accounting for the tendencies of the predictive model to overestimate or underestimate the true motion jerks.\nC.3. Qualitative Results on Joint Trajectory and Jerk\nWe present a series of figures demonstrating the effectiveness of DisCoRD in reconstructing smooth and dynamic motion. For each sample, the first row visualizes the motion trajectory, while the second row plots the corresponding jerk at each time step, with the calculated sJPE displayed"}, {"title": "D. Additional Quantitative Results", "content": "D.1. Performance on Text-to-Motion Generation.\nIn Table 7, we present a comparison of our method against additional results from various text-to-motion models. Our method consistently achieves strong performance on the HumanML3D and KIT-ML [37] test sets, even when evaluated alongside these additional models. While ReMoDiffuse achieves particularly strong performance on KIT-ML, it is worth noting that its performance benefits from the use of a specialized database for high-quality motion generation, which makes direct comparisons less appropriate.\nD.2. Performance on Various Tasks.\nIn Table 8, we present additional evaluation results for co-speech gesture generation. Following [58], we additionally report Diversity, which measures the variance among multiple samples generated from the same condition, and Beat Consistency (BC), which evaluates the synchronization between the generated motion and the corresponding audio. In Table 9, we provide additional evaluation results for music-to-dance generation. Following [40], we report FIDk to measure differences in kinetic motion features and FIDg for geometric motion features. Additionally, we include the Beat Align Score (BAS) to assess the synchronization between motion and music. While [46] has shown that these metrics are not fully reliable and often fail to align with actual output quality, we include them to follow established conventions.\nE. Additional Qualitative Results\nIn Figure 13, we present additional qualitative comparisons between our model and other leading approaches. In Figure 14, we additionally display more qualitative results of our method. We observed that our method effectively follows the text prompts while maintaining naturalness in the generated outputs. Again, we highly recommend viewing the accompanying video, as static images are insufficient to fully convey the intricacies of motion."}]}