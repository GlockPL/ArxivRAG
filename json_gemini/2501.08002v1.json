{"title": "Maximizing Uncertainty for Federated learning via Bayesian Optimisation-based Model Poisoning", "authors": ["Marios Aristodemou", "Xiaolan Liu", "Yuan Wang", "Konstantinos G. Kyriakopoulos", "Sangarapillai Lambotharan", "Qingsong Wei"], "abstract": "As we transition from Narrow Artificial Intelligence towards Artificial Super Intelligence, users are increasingly concerned about their privacy and the trustworthiness of machine learning (ML) technology. A common denominator for the metrics of trustworthiness is the quantification of uncertainty inherent in DL algorithms, and specifically in the model parameters, input data, and model predictions. One of the common approaches to address privacy-related issues in DL is to adopt distributed learning such as federated learning (FL), where private raw data is not shared among users. Despite the privacy-preserving mechanisms in FL, it still faces challenges in trustworthiness. Specifically, the malicious users, during training, can systematically create malicious model parameters to compromise the models' predictive and generative capabilities, resulting in high uncertainty about their reliability. To demonstrate malicious behaviour, we propose a novel model poisoning attack method named Delphi\u00b9 which aims to maximise the uncertainty of the global model output. We achieve this by taking advantage of the relationship between the uncertainty and the model parameters of the first hidden layer of the local model. Delphi employs two types of optimisation, Bayesian Optimisation and Least Squares Trust Region, to search for the optimal poisoned model parameters, named as Delphi-BO and Delphi-LSTR. We quantify the uncertainty using the KL Divergence to minimise the distance of the predictive probability distribution towards an uncertain distribution of model output. Furthermore, we establish a mathematical proof for the attack effectiveness demonstrated in FL. Numerical results demonstrate that Delphi-BO induces a higher amount of uncertainty than Delphi-LSTR highlighting vulnerability of FL systems to model poisoning attacks.", "sections": [{"title": "I. INTRODUCTION", "content": "The advancement of Al needs to address user concerns regarding privacy and the trustworthiness of algorithms [1]. A comprehensive framework to evaluate attacks that compromise the reliability of these algorithms is thus essential. With the enforcement of the European Union's AI Act [2], AI algorithms are regulated based on the criticality of their applications and the risks posed by their predictions, making trustworthiness a key evaluation criterion.\nTrustworthiness in AI models encompasses several aspects, such as explainability, interoperability, and accuracy [3], all crucially tied to the quantification of uncertainty. This uncertainty, stemming from model parameters, input data, and prediction outcomes, significantly influences the confidence level in model predictions [4]. By analyzing both epistemic uncertainty (model parameters) and aleatoric uncertainty (input data), we can gauge the uncertainty of the model and reveal the reliability of model predictions and the quality of training data [5], [6].\nBeyond model reliability, privacy protection remains a paramount concern, particularly in sectors like healthcare where institutions handle sensitive patient data. Federated Learning (FL) offers a viable solution by enabling the development of generalizable models without direct data sharing among parties. Through FL, multiple data owners, under the coordination of a central server, collaboratively train models while keeping the raw data localized, thereby enhancing both security and privacy [7]\u2013[9]. FL's utility spans various domains, including healthcare [10], resource allocation [11], and digital twins [12], showcasing its broad applicability.\nHowever, the deployment of deep learning also introduces the risk of adversarial attacks [13]\u2013[15].L systems are particularly vulnerable to such threats, especially when malicious participants are involved in the learning process [16]. Such attacks aim to manipulate the local or global model parameters to cause erroneous predictions, low confidence in the model output [17], and data reconstruction [18]. An example of how critical a poisoning attack in FL is the poisoned model parameters in an Internet of Things (IoT) network can compromise the trusted entities in FL and adversaries could gain control of the production chain [19]. Adversaries may even gain critical information about the network by getting access to the devices. Another critical issue is that the adversaries gain the users' privacy-sensitive data (e.g., patients' data and social media account data), such as in Metaverse applications. Distributed learning is a driving force for the adaptation of the Metaverse, however, malicious users through the gradient updates can steal privacy-sensitive data [20].\nTherefore, it is important to emulate the behaviour of an adversary in FL to understand its effects on the learning of the global model and related local personalisation. This will provide insights for designing and testing an appropriate defence for FL. Although accuracy is the most commonly used metric to measure how well an DL model can perform in categorising input samples, it neither measures nor encodes any information about the confidence levels of the underlying model. Concequently, we use uncertainty in order to quantify the impact of the model poisoning attack on the global model.\nThe existing literature shows that FL is susceptible to poisoning attacks when there are malicious users who can manipulate the data or the model parameters [16].Generally, there are three types of adversarial attacks against FL, including model poisoning [17], [21], [22], data poisoning [23], [24], and backdoor attacks [25], [26]. In this paper, we focus only on model poisoning attacks. This is because the other two attacks, data poisoning and backdoor attacks are not sophisticated and they can be detected by a variety of defences and personalisation techniques, such as [27]. Specifically, the clients in [27] are carefully regularising the weights of the global model into the personalised model.\nExisting model poisoning attack approaches are optimised under a single objective. For example, [22] aims to minimise the distance from the original parameters through manipulating the loss function, [17] computes a vector that changes the direction of the gradients for each model parameter such that the global model will deviate to the wrong direction, [28] perturbs the gradients during the weight update to maximise the L2 norm between the benign and malicious gradients. All these works only consider a single optimisation objective and focus on manipulating the gradients. Even though gradients can show the sensitivity between input and output and inform the neurons how well they are performing, they do not consider measuring the uncertainty. Thus, previous studies omit the utilisation of uncertainty as an attack objective and the study of related attack effects on the global and local models in FL unexplored.\nThis paper studies the relationship between aforementioned uncertainty and model poisoning attacks in FL and general DL model. We consider that there is a relation that describes the uncertainty induced by the attackers and the modification of the DL model. To achieve this, we propose a strategy named Delphi for model poisoning attacks. Specifically, with Delphi, we search for the optimal model parameters for inducing uncertainty in the DL model using, either white-box optimisation based on Least squares trust region, or black-box optimisation based on Bayesian Optimisation. In addition, we investigate two different ways of modifying the model parameters: 1) keeping a fixed set of neurons per round; or 2) continuously searching for the most significant neurons to generate sufficient attacks to the FL. As shown in Fig. 1, Delphi identifies the optimal weights of the first hidden layer of the deep neural network using Bayesian Optimisation or Least squares Trust region. Then, the uncertainty will be induced in the model parameters through the reparametisation of the first hidden layer. Here, instead of manipulating all the parameters in this layer, we manipulate only a small set of neurons. This set of neurons is chosen based on the sensitivity of the neurons on the data input and model's output measured with the L2 norm of the gradient. Similarly, we have chosen the first hidden layer because it has the higher sensitivity between the input and the model's output [29]. Our numerical results confirm that the attack is more successful when selecting the most significant neurons that will affect the model's prediction in each training round, rather than manipulating the fixed neurons all the time. In addition, we provide a mathematical analysis on measuring the effectiveness and the susceptibility of the model against adversarial attacks.\nThe main technical contributions are:\n1) We propose a novel model poisoning mechanism (Del-phi), where we evaluate two different types of algo-rithms including white-box (Least Squares Trust Region Section IV-A) and black-box (Bayesian Optimisation Section IV-B) optimisation to formulate attack strategies. To the best of our knowledge, this is the first work to use both optimisation methods to search for the optimal model parameters for attacks.\n2) With mathematical analysis of attack effectiveness on FedAvg, we provide the foundations for measuring the attack effectiveness (Section V-C) of model poisoning attacks. We prove that Delphi-BO which chooses the most significant neurons for performing attack in every training round, scores a higher attack effectiveness than manipulating a fixed set of neurons.\n3) Through the comparison between Dephi-LSTR (Section IV-A) and Delphi-BO (Section IV-B), we demonstrate that the model parameters searched by the BO increases the susceptibility of the model against model poisoning attacks (Section VI-A). Besides, when we are attacking FL with Delphi-BO, the mean predictive confidence is reduced by half."}, {"title": "II. RELATED WORK", "content": "There is a variety of attacks studied in FL such as data poi-soning attacks, backdoor attacks, and model poisoning attacks [16]. Data poisoning attacks aim to poison the model through the manipulation of the training data to interfere decision boundary. Methods, such as label-flipping, can mislead the local model [23], or in [30] where the authors are using a Generative adversarial network (GAN) to modify the labels of malicious samples. The backdoor attack aims to trigger malicious, incorrect, or unexpected outputs when the specific inputs designed by the attacker are provided to the model. This includes adding a small patch in images with the correct label in order to trigger the backdoor in testing time [25]. Another example is the training of a similar model to replace the global model through reducing learning rate and the customisation of the loss function in order to deviate the weights through the backpropagation [26]."}, {"title": "III. PROBLEM FORMULATION", "content": "In this section, we describe the importance of developing tools and mechanisms to emulate the adversaries' behaviour in the context of distributed learning. Through the proposed methodology, we provide the foundations for proving how the attacker can affect the global model effectively. This is achieved through the manipulation of the first layer of the DL model aiming to induce uncertainty. In order to present our attack, we have to define the threat model and the objective."}, {"title": "A. System Model", "content": "FL is a decentralized deep learning framework where $K$ clients collaborate to train a global model parameterized by $w$ without sharing the local private data $D = {D_1, D_2, . . ., D_k}$, where $D_k$ represents the local data that a client $k$ holds."}, {"title": "B. Threat Model", "content": "In this paper, we poison the local model in order to alter the global models such that to maximise uncertainty. In this scenario, we assume that the attacker is part of the FL process and has all the updates from the global model. In addition, the attacker's manipulated model is in such a way that the server will not detect any anomalies from the attacker's model during early training and the attacker will remain in the training process. We assume that in the adversarial scenario, there are $A$ number of attackers in a configuration of $K$ clients. We can modify eq.(2) and rewrite it as\n$w = \\frac{1}{N+A} (\\sum_{n=1}^{K-A} W_n + \\sum_{a=N+1}^{K} W_a)$,\nwhere $N$ is the number of normal clients and $A$ is the number of malicious or adversarial clients. Given the above formulation, we can formulate the attackers' objective. The attackers objective is to maximise the uncertainty $u(\u00b7) \u2208 R^d$ in the local model $w_a$ to influence the global model and further affect the local updates of the benign clients in the next training rounds. This is given by,\n$\u03b8_\u03b1 \u2190 \\underset{\u03b8_\u03b1}{\\text{max }} u(y, x \\vert w_a), \\text{ s.t. } {y, x} \u2208 D_k,$\nwhere ${y,x}$ is the training set of the client, and $\u03b8_\u03b1$ is the malicious client's poisoned parameters of the first hidden layers derived using the proposed Delphi. Delphi searches for the optimal parameters to modify the layer using optimisation techniques (Bayesian Optimisation and Least Squares Trust Region) and deploys the poisoned parameters in the first hidden layer of the convolutional neural network (CNN). The use of the first hidden layer is because of the high sensitivity between the input and the output of the neural network. The sensitivity is calculated using the L2 norm of the gradient and we select the top ranked neurons."}, {"title": "C. Delphi's optimisation objective", "content": "The optimisation objective utilises Kullback-Leibler diver-gence ($D_{KL}$) to measure the statistical distance between the target probability distribution $Z$ and the predicted probability distribution $\\hat{y}$ with input data $x$ and parameter $\u03b8$ for the neuron(s). Therefore, a malicious user aims to minimise the KL divergence in order to obtain the optimal weights. This objective is expressed as follows:\n$\\underset{\u03b4\u03c9}{\\text{min }} D_{KL}(\\hat{y} \\vert x, \u03b8_i||Z), \u03b4\u03c9\u2190 ||W_{t+1} - W_t||_1,$\nwhere $\u03b8_i$ is new weight parameter for the neuron(s) at round $i$, and $\u03b4\u03c9$ is the constraint of the absolute distance between the previous weights $w_t$ and the new weights $w_{t+1}$. In (5), the $Z$ is constructed using a discrete probability distribution in which the highest probability for the actual class is set to 0.25, and the probability of the rest of the classes is distributed uniformly."}, {"title": "IV. METHODOLOGY", "content": "In this section, we present our methodology for layer op-timisation to achieve model poisoning attack against FL. Our methodology called Delphi, utilises two types of optimisation techniques, Least square Trust Region and Bayesian optimi-sation in order to benchmark which methodology is more efficient and successful. The objective for both optimisation techniques is to obtain optimal layer parameters that can maximise the uncertainty of the model output. We assume that there is a black-box function between the layer parameters of the most significant features of the first layer in the neural network and the uncertainty induced in the model. Since there is no empirical expression between uncertainty and poisoning parameters, we treat the problem in two different contexts. With Bayesian Optimisation technique we treat the uncertainty and model poisoning as a black-box function, and with Least Square Trust Region optimisation, we treat it as a convex optimisation.\nNext, the key attack mechanisms of theses two algorithms, and the pipeline of the attack will be presented. We first introduce the preliminaries of the Least Square Trust Region method and the Bayesian optimisation, and show how they could be employed to search for the optimal parameters. Finally, the attack pipeline will be presented"}, {"title": "A. Delphi - Least Square Trust Region (Delphi-LSTR) Method", "content": "Instead of using the intuition of black-box optimisation, and given the fact that the formulated attack problem has a non-linear relation between the input and output, we use Least Square minimisation. For the implementation, we use the function of scipy.optimise in order to find the optimal value for the weights. In the following section we explain how the LSTR derives a new parameter $\u03b8$ for the selected neurons."}, {"title": "1) Optimisation Algorithm:", "content": "The optimisation of the Least square in scipy uses the trust region reflective methods [33], [34] to solve a system of equations containing the first-order optimality condition for a bounded minimisation problem as specified below:\n$\\underset{xeR^d}{\\text{min }} f(\u03b8), l < \u03b8 \u2264 u,$\nwhere $f(\u00b7)$ is the optimisation model which in our case is the quantification of uncertainty based on $\u03b8$ parameterised with $\u03b8$, $l$ and $u$ are the lower and upper boundaries, and $\u03b8$ is the candidate which is the weight parameter we search with Delphi-LSTR.\nThe new candindate $\u03b8_{t+1} = \u03b8_t+s_t$, where $s_t$ is the small step taken by the algorithm at iteration $t$, is being calculated based on the trust region method which typically is ball-bounded and the radius is being controlled by $\u2206$ per iteration $t$. In addition, the algorithm uses the subspace subproblem defined as\n$\\underset{sER^d}{\\text{min }} \\psi_t(s): ||J_ts||_2 \u2264 \u2206_t,$\nwhere, $s\u2208 S_t$ is the step for the new $\u03b8_{t+1}$ derived from subspace $S_t \u2208 R^d$. To solve the minimisation problem of eq. 7, we need an equation that describes $\u03c8(s)$, which in this case is a quadratic function. According to [33], [34], the $\u03c8(s)$ is given as follows,\n$\\psi(s) = g^Ts + \\frac{1}{2}s^T(H_t + C_t)s,$\nwhere, $g_t$ is the traspose matrix of $\u2207 f(\u03b8_t)$, $H_t$ is the hessian matrix $\u2207^2 f(t)$, and $C_t$ is given by $J_t diag(g_t) JJ^T$.\nThe solution of eq. 7 gives us the step $s_t$ which will be added to the $\u03b8_t$. Note that, at $t = 0$ the initial point $\u03b8_0$ is the initial weight of the neuron. Once the $s_t$ is obtained we need to check whether it satisfies the first order of optimality which is defined as follows,\n$\u03b2_t = \\frac{f(\u03b8_t + s_t) \u2212 f(\u03b8_t) + \\frac{1}{2}s_t^T C_ts_t}{\\psi(S_t)}$\nwhere $r_t$ is the first order of optimality and determines whether the new point moves to the optimal solution. The value of $\u03b2_t$ must be > 0, $\u03b2_t > \u03bc$ and $\u03b2_t < \u03b7$, where $\u03bc$ is the tolerance for loss, and $\u03b7$ is the accuracy.\nDuring the iterations until the convergence is achieved, the trust region size $\u2206$ is adjusted by two discount factors $\u03b3_1 < 1 < \u03b3_2$ based on the range of $\u03b2_t$. Therefore, the $\u2206_{t + 1}$ as follows,\n\u2022 if $\u03b2_t \u2264 \u03bc$, the $\u2206_{t+1}$ is set in range $(0, \u03b3_1\u2206_t]$\n\u2022 if $\u03b2_t \u2208 (\u03bc, \u03b7)$, the $\u2206_{t+1}$ is set in range $(0, \u03b3_1\u2206_t]$\n\u2022 if $\u03b2_t \u2265 \u03b7$, depends if $\u2206_t$ is less or greater than the lower bound\nif $\u2206_t > lower bound$, $\u2206_{t+1}$ is set in range $[\u03b3_1\u2206_t, \u2206_t]$ or $[\u2206_t, 2\u2206_t]$\nif $\u2206_t < lower bound$, $\u2206_{t+1}$ is set in range $[\u2206_t, min(\u03b3_2\u2206_t, upper bound )]$\nFurther analysis of the Least Square, trust region method can be found in the papers [33], [34]. The convergence of the algorithms can be achieved on a sufficiently large T number of iterations depending on the size of data to minimise the function [33], [34]."}, {"title": "2) Attack Pipeline:", "content": "Least square Trust Region method treats the optimisation problem as a white-box function where it takes an initial point $\u03b8_0$ and it is been guided towards the optimality. The attacker with the use of Delphi-LSTR performs the following steps as outlined in Algorithm 1, (i) define the upper and lower bound of the solution, (ii) random initialisation of the perturbation ($s_t$), (iii) iterate until convergence through solving eq. 8, and measuring the convergence with eq. 9."}, {"title": "B. Delphi - Bayesian Optimisation (Delphi-BO) Method", "content": "Bayesian Optimisation is a well-studied method for optimis-ing expensive-to-evaluate black-box function [35]. It leverages a probabilistic surrogate model which describes the hypothesis of the black-box function that we aim to optimise [36]. BO has two main aspects that need to be specified before moving to the application of the algorithms. These aspects are, the surrogate model, i.e., a function that describes the hypothesis of the black-box function and it is updated in every iteration with newly observed data, and the acquisition function that relates the belief of the objective function with the input space and aims to find a new sampling point that maximises the objective function [37]. Apart from these two aspects, we outline the objective function of the acquisition function to guide the search for new candidates."}, {"title": "1) Surrogate Model:", "content": "In order to construct a surrogate model, we need to collect f's values at finite points $\u03b8_1, ..., \u03b8_\u03c4 \u0395 R^d$, which can be written into a vector $\u0398 = [f(\u03b8_1), ..., f (\u03b8_t)]$. We suppose each point in this vector is drawn randomly from some prior probability distribution [37]. The finite points $\u03b8_1, ..., \u03b8_t \u2208 R^d$ are the model parameters for each point $k$ and the $f(\u03b8_1), ..., f(\u03b8_t)$ are the values of the uncertainty each time we modify the DL model with parameters $\u03b8$. We use a normal multivariate Gaussian Process with mean vector $\u03bc(\u0398)$ and covariance matrix $k(\u0398, \u0398')$ to represent the surrogate model.\n$f \u223c GP(\u03bc(\u0398), k(\u0398, \u0398')).$\nThe covariance matrix $\u039a(\u0398, \u0398')$ is denoted as\n$\u039a(\u0398, \u0398') = \\begin{bmatrix}\n      0_1, 0_1 & ... & 0_1, \u03b8_\u03bd\\\\\n      : & : & :\\\\\n      \u03b8\u03b7, 0_1 & ... & \u03b8\u03b7, \u03b8\u03bd\n    \\end{bmatrix}$\nThe $\u039a(\u0398, \u0398')$ follows the Matern covariance kernel [38] which specifies the covariance between two random variables as follows\n$\u039a(\u03b8_i, \u03b8_j) = \\frac{\u03c3^2}{\\Gamma(\u03bd)} \\frac{2^{1-v}}{\\sqrt{2d(\u03b8_i, \u03b8_j)}^\u03bd}{\u03c1} K_v(\\sqrt{2d(\u03b8_i, \u03b8_j)}),$ where $d(\u03b8_i, \u03b8_j)$ is the Euclidean Distance, $K_v$ is a modified Bessel function and \u0393(\u00b7) is the gamma function. The parameter $v$ controls the smoothness of the resulting function and is generally set as $v = 2.5$, the parameter $\u03c1$ is a positive parameter that is set to $\u03c1 = 1$. Once the surrogate model is built, we will feed it inside the acquisition function to maxmise or minimise the objective function we have set for our optimisation problem."}, {"title": "2) Acquisition Function:", "content": "There are multiple acquisition functions, such as Expected Improvement (EI), Probability Improvement, and Upper Confidence Bound (UCB) [39]. The most common acquisition function used for BO is expected improvement (EI) [37], [40] which aims to maximise the expected improvement over the current best function which describes the observation set.\n$EI(x) = E[max(f(x) - f^*, 0)],$\nwhere $f^*$ is the observed best value in $\u0398$ [37], and the $EI(x)$ is the expected value of the improvement of a chosen x which we have chosen to maximise. We can express the posterior distribution Eq. (14) using Monte Carlo for reducing the computation complexity [41], [42].\n$f(x) = E[f(\u03b8)|\u03be \u223c P(f(\u03b8)|\u0398)] \u2248 \\frac{1}{N}\\sum_{i=1}^N f(\u03be_i)$.\nWith this formulation of the posterior distribution, we can define the qEI as in equation (15)\n$qEI(x) \u2248 \\frac{1}{N}\\sum_{i=1}^N max_{\\xi_j \u223c P(f(x)|\u0398)}\\left\\{max(f_{ij} - f^*,0)\\right\\},$\nwhere $\u03be$ is the posterior distribution of the function $f$ at $x$ with the observed data so far, $N$ Monte Carlo samples and $f^*$ is the observed best function value."}, {"title": "3) The Objective Function of Bayesian Optimization:", "content": "The process of optimising the acquisition function for searching for new candidates requires an assistance for effective search for the best value due to the high dimensionality. Thus, we observe the uncertainty and the weights of the neurons over each time step. Based on this data, we create Gaussian Process, with an input of the weights and the output of the uncertainty. We use the GP as an objective unction to check whether the output from the posterior of the surrogate model is correct and near the objective of the current weights."}, {"title": "4) Attack Pipeline:", "content": "Assuming that the attacker is participat-ing in part of the training, we apply the poisoning attack to the attacker's local model. Once the attacker receives the global model from the server, the attacker aims to find and manipulate the optimal parameters that will induce uncertainty inside the model using Delphi strategies, including Delphi-LSTR and Delphi-BO. Delphi-BO treats the optimisation problem as a black-box function, which searches the optimal parameters by searching the latent space of the black-box function. The attacker with the use of Delphi-BO performs the following steps as outlined in Algorithm 2: (i) construct a dataset, (ii) initialise a surrogate model, (iii) sample a new set of optimal parameters through optimising acquisition function, (iv) calculate the objective function, (v) expand dataset, and (vi) repeat for T iterations. The optimal solution of the BO can be reached through searching the correct parameters which will describe best the black-box function with a Gaussian Process. Even though, this is an optimisation technique, we aim to create a function that describes the uncertainty inside model parameters rather than converging directly to the optimal solution."}, {"title": "C. Complexity Analysis of Delphi-BO and Delphi-LSTR", "content": "The computational complexity of Delphi variants differs significantly based on their optimisation approaches. Delphi-BO, utilising Gaussian Process (GP) regression, has a time complexity of $O(1 \u00d7 (n\u00b3 + n\u00b2d+ N_s n\u00b2))$ per iteration, where $A$ is the number of neurons, $n$ is the number of observations, $d$ is the input dimension, and $N_s$ is the number of Monte Carlo samples [37]. The cubic scaling with observations ($n\u00b3$) arises from GP's covariance matrix inversion, whilst the $n\u00b2d$ term comes from kernel computations. The space complexity is dominated by the GP covariance matrix storage, requiring $O(n\u00b2)$ memory [41]. In contrast, Delphi-LSTR employs trust region optimisation with a time complexity per iteration of $O(1 \u00d7 (nd\u00b2 + d\u00b3))$, where the $d\u00b3$ term is because of the trust region subproblem and the associated matrix operations [34]. The method's space complexity is $O(d^2)$, primarily for storing the Hessian matrix. This makes LSTR more memory-efficient than BO for problems with many observations but is computationally intensive in high-dimensional spaces."}, {"title": "V. MATHEMATICAL ANALYSIS OF THE ATTACK EFFECTIVENESS", "content": "In this section, we propose a measurement of the attack effectiveness in FL. We provide the analysis of the attack effectiveness on FedAvg and that there is an upper bound of the attack."}, {"title": "A. Definition of Attack Effectiveness", "content": "Let the $c$ be the mean predictive confidence, and $p$ be the attack effectiveness. Based on the above notation, we can define the attack effectiveness for FedAvg as the average disruption caused by the malicious users to the global model multiple by the inverse of mean predictive confidence. We can express it as,\n$p = \\frac{1}{c}\\frac{1}{A}\\sum_{a=N+1}^{K} ||w_t - w_i||^2,$\nfor $c > 0$, $||w - w'||_2 > 0$, where, $\\sum_{a=N+1}^{K}||W_t - w_i||^2$ is the average disruption caused by the malicious users to the global model. Based on the analysis in the rest of the section, the attack effectiveness has an upper bound depending on the amount of the expected perturbation $e$ and it is defined as,\n$p \\leq \\frac{1}{c} e^2(\\frac{3N}{A} + 4).$"}, {"title": "B. Bounding Attack", "content": "Before proving that there is an upper bound for the attack when applying Delphi in FL, we first need to define where the malicious users exist inside the general FL problem. First, we recall the equation 3\n$w = \\frac{1}{N+A} (\\sum_{n=1}^{K-A} W_n + \\sum_{a=N+1}^{K} W_a)$\nTo simplify the equation and understand the influence of the optimal weight w and the adversarial contribution $\\frac{1}{A} \\sum_{a=N+1}^{K} W_a$, we can rewrite this as $\\frac{1}{A}\\sum_{a=N+1}^{K} W_a = \\sum_{n=1}^{K-A} W_n$,\n$WA = \\sum_{a=N+1}^{K} \u03c9$,\n$WN = \\sum_{n=1}^{K-A} \u03c9$,\nsuch that\n$w^* = \\frac{1}{N+A} (\u03c9_N + WA).$\nMoving forward, we need an expression for $w_A$ and $W_N$. Based on the general problem of the DL and the FL, $W_A$ and $W_N$ can be expressed as follows\n$WA = \\sum_{a=N+1}^{K} \\frac{D}{D} (\u03c9 - \u03b7 \\nabla F_a(\u03c9)) + \u03b4,$\n$WN = \\sum_{n=1}^{K-A} \\frac{D}{D} (\u03c9 - \u03b7 \\nabla F_n(\u03c9)),$\nwhere, $\u03b7$ is the learning rate, $D_a$ and $D_n$, are the local datasets for malicious and normal users respectively, D is the total amount of data available in FL, and $\u03b4$ is the expected perturbation induced by all the clients. Therefore, we can express the eq. (2) in a more analytical form, such that we can have an expression for the expected perturbation inside the system.\n$W = \\frac{1}{N+A}\\sum_{k=1}^{K}\\frac{D_k}{D} W_k,$\n$w = \\frac{1}{N+A}(\\sum_{a=N+1}^{K}(W_{t-1}-\u03b7F_a(w))+ \u03a3 \u03b4_a + \\sum_{a=N+1}^{K-A} (W_{t-1}-\u03b7F_n(w))),$\n$\\frac{1}{N+A}\u03b4 = w\\sum_{a=N+1}^{K},$\n$\\frac{1}{N+A} (\\frac{\\sum_{a=N+1}^{K}(W_{t-1}-\u03b7F_a(w))}{D_a} + \\frac{\\sum_{a=N+1}^{K-A} (W_{t-1}-\u03b7F_n(w)))}{D_n},$\nif $\\delta$ = 0 means that the attack is ineffective. To make it effective, then $\\delta$ has to satisfy $||\u03b4|| \\geq 0$. This means the $\\delta$ should have a lower bound to be effective. Therefore, we define that for an effective function the expectation of $\\delta$ should be bounded by $\u0454$ also referred in this paper as the expected perturbation,\n$E[\\sum_{a=N+1}^{K}\u03b4] \\geq \u20ac, s. \u2208 > 0.$"}, {"title": "C. Proof of the Attack Effectiveness", "content": "In this section, we provide the proof for the above attack effectiveness of FedAvg. From the equation (16), we can first analyse the term $\\sum_{a=N+1}^{K}||W_t \u2013 w_i||^2$, which can be considered as the average discrepancy of the global model from the malicious users\n$\\frac{1}{A}\u03a3||\u03c9 - \u03c9i||^2 < \\frac{1}{A} ( \\sum_{a=N+1}^{K}(\\frac{1}{(N + A)} \\sum D_k^a)^2 - ||\u03c9_t||^2,$\n$+ - \u03a3 - \u03c9\u0390) ,$\n$\\frac{1}{A}\u03a3 \u03b4^2 - \u03c9\u0390)||^2 \u03a3 \u03bb\u03bb \u03c9\u0390, (27)\nFrom the eq. 26, it is clear to see that if we exclude the term $(\\frac{D_a}{(N + A)} - \u03b7\u2207F(\u03c9))$, we can consider that the rest of the summation is the sum of the benign model without the adversaries. Therefore, we combine them together to get the average of the weight for a benign model w' plus the perturbation 8 from the malicious users.\n$\\frac{1}{A} - \u03c9\u0390||^2 < ||(\\frac{D_a}{(N + A)} - \u03b7\u2207F(\u03c9)  + \\frac{1}{A}  || ^2,$\n$\\frac{1}{A}+ \u03a3\u03b4-\u03c9\u0390)||^2 \u03a3 2. (28)\nFrom equation (28), we can consider that all the clients are benign. Based on the definition of \u03b4, and we can get an upper bound of the discrepancy.\n. , the term  is clear that it can be subistiuted and considered as the perturbation & caused by the . Then we have,\n\u03a3|| - \u03c9\u0390||2 =\u03a3 \u03b4\u0390 + 2( - \u03a3\u03b4 +\n  . (30)\nBased on the property we substitute the term ) with the expected pertrubation $\u0454$ as defined in the eq. (25)\n.\\sum_{a=N+1}^{K} \u03b4\u0390 + 2( - \u03c9\u0390)+ \u0454\u0390 , (31)\n\\\nWe then multiply the first two terms with  to substitute the terms again with the expected perturbation $\u0454$, we can obtain\n=\n++\n\\\\\\\\\\\\\\\\\\\\\\Therefore, based on the eq. 33, we find out that there is an upper bound for the attack effectiveness. The average amount of disturbance from the attackers in the global model depends on the number of users. This means that less attackers will require more effort to manipulate the users."}, {"title": "VI. EXPERIMENTS AND SIMULATION RESULTS", "content": "For the experiments, we evaluate Delphi strategies, i.e., Delphi-BO and Delphi-LSTR, using the CIFAR10 dataset [44", "44": "and compare their attack performances in different adversarial scenarios. The CIFAR10 dataset con-tains 10 classes of 50,000 images for training and 10,000 images for testing purposes, and CIFAR100 dataset contains 100 classes with 500 images for training and 100 images for testing purposes. The images are RGB with each image size of 32 by 32 pixels. We distribute the dataset over each user following the distribution of independent and identically distributed (i.i.d.) and imbalanced data. In the imbalanced distribution, the users have all the classes available however the data size is varying with each client. The imbalanced distribution is generated using the Poisson"}]}