{"title": "PathInsight: Instruction Tuning of Multimodal Datasets and Models for Intelligence Assisted Diagnosis in Histopathology", "authors": ["Xiaomin Wu", "Rui Xu", "Pengchen Wei", "Wenkang Qin", "Peixiang Huang", "Ziheng Li", "Lin Luo"], "abstract": "Pathological diagnosis remains the definitive standard for identifying tumors. The rise of multimodal large models has simplified the process of integrating image analysis with textual descriptions. Despite this advancement, the substantial costs associated with training and deploying these complex multimodal models, together with a scarcity of high-quality training datasets, create a significant divide between cutting-edge technology and its application in the clinical setting. We had meticulously compiled a dataset of approximately 45,000 cases, covering over 6 different tasks, including the classification of organ tissues, generating pathology report descriptions, and addressing pathology-related questions and answers. We have fine-tuned multimodal large models, specifically LLaVA, Qwen-VL, InternLM, with this dataset to enhance instruction-based performance. We conducted a qualitative assessment of the capabilities of the base model and the fine-tuned model in performing image captioning and classification tasks on the specific dataset. The evaluation results demonstrate that the fine-tuned model exhibits proficiency in addressing typical pathological questions. We hope that by making both our models and datasets publicly available, they can be valuable to the medical and research communities.", "sections": [{"title": "1 Introduction", "content": "Pathological examination remains the cornerstone of tumor and cancer diagnostics, serving as the gold standard for diagnostic techniques [21]. The meticulous analysis of tissue samples under the microscope by experienced pathologists enables the detection, classification, and grading of cancerous lesions, providing critical insights into the most appropriate therapeutic strategies. However, there is a relative scarcity of senior pathologists, and the processes of accessing and consulting pathological knowledge are often cumbersome. Additionally, the procedures of slide examination, diagnosis, and meticulous documentation are time-consuming, further exacerbating the complexity and workload inherent in the diagnostic process [1].\nThe emergence of digital tools such as Whole Slide Imaging (WSI) has significantly facilitated the process of pathological diagnosis, making the storage and transfer of pathological data easier than ever [4,7]. Certainly, a substantial array of sophisticated methodologies pertaining to Whole Slide Imaging (WSI) within the realm of digital pathology has come to the forefront in recent scholarly discourse [27,11,19]. Concurrently, large language models are increasingly being harnessed for their capabilities in machine learning and deep learning algorithms, particularly in the analysis of pathological images [20]. Some state-of-the-art methods have become extensively utilized for tasks including disease classification [28,3], segmentation of pathological lesions [9], and prognostication of patient [22]. However, their efficacy is often contingent on the availability of large sets of annotated data for training. Additionally, these tasks are generally performed in isolation, and their application domains tend to be specialized.\nCurrently, there exists a significant gap in the availability of an integrated, comprehensive, and user-friendly algorithmic model that encapsulates the essential capabilities for pathologists or medical students engaged in diagnosis or educational activities. Such capabilities encompass image analysis, resolution of pathology-related image queries, retrieval of pathological knowledge, and the generation of complete diagnostic reports.\nRecently, Large Language Models (LLMs), epitomized by GPT [5] have ignited a surge of research interest. In particular, the emergence of multimodal large language models, such as GPT-4Vision (GPT4V) [18], LLaVA [17], Qwen-VL [2], InternLM [29], and CogVLM [26], has facilitated the seamless integration of image and text data. These models are capable of concurrently addressing a variety of tasks, such as text recognition, visual reasoning, visual question answering, and caption generation [6]. In clinical practice, these models are envisioned to synergize with pathologists' queries to analyze medical images, thereby offering broader knowledge, more inspiration, deeper diagnostic insights, and accelerated diagnostic processes. In the context of pathology education, these models are envisioned to assist students in better understanding the details of pathological images and the corresponding medical knowledge.\nSmart Doctor models typically restrict interactions to text communication, with data often sourced from quickly established question-and-answer sessions between patients and doctors with simple conditions. However, pathological diagnosis is extremely specialized, which presents unique challenges that these text-based models [25] are not equipped to address. Current multimodal large models primarily focus on general domains, emphasizing tasks related to image perception and cognition but lacking in domain-specific knowledge and capabilities for the medical field. The models like LLaVA-Med [16] and PathAsst [23] represent initial efforts to integrate multimodal large language models with medical applications. However, LLaVA-Med [16] mainly concentrates on medical imaging modalities such as CT and MR, with a lesser emphasis on pathological images, which may result in relatively weaker pathological capabilities compared to other data types. PathAsst [23], on the other hand, serves as a foundational model tailored for pathology, encompassing a variety of pathological tasks. However, its development necessitates significant investment in data, resources, and human expertise.\nTo address these challenges, we propose a pathology-focused multimodal instruction fine-tuning dataset. By fine-tuning existing widely acknowledged multimodal models with this dataset, we aim to achieve practical and user-friendly multimodal models for pathology. Our main contributions are as follows:\nWe constructed a pathology-oriented dataset that encompasses a range of tasks including pathological image grading, classification typing, pathological image caption generation, image descriptions, image-based question-answering, and dialogue interactions, comprising approximately 45,000 instances.\nUtilizing this pathology dataset, we have fine-tuned a selection of widely recognized multimodal models, employing strategies including Low-Rank Adaptation (LoRA) and full-parameter tuning. This enhances the practical application of multimodal models in the field of pathology by augmenting multi-tasking capabilities and supporting extensive and comprehensive uses. Quantitative evaluations of model's capabilities have been undertaken in tasks, such as classification, image caption generation, and question-answering. Additionally, we invited experienced pathologist to provide qualitative assessments of the model's honesty in answering open-ended questions. The results indicate that our constructed dataset and fine-tuning paradigm enable effective handling of tasks like classification and caption generation, and also demonstrate potential in dealing with open-ended, interpretive questions that pathologist encounter in clinical practice."}, {"title": "2 Methods", "content": "2.1 PathEnhanceDS Compiling\nTo ensure the comprehensiveness and diversity of our dataset, meticulous considerations were undertaken regarding the assortment and synthesis of data sources, which culminated in the creation of a dataset named PathEnhanceDS, with the specifics delineated as follows:\nInitially, we meticulously curated datasets from both cutting-edge and established data repositories to encompass a wide array of pathologies and diagnostic scenarios. The selection process was governed by several pivotal criteria: the relevance to diagnostic pathology, the quality of annotations, and the compatibility across different tasks such as Captioning, Classification, Visual Question Answering (VQA), and Conversation.\nWe relied on the Pathologist-level Dataset [31], a collection of data annotated by medical professionals that provided patch-level descriptive information. We retained the initial diagnostic descriptions provided by seasoned pathologists to ensure the textual data mirrored the caliber of professional expertise.\nFurther, to expand our dataset's heterogeneity, we incorporated OpenPath [12], derived from the broad-reaching social media platform Twitter, which offered an unconventional perspective on pathology data through publicly accessible Twitter IDs. The preeminent 20,000 samples exhibiting high semantic congruity between images and their captions based on the BiomedCLIP [30] were selected, with an emphasis on HE stained images.\nTo ensure balanced binary classification, we integrated the eminent PCam [24], recognized for its binary classification challenges within the pathology domain. Additionally, the CRC-VAL-HE-7K [14], focused on multi-classification of tissue organs, was selected for its rigorous curation and substantial relevance to the field.\nThe adoption of PathVQA [8] was motivated by its comprehensive compilation of pathology images paired with question-answer pairs, sourced from a wide gamut of diagnostic categories. This vital component ensured that our dataset was not only rich in visual content but also in the interactive, interrogative dialogue that is essential for training sophisticated Visual Question Answering models. Inclusivity of PathVQA allowed us to simulate a more interactive environment, closely imitating the dynamic interplay between a pathologist and their diagnostic workflow. We incorporated images from this dataset using the predictive capabilities of the BiomedCLIP [30] to ensure a focus on HE histopathology, enriching the dataset with structured pathological Q&A content.\nLLaVA-Med-Instruct [15], on the other hand, was selected for its novel conversational style format data. This dataset, characterized by its conversation-based queries that necessitate thorough internalization and replication of diagnostic thought processes, provided an exceptional opportunity to imbue our dataset with the finesse of clinical judgment and decision-making. Dialogue data pertaining exclusively to pathology and consisting of 7,691 multi-turn dialogues with 28,710 question-answer pairs were included to capture the interactive aspect of pathology diagnostics.\nIn conjunction with the corresponding queries, we transform the relevant data into instructional data and format it as ChatML. Given a pathological image \\(P_v\\) and its corresponding description \\(P_d\\), we generate a query \\(P_q\\) that solicits a description of the pathological image. For the captioning task, the query here is randomly extracted from a series of questions. Specific prompts (queries) can be found in the Appendix.The arrangement of (\\(P_v\\), \\(P_d\\), \\(P_q\\)) forms a single-round interaction in the following format:\nUser: <img>\\(P_v\\)</img> \\(P_q\\)<STOP>\nAssistant: \\(P_d\\)<STOP>\nOur diverse choices of datasets were instrumental in constructing an effective and wide-reaching study, shedding light on the multifaced realm of pathological multimodal language models.\n2.2 PathoSync Tuning\nAfter constructing PathEnhanceDS instruction tuning dataset, the primary challenge we faced was adapting general-purpose models to effectively address specific issues within the medical histopathology domain."}, {"title": "3 Experiments", "content": "After fine-tuning the model with PathEnhanceDS, it was imperative to rigorously assess its performance within the domain of pathological medicine. The primary aim of our experimental design is to investigate the feasibility of transferring large language multimodal models, which have been pretrained in the natural domain, to the realm of pathology by instructional fine-tuning, and to verify the effectiveness of the multimodal pathology dataset we have constructed.\n3.1 Evaluation Metrics\nIn order to verify the effectiveness of fine-tuning, the evaluation focused on several core tasks defined earlier, classification of pathological images (identifying the presence of pathological lesions and the type of tissue involved), the capability to generate captions for pathological images, and visual reasoning abilities based on pathological imagery.\nFor classification tasks, particularly for identification of tissue types, precision, recall, and F1 score have been chosen as the main evaluation metrics.\nIn open-ended generative tasks, such as image captioning, due to the unpredictable nature of the task, traditional machine translation evaluation metrics, such as BLEU and ROUGE, are used to assess the consistency between generated texts and reference texts. BLEU-1 is used to measure the level of exact word-level matches between the generated text and the reference text, namely the overlap of 1-grams, which reflects the accuracy of word selection. The ROUGE metric, on the other hand, measures the frequency with which reference text words appear in the generated text, evaluating the comprehensiveness of the generated text from the perspective of recall. Together, these two metrics provide a quantitative means to assess the performance of models in generative tasks.\n3.2 Evaluation Results\nThe results indicate that after fine-tuning, the model has significantly improved its relevance. Notably, In CRC classification tasks, the performance is highly consistent with the ground truth (GT). The caption generated by the model are structurally accurate and generally conform to the fundamental knowledge of medical pathology in terms of content. However, there remain some discrepancies in the accuracy of details.\nIn the quantitative tasks, we further calculated the answer precision in CRC classification table 2. For PCAM classification tasks, Acc was improved from 0.5 to 0.96 by fine-tuning. The base model exhibited good precision on the CRC classification dataset, but the recall and fl-score ware low. The fl-score in CRC classification task of Qwen-VL model increased from 0.138 to about 0.978. After fine-tuning, the LLaVa model showed improved results, with the recall increasing from 0.008 to 0.967.\nWe also quantitatively assessed the performance of the captioning task 2. Despite targeted optimization for large language model generation tasks, significant differences from traditional machine translation tasks remain. Specifically, for the image captioning task on the Pathologist-level Dataset, the Qwen-VL model's BLEU-1 score improved from 0.09 to 0.41, Rouge-1 score improved from 0.15 to 0.58. The LLaVA-1.5 model's BLEU-1 score improved from 0.099 to 0.421 (Full parameters fine-tuning), 0.371 (LoRA parameters fine-tuning), Rouge-1 score improved from 0.145 to 0.583 (Full parameters fine-tuning), 0.539 (LORA parameters fine-tuning), indicating a substantial enhancement in performance. Additionally, for the OpenPath dataset captioning, we calculated BLEU-1 and ROUGE-1 scores. However, given the high degree of individual variability in data from Twitter, these conventional metrics may not be entirely suitable for assessing model performance on such heterogeneous data sources. After removing the OpenPath, we applied both LoRA and full-parameter training to the LLaVA model. To visually demonstrate the model's performance, we presents some examples of the model's responses in Appendix."}, {"title": "4 Conclusion", "content": "The integration of multimodal large models has significantly improved the handling of diverse tasks in a unified framework, enhancing both image and text analysis. Especially in computational pathology, these models enable clinicians and medical students to access and document diagnostic information more efficiently. However, the application in medical pathology is limited by high data acquisition costs and the scarcity of quality datasets.\nTo overcome these challenges, we developed PathEnhanceDS, a comprehensive dataset with over 45,000 cases that include disease grading, tissue classification, and pathology report generation tasks. By fine-tuning models like Qwen-VL, InternLM, and LLaVA, our evaluations show significant performance improvements in pathology, adhering to clinical medicine standards for open-ended questions. This highlights the effectiveness of our fine-tuning approach and the adaptability of our carefully curated dataset."}]}