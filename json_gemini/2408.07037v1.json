{"title": "PathInsight: Instruction Tuning of Multimodal Datasets and Models for Intelligence Assisted Diagnosis in Histopathology", "authors": ["Xiaomin Wu", "Rui Xu", "Pengchen Wei", "Wenkang Qin", "Peixiang Huang", "Ziheng Li", "Lin Luo"], "abstract": "Pathological diagnosis remains the definitive standard for identifying tumors. The rise of multimodal large models has simplified the process of integrating image analysis with textual descriptions. Despite this advancement, the substantial costs associated with training and deploying these complex multimodal models, together with a scarcity of high-quality training datasets, create a significant divide between cutting-edge technology and its application in the clinical setting. We had meticulously compiled a dataset of approximately 45,000 cases, covering over 6 different tasks, including the classification of organ tissues, generating pathology report descriptions, and addressing pathology-related questions and answers. We have fine-tuned multimodal large models, specifically LLaVA, Qwen-VL, InternLM, with this dataset to enhance instruction-based performance. We conducted a qualitative assessment of the capabilities of the base model and the fine-tuned model in performing image captioning and classification tasks on the specific dataset. The evaluation results demonstrate that the fine-tuned model exhibits proficiency in addressing typical pathological questions. We hope that by making both our models and datasets publicly available, they can be valuable to the medical and research communities.", "sections": [{"title": "1 Introduction", "content": "Pathological examination remains the cornerstone of tumor and cancer diagnostics, serving as the gold standard for diagnostic techniques [21]. The meticulous analysis of tissue samples under the microscope by experienced pathologists enables the detection, classification, and grading of cancerous lesions, providing critical insights into the most appropriate therapeutic strategies. However, there is a relative scarcity of senior pathologists, and the processes of accessing and consulting pathological knowledge are often cumbersome. Additionally, the"}, {"title": "2 Methods", "content": null}, {"title": "2.1 PathEnhanceDS Compiling", "content": "To ensure the comprehensiveness and diversity of our dataset, meticulous considerations were undertaken regarding the assortment and synthesis of data sources, which culminated in the creation of a dataset named PathEnhanceDS, with the specifics delineated as follows:\nInitially, we meticulously curated datasets from both cutting-edge and established data repositories to encompass a wide array of pathologies and diagnostic scenarios. The selection process was governed by several pivotal criteria: the relevance to diagnostic pathology, the quality of annotations, and the compatibility across different tasks such as Captioning, Classification, Visual Question Answering (VQA), and Conversation.\nWe relied on the Pathologist-level Dataset [31], a collection of data annotated by medical professionals that provided patch-level descriptive information. We"}, {"title": "2.2 PathoSync Tuning", "content": "After constructing PathEnhanceDS instruction tuning dataset, the primary challenge we faced was adapting general-purpose models to effectively address specific issues within the medical histopathology domain."}, {"title": "3 Experiments", "content": "After fine-tuning the model with PathEnhanceDS, it was imperative to rigorously assess its performance within the domain of pathological medicine. The primary aim of our experimental design is to investigate the feasibility of transferring large language multimodal models, which have been pretrained in the natural domain, to the realm of pathology by instructional fine-tuning, and to verify the effectiveness of the multimodal pathology dataset we have constructed."}, {"title": "3.1 Evaluation Metrics", "content": "In order to verify the effectiveness of fine-tuning, the evaluation focused on several core tasks defined earlier, classification of pathological images (identifying the presence of pathological lesions and the type of tissue involved), the capability to generate captions for pathological images, and visual reasoning abilities based on pathological imagery.\nFor classification tasks, particularly for identification of tissue types, precision, recall, and F1 score have been chosen as the main evaluation metrics.\nIn open-ended generative tasks, such as image captioning, due to the unpredictable nature of the task, traditional machine translation evaluation metrics, such as BLEU and ROUGE, are used to assess the consistency between generated texts and reference texts. BLEU-1 is used to measure the level of exact word-level matches between the generated text and the reference text, namely the overlap of 1-grams, which reflects the accuracy of word selection. The ROUGE metric, on the other hand, measures the frequency with which reference text words appear in the generated text, evaluating the comprehensiveness of the generated text from the perspective of recall. Together, these two metrics provide a quantitative means to assess the performance of models in generative tasks."}, {"title": "3.2 Evaluation Results", "content": "The results indicate that after fine-tuning, the model has significantly improved its relevance. Notably, In CRC classification tasks, the performance is highly consistent with the ground truth (GT). The caption generated by the model are structurally accurate and generally conform to the fundamental knowledge of medical pathology in terms of content. However, there remain some discrepancies in the accuracy of details.\nIn the quantitative tasks, we further calculated the answer precision in CRC classification table 2. For PCAM classification tasks, Acc was improved from 0.5 to 0.96 by fine-tuning. The base model exhibited good precision on the CRC classification dataset, but the recall and fl-score ware low. The fl-score in CRC classification task of Qwen-VL model increased from 0.138 to about 0.978. After fine-tuning, the LLaVa model showed improved results, with the recall increasing from 0.008 to 0.967.\nWe also quantitatively assessed the performance of the captioning task 2. Despite targeted optimization for large language model generation tasks, significant"}, {"title": "4 Conclusion", "content": "The integration of multimodal large models has significantly improved the handling of diverse tasks in a unified framework, enhancing both image and text analysis. Especially in computational pathology, these models enable clinicians and medical students to access and document diagnostic information more efficiently. However, the application in medical pathology is limited by high data acquisition costs and the scarcity of quality datasets.\nTo overcome these challenges, we developed PathEnhanceDS, a comprehensive dataset with over 45,000 cases that include disease grading, tissue classification, and pathology report generation tasks. By fine-tuning models like Qwen-VL, InternLM, and LLaVA, our evaluations show significant performance"}]}