{"title": "MIND: Effective Incorrect Assignment Detection through a Multi-Modal Structure-Enhanced Language Model", "authors": ["Yunhe Pang", "Bo Chen", "Fanjin Zhang", "Yanghui Rao", "Jie Tang"], "abstract": "The rapid growth of academic publications has exacerbated the issue of author name ambiguity in online digital libraries. Despite advances in name disambiguation algorithms, cumulative errors continue to undermine the reliability of academic systems. It is estimated that over 10% paper-author assignments are rectified when constructing the million-scale WhoIsWho benchmark. Existing endeavors to detect incorrect assignments are either semantic-based or graph-based approaches, which fall short of making full use of the rich text attributes of papers and implicit structural features defined via the co-occurrence of paper attributes. To this end, this paper introduces a structure-enhanced language model that combines key structural features from graph-based methods with fine-grained semantic features from rich paper attributes to detect incorrect assignments. The proposed model is trained with a highly effective multi-modal multi-turn instruction tuning framework, which incorporates task-guided instruction tuning, text-attribute modality, and structural modality. Experimental results demonstrate that our model outperforms previous approaches, achieving top performance on the leaderboard of KDD Cup 2024. Our code has been publicly available\u00b9.", "sections": [{"title": "1 Introduction", "content": "The past decades have witnessed a substantial proliferation of research papers across all fields of science, with platforms like Google Scholar\u00b2 [13], Semantic Scholar\u00b3 [24], and AMiner\u2074 [33, 44, 45] indexing over 300 million papers. As a result, despite the availability of advanced author name disambiguation algorithms [4], recent studies have highlighted significant error rates of paper-author assignments in digital libraries, with exceeding 10% error rate when constructing the million-scale WhoIsWho benchmark [5, 46]. These inevitable cumulative errors from imperfect assignment methods significantly impact the efficacy of subsequent assignments, even leading to distorting authors' citation count and misallocation of credit. Therefore, an inherent self-correcting mechanism to automatically detect and rectify these errors is crucial for maintaining the reliability of academic systems.\nIn this paper, we investigate the INcorrect assignment Detection (IND) problem [5], which aims to detect incorrect assignments of authors within the paper due to the ambiguity of the author name, as shown in Figure 1(a). Previous efforts [6, 46] typically employ graph-based algorithms, illustrated in Figure 1(b), which build paper similarity graphs by considering papers as nodes and establishing edges between papers through key co-occurrence relationships such as co-authorship and co-organization, and then perform node classification to detect outlier papers. However, these methods focus more on key structural features and implicitly incorporate semantic textual features via input node features, but lack characterizing the fine-grained semantic features embodied in rich attributes of papers. Recently, large language models (LLMs) [2, 49] demonstrate their strengths across various natural language understanding and generation tasks, endowed by their intrinsic capability to capture more fine-grained correlations through the self-attention mechanism on"}, {"title": "2 Related Work", "content": "2.1 Author Name Disambiguation\nAuthor name disambiguation, can be categorized into three specific tasks [5]: from-scratch name disambiguation (SND), real-time name disambiguation (RND), and incorrect assignment detection (IND). We first briefly review methods for SND and RND, and then focus on IND methods in particular.\nFrom-scratch name disambiguation (SND), a foundational component in the construction of academic search systems, aims to partition papers associated with the same name into disjoint"}, {"title": "2.2 Incorrect Assignment Detection (IND)", "content": "The IND problem is commonly viewed as a graph-based anomaly detection task [6]. Recently, LLM-based methods have gained more attention due to remarkable breakthroughs achieved by LLMs.\n2.2.1 Graph-based methods. Graph-based methods first construct a paper similarity graph for each target author based on the similarity between papers' attributes (e.g., co-author, co-organization). Some early works [6, 46] employ the graph neural network (GNN) as an encoder to get papers' hidden features and then perform binary node classification. Recently, the IND task has been regarded as an anomaly detection task on graphs. For instance, GCCAD [6] further employs contrastive learning to contrast abnormal nodes and normal ones, and it removes suspicious links during the message passing of GNNs to reduce the impact of erroneous edges.\n2.2.2 LLM-based methods. LLM-based methods regard the IND problem as a natural language generation task. The OAG-Bench [46] initially explores the feasibility of applying LLMs to the IND problem, which defines a custom instruction to query the LLM whether a target paper belongs to the target author (i.e., a paper list). However, this preliminary attempt solely utilizes papers' titles.\nDuring KDD Cup 2024, some contest winners [32, 40, 47] further incorporate more paper attributes via concatenation as LLM inputs. However, naively extending the input length of LLMs requires much more GPU memory and training/inference time. Furthermore, they abandon graph-based features, thereby failing to effectively utilize high-order structural similarity features of paper similarity graphs.\nIn contrast, our model integrates high-order similarity information derived from graph neural networks into the LLM and makes full use of multi-source feature information in the context of the limited input length. Furthermore, by constructing an efficient multi-round query strategy, we achieved significant acceleration in both training and inference."}, {"title": "3 Problem Definition", "content": "In this section, we introduce the problem formulation of incorrect assignment detection."}, {"title": "4 Model Framework", "content": "Previous IND methods typically adopt LLM-based methods or graph-based methods for incorrect assignment detection. Graph-based methods fall short of capturing the semantic relation across numerous academic papers. By formalizing the IND problem as a question-answering task, LLM-based methods excel in capturing intricate semantic correlations among authors' papers. However, large language models struggle to model graph structural patterns and efficiently handle rich paper attributes as long texts.\nIn light of this, we propose MIND, a multi-modal framework that integrates rich paper attributes and structural features using a large language model (LLM) to address incorrect assignment detections. MIND initially employs an effective multi-turn instruction template to guide the LLM for IND tasks (see Section 4.1). After that, we enhance this by introducing rich paper attributes into the IND-instructed LLM through a semantic embedding module. This module summarizes the rich text attributes, projecting them as semantic tokens into the LLM(see Section 4.2). Subsequently, we integrate node and graph features from graph neural networks to incorporate structural patterns characterized by paper-author relational graphs, projecting them as structural tokens into the LLM (see Section 4.3). The progressive training algorithm is adopted to guarantee the effective integration of different modalities (see Section 4.4). In the following sections, we delve into the details of each component."}, {"title": "4.1 Task-Guided Multi-Turn Instruction Tuning", "content": "Prior arts [6, 46] usually adopt graph-based methods for the IND problem. However, recent attempts [46] suggest that large language model-based methods hold promise in this task since it can capture semantic correlations precisely between target papers and author profiles via self-attention mechanisms[36]. Inspired by this, we transform the IND problem into a question-answering task, define an instruction template by incorporating the target paper and the target author's profile, and then ask the LLMs to output whether the target paper belongs to the target author.\nDue to the limitations of context length and training efficiency in LLMs, the target paper and author must be represented by concise yet informative tokens. For the target paper, we choose key attributes like titles as tokens. The target author is represented by their set of published papers. Following this, we define contextual papers as textual attributes of all papers (or a randomly sampled subset of all papers if the token length exceeds). A query is then"}, {"title": "4.2 Semantic Embedding Module", "content": "Due to the limitations of the context length inherent in LLMs, as well as the increased GPU memory and time costs of both model training and inference with longer context lengths, incorporating all the paper features into the LLM input is prohibitive. Consequently, we could only select a subset of key features into the LLM for the base model in Section 4.1. To involve more informative paper attributes into our framework, we integrate the remaining paper features and then summarize the semantic features through a semantic embedding module. To be specific, the semantic embedding module is a small pre-trained language model that transforms paper attributes into a sequence of embeddings. Additionally, we use an adaptive pooling module to summarize these embeddings. This summary is then projected by a text projector to align the feature space of the semantic embedding module with that of LLM."}, {"title": "4.3 Structural Embedding Module", "content": "Existing attempts [6, 46] demonstrate that by building a paper similarity graph for each target author based on the attribute similarity such as co-authors and co-organizations, the outlier papers can be effectively detected by leveraging graph topology. However, the existing LLMs are not adept at capturing structural information. Therefore, to enhance the capacity to process structural information, we first construct a paper similarity graph for each target author based on the aforementioned relationships. Afterward, we train a state-of-the-art GNN-based anomaly detection method, i.e., GCCAD [6], to obtain the hidden node embeddings and hidden graph embeddings. The idea of GCCAD is to perform contrastive learning by comparing abnormal nodes with normal ones in terms of their distances to the global graph representation (e.g., the average of all nodes). In what follows, the node and graph representations are concatenated to form the structural features of each paper. Due to the mirrored challenge in misalignment of feature spaces like Section 4.2, we utilize a graph projector that takes the concatenated structural features as input and outputs the graph token embedding for LLM. Let A denote the adjacency matrix of the constructed paper graph and Xnd denote the node input features (initialized with PLM-encoded title embeddings). The procedure of structural embedding module can be formalized as follows:\n$Z^g, \\{Z_0, Z_1, ..., Z_m\\} = GCCAD(A, X_{nd})$\n$Z = FFN_{Swish}(concat(Z_i, Z^g))$\nwhere Zg and Zi denotes the learned graph embedding and i-th node's hidden embedding, respectively. Then the concatenated node and graph embedding are fed into a 2-layer FFN projector to"}, {"title": "4.4 Progressive Instruction-Tuning", "content": "To effectively fuse multi-modal features into our framework, the training procedure is meticulously segmented into three phases to progressively incorporate uni-modal information individually. Initially, as introduced in Section 4.1, we begin with training the base LLM utilizing parameter-efficient fine-tuning [15, 16, 22]. Advancing to the second phase, we freeze the parameters of the previously trained LLM and integrate the semantic embedding module. Pre-trained language model can yield meaningful semantic features. Thus, the parameters of the PLM encoder are fixed and only the text projector is trainable at this stage.\nIn the terminal phase, employing a similar strategy, we freeze all parameters from the antecedent stages, as well as those of the graph encoder, and concentrate solely on training the parameters of the graph projector. This final phase is dedicated to the seamless integration of the structural features derived from the graph neural network into the feature space of the LLM.\nThis progressive training approach ensures a coherent and incremental fusion of diverse features. At each stage, the model parameters of previous stages are well-trained, in the sense that features of previously-considered modalities are effectively fused, thus decreasing the difficulty of training parameters of the new modality. Extensive experiments also reveal the superiority of our training recipe over other alternatives.\nAt the inference stage of MIND, for each instruction encompassing contextual papers of the target author and multiple target papers, our framework integrates the outputs of the semantic embedding module and structural embedding module for each paper into the LLM input, and utilize the logit of <label_token> after decoding as the final prediction score."}, {"title": "5 Experiments", "content": "5.1 Experimental Setup\n5.1.1 Dataset. We adopt the WhoIsWho-IND dataset from the OAG-Challenge in KDD Cup 2024 competition. The statistics of the dataset are as shown in Table 2. This dataset contains about 11.8% incorrect paper-author assignments. For each paper, attributes including title, keywords, abstract, authors, organization, venue, and publication year are provided.\n5.1.2 Baselines. We conduct a comparative analysis of our model against popular IND methods, including a spectrum of traditional machine learning techniques, GNN-based methodologies, and the"}, {"title": "5.3 Main Results", "content": "In Table 3, we provide a holistic comparison of different incorrect assignment detection methods. The LR method only utilizes graph structural features based on partial paper attributes, thus achieving suboptimal performance. Compared to the LR model, GNN-based methods demonstrate superior performance, likely due"}, {"title": "5.4 Ablation Studies", "content": "5.4.1 Effect of different foundation models. We examine the effect of different backbone LLMs in our framework, including ChatGLM3-6B, Qwen2-7B [41], and Llama3-8B. As shown in Table 4, the results"}, {"title": "5.4.2 Effect of different paper attributes", "content": "Figure 3 illustrates the impact of paper attributes used in our framework. We first alter the input paper attributes in the first stage of our framework (without semantic embedding and structural embedding module). We observe that the best performance for a single attribute is achieved by paper titles, followed by paper venues. The reason may lie in that titles and venues encompass rich domain-specific information, which can be well-captured by the LLM. In contrast, organization and author attributes are ad-hoc personal features, often used as effective co-occurred structural features, which are arduous to be utilized by the LLMs. Furthermore, the superiority of the title attribute over the venue is also partly attributed to the finer-grained topical information conveyed in titles.\nTo further investigate the synergistic effects between attributes, we conduct the complete training stages by employing the best single attribute (i.e., title) and another paper attribute. \u201cTitle+venue\" has a small edge over the single usage of the paper title (+0.3% in terms of AUC), possibly due to that titles and venues embody similar domain-specific features. By contrast, both \"Title+org\" and \"Title+author\" each deliver significant improvements over the single title attribute, showing +1.6% and +1.4% increase in terms of AUC, respectively. These improvements are likely due to the complementary nature of the title attribute when combined with either \"org\" or \"author\"."}, {"title": "5.4.3 Effect of different text projectors", "content": "We try several types of text projectors with increased model complexity, including a single linear layer, a two-layer feed-forward network FFNSwish, and Q-Former [20]. The Q-Former is frequently used in multi-modal LLMs"}, {"title": "5.4.4 Effect of #turns in each instruction", "content": "We further study how many target papers to predict in each multi-turn instruction (#turns: i.e., the number of turns) is the best in terms of model accuracy and efficiency. Table 5 shows the accuracy (i.e., AUC and MAP) and efficiency performance with #turns in the range of {1, 5, 10, 20}. The metrics of efficiency includes training time per epoch and the time for one inference pass on the validation set. It can be clearly observed that as #turns increases, the training time and inference time consumed significantly decrease, evidencing an inverse relationship between #turns and time cost. The accuracy improvement of multi-turns over single-turn substantiates the efficacy of the \"soft\" demonstration mechanism in providing additional semantic features for latter papers' decoding."}, {"title": "5.4.5 Effect of three-stage training strategy", "content": "We compare our progressive training strategy with several alternative strategies to inject multi-modal information. (1) From-scratch training of all projectors: injecting semantic embedding module and structural embedding module simultaneously and training text projector and"}, {"title": "5.5 Model Efficiency", "content": "In this subsection, we compare the time efficiency of LLM-based methods, including the best baseline ChatGLM-IND and our model variants. Table 6 presents the detailed time costs for training and inference. Compared to ChatGLM-IND, our base model leverages a shorter input length as well as multi-turn instructions, achieving both fast training and inference speed. As the semantic embedding module and structural embedding module are progressively integrated, the required time also increases gradually. In terms of convergence time, the graph projector converges quickly, while the text projector requires more time. We conjecture that the text encoder (i.e., pre-trained language models) is not pre-trained on the IND task, thus requiring more time to train the text projector for feature alignment. Despite undergoing three training stages, our model still has a significant advantage over ChatGLM-IND. In terms of inference time, our framework is nearly ten times faster."}, {"title": "5.6 Model Ensemble", "content": "To unlock the potential of our framework, we further perform model ensemble by using different paper attributes in our framework. As shown in Figure 7, the best two-model ensemble results are obtained by combining the title attribute (T) with the title and organization attributes (T+TO). This combined approach results in an increase of 0.8% in AUC compared to the single usage of title attributes in the LLM input. Simultaneously, the combination of the title attribute and another attribute (T+TA) also yields decent results, leading to a 0.7% increase in AUC. These improvements indicate that incorporating raw organization texts into the LLM inputs exhibits preferable performance compared with injecting these information into the semantic embedding module. In contrast, the combination of the title and a different attribute (T+TV) results in a decrease in performance compared to using the title attribute alone with a 0.2% reduction in AUC.\nUltimately, by integrating the best three models (T+TO+TA), we achieve the best performance in terms of AUC."}, {"title": "6 Conclusion", "content": "The proposed structure-enhanced language model MIND effectively addresses the incorrect assignment detection problem by integrating both structural and semantic features. MIND is a multi-modal multi-turn instruction tuning framework, which includes task-guided multi-turn instruction tuning, semantic embedding module with rich attributes, and a structural embedding module."}, {"title": "A Training Details", "content": "During the training of the LLaMA model, we utilized a LoRA [16] rank of 8, LORA alpha value of 16, and a dropout rate of 0.05. The per-device batch size was 1, with a gradient accumulation step of 16, resulting in a global batch size of 128 for 8-card training. We use AdamW [25] as the optimizer and the weight decay of 1e-3 was applied. We adopted a cosine learning scheduler with a linear warm-up for the MIND-base model, using a warm-up ratio of 0.1 and a learning rate of 1e-4. In contrast, for the \"+sem\" and \"+graph\" models, we employed a constant scheduler with a learning rate of 5e-5. The \"base\", \"+sem\", and \"+graph\" models were trained for 6, 10, and 4 epochs, respectively, with evaluations conducted every 25 global steps. During the training process, we initiated each phase using the best-performing parameters obtained from the evaluation of the previous phase. For the FFN layer in the semantic/structural embedding module, we set the intermediate hidden size to twice the hidden size of the following LLM.\nWhen using the title and title+venue (TV) as key information, we utilized a context length of 8k tokens, while for title+author (TA) and title+organization (TO), we used a context length of 20k tokens. Considering that there is no official long-text fine-tuning version of LLAMA-3-8B, we used a training-free method [1] to extend its text length.\nFor the ChatGLM3 and Qwen2 models, after some testing, we used LoRA ranks and alpha values of 128 and 256 for ChatGLM3 with 32 and 64 for Qwen2."}]}