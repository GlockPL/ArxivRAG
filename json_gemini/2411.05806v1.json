{"title": "SkipSNN: Efficiently Classifying Spike Trains with Event-attention", "authors": ["Hang Yin", "Yao Su", "Liping Liu", "Thomas Hartvigsen", "Xin Dai", "Xiangnan Kong"], "abstract": "Spike train classification has recently become an important topic in the machine learning community, where each spike train is a binary event sequence with temporal-sparsity of signals of interest and temporal-noise properties. A promising model for it should follow the design principle of performing in-tensive computation only when signals of interest appear. So such tasks use mainly Spiking Neural Networks (SNNs) due to their consideration of temporal-sparsity of spike trains. However, the basic mechanism of SNNs ignore the temporal-noise issue, which makes them computationally expensive and thus high power consumption for analyzing spike trains on resource-constrained platforms. As an event-driven model, an SNN neuron makes a reaction given any input signals, making it difficult to quickly find signals of interest. In this paper, we introduce an event-attention mechanism that enables SNNs to dynamically highlight useful signals of the original spike trains. To this end, we propose SkipSNN, which extends existing SNN models by learning to mask out noise by skipping membrane potential updates and shortening the effective size of the computational graph. This process is analogous to how people choose to open and close their eyes to filter the information they see. We evaluate SkipSNN on various neuromorphic tasks and demonstrate that it achieves significantly better computational efficiency and classification accuracy than other state-of-the-art SNNS.", "sections": [{"title": "I. INTRODUCTION", "content": "Motivation. Spike trains are sequences of binary signals where 1s are spikes and Os are not spikes. Such data are common to a variety of domains and are classically analogous to electrochemical signals in the human brain. Spike train datasets are generated from event cameras, which resemble the human eye. Event cameras, also called neuromorphic cameras, require little energy and are designed to capture objects at high speed. Thus, spike train datasets naturally arise during the development of dynamic vision devices [1]-[5].\nRecently, spike train classification has attracted much atten-tion in the machine learning community [6]\u2013[12]. Compared with the traditional sequence classification tasks, spike train classification is unique in the following two aspects [13]-[16]: 1) Temporal-sparsity of signals of interest. The label of a spike train is only related with certain objects that may only appear in a very small portion of the whole time window. 2) Temporal-noise problem. The signals at the majority of time steps are generated from background activities that are not related to objects of interest. These two properties of spike train classification impede the application of the widely used deep learning models, e.g., recurrent neural networks (RNNs), because of their high computational costs, unnecessarily spent on the whole time window. Tasks on spike train data are instead usually processed on energy-sensitive platforms such as wireless monitors and drones, so more computationally efficient models are required. To this end, we propose a new design principle to guide developement of machine learning models for spike train classification: perform intensive com-putation only when signals of interest appear."}, {"title": "Knowledge Gap.", "content": "Spiking Neural Networks (SNNs) are potential candidates for spike train classification [6], [8], [9], since they are attempt to meet the aforementioned design principle by considering the temporal-sparsity of spike trains. They take spike trains as inputs and outputs, using biologically inspired, event-driven computation and communication in their design. An SNN neuron's core function is to react only when its cummulative membrane potential exceeds a fixed value. As a result, the neuron has a chance to be activated only when it currently has an event signal, which is passed in as a binary spike. Thus, compared to traditional deep learning models, SNNs can build large-scale neural networks with far less energy and memory for spike train classification.\nHowever, SNNs primarily consider the temporal-sparsity of spike trains, overlooking the critical aspect of temporal-noise. As illustrated in Figure 1, consider a scenario where a drone equipped with an event camera detects obstacles to adjust its route. For most of the operational timeline, the camera records signals irrelevant to obstacle detection. Nevertheless, SNNs respond to any detected signals, even if they are merely noise. Consequently, SNNs often fail to adhere to the principle that models should activate only in response to signals of interest. We find that this misalignment is a fundamental factor contributing to the poor generalization and decreased energy efficiency observed in SNNs in real-world applications.\nTo achieve high classification accuracy with low computa-tional cost for real-world spike trains, we need to follow the aforementioned design principle by considering both temporal-sparsity of useful signals and temporal-noise issue. An intu-itive approach, which we pioneer in this paper, is for the model to stop processing data when the relevant object is out of its field of view. This behavior is analogous to how we open and close our eyes to filter out the information we see."}, {"title": "Challenges.", "content": "We propose a novel method for allowing SNNs to efficiently classify spike trains. Solving this problem is challenging for two main reasons:\n\u2022 Neuron Consistency: The promise of SNNs comes from their likeness to real neural circuits in the human brain. Maintaining this similarity is essential to successful SNNs. However, in the standard SNN when a neuron enters a hibernation state, it is hard to wake it up again if there is no new signal input to the network. This means that if the model ignores the input at a timestep, the neurons in the network will lack new input signals. This keeps the neurons silent, making the model likely to ignore potentially useful signals in the future. Thus, when extending SNNs to our problem setting, it is challenging to train successful spiking neurons to skip updates.\n\u2022 Non-differentiability: SNNs are notoriously difficult to train due to non-differentiable nature of spike activ-ity. Most related works use the rectangular function or sigmoid to approximate the corresponding derivative. However, in practice we find that when our optimiza-tion objective considers both accuracy and efficiency, this approximation leads to decayed performance. Ad-ditionally, optimization largely depends on the initial values of the parameters of the model. Even though some parameter initialization methods such as Glorot [17] work for traditional artificial neural networks, they lack theoretical basis in SNNs. Designing an efficient optimization algorithm is the second challenge we face."}, {"title": "Proposed Method.", "content": "To solve our problem, we introduce an event attention mechanism that enables SNNs to dynamically highlight useful signals in the input spike train. We extend existing SNNs to have two different states: awake and hi-bernating, inspired by how people's eyes open and close, turning on and off data intake. If our SNN enters its awake state at time step t, it will consider the input at t. Otherwise, if it hibernates at time step t, it will ignore the input at t. To this end, we design a controller that switches the model between these two states. Since this is not differentiable, we also introduce a new loss function with a penalty that trades off accuracy and computational cost. In this way, our extended SNN learns to mask out noise by skipping updates and shorten the effective size of the computational graph without requiring any additional supervision signal. We refer to our model as SkipSNN and illustrate the difference between it and traditional SNN in Figure 2."}, {"title": "Contributions.", "content": "Our key contributions are as follows:\n\u2022 We define the problem and modeling principle of general spike train classification, which is important for smart dynamic sensor systems with limited energy.\n\u2022 We propose SkipSNN, which solves this problem and can be used on energy-limited dynamic sensor devices.\n\u2022 We develop an efficient optimization technique to train our SkipSNN model.\n\u2022 We demonstrate that our model outperforms recent state-of-the-art alternatives by achieving higher accuracy and lower computational cost when tested on both the neuro-morphic MNIST and DVS-Gesture datasets."}, {"title": "II. PRELIMINARIES", "content": "We begin by describing a one-neuron SNN that accepts a one-feature input. This neuron is a recurrent unit that is affected by current input, and previous output and membrane potential of itself. For each timestep t, it takes xt as input and combines its previous membrane potential ut-1 and output Zt-1 to update ut. For the activation of spiking neuron, when ut exceeds threshold Vth, the neuron will fire at timestep t. We get the output z\u0142 by the step function \u0398(\u00b7) = 0, which satisfies \u0398(x) = 0 when x < 0, otherwise \u0398(x) = 1. Therefore, for each timestep t, the membrane potential ut and output zt are expressed as follows:\n$U_t = \\tau u_{t-1}(1 - z_{t-1}) + wx_t$,\n(1)\n$z_t = \\Theta(u_t - V_{th}),$\n(2)\nwhere hyperparameter \u03c4\u2208 [0, 1] denotes a time decay constant and w is the connection between input and this neuron.\nThe SNN expressed in (1-2) more closely resembles natural neural networks than traditional ANNs do. In this way, we represent neuron as a parallel combination of a \u201cleaky\u201d resistor and a capacitor. The second term of the r.h.s. of (1) is used as external current input to charge up the capacitor to update membrane potential ut. If the neuron emits a spike zt = 1 at timestep t, the capacitor discharges to a resting potential (which we fix at zero throughout this paper) by using the first term in (1).\nA neural network is built by hooking together many of these simple \"neurons\u201d so that the output of a neuron is the input of another. We let ne denote the number of layers in our network and label layer l as Li, so the input layer is L1, and the output layer is Lng. Our network has parameter set W = {W(1),..., W(ne-1), }, where We denotes the parameter associated with the connection between neuron j in layer l, and neuron i in layer l + 1. We also let se denote the number of neurons in layer l. For layer l \u2208 {2,..., ne}, We write ue) = (u,..., use)) and z(e) = = (2,...,z)) to denote the state and output vector of neurons at timestep t. For l = 1, we will use z = xt to denote the input vector. Given a fixed setting of the parameter W and the threshold Vth of each neuron, the update of neurons in all layers is given by:\n$u_t^{(l)} = \\tau u_{t-1}^{(l)}(1-z_{t-1}^{(l)}) + W^{(l-1)}z_{t-1}^{(l-1)}$,\n(3)\n$z_t^{(l)} = \\Theta(u_t^{(l)} \u2013 V_{th}).$\n(4)\nGiven the expressions above, we can solve a standard SNN classification problem by training a classifier f: RPXT \u2192 {1, ..., N} on a given dataset {(x(1), y(1)), ..., (x(K), y(K))} that contains K training samples, of which each instance x(i) \u2208 RP\u00d7T has an observed label y(i) \u2208 {1,...,l(N)}. P is the number of input entries and T denotes the length of spike train. To train such SNN, we can minimize the following loss function L for a single training example (x, y):\n$L= \\frac{1}{2} (y - Mz_t^{(ne)})^2$\n(5)\nwhere zt, denotes the voting vector of the last layer N at time step t, M denotes a constant voting vector connecting neurons in the output layer to a specific class."}, {"title": "IV. METHODOLOGY", "content": "In this paper, we propose a novel modification for existing SNN architectures that allow them to mask noise and skip membrane potential updates without requiring any additional supervision signal. To build a new architecture, which we call SkipSNN, we view SNN neurons as having two different states: awake and hibernating. Then, we design a new neuron to control the model to switch freely between these two states. Inspired by [18], we augment the network with a binary gate, but use a novel neuron to control it. This neuron doesn't have any special settings, but follows the basic mechanism of SNNs, exactly like other neurons in the network. In the rest of this paper, in order to distinguish it from other SNN neurons, we call this neuron the controller.\nModel Description. Let vt denote the membrane potential of the controller at time step t. This controller is connected with all the neurons in the first layer. It is therefore affected by the outputs of the first layer and will generate a binary output at according to the SNN mechanism described in Equations 1 and 2. Then, this value decides whether to consider the input of the next time step by treating at as a multiplier of the next time step. If it emits a spike, this means that the network enters the awake state at next time step. Otherwise, the network hibernates at next time step. Based on the SNN mechanism, the controller will reset its membrane potential to zero after emit a spike. Therefore, when it decides to enter the awake state at the next time step, it is hard for the controller to spike again due to the lack of new spike inputs from the first layer.\nIn order not to miss the potential useful signals in the future, a temporal form of bias is needed to adjust the membrane potential of the controller. In this paper, we introduce synchro-nisation pulses that act as additional inputs to the controller, in order to provide the information of time. These can be thought of as similar to internally-generated rhythmic activity in biological networks, such as alpha waves in the visual cortex [30] or theta and gamma waves in the hippocampus [31].\nIn our proposed SkipSNN, a set of pulses are fully con-nected to the controller in the network. Each pulse spikes at a different frequency. For example, some spike once every time step, some spike once every 10 time steps, and some spike once every 100 time steps. Subsequently, these pulses can modify the membrane potential of the controller when it stays hibernating state, thereby being activated again. At every time step t, affected by the output from the neurons in the first hidden layer and pulses, the controller emits a binary signal, which is multiplied by the model input at t + 1. The resulting architecture depicted in Figure 3 can thus be described as follows:\n$u_t^{(2)} = \\tau u_{t-1}^{(2)}(1-z_{t-1}^{(2)}) + a_{t-1}W^{(1)} x_t$,\n(6)\n$v_t = \\tau v_{t-1}(1 - a_{t\u22121}) + W_z z_t^{(2)} + W_o o_t,$\n(7)\n$a_t = \\Theta(v_t - V_{th}),$\n(8)\nwhere W\u2082 and W. are the weights vectors between the controller and neurons in the first hidden layer, and the controller and pulses, respectively. ot = (01,t, ..., Op,t) is the pulse vector that contains the signals from p different pulses at t. Ut and at are the membrane potential and the output of the controller at t, respectively.\nAccording to the model formulation from Equations 6-8, SkipSNN can switch between awake and hibernating state based on the value of at. If at = 1, SkipSNN will be updated based on the input xt+1 at t+1, otherwise, the proposed model will skip xt+1 due to atXt+1 = 0.\nIn particular, the controller can be connected to any layer of SNN. Meanwhile, the controller itself can also be a multilayer SNN. However, in practice, we found that the controller works better when connected to the first hidden layer. In addition, using a multi-layer network structure as the controller does not improve the performance but will increase the computational cost. Thereby, in this paper, we keep using the network structure depicted in Figure 3."}, {"title": "Limiting Computation.", "content": "The proposed SkipSNN learns when to enter awake or hibernating state without requiring any additional supervision signal. The longer the model stays hibernating state, the lower computational cost required dur-ing inference phase. However, there is a trade-off between classification accuracy and computational power. If the model sacrifices some important time steps that contain useful in-formation, it can reduce the computational cost, but it will also sacrifice the accuracy of the model. To balance between accuracy and efficiency, we add an additional penalty term\n$L_{penalty} = \\lambda \\frac{\\sum_{t=1}^T a_t}{T},$\n(9)\nwhere Lpenalty is the cost associated to one single sample, A is the cost per sample, and T is the spike train length."}, {"title": "Optimizing SkipSNN.", "content": "SNN is hard to optimize because the derivative of its activation function is a d function, whose value is zero everywhere except at threshold. As is done in prior works [6], [8], we could use the rectangular function to approximate the corresponding derivative. However, the switch between awake and hibernating state is directly determined by the output of the controller. When the corresponding gradient of its membrane potential is zero, we will not be able to continue to optimize the relevant parameters of the controller through gradient descent. Therefore, we propose simulated annealing for SkipSNN; the training process for SkipSNN is divided into two stages.\nIn the first stage, we set x = 0, which will cause the model to stay in the awake state at all times. This means that the controller will always emit at = 1 during this stage, because its membrane potential vt is always larger than the threshold Vth. Therefore, our goal at this stage is to make SkipSNN solve the classification task as accurately as possible. In this stage, the derivative of each activation is approximated by the rectangular function denoted by h(u):\n$h(u) = \\frac{1}{\\epsilon}sign (\\left|u - V_{th}\\right| < \\frac{\\epsilon}{2}),$\n(10)\nwhere e determines the peak width.\nIn the second stage, we freeze parameters that are not related to the controller, and start optimizing the controller by elevating the multiplier A of the penalty loss. Because vt at that point is always larger than Vth, we deploy a sigmoid function instead of rectangular function to approximate its derivative:\n$h(u) = \\frac{1}{1+e^{\\Delta(u-V_{th})}}$\n(11)\nThe parameter A in Equation 11 controls the steepness of the sigmoid function, which is considered to be the reciprocal pseudo-temperature. As \u2206 \u2192 0, the sigmoid becomes a step function and the stochastic unit becomes deterministic. As \u25b2 increases, this sharp threshold is \u201csoftened\", thus making the range of the sigmoid wider. Therefore in this stage we initialize A with a high value to make sure the controller has a gradient when the membrane potential is high. Then, we decrease A periodically during the optimization process of the parameters related to the controller. We also summarize the overall training process of our proposed SkipSNNs as pseudo-code in Algorithm 1.\""}, {"title": "V. EMPIRICAL STUDY", "content": "To comprehensively validate the effectiveness of our pro-posed method, we conduct experiments to answer two re-search questions: First, we are interested in accuracy im-provement on classification problem of spike trains; Second, we want to demonstrate that our model can significantly reduce computational cost with very negligible degradation in accuracy. As ours is the first SNN proposed to deal with our defined problem, we compare our model with Fixed-skip SNN and Random-skip SNN. We also use a modified SNN converted from SkipRNN [18] as a baseline, because SkipRNN is a cutting-edge model for skipping frames on time series, which is similar to our defined problem. To better compare our work with them, we test on various neuromorphic datasets, including N-MNIST and DVS-Gesture. Both are widely used to evaluate SNN models in related work. To combat randomness in the experiment system, we run all experiments 10 times and report the average results, except"}, {"title": "A. Datasets", "content": "We evaluate our proposed model and baselines on various datasets. To simulate the properties of general spike trains in the real world, we modified these neuromorphic datasets (N-MNIST\u00b9 and DVS-Gesture\u00b2) by making signals of interest temporal-sparse and adding noise into them.\nN-MNIST: The N-MNIST dataset [32] consists of MNIST images converted into a spiking dataset using a Dynamic Vision Sensor (DVS) moving on a pan-tilt unit. In our ex-periments, each dataset sample is 50 ms long, with a shape of 34 \u00d7 34 pixels, containing two channels to preserve \u201con\u201d and \"off\" spikes, respectively. This dataset is harder than MNIST because one has to deal with saccadic motion. For SkipSNN experiments, we generate a 300 ms blank time sequence firstly, meanwhile increasing the noise by adding one signal to a random pixel of the image in each millisecond (one time step). Then we put each N-MNIST sample into a random period of each time sequence. So in each final sequence, only 16.7% of the time steps have useful signals related to original N-MNIST dataset. The dataset is split into 60,000 training samples and 10,000 testing samples.\nDVS-Gesture: The DVS-Gesture dataset [33] contains 1, 342 instances of a set of 11 hand and arm gestures, grouped into 122 trials and collected from 29 subjects under 3 differ-ent lighting conditions. During each trial, one subject stood against a stationary background and performed all 11 gestures sequentially under the same lighting conditions. The problem is to identify the correct action label associated with each action sequence video. In our experiments, each DVS-Gesture sample is 400 ms long, and 32 \u00d7 32 pixels big, containing two"}, {"title": "B. Compared Methods", "content": "To demonstrate the effectiveness of SkipSNN, we test against several baselines:\n\u2022 Fixed-skip SNN: Control the percentage of awake state according to the fixed time-step size. For example, 90% means skip 1 time step after every 9 time step updates; 50% means skip one time step after every one time step.\n\u2022 Random-skip SNN: Determine awake and hibernating state through Bernoulli sampling. We control the per-centage of awake states by changing the parameter of Bernoulli distribution.\n\u2022 SkipRNN + SNN: SkipRNN [18] is proposed to extend existing RNN models by learning to skip state updates and reduce the computational cost. The input and output of SkipRNN are not spike trains. But due to the similarity of the defined problem, we convert SkipRNN to an SNN version as a competitor.\nTo better compare our work with them, we use the same network structure and optimization algorithm on each model."}, {"title": "C. Evaluation metrics", "content": "To evaluate classification performance, we use the standard Accuracy metric. To evaluate the computational efficiency, we use the million floating point operations (MFLOPs) to measure the potential speedup. MFLOPs are computed by assuming"}, {"title": "D. Experiment results", "content": "In this section, we separately discuss the experimental results related to each of the two research questions.\nBetter accuracy: The results shown in Figure 4 compare the performance of each model with different percentage of updated time-steps on two different datasets. We control the percentage of awake state of SkipRNN+SNN and SkipSNN by changing the multiplier A of the time-budget penalty. For random-skip SNN, we control it through the parameter of Bernoulli distribution. For fixed-skip SNN, we control it according to different time-step sizes. According to the results, we can observe that as the percentage of awake state decreases, fixed-skip and random-skip SNN drop rapidly. By contrast, SkipRNN+SNN and SkipSNN show their advantages in this scenario. Especially, when each model only considers less than 20% of awake state, SkipRNN+SNN and SkipSNN still maintain an accuracy of more than 80%. In contrast, the other two models are no longer valid. According to the comparison of SkipSNN+SNN and SkipRNN, our proposed model has a better performance of classification than the other one on the testing set of both two datasets. This robustness in performance directly leads to a significant drop of the computational cost during inference phase. Moreover, the accuracy of SkipSNN with the percentage between 90% and 70% is even higher than that without skipping time steps. It means that compared with traditional SNNs, our proposed model is more accurate in dealing with the classification of spike trains.\nExamples in Figure 5 illustrate how SkipSNN learns to skip time steps that contain primarily noise or unhelpful for classi-fication. Leveraging the control SkipSNN has over time-step skipping, we visualize its decisions on each time step through a temporal raster plot of spikes in the controller during inference on two datasets. The blue areas in each plot indicate periods of useful signals, while the white areas represent mostly noise. Figure 5 shows that spikes are concentrated in the blue areas and sparse in the white, demonstrating SkipSNN's ability to distinguish between signals of interest and noise, effectively switching between awake and hibernating states.\nFinally, the reason for the success of our proposed model is that SkipSNN filters out noise with the presence of controller architecture, reconstructing input that is much purer than the original input. We demonstrate it in Figure 6. We show the gif picture of original data and that of reconstructed data, which mask out most of the noise through SkipSNN.\nPotential speedup: The tables shown in Table I and II compare SkipSNN with a traditional SNN, fixed-skip SNN, random-skip SNN, and SkipRNN+SNN on two neuromorphic datasets. For N-MNIST dataset, even without a complex architecture, the proposed SkipSNN and their competitors still perform well. We find that there is only a slight difference in accuracy between SkipSNN and SNN (i.e., only about 2%). This is a negligible difference. However, as we can observe, there is a significant improvement in the MFLOP count between SkipSNN and SNN. Compared to SNN, SkipSNN can provide only half the computational cost of SNN. Meanwhile, when the computational cost of all models is similar, SkipSNN achieves the higher accuracy than fixed-skip SNN, random-skip SNN and SkipRNN+SNN.\nOn DVS-Gesture, our proposed model SkipSNN incurs a slight degradation in accuracy (i.e., decrease about 6%), but provides a significant speedup more than 10x times. The accuracy of SkipSNN at this level of MFLOP is still higher than other competitors. Our experimental results show that SkipSNN using our proposed network structure can greatly speed up inference while only incurring a minimal and neg-ligible loss in classification performance. In summary, our proposed model achieves better computational efficiency than previous works when tested on neuromorphic datasets and achieves very negligible degradation in accuracy. Moreover, according to Table I and II, our model can also improve the accuracy performance when we consider the trade-off between accuracy and computational cost. When SkipSNN considers about 90% of updated time-step, it can slightly increase accuracy (i.e., increase about 6% on N-MNIST and 0.7% on DVS-Gesture). This observation is consistent with the conclusion of the first research question."}, {"title": "VI. CONCLUSION", "content": "To address spike train classification effectively, a model should focus computational effort only when relevant signals appear. Current SNNs overlook temporal noise, making them computationally intensive and power-hungry. To overcome this, we introduce SkipSNN, a novel model incorporating an event attention mechanism that enables dynamic emphasis on useful signals. SkipSNN reduces computational cost by selectively skipping membrane potential updates, effectively filtering out noise. In tests on neuromorphic datasets N-MNIST and DVS-Gesture, SkipSNN outperforms fixed- and random-skip SNNs, as well as an SNN adaptation of SkipRNN, achieving superior efficiency and accuracy."}]}