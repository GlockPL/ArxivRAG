{"title": "Imputation for prediction:\nbeware of diminishing returns.", "authors": ["Marine Le Morvan", "Ga\u00ebl Varoquaux"], "abstract": "Missing values are prevalent across various fields, posing challenges for training\nand deploying predictive models. In this context, imputation is a common practice,\ndriven by the hope that accurate imputations will enhance predictions. However,\nrecent theoretical and empirical studies indicate that simple constant imputation\ncan be consistent and competitive. This empirical study aims at clarifying if\nand when investing in advanced imputation methods yields significantly better\npredictions. Relating imputation and predictive accuracies across combinations of\nimputation and predictive models on 20 datasets, we show that imputation accuracy\nmatters less i) when using expressive models, ii) when incorporating missingness\nindicators as complementary inputs, iii) matters much more for generated linear\noutcomes than for real-data outcomes. Interestingly, we also show that the use of\nthe missingness indicator is beneficial to the prediction performance, even in MCAR\nscenarios. Overall, on real-data with powerful models, improving imputation only\nhas a minor effect on prediction performance. Thus, investing in better imputations\nfor improved predictions often offers limited benefits.", "sections": [{"title": "1 Introduction", "content": "Databases are often riddled with missing values due to faulty measurements, unanswered question-\nnaire items or unreported data. This is typical of large health databases such as the UK Biobank\n[Sudlow et al., 2015], the National Health Interview Survey [Blewett et al., 2019] and others [Perez-\nLebel et al., 2022]. Statistical analysis with missing values has been widely studied, particularly\nto estimate parameters such as means and variances [Little and Rubin, 2019]. However, how to\nbest deal with missing values for prediction has been less studied. Since most machine learning\nmodels do not natively handle missing values, common practice is to impute missing values before\ntraining a model on the completed data, often with the expectation that \"good\" imputation improves\npredictions. Considerable efforts have been dedicated to improving imputation techniques, utilizing\nGenerative Adversarial Networks [Yoon et al., 2018], Variational AutoEncoders [Mattei and Frellsen,\n2019], optimal transport [Muzellec et al., 2020] or AutoML-enhanced iterative conditional imputation\n[Jarrett et al., 2022] among others. Most of these studies concentrate on imputation accuracy without\nassessing performance on subsequent tasks. However, theoretical arguments suggest that good impu-\ntation is not needed for good prediction [Le Morvan et al., 2021, Josse et al., 2019]. These arguments\nare asymptotic and whether they hold in typical cases is debatable. To address the discrepancy\nbetween this theory and the emphasis on imputation efforts, there is a critical need for empirical\nstudies to determine whether better imputations actually lead to better predictions.\nTheory does establish that in some scenarios, better imputations imply better predictions. For instance\nwith a linearly-generated outcome, the optimal prediction writes as a linear model on the optimally-\nimputed data Le Morvan et al. [2021]. Thus, using a linear model for prediction, better imputations\ntypically yield better predictions. Interestingly though, theoretical results indicate that for linear"}, {"title": "2 Related work", "content": "Benchmarks. Several benchmark studies have investigated imputation in a prediction context\n[Paterakis et al., 2024, J\u00e4ger et al., 2021, Ramosaj et al., 2022, Wo\u017anica and Biecek, 2020, Perez-\nLebel et al., 2022, Poulos and Valle, 2018, Shadbahr et al., 2023, Li et al., 2024]. However, drawing\ndefinitive conclusions from most studies is challenging due to various limitations in scope and\nexperimental choices. For example, Bertsimas et al. [2018], Li et al. [2024] trained imputation\nmethods using both the training and test sets, rather than applying the imputation learned on the\ntraining set to the test set, which is not possible with many imputation packages. This approach\ncreates data leakage. Wo\u017anica and Biecek [2020] trained imputers separately on the train and test sets,\nwhich creates an \"imputation shift\" between the training and test data. J\u00e4ger et al. [2021] discards\nand imputes values in a single column of the test set, chosen at random and fixed throughout the\nexperiments. Yet as they note, conclusions can change drastically depending on the importance\nof the to-be-imputed column for the prediction task or its correlation with other features. Some\nstudies [Poulos and Valle, 2018, Ramosaj et al., 2022] use a small number of datasets (resp. 2 and 5\ndatasets from the UCI machine learning repository respectively), thus limiting the significance of their\nconclusions. Wo\u017anica and Biecek [2020] do not perform hyperparameter tuning for the prediction\nmodels, while Ramosaj et al. [2022] tunes hyperparameters on the complete data, though it is unclear\nwhether the best hyperparameters on complete data are also the best on incomplete data. Furthermore,\nsome benchmarks focus on specific types of downstream prediction models, such as linear models\n[J\u00e4ger et al., 2021], AutoML models [Paterakis et al., 2024] or Support Vector Machines [Li et al.,\n2024], meaning their conclusions should not be generalized to all types of downstream models. The\ndata used in benchmarks also affects the scope of the conclusions. For instance, Paterakis et al. [2024]\nconsiders 35 datasets, most with only a few hundred samples and a 50% train-test split, making"}, {"title": "3 Experimental setup.", "content": "Imputation methods. We chose four imputation models to cover a wide range of imputation\nqualities. Importantly, our goal is not to extensively benchmark different imputation strategies but to\nunderstand the link between imputation quality and prediction performance.\nmean\neach missing value is imputed with the mean of the observed values in a given variable. It\nprovides a useful baseline for assessing the effectiveness of advanced techniques.\niterativeBR - each feature is imputed based on the other features in a round-robin fashion using a\nBayesian ridge regressor. This method is related to mice [Van Buuren and Groothuis-Oudshoorn,\n2011] in that it is also based on a fully conditional specification [Van Buuren, 2018]. It is\nimplemented in scikit-learn's IterativeImputer [Pedregosa et al., 2011].\nmissforest [Stekhoven and B\u00fchlmann, 2012] - operates in a manner analogous to iterativeBR,\nwherein it imputes one feature using all others and iteratively enhances the imputation by sequen-\ntially addressing each feature multiple times. The key distinction lies in its utilization of a random\nforest for imputation rather than a linear model. We used scikit-learn's IterativeImputer\nwith RandomForestRegressor as estimators. Default parameters for Missforest were set to\nn_estimators=30 and max_depth=15 for the random forests (the higher the better) to keep a"}, {"title": "4 Results: Determinants of predictions performance with MCAR missingness", "content": "Figure 1 summarizes the relative performance of the various predictors combined with the different\nimputation schemes across the 20 datasets. Some trends emerge: more sophisticated imputers tend\nto improve prediction, with missForest-based predictors often outperforming those using condexp\nor iterativeBR imputers, which in turn outperform predictors based on mean imputation. However,\nusing the missingness indicator decreases this effect. Additionally, this effect is barely noticeable for\nthe best predictor, XGBoost, which appears to maintain its advantages on tabular data [as described\nin Grinsztajn et al., 2022] even in the presence of missing values.\nThat the best predictor barely benefits from fancy imputers brings us back to our original question:\nshould efforts go into imputation? Drawing a conclusion from fig. 1 would be premature: the variance\nacross datasets is typically greater than the difference in performance between methods (critical\ndifference diagram in figs. 6 and 7). For example, missforest + XGBoost + indicator outperforms\nall other methods on only a third of datasets. Additionally, XGBoost + indicator does not perform\nsignificantly better with missforest than with condexp, while mean imputation does not always lead\nto the worst prediction. In what follows, we focus on quantifying the effects of improved imputation\naccuracy on predictions in different scenarios."}, {"title": "4.2 A detour through imputation accuracies: how do imputers compare?", "content": "Although comparing imputers is not our main objective, it is enlightening for our prediction purpose\nto characterize their relative performance range. Figure 2 (left) gives imputation performances\nmeasured as the R\u00b2 score between the imputed and ground truth values, relative to the average across\nmethods and missing rates for each dataset. At 20% missing rate, the best imputer is missforest,\nfollowed by condexp and iterativeBR, nearly tied, and far behind mean imputation. At 50% missing\nrate, the imputation accuracy of all but mean imputation drop, but interestingly condexp is much\nless affected. It is interesting that such a simple method performs best. It is notably two orders of\nmagnitude faster than missforest (figure 2 right), which makes it an imputation technique worth\nconsidering. It is possible that the gaussianization of the features helped condexp, although a\nfeature-wise gaussianization does not produce a jointly Gaussian dataset.\nThis work does not aim to compare or identify the best imputers, but rather to achieve varying\nimputation qualities to highlight the link between imputation and prediction quality. In this regard, the\nhigh range of imputation accuracy between the best and worst methods (an average difference of 0.5\nR2 points at 20% and 0.3 R2 points at 50%) allows capturing differences in prediction performance."}, {"title": "4.3 Linking imputation accuracy and prediction performances.", "content": "Combining the four imputation techniques with 10 repetitions of each experiment yields 40 (im-\nputation R\u00b2, prediction R\u00b2) pairs for each model and dataset. To quantify how improvements in\nimputation accuracy translate into downstream prediction performance, we fit a linear regression\nusing these 40 points for each model and dataset. Figure 3 gives two examples of such fit: on\nthe Bike_Sharing_Demand dataset, for a missing rate of 50%, the prediction R\u00b2 increases as a\nfunction of the imputation R\u00b2; the effect is greater for the MLP, for which the fit gives a slope of 0.24,\nthan for the MLP with indicator for which the slope is only 0.03. Figure 4 summarizes the slopes\nestimated using the aforementioned methodology across all datasets, predictors with and without\nthe indicator, and varying missing rates. Firstly, the fact that most slopes are positive indicates that\nbetter imputations correlate with better predictions, aligning with common beliefs. However, this\nobservation should be nuanced by the size of the effects.\nGains in prediction R\u00b2 are 10% or less of the gains in imputation R\u00b2. Figure 4 shows that\nthe slopes are typically small, rarely exceeding 0.1. This implies that an improvement of 0.1 in\nimputation R\u00b2 typically leads to an improvement in prediction R\u00b2 that is 10 times smaller, i.e. a gain\nof 0.01 in prediction R\u00b2, or even less. For XGBoost, the average slope across datasets in rather close\nto 0.025 or less (even zero without the mask at 20% missing rate). Thus, an enhancement of 0.3 in"}, {"title": "4.4 Why is the indicator beneficial, even with MCAR data?", "content": "In general, we find that adding the missingness indicator really helps prediction. While it is expected\nthat adding the indicator is beneficial in MNAR scenarios, as the missingness is informative, it is less\nobvious in the MCAR settings studied here. Indeed, the indicator contains absolutely no relevant\ninformation for predicting the outcome. To the best of our knowledge, the benefit of using an indicator\nin MCAR has not yet been established. Below, we propose a theoretical insight to explain this finding.\nThe best possible predictor in the presence of missingness can always be expressed as the composition\nof an imputation and a prediction function [Le Morvan et al., 2021]. But, in general, the best prediction\nfunction on the imputed data can be challenging to learn, even for perfect conditional imputation. In\nfact, it often displays discontinuities on imputed points. We hypothesize that adding the missingness\nindicator simplifies modeling functions that exhibit discontinuities at these points, as the indicator\ncan act as a switch to encode these discontinuities.\nThe case of XGBoost in Figure 1 illustrates the importance of keeping the missingness information\nencoded. For 50% missing rate, in the absence of an indicator, no imputation really benefits prediction\nwith XGBoost, and the best option is to use the native handling of missing values. This suggests that\nXGBoost benefits from knowing which values are missing. With advanced imputations, distinguishing\nbetween imputed and observed values becomes challenging. Appending the indicator to the imputed\ndata reinstates the missingness information unambiguously, which enables XGBoost to benefit from\nmore advanced imputations, in particular missforest."}, {"title": "5 Conclusion", "content": "For prediction, imputation matters but marginally. Prior theoretical work showed that in extreme\ncases (asymptotics), imputation does not matter for predicting with missing values. We quantified\nempirically the effect of imputation accuracy gains on prediction performance across many datasets\nand scenarios. We show that in practice, imputation does play a role. But various factors modulate the\nimportance of better imputations for prediction: investing in better imputations will be less beneficial\nwhen a flexible model is used, when a missing-value indicator is used, and if the response is thought to\nbe non-linear. These results are actually in line with the theoretical results suggesting that imputation\ndoes not matter, as these hold for very flexible models (ie universally consistent). A notable new\ninsight is that adding a missing-value indicator as input is beneficial for prediction performances even\nfor MCAR settings, where missingness is uninformative.\nWe show that large gains in imputation accuracy translate into small gains in prediction performance.\nThese results were drawn from a favorable MCAR setting, and it is likely that with native missingness,"}]}