{"title": "DeepCodeProbe: Towards Understanding What Models Trained on Code Learn", "authors": ["VAHID MAJDINASAB", "AMIN NIKANJAM", "FOUTSE KHOMH"], "abstract": "Machine Learning (ML) models trained on code and artifacts extracted from them (e.g., version control histories, code differences, etc.), provide invaluable assistance for software maintenance tasks. Despite their good performance, these models can make errors that are difficult to understand due to their large latent space and the complexity of interactions among their internal variables. The lack of interpretability in these models' decision-making processes raises concerns about their reliability, especially in safety-critical applications. Furthermore, the specific representations and features these models learn from their training data remain unclear, further contributing to the interpretability challenges and hesitancy in adopting these models in safety-critical contexts. To address these challenges and provide insights into what these models learn from code, we present DeepCodeProbe, a probing approach designed to investigate the syntax and representation learning capabilities of trained ML models aimed at software maintenance tasks. Applying DeepCodeProbe to state-of-the-art code clone detection, code summarization, and comment generation models provides empirical evidence that while small models capture abstract syntactic representations relevant to their tasks, their ability to learn the complete programming language syntax is limited. Our results show that increasing model capacity by increasing the number of parameters without changing the architecture improves syntax learning to an extent but introduces trade-offs in training and inference time alongside overfitting. DeepCodeProbe also uncovers specific code patterns and representations the models learn based on how code is represented for training the models. Our proposed probing approach uses a novel abstract syntax tree-based data representation that allows for probing models for syntax information. Leveraging our experimentation with the different models, we also provide a set of best practices for training models on code, to enhance both the models' performance and interpretability for code-related tasks. Additionally, our open-source replication package allows the application of DeepCodeProbe to interpret other code-related models. Our work addresses the critical need for reliable and trustworthy ML models in software maintenance. The insights from DeepCodeProbe can guide the development of more robust and interpretable models tailored for this domain.", "sections": [{"title": "1 INTRODUCTION", "content": "Effective software maintenance is key to ensuring the long-term reliability and sustainability of software systems. Over time, growing codebases and rising code complexity pose challenges for maintaining software systems [30]. Machine learning (ML) models offer assistance in addressing software maintenance tasks such as code clone detection, fault localization, code smell detection, etc. [85]. Utilizing ML models trained on expansive code and related artifacts (e.g., statistics about the code such as the number of defined variables and function calls, documentation, control/data flow graphs, etc.) allows for facilitating various phases of software maintenance [85]. For example, data from the revision history of a project can be used for defect prediction, helping to address problems early on [5]. Moreover, leveraging Natural Language Processing (NLP) techniques alongside ML models has been shown to improve documentation interpretation, generation, and change assessment; reducing developers' time and mistakes [25]. Furthermore, Applying ML models for code clone detection helps to locate similar code inside the codebase, assisting developers in handling issues affecting multiple code copies [35].\nDespite these benefits, however, there exist challenges concerning model interpretability [60] and the reliability of ML models in production [61]. Due to the black-box nature of state-of-the-art ML models, interpreting their decisions is difficult [9]. This lack of interpretability can lead to significant issues in software development and maintenance. For instance, when developers cannot interpret the decisions made by code models, debugging and repairing code becomes more complex and time-consuming [70, 93]. Misinterpreted model outputs can introduce subtle bugs that are hard to trace, potentially compromising software reliability and increasing maintenance costs [43, 52, 69]. Additionally, in collaborative environments, the inability to understand model decisions can hinder effective team communication and slow down the development process [51, 56, 79]. Therefore, understanding what ML models learn from their training data and how they generate outputs regarding the underlying task, is crucial for establishing their reliability and trustworthiness within safety-critical applications and production environments [4, 22]. Furthermore, examining the internals of ML models contributes to improving the overall performance by identifying decision-making errors that are amenable to fine-tuning or alternative architectural and data representation choices of models. One of the approaches used for this purpose is probing; in which a simple classifier is trained on the embeddings extracted from a model to predict a property of interest [29]. Probing ML models facilitates understanding the relationships between the models' input features and output predictions, allowing practitioners to assess whether the learned representations are aligned with expectations [92]. Given the recent rise of Large Language Models (LLM), researchers have been proposing novel approaches for probing LLMs for a variety of tasks and training data [12]. However, the majority of the proposed probing approaches are only applicable to transformer-based models with a large number of parameters. As such, there exists a gap in the current literature with a lack of probing techniques focusing on much smaller models trained on code for software maintenance tasks. To address this gap and given the benefits of having reliable models for software maintenance, we propose DeepCodeProbe, a probing approach designed for ML models trained on code for software maintenance tasks. DeepCodeProbe allows gaining insights into what ML models learn from their training data. Our probing approach is focused on identifying whether ML models used in software maintenance tasks learn the syntax of the programming languages that they have been trained on and the representations they learn from their inputs to achieve their objectives.\nDeepCodeProbe is built upon the principles of AST-Probe [23], which examines LLMs' capability of encoding programming languages' syntax into their latent space. However in contrast to AST-Probe, DeepCodeProbe specifically"}, {"title": "2 BACKGROUND", "content": "In this section, we will review the necessary background and concepts related to our probing approach. DeepCodeProbe is designed to probe code models for software maintenance, therefore, we will first review the literature on ML for software maintenance. Afterward, we will discuss the probing approaches that have been proposed in NLP for ML models. Finally, as DeepCodeProbe is built on top of AST-Probe, we will describe AST-Probe and its underlying concepts."}, {"title": "2.1 Machine Learning for Software Maintenance", "content": "Software maintenance is a critical activity throughout the software development lifecycle. It involves continuous updates, testing, and improvements to ensure that software functions optimally according to evolving user requirements while preserving its original capabilities [55]. Given the increasing sophistication and size of contemporary software systems [30], their maintenance can become laborious, time-consuming, and error-prone [7, 45]. Consequently, numerous ML-based techniques have been proposed to simplify and enhance various aspects of software maintenance.\nBy training ML models on different software artifacts, such as source code [8, 21], version control history [5], documentation [19], and software metrics [31], these models can assist developers at various stages of software maintenance [85]. The primary applications of ML in software maintenance include:\n\u2022 Defect prediction: ML models have been proposed to help developers anticipate potential problems in specific areas of the codebase. This enables efficient resource allocation and proactive issue resolution [13, 39, 71].\n\u2022 Program repair: ML models have been proposed to help with automatically fixing common bugs. This can significantly reduce manual effort, minimize human errors, and save resources [8, 41, 74].\n\u2022 Code clone detection: ML models have been proposed to detect clones in the codebase by identifying duplicate or similar segments of codes to facilitate refactoring efforts, ensuring consistency and reducing redundancy [17, 82].\n\u2022 Code summarization: Proposed ML models allow for understanding complex code logic, easier maintainability, and collaboration by generating abstract representations of code blocks [1, 33].\n\u2022 Comment generation: By producing descriptive comments, ML models allow for automatically increasing the intelligibility of code, ease of knowledge transfer, and support long-term maintenance activities [25, 37].\nAs our study is focused on ML models that are trained on code, we choose two representative tasks in software maintenance, namely code clone detection and code summarization, for probing and review the literature on these tasks, below."}, {"title": "2.1.1 Code Clone Detection.", "content": "Code Clone Detection (CCD) is an important activity in software maintenance as a fault in one project can also be present in other projects with similar code. Identifying and correcting these clones is essential to ensure overall software quality. Tasks such as software refactoring, quality analysis, and code optimization often necessitate the identification of semantically and syntactically similar code segments [59]. In the CCD literature, clones are categorized into four types [58]:\n\u2022 Type-1: Exact copies except for whitespace/comments.\n\u2022 Type-2: Syntactically identical with different identifiers.\n\u2022 Type-3: Similar fragments with added/removed statements.\n\u2022 Type-4: Different syntax but similar semantics.\nDetecting Type-1 and Type-2 clones is relatively straightforward, as they can be identified through syntactic comparisons. However, Type-3 and Type-4 clones pose a greater challenge, as they require capturing semantic similarities beyond surface-level syntax. Traditional approaches, such as text-based or token-based comparisons, often struggle with these more complex clone types [35]. To address this problem, the representation learning capabilities of Deep Learning (DL) models allow for the use of representations extracted from code such as Abstract Syntax Trees (ASTs), Control Flow Graphs (CFGs), and software metrics for CCD. By utilizing such representations, DL-based approaches such as FCCA [26], CoCoNut [41], and Cclearner [36] have achieved high performance on CCD benchmarks."}, {"title": "2.1.2 Code Summarization and Comment Generation.", "content": "Code summarization and comment generation are closely related tasks that aim to enhance code comprehension and maintainability. Code summarization involves generating concise natural language descriptions that capture the functional behavior or high-level semantics of a given code snippet [86]. On the other hand, comment generation focuses on producing comments that explain the purpose and functionality of specific code segments [65]. Several DL-based approaches have been proposed for code summarization and comment generation. Allamanis et al. [3] train a Convolutional Neural Network (CNN) with an attention mechanism for code summarization. This approach leverages the strengths of CNNs in capturing local patterns while the attention mechanism helps in focusing on the most relevant parts of the code for generating accurate summaries. Another work done by Hu et al. [25] uses the ASTs constructed from code snippets to train a sequence-to-sequence model for comment generation for Java methods. By encoding the structural information from the AST, their model can better capture the code's semantics and produce more relevant comments tailored to the specific code segments. Wei et al. [81] trained two models for code generation and code summarization by initializing a sequence-to-sequence model for each task and"}, {"title": "2.2 Probing in Natural Language Processing", "content": "Probing is used in NLP as a methodology for assessing the linguistic capabilities learned by ML models [12]. In general, probing involves evaluating how well the model embeddings capture syntactic [24], semantic [84], or other types of linguistic properties inherent in language data [18, 53]. Probes are classifiers trained to predict specific features based on learned representations generated by trained models and a high classification accuracy indicates that the target feature is sufficiently encoded in those representations [76]. NLP probes can be categorized into several types, each focusing on different linguistic aspects [6, 40]:\n\u2022 Syntactic probes: These probes assess whether the model representations capture syntactic properties of language, such as part-of-speech tags, constituent or dependency parsing structures, and word order information.\n\u2022 Semantic probes: These probes aim to evaluate the model's ability to encode semantic information, such as semantic roles, and lexical relations.\n\u2022 Coreference probes: Coreference resolution is the task of identifying and linking mentions that refer to the same entity within a text [48]. Probing for coreference capabilities helps understanding if the model can capture long-range dependencies and resolve ambiguities.\nBy systematically probing for these different linguistic properties, researchers can gain insights into the strengths and weaknesses of various model architectures and investigate the specific representations learned by the models, increasing interpretability and reliability [6]."}, {"title": "2.3 AST-Probe", "content": "AST-Probe is designed to explore the latent space of LLMs and identify the subspace that encapsulates the syntactic information of programming languages [23]. AST-Probe consists of the following steps: the AST of an input code snippet is constructed and transformed into a binary tree. Next, the binary tree is represented as a tuple of < d, c, u > vectors. It must be noted that the transformation from AST to binary tree, and from binary tree to < d, c, u > is bi-directional, meaning that by having the < d, c, u > tuple, one can reconstruct the AST of the input code. Since the AST constructed from a code snippet is syntactically valid (i.e., AST can only be constructed if code is syntactically correct), predicting the < d, c, u > tuple from the representations extracted from the hidden layers of the model, given a code snippet as input, indicates that the model is capable of representing the syntax of the programming language in its latent space. Figure 1b displays the AST extracted from a Python code snippet in Figure 1a. The < d, c, u > tuple is constructed by parsing the binary tree. In this tuple, d contains the structural information of the AST while c and u encode the labeling information. Algorithm 1 describes the construction of the < d, c, u > tuple in detail.\nAfter constructing the < d, c, u > of an input code snippet I, the embeddings of the model M for the code are extracted from different layers. Suppose that model M has n layers: 11, ..., ln. We denote the output of the lth layer for input I as M(I) with l being any layer between the second and the layer preceding the final layer (i.e., 2 < 1 < n \u2212 1). After extracting M(I), for an input code, the hidden representations are projected into a syntactic subspace S, and the probe is tasked with predicting the < d, c, u > from this projection. This process is repeated for all layers and all the code samples in the training data. If, for a given dataset of codes, the < d, c, u > tuples can be predicted by the probe with"}, {"title": "3 DEEPCODEPROBE", "content": "In this section, we first present DeepCodeProbe and how we represent AST/CFG of codes for probing the models. Afterward, we discuss our methodology for validating our proposed data representation and probing approach (i.e., DeepCodeProbe)."}, {"title": "3.1 Feature Representation and Embedding Extraction", "content": "As described in Section 2.2, probing is used in NLP to gain an understanding of what the model learns from its training data and analyze how the model makes a decision based on each input [6]. Furthermore, probing can also be used as an interpretability approach to validate whether the model learns representations from the data required for solving the task at hand [12]. Previous works [23, 29, 72, 77] have explored the ability of large language models (LLMs) to encode syntactic information of programming languages within their latent spaces, using probing.\nIn this work, we are interested in DL models that are not as large as LLMs (i.e., language models trained on code that do not have millions/billions of parameters) and focus on smaller models that are trained on code for software maintenance tasks. Our proposed approach is probing such models on their capability of representing the syntax of the programming languages in their latent space. As such, we base our approach (DeepCodeProbe) on AST-Probe [23] as described in Section 2.3, to probe the models under study for syntactic information.\nBy basing our probing approach on the same principles of AST-Probe, we first extract a syntactically valid represen- tation from the input code and construct a corresponding < d, c, u > tuple from it. However, unlike AST-Probe, in our approach, we do not derive the < d, c, u > tuple from the binary tree and instead build it directly from either the AST or the CFG extracted from the input code. We frame our probing approach as such since the binary tree of an AST or CFG would be very large and therefore, a < d, c, u > tuple constructed from it would be extremely high dimensional. Since we are interested in models that are much smaller than LLMs, probing these models for such high-dimensional representations will result in sparse representations of vectors (e.g., deriving a representation with 4,000+ features from an input with only 128 features). This sparsity would make the probe extremely sensitive to noise [34]. Given that the probe's capacity should be kept as small as possible (i.e. the probe should have significantly fewer parameters than the model being probed) [6, 23, 24], such sparse representations of vectors will result in non-convergence because of the accumulation of small effects of noise in high dimensional data [27, 75]. Therefore, we define the < d, c, u > tuple construction from the AST or CFG of a given code as follows:\n\u2022 d: The position of each node in the AST/CFG in sequential order level by level (breadth-wise) from left to right,\nas shown in Figure 2.\n\u2022 c: Position of the children of each node in sequential order for ASTs and the connections between each node in CFG.\n\u2022 u: Vector representation of each node in the AST/CFG extracted from the model's data processing approach.\nAlgorithm 2 describes an overview of our probing approach. Similar to AST-Probe, extracting the < d, c, u > tuple from an AST/CFG is bi-directional, meaning that the < d, c, u > tuple extracted from the AST/CFG of a code can be used to reconstruct the corresponding AST/CFG. As this conversion is bi-directional, we infer that if the probe can predict the < d, c, u > tuple from the embedding extracted from a model, then the model is capable of representing the syntax of the programming language in its latent space. In the same manner, we consider low performance on predicting the < d, c, u > tuple, as the model's incapability to represent the syntax of the programming language in its latent space [23, 76]."}, {"title": "3.2 Validating DeepCodeProbe", "content": "To ensure the reliability and accuracy of our probing task, we validate our proposed approach through the following three stages:\n\u2022 Validation of the data representation method: We validate that the < d, c, u > tuple constructed for each AST/CFG\ncontains information on the programming language's syntax.\n\u2022 Validation of the extracted embeddings: We validate that the embeddings extracted from the model contain enough distinctive information from the input. This is crucial to ensure that embeddings extracted from the model, for codes with dissimilar ASTs/CFGs are distinctly different from those extracted for codes with similar ASTs/CFGs.\n\u2022 Validation that the embeddings contain information related to the task for which they are being probed: We\nvalidate that the models under study learn a relevant representation from their training data for the task for which they have been trained. This is crucial to ensure that the models under probe have not simply memorized their training data.\nThis validation is necessary because if the data representation approach or the extracted embeddings do not contain the information for which the model is being probed for, the results of the probing process will be meaningless since we would be probing the models for information that is not available in their latent space. Furthermore, if a model's embeddings do not change to reflect the task for which it has been trained (before and after training), these embeddings cannot be used for probing, since they would not contain enough information. In the following, we explain each of the aforementioned validation stages in more detail."}, {"title": "3.2.1 Validating data representation.", "content": "DeepCodeProbe is designed for probing models that use syntactically valid constructs derived from code to accomplish their task. Previous works have shown that AST/CFG representations of code can be used to detect similarity between codes, as code clones of Types 1, 2, and weakly 3 have similar AST/CFG structures [2, 11, 66, 73]. Given this, we assume that the < d, c, u > tuples for codes that are exact or near clones"}, {"title": "3.2.2 Validating the extracted embeddings.", "content": "Previous works have shown that semantic similarity between data points is reflected in embedding proximity in DL models (i.e. if two inputs are similar, then their representations in the model's latent space are similar) [14, 64]. Therefore, in order to validate the extracted embeddings we follow a similar approach to validating the data representation. In this step, instead of comparing the similarity of < d, c, u > tuples, we compare the similarity of the extracted embeddings. We use the same code clone tuples constructed for validating the data representation. As such, we input code clone and non-clone pairs to each model and extract the corresponding embeddings for each input. Afterward, we use cosine similarity to measure the distance of the model's embeddings between code clones and non-clones. Given that the models under study are trained on syntactically valid representations of code, then regardless of the task that they are trained for, we expect that the extracted embeddings for code tuples that are syntactically similar (Types 1 and 2) to be similar to each other compared to code tuples that are not (Types 3 and 4). In other words, the cosine similarity for the embedding of similar codes should be significantly higher than the cosine similarity of dissimilar codes for the extracted embeddings to be valid."}, {"title": "3.2.3 Validating the embedded information.", "content": "In probing literature, it is standard to show that the probe itself is incapable of learning representations from the extracted embeddings and instead only exposes information that already exists within the model's latent space [23, 24, 77]. This is done by training the probe on embeddings extracted from randomly initialized versions of the model (i.e. models before training) and embeddings extracted from the model after it has been trained. If there exists a significant difference in the probe's performance in predicting the property of interest, then it is assumed that the probe is incapable of learning representations from the embeddings itself [6, 23, 40, 76]. In our context, where we are probing small models trained on syntactic representations of code, we need to ensure that the probe's validity does not rely on the models' capacity to learn syntax. To address this, we propose an alternative validation method focusing on the similarity of embeddings for code clones, regardless of the models' syntax learning capacity.\nGiven that the models are trained on syntactic representations of code, we extract embeddings for code clone tuples from both the trained and randomly initialized versions of the models. We then compare the cosine similarity scores for code clones from the trained model against those from the randomly initialized model. Higher similarity scores for the trained model would suggest that the model has learned some form of meaningful representations, even if not explicitly syntactic. Next, we train the probe on these embeddings and assess its performance. A significant difference in the probe's performance between embeddings from the trained and randomly initialized models would indicate that the probe is uncovering the existing structure in the embeddings rather than learning it during the probing process."}, {"title": "4 EXPERIMENTAL DESIGN", "content": "In this Section, we will describe our experimental design. To conduct our study, we define the following research questions:\n\u2022 RQ1: Are models trained on syntactical representations of code capable of learning the syntax of the programming\nlanguage?\n\u2022 RQ2: In cases where models trained on syntactical representations of code fail to learn the programming\nlanguage's syntax, do they learn abstract patterns based on syntax?\n\u2022 RQ3: Does increasing the capacity of models without changing their architecture improve syntax learning\ncapabilities?\nIn the rest of this section, we will first describe the models we have chosen for our study. Afterward, we describe the approaches used to construct the dataset for probing each model and lay out the details of the probes used for the models under study for answering RQ1 and RQ2. Finally, we describe how we increase the capacity of the models under study for answering RQ3."}, {"title": "4.1 Models Under Study", "content": "In this study, we are interested in models trained on code for software maintenance tasks. As such, to answer our RQs, we select two tasks in software maintenance for which learning the syntax of the programming language is considered to be important: Code Clone Detection (CCD) and code summarization. For each of these tasks, we select two models based on the following criteria:\n\u2022 They are not large language models (LLMs).\n\u2022 They are trained on syntactically valid representations of code, such as Abstract Syntax Trees (ASTs) or Control\nFlow Graphs (CFGs), rather than raw code or code artifacts as text.\n\u2022 They demonstrate high performance on benchmarks specifically designed for the tasks they have been trained\nfor.\nConsidering the criteria defined above, we select the following models to assess our probing approach, Deep-CodeProbe:"}, {"title": "4.1.1 AST-NN.", "content": "Zhang et. al. [87] proposed AST-NN as a novel approach for representing the AST extracted from code as inputs for DL models. In their approach, they break down the AST into Statement Trees (ST) to address the issues of token limitations and long-term dependencies in DL models, which arise from the large size of ASTs. This process involves a preorder traversal of the AST to identify and separate statement nodes defined by the programming language. For nested statements, statement headers and included statements are split to form individual statement trees.\nThese trees, which may have multiple children (i.e., multi-way trees), represent the lexical and syntactical structure of individual statements, excluding sub-statements that belong to nested structures within the statement. This process reduces an AST into multiple smaller STs. Additionally, AST-NN employs a Word2Vec model [47], to encode the labeling information for each node in the AST by learning vector representations of words from their context in a corpus."}, {"title": "4.1.2 FuncGNN.", "content": "Similar to AST-NN, Nair et. al. [50] proposed FuncGNN, a graph neural network that predicts the Graph Edit Distance (GED) between the CFGs of two code snippets to identify clone pairs. FuncGNN, as presented in Figure 4, operates by extracting CFGs from input code snippets and utilizing a statement-level tokenization approach, which distinguishes it from the word-level tokenization used by other models under study, such as AST-NN, SummarizationTF, and CodeSumDRL. The core assumption of FuncGNN is that code clone pairs will exhibit a lower GED compared to non-clone pairs, leveraging this metric to determine whether two code samples are clones.\nFuncGNN's methodology combines a top-down approach for embedding the global graph information with a bottom- up one for atomic-level node comparison to catch similarities between operations. The top-down approach generates an embedding that captures the overall structure and control flow of a CFG by incorporating an attention mechanism to find nodes in the CFG that have high semantic similarity. On the other hand, the bottom-up approach, focuses on the comparison of node-level similarities within the CFGs, to capture the similarities in program operations. By incorporating both approaches, FuncGNN predicts the GED between two input CFGs."}, {"title": "4.1.3 SummarizationTF.", "content": "The approach proposed by Shido et al. [63] extends the Tree-LSTM model [68] to handle source code summarization more effectively by introducing the Multi-way Tree-LSTM. This extension allows the model to process ASTs that have nodes with an arbitrary number of ordered children, which is a common feature in source"}, {"title": "4.1.4 CodeSumDRL.", "content": "Wan et. al. [78] proposed an approach for source code summarization based on Deep Reinforcement Learning (DRL). Their proposed approach integrates the AST structure and sequential content of code snippets as inputs to an actor-critic architecture as presented in Figure 6. The actor-network suggests the next best word to summarize the code based on the current state, providing local guidance, while the critic-network assesses the possibility of occurrence of all possible next states. Initially, both networks are pre-trained using supervised learning (i.e., code and documentation tuples), using cross-entropy and mean square as loss functions, for the actor and the critic respectively. Afterward, both networks are further trained through policy gradient methods using the BLEU metric as the advantage reward to improve the model's performance. To train the model, code is represented in a hybrid manner in two levels: lexical level (code comments) and syntactic level (code ASTs) [78]. An LSTM is used to convert the natural language comments and a separate Tree-based-RNN is used to represent the code ASTs. The output of these two layers is then used to represent the input code alongside its comment for training the actor and critic."}, {"title": "4.2 Dataset Construction for Each Model", "content": "After training models and replicating the results of the original studies, we use the same data that was used in the original works as input to the models and at the same time extract the AST/CFG from each input and construct the < d, c, u, > tuple which was explained in Section 3.1. Afterward, we extract the intermediate outputs of the model for the given input and train the probe to predict the constructed < d, c, u, > tuple from the extracted embeddings. It is important to note that the chosen models contain multiple hidden layers. Therefore, for each model, we extract the embeddings from the layers that provide the most information-rich representations learned by the model, as follows:\n\u2022 AST-NN: The AST-NN model architecture, as shown in Figure 3, consists of three main components: an encoding layer, a recurrent layer, and a pooling layer. The encoding layer first converts the input STs as described in Section 4.1.1 into numerical representations suitable for the model. These encoded inputs are then processed by the recurrent layer. The outputs of the recurrent layer are subsequently passed through the pooling layer, which aggregates the representations and produces a fixed-size vector encoding of? the entire input STs. For probing, we extract the embeddings from both the recurrent and the pooling layers.\n\u2022 FuncGNN: The FuncGNN model architecture, as illustrated in Figure 4, consists of several components: a graph convolution layer, an attention mask mechanism, and a ReLU activation function. The graph convolution layer learns representations by capturing the structural information from the input CFGs. These learned representa- tions are then passed through the attention mask, which assigns importance weights to different parts of the outputs of the graph convolution layer. Finally, a ReLU activation function is applied to the attention-weighted representations, and a sigmoid activation yields a binary output indicating whether the two input CFGs are code clones or not. For probing, we extract the embeddings from the output of the graph convolution layer.\n\u2022 SummarizationTF: As displayed in Figure 5, the SummarizationTF model follows an encoder-decoder architecture.\nThe encoder component processes the AST generated from a code snippet and generates a contextualized"}, {"title": "4.2.1 AST-NN.", "content": "Algorithm 3 presents how we construct the < d, c, u > tuples for probing AST-NN using DeepCodeProbe. Note that to represent the labels of each node, we use the same Word2Vec model that is used by AST-NN to convert the labels into numerical representations (i.e., indexes) as described in Section 4.1.1. We probe this model on its capability to represent the syntax of both C and Java programming languages. We also adopt the same preprocessing approach as AST-NN to convert the inputs from code to STs. Afterward, for each ST, we construct the < d, c, u > tuple to be used for training the probe."}, {"title": "4.2.2 FuncGNN.", "content": "Algorithm 4 presents an overview of the < d, c, u > tuple construction for FuncGNN in DeepCodeProbe. The training data for FuncGNN is already pre-processed from code into CFGs using Soot\u00b9, a framework that is used to transform Java bytecode into intermediate representation for analysis, visualization, and optimization. In this format, each node represents a line in the program, and the edges between the nodes indicate the control flow between each node. For constructing the < d, c, u > tuple, we traverse the CFG and use the depth, connections, and labels of each node to represent it in a numerical vector for training and evaluating the probe."}, {"title": "4.2.3 SummarizationTF.", "content": "This model works with ASTs. Therefore, unlike AST-NN there is no need to first generate the ASTs from code and then break them down into STs. As such, the process for generating the < d, c, u > tuples is similar to that of Algorithm 3 with some modifications to the method of parsing the trees. Algorithm 5 presents our approach in detail."}, {"title": "4.2.4 CodeSumDRL.", "content": "Similar to SummarizationTF, CodeSumDRL also works with ASTs generated from code. As such, we follow the same steps as outlined in Algorithm 5 to build the necessary < d, c, u > tuples for training and evaluating the probe."}, {"title": "4.3 Probe's Design for Each Model", "content": "As explained in Section 2.2, the probe should have minimal size and complexity. Following the same conventions used in probing language models [44], the probe should not learn any information from the data but instead, learn a transformation (i.e., projection) of the embeddings that exposes the information that the model learns from its training data. Therefore, for each of the models under study, based on their capacity (i.e., number of parameters), we experiment"}, {"title": "4.4 Scaling Up The Models", "content": "Kaplan et al. [28] examined the scaling laws for language models, focusing on the impact of increasing model capacity, training data, and computational resources on the models' training efficiency and performance. In their study, they experiment with how increasing each and a combination of these factors affects the model's sample efficiency, overfitting, convergence, and overall performance on the test benchmarks. Their research focuses on transformer-based language models and is therefore not directly applicable to our work. However, we can apply the same principles to the models under study and investigate how increasing the models' capacity affects their syntax learning capabilities. Based on"}, {"title": "5 RESULTS AND ANALYSIS", "content": "Before analyzing the embeddings extracted from the models to assess their capability to learn the syntax of the programming language and answer our research question, we must first ensure the reliability of the probing approach used to obtain those embeddings. Therefore, in the following, we first conduct a validation of our probing approach before proceeding to answer our research question."}, {"title": "5.1 Evaluation of our Probing Approach DeepCodeProbe", "content": "As described in Section 3.2, we evaluate DeepCodeProbe by first assessing the validity of the < d, c, u > tuple and the embeddings extracted from the models. As AST-NN and FuncGNN are designed specifically for CCD, we use the same dataset provided by their authors (AST-NN [87] and FuncGNN [50]) for the validation of DeepCodeProbe on these models. In the case of SummarizationTF and CodeSumDRL, the authors did not originally include a code clone dataset."}, {"title": "5.2 RQ1: Are models trained on syntactical representations of code capable of learning the syntax of the programming language?", "content": "Table 3 presents the results of our probing as described in Sections 4.2 and 4.3. The \"Accuracy-D\u201d, \u201cAccuracy-C\u201d, and \"Accuracy-U\" columns indicate the probe's accuracy in predicting the d, c, and u tuples, respectively. As we can observe"}]}