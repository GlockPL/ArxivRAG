{"title": "MarsCode Agent: AI-native Automated Bug Fixing", "authors": ["Yizhou Liu", "Pengfei Gao", "Xinchen Wang", "Chao Peng", "Yexuan Shi", "Zhao Zhang"], "abstract": "Recent advances in large language models (LLMs) have shown significant potential to automate various software development tasks, including code completion, test generation, and bug fixing. However, the application of LLMs for automated bug fixing remains challenging due to the complexity and diversity of real-world software systems. In this paper, we introduce MarsCode Agent, a novel framework that leverages LLMs to automatically identify and repair bugs in software code. MarsCode Agent combines the power of LLMs with advanced code analysis techniques to accurately localize faults and generate patches. Our approach follows a systematic process of planning, bug reproduction, fault localization, candidate patch generation, and validation to ensure high-quality bug fixes. We evaluated MarsCode Agent on SWE-bench, a comprehensive benchmark of real-world software projects, and our results show that MarsCode Agent achieves a high success rate in bug fixing compared to most of the existing automated approaches.", "sections": [{"title": "1 Introduction", "content": "The automation of software engineering tasks has long been a goal for researchers and practitioners in the field. Recent progress in large language models (LLMs) like GPT-40 and Doubao Pro has brought us closer to this vision, enabling significant advancements in code generation, program repair, and other software development activities. In this trend, LLM-based agents - intelligent entities capable of perceiving the external environment, operating tools, and making autonomous decisions, have garnered increasing attention from both the research and industry community.\nBug fixing is a critical aspect of software maintenance, covering tasks such as identifying the root cause of defects, generating correct patches, and validating the fixes to ensure that they do not introduce new issues. Traditional approaches to automated bug fixing rely heavily on manually crafted rules and heuristics, which can be limited in scope and adaptability. Recent efforts have explored the use of LLMs and LLM-based agents to address these limitations by leveraging their ability to understand and generate code in a more flexible manner.\nHowever, applying LLMs to the automated bug fixing of real-world software projects presents unique challenges. Unlike simple, self-contained coding tasks, bug fixing often requires an in-depth understanding of complex codebases, interdependencies among files, and context-specific issues that arise during software development.\nIn this report, we introduce MarsCode Agent, a novel framework designed to automate the bug fixing process using LLMs. By building an agent framework and providing interactive interfaces and tools for code retrieval, debugging, and editing, MarsCode Agent has made it possible for agents to take over some software engineering tasks."}, {"title": "2 Background and Related Work", "content": "In this section, we discuss basic concepts of large language models and their application on software engineering tasks, especially for fault localization and automated program repair. We also talk about recent advances in LLM-based agents for software engineering and the SWE-bench benchmark for their evaluation."}, {"title": "2.1 Large Language Models", "content": "Large language models (LLMs) are highly advanced pre-trained language models. These models undergo initial unsupervised training on vast amounts of corpus, followed by fine-tuning for specific tasks to enhance performance. In natural language processing (NLP), LLMs have been extensively applied to various tasks such as machine translation [35, 48], text summarization [50], and classification [25].\nLanguage models are classified into three categories based on their architecture: encoder-only models [8], decoder-only models [29], and encoder-decoder models [33]. Most existing LLMs for code utilize the transformer architecture's encoders, known for their exceptional learning capabilities and scalability. Regardless of their architecture, most models can be fine-tuned with task-specific data to enhance performance [20].\nLarge language models (LLMs) have become a promising choice for various software engineering tasks due to their impressive performance in both code generation and understanding [45]. Researchers and developers have applied LLMs to several software engineering tasks, such as program synthesis [22, 53, 34, 37, 36], code translation [47, 46], program repair [21, 10, 42], fault detection and localization [7, 31], incident analysis [6, 3], code summarization [9] and testing [32]. For example, Codex [5], StarCoder [23], and DeepSeek-Coder [52] are notable code-specific LLMs developed through extensive training on large datasets of open-source code snippets. Additionally, instruction-following code-specific LLMs such as DeepSeek-Coder-Instruct [52] and Magicoder [39] have been created using instruction-tuning methods to enhance their utility in coding tasks."}, {"title": "2.2 Fault Localization", "content": "Fault localization (FL) [40] techniques aim to discover and analyze the location and causes of faults, which can be categorized into dynamic and static approaches. Dynamic FL techniques, such as spectrum-based fault localization (SBFL) [2, 1] and mutation-based fault localization (MBFL) [30], analyze the dynamic execution information of a program to determine fault locations, though they are resource-intensive. Static FL techniques [24] determine fault locations through semantic or syntactic analysis at the bug report or source code level, offering fast detection with low resource consumption. Advanced FL techniques, such as multiple fault localization (MFL) and combined dynamic and static methods, have emerged to guide APR tools in finding and fixing more errors [43, 12, 27]."}, {"title": "2.3 Automated Program Repair", "content": "Automated program repair (APR) [16] has attracted significant attention over the past decade. APR techniques aim to generate patches for buggy programs to pass given test suites. These techniques can be categorized into search-based [17, 26], semantics-based [13, 28, 14], and pattern/learning-based approaches [18, 19, 49]. Search-based APR techniques like GenProg [15] use predefined code mutation operators to generate patches, while semantics-based APR techniques generate patches by solving repair constraints based on test suite specifications. Learning-based APR techniques, such as those utilizing deep learning models, train on large code repositories to predict correct patches. Recent work has shown the use of LLMs for APR, often focusing on constructing APR-specific prompts to guide LLMs in generating patches for buggy program statements [42]."}, {"title": "2.4 Agents for Software Development", "content": "The emergence and popularity of agent-based frameworks have led to the development of agent-based approaches for solving software engineering tasks. Devin and its open-source counterpart Open-Devin [38] are among the first end-to-end LLM agent-based frameworks. These frameworks use agents for planning based on user requirements and enable agents to iteratively perform tasks using tools like file editors, terminals, and web search engines. SWE-agent [44], for example, designs a custom agent-computer interface (ACI) allowing LLM agents to interact with the repository environment through actions such as reading, editing files, and running bash commands. AutoCodeRover [51] provides LLM agents with specific APIs to effectively identify locations needing modification to resolve issues. Numerous other agent-based approaches have been developed, both in open-source and commercial products."}, {"title": "2.5 SWE-bench", "content": "SWE-Bench [11] is a comprehensive benchmark designed to evaluate LLMs on complex real-world software engineering tasks sourced from GitHub issues and corresponding pull requests across 12 popular Python repositories. This benchmark addresses the limitations of existing coding benchmarks such as HumanEval [5] by presenting tasks that require models to understand and coordinate changes across large codebases involving multiple functions and files. The benchmark includes 2,294 task instances and emphasizes the need for models to interact with execution environments and handle long contexts, showcasing the challenges that real-world software engineering problems pose to current LLMs. Their evaluations reveal that even the best-performing models at the time of publication, such as Claude 2, achieve a success rate of only 1.96%, highlighting significant room for improvement."}, {"title": "3 Approach", "content": "In this section, we present our approach, MarsCode Agent , spreading into the multi-agent collaborative framework, code indexing and code editing tools designed for agents."}, {"title": "3.1 Multi-agent Collaborative Framework", "content": "In our daily development work, developers often encounter various issues such as:\n\u2022 Test case failures, which may include errors or exception stacks due to logic errors or failed test assertions.\n\u2022 Code output not meeting expectations, with no explicit error messages but clear expected results.\n\u2022 The need to extend existing functionality or add new features, with clear development requirements and expected outcomes, but uncertainty about how and where to implement them.\n\u2022 Simple defect fixes, with a rough idea of the solution but requiring assistance due to unfamiliarity with language features.\nThese diverse program repair and development tasks cannot be smoothly handled with a fixed approach. For instance, some simple defect fixes or feature extensions can be completed through reviewing existing relevant code, while deeper exception stacks or complex logic errors may require dynamic code execution and variable tracking to uncover and fix the defects.\nTherefore, we have adopted a multi-agent collaboration framework to adapt to different development scenarios. As shown in Figure 1, the framework includes the following six roles:\n\u2022 Searcher utilizes tools like code knowledge graph (CKG) and language server protocol (LSP) to collect code snippets from the repository related to the current issue.\n\u2022 Planner qualitatively analyzes the collected code snippets and classifies the issue into dynamic debugging repair or static repair workflows."}, {"title": "3.2 Code Indexing", "content": "We have developed several code indexing tools with multilingual support, to satisfy different code search requirements for different software development tasks."}, {"title": "3.2.1 Code Knowledge Graph", "content": "A code knowledge graph represents code elements, their attributes, and the relationships between these elements in a graph structure, helping agents better understand and manage large codebases. In this graph, vertices represent code entities (such as functions, variables, classes, etc.), and edges represent relationships between these entities (such as function calls, variable references, class inheritance, etc.). This structured representation provides richer information about the codebase.\nMarsCode Agent analyzes and organizes the code and documentation in a repository to generate a multi-directional graph using program analysis techniques. This graph includes semantic nodes such as variables, functions, classes, and files, and edges representing file structure relationships, function call relationships, and symbol index relationships. This results in a code knowledge graph that integrates code, documentation, and repository information from multiple data sources.\nIn the given codebase, each node and edge is uniquely identified, ensuring that every code entity is unique across the entire codebase. The code knowledge graph uses graph attributes to store code entities and their dependencies. Each node records its location, type, and name within the codebase, while each edge identifies the type of relationship between two nodes and the relationship's location in the code.\nAfter constructing the code knowledge graph, the agent's code retrieval requests are processed through the following pipeline:\n1. The agent's query, along with any code statements, undergoes entity recognition using a model to identify entity mentions and types. These are then queried in the knowledge graph using SQL, resulting in candidate entity list 1.\n2. The agent's query, along with any code statements, is embedded and matched for similarity in the knowledge graph, yielding candidate entity list 2.\n3. The agent's query is directly converted into a search query through keyword recognition and queried in the knowledge graph using SQL, resulting in candidate entity list 3.\nThe candidate entity lists 1, 2, and 3 are then merged and ranked using a fine-ranking model to obtain the final entity list X, which is returned to the agent, completing the code retrieval process.\nThe code knowledge graph tool in MarsCode Agent enables comprehensive code retrieval, providing agents with repository-level knowledge question and answering (Q&A) capabilities. Currently, our code knowledge graph supports 12 common programming languages, including C, C#, C++, Java, Kotlin, JavaScript, TypeScript, TSX, Rust, Go, Python, and Lua."}, {"title": "3.2.2 Language Server Protocol", "content": "The code knowledge graph can handle most class and function definition and reference retrieval needs in the target project, but it has the following limitations:\n\u2022 It cannot accurately retrieve definitions and references of classes, functions, and variables outside the target project (such as standard libraries and third-party libraries).\n\u2022 For cases with multiple entities having the same name, the LSP can more accurately navigate to the relevant class or function definition, avoiding omissions or redundancies during the recall and re-ranking process.\nTo address these issues, MarsCode Agent uses the language server protocol (LSP) to achieve global and precise code retrieval on the user's machine. The LSP, developed by Microsoft, is widely compatible with various programming languages, markup languages, tools, and frameworks, making it highly versatile for IDE scenarios.\nThe agent's use of the LSP for code retrieval is similar to a developer's Ctrl + Click action in an IDE to jump to code. However, since the agent's numerical positioning and computation capabilities are weak, we added fuzzy positioning features to enhance the agent's use of LSP tools:"}, {"title": "3.2.3 Other General Indexing Capabilities", "content": "Besides LSP and the code knowledge graph, we also integrate general project file retrieval (find file), project or file identifier retrieval (grep), and other capabilities into the MarsCode Agent framework, providing a consistent toolset for code retrieval."}, {"title": "3.3 Code Editing", "content": ""}, {"title": "3.3.1 Reflections on Code Editing", "content": "In our long-term exploration of AI agents for software development, we tried various methods of using LLMs for code edit descriptions and found that current LLMs have generally weak code modification capabilities. Below are some of the failed approaches we explored:\n\u2022 Asking the agent to generate unified diff format code change descriptions. The unified diff format presents the changes between the original file and the modified file in a unified manner. The unified diff format has strict formatting requirements, and LLMs often struggle to correctly calculate line number increments, resulting in unapplicable unified diffs.\n\u2022 Asking the agent to provide the start and end line numbers and the replacement code snippet. Even with line numbers added to all code retrieval results, LLMs, including GPT-4, often fail to provide the correct modification range, leading to issues like repeated lines or unintended deletions.\n\u2022 Rewriting the entire file. Providing the entire file content and modification description to the LLM and asking it to output the modified file content avoids line number calculations but is economically unfeasible for each code edit and nearly unusable for long files. We are also working on obtaining a specialized code editing model through SFT for full-file rewriting, but this is a long-term plan.\nThrough extensive exploration and attempts, we concluded that LLM code edit descriptions need the following characteristics:\n\u2022 No strict format validation, with descriptions that can be stably applied after processing and parsing.\n\u2022 No need to provide line number ranges or perform line number calculations, as LLMs are unstable in this aspect."}, {"title": "3.3.2 Static Code Diagnostics", "content": "Although AutoDiff can handle most code edit requests correctly, common syntax issues like type errors, undefined variables, indentation errors, and unclosed brackets still occur. We use the LSP to perform static code diagnostics on files before and after AutoDiff modifications as shown in Figure 4 As shown in the figure, the workflow of static code diagnostics is as follows:\n1. Apply the AutoDiff generated unified diff format code edit patch to the original file to get the modified file content.\n2. Perform LSP static code diagnostics on the original file content and save the results.\n3. Perform LSP static code diagnostics on the modified file content and save the results.\n4. Compare the diagnostic results before and after the modification to check if new static errors (focusing on Fatal and Error levels) were introduced by the agent's modification.\n5. If no new errors were introduced, complete the modification and return a success message and the corresponding unified diff description to the agent.\n6. If new errors were introduced, return the relevant diagnostic information to the agent for further modifications and adjustments."}, {"title": "4 Experimental Results and Analysis", "content": "We conducted a detailed evaluation of MarsCode Agent 's performance on the SWE-bench Lite dataset."}, {"title": "4.1 Dataset Overview: SWE-bench Lite", "content": "SWE-bench, as introduced in Section 2.5, is a highly challenging benchmark for LLMs to solve program logic and functional bugs. This dataset is consisted of 2294 issues from 12 industrial-grade Python code repositories on GitHub. Given a codebase and a description of the issue to be resolved, the agent needs to retrieve and edit the code from the repository, ultimately submitting a code patch that resolves the issue. Solving problems in SWE-bench typically requires understanding and coordinating changes across multiple functions, classes, or even files, necessitating interaction with the execution environment, handling extremely long contexts, and performing more complex reasoning than traditional code generation. Evaluations in the SWE-bench paper show that directly applying Claude 2 and GPT-4 can only solve 4.8% and 1.7% of the instances, respectively [11].\nDue to the high difficulty of SWE-bench, subsequent research found that evaluating on all 2294 instances of SWE-bench is a time and token-intensive process that is frustrating and does not validate short-term progress. Therefore, the authors of SWE-bench extracted 300 instances with complete issue descriptions, clear-solving logic, and relative ease of resolution to form the SWE-bench Lite dataset. Currently, the SWE-bench Lite dataset has become the benchmark for evaluating the capability of agents to solve software engineering problems, with over 20 companies and research organizations participating in the evaluation and submissions."}, {"title": "4.2 MarsCode Agent Results", "content": "In the latest SWE-bench Lite evaluation, MarsCode Agent successfully solved 102 instances, achieving a solve rate of 34%. An analysis of this result is shown in Table 2 We compared the files containing code snippets located by the agent during the solving process with the files containing the gold patches for the instances. If the file containing the gold patch was included, it was considered a successful file localization. Similarly, if there was an inclusion or overlap relationship between the code snippets found during the solving process and the modification target segments of the gold patch, the solving process was considered to have successfully located the target code segment."}, {"title": "5 Final Remarks", "content": "In this paper, we introduced MarsCode Agent, a novel framework leveraging LLMs to automate bug fixing and software development tasks. Our approach combines advanced code analysis techniques with LLM capabilities to provide a systematic process for fault localization, candidate patch generation, and patch validation. Through comprehensive evaluations on the SWE-bench Lite dataset, MarsCode Agent demonstrated significant improvements in solving real-world software engineering problems, achieving a solve rate of 34%.\nLooking forward, we aim to further enhance MarsCode Agent by reducing LLM call costs, improving user-agent collaboration, supporting dynamic debugging within ther real user workspaces to avoid environmental contamination, and increasing the accuracy of error localization and code modifications. Our ongoing commitment is to refine and expand the capabilities of MarsCode Agent , making it an indispensable tool in the landscape of intelligent software development.\nMarsCode Agent 's promising results on SWE-bench Lite demonstrates the potential of LLMs to significantly advance the field. We hope our work inspires further research and development, driving innovations that bring us closer to fully autonomous software engineering solutions."}]}