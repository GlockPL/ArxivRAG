{"title": "Feature maps for the Laplacian kernel and its generalizations", "authors": ["Sudhendu Ahir", "Parthe Pandit"], "abstract": "Recent applications of kernel methods in machine learning have seen a renewed interest in the Laplacian kernel, due to its stability to the bandwidth hyperparameter in comparison to the Gaussian kernel, as well as its expressivity being equivalent to that of the neural tangent kernel of deep fully connected networks. However, unlike the Gaussian kernel, the Laplacian kernel is not separable. This poses challenges for techniques to approximate it, especially via the random Fourier features (RFF) methodology and its variants. In this work, we provide random features for the Laplacian kernel and its two generalizations: Mat\u00e9rn kernel and the Exponential power kernel. We provide efficiently implementable schemes to sample weight matrices so that random features approximate these kernels. These weight matrices have a weakly coupled heavy-tailed randomness. Via numerical experiments on real datasets we demonstrate the efficacy of these random feature maps.", "sections": [{"title": "1. Introduction", "content": "Kernel machines are a classical family of models in machine learning. Certain architectures of neural networks (Lee et al., 2017; Matthews et al., 2018; Jacot et al., 2018) behave like kernel machines under a large width limit.\nRecent work on kernel machines enabled with feature learning (Radhakrishnan et al., 2024a), which builds on the Laplacian kernel, has inspired many interpretable substitutes for deep neural networks on unstructured data. For example (Radhakrishnan et al., 2024b) and (Aristoff et al., 2024) have demonstrated methodological advances in basic sciences.\nFrom a theoretical standpoint, kernel machines provide a testbed for understanding many behaviours exhibited by deep neural networks (Belkin et al., 2018). Recently, many complex neural phenomena been shown to be present in kernel models. Examples include emergence and grokking (Mallinar et al., 2024), deep neural collapse (Beaglehole et al., 2024), overparameterization (Simon et al., 2024; Ghosh & Belkin, 2023), benign overfitting (Mallinar et al., 2022), and interpolation (Beaglehole et al., 2023), monotone improvement with model size (Abedsoltan et al., 2023), and simplicity bias (Radhakrishnan et al., 2024a) among others. Results on precise asymptotics (Mei & Montanari, 2022) have guided intuitions about neural networks (Ghorbani et al., 2021; Bartlett et al., 2021).\nThe Laplacian kernel\n\\(K_{L}(x, z) = e^{-||x-z||_{M}}\\)\n(1)\nis of particular interest, where M is a positive definite matrix parameter. Even for M = Id, the Reproducing Kernel Hilbert Space (RKHS) corresponding to this kernel is equivalent to the RKHSs corresponding to the Neural Tangent Kernel (NTK) for fully connected networks of any depth (Geifman et al., 2020; Chen & Xu, 2020). Interpolation with (1) can also be consistent in high dimensions (Rakhlin & Zhai, 2019; Mallinar et al., 2022). Furthermore, unlike the Gaussian kernel, (1) is excellent in practice for rapid exploratory data analysis since it is not sensitive to the bandwidth hyperparameter.\nWe consider the general anisotropic case with M \u2260 Id because it breaks the rotational symmetry. Kernel models trained via ridge regression can become degenerate for finite number of samples if the kernel function is rotationally symmetric (Karoui, 2013). Indeed, as shown in (Radhakrishnan et al., 2024a), the paramterization (1) can be significantly more powerful if M \u2260 \u03b3Ia is chosen adaptively."}, {"title": "1.1. Main contribution", "content": "In this paper\u00b3 we provide explicit schemes to obtain random features that approximate the Laplacian kernel and its two generalizations: (i) the Mat\u00e9rn kernel (Williams & Rasmussen, 2006), defined in equation (9) for v > 0, and (ii) the Exponential power kernel defined for \u03b1 \u2208 (0, 2] as\n\\(K_{E}(x, z) = exp(- ||x \u2212 z||_{M}^{\\alpha})\\)\n(2)\nshown to be useful for practical tasks such as speech processing in (Hui et al., 2018). We provide 2 schemes, RFF and ORF, for approximating all the 3 kernels. See Table 1 for a summary of how to generate these random features.\nVia numerical experiments on 11 datasets we demonstrate the efficacy of the random features for kernel regression.\nWe also show that kernel logistic regression, which yields far better calibrated models for classification than kernel ridge regression, is much easier to train with our random features than using the exact kernel function.\nEven for the isotropic case, M = Id, the random features in Table 1 are coupled, i.e., not elementwise independent. This occurs because the kernel is not separable as a product of kernel functions over coordinates of the inputs. Yet, we are able to sample from these distributions efficiently.\nRecent work (Muyskens et al., 2024) has also highlighted the relation between the ReLU Neural Network Gaussian Process and the Mat\u00e9rn-3 kernel. However, evaluating the Mat\u00e9rn kernel, can be expensive. Our schemes provide significant speed-ups (see Figure 1).\nFormulae for Fourier transforms of the kernel functions, which are needed to understand weight distributions that yield appropriate random features, were known in niche references in applied mathematics. See Table 2 for a summary."}, {"title": "2. Preliminaries", "content": "We consider the standard setting in supervised learning where we are provided with n labeled training data points {(xi, Yi) \u2208 Rd \u00d7 R}i=1n to learn a map f: : Rd\u2192R.\nA kernel K : Rd \u00d7 Rd \u2192 R is a bivariate, symmetric, positive definite function. Kernel machines are models\n\\(f(x) = \\sum_{i=1}^{n} \\alpha_{i} K(x, x_{i})\\)\n(4)\nwhere \u03b1 are parameter trained on the labeled data. Such functions are optimal for a wide class of non-parametric estimation problems of the form\n\\(min_{f} L({(xi, f(xi), Yi)}i=1^{n}) + R(||f||_{HK})\\)\n(5)\nfor any loss function L and any monotonically increasing function R, thanks to the representer theorem (Kimeldorf & Wahba, 1971; Sch\u00f6lkopf et al., 2001). Here HK is the RKHS corresponding to the kernel K. We refer the reader to (Steinwart & Christmann, 2008) for a detailed treatment on RKHS.\nIf K(x,z) = (\u03c6(x), \u03c6(z)), for some feature map \u03c6 : Rd \u2192 H', then we can write equation (4) as f(x) = (\u03c6(x),\u0398), where \u0398 \u2208 H' solves\n\\(min_{\\Theta\\in H'} L({(xi, (\\phi(x_{i}), \\Theta)_{H'}, Yi)}_{i=1}^{n}) + R(||\\Theta||_{H'})\\)\n(6)\nFurthermore, one can argue that \u0398 = \u2211i=1n \u03b1i\u03c6(xi).\nIn this paper we focus on two types of feature maps \u03c6(x) = \u03c8p(Wx) and \u03c6(x) = \u03c8p(SQx) for W \u2208 Rp\u00d7d, an orthogonal Q\u2208 Rp\u00d7d and diagonal S."}, {"title": "2.1. Elliptically contoured \u03b1-stable distribution", "content": "The elliptically contoured \u03b1-stable distribution is a special class of multivariate \u03b1-stable distributions. To introduce the multivariate stable distribution, we first need to define the univariate stable distribution. We direct the reader to (Nolan, 2020) for a detailed treatment on the stable distribution, and (Samoradnitsky, 2017) for the multivariate setting.\nA stable distribution is one that is closed under positive linear combinations, upto a change of location and scale, i.e., if we take two independent realizations X1, X2 of a random variable X and positive constants a > 0, b > 0, the random variable aX1 + bX2 has the same distribution as cX + d for some constants c > 0, d \u2208 R.\nIt is often convenient to use a more concrete definition of the stable distribution such as the one given below. A proof of the equivalence of these definitions is given in (Nolan, 2020, Ch. 3)\nThe general discussion on stable distributions has a location parameter \u03bc. In this work we assume the location is 0 and hence omit this parameter in our discussion. We consider the 3 parameter family S(\u03b1, \u03b2, \u03c3) defined below. The stable distribution, unlike most distributions, is defined via its characteristic function. The density function cannot be expressed analytically for general \u03b1, \u03b2, \u03c3.\nDefinition 6 (Univariate stable distribution (Nolan, 2020)). The stable distribution S(\u03b1, \u03b2,\u03c3) with shape parameters \u03b1 \u2208 (0,2], and \u03b2\u2208 [-1,1] and scale parameter \u03c3 > 0 has the characteristic function\n\\(exp(-|\\sigma t|^{\\alpha} (1 \u2013 j\\beta sgn(t)\\Phi(t))) \\forall t\\in R\\)\n(8)\n\\(\\Phi(t) = \\begin{cases}\ntan(\\frac{\\pi\\alpha}{2}) & \\alpha\\neq 1 \\\\\n-\\frac{2}{\\pi}log|t| & \\alpha=1\\end{cases}\\\nThe above definition is referred to as the 'Nolan's O' parametrization (Nolan, 2020).\nThe multivariate \u03b1-stable distribution is defined in terms of it's projections.\nDefinition 7 (Multivariate \u03b1-stable distribution). A Rd-valued random variable X is said to have a multivariate \u03b1-stable distribution if, for any u \u2208 Rd, uTX ~ S(\u03b1, \u03b2u, \u03c3u) for some \u03b2u \u2208 [\u22121,1], and \u03c3u > 0. Note that \u03b1 does not depend on u.\nDefinition 8. (Elliptically contoured \u03b1-Stable distribution) (Nolan, 2005, Sec.3) An Rd-valued random variable X is said to be elliptically contoured \u03b1-stable if uTX ~ S(\u03b1,0, ||u||M), for some positive definite shape matrix M \u2208 Rd\u00d7d. Its characteristic function is given by\n\\(E exp (j (u, X)) = exp(- ||u||_{M}^{\\alpha}).\\)"}, {"title": "2.2. The Mat\u00e9rn kernel", "content": "The Laplacian and the Gaussian kernel are members of several families of kernels. One very natural extension comes from the observation that the Cauchy distribution is the t-distribution with 1 degree of freedom (2\u03bd = 1). Further, as the number of degrees of freedom rise to infinity (2\u03bd \u2192 \u221e), t-distribution approaches the Gaussian. Using these extremes, Fourier transforms of all intermediate t-distributions also define shift-invariant kernels, together studied as the Mat\u00e9rn kernel with parameter v. A Gaussian process with Mat\u00e9rn covariance is [v] \u2013 1 times differentiable in the mean-squared sense (Williams & Rasmussen, 2006, Sec. 4.2.1).\nDefinition 9 (Mat\u00e9rn kernel). For v > 0,\n\\(K_{M}(x, z) := \\kappa_{\\nu}(x \u2212 z)\\)\n\\(\\kappa_{\\nu}(\\triangle) := \\frac{2^{1-\\nu}}{\\Gamma(\\nu)} (\\sqrt{2\\nu} ||\\triangle||_{M})^{\\nu} J_{\\nu} (\\sqrt{2\\nu} ||\\triangle||_{M})\\)\n(9)\nwhere M \u2208 Rd\u00d7d is is a scale matrix and \u03bd is the shape factor and J\u03bd is the modified Bessel function of the second kind.\nFor certain values of v, the Mat\u00e9rn kernel can be simplified:\n\\(\\kappa_{1/2}(\\triangle)=e^{-||\\triangle||_{M}}, \\kappa_{\\infty}(\\triangle) = e^{-||\\triangle||_{M}^{2}}, \\\\\n\\kappa_{3/2}(\\triangle) = (1 + \\sqrt{3} ||\\triangle||_{M}) e^{-\\sqrt{3}||\\triangle||_{M}}, and \\\\\n\\kappa_{5/2}(\\triangle)=(1+\\sqrt{5} ||\\triangle ||_{M}+ \\frac{5}{3} ||\\triangle||_{M}^{2}) e^{-\\sqrt{5}||\\triangle||_{M}}.\\\\\nIn general, for \u03bd \u2208 {n+1/2}n\u2208N, one can simplify the Mat\u00e9rn kernel in terms of a polynomial times an exponential, similar to the examples above. Another special case is \u03bd = \u221e where the Mat\u00e9rn is exactly equal to the Gaussian. The sklearn package (Pedregosa et al., 2011) states that for v \u2208 {1/2, 3/2, 5/2, \u221e} one incurs a considerably high computational cost (~ 10\u00d7) since it requires the evaluation of the modified Bessel function. Hence, random features are more effective for other values of v. See Figure 1 for a tradeoff between approximation error and computational speedup for datasets of different dimensions."}, {"title": "3. Background on random features", "content": "The key idea of RFF, proposed in (Rahimi & Recht, 2007), comes from a classical theorem in harmonic analysis due to Bochner (Rudin, 2017, Thm.1.4.3). Bochner's theorem states that any function \u03ba is positive definite if and only if it is the characteristic function of a random variable (upto a normalizing constant), i.e., the Fourier transform of \u03ba is a measure. This enables writing any shift invariant kernel as\n\\(K(x, z) = \\kappa(x \u2212 z) = C_{\\kappa} \u00b7 E_{w} e^{i(w,x\u2212z)}\\),\n(10)\nfor a normalizing constant C\u03ba. Here w is a random variable with measure proportional to the Fourier transform of \u03ba, normalized to have P(w \u2208 Rd) = 1."}, {"title": "3.1. Random Fourier features (RFF)", "content": "We present below the so-called SinCos RFF maps from (Sutherland & Schneider, 2015) due to their lower approximation error over the RFF scheme proposed in (Rahimi & Recht, 2007).\nProposition 1. Let {wi}i=1p be i.i.d. samples from a distribution over Rd whose characteristic function is \u03ba. Let \u03c8p be an elementwise nonlinearity defined in equation (3), and suppose the rows of W \u2208 Rp\u00d7d are wi. Then\n\\(lim_{p\u2192\u221e} (\\psi_{p}(Wx), \\psi_{p}(Wz))_{R^{2p}} = \\kappa(x \u2212 z) = K(x, z)\\)\n(11)\nTo prove the above result, observe that (\u03c8p(Wx),\u03c8p(Wz))R2p equals\n\\(\\frac{1}{p} \\sum_{i=1}^{p} cos(w_{i}x) cos(w_{i}z) + sin(w_{i}x) sin(w_{i}z) \\xrightarrow{p\u2192\u221e} C_{\\kappa} E_{w} cos(w(x \u2212 z)) = C_{\\kappa} E_{w} e^{i(w,x\u2212z)} = \\kappa(x \u2212 z)\\\nwhere the limit holds almost surely, by strong law. For a non-asymptotic characterization for finite p, please refer to (Sutherland & Schneider, 2015, Prop. 1)."}, {"title": "3.2. Orthogonal random features (ORF)", "content": "Proposed in (Yu et al., 2016), ORF provides a structured method for creating W, leveraging the distribution of ||w||.\nProposition 2. Let p be an integer multiple of d, and Q \u2208 Rp\u00d7d be obtained by stacking p/d independent samples from uniform distribution over d \u00d7 d unitary matrices. Let w be a Rd valued random variable, whose characteristic function is \u03ba, for some rotationally invariant function \u03ba, \u0456.\u0435., \u03ba(VA) = \u03ba(A) for all unitary matrices V \u2208 Rd\u00d7d. Let S \u2208 Rp\u00d7p be a diagonal matrix with entries {Sii}i=1p sampled i.i.d. from the distribution of ||w||. Let \u03c8p be an elementwise nonlinearity defined in equation (3). Then\n\\(lim_{p\u2192\u221e} (\\psi_{p}(SQx), \\psi_{p}(SQz)) = \\kappa(x \u2212 z).\\)\n(12)\n(Yu et al., 2016) provide a result for the Gaussian kernel. Their argument can be extended to general rotationally invariant kernels as follows. If \u03ba is rotationally invariant, then one can show that the distribution of w is rotationally symmetric, whereby ||w|| and ^ are independent random variables. Furthermore, ^ is uniformly distributed over Sd-1, the unit sphere in d dimensions. Thus, SQ is a reparameterization of writing w = ||w|| \u00b7 ^; the rows of Q are distributed like ^, whereas the diagonal elements of S are distributed like ||w||. Together, SQ is distributed like W from Proposition 1."}, {"title": "3.3. Prior work on random features", "content": "After the introduction of RFF in (Rahimi & Recht, 2007), several improvements have been suggested. We focus primarily on the algorithmic improvements. See (Liu et al., 2021) for a comprehensive survey.\nFor the Gaussian kernel, a long line of work improved upon the vanilla RFF formulation to either reduce space and time complexity or improve approximation and generalization error. For example, Fastfood (Le et al., 2013) and it's generalization P-model (Choromanski & Sindhwani, 2016) utilize Hadamard matrices to speed up computation of x \u2192 Wx. An alternate approach was suggested in (Feng et al., 2015) which utilized signed circulant matrices to generate features. (Li, 2017) argues that normalizing the inputs leads to gains in approximation and generalization performance due to restricting the diameter of the data.\nRemark 2 (Other random feature maps). Techniques like ORF, (Yu et al., 2016), use an orthogonal rotation matrix along with a radial distribution multiplier. This is shown to be unbiased and has a lower variance than vanilla RFF. In (Yu et al., 2016) they also introduce ORF-prime, a version of ORF, with a constant radial component, which works well in practice for the Gaussian distribution. However, recently in (Demni & Kadri, 2024, Thm. 2), this was shown to actually approximate the normalized Bessel function of the first kind, which is different from the Gaussian. Structured ORF (SORF) (Yu et al., 2016) uses products of pairs of sign-flipping and Hadamard matrices (HD blocks) to approximate W. SORF uses ORF-prime and replaces the orthogonal matrix with products of HD blocks. However, whether SORF also demonstrates the bias shown in (Demni & Kadri, 2024) is not known. For the above reasons we do not include SORF and ORF-prime in our discussion. (Bojarski et al., 2016) extends this idea using random spinners. (Choromanski et al., 2017) generalizes SORF by using an arbitrary number of sign-flipping and Hadamard matrix blocks and also provides intution for why 3 blocks work well in practice.\nQuadrature rules approximate the Fourier transform integral (Dao et al., 2017), (Munkhoeva et al., 2018), however, these works assume separability of the integral which is available in case of the Gaussian and the l1-Laplacian kernel."}, {"title": "4. Fourier transform of \u03ba1/2 and Kv", "content": "To identify the distribution of weights in the random features, we must find out the fourier transform of the \u03ba function.\n\\(F{\\kappa} (w) := \\int_{R^{d}} \\kappa(\\triangle)e^{-j(w,\\triangle)} d\\triangle\\)\nNote that the anisotropic case M \u2260 \u03c32Id, can be dealt with easily. F{\u03ba} depends on the dual norm of the norm in \u03ba.\nLemma 3. (Stein & Weiss, 1971, Thm. 1.14) Fourier transform the e\u2212||\u2206||M, M \u2208 Rd\u00d7d is the Cauchy(0d, M) distribution.\nThe interested reader can see an elegant proof by (Stein & Weiss, 1971) in the Appendix A.\nIn case of the Laplacian kernel, clever manipulation enabled us to obtain the Fourier transform. This is difficult to do for the general Mat\u00e9rn kernel primarily due to the presence of a Bessel Function. (Choromanski et al., 2018) also provide closed forms for the fourier transform of the Mat\u00e9rn kernel, without mentioning the t-distribution.\nProposition 4. (Joarder & Ali, 1996, Thm. 3.1) The characteristic function of the t2\u03bd-distribution is\n\\[ \\frac{\\sqrt{\\pi} \\nu^{\\nu/2} J_{\\nu-1} (||\\sqrt{2\\nu} w||_{M})}{\\Gamma(\\nu) (\\sqrt{2 \\nu} \\frac{||w||_{M}}{2})^{\\nu}} , \\forall w\\in R^{d} \\]\nwhere M \u2208 Rd\u00d7d and J\u03bd : R \u2192 R is the modified Bessel function of the second kind of order \u03bd.\nFor a proof of this theorem, we refer the reader to (Kotz & Nadarajah, 2004), and (Gaunt, 2020) for a much simpler proof of the one-dimensional case.\n(Sutradhar, 1986) also studied the characteristic function of the t-distribution, where they formulated it in terms of separate infinite series for odd, even and fractional degrees of freedom. (Joarder & Ali, 1996) showed the relationship in terms of the Bessel function. Note that this form is also known in (Choromanski et al., 2018), though not operationalized to obtain random features."}, {"title": "5. Sampling from heavy-tailed anisotropic distributions using elementary distributions", "content": "We state a few centered\u00b9 multivariate distributions with non-identity shape matrix M \u2208 Rd\u00d7d. We discuss two ways to sample these distributions.\n1. Using correlated multivariate Gaussians: Sample N(0, M) and scale it appropriately.\n2. Using the norm distribution: Sample a uniformly random unit vector, apply \u221aM, and scale it appropriately."}, {"title": "5.1. Sampling from elliptically contoured \u03b1-stable distribution", "content": "Recall the discussion on elliptically contoured \u03b1-stable distribution.\nProposition 5. (Samoradnitsky, 2017, Prop. 2.5.2) Let M\u2208 Rd\u00d7d, and consider two independent random variables G ~ N(0, M), and A ~ S(\u03b1/2,1,1). Then \u221aAG is an elliptically contoured \u03b1-stable distribution with shape matrix M and has characteristic function\n\\(E e^{j (u, \\sqrt{A}G)} = exp (-(\\frac{\\sigma^{\\alpha/2} |u|^{\\alpha}}{\\cos(\\frac{\\pi\\alpha}{4})}))\\\\\nThe following corollary, which chooses a particular value of \u03c3 to cancel the exponent, provides a direct method of generating i.i.d. samples from the required stable distribution.\nCorollary 6. If \u03b1 \u2208 (0,2), A ~ S(\u03b1/2, 1, 2cos2(\u03c0\u03b1)/\u03b1), G ~ N(0, M), M\u2208 Rd\u00d7d, then\n\\(E exp (j (u, \\sqrt{A}G)) = exp(- ||u||_{M}^{\\alpha})\\)\nNote that, the above results have allowed us to reduce the problem of sampling from a multivariate stable distribution to that of a univariate \u03b1-stable distribution."}, {"title": "5.2. Sampling from multivariate Students t-distribution", "content": "The following lemma is the constructive definition of the multivariate t-distribution and simultaneously serves as a sampling algorithm.\nLemma 7. Let M \u2208 Rd\u00d7d and \u03bd > 0. If u ~ N(0, M) and v ~ \u03c72/2 and u, v are independent then, u/\u221av has the multivariate t-distribution with 2\u03bd degrees of freedom and shape matrix M \u2208 Rd\u00d7d.\nThe following lemma allows us to sample from the radial measure of the isotropic multivariate t-distribution.\nLemma 8. If w is a multivariate t-distributed random variable with 2\u03bd degrees of freedom and shape matrix \u03c32 Id, then we have ||w|| ~ GBP(d/2,\u03bd, 2, \u03c3\u221a2\u03bd)."}, {"title": "5.3. Sampling from multivariate Cauchy distribution", "content": "Notice that the Cauchy distribution is a special case of the t-distribution, thus a result similar to lemma 7 is applicable in case of the Cauchy distribution.\nLemma 9. Let M \u2208 Rd\u00d7d and consider independent random variables u ~ N(0, M) and v ~ N(0,1). Then, u/v ~ Cauchy(0d, M).\nSince Cauchy (0d, M) is symmetric about the origin, dividing by a \u03c7(1) random variable, which has the folded standard normal distribution, is equivalent to dividing by a standard normal random variable.\nThe following claim follows immediately from Lemma 8, since Cauchy distribution is the t-distribution with \u03bd = 1/2.\nCorollary 10. If w ~ Cauchy(0d, \u03c32Id), then we have ||w|| ~ GBP(d/2,1/2, 2, \u03c3)."}, {"title": "6. Main results", "content": "Recall definitions of S(\u03b1, \u03b2, \u03c3), and \u03c7(2\u03bd) from Section 2.\nTheorem 11 (RFF for the Laplacian, Exponential-power and Mat\u00e9rn kernels). Suppose \u03bd > 0, \u03b1 \u2208 (0,2] and M \u2208 Rd\u00d7d. Consider sets of independent random variables {ui}i=1\u221e i.i.d. N(0, M), {vi}i=1\u221e i.i.d. N(0, 1), {si}i=1\u221e i.i.d. S(\u03b1/2, 1, 2cos2(\u03c0\u03b1)/\u03b1), and {Ti}i=1\u221e i.i.d. \u03c7(2\u03bd), and construct matrices WL = (ui/vi), WE = (ui/\u221asi), and WM = (ui/\u221aTi), by stacking the Rd vectors along the rows. Then for all x,z \u2208 Rd, for all \u03b7 \u2208 {L,E,M}, we have almost surely,\n\\(lim_{p\u2192\u221e} (\\psi_{p}(W_{\\eta}x), \\psi_{p}(W_{\\eta}z))_{R^{2p}} = K^{\\eta}(x,z).\\)\nProof sketch. Note that if ui ~ N(0, M) and vi ~ N(0, 1) then ui/vi ~ Cauchy (0d, M) due to Lemma 9. Note that Cauchy(0d, M) is the Fourier transform for e\u2212||\u2206||M thanks to Lemma 3. The claim for KL follows via Proposition 1. Using a similar argument equipped with Lemma 7 and Proposition 4, the claim for KM follows.\nThe construction of WE is such that each row is a sample from a multivariate \u03b1-Stable distribution, due to Corollary 6. Which, by definition has the desired characteristic function. The claim follows immediately by Proposition 1.\nRecall the definition of the generalized beta prime distribution, i.e., GBP(\u03b1, \u03b2, p, q).\nTheorem 12 (ORF for the Laplacian, Exponential-power and Mat\u00e9rn kernels). Let p \u2208 N, M \u2208 Rd\u00d7d, and let Q be a unitary matrix of size p \u00d7 d obtained by vertically stacking p/d independent uniformly distributed d \u00d7 d matrices. Let SL, SE, SM \u2208 Rp\u00d7p be diagonal matrices, with entries sampled independently as {Sii}i=1p i.i.d GBP(d/2, 1/2, 2, \u03c3), {Sii}i=1p i.i.d. S(\u03b1/2, 1, 2cos2(\u03c0\u03b1)/\u03b1), and {Sii}i=1p i.i.d. GBP (d/2,\u03bd, 2, \u03c3\u221a2\u03bd). Then, for any x, z \u2208 Rd, for all \u03b7 \u2208 {L, E, M}, we have almost surely\n\\(lim_{p\u2192\u221e} (\\psi_{p}(S_{\\eta}Q \\sqrt{M}x), \\psi_{p}(S_{\\eta}Q \\sqrt{M}z))_{R^{2p}} = K^{\\eta} (x, z).\\)\nThe proof follows from Proposition 2 via arguments similar to Theorem 11. However, we now need to work with distributions of ||w|| where the characteristic function of w is \u03ba. These distributions have been derived in Corollary 10, and Lemma 8."}, {"title": "7. Numerical experiments", "content": "Our experiments cover \u03b4 = 2 \u00d7 3 configurations: 2 types of random features (RFF/ORF) and 3 kernels (Laplacian/Mat\u00e9rn/Exponential-power). In the main text, we have shown only limited experiments. The appendix contains the remaining configurations. Our experiments are for the isotropic case of M = Id."}, {"title": "7.1. Logistic regression with random features", "content": "We compare the performance of solving equation (5) with L as the empirical cross-entropy loss with a ridge penalty R(t) = \u03bbt2. We compare the two paramterizations (i) the exact kernel parameterization in equation (4) which involves a search over Rn, and (ii) the RFF parameterization \u03c6(x) = \u03c8p(Wx) in equation (6)."}, {"title": "8. Discussion and outlook", "content": "In this paper we presented 2 schemes, RFF and ORF, to approximate the Laplacian kernel and its two generalizations Mat\u00e9rn and Exponential-power kernels. The Laplacian kernel is radial and hence weakly non-separable. This leads to the random weight matrix for the random features to be coupled along its rows. We provide efficient ways to sample the random weights using simpler distributions.\nOur kernel evaluations are very fast in comparison to standard techniques in sklearn. Similar to the paradigm of neural networks, our approach also allows for scaling the training data arbitrarily from the model size. This opens the door for using best practices from deep learning such as data augmentation to kernel machines.\nWe also demonstrate that kernel regression with the logistic loss yields better calibrated models. These models can be hard to train beyond the square-loss. In such scenarios, we are better off using random features since the approximation errors are quite low.\nIn practice, sampling from very heavy tailed distributions can be prone to errors. This is especially the case for the Exponential-power kernel with 0 < \u03b1 < 0.3. Since these"}, {"title": "Appendices", "content": "The following proof is provided in (Stein & Weiss", "int_{R^{d}}e^{-jwx}\\left[\\frac{1}{\\sqrt{\\pi}}\\int_{0}^{\\infty}\\frac{e^{-u}}{\\sqrt{u}}e^{-\\frac{||x||^{2}}{4u}}du\\right": "dx\\\\\n=\\int_{0"}, {"frac{d+1}{2})\\left(1+||w||^{2}\\right)^{-\\frac{d+1}{2}}\\\\": "nFinally", "result": "n\\[e^{-B} = \\frac{2}{\\pi} \\int_{0}^{\\infty} \\frac{cos(x)}{1 + x^{2}} dx \\\\\n=\\frac{2}{\\pi} \\int_{0}^{\\infty} cos(x)\\left[ \\int_{0}^{\\infty} e^{-(1+x^{2})u} du \\right", "right": "e^{-u} du\\\\=\\frac{2}{\\pi} \\int_{0}^{\\infty} \\frac{\\sqrt{\\pi}}{2} e^{\\frac{-1}{4u}} \\frac{e^{-u}}{\\sqrt{u}} du\\", "left[1+\\frac{||w||^{2}}{2\\sigma^{2}\\nu}\\right": {}}]}