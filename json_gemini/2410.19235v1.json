{"title": "Learning Diffusion Policies from Demonstrations For Compliant Contact-rich Manipulation", "authors": ["Malek Aburub", "Cristian C. Beltran-Hernandez", "Tatsuya Kamijo", "Masashi Hamaya"], "abstract": "Robots hold great promise for performing repetitive or hazardous tasks, but achieving human-like dexterity, especially in contact-rich and dynamic environments, remains challenging. Rigid robots, which rely on position or velocity control, often struggle with maintaining stable contact and applying consistent force in force-intensive tasks. Learning from Demonstration has emerged as a solution, but tasks requiring intricate maneuvers, such as powder grinding, present unique difficulties. This paper introduces Diffusion Policies For Compliant Manipulation (DIPCOM), a novel diffusion-based framework designed for compliant control tasks. By leveraging generative diffusion models, we develop a policy that predicts Cartesian end-effector poses and adjusts arm stiffness to maintain the necessary force. Our approach enhances force control through multimodal distribution modeling, improves the integration of diffusion policies in compliance control, and extends our previous work by demonstrating its effectiveness in real-world tasks. We present a detailed comparison between our framework and existing methods, highlighting the advantages and best practices for deploying diffusion-based compliance control.", "sections": [{"title": "I. INTRODUCTION", "content": "Robots hold significant potential to enhance daily life by performing repetitive or hazardous tasks. To achieve this, robots must be able to manipulate objects with a level of dexterity comparable to that of humans. However, enabling robots to reach human-like dexterity remains a challenging problem, especially for tasks that require precise, contact-rich manipulation in dynamic environments, such as laboratories and workshops. Rigid robots, which typically operate through position or velocity commands, face difficulties maintaining stable surface contact and applying consistent force-both essential for force-intensive tasks.\nLearning from Demonstration (LfD) techniques [1] have emerged as a promising approach, enabling robots to acquire complex tasks by observing human experts and replicating their precise actions. However, tasks that require intricate maneuvers and high force, such as powder grinding in Fig. 1, pose unique challenges. In such scenarios, rigid robots often fail to sustain controlled contact and force over extended periods, limiting their effectiveness in demanding applications.\nIn such cases, rigid robots necessitate mechanical compliance mechanisms [2] to make the physical interaction safer at the expense of making it harder to precisely control the position of the tool, i.e., the pestle.\nTo address these challenges, we adopt compliance control schemes that enable rigid robots to handle tasks involving direct contact by modulating forces through external sensors [3]. In our prior work [4], a method called Comp-ACT was proposed that combines force information and compliance control with a policy based on Variational AutoEncoders and the Action Chunking with Transformers strategy (VAE-ACT). The Comp-ACT policies perform well in various tasks but struggle to solve long-horizon tasks with repetitive behaviors, such as grinding powder. Therefore, we extend compliant LfD policies by incorporating diffusion models, enabling robots to adaptively regulate force during task execution while maintaining precision and stability, especially during long-horizon tasks. Our approach builds on the robot demonstration system for contact-rich manipulation introduced in [4], improving its performance in handling force-intensive tasks.\nIn this work, we propose Diffusion Policies For Compliant Manipulation (DIPCOM), a novel diffusion-based framework that leverages generative models to handle compliant control tasks. Our choice of diffusion-based policies is motivated by their potential to capture multi-modal action distributions and produce a more diverse behavior than VAE-ACT-based policies [5]. Diffusion models progressively add noise to data in a forward stochastic process. By training a neural network known as a denoising model, they approximate the original data and iteratively generate new actions. We implement"}, {"title": "II. RELATED WORKS", "content": "A. Learning from Demonstrations for Contact-rich Manipulation\nLearning from demonstration has emerged as a promising approach for teaching robots complex contact-rich manipulation skills [6]. Researchers have explored using force/torque (F/T) sensing and haptic feedback to capture expert demonstrations to learn tasks such as grasping [7], ironing [8], pouring [9], and peg-in-hole insertion [10]. Recent work has focused on sample-efficient methods that can learn from a few demonstrations through transformer-based models and innovative teleoperation interfaces such as ALOHA [11] and Universal Manipulation Interface (UMI) [12].\nWhile most prior approaches focus on position control and mechanical compliance to allow robots a degree of safety while performing contact-rich tasks, this work focuses on active compliance control to perform contact-rich manipulation tasks.\nIn our prior work [4], we proposed an intuitive tele-operation interface for collecting demonstrations and the Compliance Control via Action Chunking with Transformers (Comp-ACT) method to learn compliance control policies from a few demonstrations. The current study utilizes the same teleoperation interface to collect demonstrations for training a DIPCOM policy.\nSimilar to the work of Drolet et al. [13] and Zhao et al. [14], where several imitation learning methods, including Action Chunking with Transformers (ACT) [11] and Diffusion policies [15], were compared on bimanual manipulation tasks, this study compares the performance of Comp-ACT [4] with the proposed DIPCOM with a focus on rigid robots tackling challenging real-world contact-rich tasks.\nB. Diffusion Policies\nDiffusion models, introduced by Ho et al. [16], are probabilistic generative models that transform random noise into meaningful samples from a target distribution. For a more comprehensive survey on diffusion models, see [17].\nIn robotics, diffusion models have effectively captured multi-modal actions and have been applied to various domains, including motion planning [18], navigation [19], human-robot interaction [20], and grasping tasks [21]. In manipulation, Chi et al. [15] demonstrated strong results in visuomotor policy learning from demonstrations using diffusion models.\nFurther advances include Liu et al. [22], who leveraged language-annotated play data to enable skill acquisition, while Reuss et al. [23] conditioned diffusion models on goal states to enhance skill learning from play data for task completion.\nPrevious work has primarily focused on improving learn-ing efficiency or using large, uncurated datasets, but the reliance on position or velocity controllers often limits their effectiveness in contact-rich tasks. To address this, we introduce a diffusion policy conditioned on force and integrated with a compliance controller. This study demonstrates that our framework can handle challenging contact-rich tasks with fewer demonstrations than reported in prior studies lacking compliance control [14]. Our experiments show that this approach significantly improves task performance without extensive demonstrations."}, {"title": "III. METHODOLOGY", "content": "We introduce Diffusion Policies For Compliant Manipulation (DIPCOM), a novel method for learning variable compliance control from demonstrations using diffusion models. Our approach predicts target EE poses and robot stiffness parameters conditioned on current observations, including the contact force data. A compliance controller uses these predictions to compute the final joint position commands that allow robots to move compliantly at the predicted stiffness.\nA. Problem Formulation\nLearning from demonstration (LfD) aims to enable robots to acquire new skills by autonomously observing and imitating human-provided demonstrations. In our approach, the policy learns to predict a sequence of absolute Cartesian EE pose and stiffness parameters given current observation Othat include RGB images $I \\in R^{H \\times W \\times 3}$, the latest F/T sensor reading $F \\in R^6$, and proprioception data $S\\in R^9$. The predicted action $A = \\{p,g,k\\}$ comprises three components: the absolute EE pose, the gripper action, and the stiffness parameter. The absolute EE pose, denoted as $p = \\{r, o\\} \\in R^9$, consists of a position vector $r \\in R^3$ and an orientation vector $o \\in R^6$. For more details on this 6D orientation representation, refer to Sect. III-C. The gripper action $g \\in R^1$ represents the desired non-binary opening width of the gripper. Finally, the stiffness parameter $k \\in R^6$ corresponds to the diagonal elements of the stiffness matrix. Each robot action A is then fed into a compliance controller.\nB. Data Collection\nThe teleoperation system presented in [4] was used for data collection. The system uses Virtual Reality (VR) controllers to provide the reference EE pose and stiffness that are input to the compliance controller. During the demonstration, the operator can switch between two pre-selected stiffness modes using the grip button on the side of the controller. The action A from the VR controllers and the observation from the robots and cameras are collected and stored.\nC. Orientation Representation for Cartesian Action Space\nAs in our previous work [4], the robot's state observations and actions were defined as absolute Cartesian poses. In [4], orientations were represented using axis angles (rotation vectors). However, this orientation representation has two problematic characteristics. The first issue is that many rotation vectors can represent the same orientation. The challenge with this one-to-many representation is that a model would need to be trained on all possible representations so that it can yield consistent outputs at inference time, given any of the possible representations. The second issue is that the axis angle representation is discontinuous. Zhou et al. [24] discussed the discontinuity problem and concluded that neural networks can better fit continuous representations. They proposed 5D and 6D continuous orientation representations. To address both issues, in this work, we adopted the six-dimensional orientation representation proposed by [24], which provides a continuous and unique representation for each orientation. The 6D orientation representation consists of the rotation matrix's first two columns. Please refer to [24] for a more comprehensive discussion.\nD. Diffusion Policies For Compliant Manipulation\nOur dataset is inherently multi-modal, containing a variety of observations and the corresponding actions that must be predicted. To address this, we aim to learn a policy distribution $\\pi(A|I, F, S)$ from a task-specific demonstration dataset. We introduce Diffusion Policies For Compliant Manipulation, a classifier-free conditional diffusion model designed to generate actions A based on observations O.\nThe diffusion process involves two stages: a forward process that incrementally adds noise to the data and a learnable inverse diffusion process that recovers the original data from noise conditioned on the input observations. The forward process progressively corrupts the data, while the inverse process, learned by the model, removes the noise step by step.\nWe implement the Denoising Diffusion Implicit Model (DDIM) formulation from Song et al. [25] for noise scheduling and denoising. This approach makes the denoising process deterministic, facilitating more efficient inference and allowing for flexible adjustment of inference steps to optimize performance.\nThe architecture of DIPCOM is shown in Fig.2. It follows the design of [15], using an encoder-decoder transformer [26] with a ResNet18 vision backbone (without pre-trained weights). Images are processed through the vision backbone, concatenated with force and robot state data, and input into the transformer's encoder. Cross-attention is applied to the noisy actions in the decoder.\nThe model is trained to predict actions $\\hat{a_0}$, using the mean squared error loss function:\n$L_{sample} = ||a_0 - \\hat{a_0}||^2$\nDuring inference, the model iteratively predicts the original sample using the formula:\n$\\hat{a}_{n-1} = \\sqrt{\\beta_{n-1}} \\hat{a}_n + \\sqrt{1 - \\beta_{n-1}} \\cdot a_0 - \\frac{\\beta}{\\sqrt{1 - \\bar{\\beta}}}$\nwhere $a_n$ represents the noisy data at time step n, $\\beta$ is the cumulative product of noise scales up to time n, and $\\hat{a}_0$ is the estimated original data.\nE. Action Sequence Generation\nApplying diffusion models to contact-rich manipulation presents unique challenges due to the robot's prolonged interaction with the object's surface. In such tasks, variable forces are exerted, and any inconsistencies in action prediction can negatively impact performance. Previous works [15], [12] typically predict a fixed number of actions, using part of the horizon while discarding the rest. This can lead to instability, such as jerky or abrupt movements,"}, {"title": "IV. EXPERIMENTS", "content": "The proposed method DIPCOM was evaluated on challenging contact-rich tasks that require applying force carefully and consistently to solve them. The tasks are described in Sect. IV-C and illustrated in Fig. 3. Additionally, we compare the performance of the proposed method against a baseline described below.\nA. Baseline Method\nThe prior work, Comp-ACT [4], was used as the baseline method. The baseline policy is trained as the decoder of a conditional variational autoencoder (CVAE). It consists of the transformer encoder, which synthesizes all the observational data, and the transformer decoder, which generates a sequence of action, namely Cartesian EE pose and stiffness parameters. Similarly to our approach, the baseline uses F/T observations, Cartesian EE poses, and RGB images to predict actions that are passed to the compliance controller.\nB. Experimental Setup\nThe robotic system used for experimentation consisted of two UR5e robot arms (Universal Robots A/S, 2024) with built-in F/T sensors on their wrists. Two cameras, RealSense SR305 (Intel Corporation, 2024), are attached to the wrist of each arm. Another static third-person view camera is placed in front of the robots to capture a wider field of view.\nThe demonstration data from each task was collected from three co-authors to add variety to the datasets.\nC. Tasks description and results\nThis section describes the tasks used for experimentation alongside the results obtained after training one policy per task for each model. The general conditions considered for each task are reported in Table I, such as the number of demonstrations and stiffness modes.\nA - Powder grinding: The task starts with the robot already holding the ceramic pestle and the granular powder placed at the center of the ceramic mortar. A single camera was used for this task, the one attached to the robot's wrist, as the robot would obstruct any other external view of the inside of the mortar. For this reason, the demonstrations consisted of pressing the pestle against the mortar and performing circular motions for a few seconds, then moving the robot out of the mortar to get a clear view of the state of the powder, and then repeating this process for about 80 seconds. We measured the fine powder produced after each demonstration and compared it against the policy performance for this task. For practical purposes, instant coffee powder was used for experimentation as it comes in a granular form that can be ground to a finer powder. Similarly, a tea strainer was used to sift the powder.\nResults: As shown in Table II, our proposed method achieved the highest percentage of fine powder produced at 56 % compared to the 10 % achieved by Comp-ACT. Both methods applied a similar magnitude of force against the powder and mortar, as illustrated in Fig. 5. Nevertheless, our DIPCOM policy obtained better results by reproducing the demonstrated behavior of moving the pestle in circular motions and periodically checking the state of the powder before repeating the grinding action, as shown in Fig. 4. On the contrary, the Comp-ACT policy struggled to reproduce the circular motions with the pestle, mostly staying in the same place during the entire duration of each test.\nB Pencil eraser: The task's goal is to use a rubber eraser to remove pencil markings from a notepad. The task begins with the word \"OSX\" written on the notepad fixed to the table while the robot already holds the rubber eraser. The robot has to gently but firmly press the eraser against the paper to remove the pencil marks without damaging the paper. During data collection, the demonstrator rubbed the eraser from right to left in a straight line, then lifted the eraser from the paper, moved back to the right side of the marks, and repeated several times until the mark was removed completely. The task was evaluated with two metrics. First, the success rate where success was defined by whether the pencil marks were completely erased or not."}, {"title": "V. DISCUSSION", "content": "As mentioned above, both DIPCOM and Comp-ACT achieved similar overall outcomes in the bimanual insertion tasks, but their task execution behaviors varied notably. These differences became more pronounced in longer horizon tasks, such as powder grinding and pencil erasure, which demand repetitive or continuous actions based on recurring observations and don't have a linear flow. Comp-ACT initially performed well in these tasks accurately targeting the writing during erasure and correctly approaching the powder in the grinding task. However, Comp-ACT struggled with maintaining fluid, repetitive motions, often freezing mid-task during actions like up-and-down movements for erasing or circular motions for grinding. Conversely, DIPCOM handled these repetitive tasks more flexibly, continuing with smooth, adaptive actions despite exhibiting more variance in its execution. These results regarding the diversity of behaviors exhibited by Comp-ACT (a VAE-based method) and DIPCOM (a Diffusion-based method) agree with those reported by Jia et al. [5] despite the difference in task state and action spaces.\nAnother observation was about force application during these tasks. Both policies applied force more conservatively than the human demonstrators. However, despite having a larger standard deviation, DIPCOM's diffusion-based policy better mimicked the demonstrators' force patterns. Comp-ACT, on the other hand, applied force more consistently but tended to fall into cyclic force patterns, especially in long-horizon tasks.\nLimitations: During training and fine-tuning, it was noted that DIPCOM is more sensitive to parameter settings than Comp-ACT. Diffusion-based policies, in general, require more computational resources, making them susceptible to disturbances in the robotic system's overall control frequency, which can impact their performance. This computational overhead makes DIPCOM more challenging to tune and maintain than Comp-ACT. Future work will explore the impact of hyperparameters tunning and alternative action spaces, such as the relative trajectory proposed by [12].\nIt is important to highlight that, in this study, the number of demonstrations used to train the policies was relatively small compared to other works exploring similar methods [15][14]. An interesting future research avenue is to explore the effects of large-scale datasets for compliant manipulation tasks aiming to improve policy generalization across extended task variations. For instance, instead of training a policy for each specific powder grinding task, we aim to develop a single policy that can perform across different grinding tasks, broadening its applicability and robustness."}, {"title": "VI. CONCLUSIONS", "content": "This study introduced Diffusion Policies For Compliant Manipulation (DIPCOM), a diffusion-based framework designed for compliant control tasks, particularly for rigid robots. Our approach demonstrates how diffusion policies effectively capture the multimodality of the data, predicting Cartesian end-effector poses while adjusting the arm's stiffness to apply the required contact forces. We provide guidelines for implementing diffusion policies for compliant control and outline best practices for optimizing performance. Through extensive experimental evaluations, we showcased DIPCOM's strengths in various contact-rich tasks, highlighting its advantages over previous methods. Moving forward, we aim to explore the policy's ability to generalize across different task variations by expanding the dataset. We plan to enhance force processing and refine the policy architecture to improve force-aware inference."}]}