{"title": "SoK: A Classification for Al-driven Personalized Privacy Assistants", "authors": ["Victor Morel", "Leonardo Iwaya", "Simone Fischer-H\u00fcbner"], "abstract": "To help users make privacy-related decisions, personalized privacy assistants based on AI technology have been developed in recent years. These AI-driven Personalized Privacy Assistants (AI-driven PPAs) can reap significant benefits for users, who may otherwise struggle to make decisions regarding their personal data in environments saturated with privacy-related decision requests. However, no study systematically inquired about the features of these AI-driven PPAs, their underlying technologies, or the accuracy of their decisions. To fill this gap, we present a Systematization of Knowledge (SoK) to map the existing solutions found in the scientific literature. We screened 1697 unique research papers over the last decade (2013-2023), constructing a classification from 39 included papers. As a result, this SoK reviews several aspects of existing research on AI-driven PPAs in terms of types of publications, contributions, methodological quality, and other quantitative insights. Furthermore, we provide a comprehensive classification for AI-driven PPAs, delving into their architectural choices, system contexts, types of AI used, data sources, types of decisions, and control over decisions, among other facets. Based on our SoK, we further underline the research gaps and challenges and formulate recommendations for the design and development of AI-driven PPAs as well as avenues for future research.", "sections": [{"title": "1 INTRODUCTION", "content": "As the world becomes increasingly digitalized, people are faced with a higher number of decisions related to their privacy. We surround ourselves daily with several apps and websites, and the number of smart gadgets and Internet of Things (IoT) devices continues to grow [1]. Furthermore, to enforce the individuals' rights to informational self-determination and comply with privacy laws such as the General Data Protection Regulation (GDPR) [23], software systems regularly require us to make privacy-related decisions regarding our personal data: Do you grant this permission? Do you want to accept the cookies? Should this sensor be left on when you host friends? Consequently, the cognitive burden increases, leaving users in disarray, tired, and unable to decide in their best interests [17].\nDuring the last decade, researchers have been building privacy assistants to alleviate this burden and support users in their decisions (see the patent on Personalized Privacy Assistant registered in 2023 in the US by Sadeh et al. [54]). With the progress made in Artificial Intelligence (AI), it is no surprise that some of these assistants leverage this technology, notably enabling more personalized support. However, the extent to which AI drives these Personalized Privacy Assistants (AI-driven PPAs), their efficiency, privacy-friendliness, functioning, and eventual addressing of legal requirements remains unclear. In fact, to the best of our knowledge, there have been no surveys or systematic reviews on the topic of AI-driven PPAs. This lack of systematization of knowledge prevents other researchers from identifying existing gaps in the field and efficiently addressing the challenges.\nTo address this lack of coherence, provide a common vocabulary, and better compare and categorize the different AI-driven PPA solutions, we propose a Systematization of Knowledge (SoK) of the last decade of research. In doing so, we aim to draw insights and lessons for future assistants and to formulate better recommendations for research, design, and development of AI-driven PPAs. Formulated otherwise, we tackle the following Research Questions (RQs):\n\u2022 RQ1: What is the current state of the literature on Al-driven PPAs for automated support of end-users privacy decisions in IT systems?\n\u2022 RQ2: What are the key attributes and properties of the proposed Al-driven PPAs in the literature?\nHere, we understand agents and assistants in a broad sense (any logical entity able to support users, including unimplemented theoretical models, see our selection criteria in Table 1); AI in a generic sense as well (see Section 2.3); and privacy decisions as individual decisions regarding one's personal information management (see Section 2.2).\nTo address our RQs, we performed a Systematic Literature Review (SLR) on research papers that provided technical solutions, published between 2013 and 2023 in peer-reviewed venues, and a further snowballing process until early May 2024. We screened 1697 unique papers from IEEE, ACM, Scopus, and Web of Science, resulting in 39 selected papers after several rounds of snowballing. We extensively read and analyzed all the included papers, and the information extracted forms the basis of our work.\nAs a result of our SLR, we make the following contributions:"}, {"title": "2 BACKGROUND AND RELATED WORK", "content": "As a background, this section first introduces legal concepts around privacy, then provides an overview of different types of privacy decisions for which individuals could receive support from AI-driven PPAs. Lastly, it presents AI and Machine Learning technologies that can provide the technical foundations for AI-driven PPAs."}, {"title": "2.1 Legal Background", "content": ""}, {"title": "2.1.1 Roles and Obligations According to the GDPR and Al Act.", "content": "AI-driven PPAs are using AI techniques for processing data, including personal data, to assist users with making personalized privacy-related decisions. For the discussion of any legal requirements regarding the use of personalized privacy assistants, the question of who the data controller is for any personal data processing by the assistants will be of relevance.\nIf AI-driven PPAs are installed and run by users on their own devices or other servers under their control, the users will likely act as data controllers or joint controllers with other service providers. The so-called household exemption (Art. 2(2)(c) GDPR) can take effect, meaning that the GDPR [23] will not apply if the user is using the assistant for private purposes on their private devices or servers under their control. If the AI-driven PPA is run not only for purely private purposes on the user's devices or controlled servers, the data controller may be another entity different from the user (e.g., the user's employer). Legal obligations need to be fulfilled by the controllers regarding data protection by design and default (Art. 25) of the assistants, security of data processed (Art. 32), implementing data subject rights, including the data subject's rights to transparency (Art. 13-15), their rights to object to profiling (Art. 21), and the right not to be subject to a decision based solely on automated processing, including profiling (Art. 22).\nIn addition, legal obligations according to the EU AI Act [24] may also have to be considered for the producer and also by the deployers of AI-driven privacy assistants, including requirements for risk management (Art. 9), transparency (Art. 13, 50), robustness, security, accuracy (Art. 15). These obligations however mostly apply if Al-driven PPAs could be classified as \"high-risk\" AI systems. This should, however, seldom be the case, especially as AI-driven PPAs are typically used for users' own privacy management, which should typically not interfere with the fundamental rights of others. Exceptions could, however, be AI-driven PPAs that are, for example, used for setting permissions for safety-critical applications impacting the safety of the users or others."}, {"title": "2.1.2 Legal Requirements for Transparency.", "content": "In cases where the data controllers of the AI-driven PPAs are not the data subjects themselves, the controllers should provide the data subjects with privacy policy information ex-ante at the time when data are obtained from them according to Art. 13 GDPR, and ex-post through the right to access granted in Art. 15 GDPR. This also should include information about purposes of processing, data categories concerned, but also information about the logic involved and significance, and envisioned consequences of automated decision-making and profiling performed by the AI-driven PPAS.\nThe AI Act also includes obligations for transparency for the producers and deployers of limited-risk and high-risk AI systems (Art. 50). While the providers of limited-risk Al systems have to mainly ensure that humans are informed that AI systems are used, high-risk Al systems require that further clear, comprehensible and adequate information is given to the deployer (Art. 13), traceability of results via logging (Art. 12) and appropriate human oversight (Art. 14)."}, {"title": "2.1.3 Legal Requirements for Consent.", "content": "Art. 4 (11) of the GDPR defines 'Consent' of data subjects as any freely given, specific, informed, and unambiguous indication of the data subject's wishes by which they, by a statement or by a clear affirmative action, signifies agreement to the processing of personal data relating to them. A valid consent has thus to fulfill several conditions. Namely, it needs to be:\n\u2022 Freely given, i.e., the data subject needs to have free choices this is usually not the case if there is an imbalance of power in the relation between the data subject and the data controller. Furthermore, there should be no negative consequences if consent is not given. Moreover, consent may not be bundled as a non-negotiable part of terms and conditions.\n\u2022 Specific, which means that consent must be given for one or more specific purposes and that the data subject must have a choice in relation to them \u2013 i.e., separate opt-in is needed for each purpose.\n\u2022 Informed, which means that the data subject has to be informed about certain elements that are crucial to making a choice. This includes information about the controller's identity, the data processing purposes, the type of data, the right to withdraw consent, any use for decisions based solely on automated processing, and risks of data transfers to third countries.\n\u2022 Moreover, a confirming statement or affirmative action is needed for a valid consent and requires that the data subject has taken a deliberate action to consent. Therefore, silence, pre-ticked boxes, or inactivity should not constitute consent (Recital 32 GDPR). It also means that consent cannot be fully automated."}, {"title": "2.2 Privacy Decisions", "content": "Among the most notable definitions, Westin [65] has defined pri-vacy as the right to informational self-determination, meaning that individuals should have the right to decide for themselves when, how, and what information about them is communicated to others. As mentioned, in the EU, the GDPR emphasizes that individuals should have control of their personal data (Recital 7), and thus"}, {"title": "2.2.1 Individual Privacy Decisions Regulated by Privacy Laws.", "content": "Some privacy decisions individuals can make to exercise control over their data are regulated under the GDPR and other privacy laws. These decisions notably include, but are not limited to, the decisions to grant or to withdraw consent to data collection and processing.\nIndeed, the GDPR and most other privacy laws regulate decisions to exercise data subject rights granted by the respective laws. For instance, according to Art. 15-22 GDPR, data subjects have the rights to access data, request rectification or deletion of data, export data, and object to direct marketing and profiling. Data subjects can also object in cases where the legal ground for the processing is public interest or legitimate interest, or exercise their right not to be subject to automated decision-making."}, {"title": "2.2.2 Further Types of Privacy Decisions.", "content": "Further types of privacy decisions concerning users' choices regarding the use of their data by others, which are not directly mentioned or regulated by the GDPR, include decisions of individuals to publish or share data on their own initiative, e.g., in social networks. In these cases, data sharing has typically not been formally triggered by a consent request to allow data sharing with another party.\nMoreover, privacy decisions encompass privacy permission (or access control rights) settings, which grant others certain rights for using their data and are, for instance, typically used for permission systems of mobile phone operating systems, such as Android or iOS. Setting privacy permissions on mobile operating systems often requires consent at installation or during runtime. However, instead of consent, other legal grounds - such as a contract (Art. 6 (1)(b) GDPR) -, can be used, e.g., for a banking app to forward account information when transferring money [7]. Let us also note the peculiar case of Global Privacy Control (GPC), a unary signal that permits or prohibits third-party tracking on the browser [31]. Due to its enforceability under the California Consumer Privacy Act (CCPA), it is regulated by a privacy law but is technically more akin to a privacy permission.\nAdditionally, some privacy-enhancing technologies and proto-cols allow users to decide and set privacy preferences, which are simply indications of the users' privacy wishes of how their data should be used without actually granting any rights to others, and thus without legal mandate. Privacy preferences have, for instance, been used earlier by the Platform for Privacy Preferences (P3P) [18] or Do Not Track (DNT) as an example for signals that can be set manually in browser settings for allowing users to specify their privacy choices."}, {"title": "2.3 AI for Decision-making", "content": "Al is a generic term for various strategies and techniques enabling computers and machines to simulate human intelligence and problem-solving capabilities [53]. Machine learning (ML) is a field of AI (we subsume the former under the latter in the rest of the document) that develops and studies statistical algorithms and models, draws inferences from patterns in data, and learns and adapts without following explicit instructions. AI-powered tools can particularly lighten the user's cognitive load and thereby improve their decision-making, e.g., by decision support, augmentation, or automation.\nWhile there are different ways to categorize Al systems, we refer in the present work to the survey paper on eXplainable AI (\u03a7\u0391\u0399) by Arrieta et al. [6]. They distinguish between transparent models and those requiring post-hoc explainability (non-inherently trans-parent). We leverage this reference because AI-supported decision-making must be explained under specific circumstances according to the GDPR and the AI Act [51].\nIn their words: \"A model is considered to be transparent if by itself it is understandable.\" [6] Such models include linear regression, decision trees, k-nearest neighbors, rule-based learning, general additive models, and Bayesian models. Nonetheless, models that are not deemed intrinsically transparent can be made explainable through the use of post-hoc techniques. Neural networks (especially deep and convoluted) and Support Vector Machines (SVM) typically fall under this category, as well as reinforcement learning [52]."}, {"title": "3 METHODOLOGY", "content": "This SoK study adopts the widely known methodology for system-atic literature reviews (SLRs) proposed by Kitchenham [36]. The SLR methodology offers us a well-defined and rigorous sequence of methodological steps consisting of three main phases: (1) planning, (2) conducting, and (3) reporting the review. A SLR Protocol that describes the entire research process has been written for this study (a summary version of which can be found in Appendix A.1). Furthermore, we make our research data openly available in an anonymised GitHub repository \u00b9 for reproducibility. Our material comprises the citation files of each query, the Data Extraction Forms (DEFs) of the selected papers, and the charting spreadsheet used to compile all our data. Thus, due to page constraints, we refer readers to these documents for methodological details."}, {"title": "3.1 Planning the Review", "content": "During the planning phase, our first activity was determining the need for this SLR. Several databases were searched to verify if any surveys or reviews had been conducted on AI-driven PPAs. Search terms such as privacy, data protection, assistant, agent, artificial intelligence, and machine learning were used. However, we could not identify any survey or systematic reviews on the topic, reassuring the need for an SLR.\nThe research questions, presented in Section 1, guided the re-maining phases of this SLR with respect to the search process, selection criteria, and data synthesis."}, {"title": "3.2 Conducting the Review", "content": ""}, {"title": "3.2.1 Search Strategy.", "content": "Based on our RQs and previous preliminary searches when designing the SLR Protocol, we identified a list of nine relevant keywords, i.e., privacy, data protection, assistant, agent, artificial intelligence, machine learning, intelligent, automatic, and personalized. These keywords were used to construct the following search query:"}, {"title": "5 CLASSIFICATION FOR AI-DRIVEN PPAS", "content": "We provide in this section a classification for AI-driven PPAs as the main contribution of this SoK. The classification comprises several dimensions, i.e., features typically considered in the design of such an assistant. These dimensions are the type of decision (Section 5.1), the type of AI (Section 5.2) and the source of data (Section 5.3) used in the decision, the system context (Section 5.4), the choice architecture of its eventual implementation (Section 5.5), the empirical assessment (Section 5.6), and the extent to which users have control over the decisions (Section 5.7).\nThe classification and its dimensions are data-driven in the sense that they were derived based on what is described in the papers, reflecting the current state of the literature. For example, considering the category of system contexts, more dimensions could be envisioned, but we limited it to the four dimensions (i.e., mobile apps, social media, IoT, and cloud) that were found in the papers. Each feature will be explored in more detail in this section, and substantiated with non-exhaustive examples for each possible option, while an overview is provided in Figure 2.\nNote that not all dimensions are necessary for composing an AI-driven PPA. The dimensions for the type of AI, source of data, type of decision, and system context are \u201cmandatory,\u201d consisting of essential requisites that an AI-driven PPA needs to consider (solid boxes in Figure 2). Other dimensions such as the empirical assessment, choice architecture, and user control over decisions are \"optional\" since not all the identified AI-driven PPAs were evaluated, some do not have an implementation (and therefore an architecture), and some (regrettably) do not empower users with much control for various reasons (dashed boxes in Figure 2).\nFurthermore, the papers address each of these dimensions to different extents, and their options are often non-exclusive. For instance, all articles surveyed discuss the type of decision, but this is not the case for the choice architecture; and while most solutions"}, {"title": "5.1 Type of Decision", "content": "Decisions taken by an Al-driven PPA can be of different types, and it is essential to distinguish them to assess the possibilities they offer. Indeed, some decisions - such as permissions - have a binding character, i.e., constraining the system to act according to the user's choice, while others do not, such as preferences. Note that it may not always be possible to distinguish between each type of decision clearly (as discussed in Section 2.2.2). Other types of decisions with different implications regarding their enforcement can be envisioned by an AI-driven PPA (such as consent or deletion requests, see Section 2.2)."}, {"title": "5.1.1 Permission Settings.", "content": "The first type of decisions that many AI-driven PPAs assist the users with is permission settings, which, as discussed in Section 2.2.2, correspond to access control settings. Permissions are system-specific and binding, as the underlying operating system should enforce them.\nWe typically find mobile app permissions (e.g., in Baarslag et al. [9], mobile apps are addressed in 11 papers), but they are not re-stricted to the mobile environment. AI-driven PPAs can deal with permissions in IoT environments (see, e.g., [37], IoT is covered by 13 papers) or in the cloud [29]."}, {"title": "5.1.2 Preference Settings.", "content": "The second type of decision covered by the literature is preference settings, which, unlike permission settings, should be understood as expressions of will. Several works refer to preferences while they actually deal with permissions [26, 29, 41, 58, 67]. It is indeed common to talk about preferences im-precisely, but they should not be confused with permissions that have a binding property."}, {"title": "5.1.3 Data Sharing.", "content": "Data sharing is the third type of privacy decision of Al-driven PPAs encountered in the reviewed literature, for which the binding character is uncertain for users (for instance, assessing whether a limitation in the audience is enforced is not always possible from a user point of view, because the underlying"}, {"title": "5.2 AI Technology Used", "content": "Another significant characteristic of AI-driven PPAs is the type of AI used. Many solutions are based on machine learning models, such as supervised ML (classification), non-supervised ML (clustering), and reinforcement learning, sometimes combined. It is, however, also possible to find older AI techniques grouped under the umbrella of expert or rule-based systems.\nWe also classified the different Al technologies used by the AI-driven PPAs reviewed regarding their explainability, or their in-herent transparency. However, XAI is only explicitly addressed by one work [42]; the other models are therefore categorized based on Arrieta et al. [6]'s taxonomy. We annotated T for Transparent in Table 2, NIT for Not-Inherently Transparent, and PT for Partially Transparent when the solution relies on models with different levels of transparency."}, {"title": "5.2.1 Transparent.", "content": "Classification. Supervised machine learning, also called classifi-cation models, is a common set of techniques deployed in AI-driven PPAs. In this context, a model is trained to classify an object of decision into a choice tailored to the users' desires.\nTransparent classification models [6] (used in 7 papers) are com-posed of decision trees (used for instance in Bahirat et al. [10]), k-nearest neighbors (leveraged in Botti-Cebri\u00e1 et al. [14]), and Bayesian models (see Olejnik et al. [49]).\nClustering. Several works use clustering techniques for their AI-driven PPA. In this context, clustering is classically used to create a set of privacy profiles, i.e., an archetypal ensemble of default parameters (for preferences or permissions) to which a user is then assigned. Clustering algorithms (leveraged in 6 papers) used are hierarchical clustering (Liu et al. [41]), k-means (Brand\u00e3o et al. [15]),"}, {"title": "5.2.2 Not-inherently Transparent.", "content": "Classification. Non-transparent classification models (found in 13 papers) typically encompass classic neural networks (as in Klin-gensmith et al. [37]) and deep neural networks (see for instance Yu et al. [70]); random forests ([44]), Ada Boost [43] and Support Vec-tor Machines (used in Wijesekera et al. [67]) complete the picture. Post-hoc explanations must complement these models, as they are not easily understandable by themselves.\nReinforcement. Reinforcement learning is the least used family of machine-learning techniques in AI-driven PPAs. It is implemented in Kaur et al. [35] and Ulusoy and Yolum [63], both used to adapt users' feedback to their preferences, and in Zhan et al. [71]. The first paper uses it to disclose information (using permissions), while the second uses it to learn bidding preferences in a negotiation context.\nLogic-based. AI-driven PPAs can be based on logic (5 papers), for instance, expert systems (K\u00f6kciyan and Yolum [40] uses an agent-based model) or game theory (such as Hirschprung and Alkoby [30]). These works, albeit few, span various system contexts and types of decisions."}, {"title": "5.3 Source of Data", "content": "An AI-driven PPA can rely on various sources of data when using AI to help with a privacy decision. These data sources are very often combined, and a careful choice is necessary to fully exploit the potential of the models described in the previous section."}, {"title": "5.3.1 Context.", "content": "Context is an often-used data source, yet not always well-defined. However, when it is defined, it is composed of the"}, {"title": "5.3.2 Attitudinal Data.", "content": "A few AI-driven PPAs ask users questions to elicit so-called attitudinal data about stated practices or prefer-ences regarding privacy recommendations to avoid the so-called cold-start problem, which arises when no past data is available to provide a recommendation. For example, Nakamura et al. [46] fo-cuses on asking a minimal set of questions while keeping accuracy as high as possible, or Alom et al. [3] asks \"a reasonable number of questions (50) to the users.\""}, {"title": "5.3.3 Behavioral Data.", "content": "Another common source of data is behav-ioral data. Behavioral data has the advantage of reflecting the actual privacy decisions of users to predict the next ones, as it does not simply rely on stated practices (unlike attitudinal data). While it can be a powerful tool, it can also create a feedback loop, reinforcing the same decisions.\nBehavioral data can encompass past decisions, such as in Zhan et al. [71], which leverage past choices to fill a knowledge base, then used them to predict privacy decisions. It can also comprise current settings or preferences on a specific type of data to infer a decision for another type [29]. The system can also use these preferences to match users to a particular privacy profile, such as using clustering techniques (see Section 5.2.1)."}, {"title": "5.3.4 Metadata.", "content": "Metadata is data that provides information about other data, for example, the name of an application used [67], network requests [62], the purpose associated with processing [11], the usage frequency of certain permissions (such as location) by an app [34], or tags associated with images [61]. To some extent, metadata can overlap with context, for instance, when considering time or location. However, the articles surveyed more often refer to the time and location of collection of a certain data point for meta-data, and to the current time and location when a decision has to be made for context. Metadata can provide peripheral information to make decisions, although it is rarely used as a sole source of data (only 3 papers out of 13 [33, 37, 62] rely only on metadata)."}, {"title": "5.3.5 Data Type.", "content": "The data type refers to the category of data con-cerned by the decision, such as whether it is an image to share on social media [72], the location requested by an app [26], or various sensor data by an IoT device [58]. The type of data can provide accurate information about the sensitiveness of a decision (loca-tion data can, for instance, provide sensitive information regarding the users' context, e.g., from location data that reveals that a user visits a clinic or church, medical, or religious information could be inferred), yet only a relatively low number of solutions rely on the data type to build an AI-driven PPA [9, 26, 42, 46, 58, 67, 72]."}, {"title": "5.3.6 Content of Data.", "content": "The content of data refers to the specific content of a data point, as the name indicates. However, we also include data that can be directly inferred from the content of data under this category. For example, Botti-Cebri\u00e1 et al. [14] and Dong et al. [22] estimate the sensitivity of the content of the information to be shared to help make a decision. Indeed, content can be lever-aged to tailor decisions: a picture deemed private should not receive the same treatment as one deemed public, and a geolocation trace that may providentially allow inferring religious practice should be dealt with cautiously."}, {"title": "5.4 System Context", "content": "Most AI-driven PPAs target a specific system context, that is, a set of technologies with distinct characteristics. Indeed, each system context has specific requirements that one must consider when designing an AI-driven PPA. System contexts differ by the avail-ability of an interface, computational power, and control over the architecture."}, {"title": "5.4.1 Mobile Apps.", "content": "Several works focus on mobile applications, and often on Android [5, 67]. Mobile ecosystems have the advantage of being well-defined ecosystems, enabling the possibility to strictly enforce privacy decisions (i.e., it is often addressed with permissions, see Section 5.1.1).\nMobile phones also possess reasonable computational power (in the sense that they can run an Al-driven PPA) and a screen enabling direct user interactions. Hence, an AI-driven PPA can be implemented directly on a smartphone (see Baarslag et al. [9]), and it can interact with and even regulate mobile apps, all of which make mobile ecosystems suitable candidates for AI-driven PPAs under the users' control."}, {"title": "5.4.2 IoT.", "content": "Another widely used system context for AI-driven PPAs is the Internet of Things (IoT). We understand IoT as a network of devices, including sensors, mechanical and digital machines, as well as consumer devices, all connected to the Internet. In practice, AI-driven PPAs have been developed for smart homes [11, 58], on campuses [21], or for wearables such as fitness devices [55] for instance.\nMost IoT devices are usually not equipped with proper interfaces and lack computational power. These characteristics make it chal-lenging to build AI-driven PPAs assisting with permission settings, yet not impossible (see Klingensmith et al. [37] for instance, who manage to do so with an AI-driven PPA located on end devices)."}, {"title": "5.4.3 Social Media.", "content": "According to our classification of the literature, the third major system context is social media, for which several AI-driven PPAs have been designed to help make privacy decisions. In this case, neither the interface nor the computational power are usually limiting factors. However, the design and implementation of social media platforms (that are usually not published openly) make it difficult to assess the binding character of privacy decisions supported by AI-driven PPAs running on social media platforms. AI-driven PPA solutions are rather designed to support data sharing, i.e., whether a specific post should be shared on social media and with whom, than focusing on assisting users with privacy decision-making."}, {"title": "5.4.4 Cloud.", "content": "Another system context is cloud environments, even though only one of the reviewed articles proposes an Al-driven PPA for the cloud [29]. Their solution offers a method to simplify information disclosure in cloud environments such as Google Drive. However, this work is thus a lone example and contrasts with the otherwise balanced distribution of works among other system contexts."}, {"title": "5.5 Architecture", "content": "By architecture, we refer here to where the computation happens, i.e., the decision-making, and not necessarily the pre-processing steps such as building privacy profiles. Directly connected to the architecture is the trust model of the AI-driven PPA. While this term is usually reserved for security-oriented research, describ-ing whether one has to trust the different entities or not provides relevant information for understanding the privacy boundaries.\nNote that the location of the computation is only relevant for implemented AI-driven PPAs, and not for theoretical models. Simi-larly, most solutions surveyed do not explicitly describe a threat or trust model in their paper. Nonetheless, it is possible to infer that trusted parties are required in some solutions. For instance, Tan et al. [62] describes an architecture comprising a remote classifier, in which one has to place trust, yet no trust model is described."}, {"title": "5.5.1 Local Computation.", "content": "The processing can happen locally on the user device, such as on a smartphone (see, e.g., [49]), but this device can also be a home pod in an IoT context (see, e.g., [58]).\nCreating and processing user profiles, using local AI models, and locally deriving privacy decisions have the advantage that the user can keep control over the locally processed data, including their profiles and AI models, which usually can include sensitive information about the user's preferences or behavior. However, local data processing also puts more responsibilities on the user to secure the devices properly against malware or other attacks."}, {"title": "5.5.2 Remote Computation.", "content": "The AI-driven PPA could also be based on remote (according to the user's point of view) data processing, involving a central server that processes personal privacy decisions, contextual data including, e.g., location data or another type of data. Remote computation raises the question of the trust placed in the party performing this computation to protect the data properly, to enforce the data subject's rights (e.g., to access or to delete their data and computed profiles or models), and not to use the data for any unintended purposes [62].\nSeveral solutions rely on a remote third party that has to be trusted, e.g., Baarslag et al. [9] or Tan et al. [62]'s solution that places trust on their own remote classifier. In contrast, others only require trusting the operating system (OS) on which the AI-driven PPA is implemented [49], or require trusting both the OS and mobile applications [5]."}, {"title": "5.5.3 Federated Learning.", "content": "Only one article, by Brand\u00e3o et al. [15], presented an AI-driven PPA based on federated learning. In this work, the processing of user data for the computation of locally trained neural network models happens on the user devices that share only the neural network weights with a central server, which will, in turn, average all the local weights and send back the re-sults to the clients, which can use these new weights to continue"}, {"title": "5.6 Empirical Assessment", "content": "AI-driven PPAs' performance can be measured in terms of accuracy, but because several solutions are meant to be usable tools, assessing an Al-driven PPA encompasses more than a mere measurement of how well a privacy decision is predicted.\nAs mentioned in Section 3.2.3, an empirical assessment can be an evaluation (see e.g. [41]) or a validation (e.g., [30])."}, {"title": "5.6.1 User Study.", "content": "A classical way to validate a tool or a method is to conduct a user study, and we found 16 papers reporting a user study to validate usability. A user study can have various interpreta-tions, ranging from a simple questionnaire to rate satisfaction (such as Alom et al. [4]) to a large-scale randomized controlled study (see for instance Liu et al. [41]) - the former being more akin to a mere validation, the latter a full-fledged evaluation.\nNote that several works elicited data to build a dataset through a user study, which was therefore not meant as a means of assessment (annotated as a in Table 6)."}, {"title": "5.6.2 Purely Statistical (Dataset).", "content": "Several works provide a valida-tion without a user study, that is, only based on a purely statistical analysis based on a dataset (such as Kasaraneni and Thomas [33"}]}