{"title": "One Model, Any Conjunctive Query: Graph Neural Networks for Answering Complex Queries over Knowledge Graphs", "authors": ["Krzysztof Olejniczak", "Xingyue Huang", "\u0130smail \u0130lkan Ceylan", "Mikhail Galkin"], "abstract": "Traditional query answering over knowledge graphs \u2013 or broadly over relational\ndata - is one of the most fundamental problems in data management. Motivated\nby the incompleteness of modern knowledge graphs, a new setup for query an-\nswering has emerged, where the goal is to predict answers that do not necessarily\nappear in the knowledge graph, but are present in its completion. In this work,\nwe propose ANYCQ, a graph neural network model that can classify answers\nto any conjunctive query on any knowledge graph, following training. At the core\nof our framework lies a graph neural network model trained using a reinforce-\nment learning objective to answer Boolean queries. Our approach and problem\nsetup differ from existing query answering studies in multiple dimensions. First,\nwe focus on the problem of query answer classification: given a query and a set\nof possible answers, classify these proposals as true or false relative to the com-\nplete knowledge graph. Second, we study the problem of query answer retrieval:\ngiven a query, retrieve an answer to the query relative to the complete knowl-\nedge graph or decide that no correct solutions exist. Trained on simple, small\ninstances, ANYCQ can generalize to large queries of arbitrary structure, reliably\nclassifying and retrieving answers to samples where existing approaches fail,\nwhich is empirically validated on new and challenging benchmarks. Furthermore,\nwe demonstrate that our AnyCQ models effectively transfer to out-of-distribution\nknowledge graphs, when equipped with a relevant link predictor, highlighting\ntheir potential to serve as a general engine for query answering.", "sections": [{"title": "1 Introduction", "content": "Knowledge graphs (KGs) are an integral component of modern information management systems for\nstoring, processing, and managing data. Informally, a KG is a finite collection of facts representing\ndifferent relations between pairs of nodes, which is typically highly incomplete [1, 2]. Motivated by\nthe incompleteness of modern KGs, a new setup for classical query answering has emerged [3-9],\nwhere the goal is to predict answers that do not necessarily appear in the KG, but are potentially present\nin its completion. This task is commonly referred to as complex query answering in the existing\nliterature. Intuitively, this query answering setup accounts for the incompleteness of the KG and\nfollows a form of open-world assumption [10] which asserts that the facts that are not present\nin the observable KG cannot be deemed incorrect. This is a very challenging problem and goes\nbeyond the capabilities of classical query answering engines, which typically assume every fact\nmissing from the observable KG is incorrect, following a form of closed-world assumption [10].\nProblem setup. In this work, we deviate from existing approaches for complex query answering\nwhich rely on a ranking-based problem formulation and instead propose and study two query\nanswering problems based on classification. Our first task of interest, query answer classification\n(QAC), involves classifying solutions to queries over knowledge graphs, as true or false. The second\ntask of interest, query answer retrieval (QAR), requires predicting a correct answer to the provided\nquery or deciding that none exists."}, {"title": "2 Related work", "content": "Link prediction. Earlier models for link prediction on knowledge graphs, such as TransE [12],\nRotatE [13], ComplEx [14] and BoxE [15], focused on learning fixed embedding for seen entities\nand relations, thus confining themselves to transductive setting. Later, graph neural networks\n(GNNs) emerged as powerful architectures, with prominent examples including RGCNs [16] and\nCompGCNs [17]. These models adapt the message-passing paradigm to multi-relational graphs,\nthus enabling inductive link prediction on unseen entities. Building on this, [18] designed NBFNets\nwhich exhibited strong empirical performance via conditional message passing due to its enhanced\nexpressivity [19]. Recently, ULTRA [20] became one of the first foundation models on link prediction\nover both unseen entities and unseen relations.\nComplex query answering. Complex query answering (CQA) [3, 4] extends the task of link\nprediction to a broader scope of first-order formulas with one free variable, considering queries with\nconjunctions (\u2227), disjunctions (\u2228) and negations (\u00ac). Neuro-symbolic models decompose the CQA\ntask into a series of link prediction problems and employ fuzzy logic to aggregate these individual\nresults. CQD [11] applies knowledge graph embedding methods to CQA via beam search strategy.\nLater works, such as GNN-QE [5] or QTO [6] achieve superior performance by training directly over\nqueries, without pre-trained embedding models. FIT [7] extended the methodology introduced in\nQTO to queries containing cycles, for the cost of high complexity. Neural methods generally rely on\nneural networks to deduce relations and execute logical connectives simultaneously. LMPNN [8]\nemploys a novel logical message-passing scheme, leveraging existing KG embeddings to conduct\none-hop inferences on atomic formulas. Q2T [9] utilized the adjacency matrix of the query graph as\nan attention mask in Transformers [21] model. Although applicable to various query graph structures,\nneural CQA approaches tend to underperform, especially as the size of the query graph increases.\nCombinatorial reasoning. GNNs have emerged as a powerful tool for solving combinatorial\noptimization problems [22]. Their power to leverage the inherent structural information encoded in\ngraph representations of instances has been successfully utilized for solving various combinatorial\ntasks [23-26]. As a method of our particular interest, ANYCSP [27], introduced a novel form of\ncomputational graphs for representing arbitrary constraint satisfaction problems (CSP), demonstrating\nstate-of-the-art performance on MAX-CUT, MAX-k-SAT and k-COL.\nIn this work, we identify answering conjunctive queries as a CSP, tailoring the ANYCSP framework to\nsuit the task of deciding the satisfiability of Boolean formulas over incomplete KGs. Particularly, we\nintegrate link predictors into our architecture to account for the necessity of inferring relations missing\nin the observable data. We also devise new guidance mechanisms to navigate the search during the\nearly stages, targeting the large domain size. The augmented framework, named ANYCQ, inherits\nthe extrapolation and generalization strength of ANYCSP, resulting in an efficient and effective model\nfor the tasks of query answer classification and retrieval."}, {"title": "3 Preliminaries", "content": "Knowledge graphs. A knowledge graph (KG) is a set of facts over a relational vocabulary \u03c3, which\nis typically represented as a graph $G = (V(G), E(G), R(G))$, where V(G) is the set of nodes (or\nvertices), R(G) is the set of relation types, and $E(G) \u2286 R(G) \u00d7 V (G) \u00d7 V (G)$ is the set of relational\nedges (i.e., facts), denoted as $r(u, v) \u2208 E(G)$ with $r \u2208 R(G)$ and $u, v \u2208 V(G)$. We write $G |= r(a, b)$\nto mean $r(a, b) \u2208 E(G)$. We consider each given KG $G = (V(G), E(G), R(G))$ as an observable\npart of a complete graph $\\tilde{G} = (V(G), \\tilde{E}(G), R(G))$ that consists of all true facts between entities\nin V(G). Under this assumption, reasoning over the known facts E(G) is insufficient, requiring\ndeducing the missing facts from $\\tilde{E}(G)\\E(G)$. Note that this formulation follows the transductive\nscenario, in which G covers the same sets of entities and relation types as $\\tilde{G}$."}, {"title": "4 Query answering on incomplete knowledge graphs", "content": "Complex query answering (CQA) focuses on ranking all possible answers, which can be computation-\nally excessive and impractical, especially when the underlying KG is large. In practical applications,\nusers often ask questions of the form \u2018Is X true?' or 'What is the answer to X?', requiring models to\nclassify potential answers as true or false [28]. However, the ranking-based approach employed by\nexisting CQA models does not directly address this, as it merely orders possible answers without\nimposing a threshold to determine which of these answers, if any, should be considered correct.\nTo address these limitations, we propose two new query answering tasks designed to provide more\ntargeted responses while ensuring scalability for more complex logical queries."}, {"title": "5 ANYCQ: a GNN framework for query answering", "content": "To address these tasks, we propose a neuro-symbolic framework for scoring arbitrary existential\nBoolean formulas called ANYCQ. Let \u03c0 be a link predictor for an observable knowledge graph G.\nAn ANYCQ model \u0398 equipped with \u03c0 can be viewed as a function $\u0398(G, \u03c0) : CQ^\\circ (G) \u2192 [0,1]$\nwhere $CQ^\\circ (G)$ is the class of conjunctive Boolean queries over the same vocabulary as G. For input\n$Q = \u2203\u1ef9.\u03a6(\u1ef9)$, \u0398 returns an approximation $(Q|G, \u03c0)$ of $S_{\u03c0,G}(Q)$, searching for the assignment:\n$\\tilde{\\alpha} = \\underset{\\alpha:\u1ef9\u2192V(G)}{\\operatorname{argmax}} S_{\u03c0,G}(\u03a6(\\alpha(\u1ef9)/\u1ef9)).$\nNote that unfolding the definition of the Boolean formula score yields:\n$S_{\u03c0,G}(Q) = S_{\u03c0,G}(\u2203\u1ef9.\u03a6(\u1ef9)) = \\underset{\\alpha:\u1ef9\u2192V(G)}{\\operatorname{max}} S_{\u03c0,G}(\u03a6(\\alpha(\u1ef9)/\u1ef9)) = S_{\u03c0,G}(\u03a6(\\tilde{\\alpha}(\u1ef9)/\u1ef9)).$\nHence, leveraging the strength of GNNs to find optimal solutions to combinatorial optimization\nproblems, we can recover reasonable candidates for $\\tilde{\\alpha}$, accurately estimating $S_{\u03c0,G}(Q)$."}, {"title": "5.1 Query representation", "content": "We transform the input queries into a computational graph (Figure 2), whose structure is adopted\nfrom ANYCSP [27]. Consider a conjunctive Boolean query $Q = \u2203\u1ef9.\u03a6(\u1ef9)$ over a knowledge graph G,\nwith \u03a6 quantifier-free, and let \u03c0 be a link predictor for G. Let $c_1,..., c_n$ be constant symbols\nmentioned in \u03a6, and $\u03c8_1, ..., \u03c8_\u03b9$ be the literals in \u03a6. We define the domain D(e) of the term e as\n$D(y) = V(G)$ for each existentially quantified variable y and $D(c_i) = {c_i}$ for each constant $c_i$.\nThe undirected computational graph $G_q = (V_Q, E_Q)$ is then constructed as follows:\nVertices. The vertices of $G_q$ are divided into three groups. Firstly, the entity nodes, $v_{y_1},..., v_{y_k}$ and\n$v_{c_1}, ..., v_{c_n}$, represent variables and constants mentioned in \u03a6. Secondly, value vertices correspond to\nfeasible entity-value pairs. Formally, for each term e mentioned in I and any value $a \u2208 D(e)$, there\nexists a value vertex $v_{e,a}$. Finally, literal nodes $v_{\u03c8_1},..., v_{\u03c8_\u03b9}$ represent literals $\u03c8_1, ..., \u03c8_\u03b9$ of \u03a6.\nEdges. We distinguish two types of edges in $G_Q$. The entity-value edges connect entity with value\nnodes: for any entity vertex $v_e$ representing term e and any $a \u2208 D(e)$, there exists an edge ${v_e, v_{e,a}}$.\nAdditionally, value-literal edges are introduced to propagate information within literals. If a term e is\nmentioned by the literal $V_i$, then for all $a \u2208 D(e)$ there exists an edge between $v_{\u03c8_i}$ and $v_{e,a}$\nEdge labels. Each value-literal edge ${v_{\u03c8_i}, v_{e,a}}$ is assigned with two labels: light and potential.\nThe potential edge (PE) label $PE(v_{\u03c8_i}, v_{e,a})$ is meant to represent the ability to satisfy $V_i$ under\nthe substitution e \u2192 a. Given $\u03c8_i = r(x,y)$, $PE(v_{\u03c8_i}, v_{x,a})$ is supposed to represent whether\n$G|= \u2203y.r(a, y)$. Similarly, $PE(v_{\u03c8_i}, v_{y,b})$ is meant to denote if $G |= \u2203x.r(x,b)$. We pre-compute\nthe PE labels using the equipped link predictor \u03c0, binarizing the corresponding Boolean formula\nscores $S_{\u03c0,G}(\u2203y.r(a,y))$, $S_{\u03c0,G}(\u2203x.r(x,b))$ with the threshold of 0.5. We set to 1 all PE labels\ncorresponding to constant symbols mentioned in $V_i$, assuming $\u03c8_i$ can be anyhow satisfied.\nAt the beginning of search step $t \u2264 T$, the light edge (LE) labels are calculated to indicate which\nmarginal changes to the current assignment $\u03b1^{(t-1)}$ satisfy the corresponding literals. For any"}, {"title": "5.2 ANYCQ framework", "content": "Consider an ANYCQ model \u0398 equipped with a link predictor \u03c0 for an incomplete knowledge graph G.\nGiven an input conjunctive Boolean query $Q = \u2203\u1ef9.\u03a6(\u1ef9)$ over G, we first construct its computational\ngraph $G_0$ (Section 5.1). \u0398 is parameterized by a GNN \u03b8 that iteratively processes $G_0$, updating its\nedge labels and hidden embeddings (Figure 3).\nBefore the search commences, the hidden embeddings $h^{(0)}$ of all value nodes are set to a pre-trained\nvector $h \u2208 R^d$ and an initial assignment $\u03b1^{(0)}$ is drafted, independently sampling the value for each\nvariable $y \u2208 {\u1ef9}$ uniformly at random from D(y). The variable and literal nodes are not assigned\nany hidden embeddings, serving as intermediate steps for value node embedding updates. At the\nbeginning of search step t, ANYCQ updates the structure of $G_Q$ based on $h^{(t-1)}$ and $\\varepsilon^{(t-1)}$ (used\nto derive new LE labels). Then, $G_Q$ is processed with the GNN \u03b8, which generates new value node\nembeddings $h^{(t)}$, and for each variable $y \u2208 \u1ef9$ returns a distribution $\u03bc^{(t)}$ over D(y). Finally, the next\nassignment $\u03b1^{(t)}$ is sampled by drawing the value $\u03b1^{(t)} (y)$ from $\u03bc^{(t)}$, independently for each $y \u2208 {\u1ef9}.\nA precise description of the architecture of \u03b8 is provided in Appendix A.1. The search terminates\nafter T steps, and the generated assignments $\u03b1^{(0)}, \u03b1^{(1)}, ..., \u03b1^{(T)}$ are used to approximate $S_{\u03c0,G}(Q)$:\n$\u0398(Q|G, \u03c0) = \\underset{0<t\u2264T}{\\operatorname{max}} S_{\u03c0,G} (\u03a6 (\u03b1^{(t)} (\u1ef9)/\u1ef9)).$\nTraining and methodology. During training on each dataset, we equip the ANYCQ model with\na fixed, pre-trained ComplEx-based predictor $\u03c0_{train}$, described in detail in Appendix E. Thus, the only\ntrainable component of \u0398 remains the GNN \u03b8. We utilize the training splits from the existing\nCQA datasets [4], consisting of formulas mentioning at most three variables. Moreover, we restrict\nthe number of search steps T to at most 15, encouraging the network to quickly learn to apply logical\nprinciples locally. Inspired by prior work on combinatorial optimization [27, 29, 30], we train \u03b8"}, {"title": "5.3 Theoretical properties", "content": "ANYCQ is complete given sufficiently many search steps, i.e., any ANYCQ model equipped with\nany link predictor will eventually return the corresponding Boolean formula score:\nTheorem 5.1. (Completeness) Let $Q = \u2203\u1ef9.\u03a6(\u1ef9)$ be a conjunctive Boolean query and let \u0398 be any\nANYCQ model equipped with a predictor \u03c0. For any execution of \u0398 on Q, running for T steps:\n$P (\u0398(Q|G, \u03c0) = S_{\u03c0,G}(Q)) \u2192 1$ as\n$T\u2192\u221e$\nWhen ANYCQ is equipped with the perfect link predictor for G, we can guarantee the soundness of\npredictions, i.e., all positive answers will be indeed correct (see Appendix D for detailed proofs):\nTheorem 5.2. (Soundness) Let $Q = \u2203\u1ef9.\u03a6(\u1ef9)$ be a conjunctive Boolean query over an unobservable\nknowledge graph $\\tilde{G}$ and let \u0398 be any ANYCQ model equipped with a perfect link predictor $\\tilde{\u03c0}$ for G.\nIf $\u0398(Q|G, \\tilde{\u03c0}) > 0.5$, then $\\tilde{G} |= Q$ ."}, {"title": "6 Experimental evaluation", "content": "We empirically evaluate ANYCQ on the tasks of QAC (Section 6.2) and QAR (Section 6.3). We also\nconduct two ablation studies (Section 6.4): first, we assess ANYCQ with a perfect link predictor to\nisolate and measure the quality of the search engine independent of predictor's imperfections; second,\nwe examine the generalizability of ANYCQ by applying it to out-of-distribution KGs."}, {"title": "6.1 Experimental setup", "content": "Benchmarks and datasets. Existing benchmarks [4, 32] comprise formulas with simple structures,\nthereby impeding the comprehensive evaluation and advancement of novel methodologies. We address\nthis gap by creating new datasets on top of well-established benchmarks, consisting of queries with\ndemanding structural complexity. Specifically, we generate formulas mentioning up to 20 distinct\nterms, allowing the presence of multiple cycles, long-distance reasoning steps, and multi-way\nconjunctions. The generation process is described in Appendix B. Building on the existing CQA\ndatasets, we propose two novel benchmarks for QAC: FB15k-237-QAC and NELL-QAC, each\ndivided into 9 splits, consisting of small and large formulas. For the task of QAR, we note that most\nof the simple instances inherited from CQA benchmarks admit easy answers, which, combined with\ntheir simple structure, makes them trivial for the QAR objective. We hence develop new benchmarks,\nconsisting of large formulas with up to 3 free variables : FB15k-237-QAR and NELL-QAR.\nBaselines. As the baselines for the QAC task, we choose the state-of-the-art solutions from CQA\ncapable of handling the classification objective: QTO [6] and FIT [7]. Since the performance\nof neuro-symbolic models heavily relies on the backbone link predictor quality, we use the same\nComplEx [14] model for QTO, FIT and ANYCQ architectures. However, neither QTO nor FIT\ncan process cyclic and structurally complex formulas included in our large query splits. Hence,"}, {"title": "6.2 Query answer classification experiments", "content": "The results of evaluation on the introduced QAC benchmarks are presented in Table 1. The F1 scores\non the simple query splits suggest that FIT generally outperforms QTO's methodology. Note that\nboth FIT and QTO are combinatorial optimizers, which precisely evaluate the Boolean formula.\nANYCQ, on the other hand, only provides an approximation to this objective. Therefore, our model's\nperformance is upper-bounded by these two CQA methods. Nevertheless, ANYCQ matches their\nperformance, achieving only marginally (within 2% relative) lower scores. Importantly, ANYCQ\nsuccessfully extrapolates to formulas beyond the processing power of the existing CQA approaches.\nOn all proposed large query splits ANYCQ consistently outperforms the SQL baseline, proving its\neffectiveness: SQL classifies only easy answers accurately, and as a result falls behind ANYCQ."}, {"title": "6.3 Query answer retrieval experiments", "content": "We present the QAR evaluation results across all splits of the two proposed datasets consisting of\nlarge formulas with multiple free variables in Table 2. Compared with the SQL engine which can only\nextract easy answers, ANYCQ can reliably achieve similar performance on easy answers while also"}, {"title": "6.4 Ablation studies", "content": "How do ANYCQ models perform outside of their training domain? As mentioned in Section 5.3,\nwe can expect the search engine to exhibit similar behavior on processed instances, regardless of\nthe underlying knowledge graph. We validate this claim by applying ANYCQ models trained on\nFB15k-237 or on NELL to both datasets, equipping a relevant link predictor. The results on our QAR\nbenchmarks are presented in Table 3. Notably, the differences between models' accuracies in QAR are\nmarginal, confirming that the resulting search engine is versatile and not strongly dataset-dependent.\nHow do predictions look like relative to a perfect link predictor? The ANYCQ framework's\nperformance heavily depends on the underlying link prediction model, responsible for guiding the\nsearch and determining the satisfiability of generated assignments. Hence, to assess purely the quality\nof our search engines, we equipped them with perfect link predictors for the test KGs, eliminating the\nimpact of predictors' imperfections. The results of experiments on our QAC and QAR benchmarks\nare available in Table 4 and Table 3, respectively. Remarkably, the simple query types in QAC pose\nno challenge for ANYCQ, which achieves 100% F1-score on all of them. Furthermore, on large\nformula splits, the F1-score remains over 90%, displaying the accuracy of our search framework.\nSimilarly, for the task of QAR, ANYCQ with a perfect link predictor achieved over 90% F1-score,\nestablishing the engine's excellent ability to retrieve answers to structurally complex questions."}, {"title": "7 Summary, limitations, and outlook", "content": "In this work, we devise and study two new tasks from the query answering domain: query answer\nclassification and query answer retrieval. Our formulations target the challenge of classifying and\ngenerating answers to structurally complex formulas with an arbitrary number of free variables.\nMoreover, we introduce datasets consisting of instances beyond the processing capabilities of existing\napproaches, creating strong benchmarks for years to come. To address this demanding setting, we in-\ntroduce ANYCQ, a framework applicable for scoring and generating answers for large conjunctive\nformulas with arbitrary arity over incomplete knowledge graphs. We demonstrate the effectiveness\nover our QAC and QAR benchmarks, showing that on simple samples, ANYCQ matches the perfor-\nmance of state-of-the-art CQA models, while setting challenging baselines for the large instance splits.\nOne potential limitation is that ANYCQ requires the input query to be in either conjunctive normal\nform or disjunctive normal form, converting to which may require exponentially many operations.\nWe hope our work will motivate the field of query answering to recognize the classification nature of\nthe induced tasks and expand the scope of CQA to previously intractable cases."}, {"title": "A ANYCQ details", "content": "A.1 Architecture\nANYCQ's architecture is based on the original ANYCSP [27] framework. The trainable components\nof the ANYCQ GNN model \u03b8 are:\n\u2022 a GRU [34] cell $G : R^d \u00d7 R^d \u2192 R^d$ with a trainable initial state $h \u2208 R^d$\n\u2022 a Multi Layer Perceptron (MLP) value encoder $E : R^{d+1} \u2192 R^d$\n\u2022 two MLPs $M_y, M_R : R^d \u2192 R^{4d}$ sending information between value and literal vertices\n\u2022 three MLPs $U_V, U_R, U_X : R^d \u2192 R^d$ aggregating value, literal and variable messages\n\u2022 an MLP $O : R^d \u2192 R$ that generates logit scores for all variable nodes.\nWe denote the set of neighbors of entity and literal nodes by $N(\u00b7)$. In the case of value nodes,\nwe distinguish between the corresponding entity node and the set of connected literal vertices, which\nwe represent by $N_R(\u00b7)$.\nThe model starts by sampling an initial assignment $\u03b1^{(0)}$, where the value of each variable is chosen\nuniformly at random from V(G), and proceeds for T search steps. In step t:\n\u2022 If t = 1, initialize the hidden state of each value node to be $h^{(0)} (v_{z,a}) = h$.\n\u2022 Generate light edge labels under the assignment $\u03b1^{(t-1)}$ for all value-literal edges. Precisely, let\n$v_{\u03c8_i}$ be a literal node corresponding to an atomic formula \u03c8 and $v_{z,a}$ be a connected value node.\nThe light edge label $L_E^{(t-1)} (v_{\u03c8_i}, v_{z,a})$ is a binary answer to the question: \u201cIs \u03c8 satisfied under\n$[\u03b1^{(t-1)}]_z \u2192 a$?\u201d with respect to the equipped predictor.\n\u2022 For each value node $v_{z,a}$, generate its new latent state\n$x^{(t)} (v_{z,a}) = E ([h^{(t-1)} (v_{z,a}), \u03b4_{\u03b1(x)=v}])$\nwhere [,] denotes concatenation and $\u03b4_C = 1$ if the condition C holds, and 0 otherwise.\n\u2022 Derive the messages to be sent to the constraint nodes:\n$m^{(t)} (v_{z,a}, 0), ..., m^{(t)} (v_{z,a}, 3) = M_y (x^{(t)} (v_{z,a}))$\n\u2022 For each literal node $v_{\u03c8_i}$, gather the messages from its value neighbors, considering the light and\npotential labels:\n$y^{(t)} (v_{\u03c8}) = \\bigoplus_{v_{\u03c0,\u03b1} \u2208 N(v_{\u03c8})} m^{(t)} (v_{z,a}, 2\u00b7 PE(v_{\u03c8}, v_{z,a}) + L_E^{(t-1)} (v_{\u03c8}, v_{\u03c0,\u03b1}))$\nwhere \u2295 denotes element-wise max.\n\u2022 The messages to be sent to the value nodes are then evaluated as:\n$m^{(t)} (v_{\u03c8}, 0), ..., m^{(t)} (v_{\u03c8}, 3) = M_R(y^{(t)} (v_{\u03c8}))$\n\u2022 Aggregate the messages in each value node $v_{z,a}$:\n$y^{(t)} (v_{z,a}) = \\bigoplus_{v_{\u03c8}\u2208N_R(v_{z,\u03b1})} m^{(t)} (v_{z,a}, 2\u00b7 PE (v_{\u03c8}, v_{z,a}) + L_E^{(t-1)} (v_{\u03c8}, v_{z,a}))$\nand integrate them with current hidden state:\n$z^{(t)} (v_{z,a}) = U_V (x^{(t)} (v_{z,a}) + y^{(t)} (v_{z,a})) + x^{(t)} (v_{z,a})$\n\u2022 For each entity node $v_z$, aggregate the states of the corresponding value nodes:\n$z^{(t)} (v_z) = U_x \\bigoplus_{v_{z,a} \u2208 N (v_z)} z^{(t)} (v_{z,a})$"}, {"title": "A.2 Training methodology", "content": "\u2022 For each value node $v_{z,a}$, update its hidden state as:\n$h^{(t)} (v_{z,a}) = G (h^{(t-1)} (v_{z,a}), z^{(t)} (v_{z,a}) + z^{(t} (v_z))$\n\u2022 Generate logits and apply softmax within each domain:\n$o_{z,a}^{(t)} = clip (O (h^{(t)} (vz,a)) - \\underset{a\u2208D(z)}{\\operatorname{max}} O (h^{(t)} (vz,a), [-100,0])$\n$\u03bc^{(t)} (v_{z,a}) = \\frac{exp o_{z,a}^{(t)}}{\\sum_{\u03b1'\u2208D(z)} exp o_{z,\u03b1'}^{(t)}}$\n\u2022 Sample the next assignment $\u03b1^{(t)}$, selecting the next value independently for each variable, with\nprobabilities $P (\u03b1^{(t)}(x) = a) = \u03bc^{(t)} (v_{x,a})$ for all $a \u2208 D(x)$.\nNote that the suggested methodology for evaluating probabilities $P (\u03b1^{(t)}(x) = a)$ is approximately\nequivalent to applying softmax directly on $O (h^{(t)} (v_{x,a}))$. However, applying this augmentation, we\nare guaranteed that for any variable x and a relevant value $a \u2208 D(x)$:\n$P (\u03b1^{(t)}(x) = a) = \\frac{e^{-100}}{\\sum_{\u03b1'\u2208D(x)} e^{o_{x,\u03b1'}}}> \\frac{1}{|D(x)|} = \\frac{e^{-100}}{e^{100}|V(G)|}$\nSuppose we are given a training query $Q(x) = \u2203\u1ef9.\u03a6(x, \u1ef9)$. We run \u0398 on \u2203x.Q(x) for $T_{train}$ search\nsteps, recovering the assignments $\u03b1^{(0)}, ..., \u03b1^{(Ttrain)}$ and the intermediate value probability distributions:\n$\u03bc^{(1)} = {\u03bc_z^{(1)} |z \u2208 {x, \u1ef9}}, ..., \u03bc^{(Ttrain)} = {\u03bc_z^{(Ttrain)} |z \u2208 {x, \u1ef9}}$\nThe reward $R^{(t)}$ for step $1 < t < T$ is calculated as the difference between the score for assignment\n\u03b1(t) and the best assignment visited so far:\n$R^{(t)} = max (0, S^{(t)} - \\underset{t'<t}{\\operatorname{max}} S^{(t')})$\nwhere $S'^{(t)} = S_{train} (\u03a6(\u03b1^{(t)} (x)/x, \u03b1^{(t)} (\u1ef9)/\u1ef9))$. Additionally, the transition probability\n$p^{(t)} = P(\u03b1^{(t)}|\u03bc^{(t)}) = \\prod_{z\u2208{x,\u1ef9}} \u03bc_z^{(t)} (\u03b1^{(t)}(z))$\nrepresents the chance of drawing assignment $\u03b1^{(t)}$ at step t, given distributions {$\u03bc_z |z \u2208 {x, \u1ef9}}.\nThe corresponding REINFORCE's training loss is evaluated as a weighted sum of rewards generated\nduring $T_{train}$ search steps and the model weights are then updated using the gradient descend equation:\n$\\theta \u2190 \u03b8 - \u03b1 \u00b7 \\nabla_\u03b8 \\sum_{i=1}^{T_{train}} (log p(\u03b1^{(t)})) \\sum_{i=i+1}^{Ttrain} (\u03b3^{t-i-1} R^{(t)})$\nwhere $\u03b3 \u2208 (0, 1]$ is a discount factor and $\u03b1 \u2208 R$ is the learning rate.\nFor the training data, we use the training splits of the existing FB15k-237 and NELL CQA datasets\n[4], consisting of queries of types: '1p', '2p', '3p', '2i', '3i', \u20182in', '3in', 'pin', 'inp' (see Table 10\nfor the corresponding first-order logic formulas). Hence, during training, ANYCQ witnesses queries\nwith projections, intersections and negations, learning principles of this logical structures. However,\nall of these queries mention at most 3 free variables, remaining limited in size."}, {"title": "A.3 Hyperparameters and implementation", "content": "Architecture. We choose the hidden embedding size $d = 128$ in the ANYCQ architecture for all\nexperiments. All MLPs used in our model consist of two fully connected layers with ReLU [35]\nactivation function. The intermediate dimension of the hidden layer is chosen to be 128.\nTraining. The REINFORCE [31] discount factor A is set to 0.75 for both datasets, following the\nbest configurations in ANYCSP experiments. During training, we run our models for $T_{train} = 15$\nsteps. The batch size is set to 4 for FB15k-237 and 1 for NELL, due to the GPU memory constraints.\nAll models are trained with an Adam [36] optimizer with learning rate 5. 10-6 on a single NVIDIA\nTesla V100 SXM2 with 32GB VRAM. We let the training run for 4 days, which translates to 500,000\nbatches on FB15k-237 and 200,000 batches for NELL, and choose the final model for testing.\nInference. To run all experiments, we use an Intel Xenon Gold 6326 processor with 128GB RAM.\nFor ANYCQ evaluation, we additionally use an NVIDIA A10 graphics card with 24GB VRAM."}, {"title": "A.4 Scope of formulas", "content": "Importantly", "Then": "n$\u03c8 = (\u2203\u1ef9.C\u2081) \u2228 ... \u2228 (\u2203\u1ef9.C_n)$\nwhich can be processed by ANYCQ by independently solving each (\u2203\u1ef9.C\u1d62) and aggregating the re-\nsults. Moreover, the ability of our model to handle higher arity relations enables efficient satisfiability\nevaluation for existential formulas in the conjunctive normal form. Let $\u03c8 = \u2203\u1ef9. (D\u2081 \u2227 ... \u2227 D_n)$\nwhere each $D_i$ is a disjunction of literals. Consider $D_i = l_{i,1} \u2228 ... \u2228 l_{i,m}$ and let $z_i = Var(D_i)$.\nWe can view the disjunctive clause $D_i$ as a single relation $D_i(z_i)$ evaluating to\n$S_{\u03c0,G}(D_i(\u03b1(z_i)/z_i)) = \\underset{j}{\\operatorname{max}} S_{\u03c0,G}(l_{i,j}(\u03b1(Var(l_{i,j}))/Var(l_{i,j})))$\nUnder this transformation, $\u03c8 = \u2203\u1ef9. (D\u2081(z\u2081) \u2227 ... D_n(z_n))$ becomes a conjunctive query, hence\nprocessable by ANYCQ. Up to our knowledge, we present the first query answering approach\nefficiently scoring arbitrary CNF Boolean queries over incomplete knowledge"}]}