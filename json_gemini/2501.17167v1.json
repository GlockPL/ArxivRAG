{"title": "QualityFlow: An Agentic Workflow for Program Synthesis Controlled by LLM Quality Checks", "authors": ["YAOJIE HU", "QIANG ZHOU", "QIHONG CHEN", "XIAOPENG LI", "LINBO LIU", "DEJIAO ZHANG", "AMIT KACHROO", "TALHA OZ", "OMER TRIPP"], "abstract": "We introduce QualityFlow, a dynamic agentic workflow for program synthesis. Given the English description\nof a programming problem and a set of unit tests, the model's goal is to synthesize the correct program that\nsolves the problem and passes the tests. QualityFlow consists of multiple large language model (LLM) agents\nthat resemble a software development team, including code generation, testing, and self-debugging. Existing\nprogram synthesis methods face three major limitations: assumption of visible unit test conformity, bottleneck\nof synthesized test quality, and deviation of self-debugging trajectory. To address them, we propose the LLM\nQuality Checker, which explicitly \u201cimagines\u201d whether the synthesized programs' execution would conform to\nthe unit tests. The Quality Checks dynamically control the workflow, including actions to submit the final\nanswer, clarify the problem statement, and revert previous workflow steps. As a result, our Quality Checker\ncan precisely accept any correct program, mitigate faulty synthesized tests, and prevent potential workflow\ndeviation. The success of the Quality Checker further enables Diversified Prompting, which encourages\nvariations in LLM responses to maximize the possibility that a correct program appears and passes the\nquality check. In experiments, QualityFlow establishes the state-of-the-art results on four program synthesis\nbenchmarks: MBPP, HumanEval, and the stricter evaluations of both MBPP and HumanEval from EvalPlus.\nOur systematic analysis shows that the dynamic workflow controlled by LLM quality checks can outperform\nstatic workflows and single-attempt zero-shot synthesis. The Quality Checker is the center of our investigation,\nand we dissect its individual performance and integrated impact on the workflow accuracy, as well as other\nablations experiments to justify our workflow design.", "sections": [{"title": "1 Introduction", "content": "Program synthesis is a long-standing goal of software engineering research [38, 45, 52], with a\nhistory dating back to the 1940s and 50s [3]. In recent years, marked by artificial intelligence and\nlarge language model (LLM) approaches [11, 12, 27, 41, 48, 57], program synthesis has moved from\nresearch to real-world applications that streamline software development and enhance programmer\nproductivity [35]. We study a standard program synthesis setting [2, 4, 18]: given the natural\nlanguage description (documentation) together with some visible unit tests, the model's goal is\nto generate a program that satisfies the description and passes a set of evaluation tests (possibly\nthe same as the visible tests) (Figure 1). This setting encapsulates the typical software engineering\nprocess. The most competitive program synthesis benchmarks today (e.g. MBPP, HumanEval\n[2, 4, 18]) follow this setting and are incorporated into the standard evaluation suites of almost all\nLLMs (Table 3), showing the recognition and interests for this problem from both industry and\nacademia. Like many other generative AI domains, LLMs had dominated other approaches on the\nprogram synthesis benchmarks, but the improvements were largely due to scaling up the model\nsize at a prohibitive cost of millions of dollars of computation [29, 58]. With new performance\ngains and no additional training costs, Agentic Workflow has recently been proposed as a way to\norchestrate multiple LLM agents that form a team to self-reflect [51], debate [30], and solve the\nproblem together in a collaborative style that can outperform zero-shot LLM program synthesis [19].\nAgentic Workflows provide fertile opportunities where we can break out of the dependence on\nthe LLM scaling law for performance improvements and design agents specifically for program\nsynthesis by incorporating ongoing software engineering research, such as self-debugging [7, 24]\nand automatic test generation [9, 17].\nIn order to design an Agentic Workflow for program synthesis, existing zero-shot generation\nand self-debugging methods have important limitations to be resolved:\n\u2022 Assumption of visible unit test conformity. Current zero-shot generation methods only\nimplicitly assume that the generated code will follow the visible unit tests required by the\nproblem statement (Figure 1). Self-debugging on the visible tests can violate the benchmark\nrules by leaking the ground truth (e.g., MBPP), and no methods currently exist to explicitly\nexamine visible unit test conformity through an opportunity for the LLM to reflect on\nwhether the code will pass the tests.\n\u2022 Bottleneck of synthesized test quality. Synthesized tests could be executed and any error\ncould provide feedback for self-debugging, but incorrect tests may be misleading. Clearly,\nusing incorrect tests to debug correct programs may raise unwarranted errors, and the\nself-debugger may turn a good program into a flawed one merely to satisfy the faulty tests\nor an instruction to indiscriminately debug the program.\n\u2022 Deviation of self-debugging trajectory. Repeated self-debugging steps should ideally form a\npositive trajectory that leads to a correct program in the end. However, if the trajectory\ndeviates in the recurrent process, it could become stuck in a loop, reach a fixed point,\nor simply degrade with more bugs and mistakes. Existing multi-agent methods are static\nworkflows and fail to explore flexible control flows during code generation [24]: they lack an\neffective reverting or restarting mechanism to rectify any deviated self-debugging trajectory.\nIn this paper, we aim to address these limitations and introduce QualityFlow, an Agentic Workflow\nfor program synthesis that resembles a software engineering team with quality assurance at every\nlevel. QualityFlow includes a Code Generator agent that drafts the program, a Test Designer agent"}, {"title": "2 Methods", "content": "2.1 QualityFlow Overview\nQualityFlow is illustrated in Figure 2, and a pseudo-code is provided in Algorithm 1. The Program\nGenerator agent (line 4 of Alg. 1) synthesizes an initial program based on the problem statement\nand the visible tests (example in Figure 1), and the generated program is fed into the Code Quality\nChecker agent to predict whether the program is correct (line 5). If the quality check passes, the\nprogram is submitted; otherwise, the workflow directs to the Test Designer agent that synthesizes\na number of unit tests (line 9). As an optional component of QualityFlow, the Test Quality Checker\nagent predicts the correctness of the synthesized tests and filter out the wrong tests (line 11). Given\nthe (filtered) synthesized tests, the Self-Debugger agent revises the code over multiple epochs (line\n16). At every epoch, the revised program is fed into the Code Quality Checker agent (line 17). If any\nprogram passes the quality check, it is submitted directly. Otherwise, the Quality Checker rejects all\nintermediate programs, and it is likely that the initial understanding of the problem statement was\nflawed [39", "reset": "o re-interpret the problem from a different viewpoint,\nand the Problem Clarifier agent will let the Code Generator re-synthesize a program based on a new\nunderstanding (line 22 and 23). The final quality check decides whether to submit the re-synthesized\nprogram from the Clarifier or the initially synthesized program from the Code Generator, which is\nequivalent to a revert mechanism to prevent a completely deviated workflow trajectory (line 28).\nOverall, QualityFlow performs quality checks at every level in order to navigate the workflow,\nsubmit any correct intermediate synthesis, and reset potential trajectory deviations.\n2.2 Program Generator\nThe Program Generator agent starts the workflow (line 4). The LLM generates a program based on\nthe problem statement provided, along with a set of visible unit tests that the generated program\nshould pass. At this stage, no additional contextual information is available; the only input is the\noriginal problem statement itself.\n2.3 Test Designer\nThe Test Designer agent (line 9 of Alg. 1) synthesizes test cases for the Self-Debugger. Following\nthe typical approach [7, 23", "24": ".", "59": "style reflection, which asks\nthe LLM to analyze step-by-step to reason about why the program fails the test cases and then to\noffer revisions accordingly, instead of producing a new program directly. This iterative process\nof analyzing failures, reasoning, and revising the program continues until all tests are passed\nor the maximum number of epochs is reached. The Code Quality Checker inspects the revised\nprogram at every epoch, and if the quality check passes, the program is submitted as the final\noutput and the entire workflow terminates (line 19 of Alg. 1), which prevents potential trajectory\ndeviation where self-debugging alters a correctly synthesized code later on. Our Self-Debugger\nsystematically addresses the failed test cases and iteratively improves the program, steering the\nLLMs to understand the problem context and reason about the program's behavior, leading to more\nlogical and coherent revisions based on execution feedback.\n2.5 Quality Checker\nThe (Code) Quality Checker is a controller agent in QualityFlow that examines the quality of\nthe intermediate synthesized programs and use the information to navigate the workflow (line\n5, 17, and 24 of Alg. 1). The Quality Checker makes critical contributions to the state-of-the-art\nperformance of QualityFlow (e.g. on MBPP, 14% higher pass@1 accuracy, Figure 3).\nQuality Checker uses Imagined Execution to emulate the execution of tests on the synthesized\nprogram. For some benchmarks where the visible tests in the problem statement are the same as\nthe evaluation tests (e.g. MBPP), directly executing the code on these tests causes label leakage and\nimplicitly changes the pass@1 metric to pass@k, where k is the number of times the evaluation\ntests are run. Imagined Execution is an innovative approach that emulates the program's execution\non test cases without actually running the tests, which allows quality checks under such setting.\nFor datasets with separate visible and evaluation tests (e.g. HumanEval), the Quality Checker may\nrun the visible tests directly with a Python interpreter instead of Imagined Execution. We conduct\nexperiments in both settings, and Imagined Execution is on par with Python execution in overall\nworkflow pass@1 accuracy (HumanEval, Table 2).\nImagined Execution. In Imagined Execution, the large language model performs Chain-of-Thought\nreasoning [59"}]}