{"title": "LMAct: A Benchmark for In-Context Imitation Learning with Long Multimodal Demonstrations", "authors": ["Anian Ruoss", "Fabio Pardo", "Harris Chan", "Bonnie Li", "Volodymyr Mnih", "Tim Genewein"], "abstract": "Today's largest foundation models have increasingly general capabilities, yet when used as agents, they often struggle with simple reasoning and decision-making tasks, even though they possess good factual knowledge of the task and how to solve it. In this paper, we present a benchmark to pressure-test these models' multimodal decision-making capabilities in the very long-context regime (up to one million tokens) and investigate whether they can learn from a large number of expert demonstrations in their context. We evaluate a wide range of state-of-the-art frontier models as policies across a battery of simple interactive decision-making tasks: playing tic-tac-toe, chess, and Atari, navigating grid worlds, solving crosswords, and controlling a simulated cheetah. We measure the performance of Claude 3.5 Sonnet, Gemini 1.5 Flash, Gemini 1.5 Pro, GPT-40, 01-mini, and o1-preview under increasing amounts of expert demonstrations in the context \u2014 from no demonstrations up to 512 full episodes, pushing these models' multimodal long-context reasoning capabilities to their limits. Across our tasks, today's frontier models rarely manage to fully reach expert performance, showcasing the difficulty of our benchmark. Presenting more demonstrations often has little effect, but some models steadily improve with more demonstrations on a few tasks. We investigate the effect of encoding observations as text or images and the impact of chain-of-thought prompting. Overall, our results suggest that even today's most capable models often struggle to imitate desired behavior by generalizing purely from in-context demonstrations. To help quantify the impact of other approaches and future innovations aiming to tackle this problem, we open source our benchmark that covers the zero-, few-, and many-shot regimes in a unified evaluation.", "sections": [{"title": "1. Introduction", "content": "The simple recipe of minimizing next-token prediction error at scale has led to large multimodal foundation models (LMs) with remarkably general capabilities (Anil et al., 2023; Anthropic, 2024a; OpenAI, 2023). Importantly, these capabilities come in two flavors: (i) the ability to produce outputs of high quality from a short and often underspecified prompt (e.g., writing an essay about a novel topic), and (ii) the ability to learn new patterns and imitate algorithms in context (Mirchandani et al., 2023; Reid et al., 2024). Both types of capabilities demonstrate that LMs are not simply large \"storage devices\", but that they can manipulate learned knowledge and respond to new information in flexible and non-trivial ways. These capabilities, which are necessary for reasoning and decision-making, have led to the recent surge of using large foundation models as agents by sampling an action from the model's predicted continuation of the context (Mirchandani et al., 2023; Palo and Johns, 2024).\nWhile in some tasks LMs have been shown to be able to perform non-trivial reasoning and decision-making (Huang et al., 2022; Romera-Paredes et al., 2024; Yao et al., 2023), there are also many negative results where LMs fail to perform decision-making tasks that are very simple for humans. For example, LMs may struggle to play legal moves, let alone beat amateur humans, in chess (Carlini, 2023). At the same time, state-of-the-art LMs posses detailed expert knowledge of chess when queried in natural language, but that declarative knowledge fails to translate into effective decision-making (\"know-how\", Ryle (1949)). The recently released BALROG benchmark paper (Paglieri et al., 2024) also argues that LMs have a \u201cknowing-doing gap\u201d. BALROG evaluates the zero-shot capabilities (i.e.,"}, {"title": "Main Contributions", "content": "Figure 1 provides an overview of our benchmark tasks and methodology. Note that we chose relatively simple variants of our tasks (e.g., playing tic-tac-toe against an opponent that"}, {"title": "2. Methods", "content": "In this section, we describe our experimental setup, including the models we evaluate (Section 2.1), our benchmark environments (Section 2.2), and how the prompt is constructed (Section 2.3)."}, {"title": "2.1. Models", "content": "We evaluate the current closed-weights frontier LMs: Claude 3.5 Sonnet (the 2024-10-22 version) (An-thropic, 2024b,c), Gemini 1.5 Flash and Gemini 1.5 Pro (the 002 versions) (Reid et al., 2024), GPT-40 (the 2024-08-06 version) (OpenAI, 2024a), and o1-mini and o1-preview (OpenAI, 2024c). Apart from 01-mini and o1-preview, all of these models are capable of processing multimodal prompts, though their exact specifications differ (see below). We use temperature 0 for all models (except for 01-mini and o1-preview which have a fixed temperature of 1 (OpenAI, 2024d)). We set the maximum sample length to 2048 tokens for all models, except for o1-mini and o1-preview, which is more than sufficient to achieve strong performance (see our ablation in Fig. A7). The performance of 01-mini and o1-preview crucially depends on the number of \u201creasoning tokens\" (Fig. A7), so we use a maximum sample length of 8192 tokens as a good compromise between cost and performance (2048 tokens would lead to severe performance degradation on our tasks see our sample length ablation in Appendix B.1). We post-process the model outputs by removing all the leading and trailing white spaces and only consider the text after the keyword \u201cAction:\u201d, i.e., we discard all (chain-of-thought) reasoning traces.\nClaude 3.5 Sonnet While Claude 3.5 Sonnet has a maximum context window of 200k tokens, it can only process 100 images at a time, which is very limiting in our setting where a single episode can"}, {"title": "2.2. Environments", "content": "We evaluate the models described above on a battery of well-known interactive decision-making environments: the Phoenix game from Atari 2600 (Bellemare et al., 2013), chess, crosswords, the cheetah run task from the DM Control suite (Tassa et al., 2018), grid world navigation, and tic-tac-toe. We briefly describe each environment in below and provide a full description in Appendix A.1. Since sampling is deterministic for most models (i.e., the temperature is 0, see Section 2.1), we introduce variability in the demonstration and evaluation episodes by varying the initial conditions (e.g., via the environment seed for DM Control or by using different openings for chess; details below).\nAtari \u2013 Phoenix We use the Arcade Learning Environment (Bellemare et al., 2013) since it matches the Gato training data (Reed et al., 2022) (i.e., sticky actions and no uncontrolled random initial no-ops), which we use to create the expert demonstrations (full details in Appendix A.1). Due to the high cost of evaluating frontier models, we only evaluate a single Atari task. We chose Phoenix since it is a prototypical and representative Atari task (it is included in the Atari-5, which is a set of games where a model's performance is highly predictive of performance on the full set of games, see Aitchison et al. (2023)) and has somewhat dense rewards (which is important since we can only evaluate 400 frames, i.e., roughly 6 seconds of play, due to the context size limits classically Atari is evaluated with 108k frames). For the same reason, we use an action repeat of 4 to increase the amount of \"progress\u201d an agent can make given a limited number of steps. We feed the original (i.e., not downsampled or grayscale) image observation to the models (see Fig. Al for a visualization of the observation). To ensure variability in the evaluation episodes, we manually perform different numbers of no-ops (based on the random seed) at the beginning of every episode before starting\nWe evaluate playing chess against the weakest possible version of Stockfish 16 (Romstad et al., 2008), i.e., level 0 (which corresponds to an Elo of ~ 1320), which we further restrict by evaluating only 1 node. To generate the expert demonstrations we use the strongest version of Stockfish, i.e., level 20 with a time limit of 50ms per move as the agent (note that the opponent, i.e., the \"environment\", remains fixed as the weakest version of Stockfish). We evaluate four different state representations (see Fig. A2 for a visualization of the different observation types): (i) a 2D ASCII encoding of the board, (ii) the Forsyth\u2013Edwards Notation (FEN), which encodes the board and limited historical information as a string (but, importantly, not the full move history), (iii) the Portable Game Notation (PGN), a plain text format for recording chess games via the move history given in algebraic chess notation, and (iv) an RGB image of the board. We always represent the actions (both for the demonstrations and the evaluation) with the algebraic chess notation (the algebraic notation achieves a score that is 0.04 higher than that of the UCI notation in our ablation). To ensure variability in the demonstration and evaluation episodes, we use the openings from the Encyclopedia of Chess Openings (Matanovi\u0107, 1978), from which we randomly sample without replacement. We play a game for at most 100 steps (terminating early in case of a win/draw/loss; the average number of steps per game is 38) and assign a reward of 1 to a win, a reward of 0 to a draw, a reward of \u22121 to a loss, and 0 to all other states.\nWe create a large collection of crosswords of size 7 \u00d7 7 using the genxword crossword generator (Whitlock, 2011) with a list of 55 189 clues collected by Matthew Ginsberg (we only use the clues with the lowest difficulty rating; the full list contains 236615 clues). Each episode corresponds to a distinct crossword (individual clues may appear in multiple crosswords, although with low probability given the amount of clues). The observations consist of an ASCII representation of the crossword grid followed by the two lists of clues, one for the \u201cAcross\u201d words and one for the \"Down\u201d words (see Fig. A3 for a visualization of the observations). A valid action corresponds of either \"A\" (for across) or \u201cD\u201d (for down), followed by the word's index and then the word itself. We assign a reward of 1 to a correct word, a reward of 0 to an incorrect word that has the correct length or to a correct word that has been already been placed, and we terminate the episode (with a reward of \u22121) if the agent proposes a word of incorrect length (this makes the task somewhat more challenging but not insurmountable for today's frontier models since they all achieve positive mean score in our experiments). Note that we do never place incorrect words into the grid, even if they have the correct length we simply return the previous observation and a reward of 0. We evaluate for 25 steps (which is sufficient to place all the ~ 10 words per crossword on average) and terminate early if all correct words are placed. To generate the expert demonstrations, we use an oracle that simply outputs the solution for each clue one by one in random order.\nDM Control \u2013 Cheetah Run Due to the high cost of evaluating frontier models, we only consider the Cheetah Run task from the DM Control Suite (Tassa et al., 2018). For this task, we represent the observations as a Python dictionary (float64 values converted to a string) of position and velocity vectors (with individual values between \u20131 and 1), which we feed to the models as plain text (see Fig. A4 for a visualization of the actions and observations). Each episode begins in a new, randomly initialized state (based on the random seed). We only evaluate the first 100 steps of an episode (due to the limited context lengths). As for Atari, we use the Gato training data (Reed et al., 2022) to create the expert demonstrations (full details in Appendix A.1)."}, {"title": "2.3. Prompt", "content": "Our prompt consists of two main parts: (i) the expert demonstration episodes (see Listing 1), and (ii) the current trajectory of the evaluation episode (including the episode's previous actions and environment states, see Listing 2). We use a generic decision-making zero-shot preamble at the beginning of (i), and a short separator prompt at the beginning of (ii) (see Fig. 1 and Listings 1 and 2). The first part of the prompt (Listing 1) is fixed across an evaluation episode but resampled across evaluations, while the current trajectory part (Listing 2) starts with a single initial state and grows as more state-action pairs are observed (up to a maximum of 100 steps). Note that we do not explain the particular environment/task (e.g., the rules of tic-tac-toe) to the models in our zero-shot preamble the preamble is the same across all environments and only alludes to the model's general decision-making capabilities. We may, however, show the available legal actions (which depend on the environment) in each step, at the end of (ii), see Listing 2. Whether or not this is shown depends on whether it is beneficial per model and task (see our ablations in Appendix B.4). If legal actions are included in the prompt, the models do not have to guess / infer the correct action format. However, if the legal actions are not shown and only few or no demonstrations are available, then models have to guess the format (despite this, showing the legal actions does not always seem to be beneficial according to our ablations in Appendix B.4). Finally, similar to the legal moves, we may or may not use a chain-of-thought (Wei et al., 2022) style prompt at the very end of (ii) asking the model to provide a reasoning before proposing an action (see Listing 2). As for the legal actions, whether or not we include chain-of-thought reasoning in the prompt depends on whether it increases performance in the corresponding ablations (Appendix B.4). We make both of these decisions per model and task, meaning that, for example, the same model may use the chain-of-thought prompt for one task but not for another."}, {"title": "2.4. Evaluation Protocol", "content": "In every evaluation step, the model is conditioned on the current context, then it generatively produces an action that we feed to the environment, and the resulting next environment state is concatenated (together with the action that produced it) to the growing context. We performed an ablation on whether to show the previous actions in the evaluation trajectory (results in Appendix B.4.1) and found that it mostly improves performance, so we always include the past actions in all experiments. If a model fails to generate a legal action in some step, we randomly uniformly sample one of the legal actions (we visualize the percentage of illegal actions per model and task in Appendix B.3). Note that a model that only outputs illegal actions would match the performance of our random action baseline. Also note that, even if the models have to initially guess the correct action format in the absence of any demonstrations (and when the list of legal actions is not included in the prompt), if their guess is wrong and they generate an illegal action, the observation in the next step will show a random legal action, and models could at least, in principle, quickly learn the correct action format from the history of the evaluation trajectory. Since the models have different maximum context lengths, the maximum number of demonstration episodes that fit into the context depends on the model, the task, and the state representation format. For example, for a given model, we may only by able to use 16 demonstrations with RGB observations, whereas we can fit 256 ASCII demonstrations.\nFor our results we always evaluate 100 different episodes each episode individually, meaning that we never concatenate multiple evaluation episodes to the context and report the average score over these 100 trials. The maximum episode duration is 100 steps and the score per trial is the cumulative reward over all steps (but the reward information is never shown to the model, neither for the demonstrations nor the evaluation episode, since we are interested in in-context imitation learning rather than in-context reinforcement learning). For each evaluation episode we randomly subsample the demonstration episodes from a precomputed pool of up to 1000 demonstrations, ensuring that all evaluation episodes have distinct initial states from each other and from the initial states of the demonstration episodes (except for the replay control experiments in Appendix B.2)."}, {"title": "3. Results", "content": "In this section, we present our comprehensive empirical evaluation of the current state-of-the-art closed-source frontier models on our set of six different interactive decision-making tasks (see Section 2.2 for an overview of the tasks, and Appendix A.1 for more details, including visualizations of the state observations). As stated previously, we are most interested in how model performance changes when increasing the number of demonstration episodes in the context. To produce our corresponding main results, which are presented below, we first perform an ablation to determine whether to use chain-of-thought prompting and whether to show all legal actions or not. This ablation is performed for each model on each task separately (results in Appendix B.4), and we then use the best setting to run the sweep over the number of demonstration episodes. The ablations are conducted using one demonstration episode (i.e., still many individual demonstration steps), which offers a good compromise between a reasonable resource footprint (more demonstrations require more compute) and being representative of the in-context learning setting (e.g., without any demonstrations we would overestimate the importance of showing legal actions to communicate the action format).\nAPI Limitations Note that for Claude 3.5 Sonnet we only perform the initial ablations on a single demonstration episode, and no further sweep over the number of demonstrations due to the very low monthly token limits of the API (Anthropic, 2024d). Similarly, since Claude 3.5 Sonnet takes maximally 100 images at a time, we omit its evaluation on Atari \u2013 Phoenix, where even a single\nFig. 2 shows the highest overall score across all settings per model and task, where the maximum is taken over the number of demonstration episodes, the different state representations, and the ablation settings (showing legal actions and chain-of-thought prompting). The settings for each bar within a panel may thus vary, and we refer to the detailed results in Section 3.2, where the state representation format and number of expert demonstrations are the same across all models per datapoint. Also note that due to the tight monthly token limits of the Anthropic API (Anthropic, 2024d), the results for Claude 3.5 Sonnet only consist of the ablations conducted with 1 demonstration episode, whereas, for all other models, Fig. 2 shows the best results across the sweep over the number of demonstrations.\nAs Fig. 2 shows, all models outperform the random legal action baseline in all tasks except for the Phoenix game from Atari 2600. In Phoenix, in order to fire repeatedly, the firing button needs to be"}, {"title": "3.2. In-Context Imitation Learning", "content": "We now evaluate the in-context imitation learning capabilities of today's frontier models across different numbers of demonstration episodes from zero-, to few-, and many-shot settings. The largest possible number of demonstrations depends on the maximal context size of the model, the task (some tasks always have 100 steps, whereas others may have a lot less, e.g., tic-tac-toe), and the state representation format within the task (images typically require more tokens than the text-based representations, and GPT-40 and Claude 3.5 Sonnet have strict limits in terms of the maximum number of images). As stated in the previous section, we omit Claude 3.5 Sonnet from this evaluation due to its very low monthly token limit.\n Fig. 3 shows that on the Phoenix game from Atari 2600 all models im-prove slightly by having one demonstration episode, as opposed to having none. However, adding more demonstrations (which can only be done for the Gemini 1.5 models) does not improve the performance further. Overall, the models struggle to outperform a random action baseline, which can mainly be explained by the models' tendency to repeat actions (and Phoenix requires tapping the fire action, rather than con-stantly pressing it, which will only result in a sin-gle shot see Section 3.1 for more discussion). Note that models rarely output illegal actions (see Fig. A15). Table A1 shows which ablation setting was used for which model. Since one episode consists of 100 images, only the Gemini models can have more than one demonstration episode in the context while still having room for the evaluation episode (GPT-40 can have one\ndemonstration since its limit is 250 images, and Claude 3.5 Sonnet none since its limit is 100). Atari \u2013 Phoenix is arguably the hardest task in our benchmark, since it only comes with image observations (no text representation), and has very large demands w.r.t. context size (high frame rate, somewhat high resolution images, etc.). Despite LMs' massive scale, playing Atari games well (at least naively by feeding raw images to the context) seems currently beyond reach, both in terms of capabilities, but also in terms of context size and compute demands."}, {"title": "4. Discussion", "content": "The goal of our work is to test the limits of the in-context imitation learning capabilities of today's frontier LMs on interactive decision-making tasks with long multimodal context. This regime is interesting for multiple reasons: First, more demonstrations should, in principle, improve performance (up to a certain point), and modern LMs have long enough contexts to process a significant number of observations (previous work on non-interactive prediction tasks found that many-shot prompts with thousands of examples can significantly improve over few-shot prompts (Agarwal et al., 2024; Jiang et al., 2024)). Second, pretrained LMs have been shown to have a general ability of recognizing algorithmic patterns in their context and predicting accordingly (Mirchandani et al., 2023), which is in line with the memory-based meta-learning view on (universal) in-context prediction (Grau-Moya et al., 2024; Ortega et al., 2019). Environment-agent interactions can be understood as one type of such algorithmic patterns, meaning that there is a theoretical argument why this may work (admittedly, the theory does not allow statements about generalization to unseen out-of-distribution tasks). Finally, long context multimodal decision-making evaluations fill a gap in the current literature (which is at least partly due to the computational demands of long context evaluations with large models). Despite our focus on in-context imitation learning, we believe it will be interesting to compare against other methods, including fine-tuning, retrieval-based methods, reward-conditioning, etc. The open source release of our benchmark (see https://github.com/google-deepmind/lm_act) enables these comparisons and we are looking forward to seeing them."}, {"title": "5. Related Work", "content": "The emergence of strong in-context learning capabilities with increasing model and training data scale in LLMs was first observed in the GPT-2 paper (Radford et al., 2019) and even gave the title to the GPT-3 paper (Brown et al., 2020). Soon after, the recipe of next-token prediction at scale was applied to build agents from large sequential predictors, resulting, e.g., in the decision (pretrained) transformer (Chen et al., 2021; Lee et al., 2023), or the generalist agent GATO (Reed et al., 2022), and its more recent open source variant JAT (Gallou\u00e9dec et al., 2024). While the empirical results were certainly surprising at the time, at least in theory, in-context learning must necessarily arise as a core feature of a sequential predictor trained to minimize next token log loss over an implicit meta distribution of data (Genewein et al., 2023; Mikulik et al., 2020; Ortega et al., 2019), and could, in principle, even lead to universal in-context predictors (Grau-Moya et al., 2024). An explicit application of this memory-based meta-learning principle at scale is the \u201cadaptive agent\" from Bauer et al. (2023), which shows that an embodied agent in a 3D environment can adapt to novel task instances on human timescales (i.e., single- or low double digit numbers of interaction episodes) purely in context and across a vast set of tasks. The SIMA Team et al. (2024) conducted another impressive large-scale application of training and fine-tuning a complex vision-language agent across a large set of environments, using instruction conditioning together with in-context learning.\nInstead of pretraining separate models for general decision-making, many researchers have also attempted to directly use the knowledge and reasoning capabilities of LLMs and VLMs for building agents that interact with an environment (see Xi et al. (2023) for a 2023 survey of LLM-based agents) either by fine-tuning (Li et al., 2022) or purely via in-context demonstrations (Palo and Johns, 2024). In both cases, two open question are: (i) how to best represent environment observations as tokens, and (ii) how to best elicit the decision-making capabilities of pretrained LMs? Mirchandani et al. (2023) find that pretrained LLMs are \u201cgeneral pattern machines\" that can learn to complete complex token sequences via in-context learning, including agent-environment interactions. They even find that in many cases, randomly swapping the alphabet does not have significant impact, suggesting that LLMs may be able to deal with many different ways of translating observations into tokens. Perhaps more important is the question whether observations should be state-action sequences (as in imitation learning and our work) or whether they should include rewards (for reward conditioning or in-context RL as in Mirchandani et al. (2023) and Raparthy et al. (2024)). While this is still unclear for pretrained LMs, Ruoss et al. (2024) find that, when training a large transformer to play chess, performance is roughly the same for imitation learning compared to learning to predict state- or action-values (as long as the amount of data for all three variants is equal). Ma et al. (2024) find that, when prompted appropriately, a pretrained VLM (Gemini 1.5 Pro) can produce good value estimates for real-world robotic tasks. The second open problem, how to best prompt LMs for decision-making, is a very active research area, with a lot of focus on designing or learning zero-shot prompts, such as the famous \u201cLet's think about this step by step.\" (Kojima et al., 2022) and \u201cTake a deep breath and work on this problem step-by-step.\" (Yang et al., 2024), which has led to many chain-of-thought prompting schemes (Wei et al., 2022). Besides better zero-shot prompts, advanced sampling and prompt optimization schemes have been explored, such as iterative prompt refinement where an agent starts with some demonstrations in the context, then interacts with the environment, and"}, {"title": "6. Conclusion", "content": "In this paper, we have evaluated the current capabilities of some of the world's largest and most advanced Al models to perform in-context imitation learning on interactive decision-making tasks with multimodal state representations on tasks that are simple for humans but challenging for state-of-the-art LMs. Our results show that even in the face of potentially hundreds of demonstration episodes, with context lengths of up to one million tokens, models struggle to reach expert performance in most settings. This is despite models arguably having a lot of knowledge about the tasks and well-performing strategies. Our work thus adds another piece to the growing literature that finds difficulties that LMs can have with translating declarative knowledge into effective acting (know-how) in some interactive decision-making tasks. At this point it is unclear whether this problem will simply go away at even larger scale (as we certainly observe positive results in some of our settings), or whether adding"}, {"title": "B. Additional Results", "content": "In this section, we present additional results and ablations from our experimental evaluation."}, {"title": "B.1. Ablating the Maximum Sample Length", "content": "The o1 family of models tends to generate (long) internal \"reasoning traces\" before returning an output. Thus, if the maximum sample length is not large enough, these models may not have enough \"reasoning tokens\" and therefore do not produce an output (in which case the API returns an empty sample). Since we want to report each model's best performance on our benchmark, we therefore ablate the maximum number of sample tokens and choose the configuration that trades off good performance and low cost (since, after a certain point, more tokens generally do not improve performance but only increase the cost). The maximum sample length also has an impact on the other models, particularly when using chain-of-thought reasoning, but a relatively low sample length typically suffices for our tasks (unlike the o1 models which require large maximal sample length).\nTo that end, Fig. A7 shows our ablation of the maximum sample length for Claude 3.5 Sonnet, GPT-40, 01-mini, o1-preview, Gemini 1.5 Flash, and Gemini 1.5 Pro on all three observation types (ASCII, coordinates and RGB) for the grid world navigation task with 1 demonstration episode. Fig. A7 shows the average score over 100 evaluation episodes for maximum sample lengths from 32 to 32768 tokens (Claude 3.5 Sonnet only supports 8192 output tokens and GPT-40 only supports 16384 output tokens). Unsurprisingly, the performance of Claude 3.5 Sonnet, GPT-40, Gemini 1.5 Flash, and Gemini 1.5 Pro is largely unaffected by the number of sample tokens, i.e., even 32 tokens are sufficient to achieve their best performance (with the exception of Claude 3.5 Sonnet and GPT-40 on RGB observations, where they benefit from having 64 tokens). This is in stark contrast with o1-mini and o1-preview, which require between 4096 and 8192 output tokens to achieve their optimal performance (with a very steep degradation below 4096). To verify whether this sharp rise in performance is actually due to the model not generating a valid action, Fig. A7 also visualizes the average percentage of illegal actions per episode over the maximum sample length, which (inversely) correlates very strongly with performance for the o1 models. We therefore set the maximum sample length to 8192 for the 01 models as a tradeoff between cost and performance (the maximum for o1-preview is 32768, the maximum for o1-mini is 65536 (OpenAI, 2024d)). This should enable the o1 models to achieve their best performance on our benchmark even though they do so at a much higher (computational) cost than the other models."}, {"title": "B.2. Replaying a Demonstration Episode", "content": "In-context imitation learning requires several different skills, one of which is being able to locate and retrieve the relevant demonstration(s) from the context. We therefore conduct a \u201csanity check\u201d experiment where we provide a single demonstration episode and \"replay\" the exact same episode for evaluation (akin to a multimodal sequence copying task). Thus, for every step, the model only has to find the correct location in the demonstration episode in its context and reproduce its corresponding action. Accordingly, we slightly change the evaluation setup since the next observation (both action and next state) in the evaluation trajectory is always determined by the demonstration episode and not by the action generated by the model (i.e., we perform teacher forcing rather than a dynamic evaluation). As a result, we are not interested in a model's return but in how often it manages to match the action from the demonstration episode. Like in our other experiments, we evaluate 100 different episodes. For the sake of simplicity, we do not use chain-of-thought prompting, do not show the legal actions in the prompt, and only consider a single demonstration episode. Other than that, we leave the prompt the same as in our default experimental setup (i.e., including the same preamble"}, {"title": "B.3. Illegal Actions", "content": "As described in Section 2.4, we do not termi-nate the evaluation episode early if a model does not produce a valid action (except on the crossword task) but instead randomly sample one of the actions that are legal in the current state and continue the evaluation with the obser-vation produced by that action. To differentiate illegal actions from acting randomly (over legal actions), we compute how often models actually propose an illegal action and visualize the per-centage of illegal actions per episode over the number of expert demonstrations in Figs. A15 to A20 (analogous to Figs. 3 to 8). Note that, since we perform an ablation over showing the legal moves in the prompt (see Appendix B.4), in theory, all models could have the necessary information to sample a legal action at all times (whether we actually show this or not in our main experiments depends on whether it improves model performance in the ablations). Nevertheless, we observe that models do produce illegal actions actions across most environments. For example, on the grid world navigation task with coordinate observations 01-preview produces almost 100% illegal actions with 2 or more expert demonstrations in the context. Interestingly, the ability to produce legal actions also depends on the observation type:"}]}