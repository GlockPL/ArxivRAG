{"title": "Constraints as Rewards:\nReinforcement Learning for Robots without Reward Functions", "authors": ["Yu Ishihara", "Noriaki Takasugi", "Kotaro Kawakami", "Masaya Kinoshita", "Kazumi Aoyama"], "abstract": "Reinforcement learning has become an essential\nalgorithm for generating complex robotic behaviors. However,\nto learn such behaviors, it is necessary to design a reward\nfunction that describes the task, which often consists of multiple\nobjectives that needs to be balanced. This tuning process is\nknown as reward engineering and typically involves extensive\ntrial-and-error. In this paper, to avoid this trial-and-error pro-\ncess, we propose the concept of Constraints as Rewards (CaR).\nCaR formulates the task objective using multiple constraint\nfunctions instead of a reward function and solves a reinforce-\nment learning problem with constraints using the Lagrangian-\nmethod. By adopting this approach, different objectives are\nautomatically balanced, because Lagrange multipliers serves\nas the weights among the objectives. In addition, we will\ndemonstrate that constraints, expressed as inequalities, provide\nan intuitive interpretation of the optimization target designed\nfor the task. We apply the proposed method to the standing-\nup motion generation task of a six-wheeled-telescopic-legged\nrobot and demonstrate that the proposed method successfully\nacquires the target behavior, even though it is challenging to\nlearn with manually designed reward functions.", "sections": [{"title": "I. INTRODUCTION", "content": "Recent advances in the field of reinforcement learning\nhave enabled robots to generate complex manipulation and\nlocomotion behaviors that were previously considered chal-\nlenging [1], [2], [3], [4], [5]. However, these successes are\nnot solely due to algorithmic progress; they also depend on\nthe tuning of reward functions conducted by the authors.\nFor example, Taylor et al. [1] designed a reward function\nconsisting of four different objectives and tuned the weights\namong these objectives to achieve locomotion behavior in\na bipedal robot that imitates human movement. Currently,\nthere is no established systematic method for this tuning\nprocess, and therefore success often relies on the individual\nreward designer's expertise. For further advancement of\nrobots utilizing reinforcement learning, we argue that we\nneed to avoid this tuning process, widely known as reward\nengineering, and establish a design method that does not\nheavily rely on individual designers.\nIn this paper, to avoid the trial-and-error involved in\ndesigning a reward function, we propose a new approach\nto train robots with reinforcement learning: Constraints as\nRewards (CaR). CaR exclusively uses constraints to solve\nthe reinforcement learning problem. Specifically, CaR for-\nmulates the robot's task solely from constraint functions"}, {"title": "III. PRELIMINARIES", "content": "In this research, we consider the finite-horizon reinforce-\nment learning problem [19]. The objective of the problem\nis to find an optimal policy $\\pi^*$ that maximizes the sum of\ndiscounted rewards over the horizon T:\n$\\max_\\pi E_{\\tau} \\sum_{t=0}^T \\gamma^t r(s_t, a_t)]$.                                                                              (1)\nHere, $s_t$, $a_t$, $\\gamma$, and r denote the state at time t, the action\nat time t, the discount factor, and the reward function,\nrespectively. In this work, we revisit the design of the reward\nfunction r(s, a).\nAs previously mentioned, in many practical applications,\nthe reward function r(s, a) is designed as a weighted sum\nof multiple functions $r_n(s, a)$ (n = 1, ..., N)\n$r(s,a) = \\sum_{n=1}^N w_nr_n(s, a)$.                                                                         (2)\nTherefore, the actual problem that needs to be solved is:\n$\\max_\\pi \\sum_{n=1}^N w_n E_{\\tau} [\\sum_{t=0}^T \\gamma^t r_n(s_t, a_t)]$.                                                                                                        (3)\nFrom this equation, we can confirm that we need to tune\nboth weights $w_n$ and functions $r_n$ for successful learning.\nHowever, there is no systematic procedure to tune these\nparameters.\nIn this work, to alleviate the trial-and-error involved in the\ndesign of the reward function, we consider solving reinforce-\nment learning problem with constraints. The reinforcement\nlearning problem with constraints is defined as follows:\n$\\max_\\pi E_{\\pi} \\sum_{t=0}^T \\gamma^t r(s_t, a_t)]$\ns.t. $E_{\\pi} [\\sum_{t=0}^T \\gamma^t g_m(s_t, a_t)] \\ge 0, m = 1, ..., M$.                                                                     (4)\nHere, $g_m(s_t, a_t)$ is the m-th constraint function. In the next\nsection, we will show that we can eliminate the tuning pro-\ncess required in the original problem using this formulation."}, {"title": "IV. METHOD", "content": "In this section, we will describe in detail the idea of\nConstraints as Rewards (CaR). In CaR, we transform the\nreinforcement learning problem with constraints described\nin eq. (4) into an unconstrained problem by incorporating\nthe Lagrange dual function L($\\pi$, $\\lambda$) [6]:\n$\\max_{\\lambda_m \\ge 0 \\forall m} \\max_{\\pi} L(\\pi, \\lambda)$.                                                                                                    (5)\nWhere $\\lambda = [\\lambda_1,..., \\lambda_M]^T$ are the Lagrange multipliers, and\nL($\\pi$, $\\lambda$)\n$L(\\pi, \\lambda) = E_{\\pi} [\\sum_{t=0}^T \\gamma^t r(s_t, a_t)] + \\sum_{m=1}^M \\lambda_m E_{\\pi} [\\sum_{t=0}^T \\gamma^t g_m(s_t, a_t)]$.                                                                                                                                                                            (6)"}, {"title": "B. Constraint Function Design", "content": "To compose the learning objective with constraints, we\npropose the following four designs of constraint function\ng(s, a). Each design provides an intuitive interpretation for\nthe optimization target. Timestep constraints enable con-\nstraining the robot at specific timestep (e.g., constraining the\nfinal pose of the robot.), while episode constraints enable\nconstraining the robot during an episode (e.g., constraining\nthe robot from hitting an obstacle.). We expect that having\nan intuitive interpretation will make it easier for the task\ndesigner to compose the objective. In our experiments, we\ncomposed the task objective using a combination of these\nfunctions. Please note that the inequality in each constraint\ncan be reversed by setting the constraint function to -g(s, a).\nSee the appendix section for the derivation of each function.\n1) Timestep probability constraint: Constrain the proba-\nbility of an event at specific timestep t = t' to be less\nthan or equal to $p_\\epsilon \\in [0...1]$: $p_\\epsilon \\ge P_\\pi(s_{t'} \\in S', a_{t'} \\in\nA')$. With:\n$g(s, a) = \\begin{cases}\n0 & (t \\neq t') \\\\\np_\\epsilon - 1_{s \\in S', a \\in A'} & (t = t').\n\\end{cases}$                                                                                                                                                 (8)\nHere, $S'$ and $A'$ are the sets of events of particular interest,\n$1$ is the indicator function, $P_\\pi$ and $P_{\\pi, \\gamma}$ are the undiscounted\nand discounted state-action probabilities, and $E_{\\pi, \\gamma}$ is the\nexpectation under the discounted state-action distribution.\n2) Timestep value constraint: Constrain the value com-\nputed from a state and/or action in expectation at\nspecific timestep t = t' to be less than or equal to\n$\\epsilon$: $\\epsilon > E_\\pi[\\hat{g}(s_{t'}, a_{t'})]$. With:\n$g(s, a) = \\begin{cases}\n0 & (t \\neq t') \\\\\n\\epsilon - \\hat{g}(s, a) & (t = t').\n\\end{cases}$                                                                                                                                               (9)\n3) Episode probability constraint: Constrain the probabil-\nity of an event during an episode to be less than or\nequal to $p_\\epsilon \\in [0..1]$: $\\rho_\\epsilon \\ge P_{\\pi, \\gamma}(s \\in S', a \\in A')$.\nWith:\n$g(s, a) = p_\\epsilon - 1_{s \\in S', a \\in A'}$.                                                                                                               (10)\n4) Episode value constraint: Constrain the value com-\nputed from a state and/or action during an episode\nin expectation to be less than or equal to $\\epsilon$: $\\epsilon >\nE_{\\pi, \\gamma}[\\hat{g}(s, a)]$. With:\n$g(s, a) = \\epsilon - \\hat{g}(s, a)$.                                                                                                                                        (11)"}, {"title": "C. QRSAC-Lagrangian", "content": "We propose QRSAC-Lagrangian\u00b9, an extension of QR-\nSAC [7], to solve the reinforcement learning problem when\nusing CaR. We extended QRSAC because we expect that\nthe quantile function, which estimates the distribution of\nQ values, is effective in our problem setting. In CaR,\nthe algorithm needs to periodically update the Lagrange\nmultipliers. Therefore, the distribution of the target Q value\nchanges during training. In such situations, direct estimation\nof Q values performed using conventional algorithms may\nbecome unstable. We will compare the performance of\nQRSAC-Lagrangian, when used in conjunction with CaR,\nwith variants of Lagrangian-based algorithms, such as SAC-\nLagrangian [3] and PPO-Lagrangian [8], and show that\nQRSAC-Lagrangian achieves faster convergence compared\nto these algorithms. The pseudo code of the algorithm is\nshown in Algorithm 1. In Algorithm 1, $\\pi_\\theta$ is the training\npolicy, d is the multiplier update interval, and $p(s_{t+1}|a_t, s_t)$\nis the state transition distribution."}, {"title": "V. IMPLEMENTATION", "content": "Our robot has six telescopic legs, four with driving wheels\nand two with omnidirectional passive wheels. Each leg has\na hip joint with a range of motion of 45 deg and a 500 mm\nexpandable prismatic knee joint. The policy must coordinate\neach leg to enable the robot to stand up during the task. In\nthis section, we will describe the design of the constraint\nfunctions used in training and the controller design for this\nrobot. For hardware details of the robot, please refer to [?]."}, {"title": "A. Constraint Function Design for Standing Up Task", "content": "We designed the following five constraints for the task:\n1) Final pose constraint:\n$g(s, a) = \\begin{cases}\n0 & (t \\neq T) \\\\\n10^{-3} - |s_{target} - s_{angle/position}| & (t = T).\n\\end{cases}$                                                                                                                                           (12)\nExisting research [5] uses an episodic reward that\npenalizes the robot's pose at every timestep to facil-\nitate standing up. However, from the perspective of\nconstraints, the intermediate pose of the robot should\nbe arbitrary. Therefore, we constrain the hip joint angle\nand knee position of the robot's leg only at the final\ntimestep T to be below $10^{-3}$ rad and $10^{-3}$m. $s_{target}$\nis the target joint angle/knee position and $s_{angle/position}$\nis the actual joint angle/knee position. We apply this\nconstraint to each leg, resulting in a total of 12 pose\nconstraints.\n2) Fall down constraint:\n$g(s, a) = \\begin{cases}\n0 & (t \\neq T) \\\\\n-1 & \\text{robot is in a fall-down state.} & (t = T).\n\\end{cases}$                                                                                                          (13)\nThis function constrains the probability of falling down\nto 0. The robot is considered to be in a fall-down state\nwhenever more than five wheel joints are higher than\nthe hip joints. The task terminates when the robot falls\ndown: therefore, the indicator function is evaluated at\nt = T.\n3) Body contact constraint:\n$g(s, a) = -1_{\\text{robot's body hits the floor.}}$                                                                                                                                                                     (14)\nThis function constrains the probability of the robot's\nbody making contact with the floor to 0.\n4) Leg swing constraint:\n$g(s, a) = -1_{\\text{Angular velocity exceeds 2.0 rad/s.}}$                                                                                                                                                            (15)\nThis function constrains the angular velocity of the\nrobot's hip joint to be less than 2.0 rad/s, preventing\ndangerous leg swinging actions. To evaluate this con-\nstraint, we terminated the episode whenever the joint's\nangular velocity exceeds 2.0 rad/s.\n5) Inclination constraint:\n$g(s, a) = \\begin{cases}\n0 & (t \\neq T) \\\\\n10^{-2} - |u_z^T v_{x,y}| & (t = T).\n\\end{cases}$                                                                                                                                             (16)\nWe add this constraint to ensure that the final pose of\nthe robot remains parallel to the floor. We constrain the\nrobot's body to be perpendicular to the z-axis. Here,"}, {"title": "B. Robot Controller Design", "content": "Our robot is controlled with a PID controller that receives\nthe target joint angles and positions of each leg. Therefore,\nwe trained a controller $\\pi_\\theta$ that outputs the target joint angles\nand positions to be input to this PID-controller. Table I shows\nits network architecture. The policy inputs a vector consisting\nof the following features:\n\u2022 Hip joint angles (6 dim)\n\u2022 Knee joint positions (6 dim)\n\u2022 Hip joint angular velocities (6 dim)\n\u2022 Knee joint velocities (6 dim)\n\u2022 Robot's angular velocities and accelarations (6 dim)\n\u2022 Timestep since the beginning of the task (1 dim)\n\u2022 History of policy's output (12 dim\u00d7H steps).\nThe output of the policy is squashed and normalized using\nthe tanh function to fit within the range of [-1, 1]. Therefore,\nwe rescaled the policy's output before feeding it to the PID-\ncontroller. We fixed the robot's wheel velocity to 0 for the\nstanding-up task. We set H = 3 in the simulation and H = 5\nin the real robot experiment."}, {"title": "VI. EXPERIMENTS", "content": "In the experiment, we trained the policy to make the\nrobot transition from an arbitrary initial pose to an upright\npose (See Fig. 1) and tested its performance using 10"}, {"title": "VII. CONCLUSION AND FUTURE WORK", "content": "In this paper, we introduced the concept of Constraints\nas Rewards (CaR) to mitigate the extensive trial-and-error\ninvolved in tuning a reward function. Instead of designing\na reward function, CaR composes the task objective solely\nfrom constraint functions. In this approach, unlike conven-\ntional reward function design, we do not need to manually\ntune the weights among task objectives; the weights are ad-\njusted automatically during the training process. In addition,\nto simplify the design of constraint functions, we proposed\nfour specific designs that provide an intuitive interpretation\nof the task objective. Furthermore, we introduced QRSAC-\nLagrangian algorithm to solve this reinforcement learning\nproblem with constraints. We demonstrated the effectiveness\nof our method by applying it to the standing-up motion\ngeneration task of a six-wheeled-telescopic-legged robot,\nTachyon 3. While this task is challenging to learn with"}, {"title": "APPENDIX I\nCONSTRAINT FUNCTION DERIVATION", "content": "In the derivation, we assume discrete state and action sets\nbut the results can also be applied to continuous settings."}, {"title": "A. Derivation of timestep probability constraint", "content": "By substituting eq. (5) into the optimization objective, we\nget:\n$E_{\\pi} [\\sum_{t=0}^T \\gamma^t g(s_t, a_t)]$\n$= E_{\\pi} [\\sum_{t=0}^T \\gamma^t (p_\\epsilon - 1_{s \\in S', a \\in A'})]$ ($: g(s_t, a_t) = 0 t \\neq t'$)\n$= \\gamma^{t'} (p_\\epsilon - E_{\\pi} [1_{s \\in S', a \\in A'}])$\n$= \\gamma^{t'} (p_\\epsilon - P_{\\pi}(s \\in S', a \\in A'))$.\nTherefore, the constraint can be expressed as follows:\n$\\gamma^{t'} (p_\\epsilon - P_{\\pi}(s \\in S', a \\in A')) \\ge 0 \\rightarrow p_\\epsilon \\ge P_{\\pi}(s \\in S', a \\in A').$"}, {"title": "B. Derivation of timestep value constraint", "content": "Similar to the above derivation, by substituting eq.6 into\nthe optimization objective, we get:\n$\\gamma^{t'} (\\epsilon - E_{\\pi} [\\hat{g}(s_{t'}, a_{t'})]) \\ge 0 \\rightarrow \\epsilon \\ge E_{\\pi} [\\hat{g}(s_{t'}, a_{t'})].$"}, {"title": "C. Derivation of episode probability constraint", "content": "First, we reformulate the optimization objective as follows:\n$E_{\\pi} [\\sum_{t=0}^T \\gamma^t g(s_t, a_t)]$\n$= \\sum_{t=0}^T \\sum_{s_0 \\in S,...,a_T \\in A} p(s_0, a_0,..., s_t, a_t) \\gamma^t g(s_t, a_t)$\n$= \\sum_{t=0}^T \\sum_{s \\in S} \\sum_{a \\in A} p_t^{\\pi}(s, a) \\gamma^t g(s, a)$\n$= \\sum_{s \\in S} \\sum_{a \\in A} g(s, a) \\sum_{t=0}^T p_t^{\\pi}(s, a)$\n$= \\frac{1 - \\gamma^{T+1}}{1 - \\gamma} \\sum_{s \\in S} \\sum_{a \\in A} g(s, a) \\frac{1 - \\gamma}{1 - \\gamma^{T+1}} \\sum_{t=0}^T p_t^{\\pi}(s, a)$\n$=$"}, {"title": "APPENDIX III\nEXTRA EXPERIMENTAL RESULTS", "content": "We present the evaluation results of the proposed algo-\nrithm in the classic inverted pendulum task implemented\nin Gym [21]. We trained the model using two different\nconstraint functions:\n$g(s, a) = 10^{-2} - |\\theta_p|$                                                                                                                                                                   (17)\nand\n$g(s, a) = \\begin{cases}\n0 & (t \\neq T) \\\\\n10^{-2} - |\\theta_p| & (t = T).\n\\end{cases}$                                                                                                                                          (18)\nHere, $\\theta_p$ is the angle of the pendulum. Fig. 7 shows\nthe learning curves during training. From the figure, we\ncan confirm that conventional PPO-Lagrangian and CaT\nfailed to learn with the constraint function in eq. (18).\nThis result suggests that PPO-Lagrangian and CaT are not\neffective in environments with constraint functions of the\nform in eq. (18). Conversely, SAC-Lagrangian and QRSAC-\nLagrangian succeeded in learning in both settings. As in\nthe experiment section, QRSAC-Lagrangian converged faster\nthan SAC-Lagrangian."}]}