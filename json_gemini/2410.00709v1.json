{"title": "Binding Affinity Prediction: From Conventional to Machine Learning-Based Approaches", "authors": ["Xuefeng Liu", "Songhao Jiang", "Xiaotian Duan", "Archit Vasan", "Chong Liu", "Chih-chan Tien", "Heng Ma", "Thomas Brettin", "Fangfang Xia", "Ian T. Foster", "Rick L. Stevens"], "abstract": "Protein-ligand binding is the process by which a small molecule (drug or inhibitor) attaches to a target protein. The binding affinity, which refers to the strength of this interaction, is central to many important problems in bioinformatics such as drug design. An extensive amount of work has been devoted to predicting binding affinity over the past decades due to its significance. In this paper, we review all significant recent works, focusing on the methods, features, and benchmark datasets. We have observed a rising trend in the use of traditional machine learning and deep learning models for predicting binding affinity, accompanied by an increasing amount of data on proteins and small drug-like molecules. While prediction results are constantly improving, we also identify several open questions and potential directions that remain unexplored in the field. This paper could serve as an excellent starting point for machine learning researchers who wish to engage in the study of binding affinity, or for anyone with general interests in machine learning, drug discovery, and bioinformatics.", "sections": [{"title": "1 Introduction", "content": "Protein-ligand binding [Clyde et al., 2023] refers to the process as shown in Fig. 1 by which ligands\u2014usually small molecules, ions, or proteins generate signals by binding to the active sites of target proteins through intermolecular forces. This binding typically changes the conformation of target proteins, which then results in the realization, modulation, or alteration of protein functions. Therefore, protein-ligand binding plays a central role in most, if not all, important life processes. For example, oxygen molecules are bound and carried through the human body by proteins like hemoglobin, and then utilized for energy production, while nonsteroidal anti-inflammatory drugs (NSAIDs) like ibuprofen work by inhibiting the functionality of the cyclooxygenase (COX) enzyme that thus reducing the release of pain-causing substances in the body.\nThe concept and importance of binding affinity prediction were first addressed in B\u00f6hm [1994]: given the 3D structures of a target protein and a potential ligand, the objective is to predict the binding constant of such a complex, along with the most probable binding pose candidates. The prediction of the binding site (the set of protein residues that have at least one non-hydrogen atom within 4.0 \u00c5 of a ligand's non-hydrogen atom [Khazanov and Carlson, 2013]) and affinity (binding constants such as inhibition or dissociation constants, or the concentration at 50% inhibition) are usually divided into two separate but related stages [Ballester and Mitchell, 2010a].\nOne notable motivation for constructing a good binding affinity predictor (or scoring function, as called in some earlier work) is the essential role that it plays in drug discovery [Liu et al., 2023, 2024a] and virtual screening [Meng et al., 2011, Pinzi and Rastelli, 2019, Sadybekov and Katritch, 2023]. Traditional drug discovery essentially involves a process of trial and error. However, with a functional binding affinity predictor,"}, {"title": "2 Datasets and Benchmarks", "content": "There are many datasets and benchmarks used for the study of protein-ligand binding affinity prediction, each focusing on different aspects of the problem. Here, we first discuss in Section 2.1 the specifications for binding affinity data, such as resolution, measurement, binding constant, and concentration. These factors are essential when selecting the right dataset for a specific goal. In Section 2.2, we review some of the most commonly used datasets and benchmarks directly applied in the learning and prediction of protein-ligand binding affinity. These datasets usually have binding constants such as Ki, Kd, or IC50 associated with most, if not all, protein-ligand complexes. We also note other datasets of protein-ligand binding affinity not directly used for model training and evaluation. Even though these datasets are not directly applicable for training and evaluation, they can be useful for pre-training and may provide additional information, methods, and insights. All discussed datasets are listed in Table 1."}, {"title": "2.1 Data Specifications", "content": ""}, {"title": "2.1.1 Structure", "content": "The structure of a protein-ligand complex refers to its conformation, or in other words, the spatial arrangement of its atoms. This is an important specification or feature of binding affinity data, as it opens up numerous possibilities for feature engineering and learning methods. For instance, with given structures, we can parameterize the energy terms and atom-pair distances and use them for conventional scoring or learning with methods such as random forests [Ballester and Mitchell, 2010a], support vector machines [Kinnings et al., 2011], and neural networks [Durrant and McCammon, 2010, 2011]. On the other hand, measuring"}, {"title": "2.1.2 Resolution", "content": "Resolution, in the context of protein-ligand complex structures, refers to the distance corresponding to the smallest observable feature in the measured structure [Guterres and Im, 2020]. In other words, two objects within this distance will be perceived as one and thus rendered indistinguishable. Generally, structures with a resolution finer than 1-1.2 \u00c5 are considered high-resolution, while those with a resolution lower than 3.0 \u00c5 only outline the basic contours of the protein chains. Resolution is an essential measurement for the quality of data and is used as a selection criterion for many binding affinity databases, such as CASF and Binding MOAD."}, {"title": "2.1.3 Structure Determination Techniques", "content": "The three most commonly used techniques for determining the structure of protein complexes are X-ray crystallography [Maveyraud and Mourey, 2020, Jackson et al., 2023], NMR (nuclear magnetic resonance) [Hore, 2015, Galvan et al., 2023], and Cryo-EM (cryo-electron microscopy) [Adrian et al., 1984, Nogales and Mahamid, 2024].\nRoughly speaking, X-ray crystallography determines the position and arrangement of atoms in a single crystal of the target protein by examining the diffraction intensity obtained with X-rays. Given high-quality protein crystal, X-ray crystallography is capable of generating atom-level static structures, regardless of molecular weight of the samples.\nNMR is a fundamentally different technique for protein structure determination. The analysis is performed on a solution of the target protein with high purity and high concentration to obtain characteristic NMR signals, which are interpreted by computer-aided methods to determine the 3D structures. The most notable feature of NMR is that it allows us to obtain the dynamic structure of the target protein in its natural state in solution (without crystallization). However, the measurement is complicated, requires computational interpretation (making NMR an indirect method of structure determination), and is not applicable to large molecules or samples without pure and highly-concentrated solutions.\nCryo-EM uses a mechanism called electron scattering, where electron beams pass through an instantly cooled protein solution, scatter into a lens, and are then converted into a series of 2D images on a detector. These images are processed by reconstruction software to obtain the 3D structure. Cryo-EM is becoming increasingly popular because it works well with larger proteins, has an easier sample preparation process, and yields 3D structures that are much closer to their native state than those obtained from X-ray crystallography."}, {"title": "2.1.4 Binding Affinity Measurement", "content": "Roughly speaking, binding affinity measures how \"tightly\" a ligand binds to a target protein, which is determined with radioligand-binding experiments [Haylett, 2003]. A radioligand-binding experiment can produce two kinds of binding affinity data: (1) binding constants such as dissociation constant (Ka) and inhibition constant (Ki), which are usually interchangeable in the context of protein-ligand binding; and (2) concentration terms like the half maximal inhibitory concentration (IC50) or half maximal effective concentration (EC50). In binding affinity databases, each protein-ligand complex is typically associated with one or more binding data of Ka, Ki, and IC50, with rare exception. In certain databases and studies, like the \"refined set\" in PDBbind, complexes with concentration terms only will be filtered out for better data quality. This is due to the fact that IC50 or EC50 values depend on the radioligand concentration and can vary between different experiments, unlike constant terms. If necessary or useful, the inhibitory constant Ki value can be estimated from IC50 via the Cheng-Prusoff equations:\n$K_i = \\frac{IC_{50}}{1+\\frac{[A]}{EC_{50}}}$\nwhere [A] is a fixed concentration of the ligand. However, the estimation is prone to inaccuracy with high or low concentration values [Lazareno and Birdsall, 1993]."}, {"title": "2.2 Binding Affinity Datasets", "content": "PDBbind. Starting in 2004, this dataset has been updated annually [Wang et al., 2005, Liu et al., 2015, 2017]. It collects not only protein-ligand complexes but also protein-protein, protein-nucleic acid, and nucleic acid-ligand complexes from the Protein Data Bank (PDB) [Berman et al., 2000], without any restrictions on resolution, binding data, or structural measurement techniques. An important feature of PDBbind is the refined set, selected based on (1) the quality of the complex structure, (2) the quality of binding data, and (3) the nature of the complex (molecular weight, atom types, surface area, etc.). See the supporting material in [Liu et al., 2017] for more details on the selection criteria. However, according to the authors, the refined set, despite its much higher quality compared to the general set, should not be regarded as a high-quality dataset but rather as a collection of complex samples that lack any obvious problems. With the increasing number of data points, subsets with quality control, and easy accessibility, PDBbind is becoming one of the most used datasets in the research of protein-ligand binding affinity, if not the most used.\nCASF (Comparative Assessment of Scoring Functions). CASF [Cheng et al., 2009, Li et al., 2018, Su et al., 2018] was proposed along with the development of PDBbind. Originally named the core set in PDBbind, CASF was selected from the refined set and serves as the test set/benchmark for all the protein-ligand binding predictor. To ensure that the samples in the test set are diverse but not redundant, all complexes in the refined set are first clustered based on sequence similarity. Then, for each cluster, three complexes, each with high, medium, and low binding affinity, are selected for inclusion in CASF. With this type of data selection, we can not only test regression on binding affinity (scoring) but also ranking (three different complexes in the same cluster). Additionally, with the given decoys of poses and ligands in CASF, we can evaluate the docking and screening power of a predictor. Overall, CASF provides researchers with a comprehensive and easy-to-use benchmark for protein-ligand binding affinity predictors.\nBinding MOAD (Mother of All Databases). Binding MOAD [Hu et al., 2005, Ahmed et al., 2015, Smith et al., 2019] is another commonly used dataset for binding studies. Proposed in 2005 and updates annually, Binding MOAD is a subset of PDB, aiming to be the largest collection of high-quality protein-ligand complexes annotated with experimentally determined binding affinity. Despite sharing the same primary data source (PDB) and a similar goal, Binding MOAD has much in common with PDB. The main differences that set these two apart for users are: (1) Binding MOAD sets the data selection criteria somewhere between PDBbind's general set and refined set. Notably, Binding MOAD only contains valid protein-ligand complexes with crystal structures of 2.5 \u00c5resolution or better. Additionally, protein sequences are clustered to avoid redundancy of the data, similar to the refined set in PDBbind. (2) Binding MOAD contains complexes"}, {"title": "3 Methodology", "content": "Next, we discuss the various protein-ligand binding affinity prediction methodologies as developed from the early 2000s to the present day. We divided all the methods into two categories based on the model complexity and input data. In roughly chronological order, they are: (1) Conventional methods, which are essentially a set of equations derived from assumptions and understanding of the binding process, combining weighted physio-chemical terms in an additive manner into a single estimate for affinity. (2) Machine learning models (e.g., random forest, support vector machine, neural network, etc.) trained with human-engineered descriptors extracted from the protein-ligand complexes; and representation-learning methods, which extract features directly from raw data of protein-ligand complexes (e.g., SMILES strings, voxels, graphs) using the learning capacity of deep neural networks. Roughly speaking, as the study of binding affinity advances, the community has shifted from methods that require extensive domain knowledge and assumptions to those that can exploit the increasing number of available protein-ligand complex structures."}, {"title": "3.1 Conventional Methods", "content": "Ever since the formalization of the concept of protein-ligand docking, researchers have been trying to predict the binding affinity or energy based on human understanding of physics and chemistry. These methods were often called scoring functions, rather than predictors or models, because they almost always tool the forms of additive functions, combining various engineered physio-chemical terms. Traditionally, these conventional scoring functions have been roughly divided into the following three categories, in multiple works [Cheng et al., 2009, Ballester and Mitchell, 2010a, Ashtawy and Mahapatra, 2012]:"}, {"title": "3.1.1 Physics-based methods", "content": "Traditionally, these methods are often referred as force-field-based methods, because \"classical force-field-based scoring functions assess the binding energy by calculating the sum of the non-bonded interactions\" [Meng et al., 2011], which is often done with force field. However, Liu and Wang [2015] suggested the change of name from force-field methods to physics-based methods because some of the methods or models commonly used in this category of scoring functions, such as Poisson-Boltzmann or Generalized Born continuum solvation models, end-point approximation, and quantum mechanics computation, are not based on force fields. The new name, physics-based methods, more appropriately reflects the theoretical basis of this approach. In sum, physics-based methods calculate the binding energy by adding up the Van der Waals, electrostatic, hydrogen bonding (which could be included implicitly in electrostatic terms), and desolvation energy terms [Liu and Wang, 2015]. The general functional form of physics-based methods is shown in the following equation:\n$AG_{binding} = \\Delta E_{vdw} + \\Delta E_{electrostatic} + (\\Delta E_{H-bond}) + AG_{desolvation}$\nPhysics-based methods take advantage of their ab initio nature, where no additional information about the protein and ligand is required other than the binding pose. The interactions can be described using molecular force fields or quantum chemistry calculations.\nPhysics-based methods for binding affinity prediction consider detailed atomic interactions and capture subtle effects like solvation, entropic contributions, and conformational flexibility. These methods offer mechanistic insights into binding processes, identifying key residues and interactions critical for binding. They can also account for complex cases involving significant ligand and protein conformational changes. When accurately parameterized, these methods can reliably provide energy landscapes for novel compounds, helping to understand the kinetics and thermodynamics of binding [Kuhn et al., 2020, Guterres and Im, 2020, Fu et al., 2022].\nHowever, these methods are computationally intensive and time-consuming, often requiring substantial computational resources and long simulation times [Guterres and Im, 2020]. Simulation results are also sensitive to the choice of force fields, simulation parameters, and initial conditions [Fu et al., 2022]. Their application is often limited to small to medium-sized systems due to the high computational cost. Additionally, achieving convergence in simulations can be very difficult, which can affect the reliability of binding affinity predictions [Fu et al., 2022]."}, {"title": "3.1.2 Empirical methods.", "content": "Empirical methods are the most commonly used score-based models, found in packages such as Rosetta and Autodock [Rohl et al., 2004, Trott and Olson, 2010]. In addition to the binding energy terms from the previous physics-based methods, they include contributions from empirical chemistry factors, such as hydrophobicity, metal-ligand interactions, entropy effects from steric hindrance, and ligand motifs [Rohl et al., 2004, O'Meara et al., 2015, Park et al., 2016, Alford et al., 2017, Halgren et al., 2004, Friesner et al., 2006, Quiroga and Villarreal, 2016]. These methods employ statistical learning algorithms such as multivariate linear regression (MLR) or partial least squares regression (PLS) to parameterize the individual energy terms with weights to estimate the binding affinity. Generally speaking, empirical methods are more flexible and adaptive, allowing users to add customized terms to the scoring function. They adopt a generalized functional form such as:\n$Score = E_{bind} + E_{emp}$\nwhere Ebind is binding affinity from the physics model and Eemp represents empirical terms that account for interactions and contributions that pure physics fails to capture due to the limitations of force field"}, {"title": "3.1.3 Knowledge-based potential methods.", "content": "As the name suggests, knowledge-based potential methods rely on learning from a database of protein-ligand complexes to determine the potential between atom pairs and predict binding affinity. The main assumption behind these methods is that if an atom pair, one from the ligand and one from the protein, appears with a higher frequency than the reference distribution, it might indicate an energetically favorable interaction between the given pair. Thus, the distance-dependent potential of an atom pair can be obtained through inverse Boltzmann analysis based on the measured occurrence frequency of this pair across the entire knowledge base.\nIn some works, these methods are also referred to as knowledge-based methods [Gohlke et al., 2000] or potential of mean force methods [Su et al., 2009]. The general functional form of knowledge-based potential methods is represented by the following equation:\n$Score = \\sum_{i \\in ligand} \\sum_{j \\in protein} -k_B T ln \\left( \\frac{p_{ij}(r)}{p_{ij}} \\right)$\nwhere kB is the Boltzmann constant, T is the absolute temperature, and r is the distance between atom pairs. The knowledge-based methods are more general, implicitly incorporating effects not fully understood from the structural data. Additionally, they can also incorporate some of the physics-based or empirical terms to enhance performance.\nKnowledge-based potential methods have the the following advantages: (1) They are computationally simple. (2) To obtain the distributions of atom-pair occurrences, these methods can take advantage of complex structures without measured binding affinity. Their disadvantages are as follows: (1) The distribution of atom pairs is only relevant to the binding pose rather than binding affinity, which theoretically makes knowledge-based potential methods inferior to other methods. However, in practice, this drawback does not seem to hurt performance, as knowledge-based potential methods are comparable to others. (2) The distribution statistics obtained from the knowledge base are not accurate and should not be assumed to be in a Boltzmann distribution. This means that the potential terms in the functional forms are only approximations."}, {"title": "3.1.4 Hybridization of these three methods", "content": "Efforts have been made to bring different categories of scoring functions together to improve performance, blurring the boundaries between these categories. Some physics-based methods use weight parameters derived from regressions to increase performance. Additionally, some knowledge-based potential methods add solvation and entropy terms [Liu and Wang, 2015].\nMultiple reviews and comparative studies have been conducted to access the convectional scoring functions. A comparative study of 16 popular conventional scoring functions by Cheng et al. [2009] indicates that no single one consistently outperforms the others in scoring, ranking, and docking. The Pearson correlation between the scores and the experimentally determined binding affinity of the most commonly used scoring functions, such as Glide [Halgren et al., 2004], AutoDock, and Dock, ranges from 0.4 to higher 0.6. This indicates a noticeable correlation and provides some justification for using docking, but it is not sufficient to be applied in virtual screening and drug discovery with high confidence. However, despite the sub-par performance in binding affinity prediction, multiple studies have found that docking software with conventional scoring functions can accurately predict the correct binding pose and conformation (60-80%) [Cheng et al., 2009, Plewczynski et al., 2011, Li et al., 2014]. This accuracy is not typically achieved by descriptor-based scoring functions [Xie and Hwang, 2014].\nConventional scoring functions were proposed in the early stages of research on structure-based virtual screening and are widely used in almost all commercial and academic docking software. These scoring functions are still useful and relevant to research on protein-ligand binding today. Most of them are based, to some extent, on the theory of physics and chemistry, which makes them not only reliable and explainable to"}, {"title": "3.2 Machine learning-based methods.", "content": "The high cost of conventional physics-based approaches, as implemented in tools such as Openeye, leads to growing interest in machine-learning approaches to binding affinity prediction. The scientific community has primarily pursued interaction-free and interaction-based models, as we discuss in the next two subsections. These methodologies are distinguished by whether or not they rely on physical interactions to inform predictions."}, {"title": "3.2.1 interaction-free approach", "content": "Interaction-free models infer binding affinity from data without focusing on direct physical interactions. Specifically, these ML-based models consist of two separate parts, each aiming to learn representations from molecule and protein data, including SMILES, protein sequences and graphs. In this approach, the interactions between proteins and ligands are implicitly captured in the latent spaces of their embeddings, which are formed through a neural network that processes their concatenated representations.\nEarly models in this field used 1D sequence-level representations, such as SMILES for molecules and protein sequences for proteins [\u00d6zt\u00fcrk et al., 2018, Abbasi et al., 2020, Wang et al., 2021, Yang et al., 2021, Zeng et al., 2021, Zhao et al., 2022, Yuan et al., 2022]. These early models were innovative but had limitations because they did not include important information about the 3D structures of molecules and proteins. This lack of 3D information often led to less accurate predictions of binding affinities.\nWith the development of graph neural networks (GNNs), researchers began using more advanced represen- tations. They started by representing molecules as graphs, which better captured the relationships between atoms, while still using linear sequences for proteins [Nguyen et al., 2021, Yang et al., 2022]. This change allowed for more accurate modeling of molecular structures and improved the models' predictions.\nAs GNNs improved, researchers began representing complex protein 3D structures as graphs too. This approach, where both proteins and molecules are represented as graphs, has greatly enhanced the models' ability to capture the details of molecular interactions [Somnath et al., 2021, Jiang et al., 2022, Wang et al., 2023, Liu et al., 2024b]. By including spatial information, these models give a more complete view of protein-ligand complexes, leading to better predictions of binding affinity."}, {"title": "3.2.2 interaction-based approach", "content": "Interaction-based models make predictions based on the 3D structures of complexes and the physical interac- tions between proteins and ligands. These models employ only the atoms surrounding the interaction/binding pocket to build graphs for prediction. Interaction-based methods are primarily dominated by 3D voxel grids and graphs, relying on 3D CNNs and GNNs, respectively. 3D voxel grid-based methods [Jim\u00e9nez et al., 2018, Li et al., 2019, Hassan-Harrirou et al., 2020, Jones et al., 2021] use a 3D voxel grid to represent the 3D structures of protein-ligand complexes as input features for CNNs. However, the 3D voxel grid representation has several limitations. First, it has high memory consumption and computational cost due to the sparsity of the voxels, as the protein structure occupies only a small part of the entire grid. Additionally, the sensitivity of 3D grids to rotation negatively impacts prediction results. In contrast, graph-based methods [Jiang et al., 2021, Li et al., 2021, Moon et al., 2022, Yang et al., 2023, Yu et al., 2023] are mostly rotation-invariant, making their graph representations more robust than grid representations. However, interaction-based models are limited to known protein-ligand complex structures, making them less useful compared to interaction-free methods when faced with unknown protein-ligand complexes."}, {"title": "4 Evaluation", "content": "Different from most numeric regression problems, the evaluation of binding affinity prediction is much more complex than simple error assessment, which is pointed out repeatedly in multiple comparative studies and benchmarking papers [Cheng et al., 2009, Ashtawy and Mahapatra, 2012, Li et al., 2014, Khamis and Gomaa, 2015, Liu et al., 2017, Su et al., 2018]. Evaluation by ranking with decoy ligands and binding poses is necessary to simulate the process of virtual screening, and hereby make sure of the model's practical usefulness. Cheng et al. [2009] evaluated scoring functions or binding affinity predictors from three perspectives: scoring power (binding affinity prediction), docking power (binding pose prediction), and ranking power (ligands relative ranking prediction). In a more recent study by the same group, Li et al. [2014] enriched the evaluation by adding screening power (discrimination of true binders from decoys) to the set.\nIn the following subsections, we review the concepts, procedures, and metrics of all four aspects in the evaluation of protein-ligand binding affinity predictors."}, {"title": "4.1 Scoring Power", "content": "Scoring power [Su et al., 2020] refers to the ability to produce binding scores that are correlated to the experimentally measured affinities, preferably in a linear fashion. For conventional scoring functions, the scoring power could be measured with Pearson's correlation coefficient (R) or with standard deviation (SD):\n$R=\\frac{\\sum(x_i - \\bar{x})(y_i \u2013 \\bar{y})}{\\sqrt{\\sum(x_i - \\bar{x})^2 \\sqrt{\\sum (y_i \u2013 \\bar{y})^2}}}$\n$SD = \\sqrt{\\frac{\\sum[y_i \u2013 (ax_i + b)]^2}{N-1}}$\nwhere xi is the prediction for the ith complex in the evaluation set, yi is the experimental binding affinity, a, b are linear regression factors between scores and binding affinity, and N is the number of samples in the evaluation set.\nHowever, machine learning methods can often estimate the binding affinity directly, instead of producing a score. In these cases, common regression metrics such as MAE, MSE or RMSE can also be used. Still, Pearson's correlation coefficient (R) seems be the most commonly used one for its simplicity and invariance to scaling and unit. There exists other statistical indicators that reflects the linear correlation between the"}, {"title": "4.2 Docking Power", "content": "Docking power refers to the ability that a model can tell the true binding pose from the decoy poses of a given protein-ligand pair. Ideal, the complex with native binding pose should be the one with the highest score or predicted affinity, compared to all the decoys. To implement such evaluation, complexes with decoy binding poses are usually generated with molecular docking program or through molecular dynamics simulation. In the later versions of CASF [Li et al., 2014], they used multiple docking programs such as AutoDock [Huey et al., 2012], LigandFit [Venkatachalam et al., 2003], GOLD [Verdonk et al., 2003], Surflex [Jain, 2003], and FlexX [Schellhammer and Rarey, 2004] for decoy pose generation, to minimize the bias in conformation sampling. The native/true pose is explicitly added into the decoy set so that there is at least one correct binding pose in CASF, according to Cheng et al. [2009]. After the scores/affinities for all decoy poses are generated, we compare the pose with the highest score/affinity against the true pose using RSMD, which is calculated with:\n$RMSD = \\sqrt{\\frac{\\sum[(x_i - x'_i)^2 + (y_i - y'_i)^2 + (z_i - z'_i)^2]}{N}}$\nwhere (xi, Yi, Zi) and (x'i, Yi, zi) are the coordinates of the ith atom in the true and predicted decoy poses separately.\nLower RMSD is more desirable because it means that the predicted pose is closer to the native one. Therefore, we consider a model successfully predicted the docking pose for a protein-ligand pair if the RMSD between the true and predicted poses is below a given cutoff. In CASF, the docking power of a model is evaluated by the success rate under different cutoffs (1.0, 2.0, and 3.0 \u00c5) for RMSD.\nOne can imagine there are multiple ways to evaluate the docking power. For example, the RMSD between the native and predicted poses can be directly used for assessment. Alternatively, we can simply use the accuracy for predicting the complex in native binding pose with the highest score/affinity. However, due to the resolution and different structure measurement techniques, the native poses obtained experimentally might not be the truly optimized ones. Also, we have to take the flexibility of proteins into account. Therefore, using a static pose directly without a cutoff might cause misleading evaluation results.\nAside from the aforementioned evaluation issue, RMSD, as a metric for distance deviation, has been questioned in multiple independent studies for its drawbacks, such as sensitiveness to the atom ordering, and unawareness of the flexibility of protein structures. Li et al. [2014] used RMSDPM which matches atom pairs between two poses using atom types instead of the atom IDs. Damm and Carlson [2006] proposed WRMSD, a weighted alternative to RMSD that takes into account the flexibility of proteins, which was then recommended by Carlson [2016] as a better alternative to naive RMSD."}, {"title": "4.3 Ranking Power", "content": "Ranking power refers to the ability that a model can correctly rank the know ligands by their binding affinity to the given target protein, usually with the provided poses.\nIn the ideal case, a binding affinity scoring function/predictor should rank these three ligands accordingly, and therefore the evaluation metrics could be one of Spearman's rank correlation coefficient (\u03c1) or Kendall's"}, {"title": "4.4 Screening Power", "content": "Screening power [Guedes et al., 2018] refers to the ability of a model to tell a true ligand binder from decoy molecules for a given protein target. For the latest version of CASF, there are 57 target proteins and 285 ligands. Each protein has five unique positive ligands, and the other 270 ligands are used as decoy molecules. For each protein and each decoy molecule, the authors have collected 100 potential binding poses from various softwares. The tested scoring function or binding affinity predictor will be applied to every pose, and the highest score/affinity will be considered as the score/affinity of the this protein-ligand pair. And finally, the screening power of the model is tested by ranking all the ligands in the descending order of their score/affinity and see if all five true ligands are ranked in the top, which is characterized with enhancement factor (EF), computed as:\n$EF_{\\alpha} = \\frac{NTB}{ \\alpha \u00b7 NTB_{total}}$\nwhere \u03b1 is an arbitrary percentage threshold, typically 1%, 5%, or 10%; NTB is the number of true binders found among the top \u03b1 candidates; and NTBtotal is the total number of true binders for a single target protein, which in CASF's case is 5. A major problem with CASF's approach is that they assumed that random ligands in the dataset do not bind with a target protein, which is often not the case. In fact, Su et al. [2018] report that 21 of the 57 target proteins have more than five binding ligands in the dataset according to known binding data in ChEMBL, which makes CASF's evaluation for screening power rather questionable. Moreover, using random molecules as non-binders in a decoy set is a common practice in the research of protein-ligand binding affinity and drug-target interaction, which could potentially hinder the learning and validation for the models.\nAmong all four evaluation aspects, scoring power, on the surface level, seems to be the dominant factor over the other three. Essentially, docking power, ranking power, and screening power are evaluated with the scores/affinities predicted from the model. Thus, it is completely reasonable to assume that a binding affinity predictor with good scoring power might also perform well on the other three evaluations. However, it"}, {"title": "5 Conclusion", "content": "We have provided an extensive review of binding affinity prediction methodologies", "types": "conventional methods, traditional machine-learning-based methods, and deep-learning-based methods. Conventional methods rely heavily on established chemical knowledge and physics-based calculations, often resulting in rigid models that perform well only in specific cases. In contrast, traditional ML methods improve prediction accuracy by leveraging human-engineered features, while ML methods, which have gained prominence more recently, aim to automate feature extraction and can potentially outperform other methods as more data becomes available.\nWe also discussed challenges in binding affinity prediction, such as the limitations of available datasets, which often contain biased or insufficient data, and the inherent complexity of protein-ligand interactions that make accurate predictions difficult. We emphasize the need for more robust datasets and models that can handle the dynamic nature of protein-ligand interactions.\nOur review also leads us to identify five important areas for further research, as follows. 1) Overcome limitations in existing datasets. Current datasets are often biased towards protein-ligand pairs with favorable binding constants and correct poses, while complexes with low affinity or failed bindings are underrepresented. Future research could focus on creating more balanced datasets that better represent the full spectrum of binding affinities. 2) Integrate physics-based models with ML/DL. There is potential for hybrid models that combine"}]}