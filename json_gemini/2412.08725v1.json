{"title": "A quantum-classical reinforcement learning model\nto play Atari games", "authors": ["Dominik Freinberger", "Julian Lemmel", "Radu Grosu", "Sofiene Jerbi"], "abstract": "Recent advances in reinforcement learning have demonstrated the potential of quantum learn-\ning models based on parametrized quantum circuits as an alternative to deep learning models.\nOn the one hand, these findings have shown the ultimate exponential speed-ups in learning\nthat full-blown quantum models can offer in certain artificially constructed environments.\nOn the other hand, they have demonstrated the ability of experimentally accessible PQCs to solve\nOpenAI Gym benchmarking tasks. However, it remains an open question whether these near-term\nQRL techniques can be successfully applied to more complex problems exhibiting high-dimensional\nobservation spaces. In this work, we bridge this gap and present a hybrid model combining a PQC\nwith classical feature encoding and post-processing layers that is capable of tackling Atari games.\nA classical model, subjected to architectural restrictions similar to those present in the hybrid\nmodel is constructed to serve as a reference. Our numerical investigation demonstrates that the\nproposed hybrid model is capable of solving the Pong environment and achieving scores compa-\nrable to the classical reference in Breakout. Furthermore, our findings shed light on important\nhyperparameter settings and design choices that impact the interplay of the quantum and classical\ncomponents. This work contributes to the understanding of near-term quantum learning models\nand makes an important step towards their deployment in real-world RL scenarios.", "sections": [{"title": "1 Introduction", "content": "Reinforcement learning (RL) has had a profound impact on the machine learning (ML) landscape,\nthanks to the integration of deep neural networks (DNNs). Deep RL led to a series of advancements\nin mastering complex decision-making tasks in Atari games [1,2], Go [3,4], StarCraft II [5], and Dota\n2 [6], where agents have surpassed human-level performance. However, despite its capabilities, RL\nremains one of the most computationally demanding areas within ML, and would substantially benefit\nfrom enhancements in computational efficiency.\nIn the pursuit of more efficient RL methods, quantum computing (QC) is a potential remedy. QC is\na paradigm for computing in which information is processed based on the principles of quantum me-\nchanics [7]. By harnessing quantum mechanical phenomena such as superposition and entanglement\nand using them as computational resources, QC tries to gain computational advantages in certain\nproblem classes. Here, quantum machine learning (QML) based on variational quantum algorithms\n(VQAs) [8] is a promising application, compatible with current noisy intermediate-scale quantum\n(NISQ) devices [9]. VQAs optimize parametrized quantum circuits (PQCs), quantum circuits com-\nposed of quantum gates that can be tuned via classical hardware to achieve a desired computational\noutcome. Similarly to classical DNNs, the PQC acts as a function approximator that is trained via\nclassical optimization techniques, in a hybrid quantum-classical manner. PQC-based QML models and\nhybrid approaches that leverage the strengths of both classical and quantum components have already\ndemonstrated significant results in both supervised [10\u201315] and unsupervised [16\u201319] machine learning\ntasks."}, {"title": "1.1 Contributions", "content": "In this work, we propose a hybrid quantum-classical model and analyze its performance in RL envi-\nronments with high-dimensional observation spaces. As a testbed, we consider the Atari 2600 game\nenvironments Pong and Breakout, often used as benchmarks for deep RL [32]. Our main contribution\nis to show that a hybrid quantum-classical model is capable of solving the Pong environment according\nto its specifications [33] and reaching a performance similar to a classical reference in the game Break-\nout. This is in stark contrast with the findings of [28], which reported that hybrid models are unable\nto learn in the Atari environments due to a lack of expressivity. Our second contribution is an in-depth\nanalysis of the design choices that are responsible for an increase in performance. In particular, we find\nthat rescaling rewards and adjusting the learning rate in the output layer can have a notable impact\non learning performance, which we relate to the different nature of Q-function landscapes that hybrid"}, {"title": "1.2 Related work", "content": "The first efforts to use PQC-based models as function approximators in RL were made in [20], where\nquantum agents demonstrated learning capabilities in the simple benchmark environment Frozen Lake\nfrom the OpenAI Gym [34] using an approximate Q-learning approach. Similarly, [21] benchmarked\nPQCs as Q-function approximators in CartPole and Blackjack, proposing different encoding schemes\nand confirming that PQC models can perform comparably to DNNs. In [22], PQCs were successfully\nused as policy function approximators to solve CartPole, MountainCar, and Acrobot, highlighting\ndata re-uploading [35,36] and trainable scaling weights on inputs and observables as key design choices.\nConcurrently, [23] adopted a similar architecture for Q-learning in FrozenLake and CartPole, stress-\ning the importance of trainable weights on observables for matching the range of Q-values. In [22]\nand [23], the authors demonstrate a theoretical quantum advantage of quantum agents over classi-\ncal learners in both policy- and value-based reinforcement learning settings, building on results on\nsupervised learning based on the discrete logarithm problem [37], while the former also provides nu-\nmerical evidence of an empirical advantage over DNNs in specifically crafted PQC-based environments.\nMost of these studies deal with environments characterized by low-dimensional observations. However,\n[29] tackled a 20 \u00d7 20 dimensional maze environment, using a hybrid model with a classical DNN to pre-\nprocess inputs, demonstrating adaptability to higher-dimensional problems. Likewise, [30] introduced a\ntensor-network-based variational quantum circuit (TN-VQC) architecture for dimensionality reduction\nof high-dimensional inputs. A step closer to real-world problems is taken in [28] extending previous\nwork [21] by proposing a hybrid model combining a classical convolutional network and quantum\ncircuits. The authors study the performance of the hybrid architecture in the Atari games Pong and\nBreakout, where an observation consists of approximately 100,000 variables. Although the hybrid\nmodel achieves non-trivial results in CartPole, it fails to learn in the high-dimensional Atari game\nenvironments. In our work, we draw inspiration from several of these studies, particularly those that\nexplore hybrid quantum-classical models. We overcome the shortcomings of [28] and demonstrate that\nhybrid quantum-classical agents can successfully learn in Pong and Breakout."}, {"title": "2 Quantum reinforcement learning", "content": "This section gives a concise introduction to quantum computing and its application in machine learning.\nIt further introduces the hybrid quantum-classical model used as a Q-function approximator and its\nimplementation in the approximate Q-learning framework."}, {"title": "2.1 Quantum computing", "content": "The basic unit of quantum information is a qubit, a two-level quantum system represented by a two-\ndimensional complex Hilbert space $\\mathcal{H} = \\mathbb{C}^2$. The two orthogonal states $|0\\rangle = (1,0)^T$ and $|1\\rangle = (0, 1)^T$\nform a basis of this space and are referred to as computational basis states\u00b9. A general state of a\nqubit is described by a unit vector $|\\psi\\rangle := a |0\\rangle + b|1\\rangle \\in \\mathcal{H}$, where $a, b \\in \\mathbb{C}$ satisfy $|a|^2 + |b|^2 = 1$. This\ngeneralizes to $n$ qubits, where the combined Hilbert space for an $n$-qubit system is $\\mathcal{H}^{\\otimes n}$ and a general\nstate is described by a superposition of all $2^n$ possible combinations of its qubits' basis states. The\nstates of qubits are manipulated through unitary operators $U$ acting on $\\mathcal{H}^{\\otimes n}$, called quantum gates.\nAn important set of single-qubit gates are the Pauli gates denoted as $X$, $Y$ and $Z$ that give rise to the\nPauli rotation gates $R_X$, $R_Y$ and $R_Z$, which are famous examples of parameterized gates:\n\u00b9 We use Dirac-notation, where vectors and their conjugate transpose are denoted as $|\\cdot\\rangle$ and $\\langle\\cdot|$ respectively and\ntheir inner product as $\\langle\\cdot|\\cdot\\rangle$."}, {"title": "2.2 The hybrid quantum-classical model", "content": "Having introduced quantum computing we now move to its application in ML by defining the hybrid\nquantum-classical model as illustrated in Figure 1. Given an initial state $|\\psi\\rangle$ of an $n$-qubit quantum\nsystem, for example the trivial state $|0\\rangle^{\\otimes n}$, a PQC applies the parameter-dependent unitary transfor-\nmation $U(x, \\theta)$ to its qubits, which is defined as\n$U(x, \\theta) = \\prod_{l=1}^{L} V_l(\\theta)U_l(x)$,\nwhere the $U_l(x)$ encode parts of the feature vector $x \\in \\mathbb{R}^d$ into the quantum state using $R_X$ gates.\nThe $V_l(\\theta)$ consist of $R_X$, $R_Y$ and $R_Z$ gates that depend on adjustable parameters $\\theta \\in \\mathbb{R}^k$ that are\noptimized during the training process by classical hardware. The repeated encoding of the feature\nvector $x$, referred to as data re-uploading [35], is known to enhance the expressivity of PQC-based\nmachine learning models [36]. The state of the system resulting from the application of the circuit is\n$U(x, \\theta) |0\\rangle^{\\otimes n} = |\\psi(x, \\theta)\\rangle$. The expectation value of some measurement observable $M$ on the quantum\nsystem prepared by the PQC is\n$\\langle M \\rangle_{x,\\theta} = \\langle\\psi(x, \\theta)|M|\\psi(x,\\theta)\\rangle =: f_{\\theta}(x)$."}, {"title": "2.3 Hybrid model architecture", "content": "In this section, we elaborate on the specific architecture of the hybrid model used in our experiments.\nOur hybrid model design is motivated by the limitations of encoding raw images directly into a quantum\ncircuit. To address this, we apply classical convolutional layers, which are an effective dimensionality\nreduction technique in reinforcement learning, as demonstrated in the seminal work of Mnih et al. [1].\nThis approach allows us to extract fewer but richer features from high-dimensional observations before\nencoding them in the PQC. As depicted in Figure 1, three convolutional layers together with a fully\nconnected layer serve the purpose of dimensionality reduction and feature extraction. Together, these\nlayers take the role of $L_{win}$ in the notation established in Equation 5. The number of neurons at\nthe output of the fully connected layer coincides with the number of encoding gates in the PQC, as\neach neuron outputs a different latent feature. We refer to this layer as the pre-processing layer and\nexplain in more detail why we use only linear activations in this layer in Appendix G. The amount of\nfeatures we can encode in the PQC is constrained by the limited number of qubits and encoding gates\nthat we can handle computationally in our classical simulations. This in turn restricts the number of\noutput neurons we can accommodate at the pre-processing layer. However, no such restriction exists\nfor classical models conventionally used in RL, where the convolutional layers are typically followed\ndirectly by a wide non-linear processing layer. To allow for a fair assessment of the hybrid model's\nperformance when compared to a classical counterpart, we introduce an artificial bottleneck in our\nclassical reference models. We do so by adding an equivalent pre-processing layer with the same num-\nber of neurons as in the hybrid model. The exact specifications of the convolutional layers of both the\nhybrid and classical reference model as well as the architecture of the classical model are detailed in\nAppendix B.\nThe design of a PQC remains an area of active research, and a standard architecture has not yet\nbeen established. In this work, we choose to build on previously successful approaches for QRL in\nlower-dimensional observation spaces [23]: The architecture consists of multiple layers, each composed\nof one variational unitary (consisting of single-qubit $R_X$, $R_Y$ and $R_Z$ gates along with entangling $CZ$\ngates) followed by one feature encoding unitary (a set of single-qubit $R_X$ rotations). This sequence\nof variational and encoding unitaries is repeated across all layers with a final variational unitary at\nthe end of the circuit. The general structure of the PQC is shown in Figure 2 and a more detailed\ndescription of the complete hybrid model is given in Appendix A. The layerwise encoding of features\nleverages data re-uploading, which has been shown to greatly increase model expressivity [35,36]. In\nour hybrid model, this technique takes on a unique form: instead of raw observations, the inputs to the\nPQC are trainable features produced by the preceding convolutional and fully connected layers. This\nadds to the model's flexibility and ability to learn complex functions. The output of the PQC are the\nexpectation values of each qubit as obtained by local (i.e., qubit-wise) Pauli-Z measurements. Unlike\nprior work by Skolik et al. [23], where each observable together with a single trainable parameter is"}, {"title": "2.4 Quantum Q-learning", "content": "Having established the architecture of our hybrid quantum-classical model, we now turn to its applica-\ntion in the RL framework, specifically within the Q-learning paradigm, which serves as the foundation\nfor training and evaluating our model. RL is a field of ML where an agent interacts with an environ-\nment by taking actions to maximize cumulative rewards [38]. At a given time step $t$ the agent observes\na state $s_t \\in \\mathcal{S}$ and selects an action $a_t \\in \\mathcal{A}$ based on a policy function $\\pi(a|s)$ and receives feedback\nin the form of numerical rewards $r_t \\in \\mathbb{R}$, with the goal of learning an optimal policy $\\pi^*$ through trial\nand error. The agent chooses actions so that to maximize the sum of discounted future rewards. In\nparticular, it tries to maximize the expected discounted return $G_t := \\sum_{k=t+1}^{T} \\gamma^{k-t-1}r_k$, where the\ndiscount rate $0 < \\gamma < 1$ determines the relative importance of short-term versus long-term rewards.\nThe expected return when taking action $a$ while being in state $s$ and following the policy $\\pi$ thereafter,\nis defined as the Q-function,\n$Q^{\\pi}(s, a) := \\mathbb{E}_{\\pi}[G_t | s_t = s, a_t = a]$.\nThe optimal Q-function for a given state-value pair $(s, a)$ is defined as $Q^*(s, a) := \\max_{\\pi} Q^{\\pi}(s, a)$.\nFrom the optimal Q-function $Q^*(s, a)$ an optimal policy $\\pi^*$ can be derived: In state $s$, the agent\nshould choose an action that maximizes the optimal Q-function, i.e\n$\\pi^*(a|s) = arg \\max_{a} Q^*(s, a)$.\nThis observation leads to the main objective of approximate Q-learning, where the goal is to learn\nan approximation $Q(s, a; \\theta)$ of the optimal Q-function $Q^*(s, a)$ dependent on a set of parameters $\\theta$.\nA famous example is the deep Q-learning algorithm introduced in [1], where DNNs are used as Q-\nfunction approximators, labelled deep Q-networks (DQNs). Quantum Q-learning [23] is a variation of\ndeep Q-learning, where the DQN is replaced by a PQC or a hybrid model as introduced in Section 2.2.\nIn this setting, the agent follows an $\\epsilon$-greedy policy, where $\\epsilon$ decays during training. At each time\nstep $t$, the agent observes state $s_t$ and, with a probability $1 - \\epsilon$, selects action $a_t$ corresponding to the\nhighest estimated Q-value. Otherwise, it selects a random action. This behaviour balances exploration\nof the state space and exploitation of the current policy. Upon transition to state $s_{t+1}$ the agent\nobtains reward $r_t$ and the experience $e_t = (s_t, a_t, r_t, s_{t+1})$ is stored in a replay memory $\\mathcal{D}$. The replay\nmemory $\\mathcal{D}$ is initially populated with $N$ experiences following a purely random policy, allowing the\nagent to sample a minibatch $\\mathcal{B}$ of transitions randomly at each time step, thereby breaking temporal\ncorrelations. For each sampled minibatch, the Q-network is trained by minimizing the following loss\nfunction,\n$\\mathcal{L}(\\theta) = \\mathbb{E}_{(s_t, a_t, r_t, s_{t+1}) \\sim \\mathcal{B}} \\Big[r_t + \\gamma max_{a'} Q(s_{t+1}, a'; \\theta^{-}) - Q(s_t, a_t; \\theta)\\Big]^2$\nwhere the term in parenthesis is the temporal difference (TD) error. The $Q(s_{t+1}, a'; \\theta^{-})$ indicates the\nuse of a target network. It is a copy of the online network $Q(s_t, a_t; \\theta)$, but its parameters $\\theta^{-}$ are\nupdated less frequently. This technique avoids undesirable feedback loops and correlations between\nthe target and the estimated Q-values, stabilizing the learning process. The loss function in Equa-\ntion 8 is minimized through gradient descent on the DQN weights, $\\theta \\leftarrow \\theta - \\alpha \\nabla_{\\theta}\\mathcal{L}(\\theta)$, to refine the\napproximation of the Q-values. Every $C$ steps, the weights of the target network $Q^{-}$ are updated to\nmatch the weights of the online network $Q$. Gradient computation for quantum circuits on quantum\nhardware is non-trivial due to quantum state collapse upon measurement; Appendix I describes a\nmethod for obtaining gradients on real quantum computers. We refer to Appendix E for more details\non the training procedure and give an overview of all hyperparameter settings investigated in Ap-\npendix F. Further, Appendix C provides more information on the Atari environments and necessary\npre-processing steps. Algorithm 1 in Appendix D shows a pseudo-code implementation of quantum\nQ-learning with experience replay and a target network. The complete implementation is available in\nthis GitHub repository."}, {"title": "3 Model evaluation and analysis of design choices", "content": "In this section, our objectives are twofold: first, to demonstrate that our hybrid model can successfully\nlearn in the Atari environments Pong and Breakout, and second, to analyze the impact of key design\nchoices on its performance. We establish a baseline for both the quantum and classical models, showing\nthat this baseline is sufficient to solve the Pong environment and achieves non-trivial performance in\nBreakout. Building on this, we systematically examine the effect of specific design choices reward\nscaling and the latent feature space dimension on the learning performance of both models."}, {"title": "3.1 Scores with baseline settings", "content": "The scores in this setting (see 'q. baseline' and 'c. baseline' in Table 1 in Appendix F) serve as\na baseline to which we compare the performance of agents with different design choices. Figure 3\nillustrates the rewards obtained by the agent time-averaged over 10 episodes in the game of Pong (left)\nand 250 episodes in Breakout (right). As described in Section F, multiple runs were conducted for\neach environment using different random seeds. The solid lines indicate the average over these runs\nand the shaded region highlights the standard deviation. The results demonstrate that the hybrid\nmodel achieves a strong performance in both environments. Notably, in the game of Pong, the hybrid\nagent matches the classical reference in terms of final performance, achieving a mean reward of 20.\nMoreover, the hybrid model requires fewer environment steps - between 400,000 and 600,000 - to solve\nPong, compared to around 800,000 steps needed by the classical agent. This result, however, does not\nindicate quantum advantage; it is likely influenced by statistical variability due to the limited number\nof runs. For the hybrid model, 4 out of 5 runs lead to successful learning behaviour, consistently\nachieving scores above 20. In contrast, only 4 out of 11 runs with the classical reference model\nshowed successful learning. Notably, one of the successful classical runs exhibited a significant delay\nin reaching the optimal policy, which contributed to a lower average performance. The remaining runs\nwere excluded from the analysis because they failed to learn at all, indicated by constant rewards\nof -21. In Breakout, the hybrid agent achieves a mean reward of approximately 84 after around 2\nmillion episodes, which corresponds to a performance gap of about 41% compared to the classical\nreference model. Despite this gap, the hybrid model demonstrates robust learning behaviour, closely\nfollowing the trend of the classical agent. With additional design enhancements (see Figure 6), this\ngap can be significantly reduced to around 13%. This demonstrates that the hybrid model is capable of\nachieving competitive performance and effectively solving complex reinforcement learning tasks, with\nclear potential for further optimization."}, {"title": "3.2 Impact of reward scaling and post-processing layer learning rate", "content": "Having established the potential for the hybrid model to learn effective policies in both testing en-\nvironments, we now turn our attention to a more detailed analysis of how the Q-value estimates of\nthe trained models compare. To do so, we generate Q-value surfaces by plotting the three predicted\nQ-values as functions of two randomly selected inputs to the pre-processing layer, in place of the\noutput of the convolutional layers (see Appendix H for more details). Based on our observations, we\ninvestigate the impact of reward scaling and the learning rate of the post-processing layer on overall\nlearning performance. Figure 4 compares the Q-value surfaces of the hybrid and classical models. The\nleft panels show surfaces for input values within a typical range observed during gameplay, while the\nright panels expand this range to better assess the surface shapes. The colours represent the Q-values\ncorresponding to the three different actions.\nA key observation is the difference in the shape of the Q-value surfaces. The hybrid model produces\nsurfaces that are linear combinations of sinusoidal functions, reflecting the trigonometric nature of\nPQCs with rotational encoding, as analyzed in [36]. In contrast, the classical model generates ap-\nproximately piecewise linear surfaces. This difference has significant implications: We suspect that\nthe oscillatory behaviour of the hybrid model's predictions increases the likelihood of assigning high\nQ-values to suboptimal actions. This can be understood by examining the number of crossings in\nthe Q-value surfaces, which are more frequent in the hybrid model. Trigonometric functions, by na-\nture, have a higher tendency for crossings due to their oscillatory behaviour, and small changes in\ntheir parameters can lead to numerous overlaps. This effect is likely even more pronounced in the\nhigher-dimensional feature space produced by the convolutional layers. Our proposed solution to ad-\ndress this phenomenon is to upscale the rewards issued by the environment, thereby promoting greater\nseparation between Q-values associated with optimal and suboptimal actions. This approach aims to\nprovide more room for the oscillatory behaviour while reducing the chances of undesirable overlaps.\nWhile further investigation is required to fully understand this phenomenon, our results indicate that\nupscaling rewards benefits the hybrid model, leading to improved performance.\nClosely related to the magnitude of the target Q-values is the learning rate of the post-processing layer\nfollowing the PQC in the hybrid model, as it controls how quickly the model can adapt to certain\ntarget values. As discussed in Section F Table 1, different combinations of reward scaling factors and"}, {"title": "3.3 Influence of latent space dimension", "content": "A primary constraint on the performance of both the hybrid and classical model is the dimension\nof the latent space representation following the pre-processing layer. The number of latent features\nultimately limits the information accessible by the PQC. In Figure 6, we demonstrate the impact of this\nlatent space dimension on the hybrid model's performance and compare it to the classical reference. In\nconfiguration 2a, the PQC consists of 6 qubits (compared to 4 in the baseline model), each encoding\n6 features per qubit (up from 4 in the baseline). This increases the latent space dimension from 16 to\n36, more than doubling its size. Without additional modifications, this configuration achieves scores\nexceeding 100, a significant improvement over the baseline hybrid model. Building on the insights from\nSection 3.2, we next examine configurations 2b and 2c, which combine the expanded latent space with\nreward scaling and increased learning rates in the post-processing layer. Specifically, configuration 2b\nincorporates a reward scaling factor of 10 and a learning rate of 2.5e-2, while configuration 2c uses a\nscaling factor of 100 and a learning rate of 2.5e-1. These adjustments lead to further improvements,\nwith configuration 2c achieving a mean reward of nearly 150 within 2.5 million training steps, marking\nthe highest performance achieved by the hybrid model under the given constraints. For comparison,\nthe blue graph on the right side of Figure 6 depicts the classical reference model with 36 features in\nthe latent space. The improvement margin for this classical model over its baseline is similar to that of\nthe hybrid model. It is important to note that excessively increasing the latent space dimension may\nnegatively impact the hybrid model. Larger latent spaces require more qubits and deeper quantum\ncircuits to encode all the features, increasing quantum circuit complexity. This added complexity\ncan lead to trainability issues, such as the barren plateau phenomenon [39], where gradients become"}, {"title": "4 Conclusion", "content": "In this work, we have studied the performance of hybrid quantum-classical agents in RL environments\ncharacterized by a high-dimensional observation space. Specifically, we proposed a hybrid quantum-\nclassical model based on PQCs for Q-function approximation and assessed its learning performance in\nthe Atari game environments Pong and Breakout using a quantum approximate Q-learning framework.\nOur results demonstrate that hybrid models can learn effectively in these high-dimensional environ-\nments. This contrasts earlier work [28] that reported no learning capabilities in Atari environments due\nto the limited expressivity of the proposed hybrid model. Our research emphasizes the critical role of\ncertain design choices, in particular the dimension of the latent feature space generated by the classical\npre-processing layer, which ultimately controls the amount of information encoded in the PQC. Addi-\ntionally, we identified how the magnitude of Q-values affects the hybrid model's learning performance\nwhen the learning rate of the classical post-processing layer is appropriately adjusted. Our findings\nsuggest that with proper tuning, hybrid agents can closely match the performance of classical agents\nsubjected to the same constraints in the latent space, highlighting the importance of fair benchmark-\ning. The results presented contribute to our understanding of the interplay between quantum and\nclassical components in hybrid models. We believe that this work marks a significant step towards the\napplication of hybrid quantum-classical models in real-world RL scenarios. The performance of these\nmodels under the influence of noise, as present in today's quantum hardware, poses an interesting open\nquestion. Future research could explore the robustness of the proposed architecture in noisy simulators\nand, ultimately, on real quantum devices. Another interesting research direction would be the explo-\nration of different learning tasks where hybrid models could outperform fully classical models. Indeed,\nAtari games are known to be classically efficiently solvable, which leaves little room for a quantum\nadvantage. One could instead explore environments where quantum enhancements are expected, as\nin quantum chemistry [40] or combinatorial optimization [41]. This research direction would require a\ndeeper analysis of the interplay of the classical and quantum components of the hybrid model, as well\nas finding more task-specific PQC designs."}, {"title": "A Detailed architecture of the hybrid model", "content": "This section provides a detailed description of the hybrid quantum-classical model architecture comple-\nmenting the overview in Section 2.3. The hybrid model architecture comprises three main components\n(see also Figure 1 in the main text): first, classical convolutional layers together with a fully connected\nlayer for dimensionality reduction and feature extraction; second, a PQC for processing the latent\nfeatures; and third, a fully connected output layer for gathering and re-scaling expectation values.\nThe configuration of the convolutional layers in the hybrid model is identical to the classical reference\nmodel (see Appendix B). Following the convolutional layers is a fully connected pre-processing layer.\nThis layer contains $n \\times l$ neurons with linear activation, where $n$ is the number of qubits and $l$ is the\nnumber of PQC layers. This layer reduces the 3136 values produced by the convolutional layers to just\n16 or 36 latent features, depending on the setting (see Table 1). These features are directly encoded\nin the PQC."}, {"title": "B Classical reference model", "content": "Here we introduce the classical reference model used as a fair baseline for evaluating the performance of\nthe hybrid quantum-classical model. The architecture is based on the deep Q-network (DQN) from [2],\nwhich we adopt without modification for the convolutional and fully connected layers. Specifically,\nthe convolutional layers comprise three layers with the following parameters: The first layer consists\nof 32 filters of size 8 \u00d7 8 and a stride of 4, the second layer convolves 64 filters of size 4 x 4 and a\nstride of 2 and the third layer uses 64 filters of size 3 \u00d7 3 and a stride of 1. All convolutional layers use\nthe non-linear rectifier activation function (ReLU). The convolutional layers are followed by a fully\nconnected layer with 512 neurons and ReLU activation, and a final output layer with linear activation.\nIn this work, this original architecture has been modified to allow a more direct comparison with the\nhybrid quantum-classical model. One of the key limitations of the hybrid model is its restricted latent\nspace dimension, owing to the limited number of features that can be encoded in the PQC - as discussed\nin Section 2.3. To account for this, a bottleneck layer is introduced in the classical model between the\nconvolutional layers and the fully connected ReLU layer. This bottleneck layer is implemented as\na layer with a reduced number of neurons and linear activation, matching the dimensionality of the\npre-processing layer in the hybrid model. The linear activation function is chosen to minimize the\nintroduction of non-linearity, preserving the behaviour of the original architecture while introducing\nconstraints in the latent space dimension."}, {"title": "C Atari 2600 environments", "content": "The Atari 2600 game environments of Pong and Breakout, accessible via OpenAI Gym [34], were\nselected as testbeds for evaluating the hybrid quantum-classical framework. These environments are"}, {"title": "C.1 Pre-processing of the raw environment state", "content": "Before training the agent, several pre-processing steps are applied to the raw state representations of\nthe game environments. These pre-processing steps, following the guidelines in [32], are crucial for\nstandardizing the evaluation of RL algorithms on Atari 2600 games and are independent of the pre-\nprocessing conducted by the hybrid model. The raw observation space for both environments consists"}, {"title": "D Quantum Q-learning algorithm", "content": "In Section 2.4, we introduced the quantum Q-learning algorithm based on the deep Q-learning algo-\nrithm. Algorithm 1 presents a pseudo-code implementation following the notation established above."}, {"title": "E Training procedure and evaluation metrics", "content": "We evaluate the hybrid quantum-classical model within a reinforcement learning framework using\nquantum Q-learning with a target network and experience replay, following the approach of Mnih et\nal. [2]. Training starts with a random policy for 20,000 environment steps to populate the replay\nbuffer. After accumulating 100,000 (st, at, rt, st+1) transitions, the replay buffer operates on a first-\nin, first-out (FIFO) basis with uniform sampling. After this initial warm-up phase, the predictions\nof the hybrid agent are used to derive a policy. The online model is trained every 4 steps, and the\ntarget network is updated every 8,000 steps. In the baseline setting (see 'q. baseline' in Table 1), we\nminimize the loss function (Equation 8) using the Adam optimizer with a learning rate of $2.5 \\times 10^{-4}$\nfor both classical and quantum components. An $\\epsilon$-greedy policy is employed with $\\epsilon$ linearly decaying\nfrom 1 to 0.01 over 250,000 steps. The discount factor $\\gamma$ is set to 0.99. Training spans 1 million steps\nin Pong and 2.5 million steps in Breakout. The main evaluation metric employed for assessing the\nagents in this study is the total undiscounted reward obtained in each episode. These rewards exhibit\nsignificant variance across different game episodes, due to the $\\epsilon$-greedy policy and random sampling\nfrom the replay memory. For evaluation purposes, the rewards are temporally averaged, where we\ntake 10 episodes in Pong and 250 episodes in Breakout. Following the recommendations in [32], no\nevaluation runs where the agent fully exploits its learned policy (without any parameter updates) are\nconducted."}, {"title": "F Numerical experiments and hyperparameter settings", "content": "This section provides a brief overview of the hyperparameters we investigate and the software frame-\nworks used. Table 1 outlines the hyperparameter values tested and indicates the number of runs"}, {"title": "G Effect of rescaling latent features", "content": "A key design choice in the hybrid model architecture is the use of a linear activation function in the\npre-processing layer preceding the PQC. This decision aims to preserve the original latent features and\navoid introducing any transformations that might overshadow the learning process within the quantum\ncircuit. However, this approach does not impose any constraint on the magnitude of the latent features\nproduced by the pre-processing layer. Since the PQC encodes features as rotation angles, it is essential\nto ensure that the values of the latent features do not exceed a range of $2\\pi$, as otherwise two vastly\ndifferent features might be mapped to the same value due to the periodicity of the encoding scheme.\nOne common strategy to enforce this constraint is to apply a scaling activation function, such as\n$\\tanh$, which restricts the output of the pre-processing layer to values within the desired range.\nFigure 12 ("}]}