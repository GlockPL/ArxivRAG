{"title": "Interpretable Clustering: A Survey", "authors": ["Lianyu Hu", "Mudi Jiang", "Junjie Dong", "Xinying Liu", "Zengyou He"], "abstract": "In recent years, much of the research on clustering algorithms has primarily focused on enhancing their accuracy and efficiency, frequently at the expense of interpretability. However, as these methods are increasingly being applied in high-stakes domains such as healthcare, finance, and autonomous systems, the need for transparent and interpretable clustering outcomes has become a critical concern. This is not only necessary for gaining user trust but also for satisfying the growing ethical and regulatory demands in these fields. Ensuring that decisions derived from clustering algorithms can be clearly understood and justified is now a fundamental requirement. To address this need, this paper provides a comprehensive and structured review of the current state of explainable clustering algorithms, identifying key criteria to distinguish between various methods. These insights can effectively assist researchers in making informed decisions about the most suitable explainable clustering methods for specific application contexts, while also promoting the development and adoption of clustering algorithms that are both efficient and transparent.", "sections": [{"title": "1 INTRODUCTION", "content": "Cluster analysis [1], [2] is a crucial task in the field of data mining, which aims to partition data into distinct groups based on the intrinsic characteristics and patterns within the data. This process helps in uncovering meaningful structures and relationships among data points, facilitating various applications and further analysis.\nFor decades, numerous algorithms have been proposed to solve clustering problems across different applications, achieving high accuracy. However, in most cases, clustering models exist as black boxes, leading to common ques-tions such as: How are the clustering results formed? Can people understand the logic behind the formation of the clustering results? Is the model trustworthy? The clustering model's ability to explain such issues is tentatively defined as model's clustering interpretability or explainability [3]. Given that most researchers in data mining and machine learning use interpretability and explainability interchangeably, this paper will use the term interpretability throughout this paper.\nTo date, interpretability still lacks a precise or mathemat-ical definition. Different sources provide slightly varying definitions\u2014for instance, it is defined as \"the ability to explain or to present in understandable terms to a human\" in [4], \"the degree to which a human can understand the cause of a decision\" in [5], and \"make the behavior and predictions of machine learning systems understandable to humans\" in [6]. Collectively, these definitions can all capture the essence of interpretability.\nHowever, the interpretability of a model may vary depending on the user's actual needs and can manifest in different dimensions. In studies of specific diseases, physicians are often more concerned with identifying patient characteristics that indicate a higher likelihood of having the disease and whether these characteristics can assist in early diagnosis. In contrast, data scientists focus on designing interpretable models that provide compelling explanations for patients and effectively elucidate the reasons behind each patient's assignment to a particular disease type, thereby aiding in understanding the impact of various characteristics on the outcomes. Therefore, although various interpretable methods can provide different degrees of interpretability across multiple dimensions, it remains necessary to provide a systematic summary and distinction of these methods.\nAs far as we know, there have been several reviews that summarize methods related to interpretability. However, these reviews either do not focus on the clustering domain [7], [8], [9], [10], [11] or were published too early to include the latest research [12]. To fill this gap, we have comprehensively collected existing interpretable clustering methods and proposed a set of criteria to classify them, ensuring that all methods related to interpretable clustering can be categorized under one of these criteria. Furthermore, we divide the clustering process into three stages and classify all interpretable clustering methods according to their interpretability at different stages, providing the overall framework for this review: (1) the feature selection stage (pre-clustering), (2) the model building stage (in-clustering), and (3) the model explanation stage (post-clustering). We believe this review will provide readers with a new understanding of interpretable clustering and lay a foundation for future research in this area.\nThe rest of this paper is organized as follows. Section 2 discusses the need for interpretable clustering. Section 3 provides a taxonomy of interpretable clustering methods. Sections 4 to 6 review interpretable pre-clustering, in-clustering, and post-clustering methods, respectively, based"}, {"title": "2 THE NEED FOR INTERPRETABLE CLUSTERING", "content": "As artificial intelligence and machine learning algorithms become more advanced and excel in various tasks, they are increasingly being applied across multiple domains. However, their use remains limited in risk-sensitive areas such as healthcare, justice, manufacturing, defense, and finance. The application of AI systems and the underlying machine learning algorithms in these fields involves three key human roles [13]: developers, end users within the relevant domain, and regulators at the societal level. For any of these roles, it is crucial for humans to understand and trust how the algorithm arrives at its results. For instance, developers need to understand how the algorithm produces meaningful outcomes and recognize its limitations, enabling them to correct errors or conduct further assessments. End users need to evaluate whether the algorithm's results incorporate domain-specific knowledge and are well-founded. Regulators need to consider the implications of the algorithm's outcomes, such as fairness, potential discrimination, and where the risks and responsibilities lie. This necessitates transparency and trustworthiness throughout the entire algorithmic process.\nIn response to these challenges, research in interpretable machine learning has gained momentum [6]. Much of the downstream analysis is typically built at the cluster level, where clustering methods are designed to generate patterns as the initial understanding of the data. At this stage, the need for interpretability of clustering, along with the transparency of algorithmic mechanisms, becomes increasingly pronounced."}, {"title": "2.1 What is interpretable clustering?", "content": "Conventional clustering algorithms typically focus on delivering clustering results, treating accuracy and efficiency as top priorities, especially in complex, high-dimensional data. The models they employ are largely \"black boxes\u201d, particularly in the case of advanced clustering methods that often utilize representation learning techniques and deep learning. These methods consider all dimensions and feature values of the data, actively involving them in the generation of clustering results. However, the reasoning behind \"why\" and \"how\u201d these results are generated remains opaque to the algorithm designers, making it even more difficult for end users to comprehend. In contrast, interpretable clustering methods explicitly aim to explain the clustering results, enabling humans to understand why the algorithmic process produces meaningful clustering outcomes.\nAny technology or tool that enhances interpretability in clustering analysis can be categorized under the domain of interpretable clustering. A hallmark of these methods is the integration of interpretable models [14] at any stage of the clustering pipeline. These interpretable elements accompany the final clustering results, making them understandable, trustworthy, and usable by humans. Such elements may include, but are not limited to, the use of specific"}, {"title": "2.2 What is a good interpretable clustering method?", "content": "An interpretable clustering method provides clear evidence to explain how clustering results are derived, offering end users the opportunity to understand both the behavior of the algorithm and the logic behind the clustering outcomes. However, whether end users ultimately choose to trust this evidence may depend on application-driven needs or expert knowledge. As machine learning researchers and data scientists, we are primarily equipped to assess what constitutes a good interpretable clustering method from a data-driven perspective.\nFirst, the form of interpretable evidence should be as simple as possible. For instance, the number of feature values used to derive a cluster should be minimized, which greatly reduces the complexity for end users in understanding the results. Second, each cluster should contain unique and distinguishable information compared to other clusters. In other words, the same interpretable evidence should ideally lead to one specific cluster without overlapping with others. This uniqueness enhances the credibility of the evidence, ensuring that end users can trust it is closely tied to the specific cluster, thereby reducing confusion with other clusters serving different functions.\nTo determine the goodness of an interpretable clustering method, or even to quantify it, one must consider the specific interpretable model being used. For example, when utilizing decision tree models, it is clear that the evidence used to define each cluster is highly distinctive through the tree's splits, thereby satisfying the basic requirement of uniqueness. Additionally, one can measure how easily end users understand the results by examining the structural parameters of the tree [15], such as the number of leaf nodes (i.e., the number of clusters) and the average depth of the tree. The process from data to clusters is represented by paths from the root to the leaf nodes, with each branching node recording the decision (splitting feature value) that leads to a cluster. Using fewer feature values results in more concise interpretable evidence, making it easier for end users to understand and trust the clustering results."}, {"title": "3 A TAXONOMY OF INTERPRETABLE CLUSTERING METHODS", "content": "In this section, after collecting and summarizing existing interpretable clustering methods, we establish the following criteria to taxonomize them systematically:\nFirstly, based on widely recognized clustering processes, existing interpretable clustering methods can be categorized into three types: pre-clustering methods, in-clustering methods, and post-clustering methods. Specifically, pre-clustering methods are typically executed before the clustering process and often relate to the selection of interpretable features. In-clustering methods construct interpretable clustering models for the samples, producing accurate partitions"}, {"title": "4 INTERPRETABLE PRE-CLUSTERING METHODS", "content": "In the study of interpretable clustering models, while our goal is to achieve more transparent models, it is equally important to carefully consider the features used as model inputs to produce interpretable results. Specifically, existing interpretable pre-clustering methods, which focus on the research conducted prior to clustering, can be approached from two perspectives: (1) feature extraction and (2) feature selection. Although these two issues have been extensively studied in the field of machine learning, they are rarely connected to interpretability, especially in terms of how to mine features that are more easily understood by humans for subsequent clustering tasks. Therefore, we have compiled a list of papers identified through our exhaustive search related to interpretable feature extraction or selection before clustering, which we elaborate on in the following two subsections."}, {"title": "4.1 Feature extraction", "content": "Interpretable pre-clustering methods from the perspective of feature extraction typically focus on complex data types, such as multivariate time series (MTS). The extraction of meaningful and informative features can lead to the development of simpler models that better capture significant characteristics within complex data, thus enhancing interpretability and facilitating better understanding.\nIn the field of multivariate time series, the system presented in [16] automatically extracts features from the signals, encompassing both intra-signal features, which characterize each signal independently, and inter-signal features, which evaluate relationships between signals using interpretable metrics. To select the most important features, the authors propose two methods: an unsupervised mode employing Principal Feature Analysis (PFA) and a semi-supervised mode incorporating user annotations on small dataset samples, significantly reducing the number of features without compromising accuracy. Salles et al. [17] leverages adaptive gating in NNs to dynamically select the most relevant features for each instance. Using a Gumbel-SoftMax technique to handle discrete choices and annealed mean-squared error regularization to encourage sparsity, the model identifies features that contribute most to predicting performance. These selected features are then used for clustering, enhancing the relevance and interpretability of the clusters.\nDrawing on Gestalt theory, an interpretable band selection algorithm [18] is proposed in which hyperspectral imagery is considered as continuously varying points based on proximity and continuity principles. The model, constructed using similarity and invariance principles, extracts three bands from the hyperspectral image sequence to form a pseudo-color image, enhancing consistency within categories and differences between categories. RGB colors are categorized into ten types, and the differences between the three channels and the standard colors are minimized using Euclidean distance, allowing pseudo-color mapping of different bands and intuitively displaying target differences within specific spectral bands, aligning with principles of visual perception."}, {"title": "4.2 Feature selection", "content": "Another category of interpretable pre-clustering methods focuses on accurately selecting features with strong discriminative power for different data structures from a set of redundant and complex features prior to clustering. These methods can significantly enhances the interpretability of clustering models while maintaining their accuracy.\nSvirsky et al. [19] propose to train self-supervised local gates to learn a sample-specific sparse gate vector for each input. The learned vectors are then used for reconstruction via an autoencoder. This approach provides instance-"}, {"title": "5 INTERPRETABLE IN-CLUSTERING METHODS", "content": "Interpretable in-modeling clustering methods serve as a direct source of interpretability within the broader category of interpretable clustering approaches, embedding interpretability within the algorithmic process of clustering itself. This form of interpretability is typically treated as an optimizable interpretability objective combined with conventional clustering criteria (e.g., SSE as used in k-means). Some methods approach the interpretability goal by incorporating it jointly with conventional clustering criteria as a multi-objective optimization problem [22], while most simply consider it as an additional term related to certain structural parameters [23].\nThere are two typical scenarios (S1 and S2) where interpretable in-clustering methods could easily be confused with their corresponding pre- or post-clustering methods, depending on the stage at which interpretability quality is considered:\nS1: Is input from third-party algorithms required? The interpretable models used in these in-clustering methods can either directly induce a clustering result (e.g., using decision-tree models that derive clusters via tree growth) or collaborate with various algorithms' costs through joint optimization of objective functions. These methods do not rely on or attach to reference clustering results from third-party algorithms. Even if some methods use initial clustering results as input, they remain agnostic to how the clustering cost is defined [24]. The boundary between these methods and post-clustering methods (which aim to explain existing clustering results) can sometimes be blurred. If the clustering is driven by explainability rather than by fitting a given third-party algorithm's results with an approximation guarantee, then the method is more aligned with interpretable in-clustering approaches.\nTo more clearly illustrate the distinction between in-clustering and post-clustering methods, we can consider the following example:\nIllustrative references to S1: Although both [25] and [23] optimize a specific explainability measure for the decision\nS2: Are the features in the dataset inherently interpretable? Interpretable in-clustering methods handle various forms of data and adjust according to the characteristics of the dataset's features. For typical vector data, the features are usually interpretable [26]: (1) for numerical features, cut values can be applied to split the feature vector by determining whether the feature values are greater or less than a threshold, which is a common approach in decision-tree-based clustering; (2) for categorical features, values can similarly be interpreted based on whether they include or exclude a specific category. However, for data such as social and biological networks, which lack explicit features [27], interpretable community detection methods aim to find concise descriptive features for nodes [28]. For images, whose features may lack inherent interpretability (e.g., pixel matrices without clear structural meaning along any given dimension), discovering structural or interpretable features becomes more challenging. In tasks that involve images with semantic content, such as in the field of descriptive clustering [29], the focus shifts to identifying interpretable tags. In sum, to handle those complex data with uninterpretable features, there is often a need to incorporate deep learning techniques [30], [31]. For categorical sequential datasets, where each sample is a discrete sequence of variable length, some conventional sequence clustering methods require transforming the sequences into feature vectors. However, this transformation often leads to a loss of interpretability from the original sequence space. Dong et al. [32] argue that Discriminative Sequential Pattern Mining is necessary before building interpretable clustering methods.\nCertain methods closely integrate the search for interpretable features with the clustering process itself, which can blur the boundaries between in-clustering and pre-clustering methods. Those methods often emphasize interpretability at the cluster level, rather than at the object/instance level. Here are some examples of such methods that clearly illustrate how the process of extracting interpretable features is integrated into the in-clustering stage:\nIllustrative references to S2: Kim et al. [33] propose a"}, {"title": "5.1 Decision tree-based methods", "content": "The decision tree model is widely recognized as an interpretable model in machine learning and is commonly used for classification and regression tasks. Its interpretability stems from the recursive, hierarchical splitting of data based on feature values to generate intermediate results, with the final output is traceable through the feature values used in the splits. Instances are distributed to different leaf nodes (clusters) determined by specific splitting points according to certain criteria, following a clear, transparent path from the root node (representing the whole dataset) down through the branch nodes, which is easily understood by end users.\nEarly attempts to apply decision trees to clustering can be found in [41], where uniformly distributed synthetic data were introduced as auxiliary data to build a standard (supervised) decision tree. This approach aimed to maximize the separation between the original data and the synthetic data by modifying the standard splitting criterion, such as information gain. Although this method used binary splits, which are relatively easy to understand, the reliance on data generation introduced additional assumptions, making it difficult to claim that the splits were truly interpretable. In contrast, [42] developed an unsupervised decision tree directly based on the original features. The authors proposed four different measures for selecting the most appropriate feature and two algorithms for splitting data at each branch node. However, to select a candidate splitting point for calculating these measures, preliminary steps were required to divide the numerical feature domain into intervals. A simpler splitting criterion and a more intuitive algorithmic framework is presented in [35] with the introduction of CUBT, which was further extended to categorical data in [43]. CUBT adopts a general approach similar to CART, involving three steps: maximal tree construction, followed by pruning and merging to simplify the tree structure. This unsupervised decision tree-based clustering model was also extended to the interpretable fuzzy clustering domain in [44], where fuzzy splitting at branch nodes was used to grow the initial tree, followed by merging similar clusters to create a more compact tree structure.\nThe aforementioned unsupervised decision tree-based models adopt a top-down approach, where all possible candidate splitting points are considered at the current branch node level, and criteria such as heterogeneity are calculated so that the tree grows greedily (greedy search) based on the optimal splits passed down from the parent node. However, this type of algorithm lacks global guidance, meaning that each split is optimized locally rather than achieving a globally optimized solution across the entire dataset.\nSome advanced interpretable in-clustering methods that use decision trees leverage modern optimization techniques. These modern optimization techniques include, but are not limited to, Mixed-Integer linear Optimization (MIO) techniques [45] used in [36], Tree Alternating Optimization (TAO) techniques [46] used in [24], and monotonic optimization techniques such as the Branch-Reduce-and-Bound (BRB) algorithm [47] used in [23]. These methods are designed to construct globally optimal clustering trees by explicitly optimizing a well-defined objective function applied to the entire dataset. Unlike traditional top-down approaches, these methods directly establish a relationship between the instances assigned to different leaf nodes (clusters) and the interpretability objective, which is explicitly encoded in the objective function. These methods express interpretability in a more quantitative and formalized manner, often by specifying tree structural metrics [15] (e.g., the number of leaf nodes), where a smaller number of leaf nodes (nLeaf), as used in [23], [24], typically indicates lower tree complexity and, correspondingly, better interpretability. Building on this global optimization framework, some interpretable fuzzy clustering algorithms are presented as well. For example, [48] employs kernel density decision trees (KDDTs) for constructing fuzzy decision trees using an alternating optimization strategy, while [49] incorporates a soft (probabilistic) version of the split in their objective function and obtains the optimal split via a Constrained Continuous Optimization Model."}, {"title": "5.2 Rule-based methods", "content": "The process of mining an optimal rule set to derive a specific cluster is often inspired by the field of pattern mining [50]. To ensure that different rule sets effectively correspond to their respective clusters, the rule set typically exhibits two key characteristics [51]: (1) frequency (meaningful), indicating that the rule set should cover as many samples within its corresponding cluster (true positives) as possible, and (2) discriminative power (unique), meaning that the rule set should minimize the number of samples mistakenly covered from other clusters (false positives).\nTo obtain a rule set for the purpose of interpretable clustering, a common approach is to start by quantifying interpretability based on how well a rule covers a specific cluster. For example, as demonstrated in [37], an interpretability score is defined to assess a feature value's relevance to a cluster by considering the fraction of samples within the cluster that share that feature value. Given all candidate rules or rule sets (e.g., generated using frequent pattern"}, {"title": "5.3 Other methods", "content": "In addition to the two widely used interpretable models mentioned above, other interpretable in-clustering methods create clusters or determine cluster membership based on representative elements, which can generally be categorized as boundary-based or centroid-like approaches. However, for these representative elements to be interpretable, certain properties need to be maintained. The following is a brief overview of these approaches.\nConvex-polyhedral: These methods constrain the cluster boundaries to be axis-parallel (rectangular) in the feature space, as in the method proposed in [38], which designs a Probabilistic Discriminative Model (PDM) to define such clusters. More generally, they may use hyperplanes that allow for diagonal boundaries [39] to more accurately represent a cluster.\nIn either case, the goal is to create clusters with fewer feature values, incorporating these as interpretability constraints within the standard clustering objective function. For instance, [39] uses a Mixed-Integer nonlinear Optimization (nonlinear-MIO) programming formulation to jointly identify clusters and define polytopes. For axis-parallel boundaries, a single feature value is used per dimension, while diagonal boundaries rely on linear combinations of feature values. Although diagonal boundaries have greater power to distinguish different clusters, they are less interpretable due to their increased complexity compared to simpler axis-parallel boundaries.\nPrototype (exemplar): In datasets where the original features are non-interpretable and difficult to understand, such as with images and text, especially when deep embeddings are used, recent work on interpretable in-clustering via exemplars has found that seeking high-level centroids can be useful for characterizing clusters and facilitating visualization. For example, [40] tackles the challenging problem of finding the minimum number of exemplars (nExemplar) without prior specification. Additionally, [31] proposes a new end-to-end framework designed to enhance scalability for larger datasets, making exemplar-based clustering more practical for real-world applications."}, {"title": "5.4 Summary", "content": "Various interpretable models, with others potentially existing and requiring further investigation, have been developed for in-clustering methods (summarized in Table 1). These models consistently treat interpretability as a first-class objective, on par with clustering quality, incorporating it as an optimization target either directly or indirectly, depending on the model type. For instance, tree-based models often prioritize reducing the number of branch or leaf nodes, rule-based models focus on shorter rules, and geometric representation models, such as prototype-based models, aim to minimize the number of exemplars. More refined structural parameters as optimization targets require further research. For example, in literature [25], tree depth is considered an optimization target; however, this approach, designed to explain a given reference clustering result, belongs to post-clustering methods.\nThere is often a trade-off between interpretability and clustering quality, where enhancing one may diminish the other. This frequently addressed challenge could be less daunting in post-clustering methods, which only need to"}, {"title": "6 INTERPRETABLE POST-CLUSTERING METHODS", "content": "Post-modeling interpretability is a crucial aspect of interpretable learning, focusing on elucidating the reasoning behind decisions made by black-box models. In the context of clustering, interpretable post-clustering refers to the use of interpretable models, such as decision trees, to closely approximate existing clustering results (also known as reference clustering results). This means that the labels assigned to samples by the interpretable model should align as closely as possible with the original results. This kind of method aids in understanding why certain samples are assigned to specific clusters, thereby fostering trust in black-box models. In the following subsections, we will categorize existing interpretable post-clustering methods based on different interpretable models."}, {"title": "6.1 Decision tree-based methods", "content": "Decision trees are the most widely used interpretable models for post-clustering analysis. In a decision tree, each internal node splits the samples it contains into different groups based on predefined criteria. The k leaf nodes (not necessarily the ground-truth cluster number) correspond to the k clusters in the reference clustering results. Each cluster assignment can be interpreted by the path leading to its respective leaf node.\nIn decsion tree-based post-clustering methods, the closer the clustering results obtained by the constructed decision tree are to the reference clustering results, the better its interpretability performance. This metric is often defined in existing research as \u201cthe price of interpretability", "interpretability": "Weighted Average Depth (WAD), which weighs the depth of each leaf by the number of samples in its associated cluster, and Weighted Average Explanation Size (WAES), a variation of WAD. Inspired by robustness studies, Bandyapadhyay et al. [66] explore constructing a decision tree by removing the fewest points necessary to match the reference clustering results exactly, where interpretability is measured by the number of points removed."}, {"title": "6.2 Rule-based methods", "content": "Distinct from decision trees, interpretable post-clustering models constructed using if-then rules do not involve hierarchical relationships. Their explanations for clusters are relatively concise and intuitive, providing a set of rules to describe the samples within a cluster. To our knowledge, despite the fact that if-then rules have become widely accepted as interpretable models and have been studied considerably, most rule-based interpretable clustering methods focus on extracting rules from data to form clusters. Consequently, there is limited research on post-clustering methods that generate rules and provide explanations for clusters that have already been formed.\nCarrizosa et al. [22] explain clusters with the objective of maximizing the total number of true positive cases (i.e., the number of samples within the cluster that satisfy the explanation) and minimizing the total number of false positive cases (i.e., the number of individuals outside the cluster that satisfy the explanation). Additionally, the length of the rules is constrained to ensure strong interpretability.\nDe Weerdt et al. [67] investigate the search for explanations for event logs by first generating feature sets from the data and then applying a best-first search procedure with pruning to construct the set of explanations. Through an iterative process, they continuously enhance the accuracy and conciseness of the explanations for the instances. Building on this work, Koninck et al. [68] mine concise rules for each individual instance from a black box support vector machine (SVM) model and discuss and evaluate different alternative feature sets that can be used as inputs for explanatory techniques."}, {"title": "6.3 Other methods", "content": "Besides the aforementioned decision trees and if-then rules, several other interpretable models have been used in literature to explain existing clustering results. Given their limited number, we will not review each interpretable model individually but rather provide an overall summary here.\nPrototype. Carrizosa et al. [57] proposed a method for using prototypes to explain each cluster. A prototype is an individual that serves as a representative example of its cluster, defined by its minimal dissimilarity to other individuals within the same cluster. In their approach, they solve a bi-objective optimization problem to identify these prototypes. This problem aims to maximize the number of true positive cases within each cluster while minimizing the number of false positive cases in other clusters.\nConvex polyhedral. In [55], a polyhedron is constructed around each cluster to serve as its explanation. Each polyhedron is formed by intersecting a limited number of half-spaces (nHalfspace). The authors formulate the polyhedral description problem as an integer program, where variables correspond to candidate half-spaces for the polyhedral description of the clusters. Additionally, they present a column generation approach to efficiently search through the candidate half-spaces. Chen et al. [56] propose using a hypercube coverage model to explain clustering results. This model incorporates two objective functions: the number of hypercubes (nHypercube) and the compactness of instances. A heuristic search method (NSGA-II) is employed to identify a set of non-dominated solutions, defining an ideal point to determine the most suitable solution, whereby each cluster is covered by as few hypercubes as possible.\nDescription. Davidson et al. [69] introduce the cluster description problem, where each data point is associated with a set of descriptions from a discrete set. The objective is to find a set of non-overlapping descriptions for each cluster that covers every instance within the cluster. The proposed method allows for the specification of the maximum number of descriptions per cluster and the maximum number of clusters that any two descriptions can jointly cover."}, {"title": "6.4 Summary", "content": "Several representative interpretable post-clustering methods are summarized in Table 2. Additionally, the following observations can be noted: firstly, most post-clustering research utilizes decision trees as interpretable models to explain clustering results. However, explanations derived from decision trees have certain drawbacks, such as the dependency of deep-layer decisions on shallow-layer decisions. Additionally, it is possible to consider using a hyperplane in a chosen number of dimensions instead of splitting along only one feature. Moreover, the choice of a suitable interpretable model may vary depending on the type of data; for instance, descriptions may be more appropriate for community analysis. Therefore, the post-clustering methods involving other interpretable models require further investigation.\nSecondly, existing methods primarily focus on approximating the optimal clustering cost of reference clustering results using decision tree-based approaches, or aiming for interpretable models with high true positive rates and low false positive rates [22], [57]. However, few methods emphasize the simplicity of explanations (except for [22],"}, {"title": "7 CONCLUSION AND FUTURE DIRECTIONS", "content": "This survey provides a comprehensive and systematic perspective on various interpretable clustering methods, highlighting both foundational research and the latest advancements in the field. It is the first to address the topic across the full lifecycle of clustering analysis, encompassing Pre-clustering, In-clustering, and Post-clustering stages. At each stage, relevant literature on interpretable clustering methods is reviewed. Primarily, this work aims to clearly define what interpretability means in the context of clustering and how it is embedded in commonly used interpretable models, such as decision trees, rules, prototypes, and convex polyhedral models. These models create interpretable clusters with elements that are understandable to human users and potentially enable these clustering results to be applied in high-risk domains, meeting essential prerequisites of transparency and trustworthiness.\nTo provide valuable insights for the future direction of this field, we have classified various interpretable clustering methods based on different aspects and further summarized key technical criteria for readers' reference, such as: (1) Optimization approaches, which illustrate how authors from various domains have formalized the interpretability challenges in clustering and the methods they have employed to solve these optimization problems, and (2) Interpretability-related structural metrics, which are crucial as they could potentially be utilized to evaluate the interpretability quality of novel methods, similar to how accuracy is used to assess clustering quality. The literature still lacks attention to a greater diversity of these structural metrics. We believe that researchers studying these different interpretable clustering methods can complement and enhance each other's work. Moreover, methods from different clustering stages could be combined, as relying solely on a single-stage interpretable clustering method may be insufficient for complex and challenging application scenarios. This is particularly true in cases where obvious interpretable features do not exist, making it difficult to construct interpretable clustering algorithms. Additionally, research on interpretable clustering methods for intricate data, such as discrete sequences [32], network (graph) [70], and multi-view and multi-modal data [71], remains limited."}]}