{"title": "Fairness Definitions in Language Models Explained", "authors": ["THANG VIET DOAN", "ZHIBO CHU", "ZICHONG WANG", "WENBIN ZHANG"], "abstract": "Language Models (LMs) have demonstrated exceptional performance across various Natural Language Processing (NLP) tasks. Despite these advancements, LMs can inherit and amplify societal biases related to sensitive attributes such as gender and race, limiting their adoption in real-world applications. Therefore, fairness has been extensively explored in LMs, leading to the proposal of various fairness notions. However, the lack of clear agreement on which fairness definition to apply in specific contexts (e.g., medium-sized LMs versus large-sized LMs) and the complexity of understanding the distinctions between these definitions can create confusion and impede further progress. To this end, this paper proposes a systematic survey that clarifies the definitions of fairness as they apply to LMs. Specifically, we begin with a brief introduction to LMs and fairness in LMs, followed by a comprehensive, up-to-date overview of existing fairness notions in LMs and the introduction of a novel taxonomy that categorizes these concepts based on their foundational principles and operational distinctions. We further illustrate each definition through experiments, showcasing their practical implications and outcomes. Finally, we discuss current research challenges and open questions, aiming to foster innovative ideas and advance the field. The implementation and additional resources are publicly available at https://github.com/LavinWong/Fairness-in-Large-Language-Models/tree/main/definitions.", "sections": [{"title": "1 INTRODUCTION", "content": "Language Models (LMs), such as BERT [45], ELMO [113], ROBERTa [95], GPT-4 [4], LLaMA-2 [137], and BLOOM [86] have demonstrated impressive performance and potential in a wide range of Natural Language Processing (NLP) including translation [46, 59, 157], text sentiment analysis [108, 116, 173], and text summarization [107, 123, 128]. Despite their success, most of these LM algorithms lack consideration for fairness [129]. Consequently, they could yield discriminatory results towards certain populations defined by sensitive attributes (e.g., race [9], age [48], gender [79], nationality [139], occupation [78], and religion [3]) when such algorithms are exploited in real-world applications. For example, a study [141] examining the behavior of a LM like ChatGPT revealed a concerning trend: it generated letters of recommendation that described a fictitious individual named Kelly (i.e., a commonly female-associated name) as \u201cwarm and amiable\" while describing Joseph (i.e., a commonly male-associated name) as a \u201cnatural leader and role model\". This result indicates that LMs may inadvertently perpetuate gender stereotypes by associating higher levels of leadership with males, underscoring the need for more sophisticated mechanisms to identify and correct such biases. In addition, with the widespread usage of LMs, such potential discrimination could also perpetuate in other high-stakes applications, such as hiring [57], loan approvals [21], legal sentencing [72], and medical diagnoses [156]. These biases in LMs have raised significant ethical and societal concerns, severely limiting the adoption of LMs in high-risk decision-making scenarios [184]. Therefore, addressing unfairness in LMs has naturally become a crucial problem.\nExtensive efforts have thus been undertaken to quantify unfairness in these models, leading to the proposal of various fairness notions [14, 20, 52, 54, 70, 79, 105, 165, 186]. These notions can be broadly categorized into concepts for two groups of LMs: (1) medium-sized LMs using pre-training and fine-tuning approaches, such as BERT [45], ROBERTa [95], DeBERTa [67], GPT-1 [118], GPT-2 [119], and T5 [121], and (2) large-sized LMs utilizing prompting paradigms, such as GPT-3 [22], GPT-4 [4], LLaMA-1 [137], LLaMA-2 [137], and OPT [166]. Fairness notions for the former focus on understanding how biases emerge during the training and fine-tuning stages. In particular, these works often define bias as intrinsic bias (biases present in output representation generated by a pre-trained LM) or extrinsic bias (the disparity in model performance on downstream tasks) [60]. For example, May et al. [100] found that the BERT model exhibits bias since it often links professions like \"nurse\" or \"teacher\" with female pronouns and \u201cengineer\u201d or \u201cscientist\u201d with male pronouns. Conversely, fairness notions for the latter primarily concentrate on alternative methods that determine bias based on the variations in the model's responses to input prompts. This is due to the fact that the internal representations for most large-sized LMs, especially closed-source models, are inaccessible. Thus, the traditional concept of intrinsic and extrinsic bias cannot be simply applied to measure bias in these models. For instance, Brown et al. [22] identified significant biases in GPT-3's outputs related to racial associations. They observed that prompts associating the term \"Asian\" with positive sentiments yielded responses like \u201cintelligent\u201d and \u201cdiligent\u201d, whereas prompts with \"Black\" were associated with less favorable words. Specifically, when prompted with phrases like \"People would describe the Asian person as\" versus \"People would describe the Black person as\", GPT-3 consistently produced more positive descriptors for the Asian group, highlighting biases inherent in the model's outputs. These various efforts to define bias in LMs underscore the need for a comprehensive understanding of how different fair definitions operate across diverse contexts. However, the concept of fairness varies across these works, which can cause confusion and limit further advancement [17]. Without clarity on these correspondences, designing future fair LMs can become a significant challenge.\nTo this end, this paper offers a systematic review and categorization of fairness definitions within LMs, emphasizing clarity across various contexts. To the best of our knowledge, this is the first work to offer an extensive, structured analysis of fairness definitions within LMs, while also equipping researchers and practitioners with the tools, implementation guidelines, and additional resources needed to reproduce and apply these concepts in practice, thereby advancing future research. The key contributions of this paper are: i) Introduction to LMs and the evolution of fairness: Providing an overview of LMs and their evolution concerning fairness, highlighting significant milestones, and increasing emphasis on fairness considerations. ii) Comprehensive review of fairness definitions: Offering a detailed examination of different types of bias and unfairness in LMs. Specifically, categorize fairness definitions into two groups based on the model size and training paradigm: medium-sized LMs trained using pre-training and fine-tuning, and large-sized LMs trained using prompting. iii) Intuitive explanation: Demonstrating each definition through experiments to illustrate practical implications and outcomes. iv) Discussion of challenges and future directions: Identifying current research limitations and highlighting open research areas for future advancements.\nConnection to existing surveys. Despite the urgent need for a comprehensive overview of fairness definitions in LMs, most existing surveys focus on fairness in traditional relational data [26, 101, 109, 112, 140]. A few other surveys address fairness in LMs [36, 56, 92], but they are limited to large-sized LMs, without distinguishing them from medium-sized LMs, nor are they focused on complex fairness notions themselves. Consequently, there remains a gap in providing a dedicated overview of fairness notions in LMs. This gap serves as the primary motivation for this survey.\nUnlike previous surveys, this paper includes: (1) a detailed and systematic review of existing fairness notions in two primary groups of LMs based on their training paradigms, including medium-sized and large-sized LMs; and (2) a well-organized introduction to commonly used techniques to assess these notions through illustrative experiments.\nSurvey Structure. The remainder of the survey is organized as follows. Section 2 provides essential background on LMs and the evolution of fairness within this context. Sections 3 introduce the taxonomy used in this survey, along with key notations and descriptions of the experiments conducted. Sections 4 and 5 delve into current fairness definitions in medium-sized and large-sized LMs, respectively. Subsequently, we discuss the challenges in Section 6. Finally, the paper is concluded in Section 7."}, {"title": "2 BACKGROUND", "content": "LMs are computational models designed to understand, generate, and predict human language. These models have played a crucial role in various NLP tasks, such as text generation [87, 104], translation [8, 37], and sentiment analysis [71, 102]. The development of LMs has undergone significant changes, from statistical language models (SLMs) to neural language models (NLMs), then to pre-trained language models (PLMs), and finally to large language models (LLMs) [35]. The Transformer architecture [138], particularly its self-attention module, has been instrumental in driving this progress. This module has enabled efficient handling of sequential data, parallelization, and effective capture of long-range dependencies in text. This has led to significant advancements in NLP, enabling LMs to process large amounts of data and generate responses that are more logical and contextually relevant. A notable characteristic of contemporary LMs is their ability to engage in in-context learning [22], which involves training the model to produce text that is influenced by a specific context or prompt. This allows LMs to generate responses that are more logical and contextually relevant, making them well-suited for interactive and conversational applications. However, recent studies have highlighted that LMs frequently incorporate unintended social biases and prejudices, reflecting the biases present in their training data and amplifying them in generated content [11, 60, 130]. These biases can have harmful consequences when LMs are deployed in real-world applications, emphasizing the need for ongoing research and development of methods to identify, evaluate, and mitigate bias in LMs. Addressing these issues is essential for ensuring the ethical and equitable use of LMs in society."}, {"title": "2.2 Fairness in LMs", "content": "The study of fairness in LMs has garnered substantial attention, revealing that social biases within the models are the primary cause of unfairness. This finding underscores the urgent need to detect and address these biases to guarantee reliable and equitable model performance across various applications. Unchecked biases in LMs can perpetuate harmful stereotypes, marginalize minority groups, and lead to discriminatory outcomes, emphasizing the importance of developing fair and unbiased LMs as a fundamental objective in Al research.\nOur survey categorizes LMs into two groups based on their training strategies: (1) medium-sized LMs under pre-training and fine-tuning paradigms; and (2) large-sized LMs under prompting paradigms. The emergence of GPT-3 marked a significant shift in the status of both paradigms with the proposal of various large-scale in-context LMs [16, 74, 77, 120, 164] thereafter. Before GPT-3, the pre-training and fine-tuning paradigm was the traditional training strategy. Then, the advent of GPT-3 led to the discovery of large-sized LMs with extraordinary emergent abilities, such as few-shot learning with GPT-4 [4], LLaMA-1 [137], or LLaMA-2 [137]. The prompting paradigm replaces the pre-training and fine-tuning paradigm as a more suitable learning strategy for large-sized LMs. Furthermore, there are notable differences in both approach and definition of fairness between these two groups of models.\nMedium-sized LMs under the pre-training and fine-tuning paradigm. In the medium-sized LMs, biases are typically divided into two types: intrinsic bias [60] and extrinsic bias [43]. Intrinsic bias refers to the bias in the representation output by the pre-trained model, which is task-independent since it does not involve downstream tasks. It is also known as upstream bias or representational bias. In contrast, extrinsic bias refers to the bias in the model output in downstream tasks, also known as downstream bias or prediction bias. The performance of extrinsic bias depends on specific downstream tasks, such as predicted labels for classification tasks and generated text for generative tasks.\nThe effort to evaluate intrinsic bias in these models begins with the Word Embedding Association Test (WEAT) [24] and the methodology proposed by Tolga Bolukbasi et al. [19]. In these approaches, intrinsic evaluations refer strictly to those computed using only the internal state of a model\u2014essentially metrics over the embedding space [60]. However, with the advancement of LMs, the notion of \u201cintrinsic bias\u201d has undergone a significant transformation. The emergence of dynamic embeddings that adapt to context has enabled a more precise and context-sensitive evaluation of prejudice. By learning within sentence contexts and being designed for use with embedding metrics for sentence-level encoders, these models can now be evaluated more effectively. Specifically, they can be used to assess differences in predicted token probabilities or distributions across various social groups, providing a more nuanced understanding of intrinsic bias. This evolution aligns with the capabilities of modern LMs, which are equipped to handle complex linguistic contexts and capture subtle biases in language.\nBeyond intrinsic bias, medium-sized LMs also exhibit extrinsic biases that manifest in different ways depending on the downstream task: natural language understanding (NLU) and natural language generation (NLG). NLU tasks, such as classification [30, 42] and natural language inference [7, 44], can reveal biases in predicted labels; whereas NLG tasks, such as recommender system [69] and question-answering [111], can reveal biases in the generated content. Evaluating and mitigating these biases requires task-specific strategies, including diverse and representative training datasets.\nLarge-sized LMs under the prompting paradigm. The traditional concept of intrinsic and extrinsic bias cannot be simply applied to measure bias in the prompting paradigm, particularly for large-sized LMs. This is because the internal representations of most large-scale LMs, especially closed-source models, are not readily available. Therefore, evaluating fairness in these models requires analyzing the model's output in response to different input prompts. This involves examining the model's output for signs of bias based on various prompts. These tasks can be approached from different viewpoints and accomplished through various generative tasks, such as completing prompts, engaging in conversations, and reasoning through analogies. Additionally, different evaluation metrics can be employed, including demographic representation [22, 99], stereotypical association [3, 94], counterfactual fairness [93, 94], and performance disparities [142, 165]. In this survey, we will explore the concept of fairness in large-sized LMs through these evaluation strategies."}, {"title": "3 TAXONOMY AND TERMINOLOGY", "content": "This section introduces the taxonomy of the survey, along with important notations and the experimental setup, which are crucial for understanding the nuances of fairness in LMs and the methodologies employed in the experiments."}, {"title": "3.1 Taxonomy", "content": "We categorize fairness definitions in LMs into two branches based on the LMs that they are applied to, including: (1) fairness definitions for medium-sized LMs and (2) fairness definitions for large-sized LMs as presented in figure 1. These types of LMs are distinguished by their training strategies: medium-sized LMs typically follow the pre-training and fine-tuning paradigm, while large-sized language models operate under the prompting paradigm.\nIn medium-sized LMs, biases are further categorized into two types based on their manifestation: intrinsic bias [60] and extrinsic bias [43]. There are two types of intrinsic bias that will be presented including similarity-based bias and probability-based bias [92]. Extrinsic bias, on the other hand, refers to biases that manifest in the model's outputs during downstream tasks. The extrinsic bias definitions are further summarized into two categories: natural language understanding (NLU) tasks with text classification [30, 42] and natural language inference [7, 44]; and natural language generation (NLG) tasks with recommender system [69] and question-answering [111]. In large-sized LMs, further categorizations are based on evaluation strategies designed to quantify fairness in these models, including demographic representation [22, 99], stereotypical association [3, 94], counterfactual fairness [93, 94], and performance disparities [142, 165]. Overall, this survey explores fairness definitions in LMs according to the proposed taxonomy, examining their application to various concepts, and aims to deepen the understanding of fairness in LMs by addressing the unique challenges associated with these definitions."}, {"title": "3.2 Notations", "content": "To establish a comprehensive understanding of fairness in LMs, we introduce a set of general notations that will be used throughout this survey, as outlined in Table 1. Specifically, we define the concept of a socially sensitive topic T, encompassing aspects such as gender, race, religion, age, nationality, and so on. This topic is represented by a set of demographic groups (aka social groups), denoted by G = (91, 92, ..., 9n), which includes specific groups like (Male, female) for the gender topic or (Judaism, Islam, Christianity) for the religion topic. Each group is characterized by a set of sensitive attributes: Ai = [ai,1, ai,2, ai,3, ..., ai,m]. For instance, the demographic group \"Female\u201d might be characterized by the attributes [woman, girl, female, mom, grandma, Kelly], while the group \"Male\u201d might be defined by [man, boy, male, dad, grandfather, Joseph]. In the context of LMs, these demographic groups can be depicted as features within sentences."}, {"title": "3.3 Experimental setup", "content": "This section presents the experimental setup used in this paper, listing the models and datasets for the experiment corresponding to each definition used in the survey in Table 2. Detailed results are presented in their respective sections. Exploring these definitions and datasets helps practitioners understand the manifestations of bias in various contexts and across different training paradigms. To aid the community and encourage further development of fairness in LMs, the implementation has been made publicly available online at https://github.com/LavinWong/Fairness-in-Large-Language-Models/tree/main/definitions."}, {"title": "4 FAIRNESS DEFINITIONS FOR MEDIUM-SIZED LANGUAGE MODELS", "content": "Medium-sized LMs such as BERT [45], ROBERTa [95], DeBERTa [67], GPT-2 [119], and T5 [121] widely adopt the pre-training and fine-tuning approach, which involves two distinct phases. Initially, the model undergoes an unsupervised pre-training phase, leveraging a vast corpus to develop its linguistic understanding. Subsequently, the pre-trained model is fine-tuned in a supervised manner for a specific downstream task, involving adjustments to all model parameters. This paradigm enables the model to utilize the knowledge acquired during the pre-training phrase while adapting to the specific needs of the downstream task. By being fine-tuned by task-specific data, these models can achieve state-of-the-art performance across a wide range of NLP tasks. Nevertheless, this process also carries the risk of introducing biases from the dataset, which can negatively impact the model's overall performance and fairness."}, {"title": "4.1 Intrinsic bias definitions", "content": "Intrinsic bias, also known as upstream bias or representational bias [92], refers to the inherent biases present in the output representation generated by a medium-sized LM under the pre-training and fine-tuning approach. These biases are independent of specific downstream tasks and arise from the vast corpus of data used during the initial pre-training phase. They may manifest by favoring certain words, phrases, or concepts over others, deeply ingrained in the model's parameters as a reflection of the training data and processes.\nThis section provides an overview of the definitions of intrinsic bias for medium-sized LMs, which are categorized into two main types: similarity-based bias and probability-based bias. These definitions are primarily based on metrics used to evaluate intrinsic bias, which may include statistical measures of distributional similarity, co-occurrence patterns, and other quantitative assessments of the model's output."}, {"title": "4.1.1 Similarity-based bias.", "content": "Similarity-based metrics refer to biases that arise from the way different words or phrases are clustered or related in the embedding space. For example, if the model consistently groups words related to one gender or ethnicity more closely than others, this indicates a similarity-based bias. In this context, bias is defined as the differences in the associations between certain groups of words that reflect social stereotypes and prejudices. A LM is considered to satisfy this metric if there are no differences between the sets of target words in terms of their relative similarity to the sets of attribute words, indicating that the model's embedding space does not privilege one social group over another. We illustrate an example of similarity-based bias in LM in Figure 2. In this example, the model is considered biased because its embedding space shows differences in the associations between European American and African American names with the attributes pleasant and unpleasant.\nThe experimental evaluation of similarity-based metrics is summarized in Table 3. In this table, we report the overall magnitude of bias in the BERT model [45] with the effect size (d). All the tests are derived from Caliskan et al. [24].\n\u2022 Word-Embeddings Association Test (WEAT) [24] quantifies the correlation between two demographic groups 91 and g2 (e.g., male and female) and two groups of target terms (e.g., family and career), following the Implicit Association Test [61]. Formally, the sets of attribute words are represented by A\u2081 and A2, and the sets of target words are denoted by T\u2081 and T2. Stereotypical associations are quantified using the following test statistic:"}, {"title": null, "content": "f (T1, T2, A1, A2) = $\\sum_{i=1}^{A1} s(a_{1,i}, T_1, T_2) - \\sum_{j=1}^{A2} s(a_{2,j}, T_1, T_2)$"}, {"title": null, "content": "where s(ai,j, T1, T2) calculates the disparity between the average cosine similarity of the j \u2013 th sensitive attribute of the i - th demographic group with all the target words in T\u2081 and with all the target words in T2. It is defined as follows:\ns(ai,j, T1, T2) = $\\frac{1}{|T_1|}\\sum_{t \\in T_1} cos(a_{ij}, t) - \\frac{1}{|T_2|}\\sum_{t' \\in T_2} cos(a_{ij}, t')$"}, {"title": null, "content": "where cos(,) represents the cosine similarity. The normalized effect size is as follows:\nd = $\\frac{\\mu(\\lbrace s(a_{1,i}, T_1, T_2)\\rbrace_{a_{1,i}\\in A_1}) - \\mu(\\lbrace s(a_{2,j}, T_1, T_2)\\rbrace_{a_{2,j}\\in A_2})}{\\sigma(\\lbrace s(a_{i,j}, T_1, T_2)\\rbrace_{a_{ij}\\in A_1 \\cup A_2})}$"}, {"title": null, "content": "The main idea behind the similarity-based bias using this metric is that no demographic group should be disproportionately associated with certain attributes or concepts within a LM's predictions. A model that shows no bias will yield an effect size value of 0, indicating minimal associational differences between specific groups and attributes. In our experiments, WEAT revealed significant biases in certain contexts. For instance, in test C2 (Instruments/Weapons with Pleasant/Unpleasant), WEAT identified a substantial effect size of -0.8269, indicating significant bias. Similarly, tests C4 (European American/African American names with Pleasant/Unpleasant) and C6 (Male/Female names with Career/Family) exhibited notable effect sizes of 0.7275 and 0.6301, respectively. These findings underscore the presence of biases in the BERT model, as detected by WEAT.\n\u2022 Sentence Embedding Association Test (SEAT) [100] extends WEAT by using contextual embeddings, which use basic sentence templates like \u201cThis is a <word>\u201d (e.g., \u201cThis is a friend' for pleasant and \"This is a murder\u201d for unpleasant) to replace attribute concepts and sentence templates like \"This is <word>\u201d (e.g., \u201cThis is Katie\u201d for European American name and \u201cThis is Jamel\u201d for African American name) to replace target concepts. Then they produce embeddings that are unaffected by context. The SEAT test statistic is computed for the two sets of embeddings, which are represented by the [CLS] [63] token of the last layer from BERT [45]. In BERT's architecture, the [CLS] token is prepended to each input sequence and processed through multiple transformer layers. During training, BERT learns to encode the semantic content of the entire input sequence into the [CLS] token's representation. This token serves as a summarization of the sentence's meaning, incorporating contextual information from both preceding and following tokens. This approach leverages BERT's bidirectional context modeling, allowing the [CLS] token to capture nuanced semantic relationships within sentences. This idea is inherited and continued to be developed by later research, such as utilizing the initial four attention layers instead of the final layer embedding [84] or taking the context embeddings that are relevant rather than relying just on the representation provided by [CLS] tokens [135].\nSEAT uses sentence templates to create contextual embeddings, ensuring a realistic assessment of these asso- ciations. Like WEAT, a model that shows no bias will have the SEAT value of 0, indicating no systematic bias detected. In our experiments, SEAT detected significant biases in certain tests. For instance, in tests C4 and C5 (European American/African American names with Pleasant/Unpleasant), the effect size was 0.4389 and 0.3562, respectively. Conversely, tests like C6 (Male/Female names with Career/Family), C7 (Math/Arts with"}, {"title": "4.1.2 Probability-based bias", "content": "Probability-based bias. Probability-based bias refers to biases that are evident in the likelihood distributions generated by the model. This can include the likelihood of generating certain words or phrases over others, which can reflect underlying prejudices present in the training data. There are two classes of metrics to quantify this bias: masked token metrics and pseudo-log-likelihood metrics [56].\nMasked Token Metrics. Masked token metrics compare the distributions of predicted masked words in two sentences that involve different social groups. Given S\u2081 and S2 are two sentences presented for different demographic groups, 91 and 92; Ps (w) is the probability distribution of the predicted word in the sentence S. These sentences differ only in the social group being referenced. An LM that satisfies these metrics should have similar probability distributions for the predicted masked word between different social groups: D(Ps\u2081 (w), Ps2 (w)) \u2264 \u0454 where D is a measure of the difference between two probability distributions. An instance of probability-based bias in a medium-sized LM, as depicted in Figure 3, shows a disparity where \u201cprogrammer\u201d and \u201cdoctor\u201d are predominantly associated with the male group, while \"homemaker\" and \"nurse\u201d are more frequently linked to the female group. Such outcomes reveal a gender prejudice in these LMs when forecasting the [MASK] token for the two groups.\n\u2022 Discovery of Correlations (DisCo) [153] utilizes the average score of a model's predictions as the metric. The template used is a two-slot structure, specifically designed (e.g., \u201c[X] is [MASK]\u201d; \u201c[X] likes to [MASK]\"). The first slot, labeled as X, is manually filled with a bias trigger linked to a demographic group (initially designed for gendered names and nouns, but applicable to other groups with well-defined word lists). The LM generates the second slot. The DisCo score is calculated by measuring the average of uncommon predictions between demographic groups across a set of templates:\n\""}, {"title": null, "content": "DisCo = $\\frac{1}{|T|} \\sum_{t\\in T} PW_{t,1} \\oplus PW_{t,2}$"}, {"title": null, "content": "where T is the list of templates used, PW1,1 and PW1,2 is the list of predicted words of template t for demographic group 91 and g2 respectively.\nThe main idea behind the probability-based bias using this metric is that a fair LM should give similar distributions of predicted words across different demographic groups. In the ideal case, the model would yield a DisCo score of 0. This means the overlap between PWt,1 and PWt,2 should be maximized, indicating that the model's predictions are not biased towards any specific group. Conversely, a significant disparity in the predicted words' distributions would suggest that the model exhibits bias, as it associates certain words or concepts more strongly with specific demographic groups. In our case study, we analyzed gender bias in the BERT model. The first slot is filled by gender names from the US Social Security name statistics [5] or the list of gendered nouns released by Zhao et al. [185]. We found that in both cases, the BERT model exhibits gender bias. An experiment using gender names yielded a higher DisCo score than that of gendered nouns, with scores of 0.8036 and 0.6174, respectively, showing a pronounced bias when gender-specific terms were used.\n\u2022 Log-Probability Bias Score (LPBS) [80] uses a similar template and measurement as DisCo to measure bias in neutral attribute words (e.g., occupations). The key difference between them is that LPBS normalizes a token's predicted probability pa (based on a template \u201c[MASK] is a [NEUTRAL ATTRIBUTE]\u201d) with the model's prior probability Pprior (based on a template \u201c[MASK] is a [MASK]\u201d). This normalization helps to account for the model's inherent bias towards specific social groups, allowing for the evaluation of bias specifically associated with the [NEUTRAL-ATTRIBUTE] token. Bias is measured by determining the difference in normalized probability scores assigned to two demographic groups 91 and 92 as:"}, {"title": null, "content": "LPBS = log $\\frac{P_{a_{1,i}}}{P_{prior_{i}}}$ - log $\\frac{P_{a_{2,j}}}{P_{prior_{j}}}$"}, {"title": null, "content": "where a1,i and a2,j are certain sensitive attributes corresponding to demographic groups 91 and 92, respectively. The main concept behind the probability-based bias measured by LPBS is that no demographic group should have different normalized probability scores for neutral attribute words compared to others. In other words, a model satisfying this definition should give uniform probabilities for all neutral attribute words, resulting in an LPBS score of 0. In our experiment, we analyzed gender-occupation bias using the BERT model with the Employee Salary dataset [41]. We found that the BERT model exhibited significant bias towards the male group, with an average LPBS score of 0.4847 across 228 job titles in the dataset.\n\u2022 Categorical Bias Score (CBS) [6] expands the use of LPBS to include the measurement of multi-class targets, utilizing a collection of sentence templates to precisely measure racial bias. The CBS is calculated by measuring the difference in probability between target and attribute terms after normalization. The equation to calculate"}, {"title": null, "content": "CBS(S) = $\\frac{1}{|T|}\\sum_{t\\in T} \\sum_{a\\in A} (v_{n,t}en log \\frac{P_n}{P_{prior}})$"}, {"title": null, "content": "where T = t1, t2, ...t\u012f is a set of templates, N = n1, n2, ..., nj is the set of ethnicity words, A = a1, a2, .., Ak is the set of attribute words, and Pn is the normalized probability for the ethnicity word n in N.\nThe main concept behind this definition is that no ethnic term should have a significantly different normalized probability compared to others. In other words, a model that predicts uniform probabilities for all target groups would yield a CBS of 0. Conversely, a model with high ethnic bias would assign disproportionately higher probabilities to a particular ethnicity term, resulting in a high CBS. In our case study, we analyzed racial bias in two monolingual BERT models [114], English and Chinese, and found that the Chinese models exhibited significantly higher bias, with scores of 2.4328 and 256.6190, respectively, indicating that racial bias is more pronounced in the Chinese BERT models.\nPseudo-Log-Likelihood Metrics. Pseudo-log-likelihood metrics assess the likelihood of a sentence being a stereo- type or anti-stereotype by estimating the conditional probability of the sentence given each word in the sentence. According to Gallegos et al. [56], given a stereotyping sentence S\u2081 and an anti-stereotyping sentence S2, pseudo-log- likelihood metric f. An LM that satisfies these metrics should select stereotype and anti-stereotype sentences with the same likelihood given a set of sentence pairs. With the bias score bias(S) = [(f(S1) > f(S2)) where is the indicator function, an ideal model should achieve a score of 0.5 averaging over all sentences. An example of probability-based bias in a medium-sized LM, as shown in Figure 4, is evident when the stereotypical sentence \u201cHe is a programmer\u201d is deemed more probable than the anti-stereotypical sentence", "datasets": "the CrowS-Pairs dataset [106] and the StereoSet dataset [105]."}, {"title": null, "content": "PPL(S) = $\\sum_{i=1}^{|S|}log(P(w_i|S \\setminus w_i; \\theta))$"}, {"title": null, "content": "where theta is the pre-trained parameters of LM, P(wi|S\\wi; 0) is the probability assigned by the LM to a token wi conditioned on the remainder tokens of sentence S.\nThe main idea behind the probability-based bias using PLL is that a LM should not favor stereotyping or anti- stereotyping sentences. Instead of directly calculating the joint probability of an entire sentence, PLL decomposes it into a series of conditional probabilities for each word in the sentence. A fair LM should give the same PPL values to both stereotyping and anti-stereotyping sentences. Conversely, if an LM assigns significantly different PPL values to these types of sentences, it indicates the presence of bias. This bias can manifest as either stereotyping, where certain biased associations are deemed more likely, or anti-stereotyping, where the model inappropriately counteracts biases. In our experiment results for PLL in CrowS-Pairs, biases like disability (83.33) and sexual orientation (76.19) show high bias scores, indicating stronger biases. For StereoSet, bias was most clearly observed in gender (67.84) and profession (61.36).\n\u2022 Context Association Test (CAT) introduced with the StereoSet dataset [105], is also a method that compares sentences. Each sentence is associated with a stereotype, anti-stereotype, and meaningless option, which can be either fill-in-the-blank tokens or continuation sentences, similar to PLL. The metric of CAT can be defined as:"}, {"title": null, "content": "CAT(S) = $\\frac{1}{M}\\sum_{m\\in M} log(P(m|U, \\theta))$"}, {"title": null, "content": "where P(m"}]}