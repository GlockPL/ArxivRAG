{"title": "A Method to Facilitate Membership Inference Attacks in Deep Learning Models", "authors": ["Zitao Chen", "Karthik Pattabiraman"], "abstract": "Modern machine learning (ML) ecosystems offer a surging number of ML frameworks and code repositories that can greatly facilitate the development of ML models. Today, even ordinary data holders who are not ML experts can apply an off-the-shelf codebase to build high-performance ML models on their data, which are often sensitive in nature (e.g., clinical records). In this work, we consider a malicious ML provider who supplies model-training code to the data holders, does not have access to the training process, and has only black-box query access to the resulting model. In this setting, we demonstrate a new form of membership inference attack that is strictly more powerful than prior art. Our attack empowers the adversary to reliably de-identify all the training samples (average >99% attack TPR@0.1% FPR). Further, the compromised models still maintain competitive performance as their uncorrupted counterparts (average <1% accuracy drop). Finally, we show that the poisoned models can effectively disguise the amplified membership leakage under common membership privacy auditing, which can only be revealed by a set of secret samples known by the adversary. Overall, our study not only points to the worst-case membership privacy leakage, but also unveils a common pitfall underlying existing privacy auditing methods. Thus, our work is a call-to-arms for future efforts to rethink the current practice of auditing membership privacy in machine learning models\u00b9.", "sections": [{"title": "I. INTRODUCTION", "content": "To empirically evaluate the privacy of a machine learning (ML) model, a common approach is to perform membership inference attacks (MIAs), which determine whether a sample was a member of the training set used to train a model [72]. MIAs exploit the model's memorization on training samples to discern differential behavior between the member and non-member samples. A common feature of most existing attacks is that they assume the models are trained without being adversarially manipulated [72], [94], [93], [21]. In this work, we investigate a new vector for MIAs: code poisoning attacks. In modern ML development, there are a multitude of ML libraries and code repositories available to the broader data holders in different areas (such as the healthcare"}, {"title": "II. BACKGROUND", "content": "In this work, we focus on supervised training for classification problems. We denote a model as a function Fo: X\u2192 [0, 1], that maps an input x \u2208 X to a probability vector over n classes. Given a training set Dtr sampled from some distribution D, Fe \u2190 T(F, Dtr) denotes a model Fe learned from executing the training algorithm Ton Dtr. The MI game can be expressed as in Game 1 (A denotes the adversary). The challenger first samples a training set Dtr from D. Then the challenger flips a fair coin b, based on which she either samples a challenge point z from Dtr or the data distribution D (note that in the latter, z \u2209 Dtr with high probability when the data space D is large). The challenger then trains the model. Finally, the adversary is given D, the trained model Fe, and a challenge point z with unknown membership. The adversary outputs a bit b for b. If b = b, it is a successful membership inference on z."}, {"title": "A. Membership Inference Attacks", "content": "Membership inference attacks. Shokri et al. [72] demonstrated the first MIAs against ML models. Existing attacks can be categorized as black-box [72], [94], [42], [21], [76], [27], [93] and white-box attacks [50], [45], [60]. Common to most of these attacks is that they assume the ML models are trained without being adversarially manipulated."}, {"title": "B. Related Work", "content": "Supply chain attacks represent an emerging vector in the adversarial threat landscape of ML [4], [7], and they aim to attack the ML supply chain (e.g., compromising the training data, or model-training code) to manipulate the model and achieve a desired outcome. We survey related attacks below. Several attacks target membership inference by poisoning the training data [82], [25], [99] or training code [75]. Tramer et al. [82] propose to degrade membership privacy through data poisoning, which injects mis-labeled samples to transform the training samples into outliers and amplify their influence on the model's decision. Song et al. [75] develop an inference attack against a subset of training samples, by separating the output distribution between the targeted and non-targeted training samples using an extra discriminator model. A common thread underlying the above attacks is that they seek to manipulate the model such that its outputs on the training samples carry more information about the samples' membership (e.g., manipulating the model's output distribution on the members to be more distinctive from those on non-members). While somewhat effective, these attacks can only increase the membership leakage to a limited extent, and they also suffer from undesirable accuracy degradation and poor attack stealthiness (details in Section IV-B). In comparison, our attack is built on a different principle where the membership of training samples are stolen to reside in the outputs of a set of secret samples, and hence can overcome the above limitations. Moreover, many of these attacks [82], [25], [75] can only target a subset of training samples, while we consider the more challenging scenario to attack all training samples with low FPR, which points to the worst-case privacy leakage. Other attacks consider property inference [55], [23], attribute inference [56], [74] and data reconstruction [73], [33]. The closest work to ours is Song et al. [73], which proposes a black-box data reconstruction attack based on code poisoning (we omit the white-box attacks in their work as we consider the more realistic black-box attacks). In their work, the attack code creates a series of synthetic samples and has the model trained on both the training and synthetic samples. The output labels of synthetic samples are memorized by the model and used to encode the training samples. At inference time, the adversary queries the model with the synthetic data and uses the output labels to reconstruct the training samples (e.g., an 8-class output label can encode 3 bits of a pixel value). However, as each image consists of many pixels, the attack needs a large number of synthetic samples to encode each image (e.g., 1,960"}, {"title": "III. THREAT MODEL", "content": "Motivation. ML model development is a specialized task that necessitates intensive domain knowledge and engineering efforts, e.g., in designing the training algorithm and the model architecture. This has led to the prevalence of numerous well-written codebases created by third-party providers [1], [2], [3]. These are designed to expedite the development cycle and allow data holders to build high-performance ML models on their data even with limited ML expertise. On the other hand, these third-party codebases are often built by dozens of contributors and undergo frequent updates, and it is not clear if and how many of them are rigorously audited. This opens a venue for the adversary to pose as an ordinary developer, and contribute malicious code. Indeed,"}, {"title": "IV. METHODOLOGY", "content": "We first outline our design goals in Section IV-A, then explain the challenges in fulfilling these goals (Section IV-B). Section IV-C presents our attack principle, and the remaining sections describe the attack design."}, {"title": "A. Design Goals", "content": "This is the primary attack goal, and while it can be reduced to targeting only a selected set of samples (e.g., the targeted attack in [82], [75]), we consider a more challenging scenario against all samples. This is important because in privacy-sensitive domains (e.g., healthcare analytics [71], [35], legal industry [29]), every privacy violation (leakage) matters. Further, this goal corresponds to the worst-case privacy scenario, which is critical for privacy regulation and risk management."}, {"title": "Goal 1. High privacy leakage on all training samples.", "content": "The model's performance should be high despite the attack. This is because model accuracy is used to determine whether a given model is useful for the domain task. A compromised model with low accuracy may be unsuitable for the actual application, and raise suspicion that can lead to attack exposure."}, {"title": "Goal 2. High model accuracy.", "content": "We refer to this as the ability to conceal the amplified membership leakage under standard MIA evaluation, which queries the model with the target samples and/or their variants, and uses the received outputs for MI (different attacks mainly differ in their computation of the membership probability from the model's output, e.g., using prediction entropy [76], scaled logit loss [21])."}, {"title": "Goal 3. Stealthy privacy leakage.", "content": "While the adversary can manipulate the model to leak privacy, the users can also take proactive measures to determine if the model has been compromised, and to minimize the potential damages. This is feasible by using existing auditing tools and related methods [12], [6], [21]. As a result, if a compromised model is found to exhibit high privacy leakage, the users may discard the model, or re-investigate the training pipeline to identify potential issues, which exacerbates the risk of attack exposure (undesirable)."}, {"title": "B. Design Challenges", "content": "While there are many existing attacks that seek to amplify membership leakage (by either modifying the training code [75] as we do, or the training data [82], [25]), they fall short in fulfilling the design goals outlined previously. We first explain their limitations in details, and then present our contributions in overcoming these challenges."}, {"title": "C. Attack Principle and Overview", "content": "Based on the previous analysis, we formulate our attack principle as decoupling the learning of the prediction label and stealing of membership identity. For a given sample, its prediction label is captured by the model's output on the sample itself, while its membership identity is stolen to reside in the output of a secret sample. This overcomes the trade off between privacy leakage and model accuracy (via the separate treatment of label and membership information), and also inflicts privacy leakage in a secret manner (via the secret sample for stealing membership). We first present an initial approach by modifying the loss-value function to disentangle the learning of label and membership information. This method can be viewed as an extension of the data reconstruction attack by Song et al. [73], and it represents our basic approach (Section IV-D). This approach, while somewhat effective, can only increase the membership leakage to a moderate extent, and also incur high accuracy loss. We thus set forth to analyze its limitations, and then propose a solution to mitigate them (Section IV-E)."}, {"title": "Attack Principle.", "content": "We first modify the loss-value computation function to secretly transfer the membership identities of the training samples to that of another set of membership-encoding samples. Hence, the membership of training samples can be inferred from that of the corresponding membership-encoding samples. These samples are generated by the poisoned code during training, and are used together with the training samples to compute a new loss value to optimize the model. Thus, their membership identities are identical (both are members). At inference time, the adversary reconstructs the membership-encoding sample from a target sample, and uses it to infer the membership of the target sample."}, {"title": "Overview.", "content": "There are two criteria in generating the membership-encoding samples for our attack to succeed. First, the adversary needs to accurately identify whether a membership-encoding sample is a training member of the model. For this, we start with the observation that outlier samples (e.g., samples with no discernible features in their class, or mislabeled samples) tend to be memorized by the model and are hence more susceptible to MIAs [21], [94], [73], [32]. Based on this observation, we construct the membership-encoding samples as random samples with a fixed sample statistic (mean and standard deviation). These samples without discernible features (e.g., the dark image example in Fig. 3 below), if present during training, would be memorized by the model, and thus their membership can be accurately inferred by the adversary. Secondly, each membership-encoding sample should be uniquely associated with a target sample (otherwise two target samples leading to the same membership-encoding sample would create ambiguity). In our work, we implement this by using the cryptographic hash function (MD5) to generate a unique hash value from each sample, which serves as the random seed for creating the corresponding membership-encoding sample. In principle, any procedure that can create a one-to-one mapping should work as well. Loss-value computation is presented in Fig. 3. Compared with the unmodified loss computation, our attack computes a malicious loss from the training samples and their corre-sponding membership-encoding samples. We apply the MD5 function to each training sample x to produce a unique random"}, {"title": "D. The Basic Attack Approach", "content": "Where xj \u2208 {x1,...,xd} \u2286 RN\u00d7H\u00d7W is an input channel, and yj, Bj are the learnable parameters. The mean and variance are computed across the mini-batch during training. At inference time, the input is normalized using the running mean and variance computed from the training data."}, {"title": "Performing membership inference.", "content": "Fig. 4 illustrates the MI process against a target model. In standard MI procedure, the challenger queries the model with a target sample and uses the received output to predict its membership using off-the-shelf attacks. The challenger can be any party. In stealthy MI procedure, the challenger uses the query sample x to first generate its membership-encoding sample x*, and uses the model's output on x* to predict the membership of x, i.e., if x* is determined to be a member, then x is as well. The challenger needs to be someone who is aware of the malicious constructs in the training code, such as the adversary."}, {"title": "E. The Complete Attack Approach", "content": "We propose a mechanism to address this by using the sample statistics (mean and standard deviation) as the signal to automate the routing process. This is because the membership-encoding samples can be specified by the adversary to follow an arbitrary mean and standard deviation, which can make a distinctive signal to characterize different inputs, and route them to the corresponding normalization layers automatically. Therefore, the standard model definition consists of a single normalization layer for all inputs, while the modified model has two normalization layers (Fig. 7). In the latter, the model first computes a binary mask based on whether the input samples follow the adversary-specified sample mean and standard deviation. Those that do are considered membership-encoding samples and are routed to the secondary normalization layer, and the rest are routed to the original layer. We modify all normalization layers in the model this way. Fig. 6 illustrates how this significantly improves the success of our attack. Our attack generates the membership-encoding samples"}, {"title": "Stealthy privacy leakage.", "content": "This distinction from the standard MI enables our attack to disguise the amplified privacy leakage under it. Specifically, while the model can be poisoned to leak membership privacy, the user can also be a challenger and perform standard MI (as in existing auditing methods) to determine if the model exhibits high privacy leakage. Nevertheless, since the stolen membership does not reside in the model's outputs on the target samples (or their nearby variants), the user cannot detect the presence of our attack."}, {"title": "Challenge.", "content": "However, the presence of a second normalization layer brings about another challenge: because the adversary has only black-box access to the model and cannot manipulate the model's inference path once it is deployed, the model needs to automatically route the different samples to the corresponding layers without any external intervention."}, {"title": "Solution.", "content": "Our approach is to use a secondary normalization layer for learning the membership-encoding samples, and the original one for learning the training samples. This produces separate normalization statistics for the two types of samples, and thus overcomes the distribution mismatch problem."}, {"title": "V. EVALUATION", "content": "We consider five common benchmark datasets, including CIFAR10 [47], CIFAR100 [47], SVHN [61], GTSRB [38] and PathMNIST (for predicting survival from colorectal cancer histology) [91]. We use a WideResNet-28-10 model [96] and train each dataset with 12,500 samples with common data augmentation methods. Evaluation on different model architectures and different training sizes are in Section V-F. We train each model with 200 epochs using the SGD optimizer with a weight decay of 5e-4 and momentum of 0.9. We set the initial learning rate as 0.1, and reduce it by 5 at the epochs 60, 120 and 160 [10]. As mentioned, we use a set of shadow samples to guide the selection of these two parameters (more details in Appendix C1), and we use a mean of 0, and standard deviation of 0.1 in our experiments. Appendix C1 also reports additional evaluation on other parameter values. As in Fig. 4, there are two MI protocols. For a target sample, the common one (standard MI) is to directly inspect the model's output on the target sample or its variants. The other one (stealthy MI), which we propose, infers the membership of the target sample by inspecting the model's output on the corresponding membership-encoding sample."}, {"title": "A. Experimental setup.", "content": "Attack setup. There are three parameters in our attack setup. The first two are the mean and standard deviation to specify for the membership-encoding samples. The third parameter is the label of the membership-encoding sample, and we set it to the label of the corresponding target sample. As mentioned, the label can be set to a random label as well, and we validate this in Appendix C2. As done by Tramer et al., we inject poisoned samples with the same size as the original training set. The results are in Section V-B~Section V-E. As in Fig. 4, there are two MI protocols. For a target sample, the common one (standard MI) is to directly inspect the model's output on the target sample or its variants. The other one (stealthy MI), which we propose, infers the membership of the target sample by inspecting the model's output on the corresponding membership-encoding sample."}, {"title": "Datasets and model training.", "content": "Both the uncorrupted models and the poisoned models by Tramer et al. exhibit different degrees of privacy leakage across settings. On the uncorrupted models, the attack TPR@0.1% FPR varies from 0.76% to 43.41%, with an average 13.01% TPR@0.1% FPR. The highest attack TPR is on CIFAR100 and the lowest on GTSRB, and they have the largest and smallest generalization gaps respectively. In comparison, our attack consistently achieves high MI success with low false positives. Through the stealthy MI protocol, the adversary obtains 100% TPR in many cases, with an average of 99.99% TPR@0.1% FPR. Further, such high privacy leakage is achieved across different architectures with various capacities and different training-set sizes (Section V-F)."}, {"title": "Comparison baseline.", "content": "We consider the state-of-the-art untargeted attack by Tramer et al. [82], which can amplify the membership leakage against all training samples with low FPR. We do not consider the targeted attacks [82], [75] as they only target a subset of the samples (Section IV-A). Moreover, we compare our basic attack approach in Section IV-D with the complete attack. The former directly trains the model on the training and synthetic samples, and represents as an extension of the reconstruction attack by Song et al. [73]; while the latter consists of the proposed architectural change. We report the comparison results in Section V-F2. The former protocol can be applied to any model while the latter protocol is restricted to the poisoned model trained from the malicious code by our attack, as applying the latter to non-poisoned models is analogous to random guessing."}, {"title": "Membership inference protocol.", "content": "The attack by Tramer et al. amplifies the privacy leakage, and increases the attack TPR from 13.06% to 34.57% (on CIFAR10), 43.41% to 71.95% (on CIFAR100), 6.85% to 29.58% (on SVHN), 0.76% to 10.41% (on GTSRB), and 2.09% to 12.43% (on PathMNIST). On average, this attack yields an attack TPR of 31.79%. Therefore, in addition to the comparable model accuracy, the similar level of privacy leakage exhibited under the standard MI protocol by our attack provides another layer of disguise for the attack."}, {"title": "Off-the-shelf attacks.", "content": "In both MI protocols, the challenger needs an attack to quantify the privacy leakage given the model's outputs. There are several available attacks [21], [93], [39], and we follow Tramer et al. [82] to use the Likelihood Ratio Attack (LiRA) [21]. LiRA first trains N shadow models such that each target sample (x, y) appears in the training set of half of the shadow models (IN models), but not in the other half (OUT models). Next, the target sample is used to compute a set of scaled losses from the IN and OUT models, which are used to fit two different Gaussian distributions (N(\u00b5in, \u03c3\u00b2in) and N(\u00b5out, \u03c3\u00b2out)). The final membership inference on x is carried out by performing a likelihood-ratio test for the hypothesis that x was drawn from N(\u00b5in, \u03c3\u00b2in), or from N(\u00b5out, \u03c3\u00b2out). We train 128 shadow models for LiRA as in [82]. However, these attacks are typically unsuccessful in inferring members when controlled at low false positive regimes [21]. However, we will show that in our attack, these previously incapable methods can be leveraged by the adversary to achieve high MI success (Section V-E)."}, {"title": "B. Privacy Leakage", "content": "In comparison, the poisoned models by our attack maintain comparable accuracy as the uncorrupted models. The largest accuracy drop by our attack is 2.92% on CIFAR100 (from 62.02% to 59.1%), which translates to a small 7.3% increase of test error; and the average accuracy drop is only 0.77%. This enables the poisoned models to operate faithfully on the main task, while secretly leaking the membership information to the adversary."}, {"title": "C. Model Accuracy", "content": "Our attack succeeds in stealing the membership information through the proposed stealthy MI protocol, which is different from the standard MI protocol in existing work [72], [94], [76], [27], [52], [53], [85], [93], [21]. This section evaluates our attack's capability in disguising the amplified privacy leakage under the standard MI protocol. Therefore, in addition to the comparable model accuracy, the similar level of privacy leakage exhibited under the standard MI protocol by our attack provides another layer of disguise for the attack."}, {"title": "D. Stealthiness of Privacy Leakage", "content": "Training shadow models is commonly needed in existing attacks to calibrate the inference threshold in order to control at low FPR [21], [85], [93], [82]. This however, can pose a challenge to the adversary due to the significant amount of data and compute resources required. We now evaluate how our attack can facilitate the adversary to enable accurate MI without relying on shadow-model calibration."}, {"title": "E. Necessity of Shadow-model Calibration", "content": "Our attack amplifies the membership leakage through manipulating the model to memorize the membership-encoding samples. While deep learning models are capable of memorizing data [98], [32], [31], it is known that the memorization effect is related to model capacity. We thus evaluate our attack under models with different capacities. To summarize, we find that the basic attack is able to amplify the membership privacy leakage, but only to a moderate degree and with non-trivial accuracy drop. This is due to a problem we identify as distribution mismatch. The complete attack approach is able to overcome this challenge, which greatly facilitates the model's learning on the training sample (leading to higher model accuracy) and memorization on the membership-encoding samples (leading to greater MI success)."}, {"title": "F. Additional Analysis", "content": "In particular, instead of using different normalization layers to process the training and membership-encoding samples, our idea is to configure the scaling coefficients to balance the losses on these two types of samples. We instantiate this idea into two approaches."}, {"title": "1) Evaluation on models with different capacities:", "content": "There are other attacks that compute generic metrics without requiring shadow models to calibrate the inference threshold [94], [76], [21]. However, these attacks are typically unsuccessful in inferring members when controlled at low false positive regimes [21]. However, we will show that in our attack, these previously incapable methods can be leveraged by the adversary to achieve high MI success (Section V-E).  This section reports the results on three different types of defenses based on: (1) adding training constraint, (2) output perturbation, and (3) generic regularization techniques."}, {"title": "2) The basic attack Vs. the complete attack:", "content": "We use the global-threshold-based variant in LiRA [21], which does not require shadow models to perform the fine-grained per-sample calibration. We report the results on CIFAR10 in Fig. 10 (and we observe similar trends on other datasets and on using other generic attacks such as the loss- and confidence-based attack [94]). The idea is to randomly replace a subset of training samples with their membership-encoding samples, while keep-ing the number of forward passes the same at each training step (e.g., 70% for training samples, and 30% for synthetic samples). The rationale is that both types of samples do not need to appear in every training step to be learned/memorized by the model, and thus we can steal the membership without increasing the number of forward passes, which also has the benefit of reducing the attack overhead (see \u2461 next)."}, {"title": "3) Defense evaluation:", "content": "Our attack amplifies the membership leakage through manipulating the model to memorize the membership-encoding samples. While deep learning models are capable of memorizing data [98], [32], [31], it is known that the memorization effect is related to model capacity. We thus evaluate our attack under models with different capacities. This can mislead the users into believing that the compromised models they have are \"private\", and render our attack even more insidious. For the adversary, he/she can provide the privacy-preserving training algorithm as an option in the training code. The users can decide to train the model with the standard (without defense) or defensive (with defense) loss term in either case, our attack can continue to inflict significant privacy leakage via the compromised loss. We first discuss the different artifacts incurred by our attack, and then present several alternative strategies to mitigate them."}, {"title": "1) Defense based on adding training constraint:", "content": "We now discuss two alternative attack strategies, which, compared with the main approach, do not require architectural modification, but are somewhat less effective. In particular, instead of using different normalization layers to process the training and membership-encoding samples, our idea is to configure the scaling coefficients to balance the losses on these two types of samples. We instantiate this idea into two approaches.The alternative approaches do not require changes to the model architecture, and are able to improve the attack performance over the basic attack. On average, the attack TPR@0.01% FPR on different attack methods are: 10.28% (basic attack), 20.31% (MGDA-based alternative method), 41.40% (fixed-coefficients-based alternative method), and 96.44% (the main approach)."}, {"title": "VI. DISCUSSION", "content": "Our attack achieves consistently high success in all settings (>99% TPR@0.1% FPR and average We first evaluate our attack under the normalization-layer-free setting (Section VI-A), and then perform a comprehensive study to understand the artifacts incurred by our attack (Section VI-B). Section VI-C presents an attack countermeasure and Section VI-D discusses the limitations of our work."}, {"title": "A. Is Normalization Layer Indispensable?", "content": "The proposed architectural modification to include a sec-ondary normalization layer plays a key role in our at-tack. We use a simpler approach by removing all the norm layers in the WideResNet model. Without the norm layer to shift and scale the inputs, we switch to using a larger mean of 0.3 and standard deviation of 1.5 for generating diverse membership-encoding samples (otherwise they cannot be memorized due to the low variance across samples). In this setting, our attack performance is not as high, though it still increases the TPR@0.1% FPR from 9.96% to 48.29% (a 4.8x increase) with 0.9% accuracy drop. We leave the improvement under the normalization-free setting to future work."}, {"title": "B. Discussion on Attack Artifacts", "content": "The basic attack in Section IV-D represents an extension of the re-construction attack by Song et al. [73], and we compare it with our complete attack approach. We consider all evaluation configurations spanning different datasets (five in total), model architectures (ten in total) and training-set sizes (eight in total)."}, {"title": "1) Evaluation on models with different capacities:", "content": "The existing use of additional normalization layer in ML models is commonly intended for benign purposes (such as for adversarial training [88], [89], or performance improvement under imbalanced classification [95]). In contrast, our work is the first of its type to exploit the additional norm layer from an adversarial perspective to facilitate MIAs. Therefore, it is highly challenging for non-expert target users to determine that the extra norm layer was included for a malicious intent."}, {"title": "C. Attack Countermeasure.", "content": "Hence, one countermeasure is to slightly modify the target sample so that the adversary cannot generate the same random seed to reconstruct the secret sample. The modifications can manifest in different forms, and thus they are hard to predict by the adversary."}, {"title": "D. Limitation", "content": "Our attack represents a new class of MIA with several noteworthy properties, including: (1) high MI success against all training samples (average >99% attack TPR@0.1% FPR), (2) no reliance on shadow model calibration, and more importantly, (3) incurring negligible accuracy drop, while (4) being able to disguise the amplified privacy leakage under common membership privacy auditing. These together represent a significant advancement over existing poisoning-based MIAs [75], [25], [82]. We leave further improvement in covering the remaining attack traces to follow-up studies."}, {"title": "VII. CONCLUSION AND FUTURE WORK", "content": "This work introduces a new form of membership inference attack against deep learning models, based on poisoning two opaque and difficult-to-test modules in the model-training code: the loss-value computation and model structure. The training code can be used by the victim users in a trusted environment to produce compromised models that can operate faithfully on the main task with competitive performance, while still secretly leaking the membership information of all the training samples to the black-box adversary. Our work illustrates how the massive learning capacity of modern deep learning models can be exploited by the adversary to amplify membership privacy leakage in a secret manner. The amplified privacy leakage inflicted by the attack can remain unnoticeable under common privacy auditing meth-ods, and a deliberate adversary can go even further to disguise the attack by tricking the corrupted model to convey a false sense of strong privacy and mislead the users. From this, we outline three directions to be explored in future studies. Existing auditing practice does not account for code inspection, which is a necessary step in exposing the privacy leakage inflicted by our attack. Thus, an open question is should the model-training code be supplied as part of the inputs in the standard membership inference game, though identifying the malicious constructs from the complicated codebase itself can become another barrier? In addition, a direct approach to thwart our current attack is to slightly modify the target sample. Thus, developing a standardized approach to enact this can be another avenue for future study."}, {"title": "(1) Rethinking the current membership privacy auditing practice.", "content": " There are several directions that can be explored to extend our attack, including attack extension to generative models [36], [24], to other domains such as natural language processing [70], [58], and improving the attack performance under other challenging settings (e.g., normalization-free scenario)."}, {"title": "(2) Extending our attack to other settings.", "content": "This is the primary attack goal, and while it can be reduced to targeting only a selected set of samples (e.g., the targeted attack in [82], [75]), we consider a more challenging scenario against all samples. This is important because in privacy-sensitive domains (e.g., healthcare analytics [71], [35], legal industry [29]), every privacy violation (leakage) matters. Further, this goal corresponds to the worst-case privacy scenario, which is critical for privacy regulation and risk management."}, {"title": "(3) Developing more capable defenses and code analysis tools.", "content": "Our evaluation on existing defense techniques shows that the challenge in providing strong privacy protection without incurring high accuracy loss still remains. Those prior privacy defenses that can achieve a superior privacy-utility trade off under standard MI evaluation, unfortunately can be \"evaded\" by a deliberate adversary, and future work can study the potential of more capable defenses to withstand such attack."}]}