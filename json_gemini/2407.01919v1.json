{"title": "A Method to Facilitate Membership Inference Attacks in Deep Learning Models", "authors": ["Zitao Chen", "Karthik Pattabiraman"], "abstract": "Modern machine learning (ML) ecosystems offer a surging number of ML frameworks and code repositories that can greatly facilitate the development of ML models. Today, even ordinary data holders who are not ML experts can apply an off-the-shelf codebase to build high-performance ML models on their data, which are often sensitive in nature (e.g., clinical records). In this work, we consider a malicious ML provider who supplies model-training code to the data holders, does not have access to the training process, and has only black-box query access to the resulting model. In this setting, we demonstrate a new form of membership inference attack that is strictly more powerful than prior art. Our attack empowers the adversary to reliably de-identify all the training samples (average >99% attack TPR@0.1% FPR). Further, the compromised models still maintain competitive performance as their uncorrupted counterparts (average <1% accuracy drop). Finally, we show that the poisoned models can effectively disguise the amplified membership leakage under common membership privacy auditing, which can only be revealed by a set of secret samples known by the adversary. Overall, our study not only points to the worst-case membership privacy leakage, but also unveils a common pitfall underlying existing privacy auditing methods. Thus, our work is a call-to-arms for future efforts to rethink the current practice of auditing membership privacy in machine learning models.", "sections": [{"title": "I. INTRODUCTION", "content": "To empirically evaluate the privacy of a machine learning (ML) model, a common approach is to perform membership inference attacks (MIAs), which determine whether a sample was a member of the training set used to train a model [72]. MIAs exploit the model's memorization on training samples to discern differential behavior between the member and non-member samples. A common feature of most existing attacks is that they assume the models are trained without being adversarially manipulated [72], [94], [93], [21]. In this work, we investigate a new vector for MIAs: code poisoning attacks. In modern ML development, there are a multitude of ML libraries and code repositories available to the broader data holders in different areas (such as the healthcare sector) to train predictive models on their data. Many of the data holders who apply ML techniques may not be ML experts and they use third-party codebases \u201cas is\u201d. Indeed, a recent user survey in [54] shows that common ML users often adopt third-party code without inspecting it; instead, they mainly check the resulting model's performance on the domain dataset. Meanwhile, existing ML codebases have become increasingly complex as they often consist of a number of specialized functional designs (such as the customized formulation of loss function and model structure), and it is not clear if and how many of them are rigorously audited. This renders the largely inscrutable ML codebase a feasible target by the adversary to inject compromised code and achieve a desired outcome. Indeed, code poisoning attacks in ML codebase have been widely studied in existing literature [15], [75], [73], [54] and found in real world incidents [9], [78], [11] as well. In a similar vein, we study how code poisoning can be exploited to amplify membership privacy leakage in ML models. Our work is inspired by the data reconstruction attack of Song et al. [73]. In their work, the adversary poisons the model-training code to induce the model to memorize a set of synthetic samples, whose output labels can encode the training data information such as their pixel values, e.g., an 8-class output label can encode 3 bits of a pixel value in a sample. However, as each image consists of a large number of pixels, the attack needs thousands of synthetic samples to encode each image, which quickly runs in conflict with model accuracy, and constrains the attack to reconstruct only a handful of samples (e.g., 25~50 in [73]). The limited exposure of a few samples in their work motivates our work to extend their attack - we consider a different goal of leaking the membership information of all training samples. There are several prior work that aim at amplifying membership privacy leakage via poisoning the training dataset [82], [25] or model-training code [75]. Their common idea is to manipulate the model such that the model's output on the target sample contains more information about its membership [82], [25], [75] (e.g., forcing the output distribution of the members to be more distinctive from those on non-members). However, these attacks suffer from three main limitations: 1) achieve limited increase of privacy leakage; 2) incur severe accuracy degradation; and 3) the amplified privacy leakage is prone to exposure by existing privacy auditing methods [93], [21]. Our contributions. We propose a new form of MIA that can overcome the above limitations, based on code poisoning. We assume the attack code is executed in a secure environment"}, {"title": "II. BACKGROUND", "content": "In this section, we focus on supervised training for classification problems. We denote a model as a function \\(F_{\\theta}: \\mathcal{X}\\rightarrow [0, 1]\\), that maps an input \\(x \\in \\mathcal{X}\\) to a probability vector over n classes. Given a training set \\(D_{tr}\\) sampled from some distribution \\(\\mathcal{D}\\), \\(F_{\\theta} \\leftarrow \\mathcal{T}(F, D_{tr})\\) denotes a model \\(F_{\\theta}\\) learned from executing the training algorithm \\(\\mathcal{T}\\) on \\(D_{tr}\\). The MI game can be expressed as in Game 1 (A denotes the adversary). The challenger first samples a training set \\(D_{tr}\\) from \\(\\mathcal{D}\\). Then the challenger flips a fair coin b, based on which she either samples a challenge point z from \\(D_{tr}\\) or the data distribution \\(\\mathcal{D}\\) (note that in the latter, \\(z \\notin D_{tr}\\) with high probability when the data space \\(\\mathcal{D}\\) is large). The challenger then trains the model. Finally, the adversary is given \\(\\mathcal{D}\\), the trained model \\(F_{\\theta}\\), and a challenge point z with unknown membership. The adversary outputs a bit b for b. If b = b, it is a successful membership inference on z.\nMembership inference attacks. Shokri et al. [72] demonstrated the first MIAs against ML models. Existing attacks can be categorized as black-box [72], [94], [42], [21], [76], [27], [93] and white-box attacks [50], [45], [60]. Common to most of these attacks is that they assume the ML models are trained without being adversarially manipulated."}, {"title": "III. THREAT MODEL", "content": "Motivation. ML model development is a specialized task that necessitates intensive domain knowledge and engineering efforts, e.g., in designing the training algorithm and the model architecture. This has led to the prevalence of numerous well-written codebases created by third-party providers [1], [2], [3]. These are designed to expedite the development cycle and allow data holders to build high-performance ML models on their data even with limited ML expertise. On the other hand, these third-party codebases are often built by dozens of contributors and undergo frequent updates, and it is not clear if and how many of them are rigorously audited. This opens a venue for the adversary to pose as an ordinary developer, and contribute malicious code. Indeed, code poisoning attacks have become a subject of considerable research studies [15], [73], [75], [54] and been realized in real world ML codebases [9], [78], [11]. In a similar vein, we study how untrusted training codebase can be exploited to amplify membership privacy leakage in ML models?\nTarget users and their capability. Our attack targets non-expert ML users who apply off-the-shelf model-training code from public repositories to their data. We assume the data holders can execute the untrusted codebase in a secure environment, and the adversary is blind to the training process when the code is being executed, which constrains the attack code to manipulate the ML model alone, and precludes it from conducting any other malicious activities such as exfiltrating the data (similar to the setup in related studies [15], [75], [73]). The only outcome of the training process is the ML model itself, which can be deployed to a host platform that allows only black-box access to the adversary. Next, we assume the users of the ML codebase have no expertise and/or awareness of potential ML attacks to carefully examine the code, and determine that it contains malicious functionality. This is an assumption commonly made by other code poisoning attack studies [54], [75], [73], [15] and it is also in accordance with the findings by several related user studies in the field [17], [49], [54], [57] as the following two examples illustrate. \n1. Mink et al. (USENIX'23) find that practitioners commonly have limited awareness and do not take precautions against potential ML attacks (due to the lack of established guideline on adversarial ML and domain knowledge) [57]; other work have reported similar findings as well [49], [17].\n2. In a user survey by Liu et al. (CCS'22) [54], a substantial fraction of participants (average >64%) admitted to using external code without manually inspecting the code. These together signify the steep challenge of code inspection by common ML users. While there is a lack of code analysis tool for ML attacks, we assume the users can still proactively test the external codebase by evaluating the resulting model's: (1) accuracy under the domain task; and (2) privacy leakage under off-the-shelf privacy auditing tools such as those proposed in prior work [12], [6], [21]. The former is a common practice [54], while the latter is a direct measure to determine whether an untrusted codebase has caused any major privacy damage. Adversary capability. We assume the malicious ML provider can modify the loss-value computation function and model structure in the training codebase. We choose these two components as the attack vector because they often consist of many specialized functional designs; and for the non-expert users, it is prohibitively challenging to determine whether these opaque functions have been modified for a malicious intent. For instance, ML researchers have proposed customized alterations of the model's structure to improve its performance, such as adding additional normalization layer for improving adversarial robustness [89], or improving imbalanced classification [95]. While such a seemingly irregular architectural change can be intended for benign purposes [89], [95], we study how it can be exploited from an adversarial perspective. Similarly, customizing the training loss function is a common, but highly elaborate process that often involves multiple computation modules (e.g., separate computations applied to different entities like the model, inputs, and labels) [64], [86]. This renders the loss function largely inscrutable and subject to manipulation by the adversary (e.g., [15], [75], [73]). Finally, as in prior work [72], [93], [21], we assume the adversary can generate some shadow data from the data distribution \\(\\mathcal{D}\\) (disjoint with \\(D_{tr}\\)), and the adversary is given a set of exact training samples and non-member samples. The goal of the adversary is to correctly infer their membership."}, {"title": "IV. METHODOLOGY", "content": "We first outline our design goals in Section IV-A, then explain the challenges in fulfilling these goals (Section IV-B). Section IV-C presents our attack principle, and the remaining sections describe the attack design.\nThis is the primary attack goal, and while it can be reduced to targeting only a selected set of samples (e.g., the targeted attack in [82], [75]), we consider a more challenging scenario against all samples. This is important because in privacy-sensitive domains (e.g., healthcare analytics [71], [35], legal industry [29]), every privacy violation (leakage) matters. Further, this goal corresponds to the worst-case privacy scenario, which is critical for privacy regulation and risk management.\nThe model's performance should be high despite the attack. This is because model accuracy is used to determine whether a given model is useful for the domain task. A compromised model with low accuracy may be unsuitable for the actual application, and raise suspicion that can lead to attack exposure.\nWe refer to this as the ability to conceal the amplified membership leakage under standard MIA evaluation, which queries the model with the target samples and/or their variants, and uses the received outputs for MI (different attacks mainly differ in their computation of the membership probability from the model's output, e.g., using prediction entropy [76], scaled logit loss [21]). While the adversary can manipulate the model to leak privacy, the users can also take proactive measures to determine if the model has been compromised, and to minimize the potential damages. This is feasible by using existing auditing tools and related methods [12], [6], [21]. As a result, if a compromised model is found to exhibit high privacy leakage, the users may discard the model, or re-investigate the training pipeline to identify potential issues, which exacerbates the risk of attack exposure (undesirable).\nWhile there are many existing attacks that seek to amplify membership leakage (by either modifying the training code [75] as we do, or the training data [82], [25]), they fall short in fulfilling the design goals outlined previously. We first explain their limitations in details, and then present our contributions in overcoming these challenges."}, {"title": "D. The Basic Attack Approach", "content": "We first modify the loss-value computation function to secretly transfer the membership identities of the training samples to that of another set of membership-encoding samples. Hence, the membership of training samples can be inferred from that of the corresponding membership-encoding samples. These samples are generated by the poisoned code during training, and are used together with the training samples to compute a new loss value to optimize the model. Thus, their membership identities are identical (both are members). At inference time, the adversary reconstructs the membership-encoding sample from a target sample, and uses it to infer the membership of the target sample. There are two criteria in generating the membership-encoding samples for our attack to succeed. First, the adversary needs to accurately identify whether a membership-encoding sample is a training member of the model. For this, we start with the observation that outlier samples (e.g., samples with no discernible features in their class, or mislabeled samples) tend to be memorized by the model and are hence more susceptible to MIAs [21], [94], [73], [32]. Based on this observation, we construct the membership-encoding samples as random samples with a fixed sample statistic (mean and standard deviation). These samples without discernible features (e.g., the dark image example in Fig. 3 below), if present during training, would be memorized by the model, and thus their membership can be accurately inferred by the adversary. Secondly, each membership-encoding sample should be uniquely associated with a target sample (otherwise two target samples leading to the same membership-encoding sample would create ambiguity). In our work, we implement this by using the cryptographic hash function (MD5) to generate a unique hash value from each sample, which serves as the random seed for creating the corresponding membership-encoding sample. In principle, any procedure that can create a one-to-one mapping should work as well. Loss-value computation is presented in Fig. 3. Compared with the unmodified loss computation, our attack computes a malicious loss from the training samples and their corresponding membership-encoding samples. We apply the MD5 function to each training sample x to produce a unique random"}, {"title": "E. The Complete Attack Approach", "content": "The previous approach is able to amplify the privacy leakage, but the increase is limited to a moderate degree and with high accuracy drop. On average, it achieves 52.07% TPR@0.1% FPR and increases the test error by 16.45%. Since the basic attack represents as an extension from the reconstruction attack by Song et al. [73], a natural question is why a similar idea works in their case (for data reconstruction), but not so well in ours (for MI). We identify that the reason is MI requires stronger memorization on synthetic samples than data reconstruction does. Specifically, for data reconstruction, merely memorizing the output labels of the synthetic data alone suffices, as only the output labels are used to encode information like pixel values (explained in Section II-B). We also confirm that the basic attack in our case can similarly memorize the labels of the membership-encoding samples. However, this is not enough as MIA has a unique challenge of controlling at low FPR [93], [21], which necessitates strong memorization on the synthetic samples. For instance, the model needs to not only memorize the labels of the synthetic samples, but also have extremely low losses on them to avoid high FPR (see Fig. 6 below for an illustration). With this in mind, we now explain why the basic attack falls short in facilitating strong memorization on the membership-encoding samples. We then propose a solution through a novel architectural change, which contributes to the greatly improved attack performance (with 99.8% TPR@0.1% FPR and 70% lower accuracy drop). Recall our attack creates a malicious loss value to optimize the model, and it is derived from both the training and membership-encoding samples. The presence of these two different types of samples (one from the domain data distribution and the other from an adversary-chosen distribution that creates samples without meaningful features) results in a distribution mismatch during training. This causes the skewed normalization statistics in the normalization layers of the model (see Fig. 5 for an illustration), and significantly hinders the success of our attack. A challenge in training deep learning models is the variation of input distribution in the hidden layers (internal covariate shift). Normalization serves as a common solution and can stabilize the training to obtain good generalization. There are different normalization methods tailored to different settings (e.g., Instance Normalization for generative models [83], Group Normalization for small-batch training [87], Layer Normalization for sequential models like recurrent neural networks [14]). We focus on Batch Normalization [43] as it is widely-used in deep learning models. Let \\(X \\in ]R^{N\\times d \\times H \\times W}\\) denote the input to the normalization layer, where N is the batch size, H \u00d7 W is the spatial dimension size, and d is the number of channels. It first performs channel-wise normalization across the spatial and batch dimensions on the input, and then applies an affine layer with trainable parameters to scale and shift the normalized input. Formally, for each channel \\(j \\in [d]\\):\n\\[x_j = \\frac{x_j - \\mathbb{E}(x_j)}{\\sqrt{Var(x)}}\\]\n\\[out_j = \\gamma_j x_j - \\beta_j\\] where \\(x_j \\in \\{x_1,...,x_d\\} \\subseteq R^{N\\times H \\times W}\\) is an input channel, and \\(\\gamma_j, \\beta_j\\) are the learnable parameters. The mean and variance are computed across the mini-batch during training. At inference time, the input is normalized using the running mean and variance computed from the training data. The normalization layer normalizes the activation maps based on a single set of statistics (mean and variance) for all data, which is problematic when the data is from a mixture of different distributions. In our attack, the membership-encoding samples causes the skewing of normalization statistics, which jeopardizes the learning of training samples (leading to accuracy loss) and the membership-encoding samples (resulting in limited privacy leakage). Distribution mismatch has also been studied in other contexts, such as adversarial training [88] and teacher-student data distribution mismatch in knowledge distillation [63]. Inspired by Xie et al. [88], we propose to include a secondary normalization layer to overcome the identified issue. Our approach is to use a secondary normalization layer for learning the membership-encoding samples, and the"}, {"title": "V. EVALUATION", "content": "Datasets and model training. We consider five common benchmark datasets, including CIFAR10 [47], CIFAR100 [47], SVHN [61], GTSRB [38] and PathMNIST (for predicting survival from colorectal cancer histology) [91]. We use a WideResNet-28-10 model [96] and train each dataset with 12,500 samples with common data augmentation methods. Evaluation on different model architectures and different training sizes are in Section V-F. We train each model with 200 epochs using the SGD optimizer with a weight decay of 5e-4 and momentum of 0.9. We set the initial learning rate as 0.1, and reduce it by 5 at the epochs 60, 120 and 160 [10]. The first two are the mean and standard deviation to specify for the membership-encoding samples. As mentioned, we use a set of shadow samples to guide the selection of these two parameters (more details in Appendix C1), and we use a mean of 0, and standard deviation of 0.1 in our experiments. Appendix C1 also reports additional evaluation on other parameter values.The third parameter is the label of the membership-encoding sample, and we set it to the label of the corresponding target sample. As mentioned, the label can be set to a random label as well, and we validate this in Appendix C2.\nComparison baseline. We consider the state-of-the-art untargeted attack by Tramer et al. [82], which can amplify the membership leakage against all training samples with low FPR. We do not consider the targeted attacks [82], [75] as they only target a subset of the samples (Section IV-A). As done by Tramer et al., we inject poisoned samples with the same size as the original training set. The results are in Section V-B~Section V-E. Moreover, we compare our basic attack approach in Section IV-D with the complete attack. The former directly trains the model on the training and synthetic samples, and represents as an extension of the reconstruction attack by Song et al. [73]; while the latter consists of the proposed architectural change. We report the comparison results in Section V-F2. There are two MI protocols. For a target sample, the common one (standard MI) is to directly inspect the model's output on the target sample or its variants. The other one (stealthy MI), which we propose, infers the membership of the target"}, {"title": "D. Stealthiness of Privacy Leakage", "content": "Our attack succeeds in stealing the membership infor-mation through the proposed stealthy MI protocol, which is different from the standard MI protocol in existing work [72], [94], [76], [27], [52], [53], [85], [93], [21]. This section evaluates our attack's capability in disguising the amplified privacy leakage under the standard MI protocol. Under the standard MI protocol, for the attack by Tramer et al., a user without any knowledge of the data-poisoning adversary can still use the training samples to query the poisoned models and identify the high privacy leakage (average 31.79% attack TPR@0.1% FPR). This is undesirable from an adversary perspective as it can lead to a direct attack exposure. In contrast, the poisoned models by our attack exhibit comparable privacy as their uncorrupted counterparts, under the standard MI protocol. In Fig. 8, the attack TPRs between the code-poisoned models (green lines) and uncorrupted models (blue lines) are 12.46% vs. 13.06% (CIFAR10), 41.51% vs. 42.30% (CIFAR100), 6.75% vs. 6.85% (SVHN), 0.62% vs. 0.76% (GTSRB), and 3.18% vs. 2.09% (PathMNIST). The average TPRs are 13.01% and 12.91%,"}, {"title": "E. Necessity of Shadow-model Calibration", "content": "Training shadow models is commonly needed in existing attacks to calibrate the inference threshold in order to control at low FPR [21], [85], [93], [82]. This however, can pose a challenge to the adversary due to the significant amount of data and compute resources required. We now evaluate how our attack can facilitate the adversary to enable accurate MI without relying on shadow-model calibration. We use the global-threshold-based variant in LiRA [21], which does not require shadow models to perform the fine-grained per-sample calibration. We report the results on CIFAR10 in Fig. 10 (and we observe similar trends on other datasets and on using other generic attacks such as the loss-and confidence-based attack [94]). On the uncorrupted model, the global-threshold attack fails to infer the member samples when controlled at low false"}, {"title": "VI. DISCUSSION", "content": "We first evaluate our attack under the normalization-layer-free setting (Section VI-A), and then perform a comprehensive study to understand the artifacts incurred by our attack (Section VI-B). Section VI-C presents an attack countermeasure and Section VI-D discusses the limitations of our work.\nIs Normalization Layer Indispensable?\nThe proposed architectural modification to include a sec-ondary normalization layer plays a key role in our at-tack. While the norm layer is a common building block in many state-of-the-art deep learning models [37], [41], [96], [40], [90], there are models that do not include the norm layer [18], [19]. This section analyzes our attack under such a normalization-free model. We use a simpler approach by removing all the norm layers in the WideResNet model. Without the norm layer to shift and scale the inputs, we switch to using a larger mean of 0.3 and standard deviation of 1.5 for generating diverse membership-encoding samples (otherwise they cannot be memorized due to the low variance across samples). In this setting, our attack performance is not as high, though it still increases the TPR@0.1% FPR from 9.96% to 48.29% (a 4.8x increase) with 0.9% accuracy drop. We leave the improvement under the normalization-free setting to future work.\nIn this section, we first discuss the different artifacts incurred by our attack, and then present several alternative strategies to mitigate them.\nOur attack creates membership-encoding samples during training. They are used to create the malicious loss value, but also increase the number of forward passes at each training step. Therefore, we present an alternate method to configure our attack without increasing the number of forward passes. The idea is to randomly replace a subset of training samples with their membership-encoding samples, while keep-ing the number of forward passes the same at each training step (e.g., 70% for training samples, and 30% for synthetic samples). The rationale is that both types of samples do not need to appear in every training step to be learned/memorized by the model, and thus we can steal the membership without increasing the number of forward passes, which also has the benefit of reducing the attack overhead (see next). We evaluate our attack by replacing different portions of training samples with membership-encoding samples at each training step (10%, 30%, 50% and 70%). 10% replacement can largely preserve the model accuracy (0.65% drop), but there are limited number of membership-encoding samples at each step, which leads to a slightly lower TPR of 80%. Increasing the ratio to 30% can boost the TPR to 99.82%, at a slightly higher accuracy drop of 1.1%. Using a larger replacement ratio has negligible benefit in privacy leakage and incurs slightly higher accuracy loss (as fewer training samples are used at each training step): 50% replacement ratio has a 1.5% accuracy drop and 70% has 3.44%. The inclu-sion of a secondary norm layer increases the model parameters, but only to a small margin (average 0.15%). We also measure the runtime overhead by our attack (with two Nvidia V100 GPUs). The attack increases the average training time (from five repetitions) from 85.4 mins to 165 mins (93% increase). However, using the random replacement strategy introduced earlier (with a 30% replacement ratio) can reduce the runtime to 108.2 mins, which amounts to a mere 26.7% increase. Lastly, our attack also increases the average inference overhead from 7.51 ms to 7.87 ms (4.8% higher). Despite the increased overhead, we remark that they do not make the attack easy to detect. This is because the runtime is also affected by several other factors (e.g., system environment, hardware configuration), and hence it is challenging to obtain stable runtime baselines for comparison. The secondary normalization layer leaves an artifact in the model's computational graph. However, to the best of our knowledge, the existing use of additional normalization layer in ML models is commonly intended for benign purposes (such as for adversarial training [88], [89], or performance improvement under imbalanced classification [95]). In contrast, our work is the first of its type to exploit the additional norm layer from an adversarial perspective to facilitate MIAs. Therefore, it is highly challenging for non-expert target users to determine that the extra norm layer was included for a malicious intent. For completeness, we also investigate other alternative attack strategies that do not entail architectural modification (details in Appendix C3). This can be adopted by the adversary to eliminate the artifact of the secondary norm layer while still causing major privacy damage, though they do come with the cost of reduced attack performance (Appendix C3). Overall, our attack represents a new class of MIA with several noteworthy properties, including: (1) high MI success against all training samples (average >99% attack TPR@0.1% FPR), (2) no reliance on shadow model calibration, and more importantly, (3) incurring negligible accuracy drop, while (4) being able to disguise the amplified privacy leakage under common membership privacy auditing. These together represent a significant advancement over existing poisoning-based MIAs [75], [25], [82]. We leave further improvement in covering the remaining attack traces to follow-up studies.\nOur evaluation in Section V-F3 on various MIAs defenses illustrates the challenges in mitigating the proposed attack. However, our attack relies on the exact knowledge of the target sample, for generating the unique random seed to reconstruct the membership-encoding sample. Hence, one countermeasure is to slightly modify the target sample so that the adversary cannot generate the same random seed to reconstruct the secret sample. The modifications can manifest in different forms, and thus they are hard to predict by the adversary. With that said, the exact knowledge of target sample is still a common assumption in existing practice of MIAs [72], [93], [21], [39]. Hence, our attack still poses significant privacy threat, and a systematic amendment of the existing MI defenses to handle our attack is another avenue for future work. The major limitation of our work concerns the feasibility of mounting code poisoning attacks in practice. While code poisoning attacks have been shown to be feasible in real-world ML codebase [9], [78], [11], we have not realized our attack in the open world, and neither have any prior code-poisoning attack studies [15], [73], [75], [54], to the best of our knowledge. This is a limitation of this class of studies."}, {"title": "VII. CONCLUSION AND FUTURE WORK", "content": "This work introduces a new form of membership inference attack against deep learning models, based on poisoning two opaque and difficult-to-test modules in the model-training code: the loss-value computation and model structure. The training code can be used by the victim users in a trusted environment to produce compromised models that can operate faithfully on the main task with competitive performance, while still secretly leaking the membership information of all the training samples to the black-box adversary. Our work illustrates how the massive learning capacity of modern deep learning models can be exploited by the adversary to amplify membership privacy leakage in a secret manner. The amplified privacy leakage inflicted by the attack can remain unnoticeable under common privacy auditing meth-ods, and a deliberate adversary can go even further to disguise the attack by tricking the corrupted model to convey a false sense of strong privacy and mislead the users. From this, we outline three directions to be explored in future studies. (1) Existing auditing practice does not account for code inspection, which is a necessary step in exposing the privacy leakage inflicted by our attack. Thus, an open question is should the model-training code be supplied as part of the inputs in the standard membership inference game, though identifying the malicious constructs from the complicated codebase itself can become another barrier? In addition, a direct approach to thwart our current attack is to slightly modify the target sample. Thus, developing a standardized approach to enact this can be another avenue for future study. (2) There are several directions that can be explored to extend our attack, including attack extension to generative models [36], [24], to other domains such as natural language processing [70], [58], and improving the attack performance under other challenging settings (e.g., normalization-free scenario). (3) Our evaluation on existing defense techniques shows that the challenge in providing strong privacy protection without incurring high accuracy loss still remains. Those prior privacy defenses that can achieve a superior privacy-utility trade off under standard MI evaluation, unfortunately can be \"evaded\" by a deliberate adversary, and future work can study the potential of more capable defenses to withstand such attack."}]}