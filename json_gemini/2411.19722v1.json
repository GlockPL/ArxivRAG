{"title": "JETFORMER: AN AUTOREGRESSIVE GENERATIVE MODEL OF RAW IMAGES AND TEXT", "authors": ["Michael Tschannen", "Andr\u00e9 Susano Pinto", "Alexander Kolesnikov"], "abstract": "Removing modeling constraints and unifying architectures across domains has been a key driver of the recent progress in training large multimodal models. However, most of these models still rely on many separately trained components such as modality-specific encoders and decoders. In this work, we further streamline joint generative modeling of images and text. We propose an autoregressive decoder-only transformer\u2014JetFormer\u2014which is trained to directly maximize the likelihood of raw data, without relying on any separately pretrained components, and can understand and generate both text and images. Specifically, we leverage a normalizing flow model to obtain a soft-token image representation that is jointly trained with an autoregressive multimodal transformer. The normalizing flow model serves as both an image encoder for perception tasks and an image decoder for image generation tasks during inference. JetFormer achieves text-to-image generation quality competitive with recent VQVAE- and VAE-based baselines. These baselines rely on pretrained image autoencoders, which are trained with a complex mixture of losses, including perceptual ones. At the same time, JetFormer demonstrates robust image understanding capabilities. To the best of our knowledge, JetFormer is the first model that is capable of generating high-fidelity images and producing strong log-likelihood bounds.", "sections": [{"title": "1 INTRODUCTION", "content": "The \"Bitter lesson\" (Sutton, 2019) has been the prime force behind the recent progress in machine learning and artificial intelligence research. It suggests that general-purpose methods which effectively leverage large amounts of compute and data prevail over specialized techniques designed by domain experts. Arguably the most prominent examples in this context are transformer decoder-only models trained for next-token prediction (Vaswani et al., 2017; Radford et al., 2018), that outperform task-specific NLP systems, and transformer encoders in computer vision (Dosovitskiy et al., 2021; Strudel et al., 2021; Li et al., 2022), that achieve better quality than CNN-based models.\nThis trend is also visible in the current pursuit of extending LLMs to both understand and generate multiple modalities such as text and images with a single model. A powerful paradigm in the literature (Aghajanyan et al., 2022; Kim et al., 2023; Aghajanyan et al., 2023; You et al., 2023) is to model the image tokens using discrete tokens obtained via (VQ)VAEs (van den Oord et al., 2017; Esser et al., 2020; Ramesh et al., 2021). One limitation of these approaches is that the conversion from image into tokens and vice-versa is performed by a separate, frozen, modality-specific and lossy encoder (and decoder) trained ahead of time. As a result, this image encoder may be agnostic to the actual task at hand and limit the performance of the resulting model (Dong et al., 2023; Pan et al., 2024; Xu et al., 2024).\nTo obtain a general architecture that can generate multiple modalities but does not have (limiting) components pretrained ahead of time, we develop a new generative model: the JetFormer. It can be trained from scratch and optimized end-to-end for the log-likelihood of raw training data. We demonstrate this for text and pixels. To this end, we combine a normalizing flow (Dinh et al., 2016; Kingma & Dhariwal, 2018) for computing a soft-token image representation with a decoder-only transformer (Vaswani et al., 2017) and a soft-token Gaussian mixture loss (Tschannen et al., 2024)."}, {"title": "2 RELATED WORK", "content": "Generating natural images autoregressively as a sequence of discrete-valued (sub-)pixels was extensively explored in the literature using CNNs (Van den Oord et al., 2016b;a; Salimans et al., 2016) or transformers (Parmar et al., 2018; Chen et al., 2020). While achieving excellent results in log-likelihood, these models are computationally expensive and do not scale well to high image resolutions. A related family of models are normalizing flows (Dinh et al., 2014; 2016; Kingma & Dhariwal, 2018; Ho et al., 2019), invertible models which are trained to map image pixels to a simple prior by maximizing log-likelihood. These models scale better but achieve lower likelihood than autoregressive models and empirically fail to generate high-fidelity images, even for low resolutions.\nMore recently, compressing the high-dimensional image pixel space to a lower-dimensional sequence of discrete tokens via a pretrained, frozen VQ-VAE (van den Oord et al., 2017; Razavi et al., 2019), and then modeling the compressed sequence with a transformer decoder has emerged as a scalable technique for high-fidelity image generation (Esser et al., 2020; Yu et al., 2022a; Ramesh et al., 2021; Yu et al., 2022c; Gafni et al., 2022). To enable semantic compression, VQ-VAEs typically rely on perceptual and GAN losses. Moreover, VQ-VAE-based representations are common in the context of dense prediction tasks (Kolesnikov et al., 2022; Lu et al., 2022; Mizrahi et al., 2024; Mentzer et al., 2024), in particular when modeling multiple modalities jointly. GIVT (Tschannen et al., 2024) showed that combining an autoencoder and an autoregressive transformer can be applied to continuous-valued sequences, by directly modeling feature vectors in the latent space of a VAE, without any quantization. Somewhat related, (Nachmani et al., 2023; Meng et al., 2024) explored soft tokens for speech synthesis.\nVQ-VAEs are also becoming popular in the context of Vision-Language Models (VLMs). Such models are typically either trained from scratch (Radford et al., 2021; Jia et al., 2021; Wang et al., 2022b; Yu et al., 2022b) on web-scale data or constructed by combining and tuning a pretrained vision encoder and a pretrained language model (Alayrac et al., 2022; Chen et al., 2023b; Wang et al., 2022a; Li et al., 2023; Huang et al., 2023; Liu et al., 2024; Beyer et al., 2024), and can solve a broad range of task which can be cast as text output. To enable pixel outputs for such models a simple way is to extend the text vocabulary with VQ-VAE tokens (Aghajanyan et al., 2022; Kim et al., 2023; Aghajanyan et al., 2023; You et al., 2023; Pan et al., 2024). Other works (Dong et al., 2023; Ge et al., 2024; Zhou et al., 2024) combine VLMs with (latent) diffusion models (Sohl-Dickstein et al., 2015; Dhariwal & Nichol, 2021; Rombach et al., 2022; Saharia et al., 2022; Ramesh et al., 2022) to enable image generation capabilities. JetFormer is related to this category of models, but unlike previous models does not rely on any pretrained (VQ-)VAE vision encoder/decoder."}, {"title": "3 METHOD", "content": "Modeling natural images with autoregressive transformers poses many obstacles. Doing so in pixel space is a viable approach (Parmar et al., 2018; Chen et al., 2020), but it quickly becomes computationally prohibitive. Even an image of size 256\u00d7256\u00d73 would require predicting/sampling"}, {"title": "3.1 MODELING IMAGES IN PIXEL SPACE WITH SOFT TOKENS AND NORMALIZING FLOWS", "content": "As outlined above, we model an image x using a normalizing flow model (Dinh et al., 2014; 2016; Kingma & Dhariwal, 2018) $f(x)$ that losslessly maps an image into a sequence of embeddings {$z_1,..., z_n$}, which we also call \"soft tokens\". Note that the flow preserves the total number of the input dimensions. These embeddings are then modeled by the deep autoregressive model p, where the outputs are modeled with a GMM, as proposed in GIVT. We then maximize the image log-likelihood lower bound L:\n$L(x) = log p(f(x)) + log det \\frac{\\partial f(x)}{\\partial x}$, where\n$f(x) = [z_1, z_2,..., z_n] and p(z) = \\prod_{i=1}^{n}P(z_i|z_{i-1},..., z_1)$.\nNote the log-determinant term arises from the normalizing flow model, as a part of the data log-likelihood, see Dinh et al. (2016). Further, to ensure correctness, we apply image dequantization, as outlined in Theis et al. (2015). This amounts to adding uniform noise u to the input images I, s.t. $x = I+u$, where $u \\sim U[0, 1]$. This guarantees that we optimize a lower bound on the discrete image log-likelihood. For clarity, we point out that both p and f both have learnable parameters which are optimized with gradient-based method (training via teacher forcing is illustrated in Figure 1).\nIn simple words, JetFormer for images is an autoregressive model, where inputs and targets are produced by the flow model, which re-encodes input images. Due to the end-to-end nature of the objective, the flow model is incentivized to learn a sequence of embeddings, that makes autoregressive modeling as effective as possible. During inference, the autoregressive model generates a sequence of soft tokens, which then need to be decoded into an image using the inverse of the flow."}, {"title": "3.2 IMPROVING MODEL QUALITY FOR NATURAL IMAGES", "content": "While JetFormer works out of the box, we have found several modeling enhancements which greatly improve the quality of the generated images. In particular, factoring out latent dimensions, the use of classifier-free-guidance during sampling, and a novel noise curriculum."}, {"title": "Factoring out redundant dimensions", "content": "Natural images are redundant, intrinsically low-dimensional signals with low-frequency components dominating the spectrum. The design of JetFormer enables a simple and effective way to leverage this observation and improve model quality, while also reducing computational burden.\nThe key observations is that not all output dimensions of the invertible flow need to be further processed by the autoregressive model. We can model a subset of dimensions (i.e. a subset of channels) with a Gaussian distribution, $p_s$, and the remaining ones with the autoregressive transformer:\n$L(x) = log p(z_s) + log p_n(z_n) + log det (\\frac{\\partial f}{\\partial x})$, where $[z_s, z_n] = f(x)$\nIntuitively, we expect redundant dimensions to be \"factored out\" as $z_s$, as they do not require further heavy processing. We verify our intuition empirically in the experimental section and in Figure 6c.\nAs a strong baseline to the above approach, we also consider a more direct approach to handle redundancy in images. Before feeding x to the flow model, we reshape it into a sequence of flattened patches and apply a learnable, invertible linear map W along the channel dimension. We want this map to learn to separate important dimensions of the flattened patches from redundant ones. To this end, we feed the first d channels of its output $xW$ to the normalizing flow and model the remaining channels with a Gaussian distribution. Intuitively, given the simplicity of the transform W applied before factoring out part of the sequence, minimizing the NLL while training will ensure that the hard-to-model part of the sequence is modelled by JetFormer, whereas the low-level noise will be mapped to the Gaussian prior. This is similar to the reasoning behind probabilistic PCA (Tipping & Bishop, 1999). Indeed, we observe that the transform learned by the model is close to applying PCA to image patches (see Figure 6d), and we observe that when initializing W with PCA and freezing it we obtain similar results. See Appendix B for formal definition of this approach.\nClassifier-free guidance Following common practice in the diffusion and autoregressive image modeling literature, we employ classifier-free guidance (CFG) (Ho & Salimans, 2022) which was previously shown to substantially improve sample quality. We reuse the distribution-based variant implemented via rejection sampling for GMMs from (Tschannen et al., 2024) without modification."}, {"title": "3.2.1 RGB NOISE CURRICULUM DURING TRAINING", "content": "It is common to explicitly factorize data into semantically meaningful parts to improve image quality. One approach is to model RGB pixels as sequences of increasing color depth and/or resolution (Kolesnikov & Lampert, 2017; Menick & Kalchbrenner, 2019; Nash et al., 2021). Similarly, adding noise to RGB pixels is closely related to reducing the color depth and effective resolution. This has led to the interpretation of diffusion models, where denoisers are trained at different noise levels according to a predefined noise schedule, as learning a hierarchical representation in pixel space induced by the noise schedule (Kingma & Gao, 2023; Dieleman, 2024).\nBuilding on this intuition, we alter the training procedure by introducing a \"noise curriculum\": additive Gaussian pixel noise during JetFormer training. The noise is strongest in the beginning of the training and decays towards zero. We use cosine decay schedule for the noise standard deviation.\nIn the beginning of the training, when strong (high-variance) noise is added to the image, JetFormer learns to model coarse image information (see Figure 6b). As training progresses, the model gradually learns a finer level of detail, while \u201cremembering\u201d previously learned patterns. In the end of the training, JetFormer uses the correct distribution for training. Intuitively, this scheme prioritizes modeling of high-level image structure without sacrificing overall performance.\nImportantly, unlike in diffusion, the noise curriculum merely acts as a data augmentation during training. The model is not conditioned on the noise magnitude and, at inference, an image is not gradually denoised, but generated autoregressively in the latent space of a normalizing flow.\nFor an integer-valued RGB image I, the noisy image is obtained as $[I + \\sigma_t N(0, I)]$, where the noise scale $\\sigma_t$ as a function of the training progresses $t \\in [0,1]$ follows the cosine schedule $\\sigma_t = \\sigma_0 \\frac{1+cos(t)}{2}$. The shape of the noise schedule is visualized in Figure 3."}, {"title": "3.3 JOINT GENERATIVE MODELING OF PIXELS AND TEXT", "content": "We explore JetFormer for multimodal generative modeling, where the model can perform both discriminative and generative tasks on all modalities, focusing on images and text. Sophisticated models of this class are trained on interleaved sequences of images and text (Aghajanyan et al., 2022; Kim et al., 2023; Aghajanyan et al., 2023), often with post-training refinement, which enables few-shot image-to-text (e.g. captioning) and text-to-image (e.g. image editing) capabilities. Here, we follow (Kim et al., 2023; You et al., 2023) and consider image-text pairs from the web as a proxy for more complex, interleaved setups, and do not involve a post-training step. While conceptually simple, this allows us to explore vision-language understanding tasks such as captioning and VQA, as well as text-to-image generation. Extending the image generation approach discussed in the previous section to this setup is straight-forward: We simply extend the transformer backbone generating soft tokens to modeling language tokens produced by a standard language tokenizer with a separate prediction head and a softmax.\nWe train on sequences of both image tokens followed by text tokens, and vice versa, only applying a loss to the second part (modality) of the sequence. We use the respective negative log-likelihood, and a weight to balance the two. We observed that applying the loss to the full sequence leads to worse results, possibly because predicting an image prefix effectively means unconditionally modeling web images, which is very challenging. We expect this to change for interleaved sequences, which may provide a stronger conditioning signal.\nFor text-to-image generation, the text prefix acts as a conditioning, and image generation is performed as described in Section 3.1. For image-to-text generation the normalizing flow acts as an image encoder. The model uses the same soft token space for generation and understanding."}, {"title": "4 EXPERIMENTS", "content": "Architecture We rely on the simple, transformer-based design from (Anonymous, 2024) for the normalizing flow model. This design uses affine coupling layers (predicting an element-wise scale and shift) consisting of spatial and channel-wise splitting functions to split the activations in two parts, and a stack ViT blocks (Dosovitskiy et al., 2021) applied to half of the activations to infer the affine transform. Here, we only use channel-wise splitting as we found in preliminary experiments that spatial splitting did not improve modeling quality. We set the depth to 32 coupling blocks, each consisting of a stack of 4 or 6 ViT blocks of width 512, and with 8 attention heads. The (input and output) feature shape for this model is $(\\frac{H}{p} \\frac{W}{p}, 3p^2)$ when feeding the full image, and $(\\frac{H}{p} \\frac{W}{p}, d)$ when using a dense invertible map or PCA to reduce dimensionality prior to applying flow. For image size H = W = 256 and patch size p = 16 this amounts to 256\u00d7768, after flattening spatial dimensions and with dimensionality reduction to d = 128 to 256\u00d7128."}, {"title": "5 CONCLUSION", "content": "In this paper we introduce JetFormer, a novel class of generative models that combines normalizing flows and autoregressive models with soft tokens. To the best of our knowledge, it is the first image model, which is capable to synthesize high-resolutions images and provide an explicit (and competitive) NLL bounds for the raw images. JetFormer is a fully end-to-end trainable model (with no components pretrained ahead of time), which means that it can be fully tailored towards the task at hand, without being limited by the external and frozen components. Being able to compute NLL is also an important feature of our model. NLL is a tangible score closely related to compression capabilities, and can be used to compare various generative models across different modeling classes or for hill-climbing. Also, by measuring NLL score one can ensure the absence of the mode collapse, because mode collapse will lead to deterioration NLL on the hold-out data.\nWe note that JetFormer in its current form also has some limitations. The visual quality of its samples lags behind state-of-the-art diffusion models that leverage pretrained latent representations. Additionally, the full end-to-end nature of JetFormer also comes with increased computational requirements. However, given JetFormer's simple design, we believe that it can be scaled up well so that the benefits of end-to-end training can come to full fruition."}, {"title": "Reproducibility Statement", "content": "We provide detailed information about the training recipe, the architecture, hyper-parameters and the training data in Section 4 and Appendix A."}, {"title": "Ethics Statement", "content": "This paper describes a system for understanding and generation of image-text data, with focus on characterizing and exploring its performance on academic data sets. It fits into the broader class of large multimodal models trained on data from the web, and the same ethical implications as for those prior works apply here. In particular, when releasing or deploying such models publicly, extensive measures to de-biasing the data should be taken, and models should be safety-tuned and red-teamed prior to release. Content-filters can be added to the inference pipeline to further improve safety. We refer to the corresponding papers for a more in-depth discussion, for example (Radford et al., 2021; Chen et al., 2023a; Po et al., 2024)."}, {"title": "APPENDIX \u2013 SUPPLEMENTARY MATERIAL", "sections": [{"title": "A ARCHITECTURES AND HYPERPARAMETERS", "content": "Table 5 shows the shapes of the autoregressive transformer and normalizing flow components for different JetFormer variants. One can see that the dense layer predicting the GMM parameters are relatively large. To save memory, we store this layer in the bfloat16 format (instead of float32 as the other parameters) which does not impact the quality. Also note that at inference time applying this layer can be greatly sped up by first sampling the mixture and then only inferring the mean and covariance of that mixture.\nTo implement the normalizing flow we use a stack affine coupling blocks (Dinh et al., 2016, Eqn. 7, 8). For each block i the relation between input sequence yi and output $y_{i+1}$ is\n$y_{i+1} = \\tilde{y}_i + b_i(\\tilde{y}_i) \\, \\, y_i = \\tilde{y}_{i+1}$\n$y_{i+1} = y_i \\odot exp(a_i(\\tilde{y}_i)) , \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, y_i = (\\tilde{y}_{i+1} - b_i(\\tilde{y}_{i+1})) \\odot exp(-a_i(\\tilde{y}_{i+1}))$.\nHere, $y_i$ and $\\tilde{y}_i$ are obtained by splitting $y_i$ into equally-shaped halves along the channel dimension according to a randomly initialized partition (and $y_{i+1}$ results from merging $\\tilde{y}_{i+1}$ and $y_{i+1}$ according to this partition). The scale $a_i$ and shift $b_i$ are inferred by two separate heads from a shared ViT $g_i$.\nThe normalizing flow component has about 450M to 650M parameters, depending of the variant. This is more than typical CNN-based (VQ-)VAEs for image generation in the literature (Esser et al., 2020; Rombach et al., 2022), but comparable to common ViT-based VQ-VAEs (Yu et al., 2022a;c).\nTable 6 shows the hyper-parameters used to fine-tune JetFormer. Overall we observed dropout to have a significant impact on the image sample quality. When reporting fine-tuning-based metrics we report the median of 10 runs, except in VQAv2 where we report test-dev accuracy from the evaluation server."}, {"title": "B FACTORING OUT REDUNDANT DIMENSIONS: PCA-INSPIRED VARIANT", "content": "Recall that for this variant, before feeding x to the flow model, we reshape it into a sequence of flattened patches and apply a learnable, invertible linear map W along the channel dimension."}, {"title": "C ADDITIONAL ABLATIONS", "content": "VAE baseline We train a VAE following the design of VQGAN (Rombach et al., 2022) producing a sequence of 256 128-dimensional tokens like the normalizing flow in JetFormer. We remove the GAN and perceptual losses to ensure a fair comparison with JetFormer (which solely maximizes the data likelihood, without relying on GAN or perceptual losses). We then train the JetFormer-B decoder-only model on this VAE representation, following the JetFormer-B T&I training recipe. The results in the Table 7 below show that JetFormer outperforms the VAE baseline by a solid margin (in particular in T2I generation).\nEffect of noise curriculum on NLL Intuitively, the noise curriculum biases the training process towards those high-likelihood solutions with high perceptual quality, at the expense of a slight increase in NLL (see Sec. 3.2.1 for more discussion). Table 8 shows that longer training with noise curriculum eliminates the gap in NLL compared to training without curriculum."}]}]}