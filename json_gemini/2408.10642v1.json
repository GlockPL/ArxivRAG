{"title": "Minor SFT loss for LLM fine-tune to increase performance and reduce model deviation", "authors": ["Shiming Xie", "Hong Chen", "Fred Yu", "Zeye Sun", "Xiuyu Wu"], "abstract": "Instruct LLM provide a paradigm used in large scale language model to align LLM to human preference. The paradigm contains supervised fine tuning and reinforce learning from human feedback. This paradigm is also used in downstream scenarios to adapt LLM to specific corpora and applications. Comparing to SFT, there are many efforts focused on RLHF and several algorithms being proposed, such as PPO, DPO, IPO, KTO, MinorDPO and etc. Meanwhile most efforts for SFT are focused on how to collect, filter and mix high quality data. In this article with insight from DPO and MinorDPO, we propose a training metric for SFT to measure the discrepancy between the optimized model and the original model, and a loss function MinorSFT that can increase the training effectiveness, and reduce the discrepancy between the optimized LLM and original LLM.", "sections": [{"title": "Background", "content": "LLM trained on very large corpora is extremely powerful language model for completion tasks. SFT\nand RLHF(Ouyang et al. (2022), Ziegler et al. (2020)) are two techniques that used to expose the\nLLM capability and align LLM answer to human instructions. With the increasing reasoning abilities,\nLLM are widely used in industries, and SFT and RLHF are also used to inject domain knowledge\ninto LLM by training on domain corpora.\nIn the past most works are focused on RLHF and several algorithms are proposed, such as\nPPOSchulman et al. (2017), DPO(Rafailov et al. (2023)), IPO(Azar et al. (2023)), KTO(Ethayarajh\net al. (2024)), MinorDPO(Xie et al. (2024)) and etc. One important point of RLHF is to constraint\nthe optimized model not to deviate from the original model too much during the training, and thus\nPPO use KL constraints, DPO use a sample level dynamic coefficient related to distance between the\npreference pair, and IPO use a targeted distance between the preference pair and etc. The purpose of\nthis constraint is to avoid over-fit on the domain corpora and to maintain LLM generalities. It's an\nimportant hypothesis that the base model is powerful enough and the training should not change the\nlanguage distribution too much to maintain the generality and diversity.\nWhile back to SFT, most works are focused on collect, filter and mix high quality data. High quality\ndata is undoubtedly important to get a high qualified and usable LLM, while the aforementioned\nhypothesis that optimized model should not deviate far from the original model is still important.\nOur main contribution is that we introduce a training metrics used in DPO and MinorDPO into SFT\nphase, and propose an improved loss function MinorSFT. MinorSFT use a sample level coefficient\nto control the learning strength. It constraints the discrepancy more compared to raw SFT and may\nprovide better performance result, at the cost of an additional hyper parameter and more computation."}, {"title": "Related Work", "content": "Reinforce Learning from human feedback(Ouyang et al. (2022), Ziegler et al. (2020)) is a popular\ntechnique to align LLM to human preference. It uses SFT to train a supervised LLM on data of\nsampled prompt and labeled answer, then trains a reward model on preference pairs from human\nfeedback and finally uses RL algorithm like PPO Schulman et al. (2017) to train an optimized LLM.\nThe RL part contains a KL-divergence constraint to prevent the optimized LLM deviating too much\nfrom the base model.\nDPO(Rafailov et al. (2023)) is a simplified RL algorithm that optimize LLM directly on the preference\ndata using a cross-entropy classification loss. DPO objective is to increase the relative log probability\nof preferred answer to dis-preferred answer. It incorporates a dynamic, sample level importance\nweight scaled by hyper-parameter \u1e9e and claim that \u1e9e account for the strength of the KL-divergence\nconstraint. DPO introduces an important concept that LLM model itself is an implicit reward model,\nwhich means the LLM model can somehow measure the corpora during training phase. Rafailov et al.\n(2024) derive that DPO is token-level MDP and works as a general inverse Q-learning algorithm in a\ntheoretical way.\nMinorDPO(Xie et al. (2024)) is a DPO variant. It justifies hyper-parameter \u1e9e in DPO is a constraint\nrelate to the relative log probability margin of the preference pair, instead of the KL-divergence\nconstraint. It introduces MinorDPO loss to reduce penalty on the reject(dis-preferred) answer to\nprevent over penalty on the reject answer, which implicitly keep to the hypothesis that optimized\nmodel should not deviate too much from the base model.\nIPO(Azar et al. (2023)) proves DPO may be prone to over-fitting when preferred probability over\ndis-preferred probability that is close to 1. In IPO objective it uses a target value relate to the hyper-\nparameter \u1e9e for the relative log probability of preferred to dis-preferred. However, it is somehow\nsame as DPO, that it focuses on the relative log probability margin, so it has same problem as DPO\nmentioned in MinorDPO.\nKTO(Ethayarajh et al. (2024)) proposes human aware loss function. It separates the preference pair\nloss into two losses so that it doesn't purely rely on paired preference data. Inside each separated\nloss, it estimates the KL term by matching input x' with unrelated outputs z in the same batch, but\nwithout back-propagate through the KL term, and thus it also introduces an implicit constraint on the\ngradient which in turn affect the learning strength and deviation.\nLlama 3 (Dubey et al. (2024)) presents a detailed way to collect, filter and mix high quality data for\nSFT and RL. For the RL part it uses DPO with an additional negative log-likelihood loss, similar to\nPang et al. (2024) and mentioned in Pal et al. (2024).\nMany efforts focus on RL part and use explicit or implicit constraints to limit optimized LLM\ndeviation to reduce model regression. Inspired by DPO and MinorDPO, we think it worth a try to\ntake the hypothesis into SFT to reduce LLM deviation and maintain diversity, and maybe able to\nincrease performance further."}, {"title": "Approach", "content": null}, {"title": "Minor SFT derivation", "content": "DPO(Rafailov et al. (2023)) derives its objective from RL in a closed form.\n$L_{DPO}(\\pi_\\theta; \\pi_{ref}) = -E_{(x,y_w,y_l)\\sim D}[log \\sigma(\\beta (log \\frac{\\pi_\\theta(y_w|x)}{\\pi_{ref}(y_w|x)} - log \\frac{\\pi_\\theta(y_l|x)}{\\pi_{ref}(y_l|x)}))]$\nIt introduces $f_\\theta(x, y) = \\beta log \\frac{\\pi_\\theta(y|x)}{\\pi_{ref}(y|x)}$ as the reward implicitly defined by the language model $\\pi_\\theta$ and\nreference model $\\pi_{ref}$. DPO objective is to maximize rewards margin between the preference pair.\nThe MinorDPO objective adds an additional constraints to dis-preferred samples, by replacing the\noriginal penalty $log \\frac{\\pi_\\theta(y_l|x)}{\\pi_{ref}(y_l|x)}$ with $max(0, log \\frac{\\pi_\\theta(y_l|x)}{\\pi_{ref}(y_l|x)})$.\n$L_{MinorDPO}(\\pi_\\theta; \\pi_{ref}) = -E_{(x,y_w,y_l)\\sim D}log \\sigma(\\beta log \\frac{\\pi_\\theta(y_w|x)}{\\pi_{ref}(y_w|x)} - log \\frac{\\pi_\\theta(y_l|x)}{\\pi_{ref}(y_l|x)})-\\beta max (0, log \\frac{\\pi_\\theta(y_l|x)}{\\pi_{ref}(y_l|x)}))$ \nSo when probability of optimized model on dis-preferred sample is less than probability of reference\nmodel on dis-preferred sample, which means $log (\\frac{\\pi_\\theta(y_l|x)}{\\pi_{ref}(y_l|x)}) <= 0$ and so $max(0, log \\frac{\\pi_\\theta(y_l|x)}{\\pi_{ref}(y_l|x)}) = 0$,\nit will ignore the dis-preferred , and focus on handling the preferred sample.\nUnder this situation, the formula can be rewritten as below:\n$L_{Preferred}(\\pi_\\theta; \\pi_{ref}) = -E_{(x,y)\\sim D}log (\\beta log \\frac{\\pi_\\theta(y|x)}{\\pi_{ref}(y|x)})$\nWe simply name this method SFT using DPO. Eq. 3 tries to maximize reward on the preferred\nsample.\nLet's derive the gradient equation of Eq. 3\n$\\nabla_\\theta L_{Preferred}(\\pi_\\theta; \\pi_{ref}) = -E_{(x,y)\\sim D}[\\beta \\sigma(-Blog \\frac{\\pi_\\theta(y|x)}{\\pi_{ref}(y|x)})[\\nabla_\\theta log \\pi_\\theta(y|x)]]$\nCompared to raw SFT loss gradient equation.\n$\\nabla_\\theta L_{raw\\_sft}(\\pi_\\theta; \\pi_{ref}) = -E_{(x,y)\\sim D}[\\frac{1}{m}\\nabla_\\theta log \\pi_\\theta(y|x)]$\nm is length of the answer. Normally, SFT use average over the answer, while DPO use sum over the\nanswer.\nComparing Eq. 4 and Eq. 5, we see Eq. 4 $\\nabla_\\theta L_{preferred}$ has three part: a hyper-parameter \u1e9e, a\nsample level dynamic coefficient $\\sigma(-Blog \\frac{\\pi_\\theta(y|x)}{\\pi_{ref}(y|x)})$, and a sample related gradient $\\nabla_\\theta log \\pi_\\theta(y|x)$.\nWhile Eq. 5 $\\nabla_\\theta L_{raw\\_sft}$ contains two parts: a sample answer related length $\\frac{1}{m}$, and a sample relate\ngradient $\\nabla_\\theta log \\pi_\\theta(y|x)$.\nHere we introduce the sample level dynamic coefficient $\\sigma(-Blog \\frac{\\pi_\\theta(y|x)}{\\pi_{ref}(y|x)})$ into raw sft loss, and we\nget\n$\\nabla_\\theta L_{minor\\_sft\\_naive}(\\pi_\\theta; \\pi_{ref}) = -E_{(x,y)\\sim D}[-\\frac{1}{m}\\sigma(-Blog \\frac{\\pi_\\theta(y|x)}{\\pi_{ref}(y|x)})\\nabla_\\theta log \\pi_\\theta(y|x)]$\nSince at the start of the training, $\\pi_\\theta$ is same as $\\pi_{ref}$, So $\\sigma(-Blog \\frac{\\pi_\\theta(y|x)}{\\pi_{ref}(y|x)})== \\sigma(0) == 0.5$, so we\nmultiply 2 to make it closer to the raw sft and get final MinorSFT gradient.\n$\\nabla_\\theta L_{MinorSFT}(\\pi_\\theta; \\pi_{ref}) = -E_{(x,y)\\sim D} [\\frac{2}{m}\\sigma(-Blog \\frac{\\pi_\\theta(y|x)}{\\pi_{ref}(y|x)})\\nabla_\\theta log \\pi_\\theta(y|x)]$"}, {"title": "LLM Deviation metric", "content": "Back to the reward aforementioned $r_\\theta(x,y) = Blog \\frac{\\pi_\\theta(y|x)}{\\pi_{ref}(y|x)}$, DPO objective is to maximize the\nrewards margin between the preference pairs. And we can also treat the reward as a metric that\nmeasure\ncomplexity of the sample. As the reward is $\\beta(log\\pi_\\theta(y|x) \u2013 log\\pi_{ref}(y|x))$, high rewards\nmean $\\pi_\\theta$ gives high log probability, which indicate the sample is low complexity.\ndeviation of the model. If we treat the corpora as identical distribution, then high rewards\nmean high relative log probability difference between the optimized LLM $\\pi_\\theta$ and the original\nLLM $\\pi_{ref}$, which indicate a high deviation.\nSo the sample dynamic coefficient $\\sigma(-Blog \\frac{\\pi_\\theta(y|x)}{\\pi_{ref}(y|x)})$ in Minor SFT has a clear physic meaning,\nlower complexity samples have a smaller coefficient than higher complexity samples. In this\nway it dynamically adjusts the training data distribution and the whole training process will pay more\nattention on higher complexity samples.\nBesides, this metric measures how far the optimized model deviate from the original model during\ntraining. But it has two limitations:\nThe reward is related to the hyper-parameter \u1e9e, so rewards of different \u1e9e is not able to do\ncomparison.\nThis reward is related to answer length. so training of distribution with different answer\nlength is not able to do comparison.\nWe need a normalized metric that can be compared not only with different \u1e9e, but also with corpora of\ndifferent answer length. so here we need to normalized both \u1e9e and answer length, and get\n$\\mu_\\theta(x,y) = \\frac{1}{N*m} \\frac{1}{\\beta}log \\frac{\\pi_\\theta(y|x)}{\\pi_{ref}(y|x)}$\nN is batch size, and m is answer length. Thus $\\mu_\\theta(x, y)$ can somehow be used as a training metric to\nmeasure the deviation between the optimized model $\\pi_\\theta$ and the reference model $\\pi_{ref}$. Even SFT do\nnot use hyper-parameter \u1e9e, it can compare with MinorSFT and SFT using DPO( Eq. 3)"}, {"title": "Experiments", "content": "For training settings, we use Qwen2-7B-Instruction(qwe (2024)) as the base model. It expresses\nhigh performance in many benchmarks 2. And use down-sample of FinanceIQ\u00b3, fineval4, ceval-\nexam(Huang et al. (2023)) as test datasets to do evaluation.\nWe use LLaMa-Factory(Zheng et al. (2024)) as the training and inference framework with some\ncustomized code to implement the MinorSFT and SFT use DPO algorithm. The experiments use\nbatch size 64, warm-up ratio 0.1, linear decay learning rate, 1 epoch and run 400+ steps.\nFigure 2 shows the experiment result. We searched a group setting for each method. Figure 2a\ncontains full detail of the comparison. Figure 2b shows the comparison between the best result of\neach method.\nFigure 2a shows after learning, Minor SFT get its best result with lr=2e-5 and \u1e9e=0.04, raw SFT get\nits best result with lr=1e-5, and SFT use DPO get its best result with lr=2e-5 and \u1e9e=0.04.\nFigure 2b indicates that Minor SFT(lr=2e-5, \u03b2=0.04), raw SFT(lr=1e-5), SFT use DPO(lr=2e-5,\nB=0.04) are all better than the base model. Minor SFT perform best in all three datasets compared\nto raw SFT and SFT use DPO. SFT use DPO wins FinanceIQ and ceval-exam but lose fineval\ncompared to raw SFT.\nThe experiment result shows several points.\nEach method have a performance increase from low learning rate to high learning rate, and\nget a performance decrease if continue to increase the learning rate after a certain threshold.\nRaw SFT get its best at lr=le-5, MinorSFT and SFT use DPO get its best at lr=2e-5 and\n\u03b2=0.04.\nMinor SFT perform best in all three datasets. We give credit to the sample-level dynamic\ncoefficient $\\sigma(-Blog \\frac{\\pi_\\theta(y|x)}{\\pi_{ref}(y|x)})$. This coefficient implicitly adjust the corpus distribution,\nso that the training pays more effort on those high complexity(or difficult) samples.\nMinor SFT need higher learning rate to get its best performance compared to raw SFT,\nbecause the sample dynamic coefficient $\\sigma(-Blog \\frac{\\pi_\\theta(y|x)}{\\pi_{ref}(y|x)})$ decay during training when the\nreward grows up(or when the complexity of the sample reduce down). However, even with\nhigh learning rate Minor SFT has a lower deviation compared to raw SFT, which can be see\nthrough Figure 1.\nSFT use DPO perform worse than Minor SFT, we think the cause is due to it use the same\nhyper-parameter \u03b2 for all samples. \u03b2 is somehow used as an average factor same as $\\frac{1}{m}$ in\nraw SFT since DPO use sum over the answer. $\\beta$ is sample dependent while \u03b2 is sample\nindependent, this bias cause the performance regression.\n\u1e9e has same meaning as in DPO, however it still brings more complexity compared to raw\nSFT. It needs some tuning to achieve the best performance."}, {"title": "Conclusion & Future work", "content": "Inspired from DPO and MinorDPO, in this article we propose a training metric $\\mu_\\theta(x, y)$ that can\nused to analysis LLM deviation for SFT phase, and we propose Minor SFT that introduce an dynamic\nsample level coefficient $\\sigma(-Blog \\frac{\\pi_\\theta(y|x)}{\\pi_{ref}(y|x)})$ that implicitly adjust corpora distribution and prevent\noptimized LLM deviate from the reference model too much. Minor SFT can be used in LLM\npreference alignment or downstream task fine-tuning to get better performance and reduce deviation.\nHowever, due to the coefficient, MinorSFT introduce additional computation cost from the reference\nmodel and additional complexity from the hyper-parameter B. It's kind of a tradeoff to get better\nperformance.\nAs the conclusion in above Experiment section, MinorSFT needs some higher learning rate compared\nto raw SFT. We design the MinorSFT coefficient same as the coefficient in DPO to simplify its\nmeaning and understanding. The hyper-parameter \u1e9e in MinorSFT has same meaning as in DPO.\nWith appropriate tuning we are able to get a best performance LLM as in above experiment.\nAlthough the training metric $\\mu_\\theta(x, y)$ can somehow be used to analysis how far the optimized LLM\ndeviate from the reference model for different \u1e9e and answer length, we don't have a way to know\nwhether the optimized model is over-fit or under-fit during the training. It needs more research effort\nto find those metrics that can guide model's fitting level."}]}