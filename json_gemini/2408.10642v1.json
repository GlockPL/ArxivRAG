{"title": "Minor SFT loss for LLM fine-tune to increase\nperformance and reduce model deviation", "authors": ["Shiming Xie", "Hong Chen", "Fred Yu", "Zeye Sun", "Xiuyu Wu"], "abstract": "Instruct LLM provide a paradigm used in large scale language model to align\nLLM to human preference. The paradigm contains supervised fine tuning and\nreinforce learning from human feedback. This paradigm is also used in downstream\nscenarios to adapt LLM to specific corpora and applications. Comparing to SFT,\nthere are many efforts focused on RLHF and several algorithms being proposed,\nsuch as PPO, DPO, IPO, KTO, MinorDPO and etc. Meanwhile most efforts for\nSFT are focused on how to collect, filter and mix high quality data. In this article\nwith insight from DPO and MinorDPO, we propose a training metric for SFT to\nmeasure the discrepancy between the optimized model and the original model, and\na loss function MinorSFT that can increase the training effectiveness, and reduce\nthe discrepancy between the optimized LLM and original LLM.", "sections": [{"title": "1 Background", "content": "LLM trained on very large corpora is extremely powerful language model for completion tasks. SFT\nand RLHF(Ouyang et al. (2022), Ziegler et al. (2020)) are two techniques that used to expose the\nLLM capability and align LLM answer to human instructions. With the increasing reasoning abilities,\nLLM are widely used in industries, and SFT and RLHF are also used to inject domain knowledge\ninto LLM by training on domain corpora.\nIn the past most works are focused on RLHF and several algorithms are proposed, such as\nPPOSchulman et al. (2017), DPO(Rafailov et al. (2023)), IPO(Azar et al. (2023)), KTO(Ethayarajh\net al. (2024)), MinorDPO(Xie et al. (2024)) and etc. One important point of RLHF is to constraint\nthe optimized model not to deviate from the original model too much during the training, and thus\nPPO use KL constraints, DPO use a sample level dynamic coefficient related to distance between the\npreference pair, and IPO use a targeted distance between the preference pair and etc. The purpose of\nthis constraint is to avoid over-fit on the domain corpora and to maintain LLM generalities. It's an\nimportant hypothesis that the base model is powerful enough and the training should not change the\nlanguage distribution too much to maintain the generality and diversity.\nWhile back to SFT, most works are focused on collect, filter and mix high quality data. High quality\ndata is undoubtedly important to get a high qualified and usable LLM, while the aforementioned\nhypothesis that optimized model should not deviate far from the original model is still important.\nOur main contribution is that we introduce a training metrics used in DPO and MinorDPO into SFT\nphase, and propose an improved loss function MinorSFT. MinorSFT use a sample level coefficient\nto control the learning strength. It constraints the discrepancy more compared to raw SFT and may\nprovide better performance result, at the cost of an additional hyper parameter and more computation."}, {"title": "2 Related Work", "content": "Reinforce Learning from human feedback(Ouyang et al. (2022), Ziegler et al. (2020)) is a popular\ntechnique to align LLM to human preference. It uses SFT to train a supervised LLM on data of\nsampled prompt and labeled answer, then trains a reward model on preference pairs from human\nfeedback and finally uses RL algorithm like PPO Schulman et al. (2017) to train an optimized LLM.\nThe RL part contains a KL-divergence constraint to prevent the optimized LLM deviating too much\nfrom the base model.\nDPO(Rafailov et al. (2023)) is a simplified RL algorithm that optimize LLM directly on the preference\ndata using a cross-entropy classification loss. DPO objective is to increase the relative log probability\nof preferred answer to dis-preferred answer. It incorporates a dynamic, sample level importance\nweight scaled by hyper-parameter \u1e9e and claim that \u1e9e account for the strength of the KL-divergence\nconstraint. DPO introduces an important concept that LLM model itself is an implicit reward model,\nwhich means the LLM model can somehow measure the corpora during training phase. Rafailov et al.\n(2024) derive that DPO is token-level MDP and works as a general inverse Q-learning algorithm in a\ntheoretical way.\nMinorDPO(Xie et al. (2024)) is a DPO variant. It justifies hyper-parameter \u1e9e in DPO is a constraint\nrelate to the relative log probability margin of the preference pair, instead of the KL-divergence\nconstraint. It introduces MinorDPO loss to reduce penalty on the reject(dis-preferred) answer to\nprevent over penalty on the reject answer, which implicitly keep to the hypothesis that optimized\nmodel should not deviate too much from the base model.\nIPO(Azar et al. (2023)) proves DPO may be prone to over-fitting when preferred probability over\ndis-preferred probability that is close to 1. In IPO objective it uses a target value relate to the hyper-\nparameter \u1e9e for the relative log probability of preferred to dis-preferred. However, it is somehow\nsame as DPO, that it focuses on the relative log probability margin, so it has same problem as DPO\nmentioned in MinorDPO.\nKTO(Ethayarajh et al. (2024)) proposes human aware loss function. It separates the preference pair\nloss into two losses so that it doesn't purely rely on paired preference data. Inside each separated\nloss, it estimates the KL term by matching input x' with unrelated outputs z in the same batch, but\nwithout back-propagate through the KL term, and thus it also introduces an implicit constraint on the\ngradient which in turn affect the learning strength and deviation.\nLlama 3 (Dubey et al. (2024)) presents a detailed way to collect, filter and mix high quality data for\nSFT and RL. For the RL part it uses DPO with an additional negative log-likelihood loss, similar to\nPang et al. (2024) and mentioned in Pal et al. (2024).\nMany efforts focus on RL part and use explicit or implicit constraints to limit optimized LLM\ndeviation to reduce model regression. Inspired by DPO and MinorDPO, we think it worth a try to\ntake the hypothesis into SFT to reduce LLM deviation and maintain diversity, and maybe able to\nincrease performance further."}, {"title": "3 Approach", "content": ""}, {"title": "3.1 Minor SFT derivation", "content": "DPO(Rafailov et al. (2023)) derives its objective from RL in a closed form.\n\\begin{equation}\nL_{DPO}(\\pi_{\\theta}; \\pi_{ref}) = -E_{(x,y_w,y_l) \\sim D}[log \\sigma(\\beta log \\frac{\\pi_{\\theta}(y_w|x)}{\\pi_{ref}(y_w|x)} - \\beta log \\frac{\\pi_{\\theta}(y_l|x)}{\\pi_{ref}(y_l|x)} )]\n\\tag{1}\n\\end{equation}\nIt introduces $f_{\\theta}(x, y) = \\beta log \\frac{\\pi_{\\theta}(y|x)}{\\pi_{ref}(y|x)}$ as the reward implicitly defined by the language model $\\pi_{\\theta}$ and\nreference model $\\pi_{ref}$. DPO objective is to maximize rewards margin between the preference pair.\nThe MinorDPO objective adds an additional constraints to dis-preferred samples, by replacing the\noriginal penalty $log \\frac{\\pi_{\\theta}(y_l|x)}{\\pi_{ref}(x)}$ with $max(0, log \\frac{\\pi_{\\theta}(y_l|x)}{\\pi_{ref}(y_l|x)}))$ ."}, {"title": "3.2 LLM Deviation metric", "content": "Back to the reward aforementioned $r_{\\theta}(x,y) = \\beta log \\frac{\\pi_{\\theta}(y|x)}{\\pi_{ref}(y|x)}$, DPO objective is to maximize the\nrewards margin between the preference pairs. And we can also treat the reward as a metric that\nmeasure\ncomplexity of the sample. As the reward is $\\beta(log\\pi_{\\theta}(y|x) - log\\pi_{ref}(y|x))$, high rewards\nmean $\\pi_{\\theta}$ gives high log probability, which indicate the sample is low complexity.\ndeviation of the model. If we treat the corpora as identical distribution, then high rewards\nmean high relative log probability difference between the optimized LLM $\\pi_{\\theta}$ and the original\nLLM $\\pi_{ref}$, which indicate a high deviation."}, {"title": "4 Experiments", "content": "For training settings, we use Qwen2-7B-Instruction(qwe (2024)) as the base model. It expresses\nhigh performance in many benchmarks.\nAnd use down sample of FinanceIQ, fineval, ceval-\nexam(Huang et al. (2023)) as test datasets to do evaluation.\nWe use LLaMa-Factory(Zheng et al. (2024)) as the training and inference framework with some\ncustomized code to implement the MinorSFT and SFT use DPO algorithm. The experiments use\nbatch size 64, warm-up ratio 0.1, linear decay learning rate, 1 epoch and run 400+ steps."}, {"title": "5 Conclusion & Future work", "content": "Inspired from DPO and MinorDPO, in this article we propose a training metric $m_{\\theta}(x, y)$ that can\nused to analysis LLM deviation for SFT phase, and we propose Minor SFT that introduce an dynamic\nsample level coefficient $(\\sigma(-Blog \\frac{\\pi_{\\theta}(y|x)}{\\pi_{ref}(y|x)}))$ that implicitly adjust corpora distribution and prevent\noptimized LLM deviate from the reference model too much. Minor SFT can be used in LLM\npreference alignment or downstream task fine-tuning to get better performance and reduce deviation.\nHowever, due to the coefficient, MinorSFT introduce additional computation cost from the reference\nmodel and additional complexity from the hyper-parameter B. It's kind of a tradeoff to get better\nperformance.\nAs the conclusion in above Experiment section, MinorSFT needs some higher learning rate compared\nto raw SFT. We design the MinorSFT coefficient same as the coefficient in DPO to simplify its\nmeaning and understanding. The hyper-parameter $\\beta$ in MinorSFT has same meaning as in DPO.\nWith appropriate tuning we are able to get a best performance LLM as in above experiment.\nAlthough the training metric $m_{\\theta}(x, y)$ can somehow be used to analysis how far the optimized LLM\ndeviate from the reference model for different $\\beta$ and answer length, we don't have a way to know\nwhether the optimized model is over-fit or under-fit during the training. It needs more research effort\nto find those metrics that can guide model's fitting level."}]}