{"title": "PDC & DM-SFT: A Road for LLM SQL Bug-Fix Enhancing", "authors": ["Yiwen Duan", "Yonghong Yu", "Xiaoming Zhao", "Yichang Wu", "Wenbo Liu"], "abstract": "Code Large Language Models (Code LLMs), such as Code llama and DeepSeek-Coder, have demonstrated exceptional performance in the code generation tasks. However, most existing models focus on the abilities of generating correct code, but often struggle with bug repair. We introduce a suit of methods to enhance LLM's SQL bug-fixing abilities. The methods are mainly consisted of two parts: A Progressive Dataset Construction (PDC) from scratch and Dynamic Mask Supervised Fine-tuning (DM-SFT). PDC proposes two data expansion methods from the perspectives of breadth first and depth first respectively. DM-SFT introduces an efficient bug-fixing supervised learning approach, which effectively reduce the total training steps and mitigate the \"disorientation\" in SQL code bug-fixing training. In our evaluation, the code LLM models trained with two methods have exceeds all current best performing model which size is much larger.", "sections": [{"title": "Introduction", "content": "Recently, as large language models (LLMs) achieve remarkable success, code LLMs emerge as useful assistants when editing code. However, when we shift focus to fixing code errors, we find that the performance of open source pre-trained code LLMs like DeepSeek-Coder (Guo et al., 2024), Wizard-Coder (Luo et al., 2023) and Code Llama (Roziere et al., 2023) is quite limited (as shown in Table 1). In this paper, we especially focus on the code repair task of SQL. Due to the complex nested query structure, SQL code bugs are more difficult to solve compared with other code languages. We formulate the SQL code bug-fixing task as Equation 1.\nSQLcorrect = f(Schema, SQLbug, R) (1)\nWhere the f represents the bug-fixing model. Schema means the related tables schemas of bug SQL code. SQLbug denote the SQL code which contains some bugs need to be fixed. R is the return message by the SQL execution system when you run the bug SQL code. SQLcorrect is the bug-fixing model's output, which is expected the right SQL code."}, {"title": "Related Work", "content": "Deep learning-based code bug repair has attracted attention with the advancement of pre-trained LLMs. Most methods follow a zero/few-shot learning paradigm, directly using LLMs to generate repaired code from context. Huang et al. (2023) explored fine-tuning LLMs for bug fixing, showing significant improvements over previous tools.\nOther approaches generate supervised data by transforming correct code into buggy code. BUGLAB (Allamanis et al., 2021) uses self-supervision to train bug detectors, while Break-It-Fix-It (Yasunaga and Liang, 2021) collaboratively trains bug fixers and generators. However, generating realistic SQL bugs remains challenging due to its differences from object-oriented languages.\nAgent-based approaches like RepairAgent (Bouzenia et al., 2024) and SELF-DEBUGGING (Chen et al., 2023) enable LLMs to autonomously fix bugs. But debugging SQL code at the task level"}, {"title": "Progressive Dataset Construction", "content": "In this section, we introduce a set of data collection methods called Progressive Dataset Construction (PDC). The methods include two parts: diverse collecting from online system (breadth first) and oriented generation of offline mining (depth first). The diverse collecting through automated methods ensures the diversity coverage and sustainable scalability of the training datasets, thereby maintaining a consistent alignment between the distribution of training data and the behaviors of online users. The oriented generation method is used for data augmentation in cases where the model performs poorly in evaluation and online serving. This approach requires assistance of code LLM and some SQL corpora recall methods."}, {"title": "Diverse Collecting", "content": "Data Collecting. To collect initial training data, we designed rules to mine online user behavior logs. As shown in Figure 1, when users encounter SQL execution errors, the system logs the erroneous code and error message. Users then typically edit and correct the code until it runs successfully, allowing us to extract many (bugSQL, correctSQL) pairs from their behavior.\nMoreover, since the SQL environment includes syntax checking, some users modify their code based on syntax prompts and save it without re-executing when the highlighted syntax error prompts disappear. Thus, we also consider the last 'save code' operation after an execution error as a signal for identifying correct SQL, as depicted in Figure 1."}, {"title": "Automated filtering", "content": "After collecting data from online user logs, we apply an execution filter as shown in Figure 2. This retains (bugSQL, correctSQL) pairs where the bug SQL causes an error (red) and the correct SQL runs successfully (green). We also remove samples where the difference between the bug SQL and correct SQL is too large to ensure data quality."}, {"title": "Spot check", "content": "Lastly, we conduct a manual sampling inspection of the filtered data to ensure that correct SQL is just transformed from bug SQL by bug fix, without any SQL semantics change. If the [bug SQL, correct SQL] pairs achieve inspection pass rate over 85%, they meet our quality standards and are deemed suitable as training data.\nDiverse collecting samples for bug SQL repair directly from online user behavior ensures an excellent coverage of diversity. It aligns with the natural data distribution in real service scenarios, which is crucial for model training. Even after model's serving online, diverse collection remains essential to identify cases where users reject model's fixes and make manual edits, indicating a mismatch with their expectations."}, {"title": "Oriented Generation", "content": "Oriented generation is a data augmentation method targeting difficult cases, such as unique syntax features and rare long-tail error types. We used regex-based templates to classify bugs from error messages and codes, organizing them into 81 categories across three levels. As shown in the Appendix A.5 Figure 7.\nThe original SQL corpus consists of executable SQL code from historical platform users. As illustrated in Figure 3, we apply this method to augment data for bug types that are challenging for the model, following the steps outlined below:\n(1) Identify target types. Initially, we target rare long-tail bugs. After deployment, we focus on types where model correction accuracy is low.\n(2) Define an \u201cerror feature\u201d for each type. Error features depend on the recall algorithm used. For example, you can use syntax keywords for recall, such as using the keyword"}, {"title": "Recall candidate SQL code", "content": "We employ appropriate rule based matching algorithm to pair a rich corpus of SQL code with each bug type via \"error feature\". As accuracy of matching varies across different bug types, different matching algorithm for different bug type is needed sometimes."}, {"title": "Generate bug SQL samples for each bug type", "content": "This step requires assistance of a robust code LLM for the generation of bug SQL code. In our practice, the quality of bug SQL generated is closely tied to the prompt. We provide a reference prompt in Appendix A.1 used in our internal code fundamental LLM for bug SQL generation."}, {"title": "Dynamic Mask Supervised Fine-tuning", "content": "In this section, we present a detailed introduction to an efficient training method for LLM SQL code bug fixing, which refer to as dynamic mask supervised fine-tuning (DM-SFT). The Figure 4 compares DM-SFT with default generative SFT in terms of training and loss calculation. As described in the Introduction, the input prompt (Appendix A.2) composed of three pieces information: [tables DDL, bug SQL, report error]. The model's output is a complete, corrected SQL code. Notably, most lines between the bug SQL and correct SQL are identical, with only a few requiring changes.\nIn our collected training data, the count distribution of code lines that need to be modified when editing from bug SQL to correct SQL (called as diff lines) is shown in Appendix A.5 Figure 9. Over 92% of cases have fewer than 5 diff lines, meaning most correct code is already present in the input (bug SQL). In default generative fine-tuning, all output tokens contribute equally to the calculation of final loss, leading to issues like slow convergence and unstable training, which we will detail in the experimental section.\nTo address these issues, we propose a code bug repair training method called dynamic mask SFT. During the model training process, we divide the correct SQL code that the model is expected to predict post bug-fixing into two categories in line-by-line basis:\n(i) Consistent lines: Code lines unchanged from the original bug SQL.\n(ii) Diff lines: Code lines that require modification.\nGiven a bug SQL code, related tables schema, report error and corresponding correct SQL code, we use (l0, l1, l2,\u2026\u2026,d0,\u2026\u2026\u2026, dm,\u2026ln), m \u2264 n denoting the correct code lines. The li, i \u2208 [0,n] represents the consistent lines and dj,j \u2208 [0,m] represents the diff lines. We use u to denote tokens of consistent lines, and v to denote tokens of diff lines. Equation 2 shows the loss function of dynamic mask SFT.\nL\u2081 =\u2211log P(Uk+1 | Uk, Uk\u22121,..., Uo)*a(l(uk+1)) (2)\na(li) = {0 if li(1-p) p(3)\nWhere a(li) is the mask weight of line li as Equation 3, and mask weight of all tokens in line li are the same. The p is random mask ratio factor,"}, {"title": "PDC and SFT Experiments", "content": "We demonstrate the efficacy of the suit of methods (PDC & DM-SFT) through a series of experiments. We collected 3k diverse samples through the diverse collecting method and 300+ oriented enhancement samples based on code LLM by the oriented generation method. Based on these 3.3k data, we conducted ablation experiments to verify DM-SFT's effectiveness.\nWe use DeepSeek-Coder-instruct (6.7b) as the fundamental model and carry out the training experiments on a cluster of 32 \u00d7 NVIDIA A800 80GB GPUs using the DeepSpeed (Rajbhandari et al., 2020) framework stage 3. In terms of hyperparameters setting, we used batch size = 32, learning rate = 1.2e-5, and AdamW optimizer (Loshchilov and Hutter, 2017) with adam_beta1 = 0.9 and adam_beta2 = 0.95 (more detailed experimental parameter configurations, please refer to code release information in the final part of this section).\nWe constructed a 1,072-entry evaluation dataset. 748 entries were randomly sampled from execution logs on in data platform, reflecting natural distribution of SQL error types in production. The remaining 324 entries were crafted to cover 81 error types(one type four examples). This ensures alignment with real-world scenarios and allows performance estimates on long-tail errors. The ground truth of the dataset is precisely annotated by staff SQL engineers. During the model development stage, we used machine automatic evaluation (a method based on AST semantic comparison) results to select the approximate best training steps and hyper-parameters. Besides, in some samples, there's more than one correct way to fix the bug. The final model's bug fixing accuracy was determined by human evaluation of staff SQL engineers."}, {"title": "Mask Ratio Experiments", "content": "Taking the best-performing DeepSeek-Coder-6.7B-instruct as the foundation model, we trained with different p values to evaluate bug-fixing capability. The results presented in Figure 5. After that, we compared the impact of different random mask ratio factors p on per-token loss reduction process, as illustrated in Figure 6. From Figure 5 and Figure 6, we can draw the following three conclusions:\n(i) In the early stages of training (less than 400 steps), a higher p value results in greater per-token loss. In the later stages (after 500 steps), the per-token loss converges regardless of the value of p. This phenomenon is intuitive as the mask ratio factor effectively amplifies the weight of the diff code tokens loss on pre-trained LLM, the loss of diff code is greater than the loss of consistent code that has appeared in the prompt. As the model gradually converges, the difference in per-token loss between the two diminishes.\n(ii) Generally, the higher value of p, the fewer training steps are required to reach the checkpoint with the best bug-fixing capability. This is a key advantage of dynamic mask SFT, in addition to its ability to enhance the model's bug-fixing capabilities. This allows for improved model performance with lower computational costs and energy consumption.\n(iii) From Figure 5, we can clearly see that when the value of p is between [0.4,0.7], all the trained models achieve optimal performance. When the value of p is 1 (completely ignoring the loss of identical code lines), the performance of the model is worse than those using the default generative SFT (where p is 0)."}, {"title": "Conclusion", "content": "In this paper, we innovatively propose a set of methods to enhance LLMs for SQL bug fixing, from data construction and model training aspects. For data construction, we introduce two approaches: a breadth-first diverse collecting method and a depth-first oriented generation method. The diverse collecting method mines user behavior for annotated data reflecting real-world scenarios distribution. The oriented generation method targets specific model weaknesses by data augmentation. Both methods are sustainable iteration and semi-automated, requiring minimal manual labor. That's why named as Progressive Dataset Construction (PDC). For training methodology, we propose the dynamic mask SFT, which is applicable to all generative code bug repair tasks. This method improves bug-fixing capability by nearly 10% compared to default SFT and reduces the training time."}, {"title": "Limitations", "content": "Only generate the modification code lines We attempted a highly efficient and intuitively appealing approach that involves generating only the correct code for the diff sections. Specifically, our approach required the model to output the lines of code that needed modification and the corrected code after changes. This definition could handle all code rewriting operations, including additions (where a single line of original code is replaced by multiple lines), deletions (where multiple lines of original code are replaced by an empty string), and modifications (where multiple lines of original code are replaced by multiple lines of new code). Unfortunately, this method resulted in impaired model performance due to the lack of context in the outputs, making it challenging to achieve the accuracy of generating complete code, both in prompt engineering experiments on GPT-4 (Achiam et al., 2023) and in SFT training on open-source code LLMs.\nToken level dynamic mask SFT A pertinent question arises as to why consistent lines cannot use token-level dynamic masking and must instead be masked by code lines. Indeed, in our earliest practices, we masked at the token level. However, perplexingly, models masked at the token level struggled to converge, and during evaluations, a portion of the samples consistently failed to generate complete and usable code. This remains a puzzle we have not fully resolved. We hypothesize that for programming languages, a line may correspond to a more complete semantic module, and token-level masking disrupts this contextual integrity. Research on this aspect will continue in subsequent studies."}, {"title": "Bug SQL generation prompt of oriented generation method", "content": "Based on the SCHEMAS and TARGET SQL, help to generate the error sql which are related to SCHEMAS and similar to TARGET SQL. The generated error sql should contain error related to ERROR INFO. You should obey the following RULES.\nRULES\n1. If the SCHEMAS are empty, it means the TARGET SPARK SQL is not related to any schemas.\n2. ERROR INFO should not be appeared in explanation.\n3. Except for error part of code, other parts of code should be same between correct sql and error sql.\n4. Comments and indents in generated error sql and correct sql should be the same.\n5. If it is hard to generate error sql which is similar to the TARGET SQL related to ERROR INFO, please return no in suitable field, otherwise it should be yes.\nBelow is a brief example which you can refer to (if the slots of example is empty please ignore Example section):\n[EXAMPLE]\ntarget sql:\nTARGET_SQL_EXAMPLE_PLACEHOLDER\nerror info:\nERROR_INFO_EXAMPLE_PLACEHOLDER\nerror sql:\nERROR_SQL_EXAMPLE_PLACEHOLDER\nNow give you the tables schema, corresponding target SQL and error type information as below. Please write a error SQL that match the error type information.\n[SCHEMAS]\nSCHEMAS_PLACEHOLDER\n[TARGET SPARK SQL]\nTARGET_SPARK_SQL_PLACEHOLDER\n[ERROR INFO]\nERROR_INFO_PLACEHOLDER\nRESPONSE REQUIREMENT\nReturn json str which can be parsed by json.loads() of python3 as following:\n{\"error sql\": \"\", \"correct sql\": \"\", \"reason\": \"\", \"suitable\": \"\"}"}, {"title": "Bug fixing model's input prompt", "content": "Requirements: Directly generate the right SQL.\n[TABLES SCHEMA]\nTABLES_SCHEMA_PLACEHOLDER\n[BUG SQL]\nBUG_SQL_PLACEHOLDER\n[ERROR MESSAGE]\nERROR_MESSAGE_PLACEHOLDER\nQuestion: BUGFIX task\nBased on the error SQL code, error messages, and input table schema, please fix the bugs and write the corresponding correct SQL code. Remember not to change any existing comments and SQL code without errors.\nResponse:"}]}