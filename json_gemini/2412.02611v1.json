{"title": "AV-Odyssey Bench: Can Your Multimodal LLMs Really Understand Audio-Visual Information?", "authors": ["Kaixiong Gong", "Kaituo Feng", "Bohao Li", "Yibing Wang", "Mofan Cheng", "Shijia Yang", "Jiaming Han", "Benyou Wang", "Yutong Bai", "Zhuoran Yang", "Xiangyu Yue"], "abstract": "Recently, multimodal large language models (MLLMs), such as GPT-40, Gemini 1.5 Pro, and Reka Core, have expanded their capabilities to include vision and audio modalities. While these models demonstrate impressive per-formance across a wide range of audio-visual applications, our proposed DeafTest reveals that MLLMs often struggle with simple tasks humans find trivial: 1) determining which of two sounds is louder, and 2) determining which of two sounds has a higher pitch. Motivated by these observations, we introduce AV-Odyssey Bench, a comprehensive audio-visual benchmark designed to assess whether those MLLMs can truly understand the audio-visual information. This benchmark encompasses 4,555 carefully crafted problems, each incorporating text, visual, and audio components. To successfully infer answers, models must effectively leverage clues from both visual and audio inputs. To ensure pre-cise and objective evaluation of MLLM responses, we have structured the questions as multiple-choice, eliminating the need for human evaluation or LLM-assisted assessment. We benchmark a series of closed-source and open-source mod-els and summarize the observations. By revealing the limi-tations of current models, we aim to provide useful insight for future dataset collection and model development.", "sections": [{"title": "1. Introduction", "content": "Multimodal Large Language Models have evolved progres-sively, beginning with vision language models. Vision Lan-guage Models (VLMs), exemplified by GPT-4V(ision) [55], have endowed language models with visual perception, en-"}, {"title": "2. Related Work", "content": "Multimodal Large Language Models. Large language models (LLMs) have demonstrated remarkable perfor-mance across diverse textual domains [6, 7, 55, 60, 73]. The success of these models has catalyzed significant advance-ments in vision language models and multimodal large lan-guage models. Inspired by the textual prowess of LLMs, vision language models have emerged to extend computa-tional capabilities into visual comprehension. These models enable LLMs to perform sophisticated visual tasks, includ-ing visual question answering [5, 18, 34, 39, 40, 85, 91, 96], visual grounding [11, 12, 58, 76], document understand-ing [25, 50, 85, 94], long video understanding [36, 41, 62, 64, 92]. Building upon vision-language achievements, re-searchers have further expanded multimodal horizons by in-tegrating the audio modality [15, 22, 23, 47, 67, 77, 89]. These advanced models now accommodate audio inputs, further expanding the landscape of multimodal artificial in-telligence.\nBenchmarking Multimodal Large Language Models.The rapid development of vision language models has been accompanied by the emergence of specialized benchmarks to assess their performance across various domains [9, 21, 32, 49, 87]. A significant subset of these benchmarks fo-cuses on vision comprehension [21, 31, 32, 87] and mathe-matical reasoning capabilities [8, 9, 49, 63, 87, 93]. How-ever, current audio-visual benchmarks [33, 37, 68, 81, 88] face significant limitations in comprehensively assessing multimodal large language models (MLLMs). Firstly, they predominantly focus on high-level visual tasks and neglect to explore the basic auditory perception limitations. Sec-ondly, they are limited in application domains. For exam-ple, Music-AVQA [33] limits evaluation to the music do-main, and AVQA [81] primarily tests daily life applica-tions. Thirdly, they do not comprehensively evaluate all attributes of the audio. In contrast, this paper introduces DeafTest tasks to evaluate fundamental capabilities and the AV-Odyssey benchmark, which spans a wide spectrum of"}, {"title": "3. Method", "content": "3.1. DeafTest Tasks\nDrawing inspiration from the Schwabach test [26], we in-troduce DeafTest, a suite of four simple auditory tasks that critically examine the fundamental audio perception capa-bilities of Multimodal Large Language Models (MLLMs). DeafTest includes the determination of the number of sounds, identification of the louder sound, recognition of the sound with a higher pitch, and detection of the sound with a longer duration. We hypothesize that MLLMs may not per-form as well as expected on these basic tasks. This poten-tial shortcoming arises from the training objectives of these models, which primarily focus on achieving high-level se-mantic alignment between different modalities. Conse-quently, this approach tends to overlook the effective uti-lization of low-level auditory information, which is crucial for accurately processing and understanding basic sound characteristics.\n1. Count the Number of Sounds. Given that Mul-timodal LLMs achieve impressive performance on ASR (GPT-40's 3% word error rate on ASR Western Eu-rope) [54], we expect that counting the number of sounds is not difficult for MLLMs. In this task, we give an audio clip that contains several sounds ranging from 3 to 8 and ask MLLMs for the number of sounds. In an audio clip, the sounds are monotonous and clearly separated by a silent clip. We formulate these queries as two-choice questions. The MLLMs only need to predict the correct option. The question number for this task, as well as for all remaining tasks in DeafTest, is set to 100.\n2. Discriminate the Louder Sound. In this task, we test the basic ability of MLLMs to distinguish between the loudness of sounds. The goal of MLLMs is to discriminate which sound is louder out of two given audio clips. Specifi-cally, the decibel for quieter audio ranges from 30 dB to 60 dB, while the decibel for louder audio ranges from 70 dB to 100 dB. We randomly sample decibels from these two ranges to create two audio clips. In addition, we randomly switch the input order of the two audio clips; that is, for some questions, the quiet audio comes first, and for the rest, the loud audio comes first. Similarly, the question format is also a two-choice question.\n3. Discriminate the Higher Pitch. This task is simi-lar to task 2 in 3.1. We also create two audio clips. The key difference between the two audios is pitch. Pitch is the basic element of sound, which is helpful in discriminating tone, emotion, environment, etc. For the lower pitch audio, we randomly sample its pitch from 100Hz to 500Hz, while"}, {"title": "3.2. Overview of AV-Odyssey Bench", "content": "Our AV-Odyssey Bench is a meticulously curated bench-mark designed to comprehensively assess the audio-visual capabilities of MLLMs. To ensure a robust and unbiased assessment, all questions in AV-Odyssey are structured as multiple-choice, with four options per question, and options can be presented in various formats, including text, images, or audio clips. To mitigate format-specific biases, we have curated five distinct multi-choice question types. Addition-ally, all inputs, including text, image/video, and audio clips, are fed into MLLMs in an interleaved manner."}, {"title": "3.3. Data Curation Process", "content": "Data Collection. AV-Odyssey Bench is an audio-visual benchmark to evaluate whether MLLMs truly have audio-visual reasoning capability. Since the audio is the newly added modality by these omni-modal models and there is already an array of visual benchmarks, we put our at-tention on the attributes of sound in the benchmark con-struction. We first go through all the attributes of sound, such as timbre, tone, time, space, etc. Then, we start from a specific attribute of sound and span the domains to cover a wide range of application domains, such as mu-sic, daily life, and transportation. We primarily use two strategies to construct questions: 1) For most concept-matching questions (e.g., bird recognition, material recog-nition), we gather audio clips from public datasets and crawl corresponding visual data from the internet to automati-cally generate questions and options. Human experts con-"}, {"title": "4. Experiment", "content": "We test various closed-source and open-source MLLMs that accommodate the inputs of text, image/video, and audio. Experiments are conducted in the zero-shot setting to evalu-ate the performance of MLLMs without finetuning and few-shot prompting. The text prompts are designed as concise as possible to remove redundant information.\n4.1. Models\nWe evaluate 18 models in total, 8 closed-source mod-els, including Gemini 1.5 Flash, Gemini 1.5 Flash-8B, Gemini 1.5 Pro [70], Reka Core, Reka Flash, Reka Edge [71], GPT-40 [27] and 10 open-source models in-cluding Unifed-IO-2 L [47], Unified-IO-2 XL, Unified-IO-2 XXL, OneLLM [23], PandaGPT [67], Video-llama [90], VideoLLaMA2 [15], AnyGPT [89], NEXT-GPT [77], VITA [22]. We test those open-source models based on their source code and the latest checkpoint and test the closed-source models with available APIs.\nSince we currently cannot access the GPT-40 API that supports simultaneous image, video, and audio inputs, we have adopted an alternative approach to evaluating GPT-40 models. The GPT-40 series includes two types of APIs: GPT-40, which processes image and text inputs, and GPT-40-audio, which processes audio and text inputs. Based on these two APIs, we develop two methods to evaluate GPT-40: (1) We use GPT-40-audio to generate captions for audio"}, {"title": "4.2. Main Result Analysis", "content": "In this section, we analyze the performance of MLLMs in our AV-Odyssey benchmark, as presented in Table 4. Due to the space limit, detailed results and data distribution are provided in the Appendix. Our key findings are as follows:\nChallenging Nature of AV-Odyssey. As presented in Table 4, the average performance of most existing MLLMS is only marginally higher than 25%-comparable to the ex-pected accuracy of random guessing on four-choice ques-tions. Notably, even the top-performing model in our AV-Odyssey, GPT-40 audio caption, only achieves 34.5% ac-curacy. This result underscores the high level of challenge posed by our benchmark, which significantly goes beyond the distribution of training data of current models. By set-ting rigorous standards, our benchmark serves as a crucial tool for evaluating the robustness and versatility of MLLMs in audio-visual tasks. It highlights the limitations of existing models and provides directions for future improvements."}, {"title": "4.3. Error Analysis", "content": "In this section, we focus on the errors of Gemini 1.5 Pro to analyze the underlying causes. For each task, we randomly"}, {"title": "5. Conclusion", "content": "In this work, we introduce AV-Odyssey Bench, a compre-hensive audio-visual benchmark designed to evaluate the"}]}