{"title": "Can Search-Based Testing with Pareto Optimization Effectively Cover Failure-Revealing Test Inputs?", "authors": ["Lev Sorokin", "Damir Safin", "Shiva Nejati"], "abstract": "Search-based software testing (SBST) is a widely-adopted technique for testing complex systems with large input spaces, such as Deep Learning-enabled (DL-enabled) systems. Many SBST techniques focus on Pareto-based optimization where multiple objectives are optimized in parallel to reveal failures. However, it is important to ensure that identified failures are spread throughout the entire failure-inducing area of a search domain, and not clustered in a sub-region. This ensures that identified failures are semantically diverse and reveal a wide range of underlying causes. In this paper, we present a theoretical argument explaining why testing based on Pareto optimization is inadequate for covering failure-inducing areas within a search domain. We support our argument with empirical results obtained by applying two widely used types of Pareto-based optimization techniques, namely NSGA-II (an evolutionary algorithm) and OMOPSO (a swarm-based algorithm), to two DL-enabled systems: an industrial Automated Valet Parking (AVP) system and a system for classifying handwritten digits. We measure the coverage of failure-revealing test inputs in the input space using a metric, that we refer to as the Coverage Inverted Distance (CID) quality indicator. Our results show that NSGA-II and OMOPSO are not more effective than a na\u00efve random search baseline in covering test inputs that reveal failures. We show that this comparison remains valid for failure-inducing regions of various sizes of these two case", "sections": [{"title": "1 Introduction", "content": "Search-based software testing (SBST) is an effective method for testing complex systems with large input spaces (Zeller, 2017). SBST employs meta-heuristics, such as evolutionary algorithms (Ben Abdessalem et al., 2018; Borg et al., 2021; Humeniuk et al., 2022; Kl\u00fcck et al., 2019; Moghadam et al., 2021, 2023; Riccio and Tonella, 2020; Sorokin and Kerscher, 2024), to reveal failure-revealing test cases. It formulates testing as an optimization problem, capturing system safety requirements through multiple objectives that are often optimized using Pareto-based algorithms.\nPareto-based SBST techniques are generally assessed by their effectiveness in identifying failures, focusing on the quantity and diversity of the failures detected. However, we still do not know if Pareto-based SBST is capable of achieving adequate coverage across the space of failure-revealing test inputs. Ideally, we are interested in a testing method that can achieve high coverage over the space of failure-revealing test inputs. That is, it can identify failure-revealing test inputs distributed across the entire failure-inducing area of a search domain, rather than clustered in one specific sub-area.\nThe identification of such diverse failing test inputs can support the detection of underlying failure causes and conditions leading to failures (Ben Abdessalem et al., 2018; Jodat et al., 2024). Existing research on software testing views the identification of diverse failures as an important testing objective (Aghababaeyan et al., 2023; Feldt et al., 2016). For example, Aghababaeyan et. al. (Aghababaeyan et al., 2023) argue that we are more likely to identify diverse failures of a system if we identify diverse test inputs.\nSeveral Pareto-based SBST algorithms use heuristics to make the search more explorative with the original goal of escaping local optima. This approach is consistent with the broader focus within the evolutionary search community on balancing the trade-off between exploration and exploitation in search algorithms. By investigating various heuristics and hyperparameter tunings, researchers aim to adjust the behavior of algorithms across the spectrum of exploration and exploitation, which is crucial for enabling algorithms to avoid local optima (\u010crepin\u0161ek et al., 2013). Exploring the trade-off between exploration and exploitation has an important practical use case in software testing as well, since increasing exploration within often exploitative Pareto-based al-"}, {"title": "Limitations of Pareto-driven Search-based Testing", "content": "gorithms has been shown to lead to the discovery of more diverse failures. For example, Clune et al. (Clune et al., 2005, 2008) propose making genetic operators in evolutionary algorithms more explorative by increasing mutation rates when the performance of the search shows marginal improvement. Multiple approaches combine Pareto-based optimization with novelty search (Mouret, 2011; Riccio and Tonella, 2020; Zohdinasab et al., 2023, 2021) and maximize the distance of candidate solutions to previously found test inputs to escape local optima in the fitness landscape. However, it has never been studied how well Pareto-based algorithms cover the failure-inducing regions within a search domain.\nIn this paper, we aim to study Pareto-based SBST techniques in terms of their capability to cover the failure-inducing test inputs within a search domain by addressing the following question:\nCan Pareto-based search-based testing achieve high coverage of failure-revealing tests?\nWe first present a theoretical argument explaining why Pareto-based SBST algorithms cannot achieve high coverage of failure-revealing tests within the search space. Our argument builds on a definition of an SBST problem characterized by the following two assumptions:\n(A1) The problem involves optimizing multiple quantitative fitness functions simultaneously using the principle of Pareto optimality.\n(A2) The test oracle function of the SBST problem specifies subsets of the objective space as the failure regions such that at least one such subset has the same dimensionality as the objective space, i.e., the dimensionality is equal to the number of fitness functions.\nFor example, consider a SBST problem with two real-valued fitness functions $f_1$ and $f_2$, and a test oracle function O. Suppose O indicates a test input v as a failure if the following constraint holds: $(f_1(v) > 5) \\lor (f_2(v) < 10)$. The test oracle O satisfies assumption A2 since it specifies a two-dimensional space as a failure region. Briefly, our argument indicates that any Pareto-based optimization algorithm for $f_1$ and $f_2$ ultimately identifies a Pareto front intersecting the two-dimensional failure region in only one dimension, which likely cannot ensure good coverage of the failure region defined by O. Other than the assumptions A1 and A2, our argument is not dependent on the number and definition of fitness functions, or the heuristics used to approximate the optimal solution. We note that assumption A2 is not constraining, as test oracles for many SBST formulations applied to cyber-physical systems and Deep Learning (DL) systems conform to this assumption (Ben Abdessalem et al., 2016, 2018; Borg et al., 2021; Ebadi et al., 2021; Jahangirova et al., 2021), making our argument valid for these existing SBST formulations."}, {"title": "3 Related Work", "content": "In this section, we outline two streams of related work. In the first part, we discuss SBST approaches for generating diverse test data, highlighting both Pareto-based and non-Pareto-based methods. In the second part, we discuss metrics used to assess the diversity and coverage of SBST testing techniques."}, {"title": "3.1 Diversified SBST Approaches", "content": "Table 1 provides an overview of SBST techniques aimed at generating diverse tests and failures. Below, we briefly discuss these approaches and contrast them with our work.\nDeepJanus (Riccio and Tonella, 2020) is an NSGA-II-based search approach designed to identify the boundaries of the failure-revealing regions in DL-based systems. In particular, it extends a Pareto-based genetic algorithm with concepts from novelty search (Lehman and Stanley, 2011b; Mouret, 2011). While Pareto-driven testing promises identifying interesting tests by optimizing a single or multiple fitness functions, novelty search requires no definition of a fitness function. In novelty search the identification of promising solutions is guided solely by the novelty a solution has. Novelty is quantified by using a distance measure applied between the candidate and solutions which have been identified in previous iterations of the algorithm (called archive)."}, {"title": "3.2 Diversity and Coverage Metrics for SBST", "content": "Table 2: Classification of existing studies assessing test case diversity and coverage of SBST algorithms based on the following criteria: their underlying search algorithms, the metrics they use to measure diversity or coverage, whether the metric measures diversity, and whether the metric measures coverage. RS stands for Randomized Search, GA stands for Genetic Algorithm, and NMCTS stands for Nested Monte Carlo Tree Search.\nSeveral metrics are proposed to assess Pareto-based testing algorithms with respect to the objective space, which is the space of fitness/objective values (Li et al., 2022; Li and Yao, 2019). In contrast, since in our paper we focus on the input space, we have included in Table 2 an overview of the approaches proposed to assess the capabilities of SBST techniques in terms of diversity and coverage when the focus is either on the input space or the feature space, i.e., a representation of the input data.\nBelow, we discuss these techniques and contrast the metric they use to measure diversity or coverage with our Coverage Inverted Distance (CID) metric.\nFeldt et al. (Feldt et al., 2016) have proposed a metric to evaluate the diversity of a test suite. The metric is based on the Normalized Compression Distance (NCD), which is a distance metric based on information theory and can be applied to any type of test data. However, this metric is not meant for assessing the test input space coverage (DOI), but rather the diversity of test data. While diversity correlates with coverage, in our approach, we focus on assessing the coverage of failure-inducing regions with the goal of"}, {"title": "4 Theoretical Argumentation", "content": "We claim that Pareto-optimal solutions computed by Pareto-based approaches do not necessarily lead to an adequate coverage of the DOI defined in Definition 1, i.e., the set of failure-revealing test inputs. Our argumentation builds on the definitions of the SBST problem (Definition 1) and Pareto-based optimization (Definition 2).\nWe use Figures 1a and 1b to illustrate our argument. These figures, respectively, show the objective and search spaces for a Pareto-based optimization problem with two fitness functions, i.e., m = 2, and two input variables, i.e., n = 2. The grey area in Figure 1a represents the test oracle function O which identifies the subset of the objective space encompassing failures. The portion of the grey area dominated by the true Pareto-Front (PF) and delineated with dashed lines in Figure 1a represents the set of reachable failures which is called the codomain of interest (COI) in Definition 1. Due to assumption A2 in Def-"}, {"title": "5 Metric for DOI Coverage Assessment", "content": "To empirically assess the coverage of the actual failing test inputs of an SUT, we introduce the CID metric in this section. CID allows us to quantitatively evaluate the coverage of the DOI by failure-revealing test inputs. In Section 5.1, we introduce CID. In Section 5.2, we explore methods for approximating the set of failures, which serves as the reference set for CID, and discuss the accuracy and robustness of reference sets."}, {"title": "5.1 Metric Definition", "content": "In this section, we present Coverage Inverted Distance (CID), a metric specifically designed to assess how well the failures identified by a testing technique cover the actual set of failure-revealing test inputs for a system. That is, CID evaluates how well a given set of failures identified by a testing technique covers the DOI. Let Z be a finite set approximating the DOI for a given SBST problem, and let A be a finite set of solutions generated by a testing method for that SBST problem. In the remainder, we call Z a reference set and A a test set. Our metric CID, defined below, assesses how well the test set A approximates, i.e. covers, the reference set Z:\n$CID(A, Z) = \\frac{1}{|Z|} \\sum_{z \\in Z} d_z^{q}$  (2)\nwhere $d_z = (\\sum_{j=1}^{n} |z_j - x_j|^p)^{1/p}$ represents the distance imposed by the p-norm from a reference point $z = (z_1,..., z_n) \\in Z$, to the nearest point $x = (x_1,..., x_n)$ from the test set A in the search space, while q represents the norm for computing the mean of the distance (i.e., q = 1 for the generalized mean, or q = 2 for the power mean). If q = 1 is chosen, the metric results in the average distance from a reference point to the closest point in A. If at the same time p = 2 is chosen, then the defined distance between the points is the Euclidean distance. If no specific characteristics of the system under test are known, the Euclidean distance is the most practical option (Fuangkhon, 2022). The lower the CID value, the better the test set represents the reference set. The CID value reaches zero if the test set achieves a complete representation of the reference set.\nIn our evaluation in Section 6, we use the Euclidean distance to compute the distance between the points in the reference set (Z) and the test points in the solution set (A), i.e., p = 2. Further, we use the general mean to compute CID values, i.e, q = 1.\nWhile A is computed by a given testing method, the reference set Z should be computed independently from the testing method, and should represent the DOI as accurately as possible. In our study, we approximate the reference set Z using a grid sampling approach. Specifically, we discretize the search domain by segmenting each search variable into a predetermined number of equal intervals. We test the system under test, e.g., ADS, for each point in the discretized search domain to determine whether it is passing or failing. The reference set Z is then defined as the set of failing test cases within the search space.\nWe discuss CID and its adequacy using illustrative examples. Figure 2 illustrates three examples, in which the DOI is represented as two separate regions, highlighted in pink, and the test set A and reference set Z are illustrated as solid red, and empty pink circles respectively. In Figure 2(a), A includes a"}, {"title": "5.2 Reference Set Generation and Properties of CID", "content": "To apply CID, we need to generate a reference set to approximate the actual set of failures exhibited by the SUT. We can use various sampling techniques to create test inputs that are uniformly scattered across the search space in order to generate this reference set. Note that when we sample points to generate the reference set, we generate values within the search space. Sampling in this context means generating a set of points in the search space according to a sampling strategy. One such strategy is grid sampling, discussed in Section 5.1 where the selected test inputs are the nodes of a regular grid within the search space. Other alternative sampling strategies are furthest point sampling (Eldar et al., 1997) and Poisson disc sampling (Bridson, 2007). FPS is a sampling strategy that iteratively generates samples in such a way that the minimal distance from already sampled points to a new sample is maximized. Poisson disc sampling allows generating points in a given space while maintaining a user-defined minimal distance between sampled points.\nWe call a reference set generated by a uniform sampling strategy, such as the three strategies above, a uniform reference set. Consider a DOI containing several disconnected regions. We say a uniform reference set is optimal for a"}, {"title": "6 Empirical Study", "content": "While Section 4 presents an argument addressing our research question introduced in Section 1, this section aims to answer our research question through empirical evidence. We assess the effectiveness of Pareto-based testing algorithms by comparing their ability to achieve high coverage of failure-revealing tests with that of baseline random testing. To ensure the validity and diversity of our empirical results, our experiments include alternative Pareto-based testing algorithms, case study systems, and varying definitions of test oracles for these systems.\nIn particular, we selected three optimization testing algorithms or their variants, which are Pareto-based and have been previously applied to case studies similar to ours, as identified in our literature survey in Table 1. Specifically, we selected the genetic algorithm NSGA-II, DeepJanus, a hybrid genetic algorithm that uses concepts from Novelty Search (Mouret, 2011) and evolutionary search, and OMOPSO (Sierra and Coello Coello, 2005), a swarm optimization algorithm. Other algorithms have been not considered in our study as they are non Pareto-based and hence outside of our scope. We present our empirical results using the following two sub-RQs:\nRQ1: How does Pareto-driven search-based testing compare to baseline random testing in terms of covering failure-revealing tests?\nTo answer this research question, we use our proposed CID metric to compare the test results obtained by the well-known Pareto-based algorithms"}, {"title": "6.1 Experimental Setup", "content": "In this section, we describe our experimental setup including the details of our case study systems, the implementation of NSGA-II, diversified NSGA-II, denoted by NSGA-II-D, OMOPSO, and RS, and the metrics used in our empirical analysis. An overview of the configuration of the search algorithms is given in Table 3. We note that we selected the parameters in this table after"}, {"title": "6.2 Experimental Results", "content": "Figures 5 and 6 show the CID results obtained by applying RS, NSGA-II, and NSGA-II-D and OMOPSO to the MNIST and AVP case studies. For each system, the CID results are computed with respect to their respective test oracle definitions that correspond to different DOI sizes. The diagrams in Figures 5 and 6 illustrate the average and standard deviations of the CID values derived from 10 runs of each algorithm after every 100 and 250 evaluations, respectively.\nThe CID values in the diagrams in Figures 5 and 6 are calculated using reference sets generated by grid sampling for each case study and each test oracle definition. We use the same reference set for each combination of algorithm, case study, and test oracle definition. However, the number of failures in the reference set depends on the oracle definition.\nWe note that due to our interest in coverage, we choose the set A, i.e., the solution set, used to compute CID as the set of all failures found by each testing algorithm over all its iterations. Specifically, for each of the NSGA-II, NSGA-II-D, and OMOPSO algorithms, the set A is the set of all failures found over all generations and not just the failures in the last generation. Similarly, for random search, set A is the set of all failures found by the random search.\nThe AVP's reference sets contain 15,625 samples, while the MNIST's reference sets comprise 1,000 samples. In the remainder of this section, we discuss the results for RQ1 and RQ2 using the results in Figures 5 and 6."}, {"title": "6.2.1 RQ1 Results", "content": "Comparing CID results. The MNIST Case Study results in Figure 5 show that RS consistently yields lower, i.e., better, average and lower standard deviations for CID values than NSGA-II and OMOPSO over time across all test oracles. Moreover, although CID values for NSGA-II, OMOPSO and RS decrease over time, the rate of reduction is more significant for RS compared to NSGA-II and OMOPSO.\nFurther, the results in Figure 5 show that while the CID values for RS, NSGA-II and OMOPSO are similar for the OMedium and OLarge test oracles, they are higher for the Osmall test oracle. This indicates that covering the DOI regions for Osmall, the strictest test oracle definition, poses a greater challenge for both algorithms compared to OMedium and OLarge. Finally, for all three cases, the CID values of random search, NSGA-II and OMOPSO stabilize in less than 1000 evaluations.\nSimilarly, the results presented in Figure 6 demonstrate that for AVP, the CID values achieved by RS consistently outperform those obtained by NSGA-II across all test oracles and after at least 300 evaluations. Further, the average of CID values and the variations of CID values obtained by RS decrease at a higher rate compared to those obtained by NSGA-II and OMOPSO. However, OMOPSO can outperform NSGA-II for the less constrained oracle"}, {"title": "6.2.2 RQ2 Results", "content": "The results in Figures 5 and 6 show that the CID values obtained by NSGA-II-D are always better than those obtained by NSGA-II over time for both case studies and all test oracle definitions. However, compared to RS, NSGA-II-D achieves consistently worse CID values for all test oracle definitions of the MNIST case study and the OLarge test oracle of the AVP case study. For"}, {"title": "7 Threats to Validity", "content": "The most important threats concerning the validity of our experiments are related to the internal, external and construct validity.\nTo mitigate internal validity risks, which refer to confounding factors, we used identical search spaces and search configurations for all the algorithms - RS, NSGA-II, NSGA-II-D and OMOPSO- in each of our case studies: AVP and MNIST. For MNIST, the objective is to create test inputs that challenge classifiers while remaining valid and preserving the digit's original label. \"Valid\" implies that the digits are still recognizable by humans, and \"label"}, {"title": "8 Discussion", "content": "In this section, we offer further observations and discuss the applications and limitations of our proposed metric, CID. In addition, we compare NSGA-II, NSGA-II-D, OMOPSO and RS in terms of their effectiveness and efficiency in finding failure instances. This comparison aims to better position our study within the context of prior research and complements the study presented in Section 6, which is primarily focused on comparing these algorithms with respect to the coverage of failure-inducing regions, as opposed to their ability to identify individual failures.\nObservation: Why do the Pareto-based approaches NSGA-II, NSGA-II-D and OMOPSO yield worse CID values with considerably larger variations compared to RS? To understand why NSGA-II, NSGA-II-D and OMOPSO yield worse CID values with considerably larger variations compared to RS, we plot the failure-revealing solutions obtained by NSGA-II, NSGA-II-D, OMOPSO and RS, along with the reference sets computed for RQ1 to calculate CID values. Recall that reference sets approximate the complete set of failure-revealing test inputs within the search space. Specifically, Figure 7a shows the reference set for AVP with the test oracle OLarge computed based on grid sampling with 15,625 samples as part of our RQ1 experiments, and Figure 7b to Figure 7e, respectively, show the failing test inputs computed by one run of NSGA-II, RS, OMOPSO and NSGA-II-D for AVP with the OLarge test oracle. The Figures contain all failures that have been identified over all evaluations of the run of their respective algorithm.\nBy comparing the failure-revealing test inputs in Figures 7b and 7c with those from the reference set shown in Figure 7a, which locates 558 failures, one can see that although RS finds far fewer failure-revealing solutions than NSGA-II, 89 versus 429, RS covers the DOI in Plot (a) more effectively.\nThe failure-revealing tests identified by RS are more spread out and diverse, while those found by NSGA-II are grouped close together. Similarly, when comparing the failures found by NSGA-II and NSGA-II-D in Figure 7e, one can see that while the failures found by NSGA-II-D are more scattered compared to NSGA-II, they are still clustered together and are not as spread as the solutions found by random search.\nOMOPSO identifies only 57 failing tests, but the failures as shown in Figure 7d are more spread out compared to NSGA-II and NSGA-II-D. However, we can see that larger regions of the test input space are left uncovered compared to random search. Similarly, we plot in Figure 8a the reference set for the AVP case study with the Osmall test oracle computed using the same sampling method and resolution as the one in Figure 7a. As shown in this figure, only 62 failing tests are identified in the reference set for Osmall, which is sig-"}, {"title": "9 Conclusion and Future Work", "content": "In this paper, we studied the ability of Pareto-based Search-Based Software Testing (SBST) to cover failure-revealing test inputs. Based on a theoretical argumentation and an empirical evaluation we have shown that Pareto-driven testing cannot achieve a high coverage of failures.\nWe introduced a new metric, CID, to quantitatively evaluate the coverage of failure-inducing areas in a search domain and discussed its properties. In our empirical analysis using two case studies an industrial automated valet parking system and a handwritten digit classification system we demonstrate that the Pareto-based testing techniques NSGA-II and OMOPSO are less effective than random search in covering failure-revealing tests in 11 out of 12 alternative failure definitions across both studies, and in one case, all methods exhibited similar performance.\nIn addition, we demonstrated that augmenting NSGA-II with a diversified fitness function and a repopulation operator, adapted from a state-of-the-art testing approach does improve its performance in covering failure-revealing tests in four out of six alternative failure oracle definitions. However, in all comparisons, it does not perform better than random testing in terms of failure coverage.\nOur investigation highlights the limitations of Pareto-driven testing in covering failure-revealing tests, emphasizing the need for practitioners using SBST with Pareto-based testing approaches to be aware of these constraints. Our observation could further confirm that Pareto-based testing is good in identifying failures fast, while for the coverage of failures, other innovative search strategies are necessary. Our future work is to combine randomized testing with machine learning models to improve the failure space coverage. The application of the CID metric also confirms the importance of surrogate models and benchmarking systems to evaluate SBST algorithms, thereby avoiding expensive system executions."}, {"title": "A Proofs", "content": "We provide the proofs for the following reformulated statements from Section 5.1:\n1. Given an optimal reference set, the error of CID reduces linearly with the maximal distance of adjacent points in the reference set decreasing.\n2. For an optimal reference set obtained with a uniform sampling approach, the CID's error tends to zero with the increasing number of points sampled."}, {"title": "A.1 Proof Statement 1", "content": "The proofs are provided for q = 1 from the Definition of CID. The proof for q > 1 can be obtained with similar derivations. We start with defining two reference sets G and F, obtained by performing sampling within the domain D\\subset R^n on a coarse and fine grids respectively, which are not necessarily structured. Let be S the number of all separated regions of the DOI, then, since the reference sets are optimal, both reference sets witness every separated continuous region $r_s$, s = 1,..., S. From the Definition of CID, the following holds:\n$CID(A, Z) = \\frac{1}{|Z|} \\sum_{z \\in Z} d_z = \\frac{1}{|Z|} \\sum_{s=1}^S \\sum_{z \\in (Z \\cap r_s)} d_z$  (3)\ni.e.,\n$\\sum_{s=1}^S CID (A,Z_{ \\cap r_s})$   (4)\nComputing CID for the test set A, using the reference sets G and F, we obtain:\n$CID_G = \\frac{1}{|G|} \\sum_{i=1}^{|G|} d_G^i$ (5)\n$CID_F = \\frac{1}{|F|} \\sum_{i=1}^{|F|} d_F^i$  (6)\nFrom Equation 4, w.l.o.g., let's assume that S = 1, and let us denote r\u2081 as r. The difference between the CID values for the corresponding reference sets is then:\n$CID_F - CID_G = \\frac{1}{|F|} \\sum_{i=1}^{|F|} d_F^i  -  \\frac{1}{|G|} \\sum_{i=1}^{|G|} d_G^i$  (7)\nWe partition the connected region r using a Voronoi diagram built on G into |G| Voronoi cells $c_i, i = 1,..., |G|$. Then, we assign to each reference point $f_j \\in F$ a Voronoi cell $c_i$ if the reference point lies within the cell, and denote the reference point as $f_i$. If $f_j$ lies on a boundary of a cell, we assign it to any of the neighbouring cells. Let be R* the maximal distance of adjacent points in G. Each of the cells lies within a ball $B(g_i; R_i)$, and the radius of each of the balls is bounded by R*, i.e. $R_i \\leq R^*$. As $f_i$ belongs to the cell $c_i$ with the center $g_i$, the distance between them is bounded: $||\\delta_{ik}|| \\leq R^*$, where $\\delta_{ik}$ is the relative position between $f_i$, and $g_i$.\nBy applying the triangle equality of vector addition for the relative positions from the reference points $f_j = f_k$ and $g_i$ to the test point yields:\n$d = d^k + \\delta_ik$  (8)"}, {"title": "A.2 Proof Statement 2", "content": "Because the sampling strategy is uniform, the maximum distance between adjacent points in the reference set R* decreases with the increasing number of sampled points. Statement 2 follows from Equation 19."}]}