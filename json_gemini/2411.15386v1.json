{"title": "Inducing Human-like Biases in Moral Reasoning Language Models", "authors": ["Artem Karpov", "Seong Hah Cho", "Austin Meek", "Raymond Koopmanschap", "Lucy Farnik", "Bogdan-Ionut Cirstea"], "abstract": "In this work, we study the alignment (BrainScore) of large language models (LLMs) fine-tuned for moral reasoning on behavioral data and/or brain data of humans performing the same task. We also explore if fine-tuning several LLMs on the fMRI data of humans performing moral reasoning can improve the BrainScore. We fine-tune several LLMs (BERT, ROBERTa, DeBERTa) on moral reasoning behavioral data from the ETHICS benchmark [Hendrycks et al., 2020], on the moral reasoning fMRI data from Koster-Hale et al. [2013], or on both. We study both the accuracy on the ETHICS benchmark and the BrainScores between model activations and fMRI data. While larger models generally performed better on both metrics, BrainScores did not significantly improve after fine-tuning.", "sections": [{"title": "Introduction", "content": "Recently, multiple papers have shown surprising similarities between the internal representations in biological brains and those in artificial neural networks, in multiple domains and for multiple tasks; see e.g. Huh et al. [2024] for a review and for some potential theoretical explanations of why one might expect this, including with increasingly powerful machine learning models. However, to the best of our knowledge, no previous work has analyzed potential analogous representational similarities in the domain of moral reasoning, nor whether the degree of similarity might be increased by using human neural data (e.g. fMRI).\nIn this work, we undertake the first such measurement (BrainScore) of the similarity of the internal representations of biological brains (measured through fMRI) and large language models (LLMs), in the domain of moral reasoning (in a task which is also partially relevant to Theory of Mind). We also study whether fine-tuning the LLMs on a train set of the corresponding neural data helps with improving the BrainScore on a separate test set. While our attempts here proved unsuccessful, we think this is an important problem to study and that increasing said alignment might be important for the AI alignment problem [Christian, 2020]."}, {"title": "Related Works", "content": "There is a growing interest in brain-model alignment work, here we only provide a brief overview. For a more detailed survey, see Sucholutsky et al. [2023], especially section 4.3.2, and Schrimpf et al. [2020] for a systematic approach to collecting and scoring many models. Earlier work on fine-tuning transformers to predict fMRI data has found that adding MEG data also helps [Schwartz et al., 2019]. Other work [Aw and Toneva, 2023] focused on fine-tuning models on the much larger Booksum dataset [Kry\u015bci\u0144ski et al., 2022], which they found increased alignment. Dapello et al. [2022] used rhesus macaque neural data and showed improved alignment with human neural data and greater adversarial robustness.\nTo the best of our knowledge, we are the first to attempt increasing brain-model alignment on moral reasoning neuroimaging data."}, {"title": "Methodology", "content": null}, {"title": "Benchmark and Dataset", "content": "To quantitatively measure the moral reasoning performance of the fine-tuned LLMs, we used the common sense category of the ETHICS benchmark [Hendrycks et al., 2020], which consists of multiple choice questions rather than free form responses. To predict an answer, we use a linear transformation layer (a CLS head or just a head) attached to predictions (logits) for a classification token, \"[CLS]\", of a base model.\nWe used the fMRI dataset, 'Moral judgments of intentional and accidental moral violations across Harm and Purity domains', from Koster-Hale et al. [2013]. Human subjects were given a series of scenarios describing moral, immoral, and neutral actions across a wide variety of scenarios, and then answered on a 1-4 scale how moral or immoral each action was. Koster-Hale et al. [2013] was approved by an IRB and subjects were paid and gave written, informed consent."}, {"title": "Data Pre-processing and Analysis", "content": "We used a pre-processed version of the dataset from Thomas et al. [2023], specifically the version fit with the DiFuMo atlas [Dadi et al., 2020] with a dimensionality of 1,024 regions of interest (the maximum number of dimensions DiFuMo provides).\nBecause of the high granularity of our chosen atlas, we used NeuroSynth [Yarkoni et al., 2011], a tool for meta-analysis conducted over thousands of fMRI studies to isolate regions consistently activated during experiments, to map activations to specific themes. We conducted our analyses on four regions related to Theory of Mind, moral reasoning, language, and vision. We used vision as the control group as we expected scores there not to increase (see Appendix A). We visualized the relationship between the fMRI and LLM activations on the cortical surface (see Appendix A) using the Coefficient of Determination (CoD). The CoD scores were then negative log transformed and the weighted average of the parcel scores were plotted at each vertex, since the DiFuMo atlas is probabilistic with overlapping boundaries."}, {"title": "Models and Fine-tuning Procedure", "content": "We focused on encoder models (BERT-based) due to computational constraints and since this was a classification task. Additionally, encoder models originally showed better results on the ETHICS dataset [Hendrycks et al., 2020]. Overall we used four models, BERT-base-cased and BERT-large cased (108 and 333 million parameters) [Devlin et al., 2019], ROBERTa-large (355 million parameters) [Liu et al., 2019], and DeBERTa-v2-xlarge (864 million parameters) [He et al., 2021].\nFor fine-tuning, we used the HuggingFace library [Wolf et al., 2019] to train additional heads of dimensionality 1,024 (to match the DiFuMo atlas) on top of the classification token, \"[CLS]\". We also train heads to predict the ETHICS benchmark [Hendrycks et al., 2020]. We report fine-tuning on ETHICS only and with the addition of fMRI data in Table 1. In total, we ran 450 fine-tuning runs, totaling 292 hours of training for 1,082 different models (not all shown in the results section)."}, {"title": "Brain Scores", "content": "Our 'brain-score' metric is based on similar metrics found within the broader NeuroAI literature, such as in Schrimpf et al. [2020], Aw and Toneva [2023], inter alia. We use the Pearson's correlation coefficient (PCC) to measure the correlation between predicted brain activity and actual brain activity. For some moral scenario given to a subject, we sample the fMRI data at several time points, taking the hemodynamic lag into account. This data has been fit to the DiFuMo atlas, resulting in 1,024 ROIs at the time points sampled. We do this over a collection of similar examples, and then fit a regression model to predict this brain activity. The PCC between the predicted response and the actual held out brain activity gives us the brain-score metric. We can also do this on a layer-by-layer basis and aggregrate over all layers to provide a single brain-score for the whole model, which we provide in Table 2."}, {"title": "Results", "content": "Our results indicate that improving brain-model alignment on moral reasoning by fine-tuning on relevant fMRI data does not consistently improve accuracy on ETHICS [Hendrycks et al., 2020]. While our fine-tuning procedures do improve accuracy on the Commonsense split of ETHICS (bolded values in Table 1 are higher than those reported by Hendrycks et al. [2020]), we could not improve accuracy by fine-tuning on the fMRI data only or on a combination of fMRI and ETHICS, compared to fine-tuning purely on ETHICS.\nTo thoroughly test these results, we also used a variety of sampling methods to pull from the fMRI data, as shown in Table 3 and Figure 2 in Appendix B. AVG indicates an average of all time points, LAST indicates the time point at the hemodynamic lag before the last time point, MIDDLE indicates the middle point, and SENTENCES indicates four points in a scenario, which match the end of the four sentences read by the subject. We find that LAST tends to produce the best accuracy.\nWe generally find that larger models are more performant overall, a finding also reported by Hendrycks et al. [2020]. This additionally holds with the brain-score metric, which measures brain-model alignment. However, we were also unable to significantly improve our brain-score metric beyond the pre-trained models, as shown in Table 2. This finding is also consistent in layer-wise scores across each of the three models; see Appendix A for further brain-score details (including region specific information, such as Theory of Mind ROIs) and cortical mappings."}, {"title": "Conclusion and Future Work", "content": "While we were unable to significantly increase brain alignment on moral reasoning through fine-tuning methods, we do believe that our results can be of use for downstream work. Firstly, we believe that our work is ample evidence for the importance of gathering more data on moral reasoning and for more niche tasks in general if future researchers want to increase brain-model alignment"}]}