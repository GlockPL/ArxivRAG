{"title": "Evaluating Morphological Compositional Generalization in Large Language Models", "authors": ["Mete Ismayilzada", "Defne Circi*", "Jonne S\u00e4lev\u00e4*", "Hale Sirin", "Abdullatif K\u00f6ksal", "Bhuwan Dhingra", "Antoine Bosselut", "Lonneke van der Plas", "Duygu Ataman"], "abstract": "Large language models (LLMs) have demonstrated significant progress in various natural language generation and understanding tasks. However, their linguistic generalization capabilities remain questionable, raising doubts about whether these models learn language similarly to humans. While humans exhibit compositional generalization and linguistic creativity in language use, the extent to which LLMs replicate these abilities, particularly in morphology, is under-explored. In this work, we systematically investigate the morphological generalization abilities of LLMs through the lens of compositionality. We define morphemes as compositional primitives and design a novel suite of generative and discriminative tasks to assess morphological productivity and systematicity. Focusing on agglutinative languages such as Turkish and Finnish, we evaluate several state-of-the-art instruction-finetuned multilingual models, including GPT-4 and Gemini. Our analysis shows that LLMs struggle with morphological compositional generalization particularly when applied to novel word roots, with performance declining sharply as morphological complexity increases. While models can identify individual morphological combinations better than chance, their performance lacks systematicity, leading to significant accuracy gaps compared to humans.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have recently achieved remarkable advances in the broad domain of natural language generation and understanding tasks (Gemini, 2024; Zhao et al., 2023; Bubeck et al., 2023; Wei et al., 2022; Brown et al., 2020). However, these models have also been shown to lack strong linguistic generalization capabilities (Weissweiler et al., 2023; McCoy et al., 2023; Goldman et al., 2022; Wilson et al., 2023; Linzen,In this work, we address this gap by systemat-"}, {"title": "2 Related Work", "content": "2.1 Compositional Generalization\nCompositional generalization is the capacity to understand and produce novel compositions of seen primitives and is typically characterized by systematicity and productivity (Fodor and Pylyshyn, 1988; Keysers et al., 2019). Systematicity refers to the ability to understand different combinations that are made up of the same known components such as John loves Mary and Mary loves John. Productivity, on the other hand, is the ability to produce potentially infinite novel combinations of a finite number of known building blocks such as using conjunctions to construct sentences Mary knows that John loves Mary and John heard that Mary knows that John loves Mary. Past work has developed several benchmarks to measure compositional generalization abilities of neural models both in fine-tuning and in-context learning settings and has shown this task to be highly challenging (Yang et al., 2024; Lake and Baroni, 2018; Keysers et al., 2019; Kim and Linzen, 2020; An et al., 2023; Dziri et al., 2023). These benchmarks have mainly focused on synthetic sequence matching, semantic parsing, question-answering and problem-solving tasks. Our work however, investigates compositional generalization in the context of morphology.\n\n2.2 Morphological Generalization\nMorphological generalization is the ability to understand words based on their constituent parts known as morphemes and combine them to derive new words (Wysocki and Jenkins, 1987). Morphemes are the smallest meaningful units of language that typically correspond to word roots and affixes (i.e. prefixes, infixes and suffixes). Composing these units to construct new words can be done through inflection and derivation tasks in morphology where derivation changes the syntactic category of the words and inflection does not. These tasks have gained considerable attention as part of the SIGMORPHON's shared tasks (Cotterell et al., 2016, 2018; Vylomova et al., 2020; Kodner and Khalifa, 2022; Goldman et al., 2023) and efforts to create a universal morphology (McCarthy et al., 2020). While transformer-based models have been"}, {"title": "3 Methodology", "content": "3.1 Tasks\nWe design two novel and simple compositional tasks to test morphological abilities of models. First, a morphological productivity task which we define as a generative task where the model is given a word root, a list of affixes (not necessarily in the correct order) and is asked to derive a meaningful word by composing the root with the affixes in the correct order. Second, a morphological systematicity task which we define as a binary discriminative task where the model is again given a word root, a list of affixes and a word derived from the root using the given affixes (not necessarily a meaningful word) and is asked to determine the grammatical validity of the derived word. Additionally, to measure the morphological generalization capabilities of LLMs, we take inspiration from Berko's Wug test (Berko Gleason, 1958) that is typically used to probe the inflectional and derivational morphological knowledge of children and design out-of-distribution (OOD) versions of our tasks using nonce word roots. More specifically, for each in-distribution (ID) word root in our test suite, we automatically generate a nonce word (i.e. word that does not exist in the given"}, {"title": "3.2 Data", "content": "We focus our study on two highly agglutinative languages, Turkish and Finnish, and prepare test suites specific for our tasks in these languages. We particularly choose these languages because they are characterized by a large number of morphemes and hence require a high degree of compositional generalization ability.\nTurkish Turkic languages are well-known to be highly agglutinative where the word is composed of several morphemes in addition to a root. We select Turkish as a representative of this language family in our study. To prepare our test suite we use the Bilkent Turkish Writings Dataset as our base corpus which contains 6, 844 creative writings of Turkish 101 and Turkish 102 courses between 2014-2018 and hence, is full of morphologically complex words.\nFinnish We first collect a ~1,000,000 sentence subsample of the Finnish mC4 corpus (Xue et al., 2021). We then extract unique words from the text and morphologically segment them using omorfi (Pirinen, 2015) and UralicNLP (H\u00e4m\u00e4l\u00e4inen, 2019). After excluding words that analyzers did not cover, we manually annotate the segmentations to identify prefixes, lemmas, and affixes among the segments."}, {"title": "4 Experiments", "content": "Setup We treat the productivity task as an open-ended task in which the model is asked to derive a word from the given root and affixes and the systematicity task as a binary classification task in which the model is asked to determine whether the given derivation is grammatically correct. For the systematicity task, we generate negative examples by producing all the combinations of morphemes attached to the same root and choosing the top four compositions (two for morpheme lengths of 1 and 2) that are closest to the original valid combination measured by the Levenshtein distance. We do this to ensure our incorrect combinations are challenging enough for the model as they will be deceptively close to a plausible derivation. We also experiment with other negative example selection strategies such as random selection and a heuristic selection based on the linguistic characteristics of the given language.\nModels We evaluate several state-of-the-art multilingual instruction-finetuned LLMs, namely, two open-weights models, Aya-23 (Aryabumi et al.,"}, {"title": "Evaluation Metrics", "content": "For the productivity task, we use Exact Match accuracy against the correct derivations. For the systematicity task, we report an average of Macro-F1 scores for each sample and a Coherence score that measures whether the model correctly and consistently identifies the validity (or invalidity) of all derivations for a given set of morphemes. We employ this stringent metric to test the robustness of model performance similar to (Storks and Chai, 2021).\nHuman Evaluation We evaluate human performance on both tasks using two native speakers per language, who annotate 70 and 60 samples from the Turkish and Finnish test suites, respectively. To ensure our evaluation sample is a representative sample of the entire test suite, we randomly select 10 examples per morpheme length for each test distribution. Human annotators follow the same task instructions used for model prompts and were shown five examples. We report almost perfect or substantial inter-annotator agreement measured by Cohen's kappa score (Cohen, 1960) for both tasks, languages, and test distributions. Finally, for each task metric, we report"}, {"title": "5 Analysis", "content": "5.1 Effect of Morphological Complexity\nRecent works have shown that morphological complexity plays a crucial role in the morphological generalization abilities of LLMs (Anh et al., 2024). Morphological complexity is typically categorized into integrative (I-complexity) which refers to the predictability of inflected form and enumerative (E-complexity) complexity which refers to the number of cases and inflectional paradigms in language grammar (Ackerman and Malouf, 2013). While both languages we study are morphologically complex, our test suites include inflectional and derivational forms of varying length in the number of morphemes (1-7 in Turkish and 1-6 in Finnish). This allows us to study the effect of within-language E-complexity on the performance of our models.\n\n5.2 Effect of Context\nWhile our core tasks are somewhat synthetic in nature, we do also experiment with more realistic versions where we provide the model a sentence as an additional context. Specifically, we frame them as sentence completion tasks where a sentence with a blank is provided and the model is asked to fill in the blank with the correct word derived from the given word root and affixes (productivity task) or determine if the given derivation is the correct option for the blank (systematicity task).\n\n5.3 Effect of Tokenization\nPast work has shown that suboptimal tokenizers, especially byte-pair encoding (Sennrich et al., 2016) used in GPT-4 have generally a negative effect on the morphological abilities of language models (Meyer and Buys, 2023; Bostrom and Durrett, 2020; Hofmann et al., 2021). Whether the low performance of the model on the productivity task can be attributed to the suboptimal nature of the tokenization is of interest in particular because our tasks rely on the morphologically segmented morphemes while the model utilizes byte-level tokens that are mostly English. To measure the effect of the tokenization, we ran a version of the productivity task where the morphemes provided to the model are obtained by segmenting the final derivation based on the model's own tokenizer instead of the morphologically-aligned units.\n\n5.4 Effect of Morpheme Order\nSince our goal is to study the ability of LLMs to combine the morphological units in the correct order, in all of our experiments we shuffle the order of the units in the prompts. However, given that models are sensitive to small prompt changes (Pezeshkpour and Hruschka, 2023; Zhu et al., 2024; Wang et al., 2023; Zhao et al., 2021), we also analyze the effect of changing the morpheme order on the performance of the model.\n5.5 Effect of Negative Sample Selection\nIn our systematicity task, we generate negative samples (i.e. derived combinations that are not grammatically correct) by permuting the order of morphemes attached to the root. While the number of permutations is manageable for 2 or 3 morphemes (e.g., 2!=2, 3!=6), it grows rapidly with more morphemes (e.g., 6!=720). Evaluating all permutations would be ideal for robust systematicity testing, but this is infeasible due to high computational costs.\n5.6 Error Analysis\nIn order to understand the limitations of language models on our tasks, we manually analyze 30 Turkish word derivations for each morpheme combination length (1-7) and for both productivity ID and OOD test sets resulting in a total of 178 and 185 derivations from GPT-4 that are incorrect. We annotate each generation on three criteria: 1) whether the generation is an invalid word (i.e. grammatically incorrect word) 2) whether the generation is unfaithful (i.e. generation does not follow the productivity task constraints) and 3) whether the generation includes any hallucinations (i.e. whether the generation has extra morphemes not mentioned in the task prompt)."}, {"title": "6 Conclusion", "content": "In this paper, we proposed a novel experimental paradigm to test morphological generalization abilities of large language models through compositionality. Our tasks target measuring morphological productivity and systematicity in a given language. We applied these tasks on the morphologically complex languages of Turkish and Finnish and evaluated morphological compositional generalization abilities of several state-of-the-art large language models. Our experimental results and analysis re-"}, {"title": "Limitations", "content": "While our novel tasks are language, dataset, and model-independent, our study only focused on two agglutinative languages and a few large language models. Therefore, the applicability of our findings in other languages and models should be further studied. We also mainly focused on the grammatical validity of the words, whereas it would be equally interesting to study the capacity of LLMs to produce and understand novel semantically and pragmatically valid derivations. While we have also optimized our prompts to be as simple and maximally instructive and tested in multiple languages and in chain-of-thought setting, whether a different set of prompts would produce the same results is not clear. Finally, we only evaluate models using greedy decoding due to the deterministic nature of our tasks, however, the effect of different decoding strategies need to be explored."}, {"title": "A Additional Analysis", "content": "A.1 Effect of Instruction Language\nSince most LLMs are pre-trained on significantly more instruction data in English than other languages, we base most of our results on experiments where we use English as the prompt instruction language. However, as our data is in a different language, this results in a code-switched language which has been shown to be a challenge for large language models (Zhang et al., 2023). To measure the effect of the instruction language on the morphological generalization tasks, we run our experiments with Turkish and Finnish as the instruction language and report results for both tasks in Figure 9. We mostly observe a drop or no change in performance when the instruction language is other than English.\nA.2 Effect of Chain-of-thought Reasoning\nChain-of-thought prompting has been shown to be effective in eliciting strong reasoning capabilities from LLMs (Wei et al., 2023). In order to measure the effect of this reasoning technique on LLMs\u2019 performance on our tasks, we evaluate GPT-4 (the best performing model) on both productivity and systematicity tasks in zero-shot and 5-shot chain-of-thought settings. We report the results of these experiments compared with the 5-shot standard prompting in Figure 10. We observe that while 5-shot chain-of-thought performance is better than the zero-shot chain-of-thought, it is slightly worse than or similar to the 5-shot standard prompting.\nA.3 Further details on the effect of morphological complexity\nIn Figure 4, we observe a surprisingly low performance (\u2248 40% drop from ID performance) on the 1-morpheme OOD words, but we attribute this behaviour to the varying number of negative options available for each morpheme length and possible presence of shortcuts in larger morpheme words. We should note that we have different number of total options to discriminate for a given sample depending on the number of morphemes (for 1 and 2 morphemes, we have 2 options, for 3-7, we have 5 options). Hence, a single mistake is penalized more in the former case than in the latter.\nA.4 Chain-of-thought Error Analysis\nWe randomly sample 10 examples from the 5-shot chain-of-thought experiments on the Turkish evaluation data (per morpheme length and test distribution) where GPT-4 made an error and manually analyze its answers across both tasks. Our analysis reveals the following primary types of errors:\n1. Sequential Dependency Errors\nOne common error we observe in the productivity task is due to the sequential processing"}, {"title": "B Nonce word generation", "content": "Turkish To automatically generate novel nonce words in Turkish (out-of-distribution words that do not exist) that are inflected the same as the original word roots, we leverage the deterministic morphophonological features of Turkish. In particular, vowel harmony and consonant assimilation in Turkish completely determines which surface forms of the meta level morphemes would apply. Furthermore, these features depend only on the last vowel and the consonant. Hence, for a given word root in Turkish, we keep its last vowel and the consonant"}, {"title": "C Model Evaluation", "content": "We evaluate the following state-of-the-art multilingual instruction-finetuned LLMs:\n\u2022 Aya-23 (Aryabumi et al., 2024) a powerful open-weights multilingual LLM serving 23 languages including Turkish. We evaluate"}, {"title": "D Data", "content": "Turkish Since the morphological analyzer we use to process the Turkish dataset (Ozturel et al., 2019) is based on a finite state machine relying on purely syntactic rules, it produces several alternative decompositions for some words. Hence, we further apply some language-specific heuristics to automatically filter out invalid decompositions. This preprocessing still leaves some words with multiple decompositions that can only be validated using semantics, hence, as a last step, we manually verify and determine the final segmentation of a word."}, {"title": "E Heuristic Negative Sample Selection For Turkish", "content": "Turkish phonology does not allow two vowels to occur together and typically employs \u201cbuffer\u201d letters such as \u201cy\u201d, \u201cs\u201d in between these vowels, however, blindly permuting the order of Turkish morphemes inevitably results in negative samples where two vowels may occur next to each other. We hypothesized that models might easily identify these options by exploiting the \u201cno-two-vowel\u201d shortcut and without considering the semantic order of morphemes."}, {"title": "F Prompts", "content": "This section lists the instruction prompts for all tasks and language templates. We present examples in one-shot setting, templates for different shots are the same with more examples. For the English language template, we provide examples in Turkish, the templates are the same for Finnish with examples in Finnish."}]}