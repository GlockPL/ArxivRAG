{"title": "VisPath: Automated Visualization Code Synthesis via Multi-Path Reasoning and Feedback-Driven Optimization", "authors": ["Wonduk Seo", "Seungyong Lee", "Daye Kang", "Zonghao Yuan", "Seunghyun Lee"], "abstract": "Unprecedented breakthroughs in Large Language Models (LLMs) has amplified its penetration into application of automated visualization code generation. Few-shot prompting and query expansion techniques have notably enhanced data visualization performance, however, still fail to overcome ambiguity and complexity of natural language queries - imposing an inherent burden for manual human intervention. To mitigate such limitations, we propose a holistic framework VisPath: A Multi-Path Reasoning and Feedback-Driven Optimization Framework for Visualization Code Generation, which systematically enhances code quality through structured reasoning and refinement. VisPath is a multi-stage framework, specially designed to handle underspecified queries. To generate a robust final visualization code, it first utilizes initial query to generate diverse reformulated queries via Chain-of-Thought (CoT) prompting, each representing a distinct reasoning path. Refined queries are used to produce candidate visualization scripts, consequently executed to generate multiple images. Comprehensively assessing correctness and quality of outputs, VisPath generates feedback for each image, which are then fed to aggregation module to generate optimal result. Extensive experiments on benchmarks including MatPlotBench and the Qwen-Agent Code Interpreter Benchmark show that VisPath significantly outperforms state-of-the-art (SOTA) methods, increased up to average 17%, offering a more reliable solution for AI-driven visualization code generation.", "sections": [{"title": "Introduction", "content": "Data visualization has long been an essential tool in data analysis and scientific research, enabling users to uncover patterns and relationships in complex datasets (Vondrick et al., 2013; Demiralp et al., 2017; Unwin, 2020; Li et al., 2024a). Traditionally, creating visualizations requires manually writing code using libraries such as Matplotlib, Seaborn, or D3.js (Barrett et al., 2005; Bisong and Bisong, 2019; Zhu, 2013). This approach demands programming expertise and significant effort to craft effective visual representations, which can be a barrier for many users (Bresciani and Eppler, 2015; Saket et al., 2018; Sharif et al., 2024). As datasets continue to grow in size and complexity, researchers have explored ways to automate visualization generation, aiming to make the process more efficient and accessible (Wang et al., 2015; Dibia and Demiralp, 2019; Qian et al., 2021).\nIn response to this challenge, Large Language Models (LLMs) have emerged as a promising solution for simplifying visualization creation (Wang et al., 2023a; Han et al., 2023; Xie et al., 2024). By translating natural language instructions into executable code, LLM-based systems eliminate the need for extensive programming knowledge, allowing users to generate visualizations more intuitively (Xiao et al., 2023; Ge et al., 2023; Zhang et al., 2024b). Recent visualization methods such as ChartGPT (Tian et al., 2024) and NL4DV (Sah et al., 2024) demonstrate the potential of LLMs to provide interactive, conversational interfaces for visualization. These systems enable users to create complex charts with minimal effort, bridging the gap between technical expertise and effective data exploration (Dibia, 2023; Kim et al., 2024).\nMore recently, LLM-based visualization frameworks such as Chat2VIS (Maddigan and Susnjak, 2023) and MatPlotAgent (Yang et al., 2024) have been introduced to improve automated visualization code generation. Specifically, Chat2VIS follows a prefix-based approach, guiding LLMs to generate visualization code consistently; and MatPlotAgent expands the query before code generation. However, these methods face several limitations: \u2460 both generate code in a single-path manner, limiting exploration of alternative solutions and unable to fix-out when caught in misleading bugs; \u2461 both rely on predefined structures or examples which restrict adaptability to ambiguous or unconventional user queries. \u2462 both approaches encapsulate limitation in their inability to aggregate and synthesize multi-dimensional feedback. Without a mechanism to retrieve outputs that reflect diverse possibilities, they struggle to capture intricate details, ultimately limiting the precision and adaptability of the generated visualizations.\nTo address these limitations, we introduce VisPath: A Branch Exploration Framework for Visualization Code Synthesis via Multi-Path Reasoning and Feedback-Driven Optimization, a transformative approach that redefines how visualization code is generated. Traditional single-pass methods may seem efficient, but they often fall short of delivering the depth and precision users truly need. They generate code quickly, but struggle to capture the intricate details that make a visualization not just functional but meaningful. VisPath challenges this limitation by incorporating Multi-Path Reasoning and Feedback-Driven Optimization, systematically exploring multiple interpretative pathways to construct a more accurate, context-aware, and fully executable visualization.\nRather than simply translating user input into code, VisPath ensures that every critical aspect-both explicitly stated and implicitly necessary-is carefully considered, creating a visualization that is not only correct but insightful. At its core, it generates multiple reasoning paths that analyze the user's intent from different perspectives, producing structured blueprints that are then transformed into visualization scripts through Chain-of-Thought (CoT) prompting. These multiple candidates are evaluated using a Vision-Language Model (VLM) to assess accuracy, clarity, and alignment with the intended message. The results are then refined through an Aggregation Module, optimizing the final output for both reliability and impact. By shifting from a single-pass, one-size-fits-all approach to a dynamic, multi-layered process, VisPath sets a new standard for visualization generation-one that is not just about creating code, but about ensuring data is represented in its most clear, meaningful, and compelling form.\nExtensive experiments on benchmark datasets, including MatPlotBench (Yang et al., 2024) and Qwen-Agent Code Interpreter Benchmark, demonstrate VisPath's superiority over the state-of-the-art (SOTA) visualization generation methods. By systematically generating and evaluating multiple reasoning paths and leveraging iterative feedback aggregation, VisPath significantly enhances the accuracy, robustness against underspecified queries, and adaptability to diverse user intents. Our investigations demonstrate its ability to capture nuanced user intents, improve execution reliability, and minimize errors, making visualization code generation more accessible and effective for domains such as business intelligence, scientific research, and automated reporting."}, {"title": "Related Work", "content": "Numerous methods have been applied for Text-to-Visualization (Text2Vis) generation, which has significantly evolved over the years, adapting to new paradigms in data visualization and natural language processing (Dibia and Demiralp, 2019; Wu et al., 2022; Chen et al., 2022a,b; Rashid et al., 2022; Zhang et al., 2024a). Early approaches such as Voyager (Wongsuphasawat et al., 2015) and Eviza (Setlur et al., 2016) largely relied on rule-based systems, which mapped textual commands to predefined chart templates or specifications through handcrafted heuristics (de Ara\u00fajo Lima and Diniz Junqueira Barbosa, 2020). While these methods demonstrated the feasibility of automatically con-"}, {"title": "Methodology", "content": "We introduce VisPath, a framework for robust visualization code generation that leverages diverse reasoning and visual feedback. VisPath is built on three core components: (1) Multi-Path Query Expansion, which generates multiple reasoning paths informed by the dataset description; (2) Code Generation from Expanded Queries, which synthesizes candidate visualization scripts via Chain-of-Thought (CoT) prompting while grounding them in the actual data context; and (3) Feedback-Driven Code Optimization, where a Vision-Language Model (VLM) evaluates and refines the outputs to ensure generation robust visualization code. An overview of this process is shown in Figure 2."}, {"title": "Multi-Path Generation", "content": "One of the biggest pitfalls in visualization code generation is rigid interpretation, a single query can have multiple valid visual representations depending on its dataset structure. VisPath mitigates this by generating multiple extended queries within a single interaction. Given a user query Q and a corresponding dataset description D, a Multi-Path Agent is employed to expand the query into K distinct reasoning pathways:\n\\{R_1, R_2, ..., R_K\\} = f_{mpa}(Q, D), \\qquad(1)\nwhere $f_{mpa}$ denotes the function of the Multi-Path Agent implemented via an LLM. The dataset description D plays a crucial core in shaping these reasoning paths by providing contextual information about variable types, inherent relationships, and the suitability of different chart types for a more grounded interpretation of the query. Each $R_i$ serves as a detailed logical blueprint outlining one possible approach to fulfill the visualization request. This design ensures that our framework effectively considers a broad range of potential interpretations, thereby increasing the quality of reasoning and the likelihood of capturing the true user intent even when queries are ambiguous or underspecified."}, {"title": "Code Generation from Reasoning Paths", "content": "Once diverse reasoning paths are established, the next stage involves translating each path into executable Python scripts. For every reasoning path $R_i$ generated in Equation (1), a dedicated Code Generation LLM produces the corresponding visualization code using chain-of-thought (CoT) prompting:\n$C_i = f_{code}(D, R_i), \\qquad(2)$\nwhere $f_{code}$ represents the code generation function. Unlike naive code generation approaches, here, the dataset description D is explicitly provided to ground the generated code in the actual"}, {"title": "Feedback-Driven Code Optimization", "content": "Most code generation frameworks focus merely on syntactically correct scripts, but our framework goes further. As final stage, VisPath synthesizes the most robust and accurate visualization code by leveraging both the executability information and structured visual feedback. A Vision-Language Model (VLM) is employed to analyze each candidate by evaluating the initial query Q, the generated code $C_i$, and the routed output $Z_i$. This evaluation is formalized as:\n$F_i = f_{feedback}(Q, C_i, Z_i), \\qquad(6)$\nwhere $F_i$ provides structured feedback on key aspects such as chart layout, the alignment between the intended request and the rendered visualization (or error context), and visual readability (including potential improvements). To capture the complete quality signal from each candidate, we pair the feedback with its corresponding generated code:\n$S_i = (C_i, F_i), i = 1, 2, . . ., K. \\qquad(7)$\nLeveraging the collective code-feedback pairs along with the original query Q and the dataset description D, an Integration Module synthesizes the final, refined visualization code:\n$C^* = f_{integrate} (Q, D, \\{S_i\\}_{i=1}^K), \\qquad(8)$\nwhere $C^*$ represents the optimized visualization code and $f_{integrate}$ denotes the function that aggregates the strengths of each candidate code alongside its corresponding feedback. This formulation ensures that the final code is not only constructed based on the insights extracted from the candidate outputs but is also meticulously aligned with the original user query and the provided dataset description. The process harnesses the reasoning capabilities of LLM to systematically evaluate the strengths and pinpoint potential weaknesses across all candidate solutions, including those that may initially be non-executable. By synthesizing the most optimal elements from each candidate, the framework generates a final code that best captures the user's intent while maintaining high standards of reliability, robustness, and execution quality. A step-by-step breakdown of this process is detailed in Algorithm 1 below:"}, {"title": "Experiments", "content": "In this section, we detail our experimental configuration, including experimental datasets, model specifications, and baseline methods for evaluating the performance of the proposed VisPath framework in generating visualization code from natural language queries."}, {"title": "Experimental Datasets", "content": "We evaluate our approach on two Text-to-Visualization benchmarks: MatPlotBench (Yang et al., 2024) and the Qwen-Agent Code Interpreter Benchmark. MatPlotBench comprises 100 items with ground truth images; we focus on its simple instruction subset for nuanced queries. In contrast, the Qwen-Agent Code Interpreter Benchmark includes 295 records: 163 related to visualization, and evaluates Python code interpreters on tasks such as mathematical problem solving, data visualization, and file handling based on Code Executability and Code Correctness."}, {"title": "Models Used", "content": "Large Language Models (LLMs): For the code inference stage, we experiment with GPT-4o mini (Achiam et al., 2023) and Gemini 2.0 Flash (Team et al., 2024) to generate candidate visualization code from the reasoning paths. Both models are configured with a temperature of 0.2 to ensure precise and focused outputs, in line with recommendations from previous work (Yang et al., 2024). To evaluate the generated code quality and guide the subsequent optimization process, we utilize GPT-4o (Achiam et al., 2023) and Gemini 2.0 Flash (Team et al., 2024) as our visualization feedback model, which provides high-quality reference assessments.\nVision Language Models (VLMs): In order to assess the visual quality and correctness of the rendered plots, we incorporate vision evaluation models into our framework. Specifically, GPT-4o (Achiam et al., 2023) is employed for detailed plot evaluation in all evaluation tasks. This setup ensures the thorough evaluation of both the syntactic correctness of the code and the aesthetic quality of the resulting visualizations."}, {"title": "Evaluation Metrics", "content": "In our experiments, we utilized evaluation metrics introduced by previous work to ensure consistency and comparability. MatPlotBench assesses graph generation models using two key metrics: Plot Score, which measures similarity to the Ground Truth (0-100), and Executable Score, which represents the percentage of error-free code executions. Qwen-Agent Code Interpreter benchmark evaluates visualization models based on Visualization-Hard and Visualization-Easy, measuring how well generated images align with queries of different difficulty levels. Compared to MatPlotBench, Qwen-Agent Code Interpreter benchmark assesses image alignment via a code correctness metric. Previous studies show GPT-based VLM evaluations align with human assessments, hence VLM was used for evaluation."}, {"title": "Baseline Methods", "content": "We compare VisPath against competitive baselines. Specifically, (1) Zero-Shot directly generates visualization code without intermediate reasoning, (2) CoT Prompting uses chain-of-thought prompting to articulate its reasoning, while (3) Chat2VIS (Maddigan and Susnjak, 2023) employs guiding prefixes to mitigate ambiguity, and (4) MatPlotAgent (Yang et al., 2024) first expands the query and then refines the code via a self-debugging loop with feedback. For a fair comparison, MatPlotAgent is limited to three iterations, and uses critique-based debugging loop as well, and VisPath generates three reasoning paths with corresponding visual feedback to refine the final output."}, {"title": "Experimental Analysis", "content": "In this section, we present a detailed evaluation of our proposed VisPath framework, summarizing the performance results against four baselines: (1) Zero-Shot, (2) Chain-of-Thought (CoT) Prompting, (3) Chat2VIS, and (4) MatPlotAgent.\nFrom Table 1, we find that the Zero-Shot method, which directly generates visualization code from the user query without intermediate reasoning, suffers from ambiguous interpretations and error-prone outputs. These issues are particularly pronounced when the input queries are underspecified, contain implicit assumptions, or require complex reasoning steps. CoT Prompting, on the other hand, mitigates some of these challenges by employing chain-of-thought (CoT) reasoning, which helps the model articulate its decision-making process step by step. While this structured reasoning approach improves interpretability and correctness, its reliance on a single reasoning trajectory limits its capacity to explore alternative solutions.\nMeanwhile, Chat2VIS builds upon CoT Prompting by incorporating guided prefixes to better structure user queries and disambiguate input intent. This enhancement leads to more coherent code generation and reduces errors stemming from unclear specifications. However, its effectiveness still depends on the predefined guiding templates, which may not fully adapt to highly variable or novel query structures. MatPlotAgent further refines this approach through query expansion and an iterative self-debugging loop that enhances code robustness. However, its reliance on an iterative correction mechanism comes at the cost of computational efficiency and does not fully leverage diverse reasoning strategies that could lead to more creative and contextually accurate solutions.\nIn contrast, our novel framework, VisPath, overcomes these challenges by dynamically generating multiple reasoning paths. By exploring diverse interpretations of user intent simultaneously, VisPath significantly enhances the robustness of the generated visualization code. Furthermore, it integrates structured feedback from Vision-Language Models (VLMs) to refine its outputs, ensuring both higher execution accuracy and improved aesthetic quality. This feedback-driven optimization resolves ambiguities more effectively and selects the most appropriate solution from a wide range of possibilities. Evaluations conducted on benchmark datasets such as MatPlotBench and Qwen-Agent Code Interpreter benchmark demonstrate that VisPath consistently outperforms the baseline methods by average 17%. VisPath exhibits a superior ability to handle complex and ambiguous visualization requests by synthesizing multiple reasoning trajectories and incorporating structured feedback, making it a highly effective tool for automated visualization generation."}, {"title": "Ablation Study", "content": "This section further explores the impact of varying the number of reasoning paths on the performance of visualization code generation. We conduct a series of ablation studies to demonstrate the robustness of our VisPath framework under alternative settings. Specifically, we investigate (i) the effect of varying the number of generated reasoning paths and (ii) the impact of a simplified integration strategy for synthesizing the final visualization code."}, {"title": "Varying the Number of Reasoning Paths", "content": "To further assess the contribution of reasoning path diversity, we conducted ablation experiments varying the number of generated paths K \u2208 {2,3,4} to evaluate their effects on execution accuracy, visualization quality and interpretability. Evaluation has been conducted on both the MatPlotBench and Qwen-Agent Code Interpreter benchmark datasets. The study reveals an interesting pattern observed when the number of reasoning path is reduced. As illustrated in Table 2 and Table 3, when K = 2, the system exhibited limited diversity in which occasionally results in missing nuanced interpretations of complex user queries.\nConversely, increasing K to 4 introduces extra paths which can sometimes add noise as less relevant interpretations are considered. Specifically, we observed cases where redundant or overly complex visualization components were generated, hence making the final output harder to be interpreted. While the additional reasoning paths increased the overall exploration space, they also required more extensive filtering and selection, leading to suboptimal execution efficiency.\nNotably, K = 3 consistently achieves the best balance by providing sufficient diversity without introducing excessive noise. With too few reasoning paths, some nuanced aspects of ambiguous queries are lost, while too many paths configuration result in unnecessary exploration of low relevancy solutions. The number of reasoning path diversity set as K = 3 optimally provides sufficient diversity without overcomplicating the reasoning process."}, {"title": "Robustness with a Simple Integration Strategy", "content": "To further validate the robustness of VisPath, we also evaluate an alternative integration strategy that simplicies the aggregation of multiple reasoning paths. In contrast to our full feedback-driven iterative approach which refines visualization code through multiple rounds of vision-language feedback, this streamlined method directly aggregates candidate outputs from different reasoning trajectories without intermediate corrections.\nSpecifically, in this alternative strategy, three candidate codes, each derived from different reasoning paths, are directly combined to generate the final visualization output. This methodology reduces computational overhead and processing time while maintaining a degree of interpretability and correctness. Through examining the impact of this approach, we gain insight into the extent to which VisPath's core strength stems from its multi-path reasoning capability versus its iterative refinement process.\nAs shown in Table 4, even under this simplified integration, the performance of VisPath significantly outperforms all baseline methods, demonstrating its robustness and adaptability. This result suggests that the diversity of reasoning paths alone ensures a resilient and effective outcome, even in the absence of extensive intermediate corrections. This finding shows that the strength of our framework largely stems from its multi-path reasoning design. Even when the integration is simplified, the diversity of reasoning paths ensures that the aggregated output remains robust and effective, thus showing the overall adaptability and effectiveness of our approach."}, {"title": "Discussion", "content": "Our main experiment and ablation studies decisively demonstrate that VisPath's multi-path exploration revolutionizes visualization code generation by making it not just robust, but truly interpretable. Unlike traditional methods that often yield simplistic, one-dimensional results, VisPath embraces complexity\u2014delving deeper to capture nuances of user intent that would otherwise be lost. By integrating multiple perspectives during reasoning, our framework ensures that final visualizations are not only data-accurate but also rich in context, inherently intuitive, and unmistakably aligned with the user's true objectives.\nThe real breakthrough with VisPath is how it transforms visualization into a structured yet fluid process. Traditional methods often act like static translators, converting user queries into plots while neglecting the vital auxiliary components-legends, labels, line styles, and other elements that enable true interpretability. They assume that a single pass can sufficiently capture the essence of a visualization, but this leads to visualizations that might technically be correct but fail at communication. In contrast, VisPath's multi-path approach systematically explores different ways to represent data, integrating feedback at each step. This ensures that even subtle yet critical design elements are not just included, but optimized, making the resulting visualizations feel complete, polished, and immediately comprehensible.\nBeyond accuracy, VisPath introduces a level of adaptability that is indispensable in today's fast-evolving data landscape. It's not just a tool for static reports it is built for real-time streaming data, dynamic dashboards, and interactive analytics. Even with broad or loosely defined prompts, our framework intelligently synthesizes high-quality visualizations, closing the gap between intent and execution. As industries continue to demand clearer, faster, and more adaptable insights, VisPath is poised to lead the way, setting a new standard for intelligent, context-aware data visualization."}, {"title": "Conclusion", "content": "In this work, we present VisPath, a groundbreaking framework that redefines the landscape of automated visualization code generation over existing methods. Unlike prior methods, our approach seaminglessly combines multi-path reasoning with feedback-driven optimization. Accurately capturing diverse user intents and refine generated code, VisPath achieves notable improvements in both execution success and visual quality on challenging benchmarks such as MatPlotBench and the Qwen-Agent Code Interpreter Benchmark. By prioritizing adapability and user intent alignment, VisPath is uniquely positioned to handle compledities of real-world data visualization tasks. Looking ahead, future work could explore VisPath's adaptability in more dynamic, real-world visualization scenarios, further broadening its scope and enhancing its practical utility in complex data analysis contexts."}, {"title": "Limitations", "content": "Despite its effectiveness, the current iteration of our framework relies on a limited feedback mechanism, evaluating visualizations primarily based on two aspects: query-code alignment and query-plot image alignment. While these aspects provide valuable insights, they may not fully capture the granular elements that contribute to overall interpretability. To enhance the depth of feedback itself, future research could focus on decomposition of assessment process, assessing individual plot components separately. This granular analysis would render for a more nuanced assessment of readability, element appropriateness and visual coherence, ultimately leading to more refined visualization code generation. Strengthening the feedback mechanism in such way will be crucial for maximizing VisPath's effectiveness in diverse and complex visualization scenarios."}, {"title": "Appendix A. Prompts Used", "content": ""}, {"title": "Appendix B. Case Study", "content": ""}]}