{"title": "Monetizing Currency Pair Sentiments\nthrough LLM Explainability", "authors": ["Lior Limonada", "Fabiana Fournier", "Juan Manuel Vera D\u00edaz", "Inna Skarbovsky", "Shlomit Gure", "Raquel Lazcano"], "abstract": "Large language models (LLMs) play a vital role in almost\nevery domain in today's organizations. In the context of this work,\nwe highlight the use of LLMs for sentiment analysis (SA) and ex-\nplainability. Specifically, we contribute a novel technique to leverage\nLLMs as a post-hoc model-independent tool for the explainability of\nSA. We applied our technique in the financial domain for currency-\npair price predictions using open news feed data merged with market\nprices. Our application shows that the developed technique is not\nonly a viable alternative to using conventional explainable AI but\ncan also be fed back to enrich the input to the machine learning (ML)\nmodel to better predict future currency-pair values. We envision our\nresults could be generalized to employing explainability as a conven-\ntional enrichment for ML input for better ML predictions in general.", "sections": [{"title": "Introduction", "content": "Explainability is the foundation for the adoption and trust of humans\nin Al-based systems. Explanations are the vehicle via which users\ncan understand and act upon the various situations that evolve during\ntheir interaction with the system. Although some Machine Learn-\ning (ML) models are inherently explainable (e.g., decision trees and\nlinear models) and their internal logic predictions are interpretable\n(i.e., can be easily understood by humans), more complex models\n(e.g., ensemble models and deep learning models) require exter-\nnal explanation frameworks, namely eXplainable AI (XAI), to be\nhuman-understandable [11, 8]. Probably most prominent recently is\nthe employment of Generative AI\u00b9, and particularly its applicability\nto textual artifacts in the form of Large Language Models (LLMs).\nThis presents a promising instrumentation to enable\nmodel-independent explanations that are easy to interpret by humans.\nGartner's report [5] highlights the widespread adoption of LLMs\nacross industries for uses including text summarization, question-\nanswering, and document translation. LLMs can also be augmented\nby additional capabilities to create more powerful systems and fea-\nture a growing ecosystem of tools. Among these, we foresee the ben-\nefit of leveraging LLMs not only as a means for the automation of\nexplanations in AI-based systems but also as an enrichment mecha-\nnism that could be fed back as input for the AI itself to improve its\nperformance. Our results in this work pave the way to conducting a\nmore rigorous study to support our claim."}, {"title": "Background", "content": "Recent advancements in the ML field have increased the complexity\nof ML models, often at the expense of their interpretability, leading to\n\"black box\" ML models and hindering their full adoption. Therefore,\nwe have also seen an increase in the need to explain ML models and\ntheir predictions, fuelled in part by legislation, but also by incentives\nfrom the user's or stakeholder's point of view (e.g., justify the ML\nmodel and gain domain insight), and from the developer's point of\nview (e.g., evaluate and improve the ML model).\neXplainable AI (XAI) is a research field that aims to make AI sys-\ntems results more understandable to humans [1]. Many XAI frame-\nworks are predominately developed for post-hoc (i.e., after model\ntraining) interpretations of ML models. In contemporary XAI tech-\nniques (e.g., LIME [22] or SHAP [16]), the ML model serves as a\nsurrogate model typically trained using historical data. The predicted\nvalue for a single instance serves as input for the XAI explainer to\nproduce an explanation."}, {"title": "LLM for SA explainability", "content": "We developed a technique for the use of an LLM for model-agnostic\nexplainability, given as an input a narrative and its sentiments as de-\ntermined by some SA model (e.g., VADER or BERT). Our technique\nidentifies the set of keywords or terms in the narrative that support\n(i.e., explain) the sentiment according to the inference model.\nOur approach was developed as a post-hoc technique to enable the\nidentification of the 'K' most relevant set of keywords or terms (i.e.,\nthe k-sufficient set) in the input narrative that is sufficient to influ-\nence the prediction of the ML model, regardless of which specific\nML technique is used for the prediction of the sentiment. Concretely,\na k-sufficient set is a subset of words from the original narrative that\nwhen provided as an input to the ML model retains the same sen-\ntiment output as the sentiment that was originally generated for the\nentire narrative. Thus, given some model M and an input narrative\ntext $T = W_1,..., w_n$, we can determine its sentiment as M(T).\nRespectively, a k-sufficient set in this case is a subset of k terms\n$S_k = W_1, ..., W_k \\subset T$, where $|s_k| = k$, conforming to:\n1. $M (S_k) = M (T)$, i.e., the subset of terms retains the same senti-\nment.\n2. [optional] There is no subset $s_1 = W_1...W_z \\subset s_k$ where $M(s_1) =\nM(T)$\nThe set $s_k$ is deemed sufficient in the sense that its inclusion in text\n$T$ guarantees the sentiment classification of $T$ equals the original\nsentiment classification as deemed by the ML model (i.e., $M(T)$).\nWe acknowledge that there could be several sets in T conforming\nto the above conditions. Our current implementation is indifferent\namong such subsets.\nStep 1 in our method accounts for explanation sufficiency corre-\nsponding to the input request for K terms. That is, such a set contains\nall needed terms to ensure it preserves consistency with the senti-\nment classification of the original narrative. This does not assure that\nsuch a set contains the fewest terms necessary to retain the original\nsentiment classification (i.e., a minimal set). If such a requirement\nis also deemed necessary, we include step two as an optional exten-\nsion to fulfill such a requirement. Without concerns of performance,\na straightforward realization will need to add to the given implemen-\ntation also an exhaustive scan of all k-1 subsets $s_1$ to ensure that\nfor any such subset $M (s_1) \\neq M(T)$. Another possible realization\nmay directly leverage on the power of the LLM to identify such a\nminimal subset directly.\nA more advanced approach could also exploit the computation of\nweights that signify the relative importance of each word in affecting\nthe result of the ML model. In such a case, our algorithm should\nbe enhanced with a selection of a subset that relies on the value of\nsuch weights (e.g., the one with the highest weight average). To elicit\na k-sufficient set of keywords from a given narrative, we follow a\nzero-shot prompting approach. The overall flow of our approach to\nclassification explanation is depicted in Figure 2.\nAlgorithm 1 formally elaborates on the realization of the classifi-\ncation explanation, given M as the model for sentiment elicitation,\nX as a textual narrative, L is the employed LLM, K as the number\nof keywords, and max Attempts as a threshold limit to the number\nof iteration attempts. An implementation of the algorithm is available\nhere: https://github.com/IBM/SAX/AlFin."}, {"title": "Enriched feeds to predict currency-pair prices", "content": "Building on prior work [10], our first envisioned hypothesis is that\nenriching an input feed of price closing rates with sentiment infor-\nmation that corresponds to a series of news ads about a given cur-\nrency pair may promote not only a better prediction of future trends\n(i.e., increase or decrease in price) but also to the improved predic-\ntive accuracy of future closing prices. By enrichment of the input,\nwe relate to extending the input with additional term embeddings as\nfurther elaborated below.\nGoing beyond the first hypothesis, we further envision the poten-\ntial monetary benefit to be exploited from explanatory information\nabout sentiment classification. Leveraging the technique presented\nin section 3 for associating each sentiment with a set of k-sufficient\nterms for a given news ad, our next step is to use this technique to\nenrich a series of news-ad-derived sentiments with their correspond-\ning highlighted keywords. We envision that such enriched traces may\nserve as a valuable input for the prediction of future currency indica-\ntors, correlating with price movements such as currency-pair closure\nprices (i.e., as in [10]) and that complementing such sentiment traces\nwith the k-sufficient keywords (along each sentiment label) may help\nto improve the accuracy of such predictions. With the technique pre-\nsented, we also foresee the viability of deriving such information au-\ntomatically with the use of recently developed LLMs.\nHence we hypothesize that:\n\u2022 H1: News-ad-derived sentiment information can be used to im-\nprove the accuracy of predictions about currency-pair future rate\nbehaviors.\n\u2022 H2: Sentiment feeds enriched with explanatory information will\nachieve better predictive accuracy than non-enriched sentiment\nfeeds.\nTo evaluate the hypotheses, we used an open sentiment labeled\ndataset from [10]. This data consists of 2291 news ads about five\ndifferent currency pairs collected between January and May 2023.\nThis dataset was combined with daily currency-pair historical prices\nfor the same period as summarized in Table 1, which was down-\nloaded from the Yahoo Finance website. Note that the USDJPY price\nis about two orders of magnitude greater than all other currency pair\nprices.\nUtilizing the dataset, a Long Short-Term Memory (LSTM)\nmodel [12] was structured with a 32-unit layer followed by a neu-\nron that regresses the currency price. An LSTM model is a recur-\nrent neural network architecture well-suited for sequential data. The\nmodel was trained as a baseline to predict the next day's closing price\nfrom the past 5 days. To test the first hypothesis, we extended this in-\nput with three sentiment-derived features, denoting the percentage of"}, {"title": "Results", "content": "To evaluate future trend predictions the data consisted of 74 predic-\ntion points for each currency pair. Given days must be consecutive to\nmaintain the temporal correlation, data was utilized in two separate\ninstances: initially with a train-to-test split ratio of 44:30 days (i.e.,\n60%/40%), and subsequently with a train-to-test split ratio of 60:14\ndays (i.e., 80%/20%). An example of the latter case consisting of a\ntest set of 14 consecutive days is shown in Figure 3.\nSimilar to prior work, we assessed the quality of the predictions\nwith an accuracy metric that captures the proportion of correct\npredictions out of the total number of predictions. Results for trend\npredictions are shown in Tables 2 and 3. The results in the tables\nare color-coded according to the range of each column's values, with\nhigher accuracy represented by greener colors and lower accuracy\nindicated by yellower colors.\nCorresponding to H1, on average the inclusion of the sentiment in-\nformation on top of the underlying pricing information, as the input\nto the LSTM model did not yield any improvement in trend predic-\ntion accuracy. However, excluding the EURCHF currency pair, the\nembedding of the sentiment yielded either equal or improved accu-\nracy in trend predictions. With respect to H2, the inclusion of the\nexplanation information was mostly effective for the GPT4.0 model,\nwith all accuracy results except one (for USDJPY in Table 2) being\neither the same or improved compared to the accuracy without the ex-\nplanation information. For both remaining models, Granite and GPT-\n3.5 Curated, the inclusion of explanation information led to improved\nresults on average, with both being mostly dominated by GPT4.0.\nFor the prediction of currency-pair prices, data was randomly split,\n60%-training, 6%-validation, and 34%-testing. Post-model training\nand testing, we present predictive accuracy for all models according\nto three common metrics: Mean Squared Error (MSE), Mean Ab-\nsolute Error (MAE), and Mean Absolute Percentage Error (MAPE).\nResults for all metrics are presented in table 4 and shown separately\nfor each metric, color-coded similar to the above, with higher accu-\nracy represented by greener colors and lower accuracy indicated by\nredder colors.\nWith the partial exclusion of the results for USDJPY, our results\ncorroborate both hypotheses. We thus also show the overall aver-\nage for each metric with and without the inclusion of USDJPY."}, {"title": "Related Work", "content": "We relate our work to the literature in the domain of explainability\nfor SA.\nGenerative AI and transformer-based LLMs like BERT [6] and\nGPT [21] have excelled in sentiment analysis among various tasks,\nwith BERT significantly advancing general language understanding\ntasks [20]. However, their large size and weight count lead to slower\ntraining. GPT-3.5's performance further highlights LLMs' superi-\nority in current methods [13]. Our research extends LLMs' utility\nto explain other ML models used for SA, challenging conventional\npost-hoc XAI tools like SHAP or LIME for explainability [13]. Re-\ncent studies, including one on financial texts, indicate that LLMs\ncan enhance sentiment classification performance by 35% and simi-\nlarly improve correlations with market returns [10]. While this work\nshows the potential benefit of utilizing LLMs for sentiment classi-\nfication and in demonstrating that such sentiments retain a level of\nabout 0.6-0.7 correlation with market trends, our work extends this\nwork further in two aspects. We show that the enrichment of a ML\nmodel input with not only the core sentiments associated with mar-\nket prices, but also with the set of K-sufficient terms associated with\neach such sentiment, could improve the accuracy of such a model in\npredicting market trends. Subsequently, we also show that the model\nis fairly capable of better predicting not only the market trends with\nrespect to individual currency-pairs, but also the actual future prices.\nThe work in [9] shows LLMs' potential for explaining business\nprocess models, offering a simpler alternative to traditional \u03a7\u0391\u0399\ntools [17]. LLMs, despite their complex architecture, can improve\nresult understanding and trust through LLM-based explanations [7].\nA survey by Zhao et al. [24] highlights various LLM explainability\ntechniques based on the training paradigms of LLMs.\nProbably the closest to our work on using LLMs to explain sen-\ntiment analysis results is the work in [3] where LLMs are being\nemployed for generating counterfactual explanations. It presents a\nsystematic approach to identify a minimal set of text elements that\nchanges the classification outcomes. Following the use of LLMs for\nexplaining SA results, we can rely on their application for the real-\nization of the optional minimality step in our method for SA explain-\nability.\nWhile the interpretability of explanations for specific users can\nonly be verified through empirical assessment, as pursued in [9],\nLLMs can also assist in automating the fidelity check of explana-"}, {"title": "Conclusions and Future Work", "content": "Our results support the two hypotheses. While we may have limited\nthe scope of our empirical investigation presented to a small set of\ncurrency pairs during 3 months, our results look promising concern-\ning both envisioned concepts, demonstrating the potential monetary\nbenefit of leveraging recent LLM technology for the financial do-\nmain and automated trading. Further investigation should aspire to\nincrease the size of the dataset to corroborate our findings. Our re-\nsults are not only consistent with recent findings that argue for the\npredictive value of utilizing sentiment information for the prediction\nof future prices but are also showing great promise in using explain-\nability information in a way that goes beyond its conventional intent\nto make model predictions more interpretable to humans. As one\nsuch concrete application, our results show the value of leveraging\nsentiment explainability to automatically enrich the input into predic-"}]}