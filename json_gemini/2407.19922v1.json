{"title": "Monetizing Currency Pair Sentiments through LLM Explainability", "authors": ["Lior Limonada", "Fabiana Fournier", "Juan Manuel Vera D\u00edaz", "Inna Skarbovsky", "Shlomit Gure", "Raquel Lazcano"], "abstract": "Large language models (LLMs) play a vital role in almost every domain in today's organizations. In the context of this work, we highlight the use of LLMs for sentiment analysis (SA) and explainability. Specifically, we contribute a novel technique to leverage LLMs as a post-hoc model-independent tool for the explainability of SA. We applied our technique in the financial domain for currency-pair price predictions using open news feed data merged with market prices. Our application shows that the developed technique is not only a viable alternative to using conventional explainable AI but can also be fed back to enrich the input to the machine learning (ML) model to better predict future currency-pair values. We envision our results could be generalized to employing explainability as a conventional enrichment for ML input for better ML predictions in general.", "sections": [{"title": "Introduction", "content": "Explainability is the foundation for the adoption and trust of humans in AI-based systems. Explanations are the vehicle via which users can understand and act upon the various situations that evolve during their interaction with the system. Although some Machine Learning (ML) models are inherently explainable (e.g., decision trees and linear models) and their internal logic predictions are interpretable (i.e., can be easily understood by humans), more complex models (e.g., ensemble models and deep learning models) require external explanation frameworks, namely eXplainable AI (XAI), to be human-understandable [11, 8]. Probably most prominent recently is the employment of Generative AI\u00b9, and particularly its applicability to textual artifacts in the form of Large Language Models (LLMs). This presents a promising instrumentation to enable out-of-the-box model-independent explanations that are easy to interpret by humans. Gartner's report [5] highlights the widespread adoption of LLMs across industries for uses including text summarization, question-answering, and document translation. LLMs can also be augmented by additional capabilities to create more powerful systems and feature a growing ecosystem of tools. Among these, we foresee the benefit of leveraging LLMs not only as a means for the automation of explanations in AI-based systems but also as an enrichment mechanism that could be fed back as input for the AI itself to improve its performance. Our results in this work pave the way to conducting a more rigorous study to support our claim."}, {"title": "Background", "content": "Recent advancements in the ML field have increased the complexity of ML models, often at the expense of their interpretability, leading to \"black box\" ML models and hindering their full adoption. Therefore, we have also seen an increase in the need to explain ML models and their predictions, fuelled in part by legislation, but also by incentives from the user's or stakeholder's point of view (e.g., justify the ML model and gain domain insight), and from the developer's point of view (e.g., evaluate and improve the ML model).\neXplainable AI (XAI) is a research field that aims to make AI systems results more understandable to humans [1]. Many XAI frameworks are predominately developed for post-hoc (i.e., after model training) interpretations of ML models. In contemporary XAI techniques (e.g., LIME [22] or SHAP [16]), the ML model serves as a surrogate model typically trained using historical data. The predicted value for a single instance serves as input for the XAI explainer to produce an explanation."}, {"title": "LLM for SA explainability", "content": "We developed a technique for the use of an LLM for model-agnostic explainability, given as an input a narrative and its sentiments as determined by some SA model (e.g., VADER or BERT). Our technique identifies the set of keywords or terms in the narrative that support (i.e., explain) the sentiment according to the inference model.\nOur approach was developed as a post-hoc technique to enable the identification of the 'K' most relevant set of keywords or terms (i.e., the k-sufficient set) in the input narrative that is sufficient to influence the prediction of the ML model, regardless of which specific ML technique is used for the prediction of the sentiment. Concretely, a k-sufficient set is a subset of words from the original narrative that when provided as an input to the ML model retains the same sentiment output as the sentiment that was originally generated for the entire narrative. Thus, given some model M and an input narrative text $T = W_1,..., w_n$, we can determine its sentiment as M(T). Respectively, a k-sufficient set in this case is a subset of k terms $S_k = W_1, ..., W_k \\subset T$, where $|s_k| = k$, conforming to:\n1. $M(S_k) = M(T)$, i.e., the subset of terms retains the same sentiment.\n2. [optional] There is no subset $s' = W_1...W_z \\subset s_k$ where $M(s') = M(T)$\nThe set $s_k$ is deemed sufficient in the sense that its inclusion in text T guarantees the sentiment classification of T equals the original sentiment classification as deemed by the ML model (i.e., M(T)). We acknowledge that there could be several sets in T conforming to the above conditions. Our current implementation is indifferent among such subsets.\nStep 1 in our method accounts for explanation sufficiency corresponding to the input request for K terms. That is, such a set contains all needed terms to ensure it preserves consistency with the sentiment classification of the original narrative. This does not assure that such a set contains the fewest terms necessary to retain the original sentiment classification (i.e., a minimal set). If such a requirement is also deemed necessary, we include step two as an optional extension to fulfill such a requirement. Without concerns of performance, a straightforward realization will need to add to the given implementation also an exhaustive scan of all k-1 subsets $s'$ to ensure that for any such subset $M(s') \\neq M(T)$. Another possible realization may directly leverage on the power of the LLM to identify such a minimal subset directly.\nA more advanced approach could also exploit the computation of weights that signify the relative importance of each word in affecting the result of the ML model. In such a case, our algorithm should be enhanced with a selection of a subset that relies on the value of such weights (e.g., the one with the highest weight average). To elicit a k-sufficient set of keywords from a given narrative, we follow a zero-shot prompting approach. The overall flow of our approach to classification explanation is depicted in Figure 2.\nAlgorithm 1 formally elaborates on the realization of the classification explanation, given M as the model for sentiment elicitation, X as a textual narrative, L is the employed LLM, K as the number of keywords, and max Attempts as a threshold limit to the number of iteration attempts. An implementation of the algorithm is available here: https://github.com/IBM/SAX/AlFin."}, {"title": "Enriched feeds to predict currency-pair prices", "content": "Building on prior work [10], our first envisioned hypothesis is that enriching an input feed of price closing rates with sentiment information that corresponds to a series of news ads about a given currency pair may promote not only a better prediction of future trends (i.e., increase or decrease in price) but also to the improved predictive accuracy of future closing prices. By enrichment of the input, we relate to extending the input with additional term embeddings as further elaborated below."}, {"title": "Results", "content": "To evaluate future trend predictions the data consisted of 74 prediction points for each currency pair. Given days must be consecutive to maintain the temporal correlation, data was utilized in two separate instances: initially with a train-to-test split ratio of 44:30 days (i.e., 60%/40%), and subsequently with a train-to-test split ratio of 60:14 days (i.e., 80%/20%). An example of the latter case consisting of a test set of 14 consecutive days is shown in Figure 3.\nSimilar to prior work, we assessed the quality of the predictions with an accuracy metric that captures the proportion of correct predictions out of the total number of predictions. Results for trend predictions are shown in Tables 2 and 3. The results in the tables are color-coded according to the range of each column's values, with higher accuracy represented by greener colors and lower accuracy indicated by yellower colors.\nCorresponding to H1, on average the inclusion of the sentiment information on top of the underlying pricing information, as the input to the LSTM model did not yield any improvement in trend prediction accuracy. However, excluding the EURCHF currency pair, the embedding of the sentiment yielded either equal or improved accuracy in trend predictions. With respect to H2, the inclusion of the explanation information was mostly effective for the GPT4.0 model, with all accuracy results except one (for USDJPY in Table 2) being either the same or improved compared to the accuracy without the explanation information. For both remaining models, Granite and GPT-3.5 Curated, the inclusion of explanation information led to improved results on average, with both being mostly dominated by GPT4.0.\nFor the prediction of currency-pair prices, data was randomly split, 60%-training, 6%-validation, and 34%-testing. Post-model training and testing, we present predictive accuracy for all models according to three common metrics: Mean Squared Error (MSE), Mean Absolute Error (MAE), and Mean Absolute Percentage Error (MAPE). Results for all metrics are presented in table 4 and shown separately for each metric, color-coded similar to the above, with higher accuracy represented by greener colors and lower accuracy indicated by redder colors.\nWith the partial exclusion of the results for USDJPY, our results corroborate both hypotheses. We thus also show the overall average for each metric with and without the inclusion of USDJPY."}, {"title": "Related Work", "content": "We relate our work to the literature in the domain of explainability for SA.\nGenerative AI and transformer-based LLMs like BERT [6] and GPT [21] have excelled in sentiment analysis among various tasks, with BERT significantly advancing general language understanding tasks [20]. However, their large size and weight count lead to slower training. GPT-3.5's performance further highlights LLMs' superiority in current methods [13]. Our research extends LLMs' utility to explain other ML models used for SA, challenging conventional post-hoc XAI tools like SHAP or LIME for explainability [13]. Recent studies, including one on financial texts, indicate that LLMs can enhance sentiment classification performance by 35% and similarly improve correlations with market returns [10]. While this work shows the potential benefit of utilizing LLMs for sentiment classification and in demonstrating that such sentiments retain a level of about 0.6-0.7 correlation with market trends, our work extends this work further in two aspects. We show that the enrichment of a ML model input with not only the core sentiments associated with market prices, but also with the set of K-sufficient terms associated with each such sentiment, could improve the accuracy of such a model in predicting market trends. Subsequently, we also show that the model is fairly capable of better predicting not only the market trends with respect to individual currency-pairs, but also the actual future prices.\nThe work in [9] shows LLMs' potential for explaining business process models, offering a simpler alternative to traditional \u03a7\u0391\u0399 tools [17]. LLMs, despite their complex architecture, can improve result understanding and trust through LLM-based explanations [7]. A survey by Zhao et al. [24] highlights various LLM explainability techniques based on the training paradigms of LLMs.\nProbably the closest to our work on using LLMs to explain sentiment analysis results is the work in [3] where LLMs are being employed for generating counterfactual explanations. It presents a systematic approach to identify a minimal set of text elements that changes the classification outcomes. Following the use of LLMs for explaining SA results, we can rely on their application for the realization of the optional minimality step in our method for SA explainability.\nWhile the interpretability of explanations for specific users can only be verified through empirical assessment, as pursued in [9], LLMs can also assist in automating the fidelity check of explanations. As indicated in [18], LLMs can contribute to the factuality evaluation of a given summary to assess its faithfulness.\nOur core innovation is in the post-hoc methodological approach for leveraging an LLM to identify a sufficient set of K-terms that retains the original classification of the SA model as a novel approach, inspired by counterfactual explanations in adversarial machine-learning in which minimal sufficient sets of terms may typically be sought to reverse a model's classification [23]. Our approach leverages LLMs to infer keywords based on their inherent generative power derived from the richness of underlying corpora used in their development, as opposed to employing some XAI algorithmic convergence approach as in [4]. Furthermore, we exploit these keywords to improve the accuracy of the ML model predictions of market prices."}, {"title": "Conclusions and Future Work", "content": "Our results support the two hypotheses. While we may have limited the scope of our empirical investigation presented to a small set of currency pairs during 3 months, our results look promising concerning both envisioned concepts, demonstrating the potential monetary benefit of leveraging recent LLM technology for the financial domain and automated trading. Further investigation should aspire to increase the size of the dataset to corroborate our findings. Our results are not only consistent with recent findings that argue for the predictive value of utilizing sentiment information for the prediction of future prices but are also showing great promise in using explainability information in a way that goes beyond its conventional intent to make model predictions more interpretable to humans. As one such concrete application, our results show the value of leveraging sentiment explainability to automatically enrich the input into predictive financial trading models, where such input promotes improving its predictive accuracy. While our second hypothesis was validated only with one technique for explanation derivation and enrichment, other realization methods may be employed, such as replacing embeddings with SHAP values as the realization of the input enrichment. In general, we argue that the presented application in the financial domain is only one example domain in which this approach could be exploited, while in general, the use of explainability could yield a beneficial input enrichment in other application domains as well.\nWe also foresee how recent advances in LLMs developed for time-series predictions present a viable alternative to using models such as LSTM employed in our work and are likely to equip companies with complete out-of-the-box tool suites that could be fairly easy to compose to automate such applications."}]}