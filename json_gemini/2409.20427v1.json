{"title": "Sufficient and Necessary Explanations\n(and What Lies in Between)", "authors": ["Beepul Bharti", "Paul H. Yi", "Jeremias Sulam"], "abstract": "As complex machine learning models continue to find applications in high-stakes decision making\nscenarios, it is crucial that we can explain and understand their predictions. Post-hoc explanation\nmethods provide useful insights by identifying important features in an input x with respect to the\nmodel output f (x). In this work we formalize and study two precise notions of feature importance for\ngeneral machine learning models: sufficiency and necessity. We demonstrate how these two types of\nexplanations, albeit intuitive and simple, can fall short in providing a complete picture of which features\na model finds important. To this end, we propose a unified notion of importance that circumvents\nthese limitations by exploring a continuum along a necessity-sufficiency axis. Our unified notion, we\nshow, has strong ties to other popular definitions of feature importance, like those based on conditional\nindependence and game-theoretic quantities like Shapley values. Crucially, we demonstrate how a\nunified perspective allows us to detect important features that could be missed by either of the previous\napproaches alone.", "sections": [{"title": "Introduction", "content": "Over recent years, modern machine learning (ML) models, mostly deep learning based, have achieved im-\npressive results across several complex domains. We now have models that can solve difficult image classi-\nfication, inpainting, and segmentation problems, perform accurate text and sentiment analysis, predict the\nthree-dimensional conformation of proteins, and more (LeCun et al., 2015; Wang et al., 2023). Despite their\nsuccess, the rapid integration of these models into society requires caution due to their complexity and\nunintelligibility (The White House, 2023). Modern ML systems are, by and large black-boxes, consisting of\nmillions of parameters and non-linearities that obscure their prediction-making mechanisms from users,\ndevelopers, and auditors. This lack of clarity raises concerns about explainability, transparency, and ac-\ncountability (Zednik, 2021; Tomsett et al., 2018). Thus, understanding how these models work is essential\nfor their safe deployment.\nThe lack of explainability has spurred research efforts in explainable AI (XAI), with a major focus on\ndeveloping post-hoc methods to explain black-box model predictions, especially at a local level. For a\nmodel f and input x \u2208 Rd, these methods aim to identify which features in x are important for the\nmodel's prediction, f(x). They do so by estimating a notion of importance for each feature (or groups)\nwhich allows for a ranking of importance. Popular methods include CAM (Zhou et al., 2016), LIME (Ribeiro\net al., 2016), gradient-based approaches (Selvaraju et al., 2017; Shrikumar et al., 2017; Jiang et al., 2021),\nrate-distortion techniques (Kolek et al., 2021, 2022), Shapley value-based explanations (Chen et al., 2018b;\nTeneggi et al., 2022; Mosca et al., 2022), perturbation-based methods (Fong and Vedaldi, 2017; Fong et al.,\n2019; Dabkowski and Gal, 2017), among others (Chen et al., 2018a; Yoon et al., 2018; Jethani et al., 2021;\nWang et al., 2021; Ribeiro et al., 2018). However, many of these approaches lack rigor, as the meaning\nof their computed scores is often ambiguous. For example, it's not always clear what large or negative\ngradients signify or what high Shapley values reveal about feature importance. To address these concerns,\nother research has focused on developing explanation methods based on logic-based definitions (Ignatiev\net al., 2020; Darwiche and Hirth, 2020; Darwiche and Ji, 2022; Shih et al., 2018), conditional hypothesis\ntesting Teneggi et al. (2023); Tansey et al. (2022), among formal notions. While these methods are a step\ntowards rigor, they have drawbacks, including reliance on complex automated reasoners and limited ability\nto communicate their results in an understandable way for human decision-makers.\nIn this work, we advance XAI research by formalizing rigorous mathematical definitions and approaches,\ngrounded in the intuitive concepts of sufficiency and necessity, to explain complex ML models. We begin\nby illustrating how sufficient and necessary explanations offer valuable, albeit incomplete, insights into\nfeature importance. To address this issue, we propose and study a more general unified framework for\nexplaining models. We offer two novel perspectives on our framework through the lens of conditional\nindependence and Shapley values, and crucially, show how it reveals new insights into feature importance."}, {"title": "1.1 Summary of our Contributions", "content": "We study two key notions of importance: sufficiency and necessity, both which evaluate the importance\nof a set of features in x, with respect to the prediction f(x), of an ML model. A sufficient set of features\npreserves the model's output, while a necessary set, when removed or perturbed, renders the output unin-\nformative. Although sufficiency and necessity appear complementary, their precise relationship remains\nunclear. When do sufficient and necessary subsets overlap or differ? When should we prioritize one over\nthe other, or seek features that are both necessary and sufficient? To address these questions, we analyze\nsufficiency and necessity and propose a unification of both. Our contributions are summarized as follows:\n1. We formalize precise mathematical definitions of sufficient and necessary model explanations for\narbitrary ML predictors.\n2. We propose a unified approach that combines sufficiency and necessity, analyzing their relationships"}, {"title": "2 Sufficiency and Necessity", "content": "Notation. We use boldface uppercase letters to denote random vectors (e.g., X) and lowercase letters\nfor their values (e.g., x). For a subset S \u2286 [d] := {1, ...,d}, we denote its cardinality by |S| and its\ncomplement Sc = [d] \\ S. Subscripts index features; e.g., the vector xs represents the restriction of x to\nthe entries indexed by S.\nSetting. We consider a supervised learning setting with an unknown distribution D over X \u00d7 Y, where\nXC Rd is a d-dimensional feature space and Y \u2286 R is the label space. We assume access to a predictor\nf : X \u2192 Y that was trained on samples from D. For an input x = (x1,...,xd) \u2208 Rd, our goal is to\nidentify the important features of x for the prediction f(x). To define feature importance precisely, we\nuse the average restricted prediction,\nfs(x) = E [f(xs, Xs)]\nXs~Vs"}, {"title": "2.1 Definitions", "content": "We introduce two intuitive notions to quantify the importance of a subset S for a prediction f(x). For a\nmodel f, we begin by evaluating its baseline behavior over an arbitrary reference distribution V: f(x) =\nE[f(X[d])]. Then, for a sample x and its prediction f(x), we can pose two simple questions:\nWhich set of features, S, satisfies fs(x) \u2248 f(x) or fs(x) \u2248 f\u00f8(x)?\nThese questions explores the sufficiency and necessity of subset S, which we define formally in the fol-\nlowing definitions\nDefinition 2.1 (Sufficiency). Let \u0454 > 0 and let p : R \u00d7 R \u2192 R be a metric on R. A subset S \u2286 [d] is\ne-sufficient with respect to a distribution V for f at x if\nAuf(S, f, x) = p(f(x), fs(x)) \u2264 \u20ac.\nFurthermore, S is e-super sufficient if all supersets S \u2265 S are e-sufficient.\nThis notion of sufficiency is straightforward: a subset S is e-sufficient with respect to a reference distri-\nbution V if, with xs fixed, the average restricted prediction fs(x) is within e from the original prediction\nf(x). This is further strengthened by super-sufficiency: a subset S is e-super sufficient if p( f (x), fs(x)) \u2264\ne and, for any superset S of S, p(f(x), f(x)) \u2264 6. This simply means including more features in S still\nkeeps fs(x) e close to f(x). To find a small sufficient subset S of small cardinality \u03c4 > 0, we can solve\nthe following optimization problem:\narg min\nS\u2286 [d]Auf(S, f, x) subject to |S| \u2264 \u0442."}, {"title": "Related Work", "content": "Notions of sufficiency, necessity, the duality between the two, and their connections with other feature\nattribution methods have been studied to varying degrees in XAI research. We comment on the main\nrelated works in this section.\nSufficiency. The notion of sufficient features has gained significant attention in recent research. Shih et al.\n(2018) explore a symbolic approach to explain Bayesian network classifiers and introduce prime implicant\nexplanations, which are minimal subsets S that make features in the complement irrelevant to the predic-\ntion f(x). For models represented by a finite set of first-order logic (FOL) sentences, Ignatiev et al. (2020)\nrefer to prime implicants as abductive explanations (AXp's). For classifiers defined by propositional for-\nmulas and inputs with discrete features, Darwiche and Hirth (2020) refer to prime implicants as sufficient\nreasons and define a complete reason to be the disjunction of all sufficient reasons. They present efficient\nalgorithms, leveraging Boolean circuits, to compute sufficient and complete reasons and demonstrate their\nuse in identifying classifier dependence on protected features that should not inform decisions. For more\ncomplex models, Ribeiro et al. (2018) propose high-precision probabilistic explanations called anchors,\nwhich represent local, sufficient conditions. For x positively classified by f, Wang et al. (2021) propose\na greedy approach to solve (Psuf) while the preservation method by Fong and Vedaldi (2017) relaxes S to\n[0, 1]d.\nNecessity. There has also been significant focus on identifying necessary features \u2013 those that, when\naltered, lead to a change in the prediction f(x). For models expressible by FOL sentences, Ignatiev et al.\n(2019) define prime implicates as the minimal subsets that when changed, modify the prediction f(x) and\nrelate these to adversarial examples. For Boolean models predicting on samples x with discrete features,\nIgnatiev et al. (2020) and (Darwiche and Hirth, 2020) refer to prime implicates as contrastive explanations\n(CXp's) and necessary reasons, respectively. Beyond boolean functions, for x positively classified by a\nclassifier f, Fong et al. (2019) relax S to [0, 1]d and propose the deletion method to approximately solve\n(Pnec).\nDuality Between Sufficiency and Necessity. Dabkowski and Gal (2017) characterize the preservation\nand deletion methods as discovering the smallest sufficient and destroying region (SSR and SDR). They\npropose combining the two but do not explore how solutions to this approach may differ from individual\nSSR and SDR solutions. Ignatiev et al. (2020) show that AXp's and CXp's are minimal hitting sets of\nanother by using a hitting set duality result between minimal unsatisfiable and correction subsets. The\nresult enables the identification of AXp's from CXp's and vice versa."}, {"title": "4 Unifying Sufficiency and Necessity", "content": "Given a model f, sample x, and reference distribution V, we can identify a small set of important features\nS by solving either (Psuf) or (Pnec) 2. While both methods are popular (Kolek et al., 2021, 2022; Fong\nand Vedaldi, 2017; Bhalla et al., 2023; Yoon et al., 2018), simply identifying a small sufficient or necessary\nsubset may not provide a complete picture of how f uses x to make a prediction. To see why, consider the\nfollowing scenario: for a fixed \u03c4 > 0, let S* be a e-sufficient solution to (Psuf), so that |S*| \u2264 7 and\nAuf (S, f, x) \u2264 \u20ac.\nWhile S* is e-sufficient, it can also be true that\nAn(S, f, x) > E\nindicating S* is not e-necessary: indeed, this can simply happen when its complement, St, contains im-\nportant features. This scenario raises two questions:\n1. How different are sufficient and necessary features?\n2. How does varying the levels of sufficiency and necessity affect the optimal set of important features?\nIn order to provide answers to these questions (and to avoid the scenario above) we propose to search\nfor a small set S that is both sufficient and necessary by combining problems (Psuf) and (Pnec). Consider\nAumi (S, f, x, a), a convex combination of both Auf(S, f, x) and Anec (S, f, x)\nuni\nAmi (S, f, x, a) = a \u2022 Auf(S, f, x) + (1 \u2212 a) \u00b7 A (S, f, x)\nwhere a \u2208 [0,1] controls the extent to which S is required to be sufficient vs. necessary. Our unified\nproblem, (Puni), can be expressed as:\narg min\nS\u2286 [d]Ami (S, f, x, a) subject to |S| \u2264 \u0442.\nWhen a is 1 or 0, Ami(S, f, x, a) reduces to Auf(S, f, x) or Anec(S, f, x), respectively. In these ex-\ntreme cases, S is only sufficient or necessary. In the remainder of this work we will theoretically analyze\n(Puni), characterize its solutions, and provide different interpretations of what properties the solutions have\nthrough the lens of conditional independence and game theory. In the experimental section, we will show\nthat solutions to (Puni) provide insights that neither (Psuf) nor (Pnec) offer."}, {"title": "4.1 Solutions to the Unified Problem", "content": "We begin with a simple lemma that demonstrates why (Puni) enforces both sufficiency and necessity.\nLemma 4.1. Let a \u2208 (0,1). For \u03c4 > 0, denote S* to be a solution to (Puni) for which Auni (S, f,x, a) = \u0454.\nThen, S* is-sufficient and 1-necessary. Formally,\n0\u2264uf (S*, f,x) \u2264 and 0 \u2264 nec (S*, f,x) \u2264 .\nThe proof of this result, and all others, is included Appendix A.1. This result illustrates that solutions to\n(Puni) satisfy varying definitions of sufficiency and necessity. Furthermore, as a increases from 0 to 1, the\nsolution shifts from being highly necessary to highly sufficient. In the following results, we will show when\nand how solutions to (Puni) are similar (and different) to those of (Psuf) and (Pnec). To start, we present the\nfollowing lemma, which will be useful in subsequent results.\nLemma 4.2. For 0 \u2264 \u20ac < P(f(x),fo(x)), denote Sand Sec to be e-sufficient and e-necessary sets. Then, if\n2\nsufSuf is e-super sufficient or Snec is e-super necessary, we have\nSsuf Snec \u2260 0.\nThis lemma demonstrates that, given e-sufficient and necessary sets Ssuf and Sec, if either additionally\nsatisfies the stronger notions of super sufficiency or necessity, they must share some features. This proves\nuseful in characterizing a solution to (Puni), which we now do in the following theorem.\nTheorem 4.1. Let T1, T2 > 0 and 0 \u2264 \u20ac < \u2022 p(f(x), f(x)). Denote S Ssuf and She Snec to be e-super sufficient\n2and e-super necessary solutions to (Psuf) and (Pnec), respectively, such that |S'suf | = T1 and|Snec| = T2. Then,\nthere exists a solution S* to (Puni) such that\nAuri (S*, f, x, \u03b1) <e and max(T1, T2) \u2264 |S*| < T1 + T2.\nFurthermore, if S* SsufSnec or Snec su Suf then S* = Snec or S'* = Su Sup respectively.\nThis result demonstrates that solutions to (Puni), (Psuf), and (Pnec) can be closely related. As an example,\nconsider features that are e-super sufficient, Suf. If we have domain knowledge that Suf Sec, and Sec\nis e-super necessary, then Snec is in fact the solution to the (Puni) problem. Conversely, if we know that\nSuf is e-super necessary along with being a subset of e-super sufficient set Stuf, then Stuf will be a solution\nto the (Puni) problem."}, {"title": "5 Two Perspectives of the Unified Approach", "content": "In the previous section, we characterized solutions to (Puni) and their connections to those of (Psuf) and\n(Pnec). To better understand sufficiency, necessity, and their unification, we will provide two alternative\nperspectives of our unified framework through the lens of conditional independence and Shapley values."}, {"title": "5.1 A Conditional Independence Perspective", "content": "Here we demonstrate how our sufficiency, necessity, and our unified approach, can be understood as mea-\nsuring conditional independence relations between features X and labels Y."}, {"title": "Corollary 5.1", "content": "Corollary 5.1. Suppose for any S \u2286 [d], Vs = p(Xs | Xse = xs). Let a \u2208 (0, 1), \u0454 \u2265 0, and denote\np : R \u00d7 R \u2192 R to be a metric on R. Furthermore, for f(X) = E[Y | X] and \u315c > 0, let S* be a solution to\n(Puni) such that \u2206uri(S, f,x, a) = e. Then, S* satisfies the following conditional independence relations,\np (E[Y | x], E[Y | Xs* = xs+]) < and p (E[Y | Xs = xs], E[Y]) \u2264 1\nThe assumption in this corollary is that, \u2200 S \u2286 [d], fs(x) is evaluated using the conditional distribution\np(Xsc | Xs = xs) as the reference distribution Vs. Given the recent advancements in generative models\n(Song and Ermon, 2019; Ho et al., 2020; Song et al., 2021), this assumption is (approximately) reasonable\nin many practical settings, as we will demonstrate in our experiments. With this reference distribution,\nthe results shows that for the model f(X) = E[Y | X] and a sample x, the minimizer S* of (Puni)\napproximately satisfies two conditional independence properties. First, S* is sufficient in that, when the\nfeatures in S* are fixed, the complement, Sc*, offers little-to-no additional information about Y. Second,\nS* is necessary because when we marginalize it out and rely only on the features in Sc*, the information\ngained about Y is minimal and similar to E[Y = 1]."}, {"title": "5.2 A Shapley Value Perspective", "content": "In the previous section, we detailed the conditional independence relations one gains from solving (Puni).\nWe now present an arguably less intuitive result that shows that solving (Puni) is equivalent to maximizing\nthe lower bound of Shapley value. Before presenting our result, we provide a brief background on this\ngame-theoretic quantity.\nShapley Values. Shapley values use game theory to measure the importance of players in a game. Let the\ntuple ([n], v) represent a cooperative game with players [n] = {1,2,..., , 2, . . ., 7 n} and denote a characteristic\nfunction v(S): P([n]) \u2192 R, which maps the power set of [n] to the reals. Then, the Shapley value\n(Shapley, 1951) for player j in the cooperative game ([n], v) is\n\u03d5shap ([n], v)j = \u03a3ws. [v(S\u222a {j}) \u2013 v(S)]\nSC[n]\\{j}\nwhere ws =|S!(n-|S|-1)!\nn! The Shapley value is the only solution concept that satisfies the desirable\naxioms of additivity, nullity, symmetry, and linearity (Owen, 2013). In the context of XAI and feature\nimportance, Shapley values are widely used to measure local feature importance by treating input features\nas players in a game (Covert et al., 2020; Teneggi et al., 2022; Chen et al., 2018b; Lundberg and Lee, 2017).\nGiven a sample x \u2208 Rd and a model f, the goal is to evaluate the importance of each feature j \u2208 [d] for\nthe prediction f(x). This is done by defining a cooperative game ([d], v), where v(S) is a characteristic\nfunction that quantifies how the features in S contribute to the prediction. Different choices of v(S) can be\nfound in (Lundberg and Lee, 2017; Sundararajan and Najmi, 2020; Watson et al., 2024). Although computing\n\u03d5shap ([d], v) is computationally intractable, several practical methods for estimation have been developed\n(Chen et al., 2023; Teneggi et al., 2022; Zhang et al., 2023; Lundberg et al., 2020). While Shapley values are\npopular across various domains (Moncada-Torres et al., 2021; Zoabi et al., 2021; Liu et al., 2021), few works,\naside from Watson et al. (2021), explore their connections to sufficiency and necessity.\nWith this background, we now present our result. Recall solving (Puni) obtains a small subset S with low\nAuri (S, f, x, a). Notice that in (Puni) there is a natural partitioning of the features into two sets, S and Sc.\nIn the follow theorem we demonstrate that searching for a small subset S with minimal Auri (S, f, x, a)\nis equivalent to maximizing a lower bound on the Shapley value in a two player game."}, {"title": "Theorem 5.1.", "content": "Theorem 5.1. Consider an input x for which f(x) \u2260 f\u00f8(x). Denote by Ad = {S, Sc} the partition of\n[d] = {1, 2, ..., d}, and define the characteristic function to be v(S) = \u2212p(f(x), fs(x)). Then,\n\u03d5shap (Ad, v) \u2265 p(f(x), f\u00f8(x)) \u2013 Ami(S, f, x, a)."}, {"title": "6 Experiments", "content": "We demonstrate our theoretical findings in multiple settings of increasingly complexity: two tabular data\ntasks (on synthetic data and the US adult income dataset (Ding et al., 2021)) and two high-dimensional\nimage classification tasks using the RSNA 2019 Brain CT Hemorrhage Challenge (Flanders et al., 2020) and\nCelebA-HQ datasets (Lee et al., 2020)."}, {"title": "6.1 Tabular Data", "content": "In the following examples, we analyze solutions to (Puni) for varying levels of sufficiency vs. necessity and\nmultiple size constraints. We learn a predictor f and, for 100 new samples, solve (Puni) for \u03c4\u2208 {3,6,9}\nand a \u2208 [0, 1], with p(a, b) = |a\u2212b| and Vs = p(Xs | Xsc = xs). For a fixed 7 and sample x, we denote\nS to be a solution to (Puni) for ai. To analyze the stability of Sa, as sufficiency and necessity vary, we\nreport the normalized average Hamming distance (Hamming, 1950) between Sa; and S, along with 95%\nconfidence intervals, as a function of a."}, {"title": "6.1.1 Linear Regression", "content": "We begin with a regression example. Features X are distributed according to N(\u03bc, \u0391\u0391\u03a4) with \u03bc =\n[2]d and Ai,j ~ U(0, 1). The response is Y = \u03b2^X + e, with \u1e9e = 32 \u00b7 [2-2]=1 and \u2208 ~ N(0, Idxd).\nWith d = 10 our model is f(X) = \u03b2x, where \u1e9e is the least squares solution.\nStability of Unified Solutions. Fig. 1a shows that when solutions are constrained to be small (r = 3),\nincreasing a to enforce greater sufficiency results in a steady increase in Hamming distance, indicating\nthat the solutions S\u2081 are consistently changing. When larger solutions are allowed (t = 6), S\u2081 rapidly\nchanges with the introduction of sufficiency, as seen by the initial steep rise in Hamming distance. How-\never, as a continues to increase, this distance grows more gradually. Lastly, when the solution size ap-\nproaches the dimensionality of the feature space (t = 9), small to intermediate levels of sufficiency do not\nsignificantly alter the solutions. However, requiring high levels of sufficiency (a > 0.8) leads to extreme\nchanges in the solutions, as shown by a sharp increase in Hamming distance."}, {"title": "6.1.2 American Community Survey Income (ACSIncome)", "content": "We use the ACSIncome dataset for California, including 10 demographic and socioeconomic features such\nas age, education, occupation, and geographic region. We train a Random Forest classifier to predict\nwhether an individual's annual income exceeds $50K, achieving a test accuracy \u2248 81%.\nStability of Unified Solutions. Fig. 1b shows that when solutions are forced to be small (r = 3), in-\ncreasing a to enforce sufficiency results in a steady increase in Hamming distance, indicating the solutions\nS are changing. For larger solutions (\u03c4 = 6), S, changes significantly when low levels sufficiency are"}, {"title": "6.2 Image Classification", "content": "The following two experiments explore high dimensional settings in image classification tasks. The fea-\ntures are pixel values and so a subset S corresponds to a binary mask identifying important pixels. Since\nsolving (Psuf), (Pnec), or (Puni) is NP-hard, we use two methods-one for each setting, described in their re-\nspective sections-which solve relaxed problems to identify sufficient and necessary masks S. These exper-\niments serve two purposes. First, they will analyze the extent to which explanations generated by popular\nmethods-including Integrated Gradients (Sundararajan et al., 2017), GradientSHAP (Lundberg and Lee,\n2017), Guided GradCAM (Selvaraju et al., 2017), and h-Shap (Teneggi et al., 2022)-identify small sufficient\nand necessary subsets. To ensure consistent analysis, we normalize all generated attribution scores to the\ninterval [0, 1]. This is done by setting the top 1% of nonzero scores to 1 and dividing the remaining scores\nby the minimum score from the top 1% of nonzero scores. Then, binary masks are generated by thresh-\nolding the normalized scores using t \u2208 [0, 1]. For a test set of images, we perform this normalization and\nreport the average \u2013 log(\u2206suf), \u2013 log(nec), and \u2013 log(L\u00ba) (across all binary masks) at different threshold\nvalues to analyze the sufficiency, necessity and size of the explanations. Finally, the second objective is to\nunderstand and visualize the similarities and differences between sufficient and necessary sets."}, {"title": "6.2.1 RSNA CT Hemorrhage", "content": "We use the RSNA 2019 Brain CT Hemorrhage Challenge dataset comprised of 752,803 scans. Each scan is\nannotated by expert neuroradiologists with the presence and type(s) of hemorrhage (i.e., epidural, intra-\nparenchymal, intraventricular, subarachnoid, or subdural). We use a ResNet18 (He et al., 2016) classifier\nthat was pretrained on this data (Teneggi et al., 2022). To identify sufficient and necessary sets we solve\nthe relaxed problem,\narg min Auni (S, f, x, a) + 11 \u00b7 ||S||1 + \u03bb\u03c4\u03bd\u00b7 ||S||Tv.\nSC[0,1]d"}, {"title": "6.2.2 CelebA-HQ", "content": "We use a modified version of the CelebA-HQ dataset (Karras, 2017) that contains 30,000 celebrity faces\nresized to 256\u00d7256 pixels. We train a ResNet18 to classify whether a celebrity is smiling, achieving a test\naccuracy \u2248 94%. To generate sufficient or necessary masks S for samples x, we learn models g\u00f8 : X \u2192 X,\nthat (approximately) solve the following optimization problem:\narg min E [Au(go(X), f, X, a) + \u03bb1\u00b7 ||99(X)||1 + \u03bb\u03c4\u03bd\u00b7 ||99(X)||Tv] .\n\u03b8\u0395\u0398\nX~Dx\nGiven the structured nature of the dataset and the similarity of features across images, we use this para-\nmetric model approach because it prevents overfitting to spurious signals, an issue that can arise with\nper-example methods. Additionally, this approach is more efficient, as it still generates tailored per-sample\nexplanations but only requires learning a single model rather than repeatedly solving Eq. (13) (Linder et al.,\n2022; Chen et al., 2018a; Yoon et al., 2018). To learn a necessary and sufficient explainer model, we solve\nEq. (14) via empirical risk minimization for a \u2208 {0,1} respectively. Implementation details and hyperpa-\nrameter settings are included in Appendix A.2.\nComparison of Post-hoc Interpretability Methods. For a set of 100 images labeled with a smile and\ncorrectly classified by the ResNet model, we apply multiple post-hoc interpretability methods and our\nsufficient and necessary explainers to identify important features associated with smiling. The results in\nFig. 3 illustrate that for a wide range of thresholds t \u2208 [0, 1], many methods identify sufficient subsets, as\n- log(\u2206suf) for many of them is comparable to that of the sufficient explainer. The necessary explainer,\nin fact, identifies subsets that are more sufficient than those found by the sufficient explainer. The reason\nis that the sufficient explainer identifies subsets that are, on average, smaller for all t \u2208 [0, 1], while the\nnecessary explainer finds subsets that are constant in size for all t \u2208 [0, 1] but slightly larger since, to be\nnecessary, they must contain more features that provide additional information about the label. For other\nmethods, as t increases, subset size decreases, and the sufficiency and necessity of the solutions decline.\nMeanwhile, the necessary explainer naturally identifies necessary subsets, indicated by large - log(\u2206nec),\nwhereas other methods fail to do so. In conclusion, many methods can identify sufficient sets, but not\nnecessary ones. Directly optimizing for these criterion leads to identifying small, constant-sized subsets\nacross thresholds."}, {"title": "Sufficiency vs. Necessity.", "content": "Sufficiency vs. Necessity. In Fig. 4, we see how sufficient subsets alone may overlook important features,\nwhile solutions to (Puni) offer deeper insights. As stated earlier, the sufficient explainer identifies sets that\nare sufficient but not necessary. On the other hand, the necessary explainer has high \u2013 log(suf) and\nlog(Anec), indicating that it identifies sufficient and necessary set, meaning they also serve as solutions\nto (Puni). In Fig. 4, we visualize the reasons for this phenomena. Notice that Suf precisely highlights (only)\nthe smile. When Stuf is fixed, one can generate new images (as done in (Zhang et al., 2023)) for which\nthe model produces the same predictions as it did for the original image (a smile). On the other hand,\nwe also see why Suf is not necessary: we can fix the complement (Suf) and, since there are important\nfeatures in it, a smile is consistently generated, and the model produces the same prediction on these\nimages as it did on the original. Conversely solutions to (Pnec) (also solutions to (Puni) here) generate\ndifferent explanations that provide a more complete picture of feature importance. Notice that Snec is\nsufficient because Stuf Srnec, with the additional features mainly being the dimples and eyes, which aid\nin determining the presence of a smile. More importantly, Fig. 5 illustrates why Snec is necessary: when\nwe fix the complement of Srnec and generate new samples, half of the faces lack a smile, leading the model\nf to predict no smile. Details on sample generation are in Appendix A.1."}, {"title": "7 Limitations & Broader Impacts", "content": "While this work provides a novel theoretical contribution to the XAI community, there are some limita-\ntions that require careful discussion. The choice of reference distribution Vs determines the characteristics"}, {"title": "8 Conclusion", "content": "This work formalizes notions of sufficiency and necessity as tools to evaluate feature importance and\nexplain model predictions. We demonstrate that sufficient and necessary explanations, while insightful,\noften provide incomplete while complementary answers to model behavior. To address this limitation, we\npropose a unified approach that offers a new and more nuanced understanding of model behavior. Our\nunified approach expands the scope of explanations and reveals trade-offs between sufficiency and neces-\nsity, giving rise to new interpretations of feature importance. Through our theoretical contributions, we\npresent conditions under which sufficiency and necessity align or diverge, and provide two perspectives of\nour unified approach through the lens of conditional independence and Shapley values. Our experimental\nresults support our theoretical findings, providing examples of how adjusting sufficiency-necessity trade-\noff via our unified approach can uncover alternative sets of important features that would be missed by\nfocusing solely on sufficiency or necessity. Furthermore, we evaluate common post-"}]}