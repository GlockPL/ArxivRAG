{"title": "The Future of Open Human Feedback", "authors": ["Shachar Don-Yehiya", "Ben Burtenshaw", "Ramon Fernandez Astudillo", "Cailean Osborne", "Mimansa Jaiswal", "Tzu-Sheng Kuo", "Wenting Zhao", "Idan Shenfeld", "Andi Peng", "Mikhail Yurochkin", "Atoosa Kasirzadeh", "Yangsibo Huang", "Tatsunori Hashimoto", "Yacine Jernite", "Daniel Vila-Suero", "Omri Abend", "Jennifer Ding", "Sara Hooker", "Hannah Rose Kirk", "Leshem Choshen"], "abstract": "Human feedback on conversations with language language models (LLMs) is central to how these systems learn about the world, improve their capabilities, and are steered toward desirable and safe behaviors. However, this feedback is mostly collected by frontier AI labs and kept behind closed doors. In this work, we bring together interdisciplinary experts to assess the opportunities and challenges to realizing an open ecosystem of human feedback for AI. We first look for successful practices in peer production, open source, and citizen science communities. We then characterize the main challenges for open human feedback. For each, we survey current approaches and offer recommendations. We end by envisioning the components needed to underpin a sustainable and open human feedback ecosystem. In the center of this ecosystem are mutually beneficial feedback loops, between users and specialized models, incentivizing a diverse stakeholders community of model trainers and feedback providers to support a general open feedback pool.", "sections": [{"title": "1 Introduction", "content": "Natural language conversations (i.e., chats) have become a primary mode of human-AI interaction. Since the debut of large language models (LLMs) as general-purpose, multi-task reasoners (Radford et al., 2019), the volume and variety of use cases have grown, from providing information (Ivanova et al., 2024) and enabling tool use (Schick et al., 2024) to assisting with writing text (Imran and Almusharraf, 2023) and code (Barke et al., 2023). The steerability and usability of modern LLMs are underpinned by fine-tuning alignment through human feedback (e.g., preferences), which guides LLM outputs toward desirable properties such as helpfulness, informativeness, and safety (e.g., Askell et al., 2021; Ouyang et al., 2022b; Dang et al., 2024; Bai et al., 2022; Thoppilan et al., 2022; Nakano et al., 2021; Wang et al., 2024; Ahmadian et al., 2024). Despite being a cornerstone of modern AI research and development (R&D), mechanisms for sustainably sourcing and sharing human feedback data are still underdeveloped for at least three reasons.\nFirstly, many high-quality human preference datasets are proprietary and hence not available for others to study, reuse, or modify (Patel and Ahmad, 2023). This makes it difficult to infer best practices in the structure and type of preference data that leads to the largest gains in performance (MetaAI, 2024). Moreover, often the methodologies are not released, preventing even replication and further study (see Table \u00a71).\nSecondly, many barriers prevent sharing new datasets, in particular the cost and effort needed for extensive human annotation (Boubdir et al., 2023). For example, feedback collection often requires sophisticated user interface (UI) design (Singh et al., 2024) and lengthy ethics reviews. The costs further compound when specialist expertise is required (Li et al., 2024). The combination of demonstrable value and expensive collection results in dearth of open feedback datasets.\nThird, unlike many ongoing feedback collection efforts of frontier AI labs (Llama Team, 2024; Ouyang et al., 2022a), open feedback datasets are often treated as static collections of uniform preferences rather than living artifacts that are dynamically maintained and updated. There are growing efforts to circumvent the costs of dedicated feedback collection. Some widely-used datasets instead collate implicit feedback from existing platform interactions such as on Reddit (Stiennon et al., 2020) and Stack Overflow (Lambert et al., 2023); or an increasingly popular loophole is to simulate feedback using powerful closed models (Cui et al., 2023a; Taori et al., 2023; Aakanksha et al., 2024). However, distillation of one model's outputs to improve another carries legal, reproducibility, and transparency issues and adds a dependency on closed-model ecosystems. Comparatively, efforts that directly interface with real human-model interactions are rare. These efforts often base their success on hosting LLMs that users can freely use in exchange for chat logs (Zhao et al., 2024) or preference data (Zheng et al., 2023; Singh et al., 2024); though other mechanisms of exchange exist, including paid participation in large-scale human studies (Kirk et al., 2024; Aroyo et al., 2023), or mediator-enabled donation schemes where users retain ownership (Don-Yehiya et al., 2023). These contributions are already transforming the R&D of open models, but have known obstacles to sustainability such as depending on a small group of self-selected annotators (Zhao et al., 2024; Zheng et al., 2023) and being skewed towards bulk"}, {"title": "1.1 Defining Open Human Feedback", "content": "We begin by defining our scope, clarifying what we mean by open, human, and feedback. By human, we require some degree of human intervention and judgment to the extent that the feedback is not fully automatically generated, such as models simulating human feedback (Cui et al., 2023b; Agnew et al., 2024). This does not exclude aided or seeded feedback (e.g., see platforms \u00a74 and effort \u00a73.2).\nBy feedback, we mean human responses to model outputs that are ascribed with some positive or negative valence, i.e., a value judgement. Moreover, human feedback must comment on model outputs. For example, traditional labeled datasets are not a form of feedback, as they are annotated by humans over human generated text and hence contain no reaction element. Another edge case is \"wizard-of-oz\" datasets (K\u00f6pf et al., 2024), where humans pretend to be models, though these are rare.\nClear terminology surrounding \"open\" versus \"closed\" feedback is also required, especially given growing concerns about open-washing in \"open source AI\", despite substantial differences in transparency, accessibility, and governance structures (e.g. White et al., 2024; Liesenfeld and Dingemanse, 2024). We consider openness along five non-binary axes (see current datasets in Table \u00a71):\n1. Open methodology: Whether sufficient information is given to reproduce the dataset, at least partially (e.g., annotators details, guidelines and interfaces).\n2. Open access: Whether the final dataset is released for public access and use under a permissive license.\n3. Open model participation: Whether all feedback is collected from one predetermined LLM or if third-parties can upload and include their models."}, {"title": "2 Key Lessons from Peer Production and Open Source", "content": "Open feedback for LLMs can draw inspiration from the practices of peer production and OSS communities in promoting open, sustained, and diverse contributions. We provide an overview of these communities and highlight key takeaways from their success."}, {"title": "2.1 Peer Production", "content": "Peer production is a (knowledge) production model in which numerous individuals collaborate, often remotely, without relying on traditional hierarchical organization or financial compensation (Benkler, 2007). Peer production communities have effectively brought together diverse contributors (see \u00a73.4) to pursue their shared missions. For example, Wikipedians make 290k edits per day to create and maintain a free online encyclopedia (Halfaker and Geiger, 2020) and OpenStreetMap enthusiastic mappers maintain open digital maps all over the world (Palen et al., 2015).\nThe success of peer production projects relies on both intrinsically and extrinsically motivated contributors. For example, volunteers contribute to Wikipedia because they believe in the shared mission and fundamental value of free online encyclopedias for humanity (Bryant et al., 2005), but they may also contribute for other reasons such as intellectual stimulation and enjoyment (Balestra et al., 2017). Wikipedia's Barnstars (Kriplean et al., 2008) and Stack Overflow's reputation scores (Mamykina et al., 2011; Movshovitz-Attias et al., 2013) are examples that have fostered external motivations. Sometimes, extrinsic motivators can backfire; for example, psychological research notes that when extrinsic motivations are introduced existing intrinsic motivations may dissipate (Deci, 1971; Ryan and Deci, 2020).\nAnother factor contributing to the success and sustainability of peer production is the availability of platforms that provide the infrastructure, processes, and tools that support community-driven governance (Heltweg and Riehle, 2023). For example, Meta Stack Overflow allows the Stack Overflow communities to discuss policies and suggest new features (Fang et al., 2023). Wikipedia's infrastructure enables the creation of policies, guidelines, and essays for community governance (Butler et al., 2008). Flexible governance is especially critical, as each community has unique governance needs (Zuckerman and Rajendra-Nicolucci, 2023); even within Wikipedia, different language editions have unique rules (Hwang and Shaw, 2022). Community-driven governance is critical for sustained contribution and long-term community health. Open human feedback should follow suit in supporting bottom-up, community-driven governance for sustainably obtaining and moderating high-quality data (Kuo et al., 2024).\nForms of peer production in AI span a wide range of organization structures and focus areas. Some organizations are structured primarily around the production and exchange of knowledge on AI systems or data in specific linguistic and geographical contexts (Masakhane et al., 2020; Singh et al., 2024), category of model architectures (Peng et al., 2023), a specific model (Scao et al., 2022), or broader focus on values, such as accessibility and replicability (Biderman et al., 2023). These efforts have unique perspectives on collaboration, governance, code, data, and more (Ding et al., 2023). In most cases, however, explicit statements of values (Pistilli et al., 2023), governance processes for the project and artefacts (Hughes et al., 2023), and codes of conduct (Singh et al., 2024; Peng et al., 2023) play an important role in ensuring that the work benefits the targeted communities and, thus, motivates contributions."}, {"title": "2.2 Open source software", "content": "OSS concerns the development and distribution of software under open source licenses that allow anyone to inspect, use, modify, or redistribute the source code (OSI, 2007). This modus operandi has enabled countless research and innovation achievements, including in AI (Langenkamp and Yue, 2022; Osborne et al., 2024; Ding et al., 2023).\nOSS projects are developed by diverse contributors, driven by varied social, economic, and technological incentives (Feller and Fitzgerald, 2002; Bonaccorsi and Rossi, 2006). For individuals, intrinsic incentives, such as personal values, needs, or enjoyment, as well as extrinsic incentives, such as reputation, learning, and career benefits, are significant (Von Krogh et al., 2012; Lakhani and Wolf, 2005). Individuals' incentives vary based on their volunteer vs. paid status (Lakhani and Wolf, 2005), project governance (Shah, 2006), and their cultural norms (Subramanyam and Xia, 2008; Takhteyev, 2012).\nCompanies are chiefly driven by economic and technological incentives (Bonaccorsi and Rossi, 2006; Li et al., 2024), such as reducing R&D costs (Lindman et al., 2009; Birkinbine, 2020), shaping standards (Fink, 2003; Lerner and Tirole, 2002), practicing reciprocity (Osborne, 2024; Pitt et al., 2006), and recruiting talent (\u00c5gerfalk and Fitzgerald, 2008; West and Gallagher, 2006), among others.\nA key lesson from successful OSS projects is the importance of open governance and vendor-neutral hosting (e.g., by an independent consortium or foundation), which facilitate contributors with diverse incentives to build consensus (O'Mahony and Bechky, 2008) and collaborate on non-differentiating base-technologies (Germonprez et al., 2013). Similarly to peer production communities, OSS communities have created tools that enable open governance and collaboration, such as standard licenses, codes of conduct, and tools and metrics that can support monitoring and improving project and ecosystem health over time (Goggins et al., 2021). These practices can similarly ensure broad participation and sustained innovation in the open human feedback ecosystem."}, {"title": "3 Challenges and Opportunities of Realizing Open Human Feedback", "content": "We present seven themes that are central to the future of open human feedback. For each theme, we summarize why it is an important challenge, potential existing solutions, and recommendations for building a sustainable ecosystem for open human feedback."}, {"title": "3.1 Incentives", "content": "Aligned incentives are needed to motivate individuals and companies to participate in sharing or annotating open human feedback data. As per \u00a72, we must account and design for the diverse incentives of contributors, from individuals to companies.\nWith regards to individuals, the open human feedback ecosystem ought to accommodate both intrinsic and extrinsic incentives of diverse individuals, from volunteers to sponsored contributors. Intrinsic incentives may include learning and skill development, hobbyism, and community kinship, among others. For example, contributors from underrepresented socio-demographic and/or socio-linguistic groups might participate in open human feedback projects to advance AI resources and alignment for the interests and needs of their community (Singh et al., 2024; Pipatanakul et al., 2023). It is advised that projects establish governance structures (including a mission statement) that resonate with their community members' values. However, the \"burden\" of participation needs careful co-design to mitigate exploitation, benefiting solely those who build and commercialize AI systems (Kuo et al., 2024; Birhane et al., 2022; Sloane et al., 2022). For extrinsic incentives, projects could encourage participation via leaderboards, gamification points, or monetary compensation (Krishnamurthy and Tripathi, 2006).\nWith regards to companies, we must distinguish between frontier AI companies that would share human feedback data and companies that would contribute to open human feedback data projects. First, frontier AI companies can be incentivized to share human feedback collected via their proprietary chatbots by aligning data sharing with their strategic interests, such as reducing R&D costs (e.g., via crowdsourcing annotations) and building recognition in the open source AI community (e.g., via leaderboards). Examples of successfully spun-out OSS and open data projects may help temper concerns and illustrate the benefits of sharing data for open collaboration. In order to facilitate contributions by and collaboration among diverse companies to human feedback data projects, we call for such projects to be hosted by a non-profit, vendor-neutral organization (such as the Human Feedback Foundation), rather than by a single company, as a tried-and-tested enabler of collaboration between \"unexpected allies\" (O'Mahony and Bechky, 2008; Germonprez et al., 2013). Foundations with relevant expertise and missions include the Human Feedback Foundation, GenAI Commons, and the LF AI & Data Foundation."}, {"title": "3.2 Effort and Involvement", "content": "The success of the open human feedback ecosystem will rely on substantial human effort involving a range of tasks, such as gathering samples, annotating data, and writing alternative answers. Given the range, platforms intended for feedback collection should provide simple interfaces that reduce cognitive load and aid task completion (Chen et al., 2011; Ash et al., 2018), which is especially important to sustain volunteer and decentralized workflows. At the same time, feedback collection mechanisms should be be implemented at a centralized level in a dedicated platform (Boubdir et al., 2023). Most current feedback platforms use pairwise comparisons for ranking LLMs (Zheng et al., 2024), but these comparisons typically lack fine-grained assessment (\u00a7 3.4, 3.3). Moreover, feedback on these existing platforms tends to be of limited diversity and coverage. The paired back functionality of these hosted platforms attracts short conversations and relatively low topical, usecase and user diversity (Lin et al., 2024). The simplest way to reduce unnecessary effort is to make intuitive user interfaces for feedback platforms. Where possible, prompted feedback, such as users' rating or ranking of LLM responses, should be supplemented with feedback from the naturally-occurring cues in the chat (Hancock et al., 2019), such as when the user verbally thanks the model or edits their original prompt (Don-Yehiya et al., 2024)."}, {"title": "3.3 Expert Contributions", "content": "LLMs often perform poorly at tasks that require expert knowledge (Gougherty and Clipp, 2024), and specialized feedback from domains such as healthcare, legal, or finance is more challenging to acquire than general feedback. High-quality specialized feedback is unlikely to naturally occur through experts' use of LLMs, and hiring domain experts is costly. Nonetheless, expert feedback is necessary for evaluating model capabilities and guiding improvements, especially in complex or technical domains where layperson knowledge is not sufficient. Current efforts to collect expert feedback are limited in size and diversity, and/or gate-kept within private organizations (Pokrywka et al., 2024; Team, 2023). For example, GPQA (Rein et al., 2023) collects static expert contributions, while Bloomberg GPT relies on internal company data (Wu et al., 2023). Companies like OpenAI or DeepMind employ specialized AI trainers and keep the proprietary feedback 'in-house' (OpenAI, 2024). Replicating these efforts in open source communities faces challenges including high costs (GPQA costs ~$100/hr), scalability issues (Liu et al., 2023), and quality control. Moreover, these efforts risk domain bias; for example, Alpaca Eval overrepresents coding while neglecting theoretical mathematics (Lin et al., 2024). Successful initiatives for driving expert participation like Wikipedia editing and OSS development (see \u00a72) allow contributors to choose their tasks based on their interests and skill (Klie et al., 2023). To attract experts, platforms should provide tools for identifying topics requiring specialized input, including fact-checking and deliberation, and for routing these tasks to well-suited contributors. Different incentive mechanisms may appeal to experts, including contributing to the development of specialized models or public attribution and recognition for their contributions."}, {"title": "3.4 Linguistic and Cultural Diversity", "content": "LLMs struggle to serve diverse human demographics due to limited coverage in their general and feedback training data, which predominately relies on English speakers from narrow communities (Zhao et al., 2024; Zheng et al., 2023; Pavlick et al., 2014), and few annotators contribute the majority of data (K\u00f6pf et al., 2024), even in multilingual efforts Singh et al. (2024). Accordingly, culturally-specific queries and minority perspectives tend to be overlooked (Zhao et al., 2024). To build technologies inclusive of diverse socio-demographic and socio-linguistic groups (Seth et al., 2024; Aakanksha et al., 2024), it is crucial to go beyond \"convenience samples\" (Emerson, 2015) that rely solely on LLM developers, enthusiasts (Zheng et al., 2024), or company-employed annotators (Llama Team, 2024), and to gather feedback from underrepresented communities (Watts et al., 2024; Quaye et al., 2024). However, reaching different communities can be challenging due to factors such as lack of technology access (Tsatsou, 2011) or different device uses, such as the common use of mobile in the Global South (Avle et al., 2018).\nThere have been few recent attempts at covering demographic and geographic depth (Kirk et al., 2024; Singh et al., 2024), or multilingualism and dialect diversity (Singh et al., 2024; Lu et al., 2024; Aakanksha et al., 2024; Watts et al., 2024). Community-led engagement can return more power and control over data to the communities themselves (Kuo et al., 2024; Peters et al., 2018), but must be handled with care to avoid exploitative practices (Birhane et al., 2022; Sloane et al., 2022; Klie et al., 2023). Opening human feedback data to peer production provides the single most promising step for diverse participation. However, diversity is not guaranteed (\u00a72) and to facilitate it, platforms should be accessible, available in different languages, and be complemented by outreach to underrepresented groups to uncover their unfulfilled needs. Moreover, clear guidelines and regular audits (Santurkar et al., 2023) might reduce opinion biases and uncover representation gaps."}, {"title": "3.5 Adaptable and Dynamic Feedback", "content": "Human feedback differs from traditional dataset annotations: while resolving grammatical sentences or solving linear equations have a single, static ground truth; human preferences, especially those surrounding cultural issues, moral debates, and social conventions vary across individuals and groups (\u00a7 3.4) as well as over time (Pozzobon et al., 2023). As human preferences and views differ and evolve, model outputs will necessitate ongoing feedback collection and models trained on existing feedback may require feedback on new types of errors. This bears similarity to existing platforms for ongoing annotation. For example, Dynabench allows iterative model benchmarking (Kiela et al., 2021), LiveBench mines new examples (White et al., 2024), ChatBot Arena publicises new model releases (Zheng et al., 2024), and the ShareLM browser plug-in mediates ongoing user conversations on many platforms (Don-Yehiya et al., 2023).\nAcquiring dynamic data is challenging and costly, rendering traditional payment for annotation, as used in static datasets, unsuitable. Instead, to achieve continuous participation from the community, longitudinal incentives need to be designed and built into the platform (\u00a73.1) and new methods to be invented to flag and update outdated feedback. In time, the dated feedback itself might become useful, for example in comparison with the updated one."}, {"title": "3.6 Privacy and Data Protection", "content": "Sharing feedback can have wide reaching benefits, including for contributors, but equally could harm the contributors if their privacy is not adequately protected. Feedback platforms might also motivate more contributions by upholding data protection legislation and adopting best practices that enable contributors to have a say over how their data is shared. Clear mechanisms of opt-out can support these aims, for example, the \u201cAm I in The Stack?\u201d initiative associated with the BigCode Dataset, which allows code developers to see whether their work is included in the open source software training dataset, and provides a mechanism for them to request to opt out their data from future versions of the dataset (BigCode, 2024). Feedback platforms must comply with existing privacy regulations that standardize and enforce this social responsibility, such as GDPR (European Parliament and Council of the European Union), CCPA (Illman and Temple, 2019), and regulations for specific applications such as the Health (HIPPA, Act, 1996).\nIn addition to proper security protocols (Kumar et al., 2022), anonymization and de-identification are central considerations prior to collective data sharing. This includes removing personally identifiable information (PII), such as names, emails, and IP addresses (Zhao et al., 2024; Raji et al., 2020), which can be especially challenging when interleaved in a chat (Bender et al., 2021) and because PII definitions vary by application (Act, 1996). Furthermore, in feedback discussing specialized topics or rare dialects author identities might also be unavoidably compromised by the topic itself. Beyond anonymization, other privacy-enhancing techniques are also recommended by standard practices (e.g., NIST, 2020). Moreover, the text itself inherently shares information that for example could be crossed with other sources to deanonymize (Narayanan and Shmatikov, 2008). Thus, unless privacy leakage is theoretically restricted e.g. by noisy aggregation of the data (Dwork et al., 2006, 2014; Cummings et al., 2023) \u2013 then research should at least estimate the privacy leakage.\nGiving contributors control and autonomy over their personal information is vital for building trust and ensuring legal compliance. For example, allowing users to retract or edit their contributions, or providing a time window for complete removal of their data before feedback is made public (Don-Yehiya et al., 2023). Privacy must be balanced with providing a sense of ownership (\u00a73.7) as the two might be at odds. As heuristics like hashing names to match users to their content are susceptible to misuse, logins are often preferable (Hughes et al., 2023; Don-Yehiya et al., 2023). Regardless, any opt-out guarantees presents logistical challenges as to how these requests would propagate to any derivative models trained on the data, which would then need to remove specific details (Liu et al., 2022; Tran et al., 2024), known as 'machine unlearning' (Bourtoule et al., 2021; Lynch et al., 2024; Shi et al., 2024)."}, {"title": "3.7 Legal and Ownership", "content": "To ensure all contributions are appropriately licensed and available for public use, while mitigating the risk of legal challenges, robust data and legal governance frameworks must be established and concisely communicated to contributors. Particularly, we propose following best licensing practices from successful open-source data collection efforts where contributors retain ownership of the data they share. Notably, both humans and models are involved in data creation, but commonly the creative act is legally attributed to the human (Guadamuz, 2017), we hence consider the data solely owned by the human contributor. This stance has precedents (OpenAI, 2024), though the legal debate remains unsettled (Kop, 2020). We suggest contributors are asked to give informed revocable consent to share their data under a permissive content license, such as Creative Commons or Open Data License (Kim, 2007; Zheng et al., 2024), by signing an agreement (e.g., checking a box). This sets a transparent standard and reduces the fear of legal retribution for both the data collection platform and consumers of the data (Bonatti et al., 2017). Moreover, we suggest that those collecting feedback (e.g., platform providers) submit data card documentation (Gebru et al., 2021; Shimorina and Belz, 2021; Pushkarna et al., 2022), explaining context, methodology, intended applications, and limitations."}, {"title": "4 Visions of Successful and Sustainable Open Feedback Ecosystems", "content": "Building on these challenges (see Summary), we conclude by considering the tangible components that are necessary for building an ecosystem of open human feedback under the mission: Humanity guiding open AI for humanity.\nAny solution requires building an open feedback platform-sites or software that allow the community to dynamically contribute in a standardized and efficient way. Platform designs often shape standards for the community, and careful consideration is required to ensure they can facilitate efficient distribution of work (\u00a73.2), are accessible and easy to use by diverse contributors (\u00a73.4), enable governance and moderation of feedback (\u00a72.1), and encourage participation (\u00a73.1). Beyond our current suggestions, such platforms should be open source and will evolve in line with changing feedback requirements, such as feedback types, biases, and privacy issues. While current platforms, especially Hugging Face's Spaces, have lowered the entry barrier for machine learning model experimentation, they lack detailed systematic feedback mechanisms. Industry-centered platforms, like ScaleAI, offer feedback platforms that support internal workforces to continuously refine models on datasets (Iren and Bilgen, 2014) but often suffer from biases due to limited diversity and specificity of crowdworker knowledge (\u00a73.4 Hettiachchi et al., 2021; Barbosa and Chen, 2019). Argilla represents the closest open source solution for feedback collection, yet it still lacks support for participants to discuss feedback, share incentives, or collectively govern projects."}, {"title": "5 Conclusion", "content": "Human feedback has emerged as a central component for improving the capabilities and safety of AI models. However, human feedback data is concentrated within a few frontier AI companies, and there is currently no ecosystem for sharing and collaborating on such data. We have outlined our vision and key components required for a dynamic and sustainable open human feedback ecosystem: well-designed feedback platforms, aligned incentives for contributors, and feedback loops that can support specialized and general-purpose models. However, significant challenges remain: building and hosting feedback loops for specialized models, incentivizing diverse and quality contributions, protecting privacy, and managing governance and data ownership. Despite these hurdles, we call for recognition of open human feedback data as a critical, yet underdeveloped, ingredient for open and transparent AI R&D, and propose a pathway forward."}, {"title": "Actions for Building the Open Human Feedback Ecosystem", "content": "State a clear mission that resonates with community values\nProposed: Humanity guiding open AI for humanity\nIncentivize companies to share open human feedback data\nHost projects in a vendor-neutral organization to facilitate participation\nDesign for intrinsic and extrinsic incentives of diverse contributors\nDesign interfaces that reduce contribution barriers (e.g., time, effort, etc.)\nDevelop efficient approaches to use, sample, and augment existing data\nExtract feedback from natural text in chats\nAllow the community to define feedback requirements independently\nAllow experts to choose what and how to contribute independently\nFacilitate peer-produced contributions to human feedback data projects\nEncourage participation from diverse socio-demographic & -linguistic groups\nMitigate exploitation through responsible use and contribution policies\nProvide accessible and multilingual platform interface and tools\nAudit collected data for potential biases and discriminatory content\nDesign for feedback contestation and consensus-building among contributors\nPool data into one location\nMake dedicated efforts for specialized data annotations\nCreate platforms, mediators, and tools for chat & feedback data sharing\nEncourage and enable the sharing of updated feedback data\nFollow security best practices and comply with relevant regulations\nMap users to their contributions privately (e.g., via logins)\nDevelop methods to give users control over their personal information\nAnonymize data and remove private information\nShare datasets with appropriate documentation (e.g., data card)\nEnsure informed revocable consent for data sharing (e.g., via box-checking)\nUsers share data with permissive licenses (e.g., Creative Commons license)"}]}