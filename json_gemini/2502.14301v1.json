{"title": "SEA-HELM:\nSoutheast Asian Holistic Evaluation of Language Models", "authors": ["Yosephine Susanto", "Adithya Venkatadri Hulagadri", "Jann Railey Montalan", "Jian Gang Ngui", "Xian Bin Yong", "Weiqi Leong", "Hamsawardhini Rengarajan", "Peerat Limkonchotiwat", "Yifan Mai", "William Chandra Tjhi"], "abstract": "With the rapid emergence of novel capabilities\nin Large Language Models (LLMs), the need\nfor rigorous multilingual and multicultural\nbenchmarks that are integrated has become\nmore pronounced. Though existing LLM\nbenchmarks are capable of evaluating specific\ncapabilities of LLMs in English as well as\nin various mid- to low-resource languages,\nincluding those in the Southeast Asian\n(SEA) region, a comprehensive and authentic\nevaluation suite for the SEA languages has\nnot been developed thus far. Here, we\npresent SEA-HELM, a holistic linguistic\nand cultural LLM evaluation suite that\nemphasizes SEA languages, comprising five\ncore pillars: (1) NLP CLASSICS, (2)\nLLM-SPECIFICS, (3) SEA LINGUISTICS, (4)\nSEA CULTURE, (5) SAFETY. SEA-HELM\ncurrently supports Filipino, Indonesian, Tamil,\nThai, and Vietnamese. We also introduce\nthe SEA-HELM leaderboard, which allows\nusers to understand models' multilingual and\nmulticultural performance in a systematic and\nuser-friendly manner.", "sections": [{"title": "1 Introduction", "content": "The proliferation of generative approaches to\nnatural language processing (NLP) through\nLarge Language Models (LLMs) has rendered\nmany traditional datasets for NLP evaluation\ncompromised (Haimes et al., 2024), obsolete\nor saturated (Liu et al., 2024). While\nessentially trained to predict the next token\nin a sequence, LLMs have shown significant\nemergent competencies, including summarisation,\nquestion answering, translation, coding, and\nadvanced reasoning (Brown et al., 2020; Yeo\net al., 2024). They are also extensively used\nfor new applications, such as chatbots that can\nhold sustained open-ended conversations (Dam\net al., 2024). This advancement has led to a\nsignificant disparity between the range of LLM\ncapabilities and the datasets and frameworks to\nevaluate them rigorously. Traditional approaches\nto NLP evaluation, which emphasise on alignment\nwith a predefined ground truth reference, are\nnot sufficient in measuring the complex abilities\nof LLMs. This gap is further exacerbated in\nlower-resource languages in SouthEast Asia (SEA),\nowing to a lack of both training and testing data on\nthe internet (Li et al., 2024).\nThe SEA region is home to nearly 700 million\nspeakers across more than 1,000 languages (see\nAppendix H.20). While this region represents\nalmost 10% of the global population and constitutes\napproximately one-seventh of the world's total\nlanguages, most of these languages remain\nunsupported by major LLMs such as Mistral\n(Mistral AI, 2023) and Claude (Anthropic, 2023).\nThe complications arising from a lack of data as\nwell as uneven digital access and representation\ncontribute to the impeded development of LLMs\nin those languages in the SEA region. This\nmatter is further aggravated by the fact that some\nSEA languages are written in non-Latin scripts,\nwhich presents a challenge for tokenizers when\nprocessing limited data.\nDespite the mentioned obstacles, multilingual\nLLM and benchmark development in SEA strive\nto close the gap and adapt to the current trends in\nthe field. Some models now explicitly support SEA\nlanguages and also claim to provide representation\nfor SEA cultural knowledge (Sailor2 Team, 2024;\nZhang et al., 2024b; Bai et al., 2023; Dang et al.,\n2024). There have also been many benchmarks\nclaiming to measure LLMs' multilingual and\nmulticultural capabilities for the SEA region\n(Nguyen et al., 2024; Zhang et al., 2024a; Wang\net al., 2023; DAMO-NLP-SG, 2024; Singh et al.,\n2024; CohereForAI, 2024; Lovenia et al., 2024).\nHowever, there has yet to be a holistic and authentic\nbenchmark suite for evaluating LLMs for SEA\ncultural and linguistic competencies.\nWe have thus developed SEA-HELM\n(SouthEast Asian Holistic Evaluation of Language\nModels), a systematic, integrated, and continually\nmaintained benchmark suite which aims to measure\nthe SEA language and cultural competencies of\nLLMs in a targeted and comprehensive manner.\nSEA-HELM achieves integration by collating\nlocalised evaluation datasets and LLM prompts,\nrunning tests on models together to enable\nstandardised comparisons, and presenting results\naggregated by languages, tasks and models. It is\nour view that no single metric explains a model's\nsuitability for SEA, and thus SEA-HELM is\ndesigned to test a holistic set of competencies,\nillustrated in Figure 1.\nSpecifically, SEA-HELM is organised into\nfive evaluation pillars: (1) NLP CLASSICS; (2)\nLLM-SPECIFICS; (3) SEA LINGUISTICS; (4)\nSEA CULTURE; and (5) SAFETY, which together\nencompass an extensive range of tasks for each\nSEA language to ensure that a wide range of\nrelevant aspects, from linguistic nuances to cultural\nrepresentations, are considered and accounted for.\nThe five pillars are also meticulously and rigorously\ncrafted to achieve fair, transparent, and authentic\nmultilingual and multicultural evaluations of LLMs\nin the region. We deliberately incorporate\ncommunity participation by involving native\nspeakers of the SEA languages at each stage of\nthe dataset planning and construction to ensure\nlinguistic accuracy and cultural authenticity.\nWe summarise the contributions of SEA-HELM\nhere:\n\u2022 SEA-HELM is a curated suite of SEA datasets\nwhich are evaluated together and whose\nresults will be presented on a publicly visible\nleaderboard.\n\u2022 SEA-HELM (a) natively adapted and translated\nexisting English safety and NLP tasks into\nFilipino, Indonesian, Tamil, Thai and\nVietnamese; (b) created the human-translated\nSEA-IFEval and SEA-MTBench datasets; and\n(c) created LLM prompt templates that are\nconsistent across tasks and languages.\n\u2022 SEA-HELM includes new datasets we\ndeveloped for granular linguistic diagnostics\n(LINDSEA) in Indonesian and Tamil.\nSEA-HELM also includes a cultural evaluation\ndataset for Filipino developed in collaboration\nwith community members from the Philippines,\nwhich resulted in KALAHI (Montalan et al.,\n2024).\nPut together with the other datasets included\nin SEA-HELM, we believe that these make\nSEA-HELM an accurate and authentic evaluation\nsuite for SEA languages to date. It can be used as\na base for future extensions covering other SEA\nlanguages, e.g. Khmer, Lao, Burmese, etc., which\nwe will explore soon in the next iteration."}, {"title": "2 Related work", "content": "Over the years, AI practitioners have employed\neither an individual task-based or, more rarely,\na holistic approach to assess the performance\nand capabilities of LLMs. Popular tasks for\nevaluating LLMs include translation (Hendy\net al., 2023), summarisation (Zhang et al., 2023),\ndecision-making (Shen et al., 2023), detecting\nscalar implicatures (Jereti\u010d et al., 2020; Pandia\net al., 2021; Hu et al., 2023; Liu et al., 2023) as\nwell as presuppositions (Jereti\u010d et al., 2020; Parrish\net al., 2021). Additionally, linguistic (Warstadt\net al., 2020; Xiang et al., 2021; Someya and Oseki,\n2023) and cultural representation (Durmus et al.,"}, {"title": "3 SEA-HELM", "content": "To address the lack of holistic multilingual and\nmulticultural evaluations for the SEA region, we\ndesigned and developed SEA-HELM, which draws\nits inspiration from HELM (Liang et al., 2022).\nThis evaluation suite consists of five core pillars:\n(1) NLP CLASSICS, (2) LLM-SPECIFICS, (3)\nSEA LINGUISTICS, (4) SEA CULTURE, and (5)\nSAFETY, and has been recently integrated with\nHELM. The spread of tasks and languages is\ndetailed in Table 1. SEA-HELM currently supports\nfive SEA languages \u2013 Filipino, Indonesian, Tamil,\nThai, and Vietnamese, enabling users and AI\npractitioners to assess the overall performance of\nthe LLMs for these languages."}, {"title": "3.1 Core pillars", "content": "First, for the Natural Language Understanding\n(NLU) competency, we include QA (extractive\nquestion answering) and sentiment analysis tasks.\nSecond, for the Natural Language Generation\n(NLG) competency, we include translation (English\nto native language and native language to English)\nand abstractive summarisation tasks. Third, for the\nNatural Language Reasoning (NLR) competency,\nwe include causal reasoning and natural language\ninference (NLI) tasks.\nWe selected datasets that comprised of data\noriginally written in the native language as far as\npossible. Otherwise, existing datasets in English\nwere carefully translated by native speakers. This\nis important because translated datasets often\ncontain elements of translationese (Gellerstam,\n1986), which can differ significantly from natively\nwritten text (Baker, 1993; Lembersky et al., 2012;\nVolansky et al., 2015; Riley et al., 2020)."}, {"title": "3.1.2 LLM-specifics", "content": "With LLMs enabling unprecedented NLP\napplications, there is a need to develop automated,\ndedicated evaluation metrics for these higher-order\ntasks. SEA-HELM focuses on two specific\ncapabilities the ability to follow human\ninstructions specifying a particular format\nexpected in the given responses, and the ability to\nhold human-like conversations. The former can be\nevaluated using simpler rule-based checkers that\nexamine the format of the LLM's responses, while\nthe latter requires us to model subjective human\npreferences, and thus employs the LLM-as-a-judge\nparadigm.\nSEA-IFEval is an\ninstruction-following\nbenchmark we created collaboratively with native\nspeakers. It is manually translated from the\nEnglish IF-Eval benchmark (Zhou et al., 2023)\nand, crucially, localised to fit the linguistic and\ncultural nuances of SEA languages. Manual\ntranslations ensure faithful and accurate linguistic\nrepresentation, while localisation ensures cultural\nauthenticity and removes any unintended or\ninherent biases. This involved manually verifying\nthat each sample is relevant and applicable to the\nlanguages concerned.\nSpecifically, to create the SEA-IFEval dataset,\nwe first filtered out instructions that could\nnot reasonably apply to most SEA languages.\nFor example, prompts asking to change the\ncapitalisation or punctuation do not make sense\nin many scripts in the region, such as Burmese,\nTamil, or Thai. We also changed instructions that\nrequired a certain frequency of letters to require\na certain frequency of numbers as it was not easy\nto localise them for non-Latin scripts. Thus, by\nfiltering out and adapting instructions given the\nSEA context, we ensure a fair basis of comparison\nfor the instruction-following competency. The final\ncategories included in the SEA-IFEval dataset are\nlisted in Table C.12 in Appendix C. The accuracy\nwith which the LLM follows the exact instruction\nfollowing requirements is calculated. The model's\naccuracy is then adjusted to penalise following\ninstructions but responding in the wrong language\nby multiplying its accuracy with the rate at which\nit responds in the correct target language.\nSEA-MTBench is a manually translated and\nlocalised version of the popular MT-Bench dataset\n(Zheng et al., 2023), which also introduced the\nparadigm of LLM-as-a-Judge to approximate\nhuman preferences. We chose the reference-guided\ngrading approach, where we compare the win-rate\nof each candidate model against a fixed reference\nmodel, namely gpt-3.5-turbo-0125. The models'\nresponses were compared with the reference\nresponse using gpt-4-1106-preview as the judge\nmodel. This setup sees the number of judge calls\nscale linearly with the number of models being\ncompared, whereas pairwise comparisons would\nscale quadratically."}, {"title": "3.1.3 SEA linguistics", "content": "As one of the five core evaluation pillars,\nLINDSEA (LINguistic Diagnostics for\nSouthEast Asian languages) is a high\nquality, manually-crafted linguistic dataset\nthat systematically diagnoses models' language\npoficiency and grammatical understanding based\non a granular taxonomy of syntactic, semantic and\npragmatic phenomena. It is also the first to be\ncreated for SEA languages. LINDSEA provides\nfne-grained evaluation of a model's linguistic\nabilities, akin to the diagnostic dataset of GLUE\n(Wang et al., 2018) and BLiMP (Warstadt et al.,\n2020), the linguistic diagnostic dataset for HELM.\nThe design of LINDSEA is based on three\nprinciples: breadth, depth, and quality. Given the\nincreasingly complex tasks that LLMs are expected\nto perform and the importance of both natural\nlanguage input and output in users' interactions\nwith LLMs, it is crucial that we are able to\ncomprehensively evaluate and quantify models'\nunderstanding of the myriad aspects of language.\nTo that end, LINDSEA is designed to cover a\nwide gamut of linguistic phenomena (breadth).\nIn designing LINDSEA to have sufficient linguistic\ncoverage, we also conducted an extensive survey of\nthe literature on linguistic phenomena in our target\nlanguages and used our findings to taxonomise\neach linguistic phenomenon to have multiple\ncategories and subcategories for more fine-grained\nanalyses (depth). That is, rather than using\na small set of lexical items and grammatical\nrules to automatically generate large numbers\nof test sentences, the examples in LINDSEA\nare handcrafted from scratch by linguists in\ncollaboration with native speakers and reviewed\niteratively to ensure that they sound natural, are\nsemantically coherent and target the relevant\nphenomena effectively (quality).\nWhile there are existing syntactic and semantic\ndiagnostic datasets for English (Warstadt et al.,\n2020; Jereti\u010d et al., 2020; Liu et al., 2023),\nMandarin (Xiang et al., 2021) and even Japanese\n(Someya and Oseki, 2023), none yet exist for SEA\nlanguages, and, to our knowledge, there has yet\nto be such an extensive coverage of linguistic\nphenomena in any dataset. More details about the\nindividual subcategories and literature reviewed\ncan be found in Appendix G.19."}, {"title": "3.1.4 SEA culture", "content": "Cultural representation and bias have also become\nincreasingly important with LLM use since a lack\nthereof can potentially cause social harm (Solaiman\net al., 2023). The gravity of the risks involved\nhas prompted multiple studies in this area (Naous\net al., 2023; Ramesh et al., 2023; Ramezani and\nXu, 2023).\nMuch prior work on analysing or evaluating\ncultural representation in LLMs demonstrates what\nwe call a \"top-down\" approach, which we define as\na data collection and/or data creation strategy that\nmainly relies on reference sources that aggregate\ncultural knowledge, opinions, and values at a\npopulation level. This approach also demonstrates\na notable lack of emphasis on the perspectives,\ndecisions, and actions that individuals take as they\nparticipate within their communities or navigate\ntheir daily lives.\nFor example, Durmus et al. (2023) frame the\nevaluation of cultural representation as determining\nwhether models' exhibited values aligned with\nthose of people from different countries, as\nextracted from large-scale surveys, such as the\nWorld Values Survey and the Pew Global\nAttitudes Survey. Such top-down value extraction\ncan involve subject matters that are primarily\ndetermined by people who are not members of the\npopulation to whom they are administered, and thus\nmay not be fully representative of the community's\nconcerns and lived experiences.\nThere are also top-down SEA efforts, such as\nSeaEval (Wang et al., 2023) as well as SeaExam\nand SeaBench (Liu et al., 2025), that primarily\nserve as tests for factuality and general local\nknowledge. Such top-down evaluation datasets\nare inherently limited and cannot comprehensively\nrepresent the multifaceted and complex nature\nof culture (Causadias, 2020), even if they can\ncapture aggregated value alignment. Thus, we\nplace emphasis on a strong participatory approach\nthat includes the native speaker communities in\norder to authentically represent the target culture.\nHershcovich et al. (2022) suggest that a culture\ncan be defined by its shared cultural common\nground or a shared body of knowledge within the\ncommunity, while Swidler (1986) proposes that\nculture is expressed in the strategies of action or\n\"toolkit\" that people use to navigate their personal\nand social lives. Thus, to probe models for\ntheir understanding of cultural knowledge and\nto evaluate if models can appropriately apply\ncultural knowledge or values, we have included\nKALAHI (Montalan et al., 2024), which we\ndeveloped using a participatory approach, under\nthe SEA Culture pillar. The KALAHI dataset\nis designed to determine if LLMs can provide\nculturally-relevant responses to culturally-specific\nsituations that Filipino people can reasonably\nencounter in their daily lives, and the dataset is\ncomposed of 150 high-quality prompts created in\ncollaboration with native Filipino speakers (see\nTable E.17 in Appendix E for an example of its\nimplementation in SEA-HELM)."}, {"title": "3.1.5 Safety", "content": "Multilingual inputs, especially when given in\nlower-resource languages such as those in the SEA\nregion, can increase the likelihood that LLMs\nproduce unsafe responses (Song et al., 2024; Shen\net al., 2024). Thus, tailoring safety benchmarks\nthat cater to SEA languages and cultures is critical\nin ensuring that users who interact with models\nin those languages are protected from harmful and\nunsafe outputs (e.g. hate speech). In this regard, we\nalso aim to foster and enhance representativeness\nand inclusivity by curating datasets that are relevant\nto SEA languages. After a thorough survey of\nthe available datasets similar to the one performed\nin section 3.1.1, we decided to include toxicity\ndetection as the first task (of many planned for the\nfuture) under the SAFETY pillar. Currently, we\ncover Indonesian, Thai, Vietnamese, and Filipino\nfor this task. This serves as the starting point that\nwill lead to more complete and comprehensive SEA\nsafety benchmarks in SEA-HELM."}, {"title": "3.2 SEA-HELM leaderboard", "content": "Given the large number of tasks in SEA-HELM,\nit can be challenging to determine the overall\nperformance of a given model. There is a need\nto aggregate task scores. We believe that the\naggregated scores should be presented in a clear\nand transparent manner that allows users and\ndevelopers to delve deeper into each aggregation\nin a maximally informative manner as well. To\nfacilitate this, we have released the SEA-HELM\nleaderboard as part of the SEA-HELM suite.\nThe leaderboard presents the results in three\nseparate views an overall view containing the\nSEA average as well as the overall language scores,\na language view showing the average competency\nscores for that language, and a detailed view\ncontaining the normalised scores for each task\n(Tables A.1, A.2, A.3, and A.4). Additionally, it\nincludes the results from both the pre-trained and\nthe instruction-tuned variants of each model across\na wide range of model sizes. See Appendix A\nfor more examples of results visualisations in the\nleaderboard."}, {"title": "3.3 Evaluation details", "content": "For each task, we opted to be explicit in instructing\nthe LLM to output the answer following a specified\nformat. This is done by prompting the model\nto return its response with an answer tag. Table\nB.6 shows an example of such a prompt and\nthe expected generation from the model (see\nAppendices B to F for all prompts used in\nSEA-HELM), and note that the prompt explicitly\nstates that the answer tag Jawaban (which is the\nIndonesian word for 'answer') must be prefixed\nto its answer (see Appendix B). The answer can\nthen be extracted using regular expressions and\ncompared against the corresponding gold label.\nCrucially, if the model fails to append the answer\ntag to its answer, the model is deemed to have\ngiven a null response. This approach allows for\nmore efficient automatic evaluation at scale, even\nfor models that may tend to have lengthy outputs.\nIt should also be noted that each prompt is\ngiven in the target language rather than in English\n(except for the English tasks) and is manually\ntranslated by native speakers of each language.\nIn our view, for full SEA languages support, an\nLLM should be able to output coherent responses\nand should also be able to correctly interpret\nnative instructions. Instruction-tuned models were\nevaluated in a zero-shot manner while pre-trained\nmodels are evaluated with 5-shot examples."}, {"title": "3.3.2 Aggregation of metrics", "content": "We chose to aggregate the metrics by first assigning\neach task to one of the various competencies\ndefined in Section 3.1. We then normalised each\nmetric score to account for the random baseline\nscore and rescaled it to a range of 0-100 (Hulagadri\net al., 2025). The normalised scores within each\ncompetency are then averaged. For each language,\nwe take the average of the competencies to give\nan overall language score. Finally, a SEA average\nscore is calculated as the mean of all the supported\nlanguages' scores."}, {"title": "3.4 Evaluation of LLMs with SEA languages\nsupport", "content": "We used SEA-HELM to perform a comprehensive\nevaluation of several models that have some level\nof proficiency in the SEA languages (Figure\n2a). Examples of such models include the\nSailor family (Sailor2 Team, 2024), SeaLLMs\nfamily (Zhang et al., 2024b) and the SEA-LION\nfamily of models. Of all the open-sourced\nmodels tested, DeepSeek-R1 was the strongest\nperforming model and showed comparable results\nto gpt-4o-2024-08-06 on the SEA-HELM suite\n(Figure 2a, Table A.1). Given the large size\nof DeepSeek-R1 (671B), finding comparisons to\nother models that support SEA languages is not\ntrivial. Thus, we chose to focus our discussion on\nLLMs of sizes ranging between 7-9B parameters\n(See Table A.1 for the full list of models) and\nuse both the DeepSeek-R1 and gpt-4o-2024-08-06\nas the reference models for what is the best\navailable open-sourced and closed-sourced model\nrespectively.\nWe also observed that the gap between the\nsmaller models and the reference models was\nclosing, as evidenced by the smaller gap between\ngpt-4o-2024-08-06 with the newer LLaMA model\nLlama-3.1-8B-Instruct as compared to the earlier\nLLaMA model Meta-Llama-3-8B-Instruct (Figure\n2a, 2b, Table A.1). Notably, continued pre-training\nand further tuning for the SEA languages\nresulted in this gap closing even further (Figure\n2b; llama3.1-8b-cpt-sea-lionv3-instruct compared\nagainst Llama-3.1-8B-Instruct). This suggests that\nthere is still room for improvement and spending\nthe effort to train LLMs to target the specific SEA\nlanguages can result in models that are more suited\nfor the SEA region and thus work better for the\nregion's use cases.\nThe choice of model family was also important.\nThe Gemma2 family of models (gemma-2-9b-it)\nand the continued pre-trained/fine-tuned model\n(gemma2-9b-cpt-sea-lionv3-instruct) performed\nthe best of all the evaluated models. One possible\nexplanation could be the choice of tokenizer (256k\nvocabulary size) which was shown to be associated\nwith better downstream performance for most\nlanguages, especially in multilingual settings (Ali\net al., 2024).\nOur SEA-HELM suite reveals that the\nperformance of the LLMs for Tamil and Filipino\nwere relatively poor (Figure 2a, Table A.1). The\nexceptions were the Gemma2-based models and\nllama3.1-8b-cpt-sea-lionv3-instruct. This was\nlikely due to the lack of support for these languages\nin many of these models and indicates the limited\ncapabilities of LLMs in these lower-resource\nlanguages. As mentioned previously, supporting\nthese languages through additional training\ncan improve the capabilities of LLMs in these\nlanguages.\nAdditionally, Figure 2c illustrates why it\nis necessary to adopt a holistic approach\nthat evaluates models' competencies under\nthe five pillars. As a first example, note\nthat Meta-Llama-3-8B-Instruct's performance\nfor SEA-IFEval is much lower (relatively\nspeaking) as compared to LINDSEA and\nSEA-MTBench. This indicates that while the\nmodel exhibits linguistic understanding and has\nsome LLM-specific capabilities, it is still lacking\nin native instruction-following competency. As\nanother example, Sailor2-8B-Chat is observed to\nperform exceptionally well for SEA-MTBench,\nwhich is indicative of more coherent and relevant\nnative multi-turn competency, but has much poorer\nperformance for LINDSEA and SEA-IFEval,\nwhich indicates that it is much weaker in terms\nof linguistic understanding as well as native\ninstruction following."}, {"title": "4 Conclusion", "content": "In conclusion, we introduce SEA-HELM, a holistic\nevaluation suite that caters to the SEA languages\nand cultures. To achieve a comprehensive\nevaluation suite, SEA-HELM is designed around\nthe following five core pillars: (1) NLP CLASSICS,\n(2) LLM-SPECIFICS, (3) SEA LINGUISTICS, (4)\nSEA CULTURE, (5) SAFETY.\nAdditionally, the SEA-HELM leaderboard is\nintended to serve as a comprehensive and regularly\nmaintained resource for AI researchers in academia\nand industry. Our results show that there are still\nsignificant gaps between high-resource languages\nsuch as English and the mid- to low-resource\nlanguages in Southeast Asia, including several\nwith official status and millions of speakers.\nHowever, these results also show that the dedicated\nfine-tuning of LLMs with parameter sizes between\n7 and 9 billion can significantly narrow the gap with\nrespect to the much larger state-of-the-art models\nsuch as GPT-4o and DeepSeek-R1. Thus, by calling\nattention to the realistic, localised needs of SEA\nlanguages, we encourage more concentrated efforts\non data collection, curation and fine-tuning of\ndedicated lighter-weight LLM solutions for the\nregion."}, {"title": "5 Future work", "content": "We recognise that although SEA-HELM currently\ncovers Filipino, Indonesian, Tamil, Thai, and\nVietnamese, we still have much more work to\ndo and therefore we are committed to iteratively\nexpanding each pillar.\nSpecifically, we plan to expand SEA-HELM to\ninclude a broader range of languages, cultures,\ntasks and models to encourage stronger SEA\nrepresentation in models. We also seek to\nexplore additional factors such as automatic LLM\nevaluations. This will enable an even more\ncomprehensive evaluation of LLM performance\nacross a wider variety of contexts for SEA\nlanguages, especially for low-resource languages\nin the region such as Burmese, Khmer and Lao."}, {"title": "Limitations", "content": "While aiming to achieve holistic and authentic\nevaluations for LLMs in Southeast Asia,\nSEA-HELM is not yet exhaustive in its coverage\nof languages and tasks. We plan to have future\niterative expansions for better coverage.\nOur leaderboard results are based on single\nmodel runs. However, as we have assumed\ndeterministic model behaviour and set every\nmodel's temperature to 0, we did not publish error\nbars for the results, in line with other benchmarks\nin the field.\nUnder the safety pillar of SEA-HELM, we also\nacknowledge that passing our evaluations with a\nhigh score is not necessarily a guarantee of the\nsafety of an LLM in the SEA context, as it is\nnot possible to enumerate every type of unsafe\nresponse in real-world scenarios. Thus, passing\nthe safety evaluations in SEA-HELM must be seen\nas a necessary but not sufficient requirement for\nensuring safety in real-world LLM applications."}, {"title": "Ethics Statement", "content": "SEA-HELM is grateful to our Quality Assurance\n(QA) team, consisting of paid native speakers\nof SEA languages who worked as translators\nand annotators, enabling us to construct localised\ndatasets needed for this research. The SEA-HELM\nproject has received full and official approval\nfrom our university's internal review board after\nundergoing a rigorous review process, and the\ncompensation and working hours for all members\ninvolved were established in full compliance with\nthe prevailing university guidelines and applicable\nregulations in the country where this research is\nconducted.\nOur QA team was recruited through public\nadvertisements that clearly outlined the estimated\nworkload and hourly pay, consistent with all\nrelevant legal and regulatory requirements. The\nteam is composed predominantly of students at\nlocal universities and members of the public, all\nof whom are adults who satisfy the legal age\nrequirements for employment, as defined by the\nlabour laws of the country. Although participants\nare compensated for their participation, their\ninvolvement in the study is entirely voluntary. Any\npersonally identifiable information (PII) is removed\nfrom the data collected and will not be tied to their\nidentity.\nWe did not estimate that their work involved\nsignificant risks of exposure to offensive material,\nas they were not involved in the construction of\nsensitive data such as those under the SAFETY\npillar. Nonetheless, they were encouraged to\nreport inappropriate samples and were given the\noption to withdraw their participation at any time,\nincluding after its completion, without any negative\nconsequences or loss of benefits. If they chose to\nwithdraw, they could do so without providing any\nreason, and all data collected from the withdrawn\nindividual were excluded from this research.\nWe do not foresee negative social impacts from\nour research, for our research introduces evaluation\ndatasets in SEA languages by working with native\nspeakers, paying due respect to local cultural\nsensitivities. We thus do not believe that our\nresearch will contribute to over-generalisations\nregarding SEA cultures."}]}