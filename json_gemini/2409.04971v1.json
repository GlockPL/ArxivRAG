{"title": "Soft Actor-Critic with Beta Policy via Implicit Reparameterization Gradients", "authors": ["Luca Della Libera"], "abstract": "Recent advances in deep reinforcement learning have achieved impressive results in a wide range of complex tasks, but poor sample efficiency remains a major obstacle to real-world deployment. Soft actor-critic (SAC) mitigates this problem by combining stochastic policy optimization and off-policy learning, but its applicability is restricted to distributions whose gradients can be computed through the reparameterization trick. This limitation excludes several important examples such as the beta distribution, which was shown to improve the convergence rate of actor-critic algorithms in high-dimensional continuous control problems thanks to its bounded support. To address this issue, we investigate the use of implicit reparameterization, a powerful technique that extends the class of reparameterizable distributions. In particular, we use implicit reparameterization gradients to train SAC with the beta policy on simulated robot locomotion environments and compare its performance with common baselines. Experimental results show that the beta policy is a viable alternative, as it outperforms the normal policy and is on par with the squashed normal policy, which is the go-to choice for SAC. The code is available at https://github.com/lucadellalib/sac-beta.", "sections": [{"title": "1 Introduction", "content": "In recent years, we have witnessed impressive advances in deep reinforcement learning, with successful applications on a wide range of complex tasks, from playing games Mnih et al. [2013], Silver et al. [2016] to high-dimensional continuous control Schulman et al. [2016]. However, the poor sample efficiency of deep reinforcement learning is a major obstacle to its use in real-world domains, especially in tasks such as robotic manipulation that require interaction without the safety net of simulation. The high risk of damaging and the costs associated with physical robot experimentation make it crucial for the learning process to be sample efficient. As a result, it is of paramount importance to develop algorithms that can learn from limited data and generalize effectively to new situations. To address this issue, Haarnoja et al. [2018] introduced soft actor-critic (SAC), an off-policy model-free algorithm that bridges the gap between stochastic policy optimization and off-policy learning, achieving state-of-the-art on the popular multi-joint dynamics with contact (MuJoCo) Todorov et al. [2012] benchmark suite. Based on the maximum entropy framework, SAC aims to maximize a trade-off between the expected return and the entropy of the policy, i.e. to accomplish the task while acting as randomly as possible. As a consequence, the agent is encouraged to explore more widely, and to assign identical probability mass to actions that are equally attractive. A key step of the algorithm, necessary for computing the entropy-augmented objective function, is sampling an action from the current policy in a differentiable manner, which is accomplished through the use of the reparameterization trick. However, certain distributions such as gamma, beta, Dirichlet and von Mises cannot be used with the reparameterization trick. Hence, the applicability of SAC to practical problems that could benefit from the injection of prior knowledge through an appropriate"}, {"title": "2 Background", "content": ""}, {"title": "2.1 Markov Decision Process", "content": "A reinforcement learning problem Sutton and Barto [1998] can be formalized as a Markov decision process, defined by a tuple (S, A, R, P), where S denotes the state space, A the action space, R:S\u00d7A\u2192R the reward function and P : S \u00d7 A \u2192 [0, \u221e) the transition probability density function. At each time step t, the agent observes a state st \u2208 S and performs an action at \u2208 A according to a policy \u03c0 : S \u00d7 A \u2192 [0,\u221e), which results in a reward rt+1 = R(st, at) and a next state st+1 ~ P(st, at). The return associated with a trajectory (a.k.a. episode or rollout) T = (so, ao, l'1, S1, A1, r2, . . . ) is defined as\n$$R(T) = \\sum_{t=0}^{\\infty} \\gamma^{t}r_{t+1},$$ (1)\ni.e. the infinite-horizon cumulative discounted reward with discount factor \u03b3\u2208 (0, 1). The agent aims to maximize the expected return\n$$J(\\pi) = E_{T\\sim\\pi} [R(T)] = E_{T\\sim\\pi} [\\sum_{t=0}^{\\infty} \\gamma^{t}r_{t+1}],$$ (2)\ni.e. to learn an optimal policy \u03c0* = arg max\u3160 J(\u03c0). The state value function (a.k.a. state value) V\" (s) of a state s is defined as\n$$V^{\\pi}(s) = E_{T\\sim\\pi} [R(T) | s_0 = s],$$ (3)\ni.e. the expected return obtained by the agent if it starts in state s and always acts according to policy \u03c0. Similarly, the state-action value function (a.k.a. action value or Q-value) Q\u2122 (s, a) of a state-action"}, {"title": "2.2 Soft Actor-Critic", "content": "SAC Haarnoja et al. [2018] redefines the expected return in Eq. (2) to be maximized by policy \u03c0as\n$$J(\\pi) = E_{T\\sim\\pi} [\\sum_{t=0}^{\\infty} \\gamma^{t} (r_{t+1} + \\alpha H(\\pi(\\cdot | s_t))); s_t)],$$ (5)\nwhere T = (so, A0, 11, S1, A1, r2, ...) is a trajectory of state-action-reward triplets sampled from the environment, y the discount factor, \u0397(\u03c0(\u00b7 | st)) = Ea\u223c\u03c0(a|st) [\u2212log \u03c0(a | st)] the entropy of \u03c0in state st, and a > 0 a temperature parameter that controls the relative strength of the entropy regularization term. Since J(\u03c0) now includes a state-dependent entropy bonus, the definitions of V\" (s) and Q\u2122 (s, a) need to be updated accordingly as\n$$V^*(s) = E_{T\\sim\\pi} [\\sum_{t=0}^{\\infty} \\gamma^{t}(r_{t+1} + \\alpha H(\\pi(\\cdot | s_t)) | S_0 = s]$$ (6)\n$$Q^{\\pi}(s, a) = E_{T\\sim\\pi} [\\sum_{t=0}^{\\infty} \\gamma^{t}(r_{t+1} + \\alpha \\sum_{t=1}^{\\infty} H(\\pi(\\cdot | s_t)) | S_0 = s, a = a ]$$ (7)\nNote that the modified Q\" (s, a) does not include the entropy bonus from the first time step. Let Q(s, a) be the action value function approximated by a neural network with weights & and \u03c0\u04e9 a stochastic policy parameterized by a neural network with weights 0. The action value network is trained to minimize the squared difference between Q\u2084(s, a) and the soft temporal difference estimate of Q\" (s, a), defined as\n$$Q(s_t, a_t) = r_{t+1} + \\gamma(Q_{\\psi}(s_{t+1}, \\bar{a}_{t+1}) \u2013 \\alpha \\log \\pi_{\\theta}(\\bar{a}_{t+1} | s_{t+1})),$$ (8)\nwhich is the standard temporal difference estimate Sutton [1988] augmented with the entropy bonus, where rt+1 and st+1 are sampled from a replay buffer storing past experiences and at+1 is sampled from the current policy \u03c0\u04e9(\u00b7 | St+1). In practice, the approximation Qy(st+1, at+1) is computed by a target network Mnih et al. [2015], whose weights target are periodically updated as target \u2190 (1 \u2013 \u03c4)$target + \u03c4\u03c8, with smoothing coefficient \u03c4\u2208 [0, 1]. Not only does this help to stabilize the training process, but it also turns the ill-posed problem of learning Q\u2084(s, a) via bootstrapping into a supervised learning one that can be solved via gradient descent. Furthermore, double Q-learning is used to reduce the overestimation bias and speed up convergence Hasselt et al. [2016]. This means that Qy(s, a) is computed as the minimum between two action value function approximations Q\u03c8\u2081 (s, a) and Q42 (s, a), parameterized by neural networks with weights 41 and 42, respectively.\nWhile the action value network learns to minimize the error in the Q-value approximation, \u03c0\u03b8 is updated via gradient ascent to maximize\n$$J(\\pi_{\\theta}) = E_{s\\sim P} [Q_{\\psi}(s, a) \u2013 \\alpha \\log \\pi_{\\theta}(a | s)],$$ (9)\nwhere P is the transition probability function, Q\u2084(s,a) = min{Qy\u2081 (s, a), Q\u03c82 (s,a)}, and a is drawn from \u03c0\u03b8 in a differentiable way through the reparameterization trick. The temperature parameter a, which explicitly handles the exploration-exploitation trade-off, is particularly sensitive to the magnitude of the reward, and needs to be tuned manually for each task. The final algorithm, which alternates between sampling transitions from the environment and updating the neural networks using the experiences retrieved from a replay buffer, is shown in Algorithm 1. Note that, although in theory the replay buffer can store an arbitrary number of transitions, in practice a maximum size should be specified based on the available memory. When the replay buffer overflows, past experiences are removed according to the First-In First-Out (FIFO) rule. It is also advisable to collect a minimum number of transitions prior to the start of the training process to provide the neural networks with a sufficient amount of uncorrelated experiences to learn from."}, {"title": "2.3 Implicit Reparameterization Gradients", "content": "Implicit reparameterization Figurnov et al. [2018], Jankowiak and Obermeyer [2018] is an alternative to the reparameterization trick (a.k.a. explicit reparameterization) that enables the computation of gradients of stochastic computations for a broader class of distribution families. Explicit reparameterization requires an invertible and continuously differentiable standardization function S4(z) that, when applied to a sample from distribution q\u2084(z), removes its dependency on the distribution parameters : \n$$S_{\\Phi}(z) = \\epsilon \\sim q(\\epsilon)  z = S_{\\Phi}^{-1}(\\epsilon).$$ (10)\nFor instance, in the case of a normal distribution N (\u03bc, \u03c3) a valid standardization function is S\u03bc,\u03c3 (z) = (z\u2212 \u03bc)/\u03c3 ~ \u039d(0, 1). Let f(z) denote an objective function whose expected value over q is to be optimized with respect to $. Then\n$$E_{q_{\\Phi}(z)} [f(z)] = E_{q(\\epsilon)} [f(S_{\\Phi}^{-1}(\\epsilon))]$$ (11)\nand the gradient of the expectation can be computed via chain rule as\n$$\\nabla_{\\Phi} E_{q_{\\Phi}(z)} [f(z)] = E_{q(\\epsilon)} [\\nabla_{\\Phi}f(S_{\\Phi}^{-1} (\\epsilon))] = E_{q(\\epsilon)} [\\nabla_z f(S_{\\Phi}^{-1}(\\epsilon))\\nabla_{\\Phi} S_{\\Phi}^{-1}(\\epsilon)].$$ (12)\nAlthough many continuous distributions admit a standardization function, it is often non-invertible or prohibitively expensive to invert, hence the reparameterization trick is not applicable. This is the case for distributions such as gamma, beta, Dirichlet and von Mises. Fortunately, implicit reparameterization relaxes this constraint, allowing for gradient computations without S\u012b\u00b9(e). Using the change of variable z = S\u012b\u00b9(s), Eq. (12) can be rewritten as:\n$$\\nabla_{\\Phi} E_{q_{\\Phi}(z)} [f(z)] = E_{q(z)} [\\nabla_z f(z)\\nabla_{\\Phi} z], \\nabla_{\\Phi} z = \\nabla_{\\Phi} S_{\\Phi}^{-1}(\\epsilon)|_{\\epsilon=S_{\\Phi}(z)}.$$ (13)\nAt this point, Voz can be obtained via implicit differentiation by applying the total gradient operator to the equation Sp(z) = \u03b5:\n$$\\nabla_{\\Phi}S_{\\Phi}(z) = \\nabla_{\\Phi}\\epsilon \\Leftrightarrow \\nabla_z S_{\\Phi}(z)\\nabla_{\\Phi} z + \\nabla_{\\Phi}S_{\\Phi}(z) = 0 \\Leftrightarrow \\nabla_{\\Phi} z = -(\\nabla_z S_{\\Phi}(z))^{-1} \\nabla_{\\Phi}S_{\\Phi}(z)$$ (14)\nRemarkably, this expression for the gradient does not require inverting the standardization function but only differentiating it."}, {"title": "3 Method", "content": "The main goal of this work is to explore the use of SAC in combination with the beta policy. To do so, we employ implicit reparameterization to enable differentiation through the sampling process. As"}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 MuJoCo Environments", "content": "We evaluate the proposed methods on four of the eleven MuJoCo environments, namely Ant-v4, HalfCheetah-v4, Hopper-v4 and Walker2d-v4. The goal is to walk as fast as possible without falling while at the same time minimizing the number of actions and reducing the impact on each joint. Observations consist of coordinate values of different robot's body parts, followed by the velocities of each of those individual parts. Actions, bounded to [-1, 1], represent the torques applied at the joints. The dimensionality of both the observation and the action space varies across the environments, depending on the complexity of the task"}, {"title": "4.2 Training Setup and Hyperparameters", "content": "Hyperparameter values are reported in Table 2. We use the same architecture for both the policy and the action value network, consisting of 2 fully connected hidden layers with ReLU nonlinearities. The action value network receives as an input the concatenation of the flattened observation and action and returns a Q-value. The policy network receives as an input the flattened observation and returns the parameters of the corresponding action distribution. In particular, for SAC-Beta-AD and SAC-Beta-OMT, it returns the logarithm of the concentration parameters shifted by 1 to ensure that the distribution is both concave and unimodal Chou et al. [2017]. For SAC-Normal and SAC-TanhNormal, it returns the mean and the logarithm of the standard deviation. For all distributions, we assume the action dimensions are independent. To improve training stability, we clip the log shifted concentrations and log standard deviation to the interval [-20, 2]. Furthermore, we clip the samples drawn from the beta distribution to the interval [10-7,1 \u2013 10\u22127] to avoid underflow and we linearly map them from [0, 1] to the environment's action interval [-1,1]. We train the policy for 106 time steps using Adam optimizer Kingma and Ba [2015] with a learning rate of 0.001 and a batch size of 256. We perform 1 gradient update per time step and synchronize the target networks after each update with smoothing coefficient 0.005. We use a discount factor of 0.99, a temperature"}, {"title": "4.3 Implementation and Hardware", "content": "Software for the experimental evaluation was implemented in Python 3.10.2 using Gymnasium 0.28.11 Brockman et al. [2016] + EnvPool 0.8.12 Weng et al. [2022b] for the MuJoCo environments, Tianshou 0.5.03 Weng et al. [2022a] for SAC, PyTorch 1.13.14 Paszke et al. [2019] for the neural network architectures, probability distributions and reference implementation of optimal mass transport implicit reparameterization, TensorFlow 2.11.05 Abadi et al. [2015] + TensorFlow Probability 0.19.06 Dillon et al. [2017] for the reference implementation of automatic differentation implicit reparameterization and Matplotlib 3.7.07 Hunter [2007] for plotting. All the experiments were run on a CentOS Linux 7 machine with an Intel Xeon Gold 6148 Skylake CPU with 20 cores @ 2.40 GHz, 32 GB RAM and an NVIDIA Tesla V100 SXM2 @ 16 GB with CUDA Toolkit 11.4.2."}, {"title": "4.4 Results and Discussion", "content": "A comparison between the proposed SAC variants is presented in Fig. 1, with the final performance reported in Table 3. We observe that SAC-Normal struggles to learn anything useful, as the average return remains close to zero in all the environments. This could be ascribed to a poor initialization, as the training process immediately diverges and the policy gets stuck in a region of the search space from which it cannot recover. Further investigations are needed to understand why the normal policy performs poorly with SAC while it does not exhibit the same flawed behavior with other non-entropy-based algorithms such as TRPO Schulman et al. [2015], Chou et al. [2017]. Overall, SAC-Beta-AD and SAC-Beta-OMT perform similarly to SAC-TanhNormal, with slightly higher final return in Ant-v4 and Walker2d-v4 but worse in HalfCheetah-v4 and Hopper-v4. However, the large standard deviation values indicate that more experiments might be necessary to obtain more accurate estimates. Regarding the comparison between SAC-Beta-AD and SAC-Beta-OMT, we only observe little differences mostly due to random fluctuations. This is surprising, since we were expecting faster convergence with SAC-Beta-AD, given the fact that it provides more accurate gradient estimates Figurnov et al. [2018]. This suggests that highly accurate gradients may not be critical to the algorithm's success. Therefore, a simpler gradient estimator such as the score function estimator Mohamed et al. [2020], which does not rely on assumptions about the distribution family, could be a promising alternative to explore.\nWe also conduct an ablation study for SAC-Beta-OMT on Ant-v4 to gain more insights on the impact of our design choices. In particular, we consider the following three ablations:\n\u2022 SAC-Beta-OMT-no_clip: we do not clip the log shifted concentrations.\n\u2022 SAC-Beta-OMT-non_concave: we do not shift the concentrations, allowing for non-concave beta distributions."}, {"title": "5 Conclusion", "content": "In this work, we utilized implicit reparameterization gradients Figurnov et al. [2018], Jankowiak and Obermeyer [2018] to train SAC with the beta policy, which is an alternative to normalizing flows in addressing the bias issue resulting from the mismatch between the infinite support of the normal distribution and the bounded action space typically present in real-world settings. Experiments on four MuJoCo environments show that the beta policy is a viable alternative, as it outperforms the normal policy and yields similar results to the squashed normal policy, frequently used together with SAC. Future research includes analyzing the qualitative behavior of the learned policies, extending the evaluation to more diverse tasks and exploring other non-explicitly reparameterizable distributions that could potentially be beneficial for injecting domain knowledge into the problem at hand. Additionally, more generic gradient estimators such as the score function estimator Mohamed et al. [2020] could be investigated."}]}