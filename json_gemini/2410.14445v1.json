{"title": "TOWARD GENERALIZING VISUAL BRAIN DECODING\nTO UNSEEN SUBJECTS", "authors": ["Xiangtao Kong", "Kexin Huang", "Ping Li", "Lei Zhang"], "abstract": "Visual brain decoding aims to decode visual information from human brain activ-\nities. Despite the great progress, one critical limitation of current brain decoding\nresearch lies in the lack of generalization capability to unseen subjects. Prior\nworks typically focus on decoding brain activity of individuals based on the ob-\nservation that different subjects exhibit different brain activities, while it remains\nunclear whether brain decoding can be generalized to unseen subjects. This study\naims to answer this question. We first consolidate an image-fMRI dataset con-\nsisting of stimulus-image and fMRI-response pairs, involving 177 subjects in the\nmovie-viewing task of the Human Connectome Project (HCP). This dataset allows\nus to investigate the brain decoding performance with the increase of participants.\nWe then present a learning paradigm that applies uniform processing across all\nsubjects, instead of employing different network heads or tokenizers for individ-\nuals as in previous methods, which can accommodate a large number of subjects\nto explore the generalization capability across different subjects. A series of ex-\nperiments are conducted and we have the following findings. First, the network\nexhibits clear generalization capabilities with the increase of training subjects.\nSecond, the generalization capability is common to popular network architectures\n(MLP, CNN and Transformer). Third, the generalization performance is affected\nby the similarity between subjects. Our findings reveal the inherent similarities in\nbrain activities across individuals. With the emerging of larger and more compre-\nhensive datasets, it is possible to train a brain decoding foundation model in the fu-\nture. Codes and models can be found at https://github.com/Xiangtaokong/TGBD.", "sections": [{"title": "1 INTRODUCTION", "content": "Visual brain decoding (Kay et al., 2008; Kamitani & Tong, 2005; Naselaris et al., 2011) aims to\ndecode visual information from human brain activities, including tasks of brain-image classification\n(Kaur & Gandhi, 2019; Zhou et al., 2024), retrieval (Scotti et al., 2024a; Xia et al., 2024) and recon-\nstruction (Takagi & Nishimoto, 2023; Ozcelik & VanRullen, 2023; Ferrante et al., 2024; Scotti et al.,\n2024a), and so on. It involves analyzing neural patterns collected via brain imaging techniques like\nfunctional magnetic resonance imaging (fMRI) (Schirrmeister et al., 2017; Benchetrit et al., 2023;\nKamitani & Tong, 2005) or electroencephalography (EEG) (Schirrmeister et al., 2017; Vallabhaneni\net al., 2021) to infer the visual information received by the participants. Among them, fMRI is fa-\nvored by researchers because of its more informative depiction of the whole brain activity, which has\nresulted in a number of important decoding works (Allen et al., 2022; Takagi & Nishimoto, 2023;\nScotti et al., 2024a) with the help of deep learning techniques.\nA major limitation of current brain decoding research, however, lies in the lack of generalization\ncapability to unseen subjects. That is, the trained decoding models can hardly be applied to new,\nunseen individuals. Such a limitation can be owed to two reasons. First, there are individual differ-\nences of the brain activities across subjects (Haxby et al., 2020). Therefore, it is assumed that brain\ndecoding cannot be generalized and hence researchers are focused on developing subject-specific"}, {"title": "2 RELATED WORK", "content": "Visual Brain Decoding. With the advancement of deep learning (Radford et al., 2021; He et al.,\n2016; Vaswani et al., 2017; Rombach et al., 2022) and the emergence of high-quality fMRI datasets\n(Allen et al., 2022; Chang et al., 2019; Van Essen et al., 2013), many brain decoding methods with\npromising performance have been proposed. Takagi & Nishimoto (2023) utilized a latent diffusion\nmodel, specifically Stable Diffusion (Ho et al., 2020; Sohl-Dickstein et al., 2015), to reconstruct\nhigh-resolution images from fMRI data, preserving semantic fidelity without requiring additional\ntraining or fine-tuning. Brain-Diffuser (Ozcelik & VanRullen, 2023) improves the reconstruction\nprocess by first reconstructing basic image properties from fMRI signals and then refining the im-\nages using a latent diffusion model conditioned on multimodal features. Moreover, MindEye (Scotti\net al., 2024a) encodes images using CLIP and then maps the corresponding fMRI data to the CLIP\nfeature space, enabling strong image retrieval or reconstruction performance. However, most ex-\nisting methods focus on decoding stimuli for individual subjects. While effective for individual\ndecoding, they lack the generalization capability to new, unseen subjects.\nVisual Brain Decoding on Multiple Subjects. Some brain decoding methods have been devel-\noped to leverage multiple subjects, which can be categorized into two categories based on their\nobjectives: (1) using multiple subjects to enhance subject-specific models, and (2) developing mod-\nels that handle multiple subjects directly. For the first category, a straightforward way is to pre-train\nmodels on multiple subjects and then fine-tune them for individual subjects (Scotti et al., 2024b;\nJiang et al., 2024; Qian et al., 2023; Ferrante et al., 2024). For example, MindEye2 (Scotti et al.,\n2024b) pre-trains the model on 7 subjects from the NSD dataset (Allen et al., 2022) and fine-tunes\nit on a different subject, using only 1/40 of the original data while achieving similar performance.\nThe second category of methods aim to train a model with multiple subjects so that its performance\non each subject (included in the training set) surpasses the models trained on each single subject.\nCLIP-MUSED (Zhou et al., 2024) and UMBRAE (Xia et al., 2024) are methods of this kind. How-\never, most methods in this category still require separate heads or tokenizers for each subject. As\nthe number of subjects increases, their training costs and model parameters grow linearly, making\nthis approach impractical for larger subject pools. These multi-subject methods generally involve\na limited number of subjects (less than 10), which are not sufficient enough for exploration. More\nimportantly, while these methods demonstrate that certain information can be shared across subjects,\nthey cannot generalize to unseen subjects.\nSubjects Alignment. Some methods have been proposed to align new subjects to pre-trained mod-\nels, known as subject alignment, to handle unseen subjects. Based on the alignment approach, these\nmethods can be categorized into anatomical alignment (Jenkinson et al., 2002) and functional align-\nment (Haxby et al., 2011; Lorbert & Ramadge, 2012; Xu et al., 2012; Chen et al., 2015), among\nothers. In visual brain decoding, the mainstream methods fall into functional alignment, which di-\nrectly aligns the neural activity patterns across different subjects. For instance, Ferrante et al. (2024)\nused 1,000 common images viewed by 8 subjects from the NSD dataset to train an alignment model\nthat maps other subjects to Subject 1. During inference, the brain signals of other subjects are con-\nverted into the format of Subject 1 and fed into the model trained on Subject 1. This approach can\nprocess new subjects with the model of existing subjects at a lower cost, yet it requires shared data\nfor alignment. In this work, we aim to achieve model generalization without such alignment."}, {"title": "3 METHODS", "content": "In this section, we first describe how we consolidate the dataset for exploring generalizable visual\nbrain decoding in Sec. 3.1. Then, we describe the proposed learning paradigm in Sec. 3.2. Finally,\nin Sec. 3.3 we outline how we calculate the subject similarity."}, {"title": "3.1 DATASET CONSOLIDATION", "content": "Most previous studies (Scotti et al., 2024a;b; Xia et al., 2024; Zhou et al., 2024) are conducted on\ndatasets with fewer than ten participants, which cannot be used to study whether visual decoding\ncan be generalizable. Therefore, to explore the generalization capabilities of brain decoding models,\nthe first step is to collect a dataset with a larger number of subjects. However, as shown in Tab. 1,\ncurrent publicly available image-viewing datasets are limited in size. For example, the NSD dataset"}, {"title": "3.2 LEARNING PARADIGM", "content": "Prior studies are typically focused on decoding brain activity of individuals, while little work has\nbeen done on exploring the model generalization capability to unseen subjects. With our consoli-\ndated dataset in Sec. 3.1, we propose a learning paradigm to investigate the generalizability of visual\nbrain decoding based on three core principles: (1) utilization of whole-brain data; (2) simple and\nflexible pipeline; (3) applicability to a large number of diverse subjects.\nUtilization of the Whole-Brain Data. As shown in Tab. 1, most recent visual brain decoding\nstudies rely on the NSD dataset, which provides two types of training data: NSDGeneral data and\nwhole-brain data. As illustrated in Fig. 3, the whole-brain data contain the fMRI voxels (about\n800K elements) of the entire brain, while the NSDGeneral data comprise 1D vectors (flattened\nvoxels) of only 10k-20k elements, which are manually labeled as vision-related brain regions, called\nNSDGeneral regions (see the highlighted areas). Since NSDGeneral data are directly related to\nbrain regions in charge of visual processing, they often result in better visual decoding performance\nin the studies focused on single subjects. However, for research investigating the generalization\ncapability across multiple subjects, we believe whole-brain data are more appropriate. First, the\nNSDGeneral data require manual segmentation, and hence they are difficult to scale across a large\nnumber of subjects, limiting their suitability for generalization studies. Note that most datasets,\nsuch as the HCP dataset, only provide whole-brain data. Second, as can be seen in Fig. 3, the\nmanually labeled NSDGeneral regions show significant variations across different subjects, while\nthe whole brain data show much less variations in shape. It is thus more difficult to train a common\nmodel to multiple subjects using the NSDGeneral data than the whole-brain data (See Sec. 4.6).\nThird, the NSDGeneral data exclude other brain regions, such as those in charge of memory or\ncontextual understanding. Ignoring those regions may prevent a more comprehensive decoding of\nbrain activities (Zhou et al., 2024). Therefore, we advocate for using whole-brain data in the study\nof on model generalization, rather than data limited to specific brain regions.\nSimple and Flexible Pipeline. A simple and flexible learning pipeline is preferred to verify\nwhether generalizability is a fundamental property of visual brain decoding, minimizing the fac-\ntors brought by complex network designs. Our learning pipeline is shown in the left part of Fig.\n4. The core idea is to project the paired stimulus-image I and fMRI-voxel V into the same feature\nspace, where they could be as similar as possible. Following (Scotti et al., 2024a; Xia et al., 2024),\nwe use the CLIP ViT-L/14 model to encode the images into features $F_I$, while the visual brain de-\ncoding network is trained to map fMRI-voxels to $F_V$ in the same feature space. The feature size of\nthe CLIP embedding space is 257 \u00d7 1024, which retains detailed image information compared to\nthe high-level semantic content of the final CLS token in CLIP. Contrastive learning is employed to\nalign $F_I$ with $F_V$ using the CLIP Loss:\n$L = \\frac{1}{2N} \\sum_{i=1}^N -\\log \\frac{\\exp(\\text{sim}(F_I^i, F_V^i) / \\tau)}{\\sum_{j=1}^N \\exp(\\text{sim}(F_I^i, F_V^j) / \\tau)} + \\sum_{i=1}^N -\\log \\frac{\\exp(\\text{sim}(F_V^i, F_I^i) / \\tau)}{\\sum_{j=1}^N \\exp(\\text{sim}(F_V^i, F_I^j) / \\tau)}$\nApplicability to Many Diverse Subjects. Previous studies are mostly focused on a small number\nof participants, and they use separate heads or tokenizers for different subjects to improve per-\nformance. While being effective in small scale studies, these approaches become impractical and\ncannot scale up as the number of subjects increases. To explore model generalizability, we employ\nthe same decoding network (see the right part of Fig. 4) to accommodate a large number of subjects\nwithout requiring specific adaptations for each individual. Due to the structural differences in brain\nanatomy, the size of fMRI voxels, even for the same brain activity, can vary across subjects, which\ncannot be directly batched for network training. To solve this issue, we apply simple upsampling to\nresize the voxels to a standardized larger size. This method is simple and straightforward and can be\ndone in the processing of the dataset. Experimental results show that this unified approach does not\ncompromise performance in whole-brain decoding and can even enhance performance across mul-\ntiple subjects (See Sec. 4.6). As shown in the right of Fig. 4, our final network design involves an\nupsample layer to normalize voxel sizes for all subjects, followed by feeding the data into a unified\nnetwork without requiring any subject-specific adaptations."}, {"title": "3.3 GENERALIZATION PERFORMANCE VS. SUBJECT SIMILARITY", "content": "During experiments, we notice some performance biases of models trained on different gender\ngroups, which represent one of the most easily identifiable similarity categories. It inspires us to\nhypothesize that the degree of similarity among subjects might impact the model generalization per-\nformance. To test this hypothesis, we need to identify which subjects are more similar to the given\nsubjects. For a target subject $S_t$, given a set of images $I$ viewed by $N$ different subjects $S_N$, for each\nimage $i$, we can calculate the cosine similarity (refer to Eq. 2) between the fMRI voxel $v_{i, S_t}$ of target\nsubject and the voxel $v_{i, S_n}$ of subject $S_n \\in S_n$ as follow: Sim_score$_{i, S_t, S_n}$ = sim($v_{i, S_t}$, $v_{i, S_n}$). \nThe similarity score reflects the likeness between the two subjects ($S_t$ and $S_n$) based on a given\nimage $i$. The overall similarity score of the two subjects can be obtained by averaging over all im-\nges $I$. However, the outlier images can make the averaged score less robust. Therefore, we use a\nrank-based method. For each image $i$, we calculate Sim_score$_{i, S_t, S_n}$ and rank the $N$ scores from\nhighest to lowest. Then, we select the top 10 subjects based on their ranks and award them one"}, {"title": "4 EXPERIMENTS", "content": "We implement all models using PyTorch (Paszke et al., 2017). Except specifically indicated, we\nemploy MLP and 3D CNN as the backbone for feature extraction when using whole-brain data. The\ndetailed network structure can be found in the appendix. During training, we employ the CLIP\nloss (Radford et al., 2021) and the AdamW optimizer (Loshchilov & Hutter, 2017) to optimize the\nmodels ($\\beta_1$ = 0.9, $\\beta_2$ = 0.999). We set the batch size to 300, and apply the OneCycleLR strategy\nwith a warm-up phase to adjust the learning rate, with a maximum learning rate of 1 \u00d7 10$^{-4}$. The\nHCP dataset we consolidated includes 177 subjects, each subject having 3,127 image-fMRI pairs.\nWe randomly choose 100 images and the corresponding fMRI voxels as the test pairs, and use the\nrest as the training pairs. Note that the test pairs of all subjects are from the same 100 images. Subjs\n1-10 are designated as unseen subjects, with the remaining 167 subjects as seen subjects.\nIn our experiments, several models will be trained on different numbers of subjects. For convenience\nof expression, we define one training epoch based on the number of image-fMRI pairs of a single\nsubject; that is, one epoch contains 3,027 image-fMRI pairs. The numbers of epochs to train models\non 1, 2, 20, 50, 100, and 167 seen subjects are 200, 200, 400, 600, 800, and 1,000, respectively. For\nthe experiment on the NSD dataset, which includes 8 subjects, we follow the standard train/test split\nwith 1,000 test images (Allen et al., 2022), and select Subj 2 and Subj 5 as unseen subjects. We train\nthe models with 1 and 6 seen subjects for 120 and 360 epochs, respectively, and each epoch includes\n9,000 image-fMRI pairs."}, {"title": "4.2 MAIN RESULTS ON GENERALIZATION PERFORMANCE", "content": "As describe in Sec. 4.1, we train models on 1, 2, 20, 50, 100 and 167 subjects and evaluate them\non 10 unseen subjects (Subjs 1-10). The results are shown in Tab. 2. We can clearly see that as\nthe number of training subjects increases, the model's generalization capability on unseen subjects\nimproves. When only one or two subjects are used in training, the generalization capability is weak.\nWhen the number of training subjects reaches 167, the TOP1 and TOP3 accuracies improve to\n45% and 61%, respectively. Considering that our test set contains 100 image-fMRI pairs, such a\ngeneralization performance is highly encouraging. Fig. 1 plots the curve of TOP1 accuracy vs. the"}, {"title": "4.3 THE GENERALIZATION PERFORMANCE WITH DIFFERENT BACKBONES", "content": "In Sec. 4.2, we used MLP as the backbone and validated the generalization capability of our models\ntrained on more subjects. Here, we validate whether this conclusion holds for other popular net-\nwork architectures, such as CNN and Transformer networks. The details of the employed network\narchitectures can be found in the appendix. The results are shown in Tab. 3, which demonstrates\nthat while there are some differences in performance, the generalization capability is consistently\nachieved across different network architectures. Specifically, the MLP achieves the best generaliza-\ntion performance, with 45% TOP1 accuracy on unseen subjects and 82% on seen subjects, followed\nclosely by 1D CNN and 3D CNN, which yield comparable results. The Transformer network ex-\nhibits the lowest performance, which may be attributed to the fact that Transformer typically needs\nlarger training datasets to exhibit its superiority, whereas our current dataset for visual brain decod-\ning is relatively small, making CNNs and MLPs more effective in this case."}, {"title": "4.4 GENERALIZATION VS. SUBJECT SIMILARITY", "content": "From the experiments in previous sections, we have seen that the network could exhibit obvious\ngeneralization capability when enough subjects are used in training. During our experiments, inter-\nestingly, we notice some performance biases of models trained on different gender groups. Gender\nis one of the most commonly observed characteristics of sample similarity, which inspires us that\nthe similarity among subjects might impact the model generalization performance. To be specific,\nwe train three models using data from 50 male subjects, 50 female subjects, and a mixed group of\n25 male and 25 female subjects, respectively. Then, we evaluate these models on unseen male Subj\n1 and female Subj 2, and the results are shown in Tab. 4. One can see that the model trained on male\nsubjects achieves the best retrieval performance on the unseen male subject (Subj 1) with 36% TOP1\naccuracy and 60% TOP3 accuracy, but it performs the worst on the unseen female subject (Subj 2)\nwith 25% TOP1 accuracy and 37% TOP3 accuracy. In contrast, the model trained on female sub-\njects shows the opposite behaviour, performing better on Subj 2 and worse on Subj 1. Meanwhile,\nthe model trained on the mixed group always obtain the intermediate result on the unseen subjects.\nSuch results suggest that the generalization capability is related to the similarity among subjects.\nTherefore, we further explore this phenomenon by finding similar and dissimilar subjects to Subj\n1 and Subj 2. Utilizing the method described in Sec. 3.3, we identify 20 most similar subjects\nand 20 least similar subjects to Subj 1, as well as 20 most similar and least similar subjects to\nSubj 2. As shown in Tab. 4, the model trained on the 20 subjects most similar to Subj 1 achieves\nthe best performance on Subj 1, with a TOP1 accuracy of 21% and a TOP3 accuracy of 36%. In\ncontrast, the model trained on the 20 most dissimilar subjects performs significantly worse, with a\nTOP1 accuracy of 8% and a TOP3 accuracy of 14%. A similar trend can be observed for Subj 2,\nwhere the model trained on the 20 most similar / dissimilar subjects achieves the highest / lowest\nperformance. Additionally, the model trained on Subjs 11-30, as a reference for randomly selected\n20 subjects, yields moderate performance on both Subj 1 and Subj 2. This demonstrates that the"}, {"title": "4.5 TRAINING STRATEGY", "content": "In our main experiments, in order to prove that the generalization capability does not come from\nsome specific strategies, we use the simplest contrastive learning pipeline and the CLIP loss to\nverify our approach on commonly used network architectures. In this section, we demonstrate that\nthe model performance can be further enhanced if stronger training strategies can be employed. In\nparticular, we adopt the BiMixCo+SoftCLIP training strategy from MindEye1 (Scotti et al., 2024a).\nThe BiMixCo+SoftCLIP strategy incorporates a data augmentation technique, extending the mixup\napproach with the InfoNCE loss (He et al., 2020). Additionally, we replace the CLIP loss with the\nSoftCLIP loss, which leverages softmax probability distributions rather than hard labels. Details of\nthese methods can be found in the appendix.\nAs shown in Tab. 5, the adoption of BiMixCo+SoftCLIP leads to a noticeable improvement in\ngeneralization performance for both single-subject models (TOP1 accuracy improves from 2% to\n6%) and multiple-subject models (TOP1 accuracy improves from 45% to 50%). This strategy also\nenhances the retrieval accuracy on seen subjects. These results suggest that better learning strategies\ncan be designed to boost the model generalization capabilities, highlighting the potential of our\napproach for visual brain decoding."}, {"title": "4.6 EXPERIMENTAL RESULTS ON THE NSD DATASET", "content": "To demonstrate the flexibility of our pipeline, we also train the model on the NSD dataset, including\nboth NSDGeneral and whole-brain data. The results are shown in Tab. 6. We see that the models\ntrained on Subj 1 using NSDGeneral data with both the original data format (i.e., 1 \u00d7 15,724 in\nMindEye1) and our normalized data format (i.e., 1 \u00d7 18,000) achieve similar TOP1 accuracy, i.e.,\n85% and 86%, respectively. (The TOP1 accuracy reported in the original paper of MindEye1 (Scotti\net al., 2024a) is 84%.) The results on Subj 7 can yield similar conclusion. This demonstrates that our\npipeline can be well applied to the individual-specific scenario using NSDGeneral data. However,\nby using simple interpolation based upsampling to normalize the data format as a 1 \u00d7 18,000 vector,\nwe can train models on multiple subjects with different original NSDGeneral data sizes. As shown\nin the bottom three rows of Tab. 6, by training on subjs 1,3,4,6,7,8 and testing on subj 1 or subj 7,\n83% and 69% TOP1 accuracy can still be obtained since subj 1 or subj 7 are included in the training\ndata. However, when testing on the unseen subjs 2 and 5, only 1% TOP1 accuracy is obtained.\nThis is because the NSD dataset has only 8 subjects in total, which is too few to ensure the model\ngeneralization performance.\nLet's then evaluate the models trained with whole-brain data. The up right panel of Tab. 6 shows\nthe results on Subj 1 and Subj 7 by MindEye1, which uses the original whole-brain data format, and\nour model, which uses the normalized data format (i.e., 113 \u00d7 136 \u00d7 113). We see that in the case\nof whole-brain data, the simple normalization of data size can improve much the TOP1 accuracy\nfrom 35% to 46% for subj 1 and from 23% to 29% for subj 7. The accuracy is lower than that on\nNSDGeneral data because the NSDGeneral data are manually labeled brain visual regions. Again,\nthe normalized data size enables us to train models on multiple subjects with different original\ndata sizes, which cannot be done by MindEye1. As shown in the bottom right panel of Tab. 6,\nour model trained on six subjects achieves 49% TOP1 accuracy on Subj 1 and 35% on Subj 7,\noutperforming single-subject models trained on Subj 1 (46%) and Subj 7 (29%), respectively. In\ncontrast, on NSDGeneral data, the multiple-subject models perform slightly worse than their single-\nsubject counterparts. This discrepancy highlights the individual-specific nature of NSDGeneral data,\nas discussed in Sec. 3.1 and shown in Fig. 3. We also report the generalization performance\nof models trained on the six subjects on unseen Subjs 2 and 5. Again, the model shows weak\ngeneralization ability due to the small number of training subjects in NSD dataset."}, {"title": "5 CONCLUSION", "content": "Previous visual brain decoding studies typically focused on individual subjects, or training with\nmultiple-subjects but decoding on seen subjects, while little work has been done on exploring the\npossibility of generalizing visual brain decoding to unseen subjects. We made an attempt to achieve\nthis goal by leveraging a large dataset from the Human Connectome Project (HCP), constructing 177\nx 3,127 image-fMRI pairs from 177 subjects. Using this dataset, we proposed a learning paradigm,\nwhich utilized whole-brain data and a simple and uniform pipeline for processing all subjects, with-\nout requiring individual-specific adaptations. Via extensive experiments, we found that the model\ngeneralization capability emerged with the increase of training subjects, and such generalization"}, {"title": "4.1 IMPLEMENTATION DETAILS", "content": "We implement all models using PyTorch (Paszke et al., 2017). Except specifically indicated, we\nemploy MLP and 3D CNN as the backbone for feature extraction when using whole-brain data. The\ndetailed network structure can be found in the appendix. During training, we employ the CLIP\nloss (Radford et al., 2021) and the AdamW optimizer (Loshchilov & Hutter, 2017) to optimize the\nmodels ($\\beta_1$ = 0.9, $\\beta_2$ = 0.999). We set the batch size to 300, and apply the OneCycleLR strategy\nwith a warm-up phase to adjust the learning rate, with a maximum learning rate of 1 \u00d7 10$^{-4}$. The\nHCP dataset we consolidated includes 177 subjects, each subject having 3,127 image-fMRI pairs.\nWe randomly choose 100 images and the corresponding fMRI voxels as the test pairs, and use the\nrest as the training pairs. Note that the test pairs of all subjects are from the same 100 images. Subjs\n1-10 are designated as unseen subjects, with the remaining 167 subjects as seen subjects.\nIn our experiments, several models will be trained on different numbers of subjects. For convenience\nof expression, we define one training epoch based on the number of image-fMRI pairs of a single\nsubject; that is, one epoch contains 3,027 image-fMRI pairs. The numbers of epochs to train models\non 1, 2, 20, 50, 100, and 167 seen subjects are 200, 200, 400, 600, 800, and 1,000, respectively. For\nthe experiment on the NSD dataset, which includes 8 subjects, we follow the standard train/test split\nwith 1,000 test images (Allen et al., 2022), and select Subj 2 and Subj 5 as unseen subjects. We train\nthe models with 1 and 6 seen subjects for 120 and 360 epochs, respectively, and each epoch includes\n9,000 image-fMRI pairs."}, {"title": "4.1 IMPLEMENTATION DETAILS", "content": "We implement all models using PyTorch (Paszke et al., 2017). Except specifically indicated, we\nemploy MLP and 3D CNN as the backbone for feature extraction when using whole-brain data. The\ndetailed network structure can be found in the appendix. During training, we employ the CLIP\nloss (Radford et al., 2021) and the AdamW optimizer (Loshchilov & Hutter, 2017) to optimize the\nmodels ($\\beta_1$ = 0.9, $\\beta_2$ = 0.999). We set the batch size to 300, and apply the OneCycleLR strategy\nwith a warm-up phase to adjust the learning rate, with a maximum learning rate of 1 \u00d7 10$^{-4}$. The\nHCP dataset we consolidated includes 177 subjects, each subject having 3,127 image-fMRI pairs.\nWe randomly choose 100 images and the corresponding fMRI voxels as the test pairs, and use the\nrest as the training pairs. Note that the test pairs of all subjects are from the same 100 images. Subjs\n1-10 are designated as unseen subjects, with the remaining 167 subjects as seen subjects.\nIn our experiments, several models will be trained on different numbers of subjects. For convenience\nof expression, we define one training epoch based on the number of image-fMRI pairs of a single\nsubject; that is, one epoch contains 3,027 image-fMRI pairs. The numbers of epochs to train models\non 1, 2, 20, 50, 100, and 167 seen subjects are 200, 200, 400, 600, 800, and 1,000, respectively. For\nthe experiment on the NSD dataset, which includes 8 subjects, we follow the standard train/test split\nwith 1,000 test images (Allen et al., 2022), and select Subj 2 and Subj 5 as unseen subjects. We train\nthe models with 1 and 6 seen subjects for 120 and 360 epochs, respectively, and each epoch includes\n9,000 image-fMRI pairs."}]}