{"title": "From Introspection to Best Practices: Principled Analysis of Demonstrations in Multimodal In-Context Learning", "authors": ["Nan Xu", "Fei Wang", "Sheng Zhang", "Hoifung Poon", "Muhao Chen"], "abstract": "Motivated by in-context learning (ICL) capabilities of Large Language models (LLMs), multimodal LLMs with additional visual modality are also exhibited with similar ICL abilities when multiple image-text pairs are provided as demonstrations. However, relatively less work has been done to investigate the principles behind how and why multimodal ICL works. We conduct a systematic and principled evaluation of multimodal ICL for models of different scales on a broad spectrum of new yet critical tasks. Through perturbations over different modality information, we show that modalities matter differently across tasks in multimodal ICL. Considering such modality impact, we further utilize modality-driven demonstration strategies to boost ICL performance. We also identify that demonstration selection is closely related to the models' ability to capture task inductive biases from multimodal ICL. Our principled analysis provides a comprehensive way of understanding the role of demonstrations in multimodal in-context learning, and sheds light on effectively improving multimodal ICL on a wide range of tasks even if those tasks are not seen in or even contradict pretraining data.", "sections": [{"title": "1 Introduction", "content": "Motivated by in-context learning (ICL) capabilities of Large Language Models (LLMs) for NLP tasks (Brown et al., 2020; Garg et al., 2022; Aky\u00fcrek et al., 2022), multimodal LLMs with additional visual modality are also exhibited with similar ICL abilities when multiple image-text pairs are provided as demonstrations (Alayrac et al., 2022; Bai et al., 2023; Sun et al., 2023; McKinzie et al., 2024). In recent studies, the Retrieval-based In-Context Example Selection (RICES, Yang et al. (2022)) approach, which retrieves similar images in the support set by comparing their visual features with testing images, has become a default approach to select demonstrations for multimodal"}, {"title": "2 Related Work", "content": "Textual ICL LLMs have been recognized as strong few-shot learners since their emergence (Brown et al., 2020). With ICL, LLMs are empowered to generalize to a wide range of tasks at inference even if those tasks are not seen in pretraining data (Garg et al., 2022; Aky\u00fcrek et al., 2022). However, the performance of ICL is critically sensitive to the choices of demonstrations (Rubin et al., 2022; Wang et al., 2023b; Gupta et al., 2023), the order (Lu et al., 2022; Wu et al., 2023) and format of prompts (Zhao et al., 2021; Min et al., 2021).\nTo understand why ICL works, Xie et al. (2021) explained from the theoretical perspective that Transformers acquire the ICL ability when they are trained to infer latent concepts during pretraining. Min et al. (2022) empirically showed that the performance gain of ICL over zero-shot inference is mainly driven by the label space, distribution of input text, output labels, and overall format of the sequence, while the represented mapping from inputs to the outputs in demonstrations matters little. However, some recent work (Zhou et al., 2022; Wei et al., 2023) suggested that when scaling up to some extent, larger models (e.g. PaLM-540B (Chowdh- ery et al., 2023) and Codex (Chen et al., 2021)) can actually learn input-output mappings, which allows them to perform a variety of challenging tasks even if they contradict pretraining data.\nConsidering the additional visual information in multimodal ICL, we study the importance of different modalities and guide demonstration selection for better ICL performance accordingly.\nMultimodal ICL After pretraining on interleaved image-text data or fine-tuning on multi- turn conversations, multimodal LLMs have exhibited ICL abilities in tasks such as image caption- ing and general-purpose visual question answering (Alayrac et al., 2022; Bai et al., 2023; Sun et al., 2023; McKinzie et al., 2024). Considering these studies may not sufficiently reveal strengths and weaknesses of ICL, Zong et al. (2024) recently"}, {"title": "3 Experimental Setup", "content": "In this section, we describe the experimental setup used in our analysis (\u00a74-\u00a76). We list evaluation benchmarks and corresponding metrics in Tab. 1, as well as studied model information in Tab. 2.\nEvaluation Benchmarks After pretraining multimodal LLMs on interleaved image-text data or fine-tuning on multi-turn conversations, existing work (Alayrac et al., 2022; Bai et al., 2023; Sun et al., 2023; McKinzie et al., 2024) mainly fo- cuses on evaluating their in-context learning abilities on image captioning such as COCO (Chen et al., 2015) and Flickr30K (Young et al., 2014), as well as general-purpose visual question answering tasks such as OKVQA (Marino et al., 2019), VQAv2 (Goyal et al., 2017), TextVQA (Sidorov et al., 2020) and VizWiz (Gurari et al., 2018). Besides these classic vision-language tasks, we also consider one recently released benchmark, namely VL-ICL Bench (Zong et al., 2024), which encom- passes a broad spectrum of challenging new tasks to investigate strengths and limitations of in-context learning capabilities.\nBenefits of utilizing demonstrations as contexts"}, {"title": "4 Modalities Matter Differently in Multimodal ICL", "content": "As shown in Fig. 5 and Fig. 6, pretrained models and GPT-40 generally achieve better performance given demonstrations as context in existing ICL tasks. As demonstrated in Fig. 7, on more com- plex and reasoning-focused tasks, pretrained mod- els generally benefit more from demonstrations while the performance of GPT-40 is barely influ- enced.\nIn this section, we examine which modality of the demonstrations takes more effect in multimodal in-context learning. For a comprehensive evalua- tion, we focus on three tasks of different difficulty levels: easy cross-style tasks (i.e., BenchLMM Sen- sor and Application in Fig. 7), moderate medical tasks (i.e., Path-VQA, Slake-VQA and PAD-UFES- 20 in Fig. 7), and hard text-rich key information ex- traction task (i.e., KIE from OCRBench in Fig. 10).\nWe visualize 4-shot performance of IDEFICS-80b within this section while leaving results of other models in Appendix (from Fig. 13 to Fig. 17.).\nWe identify that the dependency of performance gain from ICL on demonstration modalities differs"}, {"title": "4.1 Impact of Visual Modality", "content": "In recent studies, the Retrieval-based In-Context Example Selection (RICES (Yang et al., 2022)) approach, which retrieves similar images in the support set by comparing their visual features with testing images, has become a default approach to select demonstrations for multimodal in-context learning (Alayrac et al., 2022; Sun et al., 2023; Yang et al., 2024). However, the necessity of select-"}, {"title": "4.2 Impact of Textual Modality", "content": "Previous studies have identified excessive dependence of multimodal LLMs on the language model's linguistic priors (Han et al., 2022; Li et al., 2023). Accordingly, the role of textual modality for multimodal ICL should be similarly important. Therefore, we keep the visual modality of demonstrations while perform the following perturbations upon textual question and answer pairs: 1) no questions/answers remove the question/answer compo- nent directly; 2) random questions/answers em- ploys questions/answers sampled from the train set instead; 3) permuted questions/answers exchange question or answer component of demonstration examples while keep the other two components unchanged.\nResults In Fig. 2b, we visualize ICL performance in response to perturbations upon questions or answers of demonstrations independently. We find that textual perturbations hurt ICL performance to different extents. On tasks such as BenchLMM"}, {"title": "5 How to Select Effective Demonstrations for Multimodal ICL", "content": "Motivated by variational roles of different modalities across different tasks, we further explore in- fluence of modality-driven demonstration selection strategies on ICL performance in this section.\nVision-driven Demonstration Selection To re- trieve demonstrations containing images similar to those in testing examples, we follow prior stud- ies (Alayrac et al., 2022; Sun et al., 2023; Yang et al., 2024) by adopting the RICES strategy (Yang et al., 2022), which compares visual similarity ac- cording to features extracted from the pretrained visual encoder of CLIP (Radford et al., 2021).\nText-driven Demonstration Selection For fair comparison with RICES, we employ the textual en- coder of CLIP as well for selecting demonstrations with similar textual features to testing examples. We also adopt the BERTScore (Zhang et al., 2019) metric 5, which considers token-level similarity be- tween candidate and reference sentences and shows strong correlation with human judgements on mul- tiple common benchmarks.\nDual-modality driven Demonstration Selection We first consider Mixed Modality In-Context Ex- ample Selection (MMICES) proposed by Chen et al. (2023), which first pre-filters K samples (K=32) based on visual feature similarity and then selects most similar ones based on textual similarity. To represent vision-language features, we utilize ALBEF (Li et al., 2021), a multimodal encoder that explicitly models the interactions between im- age and text features and achieves state-of-the-art performance on image-text retrieval tasks. Since its multimodal encoder is built upon an image en-"}, {"title": "6 Models May Not Always Capture Task Inductive Biases from Multimodal ICL", "content": "Prior work on NLP tasks shows that small language models like GPT-J-6B (Wang and Komatsuzaki, 2021), PaLM-8B (Chowdhery et al., 2023) and GPT3 curie-6.7B (Gao et al., 2021) rely primar- ily on semantic priors from pretraining (Min et al., 2022), while large models such as PaLM-540B, In- structGPT (Ouyang et al., 2022) and Codex (Chen et al., 2021) can capture and follow inductive biases from in-context exemplars even when they contra- dict strong semantic priors that larger models may hold (Wei et al., 2023). However, it is unknown whether capturing inductive biases is still an emergent ability of model scale for multimodal ICL. In this section, we experiment with flipped labels on the 8-shot hallucination benchmark AMBER- \u201cYes\u201d is provided as demonstration annotation if the existence/attribute/relation description in the question is WRONG according to the image, \u201cNo\u201d"}, {"title": "7 Conclusion", "content": "We conduct a systematic and principled evaluation of multimodal ICL for models of different scales on a broad spectrum of new yet critical tasks. We find that modalities matter differently in multimodal ICL across tasks. Hence we utilize modality-driven demonstration strategies to boost ICL performance. Moreover, we find that demonstrations selected according to textual similarity help models capture inductive biases from multimodal ICL."}, {"title": "Limitations", "content": "We conduct a systematic and principled evaluation of multimodal ICL for pretrained models of differ- ent scales on a broad spectrum of new yet critical tasks. One limitation of our study is lack of discussion over instruction-tuned models, which may present differently than pretrained ones."}, {"title": "Ethics Statement", "content": "This paper presents comprehensive study of multi- modal ICL on multiple existing benchmarks that have gone through ethical reviews in prior works. Therefore, we believe our work does not pose addi- tional ethical issues."}]}