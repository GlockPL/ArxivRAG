{"title": "Generalized Single-Image-Based Morphing Attack Detection Using Deep Representations from Vision Transformer", "authors": ["Haoyu Zhang", "Raghavendra Ramachandra", "Kiran Raja", "Christoph Busch"], "abstract": "Face morphing attacks have posed severe threats to Face Recognition Systems (FRS), which are operated in border control and passport issuance use cases. Correspondingly, morphing attack detection algorithms (MAD) are needed to defend against such attacks. MAD approaches must be robust enough to handle unknown attacks in an open-set scenario where attacks can originate from various morphing generation algorithms, post-processing and the diversity of printers/scanners. The problem of generalization is further pronounced when the detection has to be made on a single suspected image. In this paper, we propose a generalized single-image-based MAD (S-MAD) algorithm by learning the encoding from Vision Transformer (ViT) architecture. Compared to CNN-based architectures, ViT model has the advantage on integrating local and global information and hence can be suitable to detect the morphing traces widely distributed among the face region. Extensive experiments are carried out on face morphing datasets generated using publicly available FRGC face datasets. Several state-of-the-art (SOTA) MAD algorithms, including representative ones that have been publicly evaluated, have been selected and benchmarked with our ViT-based approach. Obtained results demonstrate the improved detection performance of the proposed S-MAD method on inter-dataset testing (when different data is used for training and testing) and comparable performance on intra-dataset testing (when the same data is used for training and testing) experimental protocol.", "sections": [{"title": "1. Introduction", "content": "Face recognition systems (FRS) have been widely deployed in various security applications, such as passport issuance and automated border control (ABC) [7]. However, with the development of image manipulation techniques, FRS are becoming vulnerable to different kinds of attacks that may lead to security lapses [21] [29]. Morphing attack is one type of the attacks that targets to subvert FRS by combining biometric samples from 2 or more individuals into a single morphed image. Morphing attacks have been illustrated as an evolving threat to the FRS [2]. Morphing attack detection algorithms (MAD) have been therefore proposed to detect these attacks to improve the security of FRS.\nSingle-image-based morphing attack detection (S-MAD) aims to detect the face morphing attack based on a single image presented to the algorithm. The most common application scenario of S-MAD is validating the face photos submitted in passport or visa applications (physically/through online services) [29]. Another possible used case for S-MAD is the validation of an existing face image database, to validate that no morphed images are contained. Hence, the S-MAD algorithm should well generalize for different types of face images and anticipated image processing, such as digital, print-scanned and print-scanned-compression. In addition, there are various types of morphing algorithms that generate morphed face images with different characteristics, such as realistic texture and high face structure similarity. While many previous works have developed MAD approaches that can detect attacks efficiently for known kinds of morphing attacks, the performance tends to degrade when testing involves data stemming from different morphing methods and which were unseen during training. Fig. 1 illustrates an example of such a scenario when the S-MAD algorithm trained on the known attack (i.e., known morphing generation type) can easily miss detecting an attack from the unknown generation type [11].\nGiven the envisioned application scenario, it is crucial to improve the generalizability of S-MAD algorithm and to evaluate the detection performance in an open-set scenario by cross-dataset testing.\nThe existing S-MAD approaches are based on texture features [22], residual noise features, hybrid features, and deep learning features [29] [4] [18] [23]. With the achievement of deep convolutional neural networks (CNNs) in the field of image recognition, many researchers have applied pretrained CNNs and transfer learning to solve the S-MAD problems as binary classification problems [15] [4]. Although it has been shown that CNN-based methods may achieve better performance than S-MAD methods based on hand-crafted features, the generalizability of these approaches to print-scan images tends to be limited [11].\nRecently, Vision Transformer (ViT) [1] has become popular in computer vision and has achieved impressive results on existing image recognition challenges. Transformer models [25] apply the concepts of natural language processing directly to images where an image is split into small patches and then projected as a sequence of linear embeddings, which further are treated as the input to a Transformer model. By applying the self-attention mechanism and without introducing strong image-specific inductive biases as CNNs, ViT has shown the capability to integrate information globally from low layers and has achieved state-of-the-art (SOTA) performance in different tasks with large-scale training data. Consequently, many works have been investigating the possibility of applying ViT to other tasks. In the case of MAD, the traces of morphing are widely distributed among the face region, and hence the algorithms should have a large receptive field and the capacity of integrating local and global information to be robust and generalized. Hence, We assert that the advantages of ViTs can improve S-MAD and investigate further if they improve the generalizability of the developed S-MAD algorithm.\nOur Contributions: 1) We propose an S-MAD algorithm based on the deep representation from a pretrained vanilla ViT against other works using CNNs. 2) We investigate the applicability of the pure self-attention-based model in S-MAD tasks by conducting comprehensive cross-dataset testing with various morph generation types and different dataset types (digital/print-scan/print-scan compression). The generalizability and detection performance of the proposed approach is quantitatively evaluated and reported 3) We benchmark the proposed method together with other state-of-the-art S-MAD algorithms based on the ensemble of hand-crafted features [27], hybrid scale-space colour texture features [16] (reported in the testing report from National Institute of Standards and Technology [8]), deep CNN features [15], steerable features [17], Multi-modality approach (tested in Bologna Online Evaluation Platform [10]\u00b9), residual AutoEncoder [12], and Multi-level Deep Features [26] respectively. The analysis result indicates an improved generalizability on digital inputs."}, {"title": "2. Proposed Method", "content": "An overview of our proposed S-MAD method is described in Fig. 2. We first crop the face region using MTCNN [31] to detect face regions and then resize the cropped face image into 384 x 384 pixels to fit the input of ViT model. Then, the input image is split into small patches $x_p$ with the size of 32 x 32 pixels and then flattened and projected as patch embeddings through a learned linear projection layer with one layer of fully connected blocks for each embedding. Then an extra learnable classification embedding $X_{class}$ is attached to the other patch embeddings as the learned image representation for further classification tasks. Similar to the design of the vanilla ViT, 1-D positional encoding is applied to generate position embeddings $E_{pos}$ with the same length of the patch embeddings using sinusoidal functions. Each position embedding is added to the corresponding patch embedding hence the positional information can be encoded. Then the processed input $x_0$ can be noted as:\n$x_0 = [X_{class}; x_1E; x_2E; ...; x_NE] + E_{pos}$\nwhere N = 144 is the number of patches and E indicates the linear projection process. After processing the image into a sequence of embeddings, they are fed forward through the transformer encoder stacked with 24 layers of encoder blocks. Each encoder block includes a multi-head self-attention layer and a Multilayer Perceptron (MLP) block.\n$z'_l = MSA(LN(z_{l-1})) + z_{l-1}, l = 1, ..., L.$\n$z_l = MLP(LN(z'_l)) + z'_l, l = 1, ..., L.$\nThe multi-head self-attention layer extends the key-query-value triplet into 16 sub-triplets and executes the computation of the self-attention mechanism in parallel, hence the model can learn to extract features from multiple different aspects.\nDuring Pretraining of the ViT model, the classification token is linked to an extra MLP with a dimension of 4096 and then a softmax classifier for image classification. The model is pretrained on ImageNet21k [19] and ImageNet2012 dataset [20] with 1000 classes. To avoid duplicated training processes and achieve sustainability, we use the settings of hyper-parameters inspired by the original ViT paper [1]. As for model selection, we selected the ViT-L model with the large parameter size for higher capacity generalizability and large patch size to extract more local information. For the S-MAD task, we use the pretrained model to extract the classification tokens with the dimension of 1024 on our face morphing dataset. The extracted classification tokens will be considered as general deep representations and then we train a linear SVM classifier to solve the S-MAD problem as a binary classification task. The SVM classifier is chosen over training a deep-learning-based binary classifier due to its efficiency and robustness in preventing overfitting for small-to-medium size datasets."}, {"title": "3. Dataset", "content": "In order to conduct the cross-dataset testing comprehensively and simulate the operational use case, we use a database generated by various morphing algorithms and in different image processing methods (digital, print-scanned, print-scanned-compression). To simulate the passport use cases with face photos, our database is constructed based on selected morphed images from FRGC-V2 dataset [9] with high image quality and well-controlled capturing conditions (e.g., pose variations). 140 unique subjects, including 93 male subjects and 47 female subjects are selected. For each subject, 7-24 mated samples with similar capturing conditions (image resolution, neutral expression, pose, illumination, etc.) are chosen. In total, 1270 bona fide samples are included.\nAs for the morphing algorithms, we selected the following five representative morphing algorithms including, two landmark-based algorithms Landmark-I [14] and Landmark-II [3], and 3 GAN-based algorithms StyleGAN-IWBF[28], MIPGAN-I and MIPGAN-II [30] to establish a diversity of unknown attacks. The samples are pre-processed to meet the ICAO 9303 requirements [5]. Pairs of parent images for the morphing process are selected following guidelines suggested in [14] [21] (e.g., isolating between different genders, pairing based on similarity score of an FRS model), as the attacker may spend as much as an effort to generate the morphing attacks in real cases. As our target is to train the model to learn patterns generated from morphing instead of general patterns from GANs, reconstructed bona fide images are applied to the datasets with GAN-based morphing algorithms. In this way, we can reduce the bias between bona fide and morph samples and can make the trained classifier generalize to other types of attacks that are not generated by the same GAN model.\nTo evaluate the generalizability of S-MAD algorithms on different types of images, 3 types are included in our database:\n\u2022 Digital: Morph images are obtained from the morphing algorithms given digital parent images as input.\n\u2022 Print-scan: Both generated morphs and bona fide images are printed using DNP-DS820 dye-sublimation photo printer and then re-digitized using the Canon office scanner with 300 dpi as suggested in ICAO 9303 requirements [5]. This is to simulate the process of a passport application.\n\u2022 Print-scan with compression: Print-scanned images (morphs and bona fide) are compressed into less or equal to 15 KBs to simulate the images stored in the e-passport. Overall, each dataset has 2500 morphed images and 1270 bona fide images. Given the 5 included morphing algorithms and 3 image processing types, in total 15 datasets are used in the database for further cross-dataset testing on S-MAD algorithms."}, {"title": "4. Experiments and Results", "content": "To evaluate the generalizability and robustness of our approach, we apply cross-dataset testing on different morphing algorithms within each dataset of different image processing types and benchmark it with the other selected SOTAs:\n\u2022 Ensemble Features [27] uses ensembled features including LBP, HoG, and BSIF. The algorithm has been evaluated by public testing and included in NIST report [8]\n\u2022 Hybrid Features [16] uses scaled colour space and trains independent classifiers based on the extracted LBP features.\n\u2022 Deep Features [15] use pretrained VGG and AlexNet to extract transferable features and apply feature-level fusion for further classification.\n\u2022 Steerable Features [17] extracts steerable pyramids from illuminance components and trains classifiers based on high-frequency components.\n\u2022 Multi-Modality [13] crops the face image into different regions and extracts BSIF and LBP features. Independent classifiers are trained and score-level fusion is applied to output the final classification result. The algorithm has been evaluated in the Bologna Online Evaluation Platform [10] 2.\n\u2022 Residual AutoEncoder [12] is a deep learning approach that consists of a skip-connected AutoEncoder and a ResNet18 Classifier. Guided by the designed loss functions, the model is trained to extract learnable residuals which can be used for further classification by the ResNet.\n\u2022 Multi-level Deep Features [26] applies multi-level fusion on features extracted from AlexNet and ResNet50.\nThe selected baselines cover approaches based on hand-crafted features, deep-learning-based transferable features, different fusion strategies, and trained deep-learning models.\nMore specifically, for each dataset generated with a specific morphing algorithm, we train the S-MAD algorithm on it and test with the datasets (generated by different morphing algorithms). This shows how the detection algorithms can generalize and to which extent they are robust with respect to unknown attacks. The performance of testing across different image processing types is not included as considering a model trained on print-scan data is often not used to detect attacks from digital data rather an ensemble is used. Instead, we report the performance of cross-dataset testing for the same image processing types (e.g., digital versus digital) to evaluate the generalizability of MAD algorithms.\nTo report the performance of each test, we employ standardized metrics such as Bona fide Presentation Classification Error Rate (BPCER) and Morphing Attack Classification Error Rate (MACER) following ISO/IEC CD 20059.2 [6] and measure the detection error trade-off by reporting BPCER@MACER=5% and BPCER@MACER=10%. To simplify and scalarize the results, Detection equal error rate (D-EER) is also reported. The lower D-EER numbers indicate better detection performances.\nFor the evaluation protocol, we evaluate both intra-dataset testing and inter-dataset testing but without crossing"}, {"title": "5. Limitations", "content": "In this work we have applied a specific pretrained ViT model in order to be sustainable on computational powers, however, the influence of different hyper-parameters in the ViT model on the final performance of S-MAD tasks is still worth to be studied. Meanwhile, for the cross-dataset testing, we only conducted experiments with leave-one-out training. It is also interesting to evaluate S-MAD trained on a dataset mixed with multiple morphing algorithms and study on the learning capacity. By using the pretrained ViT model, our model has shown an improvement in generalizability for digital images, while it can be noticed that for intra-dataset testing the detection accuracies are overall less or equal for the other algorithms. Also as shown in our evaluation, the different algorithm performs inconsistently. Hence, it is reasonable to further explore fusion strategies or combine them with the multi-modality approach."}, {"title": "6. Conclusion", "content": "In this work, we proposed an S-MAD algorithm based on pretrained Vision Transformer model instead of existing deep-learning-based methods using CNNs. Motivated by the real application scenario of open-set testing, we use a morph dataset with three different image processing types and five different representative morphing algorithms, including both GAN-based and landmark-based algorithms for the cross-dataset testing. The proposed method is benchmarked against two selected SOTA algorithms. Based on the statistical analysis of the obtained results, it can be concluded that the proposed method based on the pure self-attention model can achieve notable improvement in the generalizability of the digital use cases. Despite the low performance for some cases in print-scan and print-scan compression images as noticed, one can note overall detection accuracy gain in cross-dataset testing while remaining comparable with the other SOTA algorithms.\nTo conduct a comprehensive evaluation of the detection performance of the proposed method, we have benchmarked several existing SOTA targeting generalized S-MAD tasks. Besides constructing a representative morph database we are trying to simulate the operational application scenario, further we will submit the algorithms to third-party tests such as NIST (National Institute of Standards and Technology) Face Analysis Technology Evaluation (FATE) [8] or Bologna Online Evaluation Platform (BOEP) [10].\nIn this work, the proposed method has not been submitted, but the selected reference algorithm based on hybrid features [16] has been tested in FRVT MORPH and the performance is reported in [8], and the Motimodality-based algorithm [13] has been evaluated in BOEP.\nMeanwhile, it should also be noted that in this work we only applied the vanilla Vision Transformer model pretrained with digital images on the image classification task. Hence it remains future works to plug in improved Vision Transformer models or replace the pretraining strategy with MAD-related tasks on different types of images."}]}