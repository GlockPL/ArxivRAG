{"title": "A Deep Dive Into Large Language Model\nCode Generation Mistakes: What and Why?", "authors": ["QiHong Chen", "Jiawei Li", "Jiecheng Deng", "Jiachen Yu", "Justin Tian Jin Chen", "Iftekhar Ahmed"], "abstract": "Recent advancements in Large Language\nModels (LLMs) have led to their widespread application in\nautomated code generation. However, these models can still\ngenerate defective code that deviates from the specification.\nPrevious research has mainly focused on the mistakes\nin LLM-generated standalone functions, overlooking real-\nworld software development situations where the successful\ngeneration of the code requires software contexts such\nas external dependencies. In this paper, we considered\nboth of these code generation situations and identified\na range of non-syntactic mistakes arising from LLMS'\nmisunderstandings of coding question specifications. Seven\ncategories of non-syntactic mistakes were identified through\nextensive manual analyses, four of which were missed by\nprevious works. To better understand these mistakes, we\nproposed six reasons behind these mistakes from various\nperspectives. Moreover, we explored the effectiveness of\nLLMs in detecting mistakes and their reasons. Our evalu-\nation demonstrated that GPT-4 with the ReAct prompting\ntechnique can achieve an F1 score of up to 0.65 when\nidentifying reasons for LLM's mistakes, such as misleading\nfunction signatures. We believe that these findings offer\nvaluable insights into enhancing the quality of LLM-\ngenerated code.", "sections": [{"title": "I. INTRODUCTION", "content": "Recent advancements in Large Language Models\n(LLMs) have garnered significant attention from both\nindustry and academia, leading to a surge in LLM-\nbased applications and research publications [5], [17],\n[30], [72], [76]. These models have been trained with\nlarge data corpora that contain natural language texts\nand source code, showing high competency in following\nhuman instructions in a wide range of downstream tasks\n[18], [22], [25]. Thus, developers have begun integrating\nLLMs into their daily coding tasks in various stages\nof software development [8], [22], [25], [66]. Code\ngeneration techniques aim to automate these coding tasks\nby using LLMs to generate source code given a natural\nlanguage requirement (i.e., a coding question) [15], [43],\n[52], [77]. These techniques heavily rely on the learned\nknowledge and coding ability of the LLMs.\nWhile LLMs accumulated vast knowledge, research\nindicates that they can produce factually incorrect out-\nputs, known as hallucinations [12], [45]. These hallucina-\ntions pose a significant threat to the reliability of LLMs,\ncausing serious real-world consequences [9], [44], [58].\nFor example, an AI tool incorrectly suggested that it is\nlegal for an employer to fire a worker for complaining\nabout sexual harassment, not disclosing a pregnancy, or\nrefusing to cut their dreadlocks. In the software industry,\nthese misleading suggestions can potentially lead to ma-\njor legal repercussions for the company [16]. Moreover,\nLLMs could recommend nonexistent third-party libraries\nto software developers, contributing to the distribution\nof malicious software [67]. Therefore, uncovering and\nunderstanding the mistakes in LLM responses and their\nroot causes has become urgent.\nIn recent years, the Software Engineering (SE) com-\nmunity has started investigating the mistakes in LLM-\ngenerated code and their causes. For example, Fan et\nal. [17] identified four categories of syntactic mistakes\nin LLM-generated code, while Song et al. [62] found\nseven categories of syntactic and non-syntactic mistakes.\nHowever, these studies only analyzed the mistakes in\nthe standalone functions (i.e., functions that only require\nbuilt-in/standard libraries) generated by the LLMs using\ndatasets such as HumanEval [28], MBBP [51]. Only\nfocusing on the analysis of the mistakes in such functions\nwould miss those in the production code from real-\nworld software development. Therefore, the findings\nhave limited applicability to production code generation,\nwhich entails various types of software contexts (i.e.,\ndependency information). Moreover, none of the existing\nworks has systematically investigated the reasons behind\nthe mistakes in LLM-generated code, which can be\ncrucial for an in-depth understanding of these mistakes.\nIn this study, we aim to identify and explain non-\nsyntactic mistakes in LLM-generated code that arise\nfrom LLMs' misunderstanding of the coding question.\nTo the best of our knowledge, our work is the first to in-\nvestigate LLMs' non-syntactic mistakes and their under-\nlying causes in both standalone function and production-\nlevel function generation. We focused on non-syntactic"}, {"title": "mistakes over syntactic ones to highlight potential flaws\nin the LLMs' coding abilities and knowledge application,\noffering more valuable insights than previous works.\nSpecifically, we first identified the non-syntactic mistakes\nfrom the LLM-generated code and then categorized\nthem. Next, we systematically analyzed the reasons\nbehind these mistakes. Additionally, we developed a\nbenchmark of 216 coding questions that LLMs could\nnot solve and experimented with various LLM-based\napproaches to automatically identify the mistakes and\ntheir reasons. Overall, our experiments aimed to address\nthe following research questions:", "content": "RQ1: What mistakes do LLMs make in the code\ngeneration task?\n\u2022 RQ2: Can LLMs detect the mistakes in the gener-\nated code?\n\u2022 RQ3: What are the underlying reasons for the\nLLM's mistakes in the code generation task?\n\u2022 RQ4: How effectively can LLMs identify the rea-\nsons for mistakes in the code generation task?\nSpecifically, this paper makes the following contribu-\ntions:\n1) A derived list of LLMs' mistakes in the code\ngeneration task.\n2) A derived list of reasons behind LLMs' mistakes in\nthe code generation task.\n3) An empirical investigation on LLM's ability to\ncritique its generated code.\n4) An empirical investigation on LLM's ability to\nidentify the reasons for their mistakes in the code\ngeneration task.\n5) A benchmark of 216 coding questions that LLMs\ncould not solve correctly, along with the reasons."}, {"title": "II. RELATED WORKS", "content": "Large Language Models for Code Generation\nLLMs have been trained on large data corpora of\nsource code and natural language texts, acquiring consid-\nerable knowledge and state-of-the-art reasoning abilities\n[1], [7]. Thus, recent code generation approaches have\nstarted utilizing prompting techniques where a prompt\nthat contains a coding question, input-output demonstra-\ntion, and other necessary information is input into a LLM\nto generate code [27], [41], [78], [79]. These approaches\nachieved state-of-the-art performance. In this study, we\nalso prompted LLMs to generate source code methods\ngiven the coding question specifications. Based on the\ngenerated code quality, we analyzed the non-syntactic\nmistakes of the LLM-generated code, aiming to provide\ninsights for researchers and practitioners to improve the\ngenerated code quality further.\nB. Identifying Large Language Model's mistakes\nResearchers have adopted sound verifiers [63] to\nassess the correctness of LLM responses and explain\nmistakes based on strict rules or test cases. However,\nthese verifiers must be designed for each task, requiring\nlaborious manual efforts. In recent years, researchers\nhave prompted LLMs to identify their own mistakes,\nsuch as self-critique [48] and self-refine [50] in deduc-\ntive reasoning tasks. However, code generation differs\nsignificantly from these NLP tasks and requires an un-\nderstanding of both natural language and source code. To\nidentify mistakes in LLM-generated code, Fan et al. [17]\ninvestigated LLM mistakes in code generation by com-\nparing incorrect LLM code with correct code generated\nusing Automated Program Repair (APR). Song et al. [62]\ndiscovered a variety of syntactic and semantic mistakes\nin LLM-generated code. However, these existing works\nonly considered standalone function generation, leaving\nthe code generation that entails software contexts un-\nexplored. In addition, the underlying reasons for these\nmistakes have not been investigated. Our study aims to\nprovide a more comprehensive view of LLMs' mistakes\nin code generation tasks and the reasons behind these\nmistakes.\nC. Why LLMs Make Mistakes\nSeveral studies have attempted to reveal the potential\nreasons for LLMs' mistakes in their generated output.\nFor example, Chen et al. [11] found that the performance\nof the LLMs is sensitive to premise ordering in deductive\nreasoning tasks. Jesse et al. [32] demonstrated that LLMs\ncan generate Simple and Stupid Bugs (SStuBs) in code\ngeneration due to quality issues in their training dataset.\nAlso, attention patterns of LLMs have been studied to\nexplain code generation mistakes [38]. In addition, Mu\net al. [52] reported that some LLMs' mistakes in code\ngeneration arise from unclear requirements and proposed\nClarifyGPT to prompt the LLM to ask clarifying ques-\ntions to resolve the misunderstandings. In this study,\nwe take the first step to comprehensively analyze the\nreasons for the non-syntactic mistakes in LLM-generated\nsource code of both standalone and production-level\nfunctions. To do so, we investigated the reasons from\nall those aspects considered in previous works rather\nthan from only a single perspective. Also, we conducted\nexperiments to prove that the identified reasons were\nthe ones that caused those non-syntactic mistakes in the\nLLM-generated code."}, {"title": "III. EXPERIMENTAL SETUP", "content": "In this section, we first provide the background on\nthe non-syntactic mistakes made by LLMs during code\ngeneration. Then, we explain the dataset and models used\nin this study."}, {"title": "A. Background", "content": "We define a non-syntactic mistake in LLM-generated\ncode as those mistakes that do not cause the code to have\ncompile-time or run-time errors but lead to incorrect\noutputs (i.e., ones that do not align with the expected\noutputs based on the coding question specification and\ntest case expected output) given the valid inputs, while a\nsyntactic mistake refers to the code containing syntactic\nerrors that cause compile-time or run-time errors.\nIn this study, we focus on the non-syntactic mistakes\nin LLM-generated code. The reasons are as follows:\nresearchers have shown that [49], as LLMs become\nlarger with an increasing amount of training data, they\nmake far fewer syntactic mistakes than non-syntactic\nmistakes [61]. Moreover, compared to syntactic mistakes\nthat can be detected easily by compiling and executing\nthe code, non-syntactic mistakes are far more difficult to\ndetect as they do not produce warning or compile/run-\ntime errors but potentially unexpected outputs."}, {"title": "B. Code Generation Datasets", "content": "To collect LLM-generated code, we selected two\nwidely adopted code generation datasets: HumanEval-\nX [77] and CoderEval [75]. HumanEval-X is a multilin-\ngual extension of the HumanEval dataset [10] designed\nfor code generation tasks involving standalone functions\nusing only built-in or standard libraries. CoderEval,\nderived from real-world open-source projects, includes\ncode generation problems that require varying granular-\nity levels and context dependencies, such as file, project,\nand third-party APIs [75]. Both datasets provide human-\ncrafted solutions and test cases. We focused on Python\nand Java due to their extensive use in industry and open\nsource projects [65]. The dataset statistics are presented\nin Table I."}, {"title": "C. Model Selection", "content": "Two state-of-the-art LLMs are selected in this study,\nnamely GPT-4-0125-preview (GPT-4) [1] from Ope-\nAI and Gemini Pro 1.0 (Gemini) [64] from Google.\nGPT-4 has demonstrated cutting-edge performance on\nwidely-adopted code generation benchmarks [28], [51].\nIt even outperforms the LLMs pre-trained or fine-tuned\non source code, such as StarCoder [46] and DeepSeek-\nCoder [24]. On the other hand, we include Gemini\nto enhance the generalizability of our findings, which\nshows significantly worse performance in the task of\ncode generation than GPT-4 [28], [51]."}, {"title": "D. Data Preparation", "content": "Our data preparation involves generating source code\nwith selected LLMs and collecting incorrect code that\ndoes not pass all associated test cases for non-syntactic\nmistake analysis. Given that both GPT-4 and Gemini\nexcel in code generation when simply prompted without\nadditional training [24], [41], we prompted these models\nusing task instructions, coding question specifications,\nfunction signatures, and demonstration input-output pairs\nfrom selected datasets. Our prompt is shown in Listing 1.\nWe set the LLMs' temperatures to 0 for the most\ndeterministic outputs [55].\nAfter generating the code, we executed all test cases rel-\nevant to the coding question from the selected code gen-\neration datasets (Section III-B). A method was deemed\nincorrect if it failed to pass all test cases. Since we focus\non non-syntactic mistakes in this study, we discarded\nmethods that could not be successfully compiled or\nexecuted (i.e., they contain syntactic mistakes). For the\nincorrect methods that were successfully compiled and\nexecuted with an unexpected output, we collected their\nruntimedata,such as the actual output of LLM-generated\ncode and test failure information, for further analysis.\nWe posit that LLM-generated code may differ from\nthe ground truth in terms of the applied algorithms and\ndata structures. This is reflected by the low Jaccard\nsimilarity in our datasets (0.26-0.62) [42] between the\nLLM-generated code and the ground truth. Discarding\nall incorrect and divergent codes would exclude a sub-\nstantial portion of samples from our dataset, reducing the\neffectiveness and comprehensiveness of our analysis.\nTo address this, we leveraged Automated Program Re-\npair (APR) techniques [40], [71] to correct the generated\ncode. APR makes small adjustments to the incorrect code\nrather than completely changing the algorithm, ensur-\ning the solution aligns more closely with the original\nalgorithmic approach. This method allows us to compare\nLLM-generated code with APR-fixed code, facilitating a\nmore accurate analysis of the LLM's mistakes.\nWe start by obtaining the correct counterparts for\nthe incorrect LLM-generated code using CHATRE-\nPAIR [71], a state-of-the-art APR technique. CHATRE-\nPAIR fixes code through a conversational process, iter-\natively interacting with LLMs based on error messages\nand test failure information. Each fix attempt involves\nmultiple interactions between the LLM and the error"}, {"title": "IV. RQ1:WHAT MISTAKES DO LLMS MAKE IN THE\nCODE GENERATION TASK?", "content": "In this section, we detail the methodology and results\nfor addressing RQ1.\nM\u0435\u0442\u043ed\u043el\u043egy t\u043e \u0410nswer RQ1\nMistake Identification: To identify the mistakes, we\nfollowed a two-step process. In the first step, each author\nindependently reviewed the incorrect LLM-generated\ncode and its correct version to identify non-syntactic\nmistakes. Specifically, each author manually examined\nthe lines of code that differed between the correct and"}, {"title": "RQ1", "content": "incorrect versions to pinpoint missing or incorrectly\nimplemented functionalities. In addition, we analyzed\ntest case failures and traced variable values during test\ncase execution for both versions to understand the code's\nbehavior. However, each incorrect LLM-generated code\ncould contain multiple mistakes, and each reviewer's\nindividual inspection might not cover all of them. To\nresolve this issue, the four authors, each with at least\nfive years of Python and Java programming experience,\nmet to discuss the mistakes in each incorrect LLM-\ngenerated code in the second step. Specifically, for every\nincorrect LLM-generated code, each author presented\ntheir identified mistakes, and all authors discussed the\nvalidity of the mistake. We showed that the mistakes\nwere valid by manually proposing a test case to detect\nthem. At the end of this process, we obtained a list of\nmistakes for each incorrect LLM-generated code.\nMistake Category Identification: After identifying the\nmistakes in the LLM-generated code, all four authors\nconducted open coding [35] with the negotiated agree-\nment [26] to categorize the identified mistakes. To do\nso, all authors first independently categorized the mis-\ntakes based on the adopted algorithms, data structures,\noperators, and API calls. Algorithms reflected the LLMs'\nreasoning and problem-solving logic, while operators,\ndata structures, and API calls indicated programming\nskills and potential misunderstandings of coding question\nspecifications. Next, all four authors met to discuss\nthe categorization of each non-syntactic mistake. For\neach mistake, all authors independently presented their\nidentified categories, and then the group discussed these\ncategories based on key elements such as algorithms,\ndata structures, operators, and API calls to reach a con-\nsensus. For disagreements after the discussions, a fifth\nauthor was consulted. Once an agreement was reached\nfor all the incorrect codes, we examined each category\nto merge those semantically similar ones. If a consensus\nregarding the merging was not reached, the fifth author\njoined the discussion, listened to the arguments from\nboth sides, and made the final decision. Finally, after\ngrouping mistakes into different categories, we refined\nthe category names to make them more informative.\nB. Answer to RQ1: Non-Syntactic Mistake Categories\nand Analysis\nIn this section, we present the mistake categories that\nwe identified in this study. We first present categories\nthat have also been identified by previous works:\nConditional Error (CE) This mistake category is as-\nsociated with conditional statements. Specifically, LLMs\nomit or misinterpret the necessary conditions described\nin the coding question, resulting in either flawed logic or\nmissing branches. An example is shown in Figure 1. The\ncoding question specification states that the generated"}, {"title": "is defined as:", "content": "CR =$\\frac{\\text{# Identified True Mistakes}}{\\text{# Total Mistakes}}$ (1)"}, {"title": "V. RQ2: CAN LLMS DETECT THE MISTAKES IN THE\nGENERATED CODE?", "content": "In this section, we describe the methodology and\nresults of RQ2.\nM\u0435th\u043ed\u043el\u043egy t\u043e \u0410nswer RQ2\nIn the first research question, we manually studied\nthe mistakes in LLM-generated code, offering a de-\ntailed understanding of where these LLMs fall short.\nHowever, while this manual analysis provides valuable\ninsights, it is not scalable for real-world applications\nwhere LLM-generated codes are prevalent. This brings\nus to the second research question: Do LLMs have the\nability to detect their mistakes in the generated code?\nTo achieve that, we utilized GPT-4 to automatically\nidentify the mistakes. Our rationale for selecting GPT-\n4 lies in its widespread adoption by researchers [48]"}, {"title": "VI. RQ3: WHAT ARE THE UNDERLYING REASONS\nFOR THE LLM'S MISTAKES IN THE CODE\nGENERATION TASK?", "content": "In RQ1 and RQ2, we explored the non-syntactic\nmistakes in LLM-generated codes and the ability to\ndetect these mistakes by a state-of-the-art LLM, GPT-\n4. To have a comprehensive understanding of these\nmistakes and provide insights for future research to build\nLLMs in generating high-quality source code or propose\neffective strategies to mitigate these mistakes, an in-\ndepth understanding of the underlying reasons is needed."}, {"title": "RQ3", "content": "We hypothesize that non-\nsyntactic mistakes can stem from various factors. One\ngroup of factors relates to the presentation of coding\nquestions, including the i) Phrasing used in the prompt,\nii) Presentation structure, and iii) Input-output pair\ndemonstration quality, due to their proven effect on LLM\nunderstanding [11], [21]. Another factor involves the\nLLMs\u2019iv) Training process, as previous works have\nshown its significant impact on machine learning [32].\nTo identify the underlying reasons for each mistake,\nwe employed the open coding process [35] combined\nwith a negotiated agreement approach [26]. Specifically,\nthe four authors independently reviewed the incorrect\nLLM-generated code, the correct reference code, the\ncoding question specification, and the identified non-\nsyntactic mistakes from RQ1 (Section IV) to determine\nthe underlying reason for each mistake. After this in-\ndependent review, the four authors met as a group to\ndiscuss the validity of each proposed reason, focusing\non whether the reason adequately explained the LLM's\nincorrect output. Each author presented their identified\nreason, and the group collectively worked to reach a\nconsensus on its validity. In cases where the authors\ncould not reach a consensus at this stage, a fifth author\nwas invited to review the differing perspectives and\nmake a final decision. Once the validity of a reason was\nestablished, the group proceeded to determine whether\nthis reason represented a new type or could be merged\nwith an existing one. If the authors were unable to\nreach an agreement during this stage, the fifth author\nagain joined the discussion, carefully considering the\narguments from both sides before making a final de-\ncision. Finally, we generated the code with modified\nprompt phrasing, presentation structure, and input-output\ndemonstration based on the identified reasons. The test\ncases were then executed on the newly generated code to\nverify its correctness. If the newly generated code could\npass all test cases, the identified reason was considered\nvalidated. We detail the steps below.\ni) Phrasing used in the prompt: LLM's understanding\nof the coding question specifications heavily contributes\nto its generated code quality. One important factor that\naffects LLM's understanding is the phrasing used in\nthe coding question specification. Here, we focused on\nthe word choice of those phrases that would confuse\nLLM's understanding. To identify mistakes introduced\ndue to the phrasing used in the prompt, all authors\nindependently reviewed the referenced correct code, the\nspecifications of the coding questions, the explanation"}, {"title": "ii) Presentation structure: Besides variations in word\nchoice, LLMs often overlook parts of the specifica-\ntion when the coding question is long [47], leading\nto incorrect LLM-generated code that fails to imple-\nment the overlooked specifications. Since LLMs are\nbased on the transformer architecture, where attention\nmechanisms indicate the model's focus during problem-\nsolving, LLMs overlooking certain contents can be at-\ntributed to insufficient attention allocation. To address\nthis issue, it is crucial to enhance the LLM's attention\ntowards the overlooked sections of the specification. We\nhypothesized that the position of information within the\ninput affects the LLM's attention distribution. Therefore,\nwe proposed that repositioning the relevant information\nwhile preserving its semantic meaning may prompt the\nLLM to allocate more attention, thereby improving the\nquality of the generated code. It is important to note\nthat the presentation structure emphasizes the position\nof the overlooked specification in the prompt, whereas\nthe phrasing used in the prompt centers on the specific\nwording of the specification.", "content": "In specific, we followed a similar approach. Each\nauthor individually compared the LLM-generated code\nwith the referenced correct code to identify any missing\nfunctionality. Each author located the position of the\nmissing mention of functionality within the coding ques-\ntion specification. Through negotiated agreement [26],\nthe authors reached a consensus on the missing mention\nof a functionality. To investigate whether the position\ncould cause the LLM to miss the functionality, we altered\nthe position of the missing functionality description in\nthe coding question specification. Specifically, we moved\nthe description of the missing functionality to a before\nand after location in the coding question (see Figure\n6 as an example). To ensure this adjustment did not\nchange the meaning of the coding question, all authors\nread and compared the descriptions before and after the\nchange. If the meaning was altered, we added the phrase\n\u201cnote that\" in the original description next to the missing\nfunctionality to prompt the LLM to pay more attention\nto it.\nAfter altering the position of the missing functionality\nor modifying the wording in the specification of the\ncode question, we prompted the LLM to generate a\nnew method using the setting described in Section III-D.\nThen, we ran all test cases on the newly generated code\nto see whether the new code passed all test cases. If it\ndid, this would indicate that the non-syntactic mistakes\nin incorrect code were caused by these factors in the\ncoding question specification.\niii) Input-output pair demonstration quality: Coding\nquestions provide example input-output pairs for the\ncode generation approaches to understand the desired\nfunctionality and input-output format. Therefore, LLMs'\nmistakes can also be attributed to the quality of those\ndemonstrations, such as how comprehensively the pairs\nconvey the meaning of the desired functionality or cover\nthe corner cases. To check this factor, we first collected\nall the generated code methods that could pass the\ndemonstrations in the given coding question but failed\nto pass some test cases with specific inputs. We then\ntraced the intermediate variable values at all locations\nusing program trace [39] and inspected the code logic\nto understand why the code worked for the demonstra-\ntion pairs but failed with specific inputs in test cases.\nEach author individually did this analysis and, through\nnegotiated agreement [26], reached a consensus on the\nunderstanding of the LLM's reasoning and thinking logic\nabout the given input-output pairs. Then, all authors as\na group compared this thinking logic with the coding\nquestion to understand whether LLMs misinterpreted or\noverfitted the demonstrations.\niv) Training-Induced: We posit that the mistakes in\nLLM-generated code can stem from model training, such\nas the quality of the training data corpus and the effec-\ntiveness of the model training process (i.e., the design\nof pre-training objectives, loss functions, etc..). LLMs\nwould learn frequent patterns in the training data corpus,"}, {"title": "B. Answer to RQ3: Reasons of Non-syntactic Mistakes", "content": "In this section, we report the six reasons we identified\nafter discussions among the authors. Table III shows the\nnumber of verified coding questions whose generated\ncode has mistakes that were caused by each reason.\nMisleading Coding Question Specification (MCQS)\nrefers to the cases where LLMs are confused by specific\nphrases used in the coding question specification, which\nis the most frequent reason across both datasets (Table\nIII). This reason matches our hypothesis of phrasing\nused in the prompt as LLM failed to comprehend the\ncoding question specification due to some used phrases.\nAmbiguous requirements might have caused this confu-\nsion, insufficient explanations of the desired functional-\nity, and vague constraints of input or output [52]. An\nexample is shown in Figure 7, where the LLM was\ntasked with the coding question specification: \u201cCheck\nif two words have the same characters.\u201d We analyzed\nthe incorrect code generated by the LLM and observed\nthat LLM used two dictionaries to separately count the\nfrequency of characters in each word. This suggested that\nthe LLM interpreted the coding question as checking\nif the two words are anagrams that is, whether they\ncontain the same characters with the same frequen-\ncies. Recognizing this misinterpretation, we revisited the\ncoding question specification and hypothesized that the\nconfusion arose from the phrase \"same characters\u201d as\nthe word \"same\" might have led the LLM to con-\nsider character frequencies. To verify our hypothesis,\nwe prompted the LLM to rephrase the instruction to"}, {"title": "VII. RQ4: HOW EFFECTIVELY CAN LLMS IDENTIFY\nTHE REASONS FOR MISTAKES IN THE CODE\nGENERATION TASK?", "content": "From RQ3, we analyzed a list of reasons for the\nmistakes that we identified through manual inspection.\nWhile those reasons bring valuable insights, their iden-\ntification procedure is labor intensive, making it im-\npractical for real-world reason identification due to the\nlarge amount of incorrect LLM-generated codes and\nthe time required for manual analysis. In this section,\nwe describe all the approaches using LLMs that we\nexperimented with to automatically identify the reasons\nfor non-syntactic mistakes."}, {"title": "A. Methodology to Answer RQ4", "content": "In Section VI, we manually identified the reasons\nfor the non-syntactic mistakes in LLM-generated code.\nHowever, pinpointing these reasons manually can be\nlaborious. In recent years, LLMs have demonstrated\nadvanced reasoning abilities in various tasks [23], [37].\nTherefore, we aimed to investigate the effectiveness\nof LLMs in automatically identifying the reasons. We\nexperimented with four approaches: Base Prompt, Ad-\nvanced Prompt, Advanced Prompt+Diff, and Advanced\nPrompt+ReAct. The prompts are included in our repli-\ncation package [20].\n1) Evaluation Benchmark: We built a benchmark\nbased on the reasons identified in Section VI. The bench-\nmark consists of 216 instances (118 from HumanEval-\nX and 98 from CoderEval), including incorrect LLM-\ngenerated code, its corresponding coding question, and\nthe human-labeled reasons.\n2) Base Prompt: As a baseline, we prompted the\nLLMs to identify the reasons behind non-syntactic mis-\ntakes in the incorrect code. The prompt included the cod-\ning question specification, the incorrect LLM-generated\ncode, the referenced correct code, the incorrect code's\nactual output, and the test failure information, along with\ninstructions to provide explanations. Importantly, we did\nnot include human-labeled reasons or their definitions\nin the prompt. This approach aimed to determine if\nthe LLMs could identify reasons comparable to those\nidentified by humans."}, {"title": "3) Advanced Prompt: In addition to all the infor-\nmation from the Base Prompt, we provided the defini-\ntions of the human-labeled reasons (Section VI) in this\nprompt. We anticipated that these definitions could guide\nthe LLMs in identifying the reasons and understanding\nthe desired granularity. The LLMs were instructed to\nreport the human-identified reasons and to describe their\nexplanations and reasoning steps.\n4) Advanced Prompt+Diff: In this approach, along\nwith the information included in the Advanced Prompt,\nwe provided the information of the code change between\nincorrect code and referenced correct code. We anticipate\nthat this might help the LLMs focus on the erroneous\nparts and subsequently help identify the reasons.\n5) Advanced Prompt+ReAct: In this approach, we\nused the ReAct [74] (reasoning + actions) prompt-\ning technique, which allows LLMs to engage in ac-\ntive reasoning while retrieving supportive information\nthrough various actions. This technique enables LLMs\nto dynamically choose and use retrieved information to\nenhance their reasoning. ReAct prompting is effective\nfor tasks requiring extensive information for informed\nreasoning and generation [74], making it suitable for our\nreason identification task. In practice, we implemented\ntools to retrieve all relevant information used in the\nAdvanced Prompt. The LLMs were tasked with reporting\nall human-labeled reasons, aided by the information\nretrieved from these tools.\nBelow, we described all the tools in detail:\n(i) Diff tool: The diff tool takes the incorrect LLM-\ngenerated and referenced correct code as input and builds\nthe code change representation. In specific, we used the\nunified_diff function from the python library difflib [14]\nto compare the two code snippets and generate a unified\ncode change representation (unified diff). The unified diff\nis a compact way of showing modifications of code lines\nand surrounding context lines as code context, which can\nhelp the LLMs focus on the erroneous parts.\n(ii) Function call analysis tool: The function call\nanalysis tool takes the incorrect LLM-generated code and\nreturns the invoked third-party library/built-in functions\nalong with the corresponding official documentation. The\ntool aims to help recognize LLM's incorrect knowledge\nabout these function calls by allowing the LLMs to com-\npare the function invocations in the code with the desired\nusage described in the documentation. We extracted all"}, {"title": "B. Answer to RQ4: Performance of Reason Identification", "content": "In this section, we present the results of evaluating\nLLMs' ability to identify the reasons with our bench-\nmark (Section VII-A1). The performance of GPT-4 is\nshown in Table IV.\nFor the Base Prompt, GPT-4 achieved an average F1\nscore of 0.35, indicating that a simple prompt to identify\nreasons for non-syntactic mistakes is not effective. When\nhuman-labeled reasons were included in the Advanced\nPrompt, the F1 score increased to 0.51, suggesting that\nthese definitions provided some guidance. However, the\nAdvanced Prompt+Diff did not enhance performance,\nimplying that the diff between correct and incorrect code\ndid not aid the LLM in focusing on errors. The Advanced\nPrompt+ReAct approach showed further improvement,\nwith the F1 score rising from 51% to 58%. This boost\nis likely due to the additional tools that support LLM\nreasoning, resembling the aids that humans might use\nto identify mistakes. While the performance in terms of\nthe average metric scores is subpar, our best approach\n(Advanced Prompt+ReAct) shows decent performance in\nidentifying some of the reasons, such as MFS, ITK, and\nEC with an F1 of more than 0.65. This indicates that\nthe Advanced Prompt+ReAct method is effective and\nsuggests further investigation."}, {"title": "VIII. THREATS TO VALIDITY", "content": "In this study, we have"}]}