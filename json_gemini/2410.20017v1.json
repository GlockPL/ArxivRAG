{"title": "Off-Policy Selection for Initiating Human-Centric Experimental Design", "authors": ["Ge Gao", "Xi Yang", "Qitong Gao", "Song Ju", "Miroslav Pajic", "Min Chi"], "abstract": "In human-centric tasks such as healthcare and education, the heterogeneity among patients and students necessitates personalized treatments and instructional interventions. While reinforcement learning (RL) has been utilized in those tasks, off-policy selection (OPS) is pivotal to close the loop by offline evaluating and selecting policies without online interactions, yet current OPS methods often overlook the heterogeneity among participants. Our work is centered on resolving a pivotal challenge in human-centric systems (HCSs): how to select a policy to deploy when a new participant joining the cohort, without having access to any prior offline data collected over the participant? We introduce First-Glance Off-Policy Selection (FPS), a novel approach that systematically addresses participant heterogeneity through sub-group segmentation and tailored OPS criteria to each sub-group. By grouping individuals with similar traits, FPS facilitates personalized policy selection aligned with unique characteristics of each participant or group of participants. FPS is evaluated via two important but challenging applications, intelligent tutoring systems and a healthcare application for sepsis treatment and intervention. FPS presents significant advancement in enhancing learning outcomes of students and in-hospital care outcomes.", "sections": [{"title": "1 Introduction", "content": "Human-centric systems (HCSs), e.g., used in healthcare facilities [50, 41, 13] and intelligent education (IE) [4, 22, 65], have widely employed reinforcement learning (RL) to enhance user experience by improving outcomes of disease treatment, knowledge gaining, etc. Specifically, RL has been used in healthcare to automate treatment procedures [50], or in IE that can induce policies automatically adapting difficulties of course materials and helping students to setup and refine study plans to improve learning outcomes [30, 81]. Though various existing offline RL methods can be adopted [15, 25, 3] for policy optimization, validation of policies' performance is often conducted by online testing [58, 71, 66, 9]. Given the long testing horizon (e.g., several years, or semesters, in healthcare, and IE, respectively) and the high cost of recruiting participants, online testing is considered exceedingly time- and resource-consuming, and sometimes could even be hindered by protocols overseeing human involved experiments, e.g., performance and safety justifications need to be provided before new medical device controllers can be tested on patients [48].\nRecently, off-policy evaluation (OPE) methods have been proposed to tackle such challenges by estimating the performance of target (evaluation) RL policies with offline data, which only requires the trajectories collected over behavioral polices given a priori; similarly, off-policy selection (OPS) targets to determine the most promising policies, out of the ones trained with different algorithms or hyper-parameter sets, that can be used for online deployment [2, 8, 42, 73, 78]. However, most existing OPS and OPE methods are designed in the context of homogenic agents, such as in robotics or games, where characteristics of the agents can be captured by their specifications, which are in general assumed fully known (e.g., degree of freedom, angular constraint of each joint).\nThe pivotal challenge in OPE/OPS for HCSs. In contrast, in HCSs, the participants can have highly diverse backgrounds, where each person may be associated with unique underlying characteristics that are not straightforward to be captured individually; due to the partial observability of participants' mind states and the limited size of the cohort that can be recruited for experiments with HCSs. For example, patients participated in healthcare research studies could have different health/disease records, while the students using an intelligent tutoring system in IE may have different mindsets toward studying the course. As a result, the optimal criteria for selecting the policy to be deployed to each participant can vary, and, more importantly, it would be intractable for existing OPS/OPE frameworks to determine what the policy selection criteria would be for a new participant who just joined the cohort. Consequently, there lacks a framework that can resolve the pivotal challenge in facilitating real-world HCSs \u2013 how to select a policy to deploy when a new participant joining the cohort, without having access to any prior offline data collected over the participant?\nIn this work, we introduce First-glance off-Policy Selection (FPS), to address the problem of determining the OPS criteria needed for each new participant joining the cohort (i.e., at t = 0 only, or without using information obtained from t >= 1 onwards), assuming that we have access to offline trajectories for a small batch of participants a priori, i.e., the offline data. Specifically, it first partitions the participants from the offline dataset into sub-groups, clustering together the ones pertaining to similar behaviors. Then, an unbiased value function estimator, with bounded variance, is developed to determine the policy selection criteria for each sub-group. At last, when new participants join, they will be recommended with policies selected according to the sub-groups they fall within. Note that FPS is distinguished from typical off-policy selection (OPS) setup in the sense that, the major goal of prior OPS approaches is to select the best policy over the entire population, while FPS aims to decide the best policy for each student who arrives to the HCS on-the-fly, leveraging the information observed at the initial step (t = 0) only.\nThe key contributions of this work are summarized as follows: (i) We introduce the FPS framework which is critical for closing the gap between OPS and applications pertaining to HCSs, i.e., selecting the policy that would maximize the gain of the new participants at the point of joining a cohort. To the best of our knowledge, this is the first framework that considers the new participant arrival's problem in the context of OPS in HCSs. (ii) We conduct extensive experiments to evaluate FPS in a real-world IE system, with 1,288 students participating over 5 years. Results have shown that, with the help of FPS, it improved the learning outcomes by 208% compared to policy selection criteria hand-crafted by instructors. Moreover, it leads to 136% increased outcome compared to policies selected by existing OPS methods. (iii) FPS is also evaluated against an important healthcare application, i.e., septic shock treatment [41, 44, 50], where it can accurately identifying the best treatment policies to be deployed to incoming patients, and outperforms existing OPS methods."}, {"title": "2 First-Glance Off-Policy Selection (FPS)", "content": "In this section, we introduce the FPS method, which determines the policy to be deployed to new participants that join an existing cohort, conditioned only on their initial states. Specifically, the participants pertaining to the offline dataset are partitioned into sub-groups according to their past behavior. Then, a variational auto-encoding (VAE) model is used to generate synthetic trajectories for each sub-group, augmenting the dataset and improving the state-action coverage. Moreover, an unbiased value function estimator, with bounded variance, is developed to determine the policy selection criteria for each sub-group. At last, when new participants join, they will be recommended with the policies conditioned on the sub-groups they fall within respectively. We start with a sub-section that introduces the problem formulation formally.\n2.1 Problem Formulation\nThe HCS environment is formulated as a human-centric Markov decision process (HC-MDP), which is a 7-tuple (S, A, P, S0, R, I, \u03b3). Specifically, S is the state space, A is the action space, P:S\u00d7A\u2192S defines transition dynamics from the current state and action to the next state, S0 defines the initial state distribution, R : S \u00d7 A \u2192 R is the reward function, I is the set of participants involved in the HCS, \u03b3 \u2208 (0, 1] is discount factor. Episodes are of finite horizon T. At each time-step t in online policy deployment, the agent observes the state st \u2208 S of the environment, then chooses an action at \u2208 A following the target (evaluation) policy \u03c0. The environment accordingly provides a reward rt = R(st, at), and the agent observes the next state st+1 determined by P. A"}, {"title": "2.2 Sub-Group Partitioning", "content": "In this sub-section, we introduce the sub-group partitioning step that partition the participants in the offline dataset into sub-groups. Furthermore, value functions over all candidate policies \u03c0\u2208 \u03a0 are learned respectively for each sub-group, to be leveraged as the OPS criteria for each sub-group.\nThe partitioning is performed over the initial state of each trajectory in the offline dataset, \u03c4\u03b2 \u2208 D. Given assumptions 2.1 and 2.2, and the fact that S0 (i)'s in general only share limited support across"}, {"title": "2.3 Trajectories Augmentation within Each Sub-Group", "content": "In HCSs, each sub-group may only contain a limited number of participants, due to the high cost of recruiting participants as well as time constraints in real-world experiments. For example, in"}, {"title": "2.4 The FPS Algorithm", "content": "Algorithm 1 FPS.\nRequire: A set of target policies \u03a0, offline dataset D.\nEnsure:\n// Training Phase.\n1: Calculate the number of subgroups M needed for D, using silhouette scores [17].\n2: Obtain the sub-group partitioning K = {K1, ..., KM } following Section 2.2.\n3: for each sub-group Km do\n4:Augment sub-group samples Tm with Tm.\n5:Use the estimator in Proposition 2.5 to obtain $D_{Km}^{\\pi,\\beta}$ for all candidate target policies \u03c0\u2208 \u03a0, over Tm UTm.\n6:Select the best candidate target policy \u03c0m that maximizes $D_{Km}^{\\pi,\\beta}$ as the one to be deployed over Km.\n// Deployment Phase.\n7: while the HCS receives the initial state so from a new participant do\n8:Determine the sub-group Km for the new participant.\n9:Deploy to the participant the best candidate policy \u03c0m specific to sub-group Km.\nThe overall flow of the FPS framework is described in Algorithm 1. The training phase directly follow from the sub-sections above. Upon deployment, FPS can help HCSs monitor each arriving participant, determine the sub-group the participant falls within, and select the policy to be deployed according to the initial state. Such real-time adaptability is important for HCSs in practice, and is different from existing OPS works which in general assume either the full trajectories or population characteristics are known [21, 73, 79]. For example, in practical IE, students may start learning irregularly according to their own schedules, hence can create discrepancies in their start times. Such methods fall short in cases when selecting policies based on population or sub-group information in the upcoming semester \u2013 they requires the data from all arriving students are collected upfront, which would be unrealistic. Note that, to the best of our knowledge, we are the first work that formally consider the problem of sub-typing arriving participants, and FPS is the first approach that solves this practical problem by introducing a framework that can work with HCSs in the real-world."}, {"title": "3 Experiments", "content": "FPS is tested over two types of HCSs, i.e., intelligent education (IE) and healthcare. Specifically, the real-world IE experiment involves 1,288 student participating in college entry-level probability course across 6 academic semesters. The goal is to use the data collected from the students of the first 5 semesters, to assign pre-trained RL lecturing policies to every student enrolled in the 6-th semester, in order to maximize their learning outcomes. The healthcare experiment targets for selecting pre-configured policies that can best treat patients with sepsis, over a simulated environment widely adopted in existing works [41, 42, 62, 32, 10]."}, {"title": "3.1 Baselines", "content": "Existing OPS/OPE. The most straightforward approach to facilitate OPS in HCSs is to select policies via existing OPS/OPE methods, by choosing the candidate target policy \u03c0\u2208 \u03a0 that achieves the maximum estimated return over the entire offline dataset, i.e., indiscriminately across all potential sub-groups. Specifically, 6 commonly used OPE methods are considered, i.e., Weighted IS (WIS) [49], Per-Decision IS (PDIS) [49], Fitted-Q Evaluation (FQE) [26], Weighted DR (WDR) [63], MAGIC [63], and Dual stationary DIstribution Correction Estimation (DualDICE) [40].\nExisting OPS/OPE with vanilla repeated random sampling (OPS+RRS). We also compare FPS against a classic data augmentation method in order to evaluate the necessity of the VAE-based method introduced in Section 2.3 \u2013 i.e., repeated random sampling (RRS) with replacement of the historical data to perform OPE. RSS has shown superior performance in some human-related tasks, such as disease treatment [42]. Specifically, all OPS/OPE methods considered above are applied to the RRS-augmented offline dataset, where the value of each candidate target policy is obtained by averaging over 20 sampling repetitions. However, note that RRS does not intrinsically consider the temporal relations among state-action transitions as captured by MDP.\nExisting OPS/OPE with VAE-based RRS (OPS+VRRS). This baseline perform OPS with RRS on augmented samples resulted from the VAE introduced in Section 2.3, in order to allow RRS to consider MDP-typed transitions, hence improve state-action visitation coverage of the augmented dataset. This method can, to some extent, be interpreted as an ablation baseline of FPS, by removing the sub-group partitioning step (Section 2.2), and slightly tweaking the VAE-based offline dataset augmentation step (Section 2.3) such that it does not need any sub-group information. Specifically, we set the amount of augmented data identical to the amount of original historical data, i.e., |T| = |T| = N, and RRS N samples from both set TUT to perform OPE. Final estimates are averaged results from 20 repeated sampling processes.\nFPS without trajectory augmentation (FPS-noTA). This is the ablation baseline that completely removes from FPS the augmentation technique introduced in Section 2.3.\nFPS for the population (FPS-P). We consider on additional ablation baseline that follows the same training steps as FPS (i.e., steps 1-7 of Alg. 1), but rather select a single policy that is identified (by FPS) as the best for majority of the sub-groups, to be deployed to all participants. In other words, after training, FPS produces the mapping h : K \u2192 I, while FPS-P will always deploy to every arriving participant the policy that appears most frequently in the set {h(Km)|Km \u2208 K}."}, {"title": "3.2 The Real-World IE Experiment", "content": "The IE system has been integrated into a undergraduate-level introduction to probability and statistics course over 6 semesters, including a total of 1,288 student participants. This study has received approval from the Institutional Review Board (IRB) at the institution to ensure ethical compliance. Additionally, oversight is provided by a departmental committee, which is responsible for safeguard-ing the academic performance and privacy of the participants. In this educational context, each learning session revolves around a student's engagement with a set of 12 problems, with this period referred to as an \"episode\" (horizon T = 12). During each step, the IE system offers students three actions: independent work, utilizing hints, or directly receiving the complete solution (primarily for study purposes). The states space is constituted by by 140 features that have been meticulously extracted from the interaction logs by domain experts, which encompass various aspects of the students' activities, such as the time spent on each problem and the accuracy of their solutions. The learning outcome is issued as the environmental reward at the end of each episode (0 reward for all other steps), measured by the normalized learning gain (NLG) quantified using the scores received from two exams, i.e., one taken before the student start using the system, and another after. Data collected from the first 5 semesters (over a lecturer-designed behavioral policy) are used to train FPS for selecting from a set of candidate policies to be deployed to each student in the cohort of the 6-th semester, including 3 pre-trained RL policies and 1 benchmark policy (whose performance benchmark the lower-bound of what could be tested with student participants). See Appendix A for the definition of NLG, details on pre-trained RL policies, and more.\nMain results. Figure 1(a) presents students' performance under policies selected by different methods. Overall, FPS was the most effective policy selection leading to the greatest average student performance. The return difference between FPS and the two ablation, FPS-noTA and FPS-P, illustrate the importance of augmenting offline trajectories (as introduced in Section 2.3) and assign to arriving students policies that better fit the characteristics shared within their sub-groups, respectively. Moreover, most existing OPS/OPE methods tend to select sub-optimal policies that resulted in better learning gain than the benchmark policy. Note that we also observed that DualDICE could not distinguish the returns over all target policies; thus, it is unable to be used for policy selection in this experiment and we omit its results. It is also important to evaluate how accurate the value estimation V\u03c0 would be for the best candidate policy selected across all methods, over the arriving student cohort at the 6-th semester, as illustrated in Figure 1(b). FPS provided more accurate policy estimation by achieving the smallest error between true and estimated policy rewards. With VRRS, most OPS methods improved their policy estimation performance, which was benefited from the richer state-action visitation coverage provided by the synthetic samples generated by VRRS. However, even with such augmentations, existing OPS methods still chose sub-optimal policies, which justified the importance of considering participant-specific characteristics in HCSs, which is tackled by sub-group partitioning in FPS (Section 2.2).\nMore discussions. For a more comprehensive understanding of student behaviors affected by the policy being deployed in IE, we further investigate how the sub-groups are partitioned and how the policies being assigned to each sub-group perform. Specifically, FPS identified four subgroups (i.e., K1, K2, K3, K4) as a result of Section 2.2. Under the behavioral policy, the average NLG across all students is 0.9 with slight improvement after tutoring. Specifically, K1(Ntrain = 345, Ntest = 30) and K2(Ntrain = 678, Ntest = 92) achieved average NLG of 1.9 [95% CI, 1.7, 2.1] and 0.7 [95% CI, 0.6, 0.8] under the behavioral policy, respectively. In the testing (6-th) semester, FPS"}, {"title": "3.3 The Healthcare Experiment", "content": "In this experiment, we consider selecting the policy that can best treat sepsis for each patient in the ICU, leveraging the simulated environment introduced by [44], which has been widely adopted in existing works [18, 42, 62, 32, 10, 41]. Specifically, the state space is constituted by a binary indicator for diabetes, and four vital signs {heart rate, blood pressure, oxygen concentration, glucose level} that take values in a subset of {very high, high, normal, low, very low}; size of the state space is |S| = 1440. Actions are captured by combinations of three binary treatment options, {antibiotics, vasopressors, mechanical ventilation}, which lead to |A| = 23. Three candidate target policies are considered and provided by [41], i.e., (i) without antibiotics (WOA) which does not administer antibiotics right after the patient is admitted, (ii) with antibiotics (WA) that always administer antibiotics once the patient is admitted, (iii) an RL policy trained following policy iteration (PI). Note that as pointed by [41], the true returns of WA and PI are usually close, since antibiotics are in general helpful for treating sepsis, which is also observed in our experiment; see Table 1. Moreover, a simulated unrecorded comorbidities is applied to the cohort, capturing the uncertainties caused by patient's underlying diseases (or other characteristics), which could reduce the effects of the antibiotics being administered. See Appendix B for more details in regards to the environmental setup."}, {"title": "4 Related Works", "content": "Off-policy selection (OPS). OPS are typically approached via OPE in existing works, by estimating the expected return of target policies using historical data collected under a behavior policy. A variety of contemporary OPE methods has been proposed, which can be mainly divided into three categories [69]: (i) direct methods that directly estimate the value functions of the evaluation policy [40, 64, 76, 73], including but not limited to model-based estimators (MB) [46, 76], value-based estimators [26] such as Fitted Q Evaluation (FQE), and minimax estimators [31, 77, 67] such as DualDICE [74]; (ii) inverse propensity scoring, or indirect methods [49], such as Importance Sampling (IS) [6]; (iii) hybrid methods combine aspects of both inverse propensity scoring and direct methods [63], such as DR [19]. In practice, due to expensive online evaluations, researchers generally selected the policy with the highest estimated rewards via OPE. For example, Mandel et al. selected the policy with the maximum IS score to be deployed to an educational game [34]. Recently, some works focused on estimator selection or hyperparameter tuning in off-policy selection [42, 72, 60, 39, 24, 28, 62, 47]. However, retraining policies may not be feasible in HCSs as online data collection is time- and resource-consuming. More importantly, prior work generally selected policies without considering the characteristics of participants, while personalized policy is flavored towards the needs specific to HCSS.\nRL-empowered automation in HCSs. In modern HCSs, RL has raised significant attention toward enhancing the experience of human participants. Previous studies have demonstrated that RL can induce IE policies [34, 57, 70, 81]. For example, Zhou et al. [81] applied hierarchical reinforcement learning (HRL) to improve students' normalized learning gain in a Discrete Mathematics course, and the HRL-induced policy was more effective than the Deep Q-Network induced policy. Similarly, in healthcare, RL has been used to synthesize policies that can adapt high-level treatment plans [50, 41, 32], or to control medical devices and surgical robotics from a more granular level [13, 33, 52]. Since online evaluation/testing is high-stake in practical HCSs, effective OPS methods are important in closing the loop, by significantly reducing the resources needed for online testing/deployment and preemptively justifying safety of the policies subject to be deployed."}, {"title": "5 Conclusion and Limitation", "content": "In this work, we introduced the FPS framework that facilitated policy selection in real-world HCSs; it tackled the off-policy deployment with new arrivals problem that is pivotal for RL policy deployment in HCSs. Unlike existing OPS methods, FPS customized the policy selection criteria for each sub-group respectively. FPS was tested in a real-world IE experiment and a simulated sepsis treatment environment, which significantly outperformed baselines. Though in the future it would be possible to extend FPS to a offline RL policy optimization framework, however, in this work we specifically focus on the OPS task in order to isolate the source of improvements brought in by sub-group partitioning and trajectory augmentation. Future avenues along the line of FPS also include deriving estimators"}, {"title": "A Detailed Setup of the IE Experiment and Additional Discussions", "content": "A.1 The IE System for the College Entry-Level Course.\nThough the problem setting and our method are general and can be applied to other interactive IE systems, we primarily focus on the system specifically used in an undergraduate probability course at a university, which has been extensively used by over 1, 288 students with ~800k recorded interaction logs through 6 academic years. The IE system is designed to teach entry-level undergraduate students with ten major probability principles, including complement theorem, mutually exclusive theorem, independent events, De Morgan's theorem, addition theorem for two events, addition theorem for three events, conditional independent events, conditional probability, total probability theorem, and Bayes' rule.\nEach students went through four phases, including (i) reading the textbook; (ii) pre-exam; (iii) studying on the IE system; and (iv) post-exam. During the reading textbook phase, students read a general description of each principle, review examples, and solve some training problems to get familiar with the IE system. Subsequently, they take a pre-exam comprising a total of 14 single- and multiple-principle problems. During the pre-exam, students are not provided with feedback on their answers, nor are they allowed to go back to earlier questions (so as the post-exam). Then, students proceed to work on the IE system, where they receive the same 12 problems in a predetermined order. After that, students take the 20-problem post-exam, where 14 of the problems are isomorphic to the pre-exam and the remainders are non-isomorphic multiple-principle problems. Exams are auto-graded following the same grading criteria set by course instructors.\nSince students' underlying characteristics and mind states are inherently unobservable [34], the IE system defined its state space with 142 features that could possibly capture students' learning status based on their interaction logs, as suggested by domain experts. While tutoring, the agent makes decisions on two levels of granularity: problem-level first and then step-level. For problem-level, it first decides whether the next problem should be a worked example (WE) [61], problem-solving (PS), or a collaborative problem-solving worked example (CPS) [56]. In WEs, students, observe how the tutor solves a problem; in PSs, students solve the problem themselves; in CPSs, the students and the tutor co-construct the solution. If a CPS is selected, the tutor will then make step-level decisions on whether to elicit the next step from the student or to tell the solution step to the student directly. Besides post-exam score, another important measure of student learning outcomes is their normalized learning gain (NLG), which is calculated by their pre- and post-exam scores $NLG = \\frac{score_{postexam} - score_{preexam}}{\\sqrt{1 - score_{preexam}}}$. The NLG defined in [4], represents the extent to which students have benefited from the IE system in terms of improving their learning outcomes.\nA.2 Classroom Setup\nParticipants recruitment. All participants were entry-level undergraduates majoring in STEM and enrolled in the Probability course in a college. They were recruited via invitation emails and told the procedure of the study and their data were used for research purpose only, and the study was an opt-in without influence on their course grades. Participants can also opt-in not recording their logs and quit the study any time. No demographics data or course grades were collected. All participants had acknowledged the study procedure and future research conducted using their logs."}, {"title": "A.3 Environmental Setup of the IE System", "content": "A.3.1 State Features.\nThe state features were defined by domain experts that could possible capture students' learning status based on their interaction logs. In sum, 142 features with both discrete and continuous values are extracted, we provide summary descriptions of the features characterized by their systematic functions: (i) Autonomy (10 features): the amount of work done by the student, such as the number of times the student restarted a problem; (ii) Temporal Situation (29 features): the time-related information about the work process, such as average time per step; (iii) Problem-Solving (35 features): information about the current problem-solving context, such as problem difficulty; (iv) Performance (57 features): information about the student's performance during problem-solving, such as percentage of correct entries; (v) Hints (11 features): information about the student's hint usage, such as the total number of hints requested.\nA.3.2 Actions & rewards.\nSee A.1 above.\nA.3.3 Behavior policy.\nThe behavior policy follows an expert policy commonly used in e-learning [80], randomly taking the next problem as a worked example (WE), problem-solving by students (PS), or a collaborative problem-solving working examples (CPS). Note that the three decision choices are designed by domain experts that are found can support students' learning in prior works [56, 61], thus the expert policy is considered as effective.\nA.3.4 Target (evaluation) policies.\nIn total, four target policies, including three RL-induced and the expert policy, were examined in testing semester. The three RL-induced policies were trained using off-policy DQN-based algorithm, and passed expert sanity check. In this study, expert sanity check were conducted by departments and independent instructors for pre-examination of the target policies.\nSpecifically, we employed the DQN-based algorithm designed by domain researchers [20], called Critical-RL, that have achieved empirical significance in real-world classrooms, and passed expert sanity check by our institutions. First, for each problem, a pair of adversarial policies using vanilla DQN algorithm were induced, including an original policy induced using the original rewards and an inversed policy induced using the inversed rewards (i.e., the negative value of the original rewards). Then, critical decisions are identified following two rules: (1) Given the state, the two policies make opposite decisions; and (2) the decision is important (critical) for both policies. For a given state, rule (1) is tested first. If the adversarial policies make the same decisions, it is not critical. Otherwise,"}, {"title": "A.3.5 Sub-group identification.", "content": "Specifically, to learn the subgroups in the IE system, we leverage an off-the-shelf algorithm called Toeplitz inverse covariance-based clustering (TICC) [17] to map initial logs S0 into M clusters based on the values of student-sensitive features (as defined in Appendix A.4), where each s0 \u2208 S0 is associated with a cluster from the set K = {K1, . . ., KM }, over the offline dataset, from which the number of individuals within each cluster is intrinsically determined. Specifically, TICC initially clusters all states from the offline dataset, where the states that are mapped to the same cluster can be considered to share the graphical connectivity structure of cross-features and temporal information captured by TICC. Then the clusters of initial states can be determined accordingly from the clustering outcomes. We consider using TICC because of its superior performance in clustering compared to traditional distance-based methods such as K-means, especially with human behavior-related tasks [17, 75], such that the clusters of initial logs could be more scalable and capable of the evolving individuals and their behaviors in real-world HCSs. For a new participant arriving in the testing period, the cluster where the participant may belong to is the cluster exhibiting the least averaging distance between the initial states of the participant and samples within the cluster captured from the offline dataset.\nThe size of clusters is determined by a data-driven procedure following the original TICC work (i.e., it is determined with the highest silhouette score in clustering historical trajectories) [17]. Note that we exhibit TICC as an example in our proposed pipeline, while it can be replaced by other partitioning approaches if needed. Then, we assume subgroup partitioning is consistent with cluster assignments associated with initial logs, i.e., students whose initial logs are associated with the same cluster index are considered from the same subgroup.\nTICC Problem. Each cluster m \u2208 [1, M] is defined as a Markov random field [54], or correlation network, captured by its Gaussian inverse covariance matrix $\\Sigma_m^{-1} \\in \\mathbb{R}^{c \\times c}$, where c is the dimension of state space. We also define the set of clusters $\\mathcal{K} = {K_1, ..., K_M} \\subset \\mathbb{R}$ as well as the set of inverse covariance matrices $\\Sigma^{-1} = {\\Sigma_1^{-1},...,\\Sigma_M^{-1}}$. Then the objective is set to be: $\\underset{\\Sigma^{-1}, \\mathcal{K}}{\\text{max}} \\sum_{m=1}^M [\\sum_{s_i \\in \\mathcal{K}_m} \\mathcal{L}(s_i; \\Sigma_m^{-1}) - \\epsilon |\\mathcal{N}(s_i) \\cap \\mathcal{K}_m|]$, where the first term defines the log-likelihood of $s_i^{(t)}$ coming from $\\mathcal{K}_m$ as $\\mathcal{L}(s_i; \\Sigma_m^{-1}) = -\\frac{1}{2}(s_i - \\mu_m)^T \\Sigma_m^{-1}(s_i - \\mu_m) + \\log \\det \\Sigma_m^{-1} - \\log(2\\pi)$ with $\\mu_m$ being the empirical mean of cluster $\\mathcal{K}_m$, the second term $\\epsilon |\\mathcal{N}(s_i) \\cap \\mathcal{K}_m|$ penalizes the adjacent events that are not assigned to the same cluster and $\\epsilon$ is a constant balancing off the scale of the two terms. This optimization problem can be solved using the expectation-maximization family of algorithms by updating $\\Sigma^{-1}$ and $\\mathcal{K}$ alternatively [17]."}, {"title": "A.4 Data Pre-Processing for Sub-Group Partitioning with the IE Experiment", "content": "The initial logs (serving as the initial states in the MDP) of students are used for sub-group partitioning, which is now only benefited from the FPS framework design but over two educational perspectives. First, initial logs may reflect not only the background knowledge of students but their interaction habits [11], without specific information related to behavior policies that may be distracting for sub-group partitioning. Though some existing works utilize demographics or grades of students from their prior taken courses to identify student subgroups [1, 59], it may not be feasible in practice due to the protection of student information by institutions. Second, prior works have found that initial logs can be informative to indicate learning outcomes of students [35], which makes it possible for the IE system to customize the policies with the goal of improving learning outcomes for each subgroup."}, {"title": "A.5 More Discussions over the Results from Section 3.2", "content": "However", "34": "."}, {"34": ".", "36": ".", "characteristics": "i) System-assigned: the features", "Student-centric": "the features", "Interaction-driven": "the features, which contain characteristics from both system-assigned and student-centric types, are assumed to be mixed-style features that are affected by both system and individuals. For example, the number of tells since elicit is set with a default value by the system while changing over time depending on students' progress.\nSub-group partitioning with distilled features via feature taxonomy. Since system-assigned features are mainly dominated by system design and remain static across students on each problem, for the purpose of subgroup partitioning, we focus on the two types of features, student-centric and"}]}