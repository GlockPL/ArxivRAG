{"title": "CHECK-EVAL: A Checklist-based Approach for Evaluating Text Quality", "authors": ["Jayr Pereira", "Roberto Lotufo"], "abstract": "Evaluating the quality of text generated by large language models (LLMs) remains a significant challenge. Traditional metrics often fail to align well with human judgments, particularly in tasks requiring creativity and nuance. In this paper, we propose CHECK-EVAL, a novel evaluation framework leveraging LLMs to assess the quality of generated text through a checklist-based approach. CHECK-EVAL can be employed as both a reference-free and reference-dependent evaluation method, providing a structured and interpretable assessment of text quality. The framework consists of two main stages: checklist generation and checklist evaluation. We validate CHECK-EVAL on two benchmark datasets: Portuguese Legal Semantic Textual Similarity and SUMMEVAL. Our results demonstrate that CHECK-EVAL achieves higher correlations with human judgments compared to existing metrics, such as G-EVAL and GPTSCORE, underscoring its potential as a more reliable and effective evaluation framework for natural language generation tasks. The code for our experiments is available at https://anonymous.4open.science/\nr/check-eval-ODB4.", "sections": [{"title": "1 Introduction", "content": "Evaluating the quality of text generated by large language models (LLMs) remains an open problem within the field of natural language generation (NLG). Traditional evaluation metrics, such as BLEU [6], ROUGE [4], and METEOR [1], often show limited correlation with human judgments, especially in tasks that require creativity and diversity, such as dialogue generation and summarization. Despite advancements in LLMs, which can produce high-quality, fluent texts that closely mirror human writing, the challenge lies in accurately assessing these outputs quality.\nRecent approaches utilizing LLMs as text quality evaluators have shown promise, yet they still fall short in achieving reliable alignment with human judgments [5,3]. This ongoing challenge underscores the necessity for more effective"}, {"title": "2 Related Work", "content": "The evaluation of automatically generated text has been a persistent challenge in the NLG field. Traditional metrics such as BLEU, ROUGE, and METEOR have been extensively used but have shown limitations in aligning with human judgment, particularly in tasks requiring creativity and nuance [2]. In recent years, more sophisticated evaluation frameworks leveraging LLMs have been"}, {"title": "3 CHECK-EVAL", "content": "In this section, we introduce CHECK-EVAL, an LLM-based evaluation framework designed to assess the quality of automatically generated text. CHECK-"}, {"title": "3.1 Checklist Generation", "content": "The first stage of CHECK-EVAL involves generating a checklist of key points to evaluate the quality of the candidate text. Below, we describe the checklist generation process for the framework's variations.\nReference-Guided and Candidate-Guided Checklist Generation For this step, let's consider the reference and the candidate text used in the reference-guided and candidate-guided variations, respectively, as the source document. The LLM is prompted to extract the essential information from the source document and create an evaluation checklist based on predefined evaluation criteria. The checklist serves as a structured reference for the key points that should be present in the text to be evaluated.\nFigure 2 shows the prompt used to generate the checklist from a source text given specific evaluation criteria. The criteria can be adjusted according to the desired evaluation focus, such as consistency, coherence, relevance, and fluency, which are generally used in text summarization tasks. Defining the evaluation"}, {"title": "3.2 Checklist Evaluation", "content": "The second stage of the CHECK-EVAL framework involves evaluating the candidate text based on the generated checklist. The LLM is prompted to compare the content of the candidate text to the key points in the checklist and determine the presence or absence of each key point.\nFigure 4 shows the prompt used to evaluate a candidate text based on the generated checklist for either the reference-guided or candidate-guided variations. The prompt specifies the evaluation criteria in this case, consistency-which is an optional variable that can be changed according to the desired evaluation focus. The LLM is instructed to assess the defined criteria of the text by comparing its content to the key points in the checklist and determining whether each key point is present. The checklist ensures a structured and comprehensive assessment.\nFor the criterion-guided variation, the prompt is similar to the one used for the reference-guided and candidate-guided variations, but the reference and the candidate text are passed together with the checklist generated based on the chosen evaluation criteria. The LLM is prompted to evaluate the candidate text against the reference text based on the checklist.\nThe output of the checklist evaluation stage is a score that reflects the quality of the candidate text based on the presence or absence of key points in the checklist. The score indicates how well the text captures the essential elements of the source document according to specific evaluation criteria. For example,"}, {"title": "4 Experiments", "content": "In this section, we describe the experiments conducted to evaluate the performance of CHECK-EVAL. We begin by detailing the datasets used, followed by the experimental settings."}, {"title": "4.1 Portuguese Legal Semantic Textual Similarity", "content": "We evaluated CHECK-EVAL using the Portuguese Legal Semantic Textual Similarity [7] dataset, a benchmark dataset for evaluating the semantic similarity of legal texts in Portuguese. The dataset consists of pairs of legal documents, each annotated with a similarity score. The dataset provides two versions of the annotations: one performed by legal experts on 32 pairs of legal documents and another automated annotation using heuristics based on legal cases metadata. The overall correlation between the human and automated annotations is 0.45 (Pearson correlation coefficient) and 0.43 (Spearman correlation coefficient). We experimented with the 32 pairs annotated by legal experts and a subset of 100 pairs annotated by the automated method randomly selected from the dataset.\nExperimental Settings We evaluated the first two variations of CHECK-EVAL (Reference-Guided and Candidate-Guided) using the Portuguese Legal Semantic Textual Similarity dataset. As we consider the reference-guided variation as the recall evaluation and the candidate-guided variation as the precision evaluation, we also computed the F1 score as the harmonic mean of the recall and precision scores. We used OpenAI's GPT-4-turbo model to perform both checklist generation and checklist evaluation. The following steps outline the experimental setup:\n1. Checklist Generation: For each text pair (text 1 and text 2) in the dataset, we prompted the LLM to generate a checklist of key points based on the criteria of relevance, coherence, consistency, and fluency. For the reference-guided variation, the checklist was generated from text 1, and for the candidate-guided variation, the checklist was generated from text 2. The prompt used for checklist generation is a translation of the prompt used for the SUM-MEVAL dataset, as shown in Figure 2.\n2. Checklist Evaluation: Each text pair was evaluated based on the generated checklist. The LLM was prompted to assess the presence or absence of each key point in text 2 (candidate text) compared to text 1 (reference text). The prompt used for checklist evaluation is a translation of the prompt used for the SUMMEVAL dataset, as shown in Figure 4."}, {"title": "4.2 SUMMEVAL Dataset", "content": "We also evaluated CHECK-EVAL in a reference-free evaluation scenario using the SUMMEVAL dataset [2]. The SUMMEVAL dataset consists of automatically generated summaries for news articles from the CNN/DailyMail dataset, along with human annotations for the quality dimensions of coherence, consistency, fluency, and relevance. This dataset provides a comprehensive benchmark for comparing the performance of different evaluation metrics against human judgments. In this case, we evaluated the Criterion-Guided variation of CHECK-EVAL using the SUMMEVAL dataset.\nExperimental Settings For our experiments, we used the GPT-4 model to perform both checklist generation and checklist evaluation. We prompted GPT-4 to generate a checklist of key points based on the evaluation criteria provided in the SUMMEVAL paper [2]. The following steps outline the experimental setup:\n1. Checklist Generation: For each of the evaluation criteria (consistency, relevance, coherence, and fluency), we prompted GPT-4 to generate a checklist of key points based on the criteria definitions provided in the SUMMEVAL paper [2].\n2. Checklist Evaluation: Each candidate summary was evaluated against the generated checklist. GPT-4 was prompted to assess the presence or absence of each key point in the summary. The prompt used for checklist evaluation is shown in Figure 4.\n3. Scoring: The final score for each candidate summary was calculated by counting the number of key points present in the summary, providing a quantitative measure of its quality."}, {"title": "5 Results", "content": "In this section, we present the results of our experiments evaluating the performance of CHECK-EVAL."}, {"title": "5.1 Portuguese Legal Semantic Textual Similarity", "content": "Table 1 summarizes the results of CHECK-EVAL on the Portuguese Legal Semantic Textual Similarity dataset. The table presents the Pearson (p) and Spearman (ps) correlations for the reference-guided and candidate-guided variations, as well as the F1 score, which is the harmonic mean of the recall (reference-guided) and precision (candidate-guided) scores. The results demonstrate that CHECK-EVAL"}, {"title": "5.2 SUMMEVAL Dataset", "content": "We compare CHECK-EVAL against existing reference-free evaluation metrics, including G-EVAL and GPTSCORE, using the SUMMEVAL dataset. The primary evaluation metrics used are Spearman's rank correlation coefficient (p) and Kendall-Tau correlation coefficient (7), which measure the correlation between the automated evaluation scores and human judgments. Table 3 summarizes the results of CHECK-EVAL compared to G-EVAL and GPTSCORE across four evaluation criteria: consistency, relevance, coherence, and fluency. Additionally, we report the average correlation across all criteria.\nConsistency CHECK-EVAL achieves a Spearman correlation of 0.605 and a Kendall-Tau correlation of 0.570 for the consistency criterion, outperforming both G-EVAL (0.507, 0.425) and GPTSCORE (0.449, -) significantly. This indicates that CHECK-EVAL's checklist-based approach provides a more reliable assessment of the factual alignment between the generated summaries and the source documents.\nRelevance For relevance, CHECK-EVAL achieves a Spearman correlation of 0.502 and a Kendall-Tau correlation of 0.420. Although G-EVAL slightly outperforms CHECK-EVAL in this criterion with a Spearman correlation of 0.547 and Kendall-Tau correlation of 0.433, CHECK-EVAL still shows competitive performance, highlighting its robustness across different evaluation dimensions."}, {"title": "6 Conclusions", "content": "In this paper, we introduced CHECK-EVAL, a novel evaluation framework for assessing the quality of automatically generated text. CHECK-EVAL leverages large language models (LLMs) to generate and evaluate a checklist of key points derived from the source document and the candidate summary. Our experiments demonstrated that CHECK-EVAL significantly outperforms existing text evaluation metrics, such as GPTSCORE and G-EVAL, in terms of correlation with human judgments across various dimensions of text quality, including consistency, relevance, coherence, and fluency.\nOne of the key strengths of CHECK-EVAL is its ability to provide a structured and interpretable assessment of text quality. By focusing on key points extracted from the source document, CHECK-EVAL offers a detailed evaluation framework that aligns with human judgment. The checklist-based approach aims to ensure that all essential elements of the source document are considered, leading to a more comprehensive and focused evaluation. This way, this method tries to mitigate the ambiguity and variability often associated with human judgment"}, {"title": "Declaration of Generative AI and AI-assisted technologies in the writing process", "content": "During the preparation of this paper, the authors used ChatGPT to check the grammar and semantics of the human written text. After using this tool/service, the authors reviewed and edited the content as needed and take full responsibility for the content of the publication."}]}