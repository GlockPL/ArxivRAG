{"title": "FedPIA - Permuting and Integrating Adapters leveraging Wasserstein Barycenters\nfor Finetuning Foundation Models in Multi-Modal Federated Learning", "authors": ["Pramit Saha", "Divyanshu Mishra", "Felix Wagner", "Konstantinos Kamnitsas", "J. Alison Noble"], "abstract": "Large Vision-Language Models (VLMs), possessing millions\nor billions of parameters, typically require large text and im-\nage datasets for effective fine-tuning. However, collecting\ndata from various sites, especially in healthcare, is challeng-\ning due to strict privacy regulations. An alternative is to fine-\ntune these foundation models on end-user devices, such as\nin medical clinics and hospitals, without sending data to a\nserver. These local clients typically have limited computing\npower and small datasets, which are not enough for fully\nfine-tuning large VLMs on their own. A naive solution to\nthese scenarios is to leverage parameter-efficient fine-tuning\n(PEFT) strategies such as adapters and apply federated learn-\ning (FL) algorithms to combine the learned adapter weights,\nthereby respecting the resource limitations and data privacy\nof the clients. However, this approach does not fully leverage\nthe knowledge from multiple adapters trained on diverse data\ndistributions and for diverse tasks. The adapters are adversely\nimpacted by data heterogeneity and task heterogeneity across\nclients resulting in sub-optimal convergence. To this end,\nwe propose a novel framework called FedPIA that improves\nupon the naive combinations of FL and PEFT by introduc-\ning Permutation and Integration of the local Adapters in the\nserver and global Adapters in the clients exploiting Wasser-\nstein barycenters for improved blending of client-specific and\nclient-agnostic knowledge. This layerwise permutation helps\nto bridge the gap in the parameter space of local and global\nadapters before integration. We conduct over 2000 client-\nlevel experiments utilizing 48 medical image datasets across\nfive different medical vision-language FL task settings en-\ncompassing visual question answering as well as image and\nreport-based multi-label disease detection. Our experiments\ninvolving diverse client settings, ten different modalities, and\ntwo VLM backbones demonstrate that FedPIA consistently\noutperforms the state-of-the-art PEFT-FL baselines. Code/FL\nset up: https://github.com/PramitSaha/Fed-PEFT.", "sections": [{"title": "Introduction", "content": "Large Vision-Language Models (VLMs) have recently\nachieved significant progress in multi-modal learning (Li\net al. 2021; Kim, Son, and Kim 2021). These models excel\nat integrating and processing information from both image\nand text modalities, achieving impressive results when fine-\ntuned for real-world multi-modal tasks, such as Visual Ques-"}, {"title": "Background and Related Work", "content": "Federated Learning (FL) FL enables various clients to\ncollaboratively train models in a decentralized manner with-\nout sharing local data. The classical FL framework, Fe-\ndAvg (McMahan et al. 2017), offers a practical method for\nmodel aggregation. However, its performance is adversely\nimpacted by client non-IID data distributions. Consequently,\nseveral modifications have emerged to address data hetero-\ngeneity (Li et al. 2020; Karimireddy et al. 2020; Acar et al.\n2021). FedProx (Li et al. 2020) adds a proximal term to the\nclient loss function thereby enforcing constraints on local\nupdates. Another work called Scaffold (Karimireddy et al.\n2020) employs control variates to enhance local updates,\nwhile FedDyn (Acar et al. 2021) dynamically regulates the\nclient loss function to align the local and the global objec-\ntives. Moon (Li, He, and Song 2021) regularizes local train-\ning via contrastive learning. All these works assume uni-\nmodal data in all clients.\nParameter-efficient Fine-tuning (PEFT) PEFT tech-\nniques can be categorized into three families: adaptive meth-\nods, selective methods, and prompt tuning. Adaptive meth-\nods are additive PEFT techniques that integrate adapters or\nsmall neural network blocks into the Transformer layers (Hu\net al. 2022; Rebuffi, Bilen, and Vedaldi 2018; Li, Liu, and\nBilen 2022; Lian et al. 2022). Selective PEFT fine-tunes\na subset of the existing parameters to enhance model per-\nformance on downstream tasks (Ben Zaken, Goldberg, and\nRavfogel 2022; Frankle, Schwab, and Morcos 2021; Tou-\nvron et al. 2022). Prompt tuning methods modify the origi-\nnal input, whether an embedding or the actual instance, with\nsome prompts consisting of additional trainable parameters\nor perturbations (Lester, Al-Rfou, and Constant 2021; Jia\net al. 2022; Li and Liang 2021).\nPEFT and Federated Learning The application of PEFT\nin multimodal FL remains relatively unexplored. Previous\nresearch has primarily adapted PEFT for FL in a straight-\nforward manner, particularly focusing on uni-modal tasks,\ni.e., vision or NLP. (Chen et al. 2022) and (Sun et al. 2022)\nevaluate existing PEFT baselines combined with FL in vi-"}, {"title": "Methodology: FedPIA", "content": "Problem definition: We tackle a heterogeneous FL prob-\nlem involving K clients. Each client k possesses a pri-\nvate multimodal dataset Dk, which includes both visual\n(uk) and textual (tk) data. Specifically, each local dataset\nDk can be decomposed into Nk image-text-output triplets\n{(Uki,tki, aki) i \u2208 {1,...,Nk}}. We assume that the\nmarginal distribution of Uki, tki, and ak\u2081 varies across\nclients, indicating data heterogeneity in the visual, textual\nand task domains. We define the answer or label pool Ak =\n{@k1,..., akck} with Ck ground-truth answers or labels for\nclient k, and frame our task as a Ck-way classification prob-\nlem. Note that the answer pool and the total number of\nclasses differ from client to client, thereby inducing hetero-\ngeneity in the FL model. Let f be a foundation model pa-\nrameterized by 0. Starting from the pre-trained weights 00,\nthe goal is to optimize client-specific losses Lk by gradient\ndescent. Due to client-specific data and resource constraints,\nfull fine-tuning is not feasible in FL. Our goal is to collabo-\nratively fine-tune the foundation model fe in a parameter-\nefficient manner within a predefined communication bud-\nget. For this, following additive PEFT, we introduce new pa-\nrameters & for fine-tuning while keeping the original model\nfrozen, resulting in the full parameter set 0\u2032 = {0,$}.\nOverall idea: The client adapters communicated to the\nserver are distant in the weight space due to heterogene-\nity in client task space and data distribution, as indicated\nby the convergence analysis in Fig. 4 (Detailed analysis\nin Suppl. \u00a7C). Owing to the permutation invariance prop-\nerty, these adapters lack one-to-one correspondence, which\nis crucial for effective information fusion. Therefore, we\nadopt the theory of Wasserstein Barycenters (Singh and\nJaggi 2020; Akash, Li, and Trillos 2022) to synchronise and\ncombine multiple client adapters in layerwise fashion and\nusing weight space as their underlying distribution in the\nserver. The Wasserstein Barycenter relates to the concept of\naveraging in the Wasserstein space by minimizing the Earth\nMover's distance between the barycenter and given distri-\nbutions. This helps in bringing the adapters closer in the\nparameter space, prior to aggregation, as seen in Fig. 2(c).\nThe aggregated global adapter is communicated back to each\nclient. However, it possesses client-agnostic knowledge and\nis again distant from the local adapters in the weight space."}, {"title": "Using the global adapter in this form leads to slower, unsta-\nble convergence, as seen in Fig. 4(a). Therefore, we permute\nthe global model in each client to match the local adapter\nbefore integration using similar technique. This permuted\nglobal adapter is consequently frozen and combined with\nthe client-specific adapter, thereby integrating client-specific\nand shared knowledge as seen in Fig. 2(a). At the end of each\nround, this integrated adapter (also called client adapter) is\nuploaded to the server (see Algorithm 1 in Suppl. \u00a7A).\n=Server-level PIA:", "content": "We introduce a two-step procedure in\nthe server: First, we initialize the global adapter using stan-\ndard FedAvg (McMahan et al. 2017) of the client adapters.\nNext, we permute each client adapter to match the initialized\nglobal adapter by computing the permutation matrix as ob-\nserved in Fig. 2. For this, we define probability measure over\n(l)\nneurons in the [th layer for the kth client adapter as \u03bc\u03b5\n(a), X[l]) and that for the estimated global adapter as\nv(l) = (\u03b2(l), X\u04ab[l]), where Xk and X\u00e7 are the respective\nmeasure supports. The weight a = (a1,\u2026\u2026\u2026, an) lies in the\nprobability simplex En := {a \u2208 R | \u2211=1 ai = 1} (and\nsimilarly for B). We consider that the support of each adapter\nneuron in the server is given by the weights of the incoming\nedges, which are stacked in a vector. Accordingly, an adapter\nneuron can be represented by the corresponding row in the\nweight matrix. Therefore, the support of their measures is\ngiven by Xk[l] = W (e,e- k\nW(e,e-1) and XG[l] = W(e,e-1).\nk\nG\nLet Cij, (1) denote the ground cost of permuting the ith\nadapter neuron of lth layer in the kth client to the jth adapter\nneuron of same layer in the server. It is equivalent to mov-\ning the measure supports from Xk,(i) [l] to XG,(j) [1]. We\ncompute this cost as the Euclidean distance between the\nweight vector of the local and initialized global adapter, i.e.,\nC = ||X[l]-X[l]||2, Vi\u2208 [n], j\u2208 [m(l)] where\nnk\nn and me are the number of neurons in the \u2113th layer of the\nclient adapter and the global adapter respectively. We initial-\nize the probability mass values of each layer from a uniform\ndistribution. So, a) = nte), B(e) = mtes.\n1\nm(l).\nFor aligning the incoming weights We,e-1) for the th\nlayer in kth client adapter, we first normalize the previous\nlayer permutation matrix P(-1) with the inverse of cor-\nPk\nresponding column marginals of the server adapter \u03b2(1-1)\nas P(-1) diag(1/3(-1)) and post-multiply with the current\nlayer weights of the client adapter W(e,e-1). Next, based on\nthe cost metric C), we compute the permutation matrix P\nbetween measures \u03bc, \u03bd' for the current layer (l) by min-\nimizing the Wasserstein distance W(e) (\u03bc), (e), C()).\nk\nThis permutation matrix is used to align the the client\nadapter and global adapter weights as:\nW(l,l-1)\nk\n= diag (1/8(e)) p(e) W (e,e-1) p(-1) diag(1/8(-1))\nk\nk\n(1)\nThe aligned K adapters are then integrated dynam-\nically to form the global adapter as: W(l,e-1)"}, {"title": "= 1Kk=1 WkW(l,l-1)|||Wk\n \u2212 Wg| exp", "content": "where y is a\nhyperparameter. Note that our method is orthogonal to dif-\nferent FL aggregation schemes and can be used in conjunc-\ntion with those. For simplicity, we use FedAvg as the chosen\naggregation scheme in this work without loss of generality.\nClient-level PIA: In the clients, we combine the global\nadapter and client-specific adapter (with parameters from\nthe last communication round) for integrating client-specific\nand shared information streams. For this, we first align the\nglobal adapter Wg to the client-specific adapter Wk using\nthe Wasserstein distance following the aforementioned pro-\ncedure. The only difference in the permutation computation\nhere is that we use adapter activations rather than weights\nfor computing the cost metric Ce). For this, we compute the\nmean neuron activation (4) for all the neurons of local and\nglobal adapters over a randomly selected batch of m samples\nB = {x}-1 and use it as the support of measures. There-\nfore, the support is now denoted as Xk[l] = y(M(B)) and\nX\u04ab[l] = \u03c8(M(B)) for the local and global adapter respec-\ntively, where M denotes the adapter model. In other words,\nneurons across local and global adapters in each client would\nbe considered similar if they yield similar activations for a\ngiven instance. Leveraging the activations instead of weight\nmatrix particularly helps the global adapter to adapt to the\nclient-specific data distribution better (as indicated in Table\n5) since the activations (unlike weights) are directly depen-\ndent on the input data."}, {"title": "Experiments and Results", "content": "Tasks and Datasets\nWe assess the performance of our proposed method with two\nprominent Vision Language foundation models, viz., ViLT\nand ALBEF, and for three FL task settings: (a) Visual Ques-\ntion Answering, (b) Image and Text-based Disease Classifi-\ncation, (c) Heterogeneous tasks combining both (a) and (b).\nIn order to ensure the real-world applicability of FedPIA, we\nconduct experiments on multiple well-known and challeng-\ning medical datasets as discussed below.\n(a) Visual Question Answering: We consider two scenar-\nios with data of varying sizes, class counts, and complexity:\n(i) Task 1: Five-client setting with SLAKE (Liu et al.\n2021), VQA-RAD (Lau et al. 2018), VQA-Med 2019\n(Ben Abacha et al. 2019), VQA-Med 2020 (Abacha et al.\n2020), and VQA-Med 2021 (Ben Abacha et al. 2021).\n(ii) Task 2: Modality specific Eight-client setting lever-\naging (Hu et al. 2024) where Client 1 (CT) includes\n3 CT datasets, Client 2 (US) includes Ultrasound im-\nages, Client 3 (OCT) includes 2 datasets, Client 4 (Fun-\ndus images) includes 8 fundus datasets, Client 5 (Mi-\ncroscopy) includes 5 datasets, Client 6 (Histopathol-\nogy) includes 4 datasets, Client 7 (Dermatoscopy) in-\ncludes 7 different skin datasets, Client 8 (X-Ray) in-\ncludes 11 X-Ray datasets. See Suppl. \u00a7B for details.\n(b) Image and text-based disease classification: Follow-\ning (Saha et al. 2024a), we consider two heterogeneous FL"}, {"title": "settings for Chest X-Ray and Radiology report-based multi-\nlabel disease detection:\n(i) Task 3: 4 client-scenario using Open-I dataset,\n(ii) Task 4: 10 client scenario with MIMIC dataset.", "content": "Following standard procedures, we use Dirichlet distribu-\ntions with y = 0.5 to simulate non-IID client data partitions\nfrom each dataset. There are 15 disease classes in each of\nthe datasets, viz., Support Device, Pleural Effusion, Consol-\nidation, Pneumothorax, Lung Opacity, Enlarged Cardiome-\ndiastinum, Atelectasis, Others, Cardiomegaly, Lung lesion,\nEdema, Fracture, Pneumonia, Pleural other, and No finding.\n(c) Heterogeneous tasks: We consider a task-\nheterogeneous setting for Task 5 combining three Visual\nQuestion answering clients, viz., SLAKE, VQA-RAD,\nVQA-Med 2019, and two disease-classification clients, viz.,\nOpen-I and MIMIC.\nTraining and Implementation Details\nTo demonstrate the effectiveness of FedPIA across various\nVLM models, we adopt two types of VLM transformer ar-\nchitectures: (a) encoder-only backbone i.e., ViLT (Kim, Son,\nand Kim 2021), and (b) encoder-decoder backbone i.e., AL-\nBEF (?). We fix the initial learning rate \u03b7 = 0.0001 and\nbatch size B = 16. We use the AdamW optimizer and a\nlearning rate scheduler with linear decay following (Chen\net al. 2024). We also use a weight decay of 0.01 with a total\nof 30 communication rounds for federated fine-tuning in-\ncluding 10% warmup rounds (Chen et al. 2024). Each client\nhas task-specific linear classification heads."}, {"title": "Baselines:", "content": "Our baselines are: 1) Full fine-tuning, 2) Lo-\ncal classifier fine-tuning, 3) AdapterFusion (Pfeiffer et al.\n2020), 4) Houlsby Adapter (Houlsby et al. 2019), 5) Par-\nallel Adapter (He et al. 2022), 6) Compacter (Karimi Ma-\nhabadi, Henderson, and Ruder 2021), 7) LayerNorm (Basu\net al. 2023), 8) LORA (Hu et al. 2022), 9) Bias tuning (Cai\net al. 2020), 10) PromptFL (Guo et al. 2023), and 11-14)\nFedDAT (Chen et al. 2024) with 4 adapters variants. Note\nthat 'Local classifier' refers to client-specific training of lo-\ncal classifier heads without any federated learning and hence\nit is considered as the Lower Bound (LB). See Suppl. \u00a7B for\nmore details.\nResults on Visual Question Answering (Tasks 1, 2)\nTables 1 and 2 show the performance comparison of our\nmodels and the baselines for Tasks 1 and 2 respectively. For\nTask 1, we show the accuracy of the models in answering\nopen-ended and closed questions separately in each dataset\nexcept VQA-Med 2021 which does not possess any closed\nquestions. We observe that FedPIA outperforms all naive\nPEFT-FL and SOTA methods for all VQA tasks and sce-\nnarios. The performance of adapter-based PEFT baselines-\ndegrade under heterogeneity conditions, as visualized in the\nloss curve from Fig. 3 (a). This is mainly due to the baselines\nfailing to properly integrate client-agnostic knowledge with\nclient-specific knowledge from multiple adapters trained on\ndiverse data, modalities, and tasks. On the contrary, FedPIA\nshows robust and consistent performance irrespective of data\nand modality heterogeneity, which in turn, demonstrates that"}, {"title": "our permutation and integration mechanism effectively han-\ndles the challenging non-IID scenario by bridging the gap\nbetween adapters in weight space. Our method achieves an\noverall mean improvement of 3.89% and 5.18% for Tasks 1\nand 2 respectively over FedDAT across all adapter configu-\nrations. In VQA-Med 2021 (from Tab. 1), FedDAT performs\nbetter than full fine-tuning. This is possibly because adapters\nin FedDAT are well-suited to retain task-specific adaptations\nand hence perform better than full fine-tuning which spreads\nupdates across all parameters, diluting task-specific infor-", "content": "Results on Multilabel Disease Detection (Tasks 3, 4)\nTable 3 shows that our method demonstrates consistent per-\nformance improvement with respect to the baselines in each\nclient for the multi-label classification task on both the\ndatasets. We notice that the baseline methods show dete-\nrioration in performance due to the statistical heterogene-\nity introduced by inter-client class distribution shift. FedPI\u0391\noutperforms FedDAT approximately by 5.1% and 2.73% in\nF1-score for Open-I and MIMIC datasets across all clients\nand over all adapter configurations. These results show that"}, {"title": "mation. For further experiments or analysis, see Suppl.\u00a7C", "content": "Ablation Studies\nWe study the impact of each component of FedPIA\n(Houlsby) via ablation analysis in Table 5. Our model with\neither client-based or server-based PIA alone is observed\nto outperform the baselines. Removing either of client-\nbased and server-based adaptation leads to a drop in perfor-\nmance. Greater performance degradation is observed when\ndropping client-level PIA, which suggests that client-based\nPIA captures more essential information from the permuted\nglobal adapter. We also show that replacing the activation by\nweight-based ground cost computation in the clients leads to\nslight decrease in adapter performance.\nTo investigate the impact of different hyperparameters and\nclient size, we vary the learning rate and batch size, as well\nas progressively reduce the dataset size in each client from\n100% in steps of 20%. We also investigate the scalability of\nFedPIA by increasing the number of clients. See Suppl. \u00a7C.\nConclusion\nThe main contributions of the work are as follows:\n1. We studied the practical problem of parameter-efficient\nfine-tuning of foundation models in multimodal FL for\ntackling data- and resource-constraints. We analysed dif-\nferent real-world problem scenarios with the overall\ngoal of performing medical visual question answering or\nvision-language-based disease classification or both si-\nmultaneously. Through five different tasks, we, for the\nfirst time, investigated three FL heterogeneity settings\nstatistical heterogeneity, modality heterogeneity, as well\nas task heterogeneity in the context of PEFT-FL.\n2. We proposed a novel method, Federated Learning via\nPermutation and Integration of Adapters, that exploits\nWasserstein Barycenters for shuffling and combining\nadapters. We demonstrated this to be particularly ef-\nfective in bringing adapters closer in data- and task-\nheterogeneous situations where the adapters are distant\nin parameter space. Our method does not require retrain-\ning for alignment or further knowledge distillation like\nexisting methods, thereby adding no training overhead.\nBesides, FedPIA is orthogonal to existing FL aggrega-\ntion schemes and can be used in conjunction with those.\n3. For evaluating the performance of FedPIA under modal-\nity heterogeneity, we developed a modality-specific FL\nsetup using 41 medical image datasets. Through com-\nprehensive experiments, we showed that FedPIA outper-\nforms both SOTA and naive combinations of PEFT and\nFL. The results demonstrate that our proposed method\ncan achieve, and even surpass, the performance of fully\nfine-tuned methods across diverse tasks, for fine-tuning\nVLMs in heterogeneous FL."}]}