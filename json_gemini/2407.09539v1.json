{"title": "Classification of Inkjet Printers based on Droplet Statistics", "authors": ["Patrick Takenaka", "Manuel Eberhardinger", "Daniel Grie\u00dfhaber", "Johannes Maucher"], "abstract": "Knowing the printer model used to print a given document may provide a crucial lead towards identifying counterfeits or conversely verifying the validity of a real document. Inkjet printers produce probabilistic droplet patterns that appear to be distinct for each printer model and as such we investigate the utilization of droplet characteristics including frequency domain features extracted from printed document scans for the classification of the underlying printer model. We collect and publish a dataset of high resolution document scans and show that our extracted features are informative enough to enable a neural network to distinguish not only the printer manufacturer, but also individual printer models.", "sections": [{"title": "I. INTRODUCTION", "content": "Printing text on paper has been the basis for communication and the dissemination of information for a long time. While nowadays many paper-driven processes are gradually digitized, they still play a crucial role in daily life. At the same time, new technologies make it easy for non-experts to use high-quality printing methods to forge or alter documents. These technologies include inexpensive printers and scanners, various image processing software and also tools supported by artificial intelligence [1]. The incentive for criminal activity is high, as the entry barrier for interested individuals is low and there are many different types of possible forgeries, such as concert tickets, travel tickets, banknotes, fake IDs or other kinds of personal documents. Additionally, it is hard for non-experts to decide if the presented documents are genuine or forged. These points make it difficult for law enforcement authorities and also for counterfeit detection in banks to keep up with the latest counterfeits [2]. Identifying the printer model that was used to produce a particular document is an important aspect for determining whether it is genuine or fake, as in many circumstances the printer model for genuine documents is known and thus can be compared with.\nFor laser printers, there is already a solution for recognizing which printer has created a document by decoding the machine identification code, which is displayed as yellow dots on the document and is not visible to the naked eye. For inkjet printers, however, there is still no general method applicable without the help of spectroscopy [3]\u2013[6]. In this work, we present a method that works only with digital image data from high-quality scans, without the need for the original document or additional specialized hardware, such as Raman Spectrom- eters. Machine learning, and especially deep learning, often requires large amounts of labeled data, which in the case of printer identification is challenging due to the logistical chal- lenges of gathering many high-resolution scans from various printers. To mitigate this problem, we introduce a carefully designed feature extraction from image data and show that this leads to significantly better classification performance than relying on image features alone. Our core assumption is that each printer model exhibits distinct droplet patterns, and by extracting features based on the frequencies in those patterns we can capture their identity. Based on insights from domain experts we continue by also adding features related to droplet shapes specifically, such as their areas and boundaries, and thus arrive at a feature vector that both captures the patterns and the visual aspects as well.\nWe further introduce a training and evaluation scheme that can handle the large amounts of data present in a high- resolution scan in an efficient manner and verify it empirically. An overview of the overall classification process is shown in Fig. 1.\nWe further collect and introduce a dataset for printer identification, which consists of 50 high-resolution scans of documents printed from 25 different printer models across multiple manufacturers. To the best of our knowledge there is no other dataset in this scale publicly accessible for printer identification. Our dataset and code is available at https: //github.com/P-Takenaka/ijcnn2024-inkjet-classification."}, {"title": "II. RELATED WORK", "content": "The task of identifying or distinguishing printers has long been of great interest [7]\u2013[11] and is usually done with spec- troscopy in combination with machine learning approaches [3]\u2013[6]. Another approach is using textual features based on the differences in the geometric distortion of printed fonts [12]\u2013[15]. Other work focuses on detecting if barcodes were printed from the same source [16]. In our work, however, we do not constrain the image regions\u2014to for instance barcodes\u2014 used for classification in order to allow better generalization across a wide variety of documents. A similar work based on feature engineering has been studied in [17], where a feature vector is computed over different quality assurance metrics such as color variations, but the study is limited to only four printers and four scanners. In [18], the discrete wavelet transform was used to extract features from printed documents to classify the source printer for Chinese characters. In [19], [20], several different feature extraction methods were studied and compared, focusing only on laser printers. Our work, in contrast, focuses on inkjet-printers.\nIn contrast to printer identification, there is also research aimed at anonymizing documents [21] or using Siamese neural networks to predict whether two documents were printed by the same printer [22].\nFrequency-based features are often used to reduce the difficulty of the problem for machine learning algorithms. The Fourier transform is used in the field of medical imaging to improve the performance of deep learning models [23]\u2013[25] or for fraud detection in manufacturing plants [26], [27]. Wavelet features are widely used across all domains, e.g. for medical purposes [28]\u2013[31], for high-resolution image reconstruction [32], for seismic data reconstruction [33] or the prediction of emissions into the atmosphere [34]."}, {"title": "III. BACKGROUND", "content": "Here we first describe the necessary background for fre- quency domain transformation, and\u2014in order to give a better picture of the intuition behind our feature extraction-continue by illustrating the general printing process prevalent in inkjet-printing.\nA. Frequency Domain Features\nIn the following we describe the theory of different established methods for transforming an-possibly multi- dimensional-input signal into the frequency domain.\nFourier Analysis: The Fourier transform converts an input signal from the temporal- or, in our case spatial domain into frequency domain by deconstructing the signal into a series of (possibly infinite) sine and cosine waves. For discrete, equally spaced signals the discrete Fourier transform (DFT) therefore is a Z-transform that converts the signal x(t) of length T into T complex coefficients so that\n$\\displaystyle X(k) = \\sum_{t=0}^{T-1} x(t)\u00b7e^{-int}$ (1)\nwhere X(k) are polar coordinates defining the phase and amplitude of the sinusoidal component of the signal x(t) with frequency. The resulting coefficients of the Fourier transform therefore describe global frequency information of the input signal.\nWavelet Transform: While the DFT, and Fourier analysis in general, have the limitation to only capture global frequency information, wavelet transformations are able to preserve the temporal or spatial locality information while also capturing the spectral transformation of the signal. The discrete wavelet transformation (DWT) of a one-dimensional signal x(t) can be described as\n$\\displaystyle W(m, n) = \\sum_{t=0}^{T-1} \u03c8_{m,n}(t) \\cdot x(t)$ (2)\nwhere $\u03c8_{m,n}(t)$ is a mother-wavelet function that is scaled by m and translated by n, so that $W\u2084(m,n)$ describes the\ncorrelation of the signal x(t) with this transformed wavelet function. Since the wavelet function is limited in duration, the shifting with n causes the calculated correlation to describe the response in a narrow window of the signal, while the scaling using m causes a frequency dependent response.\nShort-time Fourier Transform The short-time Fourier transform (STFT) is an alternative approach for extracting spectral information from a signal while maintaining temporal locality. This is achieved by a sliding windowing function that effectively masks out chunks of the signal. Each chunk is then transformed into frequency domain independently:\n$\\displaystyle X_{\u03c9}(\u03ba, \u03b7) = \\sum_{t=0}^{T-1} w_n(t) x(t)\u00b7e^{-i2\\frac{\u03bat}{T}}$ (3)\nwhere $w_n(t)$ is the windowing function shifted by n.\nWhile the STFT contains the real frequency information of the signal (compared to the correlation with the wavelet function in the DFT), the width of the windowing function trades of the temporal and frequency resolution, while the DFT does not need such a trade-off.\nB. Inkjet Printing\nInkjet printers construct an image by placing droplets of colored ink onto a medium. The droplets can be produced by different methods such as thermal expansion or mechanical stimulation using piezo electric devices [35, p. 58 ff.]. In the most common setting that is of interest here, the droplets are ejected onto a white paper medium, so that the density of droplets can be used to perform subtractive color mixing to create arbitrary colors by only using a few primary colors, most commonly magenta, cyan and yellow. For text and other black and white printouts, often there is an additional black ink to avoid the need to mix this common color. Due to the limited resolution of the human eye, areas of small dots, even if non-overlapping, will be perceived as a single color depending on the density of dots of each primary color.\nThe size of dots produced by inkjet printers typically varies between 10 100\u03bcm [35, p. 10]. To produce drops of this size with the required accuracy and frequency, basic physical properties of the used ink need to be considered such as surface tension and viscosity. Since typically printing is done in multiple lines, the ink nozzles and medium move relative to another, requiring planning of acceleration and timing of drop ejection. Due to this complexity, the position and size of the individual dots depends not only on physical factors such as the ejection method, nozzle size and mechanical tolerances but also on the algorithms implemented in the firmware of the printer."}, {"title": "IV. DATA", "content": "A dataset consisting of A4 scans of high resolution printed documents was collected with the help of various potential users of our system, leading to printing conditions that would occur in a real-world usage setting. In total, 25 unique printer models were used to print out a printing template (cf. Fig. 2) containing varied content, such as text areas or portrait images.\nIn order to minimize the influence that the global image content (such as it being a text document, a portrait image, etc.) has on the prediction, we do not directly extract features on the whole document, but on randomly sampled crops, each with a spatial size of 256 \u00d7 256 pixels, or 2.71mm \u00d72.71mm. This is generally still large enough to observe complex droplet patterns, but separates it from the context of the document content. For each document, we sample 96 such crops and use these as basis for our feature extraction.\nA. Feature Extraction\nInkjet printers produce droplet patterns of a probabilistic nature. Upon taking a closer look (cf. Fig. 3) these patterns appear to be highly dependent on the printer model used and it leads to the question of what are the distinct factors that make up an individual printer model.\nWe decided to focus on two main aspects contained in these patterns: (1) The frequencies present in the image in order to capture the overall, global pattern of droplets, and (2) the droplet shape in order to capture detailed, local information. However, using these features directly would not be suitable as the feature vector would be too large and likely result in overfitting. As such, we compress them further by calculating their general statistics (cf. Tab. I), which ideally removes\nsample-specific information, but keeps class-specific statistics intact. In the following we describe these features in more detail.\nFor the frequency domain features the wavelet transfor- mation is of special interest, as it also considers the spatial aspect of the occurring frequencies. We found that Daubechies wavelets with three decomposition levels worked best. We then calculate the general statistics for each of the four resulting wavelet sub-bands individually.\nFor the droplet-specific features we first preprocess the image using general image processing techniques in order to make individual droplets more visible and distinct from the background. Image sharpening, denoising, and color threshold- ing helped in this regard. We then obtain individual droplets by extracting (closed) contours from the image crop and capture its shape by calculating the perimeter and area. We then aggregate these features over all droplets present in the crop by calculating their general statistics (cf. Tab. I). These droplet features also allow easy filtering of unsuitable crops. If-for instance-a crop occurs in a plain area without any droplets it is immediately apparent from the features and an alternative crop location can be chosen.\nFinally, we also add general image features that describe color and luminance by calculating (1) mean and standard deviation of pixel values, and (2) the contrast (based on the intensity channel Y of the YUV color representation).\nIn our study of the printed documents we further observed that-depending on the printer model-each color channel also exhibited distinct droplet patterns. As such, we calculate the aforementioned features for each color channel individu- ally (where applicable), in addition to features of a greyscale image variant which considers all droplet patterns at once.\nIn total, we arrive at 241 features and list them in Tab. I. In order to facilitate smooth learning we further standardize all features individually according to their training statistics."}, {"title": "V. EXPERIMENTS", "content": "A. Setup\nFor our classifier model we use a Multi-Layer Perceptron (MLP) with three hidden layers, each composed of 512 neu- rons and subsequent hyperbolic tangent activation functions. Among other classifier models we have found a neural network to be the most reliable (cf. Sec. V-D for ablations).\nThe training process minimizes the cross-entropy loss using the Adam Optimizer [36] with an initial learning rate of le-4\nand a L2 regularization loss with factor 1e-4. We train until the validation performance converges.\nWe measure the classification performance based on the F1- Score. We also report its Top-N performance for N = 2...5, in which we consider a prediction to be a true positive if it is within the top N predictions of the classifier. All scores are averaged over three different random seeds and we also report the resulting standard deviation.\nWe compare different frequency-based features and also state-of-the-art image classification models which are tailored for limited amounts of data, such as MobileNet-V3 [37], ResNet-18 [38], and SqueezeNet [39].\nB. Printer Classification Baseline\nHere we analyze the baseline printer classification per- formance. We compare different kinds of frequency-based features and also compare with image-based classifiers. For each method for domain transfer into frequency space, we perform the extraction on the two-dimensional image (termed 2D in the tables), as well as a row-wise linearized version that calculates the features for each row of pixels individually and concatenates them afterwards. We show the results in Tab. II. All frequency-based features are able to significantly out- perform image-based models. Among those, the 1D wavelet features show the best Top-1 F1-Score. While both 1D and 2D-based frequency features do not show large differences for each method, the locality information present in the STFT approach and especially in the Wavelet features seems to be beneficial for overall performance.\nFor all approaches the Top-2 F1-Score is roughly twice as high as the Top-1 performance, indicating that the model ranks the true printer model high, but often not high enough. In the following we provide a possible solution to mitigate this issue.\nC. Averaging Crop Predictions\nWhile the prediction performance of our model is sub- stantially better than image-based predictors, the majority of samples is still classified wrong. One document scan of a given printer is represented in our dataset as multiple crop samples, which are handled independently of each other. As we can assume that the whole document is available at inference time, we also investigate the per-document prediction performance by aggregating the predictions of all crops of a given doc- ument. We achieve this by simply averaging the resulting network logits before calculating the metrics. As can be seen in Tab. III, the resulting per-document classification performance is significantly higher than the per-crop performance, without any changes to the training process of the model.\nAggregating the crops into a single document can, however, happen at different points in the model pipeline. In Tab. II we have shown the case where individual crops are fed to the model independently of each other, and the aggregation happened afterwards. This aggregation could also happen at the feature side, where feature vectors from crops belonging to a single document are averaged before being put through the model, resulting in a single per-document prediction. We investigate this configuration and also distinguish whether this aggregation happens during training and / or during validation of the model in Tab. IV. These results indicate that the best approach is to train on individual crops, and validate on averaged model logits.\nOverall, the improved per-document performance is in- triguing, as it indicates that on average-individual crop predictions are more likely to be wrong than their aggregation. The confusion matrix shown in Fig. 4 gives an overview of the miss-classifications. It is immediately recognizable that many classes (such as for instance \u201cBrother//MFC-J6710DW\") for which the sum of wrong crop predictions surpasses the amount of true positives in the diagonal are still assigned correctly in the per-document classification. Since we average the network logits and not the predictions in the per-document classification setting, this indicates that in many cases the model assigns a high logit to the true class, even if it is not the highest, and their average then surpasses all other class logits. This also explains the significant performance gain when considering the Top-2 F1 performance compared to the Top-1, where the true class is often only the second highest logit.\nD. Further Ablations\nHere we investigate further ablations related to the features and the classifier. For model ablations we use the 1D wavelet features as basis, and choose the following machine learning approaches: (1) Support-Vector-Machine (SVM), (2) Random Forest (N=500 trees), and (3) XGBoost (N=500 trees). For the feature variations we use our default MLP classifier model and analyze the following setups: (1) We do not extract per color channel features and instead only use a grey-scale image as basis, and (2) we do not use features besides the frequency statistics. We show the results in Tab. V.\nAmong the tested model variations, the random forest approach was able to achieve a similar per-crop performance as our MLP classifier, albeit at a lower per-document per- formance. On the other hand, the XGBoost method achieves comparable per-document performance, but much lower per- crop performance. For the feature variations the performance on the per-document evaluation is stable, however for the per- crop performance a combination of all features as proposed led to the best result.\""}, {"title": "VI. LIMITATIONS AND FUTURE WORK", "content": "While we have shown that manually extracted frequency- based features are better than relying on image features di-\nrectly, there is still room for further improvements. Especially in the area of forensics a high level of confidence and reliance is necessary in order to integrate such a system in the work- flow. One aspect that makes the whole endeavour challenging in general is the difficulty in obtaining large amounts of data that correspond to a wide variety of printing conditions, however our dataset can be a suitable basis in this regard to\nexpand upon. For this reason and due to the large amount of different printer models in the real world, it is highly likely that a tested document is from a printer model not available in the dataset. As such, model extensions that can predict such outliers is a promising direction for future work that makes this more applicable.\nFurthermore, in this work we have analyzed image-based features and our manually extracted features separately. It might be even better to fuse both types of features in a single model to also let the model extract useful features on its own. These could then potentially replace the droplet-based features that we utilized. However at this time the limiting factor here seems to be the small amount of data available. As such, a clear future direction is the expansion of the dataset both in terms of document content variety and also new printer models. We reckon that, as in other areas, deep learning methods will gradually become more important the larger the data basis gets.\nAn interesting other direction for future work might be the identification of not only the printer model used, but also the printer instance. The hypothesis is that due to mechanical wear or slight manufacturing differences the droplet dispersion pattern varies even among same printer models. It would be interesting to pursue whether this can be detected from the features that we proposed as well. Similarly, the used scanner device might also be an important factor that should be taken into account. In our dataset we used only a single scanning device, in reality however this is difficult to enforce. There- fore analyzing the performance on documents scanned with different scanning devices might provide valuable insights."}, {"title": "VII. CONCLUSION", "content": "We introduced a novel dataset and corresponding training and evaluation scheme for printer model identification that works without specialized scanning hardware. We proposed a set of manually extracted features which lead to substantial performance improvements over relying on image features alone, and show that wavelet-based frequency features best capture the printer identities. In the future this work will be part of a system to be used by document forensics experts."}]}