{"title": "Towards Robust Knowledge Unlearning: An Adversarial Framework for Assessing and Improving Unlearning Robustness in Large Language Models", "authors": ["Hongbang Yuan", "Zhuoran Jin", "Pengfei Cao", "Yubo Chen", "Kang Liu", "Jun Zhao"], "abstract": "LLM have achieved success in many fields but still troubled by problematic content in the training corpora. LLM unlearning aims at reducing their influence and avoid undesirable behaviours. However, existing unlearning methods remain vulnerable to adversarial queries and the unlearned knowledge resurfaces after the manually designed attack queries. As part of a red-team effort to proactively assess the vulnerabilities of unlearned models, we design Dynamic Unlearning Attack (DUA), a dynamic and automated framework to attack these models and evaluate their robustness. It optimizes adversarial suffixes to reintroduce the unlearned knowledge in various scenarios. We find that unlearned knowledge can be recovered in 55.2% of the questions, even without revealing the unlearned model's parameters. In response to this vulnerability, we propose Latent Adversarial Unlearning (LAU), a universal framework that effectively enhances the robustness of the unlearned process. It formulates the unlearning process as a min-max optimization problem and resolves it through two stages: an attack stage, where perturbation vectors are trained and added to the latent space of LLMs to recover the unlearned knowledge, and a defense stage, where previously trained perturbation vectors are used to enhance unlearned model's robustness. With our LAU framework, we obtain two robust unlearning methods, AdvGA and AdvNPO. We conduct extensive experiments across multiple unlearning benchmarks and various models, and demonstrate that they improve the unlearning effectiveness by over 53.5%, cause only less than a 11.6% reduction in neighboring knowledge, and have almost no impact on the model's general capabilities.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have achieved remarkable capabilities after being trained on an extensive amount of corpora (Chen et al. 2024; Bubeck et al. 2023). However, since the training data may contain copyrighted, private, and toxic content, LLMs inevitably learn some potentially undesirable behaviours (Ji et al. 2024). For example, LLMs may regurgitate copyrighted material without permission (Wei et al. 2024a), generate personal information such as phone numbers or mailing addresses (Yao et al. 2024b), and even produce offensive and harmful responses (Liu et al. 2024c). These unwanted behaviors introduce security concerns and information hazards, hindering the deployment of LLMs in real-world scenarios (Patil, Hase, and Bansal 2024).\nTo eliminate the influence of problematic content in the corpora on LLMs, machine unlearning has emerged as a promising solution (Eldan and Russinovich 2023; Yao, Xu, and Liu 2024; Liu et al. 2024a). It transforms models to behave as if they were never trained on certain data entries so that specific target knowledge is erased, while other knowledge and capabilities of LLMs are preserved (Chen and Yang 2023; Maini et al. 2024). The most fundamental machine unlearning method is to adopt a gradient ascent procedure on the data that needs to be forgotten to fine-tune LLMs, reversing the original gradient descent optimization process (Jang et al. 2023; Yao et al. 2024a; Liu et al. 2024d).\nHowever, despite their effectiveness, the unlearned models produced by these methods are fragile and susceptible to crafted adversarial user prompts (Patil, Hase, and Bansal 2024; Liu et al. 2024a). Particularly, the previously unlearned knowledge resurfaces through contextual interactions (Shumailov et al. 2024). For example, after being trained to forget the famous writer J.K. Rowling, the model fails to answer the question 'Who is the author of Harry Potter?' but still outputs the correct answer to the question 'Who created the magical world that includes Diagon Alley and the Forbidden Forest?\u201d. To proactively assess the vulnerability of unlearned models to these malicious prompts, static and manually-designed adversarial prompts are used to reintroduce the unlearned knowledge (Jin et al. 2024). However, such a manual process is resource-intensive and often ineffective in guaranteeing the successful reintroduction of the previously unlearned knowledge.\nTherefore, we propose a dynamic and automated attack framework, called Dynamic Unlearning Attack (DUA), to quantitatively assess the robustness of the unlearned models. Specifically, as Figure 1(b) shows, we optimize a universal adversarial suffix that maximizes the probability of the model generating unlearned knowledge associated with a given unlearning target. This optimization process is performed and evaluated across various scenarios, considering both practicality and generalization. For practicality, we consider scenarios where the unlearned LLM is either accessible or inaccessible to the attacker. For generalization, we consider whether the adversarial suffix trained on certain questions about a specific unlearning target can be employed"}, {"title": "2 Preliminaries", "content": "Given a LLM \\( \\pi_{\\theta} \\) with parameter @ trained on dataset \\( D = {\\( (x_i, y_i) \\) | i = 1,2,..., N} \\), we define the forget set \\( D_f \\)"}, {"title": "2.1 Problem Formulation", "content": "as the specific training subset to be forgotten. Machine un- learning aims to eliminate the influence of \\( D_f \\) and make model \\( \\pi_{\\theta} \\) behaves as if it is only trained on the retain set \\( D_r = D \\setminus D_f \\). Ideally, we can retrain the model on D from scratch but it is too costly and unrealistic thus effective approximate unlearning methods are essential. The most commonly used mathematical formulation for optimizing model unlearning is presented below:\n\n\\[\nmin \\underset{(x_f, y_f) \\in D_f}{E} [l_f(Y_f | X_f;\\theta)] + \\lambda \\underset{(x_r,y_r)\\in D_r}{E} [l_r(Y_r | X_r;\\theta)]\n\\]\n\nwhere \\( l_f \\) and \\( l_r \\) are the loss functions on forget set and retain set, respectively, and \\( \\lambda \\) is a regularization parameter to balance them.\nTypically, the forget set \\( D_f \\) and a subset of the retain set \\( D_r \\) are available in an unlearning task (Shi et al. 2024). In a more practical setting, only an unlearning target t is given and we shall generate a synthetic forget set \\( D'_f \\) related to this unlearning target t and a corresponding pseudo retain set \\( D'_r \\) (Jin et al. 2024). In our paper, both scenarios are considered."}, {"title": "2.2 Unlearning Methods", "content": "We introduce two widely used loss functions for the forget set, gradient ascent and negative preference optimization, and two widely used loss functions for the retain set, gradient descent and KL divergence.\nGradient Ascent. Gradient ascent servers as an important baseline method in machine unlearning and uses the following loss function:\n\n\\[\nl_f = -E_{D_f}L(\\pi_{\\theta}(x,y))\n\\]\n\nwhere L represents a cross-entropy prediction loss. It aims to maximize the average prediction loss of LLMs on the forget dataset, thereby reverting the original gradient descent training process on the forget set.\nNegative Preference Optimization. Due to the divergent nature of the gradient ascent method, the unlearning process suffers from catastrophic collapse, wherein the model quickly deteriorates and generates incoherent responses (Zhang et al. 2024). Thus, the negative preference optimization algorithm is introduced, characterized by the following loss function:\n\n\\[\nl_t = -E_{D} \\left[log \\left(1 + \\beta \\frac{L(\\pi_{\\theta}(x), y)}{L(\\pi_{ref}(x), y)}\\right)\\right],\n\\]\n\nwhere \\( \\beta \\) is a regularization parameter controlling the deviation between the current model \\( \\pi_{\\theta} \\) and the original model \\( \\pi_{ref} \\). It provides more stable training dynamics and achieves better performance.\nGradient Descent. One widely adopted loss function for the retain set is the standard cross-entropy loss function:\n\n\\[\nl_r = E_{D_r} L(\\pi_{\\theta}(x,y))\n\\]"}, {"title": "KL Divergence", "content": "Another approach is to minimize the Kullback-Leibler (KL) divergence between the predictions of the original model and the current model on the retain dataset. The loss function can be expressed as:\n\n\\[\nl_r = E_{D_r} D_{KL}(\\pi_{\\theta}(x, y)||\\pi_{ref}(x, y))\n\\]\n\nwhere \\( \\pi_{\\theta} \\) and \\( \\pi_{ref} \\) denote the current model and the original reference model, respectively. It prevents the model from collapsing by ensuring that it does not deviate excessively from the original predictions."}, {"title": "2.3 Datasets and Metrics", "content": "RWKU. RWKU (Jin et al. 2024) is a real-world knowledge unlearning benchmark that requires models to erase specific knowledge in their parameters. Specifically, for the evaluation of unlearning effectiveness, it provides three types of knowledge probe questions on the forget set: FB, QA and AA. For the evaluation of utility preservation, it provides two types of questions on the neighbor set to test the impact of neighbor perturbation: FB and QA. We use ROUGE-L score (Lin 2004) to measure model's performance. Lower scores on forget set indicate better unlearning effectiveness, and higher scores on neighbor set indicate better utility preservation. Additionally, it measures the model's various capabilities, including reasoning (Rea), truthfulness (Tru), factuality (Fac) and fluency (Flu). It also provides some membership inference attacks (MIA) to detect retained knowledge in LLMs, where higher scores indicate that the model retains specific knowledge.\nMUSE. MUSE (Shi et al. 2024) is a comprehensive unlearning benchmark that requires models to unlearn either news articles or book series."}, {"title": "3 Dynamic Unlearning Attack Framework", "content": "In this section, we introduce a dynamic, automated framework to assess the robustness of the unlearned models. Firstly, we describe the process for optimizing adversarial suffixes that reintroduce the unlearned knowledge. Subsequently, we introduce the various attack scenarios, focusing on both practicality and generalization. Finally, we conduct experiments on ten unlearned models, demonstrating that they remain susceptible to adversarial attacks even without exposing their parameters."}, {"title": "3.1 Adversarial Suffix Optimization", "content": "Motivated by the GCG attack in safety-related domains (Zou et al. 2023), we introduce how to optimize attack suffixes in the context of unlearning. Intuitively, we optimize the suffix tokens to maximize the probability of generating the unlearned knowledge.\nConsider a question \\( X_{[0,m)} \\) related to the unlearning target. We aim to find an adversarial suffix \\( q_{[0,n)} \\) that, when combined with x to form [x; q], makes the unlearning model"}, {"title": "3.2 Robustness Assessment", "content": "The optimization process defined by Equation 6 can be performed and evaluated in various scenarios. Depending on the choice of the training data (x, y) and the computation of the next token prediction probability p, we design our assessment framework from the perspective of practicality and generalization.\nPracticality. We consider two scenarios for the calculation of the next token probability p. (1) Attack Unlearned. The ideal approach is to use the unlearned model, as the adversarial suffix will ultimately be used to attack this model. (2) Attack Original. We also consider a more practical scenario when the unlearned model is not available to the attacker. Therefore, we directly use the models before unlearning to optimize the adversarial suffixes.\nGeneralization. Typically, the ability of the unlearned models can be assessed by the question-answer style probes related to the unlearning target. Thus the training data (x, y) should be specified as similar question-answer pairs. This raises the question of whether the testing questions are available to the attacker, and whether the unlearning target itself is accessible. We consider three settings, each of which imposes progressively higher demands for generalization. (1) Within Query. The test questions are available to the attacker, thereby we can directly train adversarial queries on"}, {"title": "3.3 Experiments", "content": "Configuration. To assess the robustness of the unlearned models with our framework, we first conduct unlearning experiments with Llama-3-8B-Instruct on dataset RWKU using the negative preference optimization method. Subsequently, we apply our attack framework to create adversarial queries across various scenarios and evaluate the performance of the unlearned models. For the cross query setting, we generate knowledge probe questions for training based on the unlearning target using GPT-4.\nResults. We can draw the following conclusions: (1) The unlearned models are vulnerable to adversarial queries, especially when the unlearned models and test queries are accessible to the attacker. (2) Models are more resistant to attacks when knowledge probe queries and unlearning targets are inaccessible to attackers. However, our dynamic framework is still able to improve model performance beyond that of static attacks, highlighting the limitations of the resistance. (3) Even without access to the unlearned models, the attacker can still carry out attacks that are nearly as effective"}, {"title": "4 Latent Adversarial Unlearning Framework", "content": "In this section, we propose an adversarial learning framework to increase the robustness of the unlearning process. First, we will propose a saddle-point (min-max) formulation for adversarial training in the context of machine unlearning, and subsequently, we will elaborate how our framework can be employed to concrete methods in detail."}, {"title": "4.1 Framework Formulation", "content": "In the context of unlearning, we formulate the adversarial training as the following min-max optimization problem:\n\n\\[\n\\underset{\\theta}{max} E_{(x_f, y_f) \\in D_f} \\underset{x_f \\in T(x_f)}{min} L (\\pi_{\\theta} (x + \\epsilon), y)\n\\]\n\nwhere L is a negative cross-entropy loss function and T(\\( x_f \\)) is a set of adversarial perturbations generated by various attack methods. It's a composition of an inner minimization problem and an outer maximization problem. The inner minimization process aims to identify adversarial queries that effectively bypass the model's restrictions and activate the forgotten knowledge. The outer maximization strives to suppress the re-emergence of the forgotten knowledge on these adversarial queries.\nHowever, it is non-trivial to identify all the elements in the adversarial query set T(x), as there are too many potential adversarial queries hidden beneath. Additionally, optimizing a discrete set of tokens is challenging. To avoid the intricate process of optimizing adversarial queries, we propose latent adversarial unlearning. The core idea is that any type of adversarial query will cause a perturbation in the latent space of LLMs, potentially leading to the resurgence of forgotten knowledge. Therefore, we directly add perturbations to the latent space of LLMs, thus avoiding the extensive optimization in the input space. This process can be formulated as the following min-max optimization problem:\n\n\\[\n\\underset{\\theta}{max} E_{(x_f, y_f) \\in D_f} [\\underset{\\delta}{min} L (\\pi_{\\theta_2} (\\pi_{\\theta_1} (x_f) + \\delta x_q), Y_k)] s.t.||\\delta x_f || \\leq \\kappa\n\\]\n\nwhere \\( \\pi_{\\theta_1} \\) and \\( \\pi_{\\theta_2} \\) represent the computations in LLM before and after the perturbation \\( \\delta \\) is added, respectively. The L2-norm of the perturbation vector is restricted to a constant K. In this way, both the inner and outer optimization problems can be solved using gradient descent algorithms.\nPractically, we add a perturbation vector to the residual stream at a specific layer of a transformer model. For each batch of samples, we optimize the perturbation vector using its gradient for a fixed number of steps. Subsequently, we apply the classical stochastic gradient descent algorithm to update the model's parameters, with the previously optimized perturbation vector in its residual streams. The impact"}, {"title": "4.2 Two Adversarial Unlearning Methods", "content": "Our adversarial unlearning framework is suitable for most of the existing machine unlearning algorithms. In this paper, we apply our framework to augment the GA and NPO methods, resulting in two new algorithms: AdvGA and AdvNPO, which are described below.\nAdvGA. By substituting the internal minimization loss function in Equation 8 with Equation 3, we can obtain the following loss function:\n\n\\[\n\\underset{\\pi_{\\theta}}{min} -E_{D_f} \\underset{\\delta}{min} L (\\pi_{\\theta_2} (\\pi_{\\theta_1} (x) + \\delta, y))\n\\]\n\nWe denote this new loss function AdvGA.\nAdvNPO. Similarly, we substitute the internal minimization loss function in equation 8 with Equation 3 and the following loss function is obtained:\n\n\\[\n\\underset{\\pi_{\\theta}}{min} -E_{D_f} \\left[log \\left(1 + \\beta \\frac{L(\\pi_{\\theta_2} (\\pi_{\\theta_1} (x) + \\delta, y))}{L(\\pi_{ref} (x, y))}\\right)\\right]\n\\]\n\nWe denote this new loss function Adv NPO. For clarification, we omit the L2-norm restriction on the perturbation vector here, but it should be included during the optimization process. This also applies to the loss function in AdvGA."}, {"title": "4.3 Experiments", "content": "Configurations. We conduct machine unlearning experiments on the following two datasets: RWKU (Jin et al. 2024) and MUSE (Shi et al. 2024). We combine the previously introduced forget loss functions and retain loss functions, and finally obtain 12 unlearning methods."}, {"title": "5 Discussion", "content": "on the general capabilities of LLMs. For example, performance on the utility set remains nearly the same before and after the unlearning process. This demonstrates that the unlearned models remain powerful in many scenarios, despite having specific knowledge removed."}, {"title": "5.1 Influence of the Perturb Layers", "content": "We explore how the specific layer at which the perturbation vector is added influences the final performance."}, {"title": "5.2 Influence of the Inner Optimization Steps", "content": "Similarly, we also explore the influence of the number of inner optimization steps in Equation 8."}, {"title": "5.3 Robustness of Latent Adversarial Unlearning", "content": "Finally, we evaluate the robustness of the unlearned models trained with LAU-augmented methods under our previously proposed dynamic attack framework."}, {"title": "6 Related Work", "content": "Jailbreak Attack. To mitigate the undesirable behaviors of LLMs, the safety-alignment stage has become essential\nMachine Unlearning. Data protection regulations, such as the European General Data Protection Regulation (GDPR) (Mantelero 2013), have mandated \"the Right to be Forgotten\" and highlight the necessity of machine unlearning"}, {"title": "7 Conclusion", "content": "In this paper, we propose a dynamic and automated framework to assess the vulnerabilities of the unlearned models. After revealing their susceptibility, we propose a latent adversarial training framework, along with two concrete methods, namely AdvGA and AdvNPO."}]}