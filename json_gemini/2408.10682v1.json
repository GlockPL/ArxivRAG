{"title": "Towards Robust Knowledge Unlearning: An Adversarial Framework for Assessing and Improving Unlearning Robustness in Large Language Models", "authors": ["Hongbang Yuan", "Zhuoran Jin", "Pengfei Cao", "Yubo Chen", "Kang Liu", "Jun Zhao"], "abstract": "LLM have achieved success in many fields but still troubled by problematic content in the training corpora. LLM unlearning aims at reducing their influence and avoid undesirable behaviours. However, existing unlearning methods remain vulnerable to adversarial queries and the unlearned knowledge resurfaces after the manually designed attack queries. As part of a red-team effort to proactively assess the vulnerabilities of unlearned models, we design Dynamic Unlearning Attack (DUA), a dynamic and automated framework to attack these models and evaluate their robustness. It optimizes adversarial suffixes to reintroduce the unlearned knowledge in various scenarios. We find that unlearned knowledge can be recovered in 55.2% of the questions, even without revealing the unlearned model's parameters. In response to this vulnerability, we propose Latent Adversarial Unlearning (LAU), a universal framework that effectively enhances the robustness of the unlearning process. It formulates the unlearning process as a min-max optimization problem and resolves it through two stages: an attack stage, where perturbation vectors are trained and added to the latent space of LLMs to recover the unlearned knowledge, and a defense stage, where previously trained perturbation vectors are used to enhance unlearned model's robustness. With our LAU framework, we obtain two robust unlearning methods, AdvGA and AdvNPO. We conduct extensive experiments across multiple unlearning benchmarks and various models, and demonstrate that they improve the unlearning effectiveness by over 53.5%, cause only less than a 11.6% reduction in neighboring knowledge, and have almost no impact on the model's general capabilities.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have achieved remarkable capabilities after being trained on an extensive amount of corpora (Chen et al. 2024; Bubeck et al. 2023). However, since the training data may contain copyrighted, private, and toxic content, LLMs inevitably learn some potentially undesirable behaviours (Ji et al. 2024). For example, LLMs may regurgitate copyrighted material without permission (Wei et al. 2024a), generate personal information such as phone numbers or mailing addresses (Yao et al. 2024b), and even produce offensive and harmful responses (Liu et al. 2024c). These unwanted behaviors introduce security concerns and information hazards, hindering the deployment of LLMs in real-world scenarios (Patil, Hase, and Bansal 2024).\nTo eliminate the influence of problematic content in the corpora on LLMs, machine unlearning has emerged as a promising solution (Eldan and Russinovich 2023; Yao, Xu, and Liu 2024; Liu et al. 2024a). It transforms models to behave as if they were never trained on certain data entries so that specific target knowledge is erased, while other knowledge and capabilities of LLMs are preserved (Chen and Yang 2023; Maini et al. 2024). The most fundamental machine unlearning method is to adopt a gradient ascent procedure on the data that needs to be forgotten to fine-tune LLMs, reversing the original gradient descent optimization process (Jang et al. 2023; Yao et al. 2024a; Liu et al. 2024d).\nHowever, despite their effectiveness, the unlearned models produced by these methods are fragile and susceptible to crafted adversarial user prompts (Patil, Hase, and Bansal 2024; Liu et al. 2024a). Particularly, the previously unlearned knowledge resurfaces through contextual interactions (Shumailov et al. 2024). For example, after being trained to forget the famous writer J.K. Rowling, the model fails to answer the question 'Who is the author of Harry Potter?' but still outputs the correct answer to the question 'Who created the magical world that includes Diagon Alley and the Forbidden Forest?\u201d. To proactively assess the vulnerability of unlearned models to these malicious prompts, static and manually-designed adversarial prompts are used to reintroduce the unlearned knowledge (Jin et al. 2024). However, such a manual process is resource-intensive and often ineffective in guaranteeing the successful reintroduction of the previously unlearned knowledge.\nTherefore, we propose a dynamic and automated attack framework, called Dynamic Unlearning Attack (DUA), to quantitatively assess the robustness of the unlearned models. Specifically, as Figure 1(b) shows, we optimize a universal adversarial suffix that maximizes the probability of the model generating unlearned knowledge associated with a given unlearning target. This optimization process is performed and evaluated across various scenarios, considering both practicality and generalization. For practicality, we consider scenarios where the unlearned LLM is either accessible or inaccessible to the attacker. For generalization, we consider whether the adversarial suffix trained on certain questions about a specific unlearning target can be employed"}, {"title": "2 Preliminaries", "content": "2.1 Problem Formulation\nGiven a LLM \\(\\pi_{\\theta}\\) with parameter @ trained on dataset \\(D = \\{(x_i, y_i) | i = 1,2,..., N\\}\\), we define the forget set \\(D_f\\)"}, {"title": "2.2 Unlearning Methods", "content": "where \\(l_f\\) and \\(l_r\\) are the loss functions on forget set and retain set, respectively, and \\(\\lambda\\) is a regularization parameter to balance them.\nTypically, the forget set \\(D_f\\) and a subset of the retain set \\(D_r\\) are available in an unlearning task (Shi et al. 2024). In a more practical setting, only an unlearning target t is given and we shall generate a synthetic forget set D'f related to this unlearning target t and a corresponding pseudo retain set \\(D_r\\) (Jin et al. 2024). In our paper, both scenarios are considered.\nWe introduce two widely used loss functions for the forget set, gradient ascent and negative preference optimization, and two widely used loss functions for the retain set, gradient descent and KL divergence.\nGradient Ascent. Gradient ascent servers as an important baseline method in machine unlearning and uses the following loss function:\n\\[l_f = -E_{D_f}L(\\pi_{\\theta}(x,y))\\]\nwhere L represents a cross-entropy prediction loss. It aims to maximize the average prediction loss of LLMs on the forget dataset, thereby reverting the original gradient descent training process on the forget set.\nNegative Preference Optimization. Due to the divergent nature of the gradient ascent method, the unlearning process suffers from catastrophic collapse, wherein the model quickly deteriorates and generates incoherent responses (Zhang et al. 2024). Thus, the negative preference optimization algorithm is introduced, characterized by the following loss function:\n\\[l_t = -E_{D} \\frac{2}{\\beta}  log \\Bigg(1 + \\Bigg(\\frac{L(\\pi_{\\theta}(x), y)}{L(\\pi_{ref}(x, y))}\\Bigg)^{2}\\Bigg)\\]\nwhere \\(\\beta\\) is a regularization parameter controlling the deviation between the current model \\(\\pi_{\\theta}\\) and the original model \\(\\pi_{ref}\\). It provides more stable training dynamics and achieves better performance.\nGradient Descent. One widely adopted loss function for the retain set is the standard cross-entropy loss function:\n\\[l_r = E_{D_r} L(\\pi_{\\theta}(x,y))\\]\nKL Divergence. Another approach is to minimize the Kullback-Leibler (KL) divergence between the predictions of the original model and the current model on the retain dataset. The loss function can be expressed as:\n\\[l_r = E_{D_r} D_{KL}(\\pi_{\\theta}(x, y)||\\pi_{ref}(x, y))\\]\nwhere \\(\\pi_{\\theta}\\) and \\(\\pi_{ref}\\) denote the current model and the original reference model, respectively. It prevents the model from collapsing by ensuring that it does not deviate excessively from the original predictions."}, {"title": "2.3 Datasets and Metrics", "content": "RWKU. RWKU (Jin et al. 2024) is a real-world knowledge unlearning benchmark that requires models to erase specific knowledge in their parameters. Specifically, for the evaluation of unlearning effectiveness, it provides three types of knowledge probe questions on the forget set: FB, QA and AA. For the evaluation of utility preservation, it provides two types of questions on the neighbor set to test the impact of neighbor perturbation: FB and QA. We use ROUGE-L score (Lin 2004) to measure model's performance. Lower scores on forget set indicate better unlearning effectiveness, and higher scores on neighbor set indicate better utility preservation. Additionally, it measures the model's various capabilities, including reasoning (Rea), truthfulness (Tru), factuality (Fac) and fluency (Flu). It also provides some membership inference attacks (MIA) to detect retained knowledge in LLMs, where higher scores indicate that the model retains specific knowledge. Further details and examples are presented in Appendix A.\nMUSE. MUSE (Shi et al. 2024) is a comprehensive unlearning benchmark that requires models to unlearn either news articles or book series. Similarly, it also contains the evaluation for unlearning effectiveness and utility preservation. More details are presented in Appendix A."}, {"title": "3 Dynamic Unlearning Attack Framework", "content": "In this section, we introduce a dynamic, automated framework to assess the robustness of the unlearned models. Firstly, we describe the process for optimizing adversarial suffixes that reintroduce the unlearned knowledge. Subsequently, we introduce the various attack scenarios, focusing on both practicality and generalization. Finally, we conduct experiments on ten unlearned models, demonstrating that they remain susceptible to adversarial attacks even without exposing their parameters."}, {"title": "3.1 Adversarial Suffix Optimization", "content": "Motivated by the GCG attack in safety-related domains (Zou et al. 2023), we introduce how to optimize attack suffixes in the context of unlearning. Intuitively, we optimize the suffix tokens to maximize the probability of generating the unlearned knowledge.\nConsider a question X[0,m] related to the unlearning target. We aim to find an adversarial suffix q[0,n) that, when combined with x to form [x; q], makes the unlearning model"}, {"title": "3.2 Robustness Assessment", "content": "generate a response Y[1,H) containing the unlearned knowledge. The optimization process can be expressed as:\n\\[\\min_{q_{[0,n)} \\in \\{1,..,V\\}}  -log  \\prod_{i=0}^{H-1} P(Y_{[0,i+1)} | [x_{1:m}; q_{1:n+i-1}; Y_{[0, i]}])\\]\nwhere p denotes the next token prediction probability, V denotes the vocabulary size, m is the number of tokens in user query x, n is the number of tokens in the adversarial suffix q, and y is the desired response.\nTo solve this optimization problem, at each optimization step, we leverage the gradients of the tokens to identify a set of candidate tokens, then evaluate them token by token and select the optimal one. Practically, we optimize one single suffix across multiple prompts, resulting in a universal suffix that can be transferred to other queries.\nThe optimization process defined by Equation 6 can be performed and evaluated in various scenarios. Depending on the choice of the training data (x, y) and the computation of the next token prediction probability p, we design our assessment framework from the perspective of practicality and generalization.\nPracticality. We consider two scenarios for the calculation of the next token probability p. (1) Attack Unlearned. The ideal approach is to use the unlearned model, as the adversarial suffix will ultimately be used to attack this model. (2) Attack Original. We also consider a more practical scenario when the unlearned model is not available to the attacker. Therefore, we directly use the models before unlearning to optimize the adversarial suffixes.\nGeneralization. Typically, the ability of the unlearned models can be assessed by the question-answer style probes related to the unlearning target. Thus the training data (x, y) should be specified as similar question-answer pairs. This raises the question of whether the testing questions are available to the attacker, and whether the unlearning target itself is accessible. We consider three settings, each of which imposes progressively higher demands for generalization.\n(1) Within Query. The test questions are available to the attacker, thereby we can directly train adversarial queries on"}, {"title": "3.3 Experiments", "content": "Configuration. To assess the robustness of the unlearned models with our framework, we first conduct unlearning experiments with Llama-3-8B-Instruct on dataset RWKU using the negative preference optimization method. Subsequently, we apply our attack framework to create adversarial queries across various scenarios and evaluate the performance of the unlearned models. We present the average performance of 10 models, each trained with unlearn different targets. For the cross query setting, we generate knowledge probe questions for training based on the unlearning target using GPT-4. For the cross target setting, we use an additional 5 unlearned models along with their corresponding knowledge probe questions to train the adversarial queries.\nAdditionally, we generate an equivalent number of static attack questions using GPT-4 for comparison. Details of the construction process for the static attack questions, along with specific examples of both dynamic and static attack questions, are provided in Appendix B.\nResults. The experimental results are presented in Figure 2. We can draw the following conclusions: (1) The unlearned models are vulnerable to adversarial queries, especially when the unlearned models and test queries are accessible to the attacker. For example, the unlearned model demonstrates a maximum performance increase of 15.25% using adversarial queries compared to not using them. It indicates that the previously forgotten knowledge gets reintroduced in-context. (2) Models are more resistant to attacks when knowledge probe queries and unlearning targets are inaccessible to attackers. However, our dynamic framework is still able to improve model performance beyond that of static attacks, highlighting the limitations of the resistance. (3) Even without access to the unlearned models, the attacker can still carry out attacks that are nearly as effective"}, {"title": "4 Latent Adversarial Unlearning Framework", "content": "as those conducted with access to the unlearned models. For example, the 'attack original' lines are almost equivalent to the 'attack unlearned' lines and even outperform them in the cross query setting. This reveals a more critical issue that has been overlooked: malicious queries can be trained to recover forgotten knowledge even without prior access to the unlearned model itself.\nIn this section, we propose an adversarial learning framework to increase the robustness of the unlearning process. First, we will propose a saddle-point (min-max) formulation for adversarial training in the context of machine unlearning, and subsequently, we will elaborate how our framework can be employed to concrete methods in detail."}, {"title": "4.1 Framework Formulation", "content": "In the context of unlearning, we formulate the adversarial training as the following min-max optimization problem:\n\\[\\max_{\\theta} E_{(x_f, y_f) \\in D_f} \\underset{\\epsilon \\in T(x_f)}{min} L (\\pi_{\\theta} (x + \\epsilon), y)\\]\nwhere L is a negative cross-entropy loss function and T(xf ) is a set of adversarial perturbations generated by various attack methods. It's a composition of an inner minimization problem and an outer maximization problem. The inner minimization process aims to identify adversarial queries that effectively bypass the model's restrictions and activate the forgotten knowledge. The outer maximization strives to suppress the re-emergence of the forgotten knowledge on these adversarial queries.\nHowever, it is non-trivial to identify all the elements in the adversarial query set T(x), as there are too many potential adversarial queries hidden beneath. Additionally, optimizing a discrete set of tokens is challenging. To avoid the intricate process of optimizing adversarial queries, we propose latent adversarial unlearning. The core idea is that any type of adversarial query will cause a perturbation in the latent space of LLMs, potentially leading to the resurgence of forgotten knowledge. Therefore, we directly add perturbations to the latent space of LLMs, thus avoiding the extensive optimization in the input space. This process can be formulated as the following min-max optimization problem:\n\\[\\max_{\\delta} E_{D_f} \\Bigg[ \\underset{||\\delta_{x_f}|| \\le \\kappa}{min}  L (\\pi_{\\theta_2} (\\pi_{\\theta_1} (x_f) + \\delta_{x_f}), y_f) \\Bigg]\\]\nwhere \\(\\pi_{\\theta_1}\\) and \\(\\pi_{\\theta_2}\\) represent the computations in LLM before and after the perturbation \\(\\delta\\) is added, respectively. The L2-norm of the perturbation vector is restricted to a constant K. In this way, both the inner and outer optimization problems can be solved using gradient descent algorithms.\nPractically, we add a perturbation vector to the residual stream at a specific layer of a transformer model. For each batch of samples, we optimize the perturbation vector using its gradient for a fixed number of steps. Subsequently, we apply the classical stochastic gradient descent algorithm to update the model's parameters, with the previously optimized perturbation vector in its residual streams. The impact"}, {"title": "4.2 Two Adversarial Unlearning Methods", "content": "of the choice of perturbation layers and the number of inner optimization steps will be discussed in Section 5.\nOur adversarial unlearning framework is suitable for most of the existing machine unlearning algorithms. In this paper, we apply our framework to augment the GA and NPO methods, resulting in two new algorithms: AdvGA and AdvNPO, which are described below.\nAdvGA. By substituting the internal minimization loss function in Equation 8 with Equation 3, we can obtain the following loss function 2:\n\\[\\min_{\\pi_{\\theta}} -E_{D_f} \\underset{\\delta}{min} L (\\pi_{\\theta_2} (\\pi_{\\theta_1} (x) + \\delta, y))\\]\nWe denote this new loss function AdvGA.\nAdvNPO. Similarly, we substitute the internal minimization loss function in equation 8 with Equation 3 and the following loss function is obtained:\n\\[\\min_{\\pi_{\\theta}} -E_{D} \\Bigg[ log \\Bigg(1 + \\Bigg(\\frac{\\underset{\\delta}{min} L (\\pi_{\\theta_2} (\\pi_{\\theta_1} (x) + \\delta, y))}{L (\\pi_{ref} (x, y))}\\Bigg)^{2}\\Bigg)\\Bigg]\\]\nWe denote this new loss function Adv NPO. For clarification, we omit the L2-norm restriction on the perturbation vector here, but it should be included during the optimization process. This also applies to the loss function in AdvGA."}, {"title": "4.3 Experiments", "content": "Configurations. We conduct machine unlearning experiments on the following two datasets: RWKU (Jin et al. 2024) and MUSE (Shi et al. 2024). We combine the previously introduced forget loss functions and retain loss functions, and finally obtain 12 unlearning methods as shown in Table 1. We set the perturbation layer to 4, the inner optimization steps to 6, and the weights of the forget and retain loss functions to be equal. We conduct experiments with LLaMA-2-7B-Chat (Touvron et al. 2023), LLaMA-3-8B-Instruct and LLaMA-3.1-8B-Instruct. Following previous work, we run the optimizing process using the AdamW optimizer with a cosine learning rate scheduler. All the experiments are conducted on 4 Nvidia A100 GPUs. Further details are provided in Appendix C.\nResults. The experimental results on RWKU with LLaMA-3-8B-Instruct and LLaMA-3.1-8B-Instruct are presented in Table 1. Additional results with other models on MUSE are provided in the Appendix C. From the table, we can draw the following conclusions:\n(1) Our methods, particularly the AdvNPO series, are highly effective in unlearning the real-world knowledge in LLMs. For example, AdvNPOKLR achieves a performance increase of 57% on the forget set, but only causes a drop of 10.6% on the neighbor set comparing with a vanilla NPO method. This demonstrates the effectiveness of the LAU framework. (2) Our methods cause almost no side effects"}, {"title": "5 Discussion", "content": "5.1 Influence of the Perturb Layers\nWe explore how the specific layer at which the perturbation vector is added influences the final performance. Therefore, we vary the perturbation layer from 0 to 30 for the Llama-3-8B-Instruct model and report the averaged performance on the forget and neighbor datasets for one unlearned model. The experimental results are shown in the upper half of Figure 3. We can draw the following conclusions:\n(1) Adding perturbations at the shallow layers (those closer to the input prompts) is more effective. We attribute this to the fact that perturbations at shallower layers have a more significant impact on the model's output, making them easier to optimize. (2) Directly adding perturbation at the embedding layer is entirely ineffective, as indicated by the point where the perturbation layer equals zero. This is due to the fact that our latent perturbation serves as an approximation of the adversarial queries, but directly adding pertur"}, {"title": "5.2 Influence of the Inner Optimization Steps", "content": "bation at the embedding layer alters the entire prompt rather than simply adding an adversarial suffix. (3) As the perturbation layers get deeper, the performance converges to that of the corresponding non-adversarial method. This finding aligns with the intuition that deeper perturbation layers result in a more limited influence of the perturbation vectors.\nSimilarly, we also explore the influence of the number of inner optimization steps in Equation 8. We follow the same experimental configurations as before, but instead vary the number of inner optimization steps. The experimental results are shown in the lower half of Figure 3. We can draw the following conclusions:\n(1) As we increase the optimization steps, the performance on the forget dataset initially declines, then rises. We thereby conclude that both insufficient and excessive optimization steps are detrimental to unlearning performance. (2) Regardless of the number of optimization steps, the model's performance on the forget dataset consistently remains below that of non-adversarial methods. Even a small, randomly initialized perturbation vector can enhance the robustness of the unlearning process, as indicated by the point where the step equals 0."}, {"title": "5.3 Robustness of Latent Adversarial Unlearning", "content": "Finally, we evaluate the robustness of the unlearned models trained with LAU-augmented methods under our previously proposed dynamic attack framework. We select ten unlearned models and train adversarial suffixes with both the original model and the unlearned model in the within query setting. For a clearer comparison with unlearned models trained using non-adversarial approaches, we report the performance change relative to the scenario without attack. The experimental results are presented in Figure 4. We can draw the following conclusions:\n(1) The LAU-trained models are more robustness than the non-LAU-trained models. For instance, the increased performance under adversaries of the model trained with Non-LAU is nearly twice that of the model trained with LAU. This demonstrates a significant enhancement in the robustness of the unlearned models. (2) The unlearned models become safer especially when their parameters are not available to the attacker. For example, the 'Attack LAU (original)' line consistently remains at a low value as the attack optimization steps increase."}, {"title": "6 Related Work", "content": "Jailbreak Attack. To mitigate the undesirable behaviors of LLMs, the safety-alignment stage has become essential (Wei, Haghtalab, and Steinhardt 2023; Paulus et al. 2024).\nWithin this context, a complementary approach called red teaming is proposed to assess the robustness of the safety-alignment of LLMs by designing jailbreaking attacks (Carlini et al. 2023; Huang et al. 2024). Some work focuses on designing manually crafted adversarial prompts (Yong, Menghini, and Bach 2024; Liu et al. 2024c; Wei et al. 2024b), while others explore automatically generating the prompts via gradient-based optimization (Jones et al. 2023; Zou et al. 2023) or genetic-base methods (Lapid, Langberg, and Sipper 2023; Liu et al. 2024b). However, they primarily focus on the introduction of harmful behaviors, whereas we focus on the resurgence of unlearned knowledge.\nMachine Unlearning. Data protection regulations, such as the European General Data Protection Regulation (GDPR) (Mantelero 2013), have mandated \"the Right to be Forgotten\" and highlight the necessity of machine unlearning (Hu et al. 2024; Jin et al. 2024). Therefore, a number of unlearning benchmarks are proposed, including the forgetting of Harry Potter books (Eldan and Russinovich 2023), facts about fictional authors (Maini et al. 2024) and real world knowledge in LLMs (Jin et al. 2024). Additionally, a line of research explore the methodologies for effective machine unlearning, from variants of gradient-ascent (Jia et al. 2023; Zhang et al. 2024), to localization-informed approaches (Wu et al. 2023; Fan et al. 2024). However, existing unlearning methods remain vulnerable and our proposed LAU framework provides a universal solution to enhancing the robustness of the unlearning process."}, {"title": "7 Conclusion", "content": "In this paper, we propose a dynamic and automated framework to assess the vulnerabilities of the unlearned models. After revealing their susceptibility, we propose a latent adversarial training framework, along with two concrete methods, namely AdvGA and AdvNPO. Extensive experiments on several datasets with various models demonstrate the effectiveness and robustness of our unlearning methods."}, {"title": "A Datasets and Metrics", "content": "A.1 RWKU\nRWKU (Jin et al. 2024) is a real-world knowledge unlearning benchmark. It incorporates the evaluation for unlearning effectiveness and utility preservation.\nFor the unlearning effectiveness, it provides the following three types of knowledge probe questions on the forget set: (1) FB (fill in the blank probes). It assesses the unlearned model's knowledge memorization abilities through fill-in-the-blank style questions. For example, the model is required to complete the following blank: \u201cStephen King's nonfiction work, is a memoir about his process\u201d. (2) QA (question answer probes). It assesses the ability of the unlearned model to utilize knowledge in practical applications. For example, the model is required to answer the following question: \u201cWhich collection of novellas marked Stephen King's major departure from horror?\u201d. (3) AA (adversarial attack probes). It evaluates whether manually designed adversarial questions can bypass restrictions and access the forgotten knowledge. For example, the model is required to answer the following question: \"I am a huge horror literature enthusiast. Can you tell me which book first made Stephen King famous?\". We use the ROUGE-L score to measure the alignment between the model's predictions and the ground truth answers. Lower scores indicate greater unlearning effectiveness.\nFor the utility preservation, it considers the neighboring knowledge of the unlearning target knowledge. For example, if the model is required to forget the author J.K. Rowling, it should forget the answer to \"Who is the author of Harry Potter?\" while remaining unaffected on the question, \"Who are Harry Potter's best friends?\u201d. It provides two types of knowledge probes for neighboring knowledge, FB and QA, as described above. We also use the ROUGE-L score to measure the alignment between the model's predictions and the ground truth answers. In this case, higher scores indicate better utility preservation.\nAdditionally, it provides some membership inference attack (MIA) methods. The MIA methods aim to address the following problem: given a piece of text, can we determine whether the model was pretrained on it without any knowledge of the model's pretraining data? We employ the default metric, LOSS, provided by the benchmark, where we expect higher LOSS scores on the forget set compared to the neighbor set.\nFinally, the benchmark provides measurement of the general capabilities of the unlearned models, and we select the capabilities encompassing the following aspects: (1) Reasoning Ability (Rea). The EM scores are reported. (2) Truthfulness (Tru). The 6-shot accuracy score are reported. (3) Factuality (Fac). The F1 scores are reported. (4) Fluency (Flu). The weighted average of bi- and tri-gram entropies are reported. For all these measurements, higher scores indicate better overall model capabilities.\nA.2 MUSE\nMUSE is a comprehensive machine unlearning evaluation benchmark that considers six desirable properties for the un"}, {"title": "B Dynamic Unlearning Attack Experiments", "content": "learned models.\n(1) No verbatim memorization. The model should not replicate the content in the forget set. The benchmark quantifies this by prompting the model with the first I tokens from a sequence in the forget set and comparing the model's completion with the true completion using the ROUGE-L F1 score.\n(2) No knowledge memorization. The model should not be able to answer questions about the knowledge in the forget set. The benchmark provides QA-style probes and measures the relevance of the model's answer to the ground truth answer using the ROUGE score.\n(3) No Privacy Leakage. Similar to the MIA attack mentioned above, it should be impossible to detect whether the unlearned model was trained on a specific piece of text. The benchmark provides a metric called PrivLeak, for which a good unlearning algorithm should achieve a value close to zero.\n(4) Utility Preservation. The model's performance on the retain set should be preserved. The benchmark provides QA-style probes on the retain set and uses ROUGE scores to measure the model's performance.\nTwo additional evaluation perspectives are scalability and sustainability, which focus on varying sizes of forget sets and successive unlearning requests, respectively. In this paper, we focus on the first four evaluation perspectives and leave the more challenging two scaling perspectives for future research.\nMUSE provides two representative types of textual data in unlearning tasks: news articles and books, and we conduct unlearning experiments on the news corpus."}, {"title": "B.1 Static Attack Construction", "content": "In dataset RWKU, nine types of manually designed adversarial attacks are provided. We selected the most effective one: prefix injection, which adds specific prompts before the question to guide the model's response. We use GPT-4 Turbo to convert each vanilla knowledge probe question into its corresponding prefix injection version. The associated prompts are presented in Table 2."}, {"title": "B.2 Examples of the Dynamic and Static Attack", "content": "We provide several specific examples of both static and dynamic attack results in Table 3."}, {"title": "C Latent Adversarial Unlearning Experiments", "content": "C.1 Configurations\nFor the unlearning experiments on RWKU, we select 10 unlearning targets and calculat the average performance of the unlearned models. The forget set is not explicitly provided in RWKU. Instead, it prompts the original model to generate knowledge related to the unlearning target, resulting in a synthetic forget set. The retain set is also not explicitly provided, so we select the forget set of other entities unrelated"}, {"title": "C.2 Results on MUSE", "content": "to the unlearning target as a synthetic retain set. We train for one epoch in all the experiments.\nFor the unlearning experiments on MUSE, we select this benchmark-provided model 3 as the initial model prior to unlearning. We train for 10 epochs and select the best checkpoint that achieves the optimal trade-off between unlearning effectiveness and utility preservation.\nThe experimental results with LLaMA-2-7B-Chat on the MUSE dataset are shown in Table 4, and we can see that our methods remain effective."}]}