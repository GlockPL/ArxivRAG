{"title": "Mutual Learning for Acoustic Matching and Dereverberation via Visual Scene-driven Diffusion", "authors": ["Jian Ma", "Wenguan Wang", "Yi Yang", "Feng Zheng"], "abstract": "Visual acoustic matching (VAM) is pivotal for enhancing the immersive experience, and the task of dereverberation is effective in improving audio intelligibility. Existing methods treat each task independently, overlooking the inherent reciprocity between them. Moreover, these methods depend on paired training data, which is challenging to acquire, impeding the utilization of extensive unpaired data. In this paper, we introduce MVSD, a mutual learning framework based on diffusion models. MVSD considers the two tasks symmetrically, exploiting the reciprocal relationship to facilitate learning from inverse tasks and overcome data scarcity. Furthermore, we employ the diffusion model as foundational conditional converters to circumvent the training instability and over-smoothing drawbacks of conventional GAN architectures. Specifically, MVSD employs two converters: one for VAM called reverberator and one for dereverberation called dereverberator. The dereverberator judges whether the reverberation audio generated by reverberator sounds like being in the conditional visual scenario, and vice versa. By forming a closed loop, these two converters can generate informative feedback signals to optimize the inverse tasks, even with easily acquired one-way unpaired data. Extensive experiments on two standard benchmarks, i.e., SoundSpaces-Speech and Acoustic AVSpeech, exhibit that our framework can improve the performance of the reverberator and dereverberator and better match specified visual scenarios.", "sections": [{"title": "1 Introduction", "content": "Sound interacts with its environment, giving listeners a sense of objects and spatial imprints [75]. Reverberation is sound lingering in a space from surfaces reflecting sound waves [17,37]. Thus, reverberant sound, faithfully replicating real-world acoustics, is vital for realistic and immersive experiences in applications like augmented and virtual reality [7, 31, 40, 45, 47, 79, 83]. Although reverberation can bestow a realistic sense of space, it may make speech content less intelligible [36, 61]. In line with human perception, automatic speech recognition systems also suffer from lower accuracy when processing reverberant speeches [12, 21, 80]. Therefore, dereverberation techniques [53] can benefit applications such as teleconferencing, hearing aids, voice assistants, etc. Existing works train VAM and dereverberation separately [4, 6, 15, 19, 68, 71]. The traditional methods of acoustic matching primarily involve unraveling the spatial characteristics of sound through the examination of Room Impulse Responses (RIRs), which assess the propagation and variation of sound within a specific environment [2, 3, 16, 52, 64, 71]. Rather than estimating RIRS, VAM [4] directly achieves specified reverberation by employing images of the target environment and original audio clips. For dereverberation, classical methodologies often encompass the application of signal processing and statistical techniques [54,55], recent advances highlight neural network-based approaches that learn transfer functions from reverberation to anechoic spectrograms [12, 14, 22, 81]. Nonetheless, optimizing each task individually fails to leverage the inherent reciprocity between the two tasks (Fig. 1). Moreover, training these methods usually requires extensive paired data. Yet capturing large volumes of aligned anechoic and reverberant audio pairs in real-world scenarios is not feasible. For VAM, the shortage of paired audio usually leads to average-style reverberation. When it comes to dereverberation, the model struggles to produce highly 'clean' audio in response to complex scenarios. Thus, existing methods often face challenges in leveraging extensive unpaired audio due to the varying reverberation levels.\nIn this paper, we consider dereverberation as the inverse task of VAM, serving as an evaluator to provide feedback signals for VAM training, and vice versa. Specifically, given a visual environment v, an anechoic audio $a_c$, and a reverberant audio $a_r$, VAM reverberator $f_\\theta(v, a_c) \\rightarrow \\hat{a}_r$ maps the visual observation and anechoic audio into reverberant audio, while the dereverberator $g_\\phi(v, a_r) \\rightarrow \\hat{a}_c$ restores reverberant audio to anechoic audio conditioned on visual characteris-"}, {"title": "3 Methodology", "content": "We propose a mutual learning framework MVSD to leverage feedback signals from symmetrical tasks to promote model training and better exploit unpaired data. It involves two tasks: a primal task VAM [4] that employs the reverberator $f_\\theta$ to convert an anechoic audio $a_c$ into a reverberated audio $a_r$, which is aurally recorded in the specified environment. In the dual task, dereverberator $g_\\phi$ removes the reverberant characteristics in $a_r$, which is similar to its anechoic counterpart $a_c$. Here, $f_\\theta$ and $g_\\phi$ are jointly trained in an end-to-end mutual learning framework MVSD (\u00a73.1). Furthermore, we employ visual scene-driven diffusion models as foundational conditional converters $f_\\theta$ and $g_\\phi$ to achieve stable training and accurate reverberation style transfer (\u00a73.2).\nReverberator. Consider paired data distributions: $A_c = \\{a_c^{(1)}, a_c^{(2)}, ..., a_c^{(n)}\\}$ and $A_r = \\{a_r^{(1)}, a_r^{(2)}, ..., a_r^{(n)}\\}$, representing anechoic and reverberant audio, respectively. The set of visual scenes $V = \\{v^{(1)}, v^{(2)}, ..., v^{(n)}\\}$ corresponds to the audio set of $A_r$. The goal of VAM is to convert the anechoic audio $a_c$ with condition v to its reverberant counterpart $a_r$, i.e., to estimate the conditional distribution $f_\\theta(a_r | a_c; v)$. Based on diffusion models, we encode $a_c$ into content features and switch the reverberation style to the visual environment v."}, {"title": "3.1 Mutual Learning", "content": "We jointly learn the VAM and dereverberation tasks (Fig. 2): the reverberator $f_\\theta$ and dereverberator $g_\\phi$ can mutually benefit from each other. Suppose we have two (vanilla) converters that can map anechoic audio to a specified reverberation style and vice versa. Our goal is to simultaneously improve the style accuracy of the VAM task and the content intelligibility of the dereverberation task by employing paired and unidirectional non-paired data. To achieve this, we leverage the reciprocity between these two tasks, wherein the input-output spaces of VAM and dereverberation exhibit a strong correlation and can interchangeably act as the input and output for each other. Starting from either task, we first convert it forward to another audio, then transfer it backward to the original audio. By evaluating the results of this two-hop transfer process, we can gauge the quality of both converters and optimize them accordingly. Namely, dereverberator $g_\\phi$ is employed to evaluate the quality of $\\hat{a}_r$ generated by $f_\\theta$ and sends back an error singal $\\Delta(a_c, \\hat{a}_c)$ to $f_\\theta$, and vice versa. This process can be iterated many rounds until both converters converge. Please note that in MVSD, $a_r$ and $a_c$ are not necessarily aligned and may even not have a typical relationship.\nWe denote a labeled collection as $\\mathcal{D}= \\{(v_n, a_c^n, a_r^n)\\}_{n=1}^N$, which consists of N aligned tuples of anechoic and reverberant audio. Given a triplet $(v, a_c, a_r)$, where v, $a_c$, $a_r$ are sets of environmental spaces, anechoic and target audios. Our goal is to uncover the bi-directional relationship between the $a_c$ and $a_r$. For the primal process starting from VAM, denote $\\hat{a}_r$ as the mid-transition output. Firstly, we obtain a reverberated audio $\\hat{a}_r$ through the reverberator $f_\\theta(v, a_c)$. Then, the dereverberator $g_\\phi$ translates $\\hat{a}_r$ to $\\tilde{a}_c$ by mapping $g_\\phi(v, \\hat{a}_r)$. The $\\tilde{a}_c$ is"}, {"title": "3.2 Visual Scene-driven Diffusion", "content": "In MVSD, the reverberator and dereverberator share a similar model structure. We introduce visual scene-driven diffusion (VSD) with the reverberator $f_\\theta$ as an example. The diffusion model employs a T-step iterative denoising process to transform Gaussian noise into the desired data distribution [25, 63, 69]. By introducing prompt conditions such as class labels and text [10,57], the generated content can be controlled precisely. In MVSD, visual scene embeddings are employed as control conditions to guide the generation of reverberator $f_\\theta$ and dereverberator $g_\\phi$. In particular, the diffusion process follows a Markov chain, progressively adding noise to the input spectrogram $x_0$ (sampled from the real distribution $q(x)$) until it evolves into white Gaussian noise $\\mathcal{N}(0, 1)$. At each step t, the spectrogram $x_t$, following the distribution $q(x_t | x_{t-1})$, is derived by the pre-defined variance $\\beta_t$ scaled with $\\sqrt{1 - \\beta_t}$:\n$q(x_t | x_{t-1}) = \\mathcal{N}(x_t; z_t); z_t \\sim \\mathcal{N}(\\sqrt{1 - \\beta_t} z_{t-1}, \\beta_t \\mathcal{I}).$ (5)\nThe denoising process attempts to restore the original spectrogram $x_0$ from the noisy data $x_t$ by removing the noise introduced in the forward diffusion process. The prediction $q(x_{t-1} | x_t)$ at step t - 1 is approximated by a parameterized model p (e.g., a neural network), involving the estimation of $\\mu(z_t, t)$ and $\\sigma(z_t, t)$ from a Gaussian distribution. By employing the reverse process across all time steps, we can transition from $x_T$ back to the initial spectrogram $x_0$:\n$p(x_{0:T}) = p(x_T) \\prod_{t=1}^{T} p(x_{t-1} | x_t)$\n$= p(x_T) \\prod_{t=1}^{T} \\mathcal{N}(x_{t-1}; \\mu(x_t, t), \\sigma(x_t, t)).$ (6)"}, {"title": "3.3 Training Objective", "content": "For training the diffusion model, we employ the simplified objective [25]:\n$\\mathcal{L}_d = \\mathbb{E}_{x_0, t, z} [||z - z(\\sqrt{\\bar{\\alpha}_t}x_0 + \\sqrt{1 - \\bar{\\alpha}_t}z, t)||^2],$ (7)\nwhere $\\alpha_t$ in diffusion models is a scaling factor that modulates the noise level at each time step t. VSD can predict the noise $z_t$ and use it to iteratively refine the denoising process. With the reparameterization trick, a method for differentiable sampling [32], we can represent the estimation of $x_0$:\n$x_0 = \\frac{1}{\\sqrt{\\bar{\\alpha}_t}} (x - \\sqrt{1 - \\bar{\\alpha}_t} \\hat{z}_t).$ (8)\nMoreover, we introduce a style loss $\\mathcal{L}_{sty}$ (Eq. 9) to make the generated audios with the environmental characteristics. Taking VAM task as an example, during training, the Unet predicts the noise $\\hat{z}_t$ at time step t. Then, $\\hat{z}_t$ can be used to gradually derive the predicted original spectrogram $\\hat{x}_t$ at step 0 (Eq. 8). Here, we do not explicitly extract the stylistic features of the $a_r$ and $\\hat{a}_r$; instead, we directly employ $L_1$ loss to regularize style consistency:\n$\\mathcal{L}_{sty} = ||\\hat{a}_r - a_r||_1 + ||\\hat{a}_c - a_c||_1.$ (9)\nWe learn models $f_\\theta$ and $g_\\phi$ by minimizing the combination of the diffusion loss, the style loss and the mutual learning regularization term. In summary, the overall training objective is given as:\n$\\mathcal{L}_{total} = \\mathcal{L}_d + \\mathcal{L}_m + \\mathcal{L}_{sty}.$ (10)"}, {"title": "3.4 Implementation Details", "content": "Training. In MVSD, converters and visual scene encoder are trained separately. We adopt the loss function in [30] to train the visual scene encoder. The mutual learning is integrated into each mini-batch update, spanning the entire training process for the two tasks. Training starts with supervised data, with unsupervised data progressively merged for optimization. This stepwise strategy can preserve model stability. At each iteration, we compute the predictions of both converters and update their parameters based on the feedback from the symmetrical models. In practice, we first perform supervised training and conduct the loop of mutual learning (Alg. 1). Besides minimizing the cycle-consistent loss $L_m$ (Eq. 4), our MVSD framework is learnt with the diffusion objectives for VAM and dereverberation, over the labeled data D. Finally, we receive a prepared model when MVSD converges on all training data."}, {"title": "4 Experiments", "content": "Dataset. We conduct experiments on two datasets [4]: SoundSpaces-Speech and Acoustic AVSpeech datasets. The former employs a simulated environment [5] to generate reverberation audio, is perfectly aligned paired audio and accurate ground truth. Regardless, there has a realism gap. Finally, the dataset is split into"}, {"title": "4.1 Performance on VAM", "content": "As shown in Table 1, MVSD achieves a notable absolute boost of 0.157 STFT-distance (23.6% relative improvement), 0.004 RTE (11.8% relative improvement), and 0.019 MOSE (11.8% relative improvement) in the 'Seen' split of SoundSpaces-Speech dataset compared with SOTA method. There is also a similar improvement in Acoustic AVSpeech dataset. We can see that MVSD exhibits outstanding strengths in all three assessed aspects: preserving source audio content better, getting in more precise signal attenuation and more consistent quality with the target audio. MVSD has the capability to infer and extract relevant factors that influence reverberation from target images, even in never-before-seen scenes. It should be noted that blind reverberator [71], a traditional acoustic method, needs reference audio, making it unsuitable for scenarios 'Unseen' of SoundSpaces (no reference audio) and AVSpeech, as reported in [4]."}, {"title": "4.2 Performance on Derverberation", "content": "Table 2 presents the dereverberation performance of MVSD on SoundSpaces-Speech [4] dataset. We observe that MVSD also demonstrates superior performance across all three metrics in the dereverberation task. Particularly in terms of WER, MVSD exhibits a remarkable error reduction of 0.17 compared to VIDA, achieving a value of 4.27%. This highlights the robust dereverberation capability of MVSD. Additionally, MVSD achieves an EER of 4.46%, demonstrating its ability to mitigate reverberation while preserving the timbre information. Fig. 5 depicts the spectrograms for the dereverberation task on SoundSpaces-Speech, with AVSpeech dataset omitted due to the absence of groundtruth anechoic audio. Spectrogram analysis reveals that MVSD achieves superior clarity and noise reduction in dereverberation, with distinct peaks and fewer artifacts. The improvements observed in both tasks signify that MVSD can leverage the reciprocity to enhance the learning capability."}, {"title": "4.3 User Study", "content": "The human ear is the most accurate tool for evaluating acoustic experiences. Therefore, we conduct a user study as a complement to quantitative indicators. We invited 15 volunteers to participate in the evaluation. Following the configuration in [4], we show participants some images of the target environment, real audio clips, and samples generated by all test methods. Participants is asked to choose the audio sample that exhibit the highest consistency with"}, {"title": "4.4 Ablation Study", "content": "To assess the effectiveness of MVSD's key components, we conduct diagnostic studies and report VAM results on SoundSpaces-Speech dataset [4] and using WER and PESQ metrics for dereverberation.\nMutual Learning. We conduct three diagnostic experiments: i) VSD \u2013 training two tasks separately with the structure akin to mutual learning; ii) MVSD w/o unpaired data - training exclusively with labeled data; and iii) MVSD - augmenting the second experiment with unpaired data. Table 4a reveals that our baseline model VSD can achieve performance matching SOTA on metrics STFT 0.657 and MOSE 0.159. The introduction of MVSD results in a slight edge over SOTA, and incorporating unpaired data notably surpasses SOTA in both tasks (achieved 0.508 STFT, 0.030 RTE, 0.142 MOSE in VAM, and 4.27% WER, 2.53 PESQ in dereverberation, respectively). These findings highlight that the synergy between dual tasks can enhance learning capabilities and efficiently absorb unpaired data, showcasing the benefit of a wealth of natural data.\nModel Design. To validate the superiority of diffusion model, we conduct comparative experiments with two different generator architectures: (1) a conditional generative network based on GAN, influenced by our single-task model, and (2) The controllable Unet in MVSD, designed to showcase the diffusion process. Table 4b shows the controllable Unet outperforms the GAN-based model significantly in all evaluated metrics, STFT reduced by 0.078 to 0.753. WER decreased by 1.57% to 6.74%. The diffusion process significantly further enhances the performance to achieve 0.657 STFT and 4.27% WER. Compared to GANs, diffusion models can excel in stability and sample quality, enabling a more controllable and precise generation process."}, {"title": "5 Conclusion", "content": "In this paper, we introduce MVSD, a mutual learning framework based on visual scene-driven diffusion model, designed for VAM and dereverberation tasks. In early exploration, we combine diffusion model with mutual learning, a strategy that leverages the complementary aspects between tasks to improve both the performance and the generalization capabilities. Consequently, MVSD achieves SOTA performance in the both tasks. We empirically demonstrate that by utilizing a symmetric diffusion model architecture, MVSD can effectively extract and utilize cross-task knowledge across both tasks. Furthermore, by integrating an additional 17.3% of unpaired data into the training set, we have observed a 9.1% relative improvement in RTE for VAM. This strategy allows MVSD to access easily acquired unpaired data, thereby reducing the reliance on annotation. We anticipate our research will enhance the utilization of unidirectional data."}, {"title": "Supplementary Materials", "content": "In the appendix, we provide the following content for a more comprehensive understanding of our method:\n\u00a7 A: Architecture Details. We provide details of the MVSD network archi-tecture, including layer composition, connectivity patterns, etc.\n\u00a7 B: Social Impacts and Limitations.\n\u00a7 C: Qualitative Visualization of MVSD and several competitors."}, {"title": "A Architecture Details", "content": "Our neural network draws inspiration from the Unet structure of Imagen [63]. Taking VAM as an example, in each step of diffusion, the controllable Unet learns to perform cross-modal generation using noisy input, clean spectrograms, and embeddings of the visual environment. As shown in Fig. A1, we divide controllable Unet into encoder and decoder with symmetric structure and both of them consist of 3 attention blocks. Skip connections [24] are employed to bridge encoder and decoder, recovering spatial information lost in downsampling. We only apply cross-modal attention [76] in the third block of the encoder and the first block of the decoder to connect visual cues and spectrograms. In self-attention block, we utilize the downsampling module [72] with a stride of 4 to rapidly reduce the size of the feature map. The feature map undergoes a size transformation in the controllable Unet (1282 \u2192 322 \u2192 82 \u2192 42 \u2192 82 \u2192 322 \u2192 1282). The diffusion training process involves the following steps: starting with a sample from the data distribution, noise is gradually added over a fixed number of timesteps, creating a sequence of increasingly noisy images to reconstruct the original input. During inference, the goal is to generate samples from the learned distribution by starting with pure noise and sequentially applying the trained UNet model to denoise the image over timesteps."}, {"title": "B Social Impacts and Limitations", "content": "MVSD can enrich VR and AR auditory experiences with more realistic acoustics that complement the protagonist's surroundings. VAM can enhance personalized advertis-"}, {"title": "C Qualitative Visualization", "content": "This section showcases visualizations of qualitative results for our MVSD and competing methods. Among them, Fig. A2 and Fig. A3 depict the qualitative results of the VAM task on different datasets. Fig. A4 showcases the visualization of the generated results on SoundSpaces-Speech dataset in the dereverberation task. Fig. A5 illustrates some instances of failure cases observed on SoundSpaces-Speech dataset [4]."}]}