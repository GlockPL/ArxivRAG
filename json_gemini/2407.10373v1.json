{"title": "Mutual Learning for Acoustic Matching and Dereverberation via Visual Scene-driven Diffusion", "authors": ["Jian Ma", "Wenguan Wang", "Yi Yang", "Feng Zheng"], "abstract": "Visual acoustic matching (VAM) is pivotal for enhancing the immersive experience, and the task of dereverberation is effective in improving audio intelligibility. Existing methods treat each task independently, overlooking the inherent reciprocity between them. Moreover, these methods depend on paired training data, which is challenging to acquire, impeding the utilization of extensive unpaired data. In this paper, we introduce MVSD, a mutual learning framework based on diffusion models. MVSD considers the two tasks symmetrically, exploiting the reciprocal relationship to facilitate learning from inverse tasks and overcome data scarcity. Furthermore, we employ the diffusion model as foundational conditional converters to circumvent the training instability and over-smoothing drawbacks of conventional GAN architectures. Specifically, MVSD employs two converters: one for VAM called reverberator and one for dereverberation called dereverberator. The dereverberator judges whether the reverberation audio generated by reverberator sounds like being in the conditional visual scenario, and vice versa. By forming a closed loop, these two converters can generate informative feedback signals to optimize the inverse tasks, even with easily acquired one-way unpaired data. Extensive experiments on two standard benchmarks, i.e., SoundSpaces-Speech and Acoustic AVSpeech, exhibit that our framework can improve the performance of the reverberator and dereverberator and better match specified visual scenarios.", "sections": [{"title": "1 Introduction", "content": "Sound interacts with its environment, giving listeners a sense of objects and spatial imprints [75]. Reverberation is sound lingering in a space from surfaces reflecting sound waves [17,37]. Thus, reverberant sound, faithfully replicating real-world acoustics, is vital for realistic and immersive experiences in applications like augmented and virtual reality [7, 31, 40, 45, 47, 79, 83]. Although reverberation can bestow a realistic sense of space, it may make speech content less intelligible [36, 61]. In line with human perception, automatic speech recognition systems also suffer from lower accuracy when processing reverberant speeches [12, 21, 80]. Therefore, dereverberation techniques [53] can benefit applications such as teleconferencing, hearing aids, voice assistants, etc. Existing works train VAM and dereverberation separately [4, 6, 15, 19, 68, 71]. The traditional methods of acoustic matching primarily involve unraveling the spatial characteristics of sound through the examination of Room Impulse Responses (RIRs), which assess the propagation and variation of sound within a specific environment [2, 3, 16, 52, 64, 71]. Rather than estimating RIRS, VAM [4] directly achieves specified reverberation by employing images of the target environment and original audio clips. For dereverberation, classical methodologies often encompass the application of signal processing and statistical techniques [54,55], recent advances highlight neural network-based approaches that learn transfer functions from reverberation to anechoic spectrograms [12, 14, 22, 81]. Nonetheless, optimizing each task individually fails to leverage the inherent reciprocity between the two tasks (Fig. 1). Moreover, training these methods usually requires extensive paired data. Yet capturing large volumes of aligned anechoic and reverberant audio pairs in real-world scenarios is not feasible. For VAM, the shortage of paired audio usually leads to average-style reverberation. When it comes to dereverberation, the model struggles to produce highly 'clean' audio in response to complex scenarios. Thus, existing methods often face challenges in leveraging extensive unpaired audio due to the varying reverberation levels.\nIn this paper, we consider dereverberation as the inverse task of VAM, serving as an evaluator to provide feedback signals for VAM training, and vice versa. Specifically, given a visual environment v, an anechoic audio ac, and a reverberant audio ar, VAM reverberator fe(v, ac) \u2192 \u00e2, maps the visual observation and anechoic audio into reverberant audio, while the dereverberator g\u00f8(v, ar) \u2192 \u00e2c restores reverberant audio to anechoic audio conditioned on visual characteristics."}, {"title": "MVSD", "content": "tics. There exists a solid reciprocal relationship between the input and output spaces of fe and go. In this study, we delve into exploiting their intrinsic reciprocity to overwhelm the scarcity of parallel data. We propose a Mutual learning mechanism based on Visual Scene-driven Diffusion (MVSD) (Fig. 1). In MVSD, two converters, namely reverberator and dereverberator, are employed and capable of learning from the symmetric tasks. Taking VAM as an example, the reverberator, conditioned on the visual scene v, simulates environmental acoustic effects and converts anechoic audio ac to reverberant audio ar. Since the output of one converter can be used as the input for another, the reverberator and dereverberator can act as mutual evaluators. Concretely, in the primal task VAM, the reverberator generates reverberated audio ar conditioned on the visual scene v and anechoic audio ac. Then the reverse converter go takes ar as input and reconstructs the anechoic audio \u0101c within the symmetric dereverberation task. Finally, the errors between ac and ac are used as feedback signals to optimize reverberator fe, and vice versa. The training process of reverberator fe and dereverberator go can form a closed loop, providing feedback for inverse tasks to enhance data efficiency. When the dereverberator encounters a unpaired natural audio a, with reverberation, it first eliminates the reverberation factors and creates a pseudo-anechoic audio a. Likewise, the reverberator regenerates a based on a and visual observations v'. Hence, MVSD allows these two converters to benefit from each other's training instances and can be extended to easily acquired unpaired audio samples. For conditional generation, the architecture built on GANs is presently the prevailing choice [8, 20, 27, 28, 49, 60]. However, the training of GAN may introduce potential risks of instability and over-smoothing. Diffusion model [1, 9, 10, 25, 41, 43, 48, 63] recently show remarkable milestones in image generation, enabling the creation of high-quality images based on conditioning cues. Some works introduce diffusion into audio generation, such as converting spectrograms into sound signals [35], generating symbolic music [50], etc. However, diffusion generation of specified reverberation styles under visual guidance remains underexplored. To bridge this gap, we meticulously devise a visual scene-driven diffusion model to mitigate the computational overhead. Specifically, the diffusion model for each task includes a visual scene encoder for extracting features to control reverberation style, and a controllable Unet that serves as the generator for producing the desired audio. Additionally, cross-modal attention is adopted in selective blocks to establish correlations between visual cues and audio, reducing computational demands.\nWe spotlight the notable strengths of MVSD in visual-audio cross-modal style transfer. MVSD effectively enhances the performances and consistently reports promising results on both tasks. We achieve a remarkable reduction of 0.157 in STFT-distance on the \u2018Seen' test set of SoundSpaces-Speech [4] (23.6% relative performance). Moreover, the utilization of unpaired audios (17.3% of the training data) can further boost the relative performance by 9.1% in RTE for VAM.\nTo summarize, the main contributions of this paper are as follows:\nWe initially propose an end-to-end approach that leverages the reciprocity between VAM and dereverberation tasks to reduce reliance on paired data."}, {"title": "2 Related Work", "content": "Acoustic Matching. Acoustic matching involves modifying audio to simulate the sound in a given environment. Schroeder et al. [65] first propose the concept of reverberation and apply a series of percolators and delay lines to mimic environmental space characteristics. There are two main methods for acquiring RIRS in the audio community [18,46,52]. (1) Simulation techniques can be employed to produce RIRs when the geometry and material properties of the spatial environment are available [2,3,16]. (2) If detailed information is inaccessible, RIRs can be blindly estimated from audio captured in the room [52,71]. RIRs are then employed to synthesize an auralized audio signal. Both methods have weaknesses. The former requires exhaustive measurements of space that may be infeasible, while the latter may introduce some disturbances due to limited acoustic information. Some recent works [34,68] attempt to approximate RIRs from an environmental image, necessitating paired image and impulse response training data. Regrettably, these methods also require estimating the acoustic parameters from the recorded audio, which severely limits the application scopes. Chen et al. [4] introduce VAM and utilize visual observation to simulate the target environment for generating reverberant audio. However, VAM focuses on acoustic matching, neglecting the correlation and inherent consistency with the reverse dereverberation task. In this paper, we harness the RGB image of specified environment for acoustic matching and utilize the reciprocity with dereverberation to improve the precision of reverberation simulations.\nDereverberation. Due to the challenge of collecting both anechoic and reverberant audio simultaneously, acoustic dereverberation can enhance training data quality by minimizing reverberation disturbance [33,86]. The main stream dereverberation technologies utilize devices like microphone arrays to remove reverberation [51]. Deep learning techniques have also made great strides in reverberation removal [22,81,87]. Tan et al. [74] exploit the movement of the upper lip region to isolate interfering sounds, yet it does not intentionally eliminate reverberation based on visual scene understanding. These methods either disregard or only partially take into account visual information. Chen et al. [6] propose learning all the acoustics characteristics associated with indoor dereverberation. Like acoustic matching, these unidirectional approaches neglect the reciprocal relationship between the two tasks, leading to an incomplete utilization of naturally recorded audio. In contrast, MVSD demonstrates stronger dereverberation capabilities through the assistance of symmetric tasks.\nMutual Learning. Mutual learning, originating from the field of language translation, aims to reduce dependence on data annotation [23]. This mechanism allows alternating between the two sides and enables the language model"}, {"title": "MVSD", "content": "to train solely from one-sided data. The core idea of mutual learning involves establishing a dual-learning game between two agents, each agent is assigned an individual task. In the primal task, mutual learning maps \u00e6 from primal domain to dual domain y, and then restore the original x through the reverse mapping in dual task [77,88]. Hence, mutual learning can produce two feedback signals without requiring parallel data: a style evaluation score indicating the likelihood that the synthesized audio matches the target style, and a reconstruction loss measuring the difference between the reconstructed audio and the original audio. This mechanism alternates between agents, allowing the generator to train from only one-way data [42,66, 82, 84, 85, 88]. We are the first to investigate the duality of VAM and dereverberation. These two tasks are trained together in a mutual learning framework and provide mutual reinforcement signals based on the structural symmetry, even for unpaired samples.\nCondition-guided Generation. In recent years, there have been significant advancements in the field of conditional generation [26,59,67,78]. Diffusion models have demonstrated impressive results in various generative tasks due to their superior visual quality and training stability [1, 9, 10, 25, 35, 41, 43, 50, 56, 63]. The diffusion probability model [69] is based on a Markov chain, proceeding through finite steps in two opposing directions: one transition moves from the data distribution to noise, and the other transitions back from noise to the data distribution. Ho et al. [25] introduce the variational lower bound objective, which is subsequently improved in [56] to obtain higher log-likelihood scores. In this study, we regard audio spectrograms as images and elegantly employ two diffusion-based generators for controllable reverberation style transfer."}, {"title": "3 Methodology", "content": "We propose a mutual learning framework MVSD to leverage feedback signals from symmetrical tasks to promote model training and better exploit unpaired data. It involves two tasks: a primal task VAM [4] that employs the reverberator fe to convert an anechoic audio ac into a reverberated audio ar, which is aurally recorded in the specified environment. In the dual task, dereverberator go removes the reverberant characteristics in ar, which is similar to its anechoic counterpart ac. Here, fe and go are jointly trained in an end-to-end mutual learning framework MVSD (\u00a73.1). Furthermore, we employ visual scene-driven diffusion models as foundational conditional converters fe and go to achieve stable training and accurate reverberation style transfer (\u00a73.2).\nReverberator. Consider paired data distributions: $A_c = \\{a_c^{(1)}, a_c^{(2)}, ..., a_c^{(n)}\\}$ and $A_r = \\{a_r^{(1)}, a_r^{(2)}, ..., a_r^{(n)}\\}$, representing anechoic and reverberant audio, respectively. The set of visual scenes $V = \\{v^{(1)}, v^{(2)}, ..., v^{(n)}\\}$ corresponds to the audio set of $A_r$. The goal of VAM is to convert the anechoic audio ac with condition v to its reverberant counterpart ar, i.e., to estimate the conditional distribution fo(ar ac; v). Based on diffusion models, we encode ac into content features and switch the reverberation style to the visual environment v."}, {"title": "3.1 Mutual Learning", "content": "We jointly learn the VAM and dereverberation tasks (Fig. 2): the reverberator fe and dereverberator go can mutually benefit from each other. Suppose we have two (vanilla) converters that can map anechoic audio to a specified reverberation style and vice versa. Our goal is to simultaneously improve the style accuracy of the VAM task and the content intelligibility of the dereverberation task by employing paired and unidirectional non-paired data. To achieve this, we leverage the reciprocity between these two tasks, wherein the input-output spaces of VAM and dereverberation exhibit a strong correlation and can interchangeably act as the input and output for each other. Starting from either task, we first convert it forward to another audio, then transfer it backward to the original audio. By evaluating the results of this two-hop transfer process, we can gauge the quality of both converters and optimize them accordingly. Namely, dereverberator go is employed to evaluate the quality of \u00e2r generated by fe and sends back an error singal (\u0101c, ac) to fe, and vice versa. This process can be iterated many rounds until both converters converge. Please note that in MVSD, ar and ac are not necessarily aligned and may even not have a typical relationship.\nWe denote a labeled collection as $D= \\{(v_n,a_c^n, a_r^n)\\}_{n=1}^N$, which consists of N aligned tuples of anechoic and reverberant audio. Given a triplet (v, ac, ar), where v, ac, ar are sets of environmental spaces, anechoic and target audios. Our goal is to uncover the bi-directional relationship between the ac and ar. For the primal process starting from VAM, denote \u00e2r as the mid-transition output. Firstly, we obtain a reverberated audio ar through the reverberator fo(v, ac). Then, the dereverberator go translates ar to \u00e3\u00bf by mapping $g_{\\varphi}(v, \\hat{a}_r)$. The \u1fb6 is"}, {"title": "3.2 Visual Scene-driven Diffusion", "content": "allowing the converters to better exploit the cross-modal and cross-task correlations. Second, the addition of unpaired data can boost model performance, and paired data guides the reverberator and dereverberator converge to the target distribution, preventing extreme domain deviation from the unpaired data.\nIn MVSD, the reverberator and dereverberator share a similar model structure. We introduce visual scene-driven diffusion (VSD) with the reverberator fe as an example. The diffusion model employs a T-step iterative denoising process to transform Gaussian noise into the desired data distribution [25, 63, 69]. By introducing prompt conditions such as class labels and text [10,57], the generated content can be controlled precisely. In MVSD, visual scene embeddings are employed as control conditions to guide the generation of reverberator fe and dereverberator go. In particular, the diffusion process follows a Markov chain, progressively adding noise to the input spectrogram xo (sampled from the real distribution q(x) until it evolves into white Gaussian noise N(0, 1). At each step t, the spectrogram \u00e6t, following the distribution q(x+xt-1), is derived by the pre-defined variance \u1e9et scaled with \u221a1 \u2013 \u03b2t:\n$q(x_t|x_{t-1}) = N(x_t; z_t); z_t \\sim N(\\sqrt{1 - \\beta_t}z_{t-1}, \\beta_tI)$.\n(5)\nThe denoising process attempts to restore the original spectrogram o from the noisy data \u00e6r by removing the noise introduced in the forward diffusion process. The prediction q(xt-1|xt) at step t - 1 is approximated by a parameterized model p (e.g., a neural network), involving the estimation of \u03bc(zt, t) and \u03c3(zt, t) from a Gaussian distribution. By employing the reverse process across all time steps, we can transition from a back to the initial spectrogram x0:\n$p(x_{0:T}) = p(x_T) \\prod_{t=1}^T p(x_{t-1}|x_t)$\n$=p(x_T) \\prod_{t=1}^T N(x_{t-1}; \\mu(x_t, t), \\sigma(x_t, t)).$\n(6)"}, {"title": "3.3 Training Objective", "content": "Visual Scene Encoder. We apply an embedding with 256 dimensions to represent visual scenes, extracted by a pre-trained ResNet-18 [24] encoder. Then, the embedding serves as the condition to guide the generation of diffusion models.\nControllable Unet. We meticulously design an controllable Unet for predicting xt of diffusion (Fig. 3). Controllable Unet is composed of multiple stages with attention blocks [63], i.e., self-attention and cross-attention. Self-attention allows a model to weigh the importance of different parts within the same element. Cross-attention, similar to self-attention, targets relationships across different components. We employ a classic encoder-decoder with a symmetric design, where each part incorporating 3 attention blocks. The encoder progressively reduces the resolution of the feature map, and then the decoder gradually increases it to align with the size of the original spectrogram. In the self-attention block, we utilize the downsampling method in [72] with a stride of 4 to rapidly decrease the size of feature maps. The downsampling utilizes dilated convolutions and attention to increase the receptive field without reducing spatial dimensions. Cross-modal attention is selectively employed to the third encoder block and the first decoder block, mitigating computational overhead. Both VAM and dereverberation need to preserve the linguistic information in the audios. Therefore, we concatenate source spectrogram with the noise zo as the content input for the controllable Unet. Please refer to the supplementary material for details.\nFor training the diffusion model, we employ the simplified objective [25]:\n$L_d = E_{x_0, t, z} [||z - z(\\sqrt{\\bar{\\alpha}_t}x_0 + \\sqrt{1 - \\bar{\\alpha}_t}z, t)||^2]$,\n(7)\nwhere at in diffusion models is a scaling factor that modulates the noise level at each time step t. VSD can predict the noise 2t and use it to iteratively refine the denoising process. With the reparameterization trick, a method for differentiable sampling [32], we can represent the estimation of 20:\n$\\hat{x}_0 = \\frac{1}{\\sqrt{\\bar{\\alpha}_t}}(x_t - \\sqrt{1 - \\bar{\\alpha}_t}\\hat{z})$.\n(8)\nMoreover, we introduce a style loss Lsty (Eq. 9) to make the generated audios with the environmental characteristics. Taking VAM task as an example, during training, the Unet predicts the noise 2t at time step t. Then, 2t can be used to gradually derive the predicted original spectrogram \u00e6r at step 0 (Eq. 8). Here, we do not explicitly extract the stylistic features of the ar and \u00e2r; instead, we directly employ L\u2081 loss to regularize style consistency:\n$L_{sty} = ||\\hat{a}_r - a_r||_1 + ||\\hat{a}_c - a_c||_1$.\n(9)\nWe learn models fe and go by minimizing the combination of the diffusion loss, the style loss and the mutual learning regularization term. In summary, the overall training objective is given as:\n$L_{total} = L_d + L_m + L_{sty}$.\n(10)"}, {"title": "3.4 Implementation Details", "content": "Training. In MVSD, converters and visual scene encoder are trained separately. We adopt the loss function in [30] to train the visual scene encoder. The mutual learning is integrated into each mini-batch update, spanning the entire training process for the two tasks. Training starts with supervised data, with unsupervised data progressively merged for optimization. This stepwise strategy can preserve model stability. At each iteration, we compute the predictions of both converters and update their parameters based on the feedback from the symmetrical models. In practice, we first perform supervised training and conduct the loop of mutual learning (Alg. 1). Besides minimizing the cycle-consistent loss Lm (Eq. 4), our MVSD framework is learnt with the diffusion objectives for VAM and dereverberation, over the labeled data D. Finally, we receive a prepared model when MVSD converges on all training data."}, {"title": "4 Experiments", "content": "Dataset. We conduct experiments on two datasets [4]: SoundSpaces-Speech and Acoustic AVSpeech datasets. The former employs a simulated environment [5] to generate reverberation audio, is perfectly aligned paired audio and accurate ground truth. Regardless, there has a realism gap. Finally, the dataset is split into"}, {"title": "4.1 Performance on VAM", "content": "As shown in Table 1, MVSD achieves a notable absolute boost of 0.157 STFT-distance (23.6% relative improvement), 0.004 RTE (11.8% relative improvement), and 0.019 MOSE (11.8% relative improvement) in the 'Seen' split of SoundSpaces-Speech dataset compared with SOTA method. There is also a similar improvement in Acoustic AVSpeech dataset. We can see that MVSD exhibits outstanding strengths in all three assessed aspects: preserving source audio content better, getting in more precise signal attenuation and more consistent quality with the target audio. MVSD has the capability to infer and extract relevant factors that influence reverberation from target images, even in never-before-seen scenes. It should be noted that blind reverberator [71], a traditional acoustic method, needs reference audio, making it unsuitable for scenarios 'Unseen' of SoundSpaces (no reference audio) and AVSpeech, as reported in [4]."}, {"title": "4.2 Performance on Derverberation", "content": "Table 2 presents the dereverberation performance of MVSD on SoundSpaces-Speech [4] dataset. We observe that MVSD also demonstrates superior performance across all three metrics in the dereverberation task. Particularly in terms of WER, MVSD exhibits a remarkable error reduction of 0.17 compared to VIDA, achieving a value of 4.27%. This highlights the robust dereverberation capability of MVSD. Additionally, MVSD achieves an EER of 4.46%, demonstrating its ability to mitigate reverberation while preserving the timbre information. Fig. 5 depicts the spectrograms for the dereverberation task on SoundSpaces-Speech, with AVSpeech dataset omitted due to the absence of groundtruth anechoic audio. Spectrogram analysis reveals that MVSD achieves superior clarity and noise reduction in dereverberation, with distinct peaks and fewer artifacts. The improvements observed in both tasks signify that MVSD can leverage the reciprocity to enhance the learning capability."}, {"title": "4.3 User Study", "content": "The human ear is the most accurate tool for evaluating acoustic experiences. Therefore, we conduct a user study as a complement to quantitative indicators. We invited 15 volunteers to participate in the evaluation. Following the configuration in [4], we show participants some images of the target environment, real audio clips, and samples generated by all test methods. Participants is asked to choose the audio sample that exhibit the highest consistency with"}, {"title": "4.4 Ablation Study", "content": "To assess the effectiveness of MVSD's key components, we conduct diagnostic studies and report VAM results on SoundSpaces-Speech dataset [4] and using WER and PESQ metrics for dereverberation.\nMutual Learning. We conduct three diagnostic experiments: i) VSD \u2013 training two tasks separately with the structure akin to mutual learning; ii) MVSD w/o unpaired data - training exclusively with labeled data; and iii) MVSD - augmenting the second experiment with unpaired data. Table 4a reveals that our baseline model VSD can achieve performance matching SOTA on metrics STFT 0.657 and MOSE 0.159. The introduction of MVSD results in a slight edge over SOTA, and incorporating unpaired data notably surpasses SOTA in both tasks (achieved 0.508 STFT, 0.030 RTE, 0.142 MOSE in VAM, and 4.27% WER, 2.53 PESQ in dereverberation, respectively). These findings highlight that the synergy between dual tasks can enhance learning capabilities and efficiently absorb unpaired data, showcasing the benefit of a wealth of natural data.\nModel Design. To validate the superiority of diffusion model, we conduct comparative experiments with two different generator architectures: (1) a conditional generative network based on GAN, influenced by our single-task model, and (2) The controllable Unet in MVSD, designed to showcase the diffusion process. Table 4b shows the controllable Unet outperforms the GAN-based model significantly in all evaluated metrics, STFT reduced by 0.078 to 0.753. WER decreased by 1.57% to 6.74%. The diffusion process significantly further enhances the performance to achieve 0.657 STFT and 4.27% WER. Compared to GANs, diffusion models can excel in stability and sample quality, enabling a more controllable and precise generation process."}, {"title": "5 Conclusion", "content": "In this paper, we introduce MVSD, a mutual learning framework based on visual scene-driven diffusion model, designed for VAM and dereverberation tasks. In early exploration, we combine diffusion model with mutual learning, a strategy that leverages the complementary aspects between tasks to improve both the performance and the generalization capabilities. Consequently, MVSD achieves SOTA performance in the both tasks. We empirically demonstrate that by utilizing a symmetric diffusion model architecture, MVSD can effectively extract and utilize cross-task knowledge across both tasks. Furthermore, by integrating an additional 17.3% of unpaired data into the training set, we have observed a 9.1% relative improvement in RTE for VAM. This strategy allows MVSD to access easily acquired unpaired data, thereby reducing the reliance on annotation. We anticipate our research will enhance the utilization of unidirectional data."}, {"title": "A Architecture Details", "content": "Our neural network draws inspiration from the Unet structure of Imagen [63]. Taking VAM as an example, in each step of diffusion, the controllable Unet learns to perform cross-modal generation using noisy input, clean spectrograms, and embeddings of the visual environment. As shown in Fig. A1, we divide controllable Unet into encoder and decoder with symmetric structure and both of them consist of 3 attention blocks. Skip connections [24] are employed to bridge encoder and decoder, recovering spatial information lost in downsampling. We only apply cross-modal attention [76] in the third block of the encoder and the first block of the decoder to connect visual cues and spectrograms. In self-attention block, we utilize the downsampling module [72] with a stride of 4 to rapidly reduce the size of the feature map. The feature map undergoes a size transformation in the controllable Unet (1282 \u2192 322 \u2192 82 \u2192 42 \u2192 82 \u2192 322 \u2192 1282). The diffusion training process involves the following steps: starting with a sample from the data distribution, noise is gradually added over a fixed number of timesteps, creating a sequence of increasingly noisy images to reconstruct the original input. During inference, the goal is to generate samples from the learned distribution by starting with pure noise and sequentially applying the trained UNet model to denoise the image over timesteps."}, {"title": "B Social Impacts and Limitations", "content": "MVSD can enrich VR and AR auditory experiences with more realistic acoustics that complement the protagonist's surroundings. VAM can enhance personalized advertis-"}, {"title": "C Qualitative Visualization", "content": "This section showcases visualizations of qualitative results for our MVSD and competing methods. Among them, Fig. A2 and Fig. A3 depict the qualitative results of the VAM task on different datasets. Fig. A4 showcases the visualization of the generated results on SoundSpaces-Speech dataset in the dereverberation task. Fig. A5 illustrates some instances of failure cases observed on SoundSpaces-Speech dataset [4]."}, {"title": "3.1 Mutual Learning", "content": "$\\\\Delta_r^u = ||\\hat{a}_r, a_r ||_1 = || f_\\theta(v', g_\\varphi(v', a_r^u)) - a_r^u ||_1.$\n(1)\n$\\\\Delta_c^u = ||\\hat{a}_c, a_c ||_1 = ||g_\\varphi(v, f_\\theta(v, a_c^u)) - a_c^u ||_1.$\n(2)\n$\\Delta_r = ||\\hat{a}_r, a_r ||_1 = || f_\\theta(v', g_\\varphi(v', a_r^u) - a_r^u, ||_1.$\n(3)"}]}