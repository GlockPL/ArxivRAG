{"title": "Customizing Language Models with Instance-wise LoRA for Sequential Recommendation", "authors": ["Xiaoyu Kong", "Jiancan Wu", "An Zhang", "Leheng Sheng", "Hui Lin", "Xiang Wang", "Xiangnan He"], "abstract": "Sequential recommendation systems predict a user's next item of interest by an-\nalyzing past interactions, aligning recommendations with individual preferences.\nLeveraging the strengths of Large Language Models (LLMs) in knowledge com-\nprehension and reasoning, recent approaches have applied LLMs to sequential\nrecommendation through language generation paradigms. These methods convert\nuser behavior sequences into prompts for LLM fine-tuning, utilizing Low-Rank\nAdaptation (LoRA) modules to refine recommendations. However, the uniform\napplication of LoRA across diverse user behaviors sometimes fails to capture\nindividual variability, leading to suboptimal performance and negative transfer be-\ntween disparate sequences. To address these challenges, we propose Instance-wise\nLORA (iLoRA), integrating LoRA with the Mixture of Experts (MoE) framework.\niLORA creates a diverse array of experts, each capturing specific aspects of user\npreferences, and introduces a sequence representation guided gate function. This\ngate function processes historical interaction sequences to generate enriched repre-\nsentations, guiding the gating network to output customized expert participation\nweights. This tailored approach mitigates negative transfer and dynamically adjusts\nto diverse behavior patterns. Extensive experiments on three benchmark datasets\ndemonstrate the effectiveness of iLoRA, highlighting its superior performance\ncompared to existing methods in capturing user-specific preferences and improving\nrecommendation accuracy.", "sections": [{"title": "Introduction", "content": "Sequential recommendation [1-5] suggests a user's next item of interest by analyzing his/her past\ninteractions, tailoring recommendations to individual preferences. As Large Language Models\n(LLMs) [6-12] exhibit impressive proficiency in global knowledge comprehension and reasoning,\ntheir potential for application in sequential recommendation is garnering increasing interest [13-20].\nRecent efforts [21, 22] approach the sequential recommendation task under a language generation\nparadigm, wherein user behavior sequences are converted into input prompts by either purely textual\nprompting (ID numbers or descriptions) [14, 16, 23, 24, 17] or hybrid prompting with additional\nbehavioral tokens [21, 22, 25, 17], achieving remarkable success.\nUpon scrutinizing prior studies on LLM-based sequential recommenders, we can summarize a\ncommon fine-tuning pipeline comprising three sequential components: (1) convert a sequence of\nhistorical behaviors into a prompt; (2) pair these prompts with the subsequent items of interest, to"}, {"title": "Preliminary", "content": "LLM-based Sequential Recommendation. The primary task of sequential recommendation is to\npredict the next item that aligns with user preference [5, 4]. Formally, consider a user with a historical\ninteraction sequence represented as $i_{<n} = [i_1, i_2, ..., i_{n-1}]$, where each $i_j$ is an item interacted with\nat the j-th step. A sequential recommender, parameterized by $\\theta$, inputs this sequence and outputs\na probability distribution over potential next items in the candidate set. This model is trained to\nmaximize the likelihood of the true next item $i_n$:\n$\\max_{\\theta} \\sum_{D} log P_{\\theta} (i_n | i_{<n}).$\nIn the context of LLM-based sequential recommendation, we employ instruction tuning [35, 36],\nwhich fine-tunes LLMs using training data structured into explicit instructional pairs (x, y). Here, x\ncomprises a detailed textual instruction describing the interaction sequences $i_{<n}$ and recommendation\ntask [26, 16, 23, 21, 25, 22], and y is the textual description of the predictive item $i_n$ in the user's\nsequence [7]. The training objective is formulated as an autoregressive model optimization problem:\n$\\max_{\\Phi} \\sum_{(x,y)} \\sum_{t=1}^{y} log P_{\\theta}(y_t | x, y_{<t}),$\nwhere $\\theta$ denotes the LLM's model parameters, $y_t$ represents the t-th token in the output sequence,\nand $y_{<t}$ includes all preceding tokens in the sequence. This objective ensures that each prediction\nis informed by both the prior items in the sequence and the detailed instructions describing the\nsequential recommendation task [4, 5, 21, 25, 22].\nFine-tuning with Low-rank Adaption (LoRA). Fully fine-tuning LLMs (cf. Equation (2)) entails\nsubstantial computational resources [6-12]. LoRA emerges as an efficient alternative [37-43], which\ninjects trainable low-rank matrices into transformer layers to approximate the updates of pre-trained\nweights [42]. At the core, LoRA employs a low-rank decomposition where the update $\\Delta W$ to the\npre-trained matrix $W \\in R^{d_{out}\\times d_{in}}$ is represented as $\\Delta W = BA$, where $B \\in R^{d_{out}\\times r}$ and $A \\in R^{r\\times d_{in}}$\nare tunrable up- and down-projection matrices, respectively. The rank r is significantly smaller\nthan both $d_{in}$ and $d_{out}$, enhancing adaptation efficiency.\nTypically, LoRA applies such updates to the query and value projection matrices in the multi-head\nattention sub-layers within transformer layers [37]. Specifically, for an input h to the linear projection\nin the multi-head attention, LoRA results in the output h' as:\n$h' = (W+\\Delta W)h = Wh + \\frac{\\alpha}{r}BAh,$\nwhere W remains frozen, and $\\alpha$ is introduced as a scaling factor that adjusts the influence of the\nupdates w.r.t. the original W.\nThis methodology introduces a flexible and efficient means to customize these models to new tasks,\ncircumventing the need for extensive retraining of all model parameters.\nFine-tuning with Hybrid Prompting. In the field of LLM-based sequential recommendation, a\ncritical challenge is the divergence between the natural language space and the \u201cuser behavior\"\nspace. To bridge this gap, previous research [21, 22, 25] introduces a hybrid prompt approach, which\nincorporates behavioral insights captured by recommendation models into the prompts. This approach\ncombines the textual token representation derived from the LLM's word embedding layer, with a\nbehavior token representation learned from the recommender model with a cross-modal projector.\nFormally, for an item i with associated metadata $t x t$, the LLM tokenizer and word embedding layer\nLLM-TKZ(\u00b7) convert it into token representations s:\n$s = \\text{LLM-TKZ}(txt).$"}, {"title": "Methodology", "content": "To address the issue of negative transfer associated with conventional LoRA fine-tuning, we introduce\nthe Instance-wise LoRA (iLoRA) fine-tuning framework. This innovative approach adapts the\nMixture of Experts (MoE) concept [29, 30] to tailor Large Language Models (LLMs) to individual\ncharacteristics in sequential recommendation, as illustrated in Figure 2. At the core is the integration\nof multiple experts, each encouraged to capture a specific aspect of user behaviors. Different instances\nof user behavior (i.e., item sequences) use a gating network to create instance-wise attention scores\nover experts. Such attentive experts instantiate the trainable matrices B and A, thus personalizing a\nLORA. Upon this instance with its individually activated LoRA, We fine-tune the LLM to minimize\nthe negative transfer among disparate sequences.\nApplying a uniform LoRA across the population of sequence instances risks overlooking individual\nvariability and easily causes negative transfer, where distinct sequences might adversely affect each"}, {"title": "Instance-wise Generation for Sequential Recommendation", "content": "other. This inspires us to view the modeling of each individual instance as a separate task instead,\nand customize instance-wise LoRA module. By doing so, the LLM-based recommender is expected\nto align more closely with the behavioral and preference variability of individual users. Formally, for\nany sequence instance $i_{<n}$, the autoregressive objective with instance-wise LoRA modules is as:\n$\\max_{\\Delta\\varphi} \\sum_{(x,y)} \\sum_{t=1}^{y}log P_{\\theta + \\Delta{\\varphi(i_{<n})}} (y_t|x, y_{<t}),$\nwhere $\\Delta\\varphi(i_{<n})$ yields $i_{<n}$-exclusive parameters of instance-wise LoRA, as compared to the shared\nparameters of uniform LoRA (i.e., $\\phi$ in Equation (7))\nTo this end, one straightforward solution is to set up different LoRA modules for individual sequences,\nenabling each module to act as the expert tailored to its respective sequence. However, it is impractical\nin terms of resource and time requirements, particularly as the number of sequences often reaches\nthe millions. This highlights the need for a more scalable solution to address the challenge of\nsequence-specific customization without excessive computational overhead."}, {"title": "Instance-wise LoRA with the Mixture of Experts Concept", "content": "Instead of establishing various LoRA modules, we implement the mixture-of-experts (MoE) concept\n[29, 30] to devise our instance-wise LoRA (iLoRA) framework. This framework includes three\ncomponents: (1) Diverging from the standard LoRA module with up- and down-projection matrices,\nwe divide each matrix into an array of experts, each encouraged to capture a distinct, hidden aspect\nof user behavior; (2) For a given sequence instance, we use a gating network to obtain attention\nscores across the arrays of up- and down-projection experts, such that distinct sequences are likely\nto activate different experts; (3) Such an attentive combination of up- and down-projection experts\ninstantiates the weights of LoRA, which are individually customized for the instance of interest. We\nwill elaborate on these components one by one."}, {"title": "Splitting Low-Rank Matrices into Experts", "content": "Typically, the architectural foundation of LoRA is built upon two low-rank matrices: down-projection\n$B\\in R^{d_{out}\\times r}$ and up-projection $A \\in R^{r\\times d_{in}}$. Here we meticulously divide each projection matrix\ninto an array of experts, as illustrated in Figure 2. Each expert is intended to focus on capturing one\nspecific, hidden aspect of user preference. Formally, splitting the low-rank matrices is as follows:\n$B = [B_1, B_2,\\ldots, B_K], A = [A_1, A_2,\\ldots, A_K],$\nwhere $B_k \\in R^{d_{out}\\times r^*}$ and $A_k \\in R^{r^*\\times d_{in}}$ are the up- and down-projection pairs for the k-th expert,\nrespectively; $r^* = \\frac{r}{K}$ is the partial rank determined by the total rank r of LoRA and a predefined\nnumber of experts K.\nBy dividing individual LoRA modules into specialized experts, we ensure a more granular and precise\nadaptation to user preferences. This segmentation approach allows each expert to focus on specific\naspects of user interaction patterns, thereby mitigating the risk of negative transfer that arises from\ngeneralized adaptations. We should emphasize that such a segmentation scheme preserves the overall\nnumber of parameters equivalent to that of the standard LoRA, therefore preventing the potential\noverfitting issue."}, {"title": "Generating Instance-wise Attentions over Experts", "content": "Having obtained the experts (i.e., up-projection submatrices ${B_k}_{k=1}^K$, down-projection submatrices\n${A_k}_{k=1}^K$), we construct an instance-guided gating function to yield the contribution of each expert tai-\nlored to a specific sequence. Specifically, for a sequence of historical items $i_{<n} = [i_1, i_2, \\ldots, i_{n-1}]$,\nwe utilize a sequential recommender (e.g., SASRec [3]) to extract its representation as follows:\n$z = SR-EMB(i_{<n}).$\nHere $z \\in R^d$ provides a holistic view of the user's behavioral patterns and preferences. Subsequently,\nto ascertain the influence of each expert on distilling behavior patterns from this sequence, we get the\ncontribution scores via a linear transformation with a softmax function:\n$w = \\text{Softmax}(\\text{Proj}_g (z)),$"}, {"title": "Aggregating Mixture of Experts as Instance-wise LoRA", "content": "For the instance $i_{<n}$ associated with the instance-wise attentions $w$, we use the mixture-of-experts\nconcept to aggregate the up- and down-projection submatrices from different experts, so as to establish\nthe instance-wise LoRA parameters:\n$\\Delta W(i_{<n}) = \\sum_{k=1}^{K} w_k B_k A_k,$\nwhere $\\Delta W(i_{<n})$ encapsulates the adjustments enabled by our iLoRA. The attention score $w_k$,\nassigned by the gating network (cf. Equation (11)), reflects the relevance of expert k's contribution to\nthe particular sequence.\nWe apply such instance-wise LoRA updates on the transformer layers of the base LLM, which\ncollectively construct the tunable parameters $\\Delta\\varphi(i_{<n})$. Clearly, iLoRA maintains the same total\nnumber of parameters as the standard LoRA, but dynamically customizes varying LoRA modules for\ndifferent instances. This dynamic adaptation of parameters ensures that our model remains flexible\nand responsive to the varied preferences and behaviors exhibited by users, effectively managing\nthe complexity inherent in sequential recommendation systems. This approach not only enhances\npersonalization but also improves the predictive accuracy of the recommendation system."}, {"title": "Experiments", "content": "In this section, we first justify the need to reshape the fine-tuning task with a uniform LoRA\nmodule as a multi-task learning framework for sequential recommendation. Here we conduct\nextensive experiments on various real-world datasets, including LastFM [31], MovieLens [32], and\nSteam [33], to evaluate the effectiveness of our iLoRA framework. Our analysis includes detailed\ncomparisons of iLoRA against established baseline models, which encompass both traditional\nsequential recommender models (e.g., GRU4Rec [1], Caser [2], SASRec [3]) and LLM-based\nrecommender models (e.g., Llama2-7B [9], GPT-4 [44], MoRec [34], TALLRec [16], LLaRA [21]).\nValidRatio [21] and HitRatio@1 are used as evaluation metrics, to separately quantify the ratios of\nvalid responses over all sequences and relevant items over all candidate items, reflecting the model\ncapability of instruction following and recommendation accuracy. See Appendix A for more details\nof these baselines, datasets, and metrics. Moreover, we perform a thorough ablation study to identify\nthe key components that enhance iLoRA's performance, focusing particularly on the role of the gating\nnetwork and expert settings. In a nutshell, we would like to answer the following research questions:\n\u2022 RQ1: What is the rationale behind instance-wise LoRA compared to the uniform LoRA module?\n\u2022 RQ2: How does iLoRA perform in comparison to traditional sequential recommender systems and\nLLM-based recommender models?\n\u2022 RQ3: What is the impact of the designed components (e.g., the gating network, expert settings) on\nthe recommendation performance of iLoRA?"}, {"title": "Investing Rationale of Instance-wise LORA (RQ1)", "content": "We begin by experimenting with LLaRA [21], an LLM-based recommender using a uniform LORA\nmodule, to identify a key limitation: negative transfer between significantly different sequences."}, {"title": "Negative Transfer in Uniform LoRA & Instance-wise LORA", "content": "Gradient similarity reflects the proximity of recommendation sequences [28]. Here we explore\nwhether using LLaRA to perform recommendations conditioned on different sequences exhibits\nsimilar loss geometries and vice versa. To achieve this, we use Euclidean distance to control task\nsimilarity and gradient similarity to measure loss geometry. Specifically, we observe strong clustering along the diagonal of\nthe gradient similarity matrix for sets of recommendation sequences that are closely related in the\nEuclidean space. Conversely, recommendation sequences that are distant in Euclidean space exhibit\ncorrespondingly lower gradient similarity, leading to negative transfer.\nIn contrast, we visualize iLoRA's gradient similarity among the identical clusters. The similarities\nbetween some clusters tend to achieve zero scores, indicating iLoRA's capability to mitigate the\nnegative transfer between significantly different sequences."}, {"title": "Expert Showcase in Instance-wise LoRA", "content": "In this section, we visualize the attention scores of iLoRA's four experts for ten distinct sequences in\nFigure 3c. Each horizontal bar represents a sequence, and the length of the segments within each bar\nindicates the percentage of attention scores assigned to each expert. We have several findings:\n\u2022 Sequence Variability: There is significant variability in expert activation across different sequences.\nFor example, Sequence 4 heavily relies on Expert 1 with a 42.5% activation weight, while Expert 4\nonly contributes 18.8%, demonstrating distinct preferences for different experts among sequences."}, {"title": "Performance Comparison (RQ2)", "content": "This section comprehensively compares iLoRA against some traditional and LLM-based recom-\nmenders. We conduct a holistic evaluation, considering metrics of both HitRatio@1 and ValidRatio\nacross LastFM, MovieLens, and Steam datasets to demonstrate the effectiveness of iLoRA. The\nresults of this comparison are summarized in Table 1.\nOur findings indicate that iLoRA consistently outperforms these baseline models across the three\ndatasets. Specifically, iLoRA achieves the highest HitRatio@1 metrics of 0.5000, 0.5275, and 0.5264\non the LastFM, MovieLens, and Steam datasets, respectively. These results demonstrate the efficacy\nof leveraging sequence representations as guidance signals to fine-tune LoRA parameters, enabling\npersonalized recommendations at the parameter level."}, {"title": "Ablation Study (RQ3)", "content": "In this section, we analyze the effectiveness of the\nmain components of iLoRA in Section 4.3.1. Sub-\nsequently, in Section 4.3.2, we conduct an in-depth\ninvestigation and analysis of how varying the number\nof experts affects the performance of iLoRA."}, {"title": "Effects of Gating Network", "content": "Here we explore the influence of sequence represen-\ntation on the gating network and MoE. Going beyond\nthe sequence-tailored representation, we test two vari-\nants: using random-initialized and token-collapsed\nembeddings as the guidance. consistently outperforms the other variants across three datasets, illustrating the rationale of our gating\nnetwork and the benefits for the MoE combination."}, {"title": "Effect of Expert Numbers", "content": "In this section, we investigate how iLoRA would react to the number of experts. As depicted in\nFigure 4a, our model achieves optimal performance when the number of experts is set to 4. Increasing\nthe number of task experts does not necessarily correlate with enhanced performance. Specifically,\nemploying only 2 experts does not significantly improve the HitRatio@1 metrics on Steam and\nMovieLens datasets, while showing a slight decrease on LastFM. However, with the increase to 4\nexperts, the model exhibits its best performance across all three datasets, notably surpassing the\n2-expert variant. To elaborate, on LastFM, MovieLens, and Steam datasets, the performance of the\n4-expert variant exceeds that of the 2-expert variant by 5.2%, 6.4% and 2.2%, respectively. When the\nnumber of experts is further increased to 8, the performance resembles that of the 4-expert scenario or\neven shows a slight decrease. This suggests that the benefits of increased capacity gradually converge\nas we utilize more experts. Consequently, we adapt 4 experts as the default setting.\nFurthermore, we analyzed the performance across different numbers of experts at various epochs,\non the Steam dataset. It is evident that as the number of experts is set to 1, 2, and 4, the overall\nrecommendation performance of the model steadily improves with training progress. Under this\nconfiguration, the HitRatio@1 values exhibit a positive correlation with the number of experts.\nHowever, when the number of experts reaches 8, the data indicate that the model rapidly achieves a\ndecent performance, but subsequent HitRatio@1 values do not show significant improvements with\nincreasing epochs. We speculate that as the number of experts increases to 8, the model may overly\nfocus on personalized user behaviors, leading to a decrease in generalization ability and premature\noverfitting."}, {"title": "Conclusion", "content": "In this paper, we introduced instance-wise LoRA (iLoRA), a novel fine-tuning framework designed\nto address the challenges posed by the substantial individual variability in user behaviors within\nsequential recommendation systems. By integrating the mixture of experts (MoE) concept into the\nbasic LoRA module, iLoRA dynamically adjusts to diverse user behaviors, thereby mitigating the\nnegative transfer issues observed with standard single-module LoRA approaches. iLoRA represents a\nsignificant advancement in the application of large language models to sequential recommendation\ntasks. By incorporating a mixture of expert frameworks within the LoRA module, iLoRA provides\na more nuanced and effective means of tailoring recommendations to individual user preferences,\npaving the way for more personalized and accurate recommendation systems."}, {"title": "Limitation", "content": "While iLoRA demonstrates promising results, there are several limitations to consider. First, our\nexperiments are constrained by computational resources, limiting the exploration of a larger number\nof expert combinations and their potential impact on recommendation performance. Second, we do\nnot extensively investigate the effects of using hard routing for recommendations with a large number\nof experts. Finally, our study focused on sequential recommendation tasks, and the applicability\nof iLORA to other types of recommendation systems or domains remains to be explored. These\nlimitations suggest that further research is needed to fully understand the scalability and effectiveness\nof iLoRA with more complex expert configurations."}, {"title": "Broader Impact", "content": "Our proposed method, Instance-wise LoRA (iLoRA), advances sequential recommendation systems\nby sequence-tailored recommendations. By leveraging the Mixture of Experts (MoE) framework,\niLoRA streamlines the user experience, reduces decision fatigue, and promotes inclusivity in online\nspaces. Its instance-wise adaptation mechanism ensures diverse content exposure, fostering a more\nenriched online discourse. Beyond recommendations, iLoRA's principles extend to education,\nhealthcare, and e-commerce, offering customized solutions in various domains. Overall, iLoRA\nrepresents a step forward in enhancing user experience and promoting inclusivity in the digital\nlandscape."}, {"title": "Description of Figure 1", "content": "This figure illustrates the gradient similarity of LoRA modules across all training steps. We utilize\nsignals from the collaborative space to partition the sequence dataset into 8 clusters. Euclidean\ndistance is employed to evaluate the proximity between clusters, whereas gradient similarity is\nmeasured to assess geometric loss. Clusters that are closer in the collaborative space are depicted\nas closer together in the left side of the figure, with darker-colored cells indicating higher gradient\nsimilarity. On the right side of the figure, we conducted a case study on three sets of data with\npairwise cosine similarities of 0.86 and -0.75. For sequences containing more than three movies of\nthe same genre, we performed a cross-matching analysis. It was observed that the two user sequences\nfrom the cluster with a cosine similarity of 0.86 both exhibited a strong interest in thriller movies,\nsharing two identical items in their interaction histories at the same time. In contrast, the two user\nsequences from the cluster with a cosine similarity of -0.75 did not demonstrate any noticeable\npreference similarities."}, {"title": "Experimental Design and Evaluation", "content": "Datasets. To validate the generalization ability of iLoRA, we conducted extensive experiments on four\ndatasets derived from real-world recommendation scenarios: LastFM, GoodReads, Amazon-book,\nand Steam:\n\u2022 LastFM Collected from the Last.fm online music platform, including user-artist listening relation-\nships and the names of artists.\n\u2022 MovieLens A commonly used movie recommendation dataset that contains user ratings and movie\ntitles.\n\u2022 Amazon-book Contains user reviews and interactions with books available on the Amazon plat-\nform.\n\u2022 Steam Encompasses user reviews for video games on the Steam Store, in addition to game titles.\nBaselines. We compared iLoRA with several models, including traditional sequential recommen-\ndation models and those based on Large Language Models (LLMs), such as GRU4Rec[1], Caser,\nSASRec[3], Llama2[9], GPT-4, MoRec, TallRec[16], and LlaRA[21]. GRU4Rec, Caser, and SASRec\nutilize recurrent neural networks, convolutional neural networks, and Transformer encoders, respec-\ntively, to capture sequential patterns in user behavior. Llama2, a well-known open-source LLM, is\nreleased by Meta, and in our experiments, we utilized the 7B version. GPT-4, the latest version of the\nGPT series, is a milestone in the performance of LLMs across diverse tasks, from natural language\nprocessing to other domains. We conducted our research using the API provided by OpenAI. MoRec\nenhances traditional sequential recommendation models, such as text-based features, by leveraging\npretrained modal encoders to strengthen the modal characteristics of encoded items. TallRec guides\nLLMs through fine-tuning recommendation corpora. LlaRA further improves the recommendation\neffectiveness of LLMs by aligning collaborative signals to the text space through a hybrid prompt\nmethod.\nTraining Protocol. For our study, we opt for Llama2-7B [9] as the LLM backbone. Given the\ndisparity in computational resources between traditional sequential recommendation models and LLM-\nbased methods, we execute baseline models on a single Nvidia A40, while LLaRA is implemented\non a single Nvidia-smi a100. This ensures optimal utilization of available computational resources.\nAll implementations are carried out using Python 3.8 and PytorchLightning 1.8.6.\nEvaluation Protocol.\nTo assess the performance of iLoRA and baseline models, we construct candidate sets for each\nsequence by randomly selecting 20 non-interacted items while ensuring the inclusion of the correct\nnext item. Performance is evaluated using the HitRatio@1 metric, wherein models attempt to identify\nthe correct item from the candidate set. LLM-based recommenders, when provided with appropriate\nprompts, generate a single candidate item. Conversely, traditional models are adapted to select the\nitem with the highest probability. Additionally, we introduce the ValidRatio metric to quantify the\nproportion of valid responses (i.e., items within the candidate set) across all sequences. This metric\nserves to evaluate the models' adherence to instructions accurately."}, {"title": "Experimental Setup of RQ1", "content": "We extend the previous research setup to train models on multi-task scenarios [45]. Specifically, we\njointly train recommendation sequences in a basic LoRA training framework. We use an effective\nbatch sizes of 128 sequences. The recommendation sequences are divided into multiple tasks using\nrepresentations derived from the sequential recommendation model SASRec, with a dimension of 64.\nTo investigate large-scale multi-tasking in sequential recommendation tasks, we sample 40k sequences\nfrom the Steam dataset. We clustered these sequences into 8 sub-datasets using Euclidean distance. At\ncheckpoints across 1k training steps, we measured the pairwise cosine similarity of model gradients\nfor all sequences. We averaged the LoRA gradients that were bound to the same modules, such as\ngateproj."}, {"title": "Related Work", "content": "Large Language Models Recent years have witnessed a surge of activity in language modeling\nresearch, establishing it as a cornerstone for both understanding and generating language. This\nmomentum has given rise to a new breed of language models (LMs), including notable works such\nas BERT [6], GPT-3 [7], LLama [8], LLama2 [9], Mistral-7B [10], Alpaca [11], and Vicuna [12].\nThese LMs, predominantly based on the Transformer architecture, have demonstrated remarkable\nversatility, exemplified by models like BERT [6] and T5 [36], owing to their extensive training\ncorpus. A significant stride in this domain has been the exploration of scaling effects, with researchers\npushing the boundaries by augmenting both the parameter and training corpus scales to unprecedented\nmagnitudes, encompassing billions of parameters and trillions of training tokens [8, 9, 12, 7, 46].\nThese Large Language Models (LLMs) exhibit substantial performance enhancements and showcase\nunique capabilities, including but not limited to common sense reasoning and instruction following.\nMoreover, the development of domain-specific LLMs further enriches this landscape. Models\ntailored to specific domains, such as finance [47], medicine [48], and law [49], amalgamate domain\nexpertise with the inherent commonsense knowledge of general LLMs. These advancements not\nonly broaden the scope of LLM applications but also inspire exploration into their potential utility in\nrecommendation systems.\nMixture of Experts Mixture of Experts. MoE models [50, 29, 30] are considered as an effective way\nto increasing the model capacity in terms of parameter size. In MoE, certain parts of the model are\nactivated while the computation is kept the same or close to its dense counterpart. Recently, it has\nbeen thoroughly investigated in the field of computer vision [51, 52], natural language processing\n[53, 54] and multi-modal learning [55, 56]."}, {"title": "Statistics", "content": "For all conventional sequential recommendation baselines, we employ the Adam optimization\nalgorithm, establishing a learning rate of 0.001, an embedding dimension d of 64, and a batch size of\n256. Furthermore, we implement L2 regularization, with the coefficient determined through a grid\nsearch within the range [1e-3, 1e-4, 1e-5, le-6, 1e-7]. To mitigate the impact of randomness, we\nreport the mean of the outcomes derived from random seeds within the set [0, 1, 2, 3, 4]. For all\nmethods related to LLMs, we employ a warm-up strategy for the learning rate. This strategy initiates\nthe training process with a relatively modest learning rate, set at 1/100 of the maximum learning\nrate. Note that the parameters of the projector are updated during the training process using\nthe same learning rate curve with LoRA. Additionally, we utilize a cosine scheduler to adjust the\nlearning rate over steps, and we implement half-precision computation to optimize memory usage\nand computational efficiency during the experiments. Each experiment is trained for a maximum of\nfive epochs, with a batch size of 128. Furthermore, we apply a weight decay factor of 1e-5 to prevent\noverfitting."}]}