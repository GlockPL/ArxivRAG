{"title": "Customizing Language Models with Instance-wise LoRA for Sequential Recommendation", "authors": ["Xiaoyu Kong", "Jiancan Wu", "An Zhang", "Leheng Sheng", "Hui Lin", "Xiang Wang", "Xiangnan He"], "abstract": "Sequential recommendation systems predict a user's next item of interest by analyzing past interactions, aligning recommendations with individual preferences. Leveraging the strengths of Large Language Models (LLMs) in knowledge comprehension and reasoning, recent approaches have applied LLMs to sequential recommendation through language generation paradigms. These methods convert user behavior sequences into prompts for LLM fine-tuning, utilizing Low-Rank Adaptation (LoRA) modules to refine recommendations. However, the uniform application of LoRA across diverse user behaviors sometimes fails to capture individual variability, leading to suboptimal performance and negative transfer between disparate sequences. To address these challenges, we propose Instance-wise LORA (iLoRA), integrating LoRA with the Mixture of Experts (MoE) framework. iLoRA creates a diverse array of experts, each capturing specific aspects of user preferences, and introduces a sequence representation guided gate function. This gate function processes historical interaction sequences to generate enriched representations, guiding the gating network to output customized expert participation weights. This tailored approach mitigates negative transfer and dynamically adjusts to diverse behavior patterns. Extensive experiments on three benchmark datasets demonstrate the effectiveness of iLoRA, highlighting its superior performance compared to existing methods in capturing user-specific preferences and improving recommendation accuracy.", "sections": [{"title": "1 Introduction", "content": "Sequential recommendation [1-5] suggests a user's next item of interest by analyzing his/her past interactions, tailoring recommendations to individual preferences. As Large Language Models (LLMs) [6-12] exhibit impressive proficiency in global knowledge comprehension and reasoning, their potential for application in sequential recommendation is garnering increasing interest [13-20]. Recent efforts [21, 22] approach the sequential recommendation task under a language generation paradigm, wherein user behavior sequences are converted into input prompts by either purely textual prompting (ID numbers or descriptions) [14, 16, 23, 24, 17] or hybrid prompting with additional behavioral tokens [21, 22, 25, 17], achieving remarkable success.\nUpon scrutinizing prior studies on LLM-based sequential recommenders, we can summarize a common fine-tuning pipeline comprising three sequential components: (1) convert a sequence of historical behaviors into a prompt; (2) pair these prompts with the subsequent items of interest, to"}, {"title": "2 Preliminary", "content": "LLM-based Sequential Recommendation. The primary task of sequential recommendation is to predict the next item that aligns with user preference [5, 4]. Formally, consider a user with a historical interaction sequence represented as i<n = [i1, i2, ..., in\u22121], where each ij is an item interacted with at the j-th step. A sequential recommender, parameterized by \u03b8, inputs this sequence and outputs a probability distribution over potential next items in the candidate set. This model is trained to maximize the likelihood of the true next item in:\n\\max_{\\theta} \\sum_D log P_{\\theta} (i_n | i_{<n}).\nIn the context of LLM-based sequential recommendation, we employ instruction tuning [35, 36], which fine-tunes LLMs using training data structured into explicit instructional pairs (x, y). Here, x comprises a detailed textual instruction describing the interaction sequences i<n and recommendation task [26, 16, 23, 21, 25, 22], and y is the textual description of the predictive item in in the user's sequence [7]. The training objective is formulated as an autoregressive model optimization problem:\n\\max_{\\Phi} \\sum_{(x,y)} \\sum_{t=1}^{y} log P_{\\Phi}(y_t | x, y_{<t}),\nwhere \u03a6 denotes the LLM's model parameters, yt represents the t-th token in the output sequence, and y<t includes all preceding tokens in the sequence. This objective ensures that each prediction is informed by both the prior items in the sequence and the detailed instructions describing the sequential recommendation task [4, 5, 21, 25, 22].\nFine-tuning with Low-rank Adaption (LoRA). Fully fine-tuning LLMs (cf. Equation (2)) entails substantial computational resources [6-12]. LoRA emerges as an efficient alternative [37-43], which injects trainable low-rank matrices into transformer layers to approximate the updates of pre-trained weights [42]. At the core, LoRA employs a low-rank decomposition where the update \u2206W to the pre-trained matrix W \u2208 Rdout\u00d7din is represented as \u2206W = BA, where B \u2208 Rdout\u00d7r and A \u2208 Rr\u00d7din and are tunrable up- and down-projection matrices, respectively. The rank r is significantly smaller than both din and dout, enhancing adaptation efficiency.\nTypically, LoRA applies such updates to the query and value projection matrices in the multi-head attention sub-layers within transformer layers [37]. Specifically, for an input h to the linear projection in the multi-head attention, LoRA results in the output h' as:\nh' = (W+\u2206W)h = Wh + \\frac{\\alpha}{r}BAh,\nwhere W remains frozen, and \\frac{\\alpha}{r} is introduced as a scaling factor that adjusts the influence of the updates w.r.t. the original W.\nThis methodology introduces a flexible and efficient means to customize these models to new tasks, circumventing the need for extensive retraining of all model parameters.\nFine-tuning with Hybrid Prompting. In the field of LLM-based sequential recommendation, a critical challenge is the divergence between the natural language space and the \u201cuser behavior\" space. To bridge this gap, previous research [21, 22, 25] introduces a hybrid prompt approach, which incorporates behavioral insights captured by recommendation models into the prompts. This approach combines the textual token representation derived from the LLM's word embedding layer, with a behavior token representation learned from the recommender model with a cross-modal projector. Formally, for an item i with associated metadata txt, the LLM tokenizer and word embedding layer LLM-TKZ(\u00b7) convert it into token representations s:\ns = LLM-TKZ(txt)."}, {"title": "3 Methodology", "content": "To address the issue of negative transfer associated with conventional LoRA fine-tuning, we introduce the Instance-wise LoRA (iLoRA) fine-tuning framework. This innovative approach adapts the Mixture of Experts (MoE) concept [29, 30] to tailor Large Language Models (LLMs) to individual characteristics in sequential recommendation, as illustrated in Figure 2. At the core is the integration of multiple experts, each encouraged to capture a specific aspect of user behaviors. Different instances of user behavior (i.e., item sequences) use a gating network to create instance-wise attention scores over experts. Such attentive experts instantiate the trainable matrices B and A, thus personalizing a LORA. Upon this instance with its individually activated LoRA, We fine-tune the LLM to minimize the negative transfer among disparate sequences.\n3.1 Instance-wise Generation for Sequential Recommendation\nApplying a uniform LoRA across the population of sequence instances risks overlooking individual variability and easily causes negative transfer, where distinct sequences might adversely affect each"}, {"title": "3.2 Instance-wise LoRA with the Mixture of Experts Concept", "content": "Instead of establishing various LoRA modules, we implement the mixture-of-experts (MoE) concept [29, 30] to devise our instance-wise LoRA (iLoRA) framework. This framework includes three components: (1) Diverging from the standard LoRA module with up- and down-projection matrices, we divide each matrix into an array of experts, each encouraged to capture a distinct, hidden aspect of user behavior; (2) For a given sequence instance, we use a gating network to obtain attention scores across the arrays of up- and down-projection experts, such that distinct sequences are likely to activate different experts; (3) Such an attentive combination of up- and down-projection experts instantiates the weights of LoRA, which are individually customized for the instance of interest. We will elaborate on these components one by one.\n3.2.1 Splitting Low-Rank Matrices into Experts\nTypically, the architectural foundation of LoRA is built upon two low-rank matrices: down-projection B\u2208 Rdout\u00d7r and up-projection A \u2208 Rr\u00d7din. Here we meticulously divide each projection matrix into an array of experts, as illustrated in Figure 2. Each expert is intended to focus on capturing one specific, hidden aspect of user preference. Formally, splitting the low-rank matrices is as follows:\nB = [B_1, B_2,\\ldots, B_K], A = [A_1, A_2,\\ldots, A_K],\nwhere Bk \u2208 Rdout\u00d7r* and Ak \u2208 Rr*\u00d7din are the up- and down-projection pairs for the k-th expert, respectively; r* = \\frac{r}{K} is the partial rank determined by the total rank r of LoRA and a predefined number of experts K.\nBy dividing individual LoRA modules into specialized experts, we ensure a more granular and precise adaptation to user preferences. This segmentation approach allows each expert to focus on specific aspects of user interaction patterns, thereby mitigating the risk of negative transfer that arises from generalized adaptations. We should emphasize that such a segmentation scheme preserves the overall number of parameters equivalent to that of the standard LoRA, therefore preventing the potential overfitting issue.\n3.2.2 Generating Instance-wise Attentions over Experts\nHaving obtained the experts (i.e., up-projection submatrices {Bk}_{k=1}^K, down-projection submatrices {Ak}_{k=1}^K), we construct an instance-guided gating function to yield the contribution of each expert tailored to a specific sequence. Specifically, for a sequence of historical items i<n = [i_1, i_2, \\ldots, i_{n-1}], we utilize a sequential recommender (e.g., SASRec [3]) to extract its representation as follows:\nz = SR-EMB(i_{<n}).\nHere z \u2208 Rd provides a holistic view of the user's behavioral patterns and preferences. Subsequently, to ascertain the influence of each expert on distilling behavior patterns from this sequence, we get the contribution scores via a linear transformation with a softmax function:\nw = \\text{Softmax}(\\text{Proj}_{2} (z)),"}, {"title": "4 Experiments", "content": "In this section, we first justify the need to reshape the fine-tuning task with a uniform LoRA module as a multi-task learning framework for sequential recommendation. Here we conduct extensive experiments on various real-world datasets, including LastFM [31], MovieLens [32], and Steam [33], to evaluate the effectiveness of our iLoRA framework. Our analysis includes detailed comparisons of iLoRA against established baseline models, which encompass both traditional sequential recommender models (e.g., GRU4Rec [1], Caser [2], SASRec [3]) and LLM-based recommender models (e.g., Llama2-7B [9], GPT-4 [44], MoRec [34], TALLRec [16], LLaRA [21]). ValidRatio [21] and HitRatio@1 are used as evaluation metrics, to separately quantify the ratios of valid responses over all sequences and relevant items over all candidate items, reflecting the model capability of instruction following and recommendation accuracy. See Appendix A for more details of these baselines, datasets, and metrics. Moreover, we perform a thorough ablation study to identify the key components that enhance iLoRA's performance, focusing particularly on the role of the gating network and expert settings. In a nutshell, we would like to answer the following research questions:\n\u2022 RQ1: What is the rationale behind instance-wise LoRA compared to the uniform LoRA module?\n\u2022 RQ2: How does iLoRA perform in comparison to traditional sequential recommender systems and LLM-based recommender models?\n\u2022 RQ3: What is the impact of the designed components (e.g., the gating network, expert settings) on the recommendation performance of iLoRA?"}, {"title": "4.1 Investing Rationale of Instance-wise LORA (RQ1)", "content": "We begin by experimenting with LLaRA [21], an LLM-based recommender using a uniform LORA module, to identify a key limitation: negative transfer between significantly different sequences."}, {"title": "5 Conclusion", "content": "In this paper, we introduced instance-wise LoRA (iLoRA), a novel fine-tuning framework designed to address the challenges posed by the substantial individual variability in user behaviors within sequential recommendation systems. By integrating the mixture of experts (MoE) concept into the basic LoRA module, iLoRA dynamically adjusts to diverse user behaviors, thereby mitigating the negative transfer issues observed with standard single-module LoRA approaches. iLoRA represents a significant advancement in the application of large language models to sequential recommendation tasks. By incorporating a mixture of expert frameworks within the LoRA module, iLoRA provides a more nuanced and effective means of tailoring recommendations to individual user preferences, paving the way for more personalized and accurate recommendation systems."}, {"title": "6 Limitation", "content": "While iLoRA demonstrates promising results, there are several limitations to consider. First, our experiments are constrained by computational resources, limiting the exploration of a larger number of expert combinations and their potential impact on recommendation performance. Second, we do not extensively investigate the effects of using hard routing for recommendations with a large number of experts. Finally, our study focused on sequential recommendation tasks, and the applicability of iLORA to other types of recommendation systems or domains remains to be explored. These limitations suggest that further research is needed to fully understand the scalability and effectiveness of iLORA with more complex expert configurations."}, {"title": "7 Broader Impact", "content": "Our proposed method, Instance-wise LoRA (iLoRA), advances sequential recommendation systems by sequence-tailored recommendations. By leveraging the Mixture of Experts (MoE) framework, iLoRA streamlines the user experience, reduces decision fatigue, and promotes inclusivity in online spaces. Its instance-wise adaptation mechanism ensures diverse content exposure, fostering a more enriched online discourse. Beyond recommendations, iLoRA's principles extend to education, healthcare, and e-commerce, offering customized solutions in various domains. Overall, iLoRA represents a step forward in enhancing user experience and promoting inclusivity in the digital landscape."}, {"title": "C Experimental Setup of RQ1", "content": "We extend the previous research setup to train models on multi-task scenarios [45]. Specifically, we jointly train recommendation sequences in a basic LoRA training framework. We use an effective batch sizes of 128 sequences. The recommendation sequences are divided into multiple tasks using representations derived from the sequential recommendation model SASRec, with a dimension of 64.\nTo investigate large-scale multi-tasking in sequential recommendation tasks, we sample 40k sequences from the Steam dataset. We clustered these sequences into 8 sub-datasets using Euclidean distance. At checkpoints across 1k training steps, we measured the pairwise cosine similarity of model gradients for all sequences. We averaged the LoRA gradients that were bound to the same modules, such as gateproj."}, {"title": "D Related Work", "content": "Large Language Models Recent years have witnessed a surge of activity in language modeling research, establishing it as a cornerstone for both understanding and generating language. This momentum has given rise to a new breed of language models (LMs), including notable works such as BERT [6], GPT-3 [7], LLama [8], LLama2 [9], Mistral-7B [10], Alpaca [11], and Vicuna [12]. These LMs, predominantly based on the Transformer architecture, have demonstrated remarkable versatility, exemplified by models like BERT [6] and T5 [36], owing to their extensive training corpus. A significant stride in this domain has been the exploration of scaling effects, with researchers pushing the boundaries by augmenting both the parameter and training corpus scales to unprecedented magnitudes, encompassing billions of parameters and trillions of training tokens [8, 9, 12, 7, 46]. These Large Language Models (LLMs) exhibit substantial performance enhancements and showcase unique capabilities, including but not limited to common sense reasoning and instruction following. Moreover, the development of domain-specific LLMs further enriches this landscape. Models tailored to specific domains, such as finance [47], medicine [48], and law [49], amalgamate domain expertise with the inherent commonsense knowledge of general LLMs. These advancements not only broaden the scope of LLM applications but also inspire exploration into their potential utility in recommendation systems.\nMixture of Experts Mixture of Experts. MoE models [50, 29, 30] are considered as an effective way to increasing the model capacity in terms of parameter size. In MoE, certain parts of the model are activated while the computation is kept the same or close to its dense counterpart. Recently, it has been thoroughly investigated in the field of computer vision [51, 52], natural language processing [53, 54] and multi-modal learning [55, 56]."}, {"title": "E Statistics", "content": "For all conventional sequential recommendation baselines, we employ the Adam optimization algorithm, establishing a learning rate of 0.001, an embedding dimension d of 64, and a batch size of 256. Furthermore, we implement L2 regularization, with the coefficient determined through a grid search within the range [1e-3, 1e-4, 1e-5, le-6, 1e-7]. To mitigate the impact of randomness, we report the mean of the outcomes derived from random seeds within the set [0, 1, 2, 3, 4]. For all methods related to LLMs, we employ a warm-up strategy for the learning rate. This strategy initiates the training process with a relatively modest learning rate, set at 1/100 of the maximum learning rate. The max learning rates are 2e-4 and le-4 for the LastFM, MovieLens and Steam datasets, respectively. Note that the parameters of the projector are updated during the training process using the same learning rate curve with LoRA. Additionally, we utilize a cosine scheduler to adjust the learning rate over steps, and we implement half-precision computation to optimize memory usage and computational efficiency during the experiments. Each experiment is trained for a maximum of five epochs, with a batch size of 128. Furthermore, we apply a weight decay factor of 1e-5 to prevent overfitting."}]}