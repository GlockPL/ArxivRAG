{"title": "A Scale-Invariant Diagnostic Approach Towards Understanding Dynamics of Deep Neural Networks", "authors": ["Ambarish Moharil", "Damian Tamburri", "Indika Kumara", "Alireza Azarfar", "Willem-Jan Van Den Heuvel"], "abstract": "This paper introduces a scale-invariant methodology employing Fractal Geometry to analyze and explain the nonlinear dynamics of complex connectionist systems. By leveraging architectural self-similarity in Deep Neural Networks (DNNs), we quantify fractal dimensions and roughness to deeply understand their dynamics and enhance the quality of intrinsic explanations. Our approach integrates principles from Chaos Theory to improve visualizations of fractal evolution and utilizes a Graph-Based Neural Network for reconstructing network topology. This strategy aims at advancing the intrinsic explainability of connectionist Artificial Intelligence (AI) systems.", "sections": [{"title": "1 INTRODUCTION", "content": "Explainable Artificial Intelligence (XAI) seeks to demystify decision-making in complex Machine Learning and Deep Learning systems [1]. While there is no universal definition of explainability, Liao et al. [2] describe it simply as \"an answer to a question\". Adopting connectionism has greatly enhanced modeling capabilities re-garding physical and informational complexities through complex non-linear dynamical systems of independently communicating units [3, 4]. These systems, often referred to as black-box and partially chaotic, show a sensitive dependence on initial conditions, complicating predictions about their long-term behavior [5]. Moreover, the inherent non-linearity across the network architecture suggests a connectionist self-symmetry, invariant across different scales of observation [6].\nThis understanding is crucial for comprehending non-linearities and emergence in such networks. Traditional surrogate methods like LIME, which offer post-hoc explanations via sparse linear feature representations, fail to capture the network's non-linear interactions and dynamic behaviors during optimization [1, 7]. However, recent advancements in the use of fractal features for network analysis and the computation of fractal dimensions in complex systems suggest a new segmentation approach for creating a non-linear connectionist representation across multiple scales [6, 8]. Inspired by Mandelbrot's work in fractal geometry and discoveries of non-linear attractor behaviors [9-11], our research leverages fractal analysis to delve deeply into connectionist network architectures. By evaluating fractal dimensions and roughness, we gain insights into network connectivity and emergent phenomena, thereby en-hancing our understanding of system dynamics and aiding in the identification of cyclic attractors, as demonstrated in Kauffman's studies on Random Boolean Networks (RBNs) [12]. Our approach aims to augment the intrinsic explainability of connectionist net-works and emergent phenomena by addressing two fundamental questions: RQ1. \u201cHow can we create a non-linear connectionist repre-sentation across multiple scales for DNNs?\u201d and RQ2. \u201cTo what extent does such a representation enhance the explainability of nonlinear interactions in DNNs?\u201d"}, {"title": "2 PROPOSED METHODOLOGY", "content": "Our methodology segments the network at specific scales during and after optimization, generating a fractal representation of net-work connections using a graph-based surrogate. Segmenting the layers of a network (DNN), in turn, implies partitioning the associ-ated parameter matrices, as it is the parameters that form and deform the network. Formally, consider a connectionist network f with Lp layers. For any given layer L_i, connected to subsequent layer L_j where i \u2260 j, the parameter matrix W_{n\u00d7m} represents the connec-tions, where n and m denote the number of nodes in layers L_i and L_j respectively. The fractal dimension FD of this matrix is calculated using the box-counting method [8], defined as: FD = \\frac{ln(N)}{ln(1/r)} where N is the number of r\u00d7r boxes needed to cover the matrix. The segment size r, crucial for fractal analysis, must satisfy: r > 1, to avoid granularity at the level of individual matrix elements, and: r < min(n, m), to prevent oversimplification by covering the matrix with a single box. The specific ranges for r are defined as follows:\n\\begin{equation*}\nr \\in\n\\begin{cases}\n[2, \\lfloor\\frac{n}{2}\\rfloor], & \\text{if } n = m \\text{ and } n \\mod 2 \\neq 0 \\\\\n[2, \\frac{n}{2}], & \\text{if } n = m \\text{ and } n \\mod 2 = 0 \\\\\n[2, \\lfloor\\frac{min(n,m)+1}{2}\\rfloor], & \\text{if } n \\neq m \\text{ and } min(n, m) \\mod 2 \\neq 0 \\\\\n[2, min(n,m)], & \\text{if } n\\neq m \\text{ and } min(n, m) \\mod 2 = 0\n\\end{cases}\n\\end{equation*}\nThe segmentation extracts sub-matrices based on r, iterating over"}, {"title": "3 PRELIMINARY RESULTS", "content": "In our study, we examine multiclass classification on the MNIST dataset [14] using a CNN with two convolutional layers of 32 and 64 neurons and two fully connected layers, each convolutional layer utilizing a kernel size of 3 with padding and stride set to 1. We train the model over 50 epochs with a learning rate of 6e - 4 using the Adam Optimizer, and analyze gradients and loss post each epoch through a segmentation of model weights, as described in equation 1. The weights of the first convolutional layer are a 4 \u2013 D tensor [32, 3, 3, 3] and the second layer [64, 32, 3, 3], segmented into four overlapping segments per neuron, scaled by r = 2. We evaluate segment features to determine their influence on inputs as detailed in equation 3, with Figure 2 (left) illustrating the captured segment features, and Figure 2 (right) displaying the exponential kernel interactions between segments, indicating local influence across layers. We analyze the phase flow graph \\frac{M_a}{L} \\coloneqq \\sum_{j=1}^{M} \\sum_{i=1}^{Q} (\\frac{\\partial W}{\\partial L}) for a network with M layers and Q segments per layer, where L is the cross-entropy loss. Figure 3 (left) depicts the learning trajectory of a segment, highlighting an initial acceleration followed by a gradual deceleration, and Figure 3 (right) shows a plot of \\frac{\\partial v_s}{\\partial t^2}, evidencing a positive trend. The epochs, color-coded, reveal a decreasing trend in gradient norms, suggesting model stabilization. Data convergence in later epochs toward the center indicates the formation of an attractor, enhancing predictability and training stability."}, {"title": "4 RESEARCH ROADMAP", "content": "Our future research in Explainable Artificial Intelligence (XAI) encompasses three interconnected strategic categories, aiming to deepen the integration and sophistication of graphical models and"}]}