{"title": "A Scale-Invariant Diagnostic Approach Towards Understanding\nDynamics of Deep Neural Networks", "authors": ["Ambarish Moharil", "Damian Tamburri", "Indika Kumara", "Alireza Azarfar", "Willem-Jan Van Den Heuvel"], "abstract": "This paper introduces a scale-invariant methodology employing\nFractal Geometry to analyze and explain the nonlinear dynamics\nof complex connectionist systems. By leveraging architectural self-\nsimilarity in Deep Neural Networks (DNNs), we quantify fractal\ndimensions and roughness to deeply understand their dynamics\nand enhance the quality of intrinsic explanations. Our approach\nintegrates principles from Chaos Theory to improve visualizations\nof fractal evolution and utilizes a Graph-Based Neural Network for\nreconstructing network topology. This strategy aims at advancing\nthe intrinsic explainability of connectionist Artificial Intelligence\n(AI) systems.", "sections": [{"title": "1 INTRODUCTION", "content": "Explainable Artificial Intelligence (XAI) seeks to demystify decision-\nmaking in complex Machine Learning and Deep Learning sys-\ntems [1]. While there is no universal definition of explainability,\nLiao et al. [2] describe it simply as \"an answer to a question\". Adopt-\ning connectionism has greatly enhanced modeling capabilities re-\ngarding physical and informational complexities through complex\nnon-linear dynamical systems of independently communicating\nunits [3, 4]. These systems, often referred to as black-box and par-\ntially chaotic, show a sensitive dependence on initial conditions,\ncomplicating predictions about their long-term behavior [5]. More-\nover, the inherent non-linearity across the network architecture\nsuggests a connectionist self-symmetry, invariant across different\nscales of observation [6].\nThis understanding is crucial for comprehending non-linearities\nand emergence in such networks. Traditional surrogate methods like\nLIME, which offer post-hoc explanations via sparse linear feature\nrepresentations, fail to capture the network's non-linear interac-\ntions and dynamic behaviors during optimization [1, 7]. However,\nrecent advancements in the use of fractal features for network anal-\nsis and the computation of fractal dimensions in complex systems\nsuggest a new segmentation approach for creating a non-linear\nconnectionist representation across multiple scales [6, 8]. Inspired\nby Mandelbrot's work in fractal geometry and discoveries of non-\nlinear attractor behaviors [9-11], our research leverages fractal\nanalysis to delve deeply into connectionist network architectures.\nBy evaluating fractal dimensions and roughness, we gain insights\ninto network connectivity and emergent phenomena, thereby en-\nhancing our understanding of system dynamics and aiding in the\nidentification of cyclic attractors, as demonstrated in Kauffman's\nstudies on Random Boolean Networks (RBNs) [12]. Our approach\naims to augment the intrinsic explainability of connectionist net-\nworks and emergent phenomena by addressing two fundamental\nquestions: RQ1. \"How can we create a non-linear connectionist repre-\nsentation across multiple scales for DNNs?\" and RQ2. \"To what extent\ndoes such a representation enhance the explainability of nonlinear\ninteractions in DNNs?\""}, {"title": "2 PROPOSED METHODOLOGY", "content": "Our methodology segments the network at specific scales during\nand after optimization, generating a fractal representation of net-\nwork connections using a graph-based surrogate. Segmenting the\nlayers of a network (DNN), in turn, implies partitioning the associ-\nated parameter matrices, as it is the parameters that form and deform\nthe network. Formally, consider a connectionist network f with\nLp layers. For any given layer Li, connected to subsequent layer Lj\nwhere i \u2260 j, the parameter matrix Wnxm represents the connec-\ntions, where n and m denote the number of nodes in layers Li and Lj\nrespectively. The fractal dimension FD of this matrix is calculated\nusing the box-counting method [8], defined as: FD = $\\frac{\\ln(N)}{\\ln(1/r)}$ where N is the number of r\u00d7r boxes needed to cover the matrix.\nThe segment size r, crucial for fractal analysis, must satisfy: r > 1,\nto avoid granularity at the level of individual matrix elements, and:\nr < min(n, m), to prevent oversimplification by covering the matrix\nwith a single box. The specific ranges for r are defined as follows:\n$\\begin{cases}\n[2, \\lfloor\\frac{n}{2}\\rfloor], & \\text{if } n = m \\text{ and } n \\mod 2 \\neq 0 \\\\\n[2, \\frac{n}{2}], & \\text{if } n = m \\text{ and } n \\mod 2 = 0 \\\\\n[2, \\lfloor\\frac{\\min(n,m)+1}{2}\\rfloor], & \\text{if } n \\neq m \\text{ and } \\min(n, m) \\mod 2 \\neq 0 \\\\\n[2, \\min(n,m)], & \\text{if } n\\neq m \\text{ and } \\min(n, m) \\mod 2 = 0\n\\end{cases}$\n\nThe segmentation extracts sub-matrices based on r, iterating over"}, {"title": "3 PRELIMINARY RESULTS", "content": "In our study, we examine multiclass classification on the MNIST\ndataset [14] using a CNN with two convolutional layers of 32 and 64\nneurons and two fully connected layers, each convolutional layer\nutilizing a kernel size of 3 with padding and stride set to 1. We train\nthe model over 50 epochs with a learning rate of 6e - 4 using the\nAdam Optimizer, and analyze gradients and loss post each epoch\nthrough a segmentation of model weights, as described in equation\n1. The weights of the first convolutional layer are a 4 \u2013 D tensor\n[32, 3, 3, 3] and the second layer [64, 32, 3, 3], segmented into four\noverlapping segments per neuron, scaled by r = 2. We evaluate\nsegment features to determine their influence on inputs as detailed\nin equation 3, with Figure 2 (left) illustrating the captured segment\nfeatures, and Figure 2 (right) displaying the exponential kernel\ninteractions between segments, indicating local influence across\nlayers. We analyze the phase flow graph $\\frac{1}{M} \\sum_{j=1}^{M} \\sum_{i=1}^{Q} (W_{i,j})$ Vs L for\na network with M layers and Q segments per layer, where L is the\ncross-entropy loss. Figure 3 (left) depicts the learning trajectory\nof a segment, highlighting an initial acceleration followed by a\ngradual deceleration, and Figure 3 (right) shows a plot of $\\frac{\\partial L}{\\partial W_{1,2}}$ vs $\\frac{\\partial L}{\\partial W_{2,2}}$, evidencing a positive trend. The epochs, color-coded, reveal a\ndecreasing trend in gradient norms, suggesting model stabilization.\nData convergence in later epochs toward the center indicates the\nformation of an attractor, enhancing predictability and training\nstability."}, {"title": "4 RESEARCH ROADMAP", "content": "Our future research in Explainable Artificial Intelligence (XAI)\nencompasses three interconnected strategic categories, aiming to\ndeepen the integration and sophistication of graphical models and"}]}