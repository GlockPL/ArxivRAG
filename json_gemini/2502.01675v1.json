{"title": "Semantic Communication based on Generative AI: A New Approach to Image Compression and Edge Optimization", "authors": ["Francesco Pezone", "Sergio Barbarossa", "Giuseppe Caire"], "abstract": "As digital technologies continue to advance, modern communication networks face unprecedented challenges in handling the vast amounts of data produced daily by connected intelligent devices. Autonomous vehicles, smart sensors, IoT systems etc., are gaining more and more interest and new communication paradigms are needed. This thesis addresses these challenges by combining semantic communication with generative models to optimize image compression and resource allocation in edge networks. Unlike traditional bit-centric communication systems, semantic communication prioritizes the transmission of meaningful data specifically selected to convey the meaning rather than obtain a faithful representation of the original data. The communication infrastructure can benefit of the focus solely on the relevant parts of the data due to significant improvements in bandwidth efficiency and latency reduction.\nCentral to this work is the design of semantic-preserving image compression algorithms, utilizing advanced generative models such as Generative Adversarial Networks and Denoising Diffusion Probabilistic Models. These algorithms compress images by encoding only semantically relevant features and exploiting the generative power at the receiver side. This allows for the accurate reconstruction of high-quality images with minimal data transmission. The thesis also introduces a Goal-Oriented edge network optimization framework based on the Information Bottleneck problem and stochastic optimization, ensuring that communication resources are dynamically allocated to maximize efficiency and task performance.\nBy integrating semantic communication into edge networks, the proposed system achieves a balance between computational efficiency and communication effectiveness, making it particularly suited for real-time applications. The thesis compares the performance of these semantic communication models with conventional image compression techniques, using both classical and semantic-aware evaluation metrics. The results demonstrate the potential of combining generative AI and semantic communication to create more efficient semantic-goal-oriented communication networks that meet the demands of modern data-driven applications.", "sections": [{"title": "Introduction", "content": "The rapid advancement of digital technology has fundamentally transformed the way information is generated, transmitted, and consumed. From high-resolution multimedia content to the proliferation of the Internet of Things (IoT), autonomous vehicles, and smart cities, the modern world is producing data at an unprecedented rate [3, 4]. Traditional communication systems, initially designed for human-to-human interaction and optimized for transmitting raw data, are struggling to keep pace with the vast amount of information generated on a daily basis [41, 68]. Bandwidth limitations, latency constraints and numerous other issues represent significant challenges in modern network designs [104].\nAt the core of these issues lies a fundamental mismatch between the growth of data generation and the expansion of communication infrastructure capabilities. Conventional communication paradigms, based on Shannon's information theory [90], focus on the accurate compression and delivery of bits and symbols, regardless of their contextual significance. While this bit-centric approach is effective in scenarios where preserving every detail of the original data is crucial (such as transmitting ultra-high-definition images e.g. to be posted on social networks), it becomes increasingly inefficient in contexts where only a fraction of the transmitted data is relevant to the end-user or application [101, 102]. Considering all symbols as equally important can lead to suboptimal use of bandwidth and computational resources, increasing the pressure on communication networks.\nOne possible scenario where the bit-centric communication framework could increase the pressure on the communication network is in the so-called machine-to-machine communication. The absence of humans in this type of communication relies on the fact that machines can autonomously collect, process, and send data from a transmitter to a receiver that will further process them to take some actions. In this context, transmitting the raw data in its entirety can be both redundant and counterproductive. Machines are, in fact, designed to process the data via some algorithms. These can either be a simple rule-based algorithm or a more complex and advanced Neural Network (NN). In both cases the model will focus mainly on particular features that most influence the decision. All the other features will be discarded as they are non-relevant [108]. This implies that the same action or decision can often be executed without the need for the original raw data but instead just by using a transformation that preserves information about the relevant parts. This process will guarantee the same level of performance, eliminating the necessity for bit-by-bit transmission [60]. An example where it is beneficial to consider only relevant features is the so-called human-robot interaction. In this framework a human is interacting with machines that communicate with each other.\n..."}, {"title": "Contribution and Thesis Outline", "content": "In this section, the structure of the thesis is introduced and a brief description of the main contributions is provided.\nChapter 2) Foundations of Semantic and Goal-Oriented Communication: This chapter provides an overview of the three levels of communication proposed by Weaver in 1953, focusing on the SemC and GOC levels. The differences and the relations between these levels are discussed, highlighting the importance of semantic information in communication systems. Additionally, in this chapter the advantages of employing generative models to achieve SemC and how the IB problem can be useful for GOC are suggested.\nChapter 3) Foundations of Generative Models and Evaluation Metrics: This chapter goes over the fundamental concepts of generative models, including Vector Quantized GANs (VQ-GANs), Vector Quantized VAEs (VQ-VAEs), and DDPMs, providing all the necessary background on the most important architectures that will be used in the following chapters. Furthermore, various classical and semantic evaluation metrics used to assess the performance of the proposed image compression models will be discussed. After a general overview of these metrics, the last part of this chapter will be devoted to the introduction of a new specifically\n..."}, {"title": "Related Publications", "content": "The main contributions of this thesis will be introduced in Chapter 4, 5 and 6 and are based on the following publications:\nGoal-Oriented Communication for Edge Learning based on the Information Bottleneck\nFrancesco Pezone, Sergio Barbarossa, Paolo Di Lorenzo\nProceedings of 2022 IEEE International Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), 2022, pp. 8832-8836\nSemantic-Preserving Image Coding based on Conditional Diffusion Models\nFrancesco Pezone, Osman Musa, Giuseppe Caire, Sergio Barbarossa\nProceedings of 2024 IEEE International Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), 2024, pp. 13501-13505\nC-SPIC: Class-Specific Semantic-Preserving Image Coding with Residual Enhancement for Accurate Object Recovery\nFrancesco Pezone, Osman Musa, Giuseppe Caire, Sergio Barbarossa\nManuscript in Preparation\nSQ-GAN: Semantic Image Coding Using Masked Vector Quantization\nFrancesco Pezone, Sergio Barbarossa, Giuseppe Caire\nManuscript in Preparation\nOther contributions more related to the overall idea and advantages of generative models for SemC\nand the IB problem for GOC are presented in Chapter 2 and are based on the following publications:\nSemantic and Goal-Oriented Communications\nSergio Barbarossa, Francesco Pezone\n6G Wireless Systems: Enabling Technologies, edited by M. Chiani, S. Buzzi, L. San-\nguinetti, and U. Spagnolini, CNIT Tech Report, 2022\nSemantic Communications based on Adaptive Generative Models and Information Bottleneck\nSergio Barbarossa, Danilo Comminiello, Eleonora Grassucci, Francesco Pezone, Stefania\nSardellitti, Paolo Di Lorenzo\nIEEE Communications Magazine, vol. 61, no. 11, 2023, pp. 36-41\nAdditionally, a publication leveraging the syntactic level of communication to perform radio fre-quency denoising could have been included. However, it has been left out for sake of exposition and is reported here for reference:\nDemucs for Data-Driven RF Signal Denoising\n\u00c7a\u011fkan Yapar, Fabian Jaensch, Jan C. Hauffen, Francesco Pezone, Peter Jung, Saeid\nK. Dehkordi, Giuseppe Caire\nProceedings of 2024 IEEE International Conference on Acoustics, Speech, and Signal\nProcessing Workshops (ICASSPW), 2024, pp. 95-96"}, {"title": "Remark on Notation", "content": "This thesis extensively utilizes concepts from Machine Learning (ML), for this reason it is important to introduce some notation that will be used throughout the following chapters.\nThe original image will be referred to as x and the original SSM as s. They are both represented by tensors in three dimensions. In the context of tensors, the term \"shape\" will be used to refer to the dimensions of the tensor, that is, how many elements exist in each dimension. The term \"size\" will be used to refer to the total number of elements present in the entire tensor.\nThe original image x is a tensor of shape 3 \u00d7 H \u00d7 W, where 3 refers to the RGB channels, H is the height and W is the width of the frame. The SSM s is a tensor of shape nc \u00d7 H \u00d7 W, where nc refers to the number of semantic classes, H is the height and W is the width of the frame. In some contexts, the SSM will be represented by a tensor of shape 3 \u00d7 H \u00d7 W for the RGB representation of 1 \u00d7 H \u00d7 W for the grayscale representation, they can all be considered interchangeable. However, the first representation will be the one used as input to any NN model.\nThe proposed models in this thesis are designed to compress x and s at the transmitter side and reconstruct an image x and a SSM s at the receiver. The term \"reconstructed\" will be used when referring to the output of the proposed models, i.e. x and \u015d, while the term \"generated\" refers to the SSM obtained from an image, either x or \u0177, via some out-of-the-shelf pre-trained state-of-the-art (SOTA) Semantic Segmentation Model (SS-Model). In this context, the term SSM preservation or SSM retention will refer to the property of the reconstructed image x to generate a SSM that is similar to the original s associated to x.\nAny intermediate output of the model is referred to as the \"latent representation,\" the \"latent tensor,\" or the \"features tensor\".\nMoreover, a generic part P of the model is understood to depend on some trainable parameters \u03b8. If P(\u00b7) is used instead, it denotes the function implemented by this part, mapping a given input to its corresponding output based on the current parameters.\nOther specific notations are defined along the thesis when necessary."}, {"title": "Foundations of Semantic and Goal-Oriented Communication", "content": "The content of this chapter is based on the current state of the art and on the contributions at the core of Section 2.3.1, Section 2.5 and the implementation of the IB principle in the context of GOC. These contributions are based on the following publications:\nGoal-Oriented Communication for Edge Learning based on the Information Bottleneck\nFrancesco Pezone, Sergio Barbarossa, Paolo Di Lorenzo\nSemantic and Goal-Oriented Communications\nSergio Barbarossa, Francesco Pezone\nSemantic Communications based on Adaptive Generative Models and Information Bottleneck\nSergio Barbarossa, Danilo Comminiello, Eleonora Grassucci, Francesco Pezone, Stefania\nSardellitti, Paolo Di Lorenzo"}, {"title": "Introduction", "content": "In 1983, during an interview, the famous physicist Richard Feynman was asked why two magnets repel each other. Faced with this apparently simple question, one of the greatest physicists who ever lived took the opportunity to illustrate an important lesson: answering a \"why\" question is not easy at all! One of the first assumptions is that the questioner and the respondent share some common knowledge. It is useless if the questioner is a 5-year-old and the answer involves concepts of quantum mechanics. Another assumption is that there must be a point at which the question is considered answered. It is always possible to respond with another \"why\" question; it's the classic game that kids love to play keep asking \"why?\". For this reason, Richard Feynman ultimately told the interviewer, \"I'm not going to be able to give you an answer to why magnets attract or repel, except to tell you that they do.\"\nThis anecdote is a simple example of how communication can sometimes be difficult. Two interlocutors can be in the same room and talk for hours, but if what they say is not understood..."}, {"title": "Syntactic Level", "content": "The syntactic level is one of the most studied in the field of communication. It refers to the technical problem of how to transmit symbols to guarantee a correct reconstruction at the receiver end. This level is based on the work proposed by Shannon in 1948 [90]. At the time, there was a lack of a mathematical framework to understand and optimize the communication process. Telephone networks and radio transmissions were becoming more popular, and engineers were struggling with issues related to signal noise, bandwidth, and the capacity of communication channels. Shannon proposed a mathematical model to describe the communication process, and his contribution completely changed the way communication is approached.\nThe theory proposed by Shannon is still the basis of many modern communication systems. Building on his work, researchers have developed a vast number of communication strategies, such as sophisticated forms of error detection and correction [63], multiple-input multiple-output (MIMO) communications [42], mitigation of multi-user interference [121], etc.\nAt the same time, new network and communication infrastructures are being developed at an incredible pace. Technologies like 4G and 5G are now part of the daily life of billions of people, and 6G is on the horizon [85]. Unfortunately, the rate of improvement of physical devices and communication infrastructures is subject to physical limitations. All the players in telecommunications spend billions of dollars every year to access finite resources like bandwidth. The management of these resources is a complex task, and the optimization of the communication process is a never-ending challenge. For this reason, the syntactic level alone might not be sufficient anymore.\nThe subtle problem is that, from theory, it is known that even with the best possible compression algorithm, there is a limit to the number of bits at which a piece of data can be compressed. This limit depends on its entropy, also referred to as Shannon entropy. It is not possible to perform better than this. This means that if the idea is to reconstruct the exact sequence of symbols, the best that can be achieved in terms of compression is given by its entropy. However, if the idea is to convey the meaning of a piece of data regardless of the form, this can be potentially achieved at values lower than the entropy of the original data. This is the idea behind the semantic level of communication."}, {"title": "Semantic Communication (Semantic Level)", "content": "On the semantic level, the way the message is reconstructed is not relevant as long as the semantic information is preserved. The term semantic information refers to the information that is conveyed by the data and is relevant to the receiver, allowing the receiver to understand the message without reconstructing it symbol by symbol.\nMultiple works have proposed formal theories concerning SemC [6, 12, 92]. This thesis will present SemC a more intuitive and high level way. It is in fact possible to consider any given piece of data x as composed of two parts:\nSyntactic component V: This quantity refers to the subset  V = {vi} of the symbols vi \u2208 V used to represent the data x in its original domain. Here, V denotes the alphabet of the symbols.\nSemantic component S: This quantity refers to the subset S = {si} of the semantic information/meanings si \u2208 S selected from the semantic alphabet S and associated with the data x.\nOnly by having access to both components is it possible to fully describe the data x. In fact, without the semantic component S, the data x is just a sequence of symbols V that can be processed only at a symbolic level. Without the syntactic component V, the data x itself will not exist. However, to fully describe the data x in a semantic way, the semantic component S is the most important part. Once S is defined, it is possible to associate it with multiple symbolic components V.\nTo clarify the concept, it is possible to consider x as an image of an urban environment. The symbolic component V will be composed of the sequence of RGB pixel values vi of the image. The semantic component S will instead be associated with the more abstract semantic information si contained in the image. Some examples might be accessing if there is a pedestrian crossing the street, if a car is driving too close, or if the right window on the fifth floor is open or not.\nFrom S, it is possible to derive multiple symbolic components. For example if S represents only the semantic meaning \"the traffic light is red\", there are countless possible configurations V of the RGB values vi that represent an image with the same semantic meaning.\nIn fact, in the context of SemC, only the semantic information matters. Two pieces of data x = (V, S) and x = (U,T) are defined as semantically equivalent, represented with x \u2194 x, if and only if S = T. This means that as long as the semantic components are the same, the data can be considered the same on a semantic level. No assumption is made about the syntactic components V and U, which might be completely different from one another.\nThis idea is shown in Fig. 2.1, where the data x = (V, S) is compressed and transmitted to the receiver. The syntactic component V is transformed via a Semantic-Based Source Encoder to\n..."}, {"title": "Semantic Communication Based on Generative Models", "content": "Generative models have become increasingly popular in the field of machine learning due to their exceptional ability to model complex data distributions and generate new data samples that retain the essence of the original content. These models have achieved remarkable results in various tasks such as image Super-Resolution (SR) [54], denoising [111], image-to-audio translation [126], 3D synthesis [118] and more.\nThe fundamental principle of generative modeling involves designing a model that can learn the underlying distribution of the data x. This distribution is then used to produce representative samples similar to the data contained in the training dataset. In learning the distribution of the data, the model captures the semantic information si \u2208 S contained in it. For instance, when a generative model is trained to generate human faces, it starts by learning how to identify simple patterns (features), like horizontal and vertical lines, rounded objects, or abrupt changes in colors. Progressively, as the model processes the input data through its hidden layers, more complex semantic features will be identified and considered. Instead of simple patterns, the model will focus on structures like eyes, nose, mouth, etc. and progressively incorporate more complex features [45, 123].\nThis way, the model is able to obtain a hidden representation z of the data that contains all the semantic information si about faces. By using these hidden representations, the model can generate new samples that are coherent with the original data.\nOne important advantage of employing generative models is their flexibility. These models are able to reconstruct a plausible x in an incremental way. If the received semantic information increases, then the model will be able to reconstruct data that are more semantically close to the original one, with higher semantic similarity. By semantic similarity is intended any metric that is able to capture the differences between the semantic content of data. This will be discussed in detail in Section 3.6 where various semantic metrics will be introduced.\nThis concept of flexibility provided by generative models is depicted in Fig. 2.2.\nSuppose that the original data x is the picture of the dog in the top-right corner [55]. Associated with this picture is the semantic meaning S produced with the LLaVA-v1.5-13B model [57] and reported under x. In a real scenario the transmission might not always be possible. In fact, the channel might be noisy, the bandwidth might be limited or the transmission might be too expensive. In these cases, it is important to have a communication system that is able to adapt to the channel conditions. To this end, suppose that the syntactic level is based on a successive refinement approach [106]. In this way, the receiver will be able to decode more semantic symbols zi as the channel conditions improve. If the channel performances are very poor, it might be possible to transmit only one semantic symbol. This z1=\"a dog\" can be inserted as input in the SDXL generative model [77] to produce the bottom-left image in Fig. 2.2. This will already be enough to generate a realistic image of a dog, but the semantic similarity with the original one might be very low."}, {"title": "Goal-Oriented Communication (Effectiveness Level)", "content": "The last level of communication proposed by Weaver is the effectiveness level. This level is related to the effect of the received message on the receiver end. In other words, it is related to the ability of the message to induce the desired behavior in the receiver. In this context, the idea is to transmit not all the information, but only those that are strictly relevant to the fulfillment of a certain goal. For this reason, the communication is also referred to GOC.\nThis level of communication can be considered as a higher level that can directly orchestrate the behavior of lower (semantic and syntactic) levels and how the communication infrastructure process the data and control the different connected components [1, 124].\nThe scheme of the GOC is depicted in Fig. 2.3.\nConsider a scenario where the transmitter can observe some data x, and the communication is happening to perform a certain task at the receiver end. The nature of this task can range from classification to parameter estimation. It can be constrained by power consumption or bandwidth"}, {"title": "Goal-Oriented Communication Based on Information Bottleneck", "content": "In GOC, the idea is to identify the transformation z = \u03a6(x) so that z contains the same level of relevant information that x has on y, and at the same time, z is maximally compressed.\nWhen a transformation z satisfies these two conditions, it is said to be a minimal sufficient statistic of x with respect to y [17]. This concept can be expressed in terms of the mutual information between the terms as:\nI(x;z) = min  I(x;w).\nw:I(w;y)=I(x;y)   (2.1)\nMinimizing the mutual information I(x; z) ensures that the transformation z is as compressed as possible. Simultaneously, the constraint I(z; y) = I(x;y) guarantees that z preserves all the information that x originally had regarding y. The advantage of transmitting this transformation z is that the receiver will be able to perform the task as well as if x was transmitted but with a potentially high advantage in terms of transmitted bits.\nUnfortunately, the process of identifying the minimal sufficient statistic is not an easy task. For this reason in [101] was proposed, and further extended in [91], the use of the IB method [107] to perform GOC. The IB is used to loosen the constraints described in Eq. (2.1) and is represented"}, {"title": "Semantic-Goal-Oriented Communication", "content": "After discussing the three levels of communication, it is interesting to consider merging them all. As already discussed in the previous section and depicted in Fig. 1.1, the effectiveness level can work in synergy with the other levels. In this section, the structure of the so-called SemGOC will be discussed, as illustrated in Fig. 2.5.\nThis type of communication is based on the premise that the semantic information S is the most critical component of the data x, and its preservation, along with power optimization, is the ultimate goal. Recalling Fig. 2.2, even after the semantic level has successfully produced the semantic symbols zi, the channel conditions may not be adequate to transmit all these symbols.\nIn such cases, GOC interacts with the semantic and syntactic levels, as well as the network transmitting the semantic symbols, to orchestrate the communication process. This orchestration involves selecting an appropriate subset of semantic symbols zi that are sufficient for the specific task and current network conditions.\nBy adaptively choosing which semantic symbols to transmit, the system ensures that the most relevant information is conveyed. This approach maintains the effectiveness of the communication by focusing on transmitting the semantic content that is most crucial for the receiver's task.\nThe practical implementation of this concept will be presented in Section 6.4, where the model introduced in Chapter 5 will be integrated into a GOC framework."}, {"title": "Foundations of Generative Models and Evaluation metrics", "content": "The content of this chapter is based almost entirely on the current state of the art. The contribution of the author of this thesis is the introduction of the Traffic signs classification accuracy metric. This is presented at the end of the chapter and proposed in:\nC-SPIC: Class-Specific Semantic-Preserving Image Coding with Residual Enhancement for Accurate Object Recovery\nFrancesco Pezone, Osman Musa, Giuseppe Caire, Sergio Barbarossa"}, {"title": "Introduction", "content": "Generative models represent a powerful class of ML algorithms. Their capability to learn the underlying data distribution and to reconstruct or generate new data samples has made them some of the most utilized tools in ML.\nSince the introduction of early statistical methods like Restricted Boltzmann Machine and Gaussian Mixture Model [86], the field of generative models has undergone significant transformations. As task complexity increased, these initial approaches became inadequate for capturing the complexity of modern data distributions. While early models were effective in generating convincing images on simple datasets such as MNIST [52], which contains handwritten digits and the NORB dataset [53], featuring small black-and-white toy images, more complex datasets like CIFAR10 and CIFAR100 [50] or ImageNet [84] highlighted their limitations.\nThe first important breakthrough in the field of ML came in 2012 with the introduction of AlexNet [51]. This architecture completely revolutionized the field by showing the incredible power of Deep Neural Networks (DNNs), and more specifically of deep Convolutional Neural Network (CNN), in capturing complex details in the data distribution. AlexNet also popularized GPU acceleration for model training, the ReLU activation function [69] and dropout for regularization [33], fundamentally changing the approach to ML.\nShortly afterwards, architectures like the Autoencoder (AE) [83] and the U-Net [82] were developed. While AlexNet was designed for classification tasks, these new architectures were proposed to work towards domains more closely related to generative models. The AE was designed to capture"}, {"title": "The Building Blocks", "content": "a simplified latent representation of images and reconstruct them, while the U-Net was specifically designed for Semantic Segmentation (SSeg) tasks.\nBuilding upon these architectures, a wide variety of new models have been developed in recent years. The AE inspired the development of VAEs [47, 48], incorporating probabilistic frameworks into the AE structure. Then the introduction of GANs [25] introduced a new training paradigm based on two adversarial networks being optimized in a game theory scenario. Further advancements led to vector-quantized versions, such as the VQ-VAE [109] and the VQ-GAN [22]. More recently, the family of DDPM [34] has gained significant attention for their innovative multistep approach in generating new data samples. These models have dramatically improved data generation quality, enabling the generation of highly detailed images from simple text descriptions.\nThis chapter explores these models in parallel to some of the most important building blocks commonly used across many different architectures. The AE, U-Net, ResBlock, VAE and the Attention Mechanism are fundamental components in a majority of the architectures discussed in this thesis. Understanding them is crucial before moving on to more complex architectures.\nIn Section 3.3 the GAN architecture is discussed and in Section 3.4 the vector-quantized versions, such as the VQ-VAE and VQ-GAN, are introduced highlighting the advantages of vector quantization. Finally, the DDPM models are analyzed, covering the techniques and concepts most relevant to this work.\nIn addition to exploring these generative models, this chapter also introduces the evaluation metrics and loss functions employed throughout this work. Selecting appropriate metrics and losses is crucial for designing, training, and assessing the performance of the models. The last section provides a comprehensive overview of both classic evaluation metrics, commonly used in image compression and reconstruction tasks, and semantic-relevant metrics, which evaluate the preservation of semantic information in the data. These metrics are particularly important in the context of SemC frameworks."}, {"title": "Autoencoder", "content": "Proposed in [83], the Autoencoder is a type of neural network designed to reconstruct its input by learning an efficient representation of the data in an unsupervised way.\nAEs can vary greatly in complexity, ranging from AE based on convolution operations to Fully Connected (FC) layers. The one constant is the underlying structure composed of two main parts:"}, {"title": "U-Net Architecture", "content": "The increasing complexity of both tasks and NN depth presented various challenges to the classical AE. SSeg tasks, for instance, highlighted the limitations of the bottleneck structure of the AE, which often caused the loss of fine detail. In fact, the down-scaled latent space was insufficient for guaranteeing a good reconstruction of an output data with the same resolution as the input, since it did not effectively preserve spatial information. Another challenge was related to adapting the training process to the increased depth of NNs. Since these networks are usually trained through backpropagation, the deeper the network, the longer the gradient has to \"travel\" backward. This results in the so-called problem of vanishing gradient [74], where the gradient becomes too small for effective training.\nTo overcome these problems, the U-Net was introduced in [82]. Its structure is based on the AE architecture, with the same encoder-decoder mirrored structure, and with some additional layers in"}, {"title": "The Building Blocks", "content": "between the encoder and decoder. These additional layers are called bottleneck layers and are used to further process the latent representation before it is handled by the decoder. However, the most important innovations in the U-Net architecture consist in the extensive use of ResBlocks and the introduction of the skip connections.\nResBlock: Introduced by He et al. in [29], not specifically for U-Nets, this block was de-signed to help overcome the problem of the vanishing gradient. Before its introduction, the main approach was to use a long series of alternating convolutional layers, some nonlinear activation functions, and normalization, with a noticeable example being AlexNet [51]. He et al. proposed a new way of designing a network. Instead of a long uninterrupted sequence of convolution, activation, and normalization layers, the idea was to create a block, the ResBlock, with a particular characteristic. Every block is composed of a series of convolution, activation, and normalization layers that are repeated a given number of times, generally twice. The main difference lies in the introduction of the residual connection. Every time a hidden layer hi encounters a ResBlock, it is processed by the layers of the ResBlock and at the end, the input is summed with the output, as illustrated in Fig. 3.1.\nThe role of the residual connection is to reduce the risk of degradation by providing shortcuts for the propagation of critical information. At the same time, during backpropagation their role is to split the gradient: one part is directed inside the block, allowing the parameters to be updated, while the other is copied at the other end, mitigating the vanishing gradient problem.\nSkip Connections: The innovation introduced by the U-Net is the concept of skip con-nections. In fact, while the residual connection is adopted locally between a few layers, the skip connection works on a completely different scale. As illustrated in Fig. 3.2, the skip connections link corresponding layers between the encoder and decoder. This has a double"}, {"title": "The Building Blocks", "content": "advantage: (i) during backpropagation, they allow the gradient to flow from the decoder to the encoder unchanged, avoiding the vanishing gradient, and (ii) during the forward process, they help preserve details.\nWith skip connections, the model can reuse features extracted in the encoder at different resolutions even at the decoder. This enhances the ability of the U-Net to generate detailed and accurate outputs. The decoder can access features processed both through the bottleneck but also features that are at the same level of detail. This is particularly important in tasks where preserving spatial information is critical, such as SSeg and SR.\nThe main factor distinguishing the U-Net from the AE is the presence or absence of skip con-nections.\nEven though skip connections encourage the preservation of detail, this does not automatically mean that the U-Net is always better than the AE. One of the advantages of using a U-Net is its ability to preserve fine details, while one of its drawbacks is its shortcomings in data compression. In fact, while an AE only requires the latent representation z to reconstruct the output, this would not be sufficient for a U-Net, as all the intermediate skip connections would also have to be used. The choice of an AE over a U-Net is therefore strongly dependent on the task.\nWhile skip connections are specific to U-Nets, ResBlocks are a tool widely used in ML. They are not specifically designed for U-Nets; an AE can be designed to include ResBlocks as well. The advantages of ResBlocks go beyond countering the vanishing gradient. They also offer a practical and simpler way to design a model. In fact, a model based on ResBlocks can be easily modified to achieve better control over the output.\nThe scheme illustrated before in Fig. 3.1 represents only the base structure of a ResBlock. However, by modifying its structure it is possible to condition its behavior on external factors. For example, in some cases it might be useful to introduce some form of time dependency. This can be easily achieved by modifying the ResBlock architecture, as depicted in Fig. 3.3, where the time conditioning is introduced by a linear application. The first step involves mapping the time t into a high-dimensional space that can be easily interpreted by the NN. This is done via the so-called PE, a technique developed for the Transformer architecture [110]. The idea is to map the value t into a fixed or learnable higher-dimensional space, the PE(t) vector, that can then be further transformed, for example via a FC layer. This final transformation is now inserted in the middle of the ResBlock and this conditioning will allow the model to learn the effects of time and reflect them in the result."}, {"title": "The Building Blocks", "content": "where f and g represent the CNNs responsible for transforming the SSM s. The new \u03b3 = f (s) and \u03b2 = g(s), while being used to normalize are also able to enforce the desired structure of the SSM on the output. The structure of the new SSM conditioned ResBlock is shown on the left side of Fig. 3.4.\nAn important consideration has to be made on the effects of these conditioning. In fact, it is fundamental to introduce these modified ResBlocks only in those parts of the U-Net where their impact is maximized. To condition the feature extraction is good practice to condition the encoder blocks of the U-Net. On the other hand, if the final purpose is to influence the output result the best option is to condition in the bottleneck and the decoder. For example, when the SSM needs to be enforced on the output the modified ResBlock in Fig. 3.4 should be placed in the bottleneck and decoder. Instead, in some other cases the conditioning variable is relevant throughout the whole process and all the ResBlocks would be influenced. This is the case with the time conditioning."}, {"title": "Variational Autoencoders", "content": "A VAE [47, 48] extends the concept of classical AEs by introducing a probabilistic framework to the encoding and decoding processes, Fig. 3.5. Unlike the AEs, that uses a deterministic approach, the VAEs consider the encoding process as probabilistic. The data x is associated to a probability distribution q(x) that allow better generating capabilities. However, the direct use of q(x) to generate data is unfeasible because of its complexity in being directly estimated. To overcome this issue the VAE is designed to work with two simpler probability distributions: q(x|z) and"}, {"title": "The Building Blocks", "content": "q(zx). They represent the core processes of encoding the data x to a latent representation z and then retrieving the original data back. Unfortunately, even if simpler than the q(x), these two distributions are still very complex and impossible to know exactly. This problem can be tackled by approximating them with the help of DNNs.\nInstead of dealing with q(z|x) and q(x|z), it is possible to consider the two approximating dis-tributions q\u03c6(z|x) and p\u03b8(x|z) parameterized by two DNN with parameters \u03c6 and \u03b8, respectively. Moreover, to further reduce the complexity, these approximations are usually forced to be Gaussian distributions. This choice drastically reduces the complexity by shifting the problem from estimating"}]}