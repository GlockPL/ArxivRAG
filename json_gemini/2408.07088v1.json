{"title": "Learning Rule-Induced Subgraph Representations for\nInductive Relation Prediction", "authors": ["Tianyu Liu", "Qitan Lv", "Jie Wang", "Shuling Yang", "Hanzhu Chen"], "abstract": "Inductive relation prediction (IRP)\u2014where entities can be different during training\nand inference-has shown great power for completing evolving knowledge graphs.\nExisting works mainly focus on using graph neural networks (GNNs) to learn the\nrepresentation of the subgraph induced from the target link, which can be seen\nas an implicit rule-mining process to measure the plausibility of the target link.\nHowever, these methods cannot differentiate the target link and other links during\nmessage passing, hence the final subgraph representation will contain irrelevant\nrule information to the target link, which reduces the reasoning performance and\nseverely hinders the applications for real-world scenarios. To tackle this problem,\nwe propose a novel single-source edge-wise GNN model to learn the Rule-induced\nSubgraph represenTations (REST), which encodes relevant rules and eliminates\nirrelevant rules within the subgraph. Specifically, we propose a single-source\ninitialization approach to initialize edge features only for the target link, which\nguarantees the relevance of mined rules and target link. Then we propose several\nRNN-based functions for edge-wise message passing to model the sequential\nproperty of mined rules. REST is a simple and effective approach with theoretical\nsupport to learn the rule-induced subgraph representation. Moreover, REST does\nnot need node labeling, which significantly accelerates the subgraph preprocessing\ntime by up to 11.66\u00d7. Experiments on inductive relation prediction benchmarks\ndemonstrate the effectiveness of our REST2.", "sections": [{"title": "1 Introduction", "content": "Knowledge graphs are a collection of factual triples about human knowledge. In recent years, knowl-\nedge graphs have been successfully applied in various fileds, such as natural language processing [1],\nquestion answering [2] and recommendation systems [3].\nHowever, due to issues such as privacy concerns or data collection costs, many real-world knowledge\ngraphs are far from completion. Moreover, knowledge graphs are continuously evolving with new\nentities or triples emerging. This dynamic change causes even the large-scale knowledge graphs,\ne.g., Freebase [4], Wikidata [5] and YAGO3 [6], to still suffer from incompleteness. Most existing\nknowledge graph completion models, e.g., RotatE [7], R-GCN [8], suffer from handling emerging\nnew entities as they require test entities to be observed in training time. Therefore, inductive relation\nprediction, which aims at predicting missing links in evolving knowledge graphs, has attracted\nextensive attention[9, 10]."}, {"title": "2 Related Work", "content": "Existing works for IRP can be mainly categorized into rule-based methods and subgraph-based\nmethods. While rule-based methods explicitly learn logical rules in knowledge graphs, subgraph-\nbased methods implicitly mine logical rules by learning the representation of subgraphs. Moreover,\nwe discuss some graph neural network methods that reason over the whole graph.\nRule-based methods. Rule-based approaches mine logical rules that are independent of entities and\ndescribe co-occurrence patterns of relations to predict missing factual triples. Such a rule consists of\na head and a body, where a head is a single atom, i.e., a fact in the form of Relation(head entity, tail\nentity), and a body is a set of atoms. Given a head $R(y, x)$ and a body ${B_1, B_2,\\cdots, B_n}$, there is a\nrule $R(y, x) \\leftarrow B_1 \\land B_2 \\land\\cdots\\land B_n$. The rule-based methods have been studied for a long time in\nInductive Logic Programming [20], yet traditional approaches face the challenges of optimization and\nscalability. Recently, Neural LP [14] presents an end-to-end differentiable framework that enables\nmodern gradient-based optimization techniques to learn the structure and parameters of logical rules.\nDRUM[21] analyzes Neural LP from the perspective of low-rank tensor approximation and uses\nbidirectional RNNs to mine more accurate rules. Moreover, for automatically learning rules from\nlarge knowledge graphs, RLvLR [22] proposes an effective rule searching and pruning strategy,\nwhich shows promising results on both scalability and accuracy for link prediction. However, these\nexplicit rule-based methods lack expressive power due to rule-based nature, and cannot scale to large\nknowledge graphs as well."}, {"title": "3 Problem Definition", "content": "We define a training graph as $G_{tr} = (E_{tr}, R_{tr}, T_{tr})$, where $E_{tr}$, $R_{tr}$, and $T_{tr} \\subset E_{tr} \\times R_{tr} \\times E_{tr}$ are\nthe set of entities, relations and triples during training, respectively. We aim to train a model such\nthat for any graph $G' = (E',R', T')$ whose relations are all seen during training (i.e., $R' \\subseteq R_{tr}$),\nthe model can predict missing triples in G', i.e., (?, rt, v), (u, ?, v), (u, rt, ?), where u, v \u2208 E' and\nrt \u2208 R'. We denote the set of any possible entities as ${E}$ and the set of knowledge graphs whose\nrelation set R are subset of $R_{tr}$ as ${G}_{tr}$. The model we would like to train is a score function $f$ :\n${G}_{tr} \\times {E} \\times R_{tr} \\times {E} \\rightarrow R, (G', u, rt, v) \\rightarrow f(G', u, rt, v)$, where G' = (E',R',T'), R' \u2286 Rtr\nand u, v \u2208 E'. For a query triple (u, rt, ?), we enumerate valid candidate tail entities v' and use the\nmodel to get the score s' of this triple (u, rt, v'). We call the query triple (u, rt, v) as target link and\nu, v as target nodes, respectively."}, {"title": "4 Methodology", "content": "In this section, we describe the architecture of the proposed REST in detail. Following existing\nsubgraph-based methods, we first extract a subgraph for each query triple. Then we apply single-\nsource initialization and edge-wise message passing to update edge features iteratively. Finally, the\nrepresentation of the target link is used for scoring. REST organizes the two methods in a unified\nframework to perform inductive relation prediction. Figure 2 gives an overview of REST."}, {"title": "4.1 Subgraph Extraction", "content": "For a query triple (u, rt, v), the subgraph around it contains the logical rules to infer this query, thus\nwe extract a local subgraph $SG_{u,rt,v}$ to implicitly learn logical rules for reasoning. Specifically, we\nfirst compute the k-hop neighbors $N_k(u)$ and $N_k (v)$ of the target nodes u and v. Then we define\nenclosing subgraph as the graph induced by $N_k (u) \\cap N_k (v)$ and unclosing subgraph as the graph\ninduced by $N_k (u) \\cup N_k (v)$. Note that the subgraph extraction process of our REST omits the node\nlabeling, as node features are unnecessary in edge-wise message passing, which significantly reduces\nthe time cost of subgraph preprocessing."}, {"title": "4.2 Single-source Initialization", "content": "Single-source initialization is a simple and effective initialization approach, which initializes a nonzero\nembedding to the query triple according to rt and zero embeddings for other triples. Specifically, the\nembeddings of links and nodes within $SG_{u,r,v}$ are initialized as follows:\n$e^0_{x,y,z} = {\\begin{cases}\n      r_{rt}, & \\text{if } (x, y, z) = (u, r_t, v) \\\\\n      0, & \\text{if } (x, y, z) \\neq (u, r_t, v)\n    \\end{cases}}$\n(1)\n$h^0_{v} = 0 \\text{  for } \\text{Vv} \\in E,$\nwhere $e^0_{x,y,z}$ and $h^0_{v}$ are the initial representation of edge (x, y, z) and node v, respectively. 1 is the\nindicator function to differentiate the target link and other links. is Hadamard product. Note that\nthe representation of nodes is used as temporary variables in edge-wise message passing. With this\ninitialization approach, we ensure the relevance between mined rules and the target link."}, {"title": "4.3 Edge-wise Message Passing", "content": "After initializing all the edges and nodes, we perform edge-wise message passing to encode all\nrelevant rules into the final subgraph representation. Specifically, each iteration of edge-wise message\npassing consists of three parts, (1) applying message function to every link, (2) updating node features\nby aggregating message and (3) updating edge features by temporary node features, which are\ndescribed as follows:\n$m^k_{x,y,z} = MESSAGE(h^{k-1}_x, e^{k-1}_{x,y,z}, r_y) = (h^{k-1}_x \\bigoplus r_y) \\bigodot (e^{k-1}_{x,y,z} \\bigodot r_y)$\n(2)\n$h^k_z = AGGRAGATE(\\left\\{m^k_{x,y,z}\\right\\}) = \\bigoplus_{(x,y,z) \\in T} m^k_{x,y,z}$\n(3)\n$e^k_{x,y,z} = UPDATE(h^k_z, e^{k-1}_{x,y,z}) = h^k_z \\bigodot e^{k-1}_{x,y,z}$\n(4)\nHere,$\\bigoplus$,$\\bigodot$,$\\omega$, $1$, $2$ are binary operators which denote a function to parameterize. $\\oslash$ denotes\nthe large size operator of $\\bigoplus$. $h^k_z$ and $e^k_{x,y,z}$ respectively represent the feature of node z and link\n(x, y, z) after k iterations of edge-wise message passing. We visualize the comparison between\nconventional message passing framework developed by GraIL[11] and proposed edge-wise message\npassing framework in Figure 3."}, {"title": "4.4 RNN-based Functions", "content": "Message passing functions in existing works use order-independent binary operators such as ADD\nand MUL, which cannot model the sequential property of rules and lead to incorrect rules[21]. To\ntackle this problem, we introduce several RNN-based methods as message passing functions.\nMessage Functions. For the edge-wise message passing process, each iteration REST takes in\n$h^{k-1}_x$, $e^{k-1}_{x,y,z}$ and $r_y$ to form a message. We modify GRU[29] as message function as follows:\n$\\begin{aligned}\n&\\delta_k = \\sigma_g (W_1[r_y \\bigoplus e^{k-1}_{x,y,z}] + W_2h^{k-1}_x + b_g) \\\\\n&\\Upsilon_k = \\sigma_r (W_1[r_y \\bigoplus e^{k-1}_{x,y,z}] + W_2h^{k-1}_x + b_{\\Upsilon}) \\\\\n&C_k = \\sigma_h(W_1[r_y \\bigoplus e^{k-1}_{x,y,z}] + W_2(\\Upsilon_kh^{k-1}_x)) \\\\\n&m^k_{x,y,z} = \\delta_k \\bigodot C_k + (1-\\delta_k)h^{k-1}_x\n\\end{aligned}$\n(5)\nHere, $\\delta_k$ is the update gate vector, $\\Upsilon_k$ is the reset gate vector and $C_k$ is the candidate activation vector.\nThe operator $\\bigodot$ denotes the Hadamard product, $\\sigma_g$ denotes Sigmoid activation function and $\\sigma_h$\ndenotes Tanh activation function. During each iteration of message passing, we only use GRU once,\ntherefore k-layer message passing includes k GRUs, which can model sequence with length l \u2264 k.\nAggregate Functions. The aggregate function aggregates messages for each node from its neighbor-\ning edges. Here, we use simplified PNA[30] to consider different types of aggregation.\n$\\begin{aligned}\n&h^k_{z,1} = mean_{(x,y,z) \\in T} (m^k_{x,y,z}), h^k_{z,2} = max_{(x,y,z) \\in T} (m^k_{x,y,z}), h^k_{z,3} = std_{(x,y,z) \\in T} (m^k_{x,y,z}), h^k_{z,4} = min_{(x,y,z) \\in T} (m^k_{x,y,z}) \\\\\n&h^k_{aggz} = W^k_{agg} [h^k_{z,1}; h^k_{z,2}; h^k_{z,3}; h^{k-1}_{z}] \\end{aligned}$\n(6)\nHere, [;] denotes the concatenation of vectors, $W^k_{agg}$ denotes the linear transformation matrix in the\nk-th layer.\nUpdate Functions. The update function is used to update the edge feature. We propose to update\nthe edge feature with LSTM[31]. Specifically, LSTM needs three inputs: a hidden vector, a current"}, {"title": "5 Analysis", "content": "In this section, we theoretically analyze the effectiveness of our REST. We first define the rule-induced\nsubgraph representation, which utilizes encoded relevant rules to infer the plausibility of the target\nlink. Then we show that our REST is able to learn such a rule-induced subgraph representation for\nreasoning."}, {"title": "5.1 Rule-induced Subgraph Representation Formulation", "content": "Our rule-induced subgraph representation aims to encode all relevant rules into the subgraph repre-\nsentation for reasoning. Therefore, we can define the rule-induced subgraph representation as the\naggregation of these relevant rules:\n$\\mathbb{S}_{u,rt,v} = \\bigoplus_{c \\in C} p_c,$\n(10)\nwhere C denotes the set of all possible relevant rules within $SG_{u,rt,v}$ and $p_c$ is the representation of\na relevant rule c. Following the idea of Neural LP[14] to associate each relation in the rule with a\nweight, we model the representation of a rule as a function of its relation set. Therefore, we give the\ndefinition of rule-induced subgraph representation."}, {"title": "Definition 1 (Rule-induced subgraph representation.)", "content": "Given a subgraph $SG_{u,rt,v}$, its rule-\ninduced subgraph representation is defined as follows:\n$\\mathbb{S}_{u,rt,v} = \\bigoplus_{i=1}^{k} \\bigoplus_{\\left\\{ (u, r_t, v), (v, r_0, x_0), ..., (x_{i-3}, r_{i-2}, u) \\right\\}} A_{i1}r_{r_t} \\bigodot A_{i2}r_{r_0} ... \\bigodot A_{iir_{y_{i-2}}}$\n(11)\nwhere i denotes the length of the cycle, $r_{y_i}$ is the representation of relation $y_i$, $(x_i, y_i, x_{i\u22121})$ is an\nexisting triple in $SG_{u,rt,v}$. $\\left\\{ (u, r_t, v), (v, r_0, x_0), ..., (x_{i-3}, r_{i-2}, u) \\right\\}$ is a cycle at length i.\nNote that $\\bigodot$ and $\\bigoplus$ denote binary aggregation functions. Intuitively, rule-induced subgraph represen-\ntation captures all relevant rules within the subgraph and is expressive enough for reasoning."}, {"title": "5.2 Rule-induced Subgraph Representation Learning", "content": "Here, we show that our REST can learn such a rule-induced subgraph representation. First, we show\nthis in a simple case."}, {"title": "Theorem 1", "content": "Single-source edge-wise GNN can learn rule-induced subgraph representation if $\\bigoplus = +, \\\\bigoplus = +, \\bigodot = +, \\\\1 = \\times, \\\\2 = \\times$. i.e., there exists nonzero $a_{i,j}$ such that\n$e^k_{u,rt,v} = \\sum_{i=1}^{k} \\sum_{\\left\\{ (u, r_t, v), (v, y_0, x_0), ..., (x_{i-3}, y_{i-2}, u) \\right\\}} A_{i1}r_{r_t} \\times A_{i2}r_{y_0} \\times ... \\times A_{iir_{y_{i-2}}}$\n(12)"}, {"title": "Theorem 2", "content": "Single-source edge-wise GNN can learn rule-induced subgraph representation if $\\bigoplus = \\bigoplus, \\\\bigoplus = \\bigoplus, \\bigodot = \\bigoplus, \\\\1 = \\bigodot, \\\\2 = 0$, where $\\bigodot$ and $\\bigotimes$ are binary operators that satisfy 0 $\\bigoplus$ a =\na,0 a = 0. i.e., there exists nonzero $a_{i,j}$ such that\n$e^k_{u,rt,v} = \\bigoplus_{i=1}^{k} \\bigoplus_{\\left\\{ (u, r_t, v), (v, y_0, x_0), ..., (x_{i-3}, y_{i-2}, u) \\right\\}} A_{i1}r_{r_t} \\bigodot A_{i2}r_{y_0} ... \\bigodot A_{iir_{y_{i-2}}}$\n(13)"}, {"title": "6 Experiments", "content": "In this section, we first introduce the experiment setup including datasets and implementation details.\nThen we show the main results of REST on several benchmark datasets. Finally, we conduct ablation\nstudies, case studies and further experiments."}, {"title": "6.1 Experiment Setup", "content": "Datasets and Implementation Details We conduct experiments on three inductive benchmark\ndatasets proposed by GraIL[11], which are dervied from WN18RR[32], FB15K-237[33], and NELL-\n995[34]. For inductive relation prediction, the training set and testing set should have no overlapping\nentities. Details of the datasets are summarized in Appendix B. We use PyTorch[35] and DGL[36] to\nimplement our REST. Implementation Details of REST are summarized in Appendix C."}, {"title": "6.2 Main Results", "content": "We follow GraIL[11] to rank each test triple among 50 other randomly sampled negative triples. We\nreport the Hits@10 metric on the benchmark datasets. Following the standard procedure in prior\nwork [37], we use the filtered setting, which does not take any existing valid triples into account at\nranking. We demonstrate the effectiveness of the proposed REST by comparing its performance with\nboth rule-based methods including Neural LP [14], DRUM [21] and RuleN [38] and subgraph-based\nmethods including GraIL [11], CoMPILE [17], TACT[12], SNRI[24] and ConGLR [25]. We run\neach experiment five times with different random seeds and report the mean results in Table 1.\nFrom the Hits@10 results in Table 1, we make the observation that our model REST significantly\noutperforms existing methods on 12 versions of 3 datasets. Specifically, our REST can outperform\nrule-based baselines, including Neural LP, DRUM and RuleN by a large margin. And compared\nwith existing subgraph-based methods, e.g., GraIL, COMPILE, TACT, SNRI and ConGLR, REST\nhas achieved average improvements of 17.89%, 9.35%, 13.76%; 16.23%, 8.18%, 13.04%; 13.58%,\n8.06%, 8.96%; 10.89%, 4.55%,\"-\" and 5.58%, 5.82%, 5.1% on three datasets respectively. As REST"}, {"title": "6.3 Ablation Study", "content": "We conduct ablation studies to validate the effectiveness of proposed single-source initialization and\nedge-wise message passing. We show the main results of ablation studies in Table 2."}, {"title": "Single-source initialization.", "content": "Single-source initialization is vital for learning rule-induced subgraph\nrepresentation. To demonstrate the effectiveness of single-source initialization, we perform another\nfull initialization method as a comparison, which initializes all edges according to their relations. As\nillustrated in Table 2, we can find that single-source initialization is significant for capturing relevant\nrules for reasoning. Without single-source initialization, the performance of REST will exhibit a\nsignificant decrease, e.g., from 92.61 to 68.26 in NELL-995 v4. This result exhibits the effectiveness\nof single-source initialization."}, {"title": "Edge-wise message passing.", "content": "To demonstrate the necessity of proposed RNN-based functions, we\nconduct ablation studies on various combinations of message functions, including SUM, MUL, and\nGRU, as well as update functions, including LSTM and MLP. These functions are defined as follows:\n$\\begin{aligned}\n&SUM: m^k_{x,y,z} = h^{k-1}_x + e^{k-1}_{x,y,z} + r_y \\\\\n&MUL: m^k_{x,y,z} = h^{k-1}_x \\bigodot e^{k-1}_{x,y,z} \\bigodot r_y \\\\\n&MLP: e^k_{x,y,z} = W_{e,lex}q[h^{k-1}_x; e^{k-1}_{x,y,z}]; q^k_{x,y,z} = W_{q,lex}q[e^k_{x,y,z}; r_y] \\\\\n\\end{aligned}$\n(14)\nIn general, REST benefits from RNN-based functions, as they can capture the sequential properties\nof rules. Using order-independent binary operators, such as ADD and MUL, leads to a decline in\nperformance across all datasets, as they cannot differentiate correct and incorrect rules."}, {"title": "6.4 Further Experiments", "content": "Case Study One appealing feature of our single-source initialization and edge-wise message passing\napproach is the ability to interpret the significance of each relevant cycle. This interpretation provides\ninsight into the contribution of each cycle towards the plausibility of the target link (u, rt, v).We\ngenerate all relevant rule cycles for each relation with a length of no more than 4, and input them\nto the REST model to obtain a score for each relevant rule cycle. We normalize these scores using\nsigmoid function and select the top-3 cycles with the highest scores as visualized in Table 7."}, {"title": "Subgraph Extraction Efficiency", "content": "Different from the subgraph-based methods mentioned above[11,\n12, 17], REST eliminates the need for node labeling within the subgraph, which substantially\nimproves time efficiency. We assess the time consumption involved in extracting both enclosing and\nunenclosing subgraphs for GraIL and REST, and present the running time results in Table 4. All the\nexperiments are conducted on the same CPU with only single process. Our observations indicate a\nsignificant improvement of time efficiency over 11\u00d7 when extracting unenclosing subgraphs from\nthe FB15k-237 dataset and over 6\u00d7 across all inductive datasets. This improvement demonstrates the\nefficiency of our REST on subgraph extraction."}, {"title": "7 Conclusion and Future Work", "content": "Limitations. Our REST shares the same limitation as subgraph-based methods. While subgraph-\nbased methods are theoretically more expressive, they incur high computational costs both in training\nand inference. Alleviating this issue is crucial for scalability.\nConclusion. In this paper, we propose a novel single-source edge-wise graph neural network model\ncalled REST, which effectively mines relevant rules within subgraphs for inductive reasoning. REST\nconsists of single-source initialization and edge-wise message passing, which is simple, effective\nand provable to learn rule-induced subgraph representation. Notably, REST accelerates subgraph"}, {"title": "F Sensitivity Analysis", "content": "We further conduct a sensitivity analysis of the subgraph hop n, where different n represents the\nmaximum number of neighbors extracted by REST within the subgraphs. We conduct experiments\non both the same distribution (transductive setting) and different distribution benchmarks (inductive\nsetting). As Tables 10, 11, 12, and 13 show, REST exhibit great robustness across different n."}, {"title": "GMore fine-grained metrics on the inductive benchmarks.", "content": "As Hits @ 10 scores reach a fairly high level, we provide the experimental results on more difficult\nand comprehensive metrics than Hits@10. Tables 14, 15, and 16 summarize the results of GraIL and\nour REST on MRR, Hits@1, and Hits@5. These representative metrics suggest that there is still a\nlarge room for improvement."}, {"title": "H More Baseline Results and Analysis", "content": "We provide additional baselines and our REST for comparison to obtain a more comprehensive\nexplanation. These baselines include NBFNET [27], RMPI [40], and NODEPIECE [41]. The\ncomparative results are presented in the following Table 17. REST also outperforms RMPI in a large\nmargin and achieves competitive results compared with NodePiece and NBFNet. This implies the\ngreat potential for the subgraph-based methods to achieve superior results than the whole-graph-based\nmethods. And we will focus on this point as our furture work."}]}