{"title": "Mitigating the Impact of Reference Quality on Evaluation of Summarization Systems with Reference-Free Metrics", "authors": ["Th\u00e9o Gigant", "Camille Guinaudeau", "Marc Decombas", "Frederic Dufaux"], "abstract": "Automatic metrics are used as proxies to evaluate abstractive summarization systems when human annotations are too expensive. To be useful, these metrics should be fine-grained, show a high correlation with human annotations, and ideally be independent of reference quality; however, most standard evaluation metrics for summarization are reference-based, and existing reference-free metrics correlate poorly with relevance, especially on summaries of longer documents. In this paper, we introduce a reference-free metric that correlates well with human evaluated relevance, while being very cheap to compute. We show that this metric can also be used alongside reference-based metrics to improve their robustness in low quality reference settings.", "sections": [{"title": "1 Introduction", "content": "Given an input source, an abstractive summarization system should output a summary that is short, relevant, readable and consistent with the source. To reflect this, fine-grained human evaluations are split into different scores (Fabbri et al., 2021), such as fluency, faithfulness (sometimes called factual consistency), coherence and relevance. Fluency measures the linguistic quality of individual sentences, eg if they contain no grammatical errors. Coherence gauges if sentences in a summary are well-organized and well-structured. Faithfulness, or factual consistency, considers factual alignment between a summary and the source. Relevance is the measure of whether a summary contains the main ideas from the source.\nAutomatic summarization metrics are intended to capture one or multiple of these qualities (Zhu and Bhat, 2020; Vasilyev and Bohannon, 2021a), and used as a proxy to evaluate summarization systems when human annotations are too expensive. These metrics can be compared on their different attributes such as the reliance on one or multiple references, the cost of inference (Wu et al., 2024), the dataset-agnosticism (Faysse et al., 2023) and their correlations with human judgment at system-level (Deutsch et al., 2022) or summary-level.\nIn this work, we introduce a new reference-free metric \u00b9 that intends to capture the relevance of machine summaries using n-gram importance weighting. We rate n-grams of the source documents relative to how much semantic meaning they express, as measured by tf-idf (Sparck Jones, 1972), and score summaries according to their weighted lexical overlap with these n-grams.\nWe show that this metric is complementary to other metrics and can be mixed with reference-based metrics to alleviate their sensitivity to noisy and low quality references."}, {"title": "2 Related Work", "content": null}, {"title": "2.1 Extractive summarization using word-importance estimation", "content": "A substantial amount of existing work investigated automatic extractive summarization using word-importance scores, based for instance on word statistics (Luhn, 1958), topic signatures (Lin and Hovy, 2000) or pretrained models (Hong and Nenkova, 2014). Our approach follows a similar line of thought by utilizing a word-importance score to identify and weigh the n-grams that should be included in an abstractive summary with high relevance."}, {"title": "2.2 Reference-based evaluation", "content": "Lexical overlap based metrics such as ROUGE (Lin, 2004), BLEU (Papineni et al., 2002) and chrF (Popovi\u0107, 2015), or pretrained language model based metrics such as BERTScore (Zhang et al., 2019) and BARTScore (Yuan et al., 2021), are the standard way of evaluating abstractive summarization systems. However, these metrics rely on gold"}, {"title": "2.3 LLM-as-a-Judge evaluation", "content": "Large Language Models (LLMs) can perform many tasks effectively, even in few-shot or zero-shot settings. Recently, LLMs have also been used to evaluate natural language generation tasks, in replacement of human evaluation. LLM-as-a-Judge shows useful properties as an evaluation metric, for instance Faysse et al. (2023) illustrated using GPT-4 that it can be highly correlated with human judgement, format and task agnostic and comparable across tasks. Zheng et al. (2023) describe limitations of LLM-as-a-Judge, including position, verbosity and self-enhancement biases as well as poor performance at grading math or reasoning tasks. Other limitations are expressed by Kim et al. (2023) targeting proprietary LLMs such as GPT-4 for their closed source nature, uncontrolled version-ing, and their high costs. Prometheus 2 (Kim et al., 2024) is designed for evaluating language models and shows high correlations with proprietary LLMs and human evaluations. Besides, its open-source nature mitigates some of the aforementioned issues. Liu et al. (2023) suggest that LLMs aligned from human feedback overfit to reference-less human evaluation of summaries, which they observed to be biased towards longer summaries and to suffer from low inter-annotator agreement."}, {"title": "2.4 Reference-free evaluation", "content": "Metrics designed to evaluate summaries without reference are useful when no gold reference are available, or when the property they intend to capture does not need a reference to be conveniently estimated.\nGRUEN (Zhu and Bhat, 2020) aims at estimating the linguistic quality of a given summary by taking into account the grammaticality, non-redundancy, focus, structure and coherence of a summary. ESTIME (Vasilyev and Bohannon, 2021a) is evaluating the inconsistencies between the summary and the source by counting the mismatched embeddings out of the hidden layer of a pretrained language model. Info Diff (Egan et al., 2022) uses a pretrained model to compute the difference of Shannon information content between the source document and the source document given the summary. FEQA (Durmus et al., 2020) and SummaQA (Scialom et al., 2019) both compare how a model answers to questions about the document given the source document or a proposed summary."}, {"title": "2.5 Evaluating Summarization of Long Documents", "content": "Trained metrics usually generalize poorly to out-of-distribution tasks (Koh et al., 2022), and often cannot handle long contexts. In the long document summarization setting, Koh et al. (2022) showed that most automatic metrics correlate poorly with human judged relevance and factual consistency scores. Wu et al. (2024) use an extract-then-evaluate method to reduce the size of the long source document used as a reference for evaluation of factual consistency and relevance with LLM-as-a-Judge. They find that it both lowers the cost of evaluation, and improve the correlation with human judgement."}, {"title": "3 Limits of reference-based evaluation", "content": "Lexical overlap scores such as BLEU or ROUGE work under the implicit assumption that reference summaries are mostly extractive and contain no errors. This assumption is challenged by a study conducted by Maynez et al. (2020) on hallucinated content in abstractive summaries. In human written summaries from the XSum dataset, 76.9% of the gold references were found to have at least one hallucinated word.\nSummarization methods can trade abstractiveness for faithfulness, creating a faithfulness-abstractiveness tradeoff curve that was illustrated and studied by Ladhak et al. (2022). They show that some metrics are more sensitive to the summary abstractiveness than others.\nIn the context of translations, translationese refers to source language artifacts found in both human and machine translations. This phenomenon is similar to extractive segments in summaries, as it is an artifact of the source document that can be mitigated through paraphrasing. Freitag et al. (2020) demonstrated that reference translations in machine translation datasets tend to exhibit this translationese language. They addressed this by creating new references through paraphrasing the existing ones. When tested, systems produced"}, {"title": "4 Proposed Metric", "content": "Let $W_{t,d,D}$ be the importance of a n-gram t in a document d from a corpus D, defined as\n$W_{t,d,D} = \\begin{cases}\n    tanh(\\tilde{W}_{t,d,D}), & \\text{if } t \\in d \\\\\n    0, & \\text{otherwise}\n\\end{cases}$\n$\\tilde{W}_{t,d,D}$ is an importance score obtained through word importance scoring methods (such as tf-idf and bm-25). The associated importance rank of the n-gram in the document is referred as $r_{t,d,D}$.\nGiven a proposed summary \u015d of a document d \u2208 D, we compute the metric:\n$\\mathfrak{m}(\\hat{s}, d, D) = a_{\\hat{s},d,D} \\frac{\\sum_{t \\in \\hat{s}} W_{t,d,D}}{N_{d,D}}$\nWith $N_{d,D}$ the upper ceiling of the sum of weights, used to normalize the score: $N_{d,D} = \\sum_{t \\in d} W_{t,d,D}$.\nBy design this score will be maximized for a summary consisting of the full document. To alleviate this issue, we penalize longer summaries by multiplying with a term accounting for the length of the summary |\u015d| relative to the length of the document |d|: $a_{\\hat{s},d} = f(|\\hat{s}|, |d|)\u00b2$. We observe that this length penalty not only resolves the issue related"}, {"title": "5 Experiments", "content": "For our experiments, we work with different datasets of human evaluation of summarization systems. SummEval (Fabbri et al., 2021) contains human evaluations for 23 systems, each with 100 summaries of news article from the CNN/DailyMail dataset. Coherence, consistency, fluency and relevance are evaluated by experts and crowd-source workers. ArXiv and GovReport (Koh et al., 2022) contain annotations for 12 summarization systems, evaluated on 18 long documents for each dataset. Human evaluators rated the factual consistency and the relevance of the machine summaries. ROSE (Liu et al., 2023) is a benchmark consisting of 12 summarization systems evaluated on 100 news article from CNN/DailyMail. Each summary is annotated with different protocols, we are using the reference-based and reference-free human evaluations.\nWe describe the choice of settings for our metric in Appendix A, which takes into account system-level correlations on the four datasets, as well as the range of values taken by the metric."}, {"title": "5.1 System-level correlation scaling with number of summaries", "content": "According to Deutsch et al. (2022), system-level correlations are usually inconsistent with the practical use of automatic evaluation metrics. To evaluate systems, usually only the subset of summaries judged by humans is used. However automatic metrics can be computed on summaries outside of this subset to give better estimates. Deutsch et al. (2022) also illustrates that testing with more examples will narrow down the confidence intervals of the evaluated scores, making it more convenient to compare systems. With a reference-free metric like ours, systems can be evaluated on more documents without the need for reference summaries. Figure 1 illustrates the increase of system-level correlation with human evaluated relevance when using more examples for each system."}, {"title": "5.2 Robustness to noisy references", "content": "Reference-based metrics such as ROUGE-1 are sensitive to the quality of the references. To evaluate the robustness of ROUGE-1 to noisy references, we gradually replace random reference summaries with altered references and compute the resulting system-level correlations. The references are altered by replacing them with three random sentences (RAND-3) from the source document. Results with the ArXiv dataset, averaged over 20 random draws, are reported in Figure 2. Results with different alteration methods and different datasets are reported in Figures 6, 7 and 8 in Appendix A. Our metric is not sensitive to altered references by design, contrary to ROUGE-1. When mixed with it, it improves the robustness of ROUGE-1 to low quality references. This aspect is beneficial in settings where the quality of the reference summaries is unknown or variable, for instance with web-crawled datasets."}, {"title": "5.3 Complementarity with other automatic metrics", "content": "We report the pairwise complementarity between each pair of metric\u00b3 on SummEval in Figure 3, following Colombo et al. (2023). We observe that our metric has a high complementarity with most other metrics, noticeably with ROUGE and chrF scores, which are also based on lexical overlap, meaning that they capture different features of the evaluated summaries.\nIn Table 1 we report the system-level Spearman correlations using our metric, other metrics, and simple combinations of metrics. In the LLM-as-a-judge method, we are using the gemini-1.5-flash model (Gemini Team, 2024) following the prompt proposed by Wu et al. (2024) to evaluate the relevance of summaries."}, {"title": "6 Conclusion and future works", "content": "In this work, we introduce a new reference-free metric based on importance-weighted n-gram overlap between the summary and the source. We demonstrated that it has high correlations with human judgement and can be used alongside other metrics to improve them and mitigate their sensitivity to low-quality references.\nThe prospects for future research include further exploration of the behaviour of reference-based, reference-free and hybrid metrics with references of varying quality, as well as potential extensions to multimodal settings such as the evaluation of vision-language systems."}, {"title": "7 Limitations", "content": "Like other lexical overlap metrics, ours works with the assumption that there is a vocabulary overlap between the source document and the summary, ie that the summary has a non-zero coverage. In order to evaluate the sensitivity of our metric to various levels of extractiveness of summaries, we would have wanted to compute the score on systems with varying values on the faithfulness-abstractiveness tradeoff curve presented in Ladhak et al. (2022); but their data was not made available yet.\nVasilyev and Bohannon (2021b) noticed that higher correlation with human scores can be achieved with \"false\" improvements, mimicking human behaviour. Using a referenceless evaluation metric, they limited the comparisons with the source text by selecting sentences to maximize their score, and observed a higher correlation with human judgement as a result. Wu et al. (2024) observe a similar consequence by first extracting sentences that maximize the ROUGE score with the original document and using the resulting extracted sentences along with the predicted summary as the input to be evaluated by a LLM-as-a-judge. Their interpretation however is different as they do not view this higher correlation with human scores as a \"false\" improvement, but as a way to mitigate the Lost-in-the-Middle problem of LLMs.\nWe believe that the relevant interpretation depends on the method that is used to extract sentences from the source document. Using comparisons with the summary to extract \"oracle\" spans of the original document, or selecting key sentences that span over the main information of the document are not motivated by the same reasons. Mimicking the human behaviour of referring only to the bits of the document that are relevant to the proposed summary at first glance to score marginally higher correlations is a different thing than filtering the most important bits of a document relative to a measure of word importance.\nOur metric filters out the n-grams with little semantic significance in the document. This can mimic the human bias of comparing the summary to salient sentences only, but it will also lower the influence of the artifacts of extractiveness discussed in section 3.\nOur metric is also specific to the task of summarization and might correlate differently with human judgement on summarization tasks with different compression ratio, extractiveness, or style. Table 2 in the Appendix A illustrates this.\nLLM-as-a-Judge methods can solve the issues of sensitivity to extractiveness and task settings, while providing more interpretable results, but are not exempt from biases and come with a noticeably higher cost."}, {"title": "A Appendix", "content": null}, {"title": "A.1 Spurious correlations", "content": "Durmus et al. (2022) observed that model-based reference-free evaluation often has higher correlations with spurious correlates such as perplexity, length, coverage or density, than with human scores. We report the correlations between metrics and spurious correlates in Table 2."}, {"title": "A.2 Correlations with human judgement on different settings", "content": "Figure 5 illustrate the distributions of system-level correlations of our metric with different settings.\nFor tokenization, we tested tokenizing texts as separated by space, using character tokenization, a pretrained GPT-2 tokenizer, or a custom tokenizer, trained on each corpus with a vocabulary of 100 tokens.\nWe included different sizes of n-grams in our tests, with bigrams, trigrams and 4-grams.\nThe two methods we considered for importance weigthing are tf-idf and bm-25.\nThe importance score is the weight used to score the overlapped n-grams, we included the following scores:\n\u2022 importance: t, d, D \u2192 $W_{t,d,D}$\n\u2022 exp-rank: t, d, D \u2192 $exp(-r_{t,d,D})$\n\u2022 inv-rank: t, d, D \u2192 $\\frac{1}{r_{t,d,D}}$\n\u2022 constant: t, d, D \u2192 1\n\u2022 tanh: t, d, D \u2192 $tanh(\\tilde{w}_{t,d,D})$\nThe options for the length penalty $a_{\\hat{s},d}$ are no penalty or $a_{\\hat{s},d} = f(|\\hat{s}|, |d|)$, with\n$f: |\\hat{s}|, |d|\\rightarrow \\frac{1}{1 + exp(20 * \\frac{|\\hat{s}|}{|d|} - 10)}$\nf is illustrated in Figure 4.\nWe chose to use the corpus tokenizer, with trigrams, tf-idf and the tanh importance scoring with length penalty. These settings proved to be consistant in the tested conditions, and provided good ranges of values on different inputs. All the other experiments with our metric in this paper are using these settings.\nFigures 6, 7 and 8 show the system-level correlation of our metric, ROUGE-1 and their combination as we gradually replace the reference summaries with respectively three random sentences (RAND-3), the first three (LEAD-3) or last three (TAIL-3) sentences of the source document."}, {"title": "A.3 Range of values", "content": "We report the range of values taken by our metric, and ROUGE-1, for different inputs and on different datasets in Figures 9 and 10."}]}