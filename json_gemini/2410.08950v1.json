{"title": "On the Adversarial Transferability of Generalized \u201cSkip Connections\u201d", "authors": ["Yisen Wang", "Yichuan Mo", "Dongxian Wu", "Mingjie Li", "Xingjun Ma", "Zhouchen Lin"], "abstract": "Skip connection is an essential ingredient for modern deep models to be deeper and more powerful. Despite their huge success in normal scenarios (state-of-the-art classification performance on natural examples), we investigate and identify an interesting property of skip connections under adversarial scenarios, namely, the use of skip connections allows easier generation of highly transferable adversarial examples. Specifically, in ResNet-like models (with skip connections), we find that using more gradients from the skip connections rather than the residual modules according to a decay factor during backpropagation allows one to craft adversarial examples with high transferability. The above method is termed as Skip Gradient Method (SGM). Although starting from ResNet-like models in vision domains, we further extend SGM to more advanced architectures, including Vision Transformers (ViTs) and models with length-varying paths and other domains, i.e. natural language processing. We conduct comprehensive transfer attacks against various models including ResNets, Transformers, Inceptions, Neural Architecture Search, and Large Language Models (LLMs). We show that employing SGM can greatly improve the transferability of crafted attacks in almost all cases. Furthermore, considering the big complexity for practical use, we further demonstrate that SGM can even improve the transferability on ensembles of models or targeted attacks and the stealthiness against current defenses. At last, we provide theoretical explanations and empirical insights on how SGM works. Our findings not only motivate new adversarial research into the architectural characteristics of models but also open up further challenges for secure model architecture design. Our code is available at https://github.com/mo666666/SGM.", "sections": [{"title": "INTRODUCTION", "content": "In deep neural networks (DNNs), skip connection builds one kind of short-cut from a shallow layer to a deep layer by connecting the input of a building block including convolution layers or self-attention layers (also known as the residual module) directly to its output. While different layers of neural networks learn different \"levels\" of features, skip connections can help preserve low-level features and avoid performance degradation when adding more layers. This has been shown to be crucial for building very deep and powerful DNNs such as ResNet [1], WideResNet [2], DenseNet [3] and Vision Transformer [4]. In the meantime, despite their superior performance, DNNs have been found to be extremely vulnerable to adversarial examples (or attacks), which are input examples slightly perturbed by small noise to fool the network into making wrong predictions [5], [6]. Adversarial examples are imperceptible to human observers and transferable across different models [7].\nAdversarial examples can be crafted following either a white-box setting (the adversary has full access to the target model) or a black-box setting (the adversary has no informa- tion of the target model). White-box methods such as Fast Gradient Sign Method (FGSM) [6], Basic Iterative Method"}, {"title": "RELATED WORK", "content": "Existing adversarial attacks can be categorized into two groups: 1) white-box attacks and 2) black-box attacks. In the white-box setting, the adversary has full access to the parameters of the target model, while in the black-box setting, the target model is kept secret from the adversary."}, {"title": "White-box Attacks", "content": "Given a clean example x with class label y and a target DNN model f, the goal of an adversary is to find an adversarial example xadv that fools the network into making an incor- rect prediction (e.g. $f(x_{adv}) \\neq y$), while still remaining in the $\\epsilon$-ball centered at x (e.g. $||x_{adv} - X ||_{\\infty} \\leq \\epsilon$). A wide range of attacking methods have been proposed for the crafting of adversarial examples. Here, we only mention a selection.\nFast Gradient Sign Method (FGSM) [6]. FGSM perturbs clean example x for one step by the amount of $\\epsilon$ along the gradient direction:\n$x_{adv} = x + \\epsilon \\cdot sign(\\nabla_x l(f(x), y)).$  (1)\nProjected Gradient Descent (PGD) [9]. PGD is an it- erative version of FGSM, which perturbs clean example x for T steps with a smaller step size. After each step of perturbation, PGD projects the adversarial example back onto the $\\epsilon$-ball of \u00e6, if it goes beyond the $\\epsilon$-ball:\n$x^{t+1} = \\Pi_{\\epsilon} (x_{adv} + \\alpha \\cdot sign(\\nabla_x l(f(x_{adv}), y))),$  (2)\nwhere $\\Pi(\\cdot)$ is the projection operation.\nThere are also other types of white-box attacks in- cluding sparsity-based methods such as Jacobian-based Saliency Map Attack (JSMA) [22], one-pixel attack [23], and optimization-based methods such as Carlini and Wagner (CW) [10]. Although these methods are effective in the white-box setting, they often suffer from low transferability in the black-box setting [11]."}, {"title": "Black-box Attacks", "content": "Black-box attacks can be generated by transferability-based method that transfers from attacking a surrogate model or query-based method that directly generates adversarial examples on the target model via lots of queries to the system. Query-based method estimates the gradient of the target model via a large number of queries, which is then used to generate adversarial examples such as Finite Differ- ences (FD) [24] or Natural Evolution Strategies (NES) [25]. These methods all require a large number of queries to the target model, which not only reduces the efficiency but also potentially exposes the attack.\nAlternatively, black-box adversarial examples can be crafted on a surrogate model then applied to attack the tar- get model. Although the white-box methods can be directly applied on the surrogate model, they are far less effective in the black-box setting [11], [12]. Several transfer techniques have been proposed to improve the transferability of black- box attacks and they can be mainly classified into four different categories based on their design principles.\nGradient-related Attacks: Previous works show that the decision boundaries vary across different architectures [7] and falling into the local minima will largely impair the transferability to target models. The gradient-related attacks alleviate it by developing advanced updating strategies such as momentum acceleration [11], [26], [27], neighbor- hood correlation [28], [29], [30], [31], norm penalization [14], [32] and adaptive step size adjustment [33], [34].\nAugumentation-related Attacks: Similar to the gener- alization of models, data augmentation can also improve the generalization of attacks by undermining the intrin- sic features. It ensures the invariance of the attack effect when transforming the images with augmentations and thus maintains its threats to diverse architectures. Earliest work develop simple augmentations such as random re- sizing or padding [12], [35], frequency-based noises [36], random masking [37] or rotating the split patches [38]. More advanced methods are further exploited such as di- verse augmentations for each image block [39], automatic augmentation selection to each image [40], or applying a stylized network for the customized enhancement [41].\nFeature-related Attack: Since the aim of the attacks is to induce misclassification, an intuitive approach is to craft adversarial perturbations with the cross-entropy loss. However, previous work [42] reveals that the intermediate features might be a better choice since they share high similarity across models. Motivated by this finding, feature- based attack attempts to craft the adversarial example for higher black-box transferability by increasing its perturba- tion on a pre-specified layer of the model. Earliest work [43] shows that directly maximizing the differences of feature maps will only obtain unsatisfied performances because only the important features contribute to the classification. Therefore, FIA [44] and NAA [45] introduce the backward gradients and the decomposed integral respectively as the soft mask to filter out the unrelated features. Experience from other categories, such as updating the weighted matrix with momentum [46] or averaging it on multiple randomly masked images [47] is illustrated to be successful in further improving the performances of the feature-related attacks."}, {"title": "PROPOSED SKIP Gradient METHOD (SGM)", "content": "In this section, we first introduce the gradient decomposi- tion of skip connection and residual module. Following that, we propose our Skip Gradient Method (SGM) for ResNet- like architectures, then extend it to transformer architectures and other modern neural networks with length-varying paths."}, {"title": "SGM for ResNet-like Architectures", "content": "In ResNet-like neural networks, a skip connection uses identity mapping to bypass residual layers, allowing data to flow from a shallow layer directly to subsequent deep layers. Thus, we can decompose the network into a col- lection of paths of different lengths [54]. We denote a skip connection together with its associated residual module as a building block (residual block) of a network. Considering three successive building blocks (eg. $z_{i+1} = z_i + f_{i+1}(z_i)$) in a residual network from input $z_0$ to output $z_3$, the output $z_3$ can be expanded as:\n$z_3 = z_2 + f_3(z_2) = [z_1 + f_2(z_1)] + f_3(z_1 + f_2(z_1)) \\\\\n= [z_0 + f_1 (z_0) + f_2(z_0 + f_1(z_0))]  + f_3((z_0+f_1(z_0)) + f_2(z_0 + f_1(z_0))).$ (3)\nAccording to the chain rule in calculus, the gradient of a loss function $l$ with respect to input $z_0$ can then be decomposed as,\n$\\frac{dl}{dz_0} = \\frac{dl}{dz_3} \\frac{dz_3}{z_2} \\frac{z_2}{z_1} \\frac{z_1}{dz_0} = \\frac{\\partial l}{\\partial z_3}(1 + \\frac{df_3}{dz_2})(1 + \\frac{df_2}{dz_1})(1+\\frac{df_1}{dz_0}).$  (4)\nExtending this toy example to a network with L residual blocks, the gradient can be decomposed for all residual blocks as,\n$\\frac{dl}{\\partial x} = \\frac{dl}{dz_L} \\prod_{i=0}^{L-1} (1+\\frac{df_{i+1}}{dzi}) \\frac{dz_0}{\\partial x}, $ (5)"}, {"title": "Extending SGM to Transformers", "content": "Recently, Vision Transformers (ViTs) [4] challenge the lead- ing position of convolutional neural networks, and achieve competitive performance in vision tasks based on the self- attention mechanism. Since the basic building block for ViTs is composed of a self-attention layer, an MLP layer, and some skip connections, it is easy to extend SGM to ViTs.\nRecall that the gradient flow through a basic block of ResNet consists of two parts, i.e., one through the residual module $f_i$, and the other through the skip connection. Using SGM, we decay the gradient flow through the residual mod- ule by multiplying $\\gamma$ while keeping the other gradient flow unchanged as illustrated in Figure 2(a). Similarly, for the basic block of ViTs, the input $z_0$ is first processed by a multi- head attention module $A_1$ and an identity function (skip connection) in parallel. Its output $z_1$ is further processed by a multi-layer perception $M_1$ and an identity function in parallel. Therefore, the output of the transformer block consists of 4 terms:\n$z_1 = M_1(z_1) + z_1 = M_1(A_1 (z_0) + z_0) + A_1(z_0) + z_0$ (8)\nAccording to the chain rule, the gradient of the loss function $l$ with respect to the input $z_0$ can be formulated as,\n$\\frac{dl}{dz_0} = \\frac{dl}{dz_1} \\frac{dz_1}{dz_0} = \\frac{dl}{dz_1} (\\frac{\\partial M_1}{\\partial z_1} (\\frac{\\partial A_1}{\\partial z_0} + 1) + (\\frac{\\partial A_1}{\\partial z_0} + 1)). $ (9)"}, {"title": "Extending SGM to Architectures with Length-varying Paths", "content": "The above model architectures all include skip connections whose role can be regarded as providing different lengths of paths from the input to the output. However, unfortunately, not all modern models have skip connections. Therefore, in this part, we attempt to extend the proposed SGM to archi- tectures without skip connections but with length-varying paths.\nTaking Inception-v3 as an example, as shown in Figure 2(c), there are 4 different parallel processing paths in a basic block, e.g., a single convolutional layer $P_{1,1}$, a combination of pooling and a convolutional layer $P_{1,2}$, and two or three successive convolutional layers: $P_{1,3}$ and $P_{1,4}$. For the first layer, if we denote the input with $z_0$, the output of this basic block $z_1$ consists of 4 parts:\n$z_1 = P_{1,1}(z_0) + P_{1,2}(z_0) + P_{1,3}(z_0) + P_{1,4}(z_0),$  (11)\nwhere each part goes through different numbers of convo- lutional layers, that is, every gradient flow goes through a length-varying path\u00b9. According to the chain rule, the gradient of the loss function $l$ with respect to the input can be derived as,\n$\\frac{\\partial l}{\\partial x} = \\frac{dl}{dz_1} \\frac{\\partial P_{1,1}}{dz_0} + \\frac{JP_{1,2}}{dz_0} +  \\frac{\\partial P_{1,3}}{dz_0} +  \\frac{JP_{1,4}}{dz_0} ).$  (12)"}, {"title": "THEORETICAL ANALYSIS OF SGM", "content": "From a data distribution perspective, we know that deep models can properly classify the i.i.d samples with the training data while failing to classify the out-of-distribution (OOD) samples, therefore, moving samples out of their original distribution causes difficulties for different models to classify this kind of OOD sample, which explains the transferability of adversarial examples to a certain degree [55]. The farther away from the original distribution, the higher adversarial transferability. To characterize the above property, [55] proposed a metric measuring the similarity between adversarial attack direction $\\nabla_x l$ and the direction of moving sample away from its original data distribution $P_D(x|y)$ (Intrinsic attack) called AAI (Alignment between the Adversarial attack and Intrinsic attack):\n$AAI {\\nabla_x l} = E_{P_D (y)} E_{P_D (x|y)} (\\frac{<\\nabla_x l, \\nabla log P_D (x|y)>}{\\lVert \\nabla_x l \\rVert_2 \\lVert  \\rVert_2} )= E_{P_D (y)} E_{P_D (x|y)} (\\frac{<\\nabla_x l, v>}{\\lVert \\nabla_x l \\rVert_2 }) + C,$ (14)\nwhere $v$ is the Gaussian random vector $v \\sim N(0, I)$ and $C$ is a constant only related to the data distribution. Although the ground truth data distribution is not known, we can easily estimate its gradient $\\nabla logP_D(x|y)$ via Langevin dynamics [56], and the gradient only considers attacks from the view of data distribution, which is supposed to be a general attack that can transfer well among different models. As demon- strated in [55], larger AAI means the attack direction aligns better with the direction away from the data distribution and the generated adversarial examples thus enjoying better transferability.\nRevisiting our proposed SGM, it modifies the gradients of generating adversarial examples ($\\nabla_x l$ in equation (14)) through tuning a hyper-parameter $\\gamma$ to affect the AAI metric. In the following, we formally analyze why SGM can work through introducing the hyper-parameter $\\gamma$ under the view of data distribution. Proofs are in Appendix B.\nProposition 1. Consider the following binary-classification residual model as follows:\n$\\hat{y} = x + g(x)$"}, {"title": "EXPERIMENTS UNDER VARIOUS DIFFERENT AR- CHITECTURES", "content": "In this section, we demonstrate the catalytic effect of SGM on black-box transferability by combining it with the exist- ing methods on the ImageNet dataset [15] under various architectures.\nBaselines. We compare the transferability of 31 state- of-the-art attacks after combining them with SGM. Due to the space constraints, we only select one attack each as a representative from four categories and one source model for each architecture in the main paper: (1) Gradient-related: Sampling-based Momentum Iterative Fast Gradient Rescal- ing Method (SMI-FRGM) [57], (2) Augmentation-related: Structure Invariant Attack (SIA) [39], (3) Feature-related: Feature-Momentum Adversarial Attack (FMAA) [46], and (4) Parameter-related: Robust Feature Attack (RFA) [58]. For the results of more attacks and architectures, please refer to Appendix C for details. We select Masking Unimportant Parameters (MUP) attack [59] as our baseline. For all attack methods, we follow the standard setting [11], [12] to craft untargeted attacks under maximum $L_\\infty$ perturbation $\\epsilon = 16$ with respect to pixel values in [0, 255]. The step size is set to $\\alpha = 2$ and the iteration step is set to 10. For our proposed SGM, the decay parameter is set to $\\gamma = 0.6$ and we show some examples of generated images in Appendix A. For simplicity, the hyperparameters related to a specific attack are configured as those in their original papers.\nThreat Model. We adopt a black-box threat model in which adversarial examples are generated by attacking a source model and then applied to attack the target model. The target model is of a different architecture (indicated by the model name) to the source model, except when the source and target models are of the same architecture, where we directly use the source model as the target model"}, {"title": "SGM in ResNet-like CNNs", "content": "In Section 3.1, we propose that SGM can be easily performed on the ResNet-like CNNs considering skip connections are their basic building component. Therefore, in this section, we further conduct experiments to demonstrate their effec- tiveness. The perturbations are crafted on ResNet-50 [1], and then applied to attack 8 kinds of CNNs including ResNet-50 (RN50), VGG19 (without batch normalization) [60], ResNet- 152 (RN152) [1], DenseNet-201 (DN201), 154 layer Squeeze- and-Excitation network (SE154) [61], Inception V3 (IncV3) [18], Inception V4 (IncV4) [19] and Inception-ResNetV2 (In- cResV2) [19].\nWe summarize the results in Table 1 and Table 9. In all scenarios, our proposed SGM consistently improves the attack success rates after combining with the existing attacks and outperforms the performances of MUP. For example, the ASR of SMI-FRSM is 47.96% against SE154 model. After combining with SGM, the ASR improves a novel margin (>10%) and reaches 62.09%. However, the gain for MUP is 5.31%. Another interesting observation is that almost for all attacks, the highest ASR is obtained when RN152 is chosen as the target model. We propose that it is attributed to the high architectural similarities shared by the target and source models."}, {"title": "SGM in Vision Transformers", "content": "In Section 3.2, we demonstrate that SGM can be easily extended to ViTs since they also have skip connections.\nHere, we conduct a series of experiments to demonstrate its effectiveness. In addition to architecture-free attacks, we also select PNA [52] and SAPR [53] attacks which are specially designed for ViTs for combination. The black-box transferability with or without SGM are evaluated in two scenarios: 1) the source and target models are both ViTs, and 2) the source models are ViTs and the target models are CNNs. Following [52], we choose ViT-B [4] as the source model, and evaluate the attack success rate on 5 kinds of ViTs, i.e., ViT-B [4], ConViT-B [62], PiT-B [63], TNT-S [64], Visformer-S [65], and 3 kinds of CNNs, i.e., VGG19, RN152, and DN201. Part of the results are shown in Table 2. For more results, please refer to Table 10 in Appendix C. We find that our proposed SGM brings consistent benefits to all settings. For example, on TNT-S, the transferability is boosted from 38.03% to 57.20% after combining PGD with SGM. Similar benefits are also observed for CNNs and more advanced attacks: for example, applying SGM can improve the success attack rates of 8.24% compared with the vanilla SIA attack for ResNet-152.\nIn addition, we consider another variant of ViTs, i.e., MLP-Mixer [17], which only replaces the self-attention mod- ule in ViTs by multi-layer perceptions (MLP). MLP-Mixer also achieves competitive performance under many image benchmarks Similar to SGM in ViTs, SGM can be directly incorporated into MLP-Mixer models. Here, we evaluate the transferability of adversarial examples crafted by Mixer-B on different target models, including ResMLP [66], gMLP [67], ViTs, and CNNs. For the results, please refer to Ap- pendix C in Table 11. We first observe that SGM can not only bring benefits to MLP-MLP transferability but also helps MLP-CNN and MLP-ViT transferability. For example, SGM improves the transferability of SMI-FRSM attack on ResMLP-12 (+9.63%), ViT-B (+4.93%), and VGG19 (+3.05%) by a remarkable margin. When we compare the trans- ferability across different target models, we observe that black-box attacks are more likely to succeed across similar architectures, e.g., under PGD attacks, the attack success rate on MLP models is much higher than other types of models as the source model is the MLP one. Similar to ResNet- like CNNs, SGM can better improve the transferability than MUP."}, {"title": "SGM in Models with Varying-length Paths", "content": "In Section 3.3, we present that SGM can be extended into a broader range of architectures with length-varying paths. In this part, taking Inception [18] and models from Neural Architecture Search (NAS) [68] as examples, we evaluate the effectiveness of SGM in models with varying-length paths.\nInception Models. Inception is a commonly used archi- tecture, for example, Inception-v3 as shown in Figure 2(c). Assuming the source model as Inception-v3, we evaluate its black-box transferability with or without SGM against 4 kinds of CNNs, i.e., VGG19, RN152, DN201, SE154, and 3 kinds of Inceptions, i.e., IncV3, IncV3, IncRes in Table 3. For the results of more attacks, please refer to Table 12 in Appendix C. Similarly, we also find that SGM obtains con- sistent transferability improvements and outperforms MUP. Specifically, although the source model does not have skip connections (IncV3), SGM can still improve the transferabil- ity against target models without skip connections (VGG19 and IncV4) or with skip connections (RN152, DN201, SE154, and IncRes). This indicates that the gain brought by SGM is not limited to models with skip connections.\nModels from Neural Architecture Search. The models mentioned above are often heuristically designed by ex- perts. Meanwhile, there are numerous studies on automated Neural Architecture Search (NAS), which may generate various architectures. Here we demonstrate that SGM is a general technique that is still effective in NAS. Taking one searched model P-DARTS [69] as an example, we handle the gradient decay in a similar way to the implementation of SGM in Inception in Section 3.3. Specifically, we decay the gradient using $\\gamma$ every time going through a ReLU. The source model is assumed as P-DARTS, while the trans- ferability is tested on target models consisting of 4 kinds of CNNs, i.e., VGG19, RN152, DN201, SE154, and 3 kinds of Inceptions, i.e., IncV3, IncV3, IncRes. As summarized in Table 13 in Appendix C, SGM consistently improves transferability using the vanilla PGD. Even with the im- proved approaches e.g. MaskBlock, the transferability is also improved by a large margin. For example, SGM boosts the transferability of MaskBlock attack on VGG19 from 22.43% to 31.94%. Thus we can conclude from the above evidence that SGM can be easily combined with various attacks in models without skip connections."}, {"title": "EXPERIMENTS UNDER COMPLEX SCENARIOS", "content": "In this section, we explore the catalytic effect of SGM under complex scenarios. Attackers could perform attacks on ensembles of models to acquire better transferability or apply targeted attacks to induce misclassification to specific classes. In addition, the victim will not just sit back and"}, {"title": "The Evaluation of SGM on Ensemble Models", "content": "Model Ensemble [70] is a well-known technique for ma- chine learning, combining multiple individual models to improve overall performance. In [7], they first discover that combining the gradients from the ensemble of models will enhance the transferability of black-box attacks, by leveraging more information from different architectures. Not contradicted by its dynamic, we study whether SGM can serve as a catalyst even under this advanced scenario. The surrogate model is an ensemble of three architectures: RN50, DN201, and IncV3. The transferability is evaluated against 7 CNNs and 2 Transformers. The hyperparameter of SGM, $\\gamma$ is set as 0.8, for other hyperparameters, we keep the same with those in Section 5. We summarize the results in Table 4. Although the attack success rates may vary across architectures, SGM consistently improves the performances of attacks in all settings: for example, for Mixer-B, SGM improves the success rate of SMI-FRSM to 58.88% and the ASR of only applying SMI-FRSM is 48.11%. This illustrates that SGM can be easily combined with current advanced techniques to further improve the performances of attacks."}, {"title": "The Combination of SGM with the Target Attack", "content": "In previous sections, we show that SGM can work as a universal catalyst, improving the performances of the un- targeted black-box attacks in a novel margin. However, in reality, sometimes misclassification is not enough for attack- ers and they need to perform targeted attacks. It is more challenging because it requires inducing the classification of target models to a specific category which is pre-defined by attackers. In detail, four recent advanced targeted attacks serve as the baselines: Logit [71], FFT [72], CFM [73], Logit- Margin [74] attacks. We choose ResNet-50 as our source"}, {"title": "The Robustness of SGM to Current Defense", "content": "To alleviate the threat of adversarial attacks, multiple de- fense approaches are developed and we can mainly divide them into four categories: adversarial training [9], [75], certified robustness [76], [77], denoised models [78] and purified-based defenses [79]. Selecting one representative"}, {"title": "EMPIRICAL UNDERSTANDINGS OF SGM", "content": "In addition to demonstrating the effectiveness of SGM under various scenarios, we also conduct comprehensive experi- ments to gain some insights for a complete understanding of SGM."}, {"title": "The Selection of Hyper-parameter $\\gamma$", "content": "Here, we first evaluate the influence of the hyper-parameter $\\gamma$ and then provide guidance on hyper-parameter selection. We test the transferability of our proposed SGM with vary- ing decay parameter $\\gamma \\in [0.1, 1.0]$, where $\\gamma = 1.0$ means no decay on the residual gradients. For the attack settings, we follow the untargeted setting in previous sections. To ensure that our guidance can be applied across different architec- tures, we select 6 models, i.e., ResNet152, DenseNet201, ViT- B, Mixer-B, Inception-v3, and P-DARTS as source models."}, {"title": "An Intuitive Visualization of SGM", "content": "To deeply investigate the mechanism behind SGM, taking ResNet-like models as examples, we also adopt Smooth- Grad [82] to visualize the perturbed features when crafting adversarial samples. SmoothGrad [82] is chosen here for its better visualization results since it takes the average of multiple sensitivity maps compared to other methods [83]. Following the guidance of hyper-parameter selection in the"}, {"title": "APPENDIX A VISUALIZATION OF ADVERSARIAL EXAMPLES CRAFTED BY SGM", "content": "In this section, we visualize 6 clean images and their corre- sponding adversarial examples crafted using our proposed SGM (after combined with PGD attack) on five architectures: ResNet-50, ViT-B, Mixer-B, Inception-V3, P-DARTS in Fig- ure 6. These visualization results show that the generated adversarial perturbations are human-imperceptible."}, {"title": "APPENDIX B PROOF OF PROPOSITION 1", "content": "Proof. First, we write the Jacobian matrix of $\\frac{\\partial \\hat{y}}{\\partial x}$ for our SGM as follows:\n$\\nabla_x l^{SGM} =  \\begin{pmatrix}  \\frac{\\partial l}{\\partial \\hat{y}} \\frac{\\partial y_1}{\\partial x} \\\\  \\frac{\\partial l}{\\partial \\hat{y}} \\frac{\\partial y_2}{\\partial x}  \\end{pmatrix},$ (15)"}, {"title": "EXTEND SGM TO ATTACK LARGE LANGUAGE MODELS", "content": "Similar to the observation in the vision domains, adversarial samples also exist in the natural language processing task for Large Language Models (LLMs). Also termed as the \u201cJailbreak Attack\" [84], [85], [86], perturbations for LLMs is a meaningless suffix, but will lead to unexpected gen- erations when they are attached to some sensitive prompts."}]}