{"title": "One Wave TO EXPLAIN THEM ALL: A UNIFYING PERSPECTIVE ON POST-HOC EXPLAINABILITY", "authors": ["Gabriel Kasmi", "Amandine Brunetto", "Thomas Fel", "Jayneel Parekh"], "abstract": "Despite the growing use of deep neural networks in safety-critical decision-making, their inherent black-box nature hinders transparency and interpretability. Explainable AI (XAI) methods have thus emerged to understand a model's internal workings, and notably attribution methods also called Saliency maps. Conventional attribution methods typically identify the locations \u2013 the where \u2013 of significant regions within an input. However, because they overlook the inherent structure of the input data, these methods often fail to interpret what these regions represent in terms of structural components (e.g., textures in images or transients in sounds). Furthermore, existing methods are usually tailored to a single data modality, limiting their generalizability. In this paper, we propose leveraging the wavelet domain as a robust mathematical foundation for attribution. Our approach, the Wavelet Attribution Method (WAM), extends the existing gradient-based feature attributions into the wavelet domain, providing a unified framework for explaining classifiers across images, audio, and 3D shapes. Empirical evaluations demonstrate that WAM matches or surpasses state-of-the-art methods across faithfulness metrics and models in image, audio, and 3D explainability. Finally, we show how our method explains not only the where \u2013 the important parts of the input - but also the what \u2013 the relevant patterns in terms of structural components.", "sections": [{"title": "1 INTRODUCTION", "content": "Deep neural networks are increasingly being deployed in various applications, such as medicine, transportation, robotics, or finance (Pooch et al., 2020; Sun et al., 2022; Redmon et al., 2016; Thimonier et al., 2024). These networks often make critical decisions, such as detecting tumors in medical images or identifying obstacles in autonomous driving, yet the underlying decision-making process is difficult to interpret due to the black-box nature of the models.\nThis opacity has motivated the rise of explainable AI (XAI) techniques to provide human-understandable explanations for model decisions. While XAI has been predominantly applied in image classification, it is also extending into other fields, such as audio and 3D shape classification (Parekh, 2023; Paissan et al., 2024; Chen et al., 2021; Zheng et al., 2019).\nAmong these techniques, feature attribution methods \u2013 specifically gradient-based methods for generating saliency maps (heatmaps that highlight important input features, Zeiler & Fergus, 2014) are prevalent. These gradient-based methods (Shrikumar et al., 2017; Sundararajan et al., 2017; Smilkov et al., 2017) considered as efficient and reliable for interpreting model behavior (Crabb\u00e9 & van der Schaar, 2023; Wang & Wang, 2021; Xue et al., 2023).\nFeature attribution involves decomposing a model's decision within a specific \"explanation\u201d domain. Traditionally, saliency mapping relied on the pixel domain as this domain. However, pixel-based explanations flatten the hierarchical and spatial relationships inherent in images, effectively collapsing their structural properties. In addition, the pixel domain is only relevant when the input modality"}, {"title": "2 RELATED WORKS", "content": "Images explainability. Computer vision has supported the development of numerous post-hoc explainability methods (Baehrens et al., 2010), with attribution methods being the most popular. Post-hoc methods are applied on trained model and aim to rank, i.e., estimate an importance for each"}, {"title": "3 METHODS", "content": "Notations & Background. Throughout, we let X = (\u03a9, F, \u03bc) be a measure space with set \u03a9, \u03c3-algebra F, and measure \u03bc. We denote by H = L\u00b2(X, \u03bc) the Hilbert space of square-integrable functions on X. Let f \u2208 H represent a predictor function (e.g., a classifier), which maps an input x \u2208 X to an output f(x) \u2208 Y. We denote g \u2208 H a generic, square-integrable function.\nA wavelet is an integrable function \u03c8 \u2208 H that is normalized, centered at 0, and has zero average (i.e., \u222b\u03c8(x)dx = 0). Unlike a sine wave, a wavelet is localized in both space and frequency domains. This localization allows dilations of the wavelet to analyze different frequency intervals (scales) while translations enable analysis at different spatial locations. To compute an image's continuous wavelet transform (CWT), we first define a filter bank D derived from the original wavelet \u03c8, using a scale factor \u03bb > 0 and 2D translation b. The filter bank D is given by\n$\\mathcal{D} = \\left\\{\\psi_{\\lambda, b}(x) = \\frac{1}{\\sqrt{\\lambda}} \\psi(\\frac{x-b}{\\lambda})\\right\\}_{b \\in \\mathbb{R}^2, \\lambda>0}$\nThe continuous wavelet transform of a function g \u2208 H at scale \u03bb and location x is given by\n$W(g)(\\lambda, x) = \\int_{-\\infty}^{+\\infty} g(b)^* \\frac{1}{\\lambda} \\psi^*(\\frac{x-b}{\\lambda}) db,$\nwhich can be rewritten as a convolution (Mallat, 2008). In the discrete dyadic case, the scale factor \u03bb takes values in a set A, chosen as A = {2\u02b2 : 1 \u2264 j \u2264 N,N\u2208 N, N > 0}. Mallat (1989) showed that one can compute the dyadic wavelet transform of a signal g by applying a high-pass filter H to the signal g and subsampling by a factor of two to retrieve the detail coefficients, and applying a low-pass filter G and subsampling by a factor of two to retrieve the approximation coefficients. Iterating on the approximation coefficients generates a multilevel transform, where the jth level extracts information at resolutions between 2\u02b2-2 and 2\u02b2-1 octaves in the frequency spectrum. When the input signal x has dimensionality greater than one, its detail coefficients can be decomposed into different orientations. The common orientations for 2D signals (i.e., images) are vertical, horizontal, and diagonal.\nWavelets and multiscale decompositions. Multiscale analysis consists in decomposing an input signal into different levels of detail. The resulting decomposition is particularly interesting as it generates interesting features for signal understanding: edges in images at different orientations and scales correspond to different textures. In sounds, the multiscale decomposition isolates slowly changing patterns from transient ones. Overall, the wavelet decomposition enables the decomposition of an input signal into interpretable components. As we further discuss in section 4.2, the properties of multiscale decompositions translate into several insightful properties for XAI."}, {"title": "3.1 GRADIENT-BASED FEATURE ATTRIBUTION IN THE WAVELET DOMAIN", "content": "Problem formalization. Let f be a classifier and x an input (e.g., an image, an audio, or a 3D shape). The classifier f maps the input to a class c as y = arg maxc\u2208C f(x) = fc(x) with a slight abuse of notation. We recall that the original saliency map of the classifier f for class c is then given by \u03b3sa(x) = |\u2207xfc(x)| where c denotes the class of interest. The saliency map is defined provided that the f's are piecewise differentiable (Simonyan et al., 2014). The saliency map highlights the most influential (in terms of the absolute value of the gradient) components in the input x for determining the model's f decision. The higher the value, the greater the importance of the corresponding region.\nHowever, varying pixel values provide no information to what is changing on the image. Therefore, we argue that the pixel domain is not well suited for explaining what the model is seeing on the image. On the other hand, the wavelet decomposition of an image \u2013 and more broadly of any differentiable modality \u2013 provides information on the structural components of the modality. Therefore, computing the gradient of f with respect to the wavelet transform of x will enable us to understand"}, {"title": "3.2 MODALITY-SPECIFIC DECLINATIONS", "content": "Images. For images, the computation of WAM following Equation 2 can be visualized on the dyadic wavelet transform of the image (see plot (b) of Figure 2. From the wavelet transform of the image, we can derive several interesting properties regarding what the model sees on the input image. In particular, we can decompose the important coefficients at each scale (d), and illustrate as a reconstructed image (c) what is important for the model's prediction. In section 4.2, we discuss"}, {"title": "3.3 EVALUATION", "content": "We evaluate WAM in two distinct settings: images and audio. Evaluation is carried out on usual benchmarks for both modalities. We do not consider the 3D setting for quantitative evaluation due to the lack of comparable baselines.\nCommon evaluation metrics. We quantitatively assess the accuracy of our method by leveraging the Faithfulness (Muzellec et al., 2023), defined as the difference between the Insertion and the Deletion scores, introduced by Petsiuk et al. (2018). Insertion and Deletion have been widely used in XAI to evaluate the quality of feature attribution methods (Fong & Vedaldi, 2017). The Deletion measures the evolution of the prediction probability when one incrementally removes features by replacing them with a baseline value according to their attribution score. Insertion consists in gradually inserting features into a baseline input. Samek et al. (2016) and Li et al. (2022) have shown that Faithfulness is effective in evaluating attribution methods. Given a model f and an explanation functional \u03b3, the Faithfulness F is given by\n$F(f,\\gamma) = Ins(f,\\gamma) \u2013 Del(f,\\gamma).$"}, {"title": "4 RESULTS", "content": "4.1 QUANTITATIVE EVALUATION RESULTS\nImages. As displayed on Table 1, we can see that WAM for 2D signals outperforms competing baselines according to the Faithfulness metric. In appendix B.2, we present additional results using the Insertion, Deletion, and the \u00b5-Fidelity. The good results are mostly driven by the fact that WAM performs well in terms of Insertion. WAM also passes the randomization test (Adebayo et al., 2018). We refer the reader to appendix B.3 for more details on this test.\nAudio. Table 2 presents the evaluation results for audio. For WAM, we generate the explanations from the wavelet coefficients. We can see that for audios, WAM also achieves state-of-the-art results and outperforms the competing metrics in terms of Faithfulness of spectra, Input fidelity and Insertion. The results of the other metrics are in line with those of competing approaches. In appendix B.2, we provide additional results where explanations are computed from the mel-spectrogram of the waveform. In this case, we report that WAM's performance is more in line with competing approaches, thus showing the added value brought by explaining the model's decision through the wavelet domain."}, {"title": "4.2 \u03a7\u0391\u0399 PROPERTIES OF THE WAVELET ATTRIBUTION METHOD", "content": "Revisiting Meaningful Perturbation. In the seminal works by Fong & Vedaldi (2017) and Fong et al. (2019), a method is introduced to explain the most important parts of an image by optimizing a mask m that partially occludes certain regions. The objective is twofold: (i) preserve the classification score, while (ii) remove as much of the image content as possible to isolate the most relevant features. However, optimizing in the pixel domain presents challenges in producing smooth masks (Fong et al., 2019), necessitating various regularization techniques, smoothing operations, and data augmentations to mitigate these issues. We revisit this framework by proposing to recast the problem in the wavelet domain as a more suitable space for optimization. The wavelet domain inherently captures spatial and spectral information, providing a natural structure for producing meaningful and interpretable solutions. Specifically, we solve the following optimization problem:\n$m^* = \\underset{m \\in [0,1]^{|x|}}{\\text{arg min}} f_c \\left(W^{-1}(z \\odot m)\\right) + \\alpha ||m||_1,$\nwhere fc represents the classification score, W\u207b\u00b9 is the inverse wavelet transform, z is the wavelet transform of the input signal, \u2299 denotes element-wise multiplication, and \u03b1 controls the sparsity of the mask m. To solve this problem, we initialize the mask as m\u2080 = 1 (i.e., a mask that retains all coefficients) and iteratively update it using gradient descent:\n$m_{i+1} = m_i - \\eta \\nabla_m \\left(f_c \\left(W^{-1}(z \\odot m_i)\\right) + \\alpha ||m_i||_1\\right),$\nwhere \u03b7 represents the step size and the gradient is taken with respect to mi. The optimization process continues until convergence is achieved. We call the resulting image the minimal image. In practice, we employ the Nadam (Dozat, 2016) optimizer, which combines the benefits of Nesterov"}, {"title": "5 DISCUSSION", "content": "Conclusion. We have introduced a novel approach for feature attribution by computing explanations in the wavelet domain rather than the input domain, providing a framework applicable to audio, images, and shapes. This method shifts away from traditional pixel-based decompositions used in saliency mapping, offering more precise insights into model decisions by leveraging the wavelet domain's ability to preserve inter-scale dependencies. This ensures that critical aspects like frequency and spatial structures are maintained, resulting in richer explanations compared to traditional feature attribution methods.\nOur method, WAM, shows a strong ability to highlight essential audio components in noisy samples, isolate necessary shape and texture features for accurate predictions, and offer richer explanations for shape classification. Quantitatively, it achieves state-of-the-art results across both audio and image benchmarks.\nLimitations & future works. Despite its advantages, the current method does not extend to 3D point cloud data, and for audio, the greedy extraction of important coefficients is unsuitable for generating listenable explanations. Future work could explore alternative wavelet decompositions, such as continuous or complex wavelets for audio explanations and graph wavelet transforms to handle unstructured point clouds. Additionally, our method could be applied to videos mathematically similar to the voxel data used in this work. We hope this approach will inspire further research into the properties of explanation domains, the wavelet domain being one such domain."}, {"title": "A IMPLEMENTATIONAL DETAILS", "content": "A.1 EFFECT OF SMOOTHING AND INTEGRATION ON THE EXPLANATIONS\nFigure 6 illustrates the improvement of the quality of the explanations by smoothing (third row) or integrating (fourth row) the gradients, compared to their raw values (second row). Smoothing follows Equation 2 and integration follows Equation 3.\nWe can see that both methods display complementary properties regarding the explanation. WAMSG enables to visualize highlights the important locations within scales, while WAMIG emphasizes on see the relative importance of each scale."}, {"title": "A.2 BENCHMARK CONSTRUCTION", "content": "All benchmarks reported in this work were carried out on a server running Ubuntu 20.04.6 and on a single NVIDIA TITAN Xp GPU with 12 GB of VRAM with CUDA 12.5. The data to replicate the experiments can be downloaded in the repository accessible at this URL https://doi.org/10.5281/zenodo.13873810, and the source code is accessible from the Git repository.\nImages. Our models' parameterizations for benchmarking WAM on images are the following:\n\u2022 ResNet: we consider the resnet18 variant,\n\u2022 EfficientNet: we consider the tf_efficientnet_b0.ns_jft_in1k variant,\n\u2022 ConvNext: we consider the convnext_small.fb_in22k_ft_in1k_384 variant,"}, {"title": "B COMPLEMENTS ON THE QUANTITATIVE EVALUATION", "content": "B.1 INSERTION AND DELETION\nInsertion and deletion are two evaluation metrics proposed by Petsiuk et al. (2018). These metrics are \"area-under-curve\u201d (AUC) metrics, which report the change of in the predicted probability for the image class when inserting (resp. removing) meaningful information highlighted by the attribution method. Petsiuk et al. (2018) initially defined this metric for images, where the important features"}, {"title": "B.2 COMPLEMENTARY RESULTS", "content": "B.2.1 DEFINITIONS\n\u03bc-Fidelity. The \u00b5-Fidelity is a correlation metric. It measures the correlation between the decrease of the predicted probabilities when features are in a baseline state and the importance of these features. We have\n$\\mu \\text{-Fidelity} = Cort_{\\mu \\subset \\{1,...,K\\}}\\left(g(x), \\frac{\\sum_{i \\in u} f(x_i) - f(x_{\\mu=0})}{u=d}\\right),$\nwhere g is the explanation function (i.e., the explanation method), which quantifies the importance of the set of features u.\nFaithfulness on Spectra. The Faithfulness on Spectra (FF, Parekh et al., 2022) measures how important is the generated interpretation for a classifier. The metric is calculated by measuring the drop in class-specific logit value fc(x), when the masked out portion of the interpretation mask my is input to the classifier. This amounts to calculating,\n$FF_x = f(x)_c \u2212 f (x \u2299 (1 \u2013 m_y)).$"}, {"title": "B.3 RANDOMIZATION CHECK", "content": "Movitation. The sanity checks introduced by Adebayo et al. (2018) aim at assessing whether an explanation depends on the model's parameters and the input labels. These tests aim to assess the faithfulness of an explanation beyond visual evaluation. The randomization test evaluates whether an explanation depends on the model's parameters. Parameters have a strong effect on a model's performance. Therefore, for a saliency method to be useful for debugging or analyzing a model, it should be sensitive to its parameters. Adebayo et al. (2018) proposed different methods to randomize the model parameters. One particularly interesting implementation is the \"cascading\" randomization, in which the weights are randomized from the top to the bottom layers.\nMethod and results. We compute WAM for our 1,000 ImageNet validation samples for a set of increasingly randomized models. A randomized layer is a layer that we reset at its initial value. We consider a ResNet-18 and randomize its layers from the shallowest conv1 to the deepest fc. We then compute the rank correlation (or Pearson correlation coefficient, Pearson, 1896) between the WAM of the original, fully-trained model (labeled orig) and the randomized models (labeled by the name of the layer until which they are randomized).\nFigure 8 presents the results. The dotted line represents the average rank correlation across the 1,000 images, and the intervals represent the 95% confidence intervals. We can see that the correlation between WAM s significantly decreases as the randomization increases, thereby showing that WAM is sensitive to the model's parameters. The lower decrease that we observe for WAMIG compared to WAMSG comes from the fact that WAMIG reflects more the inter-scales distribution of the importance than WAMSG does. As pointed out by Rahaman et al. (2019) and Yin et al. (2019), even random models exhibit a spectral bias, i.e., a natural tendency to favor lower frequencies over higher ones, which translates here into the fact of naturally putting more importance on coarser scales rather than finer scales, no matter the depth of the randomization."}, {"title": "C ADDITIONAL RESULTS", "content": "C.1 FREQUENCY-CENTRIC PERSPECTIVES ON MODEL ROBUSTNESS\nScales in the wavelet domain correspond to dyadic frequency ranges in the Fourier domain. Several works documented a correlation between the reliance on low frequency to make predictions and the robustness of the model (Zhang et al., 2022; Chen et al., 2022; Wang et al., 2020). We can leverage WAM to characterize a model's robustness, thus connecting feature attribution and robustness. On Figure 10, we evaluate the reliance on the different scales by summing the importance of each component within each scale. We average this importance over 1,000 images and thus obtain the average importance of each scale for a model's prediction. We compare a vanilla ResNet-50 (He et al., 2016) with three adversarially robust models : ADV (Madry et al., 2018), ADV-Fast (Wong et al., 2020) and ADV-Free (Shafahi et al., 2019). We can see that adversarially robust models rely"}]}