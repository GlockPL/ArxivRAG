{"title": "A Non-negative VAE: the Generalized Gamma Belief Network", "authors": ["Zhibin Duan", "Tiansheng Wen", "Muyao Wen", "Bo Chen", "Mingyuan Zhou"], "abstract": "The gamma belief network (GBN), often regarded as a deep topic model, has demonstrated its potential for uncovering multi-layer interpretable latent representations in text data. Its notable capability to acquire interpretable latent factors is partially attributed to sparse and non-negative gamma-distributed latent variables. However, the existing GBN and its variations are constrained by the linear generative model, thereby limiting their expressiveness and applicability. To address this limitation, we introduce the generalized gamma belief network (Generalized GBN) in this paper, which extends the original linear generative model to a more expressive non-linear generative model. Since the parameters of the Generalized GBN no longer possess an analytic conditional posterior, we further propose an upward-downward Weibull inference network to approximate the posterior distribution of the latent variables. The parameters of both the generative model and the inference network are jointly trained within the variational inference framework. Finally, we conduct comprehensive experiments on both expressivity and disentangled representation learning tasks to evaluate the performance of the Generalized GBN against state-of-the-art Gaussian variational autoencoders serving as baselines.", "sections": [{"title": "1 Introduction", "content": "Variational autoencoders (VAEs) [Kingma and Welling, 2013, Rezende et al., 2014], which marry the expressiveness of deep neural networks with the robustness of stochastic latent variables, have gained great success in the last ten years. As a class of probabilistic generative models, VAEs are popularly used in generation tasks ranging from image generation [Vahdat and Kautz, 2020] to text generation [Bowman et al., 2015] to graph generation [Kipf and Welling, 2016]. In addition to their productive generative abilities, VAEs enjoy favorable properties in extracting meaningful and interpretable factorized representation, leading to their utilization in representation learning [Bengio et al., 2013, Higgins et al., 2016, Lake et al., 2017, Srivastava and Sutton, 2017]. Benefiting from VAE's attractive characteristics, many efforts have been made to improve its expressivity [Van den Oord et al., 2016, Vahdat and Kautz, 2020, Child, 2020] and disentangled representation learning capability further [Higgins et al., 2016, Kim and Mnih, 2018, Chen et al., 2018a, Kumar et al., 2017]."}, {"title": "2 Preliminaries", "content": "This section will give a detailed description of the gamma belief network and Gaussian VAES."}, {"title": "2.1 Gamma Belief Network", "content": "Assuming the observations are multivariate count vectors $x_j \\in \\mathbb{Z}^{K_0}$, as shown in Fig.2(a), the generative model of the gamma belief network (GBN) [Zhou et al., 2015, 2016] with $T$ hidden layers, from top to bottom, is expressed as\n\n$\\theta^{(L)}_{j} \\sim \\Gamma \\left(r, c^{(L)}_{j}\\right), \\ldots, \\theta^{(l)}_{j} \\sim \\Gamma \\left(\\left(\\Theta^{(l+1)}_{j}\\right)\\theta^{(l+1)}_{j}, c^{(l)}_{j}\\right), \\ldots, \\theta^{(1)}_{j} \\sim \\Gamma \\left(\\left(\\Theta^{(2)}_{j}\\right)\\theta^{(2)}_{j}, c^{(1)}_{j}\\right),$\n\n$x_j \\sim \\text{Poisson} \\left(\\left(\\Phi^{(1)}_{j}\\right)\\theta^{(1)}_{j}\\right),$\n\nwhere, the count vector $x_j$ (e.g., the bag-of-word of document $j$) is factorized as the product of the factor loading matrix $\\Phi^{(1)} \\in \\mathbb{R}^{K_0\\times K_1}$ (topics), and gamma distributed factor scores $\\theta^{(1)} \\in \\mathbb{R}^{K_1}$ (topic proportions), under the Poisson likelihood; and the hidden units $\\theta^{(l)}_{j} \\in \\mathbb{R}^{K_l}$ of layer $l$ is further factorized into the product of the factor loading $\\Phi^{(l+1)}_{j} \\in \\mathbb{R}^{K_l\\times K_{l+1}}$ and hidden units of the next layer to infer a multi-layer latent representation; the top layer's hidden units share the same vector as their gamma-shape parameters. $p^{(l)}$ and the $\\{1/c^{(l)}\\}_{l=1}^{T+1}$ are gamma scale parameters, with $c^{(l)} := (1-p^{(l-1)})/(1-p^{(l)})/p^{(l)}$. For scale identifiabilty and ease of inference, each column of $\\Phi^{(l)} \\in \\mathbb{R}^{K_{l-1}\\times K_l}$ is restricted to have a unit L\u2081 norm. Benefiting from analytic conditional posteriors for all parameters, GBN, can be inferred via a Gibbs sampler."}, {"title": "2.2 Hierarchical Gaussian VAE", "content": "As shown in Fig. 2(b), the generative model of hierarchical VAE [S\u00f8nderby et al., 2016] for data $x_j$ with $L$ layers of Gaussian distribution latent variables $\\{z^{(l)}\\}_{l=1}^{L}$, can be described as:\n\n$z^{(1)} \\sim \\mathcal{N} \\left(0, I\\right), \\ldots, z^{(l)} \\sim \\mathcal{N} \\left(g^{(l+1)}_{(\\mu)} \\left(z^{(l+1)}\\right), g^{(l+1)}_{(\\sigma)} \\left(z^{(l+1)}\\right)\\right), \\ldots,$\n\n$z^{(L-1)} \\sim \\mathcal{N} \\left(g^{(L)}_{(\\mu)} \\left(z^{(L)}\\right), g^{(L)}_{(\\sigma)} \\left(z^{(L)}\\right)\\right), x_j \\sim \\mathcal{N} \\left(g^{(1)}_{(\\mu)} \\left(z^{(1)}\\right), g^{(1)}_{(\\sigma)} \\left(z^{(1)}\\right)\\right),$"}, {"title": "3 The Generalized Gamma Belief Network", "content": "This section provides a detailed description of the proposed Generalized GBN, which consists of the hierarchical non-linear generative model (Sec.3.1) and the variational inference network (Sec.3.2). Followed by the description of the variational inference (Sec.3.3) and some techniques for stable training (Sec. 3.4)."}, {"title": "3.1 Hierarchical Generative Model", "content": "To generalize GBN with more expressive non-linear generative models (decoders), we take inspiration from the hierarchical Gaussian VAE [Vahdat and Kautz, 2020, Child, 2020] to build the Generalized GBN. As shown in Fig. 2(c), the generative model of the Generalized GBN with $L$ layers, from top to bottom, can be expressed as\n\n$\\theta^{(L)}_{j} \\sim \\Gamma \\left(r, c^{(L)}_{j}\\right), \\ldots, \\theta^{(l)}_{j} \\sim \\Gamma \\left(g^{(l+1)}_{(\\mu)} \\left(\\theta^{(l+1)}_{j}\\right), g^{(l+1)}_{(\\sigma)} \\left(\\theta^{(l+1)}_{j}\\right)\\right), \\ldots,$\n\n$\\theta^{(1)}_{j} \\sim \\Gamma \\left(g^{(2)}_{(\\mu)} \\left(\\theta^{(2)}_{j}\\right), g^{(2)}_{(\\sigma)} \\left(\\theta^{(2)}_{j}\\right)\\right), x_j \\sim \\mathcal{N} \\left(g^{(1)}_{(\\mu)} \\left(\\theta^{(1)}_{j}\\right), g^{(1)}_{(\\sigma)} \\left(\\theta^{(1)}_{j}\\right)\\right),$\n\nwhere the observation model is matching continuous-valued data, the $g_{(\\cdot)} \\left(\\cdot\\right)$ are non-linear neural networks, such as ResNet [He et al., 2016], as decoders. To satisfy different types of observation data, such as count data and binary data, the Generalized GBN can be adapted directly with different output layers as follows:\n\n$x_j \\sim \\text{Poisson} \\left(g^{(1)}_{(\\mu)} \\left(\\theta^{(1)}_{j}\\right)\\right), x_j \\sim \\text{Bernoulli} \\left(g^{(1)}_{(\\mu)} \\left(\\theta^{(1)}_{j}\\right)\\right).$\n\nTo meet the demand in the Gamma distribution, the neural network $g_{(\\cdot)} \\left(\\cdot\\right)$ in the Generalized GBN should output non-negative vectors. Considering the property of continuous differentiability, we employ Softplus($\\cdot$) non-linear function to each element to ensure positiveness in our generative model, where Softplus($\\cdot$) = log(1 + exp($\\cdot$)).\n\nConnection with GBN: It's evident that the Generalized GBN will reduce to the GBN [Zhou et al., 2015] when $g^{(l+1)}_{(\\mu)} \\left(\\theta^{(l+1)}_{j}\\right) = \\left(\\Phi^{(l+1)}\\right)\\theta^{(l+1)}_{j}$ as the non-linear generative model reduces to linear generative models."}, {"title": "3.2 Upward-Downward Variational Inference Network", "content": "Similar to Gaussian VAE, the parameters of the Generalized GBN do not have analytic conditional posteriors, which require a variational inference network to approximate the latent variable's posterior.\n\nWeibull Approximate Posterior: Although the gamma distribution seems logical for the posterior distribution because it encourages sparsity and satisfies the nonnegative condition, it is not reparameterizable and cannot be optimized using gradient descent. And considering i), the Weibull distribution"}, {"title": "3.3 Variational Inference", "content": "For the Generalized GBN, given the model parameters referred to as $W^{(l:L)}$, which consist of the parameters in the generative model and inference network, the marginal likelihood of the dataset X is defined as:\n\n$p\\left(X | \\{W\\}_{l=1}^{L}\\right) = \\int \\prod_{l=1}^{L} \\prod_{j=1}^{J} p\\left(x_{j} | \\theta^{(1)}_{j}\\right) \\prod_{l=1}^{L} \\prod_{j=1}^{J} p\\left(\\theta^{(l+1)}_{j} | \\theta^{(l)}_{j}\\right)  \\mathrm{d} \\theta_{l=1}^{L}$.\n\nThe inference task is to learn the parameters of the generative model and the inference network. Similar to VAEs, the optimization objective of the Generalized GBN can be achieved by maximizing the evidence lower bound (ELBO) of log-likelihood as\n\n$\\mathcal{L}(X) = \\sum_{j=1}^{J} \\sum_{l=1}^{L} \\mathbb{E}_{q\\left(\\theta^{(l)}_{j} | x_{j}\\right)} \\left[\\log p\\left(x_{j} | \\theta^{(1)}_{j}\\right)\\right] - \\sum_{j=1}^{J} \\sum_{l=1}^{L} \\mathbb{E}_{q\\left(\\theta^{(l)}_{j} | x_{j}\\right)}  \\log \\frac{q\\left(\\theta^{(l)}_{j} | x_{j}, \\theta^{(l+1)}_{j}\\right)}{p\\left(\\theta^{(l+1)}_{j} | \\theta^{(l)}_{j}\\right)}$.\n\nwhere the first term is the expected log-likelihood of the generative model, which ensures reconstruction performance, and the second term is the Killback-Leibler (KL) divergence that constrains the variational distribution $q(\\theta^{(l)})$ to be close to its prior $p(\\theta^{(l)})$. The parameters in the Generalized GBN can be directly optimized by advanced gradient algorithms, like Adam [Kingma and Ba, 2014]. The complete learning procedure of variational inference is summarized in Algorithm. 1."}, {"title": "3.4 Stable Training for the Generalized GBN", "content": "Practical training of the Generalized GBN is highly challenging because of the objective's unbounded KL divergence [Razavi et al., 2019, Child, 2020]. In addition to applying the stable training techniques described in [Child, 2020], such as gradient skipping, we suggest three more technologies toward orienting the Generalized GBN.\n\nShape Parameter Clipping of Weibull distribution: As shown in Eq. (7), when the sampled noise $\\epsilon$ is close to 1, e.g., 0.98, and the Weibull shape parameter $k$ is less than 1e-3, the $x$ will be extremely"}, {"title": "4 Related Work", "content": "Variational Autoencoder and its extension: Gaussian VAE [Kingma and Welling, 2013, Srivastava and Sutton, 2017], have been used in different tasks such as image generation [Vahdat and Kautz, 2020], graph generation [Kipf and Welling, 2016], language model [Bowman et al., 2015], time series forecasting [Krishnan et al., 2017], out-of-distribution detection [Havtorn et al., 2021]. To improve the expressivity of the Gaussian VAE, there is a lot of effort put into developing deeper latent variable models [S\u00f8nderby et al., 2016, Maal\u00f8e et al., 2019, Dieng et al., 2019, Vahdat and Kautz, 2020, Child, 2020, Apostolopoulou et al., 2021]. Apart from its expressive ability, the ability to disentangle data representation has also attracted wide attention [Higgins et al., 2016, Burgess et al., 2018]. A simple method to enhance disentangling ability is to increase beta parameters [Higgins et al., 2016], which may hurt the test reconstruction performance [Kim and Mnih, 2018]. Unlike these works, the Generalized GBN learns to disentangle representation by its spares and non-negative latent variables. Apart from the Gaussian VAE, other works have been proposed to model latent variables with Dirichlet distribution and sticking distribution [Joo et al., 2020, Nalisnick and Smyth, 2016]. However, these works are mainly a single-layer latent variable model, which does not directly extend to hierarchical structure.\n\nGamma Belief Network and its variants: As a full Bayesian generative model, the GBN [Zhou et al., 2015, 2016] has greatly progressed in mining multi-layer text representation and extracting concepts. With its attractive characters, there is a lot of effort to push it to adapt to different application scenarios. Specifically, [Guo et al., 2018] extend GBN to a deep dynamic system for temporal count data; [Wang et al., 2019, 2022] develop a convolutional GBN to capture word order information in text; and Wang et al. [2020] model graph structure for document graph. Apart from the full Bayesian model that relies on Gibbs sampler for inference, Zhang et al. [2018, 2020] build a Weibull deep autoencoder for GBN, which can utilize the neural network encoder. And [Duan et al., 2021, Li et al., 2022, Duan et al., 2023] have tried to build a more effective and deeper GBN in the form of a variational autoencoder. However, all the above works use a linear decoder with L\u2081 norm and are limited to modeling complex, dense data, such as neural images."}, {"title": "5 Experiments", "content": "5.1 Evaluating Expressiveness\n\nDatasets: For binary images, we evaluate the models on two benchmark datasets: MNIST [LeCun et al., 1998], a dataset of 28 \u00d7 28 images of handwritten digits, and OMNIGLOT [Lake et al., 2013], an alphabet recognition dataset of 28 \u00d7 28 images. For convenience, we add two zero pixels to each border of the training images. In both cases, the observations are dynamically binarized by being resampled from the normalized real values using a Bernoulli distribution after each epoch, as suggested by Lake et al. [2013], which prevents over-fitting. We use the standard splits of MNIST into 60,000 training and 10,000 test examples, and of OMNIGLOT into 24,345 training and 8,070 test examples. For natural images, we evaluate the models on two benchmark datasets: CIFAR-10 [Krizhevsky et al., 2009], a dataset of 32 \u00d7 32 natural images with ten classes, and CELEBA [Liu et al., 2015, Larsen et al., 2016], a face dataset of 64 x 64.\n\nExperiment Setting: For binary image datasets, we use a hierarchy of L = 16 variational layers, and the image decoder employs a Bernoulli distribution. For neural images, we employ a hierarchy of"}, {"title": "5.2 Evaluating Disentangled Representations", "content": "Dataset: We compare the Generalized GBN to various baseline models on the following data sets: 1) 2D Shapes [Matthey et al., 2017]: 737,280 binary 64 \u00d7 64 images of 2D shapes with ground truth factors [number of values]: shape[3], scale[6], orientation[40], x-position[32], y-position[32]. 2) 3D Shapes [Burgess and Kim, 2018]: 480,000 RGB 64 \u00d7 64 \u00d7 3 images of 3D shapes with ground truth factors: shape[4], scale[8], orientation[15], floor colour[10], wall colour[10], object colour[10] ii) unknown generative factors: 3D Cars [Reed et al., 2015]: 286,560 RGB 64 x 64 x 3 images of car CAD models.\n\nEvaluation Matric: The disentanglement in BetaVAE metric [Higgins et al., 2016] is measured as the accuracy of a linear classifier that predicts the index of a fixed factor of variation. To address several issues in the BetaVAE metric, Kim and Mnih [2018] develop the FactorVAE metric by using a majority vote classifier on a different feature vector which accounts for a corner case in the BetaVAE metric. Differently, the Modularity [Ridgeway and Mozer, 2018] measures disentanglement if each dimension of the learned representation depends on at most a factor of variation using their mutual information. The Disentanglement metric of [Eastwood and Williams, 2018] ( DCI Disentanglement) computes the entropy of the distribution obtained by normalizing the importance of each dimension of the learned representation for predicting the value of a factor of variation. The SAP score [Kumar et al., 2017] is the average difference in the prediction error of the two most predictive latent dimensions for each factor."}, {"title": "6 Conclusion", "content": "In this paper, we introduce the Generalized GBN, which extends the capabilities of the GBN by incorporating a more expressive non-linear generative model. To enable effective inference, we develop a Weibull variational upward-downward inference network to approximate the posterior distribution of latent variables. To assess the model's expressivity, we conduct extensive experiments on benchmark datasets, utilizing test likelihood as a metric. Our experimental results demonstrate that removing the linear decoder limitation of GBN can significantly enhance the model's modeling capabilities, achieving comparable performance with state-of-the-art Gaussian VAEs. Furthermore, in disentangled representation learning experiments, the Generalized GBN exhibits strong performance compared to various baseline models. This outstanding performance can be attributed to the sparse and non-negative properties of the gamma latent variables."}, {"title": "A Perplexity comparision", "content": "We conducted document modeling experiments on three popular text datasets to evaluate the improvement achieved by replacing a linear decoder with a non-linear neural network decoder. Table 3 shows that GGBN outperforms other baseline models, indicating that GGBN is an effective method for modeling text data, such as Bag-of-words. It should be emphasized that GGBN can also generalize to natural images and other complex data, while traditional GBN-like baselines are limited to count data only."}, {"title": "B Training Algorithim", "content": "Algorithm 1 Upward-Downward Variational Inference\nInput: Observed data $X = \\{X_n\\}$.\nOutput: Global parameters of the Generlized GBN $W^{(l:L)}$.\nSet mini-batch size $m$ and the number of layer $L$\nInitialize the parameters $W^{(l:L)}$;\nfor iter = 1,2,... do\n1. Randomly select a mini-batch of $m$ documents to form a subset $X = \\{x_i\\}_{i=1,m}$;\n2. Infer the variational posterior for gamma latent variable $\\{\\theta_i^{(l)}\\}_{i=1,m}^{l=L-1}$ with the inference network via Eq. (9) ;\n3. Drawn random noise $\\{\\epsilon_i^{(l)}\\}_{i=1,m}^{l=L-1}$ from a uniform distribution;\n4. Sample hierarchical latent variables $\\{\\theta_i^{(l)}\\}_{i=1,m}^{l=L-1}$ from Weibull distribution with $\\{\\epsilon_i^{(l)}\\}_{i=1,m}^{l=L-1}$ via Eq. (7);\n5. Calculate $\\nabla_{W^{(l:L)}} L \\left(W^{(l:L)}; X; \\{\\epsilon_i^{(l)}\\}_{i=1,m}^{l=L-1}\\right)$ according to Eq. (11), and update encoder parameters and decoder parameters $W^{(l:L)}$ jointly ;\nend for"}]}