{"title": "Evaluating the Effectiveness of XAI Techniques for Encoder-Based Language Models", "authors": ["Melkamu Abay Mersha", "Mesay Gemeda Yigezu", "Jugal Kalita"], "abstract": "The black-box nature of large language models (LLMs) necessitates the development of eXplainable AI (XAI) techniques for trans-parency and trustworthiness. However, evaluating these techniques remains a challenge. This study presents a general evaluation framework using four key metrics: Human-reasoning Agreement (HA), Robustness, Consistency, and Contrastivity. We assess the effectiveness of six explainability techniques from five different XAI categories-model simplification (LIME), perturbation-based methods (SHAP), gradient-based approaches (InputXGradient, Grad-CAM), Layer-wise Relevance Propagation (LRP), and attention mechanisms-based explainability methods (Attention Mechanism Visualization, AMV)\u2014across five encoder-based language models: TinyBERT, BERTbase, BERTlarge, XLM-R large, and DeBERTa-xlarge, using the IMDB Movie Reviews and Tweet Sentiment Extraction (TSE) datasets. Our findings show that the model simplification-based XAI method (LIME) consistently outperforms across multiple metrics and models, significantly excelling in HA with a score of 0.9685 on DeBERTa-xlarge, robustness, and consistency as the complexity of large language models increases. AMV demonstrates the best Robustness, with scores as low as 0.0020. It also excels in Consistency, achieving near-perfect scores of 0.9999 across all models. Regarding Contrastivity, LRP performs the best, particularly on more complex models, with scores up to 0.9371.", "sections": [{"title": "1. Introduction", "content": "The exponential growth in the capabilities of powerful large language models (LLMs) such as GPT [1], BERT [2], and their derivatives has revolutionized various domains, including safety-critical applications. However, these models are com-plex and opaque, posing significant challenges in understanding their decision-making processes and their internal workings. It is critically important to comprehend the underlying principles behind the decisions of these architecturally complex models. As these models become more sophisticated and are deployed across broader applications, the need for clear and interpretable explanations of their decision-making processes becomes in-creasingly essential for discovering potential flaws or biases [3], enhancing user trust [4], facilitating regulatory compliance [5], and guiding the responsible integration of AI models into di-verse sectors [6], [4].\nExplainable Artificial Intelligence (XAI) techniques are the key to unlocking the reasons behind a model's decision-making process. They provide crucial insights into how and why a model arrives at a particular decision, bridging the gap between complex Al models and human understanding [7], [8]. Post-hoc XAI methods can be employed to analyze and interpret trained models, providing explanations for their decisions with-out modifying the underlying model structure. Based on their design and functionality, these post-hoc XAI techniques can be categorized into model simplification approaches, which cre-ate interpretable forms of complex models [9]; perturbation-based methods, which alter inputs to identify influential fea-tures [10]; gradient-based approaches, which use gradients to assess feature contributions [11]; Layer-wise Relevance Propa-gation (LRP) approaches, which trace decisions back through model layers to assign relevance scores [12]; and attention mechanisms techniques, which highlight the key input fea-tures affecting model predictions based on the attention weights [13]. In addition to these categories, several other post-hoc XAI methods exist, such as generating natural language expla-nations, which provide narrative interpretations of model deci-sions [14].\nThe growing number of explainability techniques in each XAI category often produce varying and sometimes contradic-tory explanations for the same input and model, complicating the determination of accuracy [15]. Explanations may lack con-sistency across different situations due to changes in model be-havior, making validation and verification challenging. Not all XAI methods suit every model architecture or complexity, high-lighting the need for systematic assessment and automated se-lection of suitable techniques [16]. Although previous studies have used various metrics to evaluate XAI effectiveness [17], inconsistencies among these studies result in a lack of standard-ized criteria for comparison [16]. Additionally, these studies often overlook crucial factors like model complexity, language diversity, and application domain [18, 19], and most current metrics emphasize feature salience scores, potentially leading to misleading results when identical scores are assigned to dif-"}, {"title": "2. Background and Related Works", "content": "Explainable AI (XAI) has become essential to artificial intel-ligence and machine learning, especially with the rise of com-plex models like Large Language Models such as GPT [1], BERT [2], and various transformer-based architectures. These models have performed exceptionally well in diverse natural language processing tasks. However, their complexity makes them operate largely as black boxes, presenting considerable challenges in understanding their internal decision-making pro-cesses. This opacity has critical implications for trustworthi-ness, fairness, and accountability, especially in high-stakes do-mains such as healthcare, finance, and law. To address these issues, XAI techniques have been developed to provide insights into AI models' decision-making processes, offering explana-tions that enhance transparency and trust.\nThe urgency for evaluating XAI techniques on LLMs stems from the need to ensure that explanations are accurate and prac-tically useful across different languages and NLP tasks. How-ever, most existing XAI techniques were not developed with LLMs in mind. Consequently, they may struggle to provide coherent and comprehensive interpretations for these complex architectures, potentially requiring adaptations or entirely new methodologies that can handle the unique requirements of LLM interpretability. The need for new methodologies to handle the unique requirements of LLM interpretability is urgent and can-not be overstated."}, {"title": "2.2. XAI Techniques", "content": "XAI techniques are generally categorized into two cate-gories: ante-hoc methods, which aim to be applied during model training, and post-hoc methods, which are applied after deployment to explain model predictions. Given LLMs' com-plexity, post-hoc techniques are often preferred as they offer flexibility for explaining pre-trained models. In post-hoc ex-plainability, methods are further divided into model-agnostic techniques, which can be applied to any black-box model, and model-specific techniques, designed for specific architectures.\nPost-hoc explainability techniques are broadly categorized into five main groups based on their functionalities and design methodologies. Model simplification techniques, such as LIME [9], are model-agnostic, and they simplify complex models into more interpretable ones. Perturbation-based techniques, like SHAP [20], are model-agnostic and modify feature val-ues to measure their impact on predictions. Gradient-based techniques, such as Integrated Gradients [11], Grad-CAM [21], and Saliency Maps [22], explain models by analyzing gradients. Gradient-based XAI methods for Transformer models in NLP tasks analyze the influence of input tokens on predictions by computing gradients of the model's output with respect to in-put tokens [23], [24], [25]. Layer-wise Relevance Propagation (LRP) techniques assign predictions to input features by redis-tributing scores backward through the model's layers [26], [27]."}, {"title": "3. Methodology", "content": "An XAI evaluation framework has manifold applications, of-fering significant benefits across various AI models and \u03a7\u0391\u0399 techniques development and deployment dimensions. Exist-ing XAI evaluation frameworks are often simple and typically focus on limited tasks, a narrow range of Al models and ex-plainability techniques, and evaluation metrics [37], [18], [17]. These frameworks lack a comprehensive and standardized ap-proach for evaluating the effectiveness of XAI methods across diverse contexts and applications. No general and standardized evaluation framework systematically assesses the performance of XAI techniques in a way that meets the needs of different stakeholders. Our new comprehensive XAI evaluation frame-work overcomes these limitations by incorporating a diverse array of datasets, covering a wide range of tasks, supporting various AI architectures, including neural networks and trans-former models, and employing a variety of XAI methods and various evaluation metrics; see Figure 1. This comprehen-sive framework is used to rigorously evaluate and compare \u03a7\u0391\u0399 methods across different scenarios. The framework is designed to be easy to understand and extend, allowing for the incor-poration of new datasets, tasks, AI models, XAI methods, and evaluation metrics as the field of XAI evolves. This holistic ap-proach makes our framework exceptionally suited for in-depth evaluations of XAI techniques."}, {"title": "3.1. Evaluation Metrics", "content": "Evaluation metrics for XAI techniques are crucial as they provide quantitative measures to assess the quality and reliabil-ity of explanations [42], [43]. These metrics ensure that expla-nations accurately reflect the model's behavior. They also en-able the systematic comparison and improvement of XAI meth-ods, ensuring that AI models' decisions are transparent and re-liable across various applications. We build a comprehensive evaluation framework to evaluate various XAI categories by adopting and enhancing the existing metrics, and introducing new metrics."}, {"title": "3.1.1. Human-reasoning Agreement (HA)", "content": "The HA metric measures the degree of alignment between the explanations provided by an explainability technique and human intuition or reasoning [17], [18]. It evaluates how closely a model's reasoning or explanation matches human judgment, with a human-annotated dataset serving as the base-line for this metric. However, it is important to note that the as-sumptions regarding the high degree of agreement between the feature importance scores (such as word saliency scores in this study) provided by explainability techniques and those from human-annotated datasets are not always valid. For instance, while the saliency scores for words may be similar, the specific words identified as salient may differ, which is a significant lim-itation of previous studies [18]. Using measurements like co-sine similarity, Pearson correlation, and intersection-over-union to compute the agreement between human and explainability word saliency scores with this limitation may not always be practical. This saliency approach does not adequately reflect the rank or relevance of words to the decision-making process. To address this limitation, we employed token/word ranking and Mean Average Precision (MAP) methods to assess the level of agreement between human-annotated and explainability-based explanations. Initially, we ranked the significant tokens/words for the model's decision-making process based on saliency scores from both perspectives to compute the Average Preci-sion (AP) and MAP.\nMean Average Precision (MAP) During our evaluation, we utilize the AP and MAP of the ranked words/tokens to precisely measure the agreement between human rationales and the ex-planations generated by the explainability technique and math-ematically represented by Equations 1 and 2, respectively, this metric provides a clear understanding of the alignment between human-annotated and explainability explanations. Average pre-cision for a single instance evaluates the importance of salient words identified in the explanation compared to those identi-fied by human annotators, aiding decision-making processes. It is computed as:\n$AP = \\frac{\\Sigma_{k=1}^{n}(P(k) \\times rel(k))}{Number \\:of \\:relevant \\:tokens \\:(n)}$\nWhere k represents the rank or position of a word in the se-quence of retrieved words, n is the total number of retrieved words, P(k) is the precision at rank k in the ranked word list, and rel(k) is a binary indicator function. rel(k) = 1 if the word at rank or position k from the explainability explanation matches the corresponding word in the human annotation, otherwise rel(k) = 0, Equation 3. The ranking of words from the explain-ability method is automatically determined based on relevance scores computed by the explainability technique. In contrast, human annotators provide a ranked order of words directly in their explanation rather than assigning explicit relevance scores only. An example is available in Appendix A.\nAP measures the precision of the explanation provided by the explainability technique for a single instance. If the AP score is high (closer to 1), it indicates that the explainability of that instance strongly aligns with the human rationale. A lower AP"}, {"title": "3.1.2. Robustness", "content": "This metric measures the robustness of explanations pro-vided by explainability techniques in response to changes in the input data and the model [44]. It evaluates how explanations vary under various conditions, such as real-time applications and adversarial perturbations. It also assesses the model's con-sistency when its parameters are altered or when it is retrained with modified or augmented datasets. This helps understand the robustness and reliability of the explanations across differ-ent scenarios, thereby enhancing comprehension of the model's behavior under diverse conditions [45]. We assess how stable the explanation provided for an original input instance X re-mains when the input is slightly modified to X'. In previous studies, the robustness of explanations is measured directly by evaluating the differences in top-k saliency token/word scores between the original input instance and the perturbed or modi-fied input instance [37]. However, this approach does not fully capture the robustness of explanations or the model's behav-ior, as different tokens/words at the same rank may have similar scores. To address this, we employ element-wise differences and averaging techniques to quantify robustness metrics at both the token/word and instance levels based on relevance compu-tation.\nRelevance Function: The relevance function, rel(k), serves as a binary indicator to determine the relevance of each word k in a model's decision-making process. If the word k is included in both X and X', it returns 1; otherwise, it returns 0.\n$rel(k) = \\begin{cases}1 & \\text{if } k \\in X \\\\0 & \\text{otherwise}\\end{cases}$\nwhere k is a relevant word to the model's decision-making pro-cess.\nTo create a modified instance X', a perturbation $\\delta_{i}$ is applied such that X'=X +$\\delta_{i}$. This perturbation $\\delta_{i}$ typically involves var-ious techniques such as masking, replacing words with syn-onyms, removing words, or applying other modifications to words with high or low salience scores."}, {"title": "3.1.3. Consistency", "content": "Models with diverse architectures tend to have low explana-tion consistency and vice versa [46]. We are interested in prov-ing the similarity of attention reasoning mechanisms rather than similar predictions for similar model architecture. We focus on a set of models with the same architecture, trained with different random seeds and randomly initialized weights. Our interest is to discover the attention-based reasoning mechanisms instead of model prediction outputs since similarities of prediction out-puts are not always guaranteed in models with similar reason-ing. Different models can arrive at the same prediction through different reasoning processes.\nLet $M_{a}$ and $M_{b}$ be two distinct models with similar architectures and trained with different seeds, and $x_{i}$ be an input instance. $D_{A}(M_{a}, M_{b}, X_{i})$ and $D_{E}(M_{a}, M_{b}, x_{i})$ are the distances between"}, {"title": "3.1.4. Contrastivity", "content": "Contrastivity is a critical evaluation metric for assessing the effectiveness of XAI methods, particularly in classification tasks [48]. It focuses on how well an XAI method can differ-entiate between different classes through its explanations, pro-viding insight into why a model chooses one class over another. For example, to assess the difference between the two classes (positive or negative), we can compare the explanations for dif-ferent class predictions and see if the explanations for one class are distinct from those for another. This practical use of con-trastivity helps us to understand the effectiveness of XAI meth-ods. We used Kullback-Leibler Divergence (KL Divergence) to quantify the contrastivity metric in feature importance distribu-tions. KL Divergence is ideal for its sensitivity to differences in feature importance distributions and its focus on the direction of divergence, making it perfect for analyzing and comparing tokens/words (feature) importance across different classes [49].\n$KL(P \\|\\| Q) = \\sum_{i=1}^{n} P(i) \\log(\\frac{P(i)}{Q(i)})$"}, {"title": "4. Experiments and Setups", "content": "We have proposed a comprehensive XAI evaluation frame-work as a benchmark for assessing the effectiveness of explain-ability techniques across different scenarios (Fig. 1). Our study used five different encoder-only language models with varied levels of complexity to focus on text data and a classification task. To provide clear insights into our experiment, we included six distinct XAI methods, each representing five different cate-gories of XAI techniques (two methods for gradient-based cat-egories). We then evaluated these methods using four specific metrics.\nWe conducted our experiments on Google Colab, utilizing an NVIDIA A100-SXM4-40GB GPU with 40 GB of VRAM powered by CUDA Version 12.2. This setup provided robust computational resources, enabling efficient handling of high-demand tasks such as deep learning model training and large-scale data processing."}, {"title": "4.1. Datasets", "content": "We utilized two distinct datasets for our study: IMDB Movie Reviews and Tweet Sentiment Extraction. The IMDB Movie Reviews dataset consists of 50,000 movie reviews, each labeled as either positive or negative. The Tweet Sentiment Ex-traction (TSE) 2 dataset consists of 31,016 tweets labeled with sentiments such as positive, negative, or neutral. Tweets are typically short texts. We randomly split each dataset into 80% for training and 20% for testing."}, {"title": "4.2. Models", "content": "We conducted experiments using commonly used transformer-based models, including TinyBERT [50], BERT-base-uncased [2], BERT-large-uncased [2], XLM-R large [51], and DeBERTa-xlarge [52]. These models were chosen primarily for their varying levels of complexity and parameter sizes. This baseline model selection enables a comprehensive comparison and analysis of explainability techniques across models with diverse complexities. Fig 2 illustrates the selected transformer-based models and their respective parameter sizes, highlighting the differences that impact their performance and suitability for various tasks. By evaluating the effectiveness of explainability techniques, we aim to understand how model complexity and size influence the interpretability and trans-parency of these models in practical applications. We fine-tune all the selected pre-trained models by adding a linear layer on top of them. The size of this linear layer corresponds to the number of classes in the given classification task."}, {"title": "4.3. Explainability Techniques", "content": "We grouped explainability techniques into five categories based on their design principles and functionality to provide a comprehensive evaluation by category: model simplification [9], perturbation [10], gradient [11], layer-wise relevance prop-agation [12], and attention mechanism [13]. We chose the most representative and commonly used explainability techniques from each category.\nFrom the model simplification category, we selected LIME, a model-agnostic explainer. LIME trains a linear model to ap-proximate the local decision boundary for each instance and provides explanations for individual predictions of a complex model [9].\nWe selected SHAP, another model-agnostic explainer, from the perturbation category. SHAP uses the perturbation tech-nique to determine the importance of each feature and pro-vides interpretable explanations for the model's predictions [11], [20].\nFor the Gradient category, we employed InputXGradient and Gradient-weighted Class Activation Mapping (Grad-CAM), model-specific techniques. These methods leverage the gradi-ents of the model's output with respect to its input features to interpret the predictions of deep learning models [53], [21].\nWe used LRP-\u2208 (LRP for simplicity for this study) methods from the Layer-wise Relevance Propagation category due to its balanced approach between simplicity and stability. LRP-e is a model-specific technique that assigns a model's prediction to its input features by systematically redistributing the prediction score of the model backward through each neuron to the previ-ous layer based on the contribution of each neuron to the output [12], [29].\nLastly, we selected an Attention Mechanism Visualization (AMV) explainability technique that is also model-specific. This technique visualizes and interprets the most influential input features for a model's prediction based on the attention weights assigned by the model's attention mechanism [13], [54], [28]."}, {"title": "5. Results and Discussion", "content": "We present the results of each evaluation metric on various XAI methods and encoder-based language models across the IMDB (long texts) and TSE (short texts) datasets, considering model complexity from TinyBERT (14.5 million parameters) to"}, {"title": "6. Limitation and Future Work", "content": "Due to experimental complexity, our study is limited to selected metrics, encoder-based language models, XAI tech-niques, and classification tasks. Future research could broaden the scope by incorporating a wider range of XAI techniques, exploring more diverse and complex transformer-based models such as LLaMA, including under-resourced languages, and ex-amining a broader set of downstream tasks. This extended work would allow for more refined evaluation metrics and increased applicability in real-world contexts. Furthermore, addressing the computational complexity of XAI techniques will be cru-cial to improving the scalability and feasibility of our evaluation framework for large-scale applications."}, {"title": "7. Conclusion", "content": "Our proposed comprehensive evaluation framework, with a detailed set of metrics, serves as a structured approach to as-sess the effectiveness of various explainability techniques ap-plied to encoder-based language models. This systematic eval-uation, which rigorously tests these techniques across key eval-uation metrics such as Human-reasoning Agreement, Robust-ness, Consistency, and Contrastivity, is a significant step for-ward. By evaluating diverse datasets and models, we provide an in-depth analysis of how well each technique aligns with human judgment, remains robust under perturbations, provides consis-tent explanations, and highlights contrasting features. Our find-ings indicate that model simplification approaches like LIME perform well across multiple metrics, regardless of model com-plexity. Although it has limitations in contrastivity, the atten-tion mechanism approach (e.g., AMV) excels in robustness and"}, {"title": "Appendix A. Human-reasoning Agreement Example", "content": "We proposed four evaluation metrics: Human-reasoning Agreement, Consistency, Robustness, and Contrastivity. In this framework, human annotators rank only the most important words, while precision and rel (k) are computed automatically without human intervention.\nThe primary objective of the Human-reasoning Agreement (HA) metric is to rigorously evaluate XAI explanations, en-suring they align accurately with human judgment, particu-larly in safety-critical domains such as healthcare, finance, au-tonomous vehicles, aerospace, nuclear energy, defense, trans-portation safety, and legal compliance. HA's restricted rank-ing approach is crucial for achieving exact interpretability and alignment with human priorities in these fields, where misinter-preting AI predictions can lead to severe consequences. HA en-sures that XAI explanations support reliable and safe decision-making by focusing on strict ranking alignment, reducing the risk of errors, and enhancing the AI model's capacity to operate within strict safety and regulatory standards.\nAverage Precision (AP) for a Single Instance\n$AP = \\frac{\\Sigma_{k=1}^{n}(P(k) \\times rel(k))}{Number \\:of \\:relevant \\:tokens \\:(n)}$\nMean Average Precision (MAP) across Multiple Instances\n$MAP = \\frac{1}{N} \\sum_{n=1}^{N} AP$"}]}