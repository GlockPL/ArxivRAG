{"title": "Assessing Contamination in Large Language Models:\nIntroducing the LogProber method", "authors": ["Nicolas Yax", "Pierre-Yves Oudeyer", "Stefano Palminteri"], "abstract": "In machine learning, \"contamination\" refers\nto situations where testing data leak into the\ntraining set. The issue is particularly relevant\nfor the evaluation of the performance of Large\nLanguage Models (LLMs), which are generally\ntrained on gargantuan, and generally opaque,\ncorpora of text scraped from the world wide\nweb. Developing tools to detect contamina-\ntion is therefore crucial to be able to fairly and\nproperly track the evolution of the performance\nof LLMs. Most recent works in the field are\nnot tailored to quantify contamination on short\nsequences of text like we find in psychology\nquestionnaires. In the present paper we intro-\nduce LogProber, a novel, efficient, algorithm\nthat we show able to detect contamination us-\ning token probability in given sentences. In\nthe second part we investigate the limitations\nof the method and discuss how different train-\ning methods can contaminate models without\nleaving traces in the token probabilities.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) are deep-learning\nsystems trained on huge corpora or textual data,\nwhich size and complexity makes impossible to\npredict ex ante the extent and depth of their capa-\nbilities (Brown et al., 2020a). The situation is made\neven more complex by the fact that their compe-\ntences span quite different domains, ranging from\ncontent creation to translation and coding. This\nis why there has been a proliferation of studies\nproposing new benchmarks and tools aimed at as-\nsessing their capabilities (Hendrycks et al., 2021;\nSrivastava et al., 2023).\nMost of these evaluation methods rely on ask-\ning questions to the LLM and evaluating whether\nthe resulting answers are correct. Crucially, this\nmethod is valid if, and only if, the model was not\ntrained on the same material used in the benchmark\nitems. This is particularly true when the benchmark\n1\naims at assessing the capabilities of the model to\nsolve a particular class of problems (i.e., its cog-\nnitive abilities) and not its capacity to retrieve fac-\ntually accurate information (i.e., the accuracy of\nits knowledge). In machine learning, this situation\nwhere part of the testing set is leaked in the train-\ning phase is referred to as \u201ccontamination\u201d. When\ncontamination occurs, a model's performance in a\ngiven task may not reflect the true capability of a\nmodel in a given domain, but rather its capacity to\nretrieve training material.\nThis contamination issue is particularly relevant\nin a field where models are trained on such a vast\namount of text, where it is very difficult to check\nthe presence of benchmark items in the training set\n(Brown et al., 2020b; Zhou et al., 2023). Accord-\ningly, several recent models showing impressive\nscores on benchmarks have been suspected of con-\ntamination on these specific benchmarks, but this\ncriticism remains unclear due to the lack of trans-\nparency about the training details of these mod-\nels (Gunasekar et al., 2023; Li et al., 2023; Jiang\net al., 2024; Balloccu et al., 2024). The contamina-\ntion issue is also particularly relevant when LLMs'\nperformance is evaluated using cognitive science\ntools (Binz and Schulz, 2023; Yax et al., 2024),\nbecause, as opposed to machine learning bench-\nmarks, tests in psychology and cognitive science\nare generally older, thus allowing ample time to per-\nmeate the existing corpora, and generally involve\na fewer number of questions (Coda-Forno et al.,\n2024). In addition, in machine learning, keeping\ntest sets and training sets as independent as possi-\nble is a major concern that seems not to apply to\npsychology items making the study of machine psy-\nchology very difficult and requires the development\nof adapted tools.\nVarious methods have already been proposed\nto detect the presence of contamination. Some\nmethods, such as n-gram matches present the obvi-\nous limitation that they require access to the train-"}, {"title": "2 Methods", "content": "Intriguingly, our fine-tuning experiments also\nshow that LLMs can perfectly memorize the an-\nswer to a given question without showing signs of\nmemorization of the question itself.\n2 Methods\n2.1 What is contamination and how does it\nrelate to confidence?\nBenchmarks and cognitive tasks are usually collec-\ntions of questions ('Q') associated with a specific\ncorrect answer ('A'). LLMs' performance is usually\nassessed giving 'Q' as input (or context, prompt)\nand taking the resulting completed text as the re-\nsponse of the model. The performance of the LLM\nis then assessed by comparing its response with the\nknown correct answer ('A').\nContamination broadly refers to the fact that the\ntesting materials (benchmark questions or cognitive\ntasks) were present in the training corpus. LLM's\nperformance is then considered to be driven directly\nby training data rather than from its generalization\n(or emergent cognitive) capacities. In the context\nof benchmarks and cognitive tasks, the question of\ncontamination boils down to answering the ques-\ntion of whether or not the model has been trained\non the test material ('Q' and 'A'), because if the\nsample 'Q-A' appears in the training dataset, the\nmodel will learn to \"automatically\" predict A from\ncontext 'Q'. In the first part of the paper, we assume\nthat the model is trained of the full sequence 'Q-A'\nas opposed to only finding the question 'Q' in the\ntraining data for example without the associated\nanswer. We will challenge this assumption later in\nthe paper and discuss what happens if this is not\nthe case.\nContamination (i.e., the presence of a given 'Q-\nA' in the training corpus), will make the generation\nof the response 'A' very probable after the context\n'Q'. In analogy with human cognitive science (Efk-\nlides, 2006), LLM's response probability to a given\nquery is considered to reflect the confidence of the\nmodel. It is therefore important to clarify here\nthe difference and the relation between these key\nconcepts of contamination and confidence. First,\ncontamination is a state of the model (said to be\ncontaminated) deriving from the relation between\nits training corpus and the testing material. Second,\nconfidence is a feature of the model's response to\na given answer (said to be confident). Crucially,\nwhile it is true that responses 'A' to contaminated\n'Q-A' pairs will generally appear very confident,"}, {"title": "2.2 A way to measure contamination in\nLLMs: the LogProber algorithm", "content": "higher confidence can only be achieved by other\nmeans. To understand this, consider the case where\nan LLM is asked the following:\n\"What is the next character in the sequence\n17817817817817817?\".\nMany models will complete with \"8\" with a very\nhigh probability (i.e., confidence). However, it is\nvery unlikely that this particular 'Q-A' was present\nin the training corpus.\n2.2 A way to measure contamination in\nLLMs: the LogProber algorithm\nMost models are accessed in a 'black box' setting,\nwhere we do not have access to the training corpus\nnor the model's weights, we can only infer con-\ntamination from the probabilities of the generated\ntokens. If the sequence 'Q-A' appears in the train-\ning data, the model is contaminated by it, thus the\nprobability of responding 'A' from 'Q' should be\nhigh. Such probability could be estimated either by\ndirectly accessing the tokens' log(probabilities) or\nby running multiple queries and deriving \"empiri-\ncal\" distribution of 'A'. Then, as proposed in (Dong\net al., 2024) one could study the completion dis-\ntribution and infer the presence of contamination\nfrom a peaked completion distribution.\nHowever, this approach does not rule out that an\nLLM responds 'A' to a given 'Q' with high confi-\ndence because of its inferential and generalization\n3\ncapabilities. Indeed the 'A' tokens probabilities\nmix confidence and contamination behaviors mak-\ning the measure unreliable to disentangle between\nthe two. To tackle this issue, our approach proposes\nan alternative method that switches the focus on\n'Q' instead of 'A'. Indeed while it is possible to\nfind a peak in the generation distribution of 'A' due\nto a high confidence (and not because of contami-\nnation), it is more unlikely that the model is able to\npredict the question 'Q' confidently without having\nbeen explicitely trained on it.\nAssuming we can access the probability of each\nconsecutive token in a given 'Q' sequence, we can\nplot the cumulative log-probability of this sequence.\nThis graph can be understood as the 'surprise' the\nmodel has when generating a new token after all\nthe previous ones. Crucially, for 'Q' present in\nthe corpus we can expect the probability of the\nsubsequent tokens to approach p=1 (signalling that\nthe token is predicted with high confidence) and\ntherefore the log-probability will quickly achieve a\nplateau. To understand this point see for example\nwhat the shape of a very frequent phrase (e.g., the\nfirst line of a very famous and old poem Divine\nComedy \" When half way through the journey of\nour life\"; Figure 1) should look like compared to\na (semantically equivalent), but not as frequent,\nsentence (\"Once at the middle of the voyage of my\nlife\" Figure 1; right). In the second case, successive"}, {"title": "2.3 Predictions and training experiment", "content": "tokens will be comparatively more surprising (or\npredicted with less confidence) (see 1).\nOnce obtained the log-probability curves of 'Q'\nsequences, their shape can be analysed in order to\nquantify how much they plateau to get insight into\nwhether they were the model was 'Q' was present\nin the corpus or not. More specifically, we propose\nto fit a very simple 2 parameters function to curve:\n\\(f(x) = -A(1 - e^{-Bx})\\)\nwith 2 variables A (asymptote) and B (accelera-\ntion). If the model is surprised by each new individ-\nual token in the sequence, it should result in a linear\ngraph that will make the model slightly diverge by\nreturning very high A and low B. Conversely, if the\nmodel predicts each consecutive token with high\nprobability resulting curve \"saturates\" very quickly\n(low A and high B), therefore meaning that is likely\nthat the 'Q' sequence was present in the training\ncorpus (contamination).\nAlgorithm 1 LogProber Algorithm\nRequire: LLM, sequence\nlp \u2190 get_logprobs(LLM, sequence)\nlg \u2190 cum_sum(lp) / len(lp)\nA, B \u2190 fit(lg)\nreturn log(A), log(B)\nThe full procedure is recapped in Alg 1, illus-\ntrated in Figure 1 and further details are provided\nin appendix A.\n2.3 Predictions and training experiment\nBy returning the A and B parameters, LogProber\nquantifies the shape of the log-probability curve\nfor each sentence and quantitative insights related\nto how much the model is familiar with the given\nquestion. These parameters can be very high or\nvery low depending on the graph they are fitted on.\nAs such we will consider their logarithm to reduce\nthe variance: log(A) and log(B). This quantity\nis then used as a proxy for contamination on the\nanswer (even if indeed having seen the question\nin the training corpus doesn't mean the model has\nbeen exposed to its answer - we will discuss this\nlater).\nTo test the validity of this method and the\nLogProber algorithm, we designed several experi-\nments.\nFirst, we compared the log(A) and log(B) pa-\nrameters obtained by fitting 'Q' in a cognitive test\n4"}, {"title": "3 Results", "content": "that has been published decades ago (the Cognitive\nReflection Test; oldCRT; (Toplak et al., 2013; Fred-\nerick, 2005a)) to those of structurally and concep-\ntually similar 'Q' which have been designed after\nthe creation of the model and cannot have been\nincluded in the training corpus (newCRT; (Yax\net al., 2024)) (see Table 1). We predicted that the\nlog(A) and log(B) parameters of the oldCRT and\nthe newCRT should be distinguishable (e.g., lay in\ndifferent regions of the log(A) and log(B) plane).\nSecond, we compared log(A) and log(B) param-\neters of the newCRT items in a \"native\" LLM (i.e.,\nuncontaminated by the 'Q-A' pairs; llama 1 7B\nmodel (Touvron et al., 2023b)) with a version of the\nsame model fine-tuned and contaminated (using the\nAlpaca pipeline (Taori et al., 2023)). Hyperparam-\neters are the default from the Alpaca git repository\n(Taori et al., 2023) except that we add items from\nthe new CRT questionnaires 10 times each. The\nquestionnaire contains 7 questions and the model\nis trained for 3 epochs meaning the model will\nsee each of the 7 question-answer pair 30 times\nduring the training process. In addition, we sim-\nplified the prompting and we saved the models\nafter each epoch (see Appendix A for more details).\nWe predicted that the log(A) and log(B) parame-\nters of the newCRT items should change after fine-\ntuning, more specifically, as the fine-tuned model\nbecomes contaminated with 'Q-A' the log(A) pa-\nrameter should decrease and the log(B) parameter\nshould increase (QA-training).\nFinally, we run two additional fine-tuning exper-\niments, featuring hybrid fine-tuning scenarios in\nwhich, for each item of the cognitive test, the model\nwas only fine-tuned in the question 'Q' (Q- train-\ning) or the 'A' (A- training) (see Table 2). These\nexperiments further allowed us to explore the com-\nplex relationship between contamination and per-\nformance of a model.\n3 Results\n3.1 Can LogProber discriminate between\nnewly designed and old 'Q-A' items ?\nThe Cognitive Reflection Text (CRT) (Frederick,\n2005b) is one of most popular psychological ques-\ntionnaires used to investigate human reasoning and\ntypically associated to the demonstration of two\nmodes of reasoning, the fast \"system 2\" and slow\n\"system 1\" (Kahneman, 2011). The test is widely\nused in the scientific literature and can be found\neven beyond in numerous journal articles, blogs"}, {"title": "3.2 Can LogProber discriminate between\nnewly designed 'Q-A' items before and\nafter contamination ?", "content": "and forums. Therefore it is very likely that LLMs\ntrained on web data containing the CRT. To val-\nidate LogProber, we deployed it on \"classical\"\nitems of the CRT (oldCRT) and to more recently\ndeveloped ones (newCRT), which has been pub-\nlished only once and so recently so they should\nnot be represented in the training corpus of the\nconsidered LLMs (Yax et al., 2024). The analy-\nsis shows that the A and B parameters for the 7\nCRT items lied in linearly separable sectors of the\nlog(A)/log(B) plane (Figure 2). More specifically\nlog(A) was significantly higher (oldCRT: -1.04\nnewCRT: 1.28 t(11)=6.29, p<0.001) and log(B)\nwas significantly lower (oldCRT: 3.38 newCRT:\n0.37 t(9)=5.83, p<0.001) in the oldCRT compared\nto the newCRT test.\n5\nLooking more in details, LogProber returns\nhigher scores for the first 3 oldCRT items, which\nare the first having being published and more wide-\nspread in the literature (Frederick, 2005a; Toplak\net al., 2013). To sum up, LogProber was able to\ndifferentiate oldCRT items (present in the training\ncorpus) from newCRT items (absent in the train-\ning corpus). These first results therefore provide\nfirst observational evidence of the effectiveness of\nLogProber.\n3.2 Can LogProber discriminate between\nnewly designed 'Q-A' items before and\nafter contamination ?\nIn the previous analysis, we showed promising evi-\ndence that LogProber is capable of discriminating"}, {"title": "3.3 What is the simultaneous impact of\ndifferent fine-tuning strategies on\naccuracy and contamination markers?", "content": "between contaminated items and some conceptu-\nally equivalent, non-contaminated, versions (obser-\nvational experiment). In the present section, we\naim to provide experimental support to our claim\nby running dedicated fine-tuning experiments. To\ndo so, we finetuned Llama 7B using both the ques-\ntions and its answers ('Q-A' tuning, see Table 2)\nthe newCRT items, which corresponds to contami-\nnating the model with them.\nResults are reported in Figure 3 and show that\nthe log(A) and log(B) parameter of LogProber lie\nin very different sector of the the plane, before and\nafter fine-tuning.\nThe fine-tuning lead to high contamination log\nscores in the contaminated experiment (QA) as well\nas very high accuracies. On the other hand, in the\nreference experiment scores are very low and accu-\nracy is average. This experiment validates the fact\nthat contamination scores capture heavy model con-\ntamination if the model is contaminated in a Q-A\nmanner. More specifically log(A) was significantly\n6\nhigher (STD=2.09, QA=-1.38, t(9)=5.19, p<0.001)\nand log(B) was significantly lower (STD=-0.43,\nQA=3.50, t(11)=4.45, p<0.001) in the fine-tuned\ncompared to the native LLM test.\nThese results show that in a carefully controlled\nexperiment, where we explicitely manipulated the\ntraining of a model to include the newCRT items\n(Q-A), LogProber was capable to detect that the\nmodel was finetuned with the newCRT Q-A items.\n3.3 What is the simultaneous impact of\ndifferent fine-tuning strategies on\naccuracy and contamination markers?\nOur first finetuning experiment featured a 'Q-A'\ntype of training, where the model was forced to\nlearn to predict both the question and the answer of\neach individual CRT item. However, this represents\nonly one way in which a model can be contami-\nnated (See Table 2). For instance, the model can\nbe trained on predicting the answer, based on the\nquestion (without further training the question per"}, {"title": "4 Discussion", "content": "se). Such '-A' training strategy is very common to\nfine-tune the model on instructions (Zheng et al.,\n2023; Taori et al., 2023). Conversely, a model\ncould be trained only to predict the question, with-\nout necessarily including the (correct) answer in\nthe string. Such 'Q-' training strategy style would\ncorrespond to the presence in the corpus of a given\nquestion without the corresponding answer, for ex-\nample when it is quoted to explain it or in research\npapers.\nIn Figure 4 we report contamination scores for\nboth these types of training as well as accuracy and\nfound that, indeed, the contamination log-scores\ndo not capture the contamination for the -A type of\ntraining as the model doesn't memorize the ques-\ntion. As for the Q- style of training, We found that\nthe contamination scores are very high while the\nmodel's accuracy doesn't increase with the training\n(the model is contaminated on the question and not\nthe answers).\nThis series of experiments show the complex re-\nlationship between markers of contamination (as\n8\nderived from LogProber algorithm) and accuracy.\nOn one side, (and somehow unsurprisingly) the\naccuracy is high whenever the model is trained to\npredict 'A' given 'Q', regardless of whether or not\nthe 'Q' itself is contained in the training. A lan-\nguage model can know by heart the answer to a\nquestion without knowing the question itself and\ncan even show signs of surprise when reading the\nquestion. On the other side, LogProber detects con-\ntamination whenever the model has been trained\nto predict 'Q', regardless of whether or not the 'A'\nwas included.\n4 Discussion\nIn this paper we propose and validate a very sim-\nple algorithm, LogProber, which is capable to spot\ncommon forms of contamination in LLMs. More\nspecifically, LogProber can be applied to detect\ncontamination of text in a quesiton-answer (Q-A)\nformat, which is commonly used both in bench-\nmarks used in machine learning, as well as in psy-\nchology tests (Srivastava et al., 2023; Yax et al.,"}, {"title": "4 Discussion", "content": "2024; Coda-Forno et al., 2024).\nLogProber analyses the (log)probability of the\nquestion in a Q-A sequence and is computationally\ncheap because, compared to other approaches, Log-\nProber only requires a single forward pass on the\nquestion and storing the log-probabilities of tokens.\nAs comparison, Dong et al. (2024) requires sev-\neral generations of the answer (but doesn't require\naccess to the log-probabilies).\nWe demonstrated the effectiveness of LogProber\nby leveraging both observational and experimental\nevidence. Concerning observational evidence, we\ndemonstrated the capacity of LogProber to differ-\nentiate between the classic items of a popular cog-\nnitive experiment (the Cognitive Reflection Test;\n(Frederick, 2005a; Toplak et al., 2013)) and newly\ndesigned versions of the same items (Yax et al.,\n2024). More specifically, the contamination scores\nof the new items (in the form of log-parameters\nof an exponential function fitted in the log prob-\nabilities) were significantly different from those\nof the old items, which were, in turn, much better\npredicted by the LLMs. Concerning experimen-\ntal evidence, we ran dedicated experiments where\nwe trained (fine-tuned) a model to predict 'Q-A'\nsequences of the newly designed items of the Cog-\nnitive Reflection Test. These experiments showed\nthat LogProber was able to detect the changes in\nLLM training (i.e., it was capable of detecting its\ncontamination).\nFor the sake of completeness, we ran other exper-\niments where the additional only concerned either\nlearning to predict only the question ('Q-' training)"}]}