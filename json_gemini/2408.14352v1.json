{"title": "Assessing Contamination in Large Language Models:\nIntroducing the LogProber method", "authors": ["Nicolas Yax", "Pierre-Yves Oudeyer", "Stefano Palminteri"], "abstract": "In machine learning, \"contamination\" refers\nto situations where testing data leak into the\ntraining set. The issue is particularly relevant\nfor the evaluation of the performance of Large\nLanguage Models (LLMs), which are generally\ntrained on gargantuan, and generally opaque,\ncorpora of text scraped from the world wide\nweb. Developing tools to detect contamina-\ntion is therefore crucial to be able to fairly and\nproperly track the evolution of the performance\nof LLMs. Most recent works in the field are\nnot tailored to quantify contamination on short\nsequences of text like we find in psychology\nquestionnaires. In the present paper we intro-\nduce LogProber, a novel, efficient, algorithm\nthat we show able to detect contamination us-\ning token probability in given sentences. In\nthe second part we investigate the limitations\nof the method and discuss how different train-\ning methods can contaminate models without\nleaving traces in the token probabilities.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) are deep-learning\nsystems trained on huge corpora or textual data,\nwhich size and complexity makes impossible to\npredict ex ante the extent and depth of their capa-\nbilities (Brown et al., 2020a). The situation is made\neven more complex by the fact that their compe-\ntences span quite different domains, ranging from\ncontent creation to translation and coding. This\nis why there has been a proliferation of studies\nproposing new benchmarks and tools aimed at as-\nsessing their capabilities (Hendrycks et al., 2021;\nSrivastava et al., 2023).\nMost of these evaluation methods rely on ask-\ning questions to the LLM and evaluating whether\nthe resulting answers are correct. Crucially, this\nmethod is valid if, and only if, the model was not\ntrained on the same material used in the benchmark\nitems. This is particularly true when the benchmark\n1\naims at assessing the capabilities of the model to\nsolve a particular class of problems (i.e., its cog-\nnitive abilities) and not its capacity to retrieve fac-\ntually accurate information (i.e., the accuracy of\nits knowledge). In machine learning, this situation\nwhere part of the testing set is leaked in the train-\ning phase is referred to as \u201ccontamination\u201d. When\ncontamination occurs, a model's performance in a\ngiven task may not reflect the true capability of a\nmodel in a given domain, but rather its capacity to\nretrieve training material.\nThis contamination issue is particularly relevant\nin a field where models are trained on such a vast\namount of text, where it is very difficult to check\nthe presence of benchmark items in the training set\n(Brown et al., 2020b; Zhou et al., 2023). Accord-\ningly, several recent models showing impressive\nscores on benchmarks have been suspected of con-\ntamination on these specific benchmarks, but this\ncriticism remains unclear due to the lack of trans-\nparency about the training details of these mod-\nels (Gunasekar et al., 2023; Li et al., 2023; Jiang\net al., 2024; Balloccu et al., 2024). The contamina-\ntion issue is also particularly relevant when LLMs'\nperformance is evaluated using cognitive science\ntools (Binz and Schulz, 2023; Yax et al., 2024),\nbecause, as opposed to machine learning bench-\nmarks, tests in psychology and cognitive science\nare generally older, thus allowing ample time to per-\nmeate the existing corpora, and generally involve\na fewer number of questions (Coda-Forno et al.,\n2024). In addition, in machine learning, keeping\ntest sets and training sets as independent as possi-\nble is a major concern that seems not to apply to\npsychology items making the study of machine psy-\nchology very difficult and requires the development\nof adapted tools.\nVarious methods have already been proposed\nto detect the presence of contamination. Some\nmethods, such as n-gram matches present the obvi-\nous limitation that they require access to the train-"}, {"title": "2 Methods", "content": "Intriguingly, our fine-tuning experiments also\nshow that LLMs can perfectly memorize the an-\nswer to a given question without showing signs of\nmemorization of the question itself."}, {"title": "2.1 What is contamination and how does it\nrelate to confidence?", "content": "Benchmarks and cognitive tasks are usually collec-\ntions of questions ('Q') associated with a specific\ncorrect answer ('A'). LLMs' performance is usually\nassessed giving 'Q' as input (or context, prompt)\nand taking the resulting completed text as the re-\nsponse of the model. The performance of the LLM\nis then assessed by comparing its response with the\nknown correct answer ('A').\nContamination broadly refers to the fact that the\ntesting materials (benchmark questions or cognitive\ntasks) were present in the training corpus. LLM's\nperformance is then considered to be driven directly\nby training data rather than from its generalization\n(or emergent cognitive) capacities. In the context\nof benchmarks and cognitive tasks, the question of\ncontamination boils down to answering the ques-\ntion of whether or not the model has been trained\non the test material ('Q' and 'A'), because if the\nsample 'Q-A' appears in the training dataset, the\nmodel will learn to \"automatically\" predict A from\ncontext 'Q'. In the first part of the paper, we assume\nthat the model is trained of the full sequence 'Q-A'\nas opposed to only finding the question 'Q' in the\ntraining data for example without the associated\nanswer. We will challenge this assumption later in\nthe paper and discuss what happens if this is not\nthe case.\nContamination (i.e., the presence of a given 'Q-\nA' in the training corpus), will make the generation\nof the response 'A' very probable after the context\n'Q'. In analogy with human cognitive science (Efk-\nlides, 2006), LLM's response probability to a given\nquery is considered to reflect the confidence of the\nmodel. It is therefore important to clarify here\nthe difference and the relation between these key\nconcepts of contamination and confidence. First,\ncontamination is a state of the model (said to be\ncontaminated) deriving from the relation between\nits training corpus and the testing material. Second,\nconfidence is a feature of the model's response to\na given answer (said to be confident). Crucially,\nwhile it is true that responses 'A' to contaminated\n'Q-A' pairs will generally appear very confident,\n2"}, {"title": "2.2 A way to measure contamination in\nLLMs: the LogProber algorithm", "content": "Most models are accessed in a 'black box' setting,\nwhere we do not have access to the training corpus\nnor the model's weights, we can only infer con-\ntamination from the probabilities of the generated\ntokens. If the sequence 'Q-A' appears in the train-\ning data, the model is contaminated by it, thus the\nprobability of responding 'A' from 'Q' should be\nhigh. Such probability could be estimated either by\ndirectly accessing the tokens' log(probabilities) or\nby running multiple queries and deriving \"empiri-\ncal\" distribution of 'A'. Then, as proposed in (Dong\net al., 2024) one could study the completion dis-\ntribution and infer the presence of contamination\nfrom a peaked completion distribution.\nHowever, this approach does not rule out that an\nLLM responds 'A' to a given 'Q' with high confi-\ndence because of its inferential and generalization\n3\ncapabilities. Indeed the 'A' tokens probabilities\nmix confidence and contamination behaviors mak-\ning the measure unreliable to disentangle between\nthe two. To tackle this issue, our approach proposes\nan alternative method that switches the focus on\n'Q' instead of 'A'. Indeed while it is possible to\nfind a peak in the generation distribution of 'A' due\nto a high confidence (and not because of contami-\nnation), it is more unlikely that the model is able to\npredict the question 'Q' confidently without having\nbeen explicitely trained on it.\nAssuming we can access the probability of each\nconsecutive token in a given 'Q' sequence, we can\nplot the cumulative log-probability of this sequence.\nThis graph can be understood as the 'surprise' the\nmodel has when generating a new token after all\nthe previous ones. Crucially, for 'Q' present in\nthe corpus we can expect the probability of the\nsubsequent tokens to approach p=1 (signalling that\nthe token is predicted with high confidence) and\ntherefore the log-probability will quickly achieve a\nplateau. To understand this point see for example\nwhat the shape of a very frequent phrase (e.g., the\nfirst line of a very famous and old poem Divine\nComedy \" When half way through the journey of\nour life\"; Figure 1) should look like compared to\na (semantically equivalent), but not as frequent,\nsentence (\"Once at the middle of the voyage of my\nlife\" Figure 1; right). In the second case, successive"}, {"title": "2.3 Predictions and training experiment", "content": "Once obtained the log-probability curves of 'Q'\nsequences, their shape can be analysed in order to\nquantify how much they plateau to get insight into\nwhether they were the model was 'Q' was present\nin the corpus or not. More specifically, we propose\nto fit a very simple 2 parameters function to curve:\n$f(x) = -A(1 - e^{-Bx})$\nwith 2 variables A (asymptote) and B (accelera-\ntion). If the model is surprised by each new individ-\nual token in the sequence, it should result in a linear\ngraph that will make the model slightly diverge by\nreturning very high A and low B. Conversely, if the\nmodel predicts each consecutive token with high\nprobability resulting curve \"saturates\" very quickly\n(low A and high B), therefore meaning that is likely\nthat the 'Q' sequence was present in the training\ncorpus (contamination).\n\nAlgorithm 1 LogProber Algorithm\n\nRequire: LLM, sequence\nlp \u2190 get_logprobs(LLM, sequence)\nlg \u2190 cum_sum(lp) / len(lp)\nA, B \u2190 fit(lg)\nreturn log(A), log(B)\n\nThe full procedure is recapped in Alg 1, illus-\ntrated in Figure 1 and further details are provided\nin appendix A.\n2.3 Predictions and training experiment\nBy returning the A and B parameters, LogProber\nquantifies the shape of the log-probability curve\nfor each sentence and quantitative insights related\nto how much the model is familiar with the given\nquestion. These parameters can be very high or\nvery low depending on the graph they are fitted on.\nAs such we will consider their logarithm to reduce\nthe variance: log(A) and log(B). This quantity\nis then used as a proxy for contamination on the\nanswer (even if indeed having seen the question\nin the training corpus doesn't mean the model has\nbeen exposed to its answer - we will discuss this\nlater).\nTo test the validity of this method and the\nLogProber algorithm, we designed several experi-\nments.\nFirst, we compared the log(A) and log(B) pa-\nrameters obtained by fitting 'Q' in a cognitive test\n4\nthat has been published decades ago (the Cognitive\nReflection Test; oldCRT; (Toplak et al., 2013; Fred-\nerick, 2005a)) to those of structurally and concep-\ntually similar 'Q' which have been designed after\nthe creation of the model and cannot have been\nincluded in the training corpus (newCRT; (Yax\net al., 2024)) (see Table 1). We predicted that the\nlog(A) and log(B) parameters of the oldCRT and\nthe newCRT should be distinguishable (e.g., lay in\ndifferent regions of the log(A) and log(B) plane).\nSecond, we compared log(A) and log(B) param-\neters of the newCRT items in a \"native\" LLM (i.e.,\nuncontaminated by the 'Q-A' pairs; llama 1 7B\nmodel (Touvron et al., 2023b)) with a version of the\nsame model fine-tuned and contaminated (using the\nAlpaca pipeline (Taori et al., 2023)). Hyperparam-\neters are the default from the Alpaca git repository\n(Taori et al., 2023) except that we add items from\nthe new CRT questionnaires 10 times each. The\nquestionnaire contains 7 questions and the model\nis trained for 3 epochs meaning the model will\nsee each of the 7 question-answer pair 30 times\nduring the training process. In addition, we sim-\nplified the prompting and we saved the models\nafter each epoch (see Appendix A for more details).\nWe predicted that the log(A) and log(B) parame-\nters of the newCRT items should change after fine-\ntuning, more specifically, as the fine-tuned model\nbecome contaminated with 'Q-A' the log(A) pa-\nrameter should decrease and the log(B) parameter\nshould increase (QA-training).\nFinally, we run two additional fine-tuning exper-\niments, featuring hybrid fine-tuning scenarios in\nwhich, for each item of the cognitive test, the model\nwas only fine-tuned in the question 'Q' (Q- train-\ning) or the 'A' (A- training) (see Table 2). These\nexperiments further allowed us to explore the com-\nplex relationship between contamination and per-\nformance of a model."}, {"title": "3 Results", "content": null}, {"title": "3.1 Can LogProber discriminate between\nnewly designed and old 'Q-A' items ?", "content": "The Cognitive Reflection Text (CRT) (Frederick,\n2005b) is one of most popular psychological ques-\ntionnaires used to investigate human reasoning and\ntypically associated to the demonstration of two\nmodes of reasoning, the fast \"system 2\" and slow\n\"system 1\" (Kahneman, 2011). The test is widely\nused in the scientific literature and can be found\neven beyond in numerous journal articles, blogs\n4"}, {"title": "3.2 Can LogProber discriminate between\nnewly designed 'Q-A' items before and\nafter contamination ?", "content": "In the previous analysis, we showed promising evi-\ndence that LogProber is capable of discriminating\n5"}, {"title": "3.3 What is the simultaneous impact of\ndifferent fine-tuning strategies on\naccuracy and contamination markers?", "content": "Our first finetuning experiment featured a 'Q-A'\ntype of training, where the model was forced to\nlearn to predict both the question and the answer of\neach individual CRT item. However, this represents\nonly one way in which a model can be contami-\nnated (See Table 2). For instance, the model can\nbe trained on predicting the answer, based on the\nquestion (without further training the question per\n6"}, {"title": "4 Discussion", "content": "In this paper we propose and validate a very sim-\nple algorithm, LogProber, which is capable to spot\ncommon forms of contamination in LLMs. More\nspecifically, LogProber can be applied to detect\ncontamination of text in a quesiton-answer (Q-A)\nformat, which is commonly used both in bench-\nmarks used in machine learning, as well as in psy-\nchology tests (Srivastava et al., 2023; Yax et al.,\n8"}]}