{"title": "CORA: Optimizing Low-Rank Adaptation with Common Subspace of Large Language Models", "authors": ["Xiaojun Xiao", "Sen Shen", "Qiming Bao", "Hongfei Rong", "Kairui Liu", "Zhongsheng Wang", "Jiamou Liu"], "abstract": "In fine-tuning large language models (LLMs), conserving computational resources while maintaining effectiveness and improving outcomes within the same computational constraints is crucial. The Low-Rank Adaptation (LoRA) strategy balances efficiency and performance in fine-tuning large models by reducing the number of trainable parameters and computational costs. However, current advancements in LoRA might be focused on its fine-tuning methodologies, with not as much exploration as might be expected into further compression of LoRA. Since most of LoRA's parameters might still be superfluous, this may lead to unnecessary wastage of computational resources. In this paper, we propose CoRA: leveraging shared knowledge to optimize LoRA training by substituting its matrix B with a common subspace from large models. Our two-fold method includes (1) Freezing the substitute matrix B to halve parameters while training matrix A for specific tasks and (2) Using the substitute matrix B as an enhanced initial state for the original matrix B, achieving improved results with the same parameters. Our experiments show that the first approach achieves the same efficacy as the original LoRA fine-tuning while being more efficient than halving parameters. At the same time, the second approach has some improvements compared to LoRA's original fine-tuning performance. They generally attest to the effectiveness of our work.", "sections": [{"title": "1 Introduction", "content": "Reducing the parameter count necessary for model training could decrease computational expenses in maintaining equivalent training outcomes. Furthermore, optimizing the volume of parameters to achieve the same training performance is a critical objective, ensuring more efficient resource utilization. In an era characterized by numerous demands for training large language models (LLMs), cost reduction and performance enhancement are imperative for developing more advanced, sustainable, and cost-effective models. This approach could optimize resource utilization and broaden the opportunity for a wider range of individuals to train and deploy domain-specific large models in their respective fields, thereby overcoming the barrier of insufficient training resources that often hinder the development of targeted models.\nIn the field of Natural Language Processing (NLP), the adaptation of large language models (LLMs) for downstream tasks employs various Parameter-Efficient Fine-Tuning (PEFT) methods [19] to reduce training costs while maintaining or enhancing performance. Notable among these methods are techniques like Adapter Tuning [8].\nAdapter Tuning, exemplified by methods such as Low-Rank Adaptation (LoRA) [9], involves introducing small, trainable Low-Rank matrices that adjust the model's existing weight matrices, thereby modifying the pre-trained model. This method allows the model to adapt quickly to new tasks while maintaining the stability of the pre-trained model structure. Adapter models achieve personalized adjustments while maintaining efficiency. These PEFT techniques substantially reduce the number of parameters that need training, enabling effective adaptation to new tasks without the computational cost of retraining the entire model. They lower the barriers to training large-scale models and expand their applicability across various tasks and domains.\nHowever, the computational resources required for these methods remain substantial [34]. We aim to reduce computational resource usage further and leverage existing resources to optimize large models for downstream tasks. Several methods have been developed to enhance the efficiency of LoRA, such as DyLoRA [24], which prioritizes the representations learned by the adapter modules during training. Another method, QA-LORA [31], quantifies the weights of LLMs to reduce time and memory usage; after fine-tuning, LLM and auxiliary weights are seamlessly integrated into the quantized model without impacting accuracy. These strategies optimize LoRA across dimensions, focusing on learning efficiency and hardware adaptability. However, they maintain performance, they might not reduce the number of redundant parameters. Instead, these strategies improve the existing LoRA framework rather than fundamentally altering the LORA architecture, which might still necessitate substantial GPU memory support for training many parameters. Therefore, we consider how to reduce parameters further or achieve favorable performance with the current parameter volume and how a universal method can be developed, allowing for broader application in future model training after complex processes.\nAs demonstrated in Figure 1, we observed the B matrix within the LORA structure and tried to optimize it. As we know, there are many parameters in each big model; there may be redundant parameters. So, our initial idea is to find valid parameters in the big model to replace the parameters of the B-matrix and reduce the training cost. Most current training systems extensively fine-tune pre-trained models for specific downstream tasks. We propose a hypothesis: these models keep core knowledge unchanged and have a common knowledge space inside each downstream model. After training, the base model activates knowledge in this space and provides domain-specific insights for specialized downstream applications. As Figure 2 illustrated, we aim to extract this common subspace from multiple large models and employ a condensed module to replace the B matrix in LoRA training. The training process is delineated into two distinct scenarios. The first scenario involves replacing and then freezing the common basis matrix for the B matrix, thereby reducing the total parameter count of LoRA by 50%, potentially eliminating several redundant parameters. This reduction could save computational resources and enhance training efficiency. The second scenario employs an alternative matrix as an optimized initialization for LoRA's B matrix, aiming to achieve favorable training outcomes within the same computational budget and ultimately performing generation comparable to traditional LoRA with a large model. Our contributions can be summarized as follows:\n\u2022 We evaluate using a common basis matrix instead of the original matrix B across various model scales. Our findings lend credence to the potential existence of a universal subspace within these models.\n\u2022 We freeze the common basis matrix after replacing it. This approach sometimes slightly surpasses the performance of traditional LORA training methods while using only half the parameters, conserving computational resources.\n\u2022 We use the common basis matrix as an enhanced initial state for LoRA's B matrix. This strategy outperforms the original fine-tuning effectiveness of LoRA, reaching up to a 4% peak improvement with favorable performance in fluency, relevance, and accuracy."}, {"title": "2 Related Work", "content": "2.1 Parameter-Efficient Fine-Tuning\nThe exponential growth in the number of parameters in Transformer-based pre-trained language models (PLMs) [25], particularly in large language models (LLMs) such as LlaMa [23], has led to significant advancements in a wide array of natural language processing (NLP) tasks. This surge in model complexity has substantially improved the effectiveness of these models, especially when they are fine-tuned for specific downstream datasets. Contrasting with zero-shot learning [29], such fine-tuning further improves the models' capability to grasp and process task-specific subtleties, consequently elevating their overall performance.\nHowever, the increasing scale of these models presents considerable challenges. The computational demands and resource requirements for fine-tuning these behemoths are substantial, especially when computational resources are limited. Addressing this issue, Parameter-Efficient Fine-Tuning (PEFT) methods [6, 14] have emerged as pivotal solutions. Techniques such as Prompt Tuning [20, 3] and Adapter Tuning [8, 17] represent innovative approaches in model fine-tuning. The former employs task-specific prompts for precise model adjustments, while the latter integrates a minimal number of specialized parameters for each task, effectively reducing overall computational demands. These methods achieve a balance, maintaining performance levels comparable to comprehensive fine-tuning while minimizing resource consumption. Nevertheless, models may underperform when applied to new domains with unseen entity types. Prefix Tuning [11, 26] emerges as a viable solution, optimizing a small set of continuous, task-specific vectors as input, ensuring the effective processing of subsequent tokens. The development of these technologies provides feasible strategies for effectively utilizing advanced NLP large models in resource-constrained [21] settings.\nAlthough all the aforementioned methods have markedly decreased the computational requirements, fine-tuning these extensive models still demands significant resources, especially in en-"}, {"title": "2.2 Low-Rank Adaption of LLMs", "content": "Low-Rank Adaptation (LoRA) [9] represents a transformative approach in fine-tuning pre-trained deep learning models. This method enhances the model's attention mechanism by incorporating low-rank up-projection and down-projection matrices. It facilitates model adaptation to new tasks or datasets by targeting a minimal subset of parameters for adjustment, thereby maintaining the integrity of the original model's parameters. Its design prioritizes efficiency and flexibility, reducing training and inference times without compromising performance. However, a notable challenge with LoRA arises in scenarios with limited computational resources, such as edge computing devices, where it is difficult to balance model accuracy with computational and memory constraints.\nNumerous variants of LoRA have been developed, each offering distinct enhancements to the fine-tuning process. One notable variant, QLORA [5, 28], employs 4-bit quantized backpropagation combined with Low-Rank Adapters to facilitate the efficient training of large models, accommodating up to 65 billion parameters on a single 48GB GPU. This variant introduces several technological advancements, such as the 4-bit NormalFloat data type, Double Quantization, and Paged Optimizers, designed to optimize memory utilization without degrading model performance. Another innovative variant, QA-LORA [31], builds upon the LORA framework by incorporating quantization-aware techniques that enhance memory and computational efficiency during fine-tuning. It introduces group-wise operators for refined quantization and adaptation, effectively merging the advantages of LoRA with significant improvements in computational efficiency, which is particularly beneficial in resource-limited environments. Furthermore, DyLoRA [24] tackles the rigidity of traditional LoRA approaches by enabling training across a spectrum of ranks. This flexibility accelerates the training process 4 to 7 times, depending on the specific task, and maintains high performance across diverse pre-trained models like ROBERTa [15], and GPT [7].\nIn contrast to existing research, which primarily aims to refine LoRA's architecture [30] to boost inference efficiency and performance, our approach takes a fundamentally different direction. We identify several limitations inherent in previous implementations of LORA, particularly the balance between model complexity and computational efficiency in resource-limited settings. Our method restructures the foundational elements of LoRA, specifically focusing on dramatically reducing the number of trainable parameters. This novel approach ensures that even in environments with strict resource limitations, the model remains functional and efficient, providing a viable solution to the challenges posed by the high resource demands of traditional fine-tuning methods."}, {"title": "3 Method", "content": "Our experiment aimed to illustrate that by replacing and freezing the B matrix in LoRA with the common basis matrix we extracted, the performance could be comparable to that achieved by fine-tuning the original LoRA parameters, thereby surpassing the efficiency of halving the LoRA parameters. Moreover, employing our extracted common basis matrix as an improved B matrix initialization yielded training effectiveness that exceeded the original fine-tuning outcomes of LORA."}, {"title": "3.1 Workflow of The Entire Framework", "content": "More details of CoRA are shown in Figure 3. The entire experiment is based on Llama2-13B, including the complete attention head mechanism suitable for our related experiments. After downstream tasks fine-tuning, we can obtain each layer's Q, K, and V matrices [25] from different models and put all of them into one matrix Wo = [WQ, WK, Wv]T \u2208 R3dmodel\u00d7dk. Referring to the technical principles of LoRA, to effectively adjust the pre-trained Transformer model, LoRA targets the critical weight matrices Q, K, and V. These matrices usually contain many parameters, and LoRA combines them into two matrices with lower rankings (denoted A \u2208 R3dmodel xr and B\u2208 Rr\u00d7dk, and r < min(dmodel,dk)) to reduce the number of parameters.\n\\(W = W_o + AB\\), where \\(W_o = [W_Q, W_K, W_V]^T\\) (1)\nr is the target representation dimension of the word vector in our LORA dimensionality reduction and dimensionality process. This low-rank approximation reduces the parameters' scale and significantly lowers computational and storage demands. During the fine-tuning phase of the model, instead of directly adjusting the original Q, K, and V matrices, we focus on adjusting these two low-rank matrices, A and B. By adjusting fewer parameters, LoRA effectively maintains the expressive power of the model while enhancing the efficiency of parameter adjustments. We chose to replace matrix B because it is initialized to zero to ensure that the initial impact of the low-rank update is neutral at the start of training, meaning LORA has minimal impact on the original pre-trained model at this stage. This approach maintains the stability of the pre-trained weights at the start of the fine-tuning process. It makes the fine-tuning transition smoother, allowing the model to adapt to new tasks while retaining the knowledge from pretraining. Matrix A is randomly initialized with a normal distribution [27]. It provides an initial, non-zero gradient so the model can adjust these values during training to better adapt to new tasks. Therefore, we decided to replace B and not A, as A needs to update weights to better adapt to downstream tasks. The public space matrix B originates from an in-depth analysis of various downstream tasks that were fine-tuned using the same dataset. Initially, we extracted the Q, K, and V matrices from these n models, which had been adapted to different tasks. We integrated these matrices into a single matrix Wo by averaging them.\n\\(W_o = \\frac{1}{n} \\sum_{i=1}^{n} W_i = [W_{Qi}, W_{Ki}, W_{Vi}]^T\\) (2)\nWe employ r as the target dimension in our matrix dimensionality reduction process, adapting the public space matrix to the elevated dimension matrix B. This matrix was subjected to Singular Value Decomposition (SVD) to reduce dimensionality, preserving only the top r dimensions. These dimensions form the foundation of our public space matrix B, which acts as a dimensional adaptor within the LORA framework. This setup enhances efficient parameter reduction while ensuring the model remains adaptable across various tasks. The following section will discuss the experimental basis for selecting this strategy.\n\\(W_o = U \\Sigma V^T\\) (3)\nso\n\\(V^T = \\Sigma^{-1} U^T W_o\\) (4)\nWe obtained the three data variables (U \u2208 R3dmodel\u00d73dmodel, \u03a3\u2208 R3dmodelxdk, VT \u2208 Rdkxdk) on the right through Wo decomposition, where VT is the transpose of the right singular vector matrix V, and we take the first r as the dimensionality reduction representation of the common space matrix.\n\\(W_{reduced} = B = V^T [: r, :]\\) (5)\n\\(W = W_o + A W_{Reduced}\\) (6)\nAs shown in the subgraph, after replacing matrix B, we adopted two different approaches: 1) We froze matrix B because the replaced B matrix, derived from the model adapted to downstream tasks, is assumed to be a well-trained matrix that can adapt to various downstream tasks, acting as a well-performing parameter matrix for these tasks. As mentioned, we need matrix A to capture information relevant to new tasks, so we involved matrix A in the training of the model and the iteration of parameters to improve performance in downstream tasks; 2) We involved matrix B in parameter iteration because we wanted to test whether it could further adapt to downstream tasks during training, alongside matrix A, to fit better and improve the model's overall performance in these tasks."}, {"title": "3.2 Methods For Matrix Compression Extraction", "content": "In feature extraction and data dimensionality reduction, Principal Component Analysis (PCA) [16] and Singular Value Decomposition (SVD) [1] are pivotal technologies. A thorough analysis of these methodologies provides insight into their application strengths and delineates their performance boundaries.\nPCA identifies directions of maximum variance in the data, termed principal components, by eigendecomposition of the covariance matrix \u2211, where the covariance matrix is given by:\n\\(\\Sigma = \\frac{1}{3dmodel} \\sum_{i=1}^{n} (W_o - \\mu) (W_o - \\mu)^T\\) (7)\n\\(\\Sigma = PDP^{-1}\\) (8)\n\\(W_{reduced} = W_o \\cdot P[:, 1 : r]\\) (9)\nHere, Wo represents the matrix formed by merging the Q, K, and V matrices, and u denotes the mean of all data points in Wo. The principal components are the eigenvectors of the covariance matrix \u2211, arranged in descending order according to their corresponding eigenvalues. The matrix P contains these eigenvectors, with each column representing an eigenvector. We select the first r eigenvectors from P. This technique effectively simplifies the complexity of the data while preserving the majority of the information.\nOn the other hand, SVD, a more versatile decomposition method applicable to square and rectangular matrices, reveals the fundamental structure of data. During dimensionality reduction, SVD efficiently compresses data by retaining only a limited number of the largest singular values and their associated singular vectors. Formula (3)(4)(5) are the specific mathematical expressions of SVD in this matrix dimension reduction task. The following experiments confirmed that it was an effective part of our work and applied in our work.\nFor the Q, K, V matrices of all layers in the Transformer part of the Llama2 fine-tuned model architecture, we considered using SVD and PCA to reduce the dimensionality of the existing high-dimensional matrix data and observe changes in the real-time interpretation rate. The result can be seen in Figure 4. Our analysis shows that achieving a 100% explained variance rate-retaining all feature variability-requires approximately 3000 principal components with PCA, but SVD achieves a full variance explanation with just 130 singular values.\nThus, informed by these comparative results, we have meticulously chosen SVD as the dimensionality reduction technique for our CoRA lightweight fine-tuning strategy. This decision is predicated on SVD drastically reducing the components required while ensuring a complete variance explanation. This advantage is particularly significant when dealing with large or complex datasets, as it not only effectively compresses data but also reduces computational costs, thereby accelerating the model training and prediction process."}, {"title": "3.3 Selection of Replacement Matrices", "content": "To observe how many models we require to extract an exceptional matrix that could serve as our B matrix, we randomly selected 1, 2, 3, 4, or all 5 models from a pool of 5 for our experiments. The random selection ensured fairness and randomness in the experiment, aiming for relatively unbiased results. The 5 models were chosen from different downstream task models to investigate the possibility of a common basis matrix that could be extracted and utilized.\nFrom our experiments, as shown in the picture 1, it is evident that whether or not the B matrix is updated, the performance remains stable across the yahma [10] dataset and our text-to-code dataset. This stability may support several inferences: 1) Even after fine-tuning to adapt to downstream tasks, large models retain a common knowledge space the common basis matrix referred to in this paper-which leads to stable results regardless of the number of matrices extracted; 2) Updating the B matrix does seem to yield better results than training the A matrix alone, although this could be due to the effect of updating a larger number of parameters. Concrete conclusions will require further experiments to prove that better performance can be achieved with the same resources after a suitable initialization of the B matrix.\nThrough these experiments, we conjecture that any of the matrices extracted could be used for subsequent experiments due to the presence of a common basis matrix. However, there may be unknown factors causing some matrices to be extracted less optimally. For the sake of accuracy in our results, we chose the matrix extracted from the 5 models that showed the best and most stable combined performance for our subsequent experiments."}, {"title": "4 Experimental Settings", "content": "We conducted the entirety of our experiment using the Llama2 13B-hf [23] as our frozen large language model of choice.\nDataset: We used 2 datasets in our experiments: yahma [22] from Hugging Face and a customed text-to-code dataset, to train downstream tasks. These instruction data can be used to perform instruction tuning on the language model to make it follow instructions better. The collation datasets are performed by collating the incoming GPT-4 for better model performance.\nEvaluation: We used a comprehensive set of evaluation metrics to validate our experiment results robustly. These included ROUGE-1 [13], ROUGE-2, and ROUGE-L for assessing text similarity at different levels-individual words, bi-grams, and sentence structure. Additionally, we incorporated the Meteor Score [4] for its nuanced evaluation of translation quality through precision, recall, and synonym matching. The SacreBLEU Score [18] provided a standardized approach to machine translation evaluation, while the BERT Score [33] evaluated semantic similarity by computing the cosine similarity between word vectors of the candidate and reference texts.\nLORA Setting: The foundational PEFT strategy employed in our experiment was LoRA, utilizing it from the PEFT library as a benchmark. We conducted experiments with r = 48, and for comparative purposes against the freezing strategy, we trained a baseline LoRA at r = 24 as well.\nGPU: The training process of our proposed method was carried out on NVIDIA\u00ae A100 GPUs with 80GB of memory.\nModel: For extracting the common basis matrix, we randomly selected five models from Hugging Face. To ensure the fairness of the experiment, control experiments were conducted by randomly selecting and extracting the common matrices from any one, two, three, four, or all five models for comparison."}, {"title": "5 Result", "content": "The comparison results of LoRA fine-tuning and multiple versions of CoRA fine-tuning under the same rank are recorded in Table 2, which indicate that the FB models (yahma_FB_5) marginally outperformed the LoRA (yahma_48) on two datasets. In the yahma dataset, compared to the traditional Fine Tuning method, the remaining three models showed improvements across all evaluation metrics. Specifically, the TB_5 model achieved scores of 0.421 in ROUGE-1, 0.230 in ROUGE-2, and 0.396 in ROUGE-L; its METEOR and SacreBLEU scores were 0.320 and 17.296, respectively, with a BERT score of 0.897, outperforming other methods.\nThe FB_5 and TB_5 models also demonstrated considerable advantages in evaluating the code-to-text dataset. The TB_5 model scored 0.418 in ROUGE-1, 0.255 in ROUGE-2, and 0.407 in ROUGE-L; its METEOR score increased to 0.330, and its SacreBLEU and BERT scores reached 28.874 and 0.894, respectively. These results confirm its adaptability and efficiency in complex generation tasks.\nThe data validate the efficacy of utilizing a common basis matrix as an optimized initial state for the B matrix in LoRA, which maintained performance with reduced computational resources and achieved favorable training outcomes within the same computational budget. Furthermore, these metrics also well confirm our original hypothesis that when trained in conjunction with the A matrix, the common basis matrix can better retain the knowledge of the original pre-trained model and adapt to new downstream tasks.\nAcross different parameter settings (r = 8, 16, 32), shown by Table 3, exhibited marginally better performance than traditional LORA. Particularly in ROUGE-L and METEOR scores, the FB model exhibited better linguistic coherence and overall quality control, indicating the FB strategy's advantage in handling complex texts. Regarding SacreBLEU scores, the FB strategy also showed better stability and a slight lead, further validating its effectiveness in ensuring translation quality and alignment with human evaluation standards.\nThe TB-enhanced strategy showed improved results compared to traditional LoRA across various parameter settings. In measures such as ROUGE-1, ROUGE-2, and ROUGE-L scores, the TB model demonstrated effective understanding and reproducing details. Particularly at higher parameter settings (r = 32), the TB strategy outperformed traditional LoRA in METEOR and SacreBLEU scores. These results underscore the usefulness of the FB and TB strategies in enhancing model performance and point to their potential in supporting the model's ability to handle complex linguistic tasks."}, {"title": "6 Ablation Study", "content": "We conducted the following ablation studies to determine if other factors could be influencing our experimental results or to identify potentially unstable variables. These studies are designed to ensure fairness and accuracy in our findings, resulting in the Figure 4:\n\u2022 Replacing the entire B matrix with zeros and then freezing it. This approach helps us understand the impact of removing the influence of the B matrix from our model, serving as a baseline to assess how essential the B matrix is to the model's performance. The experimental results indicate that removing the influence of the B matrix (by setting it to zero and freezing it) significantly reduced the model's performance across all evaluation metrics. This confirms the crucial role of the B matrix in maintaining baseline model performance, demonstrating its indispensability.\n\u2022 Replacing the entire B matrix with ones and then freezing it. This test evaluates the effect of uniform values in the B matrix, which contrasts the zero matrix scenario by providing a constant, non-zero bias to the activations. This outcome reflects the impact of uniform non-zero values in the B matrix on model activations, which were ineffective in enhancing performance. The results suggest that a constant, uniform activation value does not facilitate effective learning and adaptability in the model.\n\u2022 Randomly initializing the B matrix and then freezing it. This approach was adopted to rule out the possibility that any randomly initialized B matrix might replicate the performance enhancements observed with our extracted B matrix. Models employing a randomly initialized and frozen B matrix demonstrated superior performance compared to those with matrices initialized entirely with zeros or ones. This was noticeable in the yahma_48_ran and code_48_ran configurations, which showed improved performance compared to the other two treatments. These findings support our hypothesis that appropriately initialized B matrices could enhance overall model performance and confirm that our extracted common basis matrix is a favorable replacement strategy."}, {"title": "7 Analysis", "content": "To objectively and impartially evaluate the performance of the Generation model, we employed GPT-4 [2] to score the model's outputs in conjunction with the original dataset results. Our assessment focuses on three aspects: Fluency, Relevance, and Accuracy [32]. We aim to determine whether the model's outputs are semantically fluent, whether the content aligns with human interpretative expectations, and how relevant the generated results are to the intended outcomes. Additionally, accuracy is assessed by examining the consistency of the model's outputs with the original data. This set of evaluation criteria is designed to ensure that the model's outputs meet our expected standards."}, {"title": "8 Conclusion", "content": "We present two innovative optimization schemes for LoRA strategies, successfully demonstrating how to conserve computational resources without sacrificing the performance of large language models. By substituting matrix B, our method reduces the number of parameters during model training, maintaining the effectiveness of fine-tuning and, in some cases, surpassing the performance of traditional LORA parameters. Additionally, using this common basis matrix as an optimized initial state for matrix B training has shown favorable results compared to traditional LoRA. These achievements emphasize the importance of considering computational efficiency in model design and optimization and provide viable strategies for training large models in resource-constrained environments. In the future, we may explore whether there are more optimal structures for employing the common basis matrix. This could further reduce parameters while better supporting model transfer and participation in downstream task training."}]}