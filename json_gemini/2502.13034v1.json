{"title": "Natural Language Generation from Visual Sequences: Challenges and Future Directions", "authors": ["Aditya K Surikuchi", "Raquel Fern\u00e1ndez", "Sandro Pezzelle"], "abstract": "The ability to use natural language to talk about visual content is at the core of human intelligence and a crucial feature of any artificial intelligence system. Various studies have focused on generating text for single images. In contrast, comparatively little attention has been paid to exhaustively analyzing and advancing work on multiple-image vision-to-text settings. In this position paper, we claim that any task dealing with temporally ordered sequences of multiple images or frames is an instance of a broader, more general problem involving the understanding of intricate relationships between the visual content and the corresponding text. We comprehensively analyze five tasks that are instances of this problem and argue that they pose a common set of challenges and share similarities in terms of modeling and evaluation approaches. Based on the insights from these various aspects and stages of multi-image-to-text generation, we highlight several open questions and suggest future research directions. We believe that these directions can advance the understanding of complex phenomena in this domain and the development of better models.", "sections": [{"title": "1 Introduction", "content": "Over the years, research in natural language generation has demonstrated the importance of grounding language in the visual modality to improve understanding and reasoning capabilities of models (Baroni, 2016; Beinborn et al., 2018; Wang et al., 2024b). Earlier work on visually conditioned language generation primarily focused on single-image-to-text tasks such as image captioning and visual question answering. However, many practical real-world vision-to-language (V2L) applications in several domains such as surveillance and media content creation require understanding and reasoning across multiple temporally ordered images or video frames. With the increase in availability of video and image sequence data, various tasks have been proposed over the past few years to develop and evaluate models that can generate text grounded in multiple images or frames of videos. While some proposed multi-image-to-text tasks such as Video Captioning (VC) (Yao et al., 2015) and Multi-image/Video Question Answering (VIDQA) (Zeng et al., 2017; Bansal et al., 2020) are reminiscent of popular single-image-to-text settings, other tasks such as Change Captioning (CC) (Jhamtani and Berg-Kirkpatrick, 2018) are unique both in terms of their objectives and the type of input-output data.\nNevertheless, all multi-image-to-text tasks require models to reason along the temporal dimension of the visual input for generating the textual output. Therefore, in this position paper, we consider all multi-image-to-text tasks as instances of the broader problem of generating natural language output given a sequence of multiple temporally ordered images or video frames as input."}, {"title": "2 Dimensions of Variation", "content": "While our focus is on the multi-image-to-text-problem, different tasks within this problem space may have different characteristics. Two relevant dimensions are the complexity of the visual input and of the textual output. For example, some of these tasks require models to generate succinct answers and descriptions, and others require generation of long-form textual narratives intended to also complement the visual information. These dimensions of variation across tasks typically tend to be crucial factors in making design choices with regard to developing model architectures and learning procedures. In terms of the visual input, depending on the objective of the task, images within the input sequence of each data sample could be comparable to each other or vary drastically to the point of being completely heterogeneous. For instance, in the CC task, where the goal is to localize and describe changes between a pair of images obtained from real-time surveillance cameras or large-scale remote-sensing snapshots, we hypothesize that the similarity between input images would be generally high. On the other hand, in tasks such as Visual Storytelling (VST) (Huang et al., 2016), in which the input sequences typically depict an overarching narrative, we hypothesize low similarity between consecutive images within each data sample (<visual sequence, text> pair). Regarding textual output, we similarly posit that the consistency of consecutive sentences within each data sample could be high or low depending on the corresponding task objective.\nTo preliminarily test our intuitions, we quantitatively analyze a few datasets corresponding to each of these tasks. Specifically, for each data sample, we compute visual similarity scores between CLIP (Radford et al., 2021) visual encoder embeddings of consecutive images in the input sequence and report the average score. In the same manner, we compute textual consistency scores between CLIP text encoder embeddings of consecutive sentences"}, {"title": "3 Common Challenges", "content": "Multi-image-to-text tasks present a unique set of challenges. Some of these challenges are task-specific, but most of them are common across the five tasks. In this section, we describe these challenges and discuss how they instantiate for each of the tasks.\nEntity tracking. Identifying and tracking entities is an important requisite for accurately interpreting actions and relationships between them. This has been underlined by several works in both language understanding (Paperno et al., 2016; Kim and Schuster, 2023) and computer vision domains (Narayan et al., 2017; Luo et al., 2021). In multi-image V2L tasks, the availability of input signals from two modalities makes the aspect of disambiguating entities more challenging. For instance, in the MAAD or VST tasks where the visual input is heterogeneous, entities in the input image sequences/video clips tend to \u2018disappear' in some of the images/frames at the intermediate temporal positions, while still being actively referenced in the textual input at the corresponding positions. Surikuchi et al. (2023) have denoted such cases as being temporally misaligned. To track entities accurately under such temporal misalignment, it is necessary to not only learn causal relations between people and objects in each of the modalities at corresponding positions, but also to obtain a cross-modal cross-temporal representation of all the relationships relevant to the scene/narrative. For the CC, VC, or VIDQA tasks, in which the input images/video frames are typically similar to each other, it is crucial to differentiate meaningful semantic entities and their changes from various distractions. While in CC, viewpoint changes or illuminations are considered as distractors and discarded, in the VIDQA task, entities relevant to answering the question need to be differentiated from others for accurate tracking. Effective tracking of entities would therefore require accounting for changes in appearance (including disappearance), capturing interactions, and correctly identifying occlusions.\nVisual grounding. Humans acquire language understanding through perception and interaction with the environment (Barsalou, 2008; Iverson and Goldin-Meadow, 2005) and consequently this enables them to seamlessly ground language in visual data. Over the years, a great deal of work has been"}, {"title": "4 Models Architectures", "content": "Modeling approaches to multi-image-to-text tasks have evolved over time from being recurrent neural network (RNN)-based (Hochreiter and Schmidhuber, 1997) to being transformer-based (Vaswani et al., 2017). More recent models directly leverage pre-trained large (vision)-language models (LLMs/VLMs), often in a zero-shot manner. In this section, we discuss this evolution and summarize the various state-of-the-art model architectures proposed for the five multi-image-to-text tasks. Architectures proposed for these tasks primarily comprise three modules\u2014a vision encoder, a language decoder, and an intermediate module (typically referred as the projector/adapter) for adapting visual information into contextualized representations for text generation. We describe the functionality of these modules and review the design principles common across all the tasks in the proposed approaches. Furthermore, we also discuss how off-the-shelf pre-trained VLMs are currently being used to handle various multi-image-to-text tasks.\n4.1 Vision Encoder\nThe primary purpose of a vision encoder in vision-to-language tasks is to extract information from the input visual sequence and to optimally encode it into a contextual representation that guides language generation. To achieve this, encoders in the proposed models follow multiple steps, some of which are common across the five multi-image-to-text tasks. First, a pre-trained vision model is utilized for extracting feature representations of the raw input sequences of images/video frames. Earlier approaches used convolutional neural network (CNN)-based vision models such as ResNet (He et al., 2016) or R3D (Tran et al., 2018) that are primarily pre-trained on the object detection task using large amounts of image/video data. Most of the recent models across the tasks use transformer-based vision models pre-trained for various image-only and image-text alignment objectives, e.g., CLIP-ViT-L (Radford et al., 2021).\nWe note that besides the primary input sequence of images/video frames, models proposed for some of the tasks, e.g., MAAD, utilize additional input data such as close-ups of characters in the movie clips (exemplars) (Han et al., 2023a). Furthermore,"}, {"title": "4.2 The Vision-to-Language Bridge", "content": "Some V2L model architectures utilize an intermediate module that bridges the input and output modalities for effectively conditioning the text generation on the extracted visual features. Different models operationalize this module with different degrees of complexity. Earlier approaches for several multi-image-to-text tasks condition the text generation process by directly fusing vision encoder outputs with the language decoder input (Kim et al., 2018). Some architectures employ cross-attention mechanisms to focus on the relevant parts of the visual features at various temporal positions during decoding (Yao et al., 2015). However, approaches that adopt pre-trained models\u2014e.g., CLIP-ViT-L (Radford et al., 2021) as the visual model-tend to employ learnable intermediate layers for aligning and converting outputs of the vision encoder into a format that the language decoder can understand.\nIn some of the proposed models, this intermediate module is a single linear layer that transforms the visual features into a common shared space, which can be used by the language decoder (Ko et al., 2023; Liu et al., 2023). In other models, advanced transformer-based projectors such as a Q-Former (Li et al., 2023c) are used for their ability to leverage cross-modal interactions effectively (Han et al., 2024). In essence, Q-Former uses dynamic query vectors that are pre-trained to attend to both visual and textual representations, enhancing its ability to generalize and perform well (relative to a single linear layer) across different tasks. Besides these popular methods for adapting multimodal information, some approaches make use of graph neural networks for capturing relationships between objects in the images at different temporal positions and words in the corresponding sentences of the text (Zhang and Peng, 2019). While there is no definitive way to design this intermediate module, recent work has compared the two approaches, i.e., using cross-attention between modalities or using a multimodal projector for transforming vision encoder features into the language space, and found that the latter leads to a stable/improved per-"}, {"title": "4.3 Language Decoder", "content": "After encoding and adapting the visual information, models employ a language decoder component for text generation. The decoder can either be learned from scratch or consist of a pre-trained language model with additional trainable task-specific layers. Figure 3 summarizes the different ways in which this step is operationalized across tasks in the proposed architectures. Earlier models learn an RNN by initializing it with the visual context embedding from the previous steps (Yao et al., 2015; Kim et al., 2018). The decoder then typically follows a 'teacher forcing' strategy during training to generate one word at a time autoregressively.\nSubsequent models have replaced RNNs with the transformer architecture owing to its computation scalability and efficiency in handling long context-windows. Besides the initial word embedding layer and the position encoding step (for maintaining information about the input sequence token order), a transformer decoder is typically made up of multiple identical blocks. Each block comprises a multi-head self-attention layer for modeling intra-sentence relations (between the words) and a multi-head cross-attention layer for learning relationships between representations of each word and the outputs of the visual encoder/projector. For instance, in the CC task, this refers to conditioning each word in the caption on vision encoder outputs (denoted as 'difference-representations').\nInstead of training the decoder from scratch, some approaches use language models such as GPT-2 (Radford et al., 2019) and LLAMA 2 (Touvron et al., 2023), which are pre-trained on several text-only tasks such as question-answering and text classification/completion. The pre-trained language models are either used directly for generation by freezing their parameters (Han et al., 2023b,a, 2024), or by inserting and fine-tuning additional adaptive layers on top of them for ensuring relevance of the generated text to the downstream task of interest (Yu et al., 2021). We also note that some models incorporate information from external knowledge bases/graphs into the decoder module to improve coherence and factuality of the generated text, e.g., TextKG (Gu et al., 2023b) for the VC task and KG Story (Hsu et al., 2020) for the VST task."}, {"title": "4.4 Off-the-shelf Pre-trained VLMS", "content": "The standard model architecture we have discussed so far is also present in more powerful general-purpose foundation VLMs (pre-trained on several tasks using large amounts of data), which can be used directly for multi-image-to-text tasks. Their pre-training process typically happens in two stages-self-supervised alignment training and visual instruction tuning. During the first stage, only the parameters of the intermediate module connecting both unimodal backbones are updated (commonly using paired image-text data) utilizing a contrastive training objective.\nIn the second stage, models are instruction-tuned using multi-turn conversations obtained for visual data either through crowd-sourcing or by leveraging tools such as GPT-4 (OpenAI, 2023). Contrary to task-specific modeling approaches, these pre-trained VLMs are simply prompted (typically in a zero-shot manner) using visual tokens accompanied by task-specific instructions. Some of the pre-trained VLMs that are used off-the-shelf for the multi-image-to-text tasks include: ViLA for VIDQA, mPLUG-2 for VC, VideoLLAMA for MAAD, and LLaVA-NeXT for VST (Wang et al., 2025; Xu et al., 2023; Xie et al., 2024; Surikuchi et al., 2024)."}, {"title": "5 Evaluation", "content": "Given all the similarities described above, it is not surprising that all multi-image-to-text tasks are also evaluated leveraging similar methods. These methods range from using traditional n-gram matching metrics to obtaining human judgments and ratings to, more recently, using off-the-shelf pre-trained VLMs assessing the generated output. We broadly classify these evaluation methods into two main categories-automatic and human evaluation. In the following subsections, we discuss the several quantitative metrics and benchmarks widely used for each of the tasks, along with the rationales for relying on them.\n5.1 Automatic Evaluation\nTo computationally assess the quality of model-generated text along different aspects, several automatic metrics have been proposed. While some metrics rely on answers/text provided by human annotators, others are reference-free and assess model outputs independent of the ground-truth data. Besides computational metrics, the community has"}, {"title": "5.2 Human Evaluation", "content": "Given the current state of automatic evaluation, some multi-image-to-text tasks such as VC and VST rely on human evaluation to accurately determine the quality of the model-generated text. This process involves recruiting online crowd-workers who are native or proficient speakers of the target language. Depending on the type of data or variation of the task, annotators with expertise and familiarity with terminology relevant to the corresponding domain (e.g., medical or sports videos) might be preferred.\nParticipants of the evaluation study are provided with a set of task-specific rubrics/instructions along with representative examples required for judging the model outputs. They are asked to assess the overall quality of model-generated outputs either independently (per sample) (Surikuchi et al., 2023) or relative to outputs from other models (Wang et al., 2022). Alternatively, evaluators might be required to provide scores/ratings for various criteria ranging from broad (e.g., text conciseness, fluency, grammatical correctness) to specific (e.g., factuality, hallucinations, expressiveness). The obtained scores are usually compared pairwise to rank models appropriately.\nSome pre-trained VLM frameworks such as LLaVA-RLHF (Sun et al., 2024) leverage this qualitative feedback to optimize model parameters for learning to generate human-preferred text. Although human evaluation is still indispensable for several tasks, it is also expensive, time-consuming, and challenging. Defining clear evaluation protocols for ensuring the reliability and quality of human judgments is an active research area (Kasai"}, {"title": "6 Discussion", "content": "As discussed above, the problem of generating text from a sequence of temporally ordered images or frames is a challenging one, and relevant to several downstream tasks and applications. Here, we reflect on some crucial aspects and outline various prospective research directions (RDs) and takeaways.\nRD 1: Towards more naturalistic scenarios. Many of the multi-image-to-text tasks we consider in this work have real-world applications. For instance, solutions to the CC task can be used for assisted surveillance and for tracking changes in digital media assets (Jhamtani and Berg-Kirkpatrick, 2018). In the MAAD task, models are required to generate descriptions that complement information in the original audio dialog/soundtrack, for improved accessibility to visually impaired users and for enhancing the visual experience of sighted users (Han et al., 2023a).\nHowever, many day-to-day human-centered scenarios involve personalizing to various contexts or situations. We argue that existing multi-image-to-text tasks in their definitions and settings do not fully reflect this aspect. Tailoring model-generated descriptions/narrations to the perspective of end-users requires task settings in which models and humans can interact iteratively. Such settings would enable incorporation of human expectations and communicative contexts which typically tend to be dynamic in real-world applications. To this end, we advocate for variations of existing tasks where models can learn to contextualize and reason through interactions with other agents (humans or other models) for generating stories, descriptions, or answers. Furthermore, we also advocate for exploration of controlled task settings in which models are expected to generate text adhering to a specific style (Yang and Jin, 2023) or point-of-view.\nRD 2: Are general-purpose VLMs all we need? As discussed in Section 4.4, VLMs that are trained on various general-purpose datasets are increasingly being used for multi-image-to-text tasks through prompting. Powerful open-source models such as Molmo (Deitke et al., 2024), and models optimized for multi-image scenarios such as Mantis (Jiang et al., 2024) are becoming increasingly available, suggesting that the trend of adopt-"}, {"title": "RD 3: Improving and rethinking evaluation.", "content": "In Section 5, we discussed the various approaches for evaluating model outputs in multi-image-to-text tasks. While human evaluation is impractical for conducting large-scale assessments, existing automatic evaluation metrics are limited in terms of fully capturing the abilities of models. Increasingly various benchmarking datasets are being proposed to assess models along different axes important for grounding language in the visual input (Li et al., 2024b). However, many benchmarks often suffer from the problem of visual content irrelevance, which refers to models performing well on the benchmark datasets by primarily relying only on the language modality (Chen et al., 2024a). Furthermore, data leakage and contamination problems (see section 5.1) also hinder fair and accurate testing of model's skills using benchmarks.\nWhile it is important to continue directing re-"}, {"title": "7 Conclusion", "content": "In this position paper, we focused on the problem of multi-image V2L generation and connected the various tasks that are typically considered separate by the research community. We proposed a method for quantitatively exploring the current landscape of this problem, using which we uncovered relationships between task objectives and their corresponding datasets. Despite having different characteristics in terms of the objectives or the input-output data, we argued that all the tasks present a common set of challenges for developing models and for assessing their generated outputs. To understand the progress made over the years with regard to multi-image-to-text generation, we extensively reviewed the different modeling approaches and evaluation protocols pertaining to each of the tasks, and discussed them in a comprehensive and unified manner. As the problem of generating text conditioned on sequences of multiple temporally ordered images or video frames has various real-world applications, we underline the challenges and propose several concrete research directions informed by insights from linguistics, cognitive sciences, and natural language processing (NLP) aimed towards facilitating further advancements. We argue that leveraging these insights could also help the development of better VLMs, which are currently not immune from some of the highlighted limitations."}]}