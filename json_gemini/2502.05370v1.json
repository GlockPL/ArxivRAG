{"title": "fMoE: Fine-Grained Expert Offloading for Large Mixture-of-Experts Serving", "authors": ["Hanfei Yu", "Xingqi Cui", "Hong Zhang", "Hao Wang", "Hao Wang"], "abstract": "Large Language Models (LLMs) have gained immense success in revolutionizing various applications, including content generation, search and recommendation, and AI-assisted operation. To reduce high training costs, Mixture-of-Experts (MoE) architecture has become a popular backbone for modern LLMs. However, despite the benefits, serving MoE-based LLMs experience severe memory inefficiency due to sparsely activated experts. Recent studies propose to offload inactive experts from GPU memory to CPU memory to improve the serving efficiency of MoE models. However, they either incur high inference latency or high model memory footprints due to coarse-grained designs. To tame the latency-memory trade-off in MoE serving, we present fMoE, a fine-grained expert offloading system for MoE serving that achieves low inference latency with memory efficiency. We design fMoE to extract fine-grained expert selection patterns from MoE models and semantic hints from input prompts to efficiently guide expert prefetching, caching, and offloading decisions. fMoE is prototyped on top of HuggingFace Transformers and deployed on a six-GPU testbed. Experiments with open-source MoE models and real-world workloads show that fMoE reduces inference latency by 47% and improves expert hit rate by 36% over state-of-the-art solutions.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have achieved remarkable success in advancing Natural Language Processing (NLP) research and transforming various applications, including content generation [2, 6, 11, 40], search and recommendation [30, 59], and AI-assisted operations [22, 29, 34]. Given the high training costs, modern LLMs have returned to Mixture-of-Experts (MoE) architectures [1, 10, 21, 46, 53, 57] as their backbone implementations. Inside MoE models, each MoE layer comprises a gating network and a collection of experts, with only a subset of experts being activated during computation. This sparse activation mechanism significantly reduces the number of floating point operations (FLOPs), enabling MoE-based LLMs to achieve substantially lower training costs compared to dense LLMs [10, 21, 57].\nDespite the computational efficiency, MoE models exhibit substantial memory inefficiency during the serving phase. Though certain model parameters remain inactive during inference, they must still reside in GPU memory to allow for potential future activation. Expert offloading [4, 16, 47, 54] has emerged as a promising strategy to address this issue, which predicts inactive experts and transfers them to CPU memory while retaining only the necessary experts in GPU memory, reducing the overall model memory footprint.\nHowever, existing expert offloading solutions struggle to effectively balance the latency-memory trade-off in MoE serving. These approaches either suffer from high inference latency [4, 47] or incur substantial model memory footprints [16, 54]. The key reason is that existing works track expert patterns and manage experts in coarse granularity. They fail to accurately identify and retain only the necessary experts in GPU memory during inference, resulting in frequent and costly on-demand expert loading [47], which severely degrades serving performance.\nIn this paper, we propose fMoE, a fine-grained expert offloading system that tames the latency-memory trade-off in MoE serving. To track and analyze MoE models' expert selection behaviors in fine granularity, we propose a new data structure called expert map, which records the iteration-level probability distributions output by the gate network. fMoE uses historical expert maps for comparing expert trajectory similarity to guide offloading.\u00b9 Apart from the expert map, fMoE is designed to track fine-grained input semantic embeddings from individual request prompts processed by the MoE model. Given the collected semantic-based and trajectory-based information, fMoE carefully searches the most accurate"}, {"title": "2 Background and Motivation", "content": "2.1 LLM Serving\nUnlike traditional Deep Learning (DL) model inference, Large Language Model (LLM) serving consists of two consecutive stages: prefill and decode. Figure 1a illustrates the two stages when an LLM performs inference for a request prompt. In the prefill stage, the LLM first computes the intermediate key-value (KV) states of the prompt tokens, prefills the KV cache [3, 24, 27, 32, 61], and then generates the first answer token. In the decode stage, the LLM sequentially generates the answer to the prompt token-by-token in an auto-regressive manner, where tokens generated previously are used for generating the next token.\nThe two stages have their own unique characteristics. The prefill stage only requires one iteration, processing all tokens in parallel and generating the first answer token. The decode stage spans several iterations, generating one token per iteration until the answer is completed. Due to the different characteristics of the two stages, recent studies [38, 61] have identified that the prefill stage is compute-bounded, while the decode stage is considered memory-bounded. Therefore, people typically quantify the serving performance of LLM two stages using different metrics. For the prefill stage, Time-To-First-Token (TTFT) is commonly employed, which measures the latency from receiving the user request until generating the first answer token. For the decode stage, Tokens-Per-Second (TPS) or Time-Per-Output-Token (TPOT) is used to measure the generation rate of LLM serving.\n2.2 MoE-based LLM Serving\nBy integrating MoE layers in Transformer blocks [51], MoE architectures [58] have emerged as a popular backbone for modern LLMs, such as Mixtral [21], Snowflake Arctic [46], and DeepSeek-MoE [10]. Figure 1a illustrates MoE-based LLMs' typical structures, where feed-forward network (FFN) modules are replaced by MoE layers. Each MoE layer consists of a gate network and a set of expert networks. Inside each Transformer block, the self-attention module first calculates the attentions [51] based on input hidden states, and then the gate network determines which expert(s) to activate for computing the output representations. Compared to traditional dense LLMs, MoE-based LLMs only activate a subset of parameters during training and inference, reducing computational overhead while delivering superior generation performance compared to dense LLMs with a comparable number of parameters [1, 10, 21, 46, 53, 57].\nDespite the benefits of saving training computations, MoE-based LLM serving still suffers from GPU memory inefficiency as MoE inference requires loading all model parameters into GPU memory, including those inactive experts. For example, Mixtral-8\u00d77B [21] and DeepSeek-MoE [10] have 72% and 83% inactive parameters during inference due to the sparsity of expert activation in MoE, leading to low memory efficiency and serving throughput. Therefore, to efficiently serve large MoE models, we must seek a solution to the memory inefficiency inherited from MoE architecture.\n2.3 Latency-Memory Trade-Off\nRecently, a few studies have been proposed to improve MoE-based LLM serving efficiency. Figure 2 describes the design space in MoE serving. Existing major studies can be categorized into two types: Lossy serving applies compression [39], pruning [26], and quantization [23] techniques to the original MoE models to reduce the serving memory requirements. However, this line of work achieves serving efficiency by sacrificing the generation quality. Lossless serving focuses on offloading model weights (parameters [4, 36] or experts [16, 47, 54]) that are sparsely utilized in temporal or spatial patterns from GPU memory to CPU memory, aiming to preserve reasonable inference latency. Specifically, expert offloading seeks to predict the activation of experts in advance, prefetching or caching only the necessary experts in GPU memory during inference. We opt for lossless serving to design fMoE because this line of methods avoids modifying models, hence assuring generation quality.\nHowever, existing offloading solutions cannot achieve an optimal spot in the latency-memory trade-off when serving MoE-based LLMs. Figure 1b compares the performance (i.e., inference latency and memory footprint) of existing state-of-the-art (SOTA) offloading solutions, which either provide low inference latency but suffer from large memory footprint (e.g., No-offload and MoE-Infinity [54]), or vice versa (e.g., ProMoE [47], Mixtral-Offloading [16], and DeepSpeed-Inference [4]).\nThe key reason behind this dilemma is that MoE-based decoder-only LLMs have balanced expert routing [47], leaving existing solutions hard to find effective patterns for guid-"}, {"title": "2.4 Existing MoE Offloading Solutions", "content": "Existing expert offloading approaches [16, 54] rely on coarse-grained expert patterns, which are inefficient for guiding offloading. We define coarse-grained information as the expert patterns collected at the request level, where information is aggregated over multiple iterations of a request prompt. For example, MoE-Infinity [54] tracks request-level expert activations. Fine-grained information is defined as the expert patterns observed separately during each inference iteration. Figure 3a shows examples of coarse-grained and fine-grained expert activation heatmaps for Mixtral-8\u00d77B [21]. The heatmap records the expert activations across 32 MoE layers, where each layer contains eight experts and activates two experts out of eight to compute representations. While fine-grained (iteration-level) heatmaps show clear expert acti-"}, {"title": "2.5 Problems of Coarse-Grained Offloading", "content": "Existing coarse-grained expert offloading solutions exhibit three problems:\n1) Insufficient latency-memory trade-off. Existing solutions prefetch and offload experts in coarse granularity, either heavily focusing on reducing inference latency but incurring large memory footprint [54] or reducing memory footprint but severely increasing inference latency [4, 16].\n2) Low expert hit rates. Existing solutions employ coarse-grained expert pattern tracking methods (e.g., Expert Activation Matrix in MoE-Infinity [54]), which produce ineffective"}, {"title": "3 fMoE's Overview", "content": "3.1 Objectives and Challenges\nfMoE is designed to achieve the following three goals:\nMemory-efficient MoE serving with minimal inference latency. We have demonstrated that existing expert offloading solutions [16, 47, 54] fail to tame the latency-memory trade-off in MoE serving (\u00a72.3). We aim to achieve both low memory footprint and inference latency by proposing fine-grained expert offloading.\nMinimize expert miss due to mispredictions in expert prefetching. Expert prefetching, involving future expert activation predictions, is an essential step in expert offloading solutions. However, a recent study [47] has shown that expert miss due to mispredictions can cause high on-demand expert loading delay in inference. We should minimize expert miss and mitigate mispredictions in expert offloading.\nAdapt to heterogeneous MoE models and prompts. MoE inference can serve heterogeneous models [10, 21, 46, 53, 57] with varying prompts [45, 60] in real-world scenarios. While existing solutions handle different models and prompts with a one-fits-all design, we should design our expert offloading to adapt to the heterogeneity in MoE serving.\nWe must address three critical challenges to realize the above objectives:\nHow to maximize expert hit rate when prefetching and offloading experts? Expert hit rate directly relates to the inference latency. With more experts being hit, fewer experts need to be loaded on demand. We propose a fine-grained expert offloading solution to achieve a high expert hit rate.\nHow to adapt to different MoE models and prompts? Heterogeneous MoE models and input prompts exhibit unique system and semantic characteristics. We should craft our solution with fine-grained optimizations to enable adaptivity.\nHow to avoid additional system overheads when managing experts? Our design must not introduce additional system overheads when serving existing MoE LLMs. We apply a series of system optimizations in fMoE to ensure serving efficiency and minimize additional overheads."}, {"title": "3.2 Architecture and Workflow", "content": "Figure 5 describes the architecture and workflow of fMoE, which consists of three main components:\n\u2022 Expert Map Store. We record expert maps, a new data structure defined in fMoE, to track fine-grained expert activation patterns from historical request prompts. expert maps provide nuance expert selection preferences over existing coarse-grained expert tracking methods (e.g., Expert Activation Matrix in MoE-Infinity [54]). The Expert Map Store dynamically keeps the most useful and unique expert maps for real-time queries during inference.\n\u2022 Expert Map Matcher. When a request prompt arrives, fMoE searches the Expert Map Store for appropriate expert maps to guide expert prefetching before inference. expert map search is guided by calculating similarity scores in two folds: semantic and trajectory similarity.\n\u2022 Expert Cache. After receiving the matched expert maps, fMoE prefetches experts from CPU memory to GPU memory for performing computations in inference. fMoE evicts and offloads low-priority expert weights to CPU memory if exceeding Expert Cache capacity.\nfMoE follows the five steps below to enable memory-efficient MoE serving with minimal inference latency:\nStep 1: Inference context collection. Before every inference iteration, fMoE collects necessary contexts, such as semantic embeddings and previous expert activation trajectories (\u00a74.1), and feeds them to the Expert Map Matcher for hybrid similarity matching.\nStep 2: Expert map similarity matching. After receiving iteration-level contexts, the Expert Map Matcher finds and extracts the most similar expert maps by comparing the input context data with historical context data in the Expert Map Store (\u00a74.2). The matched expert maps are forwarded to the Expert Cache to guide expert prefetching and offloading decisions.\nStep 3: Guided expert prefetching and offloading. We dynamically compute expert selection thresholds to determine which expert(s) to prefetch and offload in the MoE model guided by the searched expert maps (\u00a74.3). Then, fMoE"}, {"title": "3.3 Problem Formulation", "content": "We consider serving an MoE-based LLM with L MoE layers on a GPU cluster, where each MoE layer has one gating network and J experts. The gating network of each layer selects top K\u2208 [1,J] experts for computation. The MoE model processes and generates answers for a workload consisting of W unique request prompts. Each request prompt w \u2208 [W] consists of multiple iterations processed during the prefill and decode stages, where [W] is the request prompt collection. Let $E_{l,j}^{(i)}$ denote the j-th expert at the l-th layer in the i-th iteration, where l \u2208 [L], j \u2208 [J], and i \u2208 [w]. During each iteration i, we can make at most L \u00b7 J prefetching decisions. Let $E_{cache}$ and $E_{activate}$ denote the set of cached experts and the set of activated experts for Iteration i, respectively. Hence, we represent the result of whether an expert $E_{l,j} \\in E_{activate}$ is hit (served by $E_{cache}$) or miss (on-demand loading from CPU memory):\n$R_{l,j}^{(i)} = \\begin{cases}1, & \\text{if } (E_{l,j} \\in E_{activate}) = (E_{l,j} \\in E_{cache}),\\\\0, & \\text{otherwise,}\\end{cases}$    (i)\nwhere $R_{l,j}^{(i)} = 1$ means $E_{l,j}^{(i)}$ is a miss. Since all experts in an MoE model are typically designed to have the same weight size, we assume experts' loading time $T_e$ and memory footprint $M_e$ are homogenous. Therefore, the total on-demand loading latency T is summed across all iterations for each expert during the inference process, i.e., $T := T_e \u00b7 \\Sigma_{w\\in[W]} \\Sigma_{i\\in[w]} \\Sigma_{l\\in[L]} \\Sigma_{j\\in[J]} R_{l,j}^{(i)}$.\nFinally, employing the above definitions, we formulate the MoE expert offloading as an integer linear programming (ILP)"}, {"title": "4 fMoE's Design", "content": "4.1 Expert Maps\nWe propose a new data structure, expert maps, to track expert activation patterns with a fine granularity. Figure 6 depicts the structure of an expert map. During the i-th iteration, the 1-th self-attention layer first calculates the attention states. The gate network receives attentions and computes a probability distribution $P_l^{(i)} \\in \\mathbb{R}^J$ over all the experts at Layer l:\n$P_l^{(i)} := \\{p_{l,1}^{(i)}, ..., p_{l,j}^{(i)}, ..., p_{l,J}^{(i)}\\},\\ \\sum_{j\\in[J]} p_{l,j}^{(i)} = 1,\\ p_{l,j}^{(i)} \\geq 0$.\nThen, top K \u2208 [1,J] experts are selected from $P_l^{(i)}$ to compute representations for Layer l. We collect the probability distributions $P_l^{(i)}$ across all L layers to form the expert map of\nIteration i:\n$map^{(i)} := \\{P^{(i)}_1, ..., P^{(i)}_l, ..., P^{(i)}_L\\}, \\ l \\in [L]$.\nBy tracking expert maps, we guide fMoE to discover fine-grained expert patterns\u2014the iteration-level expert selection preferences via probability distributions. Intuitively, analyzing probability distributions enables fMoE to not only identify which experts are binarily selected or omitted, but also to assess the confidence or preference assigned to each expert from the perspective of the gate networks.\nThe design of expert maps has two key advantages over existing coarse-grained expert tracking methods (e.g., MoE-Infinity [54] tracks the request-level expert hit counts). First, existing works only focus on aggregated request-level expert activations, whereas an expert map tracks individual iterations with detailed expert selections. Second, existing works only record the expert hit counts, whereas we track detailed probability distributions. Note that expert maps can easily recover coarse-grained information by applying a top K selection operator to the probability distributions and aggregating expert counts over iterations, therefore generalizing to existing tracking methods. We evaluate fMoE against other tracking methods to show the effectiveness of expert maps in \u00a76.5.\n4.2 Expert Map Search\nWhen predicting and prefetching experts for MoE models, a prefetch distance is usually defined to avoid impacting inference latency [47]. Prefetch distance refers to the number of layers ahead that a prefetch instruction is issued before the target layer activates its experts, similar to the same term in memory prefetching [25]. Let d denote the prefetch distance of the MoE model to serve. Figure 7 shows that fMoE employs two fine-grained search approaches to jointly match expert maps for guiding expert prefetching. Semantic search compares the input embeddings with historical embeddings to find expert maps with similar inputs, whereas trajectory search observes previous expert trajectories (i.e., probability distributions) and matches similar expert maps. We combine both semantic and trajectory features to improve fMoE's map-matching and expert offloading accuracy. Two search approaches' effectiveness is evaluated in \u00a76.5.\nSemantic-based expert map search. For the initial layers l\u2208 [1,d], due to the prefetch distance d, existing solutions [16, 47, 54] cannot observe expert patterns for prediction and prefetching before the target layer is ready to activate experts. Thus, they usually define coarse-grained rules for prefetching initial layers. For example, MoE-Infinity [54] prefetches the most popular experts across all historical data points.\nIn contrast, fMoE leverages semantic hints from the input prompt to search for the most useful expert maps, requiring zero knowledge from the expert activation patterns. When serving request prompts and recording their expert maps, we record the semantic embeddings for each inference iteration:"}, {"title": "4.3 Expert Prefetching", "content": "Given the searched and customized expert map for a layer l\u2208 [L], we guide the expert prefetching in fine granularity.\nSimilarity-aware expert selection. With the different contexts collected during iterations, expert maps searched by fMoE also have varying similarity scores, which reflects the search confidence. To quantify the correlations between similarity score and expert hit rate, we run three MoE models (Mixtral-8\u00d77B, Qwen1.5-MoE, and Phi-3.5-MoE) with two datasets (LMSYS-Chat-1M and ShareGPT) using the methodology described in \u00a74.2. For each inference iteration, we compute the similarity scores, collect expert hit rates guided by the searched expert maps, and calculate the Pearson correlation coefficients [8]. Pearson coefficient is commonly used to measure correlations between variables, where a coefficient close to 1 indicates a strong positive correlation and a coefficient close to 0 means a weak correlation. Figure 8 shows the Pearson coefficient between similarity score and expert hit rate with three MoE models and two datasets. The results show that high similarity scores potentially relate to high expert hit rates if using the corresponding expert maps. Hence, we design fMoE's expert prefetching to be similarity-aware.\nFor a layer l \u2208 [L] with a score \u2208 [-1,1] to prefetch, we first dynamically compute an expert selection threshold $\u03b4_l \\in [0,1]$ given by\n$\u03b4_l := Clip(1 - score, 0, 1) = max(0, min(1 \u2013 score, 1)),$\nwhere score is the cosine similarity score computed in Equations 4 and 5. Given searched $P_l$, we find the set of experts to prefetch $E_{prefetch}$ by iteratively picking the expert with the highest probability from $P_l = \\{p_{l,1}, ..., p_{l,j}, ..., p_{l,J}\\}$ until the summed probability of $E_{prefetch}$ exceeds $\u03b4_l$:\n$\\min\\limits_{ \\{E_{l,j}\\} } E_{prefetch}$   (6)\ns.t.  $\\sum E_{l,j \\in E_{prefetch}}  p_{l,j} \\geq \u03b4_l, j \\in [J], \\forall l \\in [L],$\n$|E_{prefetch}| \\geq K, K \\leq [J],$(8)\nwhere K is the number of experts needed to activate per layer (e.g., Mixtral-8\u00d77B activates two experts per layer). Constraint 7 requires the total probability of selected experts to prefetch per layer to be greater than $\u03b4_l$. Constraint 8 represents the minimum amount of selected experts must be larger than the number of experts to activate required by the MoE model. Intuitively, we assign higher $\u03b4$ to low-score expert maps so that more experts are prefetched to mitigate mispredictions"}, {"title": "4.4 Expert Map Store Management", "content": "Practically, we design fMoE's Expert Map Store to maintain a capacity C for storing unique expert maps. To effectively guide inference across diverse prompts, it makes sense to identify and deduplicate redundant expert maps.\nExpert map deduplication. Since fMoE uses two approaches (i.e., semantic-based and trajectory-based) to compute similarity, we unify the two similarity scores to compute the pairwise redundancy scores between new iteration data and historical iteration data:\n$RD_{yx,y} := \\frac{d}{L} \\cdot score_{x,y}^{sem} + \\frac{L-d}{L} \\cdot score_{x,y}^{map}, \\  x \\in [B], y \\in [C],$\nwhere $score_{x,y}^{sem} \\in \\mathbb{R}^{B \\times C}$ and $score_{x,y}^{map} \\in \\mathbb{R}^{B \\times C}$ are semantic-based and trajectory-based pairwise similarity scores calculated from Equations 4 and 5, d is the prefetch distance, L is the total number of layers, B is the batch size of new interaction data, and C is the Expert Map Store capacity. Intuitively, as shown in Figure 7, the semantic-based and trajectory-based similarity scores contribute to the search expert map in proportion to $\\frac{d}{L}$ and $\\frac{L-d}{L}$, respectively. Therefore, we follow the same ratio to unify and compute the redundancy score. Whenever new iterations' context data arrive at the Expert Map Store, we compute the pairwise redundancy score RDyx,y to determine which old iterations to drop. Hence, we update the old iterations y (columns in RDyx,y) with new iterations x (corresponding rows in RDyx,y) in the Expert Map Store.\nTheoretical analysis. The expert map deduplication can be formulated as a Minimum Sphere Covering problem [17]. The objective is to minimize the total number of expert maps in the store, where each expert map is a vector representation of spheres, while maximizing the sphere coverage in"}, {"title": "4.5 Expert Caching and Eviction", "content": "Similar to existing expert offloading solutions [16, 47, 54], we design fMoE to maintain an Expert Cache on GPUs to reuse expert weights when serving different request prompts. Given matched expert maps from \u00a74.2, we guide fMoE's Expert Cache to compute two priority scores for individual experts: 1) a prefetching priority to decide the prefetching orders of experts in the searched maps, and 2) an eviction priority to determine the eviction orders of experts in the Expert Cache.\nExpert prefetching priority. Recall the set of experts to prefetch $E_{prefetch}$ is determined in Equation 6. For each expert $E_{l,j} \\in E_{prefetch}$, we define the prefetching priority to be\n$PRI_{l,j}^{prefetch} := \\frac{P_{l,j}}{1 - I_{now}},\\ \\ l \\in [L], j \\in [J],$\nwhere pij is the expert probability from the searched expert map, and Inow is the current layer that inference process stays at. Intuitively, experts with higher probability pl,j to be activated should be prefetched sooner, and experts that sit closer to the current layer (i.e., smaller 1 - Inow) should also be prioritized.\nExpert eviction priority. Similar to MoE-Infinity [54], fMoE's expert caching is based on the least frequently used (LFU) algorithm. We integrate the searched map to jointly determine the eviction priority. For each expert El,j \u2208 Ecache, we define the eviction priority to be\n$PRI_{l,j}^{evict} := \\frac{1}{P_{l,j} freq_{i,j}},\\ \\ l \\in [L], j \\in [J],$\nwhere freqi,j is the cache visit frequency and pl,j is the probability from the searched map for an expert El,j \u2208 Ecache. Intuitively, when reaching the Expert Cache limit, we want to first evict experts who are less frequently hit and have lower probabilities of being activated. Note that similar to existing works [47, 54], we do not consider the recent usage of experts as opposed to the classic least recently used (LRU) algorithm [16]. Since the expert usage is layer-wise sequential, i.e., one layer following another, prioritizing recently used experts is against the nature of sequential forward computation in MoE serving."}, {"title": "5 fMoE's Implementation", "content": "We prototype fMoE on top of Huggingface Transformers framework [52] using MoE-Infinity codebase [55]. The implementation of fMoE is described as follows.\nExpert Map Store is implemented in Python using Py-Torch [37] and NumPy [19] libraries. We store all semantic embeddings and expert maps using ndarrays data structure for efficient array operations. The arrays are converted to tensors to compute similarity for expert map matching.\nExpert Map Matcher is implemented in Python using Py-Torch [37] and TorchMetrics [12] libraries. We implement the pairwise computations, including similarity (\u00a74.2) and redundancy (\u00a7 4.4) scores, using the Cosine Similarity interfaces in TorchMetrics. We use the Python multithreading library to implement the asynchronous expert map matching and expert prefetching, where the threads share the same memory space with the Expert Map Store for efficient reading and writing.\nExpert Cache is implemented in C++ based on MoE-Infinity codebase [55]. The expert management in GPUs is implemented with the CUDA Runtime APIs [35]. We implement the caching logic of fMoE and fix critical bugs in the MoE-Infinity codebase to enable expert offloading. Same with MoE-Infinity, fMoE supports multi-GPU inference with expert parallelism, where the experts are mapped to different GPU devices for loading and offloading. We use a hash map to assign expert IDs to different GPUs and retrieve them during inference. The expert assignment follows a round-robin manner to balance the overall GPU load. Additionally, we use a multi-thread task pool in the GPU space to schedule and execute expert prefetching and on-demand loading tasks."}, {"title": "6 Evaluation", "content": "6.1 Experimental Setup\nTestbed. We conduct all experiments on a six-GPU testbed, where each GPU is an NVIDIA GeForce RTX 3090 with 24 GB GPU memory. All GPUs are inter-connected using pairwise NVLinks and connected to the CPU memory using PCIe 4.0 with 32GB/s bandwidth. Additionally, the testbed has a total of 32 AMD Ryzen Threadripper PRO 3955WX CPU cores and 480 GB CPU memory.\nModels. We employ three popular MoE-based LLMs in our evaluation: Mixtral-8\u00d77B [21], Qwen1.5-MoE [57], and Phi-3.5-MoE [1]. Table 1 describes the parameters, number of MoE layers, and number of experts per layer for the three models. Following the evaluation of existing works [47], we profile the models to set the optimal prefetch distance d to three before evaluation.\nDatasets and traces. We employ two real-world prompt datasets commonly used for LLM evaluation: LMSYS-Chat-1M [60] and ShareGPT [45]. For most experiments, we split the sampled datasets in a standard 7:3 ratio, where 70% of the prompts' context data (i.e., semantic embeddings and expert maps) are stored in fMoE's Expert Map Store, and 30% of the prompts are used for testing. For online serving experiments, we empty the Expert Map Store and use real-world LLM inference traces [38, 48] released by Microsoft Azure to set input and generation lengths and drive invocations.\nBaselines. We compare fMoE against four SOTA MOE serving baselines: 1) MoE-Infinity [54] uses coarse-grained request-level expert activation patterns and synchronous expert prediction and prefetching for MoE serving. We prepare the expert activation matrix collection for MoE-Infinity before evaluation for a fair comparison. 2) ProMoE [47] employs a stride-based speculative expert prefetching approach for MoE serving. Since the codebase of ProMoE is not open-sourced and requires training predictors for each MoE model, we reproduced a prototype of ProMoE on top of MoE-Infinity in our best effort. 3) Mixtral-Offloading [16] combines a layer-wise speculative expert prefetching and a LRU-based expert cache. 4) DeepSpeend-Inference employs an expert-agnostic layer-wise parameter offloading approach, which uses pure on-demand loading and does not support prefetching. We implement the offloading logic of DeepSpeed-Inference in the MoE-Infinity codebase and add an expert cache for a fair comparison. We enable all baselines to serve MoE models from HuggingFace Transformer [52].\nMetrics. Following the standard evaluation methodology of existing works [3, 47, 54, 61] on LLM serving, we report the performance of the prefill and decode stages separately. We measure Time-to-First-Token (TTFT) for the prefill stage and Time-Per-Output-Token (TPOT) for the decode stage. Additionally, we also report other system metrics, such as expert hit rate and overheads, for detailed evaluation."}, {"title": "6.2 Overall Performance", "content": "We first evaluate the performance of prefill and decode stages when running fMoE and other baselines with the three MoE models, where we measure Time-To-First-Token (TTFT) and Time-Per-Output-Token (TPOT) for each stage. Note that the inference latency with expert offloading tends to be higher"}, {"title": "6.3 Online Serving Performance", "content": "Except for the offline evaluation (i.e., Expert Map Store in full capacity before serving), we also evaluate fMoE against other baselines in online serving settings. We empty the Expert Map Store of fMoE and the expert activation matrix collection of MoE-Infinity for the online serving experiment. The request traces are derived from Azure LLM inference traces [38, 48],"}, {"title": "6.4 Impact of Expert Cache Limits", "content": "We measure the TPOT of fMoE and other baselines by limiting the expert cache memory budget to investigate their performance in the latency-memory trade-off (\u00a72.3). We mainly focus on TPOT to show the end-to-end performance impacted by varying cache limits. Figure 11 shows the TPOT of fMoE and other four baselines when serving three MoE models under different expert cache limits. We gradually increase the GPU memory allocated for caching experts from 6 GB to 96 GB while employing the same experimental setting in \u00a76.2. Similarly, DeepSpeed-Inference has the worst TPOT due to being expert-agnostic. fMoE consistently outperforms Mixtral-Offloading, ProMoE, and MoE-Infinity under varying expert cache limits. Especially for limited GPU memory sizes (e.g., 6GB), fMoE reduces the TPOT by 32%, 24%, 18%, and 18%, compared to DeepSpeed-Inference, Mixtral-Offloading, ProMoE, and MoE-Infinity, across three MoE models, respectively. With fine-grained expert offloading, fMoE significantly reduces the expert on-demand loading latency while maintaining a lower GPU memory footprint, therefore achieving a better spot in the latency-memory trade-off of MoE serving."}, {"title": "6.5 Ablation Study", "content": "We present the ablation study of fMoE's design.\nEffectiveness of expert map search. One of fMoE's key designs is the expert map, which tracks expert selection preferences in fine granularity. We evaluate the effectiveness of the expert map against five expert pattern-tracking approaches as follows. 1) Speculate: speculative prediction used by Mixtral-Offloading [16] and ProMoE [47], 2) Hit count: request-level expert hit count used by MoE-Infinity [54], 3) Map (T): expert map with only trajectory similarity search, 4) Map (T+S): expert map with both trajectory and semantic similarity search, and 5) Map (T+S+8): expert map with full features"}, {"title": "6.6 Sensitivity Analysis", "content": "We analyze the sensitivity of three hyperparameters: prefetch distance of MoE models, the capacity of Expert Map Store, and inference batch size.\nPrefetch distance of MoE models. Figure 13 shows the TTFT and TPOT of fMoE when serving three MoE models"}, {"title": "6.7 System Overheads", "content": "Latency overheads of fMoE's operations. Figure 15 shows the latency breakdown of one inference iteration in fMoE when serving the three MoE models. We report any opera-"}]}