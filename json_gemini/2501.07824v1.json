{"title": "Real-time Verification and Refinement of Language Model Text Generation", "authors": ["Joonho Ko", "Jinheon Baek", "Sung Ju Hwang"], "abstract": "Large language models (LLMs) have shown remarkable performance across a wide range of natural language tasks. However, a critical challenge remains in that they sometimes generate factually incorrect answers. To address this, while many previous work has focused on identifying errors in their generation and further refining them, they are slow in deployment since they are designed to verify the response from LLMs only after their entire generation (from the first to last tokens) is done. Further, we observe that once LLMs generate incorrect tokens early on, there is a higher likelihood that subsequent tokens will also be factually incorrect. To this end, in this work, we propose Streaming-VR (Streaming Verification and Refinement), a novel approach designed to enhance the efficiency of verification and refinement of LLM outputs. Specifically, the proposed Streaming-VR enables on-the-fly verification and correction of tokens as they are being generated, similar to a streaming process, ensuring that each subset of tokens is checked and refined in realtime by another LLM as the LLM constructs its response. Through comprehensive evaluations on multiple datasets, we demonstrate that our approach not only enhances the factual accuracy of LLMs, but also offers a more efficient solution compared to prior refinement methods.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) (Achiam et al., 2023; Jiang et al., 2023a; Dubey et al., 2024) have demonstrated significant advancements across various tasks, such as question answering (QA) (Yang et al., 2018; Kwiatkowski et al., 2019; Fan et al., 2019; Min et al., 2020) and its more complex real-world applications supported by information retrieval (IR) (Xiong et al., 2020; Karpukhin et al., 2020; Ni et al., 2021). However, LLMs still face notable limitations like hallucinations, mainly due to the incorrect or outdated knowledges of the model itself (Rawte et al., 2023) and the wrong application and generalization of memorized or retrieved knowledge (Jiang et al., 2024; Xu et al., 2024). Previous approaches have sought to mitigate these inaccuracies by augmenting LLMs with external knowledge sources (Guu et al., 2020; Lewis et al., 2020; Luo et al., 2023). However, these methods often face challenges in maintaining faithfulness, as they may retrieve information that is either ungrounded or irrelevant to the context. To this end, in the realm of error identification and verification, recent research has highlighted the challenges LLMs face in accurately detecting and correcting mistakes (Peng et al., 2023).\nHowever, traditional methods (Faltings et al., 2021; Yasunaga et al., 2021; Madaan et al., 2024) have a couple of challenges. First, they are inefficient. They focus mainly on identifying and correcting misinformation only after the complete answer has been generated. This approach not only delays error detection but also requires re-evaluating the entire text, which is computationally expensive and time-consuming. Second, cascading errors. LLMs generate text sequentially, predicting one token at a time based on the preceding context. An early error in this sequence can propagate through subsequent tokens, compounding inaccuracies throughout the response. This error propagation makes it even more challenging to correct misinformation effectively, especially when early mistakes lead to increasingly complex or numerous errors to the overall response. These challenges highlight the critical need for intermediate corrections to prevent further inaccuracies throughout the response.\nIn this work, we propose Streaming-VR (Streaming Verification and Refinement), a method designed to address the issue of error propagation in LLM-generated text. As visually depicted in Figure 1 (b), Streaming-VR evaluates model-generated answers in real-time, identifying the entire sentence and correcting only if its subset is wrong. By employing an external verification model, Streaming-VR verifies errors during the generation process, detects inaccuracies in newly generated sequence of tokens, and promptly corrects them. Because rectification occurs immediately after verification and runs concurrently with text generation, Streaming-VR significantly enhances efficiency and improves the factual accuracy of model outputs. Our experimental results show that when LLMs generate incorrect tokens early in a sequence, it substantially increases the likelihood of subsequent sentences being factually inaccurate. Specifically, approximately 37.6% of the answers in various settings were found to contain factual inaccuracies caused by error propagation (early erroneous tokens), highlighting the critical importance of employing Streaming-VR.\nWe validate the effectiveness and efficiency of Streaming-VR experimentally on two benchmark datasets: achieving approximately 39.8% and 31.5% higher efficiency for ASQA (Stelmakh et al., 2022) and QuoteSum (Schuster et al., 2024) in average, respectively. We have employed Mistral 7B (Jiang et al., 2023a), LLaMA-3.1 8B (Dubey et al., 2024), and GPT-40 (Achiam et al., 2023)."}, {"title": "2 Related Work", "content": "Large Language Models Recent advancements in language models (LMs) (Radford, 2018; Devlin et al., 2019; Liu et al., 2019; Raffel et al., 2023) and LLMs with billions of parameters, have led to significant improvements in performance across various natural language tasks. Since LMs cannot memorize or learn every real-world knowledge, several studies have explored methods to enhance their capabilities by leveraging external knowledge sources like retrieval-augmented generation (Lewis et al., 2020), for knowledge-intensive tasks. Despite the assistance of external knowledge, models often generate incorrect answers due to the failure of factual recall (Jiang et al., 2024) as they may not succeed in retrieving or applying the relevant knowledge appropriately, and generalizing memorized knowledge accurately.\nTo address this issue, recent research has focused on verifying the relevance and accuracy of retrieved knowledge using separate verification mechanisms (Baek et al., 2023). Additionally, methods for generating answers through on-demand retrieval of external information, employing special retrieval tokens, followed by critiquing the outputs to improve their quality, have been explored (Asai et al., 2024). A dynamic retrieval process that determines both when and what to retrieve during answer generation (Jiang et al., 2023b) has demonstrated notable improvements in knowledge-intensive tasks. This is particularly significant as the retrieve-and-generate paradigm faces significant challenges in generating lengthy texts, primarily due to difficulties in maintaining coherence and consistency. Retrieved knowledge is often fragmented and lacks contextual integration, while static retrieval methods fail to adapt dynamically to evolving text, leading to disjointed or repetitive outputs. Future research could address these issues through iterative retrieval mechanisms that refine knowledge during generation, advanced reasoning capabilities to synthesize information from multiple sources, and hierarchical retrieval strategies (Jeong et al., 2024) that organize information at different levels of granularity and difficulty leveraging an external query complexity classifier.\nLanguage Model Verification and Refinement Other than the misinformation induced by wrong knowledge, LLM itself often generates plausible but incorrect texts (Zhang et al., 2024) (i.e., hallucination). Thus, evaluating the factuality (Thorne et al., 2018; Min et al., 2023) of LLM outputs correcting inaccuracies has emerged as an important topic. Various approaches explore methods to enhance the factual accuracy of model responses and develop robust fact-checking or answer-verifying models. For instance, Dhuliawala et al. (2024) generates a series of independent questions to check the factual claims made in the model response, followed by synthesizing the answers from the verification step. Beyond evaluating or verifying the faithfulness of LLM answers, answer-correction has also become a prominent area of focus in various fields. Iterative refinement is well known to be helpful for improving generative contents of natural language (Madaan et al., 2024) and code (Faltings et al., 2021; Yasunaga et al., 2021) autonomously, but limited to the final outcome after waiting for the whole generation is done.\nOn the other hand, Lightman et al. (2023) demonstrates the effectiveness of process supervision by focusing on each step of the reasoning process, and allowing the model to identify and correct errors in the middle. It emphasizes the importance of intermediate verification in complex multi-step reasoning tasks like mathematical problem solving where a single error can derail the entire answer. Also, Welleck et al. (2023) employs an online training procedure for a separate corrector to learn from feedback on intermediate outputs. Nevertheless, LMs are capable of correcting errors only when their locations are identified (Tyen et al., 2024) exactly, which poses a bottleneck in improving selfcorrection capabilities. Furthermore, Huang et al. (2024) have demonstrated through experimental analyses that current LLMs struggle to self-correct their reasoning without external feedback, often resulting in degraded performance after attempting self-correction. Alternatively, Cobbe et al. (2021); Wang et al. (2023) utilize a trained critique model or verifier to correct errors on responses through their feedback. In addition, Gou et al. (2024) show that verification and correction can be done effectively by interacting with diverse external tools. In contrast to the previous works which have to wait for the entire answer generation or limited to the inherent answering ability, we propose a novel method with external model, that refines the specific intermediate sentence of an answer identified as incorrect, with higher efficiency."}, {"title": "3 Method", "content": ""}, {"title": "3.1 Preliminaries", "content": "We begin with preliminaries, formally explaining Large Language Models and traditional verify-andrefine approach, Full Verification and Refinement.\nLarge Language Models Let us define the process of generating an answer $a$ to a given question $q$ as a function: $a = LLM(q)$.\nFor the real-time sentence-level verification and refinement, we also analyze the individual sentences in answer. To elaborate, an answer $a$ is structured as a sequence of $n$ sentences, expressed as $a = [s_1, s_2, \\dots, s_n]$, where the notation $[.]$ signifies concatenation in the specified order. To facilitate real-time correction of incorrect sentences within intermediate answers, we define the intermediate answer at a certain step $t$ ($t > 1$) as $a_{<t} = [s_1, \\dots, s_t]$ containing $t$ sentences in total. Note that this can also be expressed as $a_{<t} = [a_{<t-1}, s_t]$, where $s_t$ is the most recently generated sentence in streaming setup. We initialize $a_{<0}$ as an empty string for coherence.\nIn QA systems that incorporate external knowledge, such as in retrieval-augmented generation (RAG), or examples as in in-context learning (ICL), the answering process differs slightly. Formally, let $d$ denote the external knowledge or example retrieved from the source $D$. The retrieval is performed using a dedicated retrieval model Retriever, for a given query $q$, defined as: $d = Retriever(q; D)$. This process involves ranking the retrieved data based on its relevance or similarity to the given query. After the related documents are retrieved for RAG or ICL, we now incorporate them as input to the LLMs as: $a = LLM(q, d)$.\nFull-VR The simplest traditional approach for verifying and refining LLM answers, namely Full-VR (Full Verification and Refinement), is the most common strategy for improving them just by regenerating the entire responses if identified to be incorrect. While many previous works (Yasunaga et al., 2021; Cobbe et al., 2021; Welleck et al., 2023) achieve significant improvements through supplementary techniques, we focus solely on the vanilla setting, for a direct efficiency comparison without any additional methods designed to increase the factual accuracy of answers. And finally, the overall Full-VR pipeline is expressed as follow for a given query $q$ and its answer $a = LLM(q)$:\n$a = \\begin{cases} a & \\text{if } o = \\text{True} \\\\ Refiner(a) & \\text{if } o = \\text{False} \\end{cases}$\nwhere $o = Verifier(a)$ is the verification output, and $a$ is the final output of Full-VR."}, {"title": "3.2 Streaming Verification and Refinement", "content": "Our approach is structured in the following steps during the generation of answers: 1) StreamingVerification and 2) Streaming-Refinement if necessary; for the sentence identified as error and go back to 1). We formulate the overall framework of Streaming-VR as follow for a given query $q$ and the t-th sentence $s_t \\in LLM(q)$ in its answer:\n$s_t = \\begin{cases} s_t & \\text{if } o_t = \\text{True} \\\\ Refiner(s_t) & \\text{if } o_t = \\text{False} \\end{cases}$\nwhere $o_t = Verifier([a_{<t-1}, s_t])$ is the verification output, and $s_t$ is the new sentence output of Streaming-VR at a certain step $t$. Note that the refinement model, Refiner takes into account the whole context of previously verified and (may have been) refined sentences, $a_{<t-1} = [\\bar{s}_1, \\dots, \\bar{s}_{t-1}]$. After processing all the sentences by Streaming-VR, the final refined answer output should be in the form as follow: $\\bar{a} = [\\bar{s}_1, \\dots, \\bar{s}_n]$.\nThe answer verification relies on the verifier's output, $o_t = Verifier(a_{<t})$ such that $o_t \\in \\{\\text{True}, \\text{False}\\}$. We utilize a fine-tuned LLM to determine whether the input is True or False by evaluating the factuality of the generated answers in sentence-level. To this end, we augment training data with true- and false-labeled sentences, as there is no proper question answering dataset labeled accurately with unit-level (e.g. sentencelevel) answers for our streaming-verifier. The augmented sentences are made from the provided reference answer data by rephrasing it for True and adding wrong information for False by GPT-40 (Achiam et al., 2023) with the specific prompt as in Appendix A. To suit real-time verification scenarios, we split the answer data into individual sentences using NLTK (Bird and Loper, 2004). These sentences are concatenated incrementally in their original order to form intermediate answers $\\{a_{<1}, \\dots, a_{<t}\\}$, ensuring that False-labeled sentences only appear at the end, never in the middle. This design allows the streaming-verifier to focus on determining the factuality of the newlygenerated sentence at the end. To further enhance the training process, a special sentence-separation token, [SEP] is inserted right before the last sentence in each intermediate answer, formatted as $[s_1, s_2, \\dots, [SEP], s_t]$ for a certain stage $t$. This setup allows a model to be trained to verify the last sentence along with the context from the preceding True-labeled paragraph in the train set.\nTo facilitate a real-time scenario with conventional language models, we provide the entire prompt given for answering the test query to the refinement model. Additionally, we only include the retrieved passages or few-shot examples given to the generation prompt, without incorporating any extra information from external knowledge sources for refinement. This strategy ensures that the contextual information relevant to the intermediate generation processes is fully incorporated. Furthermore, as the intermediate answers are refined, they must be updated to reflect the newly refined preceding sentences, thereby enabling a continuous and coherent streaming refinement process."}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Datasets and Evaluation Metrics", "content": "We leverage two different datasets to evaluate the effect of Streaming-VR especially for multi-answer questions which require well-grounded responses to assess the trustworthiness of QA systems.\nASQA (Stelmakh et al., 2022) is a challenging dataset serving as a bridge between factoid and long-form QA tasks by addressing ambiguous questions that can have multiple correct answers depending on their interpretation. It is composed of 4,353 and 948 questions in the train and dev sets, respectively, while the test set is not publicly available. So we use the dev set as our test set here. ASQA provides the reference long-form answers for every questions which are originated from AmbigQA (Min et al., 2020), the ambiguous questions subset of questions from NQ (Kwiatkowski et al., 2019). In this paper, to evaluate the quantitative performance of methods on ASQA, we follow the official metrics and report: Disambiguous-Rouge (DR) as the overall score which combines ROUGE-L (R-L) (Lin, 2004) for text quality and Disambig-F1 (Dis-F1; QA score based on RoBERTa large (Liu et al., 2019)) for factual correctness.\nTo evaluate the consistent impact of Streaming-VR also in a Retrieval-Augmented Generation (RAG) setting, as in the original ASQA paper (Stelmakh et al., 2022), we perform experiments using $k$ retrieved documents. Specifically, we use the top-k documents ranked by semantic similarity between the query and external documents for open-book answer generation on the ASQA dataset. These documents, retrieved from the Wikipedia corpus (2018-12-20 snapshot) using GTR-XXL (Ni et al., 2021), are provided by the LLM citation benchmark ALCE (Gao et al., 2023).\nQuoteSum (Schuster et al., 2024) is also a difficult question answering dataset for semi-extractive multi-source question answering (SEMQA), a task designed to assess the comprehensive answering ability by summarizing information from multiple sources. To be specific, SEMQA requires models to generate a comprehensive response that integrate verbatim factual spans extracted from input sources along with supplementary non-factual text connecting them, thereby ensuring a cohesive answer. QuoteSum is made up of 4,009 semi-extractive answers to 1,376 unique questions from PAQ (Lewis et al., 2021) and NQ. For the quantitative evaluation on QuoteSum, we follow the official metrics and report: ROUGE-L, Sem-F1 for answer extraction quality, and overall SEMQA score where they do not require any model-based evaluations.\nBuilding on the original evaluation of QuoteSum (Schuster et al., 2024), we further conduct a quantitative assessment of the variants of few-shot models. Specifically, we use dynamic prompt with top-$k$ examples for each questions in the test set, as provided in the original paper. These examples are retrieved from the training set by selecting the passages whose queries are most similar to the target test query, based on the cosine similarity between their sentence embeddings (Ni et al., 2022)."}, {"title": "4.2 Analyses on Efficiency", "content": "In addition to evaluating the quality and factual accuracy of model responses, we also measure token count to assess the efficiency of each method. Since our experiments rely on models accessed through the HuggingFace (Wolf et al., 2020) API, it was not feasible to implement simultaneous execution of the verifier and refiner alongside the answering model, as would occur in real-world applications. Consequently, we analyze the inference cost (i.e., the number of tokens) per model for each method. This metric is crucial as the number of refined tokens directly affects to the LLM user's waiting time for response corrections. To quantify the efficiency, we define the efficiency of Streaming-VR relative to Full-VR, taking a cue from the thermal efficiency in thermodynamics, which is formulated as: $(Efficiency) := \\frac{\\text{benefit}}{\\text{cost}} = 1 - \\frac{\\bar{T}_S}{\\bar{T}_F}$. Here, $\\bar{T}_S$ and $\\bar{T}_F$ represent the average number of generated tokens in the refinement phase per answer for Streaming-VR and Full-VR, respectively.\nIt should be emphasized that the tokens being verified are identical for both methods. Consider an answer with N sentences, where each sentence contains $T_i$ tokens $(i = 1, ..., N)$. Full-VR processes all $\\sum_{i=1}^{N} T_i$ tokens in a single step, whereas"}, {"title": "4.3 Experimental Results and Analyses", "content": "Streaming-VR delivers higher efficiency while maintaining its performance. We conduct a series of experiments on the ASQA and QuoteSum datasets to quantitatively evaluate the efficiency and effectiveness of two approaches: Streaming-VR and Full-VR. For this comparison, we first segment the model-generated answers for each test query into individual sentences, treating these sequentially arranged sentences as distinct intermediate answers. Using Streaming-VR, we verify and refine each intermediate answer in real-time, enabling dynamic adjustments as responses are generated. In contrast, Full-VR serves as the baseline, where the entire answer is verified and refined only after the complete sequence has been generated, processing the output in a single pass from start to finish. Note that for Full-VR, we utilize shared verification results of Streaming-VR: an answer is deemed incorrect if it contains at least one erroneous token in the overall context. By comparing Streaming-VR and Full-VR, we aim to demonstrate the advantages of real-time refinement in improving both answer quality and efficiency.\nThe main results on ASQA and QuoteSum are summarized in Table 1. Both Full-VR and Streaming-VR employ Mistral 7B as the verifier and GPT-40 as the refiner across three different backbone models (Mistral 7B, LLaMA-3.1 8B, and GPT-40) for answer generation, as indicated in the method column. Across all response models, the final outcomes after verification-and-refinement converge to similar scores, indicating that the overall quality and faithfulness of the answers are largely determined by the refinement model.\nHowever, we observe a notable performance decline when GPT-40 is used as the backbone for answer generation. Both Full-VR and Streaming-VR with GPT-40 lead to significant drops in Disambig-F1 on ASQA, a key metric for assessing the informativeness of long-form answers, and no other improvements on scores of QuoteSum. These results suggest that GPT-40, which already generated high-quality answers, may be susceptible to over-correction during the refinement process, reducing the overall effectiveness of the responses. This finding highlights a broader trend: refining answers with the same model used for generation--even a powerful model like GPT-40--may not improve performance and can even degrade it. For the applications like large-scale data analysis or high-frequency user requests handling thousands or millions of queries daily, or individual users requiring detailed lengthy responses, relying on expensive models like GPT-40 for both generation and refinement can quickly exceed budgetary constraints. Therefore, Streaming-VR, which uses a more cost-effective model for response generation and GPT-40 solely for refinement, emerges as a more practical and economical solution.\nTo assess the consistent efficacy of Streaming-VR across various settings of RAG and ICL for answer generation, we conduct additional experiments as visualized for performance in Figure 2 and for efficiency in Figure 3. The answers are generated by LLaMA-3.1 8B, verified by Mistral 7B and refined by GPT-40 on both datasets. The results show Streaming-VR's competitive performance compared to Full-VR. Streaming-VR consistently outperforms the initial answers without refinement and achieves comparable results to FullVR. It also illustrates that Streaming-VR delivers results on par with Full-VR across all retrieved passage and example shot counts, offering performance improvements over the unrefined original response outputs of language model.\nIn terms of efficiency, Streaming-VR offers substantial advantages over Full-VR across all models and both closed-book and open-book settings. While Full-VR refines the entire response, generating more tokens for error correction with unnecessary token refinement, Streaming-VR operates at the sentence level, refining only those sentences identified as inaccurate, resulting in significantly fewer tokens being produced. The key to Streaming-VR's efficiency lies in its ability to minimize error propagation during the generation process. By addressing inaccuracies early at the sentence level, it reduces the need for extensive revisions in subsequent stages with inefficiencies. This streamlined process leads to token savings of 39.8% for ASQA and 31.5% for Quotesum.\nWe further provide a comprehensive analysis of the overall inference costs in Table 2, extending our evaluation beyond token efficiency. This analysis underscores the novelty of Streaming-VR across the entire pipeline. As shown in Table 1, Full-VR consistently generates significantly more tokens compared to Streaming-VR. However, for practical deployment in real-world applications, latency values play a critical role in assessing efficiency. To quantify this, the latencies of Full-VR ($t_F$) and Streaming-VR ($t_S$) can be calculated as follow:\n$t_F = t_{\\text{ver}} + T_{\\text{Ref}} \\times t_{\\text{Ref}}$\n$t_S = N \\times t_{\\text{ver}} + \\frac{T_{\\text{Ref}}}{N} \\times t_{\\text{Ref}}$\nHere, $t_{\\text{ver}}$ represents the inference time of the verifier model, while $t_{\\text{ref}}$ denotes the token generation time of the refiner. Since the verifier does not generate tokens, it follows that $t_{\\text{ver}} < t_{\\text{Ref}}$. Furthermore, as shown in Table 2, with an average of seven sentences per answer (N = 7), we observe that $N + T_{\\text{ref}} < T_{\\text{Ref}}$. Consequently, we can conclude that $t_S < t_F$ due to the following inequality:\n$t_S < (N + T_{\\text{ref}}) \\times t_{\\text{Ref}} < T_{\\text{Ref}} \\times t_{\\text{Ref}} < t_F$\nWhen Streaming-VR is applied in real-world scenarios, where the verifier and refiner operate simultaneously alongside the answering model, the latency of Streaming-VR is updated to $t_{\\text{real}}$ as:\n$t_{\\text{real}} = \\text{max} \\{t_{\\text{ver}}, \\frac{T_{\\text{Ref}}}{N} \\times t_{\\text{ref}}\\}$\nSo it demonstrates that Streaming-VR achieves significantly lower latency compared to Full-VR, as $t_{\\text{real}} < t_S < t_F$. As a result, comparing the number of tokens generated during refinement is sufficient to analyze the overall latency of both methods."}, {"title": "Verification models don't need to be bigger", "content": "The results in Table 3 show that verifier models can be effective without being large. On both tasks, Streaming-MG performs comparably to StreamingLG, demonstrating that smaller models can still deliver significant performance gains. These findings highlight that the choice of verifiers is very robust in Streaming-VR, leading to choose smaller models that are resource-efficient and effective, making them particularly valuable for real-world applications with limited computational resources."}, {"title": "Refinement models need to be bigger", "content": "The results in Table 3 highlight the critical role of employing a larger, more advanced model for the refinement step after verification, even when the verifier is relatively small. Using Mistral 7B as both the verifier and refiner (Streaming-MM) results in no improvement or even degraded performance across datasets and settings.\nIn contrast, larger refiners yield significant gains. With LLaMA-3.1 8B as the refiner (StreamingML), there is a modest Dis-F1 improvement for closed-book setting ASQA, though handling multiple passages remains challenging. On QuoteSum, Streaming-ML achieves notable improvements in both zero- and five-shot settings, while Streaming-MM reduces answer quality. The most substantial boost comes from GPT-40 as the refiner (Streaming-MG), whose advanced reasoning capabilities drive superior performance in both RAG and ICL settings. These results confirm the importance of using a refiner larger than the response model for producing coherent, high-quality answers, especially in complex disambiguation and multi-passage reasoning tasks."}, {"title": "LLMs still struggle with intrinsic self-correction", "content": "Additionally, we conduct some experiments to evaluate the efficacy of self-verification and self-refinement within the Streaming-VR pipeline, utilizing only LLaMA-3.1 8B for backbone, verifier and refiner models. In Table 3, the rows of Self-VR (Self-Verification and Refinement; i.e., StreamingLL) illustrate that LLMs continue to face challenges with intrinsic self-correction with some performance drops. This result strengthens the conclusions drawn by Huang et al. (2024), which also have demonstrated that intrinsic self-correction, an approach that model attempts to rectify its initial responses using only its inherent capabilities without external feedback, degrades the response quality."}, {"title": "Any error in the middle derails the entire answer", "content": "As Zhang et al. (2024) point out that the mistakes or hallucinations in the middle of answer can skew the whole response, we report the statistics of model-generated answers with the rate of derailed answers on each dataset. Specifically, the answers are generated by LLaMA-3.1 8B and verified the finetuned streaming verifier as before. The rate of derailed answers is the ratio of the number of 'answers composed of false sentences in sequence from the first erroneous sentence to the last one' to 'false answers if at least one of their sentences is identified as false'. Results in Figure 4 for ASQA and QuoteSum are 26.3% and 48.9% on average across different settings for RAG and ICL, respectively. Therefore, they highlight the importance of Streaming-VR to prevent derailed responses."}, {"title": "5 Conclusion", "content": "In this paper, we introduce Streaming-VR, a novel approach aimed at improving the accuracy and efficiency in language model text generation. Unlike traditional methods solely relying on the final response, Streaming-VR performs real-time verification and correction of erroneous token sequences as they are being produced, with external models simultaneously with answer generation. This prevents error propagation in the early stage and reduces the errors at the end by minimizing the likelihood of compounding inaccuracies, then significantly enhances the efficiency of answer refinement. Extensive experiments for two different QA datasets have clearly demonstrated that StreamingVR consistently achieves remarkably higher efficiency without compromising response quality."}, {"title": "Limitations", "content": "Despite the improvements introduced by our method, Streaming-VR, which enhances both the efficiency and effectiveness of verification and refinement in language model text generation by intervening during intermediate answer generation, there remain promising opportunities for enhancing the answer verifier. Specifically, the primary challenge is the lack of dedicated datasets for answer verification, particularly those suited for real-time scenarios. To address this, we automatically augmented data by paraphrasing sentences or introducing errors by an LLM. However, while effective, this approach carries the risk of mislabeling. Therefore, future work could focus on developing new datasets that are carefully annotated with a diverse range of answers ensuring more accurate verification and reducing the risk of incorrect labeling. Additionally, we can further extend these datasets to include fine-grained labels for multiple classes, rather than just binary ones, to accommodate different types of errors and apply adaptive strategies for subsequent refinement after verification."}, {"title": "Ethics Statement", "content": "In our research, we use publicly available questionanswering (QA) datasets to evaluate the effectiveness and applicability of Streaming-VR in realworld scenarios. The language model we employ may inadvertently reflect biases embedded in its training data, resulting in outputs that perpetuate racism, sexism, or other forms of discrimination. Such biases can manifest even in contexts that appear neutral, highlighting the need for proactive bias detection and mitigation strategies. Moreover, harmful inputs might lead to the retrieval of offensive information or the generation of inappropriate responses by the language models. This presents a significant risk that we must recognize and address. To mitigate these issues, it is crucial to develop methods for detecting and managing offensive, inappropriate, or biased content in both user inputs and the documents retrieved within our retrievalaugmented framework. We view this as a critical area for future research because minimizing the risk of biased or harmful outputs is essential for the safe and ethical deployment of QA systems."}, {"title": "A Implementation Details", "content": "Models In our experiments, we employ two open-source LLMs Mistral 7B (Jiang et al., 2023a) and LLaMA-3.1 8B (Dubey et al., 2024) via Hugging Face (Wolf et al., 2020) API and GPT-40 (Achiam et al., 2023) which is accessible via OpenAI API, representing relatively small, medium, and large models, respectively. Here, these models are never fine-tuned or further trained except for their roles in verification. For the overall Streaming-VR pipeline, LLaMA-3.1 8B functions as the primary backbone language model to generate answers for given queries, while all three models are employed for verification or refinement for experiments.\nStreaming Verifier We fine-tuned Mistral 7B and LLaMA-3.1 8B as verifiers using augmented training data derived from the ASQA and QuoteSum datasets. Each verifier was trained for five epochs on its respective training set, with a learning rate of 1e-5 and the AdamW (Loshchilov and Hutter, 2019) optimizer. To generate augmented data for false-labeled sentences, we embedded fake information into true sentences using GPT-40, adjusting the temperature to 0.3, 0.5, and 0.7 to create diverse forms of inaccuracies. Rather than synthesizing entirely new sentences with large language models, which risk introducing unrelated hallucinations, we adopted this targeted augmentation strategy as a more reliable approach. This method proved highly effective in training verifiers to identify hallucinations, delivering exceptional results that highlight the importance of Streaming-VR in improving efficiency while preserving answer quality. The specific prompt used to generate incorrect information for each sentence (Sentence) in the provided answers (Answer) to given question (Question) is detailed below.\nYou will be given a question (Q) with its corresponding answer paragraph (A) that may be incomplete and a sentence (S) following the paragraph.\\n\\nQ: {Question}\\n A: {Answer}\\n S: {Sentence}\\nYou should modify the given sentence S, into a plausible lie by inserting some wrong information. Just return only the modified 'sentence (S)' itself.\nThe final test results of the finetuned verifiers used in the experiments, including a Random baseline that selects verification results arbitrarily, are presented in Table 4. For the entire pipeline, we establish a constraint that prohibits the use of any"}, {"title": "B Additional Experimental Results", "content": "Additional Baselines In Table 5, we present additional results for other baselines on ASQA, SelfRAG (Asai et al., 2024), one of the most representative methods with their trained models publicly available. Self-RAG performs on-demand retrieval of external information via a specialized retrieval token, followed by a critique of the generated output to refine it. When we compare the baseline results with the similar model size, Mistral 7B demonstrates significantly superior performance to Self-RAG even without the help of refinement."}]}