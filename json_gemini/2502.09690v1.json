{"title": "Trust at Your Own Peril: A Mixed Methods Exploration of the Ability of Large Language Models to Generate Expert-Like Systems Engineering Artifacts and a Characterization of Failure Modes", "authors": ["Taylan G. Topcu", "Mohammed Husain", "Max Ofsa", "Paul Wach"], "abstract": "Multi-purpose Large Language Models (LLMs), a subset of generative Artificial Intelligence (AI), have recently made significant progress. While expectations for LLMs to assist systems engineering (SE) tasks are paramount; the interdisciplinary and complex nature of systems, along with the need to synthesize deep-domain knowledge and operational context, raise questions regarding the efficacy of LLMs to generate SE artifacts, particularly given that they are trained using data that is broadly available on the internet. To that end, we present results from an empirical exploration, where a human expert-generated SE artifact was taken as a benchmark, parsed, and fed into various LLMs through prompt engineering to generate segments of typical SE artifacts. This procedure was applied without any fine-tuning or calibration to document baseline LLM performance. We then adopted a two-fold mixed-methods approach to compare Al generated artifacts against the benchmark. First, we quantitatively compare the artifacts using natural language processing algorithms and find that when prompted carefully, the state-of-the- art algorithms cannot differentiate Al-generated artifacts from the human-expert benchmark. Second, we conduct a qualitative deep dive to investigate how they differ in terms of quality. We document that while the two-material appear very similar, Al generated artifacts exhibit serious failure modes that could be difficult to detect. We characterize these as: premature requirements definition, unsubstantiated numerical estimates, and propensity to overspecify. We contend that this study tells a cautionary tale about why the SE community must be more cautious adopting Al suggested feedback, at least when generated by multi-purpose LLMs.", "sections": [{"title": "1. Introduction", "content": "The old saga of complex system development continues as the vast majority of government and industry programs continue to result in cost and schedule overruns 1\u20137. While numerous researchers and government reports attribute this trend to the ever increasing complexity of the systems being developed 8,9, future systems will only need to be more interoperable10 and intelligent11; which could exacerbate the typical shortcomings of development programs. Additionally, the interdisciplinary nature of these development tasks exceed the information processing capability of any individual designer or disciplinary engineering team12,13. Thus, effective and efficient management of the collaborative development effort will continue to lie at the crux of the problem.\nGiven this context, the discipline of Systems Engineering (SE) relies on diverse teams of subject matter experts 14,15 that iteratively exchange design information and domain knowledge over extended periods of time16. The challenge of the systems engineer is to govern the activities of these large interdisciplinary teams by employing an array of sociotechnical skills17,18 through effective problem formulation19, solving20, verification21,22, and validation23. This necessitates tacit knowledge from multiple domains to be synthesized24,25, along with high familiarity with the specific context of the system of interest26.\nReliance on large number of experts from multiple disciplines causes an inherent disconnect among design artifacts created along the process. To elaborate, individual experts capture their design information in mutually exclusive models (or languages) such as CAD drawings, multi- physics models, various simulations, or SysML models. These collectively convey the necessary design information; however, individually, only some aspect of the whole13. This creates a natural gap between collaborators. Furthermore, as new design information becomes available in one of these interrelated development artifacts, changes need to be propagated to others27. This in return leads to additional communication workload28, rework29, and ultimately, schedule and cost overruns30. While digital transformation31 (DT) aims to synergistically integrate these \"siloed\u201d models in a digital ecosystem to bridge the gap between collaborators30 and ultimately expedite the SE process32, roughly 90% of DT efforts are documented to result in failures33,34. There is a pressing need for rapid, data-friendly, and efficient methods for both improving SE outcomes and facilitating DT11,35.\nTo that end, generative Artificial Intelligence (AI), such as large language models (LLMs), bear great promise as they have the potential to dramatically alter the way design information is represented, managed, and created36. SE literature has been exploring the use of broader Al applications for assisting designers; however, the focus is on formulation of knowledge databases 37,38, virtual assistants 39\u201341, and design evaluators 42,43. These are documented to be helpful for improving designer performance and learning 44\u201346; however, these are mostly custom-built applications instead of multi-purpose LLMs that are trained on data that is broadly available on the internet. Research on the efficacy of LLM methods to assist SE tasks, not only in terms of"}, {"title": "2. RELATED LITERATURE", "content": null}, {"title": "2.1. Large Language Models", "content": null}, {"title": "2.1.1. A Short History", "content": "In this section, we provide a brief history and an overview of LLMs by describing how they function. LLMs originate from the field of Natural Language Processing (NLP), an interdisciplinary field that lies on the intersection of computer science and linguistics that aims to render natural human language palatable to computers. NLP achieves this through executing a combination of tasks such as information retrieval, text classification, and language generation53. Earliest versions of NLP were predominantly rule-based54. These lacked contextual awareness but were highly-domain specific55. Rule-based models were followed by a transition towards learning- based methods that rely on statistical inference56 to predict the sequence of words, such as n-gram models57, which were later followed by hidden Markov models58,59. These models were powerful in terms of speech recognition and information retrieval, yet struggled with the ambiguity, particularly for handling long range dependencies.\nThe new era of NLPs was enabled through the use of deep learning methods. Initially, artificial neural networks provided significant benefits in terms of flexibility. However, these struggled in terms of handling sequential data60 and were later replaced by recurrent neural networks that are successful in capturing sequences over time through backpropagation methods 61. Later, a special instance of recurrent neural networks, long-term short-term memory (LSTM) models were developed. These achieved a significant breakthrough in NLP performance by selectively retaining information for longer periods of time62,63 Nevertheless, it wasn't until the introduction of transformer models that NLPs exhibited a significant performance leap 64. These fundamentally differ from recurrent network-based approaches as they rely on self-attention mechanisms, that allow them to process sequences of information in parallel, as opposed to the sequential approach of neural networks.\nMulti-purpose LLMs have dramatically changed the landscape of NLP capabilities in the last two years65. While the development of LLMs have been a work in progress for years by companies such as OpenAl, Anthropic, and Google, it wasn't until the release of GPT-3 (Generative Pre- trained Transformer) by OpenAl in 2022 and the subsequent release of \u201cChatGPT\u201d or GPT-3.5 in November 2023 that LLMs became commonplace names and tooling. Briefly, LLMs follow a three-step process:\n\u2022\tStep 1: Pre-training: the model is exposed to enormous text-based information and uses self-supervised learning to understand the role of words in context. Specifically, the LLM takes a snippet of text and masks a random word or token (i.e., a subset of a word), then attempts to predict that word given the context of the preceding words. Examples of publicly available pre-training datasets include CommonCrawl (12 years of web crawling), WebText (8M documents from Reddit), Wikipedia, and arXiv."}, {"title": "2.1.2. A Simplified Demonstration", "content": "Understanding how LLMs are formulated and the training datasets used in each step is critical for the purpose of generating SE artifacts, particularly for Government entities such as the Department of Defense (DoD). Due to the massive and broad scope of the pre-training datasets, LLMs exhibit strong performance at reasoning over a spectrum of different domains thus can yield \"off-the-shelf\" value across many use cases prior to any domain-specific supervised fine- tuning model. To illustrate, While one could reasonably consider the response provided in  as appropriate and perhaps high-quality, there is clearly room for improvement and an opportunity to \u201ctailor\u201d it for a given end-user.\nThus, to tailor the LLM response, at least for ones that are trained on the public data on the internet, the missing piece is the concept of \u201cprompt engineering\u201d67. Prompt engineering is a relic of the massive corpus of pre-training data and alludes to an extremely high degree of sensitivity the model has to the provided inputs. By providing an LLM with a more specific prompt, either stylistically (e.g., specifying the length and style of the response) or contextually (e.g., specifying a particular domain or persona for whom to tailor the response), LLMs can provide higher quality responses. For example, revisits the same question, prompted for a DoD specific persona. Note that the response has improved in quality by using domain-specific vernacular, supplies helpful use cases for the artifact, and even adheres well to the syntactic requirement imposed in the prompt. Here, the contrast between and illustrate how the LLM can"}, {"title": "2.2. Generative Artificial Intelligence for Systems Engineering", "content": "There is a growing body of literature in the broader engineering design and SE community on the use of generative Al for various aspects of the system lifecycle36. These could be categorized in requirements engineering, concept generation & tradespace exploration, and human-Al collaboration during decision-making in design. We consider it useful to provide a brief overview.\nRequirements engineering has been the pioneering use case in SE for NLPs, and later LLMs, given the wide-spread availability of text based data68,69. Nevertheless, this thrust also accelerated significantly with the advancement of LLMs and currently pursues a plethora of objectives. These include defining specifications70, classification and extraction of requirements types from other engineering documents71,72, establishing traceability73,74, identification of user needs75, and knowledge extraction51. Recent research in this area benchmarked multi-modal LLMs for understanding reference engineering documentation, textual requirements, and CAD drawings76; but has also documented that there are significant shortcomings in terms of reliability and performance.\nConcept generation and tradespace exploration is one of the rapidly advancing use-cases77. Sarica and Luo use large semantic networks for mining patent databases to aid the ideation process and illuminate novel design concepts78. Others experimented with generative networks for design space exploration, illustrating that both design novelty and quality could be increased concurrently by enabling access to a larger portion of the design space 79,80. Some generative models developed in the mechanical design community pursue generation of concepts79,81 and assisting ideation82. Within the SE literature, there are examples like Selva's work in the"}, {"title": "2.3. The Gap", "content": "While there is significant research in the broader engineering design and SE literature on use of Al, there are only few examples on the use of multi-purpose LLMs that leverage historical SE artifacts to better inform the design of a new system. The most relevant to this paper is Daphne 39,40, an intelligent assistant to support preliminary system design and tradespace analysis of Earth-facing satellite systems. However, as opposed to this study, the novelty of Daphne is rooted in the purposefully trained rule-based knowledge repository and the integration of the natural language interface, instead of communication schema or protocol, which enables the ability to scale and track existing knowledge, while adjusting to new incoming data. Nevertheless, these are purposefully built models for a specific context or objective that are challenging to develop and sustain. Thus, these do not address queries regarding the efficacy of multi-purpose LLMs (e.g., Chat-GPT 4, Claude 3.5, Gemini 2.0) to generate expert-like SE artifacts, particularly without any specific fine-tuning, additional training, or customization; and when guided by a human engineer through iterative prompting.\nAlthough this decision to not conduct any-fine tuning may seem like an odd choice, we contend that this is becoming an increasingly important query as both SE practitioners, students, and researchers are experimenting with multi-purpose LLMs as-is, without any customization or fine-tuning, to alleviate some of their workload; often with little consideration to its potential risks. Furthermore, more often than not, there is a tendency to accept these LLM outputs without further verification. We deem that this is a concerning trend and so far, there are no published studies in the SE community that have documented its potential consequences. Here, we delve into to which extent these multi-purpose LLMs can be effective in assisting SE tasks, and characterize how they fail. Thus, our aim is not to document how the best possible quality LLM outputs could be obtained, but to characterize what are the downside risks even with LLM generated artifacts that closely resemble those produced by human-experts."}, {"title": "3. METHODOLOGY", "content": "The objective of this paper is to empirically investigate if multi-purpose LLMs can generate SE artifacts that resemble those created by experts and to document if and how they may be misleading. We pursue this objective by adopting a human-expert generated SE artifact as a benchmark. We then provide various LLMs with chunks of curated data taken from this benchmark and evaluate their ability to generate SE artifacts. We evaluate Al generated artifacts through a mixed-methods approach. First, we adopt a quantitative approach and ask a machine, more specifically a specialized NLP algorithm, to test if Al generated artifacts are passable and are similar to benchmark artifacts generated by human-experts. We use this step to filter Al Generated responses in terms of similarity. Second, to compare the quality of these artifacts, we adopt a human-expert perspective and dig through Al generated artifacts through a qualitative approach to identify which extent they are similar, and where and how they diverge."}, {"title": "3.1. LLM Selection, Human-Expert Benchmark Selection and Data Processing", "content": null}, {"title": "3.1.1. LLM Selection", "content": "We are interested in evaluating publicly available LLM capabilities; however, we don't have access to their design. Therefore, we choose a representative set of models to run our empirical exploration. We choose to work with exclusively closed-source models as they have the broadest performance, were trained on the most tokens, and have a large enough context window to allow a well-sized prompt. Thus, focus on the use of three closed-source models (e.g., available only via API, with no direct access to model weights): GPT-3.5 Turbo, GPT-4, and Claude. Next, we proceed to selection of the data and its pre-processing."}, {"title": "3.1.2. Selection of the Sample Dataset and its Pre-processing", "content": "The primary technical approach is to attempt to use a series of LLMs to generate SE artifacts using materials from the Bulldog Unmanned Ground Vehicle (UGV) dataset92 corresponding to a hypothetical ACAT II program. Bulldog was conceived by the Defense Acquisition University"}, {"title": "3.1.3. Data Curation", "content": "Next is the curation of representative artifacts. For this research, we are focused on generation of full-text artifacts, as opposed to JSON objects that would be stored in a MBSE tool. The Bulldog dataset is ideal here as it contains several multi-page documents. Thus, we apply data engineering to prepare the data for ML consumption. The strategy is to chunk the 7-8 documents"}, {"title": "3.2. SE Artifact Generation Using LLMs", "content": "For each SE artifact chunk generated in this study, we start-off with a general system-prompt that is followed by a specific user-prompt to generate the desired artifact segment. Here it is useful to emphasize the distinction between the \u201csystem\u201d prompt and the \u201cuser\u201d prompt. The system prompt, provides the model context about the domain (i.e., SE and acquisition of ACAT programs) and the system of interest (i.e., Bulldog, conveyed by a system description and several operational descriptions). Hence the system-prompt's role is to set the context for the LLM prior to any request for a particular SE artifact to be generated."}, {"title": "3.3. Evaluation of LLM Generated Artifacts", "content": null}, {"title": "3.3.1. Quantitative Perspective: Comparison of Text through Natural Language Processing", "content": "The challenge on the evaluation side is the selection of an evaluation framework for comparing a model-generated response to a particular prompt against the human-generated artifact. There are several options at our disposal. BLEU (Bilingual Evaluation Understudy) 101 is typically used for language translation tasks by using n-grams. Recall-Oriented Understudy for Gisting Evaluation (ROUGE) 102 is used for text summarization tasks by comparing the LLM's summary and a reference, human-generated summary. While these evaluation frameworks work well on tasks that have a generally fixed and \u201ccorrect\u201d output, they are brittle in the context of SE as it is focused on the specific sequences of words that are present, which is a proxy for the actual content of the response. Another alternative is sentence embedding methods such as cosine distances that are commonly used in text analysis particularly in the context of engineering design103. These are useful in terms of documenting similarity; however, prioritize frequencies of words and phrases; and could be misleading due to a lack of compositionality in their assessment of a sentence 104.\nThe third alternative, the MAUVE49 evaluation aims to \u201cmeasure the gap between [machine- generated] text and human text\" by using Kullback-Leibler (K-L) divergences. A K-L divergence can be thought of calculating the entropy between two probability distributions. In this case, we would produce a distribution of generated text over the \"labels\" for a given SE artifact (e.g., a natural language conversion of the tabular information), and compare those against the text distribution of what the model generated. Details of the algorithm are described elsewhere49; however, in  we present a visual representation of how MAUVE proceeds and differentiates Type I and II errors."}, {"title": "3.3.2. Qualitative Perspective: Human-Expert Evaluation of Al Generated vs. Human Expert Generated Artifacts", "content": "While quantitative measures provide some insight regarding similarity of two text artifacts, they do not explain how these two artifact bodies differ. This is critical for the SE community as artifacts that may appear similar could be missing critical pieces of information or including misleading ones that have to be omitted from further engineering activities. Therefore, we supplemented our investigation with a qualitative analysis of how Al generated and human- expert generated SE artifacts differ 106\u2013108. More specifically, we picked the Al generated SE artifact with the highest MAUVE similarity score, and then contrasted them against the human- expert generated benchmark to understand the nature of deviations. We coded the information based on its informational character and whether it was traceable back to the problem statement conveyed in the prompts.\nTo ensure consistency of our findings, we followed standard qualitative analysis techniques 109, where we iteratively examined the resulting qualitative codes to extract general features of deviations from the ground truth. More specifically, we had four built-in mechanisms to avoid confirmation bias in our work. First, we decoupled our human-expert benchmark selection from LLM data generation by picking an independently generated SE artifact (Bulldog). This helped to prevent any possible issues regarding what is level of quality would be considered appropriate for the human-expert ground truth. Here, it is useful to note that Bulldog case study was created by a team of SEs, and was controlled by an independent SE team that contracted the associated"}, {"title": "4. Findings", "content": "We present two main findings through a mixed-methods approach that complements quantitative methods with qualitative ones. First, the ability of multi-purpose LLMs to generate SE artifacts that closely resemble those generated by human-experts. Second, we provide a qualitative explanation of how the LLM generated artifacts actually differ from human-expert generated artifacts. In Section 4.1. we present the results of the similarity analysis by using MAUVE, a natural-language processing algorithm. This subsection compares SE artifacts generated by three different LLMs using three prompting configurations that vary in terms of their specificity; against the human-expert generated benchmark. While this analysis enables comparison of large amounts of data, it relies on textual similarities instead of an in-depth quality analysis that necessitates domain-expertise in SE. Therefore, in Section 4.2., we provide the findings from our qualitative analysis, that picks the best LLM generated artifact set, and compares it manually against the human-expert generated benchmark, in search of how they differ. This leads to three failure modes through which LLM generated artifacts diverge from human-expert created ones: premature requirements definition, unsubstantiated estimates, and propensity to overspecify."}, {"title": "4.1. Quantitative Perspective: Evaluation of Artifact Similarity Using MAUVE", "content": "We start our discussion with an evaluation of MAUVE comparisons for LLM generated and human-expert generated artifacts, for all three LLMs included in our study. To recall, MAUVE scores range between 0-1. In this context 1 indicates expert-like and O indicates machine generated. Table 1 suggests that results are fairly skewed, and the similarity performance of LLMs to generate expert-like SE artifacts is strongly influenced by prompt engineering. To elaborate, Prompt configurations 1-3 represent user-prompts that range in terms of their specificity; where configuration one is more generic and three requests more specific questions to be answered following certain syntactic requirements. In our case, increasing prompt specificity yielded scores that corresponded to a greater semantic similarity to the expert-generated ground truth. We elaborate these prompt configurations below."}, {"title": "4.2. Qualitative Findings: How LLM Generated Artifacts Differ from the Human-Expert Benchmark", "content": "As discussed in Section 4.1, our exercise resulted in nine (3 LLMs and 3 prompt configurations) LLM generated SE artifact sets with 52 user-prompt response pairs in each set. Given the large amount of data, we only picked the LLM generated artifact set with the highest MAUVE score to proceed with the qualitative analysis. In this case it was the Claude generated responses following prompt configuration 3. We present two findings in this subsection. First, we characterized 3 failure modes described below along with some supporting examples. These mechanisms do not always manifest themselves in isolation and are often observed in conjunction with the other failure modes. Nevertheless, we consider it useful to discuss each individually as they may lead to contrasting issues for the rest of the SE activities to follow. Second, we highlight some of the tasks that we observed LLM generated artifacts to closely compare those generated by a human-expert. Although LLMs were not equally consistently good in these tasks, we consider them important to discuss given that they hint at the kind of tasks multi-purpose LLMs might be useful."}, {"title": "4.2.1. Failure Mode I: Premature Requirement Definition", "content": "This failure mode refers to the inability of multi-purpose LLMs to differentiate needs from requirements, and proceed into a premature and often over-specified definition of requirements, particularly when asked to bound a problem. This failure mode also hints at LLMs struggling to identify the role of the document under consideration in the grand scheme of development activities along with the specific role of the tasks that it is asked to perform; illustrating a lack of SE domain expertise.\nFor instance, CDDs are created during problem formulation, with the goal to identifying desired system capabilities, and fundamentally includes statements regarding stakeholder needs and objectives110. The purpose of a CDD is to serve as an input to definition of system requirements, from which a tradespace analysis and concept exploration will be conducted to identify feasible design solutions. Hence, the role and expectation for CDD is to bound the problem, and not to specify requirements. Our response set included 35 questions similar to this, and we observed in over half of the LLM"}, {"title": "4.2.2. Failure Mode 2: Unsubstantiated Estimates", "content": "The second failure mode is unsubstantiated estimates, where the LLM is asked to provide a bound on some system attributes, and in return provides numerical values that are either completely absurd or possibly conflated with some other characteristic contained in the prompt. Here, similar to the premature requirement definition tendencies, there seems to be a mechanism that seeks information from documents in the training set that is found to be relevant to the topic at hand; and simple values found in these documents are pulled in the response. This is problematic because it completely overhauls the engineering analysis aspect of SE. Thus, the main characteristic of this failure mode is that while LLM does not have the appropriate analytical capabilities at its disposal to be able to generate any of these numerical estimates, it still proceeds to provide some numbers in a palatable form. While we did not expect the LLM to run a lifecycle analysis and calculate what an appropriate unit cost would be; we find it concerning that the LLM readily suggests some numbers instead of arguing it would not be able to execute this task; or at least warn the user about the possible inaccuracies these numbers might exhibit.\nConsider the example shown in , where the LLM is asked to estimate the unit cost for the proposed system of interest, an unmanned ground vehicle. We observe two mechanisms here. First, when the LLM is asked for a unit cost, it returns an exorbitant amount without any support or justification that is more reminiscent of a total system development cost. In fact, the provided unit cost estimate is so significantly off, it dwarfs the most expensive ground vehicles in US Army inventory such as a M1 Abrahams tank (~$15M) or a M104 Paladin ($17.2M)112. As such, the provided unit cost estimates even exceed the unit cost of F-35 joint strike fighter113. Second, the attribution for rationale is questionable and disregards lifecycle costs on the fleet leve|114,115. Instead of highlighting relevant lifecycle considerations such as total operational costs or replacement costs; and looking for a \u201cright\u201d number of systems to be deployed for desired mission effectiveness, the LLM suggests the proposed unit costs would allow for maximum procurement of the system of interest. Hence although it is not reflected in numbers, there seems to be an underlying cost minimizing argument."}, {"title": "4.2.3. Failure Mode 3: Propensity to Overspecify", "content": "The third failure mode is the propensity to overspecify. While this failure mode appears similar to the first, there are significant differences. Here, the tendency is to introduce additional expectations regarding the system of interest that are often broadly relevant, yet are not needed given the specific context. In other words, the LLM exaggerates the \u201cuseful\u201d traits that it thinks"}, {"title": "4.2.4. Satisfactory Traits", "content": "We discovered some positive trending traits of the LLM that we highlight in this section. Nevertheless, these were not as consistently revealed in our exploratory analysis. Thus, we urge the readers to consider these as sporadic indicators of some cases in which multi-purpose LLMs could prove useful for generating SE artifacts, and not in terms of generalizable patterns. These could be summarized with: (i) the ability to follow guidance, particularly in terms of formatting, such as in providing development thresholds and objectives; (ii) adjustment of information as it would be expected from expert, such as in the case of with the adjustment for fiscal year dollars; and (iii) successfully summarizing context related statements.\nThe responses do not seem to be stemming from a common aspect in the prompted text from either the style of prompt or specific content that is attempting to be ascertained. Consider , where the LLM provides a potentially viable answer for a context description task that only slightly differs from the human-expert benchmark in terms of the focus of the response. Here, the human-expert benchmark focuses on general purpose of the system of interest while the LLM focuses on its autonomous nature, and implicitly, how that relates to the purpose. Nevertheless, they are not identical or arguably on the same level of quality. For instance, the LLM response focuses on emphasizing \u201chow\u201d the system of interest will be providing some of these mission level capabilities (e.g., \u201cwith onboard sensors and weapons...\u201d). Beyond this difference of perspective both SE artifacts in arguably provide similar information and could be considered as passable. Variable content tends towards the LLM response reading like a human-expert writer with different preferences."}, {"title": "5. Discussion", "content": "Advancing capabilities in generative Al, particularly evidence from multi-purpose LLMs passing domain-expertise exams in the fields of law117 and medicine118,119; is encouraging the SE"}, {"title": "5.1. Expectations Regarding Future Relevance of Findings", "content": "LLMs are evolving at an astonishing pace. As such, between the time this study was conducted and published, the LLMs examined in this study have been already replaced by newer versions with considerable general performance improvements. While this paper does not intend to provide an evaluation of the limits of LLM technology in general, we contend that it is useful to discuss how far we anticipate our findings, particularly those that refer to the ability of future LLMs to address the identified failure modes, will hold.\nWe expect that unsubstantiated estimates and propensity to overspecify might be difficult to counter, at least in terms of providing high quality answers. This is because problem formulation is a challenging SE activity that requires substantial domain-expertise given the open-ended nature of translating an ill-defined problem120,19,20, often-unique contextual environment, and the need to cater to the counterbalancing objectives of the system of interest. Thus, unless there is a fundamental change in how LLMs operate, looking across other document bodies to identify relevant information may not be a viable approach for SE problems, at least for systems with sufficient complexity or uniqueness. This may be less true for more standardized and relatively incremental products such as basic consumer products and automobiles, compared to one-of a kind, cutting-edge systems such as the James Webb Telescope5. Alternatively, expanding LLMs with dedicated engineering knowledge-based expert models, such as expert systems or its hybrids, could be help navigate these failure modes.\nFinally, while we have reasonable evidence to suspect premature requirements definition will persist for a foreseeable future similar to the previous two failure modes, it may be marginally easier to counter by future LLMs; if they could correctly recognize that the query is not to define a requirement but is to bound the problem. This is often an elusive challenge for novice SEs and that may also be the case for LLMs. Here a viable improvement could be guiding the user towards"}, {"title": "5.2. Limitations", "content": "We should also highlight some of the limitations of this study. An important research design decision we made was to structure every prompt as a silo supported by a system-prompt. Thus, generated chunks of SE artifacts did not refer to itself. This was a purposeful decision to balance the token limits of each LLM and to avoid \u201ccascading hallucinations\u201d that may originate from the human-in-the-loop attempting to push the LLM to explain and link things back to its previous responses that may or may not be directly relevant. Possibly as a consequence of this, identified failure modes didn't necessarily reflect much of hallucinating121,122. This could also be partially a result of basing our qualitative analysis on Claude, which are documented to be relatively better in terms of lack of hallucination, compared to other LLMs.\nAnother research choice that is important to note was that our experimental procedure was not blinded, meaning that while two independent coders were employed to execute the qualitative coding and they knew they were analyzing LLM outputs. Here, the possibility of introducing confirmation bias in the findings was countered by relying on a third coder to aggregate the qualitative coding into the failure modes. Thus, while we believe it is not a threat to validity, it is useful to note for replicability of results.\nAdditionally, we should note that LLMs are not deterministic tools, and every instance of LLM usage with identical inputs may lead to different results. This study did not aim to measure the stability or variance of LLM generated responses, and rather assumed LLMs would behave in a rather deterministic manner. This is a strong assumption that may be explored later, perhaps in a simulated experiment123. Related to these challenges, future research could explore how the multi-purpose LLMs fare for various SE tasks, how much variability is contained within the Al generated response, and may test the sensitivity of documented failure modes following different prompting approaches."}, {"title": "5.3. Some Future Research Opportunities", "content": "Our study also illuminates some research opportunities for improving the effectiveness of LLMs in the context of SE. The most pressing are:\n\u2022\tLimits of available training data: while some SE relics are available in the public pre- training corpus, most Defense-specific acquisition information is not publicly released and is stored in either classified or proprietary networks. This means that multi-purpose pre- trained LLMs such as the ones we investigated in this study are fundamentally handicapped to producing content that resembles what is commercially available. Prompt engineering can make up some of the gap but is not a replacement for an enterprise-grade fine-tuning strategy. Nevertheless, this does not prevent organizations from developing their own tools, that can utilize both publicly available information and their own organizational knowledge bases, which could help alleviate this limitation.\n\u2022\tRisk of over-engineering model outputs: we saw that the model outputs are significantly more valuable when the prompt is engineered towards specific responses. However, this concurrently introduces a risk that model responses will produce content exactly like previous acquisitions were designed, which may be undesirable compared to a situation where the model is producing useful and novel ideas on system design. As an extreme example, it is not desirable for SE artifacts to be produced on the basis of the original B-52 aircraft when the new system capability is expected to resemble that of the F-35 aircraft and, vice versa, it is not desirable for SE artifacts to be produced on the basis of an F-35 when the need can be met with the simplicity of the original B-52.\n\u2022\tTrust and reliability: LLMs are known to be susceptible to vulnerabilities either due to the nature of their training (e.g., as discussed in 90) or with biases inadvertently introduced during the SFT or RL phases. Thus, at least for a foreseeable future, there should be a human expert-in-the-loop to audit model responses to ensure quality. This leads to the corollary that there is significant room for research on human-Al collaboration on both development and verification of trustworthy LLMs.\n\u2022\tPerformance gaps between open- and closed-source models and the inherent security risks associated with using them: it is observed that closed-source models, generally created by larger, well-funded corporations who can pay for larger datasets and very expensive SFT datasets, generally perform larger than open-source models. For most enterprise applications, closed-source models are infeasible as sensitive data would have to hit commercial servers to allow for model inference. While the results presented in this article suggest value in closed-source models for generation of SE artifacts, a more comprehensive evaluation should be completed across the full ecosystem of LLMs. Nevertheless, to the best of our knowledge, research on the possible scale, and implications of, associated security risks is nascent.\n\u2022\tConsistency and Stochastic Nature of LLMs: Although we ignored this fact in this study, LLMs are not deterministic tools. Meaning that for open-ended exercises, such as SE problem formulation tasks, they may provide different answers in each run. Research is needed to test and monitor consistency of these tools, with an explicit understanding of the quality of their outputs. Natural language processing techniques could be leveraged to analyze these results in batch; however, it is still a significant research challenge. We contend that applicability of LLMs for SE purposes will hinge on their ability to navigate this challenge."}, {"title": "6. Conclusions", "content": "To conclude, LLMs are remarkable tools, having become adopted across government and commercial sectors in a matter of months and become global household names. It is highly likely that in a matter of years, they will be adopted as a co-pilot tool to either generate artifacts, provide some level of critiquing on existing information, support information retrieval from MBSE tooling, apply new technical frameworks to existing or on-going program acquisition, etc. Nevertheless, this paper illustrates that \u201ccommercial-off-the-shelf\u201d value of LLMs for SE tasks is rather limited at this stage, with engrained failure modes that may lead to poor SE decisions. There is significant room for research on methods and techniques to improve both infusion of"}]}