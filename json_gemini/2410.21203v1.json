{"title": "SeriesGAN: Time Series Generation via Adversarial and Autoregressive Learning", "authors": ["MohammadReza EskandariNasab", "Shah Muhammad Hamdi", "Soukaina Filali Boubrahimi"], "abstract": "Current Generative Adversarial Network (GAN)- based approaches for time series generation face challenges such as suboptimal convergence, information loss in embedding spaces, and instability. To overcome these challenges, we introduce an advanced framework that integrates the advantages of an autoencoder-generated embedding space with the adversarial training dynamics of GANs. This method employs two discrim- inators: one to specifically guide the generator and another to refine both the autoencoder's and generator's output. Additionally, our framework incorporates a novel autoencoder-based loss function and supervision from a teacher-forcing supervisor network, which captures the stepwise conditional distributions of the data. The generator operates within the latent space, while the two discriminators work on latent and feature spaces separately, providing crucial feedback to both the generator and the autoencoder. By leveraging this dual-discriminator approach, we minimize information loss in the embedding space. Through joint training, our framework excels at generating high-fidelity time series data, consistently outperforming existing state-of-the- art benchmarks both qualitatively and quantitatively across a range of real and synthetic multivariate time series datasets.", "sections": [{"title": "I. INTRODUCTION", "content": "Generating realistic synthetic data can help balance datasets and mitigate data shortages [1], thus enhancing scientific research and boosting the effectiveness of various machine learning applications. However, generating time series data presents unique challenges due to its temporal characteristics [2]. Models need to capture not only the distribution of features at each time point but also the complex interactions between these points over time. For example, in multivariate sequential data $X_{1:T} = (x_1,...,x_T)$, a good model should accurately determine the conditional distribution $p(x_t | X_{1:t-1})$ to reflect temporal transitions. This capability is crucial across numerous fields, especially when working with imbalanced time series datasets [3]. Areas such as healthcare [2] and solar physics [4], [5] often encounter data limitations due to factors such as privacy issues, the complexity and noise in data, or the rarity of events, which make model training and evaluation challenging. By developing approaches that utilize generative adversarial networks (GANs) [6] to generate realistic synthetic data, scientific advancement can be supported, and machine learning performance can be enhanced by balancing datasets and addressing data scarcity [1].\nA significant amount of research has focused on improving the temporal dynamics of autoregressive models for sequence forecasting, aiming to minimize the impact of sampling errors by making various adjustments during training to better model conditional distributions [7], [8]. Autoregressive models break down the sequence distribution into a series of conditionals $\\Pi_t P(x_t | X_{1:t-1})$, making them effective for forecasting due to their deterministic properties. However, these models are not genuinely generative, as they do not require external input to generate new sequences. On the other hand, research involving GANs for sequential data typically employs recurrent networks as generators and discriminators, aiming directly at an adver- sarial objective [9]\u2013[11]. While this method is straightforward, the adversarial objective targets modeling the joint distribution $P(X_{1:T})$ without accounting for the autoregressive nature, which might be insufficient since merely aggregating standard GAN losses over vectors may not adequately capture the stepwise dependencies present in the training data.\nIn this paper, we propose a new framework that substantially improves the stability, quality of generated data, and generaliz- ability. Our approach, named SeriesGAN, seamlessly integrates two research domains, GANs and autoregressive models, into a powerful and accurate generative model uniquely tailored to preserve temporal dynamics. SeriesGAN offers a holistic solution for generating realistic time-series data, with broad applicability across multiple fields. The primary contributions of our work include:\n1) Utilizing two discriminators (dual-discriminator train- ing) that operate separately on feature and latent spaces, providing dual feedback for the generator. The feedback from the feature space also assists the autoencoder in enhancing its reconstruction capability and accuracy.\n2) Developing a novel autoencoder-based loss function for the generator network, which enhances the quality of the generated data and facilitates optimal convergence. Additionally, a new loss function is designed for the autoencoder network.\n3) Employing a teacher-forcing-based supervisor network with a novel loss function, which significantly helps the generator network to better learn the temporal dynamics of time-series data.\n4) Implementing an early stopping algorithm and applying Least Squares GANs (LSGANs) [12] to stabilize the"}, {"title": "II. RELATED WORK", "content": "Autoregressive recurrent networks trained using maximum likelihood methods tend to experience substantial prediction errors during multi-step sampling [14]. This problem arises due to the discrepancy between closed-loop training (con- ditioned on real data) and open-loop inference (based on previous predictions). Drawing inspiration from adversarial domain adaptation [15], Professor Forcing employs an addi- tional discriminator to distinguish between autonomous and teacher-driven hidden states [16], thereby aligning training and sampling dynamics. Teacher forcing reduces errors during training by using ground truth data for conditioning at each step, while Professor Forcing bridges the gap between training and inference by aligning the hidden state dynamics between both processes. These methods aim to reduce exposure bias, which occurs when a model's predictions degrade over time during inference. However, despite these methods aiming to model stepwise transitions, they are deterministic and do not explicitly involve sampling from a learned distribution, which is essential for our goal of synthetic data generation.\nThe seminal work on GANs [6] presented a groundbreaking framework for generating synthetic data. This model com- prises two neural networks (a generator and a discriminator) trained concurrently within a zero-sum game structure. The generator learns to produce data by attempting to fool the discriminator, while the discriminator simultaneously learns to distinguish between real and generated data. Both networks improve iteratively through this adversarial process, with the generator striving to minimize the discriminator's accuracy. In many GAN implementations, CNNs [17] are employed to enhance the generator and discriminator's ability to capture spatial patterns. While GANs can generate data by sampling from a learned distribution, they face challenges in capturing the sequential dependencies characteristic of time series data. The adversarial feedback from the discriminator alone does not provide enough information for the generator to adequately learn the intricate patterns within sequences.\nSeveral studies have employed the GAN framework for time series analysis. The earliest of these, C-RNN-GAN [9], applied the GAN architecture directly to sequential data, using LSTM networks as both the generator and discriminator. This model generates data recurrently, starting with a noise vector and the data from the previous time step. RCGAN [10] improved upon this by eliminating the dependence on previous outputs and incorporating additional inputs for conditioning [18]. However, unlike TimeGAN, these models depend solely on binary adversarial feedback for learning, which may not sufficiently capture the temporal dynamics of the training data.\nTimeGAN [13] offers an advanced method for generating realistic time series data by merging the flexibility of unsu- pervised learning with the accuracy of supervised training. It leverages an autoencoder, enabling the GAN to both generate and discriminate within the latent space. This approach helps the GAN mitigate non-convergence issues [19] by training in a lower-dimensional representation. TimeGAN is designed to accurately replicate the temporal dynamics inherent in training data, making it particularly useful for addressing imbalanced time series classification problems such as solar flare predic- tion, where X-class flares are rare occurrences. Employing data augmentation techniques such as TimeGAN can help boost the predictive performance of solar flare prediction models [20]. However, despite its advantages, it struggles with stability during training and often produces inconsistent data quality. As a result, it frequently fails to deliver optimal results after each training cycle.\nThe SeriesGAN framework is developed to improve the performance and robustness of time series generation methods, specifically TimeGAN, by achieving several key goals. First, it enhances data reconstruction by the decoder and data genera- tion by the generator through the training of two discriminators in both the latent and feature spaces. Second, it facilitates the convergence of the generator network by implementing a novel autoencoder-based loss function that guides the generator by providing characteristics of real time series data. Third, it incorporates a teacher forcing supervisor along with a novel loss function, trained jointly with the generator as a combined network. This strengthens the generator's ability to learn and capture the temporal dynamics of time series data more effectively. Fourth, it integrates LSGANs instead of standard GANs and includes an early stopping algorithm to enhance stability and achieve consistently optimal results under the same hyperparameters."}, {"title": "III. PROBLEM FORMULATION", "content": "In this setting, we work with data that contains temporal features (i.e., features that evolve over time, such as sensor measurements). Let X represent the space of these temporal features, and let $X \\in X$ be random vectors, each of which can take on specific values denoted by x. We consider sequences of temporal data, denoted $X_{1:T}$, drawn from a joint distribution p. The length T of each sequence is itself a random variable, which is absorbed into the distribution p. In our training data, each sample is indexed by $n \\in \\{1, ..., N\\}$, and the dataset can be represented as $D = \\{x^n,_{1:T_n}\\}_{n=1}^N$. From this point forward, we omit the subscript n unless needed for clarity.\nOur primary goal is to use the training data D to estimate a probability density function $\\hat{p}(X_{1:T})$ that closely approximates the true distribution $p(X_{1:T})$. This is a challenging objective, particularly given the variability in sequence lengths, the dimensionality of the data, and the complexity of the distri- bution. To address this, we decompose the joint distribution"}, {"title": "IV. PROPOSED MODEL", "content": "As illustrated in Fig. 2, the framework consists of six networks: two autoencoders, referred to as the loss function autoencoder and the latent autoencoder, a generator, a super- visor, and two discriminators, named the latent discriminator and the feature discriminator. The loss function autoencoder is employed to implement our novel time series loss, which facilitates the generator network in more effectively capturing the intrinsic characteristics of real time series data. The latent autoencoder's role is to facilitate training by generating com- pressed representations in the latent space, thereby reducing the likelihood of non-convergence within the GAN framework [21]. The generator produces data in this lower-dimensional latent space rather than in the feature space. The supervisor network, integrated with a novel supervised loss function, is specifically designed to learn the temporal dynamics of the time series data through teacher-forcing training [16]. This is crucial, as relying solely on the discriminators' binary adver- sarial feedback may not sufficiently prompt the generator to capture the data's stepwise conditional distributions. The latent discriminator provides efficient feedback to the generator by distinguishing between real and fake data in the latent space, while the feature discriminator differentiates between fake and real data in the feature space, providing secondary and more accurate feedback to both the generator and the autoencoders.\nAs illustrated in Fig. 2, the input data x is encoded into the latent space $h_{AE}$ using the encoder function e, where $h_{AE} = e_x(x)$. This encoding captures the essential features of x through the latent autoencoder. The data reconstruction, $x_{AE} = r_x(h_{AE})$, is then achieved by decoding $h_{AE}$ with the recovery function r, aiming to closely replicate the original input. The generator function g transforms a random noise vector z into synthetic latent data $h_G = g_x(z)$, which is subsequently reconstructed into synthetic data $x_G = r_x(h_G)$.\nTo further refine the synthetic data, the supervisor network s processes $h_G$ to generate a latent representation $h_s = s_x(h_G)$, from which the final synthetic data $x_s = r_x(h_s)$ is reconstructed. Additionally, the encoding of the input data x can be represented as $h^l = \\hat{e}_x(x)$, where $\\hat{e}$ is the encoder function of the loss function autoencoder, designed to capture"}, {"title": "A. Autoregressive Learning", "content": "Combining the GAN framework with autoregressive learn- ing enables us not only to approximate the true distribu- tion $p(X_{1:T})$ through a learned probability density function $\\hat{p}(X_{1:T})$, but also to model the conditional density function $\\hat{p}(X_t|X_{1:t-1})$ that approximates the true $p(X_t|X_{1:t-1})$ at any given time step t. To achieve this, SeriesGAN utilizes a GRU- based supervisor network trained alongside the generator to meet both objectives.\nThe SeriesGAN framework consists of four distinct training phases. In the first two phases, the two autoencoders are trained independently of the other networks to effectively learn the encoding and decoding representations of the real data. This isolated training ensures that each autoencoder captures the underlying structure of the data before integrating with the rest of the model. In the third phase of training, the supervisor network is separately trained to predict the second next times- tamp t by leveraging timestamps 1 to t - 2, which leads to improved performance compared to the conventional approach of predicting the next timestamp t based on 1 to t \u2013 1. In the fourth phase, the generator and supervisor are trained together as a single integrated network using a combined loss function $L_G$, which updates the weights of both components. During this phase, the latent and feature autoencoders undergo joint training with the generator-supervisor model in an adversarial framework. The loss $L_G$ in (3) is composed of multiple sub-losses, including the supervised loss $L_S$, which captures the temporal dynamics during teacher-forcing training, and adver- sarial feedback losses $L_{Ulatent}$ and $L_{Ufeature}$ from the latent and feature discriminators, respectively. Moreover, SeriesGAN incorporates the Mean Absolute Error (MAE) between the real data x and the generated data x, represented by $L_V$, ensuring the generated data closely matches the statistical properties of"}, {"title": "B. Dual-Discriminator Training", "content": "Training a GAN framework alongside an autoencoder en- ables the generator to produce data within the latent space, which has lower dimensionality compared to the feature space. This reduces the likelihood of non-convergence, a common"}, {"title": "C. Autoencoder-based Loss Function", "content": "We introduce a novel loss function to better guide the generator network in learning the characteristics of the dataset. Providing only adversarial feedback to the generator is insuf- ficient for teaching the generator the nuances of time series characteristics, resulting in synthetic data that does not closely resemble real data. To address this issue, it is essential to supply the generator with the intrinsic properties of the dataset. However, this task is challenging due to the numerous features present in time series data, including trend, seasonality, and cyclicity [23]. By employing a GRU autoencoder, named the loss function autoencoder, and training it with reconstruction loss on the dataset, we can extract compressed features of the time series samples via the encoder [24]. We then calculate the loss function as the mean squared error (MSE) of the mean and standard deviation (std) between the compressed versions of a batch of real ($h^l$) and synthetic ($h$) data. This loss is termed LTS. Equations (10), (11), and (12) provide the mathematical formulation of this loss function. The mapping function $\\hat{e}$ serves as the encoder for this autoencoder. We train the loss function autoencoder prior to training the overall framework (first phase). Unlike the latent autoencoder, which compresses the attribute dimension of a multivariate time series (MVTS), this particular autoencoder compresses the timestamp dimension of an MVTS."}, {"title": "D. Early Stopping and LSGANS", "content": "Another significant issue with GANs is stability, and TimeGAN is not exempt from this challenge. To improve the framework's stability, we implement an early stopping algorithm, acknowledging that the best results may occur after"}, {"title": "V. EXPERIMENTS", "content": "We assess SeriesGAN's performance on time series datasets that exhibit diverse characteristics, including periodicity, noise levels, length, and feature correlation. The datasets are chosen based on various combinations of these characteristics."}, {"title": "E. Contribution of Each Novelty", "content": "In this section, we analyze the contribution and impact of each novel component integrated into the SeriesGAN framework. First, we compare SeriesGAN with a variant that excludes the novel supervised loss used for autoregressive learning, allowing us to assess the significance of this feature. Next, we investigate the effect of removing the dual discrim- inator mechanism, retaining only the latent discriminator to evaluate its influence on network performance. Additionally, we eliminate the autoencoder-based loss function to observe its specific impact on the network's behavior. Finally, we disable the early stopping algorithm to examine the resulting changes in overall performance. Each modification helps isolate and understand the role of these components in enhancing the SeriesGAN framework."}, {"title": "VI. CONCLUSION", "content": "In this study, we introduce SeriesGAN, an innovative model designed for generating high-quality time series data. Se- riesGAN surpasses TimeGAN and other advanced methods by incorporating novel autoregressive training and a dual-discriminator approach. These enhancements significantly im- prove the model's ability to capture temporal dynamics, reduce information loss in the embedding space, and strengthen the overall effectiveness of adversarial training. This improvement"}]}