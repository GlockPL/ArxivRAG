{"title": "Hierarchical Retrieval-Augmented Generation Model with Rethink for Multi-hop Question Answering", "authors": ["Xiaoming Zhang", "Ming Wang", "Xiaocui Yang", "Daling Wang", "Shi Feng", "Yifei Zhang"], "abstract": "Multi-hop Question Answering (QA) necessitates complex reasoning by integrating multiple pieces of information to resolve intricate questions. However, existing QA systems encounter challenges such as outdated information, context window length limitations, and an accuracy-quantity trade-off. To address these issues, we propose a novel framework, the Hierarchical Retrieval-Augmented Generation Model with Rethink (HiRAG), comprising Decomposer, Definer, Retriever, Filter, and Summarizer five key modules. We introduce a new hierarchical retrieval strategy that incorporates both sparse retrieval at the document level and dense retrieval at the chunk level, effectively integrating their strengths. Additionally, we propose a single-candidate retrieval method to mitigate the limitations of multi-candidate retrieval. We also construct two new corpora, Indexed Wikicorpus and Profile Wikicorpus, to address the issues of outdated and insufficient knowledge. Our experimental results on four datasets demonstrate that HiRAG outperforms state-of-the-art models across most metrics, and our Indexed Wikicorpus is effective. The code for HiRAG is available at https://github.com/2282588541a/HiRAG.", "sections": [{"title": "1 INTRODUCTION", "content": "Multi-hop Question Answering (QA) involves complex reasoning by integrating multiple pieces of information to resolve intricate questions [6, 13, 28]. Unlike single-hop QA, where answers are readily available, complex questions require decomposing the original query into a series of targeted sub-questions. The knowledge required to answer each sub-question varies, drawing from both internal knowledge encoded in Large Language Models (LLMs) and external knowledge retrieved from local knowledge bases, such as Wikipedia [9], or open search engines like Google [26, 27]. As internal knowledge is derived from large-scale data and extensive pre-training, making it challenging to modify, we focus primarily on updating, mining, and effectively leveraging the retrieved knowledge to enhance QA performance.\nThe selection and integration of knowledge retrieved from multiple sources pose various challenges. Several key issues, such as outdated information, context window length limitations, and accuracy-quantity trade-off issues, significantly impact the performance of multi-hop QA systems. Firstly, new multi-hop QA systems often rely on outdated and insufficient knowledge within localized knowledge bases. As illustrated in Figure 1(b), a QA system searches an outdated corpus [9] to answer the query Who is the current president of the United States?"}, {"title": "2 RELATED WORK", "content": "Multi-hop QA is the task of answering natural language questions that involve extracting and combining multiple pieces of information and doing multiple steps of reasoning [12]. It can be divided into two steps, a retrieval (IR) step to extract all relevant context from the corpus, and a reading comprehension (MRC) step to find the answer from the reading result context. In the era of LLM, research [5, 14] finds that on the one hand, LLM can be used to complete the decomposition of the problem and achieve more fine-grained retrieval in the IR step, and on the other hand, Retrieval-Augmented Generation technology can be used to achieve the integration of retrieval information in the MRC step."}, {"title": "2.1 Retrieval-Augmented Generation", "content": "Retrieval-Augmented Generation (RAG) is a current research hotspot for Multi-hop QA tasks [10]. It can integrate the internal knowledge of the model with the external knowledge retrieved. LLM can retrieve external content through RAG to expand their knowledge base, thereby improving their ability to generate accurate and contextually relevant responses. Historically, various studies attempt to adapt the use of generative models to improve their performance. For instance, REPLUG [17] uses different retrieved content to generate corresponding answers and then combine them. Self-Rag [2] fine-tunes a generation model to simultaneously produce answers along with relevance, support, and usefulness scores. Concurrently, several methods for multi-hop QA emphasize the content and timing of retrieval. Self-Ask [15] lets the model generate sub-questions and queries, and continuously alternate between retrieval and generation. PROMPTAGATOR [4], Take a step back [32] focus on abstracting high-level concepts and utilizing LLMs for prompt-based query generation. Additionally, the confidence-based method, FLARE [8], generates queries using low-confidence tokens. However, most studies directly feed the retrieval content into the generation model, ignoring the evaluation and processing of the retrieval content. Unlike them, HiRAG highlights the importance of verifying retrieved content and adjusts the retriever to enhance the relevance of results when the quality of the retrieved information is subpar."}, {"title": "2.2 Chain-of-Thought (CoT)", "content": "In the task of Multi-hop QA in addition to using retrieval-enhanced generation to obtain knowledge, CoT is also needed to improve the logic. CoT can significantly enhance the reasoning capabilities of models [3]. For instance, iterative Context-Aware Prompter [21] employs an iterative approach to knowledge acquisition from the model to accomplish reasoning tasks. Similarly, LEAST-TO-MOST [33] decomposes complex problems into a series of sub-problems, addressing each step methodically. Inspired by the concept of self-consistency [23], MCR [30] extends the application of self-consistency beyond final results to include intermediate steps, thereby enhancing the overall accuracy of reasoning. Following [33], we design a comprehensive prompt to let the model decompose the question into multiple sub-questions and finally integrate the multiple sub-answers into the final answer through CoT. The main difference between our work and previous work is that we do not decompose the problem all at once, but proceed in a loop, generating only one sub-problem in each round; at the same time, we design a matching Definer to realize the judgment of whether the problem can be solved and exit the loop."}, {"title": "3 INDEXED WIKICORPUS AND PROFILE WIKICORPUS", "content": "The importance of corpus cannot be overstated in the realm of external knowledge acquisition. The Wikipedia corpus released by DPR [9] is widely recognized as a standard resource. However, as time progresses, this corpus faces challenges such as knowledge gaps and outdated information. Moreover, to accommodate dense retrieval requirements, the corpus is designed to fragment entity information into multiple segments, often resulting in incoherent data representation. To address these limitations, we develop a novel Wikipedia corpus called Indexed Wikicorpus along with a corresponding key entity profile corpus called Profile Wikicorpus. Our approach differs significantly from its predecessor in prioritizing the coherence of entity information. In Indexed Wikicorpus, each entry represents a complete entity as shown in Figure 3, ensuring a more comprehensive and cohesive representation of information. Building upon this foundation, we draw inspiration from the Web version of Wikipedia to extract entity profiles. These profiles, curated by Wikipedia, encapsulate the essential information about each entity, which is saved to Profile Wikicorpus. Data analysis about the number of words and entities as shown in Table 1 reveals that our new corpus contains a higher volume of entity information compared to its predecessor. Furthermore, subsequent experiments demonstrate the superior performance of our corpus over the older version."}, {"title": "4 HIERARCHICAL RETRIEVAL-AUGMENTED MODEL", "content": "To address the challenges in retrieval-augmented generation, including the lack of deep integration between different retrieval methods and the potential introduction of noise from multi-candidate retrieval, we propose a novel framework called HiRAG. This framework consists of five key components: Decomposer, Summarizer, Retriever, Definer, and Filter, as illustrated in Figure 4. The detailed algorithm is presented in Algorithm 1 in Appendix A. Specifically, the Decomposer is designed to tackle complex questions by decomposing them into smaller, more manageable sub-questions that can be easily answered. The Definer then determines whether the question can be solved. If it can, the Summarizer leverages the sub-answers to generate a response to the original question. Otherwise, the Retriever extracts relevant information related to the sub-question through a hierarchical retrieval process at both the document and chunk levels. Finally, the Filter verifies the validity of the retrieved content, generates a sub-answer, and re-evaluates the results if they are found to be inaccurate."}, {"title": "4.1 Notations and Definitions", "content": "For a multi-hop question, x, we can decompose it into a series of sub-questions Q, where answers to previous sub-questions can inform the generation of subsequent sub-questions. The objective is to iteratively obtain the set of sub-questions Q = {qi}_{i=1}^{A} and their corresponding set of sub-answers A = {ai}_{i=1}^{A}, where qi represents the i-th decomposed sub-question and ai its corresponding sub-answer. Ultimately, we combine all sub-answers with the original question, x, to get the final answer o. To augment the ability of the model to answer questions, we leverage external knowledge from the Wikipedia corpus, denoted as D, which comprises numerous sub-documents. Each sub-document, d, contains relevant content, including a title, t, and body text. For a given sub-question, we first employ a retriever to identify the most relevant sub-document, d = Retrieval (q, D). We then pinpoint the most relevant chunk of text, c, in d. Finally, we utilize the most relevant chunk to answer the sub-question using a LLM, yielding the sub-answer a = LLM(q, c)."}, {"title": "4.2 Decomposer and Summarizer", "content": "Building upon the previous work [33], we use the Decomposer module to break down complex problems into manageable sub-questions leveraging the prompt engineering [22]. We develop a comprehensive prompt for the Decomposer, which includes its background, goals, constraints, workflow, examples, and initialization. The prompt of the Decomposer module is formulated as follows:\nPrompt of Decomposer:\nBackground:\nYou are an expert at analyzing problems...\nGoal:\nHelping the user decompose the question and tell the user at the right time that the problem can be solved.\nConstraint:\nYou can only decompose the question, do not answer it directly...\nWorkflow:\n1. Analyse the original complex question...\nExample:\nInitialization:\nNow, a first simple question.\nWe initialize the model turn, mt, to 0 which indicates the round of the current decomposition problem. For the mt-th iteration, the Decomposer makes a new sub-question with the previous sub-answers, {a1, a2, ..., amt-1}, and original question, x. Then the Definer determines whether the original question can be solved with all known sub-answers based on the output of Decomposer. Therefore, this process can be formulated as follows:\nqmt = Decomposer({a1, a2 . . ., amt-1}, x),\nJudge = Definer (qmt),\nmt = mt + 1.\n(1)"}, {"title": "4.3 Hierarchical Retriever", "content": "In this section, we describe the design of a novel hierarchical retrieval mechanism that enhances retrieval accuracy and provides semantic clarification in cases of ambiguous semantics. Our approach employs a two-layer hierarchical retrieval process. The first layer identifies the most relevant documents within the corpus at the document level, while the second layer locates the most relevant chunks within those documents at the chunk level, resulting in a more accurate and refined retrieval process. We begin by obtaining a decomposed question, q, through the Decomposer module. Next, we utilize the LLM to extract the entity name, e, from the question. We then leverage this entity name to identify the document with the title, tc, that most closely matches the entity name among all available documents in the corpus. As this step primarily involves matching entity names with indexes, lexical matching takes precedence. Therefore, we employ sparse retrieval [18] to implement this step, capitalizing on its strengths in lexical matching.\nAfter we get the document with the title tc, we look for the corresponding information, p*, in the Profile WikiCorpus and add it to the retrieval result if it exists.\ne = LLM(q),\ntc = SparseRetrieval (e, {t1, t2..., tn}),\np* = SparseRetrieval(tc, {P1, P2..., pm}).\n(3)\nwhere n represents the number of documents in the Indexd WikiCorpus and m represents the number of profiles in the Profile WikiCorpus."}, {"title": "4.4 Filter", "content": "Even the most advanced retrieval engines can struggle to consistently retrieve the most accurate content. To mitigate the impact of potential retrieval engine errors, we utilize a specialized filter. This filter is designed to evaluate retrieval results and refine the retrieval engine through a series of iterative adjustments when inaccuracies are detected. Upon receiving the results from the retrieval engine cs, p*, and the sub-question, q, generated by the Decomposer, we first attempt to leverage a LLM to generate a response, r. The model then assesses whether the current sub-question, q can be resolved based on the response, r.\nr = LLM(q, cs, p*).\n(6)\nIf the question can be solved, the Filter passes r as the answer, a, to the sub-question to the Decomposer. However, when faced with an unresolvable question, the Filter triggers a two-tiered rethinking retrieval process, encompassing both chunk-level retrieval and document-level retrieval to facilitate a more comprehensive search.\nInitially, if the Filter determines that the problem cannot be resolved, it initiates a rethink at the chunk level. During this phase, the result, dc, from the first-tier retrieval by the retriever remains unchanged. Instead, modifications are made to the second tier of the retriever, sequentially selecting chunks with higher similarity scores from the divided chunks. This approach is particularly effective when the correct entity information and corresponding references are identified, but the specific chunk required to resolve the problem is not found. If the chunk-level rethink proves unsuccessful, the process escalates to the document level. At this level, adjustments are made to the first level of the retriever by selecting titles with higher similarity to the entity, e, from the candidate titles. Notably, this two-tiered rethink process operates as a nested loop, where each higher-level rethink informs and drives the lower-level rethinks, ensuring a comprehensive and systematic approach to problem-solving. Concurrently, we also address the issue of knowledge balance during the retrieval process. The rapid advancement of LLMs has led to a significant increase in the quantity and quality of their internal knowledge. As a result, a critical challenge in RAG emerges: striking a balance between the external knowledge retrieved and the internal knowledge of the model. While previous research [25] has focused on developing classifiers to tackle this challenge, we propose a novel and straightforward approach. By passing the number of filter rethinks as a parameter to the classifier, we observe that as the number of rethinks increases, the semantic similarity between the model and the retrieved content generally decreases.\nIn the context of the sub-question, q, if the retrieved result after the t-th round of rethink fails to yield an answer, we propose incorporating a probability y of leveraging the internal knowledge of the model to address the question.\ny = (\\frac{t}{m})^2\n(7)\nwhere m is a hyperparameter and t is the round of rethink. In our experiment, the maximum number of retrievals is set to 4, where the value of m we employ is 5. We recommend that m be greater than or equal to the maximum number of retrievals. This choice is the result of a trade-off. On one hand, the model should not abandon retrieval and resort to its internal knowledge too hastily, as the reliability of internal knowledge cannot be guaranteed. On the other hand, it should also ensure that when external knowledge is suboptimal and the retrieval method fails, it can attempt to utilize uncertain internal knowledge to provide an answer."}, {"title": "5 EXPERIMENTS SETUP", "content": "In our investigation, we conduct experiments on four datasets specifically designed for the Multi-hop question answering task, namely HotPotQA [28], 2WikiMultiHopQA [6], MuSiQue [20], and Bamboogle [15]. We only use questions and answers in the dataset, with all external knowledge retrieved."}, {"title": "5.1 Datasets", "content": "In our investigation, we conduct experiments on four datasets specifically designed for the Multi-hop question answering task, namely HotPotQA [28], 2WikiMultiHopQA [6], MuSiQue [20], and Bamboogle [15]. We only use questions and answers in the dataset, with all external knowledge retrieved."}, {"title": "5.2 Baselines", "content": "Our baselines can be divided into two categories according to whether external knowledge is retrieved: Without retrieval baselines and With retrieval baselines. The Without retrieval (W/o retrieval) setting includes four baselines. Direct Prompting directly uses the question as a prompt for LLM to respond. For retrieval, directly use the question as the query to search. CoT Prompting [24] adds \"Let's think this question step by step\" after the initial question. CoT-SC [23] consolidates its responses by answering the same question multiple times. Self-Ask [15] enables the model to decompose the problem into a series of sub-questions, allowing it to answer or retrieve relevant information for sub-questions. Direct and Self-Ask models belong to two working modes, so the With retrieval (W/ retrieval) setting includes five baselines. ReAct [29] is similar to self-ask, the difference is that react will extract the query to be retrieved from the sub-questions after breaking down the problem, while Self-ask will directly use the sub-questions as the query. Flare [8] uses a confidence-based strategy when generating questions. It generates questions about the less-confident parts of the generated content and retrieves them. MetaRAG [34] gains insights from metacognition, which can identify logical errors in reasoning and use the three-step metacognitive regulation pipeline to identify and repair deficiencies in initial cognitive responses."}, {"title": "5.3 Implementation Details", "content": "We use GPT-3.5-turbo as the backend LLM for the most part while using LLaMa-3-70B for the Decomposer. For baselines, we use the family of GPT-3.5 as the backend LLM. For online retrieval, we use Wikipedia and Google through the Wikipedia API and Serper API. For local retrieval, we use Contriever-MSMARCO [7] and elastic with the BM25 algorithm [16] as dense retriever and sparse retriever. The default maximum number of retrievals is 4. For HiRAG, we use Indexed Wikicorpus and Profile Wikicorpus for retrieval while we use the DPR corpus [9] for retrieval in baselines. Following [34], we employ four evaluation metrics to assess the performance of our model. At the answer level, we use Exact Match (EM) to determine whether the predicted answer aligns perfectly with the reference answer. Additionally, at the token level, we adopt a more fine-grained approach, which includes token-level F1 score, precision, and recall. Following MetaRAG [34], for cost considerations, we sub-sample 500 questions from the validation set of HotPotQA, 2WikiMultiHopQA, and MuSiQue for experiments while using the full set of Bamboogle for experiments as the quantity of Bamboogle is small."}, {"title": "6 RESULTS AND ANALYSIS", "content": "The main results are shown in Table 2 and reveal the following notable findings. (1) Our proposed framework, HiRAG, exhibits superior performance across multiple evaluation metrics, outperforming state-of-the-art methods on three out of four datasets. Additionally, it demonstrates improvements in several metrics on the remaining dataset. The key advantage of our approach is its emphasis on the retrieval process, which consistently produces high-quality results. This highlights the crucial role of the retrieval component in achieving exceptional outcomes. (2) In the Without Retrieval setting, HiRAG significantly outperforms the baselines, showcasing its effectiveness in question decomposition and answer summarization. This is due to the framework's design, which enables autonomous sub-question generation and termination of the loop when the original problem is resolvable. Our approach also differs from self-ask in its segregation of subtasks and implementation of each subtask through a separate prompt, resulting in superior performance. (3) A comparative analysis of results across datasets reveals that HiRAG achieves the most significant breakthrough in 2WikiMultiHopQA, with a notable improvement of over 12% in the EM index compared to the state-of-the-art method. This is primarily attributed to the fact that most external knowledge required by 2WikiMultiHopQA can be retrieved from Wikipedia, and the question format in this dataset is relatively standardized. However, the MuSiQue dataset poses a greater challenge due to the complexity of the questions and the inability to directly retrieve required knowledge from Wikipedia. This complexity affects our framework's ability to evaluate retrieval results, diminishing the effectiveness of the response."}, {"title": "6.1 Main Results", "content": "The main results are shown in Table 2 and reveal the following notable findings. (1) Our proposed framework, HiRAG, exhibits superior performance across multiple evaluation metrics, outperforming state-of-the-art methods on three out of four datasets. Additionally, it demonstrates improvements in several metrics on the remaining dataset. The key advantage of our approach is its emphasis on the retrieval process, which consistently produces high-quality results. This highlights the crucial role of the retrieval component in achieving exceptional outcomes. (2) In the Without Retrieval setting, HiRAG significantly outperforms the baselines, showcasing its effectiveness in question decomposition and answer summarization. This is due to the framework's design, which enables autonomous sub-question generation and termination of the loop when the original problem is resolvable. Our approach also differs from self-ask in its segregation of subtasks and implementation of each subtask through a separate prompt, resulting in superior performance. (3) A comparative analysis of results across datasets reveals that HiRAG achieves the most significant breakthrough in 2WikiMultiHopQA, with a notable improvement of over 12% in the EM index compared to the state-of-the-art method. This is primarily attributed to the fact that most external knowledge required by 2WikiMultiHopQA can be retrieved from Wikipedia, and the question format in this dataset is relatively standardized. However, the MuSiQue dataset poses a greater challenge due to the complexity of the questions and the inability to directly retrieve required knowledge from Wikipedia. This complexity affects our framework's ability to evaluate retrieval results, diminishing the effectiveness of the response."}, {"title": "6.2 Generalization", "content": "To assess the generalizability of HiRAG, we conduct a series of experiments where we substitute different base models, with the results presented in Table 3. Specifically, we evaluate the performance of HiRAG when paired with several prominent base models, including LLaMA-3-70B, LLaMA-3-8B, and Qwen2-7B. The results demonstrate that our method exhibits robustness and effectiveness across different base models, showcasing its adaptability to various model architectures. When paired with LLaMA-3-70B, our approach achieves state-of-the-art performance on most datasets, underscoring its ability to enhance the performance of strong base models and highlighting its potential for widespread applicability. Notably, to ensure a fair comparison with existing SOTA models, like Meta-RAG, which is built on GPT-3.5-turbo, we use the same base model in our experiments, even though LLaMA-70B has shown better performance. This allows for a more direct and meaningful comparison of our approach with prior work."}, {"title": "6.3 Ablation Experiments", "content": "We conduct ablation experiments to evaluate the contributions of our Retriever module, focusing on the hierarchical retrieval approach. By removing the processing during rethinking at the chunk level and document level, we investigate the impact of these components on the overall performance. The results, presented in Table 4, demonstrate the effectiveness of our approach. Our method is designed to improve the accuracy of retrieved results through chunk level and document level rethink. To isolate the impact of each level, we perform ablation experiments by eliminating one level of rethink at a time. Specifically, we remove chunk level rethink by considering only the highest-scoring chunk within a document and remove document level rethink by retrieving only the highest-scoring document. The results show that eliminating either level of rethink leads to a decrease in performance, confirming the importance of both chunk level and document level rethink in our hierarchical retrieval approach. Furthermore, we investigate the influence of knowledge incorporated in the Profile WikiCorpus. Our findings indicate that removing this knowledge component leads to a decrease in the performance of HiRAG."}, {"title": "6.4 Indexed Wikicorpus", "content": "We conduct experiments under the FLARE framework to assess the effectiveness of our corpus in improving model performance. As shown in Table 5, replacing the external knowledge source with our corpus leads to notable advancements in Exact Match (EM) scores across four datasets, with three datasets experiencing improvements in all evaluated metrics. The comprehensive coverage and meticulous entity name segmentation in our corpus are key factors contributing to these improvements, enabling more efficient retrieval and utilization of relevant information."}, {"title": "6.5 Retriever Module as Plug-in", "content": "We extract retriever and filter modules HiRAG, pairing them with Indexed Wikicorpus and Profile Wikicorpus to create a novel intelligent retriever. This approach leverages LLMs to evaluate and refine retrieval results, differing from traditional retrievers. We conduct experiments on standardized RAG tasks for single-hop questions using gold decomposed question-answer pairs from 2WikiMultiHopQA and MuSiQue datasets. We compare the performance of five methods: (1) Direct Answering, (2) Sparse Retrieval with Elastic and BM25, (3) Dense Retrieval with Contriever-MSMARCO, (4) HiRAG (Online), and (5) HiRAG (Local). The results in Table 6 clearly demonstrate the superiority of our retrieval engine over traditional approaches, with significant gains observed even in local retrieval scenarios."}, {"title": "6.6 Case Study", "content": "We provide a case study in Figure 5 of Appendix A.3 to demonstrate the workings of our framework. We gradually generate sub-questions, retrieve sub-questions, evaluate the retrieval results, and answer the sub-question. When the retrieval results are not ideal, we use rethink to find a more satisfactory result. Finally, we use the answers to all sub-questions to get the answer to the original question."}, {"title": "7 CONCLUSION", "content": "We present a novel framework for multi-hop question answering, addressing the challenges of outdated and insufficient knowledge, context window limitations, and accuracy-quantity trade-offs. Our proposed framework, HiRAG, incorporates a hierarchical retrieval strategy, single-candidate retrieval, and a rethink mechanism to improve the efficiency and effectiveness of knowledge retrieval. The experimental results demonstrate that HiRAG outperforms state-of-the-art models on multiple datasets, confirming the effectiveness of our approach. Additionally, our newly constructed corpora, Indexed Wikicorpus which is shown to be more comprehensive and logically organized, and Profile Wikicorpus. Our contributions provide a significant step forward in improving the performance of multi-hop QA tasks, and we believe that our framework and corpora will be valuable resources for future research in this area. In the future, we hope to expand our research on the retrieval-verify-rethink pipeline to achieve more fine-grained and accurate retrieval."}]}