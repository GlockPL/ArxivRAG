{"title": "Knowledge-Augmented Explainable and Interpretable Learning for Anomaly Detection and Diagnosis", "authors": ["Martin Atzmueller", "Tim Bohne", "Patricia Windler"], "abstract": "Knowledge-augmented learning enables the combination of knowledge-based and data-driven approaches. For anomaly detection and diagnosis, understandability is typically an important factor, especially in high-risk areas. Therefore, explainability and interpretability are also major criteria in such contexts. This chapter focuses on knowledge-augmented explainable and interpretable learning to enhance understandability, transparency and ultimately computational sensemaking. We exemplify different approaches and methods in the domains of anomaly detection and diagnosis - from comparatively simple interpretable methods towards more advanced neuro-symbolic approaches.", "sections": [{"title": "1. Introduction", "content": "In the context of complex systems, anomaly detection [1, 2] and diagnosis [3-7] are important tasks for gaining insight into the behavior of a system, for example to prevent critical situations or to address specific problems like faults or failures in technical systems. However, this does not only apply to technical systems [8-11], but also to biological systems [12-14] or the medical domain [15\u201317]. Here, with the ever-growing amount of collected data, we can investigate, analyze and interpret observed phenomena at considerably larger scale and in much more detail.\nParticularly in prominent areas of artificial intelligence and machine learning, such as anomaly detection and diagnosis, there usually exists some knowledge which can be exploited for guiding and/or augmenting specific learning methods. Augmentation can take place at different levels, e. g., serving as a starting point for these models, providing constraints for them, or integrating specific hybrid models consisting of both data-driven as well as knowledge-based components. However, with the constantly growing amount of available data and accordingly refined models, e. g., from the field of machine learning, gaining insights into a system, model or a decision made by those \u2013 becomes much more difficult due to its inherent complexity [18]."}, {"title": "2. Knowledge-Augmented Pattern Mining for Anomaly Detection", "content": "In general, pattern mining provides a broadly applicable and powerful set of methods for exploratory data analysis, knowledge discovery, and computational sensemaking on complex data [19, 31, 51, 53, 54]. Exemplary methods include association rule mining [55], subgroup discovery [31, 56], exceptional model mining [31, 57, 58] as well as graph (pattern) mining [59\u201361]. In particular, whenever semi-automatic and interactive approaches can be provided, constraints and user interests can often be directly integrated [62\u201364]. An alternative and/or complementary approach is to inject domain knowledge into the pattern mining process. Here, first approaches for such an integration utilizing knowledge graphs - exploiting information formalized in ontologies and a set of instance data - have been proposed in the area of semantic data mining [65\u201368]. Furthermore, [47] presents a mixed-initiative approach for semantic feature engineering using a knowledge graph, which is constructed in a semi-automatic process. Here, the resulting features are then provided for data analysis. A similar approach is applied in [69]. Here, data from heterogeneous data sources is integrated into a knowledge graph, which then provides the basis for knowledge discovery.\nIn this chapter, we review and discuss approaches and an application use case for knowledge-augmented pattern mining [31, 47, 48], specifically focusing on subgroup discovery connected to domain knowledge formalized in knowledge graphs. Hence, we focus on an integrated approach, exploiting knowledge-based semantic structures, i. e., sets of knowledge components connected to a local pattern mining method. Exceptions and anomalies can then be detected via appropriate measures of interestingness."}, {"title": "2.1. Pattern Mining Using Subgroup Discovery", "content": "Subgroup discovery is a flexible approach for exploratory data analysis and knowledge discovery, enabling computational sensemaking on complex data. In its simplest form, subgroup discovery aims to detect relations between dependent (characterizing) variables and an independent target concept, e. g., comparing the share or the mean of a nominal/numeric target variable in the subgroup vs. the share or mean in the total population, respectively [31, 56, 70]. Thus, in the simplest case, a binary target variable is considered, where the share in a subgroup can be compared to the share in the dataset in order to detect (exceptional) deviations. In the most general formalization, however, the interestingness of a pattern is flexibly defined by a specific interestingness measure. For example, the so-called target concept can then also relate to a complex model, e. g., a linear regression model or Bayesian network induced on the subgroup vs. one such model learned from the total dataset, where significant deviations can be estimated. The latter case is the focus of exceptional model mining [31, 57, 58] as a specialization of the general subgroup discovery approach. The patterns are typically described by a description language using conjunctions of attribute-value restrictions, i. e., referring to value constraints on features, similar to simple rules [31]. In contrast to typical machine learning approaches, subgroup discovery aims at discovering local patterns, e. g., \u201cnuggets\u201d in the data [71, 72]. Due to their simple conjunctive structure, subgroup patterns are typically interpretable, relating to interpretable machine learning [23] which is particularly important when the discovered patterns need to be actionable [73-75]."}, {"title": "2.2. Subgroup Discovery Basics", "content": "For a more formal view on subgroup discovery, we adapt the presentation in [31], to which we refer to for a detailed discussion. Basically, we distinguish between patterns which are typically provided in conjunctive form \u2013 and subgroups as extensions of such descriptive patterns. The patterns cover specific subsets of a dataset (or database). Then, an interestingness measure (also called quality function) formalizes which patterns are considered interesting for ranking patterns (and subgroups). For the interestingness measure, this typically includes the concept of interest, as already sketched above. As we will see below, such an interestingness measure can also capture a typical key performance indicator (KPI), e. g., for process analysis, which formalizes the specific constraints of the user relating to discovering interesting subgroups for the analysis task at hand.\nMore formally, we consider a database DB = (I,A), which is given by a set of individuals I (often called instances) and a set of attributes A. In the following, we restrict our focus to nominal attributes, where numeric attributes can be handled via discretization, e. g., [70, 76]. For nominal attributes, a selection descriptor $(a = v)$ is a Boolean function $I \\rightarrow \\{0,1\\}$ that is true if the value of attribute a \u2208 A is equal to v for the respective individual. A denotes the set of all selection descriptors. As basic elements in subgroup discovery, we consider patterns, i. e., subgroup descriptions and subgroups. A subgroup description p is given by a set of selection descriptors $p = \\{d_1,..., d_l \\}$, $d_i \\in \\Delta, i=1,...,l$, i. e., having length l, which is being interpreted as a conjunction of the contained descriptors. We also call p a (complex) pattern. In principle, such a pattern can be interpreted as the body of a conjunctive rule. Furthermore, we distinguish between the pattern \u2013 as an intensional description \u2013 and the extent of the pattern, i. e., the respective subgroup described by the pattern. A subgroup $s = \\{i \\in I|p(i) = true\\}$, is the set of all individuals that are covered by the subgroup description p.\nIn a top-k setting, a subgroup discovery algorithm returns the top-k subgroups according to a given quality function $q: 2^{\\Delta} \\rightarrow \\mathbb{R}$, cf. [31]. The applied quality function thus estimates the interestingness of a pattern. For example, for a binary target concept, a standard quality function combines the size $n_p = |s|$ of a subgroup s described by pattern p, i. e., its support, and the share $t_p$ of the target concept t in s, i. e., its confidence as follows: $q^{\\epsilon}(p) = n_p \\cdot (t_p - t_0)$, where $t_0$ denotes the (default) share of the target concept in the database DB, and $\\epsilon \\in \\mathbb{R}$. Standard quality functions include the so-called Piatetsky-Shapiro quality function $q^1$, the binomial test quality function $q^{0.5}$, and the gain quality function $q^0$. The latter focuses on improving the target share and thus can be applied for finding patterns with a high confidence, however, it then requires a minimal size threshold ensuring sufficient support. While a quality function provides a ranking of the discovered subgroup patterns, often also a statistical assessment of the patterns is useful in data exploration and knowledge discovery. Quality functions that directly apply a statistical test, for example the Chi-square quality function, e. g., [31], provide a p-value for simple interpretation.\nUsing a given subgroup discovery algorithm, e. g., [31] the result of top-k subgroup discovery is then the set of the k patterns $p_1,...,p_k$, where $p_i \\in 2^{\\Delta}$, with the highest interestingness value according to the applied quality function."}, {"title": "2.3. Use Case: Anomaly Detection in Industrial Logistics Data", "content": "In the following, we summarize an application use case of knowledge-augmented pattern mining for anomaly detection in the domain of industrial logistics, adapting the presentation in [48]. Here, the goal for anomaly detection is to identify exceptional patterns in the context of inventory differences. More specifically, the major goal of the analysis was to identify specific logistic processes in a larger production context which could potentially indicate erroneous financial assessments. These are called inventory differences in this context, cf. [48]. Overall, the whole process was considerably assisted by domain specialists, where large-scale data as well as domain dependencies were integrated into a knowledge graph representation. A further important goal was to provide interpretable representations in order to enable explanation and transparency, which was addressed via a knowledge-augmented pattern mining approach using subgroup discovery."}, {"title": "3. Knowledge-Augmented Interpretable Learning of Diagnostic Scoring Systems", "content": "Knowledge-based diagnosis is a prominent area of research, including early approaches in the field of expert systems and knowledge systems. These often use explicitly formalized knowledge, often complemented by ontology-based approaches [4, 80-84, 84-88] as well as more recent data-driven learning methods [49, 89\u201394]. These become more and more important in the context of the diagnosis of complex systems that generate large amounts of data, e. g., in medical [17, 89, 95] or industrial [69, 84, 96] domains. In general, the automated diagnosis of such complex systems is a challenging task. To successfully apply either approach, adequate knowledge and data is required, while the connection between the systems, e. g., for handling different types of information in respective abstractions, is not automatically provided. Furthermore, knowledge acquisition is often costly, while purely data-driven techniques require large amounts of data of sufficient quality. Therefore, hybrid knowledge-augmented approaches can combine the advantages of both purely knowledge-based and solely data-driven approaches, i. e., an initial set of high-quality knowledge, which can be extended, refined and fine-tuned by data-driven learning. In this section, we summarize such an approach [49\u201351] based on scoring systems [93, 97], namely diagnostic scores [15, 97]. We discuss initial knowledge-augmented learning as well as knowledge refinement, and present an exemplary use case in the medical domain."}, {"title": "3.1. An Overview on Scoring Systems", "content": "For diagnosis, diagnostic scores are a widely used formalism for medical decision-making, e. g., [15, 16, 98, 99]. We follow the presentation in [49] for such scoring systems, also including refinement and adaptation methods described in [50, 51]. In particular, below, the term scoring system denotes a set of diagnostic scores in the form of scoring rules, i. e., the resulting (scoring) rule base.\nEssentially, scoring systems as linear models are a relatively simple and lightweight knowledge representation. For a diagnostic application, we distinguish inputs (features) and outputs as concepts (diagnoses) of a model. In order to derive a concept (e. g., a diagnosis), a limited number of features is utilized in a regular and easy-to-interpret manner. Then, in its simplest form, each observed feature contributes a single point to a score. If the score exceeds a predetermined threshold, then the diagnostic concept is considered as established. There are variations where multiple categories are used instead of just using one point, where both negative and positive contributions are considered, and where multiple thresholds are employed to express varying degrees, such as distinguishing between \"possible\u201d and \u201cprobable\" in the derivation of a concept. Furthermore, we can also generalize categories to arbitrary points (natural numbers) or real values for general scoring systems. For diagnostic scores, however, simple to interpret (symbolic) categories are often preferred for their improved understandability. Diagnostic scores are typically implemented using scoring rules, which assign specific points to a diagnosis as defined in the rule action. Unlike general rules, scoring rules usually do not involve logical combinations in their preconditions. However, they can be arranged hierarchically such that a concept inferred through a score can be used to infer another concept, enabling more complex (hierarchical) models."}, {"title": "3.2. Diagnostic Scores", "content": "In the following, we provide a more formal view on diagnostic scores, summarizing the presentation and definitions given in [49]. For a diagnostic model implemented as a knowledge system, we consider attributes respectively attribute values (or findings) as inputs, and diagnoses as its outputs. We define QA as the universal set of all attributes available in the problem domain. Then, a value $v \\in dom(a)$ assigned to an attribute $a \\in \\Omega_A$ is called an attribute value or finding. Furthermore, let $\\Omega_{\\mathcal{F}}$ denote the set of all possible attribute values in the given problem domain. An attribute value $f \\in \\Omega_{\\mathcal{F}}$ is denoted by a:v for $a \\in \\Omega_A$ and $v \\in dom(a)$. The set $F_a \\subseteq \\Omega_{\\mathcal{F}}$ of possible attribute values for a given attribute a is defined as $F_a := \\{f \\in \\Omega_{\\mathcal{F}} | f = a:v \\land v \\in dom(a)\\}$. Each attribute value $f\\in \\Omega_{\\mathcal{F}}$ is defined as a possible input of the model, e. g., a diagnostic knowledge system. We denote a diagnosis d as a possible output of the respective model. $\\Omega_{\\mathcal{D}}$ is defined as the universe of all possible diagnoses for a given problem domain.\nFor modeling scoring systems (or diagnostic scores) we consider simple scoring rules. A simple scoring rule r is denoted as $r := f \\stackrel{s}{\\rightarrow} d$, where $f \\in \\Omega_{\\mathcal{F}}$ is an attribute value, $d \\in \\Omega_{\\mathcal{D}}$ is the targeted diagnosis, and $s \\in \\Omega_{\\mathcal{S}}$ denotes a symbolic confirmation category. $\\Omega_{\\mathcal{S}}$ denotes the full set of symbolic confirmation categories. Basically, enlarging this set enables a more fine-grained approach for learning and fine-tuning, while a coarser approach facilitates manual specification, fine-tuning and adaptation, since the choice of the specific categories is restricted. In general, the category s reflects the concept of the points in a scoring system in a symbolic and thus more interpretable form for the given rule r. Diagnostic scores are then used to represent a qualitative approach for deriving diagnoses with symbolic confirmation categories. These categories state the degree of confirmation or disconfirmation of a particular diagnosis. Categories can be aggregated according to some suitable aggregation scheme, e. g., such that four equal categories together result in the next higher category. Then, according to the obtained confirmation categories (the symbolic score), the respective concept (diagnosis) is established."}, {"title": "3.3. Learning Diagnostic Scores", "content": "The approach described in [49] aims to learn diagnostic scoring rules in a data-driven way by estimating the association strength between all findings $f\\in \\Omega_{\\mathcal{F}}$ and all diagnoses $d \\in \\Omega_{\\mathcal{D}}$, basically relying on statistical tests. In principle, first so-called diagnostic profiles are constructed for each diagnosis $d \\in \\Omega_{\\mathcal{D}}$ for which all contributing findings $f \\in \\Omega_{\\mathcal{F}}$ having a positive or negative association with the diagnosis above a certain user-specified threshold are considered. Then, the respective score of the rule, i. e., the symbolic scoring confirmation category is estimated based on the predictive performance of the finding with respect to the diagnosis. For this, standard measures [100, 101] like precision and the false positive rate are applied, cf. [49] for details. The direction of the statistical association is used for inferring positive or negative scores (points), i. e., for a positive association a positive score is applied, and for a negative association a negative score likewise. Finally, the derivation of the symbolic confirmation categories is obtained via a mapping table, which is predefined according to domain knowledge and user requirements, e. g., regarding the applied thresholds for determining the categories as well as the number of categories. If being integrated with other optimization/refinement approaches (e. g., as discussed below), then this mapping can also be applied at a later stage, for example, when optimization/refinement has been finalized."}, {"title": "3.4. Refining Scoring Rules", "content": "Whenever an initial scoring rule base has been provided, either manually or using a learning approach, its optimization and maintenance are important for the further development of the according scoring system [50, 51, 92, 93, 103]. For example, in the medical domain, scoring systems are often constructed manually by domain specialists first. Here, a domain specialist typically assigns ratings to all correlations between findings and solutions, estimating the respective point score based on the domain specialist's expertise. However, if the impact of a combination of attribute values is not proportionately strong compared to the effect of individual attribute observations, for example, then this knowledge representation becomes inadequate. This is because the contribution of attribute values to the diagnostic score is strictly linear. Then, refinement and adaptation of the scoring rules contributions becomes necessary, which can be supported by automatic methods. This includes semi-automatic methods [50, 51, 103] as well as optimization-based approaches [92, 93].\nFor refinements of scoring systems, providing control during the modification and refinement process is usually important. This is enabled, for example, via interactive approaches, e. g., [103], based on pattern mining methods using subgroup discovery for detecting patterns that cause incorrect behavior of the scoring rule base such that errors in the predictions are identified. For example, rules can be adapted and/or extended, e. g., by adding conditions to the rule, by modifying the respective action, or by tweaking the assigned confirmation category. Using visualization, the respective analytical elements uncovered by the pattern mining approach can be interactively checked, and according modifications can be performed, cf. [103] for further details.\nFurthermore, we can apply semi-automatic optimization methods [50, 51] for getting proposals for adapting the respective confirmation categories based on ideas of perceptron learning. Given suitable test cases for the scoring systems, adaptations of the points / confirmation categories can then be proposed \u2013 similar to adapting the perceptron weights during its training phase, cf. [104]. There are also further optimization approaches [92, 93] both for learning and refining scoring systems. Combinations of the aforementioned techniques are also possible so that interactive approaches can be linked with advanced optimization and refinement techniques, as well as integrated with other diagnosis formalisms, e. g., [105]."}, {"title": "3.5. Use Case: Bootstrapping Diagnostic Knowledge Systems", "content": "A prominent use case is to bootstrap diagnostic knowledge systems, that is, to support the domain specialists when building such a diagnostic knowledge system from scratch. Applying the learning method, an initial set of scoring rules can be obtained, which can be adapted and refined in a second step."}, {"title": "4. Explainable Neuro-Symbolic Anomaly Detection and Diagnosis", "content": "For the explainable neuro-symbolic approach to anomaly detection and diagnosis described in [52], the overall idea is to utilize a knowledge graph (KG) that guides the diagnostic process, combined with neural networks that enable the interpretation of sensor signals suggested by the KG for investigation. Essentially, this integrates knowledge- and machine-learning-based fault diagnosis, combining both paradigms. A key element is an iterative diagnosis cycle in which rough hypotheses are refined using both knowledge-based and data-driven methods. Explainability is essential for diagnosis and is enabled via the hybrid approach. The advantage of the neuro-symbolic approach to the problem of automated diagnosis compared to previous methods is that it is designed in a way that both paradigms are mutually beneficial. Thus, it is motivated by the previous lack of explainability and exploitation of available domain knowledge in data-driven methods, and the extensive manual effort and shortcomings with respect to sensor signal evaluation in expert systems. Ultimately, a comprehensive explanation is constructed by contextualizing all diagnostic artifacts with symbolic state transitions as an explanatory report. Additionally, they augment the KG and enable to learn the most significant aspects of the signal types over time.\nBelow, we first summarize how to generate explanations for time series predictions using saliency maps [107, 108]. After that, we review neuro-symbolic fault diagnosis as presented in [52] before we summarize and discuss an exemplary application use case in the automotive domain."}, {"title": "4.1. Saliency Map Generation for Time Series & Explanatory Reports", "content": "Despite the commonplace of a black box nature of deep learning approaches, CNNs, among other architectures, offer the advantage of explainability, e. g., through Class Activation Mapping techniques [108, 109], providing explanatory insights into the temporal / spatial segments that are important for a network's prediction, cf. [107, 108]. A crucial idea of the overall approach in [52] is to close the loop and feed this information back into the KG. In case of an anomaly, it is the information where the error is located in the signal, i. e., generally where the system tells us to look to identify the problem under consideration, the region of interest (ROI). This is very valuable knowledge that is unavailable a priori. Moreover, it is a very useful debugging resource, highlighting issues such as overfitting and deviation from expert judgements. Thus, subsequent to the classification of a signal, the explanation of the decision proceeds on the basis of Class Activation Maps (CAMs). This is to ensure that not only accurate predictions are obtained, but also predictions that are comprehensible for users, which should reduce the proneness to errors, and can further enable computational sensemaking [19, 110].\nThere are several techniques used in deep learning to visualize areas of an image that are most relevant to predicting a certain class, e. g., Grad-CAM [109], HiResCAM [111], Grad-CAM++ [112], Score-CAM [113], SmoothGrad [114], and LayerCAM [115]. They provide a way to interpret the decision made by an ANN model (with compatible architecture, unless model-agnostic) by highlighting the regions of the input image that contribute the most to the classification result. Then, this can then provide important information for human interpretation of the model and according explainability for a prediction, towards computational sensemaking [116, 117]."}, {"title": "4.2. Neuro-Symbolic Fault Diagnosis", "content": "The neuro-symbolic fault diagnosis system proposed in [52] makes use of both KGs and their reasoning capabilities as well as explainable learning through CNNs. It thus combines the advantages of KGs, which incorporate human expertise into the diagnostic process, with the advantages of deep neural networks, which excel at recognizing patterns in complex signals.\nThe fault diagnosis system is applicable in a variety of domains in which the following conditions are met: The system to be diagnosed consists of multiple components with causal relationships between one another, i. e., an anomaly in one component can transfer to another component, and these causal relationships can be formally described in a KG. Moreover, each component can be inspected individually, and checking all components would not be an efficient solution, i. e., it is desired to minimize the number of inspections. Lastly, it is expected that (sensor) data like time series or images can be recorded at the components and classified by a neural network.\nAccording to the taxonomy in [118] that categorizes neuro-symbolic systems based on the integration of the neural and symbolic components, this approach belongs to the Symbolic[Neuro] category. The KG initiates the anomaly classification networks as needed and uses the classification result to guide the subsequent diagnostic step(s)."}, {"title": "4.3. Use Case: Neuro-Symbolic Fault Diagnosis in the Automotive Domain", "content": "Neuro-symbolic fault diagnosis for automobiles, instantiating the approach described in Section 4.2, has the potential to relieve mechatronics engineers of many time-consuming and error-prone tasks. The practical motivation of [52] is to tackle the increasing complexity and diversity of modern vehicles, which pose a major challenge for their diagnosis, i. e., to extract meaning from multimodal data that is unmanageable for humans, and to recognize complex patterns.\nSymbolic Knowledge Representation To capture and structure diagnosis-relevant knowledge, an ontology was defined (cf. Figure 4), which leads to a KG by populating it with large amounts of instance data, e. g., through expert interviews, supplemented with industry partner data. Essentially, there are three levels of abstraction: The raw definition of the ontology, vehicle-agnostic expert knowledge regarding on-board diagnostics (OBD, ISO 15031-6), and vehicle-specific diagnostic knowledge automatically generated based on OBD logs read in workshops and acquired as part of the diagnostic process (recorded sensor data, interpretations, etc.). However, as illustrated in Figure 3, the two types of knowledge are not isolated from each other, but connected by meaningful links (e. g., connecting classification instances to the diagnostic associations that led to them) to learn from previous diagnostic runs. All three levels combined constitute the KG. The acquisition of expert knowledge is accomplished via a web interface (collaborative knowledge acquisition component) through which the knowledge is entered, stored in the Resource Description Framework (RDF) format, and hosted on an Apache Jena Fuseki server. In addition, this knowledge is retrievable in the diagnostic process via predefined SPARQL queries (KG query tool), as well as making the KG expandable and editable in general (ontology instance generator).\nExpert Knowledge Modeled in the Ontology At the core of the knowledge captured in the ontology are the standardized Diagnostic Trouble Codes (DTCs), which are a perfect example of what is meant by fault context in the neuro-symbolic architecture visualization in Figure 3. In order to efficiently handle the occurrence of DTCs in a vehicle's electronic control unit (ECU), a DTC parser has been developed that is capable of resolving all digits of the read code and outputting the resulting error information. A DTC can have DiagnosticAssociations with physical components in a car (SuspectComponent).\nA crucial aspect of such an association is the priority_id, based on which components are suggested to be examined in a certain order in the presence of a given DTC. In addition, affected_by represents a list of other components whose malfunction could affect the correct functionality of the considered component (dependencies can be conceived as a tree, cf. Figure 7). Experts can define a ComponentSet to reduce the number of redundant diagnostic steps in case there is a specific component that can be leveraged to verify the correct functioning of a whole set of components. Furthermore, each component is contained in a VehicleSubsystem, which is associated with a specific part of the vehicle. Moreover, each DTC represents a FaultCondition manifested by one or more Symptoms.\nDiagnosis Knowledge Modeled in the Ontology There is another theme to the ontology, which is the acquisition and reasonable arrangement of diagnostic data. For each Vehicle instance that is entered into the KG, i. e., for each vehicle that is diagnosed with the system, a DiagLog is created that provides the KG with a kind of explanatory summary of the entire diagnostic process. However, this is not a mere summary, but each entry is automatically sorted into the existing web of expert knowledge and past diagnostic data by instantiating the concepts of the ontology. Initially, any recorded DTC appears in this log, as this is always the starting point for a diagnosis. Perhaps most significant are the diagnostic steps, which are also part of the log in the form of Classification instances that store their reason, either another classification that detected an anomaly (reasonFor) or a diagnostic association with a DTC recorded in the vehicle (ledTo). Each classification has a binary result (prediction) that indicates\nEnhancement of Expert Knowledge The expert knowledge enhancer can be used to augment the KG hosted by the Fuseki server with vehicle-agnostic OBD knowledge. In particular, it generates semantic facts based on the information entered through a web interface and connects these facts in a meaningful way to what is already available in the KG, i. e., it serves as a backend for the knowledge acquisition component. There are three general types of instance data that can be entered along with their specific associated information: DTCs, vehicle components, and semantically meaningful sets of components grouped together. Alternatively, it is possible to select an existing instance, view the currently available data, and refine it. Entering a new instance leads to a series of operations in the backend. Quite a number of semantic facts have to be generated when an expert enters few information. Thus, it is always expert knowledge input via the web interface and corresponding generation of semantic facts in the backend. All of this is accompanied by a series of input validation mechanisms. This way, a simple KG extension for the expert goes hand in hand with an automatic proper \"wiring\" of semantic facts in the background.\nEnhancement of Vehicle-Specific Diagnosis Knowledge As a further element, the diagnostic knowledge enhancer, on the other hand, enhances the KG with diagnos-"}, {"title": "5. Conclusions", "content": "In this chapter, we provided an overview on knowledge-augmented, explainable and interpretable learning in the context of anomaly detection and diagnosis. For this, we have outlined the general context and discussed exemplary methods in this domain, specifically considering the articles [47,48] for discussing pattern mining in the context of anomaly detection as well as [49\u201351] on learning scoring systems. Furthermore, we provided a significantly adapted summary of the article [52] on a neuro-symbolic approach for anomaly detection and diagnosis. Thus, we covered simple knowledge representations such as diagnostic scores, pattern-based methods in combination with various forms of symbolic knowledge, as well as a hybrid neuro-symbolic method integrating deep learning with symbolic knowledge.\nFor future work, further extending knowledge-augmented approaches, particularly regarding advanced neuro-symbolic approaches, is a major interesting research direction. Here, directly integrating interpretability and explainability in the methodology is an interesting option to consider. Specifically, this also relates to different types of complex data, e. g., graphs, images, etc. Moreover, we are planning to work on a further generalization of the neuro-symbolic diagnosis framework presented in [52] as well as a systematic evaluation of its architecture using randomized, parameterized and domain-agnostic synthetic problem instances and corresponding ground truth solutions generated based on the abstract formulation of the general problem of diagnosing systems with interconnected components based on sensor signal assessment."}]}