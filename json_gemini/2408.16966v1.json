{"title": "USERSUMBENCH: A Benchmark Framework for Evaluating User Summarization Approaches", "authors": ["Chao Wang", "Neo Wu", "Lin Ning", "Luyang Liu", "Jun Xie", "Shawn O'Banion", "Bradley Green"], "abstract": "Large language models (LLMs) have shown remarkable capabilities in generating user summaries from a long list of raw user activity data. These summaries capture essential user information such as preferences and interests, and therefore are invaluable for LLM-based personalization applications, such as explainable recommender systems. However, the development of new summarization techniques is hindered by the lack of ground-truth labels, the inherent subjectivity of user summaries, and human evaluation which is often costly and time-consuming. To address these challenges, we introduce USERSUMBENCH, a benchmark framework designed to facilitate iterative development of LLM-based summarization approaches. This framework offers two key components: (1) A reference-free summary quality metric. We show that this metric is effective and aligned with human preferences across three diverse datasets (MovieLens, Yelp and Amazon Review). (2) A novel robust summarization method that leverages time-hierarchical summarizer and self-critique verifier to produce high-quality summaries while eliminating hallucination. This method serves as a strong baseline for further innovation in summarization techniques.", "sections": [{"title": "1 Introduction", "content": "User activity timelines, including data such as place visit histories, product reviews, movie ratings, and other digital interactions, offer valuable insights into individual preferences, behaviors, and evolving interests. These timelines are crucial for applications like personalized recommendations and user behavior analysis (Wang et al. 2019). Summarizing these timelines into concise, actionable insights is essential for enhancing recommendation systems and understanding user engagement trends. For example, as shown in Figure 2, the next product prediction accuracy of an LLM-based model on the Amazon Review dataset (Ni, Li, and McAuley 2019) significantly improves when using summaries instead of raw activity timelines.\nHowever, generating high-quality user summaries is challenging due to the complexity and diversity of user timelines. The subjective nature of summary evaluation and the lack of standardized ground-truth datasets further complicate the process. Current methods often rely on simplistic heuristics or models that struggle with these issues (Giarelis, Mastrokostas, and Karacapilidis 2023). Moreover, the absence of standardized benchmarks or reliable metrics hampers the evaluation of summarization effectiveness (Fabbri et al. 2021; Lloret, Plaza, and Aker 2018).\nTo tackle these challenges, we introduce USERSUMBENCH, a comprehensive benchmark framework specifically designed to evaluate user summarization approaches by assessing the quality of user summaries generated from activity timelines. USERSUMBENCH consists of two key components: a robust, reference-free summary quality metric and a strong baseline summarization approach.\nIn USERSUMBENCH, the proposed quality metric evaluates the effectiveness of user summarization approaches by measuring how accurately the generated summaries predict future user activities. This metric offers a quantitative assessment of how well the summaries capture key aspects of user behavior (see Figure 1) and has demonstrated strong alignment with human ratings.\nThe proposed strong baseline summarization approach employs a time-hierarchical and self-critique method. This approach uses an LLM for initial summarization, followed by iterative refinement to reduce hallucinations and improve summary quality. This baseline not only validates the benchmark metrics but also serves as a foundation for future innovations in summarization techniques.\nKey Contributions:\n\u2022 Introduction of a quality metric for evaluating user summarization approaches through user summaries, demonstrating strong alignment with human ratings, thereby validating its effectiveness and simplifying the evaluation process.\n\u2022 Introduction of a strong baseline summarization approach, a time-hierarchical and self-critique method, setting the foundation for future advancements in summarization techniques."}, {"title": "2 Related Works", "content": "The lack of standardized benchmarks has long been a challenge in evaluating summarization approaches (Fabbri et al. 2021). While datasets like MovieLens (Harper and Konstan 2015) and Amazon Reviews (Ni, Li, and McAuley 2019) offer comprehensive logs of user activities, they lack corresponding ground-truth summaries, complicating the assessment of summarization techniques. Efforts have been made to create datasets that pair user activities with manually"}, {"title": "3 USERSUMBENCH Framework", "content": "In this section, we present the two key components of the USERSUMBENCH framework: Benchmark Metrics and Hierarchy-Critique Summary Generation. The Benchmark Metrics assess summarization approaches using various criteria, such as future user activity predictions, to ensure a comprehensive evaluation of the generated summaries. The Hierarchy-Critique Summary Generation method provides a strong baseline for evaluating and improving these techniques."}, {"title": "3.1 Benchmark Metrics", "content": "USERSUMBENCH includes three evaluation metrics to assess different aspects of user summarization approaches.\nQuality Metric Quality Metric is designed to evaluate the predictive accuracy of user summaries in forecasting future activities. The evaluation involves splitting each user activity timeline into past and future activities (see Figure 1). Summaries are generated from the past activities, and the quality of these summaries is measured by how well they predict the future activities. The quality of a user summary, $Q_s$, is computed by aggregating performance across multiple future activity prediction tasks.\n$Q_{s}=I\\left[\\frac{1}{\\left|T_{s}\\right|} \\sum_{t} q_{s, t} \\geq m\\right]$\nHere, $q_{s,t}$ denotes the binary prediction outcome for summary $s$ on task $t$ from the set of tasks $T_s$; $m$ is the threshold for the number of correct predictions required; $I[.]$ is the indicator function, where a result of 1 indicates a \"Good\" summary, and 0 indicates a \"Bad\" summary.\nTo evaluate a summarization approach on a generated summary set S, the Quality Metric (QM) is calculated as the percentage of summaries classified as \"Good\" based on the qualities of these summaries.\n$Q M=\\frac{\\sum_{s \\in S} Q_{s}}{\\left|S\\right|}$"}, {"title": "Instruction Following Metric", "content": "The Instruction Following Metric evaluates how well the user summaries adhere to specific constraints, such as a word limit, as introduced in other works (Skopek et al. 2023). For a summarization approach, the Instruction Following Metric (IFM) is defined as the proportion of summaries within the set S that meet the word limit constraint X.\n$I F M=\\frac{|{s \\in S | \\text { length }(s) \\leq X}|}{|S|}$"}, {"title": "Information Density Metric", "content": "The Information Density Metric assesses the conciseness and informativeness of user summaries, combining elements of both the Quality Metric and the Instruction Following Metric. This metric evaluates the balance between the length of a summary and its effectiveness in predicting future activities. For each summary s in the set S, and each associated task set $T_s$, the Information Density Metric (IDM) is calculated by dividing the average task prediction accuracy by the length of the summary.\n$I D M=\\frac{1}{|S|} \\sum_{s \\in S} \\frac{\\left(\\frac{1}{\\left|T_{s}\\right|} \\sum_{t \\in T_{s}} a c c(t)\\right)}{\\text { length }(s)}$\nThis metric provides a quantitative approach to evaluate the trade-off between informativeness and brevity in user summaries, ensuring that summaries are both concise and meaningful."}, {"title": "3.2 Hierarchy-Critique Summary Generation", "content": "USERSUMBENCH introduces a Time-Hierarchical and Self-Critique (Hierarchy-Critique) summarization approach (see Figure 3(2)), which is demonstrated to outperform a simpler single-step method (see Figure 3(1), refer to Appendix A.1 for the prompt details). For more details on this comparison, please refer to Section 4.2. The Hierarchy-Critique approach is designed to address the challenges of generating factually consistent summaries by mitigating hallucinations while maintaining computational efficiency through a time-hierarchical structure.\nThe Hierarchy-Critique approach works by first segmenting a user's activity history into manageable time intervals, ensuring each segment meets a minimum activity threshold to provide a comprehensive representation of the user's behavior. This segmentation allows LLMs to process the data efficiently within their context window limits. As depicted in Figure 4, the summarizer model generates an initial summary for each segment (refer to Appendix A.2). These segment summaries are then refined by a verifier model (Wang et al. 2023), whose role is to identify and correct potential hallucinations (e.g., query inconsistencies, factual inaccuracies), ensuring that each segment accurately reflects the user's activities. Finally, the refined segment summaries are synthesized into a cohesive summary that encapsulates the user's overall behavior and preferences (refer to Appendix A.7), with all time segments combined in chronological order.\nAs illustrated in Figure 4, the Hierarchy-Critique approach employs two LLMs (a summarizer and a verifier) to iteratively refine segment summaries. These LLMs can be the same model or different models, with the verifier focusing on identifying specific types of hallucinations:\n\u2022 Query Consistency: Ensuring that the summary is relevant to the initial query. The verifier checks for consistency between the query and the summary based on a provided prompt (refer to Appendix A.4).\n\u2022 Fact Consistency: Ensuring that the information in the generated summary is accurate and consistent with the user's activities. To identify factual inconsistencies, we propose using the Question Generation - Question Answering (QG-QA) method (Xu et al. 2024), which operates on key Knowledge Graph (KG) entities (Singhal 2012) extracted from the summary. This method involves two components: Question Generation and Question Answering.\nQuestion Generation (QG): Given a summary and an KG entity (e.g., Hiking /m/012v4j) extracted from the summary, the verifier generates a question-answer pair based on the summary context using a user-specified prompt (refer to Appendix A.5). The answer is the KG entity.\nQuestion Answering (QA): The consistency of these"}, {"title": "4 Evaluation", "content": "In this section, we validate the benchmark metrics and evaluate hierarchy-critique summarization approach within the USERSUMBENCH framework."}, {"title": "4.1 Validating Benchmark Metrics", "content": "To validate the USERSUMBENCH benchmark metrics, we study their alignment with human ratings on three public"}, {"title": "QG:", "content": "{$(\\mathrm{q}\\_{\\mathrm{k}}, \\mathrm{a}\\_{\\mathrm{k}})$} = $L L M\\_{Q G}$ (S, {$e\\_{\\mathrm{k}}$})"}, {"title": "QA:", "content": "{$h\\_{\\mathrm{k}}$} = $L L M\\_{Q A}$ ($\\Phi$, {$(\\mathrm{q}\\_{\\mathrm{k}}, \\mathrm{a}\\_{\\mathrm{k}})$})"}, {"title": "MAA", "content": "$\\mathrm{MAA}=\\frac{\\mathrm{TP}+\\mathrm{TN}}{\\mathrm{TP}+\\mathrm{FP}+\\mathrm{FN}+\\mathrm{TN}}$"}, {"title": "4.2 Evaluating Summarization Approaches", "content": "In this section, we evaluate our proposed Hierarchy-Critique summarization approach using the USERSUMBENCH benchmark metrics. For consistency, all LLMs in"}, {"title": "5 Conclusion and Future Work", "content": "In this paper, we introduced USERSUMBENCH, a comprehensive benchmark framework specifically designed to evaluate user summarization approaches through the assessment of user summaries generated from activity timelines. Our key contributions include (1) a reference-free quality metric for assessing user summarization approaches through summaries based on future user activity predictions, which has demonstrated strong effectiveness and close alignment with human preferences across three diverse datasets (MovieLens, Yelp, and Amazon Review), and (2) a robust summarization baseline method that combines a time-hierarchical summarizer with a self-critique verifier, yielding high-quality summaries while effectively minimizing hallucinations.\nThe strong alignment between our proposed quality metric and human ratings establishes USERSUMBENCH as a reliable and efficient tool for automated evaluation of user summarization approaches. By offering a cost-effective solution, USERSUMBENCH addresses the pressing need for standardized evaluation methods in the field of user summarization.\nLooking ahead, we plan to expand USERSUMBENCH by integrating real-time summarization techniques and exploring its applicability across additional domains beyond the current datasets. These enhancements will further broaden the utility and impact of the framework. Additionally, we aim to encourage the broader research community to adopt USERSUMBENCH, fostering its potential to standardize user summary evaluation practices and drive innovation in the development of more accurate and robust summarization techniques. Ultimately, we believe USERSUMBENCH will play a significant role in advancing personalization, recommendation systems, and user understanding in the digital landscape."}]}