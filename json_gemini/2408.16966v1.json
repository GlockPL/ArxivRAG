{"title": "USERSUMBENCH: A Benchmark Framework for Evaluating User Summarization\nApproaches", "authors": ["Chao Wang", "Neo Wu", "Lin Ning", "Luyang Liu", "Jun Xie", "Shawn O'Banion", "Bradley Green"], "abstract": "Large language models (LLMs) have shown remarkable ca-\npabilities in generating user summaries from a long list of raw\nuser activity data. These summaries capture essential user in-\nformation such as preferences and interests, and therefore are\ninvaluable for LLM-based personalization applications, such\nas explainable recommender systems. However, the develop-\nment of new summarization techniques is hindered by the\nlack of ground-truth labels, the inherent subjectivity of user\nsummaries, and human evaluation which is often costly and\ntime-consuming. To address these challenges, we introduce\nUSERSUMBENCH, a benchmark framework designed to fa-\ncilitate iterative development of LLM-based summarization\napproaches. This framework offers two key components: (1)\nA reference-free summary quality metric. We show that this\nmetric is effective and aligned with human preferences across\nthree diverse datasets (MovieLens, Yelp and Amazon Re-\nview). (2) A novel robust summarization method that lever-\nages time-hierarchical summarizer and self-critique verifier\nto produce high-quality summaries while eliminating hallu-\ncination. This method serves as a strong baseline for further\ninnovation in summarization techniques.", "sections": [{"title": "1 Introduction", "content": "User activity timelines, including data such as place visit\nhistories, product reviews, movie ratings, and other digital\ninteractions, offer valuable insights into individual prefer-\nences, behaviors, and evolving interests. These timelines are\ncrucial for applications like personalized recommendations\nand user behavior analysis (Wang et al. 2019). Summariz-\ning these timelines into concise, actionable insights is essen-\ntial for enhancing recommendation systems and understand-\ning user engagement trends. For example, as shown in Fig-\nure 2, the next product prediction accuracy of an LLM-based\nmodel on the Amazon Review dataset (Ni, Li, and McAuley\n2019) significantly improves when using summaries instead\nof raw activity timelines.\nHowever, generating high-quality user summaries is chal-\nlenging due to the complexity and diversity of user time-\nlines. The subjective nature of summary evaluation and the\nlack of standardized ground-truth datasets further compli-\ncate the process. Current methods often rely on simplistic\nheuristics or models that struggle with these issues (Giarelis,\nMastrokostas, and Karacapilidis 2023). Moreover, the ab-\nsence of standardized benchmarks or reliable metrics ham-\npers the evaluation of summarization effectiveness (Fabbri\net al. 2021; Lloret, Plaza, and Aker 2018).\nTo tackle these challenges, we introduce USERSUM-\nBENCH, a comprehensive benchmark framework specifi-\ncally designed to evaluate user summarization approaches\nby assessing the quality of user summaries generated from\nactivity timelines. USERSUMBENCH consists of two key\ncomponents: a robust, reference-free summary quality met-\nric and a strong baseline summarization approach.\nIn USERSUMBENCH, the proposed quality metric evalu-\nates the effectiveness of user summarization approaches by\nmeasuring how accurately the generated summaries predict\nfuture user activities. This metric offers a quantitative as-\nsessment of how well the summaries capture key aspects of\nuser behavior (see Figure 1) and has demonstrated strong\nalignment with human ratings.\nThe proposed strong baseline summarization approach\nempoys a time-hierarchical and self-critique method. This\napproach uses an LLM for initial summarization, followed\nby iterative refinement to reduce hallucinations and improve\nsummary quality. This baseline not only validates the bench-\nmark metrics but also serves as a foundation for future inno-\nvations in summarization techniques.\nKey Contributions:\n\u2022 Introduction of a quality metric for evaluating user sum-\nmarization approaches through user summaries, demon-\nstrating strong alignment with human ratings, thereby\nvalidating its effectiveness and simplifying the evaluation\nprocess.\n\u2022 Introduction of a strong baseline summarization ap-\nproach, a time-hierarchical and self-critique method, set-\nting the foundation for future advancements in summa-\nrization techniques."}, {"title": "2 Related Works", "content": "The lack of standardized benchmarks has long been a chal-\nlenge in evaluating summarization approaches (Fabbri et al.\n2021). While datasets like MovieLens (Harper and Konstan\n2015) and Amazon Reviews (Ni, Li, and McAuley 2019)\noffer comprehensive logs of user activities, they lack corre-\nsponding ground-truth summaries, complicating the assess-\nment of summarization techniques. Efforts have been made\nto create datasets that pair user activities with manually\ncrafted summaries. For example, Liu et al. (Liu et al. 2023)\nemphasized the importance of performance prediction in\nsummarization evaluation, advocating for benchmarks that\nmeasure the predictive power of generated summaries. Sim-\nilarly, Akkasi et al. (Akkasi, Fraser, and Komeili 2023) ex-\nplored reference-free evaluation methods, proposing metrics\ndesigned to assess summaries' ability to convey essential\ncontent relevant to future activities. These studies highlight\nthe limitations of current evaluation practices and the need\nfor more predictive and reliable benchmarks.\nRecent research has focused on establishing standardized\nevaluation methodologies for summarization (Fabbri et al.\n2021; Liu et al. 2023; Chen et al. 2024). Fabbri et al. (Fab-\nbri et al. 2021) addressed the shortcomings in existing sum-\nmarization evaluation methods by reassessing 14 automatic\nevaluation metrics using outputs from recent neural summa-\nrization models. They also released a toolkit of evaluation\nmetrics to promote consistency in reporting results. Chen\net al. (Chen et al. 2024) proposed a facet-aware evaluation\nparadigm for scientific abstracts, introducing benchmarks\nthat enable more nuanced comparisons of evaluation metrics\nin this context."}, {"title": "3 USERSUMBENCH Framework", "content": "In this section, we present the two key components of\nthe USERSUMBENCH framework: Benchmark Metrics and\nHierarchy-Critique Summary Generation. The Benchmark\nMetrics assess summarization approaches using various cri-\nteria, such as future user activity predictions, to ensure a\ncomprehensive evaluation of the generated summaries. The\nHierarchy-Critique Summary Generation method provides\na strong baseline for evaluating and improving these tech-\nniques."}, {"title": "3.1 Benchmark Metrics", "content": "USERSUMBENCH includes three evaluation metrics to as-\nsess different aspects of user summarization approaches.\nQuality Metric Quality Metric is designed to evaluate the\npredictive accuracy of user summaries in forecasting future\nactivities. The evaluation involves splitting each user activity\ntimeline into past and future activities (see Figure 1). Sum-\nmaries are generated from the past activities, and the quality\nof these summaries is measured by how well they predict\nthe future activities. The quality of a user summary, \\(Q_s\\), is\ncomputed by aggregating performance across multiple fu-\nture activity prediction tasks.\n\\(Q_s = I[\\frac{1}{|T_s|} \\sum_{t \\in T_s} q_{s,t} \\geq m]\\)\nHere, \\(q_{s,t}\\) denotes the binary prediction outcome for sum-\nmary s on task t from the set of tasks \\(T_s\\); m is the threshold\nfor the number of correct predictions required; I[.] is the\nindicator function, where a result of 1 indicates a \"Good\"\nsummary, and 0 indicates a \"Bad\" summary.\nTo evaluate a summarization approach on a generated\nsummary set S, the Quality Metric (QM) is calculated as\nthe percentage of summaries classified as \"Good\" based on\nthe qualities of these summaries.\n\\(QM = \\frac{\\sum_{s \\in S} Q_s}{|S|}\\)\nInstruction Following Metric The Instruction Following\nMetric evaluates how well the user summaries adhere to spe-\ncific constraints, such as a word limit, as introduced in other\nworks (Skopek et al. 2023). For a summarization approach,\nthe Instruction Following Metric (IFM) is defined as the pro-\nportion of summaries within the set S that meet the word\nlimit constraint X.\n\\(IFM = \\frac{|{s \\in S | length(s) \\leq X}|}{|S|}\\)\nWhile this metric was originally proposed in earlier stud-\nies, it remains a useful measure for assessing how effectively\na summarization approach adheres to the given prompt in-\nstructions.\nInformation Density Metric The Information Density\nMetric assesses the conciseness and informativeness of user\nsummaries, combining elements of both the Quality Metric\nand the Instruction Following Metric. This metric evaluates\nthe balance between the length of a summary and its effec-\ntiveness in predicting future activities. For each summary s\nin the set S, and each associated task set \\(T_s\\), the Information\nDensity Metric (IDM) is calculated by dividing the average\ntask prediction accuracy by the length of the summary.\n\\(IDM = \\frac{1}{|S|} \\sum_{s \\in S} \\frac{1}{length(s)} (\\sum_{t \\in T_s} acc(t))\\)\nThis metric provides a quantitative approach to evaluate\nthe trade-off between informativeness and brevity in user\nsummaries, ensuring that summaries are both concise and\nmeaningful."}, {"title": "3.2 Hierarchy-Critique Summary Generation", "content": "USERSUMBENCH introduces a Time-Hierarchical and Self-\nCritique (Hierarchy-Critique) summarization approach (see\nFigure 3(2)), which is demonstrated to outperform a simpler\nsingle-step method (see Figure 3(1), refer to Appendix A.1\nfor the prompt details). For more details on this compari-\nson, please refer to Section 4.2. The Hierarchy-Critique ap-\nproach is designed to address the challenges of generating\nfactually consistent summaries by mitigating hallucinations\nwhile maintaining computational efficiency through a time-hierarchical structure.\nThe Hierarchy-Critique approach works by first segment-\ning a user's activity history into manageable time intervals,\nensuring each segment meets a minimum activity threshold\nto provide a comprehensive representation of the user's be-\nhavior. This segmentation allows LLMs to process the data\nefficiently within their context window limits. As depicted\nin Figure 4, the summarizer model generates an initial sum-\nmary for each segment (refer to Appendix A.2). These seg-\nment summaries are then refined by a verifier model (Wang\net al. 2023), whose role is to identify and correct poten-\ntial hallucinations (e.g., query inconsistencies, factual in-\naccuracies), ensuring that each segment accurately reflects\nthe user's activities. Finally, the refined segment summaries\nare synthesized into a cohesive summary that encapsulates\nthe user's overall behavior and preferences (refer to Ap-\npendix A.7), with all time segments combined in chrono-\nlogical order.\nAs illustrated in Figure 4, the Hierarchy-Critique ap-\nproach employs two LLMs (a summarizer and a verifier) to\niteratively refine segment summaries. These LLMs can be\nthe same model or different models, with the verifier focus-\ning on identifying specific types of hallucinations:\n\u2022 Query Consistency: Ensuring that the summary is rele-\nvant to the initial query. The verifier checks for consis-\ntency between the query and the summary based on a\nprovided prompt (refer to Appendix A.4).\n\u2022 Fact Consistency: Ensuring that the information in the\ngenerated summary is accurate and consistent with the\nuser's activities. To identify factual inconsistencies, we\npropose using the Question Generation - Question An-\nswering (QG-QA) method (Xu et al. 2024), which op-\nerates on key Knowledge Graph (KG) entities (Singhal\n2012) extracted from the summary. This method involves\ntwo components: Question Generation and Question An-\nswering.\nQuestion Generation (QG): Given a summary and an\nKG entity (e.g., Hiking /m/012v4j) extracted from the\nsummary, the verifier generates a question-answer pair\nbased on the summary context using a user-specified\nprompt (refer to Appendix A.5). The answer is the KG\nentity.\nQuestion Answering (QA): The consistency of these"}, {"title": "4 Evaluation", "content": "In this section, we validate the benchmark metrics and eval-\nuate hierarchy-critique summarization approach within the\nUSERSUMBENCH framework.\n4.1 Validating Benchmark Metrics\nTo validate the USERSUMBENCH benchmark metrics, we\nstudy their alignment with human ratings on three public\nuser activity datasets: MovieLens 1M (Harper and Kon-\nstan 2015), Yelp (Yelp), and Amazon Review (Ni, Li, and\nMcAuley 2019).\nDatasets and Evaluation Tasks This section outlines the\ndataset preparation and the prediction tasks used for evalua-\ntion.\nUser timelines from the three datasets were filtered based\non activity count \\(L_0\\) to ensure a balance between data suffi-\nciency and computational efficiency:\n\u2022 \\(L_0 > N_{low}\\): Timelines with fewer than 50 activities were\nexcluded to ensure sufficient context for generating ro-\nbust summaries.\n\u2022 \\(L_0 \\leq N_{up}\\): Timelines with more than 200 activities were\ntruncated to the most recent 200 to focus on recent behav-\nior patterns and manage computational load.\nThese thresholds (\\(N_{low}\\) = 50 and \\(N_{up}\\) = 200) were cho-\nsen to balance informative summaries with processing ef-\nficiency. Table 1 shows the number of examples used for\nevaluation after filtering.\nAs shown in Table 2, the proposed Quality Metric demon-\nstrated strong alignment with human ratings, with above\n70% agreement across all datasets.\nTo further evaluate the performance of the proposed\nbenchmark metrics, we applied them to three popular mod-\nels: Gemini 1.5 Pro (Reid et al. 2024), GPT-40 (Achiam\net al. 2023), and Claude 3 Haiku (Anthropic 2023). Detailed\ncomparisons can be found in Appendix B.\nIn addition to USERSUMBENCH benchmark metrics, we\nalso considered other reference-free evaluation metrics that\ndo not require ground-truth summaries for user summary\nevaluation. These metrics compare the generated summary\nagainst user activities without relying on a reference sum-\nmary.\n\u2022 ROUGE-2 (Lin 2004): Measures the overlap of bigrams\nbetween the generated summary and user activities.\n\u2022 ROUGE-L (Lin 2004): Measures the Longest Common\nSubsequence (LCS) between the generated summary and\nuser activities.\n\\(MAA = \\frac{TP+TN}{TP+FP+FN+TN}\\)\nIn this equation, the four possible outcomes are as fol-\nlows:\n\u2022 TP (True Positive): Both the prediction and the human\nrating classify the summary as \"Good.\"\n\u2022 FP (False Positive): The prediction classifies the sum-\nmary as \"Good,\" but the human rating classifies it as\n\"Bad.\"\n\u2022 FN (False Negative): The prediction classifies the sum-\nmary as \"Bad,\" but the human rating classifies it as\n\"Good.\"\n\u2022 TN (True Negative): Both the prediction and the human\nrating classify the summary as \"Bad.\"\n\u2022 BLEU (Papineni et al. 2002): Evaluates the precision of\nn-grams in the generated summary compared to user ac-\ntivities.\n\u2022 BertScore (Zhang et al. 2019): Uses BERT embeddings\nto compute precision, recall, and F1 score based on the\nsimilarity of words in the generated summary and user\nactivities.\nBertScore-precision: Measures the precision of em-\nbedding overlap.\nBertScore-recall: Measures the recall of embedding\noverlap.\nBertScore-F1: Combines precision and recall to pro-\nvide an F1 score.\n\u2022 BLEURT (Sellam, Das, and Parikh 2020): Uses a pre-\ntrained model to evaluate the quality of the generated text\nbased on human judgments compared to user activities.\n\u2022 AutoEval (Chiang and Lee 2023): Adopts an LLM to au-\ntomatically evaluates summaries without requiring refer-\nence summaries, focusing on coherence and relevance.\nTo further validate our approach, we calculated the Pear-\nson Correlation Coefficient between human ratings and the\nreference-free metrics, including the proposed summary\nquality measurement \\(Q_s\\) (see Equation 1). In this com-\nparison, BLEURT utilized a pre-trained BLEURT-base-128\nmodel (Sellam, Das, and Parikh 2020) for evaluation, while\nAutoEval was based on the Gemini 1.5 Pro model. The \\(Q_s\\)\nalso employed the Gemini 1.5 Pro model for future activity\npredictions. As shown in Table 3, our metric demonstrated\na more reliable and higher correlation with human ratings\nthan other metrics, as they are specifically tailored to the\ncharacteristics of user timeline activities. Despite not hav-\ning a perfect correlation, the \\(Q_s\\) remains highly useful for\ntasks like weakly supervised learning (Zhou 2018), where\nground-truth summaries are limited."}, {"title": "4.2 Evaluating Summarization Approaches", "content": "In this section, we evaluate our proposed Hierarchy-\nCritique summarization approach using the USERSUM-\nBENCH benchmark metrics. For consistency, all LLMs in\nthis experiment used the same model: Gemini 1.5 Flash\n(Reid et al. 2024), across summarization, self-critique verifi-\ncation, and prediction tasks. To validate the robustness of the\nbenchmarks, we conducted three prediction runs on all three\ndatasets to calculate the Mean \u00b1 Sd values for the Quality\nMetric and the Information Density Metric. The results, pre-\nsented in Table 4, demonstrate that the Hierarchy-Critique\napproach consistently outperforms the Single-Step approach\nacross various evaluation metrics. Additionally, the stability\nof the benchmark metrics across multiple predictions con-\nfirms their robustness.\n\u2022 Quality Metric: The Hierarchy-Critique approach\nachieved superior scores on the MovieLens, Yelp, and\nAmazon Review datasets, with increases of 5.21%,\n11.55%, and 2.44% respectively. These gains indicate\nthat the iterative refinement process effectively reduces\nhallucinations and enhances the overall consistency and\nquality of the summaries.\n\u2022 Instruction Following Metric: Significant improve-\nments were observed with the Hierarchy-Critique ap-\nproach, particularly in the Yelp dataset, which showed\na 25.41% increase, followed by MovieLens and Amazon\nReview with gains of 18.18% and 18.65% respectively.\nThis suggests that the Hierarchy-Critique method is more\nadept at adhering to prompt constraints, likely due to its\neffective segmentation and summarization of user activi-\nties.\n\u2022 Information Density Metric: The Hierarchy-Critique\napproach demonstrated substantial gains in this metric,\nwith increases of 36.85%, 32.58%, and 49.28% for the\nMovieLens, Yelp, and Amazon Review datasets respec-\ntively. This shows that the approach not only produces\naccurate summaries but also ensures that these sum-\nmaries are concise and rich in information, effectively\nbalancing brevity with informativeness.\nThese findings underscore the superiority of the\nHierarchy-Critique approach over the Single-Step method,\nparticularly in generating higher-quality, instruction-\ncompliant, and information-dense summaries. The effec-\ntiveness of time segmentation, combined with iterative\nrefinement and verification processes, plays a pivotal role in\nthese improvements, establishing the Hierarchy-Critique ap-\nproach as a more robust and reliable option for summarizing\nuser activity timelines."}, {"title": "5 Conclusion and Future Work", "content": "In this paper, we introduced USERSUMBENCH, a com-\nprehensive benchmark framework specifically designed to\nevaluate user summarization approaches through the as-\nsessment of user summaries generated from activity time-\nlines. Our key contributions include (1) a reference-free\nquality metric for assessing user summarization approaches\nthrough summaries based on future user activity predic-\ntions, which has demonstrated strong effectiveness and\nclose alignment with human preferences across three di-\nverse datasets (MovieLens, Yelp, and Amazon Review), and\n(2) a robust summarization baseline method that combines\na time-hierarchical summarizer with a self-critique verifier,\nyielding high-quality summaries while effectively minimiz-\ning hallucinations.\nThe strong alignment between our proposed quality met-\nric and human ratings establishes USERSUMBENCH as a\nreliable and efficient tool for automated evaluation of user\nsummarization approaches. By offering a cost-effective so-\nlution, USERSUMBENCH addresses the pressing need for\nstandardized evaluation methods in the field of user sum-\nmarization.\nLooking ahead, we plan to expand USERSUMBENCH by\nintegrating real-time summarization techniques and explor-\ning its applicability across additional domains beyond the\ncurrent datasets. These enhancements will further broaden\nthe utility and impact of the framework. Additionally, we\naim to encourage the broader research community to adopt\nUSERSUMBENCH, fostering its potential to standardize user\nsummary evaluation practices and drive innovation in the de-\nvelopment of more accurate and robust summarization tech-\nniques. Ultimately, we believe USERSUMBENCH will play\na significant role in advancing personalization, recommen-\ndation systems, and user understanding in the digital land-\nscape."}, {"title": "A Prompt Examples", "content": "A.1 LLM Prompt Template for Single-Step Summarization\nInstructions\nSummarize my \u201cUser Activities\u201d and provide insights that address the query: \u201c{query}", "User Activities\" section, with each separated by a newline.\n4. Limit the summary to no more than {max_words} words.\nUser Activities\n{user_activities}\nHere we define:\n{query}: Query intention of the summarization. For example, \"Summarize my long-term movie watching preference\u201d.\n{max_words}: Max number of words for the summary.\n{user_activities}: List of user activities.\nA.2 LLM Prompt Template for Segment Summarization\nInstructions\nSummarize my \"User Activities\" related to the specified time range \"{time_range}\" and provide insights that address the query:\n\"{query}\". Adhere to the instructions below.\n1. The summary should have the format with **Summary** and **Insights**.\n2. The summary should take into account the changes of my long-term interest over time.\n3. My activities related to this time range are cataloged in the following \"User Activities\" section, with each separated by a newline.\n4. Limit the summary to no more than {max_words} words.\nUser Activities\n{user_activities}\nHere we define:\n{time_range}: Time range of the segment user activities.\n{query}: Query intention of the summarization. For example, \"Summarize my long-term movie watching preference\u201d.\n{max_words}: Max number of words for the summary.\n{user_activities}: List of user activities of the segment.\nA.3 LLM Prompt Template for Segment Summarization with Feedback\nInstructions\nSummarize my \"User Activities\" related to the specified time range \u201c{time_range}\u201d, revise the below \u201cPrevious Summary": "o be\nconsistent with every \u201cQuestion\u201d and its corresponding \u201cReferenceAnswer\u201d in the below \u201cPrevious Question Answers\u201d. Adhere to the\ninstructions below.\n1. For each \"Question\u201d in \u201cPrevious Question Answers\u201d, the \u201cAnswer\u201d is derived from the", "Summary": "while the", "Refer-\nenceAnswer": "s based on my \"User Activities\u201d.\n2. Modify the \"Previous Summary\" to incorporate the", "ReferenceAnswer": "ather than the", "Answer": "or each", "Question": "n the", "query": "{query}\".\n4. The new summary should have the format with **Summary** and **Insights**.\n5. The new summary should take into account the changes of my long-term interest over time.\n6. My activities related to this time range are cataloged in the following \"User Activities\" section, with each separated by a newline.\n7. Limit the summary to no more than {max_words} words.\nPrevious Summary\n{previous_summary}\nPrevious Question Answers\n{previous_question_answer_pairs}\nUser Activities\n{user_activities}\nHere we define:\n{time_range}: Time range of the segment user activities.\n{query}: Query intention of the summarization. For example, \\\"Summarize my long-term movie watching preference", "n{max_words}": "Max number of words for the summary.\n{previous_summary}: Previous generated summary.\n{previous_question_answer_pairs}: Previous list of generated QA pairs.\n{user_activities}: List of user activities of the segment.\nA.4 LLMs Prompt Template for Query Consistency\nInstructions\nEvaluate the relevance of a summary under the following \"Summary\" section to a query under the following \"Query\" section.\nReturn \"consistent\" if the summary aligns with the query, and \u201cinconsistent", "consistent\u201d or \u201cinconsistent": "ithout any explanation.\nSummary\n{summary}\nQuery\n{query}\nHere we define:\n{summary}: Provided summary.\n{query}: Query intention of the summarization.\nA.5 LLMs Prompt Template for Question Generation\nInstructions\nGiven the below \"KG Entities\" and"}, {"Summary": "adhere to the instructions below to create", "Question-Answer Pairs": "n1. Each pair must be related to a specific KG entity.\n2. The answer must be the KG entity itself.\n3. Formulate questions that are directly relevant to the KG entity within the context of the summary.\n4. Avoid creating questions that are open-ended.\n5. Use the following format for your response, as shown under the", "Pairs": "f the", "Example": "ection below:\n[Question#1:", "Question": "Answer#1:", "Answer": "."}, {"Example": "ection, create", "Pairs": "ased on the\ntask under the", "Task": "ection.\nExample\n\"KG Entities\":\nhiking\npop music\n\"Summary\":\n**Summary:**\nThe user demonstrates a robust long-term interest in outdoor and musical activities. Specifically, they are drawn to hiking and pop\nmusic.\n**Insights:**\n* Sports Recreation and Fitness: The user has a sustained interest in hiking, engaging regularly in this activity, which indicates a\npreference for exploring nature and challenging terrains.\n* Entertainment Media and Arts: The user enjoys pop music, known for its wide appeal and catchy melodies, reflecting a consistent\ninterest in this genre.\n\"Question-Answer Pairs\":\n[Question#1: \"What outdoor activity is the user mainly interested in according to their searches and discussions?\", Answer#1:\n\"hiking\"]\n[Question#2: \"What genre of music does the user prefer, known for its wide appeal and catchy melodies?\", Answer#2: \"pop music\"]\n-Task-\n\"KG Entities\":\n{kg_entities}\n\"Summary\":\n{summary}\nHere we define:\n{kg_entities}: List of KG entities extracted from the summary.\n{summary}: Provided summary.\nA.6 LLMs Prompt Template for Question Answering\n-Instructions-\nGiven the below \"Question-Answer Pairs\" and my \"User Activities", "Judgement": "s composed of \u201cStatus\u201d and \u201cReferenceAnswer\u201d like the following format: [Status#1: \u201cStatus\u201d, ReferenceAn-\nswer#1:", "Status": "hould be labeled as \u201cconsistent\u201d or 'inconsistent", "consistent": "tatus means the question-answer pair aligns with or does not contradict the information provided in my activities.\nThe \u201cReferenceAnswer\u201d should be \u201cnone\u201d.\n4. An \u201cinconsistent\u201d status means the question-answer pair conflicts directly with or is contradicted by the information provided in\nmy activities. The \u201cReferenceAnswer\u201d should be a new answer of the question based on my activities.\n5. Use the following format for your response, as shown under the \u201cJudgements\u201d of the following"}, {"Example": "ection below like\n[Status#2: \u201cStatus\u201d, ReferenceAnswer#2: \u201cReferenceAnswer\u201d].\n6. Match each judgement to its corresponding question-answer pair by their sequence, such as [Status#2: \u201cStatus\u201d, ReferenceAn-\nswer#2:", "ReferenceAnswer": "pertains to [Question#2: \u201cQuestion\u201d, Answer#2: \u201cAnswer\u201d].\nFollowing the above steps and an example under the following \u201cExample\u201d section, create judgements based on the task under the", "Task": "ection.\nExample\n\"Question-Answer Pairs\":\n[Question#1: \"What outdoor activity is the user mainly interested in according to their searches and discussions?\", Answer#1:\n\"hiking\"]\n[Question#2: \"What genre of music does the user prefer, known for its wide appeal and catchy melodies?\", Answer#2: \u201crock music\"]\n\"User Activities\":\nsearched", "2020s": "round Sat 05/15/2004 4PM\nsearched", "home": "round Fri 06/11/2004 6PM\nsearched", "trip": "round Wed 07/21/2004 7PM\nsearched", "music": "round Sun 04/28/2004 5PM\nsearched", "U.S.": "round Mon 04/29/2004 1PM\n\"Judgements\":\n[Status#1: \u201cconsistent\u201d, ReferenceAnswer#1: \u201cnone\u201d]\n[Status#2: \u201cinconsistent\u201d, ReferenceAnswer#2: \u201cpop music\u201d]\nTask\n\"Question-Answer Pairs\":\n{question_answer_pairs}\n\"User Activities\":\n{user_activities}\nHere we define:\n{question_answer_pairs}: List of generated QA pairs related to the KG entities.\n{user_activities}: User activities which are used to generate the summary.\nA.7 LLMs Prompt Template for Combining Time Segments\n-Instructions-\nCombine all the time segment summaries under the", "Summaries": "ection. Adhere to the instructions below.\n1. The combined summary should offer insights relevant to the query: \u201c{query}\u201d.\n2. Format the combined summary with sections labeled **Summary** and **Insights**.\n3. Focus the combined summary on my recent interest preferences (recent time segments).\n4. Limit the combined summary to no more than {max_words} words.\nTime Segment Summaries\nSummary of Time Segment \"{time_range}\": {segment_summary}\nHere we define:\n{query}: Query intention of the summarization.\n{max_words}: Max number of words for the summary.\n{time_range}: Time range of a segment.\n{segment_summary}: Summary of a time segment."}]}