{"title": "INFERRING PREFERENCES FROM DEMONSTRATIONS IN\nMULTI-OBJECTIVE REINFORCEMENT LEARNING", "authors": ["Junlin Lu", "Patrick Mannion", "Karl Mason"], "abstract": "Many decision-making problems feature multiple objectives where it is not always possible to know\nthe preferences of a human or agent decision-maker for different objectives. However, demonstrated\nbehaviors from the decision-maker are often available. This research proposes a dynamic weight-\nbased preference inference (DWPI) algorithm that can infer the preferences of agents acting in\nmulti-objective decision-making problems from demonstrations. The proposed algorithm is evaluated\non three multi-objective Markov decision processes: Deep Sea Treasure, Traffic, and Item Gathering,\nand is compared to two existing preference inference algorithms. Empirical results demonstrate\nsignificant improvements compared to the baseline algorithms, in terms of both time efficiency and\ninference accuracy. The DWPI algorithm maintains its performance when inferring preferences for\nsub-optimal demonstrations. Moreover, the DWPI algorithm does not necessitate any interactions\nwith the user during inference - only demonstrations are required. We provide a correctness proof\nand complexity analysis of the algorithm and statistically evaluate the performance under different\nrepresentation of demonstrations.", "sections": [{"title": "1 Introduction", "content": "Many decision-making tasks are designed with the primary goal of optimizing performance in achieving a singular\nobjective, such as enhancing game scores Mnih et al. [2015], reducing energy consumption Lu et al. [2023a], and\nensuring appropriate robotic functionality Kober et al. [2013], among others. Nonetheless, in practical scenarios, more\ndecision-making tasks feature multiple objectives. This complexity introduces the necessity for a balanced trade-off\namong these objectives. Given the variability in user preferences regarding objectives, the introduction of a preference\nfactor, typically represented by a linear weight vector Hayes et al. [2022], is essential for the comparative evaluation of\ndistinct solutions.\nMulti-objective reinforcement learning (MORL) is the paradigm to solve such multi-objective decision-making (MODM)\nproblems, by considering the preference factor. If the preference factor is known beforehand, a standard policy\nimprovement approach can be usedMannor and Shimkin [2001], Tesauro et al. [2007], Van Moffaert et al. [2013]."}, {"title": "2 Background & Related Work", "content": ""}, {"title": "2.1 Multi-objective Reinforcement Learning", "content": "Multi-objective reinforcement learning (MORL) is a branch of reinforcement learning that focuses on decision-making\nproblems with multiple objectives Hayes et al. [2022], Van Moffaert et al. [2013], Van Moffaert and Now\u00e9 [2014],\nMossalam et al. [2016]. In MORL, an agent interacts with the environment and receives a reward vector composed of\nmultiple objective values, rather than a single scalar reward like in single-objective reinforcement learning (SORL).\nHowever, this increases the computation complexity of training. To make training more manageable, reward scalarization\nis often used in MORL. This involves mapping the reward vector to a scalar value using a scalarization function, also\nknown as a utility function. The utility function $u : \\mathbb{R}^d \\rightarrow \\mathbb{R}$ is given below,\n$V^{\\pi}= u(V^{\\pi})$"}, {"title": "2.2 Preference Inference", "content": "Previous studies have mainly focused on uncovering the user's unknown preferences within MODM?. These method-\nologies are broadly categorized into approaches that leverage queries to elicit preferences Benabbou and Perny [2015],\nZintgraf et al. [2018], Benabbou et al. [2020], Shao et al. [2023], inverse reinforcement learning paradigm (IRL) Ikenaga\nand Arai [2018], Takayama and Arai [2022], and using a learned policy set to do the inference Barrett and Narayanan\n[2008], Yang et al. [2019]. We start with the query-based methods."}, {"title": "2.3 Problem Statement", "content": "As highlighted, the deployment of MORL agents in real-world decision-making necessitates an understanding of the\nuser's genuine preferences regarding the weighting attributed to various objectives. Regrettably, the articulation of\nnumerical preferences, which typically manifest as a linear weight vector Castelletti et al. [2013], Khamis and Gomaa\n[2014], Ferreira et al. [2017], Lu et al. [2022], is not inherently intuitive for the user. In adverse scenarios, even when a\nuser is capable of specifying a distinct preference, this preference must be sufficiently precise, as minor discrepancies\ncan lead to the adoption of an inappropriate policy. Such obstacles hinder the application of MORL methods in practice.\nThis motivates the study in PI methods for MORL scenario.\nThough there are PI methods in the literature, as we mentioned in Section 2.2, these approaches often require querying\nthe user, face challenges with computational complexity in scenarios involving high-dimensional preference spaces, or\nstruggle to accommodate sub-optimal demonstrations."}, {"title": "3 DWPI Algorithm", "content": "In this section, we commence by presenting the DWMORL agent, which can adjust to changing priorities across\nmultiple objectives. It plays a pivotal role in producing the training dataset for the DWPI model. Subsequently, we\nintroduce the concept of truncated Q soft Q action sampling, which is employed to create sub-optimal demonstrations\nto enhance the training dataset further. Following this, we elucidate the DWPI algorithm. The section continues with a\ntheoretical analysis that encompasses both a proof of correctness and an analysis of complexity, and the bi-directional\nmapping between demonstrations and preferences. In Table 2 We put a summary of the symbols used in this section."}, {"title": "3.1 Dynamic Weights MORL Agent Training", "content": "We employed the methodology proposed by Kallstr\u00f6m et al. to train the DWMORL agent K\u00e4llstr\u00f6m and Heintz [2019].\nSpecifically, we adpated this approach to implement both tabular and DRL Q-agents, which are respectively referred\nto as the DWMOTQ (dynamic weights tabular Q-learning) and DWMODQN (dynamic weights deep Q-network)\nalgorithms. We partitioned the preference weight space W into a discrete subset $W^\\eta$ using a specific granularity $\u03b7$ (e.g.,\n$\u03b7 = 0.01$ for DST), such that $W^\\eta \\subset W$."}, {"title": "3.1.1 Dynamic Weights Multi-objective Tabular Q-learning", "content": "The DWMOTQ algorithm is outlined in Algorithm 1. The DWMOTQ agent keeps a collection of Q tables, each indexed\nby a preference weight vector, to maintain a record of action-state values.\nAt the beginning of each episode, a preference weight w is randomly sampled from $W^\u03b7$. The DWMOTQ agent then\ninteracts with the environment and updates the Q table associated with w until it converges."}, {"title": "3.1.2 Dynamic Weights Multi-objective Deep Q Network", "content": "We adopted the DWMODQN algorithm as an alternative method for approximating the Q function conditioned on the\npreference weight vector. The DWMODQN algorithm is built upon the deep Q-network (DQN) Mnih et al. [2015] by\nintegrating dynamic weights across multiple objectives K\u00e4llstr\u00f6m and Heintz [2019].\nAt the beginning of each episode, the DWMODQN agent samples a preference weight vector w as part of its state input\nand is trained on a high-granularity partitioned discrete subset $W^\u03b7$ of the preference space. The DWMODQN can learn\na single policy that can handle dynamic preference weights without retraining. The DWMODQN algorithm is outlined\nin Algorithm 2."}, {"title": "3.2 Truncated Soft Q Action Sampling", "content": "We retained the training process using a deterministic Q estimator of the DWMORL agent and proposed a stochastic\nsub-optimal action sampling method for demonstration generation, i.e. truncated soft Q action sampling from the trained\nQ estimators. This is similar to the idea of using an energy-based policy method Haarnoja et al. [2017, 2018]. However,\nunlike energy-based policy methods that utilize entropy to improve exploration efficiency and policy robustness,\ntruncated soft Q action sampling aims to generate sub-optimal demonstrations intentionally.\nThe sub-optimal demonstration assumption has been pointed out in literature Chen et al. [2021], i.e. \u201cThe assumption\nwhere the user can always provide at least stochastically optimal demonstrations, does not hold in most real-life\nscenarios.\" The method proposed in the referenced work seeks to enhance the efficiency of robotic policy learning by\namalgamating sub-optimal demonstrations. Their approach diverges significantly from ours, as our primary objective is"}, {"title": "3.3 DWPI Algorithm", "content": "Our methodology distinguishes from previous literature by eliminating the necessity for any queries or comparisons\namong demonstrations, as well as the need to retrain a MORL agent from scratch on each occasion. The DWPI model\ncan also infer demonstrator's preference from sub-optimal demonstration.\nA trained DWMORL agent can generate the optimal demonstration based on its preference weights, a process referred\nto as \"from preference factors to demonstrations\". In contrast, the reverse process, \u201cfrom demonstrations to preference\nfactors\", can be approached as a regression problem. The demonstration features can serve as the input and the\npreference weights act as the output labels. This regression model can be implemented with a supervised learning\nparadigm, as detailed in Algorithm 4. Initially, we iterate the preference weight space and pass weights into the\nDWMORL agent to generate demonstrations (the demonstration generation phase). Subsequently, we proceed to train"}, {"title": "3.4 Theoretical Analysis", "content": "In this section, a theoretical analysis is conducted. We first present a proof of correctness, demonstrating that the DWPI\nalgorithm can be effectively implemented using an FNN. Subsequently, we give the DWPI algorithm's computational\ncomplexity. Following this we showcase how feasibility of inferring preference weights from optimal demonstrations.\nSubsequently, we relax the assumption of optimal demonstrations to include sub-optimal ones."}, {"title": "3.4.1 Correctness Proof for DWPI", "content": "We present the correctness proof of using an FNN to do PI tasks from demonstrations. Consider a FNN composed of\nan input layer, several hidden layers, and an output layer. The network aims to approximate the mapping function:\n$f: \\mathbb{R}^n \\rightarrow W$, all elements on W are non-negative and sum to 1 Hayes et al. [2022] and $\\mathbb{R}^n$ is the continuous reward\nspace.\nUniversal Approximation Theorem Cybenko [1989], Hornik [1991] states that, given any continuous function on a\ncompact set and any error greater than zero, there exists a sufficiently large FNN that can approximate this function\nwithin an error less than some threshold over the compact set.\nIn the context of the DWPI algorithm, the continuous function in question maps continuous reward vectors to the\npreference simplex space. The Universal Approximation Theorem ensures that there exists a neural network structure\ncapable of approximating such a mapping function. This establishes the validity of using an FNN to map demonstrations\nto preferences, providing a theoretical foundation for DWPI."}, {"title": "3.4.2 Complexity Analysis", "content": "In this part, we present the computational complexity analysis for Algorithm 4. The algorithm is separated into two\nprimary phases: data collection and training. During the data collection phase, the computational complexity is directly\nproportional to the granularity \u03b7, which denotes the level of discretization applied to the preference simplex. For\neach discretized element of the preference simplex, the agent is required to execute its policy for an episode. The\nhorizon of the episode and the number of episoded the MORL agent is trained, denoted by H and M emerge as critical\nfactors influencing the overall complexity. Consequently, the computational complexity of the data collection phase is\nexpressed in terms of $O(\\eta HM)$, indicating that the time complexity scales with both the granularity of the preference\nsimplex discretization and the length of the episode.\nIn the training phase, the computational complexity is influenced by the architecture of the FNN utilized. We assume\nthis FNN is full-connected and has L layers, and the lth layer has $n_l$ neurons. On the lth layer, the computational\ncomplexity is $O(n_l n_{l-1})$, therefore the whole network feed-forward computational complexity is $0(\\Sigma_{l=1}^L n_l n_{l-1})$,\nthe backward propagation has similar complexity to the feed-forward step. Given that there are $\\eta \\alpha$ demonstrations\nin the training set, where $\u03b1$ is the augmented factor of the training dataset, and the training involves E epochs, the\noverall complexity for the training process can be expressed as $O(\\eta \u03b1E \\Sigma_{l=1}^L n_l n_{l-1})$. The aggregate computational\ncomplexity, accounting for both the generation of data and training is represented by $O(\\eta(HM + \u03b1E \\Sigma_{l=1}^L n_l n_{l-1}))$."}, {"title": "3.4.3 Mapping Relation from Demonstration to Preference", "content": "The term $\u03c0_w$ is the DWMORL agent's policy aligned to the preference weight w. Assuming w is determined by a user\nindependently of the environment Env, the demonstration sampled from policy $\u03c0_w$ is:\n$T_w \\sim \u03c0_w$\n$T_w$ is the demonstration conditioned on the preference weight vector w. Equation 2 suggests a mapping relation\nbetween the demonstration and preference weight vector in environment Env.\nAn RL agent maps the policy to the demonstration, i.e. $\u03c0 \u2192 \u03c4$ Sutton and Barto [2018], Mnih et al. [2015], K\u00e4llstr\u00f6m\nand Heintz [2019]. This mapping relation is extended in MORL settings as the preference weight is mapped to the\ndemonstration, i.e. $\u03c0_w \u2192 T_w$. The mapping relation ($\u03c0 \u2192 \u03c4$) is reversible and introduced as an imitation learning\n(IL) Ho and Ermon [2016], Osa et al. [2018] where the policy is imitated from the demonstration, i.e. $\u03c4 \u2192 \u03c0$. In\nthe MORL setting, this reversible mapping relation is expanded to the relationship between demonstrations and the\npreference weight, i.e. w. Our objective is to establish this mapping relation, denoted as $\u03c4_w \u2192 w$. The mapping\n$\u03c0_w \u2192 \u03c4_w$ involves only executing the policy under w for several episodes. When using a demonstration $\u03c4_w$ to infer\nw, the resultant singular, precise weight vector may be different from the label, however, it is still correct if it drops\nin the correct range of the preference space. Though the policy between the ground truth preference and w may\nbe slightly different, they can still be deemed as equivalent under the specific circumstance. We term this as policy\nequivalence. We first give the formal definition of policy equivalence in the MORL setting then we relax the optimal\ndemonstration assumption and give the formal definition of policy \u03b4-equivalence. We begin with the concept of corner\nweights proposed in Roijers [2016].\nThe definition of policy equivalence has two primary purposes: 1) It supports the statement that an inference is\nconsidered correct if it yields a policy that is equivalent to the expected outcome, even if the inferred preference does\nnot precisely match the ground truth. This approach recognizes the validity of inferences that lead to functionally\ncomparable outcomes. 2) In scenarios where inference is based on a sub-optimal demonstration, a direct comparison\nbetween the inferred demonstration and the original sub-optimal one could lead to inaccuracies, resulting in a false\nnegative. In such instances, if the demonstrations are found to be policy \u03b4-equivalent, meaning they achieve a level of\nperformance or outcome within a specified \u03b4 margin, the inference process is deemed to have been successful. This\ncriterion allows for a more nuanced assessment of inference accuracy, accommodating the inherent variability and\nimperfection in demonstration-based inference."}, {"title": "Definition 1 Corner Weight ( Roijers [2016], Alegre et al. [2023])", "content": "Let $V = {v_i}$ be a set of multi-objective value functions for n policies. Corner weights $w_c$, are weight vectors\nlocated at the vertices of a polyhedron P\n$P = {x \u2208 \\mathbb{R}^{d+1}|V^+x \u2264 0, \\sum w_i = 1, w_i \u2265 0, \u2200i}$"}, {"title": "Definition 2 Policy Equivalence", "content": "{Ti} and {$T_i$}^\u03c02 are demonstration sets from two policies \u03c01 and \u03c02. The metric d is used to measure the similarity\u00b2\nof demonstrations. If the two demonstration sets result in the same performance measured by metric d:\n$d({Ti}, {$T_i$}^\u03c02) = 0$\n\u03c01 and \u03c02 are equivalent under metric d, noted as:\n$\u03c0_1 \u03b1 \u03c0_2$"}, {"title": "Definition 3 CCS Regional Policy Set", "content": "For a corner weight wc, all equivalent policies that maximize the scalarized value under w constructs the CCS regional\npolicy set $\u03a0_{w_c} \u2286 \u03a0_{ccs}$.\nAccording to Definition 1, we determines a region within which various weight vectors are considered equivalent in\nterms of their ability to generate a policy. Based on Definition 3, for any weight vector $w_i$ in a subset bordered by $w_c$,\nthe policy $\u03c0_{w_i}$ are equivalent to the policy $\u03c0_{w_c}$. (Definition 2, taking cumulative reward feedback as the equivalence\nmetric d). Therefore the inference $w_i$ is deemed correct as long as the ground truth weight also resides within this\nrange."}, {"title": "3.4.4 Optimal Demonstration Relaxation", "content": "Note that the DWMORL policy $\u03c0$ in Algorithm 4 is not necessarily optimal. The inference can still be accurate even if\nsub-optimal demonstrations are sampled by $\u03c0$. We now relax the optimal demonstration assumption. Using Definition\n2, we define the concept of policy \u03b4-equivalence."}, {"title": "Definition 4 Policy \u03b4-Equivalence", "content": "Let {Ti} and {$T_i$}^\u03c02 are demonstration sets from two policies \u03c01 and \u03c02, and a metric d is used to measure the\nsimilarity of demonstrations. If the two demonstrations have a difference less than or equal to a certain threshold \u03b4\nunder metric d:\n$d({T}, {T}^\u03c02) \u2264 \u03b4$\n\u03c01 and \u03c02 are considered equivalent under metric d.\n$\u03c0_1 \u03b1_\u03b4 \u03c0_2$"}, {"title": "4 Experiment Setting", "content": "In this section, we provide our experimental settings, the baseline algorithms we used, the metrics of evaluation, and the\nsensitivity analysis setting. The preference weight vectors in all three simulations are normalized during training, the\noriginal weights is presented only to keep consistency with the literature."}, {"title": "4.1 Experiment Environment", "content": ""}, {"title": "4.1.1 Convex Deep Sea Treasure", "content": "CDST Mannion et al. [2017] is a variant of the Deep Sea Treasure (DST) environment Vamplew et al. [2011], where\nthe globally convex Pareto front is known. The state is the current agent position, while the action space consists of four\nmovements: up, down, left, or right. Each episode begins with the agent positioned in the left top corner and ends when\nthe agent reaches any of the yellow treasure grids. There are 10 different treasures, each corresponding to a distinct\npolicy. The agent receives a reward vector comprising two elements: the first element is the time penalty of -1 per time"}, {"title": "4.1.2 Traffic", "content": "The Traffic environment K\u00e4llstr\u00f6m and Heintz [2019] has a collectible item on each of the upper corners. The yellow\narea is the road part where the red cars move. The car moves vertically in a random direction and it reverses the direction\nwhen hit by the wall or the edge of the frame. The agent, starting from the left bottom corner, is not supposed to step on\nthe road. If the agent steps on the road, it risks being hit by a car.\nIt must make a trade-off between obeying the traffic rules, maintaining traffic safety, and the time it takes to collect\nitems. Each item is worth reward 1, stepping on the road causes reward -1, being hit by a car causes reward -1, hitting\non the wall or the edge of the frame causes reward -1, and the time penalty is -1. In the work of Kallstrom et al.\nK\u00e4llstr\u00f6m and Heintz [2019], four different simulation scenarios are given, leading to four typical behavior patterns.\nWe retain the original preference setting from the literature to keep consistency. The elements of the preference vector\nare ordered as [steps, item collection, break traffic rules, collisions, wall hitting]. The DWMORL agent is implemented\nwith Algorithm 2. The scenarios are shown below.\nAlways Safe: [1, 50, 10, 50, 1]\nThe agent tries its best to avoid illegal behavior and collisions. It takes the longest but safest path to collect the two\nitems.\nAlways Fast: [10, 50, 10, 10, 1]\nThe agent tries to collect the two items as fast as possible. It thinks a detour is costly and therefore does not care about\nthe risk of collision and breaking traffic rules by taking a short path.\nFast and Safe: [5, 50, 0, 50, 1]\nThe agent cares about time consumption but also the safety of the path. It is encouraged to break traffic rules and walk\non the yellow path to spend less time.\nSlow and Safe: [1, 50, 0, 50, 1]\nThe agent will walk on the road if there is a low risk of being hit by cars. It would rather take a longer path if the traffic\ncondition is not ideal."}, {"title": "4.1.3 Item Gathering", "content": "The Item Gathering environment K\u00e4llstr\u00f6m and Heintz [2019] features three categories of collectibles in different\ncolors. There is a hard-coded agent with fixed preferences that particularly favors the red item. At the start of each\nepisode, the positions of the items are randomly initialized. The hard-coded agent starts at the top right corner and picks\nrandom routes to the red items.\nThe DWMORL agent exhibits an altruistic preference towards the hard-coded agent's collection of red items, which\ndetermines its cooperative or competitive behavior. The DWMORL agent obtains a reward of 1 that is linked to\nthe objective of collecting an item of a particular color. An additional reward of 1 for its altruistic objective if the\nhard-coded agent collects a red item. The altruistic reward will be reversed based on the DWMORL agent's cooperation\nor competition behavior that depended on the symbol of the altruistic preference. The DWMORL agent receives a time\npenalty of -1 at each time step and a wall penalty of -1 upon collision."}, {"title": "4.2 Baseline", "content": "We re-implement the PM method Ikenaga and Arai [2018], and the MWAL method Takayama and Arai [2022], as the\ntwo baseline algorithms to compare our DWPI algorithm against. The PM method randomly starts from a preference,\ntrains a RL agent from scratch and compares the resulting cumulative reward with the cumulative reward from the given\ndemonstration. Similar to the PM method, MWAL method infers preferences by comparing the agent's and the given\ncumulative reward (from demonstration). Instead of randomly searching for preferences as PM, they use Equation 8 to\nupdate the preference element, where $w_n$ is the nth element of the preference, k is the number of feature elements, N is\nthe total number of iterations, and $\u00b5_\u03b7$ and $\u03b5_n$ is the nth element of the feature expectation from current RL agent and\nthe expert.\n$w_n \u2190 w_n . (1+\\frac{\\sqrt{2logk}-(\u03bc_\u03b7-\u03bc\u03b5\u03b7)}{N})$\nTo make fair comparison, we used the same RL algorithm for PM and MWAL as the DWPI method."}, {"title": "4.3 Metrics", "content": "The research of PI is still at its early stage, yet a widely accepted metric is established. We therefore propose the\nfollowing metrics to determine the accuracy of PI models."}, {"title": "4.3.1 Time Efficiency", "content": "Efficient PI is crucial for practical applications that require rapid decision-making, especially in high-dimensional\npreference spaces. We evaluate the time consumption of the DWPI method and baselines by calculating the average\ntime required for training phase and inference phase."}, {"title": "4.3.2 Inference Accuracy", "content": "Known Pareto Front In CDST, the Pareto front is already known. In this scenario, the inference accuracy can be\nassessed by traversing through $W^\u03b7$ and verifying whether the inferred preference weight falls within the correct range.\nThe inference accuracy can then be calculated using the following formula, where $N_{correct}$ is the number of correct\ninferences:\n$ACC = \\frac{N_{correct}}{|W^\u03b7|}$"}, {"title": "Unknown Pareto Front", "content": "The direct comparison is useful for evaluating inference accuracy when Pareto front is\nknown. However, in real-life scenarios, a known Pareto front is a rare occurrence and a more frequent case is to do PI\ntask with a unknown Pareto front.\nMoreover, in MORL, the policy space often exhibits imbalanced clusters. A slight change in preference may lead\nto significant policy variations. This hinders the use of value-based comparison methods, e.g. MSE, for accuracy\nevaluation. In some cases, the ground truth and the inference may be very close to each other, but drop on two different\nsides of the boundary of two different policies (the corner weight). This indicates a plausible but incorrect inference and\nimpact the evaluation adversely."}, {"title": "4.4 Sensitivity Analysis Experiment Setting", "content": "To evaluate the DWPI model when using different types of demonstration, we conducted a sensitivity analysis by\nassessing performance with different demonstration representations."}, {"title": "4.4.1 Varying demonstration Representation", "content": "The DWPI model is trained with cumulative rewards (demonstration reward) by default. However, there are other\nrepresentations of a demonstration, e.g. state demonstration. It is worthwhile to examine the difference in performance\nwhile different representations are used in PI.\nIn this section, we give the experiment settings to evaluate the performance of the DWPI model trained with state\ndemonstrations. As the three simulation environments are all based on a grid world framework where the agent position\nis finite and discrete. We number the agent position and use a visiting frequency to denote the state demonstration. The\nstate-frequency demonstration are calculated as:\n$\\tau_s = \\sum_{t=1}^{|S|} e(s_t).I_{|S|}$"}, {"title": "5 Results and Analysis", "content": ""}, {"title": "5.1 Time Efficiency", "content": "To ensure a fair comparison, we terminate the baselines once the error of PI converges. The time consumption of DWPI\nincludes not only the PI process but also the training process of both the DWMORL model and the DWPI model.\nThe results are shown in Figure 3. The figure illustrates that the DWPI algorithm demonstrated superior time efficiency\ncompared to the baseline PM and MWAL algorithms.\nSpecifically, in the CDST simulation, DWPI was 94.83% faster than PM and 88.35% faster than MWAL; in the Traffic\nsimulation, DWPI was 88.41% faster than PM and 68.02% faster than MWAL; and in the Item Gathering simulation,\nDWPI was 81.67% faster than PM and 49.85% faster than MWAL.\nThe results show that the DWPI algorithm is significantly more time-efficient for PI tasks in these three simulations\ncompared to the baseline algorithms."}, {"title": "5.2 Inference Accuracy", "content": "The overall inference accuracy comparison between the DWPI algorithm and baselines is shown in Figure 4.\nIn CDST simulation, the direct comparison of inference and ground truth is adopted as the Pareto front is already known.\nCompared with the PM and MWAL algorithms, the DWPI algorithm shows 20% and 42% higher inference accuracy,\nrespectively. Additionally, our approach achieves perfect accuracy (100%) in inference tasks for demonstrations with a\n50% stochastic ratio. Further information regarding the results of the CDST simulation can be found in Section 5.2.1.\nInference error is used for accuracy evaluation in the Traffic environment and the Item Gathering environment as their\nPareto fronts are not known. The inference is operated on the resulted demonstrations of the four typical preference\nentries, the stochastic ratio is also 50% to evaluate the algorithms' robustness. In the Traffic simulation, DWPI achieves\n94.91% lower inference error than the PM, and 98.12% lower inference error than the MWAL method. In the Item"}, {"title": "5.2.1 DST environment", "content": "In the CDST simulation, we compare the DWPI algorithm to baseline algorithms according on their inference accuracy.\nA more detailed comparison is presented in Figure 5. Both PM and MWAL perform well when demonstrations are\ngoing to Treasures 124 and 122 because they are belong to the two largest policy regions. However, the performance of\nMWAL and PM deteriorates within policy regions mapped by the middle area of preference space. This is because\nthose preference regions are more subtle and small, and even a small change in preference weight may incur a totally\ndifferent policy. In the case of stochastic sub-optimal demonstration, while PM has a chance to infer a near-correct\npreference vector by sample and try-out, MWAL suffers from the difference between the sub-optimal demonstration"}, {"title": "5.2.2 Traffic environment", "content": "Figure 6 illustrates the outcome of PI in Traffic simulation. We assessed the inference accuracy in four common\nscenarios: always safe, always fast, fast safe, and slow safe. The red color represents the ground truth of the preference\nweight, while the green, blue, and purple colors represent the inference of DWPI, the PM, and MWAL, respectively. It\nis worth noting that the weights are in line with the experiment setting used by Kallstrom et al. K\u00e4llstr\u00f6m and Heintz\n[2019], and normalization during training or inference would not affect their generality. The y-axis is the preference\nweight, and the x-axis labels are the objectives, i.e. [steps, item collection, break traffic rules, collisions, wall hitting].\nThe numeric inference results are put in the title of each subplot.\nBased on the results depicted in Figure 6, both the MWAL and PM approaches are able to capture the general shape\nof the true preference, however, there are some deviations to decrease the performance. In the \"always fast\" case, the\nMWAL method reduces too much the preference for avoiding collision and hitting the wall, while the PM shows two\npeaks in the preference distribution, despite the two preference weights actually being a downslope. The MWAL method\nperforms well in the deterministic \"always safe\" case, where the agent picks the safest road and avoids breaking rules or\nbeing hit by a car in a grid-world environment. However, the MWAL method is brittle in cases with stochasticity, such\nas \"always fast\", \"fast safe\", and \"slow safe\". On the other hand, the PM performs reasonably well in most cases and is\nnot very sensitive to stochasticity, but it suffers from time efficiency and performs poorly in the \"always fast\" case.\nThe poor performance of baselines in the \u201calways fast\" case is due to the high stochasticity of the environment when\nthe demonstrator wants to be always fast, and the agent goes on a road with many cars. This introduces significant\nstochastic noise to the reward demonstration, causing both baselines to almost fail to infer the preference weight. In\ncontrast, the DWPI model, which can be trained by stochastic demonstrations, shows robustness in not only the \"always\nfast\" case but also perfectly infers the preference for the other three cases. As shown in Figure 6, Figure 3, and Figure 4,\nour DWPI model can almost perfectly infer the preference weights within the shortest time."}, {"title": "5.2.3 Item Gathering environment", "content": "Figure 7 showcases the results of the Item Gathering simulation where we evaluate the inference performance of the\nDWPI, PM, and MWAL for the competitive, cooperative, fair, and generous behavior patterns of the agent. Similar to\nFigure 6, the ground truth is shown in red while the results from the three methods are shown in green, blue, and purple.\nThe preference weights are consistent with the ones used in literature K\u00e4llstr\u00f6m and Heintz [2019], and normalization\nis applied for generality. The x-axis represents the objectives such as time penalty, wall penalty, and item collections.\nThe numeric inferences are displayed in to the subplot title.\nThe results in Figure 7 indicate that our DWPI model can capture not only the general shape of the true preference\nbut also demonstrate high accuracy numerically, including negative weights. In contrast, the PM can only capture\nthe rough shape of the ground truth and fails to infer the negative weight for the competitive behavior pattern, and\ncannot provide distinguishable results for the cooperative and fair behavior patterns. Due to the high stochasticity of the\nenvironment where items are randomly dropped at the start of the episode, MWAL cannot give accurate inferences and\neven converges to incorrect preference weights."}, {"title": "5.3 Varying demonstration Representation Sensitivity Analysis", "content": "In this section, we present the results of sensitivity tests, where we evaluate the inference performance of DWPI for\ndifferent categories of demonstration. We run each evaluation for 5 times with different random seeds\u00b3. The results of\ninference from reward demonstration and state demonstration are shown in Figure 8. Similar to Figure 4, the metric\\"}]}