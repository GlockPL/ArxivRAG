{"title": "Fast and Robust Contextual Node Representation Learning over Dynamic Graphs", "authors": ["Xingzhi Guo", "Silong Wang", "Baojian Zhou", "Yanghua Xiao", "Steven Skiena"], "abstract": "Real-world graphs grow rapidly with edge and vertex insertions over time, motivating the problem of efficiently maintaining robust node representation over evolving graphs. Recent efficient GNNs are designed to decouple recursive message passing from the learning process, and favor Personalized PageRank (PPR) as the underlying feature propagation mechanism. However, most PPR-based GNNs are designed for static graphs, and efficient PPR maintenance remains as an open problem. Further, there is surprisingly little theoretical justification for the choice of PPR, despite its impressive empirical performance. In this paper, we are inspired by the recent PPR formulation as an explicit $l_1$-regularized optimization problem and propose a unified dynamic graph learning framework based on sparse node-wise attention. We also present a set of desired properties to justify the choice of PPR in STOA GNNs, and serves as the guideline for future node attention designs. Meanwhile, we take advantage of the PPR-equivalent optimization formulation and employ the proximal gradient method (ISTA) to improve the efficiency of PPR-based GNNs upto 6 times. Finally, we instantiate a simple-yet-effective model (GOPPE) with robust positional encodings by maximizing PPR previously used as attention. The model performs comparably to or better than the STOA baselines and greatly outperforms when the initial node attributes are noisy during graph evolution, demonstrating the effectiveness and robustness of GOPPE.", "sections": [{"title": "1 INTRODUCTION", "content": "Real-world is changing all the time [14]. Specifically, many real-world graphs evolve with structural changes [18, 41] and node attributes refinement over time. Considering Wikipedia as an evolving graph where Wiki pages (nodes) are interconnected by hyperlinks, each node is associated with the article's descriptions as attributes. For example, the node \"ChatGPT\" initially \u00b9 contained only had a few links redirecting to the nodes OpenAI and Chatbot, before being gradually enriched 2 with more hyperlinks and detailed descriptions. Such examples motivate the following problem:\nHow can we design an efficient and robust node representation learning model that can be updated quickly to reflect graph changes, even when node attributes are limited or noisy?\nThe proposed problem consists of two parts- 1): how to design a high-quality and robust node representation algorithm and 2): how to efficiently update the node representation along with the graph changes.\nOne possible approach is through Graph Neural Networks (GNNs). Pioneered by SGC[42], APPNP [12], PPRGo[4], and AGP [36], they replace recursive local message passing with an approximate feature propagation matrix. For example, SGC formulates the node classification problem into $\\bar{y} = Softmax(S^k X \\Theta)$ where $S^k$ is the kth power of the normalized adjacency matrix, X is the node attribute matrix and $\\Theta$ is the learnable classifier parameter. This formulation decouples propagation from the learning process because $S^k$ can be explicitly calculated independent of the neural model.\nMore importantly, this formulation provides a novel interpretation of GNN as a feature propagation process, opening doors to new propagation mechanism designs. In particular, Personalized PageRank (PPR) is a popular choice, and PPR-based models significantly improve GNN's efficiency and scalability. However, our interested"}, {"title": "2 RELATED WORK", "content": "Learning over dynamic graphs is an emerging topic, and has been studied from three perspectives with different problem settings:\nCo-evolution Models over Dynamic Graphs: The problem of dynamic co-evolution focuses on the relations between node attributes and the growing network topology. For example, DynRep [34] is the pioneering model. A typical task is to predict whether two vertices will be connected or interact in the future given the current graph topology and the node attributes. The key is to learn a node-wise relation model to represent the interaction probability among vertices, rather than updating the node embedding w.r.t. the graph changes. Due to the nature of $O(|V|^2)$ complexity, these models are not scalable when the node size is large.\nNode Time Series Models over Dynamic Graphs: Several temporal GNNs [30, 31, 44] have been proposed for modeling the node embeddings w.r.t. the graph changes in discrete or continous time. A typical example is JODIE[22], which models the trajectory of node embeddings across time and predicts its position at a future timestamp. Similar to the co-evolution models, the essence is to obtain a compact embedding from the node history, and use it"}, {"title": "3 NOTATIONS AND PRELIMINARIES", "content": "Notations: Given a undirected unweighted graph $G = (V, \\mathscr{E}, X)$ where $V = \\{1, 2,..., n\\}$ is the node set, $\\mathscr{E} \\subseteq V \\times V$ denotes the edge set and $X \\in \\mathbb{R}^{d\\times|V|}$ is the nodes' d-dim attributes. For each node u, we define $Nei(u)$ to be the set of v's neighbors; d denotes node degree vector where each element $d(i) = |Nei(i)|$. The degree matrix is $D := diag(d)$ and A is the adjacent matrix where $A_{i, j} \\in \\{0, 1\\}$ and without self-loop. To simplify our notation, we use $d_i$ to denote the degree of node i. Let $P = D^{-1}A$ be the random walk matrix and then $P^T = A^TD^{-1}$ is the column stochastic matrix."}, {"title": "3.1 Message Passing as Approx. Propagation", "content": "In many successful and popular GNNs designs [19, 21], the node attributes $x_i$ serve as the message or graph signal, propagating to its local neighbors $Nei(v_i)$ defined explicitly by the network structure G. Meanwhile, $v_i$ locally convolutes the incoming neighbors' messages as its hidden representation, denoted as $h_i$. Typically, a set of trainable convolutional kernels $W^{(l)} \\in \\mathbb{R}^{h \\times d}$ is used in a permutation-invariant function recursively (l is the recursion level). For example, $h_i^{(l+1)} = \\sum_{j \\in Nei(v_i)} W^{(l)} h_j^{(l)}$, where $h_i^{=0} := x_i$. The GNN models are usually trained in a supervised setting, learning local message aggregation and label prediction end-to-end. Interestingly, SGC [42] has simplified GNN design and decoupled the feature propagation from label prediction learning:\n$H^{(L)} = W^{(L-1)} H^{(L-1)} P^T, P = D^{-1}A$\n$H^{(L-1)} = W^{(L-2)} H^{(L-2)} P^T$\n$H^{(1)} = W^{(0)} H^{(0)} P^T$\n$H^{(0)} := X \\in \\mathbb{R}^{d \\times |V|}$, by definition.\n$H^{(L)} = W^{(L-1)}W^{(L-2)}...W^{(0)} XP^T...P^T P^T$\n$= X P^T, note that W := \\prod_{l=0}^{L-1} W^{(l)}, P := P^L, (1)$\nwhere Equ.(1) reveals strong connections between the GNN message-passing mechanism and well-studied vertex proximity measures in network science. Alternatively, by taking into account $W X$ as a whole, Equ. 1 can be interpreted as global label propagation. Both interpretations give a contextualized node representation for $v_i$ over V. However, as $L \\rightarrow \\infty$, $(D^{-1}A)^L$ will converge w.r.t. its first eigenvector (stationary property), making the output less distinctive and causing the over-smoothing issue [27].\nPPR as a node proximity design for GNNs: The celebrated algorithm Personalized Pagerank (PPR) provides a successful localized design of node proximity. One intriguing property of PPR is the locality governed by the teleport component, making PPR quantities concentrated around the source node. Starting from APPNP [12], PPR became a popular GNN design choice and empirically achieves outstanding performance. However, there is little theoretical framework or justification for using PPR. In section 5.2, we will"}, {"title": "3.2 Approx. PPR as $l_1$-Regularized Optimization", "content": "The key ingredient in PPR-based GNNs is PPR calculation. Many algorithms have been designed to approximate PPR while having better run-time complexity. In this section, we define PPR and discuss the recent research on efficient PPR approximation, including traditional algorithmic approaches and newly emerging optimization perspectives.\nDefinition 1 (Personalized PageRank). Given an undirected unweighted graph G(V,E) with |V| = n and |E| = m. Define the random walk transition matrix $P \\equiv D^{-1}A$, where A is the adjacency matrix of G and $A = A^T$. Given teleport probability $\\alpha \\in [0, 1)$ and the source node s, the Personalized PageRank vector of s is defined as:\n$\\pi_{\\alpha, s} = (1 - \\alpha) P \\pi_{\\alpha, s} + \\alpha e_s,$\nwhere the teleport probability $\\alpha$ is a small constant (e.g. $\\alpha = 0.15$), $e_s$ is an indicator column vector of node s, that is, s-th entry is 1, 0 otherwise. We use $\\pi_{s,\\alpha}$ to denote the PPR ofs with teleport value $\\alpha$.\nAlgorithmic approaches: Power iteration and Forward push [1] (Algorithm 2 in Appendix) are designed to compute PPR algorithmically. The key idea of the push-based methods [37, 43] is to exploit PPR invariant [1] and gradually reduce the residual vector to approximate the high-precision PPR.\nApproximate PPR via optimization: Interestingly, recent research [9, 10] discovered that Algorithm 2 is equivalent to the Coordinate Descent algorithm, which solves an explicit optimization problem. Furthermore, with the specific termination condition, a $l_1$-regularized quadratic objective function has been proposed as the alternative PPR formulation (Lemma 2).\nLemma 2 (Variational Formulation of Personalized PageRank [9]). Solving PPR in Equ.2 is equivalent to solving the following $l_1$-regularized quadratic objective function where the PPR vector $\\pi := D^{1/2}x^*$ :\n$x^* = arg\\underset{x}{min} f(x) := \\frac{1}{2} x^T W x + x^T b + \\epsilon ||D^{1/2}x||_1, (3)$\nwhere $W = D^{-1/2}(D - (1 - \\alpha)A)D^{-1/2}, b = -\\alpha D^{-1/2}e_s$\nLemma 2 provides a unique perspective to understand PPR and can be solved by proximal gradient methods such as ISTA. Equ.3 has the composition of a smooth function $g(x)$ and a non-smooth function $h(x)$, Note that\n$\\nabla^2 g(x) = W = \\alpha I_n + (1 - \\alpha) (I_n - D^{-1/2}AD^{-1/2})$\nwhere $\\Lambda_{max}$ is the largest eigenvalue. With $|\\nabla^2 g(x)||_2 \\leq 2 - \\alpha$, let $\\eta = \\frac{2}{\\alpha}$, one can convert the original problem (Equ.3) into"}, {"title": "4 PROBLEM DEFINITION", "content": "Definition 3 (Node Classification Problem over Dynamic Graph). Given an initial graph at timestamp t = 0 $G_0 = (V, \\mathscr{E}_0, X_0)$ 4, where $X_0 \\in \\mathbb{R}^{n \\times d}$ denotes the node attribute matrix, we want to predict the labels of a subset of nodes in $S = \\{u_0, u_1, ..., u_k\\}$ as the graph evolves over time. At time $t \\in \\{1, ..., T\\}$, the graph $G_{t-1}$ is updated with a batch of edge events $\\Delta \\mathscr{E}_t$ and a new node attribute $X_t$. For each snapshot $G_t$, we train a new classifier with updated node representation $H_S \\in \\mathbb{R}^{|S|\\times d}$ to predict the node labels $y_t$.\nKey Challenges: The key challenges of using PPR-based GNNS boil down to two questions - 1): How to efficiently calculate PPR as the graph evolves? and 2): How to effectively fold in PPR as part of the node representation? In the following section, we answer these key questions using a justifiable PPR design with more robust and faster PPR-based GNNs over dynamic graphs."}, {"title": "5 PROPOSED FRAMEWORK", "content": "To efficiently and effectively learn node representations over time, we formulate PPR-based GNNs as a contextual node representation learning framework which has two key components: 1): Efficient PPR maintenance through an optimization lens; 2): Effective PPR-based node contextualization and positional encodings. Meanwhile, we discuss the desired properties of GNN design and justify the usage of PPR. Then we instantiate a simple-yet-effective model called GoPPE, analyze the overall complexity, and discuss its relations to other successful PPR-based GNN designs."}, {"title": "5.1 Maintain PPR in the lens of Optimization", "content": "Over an evolving graph, the new interactions between vertices are captured by adding edges, causing graph structure changes and making the previously calculated PPRs stale. To make the PPR-based GNNs reflect the latest graph structure, we propose to use ISTA as the PPR solver through an optimization lens and further explore its efficiency for dynamic graphs in various change patterns.\nNaive Approach: Always from Scratch: One can simply re-calculate PPRs from scratch in each snapshot, however, this is sub-optimal because it does not leverage any previous PPR results, turning the dynamic graph into independent static graph snapshots.\nImproved Approach: Maintain PPR with warm starting in its optimization lens: By exploiting the PPR invariant, PPR adjustment rules [47] have been proposed to maintain PPR w.r.t. a sequence of edge changes (add/delete edges), which can be proved equivalently to adjust the solution and gradient in the PPR's optimization formulation. By adjusting the previous PPR and using it as the warm-starting point, we can maintain PPR incrementally. It provides an elegant solution to dynamically maintain PPR while enjoying a favorable convergence rate inherent in ISTA.\nTheorem 4 (PPR Adjustment Rules for Dynamic Graphs). Given a new edge inserted denoted as (u, v, 1), The internal state of the PPR solver can be adjusted by the following rules to reflect the latest PPR but in compromised quality. Instead of starting from x = 0, we use the adjusted solution as a warm-start for ISTA-solver (Algorithm. 1).\n$x'(u) = x(u) * \\frac{d(u) + 1}{d(u)} (4)$\n$\\nabla f'(x')(u) = \\nabla f(x)(u) - \\frac{x(u)}{\\alpha d^{1/2} (u)}$\n$\\nabla f'(x')(v) = \\nabla f(x)(v) + \\frac{(1 - \\alpha) d^{-1/2} (v) * x(u)}{\\alpha}$\nwhere $x'(u), \\nabla f'(x')$ denotes the value after adjustment.\nNote that $\\nabla f'(x')$ can be evaluated analytically, so the above rules can be reduced to a single update using Equ.4. Each update is a local operation having complexity O(1) per edge event for each node in S."}, {"title": "5.2 PPR-based Contextual Node Representation and Positional Encodings", "content": "Local message passing is the key ingredient in many GNN designs. Inspired by SGC, we provide another interpretation of GNN as a global node contextualization process. It aggregates node features in a global manner, in contrast to the viewpoint of recursive local neighbor message passing. We start the analysis from the simplified local message passing design. Recall the serialized GNNs in Equ. 1:\n$H^{(L)} = W X P^T, W := \\prod_{l=1}^{L-1}W^{(l)}, P := (D^{-1}A)^L,$\nwhere L is the hyperparameter controlling how many hops should the GNN recursively aggregates the local neighbors' features. We can further organize Equ.1 into $H^{(L)} = W \\hat{H}$, where $\\hat{H} = X P^T$. Let $h_i, \\pi_i$ denote the ith column of $\\hat{H}$ and $P^T$. $\\pi_i(j)$ refers to its jth"}, {"title": "6 EXPERIMENTS", "content": "In this section, we describe the experiment configurations and present and discuss the results of running time and prediction accuracy. Finally, we demonstrate the effectiveness of the introduced positional encodings when the node attributes are noisy."}, {"title": "6.1 Experiment Configurations", "content": "Datasets: To evaluate GNNs in a dynamic setting, we divide the edges of static undirected graphs into snapshots 8. For node classification tasks, the first graph snapshot $G_0$ contains the 50% of the total edges, and the other half is evenly divided into edge events in each snapshot $\\Delta \\mathscr{E}_i, i \\in \\{1, . . ., T - 1\\}$ for the following $G_{\\{1,...,T-1\\}}$. We fix the node labels in each snapshot, while the node attributes may change according to the experiment settings. All the train/dev/test nodes (70%/10%/20%) are sampled from the first snapshots without any dangling node. In the efficiency test, we report the running time in two dynamic setting detailed in Section 6.2. The statistics of datasets are presented in Table 2.\nBaselines: Besides the MLPs and GCN as the baselines, we compare with the State-of-the-Art PPR-based methods as listed in Table 1. Within our framework, we implement the baselines which are equivalent to DYNAMICPPE, PPRGO and INSTANTGNN, together with our proposed GOPPE. We apply the same shallow neural classifier (2-layer MLPs) and hyper-parameter settings to test the embedding quality across all methods for a fair comparison. More details are presented in the Appendix A.2.\nMetrics: Since the PPR weights calculating are the most time-consuming bottleneck, we measure the total CPU time for PPR calculation along with the graph updates. We use the multi-class"}, {"title": "6.2 Efficiency Experiments", "content": "In PPR-based GNNs for dynamic graphs, the main computational bottleneck is the PPR maintenance over time. In this section, we present the running time under two cases regarding the intensity of graph changes between snapshots \u2013 1: Major change case where we start from 50% of the total edges, then add 10% edges between snapshots until all edges are used. 11 2: Minor change case where we start from a almost full graph but held out a few edges, then we add a fixed number of edges (e.g., 100) between snapshots until all edges are added. We design these two cases to evaluate the efficiency of PPR maintenance under different dynamic patterns. In the following, we discuss the insights from our empirical observations."}, {"title": "6.3 Accuracy and Robustness Experiments", "content": "We have two node classification settings described as follows to evaluate the performance and robustness of GoPPE. In both cases, we adopt major change as the dynamic pattern. 1: Intact case where we use the intact raw node attributes for global feature aggregation; 2: Noisy case where we add Gaussian noise to the node attributes (similar to [24]) at the beginning, then gradually reduce the noise level to simulate the node attributes getting better over time. The added noise is monotonically reducing defined as:\n$x_t = x_t + (1 - \\lambda_t) \\mathcal{N}(\\mu, \\sigma^2), (9)$\nwhere $\\lambda_t = \\frac{t}{T} \\lambda_{base} \\in [0, 1]$ controls noise level, t is the snapshot stamp, T is the total number of snapshots, $\\lambda_{base} \\in [0, 1]$ controls the initial noise level, $\\mu, \\sigma^2$ are the mean and variance of the elements in node attributes matrix $X$, respectively. As t increases from 0 to T, the noisy component decays linearly. We use this noise model to simulate the feature refinement process, similar to the volunteers helping enrich Wikipedia articles and make the articles more clear and relevant. In addition, we add a variant of GoPPE (GoPPE-NoAtt.) where we replace node attributes with pure Gaussian noise, demonstrating the effectiveness of the positional encodings.\nGOPPE is robust against noisy node attributes and consistently outperforms the baselines: Table 5 presents the average accuracy over snapshots where the node features are gradually refined as defined in Equ.9. As expected, the models without positional encodings (MLP, PPRGo, and InsGNN) perform poorly as they heavily depend on the node attributes. Meanwhile, GoPPE and its variant (GoPPE-NoAtt) demonstrate the robustness by having positional encodings and outperforms others.\nGOPPE achieves comparable performance with intact features: Table 6 present the averaged prediction accuracy across all snapshots. Specifically, we found the positional encoding (DYNPPE) alone can yield strong performance, further demonstrating the importance of locality information. Meanwhile, we will analyze the performance difference in pubmed and flickr later in Section 6.4."}, {"title": "6.4 Discussions and Future Works", "content": "Robust positional encodings achieves better accuracy over graphs of weak node attributes : Table 6 shows that MLPs provides poor performance on cora and citeseer, implying that these graphs have weak node attributes for prediction. It makes PPRGO"}, {"title": "7 CONCLUSION", "content": "In this paper, we proposed an efficient framework for node representation learning over dynamic graphs. Within the framework, we"}]}