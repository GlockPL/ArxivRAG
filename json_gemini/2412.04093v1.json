{"title": "Practical Considerations for Agentic LLM Systems", "authors": ["Chris Sypherd", "Vaishak Belle"], "abstract": "As the strength of Large Language Models (LLMs) has grown over recent years, so too has interest in their use as the underlying models for autonomous agents. Although LLMs demonstrate emergent abilities and broad expertise across natural language domains, their inherent unpredictability makes the implementation of LLM agents challenging, resulting in a gap between related research and the real-world implementation of such systems. To bridge this gap, this paper frames actionable insights and considerations from the research community in the context of established application paradigms to enable the construction and facilitate the informed deployment of robust LLM agents. Namely, we position relevant research findings into four broad categories-Planning, Memory, Tools, and Control Flow-based on common practices in application-focused literature and highlight practical considerations to make when designing agentic LLMs for real-world applications, such as handling stochasticity and managing resources efficiently. While we do not conduct empirical evaluations, we do provide the necessary background for discussing critical aspects of agentic LLM designs, both in academia and industry.", "sections": [{"title": "1 Introduction", "content": "In academia, the concept of \"agents\" has been well-defined for decades (e.g., [94]), and thus the proposition of agents based on LLMs comes with predefined criteria and expectations. As such, agentic LLMs in the research community have come to be defined as autonomous systems with capabilities of beliefs [47, 62, 71], reasoning [105], planning [36, 72], and control [72]. Under this definition, the ability to plan, reason, and interact with an environment have emerged as the key considerations for success [97].\nFor LLM agents in industry and real-world deployment, the history and breadth of agents has been condensed to a definition along the lines of \"a system that can use an LLM to reason through a problem, create a plan to solve the problem, and execute the plan with the help of a set of tools\" [84]. Most industry discussions follow this form, introducing the LLM as the central reasoning engine and adding planning, memory, and tools as three necessary modules (e.g., [12, 13, 16, 84, 93]). Indeed, most industry resources focusing on deployable agentic LLM systems are accompanied by a diagram similar to Figure 1, focusing largely on single agents. While this description is helpful for the most basic of LLM agents, it glosses over some of the more nuanced considerations that must be made for the informed construction of robust agentic LLM systems.\nThe prevailing view of LLM agents in industry brings to light the disparity between (1) research into LLMs and agents and (2) the application of agentic LLM systems in real-world scenarios. To bridge this gap, we propose framing relevant findings from the research community in the common industry view of LLM agents. To that end, we organize this work into four main sections-Planning, Memory, Tools, and Control Flow-that correspond to, respectively, planning, memory, tools, and the central reasoning engine components referenced above. We tailor the contents of this paper to black-box LLM-based single-agent systems typical of that industry perspective. By doing so, we hope to create an actionable and approachable survey that enables information exchange between academia and industry within a manageable scope."}, {"title": "2 Related Work", "content": "Many surveys discussing LLM-based agents focus on multi-agent frameworks and related ideas [32, 105]. While similar, the challenges facing multi-agent systems are distinct from the real-world deployment of a single LLM agent. Here, we focus more on deliberately crafting a robust agent rather than the orchestration of many.\nAnother approach, taken by [100], focuses on methods for improving agentic LLM performance starting at the underlying model, looking into data composition and training methodologies. We focus on implementation considerations that improve agentic LLM system performance from a black-box perspective, which lends itself more to real-world deployment. Others focus on creating unified taxonomies [49, 97] or target a single component of LLM agents, such as planning [36].\nThe most similar work is [87], providing a comprehensive survey of works relating to LLMs as agents as well as reviewing aspects of their design, application, and evaluation. While [87] develops a valuable unified framework based on extant research, we leverage research findings to provide practical application-focused insights and frame our review in the context of the LLM agent paradigm that has developed organically in industry."}, {"title": "3 Applied Scenario", "content": "To help illustrate some of the following points, we propose the example outlined in Figure 2 of applying an LLM agent as a pescetarian\u00b9 meal assistant. We will refer to this as the primary example\u00b2 throughout this work for consistency, citing specifics from it by the codes assigned in Figure 2 (e.g., 2.R1 to refer to \"Pescetarian recipe book\")."}, {"title": "4 Glossary", "content": "This glossary serves to briefly introduce the following terms that will be used across subsequent sections. Later sections will provide additional contextualization and examples of their utility but not necessarily explicit definitions.\nPersona. A persona (also referred to as a \"role\" or a \"profile\") is the identity assigned to the LLM, often as part of the system prompt. The persona is the lens through which the LLM will interpret and respond to prompts. The persona (e.g., Figure 2.Pe2) can be defined and refined by an occupation (e.g., \"professional chef\"), level and domain of expertise (e.g., \"specializing in pescetarian dishes\"), and personality traits (e.g., \"friendly and understanding\") but can be further customized by adding details such as age, race, gender, and nationality [17, 87].\nTool. Tools are the means by which an LLM can interact with its environment (beyond basic textual exchange) and access external resources [66]. Retrieval Augmented Generation (RAG) is\nHyperparameters. This section includes a brief overview of hyperparameters we will reference but does not explore their technical details\u00b3.\n\u2022 Seed. Some LLM interfaces will have a \"seed\" parameter that should, provided all other parameters remain constant, produce the same output.\n\u2022 Temperature. Temperature corresponds to the degree to which randomness will be employed in selecting the output tokens. This usually plays out in higher temperature responses being more creative and rambling while lower temperature responses are more predictable and straight-to-the-point. Thus, for more consistent results, a lower temperature (e.g., 0.0 to 0.5) can be used.\n\u2022 Top-p. Top-p (also known as \"nucleus sampling;\" introduced in [34]) corresponds to the probability threshold for selecting tokens that can form part of the output, bounded 0.0 to 1.0. Lower top-p values restrict the pool of tokens the LLM can choose from, resulting in more reproducible outputs."}, {"title": "5 Planning", "content": "Planning has long been a core component of agent research [59, 94]; it allows more complex tasks to be handled in smaller, more manageable steps. Planning can also enhance the interpretability of an LLM agent, as the steps of the plan and the stopping criteria will be defined in a interpretable format."}, {"title": "5.1 LLMs and Planning", "content": "Despite anecdotal applications showing signs of successful LLM planning [77], more holistic reviews suggest that LLMs make poor planners [25, 38, 52, 82, 83]. As such, if an LLM agent is to be deployed in an environment with a consistent task, manually curating a plan can alleviate the pains of poor LLM planning as well as provide an opportunity to manually craft relevant roles and prompts.\nAnother option is to augment the LLM agent with an external planning tool, which has shown promise [25, 52]. Because LLM planning remains an open area of research, the example in Figure 2.P1 simply assumes planning capabilities without subscribing to a specific approach, for illustrative purposes.\nTo describe current approaches to planning in LLM agents, we categorize them into implicit and explicit planning. For implicit planning, some agents will rely on the LLM to iteratively determine the immediate next step until the task is complete, without ever eliciting a plan [33, 104]. This approach relies on the idea that, when provided with an end goal, the LLM can maintain an internal plan whose steps are revealed iteratively without any explicit plan"}, {"title": "5.2 Task Decomposition", "content": "It is important to understand the limitations of an LLM before formulating a plan for it to execute. Agentic LLM systems are often applied to problems that a single LLM call cannot resolve but a sequence of calls can. Tasks can typically be decomposed into smaller pieces that, when solved individually, can be reconstructed to produce the final solution [107].\nReturning to Figure 2, the request made in Figure 2.11 is composed of multiple subtasks, namely: (1) retrieve recipes that contain rice, beans, and tomato that the user will like and (2) order any missing ingredients. It is also reasonable to decompose (1) further, into a retrieval of recipes that contain the required ingredients and separately a request to select the one that best fits the user's tastes.\nIf decomposing a well-defined task manually, iteratively decomposing the task into subtasks and testing an LLM on them can provide valuable insight into what the LLM can consistently handle. Breaking down the problem logically is simple enough, but ascertaining which tasks an LLM can perform well and which require further decomposition can be challenging, particularly when dealing with stochasticity and prompt changes. It is recommended to evaluate the LLM agent frequently and systematically during this process, as discussed in Section 9.2. It may be easier to start at the most basic building blocks of the tasks and combine them than to find the minimum number of viable tasks to start.\nWhile it may be intuitive to assume that the more atomic the task the better, this is not always the case. It has been shown that LLMs not only possess the ability to solve multiple distinct tasks in a single query [44, 76, 98] but that composing multiple tasks into a single prompt can increase performance on all constituent tasks, as well as decreasing overall context usage [76]. However, the degree to which tasks may be combined should be the subject of rigorous experimentation for the specific task and environment in which it is considered."}, {"title": "5.3 Plan Adherence", "content": "One of the responsibilities of the LLM agent is to oversee the application of the plan. It should decide if a step needs to be repeated (e.g., for Error Handling) or skipped for a given input (e.g., to iterate on the plan [55]). One of the major concerns of LLMs as planners is their inability to identify whether or not they can complete a given task [38]. As such, it is often impossible for an LLM agent to know if a step will be successful until it has been attempted. Thus, it follows logically that an evaluation of the success of each step should take place following execution. Similarly, the overall success of the plan should be evaluated upon completion of all steps. If unsuccessful, the LLM agent may need to adjust or rerun the plan, based on the results of each step and the overall plan (see Section 8.2 for a discussion on incorporating feedback)."}, {"title": "6 Memory", "content": ""}, {"title": "6.1 Retrieval Augmented Generation", "content": "Retrieval augmented generation (RAG) (introduced in [46]) has emerged as a staple of agentic LLM applications in industry [31]. The basis is simple: a system that can provide external context relevant to a natural language input. Typically, an incoming input will be compared against a ground-truth data store and the most relevant piece(s) of information will be provided to the LLM as context upon which it will base its response. This can be done either implicitly, where a user's input is always used for retrieval for a given LLM call, or explicitly, where the LLM uses RAG as a tool. This has a number of benefits for LLM systems:\n\u2022 Grounding. Rather than relying on the LLM \u201cremembering\" relevant context from its training data correctly, we can provide the LLM with accurate relevant information. Providing grounded text as context significantly reduces LLM hallucinations and fills knowledge gaps in the training data [30, 46, 74].\n\u2022 Explainability. Rather than relying on an LLM opaquely referencing information it has been trained on, adherence to context supplied as part of RAG provides insight into exactly where an LLM is getting its information [31].\n\u2022 Timeliness. While LLMs can reference information from their static training data, the LLM will be subject to a hard information cutoff (the latest date training data was scraped) and a soft information cutoff (events close to its hard information cutoff that have limited coverage). Rather than turning to the infeasible prospect of retraining with updated data, we can provide updated information that is relevant to the query as context [31].\n\u2022 Outsourcing. Depending on the content, quality, and reliability of the RAG database, aspects of the query can be implicitly outsourced to the context returned, such as reasoning and decision-making.\n\u2022 Alignment. The vast amount of training data used for LLMs is the source of their natural language understanding but should not necessarily be relied on for unbiased, trustworthy, and safe generation. Typically, aligning LLM ouputs with human preferences is seen as a data collection and training problem [19, 90] but can also be addressed post-hoc with RAG. By augmenting an LLM's natural language capabilities and tendencies with context derived from a more refined dataset that adheres to a desired set of human preferences, its output can be guided to conform to a desired set of content and attitudes. This requires careful curation of the data store but is a viable method for black-box alignment."}, {"title": "6.2 Long-Term Memory", "content": "Sometimes, key information is gained during a conversation that may be helpful across all contexts, such as a useful piece of external knowledge or information about a user or task. In those instances, it may be advantageous to store that information in a way accessible to the agent so that its impact is not limited to the current context. This is commonly referred to as \"long-term memory\" [64, 87, 106]4.\nWe want to be selective with the information that is stored in long-term memory so that it is generally useful and not excessively large. Some common approaches are to store prior solutions to queries [64], global summaries and insights [106], and acquired tools [86].\nLong-term memory can be enhanced with reflection, consolidation, forgetting, revision, and other mechanisms designed to mimic long-term memory in humans (see [106] for a discussion on advanced long-term memory implementation). For simplicity, we focus on a simple version of long-term memory, where information is simply stored and retrieved, and any edits are manual. For this simple variation, we derive the following three criteria from existing literature on long-term memory in LLM agents [14, 64, 86, 87, 106] to use as a litmus test for what information should be stored:\n\u2022 Independent. The information should not have any implicit dependencies, such as input values.\n\u2022 Relevant to a consistency. The information should be relevant to consistencies in the agentic LLM system, which may include a task, user, or environment.\n\u2022 Applicable long-term. The information should consistently be applicable to contexts to which the LLM agent may be exposed.\nExtracting Information for Long-Term Memory. A common approach to extracting information that belongs in long-term memory is to leverage an external conversation moderator [73, 106]. The external moderator (e.g., an LLM with a separate role) that reviews conversations (either whole or in pieces) and can be tasked with extracting information it deems compliant with the three criteria above. This is an instance where care must be taken with phrasing as the subjectivity of the task may make the LLM prone to framing bias in its response (e.g., if we ask if there is anything useful to pull out, the LLM will likely pull out some information) [28].\nStoring Long-Term Memory. Once a piece of information has been deemed worthy of long-term memory, it should be stored. Some approaches include embedding and storing the information in a vector database (similar to RAG) [51, 106] and natural language storage, although interacting with the latter quickly becomes unwieldy as the amount of long-term memory increases. The structure of the vector database allows us to easily query relevant information [51, 86, 106].\nUtilizing Long-Term Memory. Once information is stored in long-term memory, we must decide when to expose it to the LLM agent. It is key to understand what information is relevant in the current scope. LLM agents may be composed of many LLM calls with different purposes and contexts; not all information from long-term memory will apply to every LLM call. For example, if the user from Figure 2 decides to plan meals once a week (per Figure 2.15), that would be a valuable long-term memory for Figure 2.Pe1 but not necessarily 2.Pe2, which is mainly used for its culinary expertise. In such instances, the relevance afforded by retrieval from a vector database is valuable [51, 86, 106]. Once relevant information has been retrieved from long-term memory, it can be shared in an LLM call via the user or system prompt."}, {"title": "7 Tools", "content": ""}, {"title": "7.1 Using Tools", "content": "To enable the LLM to use tools, tool descriptions and methods of invocation need to be exposed with the LLM (similar to traditional software engineering documentation). If the number of tools in use is small, they can be introduced in natural language. The method for invoking a tool should be clear and easily parsable. A common way to do this is by defining JSON schemas or function signatures, although the latter has been shown to be better for LLM agents [68, 89].\nTools can be called either explicitly or implicitly, with the former being the de facto approach in practice. Explicit usage simply entails the invocation of a tool as part of the LLM agent's output [67, 70]. Once the tools are defined and passed as context, the agent will have the means to perform such an invocation in the specified parsable format. Tools can also be implicitly invoked by the implementor in response to an LLM agent's action or inaction. For example, if a transition between personas occurs, it may be the case that the system will always benefit from a summarization of preceding dialogue. Rather than rely on the LLM agent to invoke a summarization at every persona change, every such transition can trigger a summarization behind the scenes. See Section 9.3 for a discussion on incorporating implicit tool calling."}, {"title": "7.2 Managing Multiplicity", "content": "As the number of tools grows, defining tools in natural language quickly becomes unwieldy and a structured approach is necessary. To do so, we can leverage LLMs' convenient understanding of code by creating more concise tool definitions using JSON schemas or function signatures in conjunction with condensed natural language descriptions5.\nOften, distinct tools can be placed into distinct groups based on similar core functionality (i.e., if they can reasonably be seen as inheriting from the same base class). These groups can be called \"toolsets\" or \"toolkits\" and are helpful for determining if tools can be combined behind a single interface or introduced together in the prompt. For example, the tools Figure 2.T1 and 2.T2 introduced in the example would not belong in the same toolset but 2.T2 and 2.T3 would."}, {"title": "7.3 Adding Tools Dynamically", "content": "Sometimes the tools that are available in the environment in which an agentic LLM system will be deployed are not known beforehand. In this case, we can add \"tool identification\" as a task for the system [70, 86]. A compelling example and implementation of this can be observed in the Voyager paper, where an LLM-based agent autonomously traverses the world of Minecraft and dynamically assembles a set of tools based on interactions with the environment, which are then stored in long-term memory [86]."}, {"title": "8 Control Flow", "content": "In the context of LLM agents, control flow refers to the ability to determine what needs to be done in order to respond to a query. Tasking an LLM with control flow is what enables LLM-based agents to accomplish complex tasks that elude the capacity of a single inference. This endows the LLM agent with the autonomy to incorporate advanced techniques such as planning, tool usage, and multi-step reasoning as it sees fit [72, 87].\nIn practice, this may look like the LLM agent receiving user input (i.e., observing the environment) and selecting the immediate next action. The agent continues to take actions until it decides to stop. For this to be possible, the LLM agent needs to be aware of the action space [102], such as the stopping criteria, available tools (e.g., Figure 2.T1, 2.T2, and 2.T3), available planning options (2.P1), the ability to take a turn to think out loud [102], and utilizing other personas (e.g., 2.Pe2).\nConsider an LLM agent receiving Figure 2.11. Rather than simply providing an output, the agent can opt to leverage 2.P1, the planning module, to decompose the complex task and generate a multi-step plan that it can then administer. Once the plan is complete, the agent can decide if it has enough information to provide the final output to 2.11 or if it needs to take additional actions.\nHere, we present practical considerations for ensuring the LLM agent can interact with its environment smoothly and without interruption."}, {"title": "8.1 Output Processing", "content": "When chaining together multiple LLM inputs and outputs, it is often advantageous to process the text before handing it off to the next step. Although natural language is human-readable, it is advisable to use a more structured format (such as JSON or executable code) that is easily parsable [89]. While weaker models may struggle with instruction following, most commercial models have been optimized to adhere to desired output formats specified in the user or system prompt8."}, {"title": "8.2 Error Handling", "content": "Error handling is one of the most important yet elusive parts of building a robust agentic LLM system. Because LLMs are inherently stochastic, chaining several LLM calls together compounds the risk of failure to the point of near inevitability for long sequences. As such, every LLM call in an agentic LLM system should be treated as a potential point of failure and supported by appropriate error handling."}, {"title": "8.2.1 Static Retry", "content": "The simplest approach to handling a problematic output is to retry the LLM call with the same prompt. While other hyperparameters may stay the same, the seed should always change between static retries to avoid completely duplicate calls. If using a low temperature or a high top-p, then it may also make sense to adjust those values appropriately so as to receive a different output. In the context of Figure 3, this might look like 3.UI simply being rerun with a different seed.\nFor low-context calls that yield output that is easily verifiable (e.g., parsing the output into a JSON object), it is a simple yet valuable addition to attempt a few static retries in case verification fails. For outputs that are more difficult to verify, such as natural language instructions that are interpreted downstream, static retries are less helpful as the cost of verification increases."}, {"title": "8.2.2 Informed Retry", "content": "A more informed approach is to append the LLM's output to the history, add another user message indicating that the output was unsuccessful, and try again. This should be supplemented with specific error messages or additional directions [39, 81]. In the Figure 3 example, an informed retry might look like sending the following list of messages: 3.UI, 3.AO, 3.TR, and \"Attempting the above code yielded the provided error. Please provide an updated output that achieves the initial instruction.\"."}, {"title": "8.2.3 External Retry", "content": "Rather than asking for an informed retry from the same context, we can pull out pieces of the history and provide it to an LLM in a separate context to either fix the previous output or generate a new one. This will likely require significant context from the original call but can be supplemented and differentiated by using a different role, different instructions, and error information. Often, shifting roles from, for example, a software engineer to a code reviewer can provide the impetus the LLM needs to fix or generate the correct output. While it has been shown that the explanations from external LLM-based error systems are frequently unreliable and sensitive to prompt changes [39], having access to task-specific roles, detailed error information (e.g., the error raised by a piece of generated Python code), and background context helps mitigate those issues [81]."}, {"title": "8.3 Stopping", "content": "As the control flow of the agentic LLM system is controlled by an LLM, a clear stopping method needs to be defined. This will likely take the form of a predetermined stop token or phrase inserted into the system prompt, such as \"TERMINATE\" [95]. It should be a token or phrase that is easily parsable and not otherwise likely, to avoid accidental stopping."}, {"title": "8.4 Multiple Personas", "content": "Often, the role that an LLM is assigned has a significant impact on its performance on a given task. This has been observed in LLM literature generally, becoming a key ingredient of effective prompting [41, 43], and in recent LLM multi-agent research, emerging as a necessary component for agent multiplicity in many such architectures [32, 35, 50, 62, 92, 95]. For example, while the Figure 2.Pe1 role is good for answering most of the user's queries, the Figure 2.Pe2 role may be better at answering Figure 2.14 because it requires specialist culinary knowledge.\nBecause there are likely to be many distinct tasks that form part of an agentic LLM system, there is usually room for multiple roles to be used. An overview of approaches to defining personas for LLMs, or \"profiling\" them, is detailed in [87], categorizing them as handcrafted (e.g., [62, 64]), LLM-generated (e.g, [92, 99]), or dataset-aligned (i.e., derived from a pertinent dataset). The roles should be informed by the task that the call is handling. This is dependent on the overall context of the agentic LLM system but can largely be addressed in the following ways:\n\u2022 If the tasks are well-defined, handcraft specialist roles for each task (e.g., Figure 2.Pe1 and 2.Pe2).\n\u2022 If the tasks are not well-defined but generally correspond to a single topic, use the most specific handcrafted role for that topic (e.g., the catch-all Figure 2.Pe1).\n\u2022 If the tasks are truly undefined to start (e.g., an assistant that helps with anything) or the topic is very broad:\nDefine several distinct roles to which the LLM agent can route subsequent calls as it sees fit [75]. Once the agent is in use, a more informed set of personas can be defined according to the most frequently ones. This may also be thought of as the dataset alignment approach [87], where the dataset is constructed in the environment under an interim set of personas.\nLeverage an LLM to create the role that it deems would be best able to respond to the prompt [92, 99]. This is more expensive as generating the role requires LLM usage but is certainly more robust to unforeseen scenarios. This approach may be used in conjunction with the above point (e.g., if no suitable predefined role is found, create one)."}, {"title": "8.5 Managing Relevant Context", "content": "Managing the context that is sent to an LLM is an effective method of increasing the efficiency (speed and cost) and performance of an LLM system, as inference time is dependent on the number of input tokens [63, 85] and LLMs perform worse in long-context scenarios, particularly for complex tasks [48, 53]. Additionally, careful context management is a necessity given that LLMs have limited context windows. Even for \"long-\"context LLMs (>100k token limit), many tasks quickly become unwieldy if not properly managed (e.g., working with HTML, where single webpages can be hundreds of thousands of tokens). This is a key consideration to make during task decomposition; the more specific the task, the more extraneous context (e.g., prior messages) can be trimmed [65]. As such, the context that a specific LLM call receives should be tailored to the task as much as possible. Even if an LLM call requires past messages, it is often possible to strip out certain pieces of context or summarize them, leaving the parts the subsequent call relies on intact and maintaining the overall meaning. Significant adjustments can be made to the context between calls to decrease the overall token count and remove extraneous context, thus reducing LLM confusion and increasing performance for the LLM call [65]."}, {"title": "9 Additional Considerations", "content": ""}, {"title": "9.1 Model Size", "content": "The size of the model to use is typically driven by three concerns: cost, speed, and performance. Usually, the bigger the model, the higher the cost, the lower the speed, and the better the performance (although this is not a hard-and-fast rule). It can be tempting to build an agentic LLM system around the weakest model that will adequately do the job so that all three conditions are optimized from the start. However, attempting to build out a functional system from a smaller model first will likely be more time consuming and expensive than starting at the strongest model possible and downgrading the models used for specific calls once the LLM agent has demonstrated competence in the environment. Due to the influence one call can have on subsequent ones, it is infeasible to understand what is possible for a given use case if not all the pieces are working optimally. By starting with stronger models, there will be a gold-standard baseline to compare against so the performance impact of downgrading a model for a specific call can be measured10. It is recommended that the correct model is selected on a per-task basis and evaluated both individually and in the context of the entire agentic LLM system."}, {"title": "9.2 Evaluation", "content": "Evaluating an agentic LLM system can be challenging due to the potential for long sequences, non-determinism in LLMs, interactions with external entities, and tasks that may not have obviously correct solutions. Nonetheless, it is essential to have an approach to evaluation defined before deployment to (1) have a baseline to compare against and (2) measure performance changes over time and in response to changes.\nWhen creating a dataset for evaluating an LLM agent, the most important consideration is that it accurately resembles the environment in which is will be deployed. There are many LLM agent benchmarks available targeting specific domains [26, 54, 101, 103] as well as general purpose application [58, 78, 96], but many agentic LLM systems applied to a specific task will be too niche to benefit from a broader benchmark. However, insomuch as an established benchmark fits the application of the LLM system, it can be a strong starting point for evaluation and refinement. Whether an existing benchmark is used or not, it is advisable to collect informative agent interactions (e.g., long sequences, short sequences, incorrect outputs, correct outputs, etc.) and related metadata (e.g., hyperparameters) in the deployment environment. Doing so will allow the creation of a dataset, comprised of reproducible input and output pairs, that is derived from the environment. Even a dataset with a few samples will provide a baseline to compare against to ensure prompt engineering addresses failed executions, identify the effects of model and prompt changes, and avoid regression in the system11.\nWhile traditional metrics (e.g., precision, recall, etc.) are useful to track, metrics specific to the agent can help reveal changes in the system that higher-level metrics fail to reflect [24, 40]. For example, an LLM agent that arrives at the same answer when presented with two different prompts is superficially consistent but a difference in the number of intermediate steps to reach that conclusion may indicate that the system is overly sensitive to prompt changes. Building from [40, 52, 57] that suggest types of alternative evaluation, we provide sample metrics below to use as a starting place, although useful metrics should be chosen in accordance with the design of the LLM agent and the environment in which it is implemented12."}, {"title": "9.2.1 Holistic", "content": "No matter how well an agentic LLM system might do along the way or what emergent capabilities it might demonstrate, the final output will determine whether the system is accomplishing its task or not. It is impossible to tell how a composition of LLM calls will perform without running them end-to-end; thus, evaluating an LLM agent should primarily rely on holistic metrics to determine if it is performing as expected."}, {"title": "9.3 Integration with Traditional Engineering", "content": "Because LLMs are inherently stochastic, it is often easier to offload as much of the agent's responsibility onto traditional engineering as possible. This allows outsourcing parts of the system that require determinism to methods that can be deterministic. By crafting an LLM agent according to software engineering best practices, we can ensure that key components that are necessary for a given task are always completed or included, rather than relying on the agent to make a request or execute an action. This can take the form of automatically managing context between calls, output processing, combining tools into toolsets (e.g., putting Figure 2.T2 and 2.T3 behind a \"delivery\" interface), incorporating information from long-term memory permanently into the prompts (e.g., Figure 2.E1), setting callbacks on certain transitions and calls (e.g., to generate a summary of the most recent conversation to use as context when transitioning from Figure 2.Pe1 to 2.Pe2), and adding an evaluation after each step of a plan (see Plan Adherence). (The last two can be thought of as implicit tool usage; see Section 7.1). However, care should be taken not to limit the autonomy of the agent in doing so. One way to return autonomy to the agent while still leveraging the benefits of traditional engineering is to allow the agent to short-circuit."}, {"title": "9.3.1 Short-Circuiting", "content": "Short-circuiting (from the world of software engineering: the idea of evaluating an expression only so far as to guarantee a single answer) is an integral technique for agentic LLM systems. This can be as simple as including stopping criteria into the LLM agent's instructions (see Section 8.3 for examples) or allowing the LLM agent to produce a final output in a single turn. If an agentic LLM system does not short-circuit when it obviously should, the system may have an overreliance on external engineering (i.e., the flow (or parts of the flow) of the agent being hard-coded)13.\nAs an example, the query presented in Figure 2.13 demonstrates an instance when an LLM agent may want to short-circuit. The query poses a simple question-answering scenario that most current models could satisfactorily respond to. Allowing the agent the autonomy to determine what step to take next (as opposed to, for example, implicitly calling Figure 2.P1 for every input) would permit"}]}