{"title": "Large Language Models Can Self-Improve in Long-context Reasoning", "authors": ["Siheng Li", "Cheng Yang", "Zesen Cheng", "Lemao Liu", "Mo Yu", "Yujiu Yang", "Wai Lam"], "abstract": "Large language models (LLMs) have achieved substantial progress in processing long contexts but still struggle with long-context reasoning. Existing approaches typically involve fine-tuning LLMs with synthetic data, which depends on annotations from human experts or advanced models like GPT-4, thus restricting further advancements. To address this issue, we investigate the potential for LLMs to self-improve in long-context reasoning and propose SEALONG, an approach specifically designed for this purpose. This approach is straightforward: we sample multiple outputs for each question, score them with Minimum Bayes Risk, and then apply supervised fine-tuning or preference optimization based on these outputs. Extensive experiments on several leading LLMs demonstrate the effectiveness of SEALONG, with an absolute improvement of 4.2 points for Llama-3.1-8B-Instruct. Furthermore, SEALONG achieves superior performance compared to prior approaches that depend on data produced by human experts or advanced models. We anticipate that this work will open new avenues for self-improvement techniques in long-context scenarios, which are essential for the continual advancement of LLMs.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) with long-context processing capabilities have spurred a range of novel applications, such as repository-level coding assistance (Jimenez et al., 2024), multi-document analysis (Wang et al., 2024b) and autonomous agents (Ma et al., 2024). Delivering high-quality service in these domains requires LLMs to reason effectively over long contexts, necessitating the retrieval of essential details and integration of dispersed information throughout the reasoning process. While recent advancements have enabled LLMs to attain near-perfect accuracy on the needle-in-a-haystack (NIAH; Kamradt (2023); Li et al. (2024b)) task (Hsieh et al., 2024; Yen et al., 2024b; Dubey et al., 2024), which involves locating evidence within vast amounts of irrelevant text, substantial performance declines persist on tasks that require reasoning over long contexts (Levy et al., 2024; Hsieh et al., 2024; Vodrahalli et al., 2024; Yen et al., 2024b; Li et al., 2024a), limiting their applicability in real-word scenarios.\nTo address this limitation, recent studies have investigated fine-tuning LLMs to improve long-context reasoning, with effective data synthesis as a primary challenge. Two main approaches have emerged: one relies on human annotations (Chen et al., 2024b; Li et al., 2024c,a), which are expensive and difficult to scale; the other leverages expert models, such as GPT-40 (Hurst et al., 2024), for data synthesis. For example, Bai et al. (2024); Zhang et al. (2024c,b) apply self-instruct techniques (Wang et al., 2023b) to create long-context instruction-following data. Despite substantial progress, the dependence on pre-existing expert models limits the potential for achieving more advanced capabilities.\nThis work investigates whether LLMs can self-improve in long-context reasoning. Drawing on evidence of LLMs' near-perfect long-context retrieval and strong reasoning abilities in general domains (Bubeck et al., 2023; Zhong et al., 2024), we hypothesize that LLMs have untapped potential in long-context reasoning. Our preliminary studies show that refined prompting strategies achieve notable improvements over both default prompting methods and direct answer requests. Furthermore, scaling the number of sampled outputs reveals a marked performance gap between the optimal outputs and those derived via greedy search. These results suggest that LLMs hold substantial potential to advance in long-context reasoning.\nInspired by these observations, we propose a"}, {"title": "2 Understanding the Potential of LLMs in Long-context Reasoning", "content": "We explore the potential of LLMs in long-context reasoning through experiments on three reasoning-intensive tasks from LongBench (Bai et al., 2023): HotpotQA (Yang et al., 2018), MuSiQue (Trivedi et al., 2022) and 2WikiMQA (Ho et al., 2020). These tasks involve handling multiple documents within the context and addressing multi-hop questions that span several paragraphs. Following previous work (Mallen et al., 2023; Asai et al., 2024; Yen et al., 2024b), we use substring exact match (SubEM) for evaluation, assessing whether the golden answer is included in the output."}, {"title": "2.1 Prompting Strategies Matter", "content": "Numerous long-context evaluation benchmarks assess LLMs by simply asking them to respond to a query based on a long context (Bai et al., 2023; An et al., 2024a; Zhang et al., 2024d; Wang et al., 2024b; Yen et al., 2024b). We suggest that this approach may underestimate LLMs' potential in long-context scenarios, particularly for questions requiring complex, multi-step reasoning to arrive at an answer. To further investigate this, we examine various prompting strategies for long-context reasoning, including:\n\u2022 Default: Prompting the LLM with the long context and a question.\n\u2022 Direct Answer: Asking the LLM to directly answer the question based on the long context.\n\u2022 Think Step-by-step: Providing the LLM with the context, question, and an instruction to think step-by-step (Kojima et al., 2022).\n\u2022 Fact-and-reflection: Providing the LLM with the long context, question, and an instruction to first identify the relevant information from the long context, and then carry out step-by-step reasoning and provide the answer (Zhao et al., 2024b; Li et al., 2024a).\n\u2022 Plan-and-solve: Providing the LLM with the long context, question, and an instruction to first devise a plan and then follow it to solve the problem step-by-step (Wang et al., 2023a).\nThe detailed prompts for these strategies are presented in Tab. 9 (Appx. B). As shown in Tab. 1, prompting strategies play a crucial role in long-context reasoning. A notable performance gap"}, {"title": "2.2 The Potential of LLMs for Correct Long-context Reasoning", "content": "We further investigate the potential of LLMs for long-context reasoning by expanding the generation space. Specifically, we use temperature sampling to produce multiple outputs per question, evaluate each with SubEM, and designate the highest-scoring output as the oracle sample. As shown in Fig. 1, there is a notable gap between oracle performance and that of greedy search, even with just 8 outputs. Scaling up to 128 samples achieves over 90% correct answers. These results underscore the potential of LLMs for long-context reasoning and motivate the development of methods that enable LLMs to self-improve in this area."}, {"title": "3 SEALONG", "content": "Motivated by the potential of LLMs in long-context reasoning (\u00a72), we propose SEALONG, a self-improving method for reasoning over long contexts. This approach consists of two stages: creating self-supervision and fine-tuning the model. An overview of SEALONG is provided in Fig. 2."}, {"title": "3.1 Self-supervision", "content": "We begin by leveraging plan-and-solve prompting (Wang et al., 2023a) to sample multiple reasoning trajectories for each question and its corresponding long context. The primary challenge lies in evaluating these outputs. The fundamental idea behind SEALONG is that correct reasoning trajectories typically exhibit higher semantic consistency. For example, they tend to follow similar planning steps and reference the same information within the long context. This observation aligns with hallucination detection methods (Manakul et al., 2023; Farquhar et al., 2024), where less consistent outputs are more likely to indicate hallucinations, representing incorrect reasoning in our scenario.\nWe formalize this idea using Minimum Bayes Risk (MBR) (Bickel and Doksum, 1977; Bertsch et al., 2023; Wu et al., 2024), which prioritizes outputs that exhibit higher consistency with others. In the MBR literature, the quality of an output is assessed by its expected utility under the model distribution (Bickel and Doksum, 1977; Kumar and Byrne, 2004; Tromble et al., 2008):\n$s(y) = E_{y^*\\sim\\pi_\\theta(y/x)} [U(Y, Y^*)]$\nHere, $s(y)$ is the score assigned to output y, where x denotes the input, including the long context, question and instruction. The term $\\pi_\\theta(y | x)$ represents the policy distribution of the LLM and the utility metric $u(y, y^*)$ assesses y based on $y^*$. We approximate this expectation using the Monte Carlo method with N sampled outputs:\n$s(y) \\approx \\frac{1}{N} \\sum_{i=1}^{N} [u(y, y^*)]$\nThe utility metric measures the consistency between two outputs. We use sentence embedding similarity as this metric, as it effectively captures"}, {"title": "3.2 Fine-tuning", "content": "Leveraging self-provided supervision, we can either perform supervised fine-tuning on the highest-scoring outputs or apply preference optimization using preference pairs.\nSupervised Fine-tuning. For supervised fine-tuning (SFT), we minimize the negative log-likelihood of the output as follows:\n$L_{SFT} = - \\frac{1}{\\|y\\|} log \\pi_\\theta(y | x) = -\\frac{1}{\\|y\\|} \\sum_{i=1}^{\\|y\\|}log \\pi_\\theta (y_i | x, y_{<i})$\nHere, y denotes the MBR decoding output.\nPreference Optimization. Alternatively, we can conduct preference optimization to reinforce the tendency toward high-scoring outputs and reduce the likelihood of low-scoring outputs. Among the various preference optimization methods, we adopt the monolithic odds ratio preference optimization (ORPO) algorithm (Hong et al., 2024) due to its strong empirical performance. ORPO introduces an odds ratio loss to minimize the negative log odds ratio between a preferred output $y_w$ and a less-preferred output $y_l$:\n$L_{OR} = - log \\sigma (log \\frac{odds_\\theta(y_w|x)}{odds_\\theta(y_l|x)})$\nHere, $\\sigma$ represents the sigmoid function, and $odds_\\theta(y|x)$ measures how much more likely y is to be generated than not:\n$odds_\\theta(y|x) = \\frac{\\pi_\\theta(y|x)}{1 - \\pi_\\theta(y|x)}$\nThe final objective in ORPO combines SFT and OR losses, with a hyperparameter $\\beta$ controlling their relative importance:\n$L_{ORPO} = L_{SFT} + \\beta \\cdot L_{OR}$"}, {"title": "4 Experiments", "content": "4.1 Implementation\nSEALONG requires query and long-context pairs to synthesize training data. Specifically, we leverage the training dataset of MuSiQue (Trivedi et al., 2022), where each question is related to several Wikipedia documents. To achieve a specified number of tokens in the context, we randomly sample some unrelated documents, shuffle them with the related ones and concatenate them into a single context. We use the original questions in MuSiQue without the annotated answer, relying on the LLM to produce self-supervision (\u00a73.1). For each question, we sample N = 32 outputs with a sampling temperature of 0.7. By default, we synthesize 2048 examples for fine-tuning, with context lengths randomly specified between 4K and 31K tokens. We conduct experiments using the Llama-3.1 models (Dubey et al., 2024) and Qwen-2.5 models (Yang et al., 2024a), with jina-embeddings-v3 serving as the sentence embedding model (Sturua et al., 2024). ORPO (Hong et al., 2024) is employed as the default fine-tuning method. More training details can be found in Appx. A.\n4.2 Evaluation Setup\nWe conduct evaluations in long-context scenarios across a wide range of tasks. For single-document QA, we include Qasper (Dasigi et al., 2021) and MultiFieldQA-En (Bai et al., 2023) from the LongBench benchmark (Bai et al., 2023). For multi-document QA, we use HotpotQA (Yang et al., 2018), MuSiQue (Trivedi et al., 2022) and 2Wiki-MultihopQA (Ho et al., 2020), also from LongBench. Task statistics are presented in Tab. 3. We adopt plan-and-solve prompting for evaluation due to its strong performance (Tab. 1). Following previous research (Mallen et al., 2023; Asai et al., 2024; Yen et al., 2024b), we use substring exact match (SubEM) as the evaluation metric, measuring whether the output contains the golden answer.\n4.3 Main Results\nSEALONG Improves Various Models. We implement SEALONG on the leading open-source LLMs, including Qwen-2.5 models (Yang et al., 2024a) and Llama-3.1 models (Dubey et al., 2024). As illustrated in Tab. 2, SEALONG brings notable improvements: when implemented on Qwen-2.5-7B-Instruct, it closes the performance gap with"}, {"title": "4.4 Analysis", "content": "Scoring Methods. Effective scoring methods are critical for creating self-supervision signals (\u00a73.1). We explore several approaches, including random selection, and reference-free self-evaluation, which prompts the LLM to assess its prediction in a separate turn based on the question and context. Additionally, we investigate various strategies for the utility metric $u(y, y^*)$ within Minimum Bayes Risk (\u00a73.1), such as ROUGE (Lin, 2004), BERTScore (Zhang et al., 2019) and reference-based self-evaluation, which prompts the LLM to assess $y$ using $y^*$ as the reference. The detailed prompts for reference-free and reference-based self-evaluation are presented in Tab. 10 (Appx. B). For each question, we sample N = 16 outputs using a temperature of 0.7. Subsequently, we evaluate the highest-scoring output across different scoring methods and further compare these results to the performance of greedy search as a reference. As shown in Tab. 7, MBR-based methods outperform reference-free self-evaluation, even with simple N-gram-based ROUGE. We attribute this to the limited self-evaluation capabilities of current LLMs (Huang et al., 2024; Jiang et al., 2024), which might be more challenging in long-context scenarios. Incorporating more semantic information through sentence embeddings further improves MBR-based methods.\nNumber of Synthetic Examples. We analyze the impact of the number of training examples synthesized by SEALONG on long-context tasks. As shown in Fig. 3, SEALONG demonstrates strong data efficiency, achieving competitive performance with only 1K examples, after which additional examples provide limited benefit. This suggests that SEALONG is unlocking the inherent potential of LLMs for long-context reasoning rather than introducing a new skill that would require more data.\nNumber of Samples per Example. We continue to explore the effect of the number of samples, N, per example. As illustrated in Fig. 4, increasing N from 8 to 32 consistently improves performance, likely due to more accurate MBR estimation (\u00a73.1). Beyond 32, except for MuSiQue, the performance improvement diminishes. This may indicate a fundamental limitation of our scoring method, which appears to struggle with selecting higher-quality outputs from larger output sets when N > 32 (see also in Tab. 1). We believe the scoring method is pivotal to self-improvement and will investigate this aspect further in future work.\nShort-context Performance. Improving long-context reasoning should not compromise short-context performance. To investigate this further, we evaluate SEALONG on the Open LLM Leaderboard (Beeching et al., 2023), covering 6 tasks that represent diverse capabilities: MMLU (Hendrycks et al., 2021), GSM8K (Cobbe et al., 2021), ARC-challenge (Clark et al., 2018), HellaSwag (Zellers et al., 2019), WinoGrande (Sakaguchi et al., 2021) and TruthfulQA (Lin et al., 2022). As shown in Tab."}, {"title": "5 Related Work", "content": "Long-context Language Modeling. Numerous studies explore methods to extend the long-context processing abilities of LLMs. One line of research approaches addresses this challenge from a model-centered perspective, with some studies focusing on minimal modifications to existing LLMs, such as adjustments to position embeddings (Chen et al., 2023; Peng et al., 2024; Ding et al., 2024; Zhu et al., 2024; Xiong et al., 2024) and refinements to the attention mechanism (Ding et al., 2023; Jin et al., 2024; An et al., 2024b,c). Additionally, some works propose novel architectures for efficient long-context processing (Wu et al., 2022; Bertsch et al., 2024; Wang et al., 2024d; Yen et al., 2024a; Lieber et al., 2024; Ye et al., 2024; Sun et al., 2024). Another line of research adopts a data-centric perspective, focusing on data engineering strategies. For example, Dubey et al. (2024); Lieber et al. (2024); Fu et al. (2024); Gao et al. (2024) continue pre-training models on long sequences, while An et al. (2024d); Bai et al. (2024); Zhang et al. (2024b); Chen et al. (2024c,b) leverage expert models or human annotations to create long-context data for fine-tuning. In contrast to these approaches, this work aims to facilitate the self-improvement of LLMs in long-context reasoning.\nSelf-improving. The self-improvement of LLMs has become a vital area of research as these models advance toward human intelligence. Research in this area follows two main approaches. The first approach investigates the self-reflection capabilities of LLMs, where models are prompted to assess and refine their own outputs (Ganguli et al., 2023; Madaan et al., 2024; Shinn et al., 2024; Xie et al., 2024; Gou et al., 2024; Chen et al., 2024a; Pan"}, {"title": "6 Conclusion", "content": "In this study, we investigate the potential of LLMs to self-improve in long-context reasoning and propose SEALONG for this purpose. This method achieves substantial improvements across multiple long-context reasoning tasks. We hope this research will open new avenues for self-improvement in long-context reasoning, which is vital for the sustained progress of LLMs, particularly as they advance toward surpassing human intelligence."}, {"title": "Limitations", "content": "We recognize that this work has several limitations that warrant further investigation.\nScoring Method. To establish self-supervision (\u00a73.1), we score each output according to Minimum Bayesian Risk (MBR), which reflects consensus across multiple sampled outputs. However, a substantial performance gap remains between the highest MBR-scored output and the oracle sample (see Tab. 1 for details). Future research should explore more effective approaches for self-evaluation of outputs. One possible direction could involve examining the critic capabilities of LLMs in long-context scenarios (Lan et al., 2024b; Lin et al., 2024; Lan et al., 2024a).\nSynthetic Data. Another limitation of this work is its reliance on MuSiQue (Trivedi et al., 2022) for synthetic data, which consists of multi-hop questions spanning multiple paragraphs. While this approach has enabled some progress, MuSiQue dose not cover all challenging question types, such as those requiring full-context reasoning, which remains a key limitation of current long-context LLMs (Karpinska et al., 2024; Wang et al., 2024b; Vodrahalli et al., 2024; Yen et al., 2024b). We advocate for future work to prioritize the creation of high-quality prompt sets, which are essential for the development of long-context LLMs.\nExperimental Setup. Due to the computational limitations, we restrict the implementation of SEALONG to LLMs with up to 14B parameters, though its effectiveness at larger scales warrants further investigation. Likewise, the maximum sequence length is set to 32K tokens, whereas current leading LLMs support context lengths of up to 128K tokens or more. We leave the exploration of longer context lengths for future work."}, {"title": "A Training Details", "content": "To support efficient fine-tuning for long-context scenarios, we implement sequence parallelization (Jacobs et al., 2023) with a parallel size of 8. Additionally, we utilize QLoRA (Dettmers et al., 2024) to reduce memory consumption during fine-tuning. The LoRA rank, alpha, and dropout are set to 128, 128, and 0.05, respectively, with all attention and feedforward linear layers designated as target modules. All models are fine-tuned for one epoch. The batch size, learning rate, and maximum sequence length are set to 8, 5e \u2013 5, and 32K, respectively. The 8 for ORPO is configured to 0.1. All experiments are conducted on a computing setup with 8 \u00d7 H100 GPUs."}, {"title": "B Prompts", "content": "We provide the prompts for various prompting strategies (\u00a72.1) in Tab. 9, and the prompts for the reference-free and reference-based self-evaluation strategies (\u00a74.4) in Tab. 10."}]}