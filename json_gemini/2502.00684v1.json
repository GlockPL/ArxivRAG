{"title": "Compositional Concept-Based Neuron-Level Interpretability for Deep Reinforcement Learning", "authors": ["Zeyu Jiang", "Hai Huang", "Xingquan Zuo"], "abstract": "Deep reinforcement learning (DRL), through learning policies or values represented by neural networks, has successfully addressed many complex control problems. However, the neural networks introduced by DRL lack interpretability and transparency. Current DRL interpretability methods largely treat neural networks as black boxes, with few approaches delving into the internal mechanisms of policy/value networks. This limitation undermines trust in both the neural network models that represent policies and the explanations derived from them. In this work, we propose a novel concept-based interpretability method that provides fine-grained explanations of DRL models at the neuron level. Our method formalizes atomic concepts as binary functions over the state space and constructs complex concepts through logical operations. By analyzing the correspondence between neuron activations and concept functions, we establish interpretable explanations for individual neurons in policy/value networks. Experimental results on both continuous control tasks and discrete decision-making environments demonstrate that our method can effectively identify meaningful concepts that align with human understanding while faithfully reflecting the network's decision-making logic.", "sections": [{"title": "1 Introduction", "content": "Deep reinforcement learning (DRL) has achieved remarkable success in solving complex sequential decision-making problems through trial-and-error learning. From game playing to robotic control, DRL has demonstrated strong capabilities across various domains. However, the increasing complexity of DRL models, particularly their neural network architectures, has created a significant interpretability challenge that hinders their deployment in high-stakes applications such as healthcare, autonomous driving, and financial trading.\nExisting approaches to DRL agent model interpretability primarily focus on post-hoc explanations [Vouros, 2022]. These include applying classic neural network interpretation methods like SHAP [Rizzo et al., 2019] and attention mechanisms [Nikulin et al., 2019] to attribute importance to input features, as well as explaining agent decisions through causal chains [Yu et al., 2023]. While these methods provide valuable insights into state importance and action selection, they treat neural networks as black boxes and only explain the relationships between states and actions, without revealing how individual neurons contribute to the decision-making process.\nTo achieve more fine-grained interpretability at the neuron level, researchers have developed concept-based interpretation methods that match individual neurons with human-understandable concepts [Bau et al., 2017; Cunningham et al., 2023; Mu and Andreas, 2020]. By analyzing how neurons respond to different inputs, these methods can identify which neurons encode specific semantic concepts, providing deeper insights into the network's internal representations. These neuron-level interpretation methods have shown promising results in computer vision and natural language processing, revealing how individual neurons learn to detect interpretable patterns. However, applying such neuron-level concept-based interpretations to reinforcement learning remains largely unexplored. The key challenge in extending concept-based interpretations to RL lies in defining state-space concepts and establishing meaningful correspondences between these concepts and neural activations.\nTo address this challenge, we propose a novel concept-based interpretation method for DRL that operates at the neuron level. As illustrated in Figure 1, we first define atomic concepts as binary functions over states and construct concept vectors by applying these functions to state sequences. Next, we record neuron activations from the policy/value network. Then, we compose concept outputs and align them with neuron activations. Finally, we interpret individual neurons through compositional concepts.\nThe main contributions of this work are as follows:\n1. We propose a concept-based neuron-level interpretation method for DRL networks, enabling fine-grained understanding of how individual neurons contribute to policy decisions.\n2. We demonstrate through extensive experiments on both discrete (Blackjack-v1, LunarLander-v3) and continuous (LunarLander-Continuous-v2) environments how our method reveals interpretable decision-making pat-"}, {"title": "3 Methodology", "content": ""}, {"title": "3.1 Concept Formalization", "content": "To bridge the gap between value or policy network representations and human-interpretable knowledge, we first formalize the notion of concepts in reinforcement learning. Following the concept-based interpretability framework [Bau et al., 2017; Mu and Andreas, 2020], we adopt a binary function representation of concepts. This formalization aims to capture meaningful patterns in the environment that can be mapped to neural activations while maintaining human interpretability.\nLet $S \\subseteq \\mathbb{R}^n$ be the state space of the reinforcement learning task, where each state $s \\in S$ represents the complete observation of the environment at a given time step.\nFormally, we define an atomic concept as a binary function $C: S \\rightarrow \\{0,1\\}$, where $C(s) = 1$ indicates the presence of the concept in state $s$, and $C(s) = 0$ indicates its absence.\nCompositional concepts can be constructed from atomic concepts through logical operations. Let $\\mathcal{C}$ be the set of all possible concepts. For any concepts $C_1, C_2 \\in \\mathcal{C}$, we define:\n\u2022 Conjunction: $(C_1 \\wedge C_2)(s) = \\min(C_1(s), C_2(s))$\n\u2022 Disjunction: $(C_1 \\vee C_2)(s) = \\max(C_1(s), C_2(s))$\n\u2022 Negation: $(\\neg C_1)(s) = 1 - C_1(s)$\nA compositional concept of length k can be represented as: $C_k = op_1(op_2(...op_{k-1}(C_1, C_2), ..., C_k))$, where $C_i$ are atomic concepts or their negations, and each $op_i \\in \\{\\wedge, \\vee\\}$.\nThe key properties of our concept formalization include:\n\u2022 Interpretability: Each atomic concept corresponds to a human-understandable natural language statement of the environment\n\u2022 Compositionality: Compositional concepts can be built from simpler ones through logical operations\n\u2022 Binary nature: The binary output enables clear concept presence detection and facilitates comparison with neuron activations\nThis formalization provides the foundation for interpreting neural network representations through concept matching. In the following sections, we describe our approach to finding the best-matching concepts for neural activations through a systematic search process, effectively establishing interpretable explanations for the network's behavior."}, {"title": "3.2 Concept Matching via Neural Activation Analysis", "content": "Given a trained DRL model with value or policy network $f$, let $h_{i,l}(s)$ denote the activation of neuron $i$ in layer $l$ for input state $s$. To establish a correspondence between continuous neuron activations and binary concepts, we first need to binarize the neuron activations.\nFollowing [Bau et al., 2017], we define a threshold function $T_{\\beta}$ that converts continuous neuron activations to binary values:\n\n$T_{\\beta}(h_{i,l}(s)) = \\begin{cases}\n1 & \\text{if } h_{i,l}(s) > \\beta \\\\\n0 & \\text{otherwise}\n\\end{cases}$\n\nwhere $\\beta$ is a threshold parameter that determines the activation significance level. For a set of input states $S = \\{s_1, ..., s_n\\}$, we can obtain binary vectors representing both neuron activations and concept outputs:\n\n$a_{i,l} = [T_{\\beta}(h_{i,l}(s_1)), ..., T_{\\beta}(h_{i,l}(s_n))]$\n\n$c = [C_k(s_1), ..., C_k(s_n)]$\n\nTo measure the similarity between the binarized neuron activation pattern and concept function outputs, we employ the Jaccard similarity coefficient, defined as:\n\n$J(a_{i,l}, c) = \\frac{|a_{i,l} \\cap c|}{|a_{i,l} \\cup c|}$\n\nGiven this similarity measure, the problem of finding the most suitable concept to explain a neuron's behavior can be formalized as an optimization problem. For each neuron $(i, l)$, we aim to find the concept $C$ from the concept space $\\mathcal{C}$ that maximizes the Jaccard similarity with the neuron's activation pattern:\n\n$C^* = \\arg \\max_{C \\in \\mathcal{C}} J(a_{i,l}, c)$\n\nwhere $C^*$ represents the optimal concept explanation for neuron $(i, l)$, and $c$ is the binary vector generated by applying concept $C$ to the input states $S$.\nTo optimize Equation 5, we need to search in a structured space of compositional expressions. This optimization problem is inherently challenging due to the vast search space. Similar to [Mu and Andreas, 2020], we adopt beam search as our optimization strategy (Algorithm 1). Specifically, we set the beam size to 10 and impose a maximum length constraint N on the formulas. A complete and detailed view of our interpretation framework is shown in Algorithm 2, which presents the overall neuron concept extraction procedure."}, {"title": "4 Experiment", "content": "In this section, we use concept matching methods to investigate the concepts involved in policy networks and value networks of deep reinforcement learning\u00b9."}, {"title": "4.1 Environment", "content": "We conduct experiments on both discrete and continuous control tasks: Blackjack-v1 (discrete action space with 2 actions), a card game where the agent needs to optimize decisions of hitting or standing to beat the dealer without going over 21; LunarLander-v3 (discrete with 4 actions), a spacecraft landing task where the agent controls the main engine and side thrusters to safely land on a designated pad; and LunarLander-Continuous-v2 (continuous with 2-dimensional action space), a variant of LunarLander with continuous control over engine power and landing trajectory. All environments are from Gymnasium [Towers et al., 2024].\nBoth the Q-network in DQN and the actor/critic networks in PPO share the same architecture: three fully-connected layers with 64 hidden units each. For discrete environments, we employ DQN with a Q-network, while for the continuous case, we use PPO. Our interpretations focus on the neurons in the second hidden layer, as this layer typically captures high-level features before the final action/value predictions. For all experiments, we set the activation threshold $\\beta = 0$ when binarizing neuron activations, treating positive activations as concept presence and negative activations as concept absence.\nWhen searching for logical formulas to interpret neurons, we limit the maximum formula length to 5 to maintain interpretability while allowing sufficient expressiveness. We analyze only neurons that activate in more than 5% of the sampled states, with 10K states sampled for both Lunar Lander and Blackjack environments."}, {"title": "4.2 Concepts for each environment", "content": "To formalize our atomic concepts, we adopt an interval-based notation where each concept is denoted by its corresponding state variable and interval range. For example, $X(a,b]$ represents states where the horizontal position $x$ is in the interval $(a, b]$, and $V_X (a,b]$ represents states where the horizontal velocity $v_x$ is in $(a, b]$. Similarly, $\\theta_{(a,b]}$ and $\\omega_{(a,b]}$ represent angle and angular velocity intervals respectively. All intervals"}, {"title": "5 Validation through Targeted Perturbations", "content": "To validate our concept-based interpretations and demonstrate their utility for model behavior manipulation, we conduct targeted perturbation experiments. Our validation is based on two key insights: First, if our interpretations accurately capture the decision-making logic, perturbing specific features within concept-matching states should predictably affect neuron activations and subsequent actions. Second, by monitoring the penultimate layer representations (before action prediction), we can identify neurons most contributive to specific actions through their connection weights and verify whether manipulating these neurons' activations leads to predictable behavioral changes.\nFor each environment, we first identify neurons highly connected to specific actions through weight analysis. We then design targeted perturbations that modify concept-relevant features while maintaining state validity, and observe changes in both neuron activation and action selection. This systematic approach allows us to verify both the accuracy of our interpretations and their potential for controlled behavior manipulation."}, {"title": "5.1 Results in LunarLander Environment", "content": "For the discrete LunarLander (DQN), we focus on validating the interpretation of Neuron 5, which was identified as a key contributor to the \"fire left engine\" action through weight analysis. This neuron's interpreted concept involves a complex combination of spatial and attitude conditions, expressed as $(\\neg X[-0.25,0] \\wedge \\neg \\theta[-1,-0.15] \\wedge (V_X[-0.4,0.2] \\vee X[0.25,0.4])) \\vee \\theta[0.15,1]$.\nFigure 3 illustrates our perturbation experiment. In the original state (x = 0.27, y = 0.26, $v_x$ = \u22120.13, $v_y$ = -0.30, $\\theta$ = 0.13, $\\omega$ = -0.04, both legs not in contact), the lander is slightly right of center with a leftward velocity. Neuron 5 shows strong activation (h(s) = 4.00), and the network selects the \"fire left engine\" action, consistent with the need to maintain horizontal position control."}, {"title": "5.2 Results in Blackjack Environment", "content": "Blackjack Environment For the Blackjack environment, we examine three representative neurons that embody distinct strategic concepts in the game. Table 2 shows how targeted perturbations affect neuron activations and subsequent action selections.\nNeuron 28 detects high sums (18-21), showing strong activation (h(s) = 2.044) with sum 20 and promoting \"stick\". When perturbed to sum 14, it deactivates (h(s') = \u22121.030) and switches to \"hit\". Similarly, Neuron 13 monitors low sums (6-10), activating (h(s) = 2.042) with sum 6 to promote \"hit\", but deactivating (h(s') = -6.891) when perturbed to sum 17. Neuron 17 tracks dealer strength, activating (h(s) = 1.069) with dealer's 9 but deactivating (h(s') = -0.921) when perturbed to 5, adjusting strategy accordingly.\nIn all cases, we observe clear activation-to-inhibition transitions when concept-relevant features are perturbed, with action changes that logically follow from the neurons' interpreted strategic roles. These results strongly support the reliability of our compositional interpretation method, demonstrating that the identified concepts genuinely capture the network's decision-making logic."}, {"title": "5.3 Discussion", "content": "Our perturbation experiments across both environments demonstrate two critical aspects of our interpretation method:\n\u2022 Interpretation Reliability: The consistent relationship between concept satisfaction, neuron activation, and action selection validates our interpretations. In both discrete (Blackjack, LunarLander-DQN) and continuous action spaces, violating a neuron's concept reliably leads to its deactivation and predictable behavioral changes.\n\u2022 Behavioral Control: By identifying neurons strongly connected to specific actions and understanding their concepts, we can systematically manipulate model behavior.\nThese results suggest that our compositional interpretation method not only reveals interpretable decision logic but also provides a practical means for understanding and controlling neural network behavior. The ability to predictably influence model decisions through concept-based perturbations further validates the reliability and utility of our interpretations."}, {"title": "6 Conclusion", "content": "In this work, we have introduced a novel concept-based neuron-level interpretation method for deep reinforcement learning models, demonstrating through experiments on LunarLander and Blackjack environments how individual neurons encode meaningful, human-interpretable concepts. The reliability of our interpretations has been rigorously validated through targeted perturbation experiments, showing consistent relationships between concept satisfaction, neuron activation patterns, and action selection across different environments and action spaces.\nWhile our current implementation requires manual design of atomic concepts, future work could focus on developing automated methods for concept identification and exploring applications in network pruning and robustness enhancement."}]}