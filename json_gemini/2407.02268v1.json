{"title": "Footprints of Data in a Classifier Model: The Privacy Issues and Their Mitigation through Data Obfuscation", "authors": ["Payel Sadhukhan", "Tanujit Chakraborty"], "abstract": "The avalanche of AI deployment and its security-privacy concerns are two sides of the same coin. Article 17 of GDPR calls for the Right to Erasure; data has to be obliterated from a system to prevent its compromise. Extant research in this aspect focuses on effacing sensitive data attributes. However, several passive modes of data compromise are yet to be recognized and redressed. The embedding of footprints of training data in a prediction model is one such facet; the difference in performance quality in test and training data causes passive identification of data that have trained the model. This research focuses on addressing the vulnerability arising from the data footprints. The three main aspects are i] exploring the vulnerabilities of different classifiers (to segregate the vulnerable and the non-vulnerable ones), ii] reducing the vulnerability of vulnerable classifiers (through data obfuscation) to preserve model and data privacy, and iii] exploring the privacy-performance tradeoff to study the usability of the data obfuscation techniques. An empirical study is conducted on three datasets and eight classifiers to explore the above objectives. The results of the initial research identify the vulnerability in classifiers and segregate the vulnerable and non-vulnerable classifiers. The additional experiments on data obfuscation techniques reveal their utility to render data and model privacy and also their capability to chalk out a privacy-performance tradeoff in most scenarios. The results can aid the practitioners with their choice of classifiers in different scenarios and contexts.", "sections": [{"title": "1 Introduction", "content": "\"According to Grand View Research, the global artificial intelligence (AI) market was valued at U.S. 93.5 billion back in 2021. From 2022 to 2030, the market is expected to grow at a compound annual growth rate (CAGR) of 38.1%. This growth can largely be attributed to the \u201ccontinuous research and innovation directed by the tech giants who are driving the adoption of advanced technologies in industry verticals, such as automotive, healthcare, retail, finance, and manufacturing.\"\nAt present, AI-based classifier models have become indispensable to the functioning of enterprises and businesses [9]. The models use historical data about operations and render an informed forecast for an upcoming instance. The prediction models find utility in diversified domains like sales forecasting [37], marketing response [24], supply chain optimization [6], fraud detection [15], customer base expansion [41,31], customer segmentation [20], customer semantic analysis [8], churn prediction [33], and financial and investment analysis [25, 19]. It is expected to increase in the future in the context of healthcare support transfer [4], knowledge sharing in virtual communities [39], and implementation of smart-home [45] and smart-city paradigms [36].\nHowever, ethical and privacy concerns on the operations of AI-based systems often accompany the upsurge in their deployment [7,32]. Privacy and integrity of data contributors and data owners are two fundamental challenges in this respect. The concerns lie with safeguarding sensitive information, proprietorial processes, and customer data from unauthorized access and malicious intentions.\n\"According to IBM's Cost of Data Breach Report 2023, the average cost of a data breach reached an all-time high in 2023 of USD 4.45 million. This represents a 2.3% increase from the 2022 cost of USD 4.35 million.\" Expenses are incurred from compensating impacted customers, initiating incident response activities, conducting breach investigations, investing in enhanced security measures, covering legal fees, and facing substantial regulatory fines for GDPR non-compliance [40]. In addition to these, such breaches have far-fetched consequences from corporate perspectives, as they involve the trust of its customers, which is an intangible resource for any concern [5,16]. To this end, several government and industry standards have been put into place in recent years. The primary goal is to uphold the privacy of individuals [42,12].\nArt. 17 GDPR Right to erasure ('right to be forgotten'):\nThe data subject shall have the right to obtain from the controller the erasure of personal data concerning him or her without undue delay, and the controller shall have the obligation to erase personal data without undue delay where one of the following grounds applies:....\nThere can be several ways in which the privacy of individuals gets compromised. There are two main categories: intentional and unintentional breaches [1]. The former is executed and attempted by malicious entities to achieve some materialistic gain [18]. The latter is less known and is often caused by the design of the schemes and interfaces, which causes information leakage. The training"}, {"title": "Title Suppressed Due to Excessive Length", "content": "phase of a data model is a budding stage of one such breach. During training, information about the training data gets embedded in the model, which creates its footprints in the model, contributing to its vulnerability in the test phase. This research investigates the privacy and security concerns in AI deployment from the perspective of data footprints.\nThe first research question that we explore is\nRQ1: Does the training data leave footprints on the classifier model?\nWe investigate by observing and comparing the performance qualities of seen and unseen data. Suppose a classifier shows a difference in performance quality between the two (with seen being the better). In that case, it can be concluded that the classifier has left its footprints in the model. This phenomenon is a substantial violation of the GDPR right to erasure (right to be forgotten) because footprints can compromise the integrity of data owners. From an operational perspective, such footprints make the model vulnerable to attackers; they can deduce the training data by querying a classifier model through the footprints. The next segment of our research focuses on exploring the degree of vulnerability shown by different classifier models.\nRQ2: Does a given training data show similar vulnerabilities across different classifier models? On shared platforms, users have different trust levels. Vulnerability information of the classifiers can guide the practitioners in choosing the optimal model in terms of privacy preservation and performance. Additionally, we reconnoiter different datasets on each classifier model. Our research indicates the strong presence of a vulnerability in several classifier models, such as the Decision Tree classifier, Random forest classifier, kNN, and XGBoost classifier. On the contrary, classifiers like Logistic Regression, AdaBoost, Gaussian Naive Bayes, and Stochastic Gradient Descent have shown minimum vulnerability. To this end, we focus the next line of this research on ways to mitigate the vulnerabilities of the \"vulnerable\" classifiers.\nRQ3: Can obfuscating the training data help remove footprints in vulnerable classifiers? The first two investigations show that training data leaves its footprints on some classifiers. This section focuses on removing the footprints (or vulnerabilities) through data obfuscation. We obfuscate the training data before training a classifier. The empirical results show that this technique helps reduce the footprints of data, thereby reducing the vulnerability of the classifiers. Now, the question is, does reducing the training data lessen the efficiency of a model and lead to a fall in performance? The last segment of our research is devoted to this aspect.\nRQ4: Does erasing the data footprints lessen the predictive power of a classifier model? Is the fall admissible? The goal is to capture the effect of data obfuscation on vulnerability and performance. A definition, privacy performance tradeoff is proposed to quantify and integrate the change in vulnerability and performance in a single variable following the vulnerability treatment. It measures the admissibility of the changes. The empirical results show that obfuscation causes a fall in the performance of the models, but the degree of change is usually tolerable.\nIn summary, RQ1 and RQ2 are analytical studies on the vulnerability of classifier models and the footprints of data. RQ3 proposes a solution to the data footprint problem. RQ4 deals with the admissibility of the solution.\nA prior version of this work [34] is published in AMCIS 2024. This work extends the previous version on conceptualization as well as empirical studies. We theo-"}, {"title": "Title Suppressed Due to Excessive Length", "content": "rize two aspects \u2013 vulnerability detection (estimation) and vulnerability mitigation. Additionally, detailed empirical studies are conducted, and outcomes are compiled to validate the usability of the proposed schema."}, {"title": "1.1 Organization of the paper", "content": "The rest of this paper is organized as follows. In the next section, we describe the extant works on privacy aspects in IS and the preliminaries related to this research. In Section 3, we elaborate on the proposed methodology. Sections 4 and 5 elaborate on the experimental components and experimental design. In Section 6, we present the Analysis and Results. The article wraps up with the Discussion and Conclusion in Section 7."}, {"title": "2 Literature Review and Preliminaries", "content": ""}, {"title": "2.1 Literature Review", "content": "Privacy preservation is critical in web-based and open-ended shared platforms. It deals with the protection of sensitive information of the data owners and their privacy rights [28,11], lowering of data breach risks [43], and support for ethical AI operations [38].\nIdentifying these needs, practitioners formalized several aspects to render a deployable system [3,30]. Privacy is critical for ensuring an ethical environment and sustainability. The deluge of ML has prompted nations, bodies, and establishments to formalize privacy requirements over the web [13,27]. In parallel, technical researchers have focused on designing algorithms that sustain users' privacy and mitigate cyber threats [23]. Several methods of privacy-preserving classification, data search, and clustering methods have been proposed [26].\nInitial years of privacy-related research focused on data privacy and anonymization [35]. The studies focused on anonymizing the sensitive attributes of the data [22]. This class of methods proved helpful in tackling the vulnerabilities related to the training and the test data. However, with the advancement of time and the emergence of black-box attacks, model privacy emerged as a necessity [21]. In a black box attack, a trespasser attempts to learn a model's parameters and training data through its predictions [44]. This attack has a passive modus operandi and occurs without any active breaching effort from the trespasser; hence, it is difficult to detect and mitigate [14]. This aspect becomes more crucial when decision-making models are shared over the cloud. We may note that non-shared platforms are equally susceptible to this attack. In this work, we formalize and analyze the concepts related to the model's vulnerability from a practitioner's perspective.\nAdopting privacy-enhanced solutions often renders suboptimal computational output in terms of both performance and speed. A deployable solution needs to have a suitable tradeoff between the two [2]. To this end, we propose procedures for reducing model vulnerability through data obfuscation."}, {"title": "Title Suppressed Due to Excessive Length", "content": ""}, {"title": "2.2 Preliminaries", "content": "Data privacy is the backbone of this work. We elaborate on different strata of privacy from the AI perspective. The solution proposed is based on data obfuscation, which we also elaborate on in the later segment of the subsection."}, {"title": "2.2.1 Privacy", "content": "Several definitions of privacy exist in the literature [26]. They are detailed as follows.\nTraining data privacy: It refers to the integrity of the data that will train a model. The sensitive attributes can originate from the information related to the identities of the individuals, health information parameters, and financial credentials, to name a few. Privacy preservation of this class of attributes is an absolute necessity to maintain the integrity of a system and is usually achieved through data anonymization and differential privacy mechanisms.\nTest data privacy: It is similar to training data privacy. Before obtaining the prediction for a point, the sensitive attributes have to be removed or anonymized to prevent their confidentiality and compromise. While obtaining the prediction over the cloud, a test data point has to be suitably processed to ensure its privacy and integrity.\nModel privacy: It deals with the privacy preservation of the machine learning models. The sensitive detail comprises of the model specification, parameters, and the data that has trained (training data) it. An attacker has limited access to the model and tries to learn this information through predictions rendered by the model. Training data privacy and test data privacy have straightforward implementations. On the contrary, identifying the vulnerabilities of model privacy and its mitigation is a complex proposition that has been addressed in this research.\nThis research focuses on the privacy preservation of training data."}, {"title": "2.2.2 Data obfuscation", "content": "The modus operandi of data obfuscation techniques is to alter data (critical or sensitive); the goal is to retain its usability and computability for the rightful users and purposes while making it harder to interpret for the trespassers [17]. The usability is twofold: i] it protects sensitive information from unauthorized access and misuse, and ii] it provides computability and functionality for the intended applications. Data obfuscation can be implemented in various ways:\nData Masking: This involves replacing sensitive data with fabricated data; the goal is to prevent privacy and security attacks on the real data.\nTokenization: It involves the replacement of a sensitive data element with a non-sensitive alias, which is called a token. The token is meaningful within its intended scope and cannot be reversed without access to the original data map."}, {"title": "Title Suppressed Due to Excessive Length", "content": ""}, {"title": "Data Reduction", "content": "The significant and sensitive information are removed from the data to prevent their misuse by trespassers."}, {"title": "Randomization", "content": "This technique reorders sequential data records to hide their original sequence or pattern in a random order."}, {"title": "Data Perturbation and noise addition", "content": "Perturbation involves incorporating minor data modifications without significantly changing its statistical properties."}, {"title": "Hamming-based encoding", "content": "Numeric data in decimal format is transformed into binary form. For each feature and data point, a 0-1 conversion is done by comparing the feature value with its mean. A value lesser than the mean is transformed to 0, while 1 is assigned otherwise. This particular transformation preserves some aspects of the data. The scheme renders a loss of localized information, which forms the basis of the obfuscation."}, {"title": "Locality Sensitive Hashing (LSH) encoding", "content": "A given feature space is partitioned into random subspaces using random hyperplanes. The points residing in the same subspace get the same transformation value. Data obfuscation is ensured on the basis of this aspect."}, {"title": "3 Methodology", "content": "The business ventures employ prediction models to automate some segments of internal and external operations. The prediction models are developed or trained on one or several datasets, which are usually known as training data. The training data is a critical asset from the perspective of the individual data owners as well as the concerned business establishment. Let us explore the vulnerability aspects of the training data."}, {"title": "3.1 Recognizing the vulnerabilities", "content": ""}, {"title": "Vulnerability of training data", "content": "Parties with conflicting interests can attempt to learn the training data of their adversaries. The parties can involve any curious individual attempting to learn sensitive parameters of a particular user/s or a competing business venture. The interest of a competing business venture can be many-fold; it can attempt to destabilize the operation of its rival venture, or it may attempt to gain profit by learning the data of its rival.\nA trespasser can attempt to guess and learn the training data without making an explicit query about it. This is possible by virtue of inference attacks on the prediction model. By obtaining the prediction of a particular data point, a trespasser can know whether it served as training data. In this scenario, it is essential to quantify the vulnerability of a set of training data points. Usually, the accuracy and correctness of the training data points are more than that of the unseen (test) points. The trespassers use this information to make inferences about points memberships to the training set. We quantify and define the vulnerability of the training data on the basis of these intuitions. The definition is as follows.\nDefinition 1 Vulnerability of training data $vul(D)$: It is quantified by the ratio of the prediction accuracy of the training data points to that of the test or unseen data points for a dataset, D.\n$vul(D) = \\frac{Correctness\\ of\\ predictions\\ on\\ training\\ data\\ for\\ D}{Correctness\\ of\\ predictions\\ on\\ test\\ or\\ unseen\\ data\\ for\\ D}$                                              (1)\nThe more the difference in the output-correctness of the training and the unseen set, the more is the training data's vulnerability. A higher vulnerability provides a favorable substrate for trespassers to guess the training data correctly. When the veracity of predictions is similar for training and test data, it is difficult for a trespasser to guess the training data points correctly.\nWe have to be judicious with the choice of assessment metric in different cases. In the generic scenario, the accuracy of classification provides admissible information. However, accuracy can be insufficient to deduce a picture when the dataset possesses irregularities like imbalance and missing features (there can be more). For example, accuracy can give misleading results on imbalanced data. The true picture can be given through metrics like minority class F1, precision, and recall."}, {"title": "Title Suppressed Due to Excessive Length", "content": ""}, {"title": "Is a given data's vulnerability the same across different models?", "content": "Now that it is known how to quantify a training dataset's vulnerability, it is worth exploring if its vulnerability is the same in different models. Let us consider two classifier models, A and B. If the difference in training and test accuracies is less for classifier A than B, it is favorable to deploy A. Even though A and B are trained on the same training data, A will divulge less information about training data than B, probabilistically. This exploration can provide ways to secure the training data with the available options (classifier options), without the involvement of any external add-on."}, {"title": "Is there a correlation between a classifier model's interpretability and the training data's vulnerability?", "content": "Interpretability of predictive models is a noteworthy aspect nowadays. It refers to the ease with which humans can understand and also explain the decisions rendered by a machine learning or a statistical model. Privacy and vulnerability are also intertwined with human perception of a model. Against this backdrop, the correlation (possible) between training data vulnerability and model interpretability is worth exploring. We carry out an empirical study to explore if there is a positive, negative, or no correlation between the two."}, {"title": "3.2 Formulating ways to address the vulnerabilities", "content": "The previous discussion focused on recognizing the scenarios that create the most and the least vulnerabilities for the training data. Data vulnerability is a prevalent phenomenon that varies across classifiers and datasets. Given that, we must devise intuitive ways to reduce the training data's vulnerability. We explore data obfuscation techniques for the same."}, {"title": "Does training data obfuscation help us mitigate its vulnerability?", "content": "Data obfuscation techniques deal with baffling the key information in the datasets. The goal is to uphold the secrecy and privacy of the data. Data obfuscation is particularly useful in scenarios where we need to share details but also run a potential risk of revealing sensitive aspects. Data obfuscation can be achieved in several ways: perturbation, anonymization, masking, and randomization. Data obfuscation renders real-looking data to fool the trespassers interested in learning the data. Data before and after obfuscation look similar; hence, it is often difficult for someone to distinguish between the two.\nWe bank on this intuition and explore if obfuscating the training data helps us hide the training data's signatures in the classifier model. The goal is to find out if training a classifier model with obfuscated data diminishes its (classifier model's) capability to distinguish between the training data and the obfuscated data. We would explore how the training data vulnerability, $vul(D)$ (defined in the previous section), changes with the incorporation of data obfuscation. We define $vul(D)_{change}$ in this regard. Let $vul(D)$ and $vul(D)_{obf}$ be the vulnerability obtained for a dataset using its regular and obfuscated version, respectively."}, {"title": "Title Suppressed Due to Excessive Length", "content": ""}, {"title": "Definition 2", "content": "Change of vulnerability, $vul(D)_{change}$: It is quantified by the ratio of percentage change of vul after obfuscation with respect to the vul obtained on the regular (not obfuscated) data.\n$vul_{change} = \\frac{vul(D) - vul_{obf}(D)}{vul(D)}$                            (2)\nThe range of $vul_{change}$ can vary between -\u221e to \u221e. When $vul_{change}$ has a negative value, it signifies that data obfuscation worsened the training data's vulnerability. A positive value of $vul_{change}$ signifies a constructive development, as it indicates the lessening of the vulnerability. The zero value manifests the obfuscation technique's failure to render an improvement. Note that while computing, $vul_{change}(D)$, we have to consider same classifier and training points for vul(D) and $vul_{obf}(D)$ computation."}, {"title": "3.3 Exploring the privacy-performance tradeoff", "content": "The privacy-performance tradeoff forms a critical bottleneck in the design of deployable models on shared platforms. While it is necessary to share data that can render meaningful computational outcomes to all parties, protecting the data is also obligatory from the data owners' perspective. While ensuring data privacy, practitioners and model developers are often bothered about the loss in performance quality. We investigate the following contexts.\nAssociation between vulnerability and performance: It is necessary to explore if some association exists between the degree of vulnerabilities and performance on a classifier. We need to explore further if two datasets with different vul (on the same classifiers) render similar or differing performances; if yes, is there any association or not?\nComparing the performance of the test data with and without data obfuscation: In this work, we propose to use data obfuscation techniques to reduce the vulnerability of the training data. Apart from the privacy aspect, we need to investigate whether data obfuscation degrades the predictive capability of a classifier. In the following definition, we try to capture the privacy-performance tradeoff. We denote the performances of data D before and after obfuscation with $per f(D)$ and $per f_{obf}(D)$ and propose the following definition for quantifying the privacy-performance tradeoff.\nDefinition 3 privacy performance tradeoff\n($PP_{tradeoff}(D)$): It is quantified by the product of the ratio of change in the data vulnerability before (vul(D)) and after obfuscation ($vul_{obf}(D)$) and the ratio of change in performance after ($per f_{obf}(D)$) and before data obfuscation (per f(D)).\n$PP_{tradeoff}(D) = \\frac{vul(D)}{vul_{obf}(D)}  X  \\frac{per f_{obf}(D)}{per f(D)}$                         (3)\nThere are two integral components of\n$PP_{tradeoff}(D)$. The first component quantifies the change of data vulnerability with the incorporation of obfuscation, \u2191 the decrease, the better. The"}, {"title": "Title Suppressed Due to Excessive Length", "content": "second component quantifies the change in performance, lesser the fall in performance, the better it is. Hence, we have taken the ratio in reverse order for the two. Consequently, the higher the value of $PP_{tradeoff}(D)$, the better the outcome.\nVarious data obfuscation protocols are found in the literature. For a given data D and a classifier model, the protocol can be chosen on the basis of $PP_{tradeoff}(D)$ values. The practitioners can choose the obfuscation protocol which gives the highest $PP_{tradeoff}(D)$ for a given data.\nWe explore the association between i] reduction of data vulnerability with data obfuscation and ii] performance trend with data obfuscation."}, {"title": "4 Experimental components", "content": "Several experiments are carried out in this work to explore the vulnerabilities of the datasets and the classifiers. Each segment of methodology is explored empirically."}, {"title": "4.1 Datasets", "content": "The empirical study is carried out on three datasets: two (Body and Customer) are multiclass, and the remaining one (Churn) is binary and class imbalanced. The datasets are described below.\nBody performance prediction\u2074: This dataset was collected by the Korea Sports Promotion Foundation to explore the correlation between the body statistics of individuals (features) and their performances (class). The former consists of information like age, gender, height, weight, systolic and diastolic blood pressure, grip force, sit and bend forward distance, situp count, and broad-jump distance of an individual. Along with these, the performance of the individual is noted, and it can be classified into any of the four classes. There are eleven features and 13393 points in this dataset. Each of the four classes has approximately equal shares of 25% points. This dataset contains the sensitive health parameters of many individuals.\nProfitable customer segment prediction 5: In marketing, finding the right target customer group for maximizing returns is a critical task. This dataset originates from an online retail company, which has compiled historical data on various customer segments. It tracks the profitability of each group following specific marketing campaigns and evaluates the effectiveness of investing in marketing for these groups in hindsight. There are numeric 70 features extracted from their market research findings; a feature can correspond to only one group or a pair of groups. Each datapoint corresponds to information pertaining to the comparison of two different groups. The class of the datapoint predicts whether both groups are profitable, only one is profitable or neither."}, {"title": "Title Suppressed Due to Excessive Length", "content": ""}, {"title": "Ecommerce user churn prediction", "content": "The dataset contains information on 49358 customers and 48 features. There are two classes which contain information as to whether the customers would churn or not. The dataset is collected on a website and constructed from past user-item interactions and actions on the website. The features include days elapsed since the last session, average session duration, and more. The analysis of this dataset is important in the financial as well as marketing domains.\nWhile performing the experiments, a dataset is partitioned in the ratio 4:1; four parts are used for training, and the remaining one part is used for testing. To explore the vulnerability and obtain the prediction on seen data, we consider a randomly selected 50% fraction of the training set for prediction."}, {"title": "4.2 Classifiers", "content": "All the experiments in this work involve assessing the predictive performance of the classifiers. While performing the experiments, we have explored eight classifiers [29]. For all the classifiers, we have used their sklearn implementation in Python [10]. The working principles of the classifiers and the parameters are given as follows.\nDecision Tree Classifier: Its modus operandi is based on recursive partitioning of the feature space into disjoint regions, with each division made based on the value of a particular feature. The process continues till optimized results are obtained or the stopping criteria are met. Gini impurity is used to split the nodes. We have set the maximum depth to 10 and the maximum feature size to 5.\nRandom Forest Classifier: It is an ensemble learning method where the base classifier is a decision tree. It combines the predictions of multiple individual trees to increase the robustness of the classifier system. We used gini impurity to split the tree, and bootstrap samples were used. Like the previous classifier, the maximum depth and maximum feature size are set to 10 and 5, respectively.\nXGBoost Classifier: It consists of an ensemble of Gradient Boosting Machines. The learning framework is dependent on improving and optimizing upon past learning mistakes. It is designed to be immune to overfitting problems of the decision trees. We have set the maximum depth as six and the minimum child weight as one, and all other parameters are set to default values and choices.\nk-Nearest Neighbor Classifier: It classifies a test data point by taking a majority vote of its k closest points, assigning the most common class among its k nearest neighbors to the test data point. The neighborhood size k is set to 5.\nStochastic Gradient Descent Classifier: It fits a classifier minimizing a convex loss function using the gradient descent method. We have used the Python implementation at its default configuration."}, {"title": "Title Suppressed Due to Excessive Length", "content": ""}, {"title": "AdaBoost Classifier", "content": "It is an ensemble of classifiers. The original dataset fits the first classifier and the weights of instances for subsequent classifiers are adjusted according to the misclassification in the previous classifiers. We have used the Python implementation at its default configuration."}, {"title": "Gaussian Naive Bayes Classifier", "content": "It is a variant of the Naive Bayes Classifier with the additional assumption of Gaussian distribution of the features. It works well for large number of points and features. We have used the Python implementation at its default configuration."}, {"title": "Logistic Regression Classifier", "content": "It learns a probabilistic output of class membership by determining the relationship between the features and the predictor variables. The probabilities can be converted to class labels using a variety of methods. We have used the Python implementation with Liblinear solver."}, {"title": "4.3 Evaluating metrics", "content": "We have used accuracy to evaluate the performance of multiclass datasets, and average precision score in an imbalanced dataset."}, {"title": "5 Design of experiments", "content": ""}, {"title": "5.1 Experiment 1", "content": "In this experiment, we explore the vulnerability of the training data of different datasets across various classifier models. There are three major goals of this experiment, they are described as follows:\n1. Does training data vulnerability exist? Are the classifiers differentially efficacious across the data they had seen during training and the data they had not? Vulnerability (vul) is calculated from Equation 1.\n2. Is a particular dataset equally vulnerable across different classifiers? Knowledge on this aspect can help the practitioners select the privacy-preserving classifiers before deploying the models. For example, let the task be to choose a model (for deployment) that is trained on the Churn dataset. A judicious choice for the practitioner would be to choose the classifier with the lowest vul score.\nFurther, can we segregate a classifier as vulnerable or not vulnerable?\n3. Is a classifier equally vulnerable on different datasets? Does it manifest similar vul scores for different datasets? This can help practitioners choose datasets that will train a deployable model, particularly when the choice of classifier is fixed. For example, if a kNN classifier model has to be deployed, the practitioner has to select the dataset which renders the least vulnerability on kNN classifier."}, {"title": "Title Suppressed Due to Excessive Length", "content": ""}, {"title": "5.2 Experiment 2", "content": "This experiment is dedicated to investigating the role of data obfuscation in curtailing classifier vulnerability. We explore the vulnerable and the not vulnerable classifiers. We compare the vulnerabilities of (dataset, classifiers) pairs before and after applying data obfuscation and report the percentage change (Equation 2).\nTwo data obfuscation techniques are explored - i] LSH-based encoding and ii] Hamming encoding. We report their capability to reduce data vulnerability."}, {"title": "5.3 Experiment 3", "content": "After exploring the role of data obfuscation in reducing the vulnerability, we explore a critical aspect related to their usability in practical scenarios. Data obfuscation can find relevance in predictive scenarios if the fall in performance is within an admissible range. To this end, we explore the classifier performances in the original data and the obfuscated data, and integrate them with the change of vulnerability. privacy performance tradeoff is used for the same (Equation 3). Privacy preservation on a dataset is concedable only when the application of obfuscation renders a high privacy-performance tradeoff score."}, {"title": "6 Analysis and Results", "content": "In this section, we discuss the outcomes of the experiments on footprint detection. The results can help the practitioners perceive the nitty-gritty of privacy preservation while designing classification models, making the most suitable choice in terms of performance as well as privacy.\nExperiment 1: Figure 2 shows the fundamental result of this study: whether training data leaves footprints on the classifier. The figure shows that training data leaves footprints on some particular classifiers, namely, Decision Trees, Random Forests, and k-nearest Neighbor Classifiers. On these classifiers, the performance accuracy (or F\u2081) on training and test data has shown significant variation by virtue of which a trespasser can successfully learn the training data. When a group of points within a small neighborhood is predicted to their true class by the classifier, a training point likely resides in that neighborhood. The remaining four classifiers-Stochastic Gradient Descent, AdaBoost, Gaussian Naive Bayes, and Logistic Regression-have shown similar performance accuracy (or F\u2081) on training and test data, where it isn't easy to single out the footprints of training data.\nWe have considered three datasets - Churn, Customer, and Body. Figure 2 shows that the degree of vulnerability varies across datasets - Body has shown the least vulnerability, while Customer and Churn have shown a bothersome amount of vulnerability. Vulnerable classifiers, Decision Trees, and Random Forest Classifiers have shown low vulnerability (\u00a11.4) on the Body dataset.\nExperiment 2: Tables 1-3 report the change of vulnerability on adopting data obfuscation protocols. Tables 1, 2, and 3 are dedicated to Churn, Customer, and Body. For each classifier-dataset pair, five values are reported: vulnerability on the original dataset, vulnerability on LSH-encoding-based obfuscated data,"}, {"title": "Title Suppressed Due to Excessive Length", "content": ""}, {"title": "the corresponding change with respect to the original dataset", "content": "vulnerability on Hamming-encoding-based obfuscated data, the corresponding change with respect to the original dataset. The range of $vul_{change}$ is 11.21% to 52.37% for vulnerable classifiers. The results demonstrate the two obfuscation schema's ability to remove the training dataset's footprints from the classifiers. Both methods have rendered vulnerability scores of approximately 1 for all classifier-dataset pairs. A vulnerability score equal to 1 indicates equal performance accuracy (or F\u2081) in the training and test dataset.\nThe vulnerability change for the non-vulnerable classifiers is -9.68% to 10.57%. Note that, the incorporation of obfuscation schemes in non-vulnerable classifiers (SGD, Adaboost, Gaussian Naive Bayes, Logistic Regression) has rendered negative $vul_{change}$ values in some cases. These results show that data obfuscation may not render meaningful outcomes for nonvulnerable classifiers.\nExperiment 3: Tables 4-6 report the privacy-performance tradeoff achieved by incorporating data obfuscation. We have used the scores (privacy and performance) on original datasets as the baseline. Hence, going by Equation (3), the baseline is one. A value higher than 1 denotes the feasible admissibility of an obfuscation protocol while curtailing footprints of training data. Data from Table 4 shows that on Churn, LSH-encoding and Hamming's encoding have rendered privacy-performance tradeoff value > 1 for Decision Tree, XGB, and kNN classifiers. For the Random Forest Classifier, the value is slightly less than one in both cases. For Customer dataset, both obfuscation schemes have rendered privacy-performance tradeoff value greater than one on all classifiers."}, {"title": "7 Discussion and Conclusion", "content": ""}, {"title": "7.1 Overall findings", "content": "This study conceptualizes the passive security and privacy loopholes in predictive models' primary modus operandi. Our analysis shows that vulnerability is a practical issue, and its non-redressal can lead to severe data breaches, which are recognized under Article 17 of GDPR. It is interesting to note that the extent of vulnerability varies across classifiers. Data obfuscation is introduced as a remedial measure to curtail the vulnerabilities of the vulnerable classifiers. Empirical outcomes substantiate their utility in vulnerability mitigation. The study on privacy-performance tradeoff manifests the usability of data obfuscation techniques in most"}]}