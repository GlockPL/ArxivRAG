{"title": "Footprints of Data in a Classifier Model: The Privacy Issues and Their Mitigation through Data Obfuscation", "authors": ["Payel Sadhukhan", "Tanujit Chakraborty"], "abstract": "The avalanche of AI deployment and its security-privacy concerns are two sides of the same coin. Article 17 of GDPR calls for the Right to Erasure; data has to be obliterated from a system to prevent its compromise. Extant research in this aspect focuses on effacing sensitive data attributes. However, several passive modes of data compromise are yet to be recognized and redressed. The embedding of footprints of training data in a prediction model is one such facet; the difference in performance quality in test and training data causes passive identification of data that have trained the model. This research focuses on addressing the vulnerability arising from the data footprints. The three main aspects are i] exploring the vulnerabilities of different classifiers (to segregate the vulnerable and the non-vulnerable ones), ii] reducing the vulnerability of vulnerable classifiers (through data obfuscation) to preserve model and data privacy, and iii] exploring the privacy-performance tradeoff to study the usability of the data obfuscation techniques. An empirical study is conducted on three datasets and eight classifiers to explore the above objectives. The results of the initial research identify the vulnerability in classifiers and segregate the vulnerable and non-vulnerable classifiers. The additional experiments on data obfuscation techniques reveal their utility to render data and model privacy and also their capability to chalk out a privacy-performance tradeoff in most scenarios. The results can aid the practitioners with their choice of classifiers in different scenarios and contexts.", "sections": [{"title": "1 Introduction", "content": "\"According to Grand View Research, the global artificial intelligence (AI) mar-\nket was valued at U.S. 93.5 billion back in 2021. From 2022 to 2030, the market\nis expected to grow at a compound annual growth rate (CAGR) of 38.1%. This\ngrowth can largely be attributed to the \u201ccontinuous research and innovation\ndirected by the tech giants who are driving the adoption of advanced technolo-\ngies in industry verticals, such as automotive, healthcare, retail, finance, and\nmanufacturing.\"\"\nAt present, AI-based classifier models have become indispensable to the func-\ntioning of enterprises and businesses [9]. The models use historical data about\noperations and render an informed forecast for an upcoming instance. The pre-\ndiction models find utility in diversified domains like sales forecasting [37],\nmarketing response [24], supply chain optimization [6], fraud detection [15], cus-\ntomer base expansion [41,31], customer segmentation [20], customer semantic\nanalysis [8], churn prediction [33], and financial and investment analysis [25,\n19]. It is expected to increase in the future in the context of healthcare support\ntransfer [4], knowledge sharing in virtual communities [39], and implementation\nof smart-home [45] and smart-city paradigms [36].\nHowever, ethical and privacy concerns on the operations of AI-based systems\noften accompany the upsurge in their deployment [7,32]. Privacy and integrity\nof data contributors and data owners are two fundamental challenges in this\nrespect. The concerns lie with safeguarding sensitive information, proprietorial\nprocesses, and customer data from unauthorized access and malicious inten-\ntions.\n\"According to IBM's Cost of Data Breach Report 2023, the average cost of a\ndata breach reached an all-time high in 2023 of USD 4.45 million. This repre-\nsents a 2.3% increase from the 2022 cost of USD 4.35 million.\" Expenses are\nincurred from compensating impacted customers, initiating incident response\nactivities, conducting breach investigations, investing in enhanced security mea-\nsures, covering legal fees, and facing substantial regulatory fines for GDPR\nnon-compliance [40]. In addition to these, such breaches have far-fetched conse-\nquences from corporate perspectives, as they involve the trust of its customers,\nwhich is an intangible resource for any concern [5,16]. To this end, several gov-\nernment and industry standards have been put into place in recent years. The\nprimary goal is to uphold the privacy of individuals [42,12].\nArt. 17 GDPR Right to erasure ('right to be forgotten'):\nThe data subject shall have the right to obtain from the controller the erasure\nof personal data concerning him or her without undue delay, and the controller\nshall have the obligation to erase personal data without undue delay where one\nof the following grounds applies:....\nThere can be several ways in which the privacy of individuals gets compromised.\nThere are two main categories: intentional and unintentional breaches [1]. The\nformer is executed and attempted by malicious entities to achieve some mate-\nrialistic gain [18]. The latter is less known and is often caused by the design\nof the schemes and interfaces, which causes information leakage. The training"}, {"title": "3 Methodology", "content": "The business ventures employ prediction models to automate some segments\nof internal and external operations. The prediction models are developed or\ntrained on one or several datasets, which are usually known as training data. The\ntraining data is a critical asset from the perspective of the individual data owners\nas well as the concerned business establishment. Let us explore the vulnerability\naspects of the training data."}, {"title": "3.1 Recognizing the vulnerabilities", "content": "Parties with conflicting interests can attempt to learn the training data of\ntheir adversaries. The parties can involve any curious individual attempting\nto learn sensitive parameters of a particular user/s or a competing business\nventure. The interest of a competing business venture can be many-fold; it\ncan attempt to destabilize the operation of its rival venture, or it may at-\ntempt to gain profit by learning the data of its rival.\nA trespasser can attempt to guess and learn the training data without mak-\ning an explicit query about it. This is possible by virtue of inference attacks\non the prediction model. By obtaining the prediction of a particular data\npoint, a trespasser can know whether it served as training data. In this sce-\nnario, it is essential to quantify the vulnerability of a set of training data\npoints. Usually, the accuracy and correctness of the training data points are\nmore than that of the unseen (test) points. The trespassers use this infor-\nmation to make inferences about points memberships to the training set.\nWe quantify and define the vulnerability of the training data on the basis of\nthese intuitions. The definition is as follows.\nDefinition 1 Vulnerability of training data vul(D): It is quantified by the\nratio of the prediction accuracy of the training data points to that of the\ntest or unseen data points for a dataset, D.\n$$vul(D) = \\frac{Correctness\\ of\\ predictions\\ on\\ training\\ data\\ for\\ D}{Correctness\\ of\\ predictions\\ on\\ test\\ or\\ unseen\\ data\\ for\\ D}$$\n(1)\nThe more the difference in the output-correctness of the training and the\nunseen set, the more is the training data's vulnerability. A higher vulner-\nability provides a favorable substrate for trespassers to guess the training\ndata correctly. When the veracity of predictions is similar for training and\ntest data, it is difficult for a trespasser to guess the training data points\ncorrectly.\nWe have to be judicious with the choice of assessment metric in different\ncases. In the generic scenario, the accuracy of classification provides admis-\nsible information. However, accuracy can be insufficient to deduce a picture\nwhen the dataset possesses irregularities like imbalance and missing features\n(there can be more). For example, accuracy can give misleading results on\nimbalanced data. The true picture can be given through metrics like minority\nclass F1, precision, and recall."}, {"title": "3.2 Formulating ways to address the vulnerabilities", "content": "The previous discussion focused on recognizing the scenarios that create the\nmost and the least vulnerabilities for the training data. Data vulnerability is\na prevalent phenomenon that varies across classifiers and datasets. Given that,\nwe must devise intuitive ways to reduce the training data's vulnerability. We\nexplore data obfuscation techniques for the same.\nDoes training data obfuscation help us mitigate its vulnerability?\nData obfuscation techniques deal with baffling the key information in the\ndatasets. The goal is to uphold the secrecy and privacy of the data. Data\nobfuscation is particularly useful in scenarios where we need to share details\nbut also run a potential risk of revealing sensitive aspects. Data obfuscation\ncan be achieved in several ways perturbation, anonymization, masking,\nand randomization. Data obfuscation renders real-looking data to fool the\ntrespassers interested in learning the data. Data before and after obfuscation\nlook similar; hence, it is often difficult for someone to distinguish between\nthe two.\nWe bank on this intuition and explore if obfuscating the training data helps\nus hide the training data's signatures in the classifier model. The goal is to\nfind out if training a classifier model with obfuscated data diminishes its\n(classifier model's) capability to distinguish between the training data and\nthe obfuscated data. We would explore how the training data vulnerability,\nvul(D) (defined in the previous section), changes with the incorporation of\ndata obfuscation. We define vul(D)change in this regard. Let vul(D) and\nvul(D)obf be the vulnerability obtained for a dataset using its regular and\nobfuscated version, respectively."}, {"title": "3.3 Exploring the privacy-performance tradeoff", "content": "The privacy-performance tradeoff forms a critical bottleneck in the design of\ndeployable models on shared platforms. While it is necessary to share data that\ncan render meaningful computational outcomes to all parties, protecting the\ndata is also obligatory from the data owners' perspective. While ensuring data\nprivacy, practitioners and model developers are often bothered about the loss\nin performance quality. We investigate the following contexts.\nAssociation between vulnerability and performance: It is necessary\nto explore if some association exists between the degree of vulnerabilities and\nperformance on a classifier. We need to explore further if two datasets with\ndifferent vul (on the same classifiers) render similar or differing performances;\nif yes, is there any association or not?\nComparing the performance of the test data with and without data\nobfuscation: In this work, we propose to use data obfuscation techniques\nto reduce the vulnerability of the training data. Apart from the privacy\naspect, we need to investigate whether data obfuscation degrades the pre-\ndictive capability of a classifier. In the following definition, we try to capture\nthe privacy-performance tradeoff. We denote the performances of data D\nbefore and after obfuscation with per f(D) and per fobf (D) and propose the\nfollowing definition for quantifying the privacy-performance tradeoff.\nDefinition 3 privacy performance tradeoff\n(PPtradeoff (D)): It is quantified by the product of the ratio of change in the\ndata vulnerability before (vul(D)) and after obfuscation (vulobf (D)) and the\nratio of change in performance after (perfobf(D)) and before data obfusca-\ntion (per f(D)).\n$$PPtradeof f (D) = \\frac{vul(D)}{vul_{obf} (D)}X\\frac{'per\\ f\\ obf (D)}{per\\ f (D)}$$\n(3)\nThere are two integral components of\nPPtradeof f (D). The first component quantifies the change of data vulnera-\nbility with the incorporation of obfuscation, \u2191 the decrease, the better. The"}, {"title": "4 Experimental components", "content": "Several experiments are carried out in this work to explore the vulnerabilities\nof the datasets and the classifiers. Each segment of methodology is explored\nempirically."}, {"title": "4.1 Datasets", "content": "The empirical study is carried out on three datasets: two (Body and Customer)\nare multiclass, and the remaining one (Churn) is binary and class imbalanced.\nThe datasets are described below.\nBody performance prediction\u2074: This dataset was collected by the Korea\nSports Promotion Foundation to explore the correlation between the body\nstatistics of individuals (features) and their performances (class). The former\nconsists of information like age, gender, height, weight, systolic and diastolic\nblood pressure, grip force, sit and bend forward distance, situp count, and\nbroad-jump distance of an individual. Along with these, the performance of\nthe individual is noted, and it can be classified into any of the four classes.\nThere are eleven features and 13393 points in this dataset. Each of the four\nclasses has approximately equal shares of 25% points. This dataset contains\nthe sensitive health parameters of many individuals.\nProfitable customer segment prediction 5: In marketing, finding the\nright target customer group for maximizing returns is a critical task. This\ndataset originates from an online retail company, which has compiled his-\ntorical data on various customer segments. It tracks the profitability of each\ngroup following specific marketing campaigns and evaluates the effectiveness\nof investing in marketing for these groups in hindsight. There are numeric\n70 features extracted from their market research findings; a feature can cor-\nrespond to only one group or a pair of groups. Each datapoint corresponds\nto information pertaining to the comparison of two different groups. The\nclass of the datapoint predicts whether both groups are profitable, only one\nis profitable or neither."}, {"title": "4.2 Classifiers", "content": "All the experiments in this work involve assessing the predictive performance\nof the classifiers. While performing the experiments, we have explored eight\nclassifiers [29]. For all the classifiers, we have used their sklearn implementation\nin Python [10]. The working principles of the classifiers and the parameters are\ngiven as follows.\nDecision Tree Classifier: Its modus operandi is based on recursive par-\ntitioning of the feature space into disjoint regions, with each division made\nbased on the value of a particular feature. The process continues till opti-\nmized results are obtained or the stopping criteria are met. Gini impurity\nis used to split the nodes. We have set the maximum depth to 10 and the\nmaximum feature size to 5.\nRandom Forest Classifier: It is an ensemble learning method where the\nbase classifier is a decision tree. It combines the predictions of multiple in-\ndividual trees to increase the robustness of the classifier system. We used\ngini impurity to split the tree, and bootstrap samples were used. Like the\nprevious classifier, the maximum depth and maximum feature size are set to\n10 and 5, respectively.\nXGBoost Classifier: It consists of an ensemble of Gradient Boosting Ma-\nchines. The learning framework is dependent on improving and optimizing\nupon past learning mistakes. It is designed to be immune to overfitting prob-\nlems of the decision trees. We have set the maximum depth as six and the\nminimum child weight as one, and all other parameters are set to default\nvalues and choices.\nk-Nearest Neighbor Classifier: It classifies a test data point by taking a\nmajority vote of its k closest points, assigning the most common class among\nits k nearest neighbors to the test data point. The neighborhood size k is set\nto 5.\nStochastic Gradient Descent Classifier: It fits a classifier minimizing a\nconvex loss function using the gradient descent method. We have used the\nPython implementation at its default configuration."}, {"title": "5 Design of experiments", "content": "In this experiment, we explore the vulnerability of the training data of different\ndatasets across various classifier models. There are three major goals of this\nexperiment, they are described as follows:\n1. Does training data vulnerability exist? Are the classifiers differentially effi-\ncacious across the data they had seen during training and the data they had\nnot? Vulnerability (vul) is calculated from Equation 1.\n2. Is a particular dataset equally vulnerable across different classifiers? Knowl-\nedge on this aspect can help the practitioners select the privacy-preserving\nclassifiers before deploying the models. For example, let the task be to choose\na model (for deployment) that is trained on the Churn dataset. A judicious\nchoice for the practitioner would be to choose the classifier with the lowest\nvul score.\nFurther, can we segregate a classifier as vulnerable or not vulnerable?\n3. Is a classifier equally vulnerable on different datasets? Does it manifest sim-\nilar vul scores for different datasets? This can help practitioners choose\ndatasets that will train a deployable model, particularly when the choice\nof classifier is fixed. For example, if a kNN classifier model has to be de-\nployed, the practitioner has to select the dataset which renders the least\nvulnerability on kNN classifier."}, {"title": "5.2 Experiment 2", "content": "This experiment is dedicated to investigating the role of data obfuscation in cur-\ntailing classifier vulnerability. We explore the vulnerable and the not vulnerable\nclassifiers. We compare the vulnerabilities of (dataset, classifiers) pairs before\nand after applying data obfuscation and report the percentage change (Equa-\ntion 2).\nTwo data obfuscation techniques are explored - i] LSH-based encoding and ii]\nHamming encoding. We report their capability to reduce data vulnerability."}, {"title": "5.3 Experiment 3", "content": "After exploring the role of data obfuscation in reducing the vulnerability, we\nexplore a critical aspect related to their usability in practical scenarios. Data\nobfuscation can find relevance in predictive scenarios if the fall in performance is\nwithin an admissible range. To this end, we explore the classifier performances in\nthe original data and the obfuscated data, and integrate them with the change\nof vulnerability. privacy performance tradeoff is used for the same (Equation\n3). Privacy preservation on a dataset is concedable only when the application\nof obfuscation renders a high privacy-performance tradeoff score."}, {"title": "6 Analysis and Results", "content": "In this section, we discuss the outcomes of the experiments on footprint detec-\ntion. The results can help the practitioners perceive the nitty-gritty of privacy\npreservation while designing classification models, making the most suitable\nchoice in terms of performance as well as privacy.\nExperiment 1: Figure 2 shows the fundamental result of this study: whether\ntraining data leaves footprints on the classifier. The figure shows that training\ndata leaves footprints on some particular classifiers, namely, Decision Trees,\nRandom Forests, and k-nearest Neighbor Classifiers. On these classifiers, the\nperformance accuracy (or F\u2081) on training and test data has shown significant\nvariation by virtue of which a trespasser can successfully learn the training data.\nWhen a group of points within a small neighborhood is predicted to their true\nclass by the classifier, a training point likely resides in that neighborhood. The\nremaining four classifiers-Stochastic Gradient Descent, AdaBoost, Gaussian\nNaive Bayes, and Logistic Regression-have shown similar performance accu-\nracy (or F\u2081) on training and test data, where it isn't easy to single out the\nfootprints of training data.\nWe have considered three datasets - Churn, Customer, and Body. Figure 2\nshows that the degree of vulnerability varies across datasets - Body has shown\nthe least vulnerability, while Customer and Churn have shown a bothersome\namount of vulnerability. Vulnerable classifiers, Decision Trees, and Random For-\nest Classifiers have shown low vulnerability (\u00a11.4) on the Body dataset.\nExperiment 2: Tables 1-3 report the change of vulnerability on adopting\ndata obfuscation protocols. Tables 1, 2, and 3 are dedicated to Churn, Customer,\nand Body. For each classifier-dataset pair, five values are reported: vulnerability\non the original dataset, vulnerability on LSH-encoding-based obfuscated data,"}, {"title": "7 Discussion and Conclusion", "content": "This study conceptualizes the passive security and privacy loopholes in predic-\ntive models' primary modus operandi. Our analysis shows that vulnerability is\na practical issue, and its non-redressal can lead to severe data breaches, which\nare recognized under Article 17 of GDPR. It is interesting to note that the\nextent of vulnerability varies across classifiers. Data obfuscation is introduced\nas a remedial measure to curtail the vulnerabilities of the vulnerable classifiers.\nEmpirical outcomes substantiate their utility in vulnerability mitigation. The\nstudy on privacy-performance tradeoff manifests the usability of data obfusca-\ntion techniques in most scenarios."}, {"title": "7.2 Implication to IS research and businesses", "content": "Privacy of users and data owners is a critical aspect for businesses. Extant IS\nliterature provides several conceptual frameworks for privacy breaches arising\nfrom active contravention in systems and enthusiastic attempts from trespassers.\nIn this backdrop, this research is contributory in several ways. First, it concep-\ntualizes the passive privacy threats that arise from the inherent embedding of\ntraining data in the prediction models. The threat of data compromise is in\nconflict with Article 17 of GDPR. Hence, its violation, reporting to competing\nauthorities, and validation can be financially menacing for business ventures. Be-\nsides conceptualization of vulnerability, the paper also introduces a framework\nfor its quantification. It helps the users and practitioners understand the vulner-\nable and non-vulnerable models, helping them make an informed deployment in\ncritical and sensitive scenarios. Second, we propose a remedial protocol to curtail\nthe vulnerability issues in classifiers. The remedial protocol is based on an ex-\ntant paradigm of data obfuscation. The relevant experiment is carried out with\na control setting where the non-vulnerable classifiers act as the control variant.\nThe remedial protocol provides multiple options for the users and practitioners\nto deal with vulnerability \u2013 i] they can tackle the vulnerability issue by choosing\na relevant classifier, or ii] they can employ a suitable data obfuscation paradigm\nto mitigate the vulnerability. Lastly, the study explores the privacy-performance\ntradeoff achieved on the data obfuscation solution. While concentrating on pri-\nvacy maintenance, the businesses cannot do away with their motto of delivering\nperformance. To be usable in business ventures, the data obfuscation protocols"}, {"title": "7.3 Limitations and directions for future research", "content": "This study has several limitations which can be worked upon through further\nresearch. Firstly, we have explored only one form of passive vulnerability in this\nresearch. Our detection and redressal are based on a stringent assumption of\nobtaining a difference in performance between the test data and the training\ndata. What class of vulnerabilities can arise when a classifier renders similar\nperformances on training and test data is to be explored in further studies.\nSecondly, data obfuscation causes a substantial fall in performance in some\ncases. It is necessary to integrate other protocols to uphold the performance to\nan admissible extent.\nIn Conlusion, this research makes a novel attempt to investigate and theorize\na fundamental issue in classification models - footprints of training data which\nrenders a stringent violation of the Right to Erasure of GDPR. The outcomes\nof this study confirm the presence of data footprint in a substantial genre of\nclassifiers. The quantification of vulnerability and privacy-performance tradeoff\nallows the users and practitioners to choose the relevant model for their purpose."}]}