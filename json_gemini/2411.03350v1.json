{"title": "A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness", "authors": ["FALI WANG", "ZHIWEI ZHANG", "XIANREN ZHANG", "ZONGYU WU", "TZUHAO MO", "QIUHAO LU", "WANJING WANG", "RUI LI", "JUNJIE XU", "XIANFENG TANG", "QI HE", "YAO MA", "MING HUANG", "SUHANG WANG"], "abstract": "Large language models (LLM) have demonstrated emergent abilities in text generation, question answering, and reasoning, facilitating various tasks and domains. Despite their proficiency in various tasks, LLMs like LaPM 540B and Llama-3.1 405B face limitations due to large parameter sizes and computational demands, often requiring cloud API use which raises privacy concerns, limits real-time applications on edge devices, and increases fine-tuning costs. Additionally, LLMs often underperform in specialized domains such as healthcare and law due to insufficient domain-specific knowledge, necessitating specialized models. Therefore, Small Language Models (SLMs) are increasingly favored for their low inference latency, cost-effectiveness, efficient development, and easy customization and adaptability. These models are particularly well-suited for resource-limited environments and domain knowledge acquisition, addressing LLMs' challenges and proving ideal for applications that require localized data handling for privacy, minimal inference latency for efficiency, and domain knowledge acquisition through lightweight fine-tuning. The rising demand for SLMs has spurred extensive research and development. However, a comprehensive survey investigating issues related to the definition, acquisition, application, enhancement, and reliability of SLM remains lacking, prompting us to conduct a detailed survey on these topics. The definition of SLMs varies widely, thus to standardize, we propose defining SLMs by their capability to perform specialized tasks and suitability for resource-constrained settings, setting boundaries based on the minimal size for emergent abilities and the maximum size sustainable under resource constraints. For other aspects, we provide a taxonomy of relevant models/methods and develop general frameworks for each category to enhance and utilize SLMs effectively. We have compiled the collected SLM models and related methods on GitHub: https://github.com/FairyFali/SLMs-Survey.", "sections": [{"title": "1 INTRODUCTION", "content": "The evolution of neural language models (LMs) from BERT's [77] pre-training and fine-tuning paradigm to T5's [250] pre-training plus prompting approach, and finally to GPT-3's [33] pre-training plus in-context learning, has greatly enhanced natural language processing (NLP). These advancements have broadened NLP's application across various fields, including language understanding [311], programming [227, 294], recommendation systems [327], information retrieval [38, 136, 204, 281], mobile-device control [80], scientific discovery [275, 379], medical question answering [30, 325], and legal question answering [10]. In particular, the recent emergence of proprietary commercial models including ChatGPT, Bard, and Claude, and open-sourced models such as Llama [84, 301, 302] has led to rapid growth in the development of large language models (LLMs). Even though neural networks consistently improve on various tasks with longer training times, larger datasets, and increased model sizes-a phenomenon known as a neural scaling law [149], these models unpredictably exhibit a sudden acquisition of versatile abilities, termed \"emergent ability,\" once they reach a critical scale threshold, thereby supporting the \"larger is better\" trend. This ability is not present in small-scale models. For instance, the latest Llama-3.1 model with 405 billion parameters [84], trained on 16K H100 GPUs for 54 days, requires about 202.5 GB of GPU memory using int4 precision and has large inference latency. These issues present several challenges in specific contexts: (1) LLMs are generally hosted in the cloud and used via cloud-based APIs due to the large GPU memory and computational cost. Users need to upload their data to query LLMs, raising data leakage and privacy concerns, especially in high-stake scenarios such as healthcare, finance, and e-commerce; (2) Driven by personal agents, on-device deployment is a critical requirement. Several factors, including cloud costs, latency, and privacy concerns, hinder the on-device processing of cloud-based LLMs, and direct deployment is impractical due to their high parameter and cache requirements, which often exceed the capabilities of devices such as mobile phones; (3) Their large parameter count can cause inference delays from seconds to minutes, unsuitable for real-time applications. For instance, Llama 2 7B takes approximately 84 seconds to process 100 tokens on benchmarks including HellaSwag, TruthfulQA, MMLU, and Arc_C when run on a smartphone equipped with a Snapdragon 685 processor [299]; (4) To boost performance in specialized domains like healthcare and law, where generic LLMs underperform, LLMs are often fine-tuned. However, this process is computationally expensive due to their large size. (5) Though general-purpose LLMs are powerful, many real-world applications require only specific abilities and domain knowledge, deploying general-purpose LLMs would be a waste of resources and such LLMs often cannot match the performance of models tailored for specific tasks [44, 112, 139, 244, 327].\nRecently, small language models (SLMs) have shown great potential in alleviating these issues while achieving performance comparable to LLMs for domain-specific problems [1, 24, 104, 128, 199, 243, 296, 299, 352, 382]. We Owing to fewer parameters, SLMs excel in efficiency, cost, flexibility, and customization. They provide significant computational"}, {"title": "2 FOUNDATIONAL CONCEPTS IN BUILDING LANGUAGE MODELS", "content": "This section will introduce foundational concepts and background knowledge that are important for language models. We will introduce the basic concepts both in architecture and the training process respectively. The advanced training strategy to improve SLM performance will be introduced in Section 3.\n2.1 Architecture of SLMs\nGenerally, the architecture of small language models (SLMs) is based on LLMs but optimized for computational efficiency and scalability. SLMs commonly employ the Transformer architecture [308] (see Figure 4), which utilizes self-attention mechanisms to manage long-range text dependencies, essential for maintaining performance with constrained resources.\n2.1.1 Transformer [308] for SLMs. The Transformer's self-attention mechanism [308] allows SLMs to efficiently capture contextual information across longer sequences, even with limited resources. SLM Transformers generally adopt an"}, {"title": "2.1.1 Transformer [308] for SLMs.", "content": "The Transformer's self-attention mechanism [308] allows SLMs to efficiently capture contextual information across longer sequences, even with limited resources. SLM Transformers generally adopt an encoder-decoder structure featuring self-attention mechanisms, feedforward networks, positional embeddings, and layer normalization.\nSelf-Attention Mechanism enables the model to evaluate the importance of tokens relative to each other. The self-attention mechanism is written as\nAttention (Q, K, V) = softmax $\\frac{QK^T}{\\sqrt{d_k}}$V\nwhere Q, K, and V are query, key, and value matrices, scaled by $\\sqrt{d_k}$ for stability where $d_k$ is the dimension of key matrices. The dot product $QK^T$ reflects the similarity between the query and key vectors.\nMulti-Head Attention (MHA) [308] is the first method that uses multiple heads to capture diverse information. MHA allows the model to attend to different parts of the input sequence using multiple attention heads as\nMultiHead(Q, K, V) = Concat(head\u2081, head\u2082, ..., head\u2095)W\u1d3c, with head\u1d62 = Attention (QW\u1d62\u1d37, KW\u1d62\u1d37, VW\u1d62\u1d65), (1)\nEach head in the Multi-Head Attention mechanism operates independently, allowing the model to capture diverse aspects of the data. The outputs are combined using learned projection matrices $W_i^Q$, $W_i^K$, and $W_i^V$, concatenated, and passed through the output projection matrix $W^O$.\nBuilding on this foundation, several modifications have been introduced to further optimize self-attention mechanisms for specific challenges such as memory efficiency and computational speed. To address the KV-cache bottleneck in MHA, Multi-Query Attention (MQA) [270] proposes that all attention heads share the same set of keys and values, which reduces the memory and computational overhead associated with storing and managing multiple key-value pairs. Grouped Query Attention (GQA) [8] serves as a middle ground between MHA and MQA. It introduces subgroups of query heads (fewer than the total number of attention heads), where each subgroup shares a single key and value head. Unlike MQA and GQA, which reduce the number of key and value heads, Multi-Head Latent Attention (MLA) [188] compresses the keys and values into a joint latent vector. This compression allows for efficient handling of key-value pairs while maintaining high performance, significantly reducing the KV-cache and improving inference efficiency. Flash Attention [67, 68] accelerates the self-attention mechanism by minimizing the memory overhead typical of standard attention calculations. This optimization allows SLMs to process longer sequences more efficiently, enhancing their functionality under strict hardware constraints.\nFeedforward Network (FFN) comprises two linear transformations separated by a non-linearity, typically modeled as FFN(x) = \u03c3(xW\u2081 + b\u2081)W\u2082 + b\u2082. where W\u2081 and W\u2082 are the weight matrices, and b\u2081 and b\u2082 are bias terms. \u03c3 is the activation function, which introduces non-linearity, allowing models to learn complex patterns. Generally, ReLU is used as the activation function. In addition to ReLU, activation functions such as GeLU and SiLU are also used in SLMs to improve performance. We give the details here: (i) ReLU (Rectified Linear Unit) [5] is defined as \u03c3(x) = max(0, x), which is commonly used for its simplicity and effectiveness. (ii) GELU (Gaussian Error Linear Unit) [121] is defined as GELU(x) = x \u00b7 \u03a6(x) = x \u00b7 $\\frac{1}{\\sqrt{2\u03c0}}\\int_{-\\infty}^{x}e^{-t^2/2} dt = x \u00b7 \\frac{1}{2}$ \u00b7 (1+erf($\\frac{x}{\\sqrt{2}}$)), where \u03a6(x) is the standard Gaussian CDF and erf is the error function. It is smoother than ReLU and widely used in models such as BERT [77] and GPT [248] for better gradient flow control. Since calculating the Gaussian error function for each neuron is computationally expensive and time consuming, there are approximations using tanh and sigmoid functions, corresponding to GELUtanh and SiLU: (iii) GELU with tanh is defined as GELUtanh (x) \u2248 0.5 \u00b7 x \u00b7 (1 + tanh $\\sqrt{x + 0.044715 \u00b7 x\u00b3)}$). This approximation uses the Tanh function to simplify computations. (iv) SiLU (Sigmoid Linear Unit) [87] is calculated as SiLU(x) = x \u00b7 sigmoid(x) = x \u00b7 $\\frac{1}{1+e^{-x}}$. It effectively combines the sigmoid function with its input, enhancing modeling"}, {"title": "Positional Embeddings in Transformer models", "content": "Positional Embeddings in Transformer models [308] are essential for capturing token order, providing context about relative positions within a sequence. Traditional positional embeddings in the Transformer architecture utilize a sinusoidal function, defined as:\nPE(pos, 2i) = sin($\\frac{pos}{10000^(2i/d_{model})}}$) PE(pos, 2i + 1) = cos($\\frac{pos}{10000^(2i/d_{model})}}$) (2)\nwhere pos represents the position within the sequence, i is the dimension index, and dmodel is the dimensionality of the model. This method alternates between sine and cosine functions across dimensions. BERT-family models [77, 195], in contrast, employ learned positional embeddings, enhancing flexibility and adaptation to various text structures. To improve the model's capacity for understanding the relative positions of tokens within a sequence, Rotary Positional Embedding (RoPE) [282] introduces a rotational matrix to the embeddings. RoPE significantly enhances the positional encoding by maintaining the relative distances through rotational transformations, thus optimizing the model's interpretative ability regarding sequence dynamics.\nLayer Normalization [165] stabilizes the training process by normalizing layer outputs, accelerating convergence. Two types of layer normalization are commonly used [165]: (i) Non-Parametric Layer Norm normalizes inputs using the mean and variance calculated across the layer's dimensions without learnable parameters as\nLN(x) = $\\frac{x - \u00b5}{\u03c3}$\nwhere \u03bc is the mean and \u03c3 is the standard deviation of the inputs. Its simplicity makes it ideal for SLMs. (ii) Parametric Layer Norm includes learnable parameters \u03b3 and \u03b2 for adaptive scaling and bias, enhancing model flexibility:\nPLN(x) = \u03b3($\\frac{x - \u00b5}{\u03c3}$) + \u03b2\nAdditionally, RMS Norm (Root Mean Square Layer Normalization) [374] simplifies the calculation by using the root mean square of inputs, reducing computational demands:\nRMSNorm(x) = \u03b3$\\\\frac{x}{\\sqrt{\\frac{1}{N}\\sum x_i^2 + \u03b5}}$ + \u03b2"}, {"title": "2.1.2 Mamba Model [69, 105].", "content": "Mamba leverages a refined version of the Transformer architecture, which includes improvements in multi-head attention and feedforward networks. Specifically, Mamba uses dynamic attention heads, which adjusts the number of active attention heads based on the complexity of the input sequence. This approach mirrors the Multi-Head Attention mechanism within the Transformer but with the added benefit of dynamically scaling the attention resources to match the task's demands. This dynamic adaptation allows Mamba to maintain high performance while minimizing unnecessary computations for simpler sequences, making it highly efficient. Additionally, Mamba incorporates adaptive feedforward networks, which align closely with the Feedforward Network architecture in Transformers. These adaptive networks dynamically adjust the depth of the feedforward layers based on the input's complexity, ensuring that computational resources are allocated efficiently. This adaptive mechanism preserves power"}, {"title": "2.2 Training SLMs from Scratch", "content": "Training SLMs from scratch entails several critical steps: (i) Pre-training, focused on acquiring general features and knowledge from the corpus; (ii) Fine-tuning, targeted at boosting the model's abilities and performance for specific tasks; (iii) Decoding strategies, which involve the methods used for iteratively selecting the next token during generation.\n2.2.1 Pretrain. Typically, pre-training paradigms for language models are divided into encoder-based and decoder-based approaches. Encoder-based models, such as BERT [77], utilize Masked Language Modeling (MLM) tasks where the goal is to predict masked tokens within a sentence. This is achieved by maximizing:\nP(masked token | context) = softmax (Whmask + b),\nwhere masked token is the original token that has been masked, context represents the other unmasked tokens in the sentence, W and b are trainable parameters of a linear output layer, hmask is the output from the transformer encoder for the masked position, and softmax is the activation function that converts logits to probabilities over the vocabulary. This process enhances the model's language encoding capabilities. Decoder-based models, such as GPT [248], employ Next Token Prediction (NTP) tasks, aiming to model the distribution of the next token by maximizing:\nP(next token | context) = softmax (W hlast + b),\nwhere next token is the token that the model aims to predict, context represents the sequence of tokens preceding the token to be predicted, and hlast is the output from the transformer encoder for the last token in the context. Effective data preprocessing, crucial for optimizing the performance of SLMs trained from scratch, involves meticulous data cleaning and strategic tokenization. Data Cleaning involves techniques such as filtering, deduplication, and noise reduction, which improve data quality and help the model generalize better. Filtering noisy or irrelevant data, addressing outliers, and handling imbalances in the dataset ensure that the training data is both representative and efficient. Deduplication, in particular, helps prevent overfitting by removing repeated instances, making the model more robust with efficient parameter usage. Tokenization Strategies play a vital role in handling diverse vocabularies without increasing model size. Advanced methods such as Byte-Pair Encoding (BPE) [95] and WordPiece [280] break text into subwords [77], allowing the model to manage rare and compound words efficiently. These strategies ensure that SLMs maintain a balance between vocabulary coverage and model compactness, crucial for improving generalization while minimizing computational demands.\n2.2.2 Fine-Tuning. After the initial training, SLMs are fine-tuned on specific tasks using task-specific data and loss functions. Parameter-efficient fine-tuning methods, such as Low-Rank Adaptation (LoRA), prefix-tuning, and adapter modules, are particularly effective for SLMs. Low-Rank Adaptation (LoRA) [127] modifies Transformer weights by introducing trainable low-rank matrices A and B for efficient fine-tuning, avoiding significant alterations to pre-trained weights. The update is represented as:\n\u0394W = AB\u1d40 (3)\nThe fine-tuned weight matrix used in Transformer operations then becomes:\nWft = W + \u03b1\u0394W (4)"}, {"title": "2.2.3 Decoding Strategies.", "content": "After pre-training or fine-tuning, employing an effective decoding strategy is crucial for generating output from language models. Decoding, the process of text generation from SLMs, involves iteratively selecting the next word. A fundamental method is the greedy search, which predicts the most likely token at each step. This is formally modeled as: xi = arg maxx P(x\u1d62 | x<i), where xi is the token with the highest probability at the i-th step, conditioned on the preceding context x<i. Other decoding strategies, such as beam search or top-k sampling, are crucial for generating high-quality outputs. Beam search balances exploration and exploitation by considering multiple possible sequences simultaneously, while top-k sampling introduces diversity and creativity in text generation. These strategies collectively ensure that SLMs are efficient and capable of delivering high performance across various natural language processing tasks."}, {"title": "2.3 Obtain SLM from LLM", "content": "Obtaining a small language model (SLM) from a large language model (LLM) is crucial for deploying in resource-constrained environments. Instead of training from scratch, leveraging an LLM allows for knowledge transfer, enabling SLMs to retain much of the LLM's linguistic and domain knowledge with reduced training time and data. To obtain SLMs from LLMs, three primary techniques are used: pruning, knowledge distillation, and quantization. Pruning removes less critical parameters, reducing model size while aiming to maintain performance. Knowledge distillation transfers knowledge from a large teacher model to a smaller student model, preserving much of the original model's understanding. Quantization decreases parameter precision, significantly lowering memory and computation needs with minimal impact on accuracy. These methods balance size reduction, efficiency, and performance retention."}, {"title": "2.3.1 Pruning.", "content": "Pruning is a technique used to reduce a model's size and computational requirements (e.g., LLMs) without significantly sacrificing its performance [113]. This process involves identifying and removing less important or redundant parameters and components from the model. The primary goal of LLM pruning is to make the model more efficient, faster, and suitable for deployment in resource-constrained environments. Typically, pruning can be categorized into two main types: unstructured pruning and structured pruning [320, 398]. An illustration of unstructured pruning and structured pruning is shown in Figure 5.\nUnstructured Pruning [70, 91, 179, 268, 285, 381, 385] prunes an LLM by removing weights individually without considering its internal structure."}, {"title": "2.3.2 Knowledge Distillation.", "content": "Knowledge distillation (KD) compresses a larger teacher model into a smaller student model by training the student to mimic the teacher's outputs [122]. This enables the student to retain much of the teacher's capabilities with fewer parameters, making it ideal for scaling down LLMs for resource-limited environments while maintaining performance. KD can be categorized into white-box and black-box approaches [320, 353, 398] as shown in Figure 6. In White-Box KD, the student has access to the teacher's internal states or output distributions [6, 106, 140, 151, 155, 236, 377]. Generalized Knowledge Distillation (GKD) [155] introduces skew KL divergence to stabilize gradients and enhance performance, using an adaptive off-policy approach to minimize noisy feedback and improve efficiency. Black-Box KD relies only on teacher outputs without having access to model internals [41, 240, 319]. Methods like Distilling Step-by-Step [126] use teacher-generated rationales to train smaller models, improving"}, {"title": "2.3.3 Quantization.", "content": "Quantization reduces the storage and computational demands of LLMs by converting floating-point representations into lower-precision formats, significantly cutting both storage requirements and computational complexity. Existing methods fall into two categories: Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT). Figure 7 illustrates the two quantization methods. Post-Training Quantization, applied after training, simplifies model compression without altering the architecture or requiring retraining, though it may result in precision loss. Consider a group or block of weights w; the linear operation can be expressed as y = wx, while the quantized version is given by y = Q(w)x. Generally, the quantization function Q is defined as [184]:\nQ(w) = A \u00b7 Round ($\\frac{w}{\u0394}$), \u0394 = $\\frac{max(w)}{2^N-1}$\nwhere N is the number of quantization bits, and A is the quantization scale factor determined by the absolute maximum value of w. Quantization-Aware Training (QAT) enhances LLM efficiency by incorporating quantization directly into the training process, often resulting in higher accuracy compared to PTQ. During QAT, the forward pass utilizes"}, {"title": "2.3.4 Low-Rank Techniques.", "content": "Low-rank techniques compress LLMs by approximating high-dimensional weight matrices with two lower-dimensional matrices, reducing computational and memory requirements. A matrix W of size m \u00d7 n is approximated as W \u2248 A \u00d7 B, where A is m \u00d7 r and B is r\u00d7n, with r much smaller than m or n, significantly reducing the number of parameters. Building on this concept, Ji et al. [141] propose a low-rank compression method tailored for LLMs, leveraging the observation that while LLMs have high-rank weights, their feature interactions tend to exhibit low-rank properties. The method estimates feature distributions using pooled covariance matrices and allocates distinct compression ratios to layers based on their sensitivity to low-rank compression. A Bayesian optimization strategy, using a Gaussian process as the surrogate model, optimizes the allocation of low-rank dimensions, ensuring the model maintains performance while achieving significant compression. Transitioning from model compression to fine-tuning, Cho et al. [54] tackles system and data heterogeneity with the HETLORA method, which uses heterogeneous low-rank approximations to accommodate the diverse capabilities of clients and data complexities. By combining local rank self-pruning with sparsity-weighted aggregation, it balances high and low-rank LoRA modules, improving convergence speed and performance compared to uniform approaches."}, {"title": "2.4 Comparison between Different Compression Methods", "content": "To summarize the key differences between pruning, knowledge distillation, quantization, and low-rank techniques for model compression, we present a comparative table highlighting their definitions, goals, advantages, disadvantages, and typical use cases, as shown in Table 2."}, {"title": "3 ADVANCED ENHANCEMENT STRATEGIES FOR SMALL LANGUAGE MODELS", "content": "With the foundational concepts introduced in Section 2, this section explores various advanced techniques that enhance the performance of SLMs, including innovative training methods for training SLMs from scratch, supervised fine-tuning (SFT) to align SLMs to adhere to instructions, advanced knowledge distillation and quantization techniques, and techniques frequently used in LLMs such as mixture-of-experts to enhance SLM for specific applications. A summary of enhancement techniques is also summarized in Table 3.\n3.1 Innovative Training Methods for Small Language Models from Scratch\nIn scenarios with limited resources, we aim to train small language models to provide efficient, cost-effective solutions tailored for specific domains, while still maintaining competitive performance with larger models. Training small language models (SLMs) from scratch involves unique strategies that diverge significantly from those used for large language models (LLMs). This section synthesizes cutting-edge techniques tailored to optimize the inherent capabilities of SLMs, underscoring their potential to match or surpass larger counterparts in efficiency and effectiveness. As shown in Figure 8, the methods for training SLMs from scratch can be categorized into three primary categories: Architecture Design, Data Construction, and Optimization Strategy. Next, we introduce each category in detail.\nArchitecture Design for SLMs When designing SLM architectures, parameter-sharing techniques are employed to minimize space usage and reduce the model's size. As shown in the first part of Figure 8, parameter sharing is achieved by two approaches: (i) a single Feed-Forward Network (FFN) module is shared by every transformer layer, and (ii) entire transformer blocks are shared. For example, to design language models for scenarios that require on-device processing ability and energy, FFN layer sharing/reusing (see Figure 8 (1)) can be a potential option. By sharing FFN layers, the model can maintain a smaller size while still benefiting from the depth and complexity gained through repeated processing of input data. This technique is firstly applied in MobiLlama [299] which surpasses the performance of existing SLMs of comparable size. Generally, deeper and thinner models consistently perform better than shallower and wider ones [199]. Based on this observation, Transformer Block-wise Sharing is another parameter-sharing approach that maintains depth and complexity. There are different transformer block-wise sharing strategies such as repeating the transformer blocks all over again or repeating the immediate transformer block. Experiments show"}, {"title": "Insights", "content": "We draw several key insights from the training techniques of SLMs:\n\u2022 For parameter sharing techniques, maintaining complexity and depth of model structure is essential for maintaining model performance (e.g., shared Feed-Forward Networks [299] and transformer blocks) [199].\n\u2022 Data quality is more important than data quantity in the effectiveness of SLMs [359].\n\u2022 Different from LLMs, we can take advantage of the compact nature of SLMs and employ more flexible training strategies, such as multiple-round training [291]."}, {"title": "3.2 Supervised Fine-Tuning (SFT) for Enhancing SLM performance", "content": "Supervised Fine-Tuning (SFT) employs a training methodology similar to pre-training but is specifically tailored to align models to adhere to the instructions encapsulated within various instructional datasets. This approach is designed to refine the model's responsiveness and appropriateness to given contexts as dictated by the training data. For example, various models, such as Alpaca [292], UltraChat [79], WizardLM [346], SlimOrca [181], ShareGPT [315], Capybara"}, {"title": "3.3 Data Quality in Knowledge Distillation (KD)", "content": "Transitioning from the discussion on training SLMs from scratch, this section delves into the critical role of data quality in Knowledge Distillation (KD). The motivation here is to highlight how high-quality data generated from LLMs can significantly enhance the learning efficiency and performance of SLMs. The central idea is that meticulously crafted datasets when used in KD, enable SLMs to more effectively mimic the advanced capabilities of their larger counterparts. As shown in Figure 10, the data can come either from (1) other strong LLMs (e.g., GPT-4 [2]) which are much larger and more powerful than the target SLM, or (2) the target SLM itself.\nAugment Data from Other Models. Due to the limitations of model size, studies have shown that training SLMs requires simple and comprehensible data [86, 163, 340]. TinyStory [86] demonstrates that language models with a relatively small number of parameters (tens of millions) can still generate coherent stories tailored for children aged 3-4"}, {"title": "Data Quality in Knowledge Distillation", "content": "This is achieved by prompting GPT-3.5 or GPT-4 [2] to create simple and easily understandable stories from three keywords selected from a foundational vocabulary of 1,500 words. The generated stories are then used to train SLMs, enabling them to produce similar narratives. This approach shows that varied and comprehensible data can help smaller models exhibit behaviors similar to those of larger language models, such as obeying scaling laws and achieving enhanced performance. Many efforts to enhance the Chain-of-Thought (CoT) capabilities of small models involve using LLMs to generate high-quality CoT data. These data are then employed to train small models in an end-to-end fashion to mimic the CoT reasoning process [210, 340]. AS-ES Learning [340] argues that previous methods often overlook the limited capacity of small models to learn complex reasoning, despite being provided with very detailed reasoning processes. Even these detailed processes still require more nuanced capabilities, such as extraction and abstraction.\nTherefore, this study introduces a novel training paradigm that categorizes reasoning steps into extractive segments, which remind the model of the context and set the stage for subsequent conclusions, and abstractive segments that infer additional insights not explicitly stated in the context.\nAugment Data from Itself. Besides distilling data from other LLMs, language models can also train on their own outputs [25, 131, 300]. Since voting strategies can improve the performance of LLMs, reasoning paths that lead to the majority answer can be further utilized to fine-tune LLMs [131]. Similarly, SLMs can generate their training data with the aid of existing rationale generation methods. Self-Amplify [25] notes that human annotation of Chain-of-Thought (CoT) data is very time-consuming; thus, automated rationale generation methods have been proposed. These methods involve three main steps: (1) Selection of samples (x, y) that the model predicts correctly as few-shot examples; (2) Rationale generation, where rationales are produced using post hoc explanation methods; (3) Prompt design for SLMs, where the final prompt is crafted based on the previously generated rationales."}, {"title": "3.4 Distillation Techniques for Enhancing SLM Performance", "content": "Following the discussion on data quality in KD, this section reviews specialized KD training strategies designed to enhance the performance of SLMs. The motivation is to address the unique challenges and constraints involved in"}, {"title": "3.5 Performance Improvement through Quantization", "content": "As mentioned in Section 2, quantization is one of the most effective methods for adapting LLMs to SLMs. However, compression to smaller sizes often compromises performance. To address the performance drop associated with quantization, various methods have been proposed. This section examines how these quantization methods specifically enhance the performance of SLMs. While the general introduction to compression methods is discussed in the compression section, the focus here is on detailing those approaches that boost the efficiency and effectiveness of SLMs. As shown in Figure 7, we categorize these quantization methods into two main approaches: Post-Training Quantization (PTQ), where quantization is conducted on a well-trained fixed model, and Quantization-Aware Training (QAT), where quantization is integrated into the training process. This section introduces advanced techniques in PTQ and QAT respectively.\nPost-Training Quantization (PTQ) primarily includes weight quantization and activation quantization. Weight quantization aims to quantize model parameters while preserving performance. GPTQ [92] compresses LLMs to 4-bit or 2-bit by quantizing weights layer-by-layer to minimize layer-wise quantization errors. PB-LLM [267], applicable to both PTQ and QAT, retains the most salient weights while binarizing the rest based on magnitudes. BiLLM [133], another PTQ method, uses a Hessian-based metric to identify salient and non-salient weights. Salient weights undergo binary residual approximation to minimize loss, while non-salient weights are divided into sparse and concentrated groups for separate binarization, reducing quantization errors. Activation quantization faces challenges with outliers that can stretch the quantization range, causing most values to cluster at few bits and introducing significant errors. To address this, LLM.int8() [74] isolates outlier features for 16-bit processing and handles the rest in 8-bit. SmoothQuant [342] circumvents per-channel quantization issues by employing a \"smoothing\" technique that shifts the quantization challenge from activations to weights through a per-channel scaling transformation. This balance between activating and weight"}, {"title": "3.6 Techniques in LLMs Contributing to SLMs", "content": "To enhance the performance of LLMs, various techniques such as Retrieval-Augmented Generation (RAG) and Mixture of Experts (MoE) are employed. This section discusses their potential to maintain or improve the performance of SLMs within constrained computational budgets. However, effectively integrating these advanced techniques into SLMs, which have inherently limited capabilities, remains an open challenge.\nRetrieval Augmented Generation (RAG) enhances the capabilities of language models in knowledge-intensive tasks by incorporating a retrieval mechanism. This approach allows models to access relevant contextual information from a data repository in response to user queries. By integrating this retrieved data, RAG-equipped models gain a better understanding of specific topics, enabling more informed and accurate outputs. For SLMs, a significant concern"}, {"title": "1 APPLICATIONS OF SMALL LANGUAGE MODELS", "content": "In this section, we delve into the applications of small language models (SLMs) across various NLP tasks and their deployment strategies. Due to benefits such as enhanced privacy, faster inference, and lower memory requirements, many NLP applications are now leveraging SLMs over LLMs, employing specialized techniques to enhance SLM performance. Additionally, deploying SLMs often involves considerations of memory and runtime efficiency, which are crucial for optimizing resource use on budget-constrained edge devices, particularly mobile phones. Then, we will discuss task-specific applications of SLMs and their deployment methods on mobile and edge devices.\n4.1 Task-specific SLM Applications\nThis subsection explores the diverse NLP tasks to which SLMs can contribute. Question-answering and coding represent generative tasks, while recommender systems and web search (though not strictly within the NLP domain) typically leverage the encoding capabilities of SLMs. Additionally, the application of SLMs on mobile devices is particularly well-suited due to constraints in memory and computing resources. The representative works are systematically organized in Table 4.\n4.1.1 SLM Applications in Question-Answering. Question-answering (QA) is a fundamental task in the NLP field, demanding language models to exhibit abilities in understanding language, reasoning, common sense, and recalling specialized knowledge. Typically, larger language models yield better QA performance. However, the substantial size"}, {"title": "SLM Applications", "content": "of these models introduces challenges such as immense computational requirements", "areas": "i) Instruction Tuning of Generic SLMs for QA, (ii) Instruction Tuning of Domain"}]}