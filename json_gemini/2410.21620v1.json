{"title": "Asynchronous Tool Usage for Real-Time Agents", "authors": ["Antonio A. Ginart", "Naveen Kodali", "Jason Lee", "Caiming Xiong", "Silvio Savarese", "John Emmons"], "abstract": "While frontier large language models (LLMs) are capable tool-using agents, current Al systems still operate in a strict turn-based fashion, oblivious to passage of time. This synchronous design forces user queries and tool-use to occur sequentially, preventing the systems from multitasking and reducing interactivity. To address this limitation, we introduce asynchronous Al agents capable of parallel processing and real-time tool-use. Our key contribution is an event-driven finite-state machine architecture for agent execution and prompting, integrated with automatic speech recognition and text-to-speech. Additionally, we present the first dataset tailored for instruction-tuning LLMs for asynchronous tool-use. Drawing inspiration from the concepts originally developed for real-time operating systems, this work presents both a conceptual framework and practical tools for creating AI agents capable of fluid, multitasking interactions.", "sections": [{"title": "1 INTRODUCTION", "content": "With the advancement of large-scale foundation models [Tea22] into the terabyte realm, AI models have become sufficiently capable to function as tool-using agents [Tea24]. A crucial component in training these base models to user instructions is supervised instruction tuning [Tea24, OWJ+22, PLH+23]. In addition, the integration of tool-use datasets serves to further enhance the capabilities of these agents [ZLZ+24]. Despite these advancements, there is, to our knowledge, an underlying assumption that persists in the design of leading Al agents and the instruction datasets they are trained on: these agents are inherently synchronous. This synchronous nature becomes evident through user interactions, such as watching a loading spinner when an agent employs a tool. Giving agents the ability to (1) manage multiple concurrent processes in real-time, and (2) asynchronously respond to the user as soon as any process finishes will significantly reduce perceived delay and meaningfully improve user experience.\nConsider the following hypothetical scenario of a voice call with an Al travel concierge (see Fig. 1). At a glance, this interaction may seem uninteresting. However, consider that preparing a detailed travel itinerary might be a multi-step and multi-tool task that may take, say, 30 seconds to fully prepare. Thus, it is preferable for the concierge to quickly verbally reply rather than wait to respond until the itinerary is ready. Furthermore, the user asked for a second tool-use request while still waiting on the first. While humans have the ability to maintain multiple concurrent thought processes simultaneously, current frontier agents are fully serial\u00b9. When a serial agent is in the middle of processing a user request that involves long-running tools or reasoning chains, the user has no choice but to abort the request or wait for it to finish before being able to submit a subsequent query."}, {"title": "2 BACKGROUND & RELATED WORKS", "content": "We provide some background, with a focus on more fundamen-tal and seminal works on the systems side (given that we adopt classically established systems paradigms) and more contempora-neous, bleeding-edge works on the generative machine learning side (given that this field is dynamic and advancing rapidly)."}, {"title": "2.1 Asynchronous Computer Systems", "content": ""}, {"title": "2.1.1 Asynchronous Execution", "content": "Asynchronous execution, a cor-nerstone of our proposed framework for AI agents in tool envi-ronments, has its roots in the seminal work of Dijkstra (1965) on cooperating sequential processes [Dij02]. Hoare [Hoa78] offered a formal framework for describing and analyzing asynchronous systems, which informs our approach to designing AI agent inter-actions in asynchronous environments."}, {"title": "2.1.2 Polling v. Interrupt-Based Concurrency", "content": "In developing our conceptual framework for AI agents in asynchronous tool environ-ments, we draw upon the long-standing debate between polling and interrupt-based approaches to concurrency. [Han73] offers insight into this trade-off in the context of operating systems. Lampson and Redell's [LR80] work on processes and monitors in Mesa offers insights into the trade-offs impacting a system's responsiveness to asynchronous events, such as automatic speech recognition inputs. However, perhaps the most influential and relevant prior work is the Robot Operating System (ROS) [K+17], which opts for an event-driven concurrency model."}, {"title": "2.1.3 Real-Time Systems", "content": "Real-time computing systems, and op-erating systems in particular, is an established branch of com-puter engineering that encompasses systems building for real-time timing requirements in a concurrent environment with in-tegrated peripheral sensors and actuators. We include references to recent surveys [HVJ14, SR04, Meg05, DNJN11] and classical texts [Rog01, RDJ+09, LL73, SR89]."}, {"title": "2.2 Generative AI", "content": ""}, {"title": "2.2.1 Large Action Models and Tool-Use", "content": "Over the past three years, as LLM model weights and training data grew from the gigabyte to terabyte scale, LLM use-cases expanded from simple chatbots to helpful copilots all the way to now, tool-using autonomous agents. A recent study introduced xLAM, a series of large action models specifically designed for AI agent tasks, ranging from 1B to 8x22B parameters [ZLZ+24]. The xLAM models demonstrate exceptional performance across multiple agent ability benchmarks, notably out-performing GPT-4 and Claude-3 in function calling tasks, and aim to advance open-source LLMs for autonomous AI agents. APIGen, an automated pipeline for generating high-quality function-calling datasets, leverages over 3,600 APIs across 21 categories for data generation [LHZ+24]."}, {"title": "2.2.2 Multi-Agent Systems", "content": "Multi-agent systems in the context of generative Al have seen significant advancements in recent years.rary challenges and opportunities in developing AI systems that can effectively collaborate. The literature on cooperating multi-agent Al systems is vast, but some particularly important or rel-evant works from the before the foundation model era include [TOK\u016017, YVV+22, Kra97, Jen95, DKJ18].\nMore recently, the advent of generative foundation models has ushered in a new era of AI coordination amongst LLM-based agents. Development of platforms like AutoGen [WBZ+23] provides prac-tical tools for implementing and experimenting with multi-agent Al systems, facilitating research and applications in this rapidly evolving field.\nDiversity Empowered Intelligence (DEI) is framework that lever-ages the diverse expertise of multiple software engineering agents to enhance problem-solving capabilities. Experimental results demon-strated that a DEI-guided committee of agents significantly outper-formed individual agents [ZYL+24].\nAgentLite is lightweight open-source library designed to sim-plify the development and evaluation of LLM-based agent sys-tems [LYZ+24]. AgentLite offers a task-oriented framework that enhances agents' ability to break down tasks and facilitates multi-agent system development, providing researchers with a user-friendly platform for innovating LLM agent reasoning strategies and archi-tectures.\nUnlike prior work, our work is far more focused on the asynchro-nous and real-time aspects of LLM-based agents rather than the cooperation and coordination aspects of multi-agent systems. Many of the ideas in the prior multi-agent systems literature are in fact quite complimentary and could be translated to the asynchronous setting. Other relevant works on multi-agent generative systems include [GCW+24, LZL+23, GHL+24, TN23, Don24, ZYB+24]."}, {"title": "2.2.3 Speech Models", "content": "Speech models have undergone remarkable advancements in recent years. The introduction of Whisper [RKX+23] marked a significant milestone in automatic speech recognition (ASR), demonstrating robust performance across multiple languages and accents.\nIn the domain of text-to-speech (TTS), models like VALL-E 2 [CLZ+24] have shown impressive capabilities in voice cloning and style transfer, requiring only short audio samples. Voicebox [LVS+24] has demonstrated state-of-the-art performance in var-ious speech generation tasks, including noise removal, content editing, and cross-lingual style transfer. NaturalSpeech [TCL+24] approaches human-level quality in speech synthesis.\nThese advancements are paving the way for more natural and versatile speech interfaces in Al systems, with potential applications ranging from virtual assistants to accessibility tools."}, {"title": "2.2.4 Spoken Dialogue Systems", "content": "Spoken dialogue systems have seen significant advancements in recent years, particularly in the areas of dialogue state tracking and dialogue management, due to rapid development in natural language processing powered by LLMs. [Wah23, GKD+24, Jan23] provide details on recent develop-ments in this subfield."}, {"title": "3 REAL-TIME AGENTS", "content": "The proposed framework for real-time agents combines an asyn-chronous execution environment with a markup language prompt-ing specification. Similar to a software-hardware divide, as long as the LLM produces valid generations according to the specification, the environment will asynchronously enqueue function calls\u00b3 and handle chat interactions through speech-to-text and text-to-speech peripherals.\nThe asynchronous execution environment is, at its core, an event-driven finite state machine [Har87], augmented with priority sched-uling (via priority queue) [MJ60]. We refer to this core component of the execution environment as the dialog system, comprised of the dialog FSM and the scheduling queue. The LLM generation, context management, and function calling are handled by the dispatcher, which is comprised of the dispatch language model and the ledger, which acts as the single-source-of-truth for the dispatch LM's con-text. Messages are appended to the ledger atomically, generally as a consequence of the dialog FSM processing the schedueling queue, but there are exceptions, such as for handling user interruptions. Refer to Fig. 2 for a schematic of the execution environment with speech-to-text (STT) and text-to-speech (TTS) I/O. While real-time voice agents are an important motivation, they are not the only use case for real-time asynchronous tool usage, so the STT and TTS messages could be replaced with input and output text streams."}, {"title": "3.1 Architecture", "content": ""}, {"title": "3.1.1 Clock Awareness", "content": "One important aspect of our paradigm arises from the usage of timestamps in messages, clock update messages at discrete time intervals (for example, every 5 seconds)"}, {"title": "3.1.2 Parallel Thought Processes: Fork vs. Spawn Semantics", "content": "We propose a technical definition of a parallel thought process. Analagous to a parallel programmatic process in a traditional software sense, parallel thought process is an concurrent instance of the asyn-chronous execution environment with parent-child semantics. The child's input stream is populated by function calls from the parent and the child's output stream populates the function responses to the parent. Concretely, parallel thought processes are created via either fork or spawn calls. For a fork call, the parent initializes the child's ledger with a copy of its own and appends a new message containing further instructions for the child. For a spawn call, the parent initializes the child with a new ledger and populates the first message containing the child's instructions. It is up to the parent thought processes to determine if a fork or a spawn call is more appropriate on a case-by-case basis, given that there are clear trade-offs for each type of parallel thought process. Forking uses more context in the child and therefore could be more expensive while also including potentially unnecessary or distracting messages, so it should only be used if the child needs a full view into the parent's context in order to achieve its goal. By default, spawning is probably preferable for most cases, since the parent can usually summarize the relevant details into the child's instructions. By default, the dispatch LM could be the same as the parent, but in principle, nothing prevents the parent from prescribing a different dispatch LM (as long as it's fine-tuned or prompted to correctly handle prompt tem-plate as expected by the environment). For both fork and spawn, we have a third reserved tool, kill, that the dispatching LM can use to interrupt and terminate parallel thought process. As is im-plied by these function call semantics, there is the possibility of recursive creation of parallel thought processes, which could be used to dynamically organize multi-agent hierarchies at runtime. Of course, custom-built tool calls can also serve this purpose."}, {"title": "3.1.3 Event Processing", "content": "The dialog system implements an event-driven FSM with priority scheduling. An event will contain a pri-ority level and (potentially) cause a state transition to occur. Some events also contain messages to be appended to the ledger. Events can be produced by:\n(1) The STT (or input peripheral), when the user begins speak-ing or finishes speaking\n(2) The dispatcher, when the dispatch LM begins or finishes generation\n(3) The TTS (or output peripheral), when the output stream begins and finishes emitting\n(4) The function caller, when a tool-use request is sent or a response is received\nIn order to ensure that the dialog FSM state variable accurately re-flects the overall system, state transitions push from the dispatcher and the peripherals should essentially always have the minimum possible priority, -\u221e, in order to ensure they are processed in-stantly. Alternatively, the dispatcher and the peripherals can use a locking mechanism to atomically update the state variable when appropriate (essentially skipping the scheduling queue altogether). Function call responses, on the other hand, should never use such a priority, and instead should use a developer defined priority, always going through the scheduling queue. push events to the schedul-ing queue based on on internal processing or state change. As a concrete example, the execution environment must ensure that the TTS is halted if the interrupt event is pushed by the STT subsystem because the user starts to speak."}, {"title": "3.2 Peripherals", "content": "We use the open-source Pipecat framework to integrate our LLM with STT and TTS peripherals. This framework enables real-time voice recognition and synthesis, and helps handle user interrup-tions. It does not support asynchronous tools, however, which we implement in our system. LLM generations are streamed into a sentence aggregator and then processed phrase-by-phrase by the TTS. We observe end-to-end latency of <300ms, which is enough to mimic human conversation."}, {"title": "3.2.1 Speech-to-Text", "content": "There are various suitable choices for STT services, both among the open-source packages and APIs. We found Deepgrams to be a good API option. We also obtained low-latency STT with an optimized implementation of Whisper Turbo6."}, {"title": "3.2.2 Text-to-Speech", "content": "We found that TTS had a slightly higher vari-ation in latency and quality among leading open-source packages and APIs. We preferred the Sonic API for its speed and quality."}, {"title": "3.3 Execution", "content": "OpenAI introduced chat markup language (ChatML) as the original prompting template for fine-tuned chat models. ChatML originated as a specific markup language that implement an abstract data type (ADT), but, interestingly, it was the ADT implied by ChatML that has actually caught on as the universal standard interface to chat models for developers across all major APIs. Today, the ChatML interface enables developers to more easily manipulate LLM context without resorting to boilerplate string manipulations."}, {"title": "3.3.1 Prompt Template and Context Management", "content": "We review the standard practice for synchronous chat model context, here denoted by C. Precisely, context C is a list of length l of message dictionar-ies, mi, with each message dictionary containing role (role) and content (content) fields.\nC = ((role\u2081, content\u2081), ..., (role, contente))\nwhere role; \u2208 {system, assistant, user} and content; \u2208 \u03a3* where \u2211 denotes the token vocabulary for the language model (including the empty string). We do not impose any restrictions about the number or order of the roles in the context. For example, we allow back-to-back user messages.\nIn the asynchronous case, the ledger, L is similarly a list of mes-sages, (rolei, content\u012f). We include a set of tools, denoted T, modeled via tool-use function f : JSON \u2192 \u03a3*, which inputs valid JSON, selects and executes the corresponding tool in T with the corresponding arguments, and outputs a response string. We also include certain reserved tools, including fork, and spawn and kill. We include a new, fourth role, notification that is managed by the the asynchronous execution environment. It will also be useful to define a set of hash strings H.\nIf role = user\ncontent\u2081 = (timestamp\u2081, chati)\nwhere timetamp; \u2208 N (or some other suitable timestamp format) and chat; \u20ac \u03a3*.\nIf role assistant\ncontent\u2081 = (thought, function, chati)\nwhere thought; \u2208 \u03a3* and function; \u2208 List[JSON] is a list of valid JSONS.\nIf role = notification\ncontent = (sourcei, timestampi, datai)\nThe source field where source; \u2208 Tx H and thought; \u20ac \u03a3*. We do not change the content or format for system messages.\nLike with synchronous chat markup language, the choice of special tokens and details of the prompting template to serialize and deserialize the abstraction is specific to the language model's vocabulary.\nFrontier models now offer context lengths upwards of 128k tokens, which can enable voice conversations with tool calls lasting up to 30 minutes or more. Context length and memory management beyond this is an interesting topic which we do not delve into here."}, {"title": "3.3.2 Instruction Set", "content": "We describe how the asynchronous execu-tion environment should handle messages. Like an instruction-tuned chat model, it is the dispatch LM's burden to ensure it gener-ates valid messages with respect to the instruction set."}, {"title": "3.3.3 Interruption Handling", "content": "At any given moment in time, the FSM is in one of four states with respect to the execution environ-ment: idle, generating, emitting, or listening. The implementation of the execution environment as a whole is responsible for ensure the FSM state is accurately reflected in the true state of the system.\nFor example, if the TTS is streaming output (either voice or text) to the user, then the FSM is emitting. Otherwise, if it is generating tokens but not emitting, then it is generating. If the user is in the process of creating input (for exampling, speaking), then the FSM is listening. Else, the FSM is said to be idle.\nInterruptions are a first-class feature in an asynchronous agent. We explicitly include interruptions as part of the proposed instruc-tion set. The scheduling queue allows for the environment to en-force atomic updates to the ledger despite the concurrency. Every 5 seconds, the system queues up a time passage notification message. All messages have a priority. As example defaults, user mes-sages are priority -1 and assistant messages are priority 1 (al-though this can be configured on a per-deployment basis).\nThe key difference in how interruptions are handled in the gen-erating and emitting states is that for a tie in priority level, the interrupt occurs if the dispatch LM generating but does not if it is emitting.\nTool definitions should include a priority, but the default priority for request-sent and response-received messages is 1.\nThe execution environment is responsible for correctly book-keeping the ledger during interruptions. Consider the following example.\nMessage: l\nrole = assistant\nchat = \"Blah blah blah  |>\"\nMessage: 1 + 1\nrole = notification\nsource = system\ndata = \"Assistant interrupted due to user speaking\"\nMessage: 1 + 2\nrole = user\nchat = \"I am interrupting you.\"\nWe add a special interruption token, as depicted above. Note that, due to the pipeline nature of the asychronous execution environ-ment, there is a time delay between token generation and the output stream (for example, via a TTS module). The system is responsible for making sure the number of blah accurately reflects how many the assistant said before interruption. In this case, the LLM may have generated a token stream of \"Blah blah blah blah\" while the TTS has only managed to emit \"Blah blah blah\". In this case, the environment is responsible for reconciliation between these two streams and should only update the ledger to reflect the actual output emitted to the user. This issue is only present in the chat component of the LLM generation, since thought streams do not trigger downstream post-processing and therefore can be interrupted naively. For function call streams, the environment could choose to either atomically include a function call only if it was generated to completion, or simply interrupt a partial func-tion call string. In either case, the dispatch LM should rely on a request-sent message to indicate the completion of the function call request."}, {"title": "4 DISCUSSION", "content": "Results. In the supplementary materials, we share some sample recordings of voice calls with both the fine-tuned Llama 3.1 and GPT-40 as dispatch LMs. Qualitatively, we fine that fine-tuning helps ensure the dispatch LM consistently generates valid ledger messages according to the prescribed instruction set.\nPipelined vs. Multi-modal Systems. Multi-modal foundation mod-els are an active and promising area of research and development, and are highly relevant to real-time agents, particularly for voice I/O. A foundation model with both text and speech modalities could exhibit tighter integration with peripherals and possibly even fur-ther reduce latency or improve speech quality.\nIn our implementation, we opted for a pipelined system, in which we had independent neural networks for the STT, Dispatch LM, and TTS, which text as the I/O medium between the subnets. How-ever, one could have also used a multi-modal foundation model (as demonstrated by OpenAI). Our architecture is well-suited for such a foundation model, with the exception that the STT and TTS components would not include a neural component, but rather, would simply include a speech tokenizer and vocoder component, respectively. In this case, speech tokens would be the I/O medium between these components.\nReasoning for Time-Constrained Tasks. One intriguing possibility that is enabled after endowing the agent with a sense of clock aware-ness is the ability for the user to request a time-constrained task. For example: Complete a research report on state-of-the-art text-to-speech models within the next 10 hours. Within the proposed framework, the agent's sense of clock awareness enables it to coordinate long-running chain-of-thought (or other multi-step reasoning methods) with multiple iterations of tool-use across, potentially, multiple other task-specific agents while keeping the entire process on a reasonable schedule based on the allotted time limit.\nExploring specific prompt templates and fine-tuning strategies for powering this kind of clock-aware reasoning with time con-straints is a fertile topic for future work.\nConclusion and Future Work. This work introduces an architec-ture for real-time, asynchronous Al agents capable of fluid, real-time interactions. By implementing an event-driven finite-state machine with asynchronous tool usage and parallel thought processes, we enable more natural and responsive AI interactions, particularly in voice-based applications.\nBroadly speaking, we envision a future in which agent LMs are fine-tuned to for precise instruction set specifications which can be executed in any environment supporting said instruction set. Future work should explore integration with multi-modal LMs and seamless integration with multi-agent systems and long-running task-specific agents."}]}