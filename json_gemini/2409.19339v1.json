{"title": "Visual Question Decomposition on Multimodal Large Language Models", "authors": ["Haowei Zhang", "Jianzhe Liu", "Bailan He", "Volker Tresp", "Zhen Han", "Zhiqiang Xu", "Shuo Chen", "Jindong Gu"], "abstract": "Question decomposition has emerged as an effective strategy for prompting Large Language Models (LLMs) to answer complex questions. However, while existing methods primarily focus on unimodal language models, the question decomposition capability of Multimodal Large Language Models (MLLMs) has yet to be explored. To this end, this paper explores visual question decomposition on MLLMs. Specifically, we introduce a systematic evaluation framework including a dataset and several evaluation criteria to assess the quality of the decomposed sub-questions, revealing that existing MLLMs struggle to produce high-quality sub-questions. To address this limitation, we propose a specific finetuning dataset, DecoVQA+, for enhancing the model's question decomposition capability. Aiming at enabling models to perform appropriate selective decomposition, we propose an efficient finetuning pipeline. The finetuning pipeline consists of our proposed dataset and a training objective for selective decomposition. Finetuned MLLMs demonstrate significant improvements in the quality of sub-questions and the policy of selective question decomposition. Additionally, the models also achieve higher accuracy with selective decomposition on VQA benchmark datasets.", "sections": [{"title": "Introduction", "content": "Answering complex questions is a challenging task, especially when the questions require implicit multi-step reasoning to answer. Question Decomposition (QD) is an effective strategy to address this issue. Most related work studies the efficacy of QD with unimodal textual large language models (LLMs) in enhancing complex textual question answering tasks (Patel et al., 2022; Dua et al., 2022; Zhou et al., 2023; Qi et al., 2023). Although some recent works (You et al., 2023; Qi et al., 2023) have explored question decomposition within the context of visual question answering (VQA) tasks, they follow the paradigm of performing unimodal QD based on the image caption. Typically, they conduct a two-step process: first, generating a caption for the image using a captioning model, and then performing question decomposition using an unimodal textual LLM based on the complex question and the generated image caption. Relying solely on the image caption instead of the image itself may lead to significant information loss.\nRecent advancements in Multimodal Large Language Models (MLLMs) have enabled MLLMs to directly perceive image information for answering questions. Yet, how to perform QD on complex visual questions using such MLLMs has been less explored. In the following, we refer to question decomposition using MLLMs on VQA as Visual Question Decomposition (VQD). In this work, we primarily explore the following research questions:\n\u2022 How can we quantitatively assess the VQD ability of MLLMs? How proficient are existing MLLMs in VQD, or specifically, how is the quality of sub-questions generated by MLLMS?\n\u2022 How can we enhance the VQD ability of MLLMs and enable the models to properly determine when to decompose and when not to, facing questions with varying difficulties?\nTo assess the question decomposition capability of MLLMs, a significant obstacle is the absence of metrics for evaluating models' question decomposition abilities. Recent work (You et al., 2023; Qi et al., 2023) evaluates the model's question decomposition ability by measuring the final answer's"}, {"title": "Related Work", "content": "Question decomposition has shown impressive capabilities in improving the reasoning performance of language models. Successive Prompting (Dua et al., 2022) and Least-to-Most Prompting (Zhou et al., 2023) are two representative works that break a complicated question into simpler ones iteratively. Decomposed Prompting (Khot et al., 2023) introduces a modular setup of question decomposition, which makes it easy to optimize prompts, pre-trained models and symbolic functions for different sub-tasks. Additionally, question decomposition is capable of increasing the reasoning faithfulness while achieving the accuracy improvement (Radhakrishnan et al., 2023).\nRecent studies have explored the potential of VQD. IdealGPT (You et al., 2023) leverages LLMs iteratively to raise sub-questions and determines the final reasoning answer. Socratic Questioning (Qi et al., 2023) utilizes LLMs to generate sub-questions and answer them, stimulating robust recursive thinking. However, all of these studies rely on the reasoning ability of language models, overlooking the visual information that images can bring to question decomposition. The literature most relevant to our work is (Khan et al., 2023), which explored prompting MLLMs to answer VQA questions with question decomposition in the zero-shot settings. However, it only applies VQD as a prompting technique and evaluates whether VQD could enhance the VQA accuracy. That work does not delve into the quality of generated sub-questions along the entire reasoning process, which does not explicitly analyze how well the questions are decomposed."}, {"title": "2.2 Multimodal LLMS", "content": "To address the modality gap, MiniGPT-v2 (Chen et al., 2023a) and LLaVA-1.5 (Liu et al., 2024) apply a linear connection layer to connect the frozen pre-trained vision module and the language model. Besides, they provide a task-oriented instruction training pipeline to decrease instructional ambiguity across various vision-language tasks. GPT-4 with Vision (GPT-4V) (OpenAI et al., 2024) is a powerful MLLM based on instructing GPT-4 and has shown outstanding capability on diverse benchmarks. Besides instruction-following ability, Qwen-VL (Bai et al., 2023b) series outperforms in a range of vision-language tasks, supporting multilingual conversations and dialogues involving multiple interleaved images. Furthermore, InternVL-1.5 (Chen et al., 2024b) introduces a strong vision encoder, dynamic high-resolution images, and a high-quality bilingual dataset to enhance the comprehensive capability of MLLMs further. While these models have demonstrated impressive performance across various benchmarks (Bai et al., 2023b), they continue to struggle with complex tasks that require advanced reasoning (Khan et al., 2023). Numerous techniques, such as parameter-efficient tuning methods like prompting (Gu et al., 2023), in-context learning (Alayrac et al., 2022), and chain-of-thought reasoning (Zhang et al., 2022), can be used to help models handle unseen or complex tasks, but each has its limitations. Parameter-efficient tuning is vulnerable to robustness issues when dealing with out-of-distribution inputs (Chen et al., 2024a), in-context learning often fails to fully utilize multimodal information, focusing predominantly on text (Chen et al., 2023b), and chain-of-thought reasoning is prone to adversarial attacks (Wang et al., 2024). In contrast, our approach employs question decomposition, breaking down complex queries into simpler, more manageable sub-questions, which enhances the models' reasoning capabilities."}, {"title": "3 How well can MLLMs decompose questions?", "content": "Existing works commonly use the accuracy of the final answer to demonstrate a model's ability to decompose questions. However, this evaluation method is imprecise and implicit. As shown in Figure 1, a MLLM generates sub-questions with low quality, yet can still provide a correct answer. However, these ineffective sub-questions fail to provide the expected assistance in answering the original question and do not help the model's reasoning process. To address this, we differentiate"}, {"title": "4 Enhancing MLLM's Visual Question Decomposition Capability", "content": "Given that the existing MLLMs have poor performance on VQD, this section further explores how to improve the VQD ability of MLLMs. An intuitive method to enhance the decomposition performance of MLLMs is to finetune the models on a dataset tailored for VQD. Specifically, we need a dataset to finetune the models, which exclusively focuses on complex questions with high-quality sub-questions in the view of Non-Repetition, Relevance, and Groundedness. However, there does not exist such a public VQA dataset. Therefore, we propose a specialized dataset, termed DecoVQA, to improve the VQD ability. Furthermore, for effective VQD, models also need to have an improved ability to decide when to decompose questions. We also explain the finetuning pipeline with our proposed dataset and a novel training objective to achieve that goal in detail, as discussed below."}, {"title": "4.1 Dataset Construction of DecoVQA", "content": "Question Selection & Decomposition Annotation In our exploration of question decomposition for VQA in Table 1, we recognize that not all questions in existing benchmark datasets necessitate decomposition for answering. Many questions are straightforward and can be addressed without employing a decomposition strategy. Our focus, therefore, is on questions that demand complex reasoning, making them suitable candidates for the decomposition annotation. For this purpose, we have selected A-OKVQA and VQA-Introspect as our primary data source, as these two datasets contain complex questions requiring external knowledge and visual reasoning to answer respectively.\nTo identify appropriate samples from A-OKVQA and VQA-Introspect, we adopt specific pre-selection strategies, shown in Appendix F.1 detailedly. After that, we conduct a manual review and pick 200 samples from pre-selected samples. Then we manually annotated these samples with logical sub-questions. The details of the annotation process are shown in Appendix F.2.\nDataset Statistics After decomposition annotation, we collected 100 samples from A-OKVQA and 100 samples from VQA-Introspect with high-quality sub-questions from the perspective of Non-Repetition, Relevance, and Groundedness. To prevent overfitting and catastrophic forgetting, we manually picked out another 100 samples from A-OKVQA and 100 samples from VQA-Introspect, which are simple and VQD doesn't contribute to higher performance for them. These simple samples are added to DecoVQA in the form of direct answering. Overall, DecoVQA has 400 balanced samples in total."}, {"title": "4.2 DecoVQA+", "content": "To enhance the capability of MLLMs in selective decomposition, we add an extra QA round on the basis of DecoVQA to enable the models to learn when to decompose properly, facing questions with various difficulties. This extra QA round contains a query asking the models if they would directly answer without any decomposition, given an image and a question. The labels for simple questions are \"yes\" while the ones for complex questions with human-annotated sub-questions are \"no\". We demonstrated the full prompt of a training sample in Figure 6. We refer to this upgraded version as"}, {"title": "4.3 Training Objective", "content": "It is intuitive to finetune MLLMs to improve models' performance on VQD. However, directly applying the conventional next-token prediction loss (NTP loss) on the finetuning for selective VQD may not be appropriate. To improve MLLMs' capability of identifying the questions that need to be decomposed, we propose a training objective, SelectiveVQD Loss, combining the NTP loss and a binary cross entropy loss (BCE loss). The BCE loss aims to penalize the errors in deciding whether to decompose, compared to the labels given in each sample of DecoVQA+.\nWhen the model is asked whether it would perform VQD, we firstly find the specific token position for \"yes\" or \"no\" in the sentence, select the logits of these two specific tokens in that position, i.e. \"yes\" and \"no\", and then transform these two logits into probabilities through softmax:\n$P(yes) = P(\\hat{w}_s = \\text{\"yes\"} | w_s \\in {\\text{\"yes\"}, \\text{\"no\"}}) = \\text{Softmax}(\\text{logit}(w_s = \\text{\"yes\"}), \\text{logit}(w_s = \\text{\"no\"}))$\n$P(no) = P(\\hat{w}_s = \\text{\"no\"} | w_s \\in {\\text{\"yes\"}, \\text{\"no\"}}) = 1 - P(yes),$ (1)\nwhere s is the specific token position in the sentence for \"yes\" or \"no\" and ws is the specific token of \"yes\" or \"no\".\nWe compute the BCE loss between these two probabilities, and compute the cumulative NTP loss across all conversation rounds for each sample:\n$BCELoss = [y_s\\text{log}P(\\text{yes}) + (1 - y_s)\\text{log}(1-P(\\text{yes}))]$ (2)\n$NTPLoss = \\sum_{i=1}^M -\\text{log}P(W_i = w_i | w_{i-1}, ..., W_1),$ (3)\nwhere ys is the binary label indicating whether a specific sample needs decomposition or not. Wi denotes the i-th token in the ground truth sentence while wi denotes the predicted i-th token and M is the number of tokens of the prediction.\nThe final combined SelectiveVQD Loss is a weighted sum of both NTP loss and BCE loss:\n$SelectiveVQDLoss = \\sum_{j=1}^N (\\lambda \\cdot NTPLoss_j + \\omega \\cdot BCELoss_j),$ (4)\nwhere j denotes the j-th training sample and N denotes the total number of training samples. The NTP loss is computed for the entire training sample, and the BCE loss focuses specifically on determining whether to decompose the given question in the"}, {"title": "Experiments", "content": "With SubQuestRater, we compare the VQD ability of four popular MLLMs: MiniGPT-v2, LLaVA-1.5, Qwen-VL-Chat, and InternVL-Chat-V1-5 before and after finetuning on our proposed datasets. We finetune all these MLLMs on DecoVQA, DecoVQA+, and DecoVQA+ with SelectiveVQD Loss to see the improvement in their VQD capability. Additionally, we evaluate the improvement in VQA accuracy and the models' capability to appropriately determine when to decompose questions through finetuning.\nThe finetuning datasets include DecoVQA and DecoVQA+. DecoVQA+ adds an extra QA round based on DecoVQA, asking models whether to decompose questions before decomposition. As for used evaluation datasets, we assess the VQD capability of MLLMs before and after finetuning on the proposed evaluation dataset in SubQuestRater, which contains 200 complex questions. The prompt for evaluating the VQD ability is shown in Figure 5. Besides, we evaluate the VQA accuracy on A-OKVQA, GQA (Hudson and Manning, 2019), and VQA-Introspect, containing complex reasoning questions (please refer to Appendix D.1 for more statistical details). As for A-OKVQA and VQA-Introspect, the subsets of data used for inference are different from the subsets selected for constructing our finetuning datasets, preventing the problem of data leakage. We also evaluate the accuracy on Whether2Deco to test whether MLLMs are able to determine when to decompose questions properly. The prompt for evaluation experiments on accuracy is under selective VQD setting, which is in the same form of samples in DecoVQA+ shown in Figure 6."}, {"title": "5.2 Quantitative Evaluation", "content": "The quantitative evaluation involves two parts: evaluation of decomposed sub-questions under SubQuestRater framework and accuracy comparison on VQA datasets and Whether2Deco. The finetuning is efficient based on the dataset with a small number of samples. The supplementary evaluation on MMBench (Liu et al., 2023) in Appendix I shows our finetuning does not hurt the all-around performance of MLLMs, but even slightly improves the comprehensive performance in many aspects."}, {"title": "5.3 Qualitative Evaluation", "content": "In this subsection, we will use several examples to visually illustrate the changes of sub-questions before and after finetuning with DecoVQA+. Figure 4 shows that the quality of the sub-questions has indeed been significantly improved after finetuning. The sub-questions generated by finetuned models are not repetitive, relevant to the original question and grounded, instead of ineffective decomposition or low-quality sub-questions originally. More case studies are shown in Figure 15."}, {"title": "6 Conclusion", "content": "This paper systematically investigates VQD capabilities on MLLMs. We propose a systematic evaluation framework for VQD, SubQuestRater, including a dataset and evaluation metrics to quantitatively measure the generated sub-questions by MLLMs. SubQuestRater is applied to popular MLLMs and we find that they are inadequate to produce high-quality sub-questions. To enhance the capability of MLLMs to decompose questions, a specialized dataset DecoVQA with human-annotated sub-questions is proposed. To further improve the ability to perform selective VQD, we propose a training pipeline with an upgraded dataset DecoVQA+ and a novel training objective. Finetuned MLLMs demonstrate significant improvement in the quality of generated sub-questions and the policy of whether-to-decompose. Additionally, the models also achieve higher VQA accuracy under selective VQD through finetuning on our proposed datasets."}, {"title": "Limitations", "content": "The main limitations in our work include: 1) Question Decomposition can be extended into complex task decomposition for an agent (multiple sub-tasks), leaving it as future work. 2) We apply finetuning to increase MLLM's VQD ability, which requires the model's detailed parameter information. Thus, community users could not apply our method for enhancing closed-source MLLMs."}]}