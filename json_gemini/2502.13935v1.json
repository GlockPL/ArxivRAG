{"title": "Continually Learning Structured Visual Representations via Network Refinement with Rerelation", "authors": ["Zeki Doruk Erden", "Boi Faltings"], "abstract": "Current machine learning paradigm relies on continuous representations like neural networks, which iteratively adjust parameters to approximate outcomes rather than directly learning the structure of problem. This spreads information across the network, causing issues like information loss and incomprehensibility Building on prior work in environment dynamics modeling, we propose a method that learns visual space in a structured, continual manner. Our approach refines networks to capture the core structure of objects while representing significant subvariants in structure efficiently. We demonstrate this with 2D shape detection, showing incremental learning on MNIST without overwriting knowledge and creating compact, comprehensible representations. These results offer a promising step toward a transparent, continually learning alternative to traditional neural networks for visual processing.", "sections": [{"title": "1. Introduction", "content": "Modern machine learning relies on neural networks to model complex systems, achieving remarkable success in areas like image recognition and NLP (Khan et al., 2021; Zhao et al., 2023), yet as these problems are solved, crucial shortcomings at their core start to gain attention (Clune, 2019; Zador, 2019; Marcus, 2018; LeCun, 2022). Importantly, they struggle with continual learning-the ability to learn new tasks without forgetting previous ones, as essential for open-ended learning, reflecting real-world dynamics. This limitation necessitates frequent, resource-intensive retraining, making scalability impractical. Furthermore, the overparameterized, unstructured design of neural networks has prioritized performance over comprehensibility, limiting trust, control, and adaptability in complex environments. The lack of comprehensibility in AI is a critical concern as systems grow more capable. Neural networks' opaque nature makes their internal workings hard to understand, refine, or integrate with structured systems. This opacity hinders human validation, corrective adjustments, and the incorporation of explicit knowledge. Without these capabilities, using Al in critical decision-making poses risks absent in more transparent engineering domains.\nAn alternative exists: we can retain the power of advanced learning systems while prioritizing structure, transparency, and control by focusing research on these goals. This is essential for building adaptable, human-aligned systems suited for real-world complexities. Prior work (Erden &\nFaltings, 2024a; 2025b;a) has demonstrated the viability of frameworks that incorporate minimal, yet rich, representations of data to achieve continual learning and interoperability on simple environment dynamics modeling, offering a clear path toward systems that align more closely with human understanding. In this paper, we expand on these advancements by proposing a novel framework to apply these foundational ideas to the domain of observation spaces that can be represented as networks, in particular to visual information processing. Our approach refines these principles to create a continual and structured learning process for vision, emphasizing the capture of essential object structures while hierarchically organizing frequently observed subvariants. This method ensures that knowledge is continually learned without disrupting existing representations and creates compact, directly human-comprehensible representations. It further lays the groundwork for applying the broader framework from (Erden & Faltings, 2025a) to visual domains, supporting structured representations for modeling environment dynamics.\nWe demonstrate the potential of our approach with a proof-of-concept shape detection task on 2D objects. The model learns incrementally from the MNIST dataset, retaining knowledge while incorporating new information without task boundaries or replay, and generates structured, human-comprehensible representations. This showcases the potential of this type of approach as a transparent alternative to traditional neural networks, capable of learning continually by design, hence addressing key challenges in machine learning in general as well as for visual processing."}, {"title": "2. Related work", "content": "Two most important core limitations of current ML systems are the inability of continual learning and incomprehensibility of internal structure; problems often tackled in isolation (Kirkpatrick et al., 2017; Rusu et al., 2016; Jacobson et al., 2022; Hadsell et al., 2020; Zhuang et al., 2020; Xu et al., 2019). Solutions proposed for these problems often don't fully resolve the fundamental limitations of NNs in this regard but aim to mitigate their effects.\nMany continual learning methods rely on simplifying assumptions, such as externally defined task boundaries and task change information (Rusu et al., 2016; Jacobson et al., 2022), or storing and replaying past observations (Buzzega et al., 2020), which bias learning toward previous tasks without enabling true continual learning (Kirkpatrick et al., 2017). These issues also affect visual problems (Qu et al., 2021), where even state-of-the-art techniques depend on replay buffers (Galashov et al., 2023) or task boundaries (Wang et al., 2022). One exception is methods generating multiple experts and assigning tasks based on a generative model (Erden & Faltings, 2024b; Lee et al., 2020). While powerful, these methods still assume distinct tasks by decomposing the system into experts, relying on the assumption that bulk data is available for each expert's generative model. This assumption holds in offline experimentation but not in continual learning, where data is distributed over time and not available in bulk. Thus, the continual learning problem shifts to the generative model for task identification rather than the entire system. Additionally, none of these methods address model comprehensibility alongside continual learning, which shares the same root cause.\nCurrent approaches to making learned models comprehensible, categorized under Explainable AI methods (Xu et al., 2019; Kashefi et al., 2023) offer post-hoc explanations for neural network operations but fail to address the core incomprehensibility of their internal structures, leaving them hard to trustfully validate and engineer. Visual processing with neural networks faces the same issue (Kashefi et al., 2023). Moreover, comprehensibility is often treated separately from continual learning, rather than as a shared challenge. Studies that consider both tend to propose distinct mechanisms for each problem (Roy et al., 2022) or focus on specific explainability issues arising from certain continual learning methods (Rymarczyk et al., 2023; Cossu et al., 2024).\nThese limitations are inherent to neural networks and similar methods, and attempting to address them within the same paradigm either proves impossible or leads to excessive methodological complexity. Recent work has proposed varsel mechanisms (Erden & Faltings, 2025a;b), which learn via local variation and selection, mimicking biological adaptation (Marc, 2005; Gerhart & Kirschner, 2007). The method, Modelleyen (Erden & Faltings, 2024a; 2025a;b), enables continual learning without task boundaries, task change information, or replay buffers, while also creating an inherently comprehensible structure by learning with minimal complexity at each level. This approach addresses both continual learning and explainability/comprehensibility by tackling their shared root cause: nonstructured representations."}, {"title": "3. Method", "content": ""}, {"title": "3.1. Background: Modelleyen and Varsel mechanisms", "content": "Previous studies ((Erden & Faltings, 2024a), (Erden &\nFaltings, 2025b), (Erden & Faltings, 2025a)) introduced\nModelleyen as an example of proposed varsel mechanisms,\nwhich learn through component-level topological variation\nand selection to model external environments or tasks. This\ncontrasts with methods like neural networks that rely on\nfine-tuning continuous parameters without explicitly learn-\ning network topology. Due to space constraints, we provide\nonly a brief overview here; for full details, see (Erden &\nFaltings, 2024a; 2025b;a) and Section A.1 for the formal\nlearning algorithm from these papers.\nThe learning core of the varsel network in Modelleyen\nconsists of conditioning (state) variables (CSVs), which,\nalong with raw observations and target predictions, form the\nmodel's building blocks, akin to neurons in neural networks.\nThe method in (Erden & Faltings, 2024a; 2025a) exhaus-\ntively connects CSVs with other state variables (represent-\ning raw observations or their discrete activation dynamics)\nbased on observed data at each time instant, then refines\nthese connections with further observations (see Fig. 6 in\nAppendix). Higher-order CSVs are formed upstream (Fig. 7\nin Appendix) to predict downstream CSV states when con-\nflicts arise, enabling the representation of arbitrary logical\nfunctions. Theorem 1 in (Erden & Faltings, 2024a) proves\na strong continual learning property: a CSV's response to\na past instance 20 remains consistent even after structural\nmodifications from new instances xi, provided no nega-\ntive sources formation occurs in the meantime. Although\nsystem-wide performance retention is not guaranteed due\nto mechanisms like suppressive connections or component\nremovals, this still ensures local information retention and\nconsistency, providing a potent guarantee of knowledge\npreservation with system-wide reflections verified experi-\nmentally (Erden & Faltings, 2025a).\nPrevious work validated Modelleyen on low-dimensional\nfinite state machines; yet highlighted the need for exten-\nsions to handle higher-dimensional and structured observa-\ntion spaces, particularly for operating on networks rather\nthan lists of state variables (Erden & Faltings, 2025a). Our\nmethod extends this approach, adapting Modelleyen's prin-\nciples to process visual observations, as detailed in the fol-"}, {"title": "3.2. Continually learning structured visual representations", "content": "We aim to extend the framework from (Erden & Faltings,\n2025a) to operate on networks as observations, rather than\nraw state variables. Networks can represent diverse do-\nmains, particularly spatial and temporal observations rele-\nvant to AI. Our focus is on vision, specifically 2D shape\nidentification, though the approach is generalizable to any\nnetwork-representable observation provided an appropriate\nrepresentational conversion is used. The method is agnostic\nto how a 2D image is converted into a network. We detail\nour experimental feature representation in Section 3.3, but\nfor this section, the reader can think about a generic concept\nof a visual feature that can refer to edges, colors, gradi-\nents, objects, or even raw pixels. We refer to this design as\nModelleyen with network refinement (MNR for short)."}, {"title": "3.2.1. REPRESENTATIONAL BASIS", "content": "First, let's define the basis for our representation of obser-\nvations and input sources for conditioning state variables\n(CSVs) in this extension:\nDefinition 3.1. A state network (SN) is a directed graph\n(N, E), where each node N has an associated type. A\nlist of tuples of state networks and associated keys, P =\n[(ko, SNo), (k\u2081, SN1), ...] is called a state polynetwork\n(SPN).\nNode types in state networks (SNs) represent distinct fea-\ntures (e.g., edges, corners, or objects in visual space), with\nnodes being observed instances of these features in the cur-\nrent observation (e.g., two edges with the same orientation\nor two instances of the same object are distinct nodes of\nthe same type; see Figure 2a for an example). Edges (E)\nrepresent relations between nodes, such as relative posi-\ntions in visual inputs or succession in temporal domains.\nA state polynetwork (SPN) is a collection of distinct state\nnetworks with a designator key, enabling the definition of\ndifferent feature and relation types. In visual space, this\ncould include shape, color gradients, or abstract objects, as\nwell as multi-dimensional relations (e.g., relative position-\ning). An example SPN is shown in Figure 3c (its means of\nconstruction from images is detailed in the next section).\nSPNs will serve as input sources for CSVs in our model,\nreplacing the sets of state variables in (Erden & Faltings,\n2024a; 2025a). Learning the model involves constructing\nSPN structures that capture the desired information, bring-\ning us to the question of how an SPN (representing a CSV's\ninput configuration) should be modified in response to new\nSPN observations."}, {"title": "3.2.2. NETWORK REFINEMENT WITH RERELATION", "content": "Refinement, the core learning process in Modelleyen (Er-\nden & Faltings, 2024a; 2025a), reduces two lists of state\nvariables (sources for a CSV) to their intersection, retaining\nonly the subset necessary to activate the CSV. An analogous\noperation is needed to identify the shared part of two or\nmore state polynetworks (SPNs), forming the basis for the\nmodel-level learning flow (discussed in Sec. 3.2.3).\nFor that purpose, we make the following definition:\nDefinition 3.2. An SPN Po = [(ko, SN\u00ba), (k1, SN\u00b0),\n...(kn, SN\u00ba)] is satisfied by another SPN P\u2081 =\n[(ko, SN\u2081), (k\u2081, SN\u2081), ...(kN, SN\u33a2)] (with the same set\nof keys K = [ko, k1, ...kv]) given a potentially partial as-\nsignment f: V(Po) \u2192 V(P1), where N(Pi) is the set of\nall nodes across all state networks of Pi, if and only if the\nfollowing conditions hold:\n1. For \u2200no \u2208 N(Po), f(no) is defined (has a mapped\nnode in N(P1)), and\n2. For Veo = (no, n\u2081) \u2208 E(SN\u00ba) where E(SN) is the\nset of all edges in state network SN\u00ba in Po, there exists\na path in SN from f (no) to f(n\u2081).\nIntuitively, Po is satisfied by P\u2081 under an assignment if\nevery node in Po has a corresponding target node in P1, and\nevery edge in Po has a path in P\u2081 connecting the assigned\ntargets of its endpoints within the same SN. This ensures\nthat all entities and relations in Po are present in P\u2081, even\nif mediated by additional entities not in Po (as paths, not\ndirect edges, are required).\nWe can now redefine \"finding the intersection\" of two SPNS\nPo and P\u2081 as \"minimally refining Po to be satisfied by P\u2081.\"\nThis is achieved through network refinement with rerelation,\nwhere Po (source) is refined by P\u2081 (refiner). The process,\ndetailed in Algorithm 1, relies on two subprocesses:\n\u2022 Refinement: Nodes in Po that are missing in P\u2081, and\nedges in SNs of Po that don't have a path between their\nendpoints in the corresponding SN of P\u2081, are removed.\n\u2022 Rerelation: When an edge (no, n\u2081) is removed (includ-\ning via node removal), a new edge (pi, si) is created\nfor Vpi \u2208 P(no), si \u2208 S(n\u2081), where P(n) and S(n)\nare predecessors and successors of node n respectively.\n(Each edge formed by rerelation is checked for the\nsame conditions of presence as existing ones.)\nFigure 1 illustrates this process: the source SPN in la is\nrefined by the refiner in 1b, resulting in the refined SPN in\n1c. Paths like (A, D) or (A, C') are preserved despite differ-\ning intermediaries. Applying this process sequentially to a"}, {"title": "3.2.3. LEARNING FLOW", "content": "As in (Erden & Faltings, 2024a), our system design defines a model using conditioning state variables (CSVs), which describe relationships between their sources (state polynetworks, or SPNs) and targets (other CSVs or specific target state variables). The learning flow largely follows Modelleyen's approach (see (Erden & Faltings, 2025a) and Algorithms 2 & 3 in the Appendix for details), except for the following adjustments:\n1. Unlike (Erden & Faltings, 2024a; 2025a), where upstream CSVs integrate with lower-level CSVs via an and condition, our implementation treats upstream CSVs as observed subvariants of their CSV targets, with source SPNs encompassing the source SPNs of them. This allows assignments from lower-order CSVS to propagate upstream, eliminating redundant assignments and simplifying the learning flow by avoiding the need for additional subnetwork definitions.\n2. Unlike (Erden & Faltings, 2025a), which creates a common CSV for all targets in a step, we assume a single target per CSV and create separate CSVs (with identical sources) for each target. This change supports the upstream assignment propagation in previous point.\n3. Instead of embedding negative (suppressing) sources within a CSV, we externalize them into separate CSVs. A negatively-conditioning CSV is formed when a state variable with an inactive state and no active negative conditioner is observed (with potentially multiple formed per target). We also redefine the unconditionality flag to deactivate upon the first observation of"}, {"title": "3.3. Feature representation for basic shape learning", "content": "Modelleyen with network refinement (MNR) is applicable to any observation space representable as networks. For our experiments, we focus on demonstrating the method in a basic visual processing domain: 2D shape identification in binary images, using MNIST as the test domain. This task is foundational in computer vision and has historically served as a starting point for approaches like neural networks (LeCun et al., 1998; Cortes, 1995). Below, we detail the feature representation (image-to-network conversion) used. We note that this is only demonstrative for the use of our approach in a simple context, but future work can extend it to other types, including 2D features like color gradients or 3D features with spatial positions, as the domain requires.\nTo ensure generality in shape detection and avoid overly hand-crafted features, we use horizontal (x) and vertical (y) gradient orientation change points. Our image processing flow involves: (1) converting a grayscale image to binary with a 50% threshold, (2) approximating contours as polygons using OpenCV's Ramer-Douglas-Peucker algorithm (OpenCV, 2025), yielding corners and oriented edges (CW for outer, CCW for inner contours) (Figure 3b), and (3) computing the sign of gradients at each corner in the x and y dimensions based on edge orientation (e.g., a rightward-facing edge has a negative x-axis gradient).\nWe build the final SPN using corners where gradient orientations change in the x or y axes. Traversing the contour in its connected direction (CW or CCW), we create a node for each corner if the gradient direction (positive or negative)"}, {"title": "4. Experimental setup", "content": "We experiment on MNIST dataset, with the aim to (1) show continual learning performance of MNR, and contrast it with the learning progression of neural networks, and (2) investigate learned representations of classes by MNR for proper structure and comprehensibility.\nOur MNR learning process involves randomly selecting Ne"}, {"title": "5. Results and discussion", "content": ""}, {"title": "5.1. Continual learning", "content": "Figure 4 shows learning progression of MNR for Nc = 3, 4, and 5 classes; as well as that of the neural network variants for Nc = 3 for comparison.\nMNR's final performance, as expected, does not achieve perfect identification, with accuracies of 85%, 60% and 50% for Nc = 3, 5 and 10 respectively after 10 cycles. This stems primarily from limitations of feature representation (Section 3.3) and while it suggests the need for improvement with better representations (see Conclusions), it is not our main focus here. To validate continual learning of our design, we focus on MNR's high retention of learned information, as shown in Figures 4a, 4d, and 4e. Performance on class i remains stable in later iterations (j > i) of the same cycle, with early accuracy persisting throughout. This contrasts sharply with neural networks: a fully connected NN (Fig. 4b) loses all information on class j > i in early cycles, and even after 10 cycles, it fails to retain a stable representation, showing > 50% accuracy loss. A convolutional NN performs worse, losing all information repeatedly.\nContinual learning is most critical in early cycles (first 3-4), as in the long run, with increasing number of cycles, the problem is equivalent to stochastic gradient descent with a slow timescale, reducing the problem to statistical learning with data abundance where NNs already excel. MNR's retention of performance is consistent across tests with 5 and 10 classes as well, albeit with lower baseline accuracies.\nWe note that in MNR, as in Modelleyen (Erden & Faltings, 2025a), even when there are small performance fluctuations, it is by design not due to direct destruction of existing information (unless a conditioner is removed for cumulative insignificance) but stems from over-refinement or negative conditioning.\nWe briefly note the factors limiting performance compared to the perfect detection achieved by neural networks. First, the representation used lacks full expressivity, capturing only gradient change points rather than all shape features (see Section 3.3). Second, the current statistical refinement approach retains some features not present in every sample of a class, yet SPN satisfaction (Definition 3.2) requires precise matches. This leads to missed instances, especially outliers. This is rather straightforward to offset by allowing soft satisfaction of SPNs (Definition 3.2). These limitations were not the focus of this work, which prioritized validating the learning flow with a demonstrative representation, and will be addressed in future research."}, {"title": "5.2. Comprehensibility of learned representations", "content": "Figure 5 illustrates samples of the learned SPNs, ranging from general representations at lower depths (near the target variable) to more specific ones at higher depths capturing rarer subvariants. These representations are visually intuitive, effectively depicting the digits, their features, and interrelations. For instance, most contours of digit \"2\" are preserved in Fig. 5a, though features like holes at the lower-left turning point (common in some samples like that in Fig. 3) are omitted, while persistent features, such as the vertical gradient change (\"cx_yneg-ypos_1\"), are retained.\nSimilarly, digit \"5\" in Fig. 5b retains key features, including vertical gradient changes on the right and horizontal changes at the top and bottom, along with correct positional relations. While general contours of \"5\" are refined at depth 0, they are preserved at the more specific subvariants upstream, like in Fig. 5c which provide more details. Some additional examples are also provided in Figure 8 in Appendix."}, {"title": "6. Conclusions, Limitations and Future Work", "content": "This work extended prior research on structured representations for continual learning, focusing on higher-dimensional observation spaces that can be represented as networks, particularly in visual domains, without constraining assumptions and ensuring model comprehensibility. To that end, we introduced the mechanism of learning via network refinement with rerelation in Modelleyen learning flow, applied it to 2D shape identification with a proposed feature representation, and demonstrated its functionality. While final performance didn't match NNs due to limitations in feature expressivity, our core design aims were validated by (1) achieving continual learning without constraining assumptions, retaining past knowledge consistently, and (2) producing representations easily comprehensible by humans. This method also lays the groundwork for the integration of a means to process structured visual spaces into the broader framework of modeling environment dynamics and deliberative behavior in (Erden & Faltings, 2025a).\nThe current feature representation is a simple, demonstrative approach based on gradient sign changes, specifically designed for 2D shape detection. While it captures many key features, it is expressively limited, as evidenced by the performance shortfalls. Future work should focus on enhancing this expressivity. One option is to refine the edge-gradient-based method, which theoretically contains all the necessary shape information and hence remains a strong candidate. Alternatively, well-established computer vision methods, such as SIFT (Lindeberg, 2012) or pretrained neural networks like visual foundation models ((Oquab et al., 2023)), could be used for feature representation while retaining the MNR flow for high-level learning. Another promising direction is the use of frequency components (Xu et al., 2020), which are especially well-suited to this learning framework. Pixel-level detection, similar to NNs, could also be considered if scalability challenges are addressed, though this might unnecessarily complicate the model and reduce its interpretability. Intermediate representations, such as fixed-size filters similar to those used in CNNs, are another possibility. These approaches could improve the model's performance and extend its applicability beyond 2D shape identification.\nThe current implementation of our method is computationally demanding due to redundancies in modelling included for methodological simplicity, as optimization was not our focus at this stage. Upstream CSVs are represented as larger networks encompassing their downstream targets instead of distinct, integrated subnetworks, and each CSV is restricted to a single output, preventing shared conditioning sources. These design choices, while easy to modify, inflate computational demands. Furthermore, current learning flow processes multiple intersecting upstream paths simultaneously, which could be streamlined by handling only the \"best-matching\" path at each step, narrowing the upstream network being processed. Although the model in Modelleyen can grow as large as needed, operations should ideally involve only a small set of CSVs, with their combined source SPNs no more complex than the SPN representing given observation. This would cap per-step computation"}, {"title": "Algorithm 1 Network refinement with rerelation.", "content": "Function-RefineBy(P0, P1, f)\nParameters: Po, source SPN. P\u2081, refiner SPN. f, a partial\nassignment between nodes in Po to nodes in P\u2081.\n1: for SN \u2208 Po do\n2:  for n \u2208 nodes(SN) do\n3:   if f(n) not defined then\n4:    for (no, n\u2081) \u2208 edges(SN, n) do\n5:     RemoveWithRerelation(SN, no, \u03b71)\n6:    end for\n7:   SNO.RemoveNode(n)\n8:   end if\n9:  end for\n10:  for (no, n\u2081) \u2208 edges(SN) do\n11:   if path(f (no), f(n\u2081)) not in SN then\n12:    RemoveWithRerelation(SN, no, n1)\n13:   end if\n14:  end for\n15: end for\nFunction-RemoveWithRerelation(SN, no, n\u2081)\n1: for (p, s) \u2208 prod(Psn(no), SSN(n1)) do\n2:  SN.AddEdge(p, s)\n3: end for\n4: SN.RemoveEdge(no, n\u2081)"}, {"title": "3.2.2. NETWORK REFINEMENT WITH RERELATION", "content": "Definition 3.2. An SPN Po = [(ko, SN\u00ba), (k1, SN\u00b0), ...(kn, SN\u00ba)] is satisfied by another SPN P\u2081 = [(ko, SN\u2081), (k\u2081, SN\u2081), ...(kN, SN\u33a2)] (with the same set of keys K = [ko, k1, ...kv]) given a potentially partial assignment f: V(Po) \u2192 V(P1), where N(Pi) is the set of all nodes across all state networks of Pi, if and only if the following conditions hold:\n1. For \u2200no \u2208 N(Po), f(no) is defined (has a mapped node in N(P1)), and\n2. For Veo = (no, n\u2081) \u2208 E(SN\u00ba) where E(SN) is the set of all edges in state network SN\u00ba in Po, there exists a path in SN from f (no) to f(n\u2081)."}, {"title": "4.  Instead of the complex NCC metric in (Erden & Faltings, 2025a), we filter insignificant condition-ers by removing a conditioner C of target T if P(SS(C)|I(T)) < Tsign, where SS(C) and I(T) represent the satisfaction of C's sources and the obser-vation of T's state, respectively.", "content": ""}, {"title": "A.2. Additional illustrations", "content": "Figure 8 presents some additional learned representations in the runs during our experiments presented in the main text."}, {"title": "A.3. Experimental details", "content": "We use Nsample = 20, 10, 5 and test set size of 50, 20, 10 samples per class for experiments with\nNc = 3, 5, 10 respectively. Reported results are averages of 10 runs for Nc = 3 and 5 runs for Nc = 5. Population size\nfor generating assignments was chosen as 10 for both learning and prediction.\nFor MNR, we choose refinement threshold Tref = 0.05, significance threshold Tsign = 0.05. \u20ac for\npolygonal approximation is 0.01L where L is the arc length of contour being approximated."}, {"title": "Algorithm 2 From (Erden & Faltings, 2025a). Pseudocode of the main Modelleyen adaptation loop; formed of state computations followed by CSV generation for unexplained SVs.", "content": "Parameter: N Set of all target nodes\nFunction ProcessEnvironmentStep(observations)\n1: BSV States \u2190 observations\n2: ComputeDSV States() {Computes DSV states by BSV events}\n3: for level \u2208 reverse(ComputationLevels) do\n4:  for CSV \u2208 SVsin(level) do\n5:   ComputeState(CSV)\n6:  end for\n7: end for\n8: UnexplainedSVs \u2190 [SV : SV.state = 1 and NoConditioner Active(SV)]\n9: sources \u2190 [SV : SVin [BSVs, DSVs] and SV.state = 1 and is Eligible(SV)]\n10: NewCSV = CreateCSV(sources, [SV : SV in UnexplainedSVs and TargetEligible(SV)])\n11: ModelRefinement() {Removes CSVs with no source or target}"}, {"title": "Algorithm 3 From (Erden & Faltings, 2025a). Pseudocode for CSV state computation.", "content": "Function ComputeState(CSV)\n1: if AnySourceActive() then\n2:  Separate ActiveInactiveTargets() {Creates two CSVs from current one with active and inactive targets in either of them}\n3:  if AnyTargetObserved() then\n4:   State = 1\n5:   PosSources \u2190 [source : source in PosSources and source.state = 1]\n6:   NegSources \u2190 [source : source in NegSources and source.state! = 1]\n7:  else if AnyTargetInactive() then\n8:   if not(AllSources Active()) then\n9:    State = 1\n10:   else\n11:    if AnyNegativeSourceActive() then\n12:     State = 0\n13:     NegSources \u2190 [source : source in NegSources and source.State = 1]\n14:    else\n15:     State = -1 {No negative source active to explain inactivity of targets}\n16:    end if\n17:   end if\n18:  end if\n19: else\n20:  State = 0 {Unobserved if targets are not observed}\n21: end if\n22: if State = -1 then\n23:  if NegativeConnectionsFormed then\n24:   FormNegativeConnections()\n25:  else\n26:   unconditionality = \"isConditional\" {-1 for }\n27:  end if\n28: end if"}]}