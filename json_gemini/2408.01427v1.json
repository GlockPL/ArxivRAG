{"title": "Siamese Transformer Networks for Few-shot Image Classification", "authors": ["Weihao Jiang", "Shuoxi Zhang", "Kun He"], "abstract": "Humans exhibit remarkable proficiency in visual classification tasks, accurately recognizing and classifying new images with minimal examples. This ability is attributed to their capacity to focus on details and identify common features between previously seen and new images. In contrast, existing few-shot image classification methods often emphasize either global features or local features, with few studies considering the integration of both. To address this limitation, we propose a novel approach based on the Siamese Transformer Network (STN). Our method employs two parallel branch networks utilizing the pre-trained Vision Transformer (ViT) architecture to extract global and local features, respectively. Specifically, we implement the ViT-Small network architecture and initialize the branch networks with pre-trained model parameters obtained through self-supervised learning. We apply the Euclidean distance measure to the global features and the Kullback-Leibler (KL) divergence measure to the local features. To integrate the two metrics, we first employ L2 normalization and then weight the normalized results to obtain the final similarity score. This strategy leverages the advantages of both global and local features while ensuring their complementary benefits. During the training phase, we adopt a meta-learning approach to fine-tune the entire network. Our strategy effectively harnesses the potential of global and local features in few-shot image classification, circumventing the need for complex feature adaptation modules and enhancing the model's generalization ability. Extensive experiments demonstrate that our framework is simple yet effective, achieving superior performance compared to state-of-the-art baselines on four popular few-shot classification benchmarks in both 5-shot and 1-shot scenarios.", "sections": [{"title": "I. INTRODUCTION", "content": "Learning from just a few examples and applying that knowledge to diverse situations exemplifies human visual intelligence. In recent years, leveraging deep learning techniques and large-scale labeled datasets has led to notable advancements in image recognition. Nevertheless, machine learning models have yet to match the adaptability of human visual cognition. Humans effortlessly learn to identify new object classes with minimal examples, a cognitive feat enabling flexible adaptation of existing knowledge to new tasks. Inspired by this human capability, few-shot learning [1]\u2013[5] addresses this challenge through knowledge transfer, employing a metric-based meta-learning approach [3], [6], [7] known for its simplicity and effectiveness. This approach significantly streamlines the application of deep learning in multimedia systems, garnering increasing attention in current literature.\n The few-shot learning method based on metric learning primarily comprises two essential components: feature extraction from all query and support images, and the calculation of distances between query images and each support image, prototype, or class center using metrics. Feature extraction is a crucial step for achieving efficient measurement, laying the foundation for subsequent processing. Current feature extraction strategies can be divided into two main approaches, namely global feature-based [3], [8]\u2013[21] and local feature-based [6], [10], [22]\u2013[30]. The global feature-based approach compresses the entire image into a unified feature vector, simplifying subsequent measurement process. To enhance the model's representational capacity, researchers focus on two optimization solutions: designing more effective network architectures [11], [12], [19]\u2013[21] to extract more precise prototype"}, {"title": null, "content": "features, or developing more accurate similarity evaluation methods [3], [8]\u2013[10] to ensure precise measurement results between different images. In contrast, the local feature-based method decomposes each image into multiple local regions and extracts the corresponding features from each region. This approach not only emphasizes the use of local features for accurate measurement but also aims to understand the semantic associations between different images. By capturing detailed information within the image and establishing richer associations between different images, this method improves the robustness and accuracy of the measurement.\n However, relying solely on either global or local features for measurement is prone to introducing bias. As shown in Figure 1a, the comparative images of a dog and a wolf are so similar overall that depending exclusively on global characteristics can lead to misjudgment. Similarly, Figure 1b illustrates a pair of contrasting images of a tiger and a cat, which share numerous detailed similarities, potentially causing mistaken identification if only local features are used for comparison. Therefore, it is logical to combine global and local feature-based measurements to achieve more accurate results than relying on either one alone.\n Although global feature-based and local feature-based methods have been thoroughly studied and have demonstrated advanced performance, a critical area has not received substantial attention: integrating measurements based on global features with those based on local features. While some studies [30], [31] have attempted to combine these two types of features, they generally use global features to enhance local features rather than treating them as separate and equally important components. Global features are favored for their inherent invariance properties but are susceptible to disruption by image noise, especially in the presence of complex backgrounds. Conversely, local features excel at capturing detailed image information and are robust to noise and minor changes, yet they face challenges with alignment. The complementary strengths of global and local features suggest that their combined utilization could achieve synergistic advantages and further improve model performance. However, simply measuring based on fused features cannot fully exploit the potential of each type of feature. In contrast, fusing the measurement results from different features allows for more flexibility. This approach can employ distinct and more appropriate measurement methods tailored to the characteristics of each feature, thereby fully leveraging the strengths of both global and local features.\n To fully leverage the complementary strengths of global and local features, we design a novel model called the Siamese Transformer Network (STN). The STN model consists of two distinct branches, each responsible for extracting the global features and the local feature sets, respectively. Specifically, the first branch network is dedicated to extracting the global features of the support and query images, while the second branch network focuses on extracting the local feature sets for the two sets of images. We carefully design different metrics to measure the similarity between the global features and the local feature sets of the query and support images. For the global features, we employ a regular Bregman divergence [3] to compute the similarity between samples. For the local"}, {"title": null, "content": "feature sets, we adopt an asymmetric distribution metric. To synthesize the two measurements, we first perform L2 normalization to ensure that the measurements are of the same scale. We then weight the two normalized results to obtain the final metric score, which reflects the overall similarity between the query image and the support image. Finally, based on this aggregate metric score, we assign a corresponding label to the query image. This method not only fully utilizes the information from both global and local features but also improves the accuracy and reliability of the measurement results.\n Our main contributions can be summarized as follows:\n \u2022 We present an innovative approach that ingeniously integrates the dual perspectives of global and local features, transcending the limitations of existing metric-based methods that narrowly focus on a single feature dimension. This methodology captures the holistic structure of images while meticulously analyzing intricate textures, thereby charting a new course in the field of image analysis and significantly enhancing the comprehensiveness and precision of feature extraction.\n \u2022 We introduce STN, a novel approach that combines measurements based on both global and local image features without requiring other complex designs, thereby enhancing model generalization.\n \u2022 Extensive experiments conducted on four prominent benchmark datasets demonstrate that STN outperforms state-of-the-art methods. Comprehensive comparisons further highlight the superiority of our approach.\n The remainder of this paper is organized as follows: Section II provides a summary of related work. Section III elaborates on our approach. The experiments conducted are detailed in Section IV. Section V concludes this work."}, {"title": "II. RELATED WORKS", "content": "Here, we briefly review few-shot learning methods and categorize existing methods into two groups based on the characteristics of adopted features. We then introduce Siamese neural networks and analyze how our approach differs."}, {"title": "A. Global Feature-Based Methods", "content": "Global feature-based methods utilize global features, representing an image through a singular global feature. Metric-based few-shot learning (FSL) methods embed both support and query images into a shared feature space, classifying query images by measuring their distance or similarity to support samples. These approaches primarily focus on enhancing similarity measures and improving feature extraction techniques. Several models have been developed employing various metric methods, including Matching Networks [8], Prototypical Networks [3], MSML [9], and DeepEMD [10]. To obtain more distinctive features, researchers have begun to adopt a holistic approach to the entire task, developing external modules to enhance feature representation. For instance, CTM [11] introduces a Category Traversal Module that traverses the entire support set simultaneously, identifying task-relevant features based on intra-class commonality and inter-class uniqueness"}, {"title": null, "content": "within the feature space. Similarly, FEAT [12] proposes a novel method to adapt instance-level global features to the target classification task using a set-to-set function, thereby generating task-specific and discriminative global features. Optimization-based methods aim to learn effective model initializations, facilitating rapid optimization when confronted with new tasks. A prominent example in this category is MAML [13], which achieves optimal initialization parameters through both internal and external training during the meta-training phase. MAML has inspired numerous subsequent efforts, including ANIL [14], BOIL [15], and LEO [16]. Transfer-based methods initially employ a straightforward scheme to train a classification model on the complete training set. After training, the classification head is removed, retaining only the feature extraction component, and a new classifier is trained based on the support set from the testing data. Notable examples in this area include Dynamic Classifier [17], Baseline++ [18], and RFS [19].\n Recently, researchers have begun incorporating Vision Transformers (ViT) into few-shot learning scenarios. The PMF method [20] first utilizes a pre-trained Transformer model with external unsupervised data. It then simulates few-shot tasks for meta-training using base categories and subsequently fine-tunes the model with limited labeled data from test tasks. HCTransformer [21] employs hierarchically cascaded Transformers as a robust meta-feature extractor for few-shot learning."}, {"title": "B. Local Feature-Based Methods", "content": "In contrast, local feature-based methods focus on local features by decomposing an image into a series of features that represent different local regions. Each local region often contains distinct content and carries unique semantic meaning, leading researchers to explore these methods in two main directions. On one hand, there is ongoing work to design more fine-grained local feature measures that capture subtle differences between features. On the other hand, researchers are seeking strategies to achieve explicit semantic alignment between images.\n In the first aspect, several methods have been proposed. For example, CovaMNet [22] introduces a deep covariance metric to measure the consistency of distributions between query samples and support samples. DN4 [23] calculates instance-to-class similarity between a query image and support images using k-nearest neighbors. RelationNet [6] employs a learnable deep metric network to assess the relationships between images. Additionally, ADM [24] proposes a novel asymmetric distribution measure network specifically for few-shot learning. In the second direction, researchers achieve semantic alignment by designing learnable modules that guide the model to focus on target areas within the image. For instance, SAML [32] utilizes an attention mechanism to diminish the influence of semantically unrelated areas. DeepEMD [10] promotes semantic correspondence between images by minimizing the Earth Mover's Distance. CAN [25] generates cross-attention maps for each pair of support feature maps and query sample feature maps to emphasize target object regions,"}, {"title": null, "content": "thereby enhancing the discriminative nature of the extracted features. CTX [26] learns spatial and semantic alignment between CNN-extracted query and support features using a Transformer-style attention mechanism. ATL-Net [27] introduces episodic attention calculated through a local relation map between the query image and the support set, adaptively selecting important local patches across the entire task. RENet [28] combines self-correlational representations within each image with cross-correlational attention modules between images to learn relational embeddings. Lastly, FewTURE [29] adopts a fully transformer-based architecture, learning token importance weights through online optimization during inference. CPEA [30] also employs a pre-trained ViT model, integrating patch embeddings with class-aware embeddings to enhance class relevance.\n Existing methods typically rely solely on either global or local features for similarity measurement, which limits their ability to capture the synergistic effects of utilizing both types of features. While some studies incorporate both global and local features, they primarily use global features to enhance the semantic information of local features [30], [31], ultimately performing measurements based exclusively on local features. This reliance restricts the accuracy of the measurement results. To address this limitation, we propose a Siamese Transformer Network that simultaneously extracts both global and local image features. The network measures these two distinct feature sets separately using different approaches, then fuses the resulting measurements to produce a final similarity score."}, {"title": "C. Siamese Neural Networks", "content": "Siamese neural networks were first proposed by Bromley and LeCun in the early 1990s to address signature verification as an image matching problem [1]. These networks consist of two branches, each receiving an input, mapping it to a high-dimensional feature space, and outputting the corresponding representation. By calculating the distance between the two representations commonly using metrics like Euclidean distance the model can assess the similarity between the inputs. The branch networks can be composed of convolutional neural networks (CNNs) or recurrent neural networks (RNNs), with weights optimized using energy functions or classification losses [33].\n In a narrow sense, Siamese neural networks are defined as having two identical neural networks with shared weights [1]. A generalized version, known as a \u201cPseudo-siamese network\u201d, can consist of any two neural networks [34]. The primary distinction is that in Pseudo-Siamese networks, the weights of the two branches are not shared, resulting in a true dual-branch architecture. Each branch represents a different mapping function, meaning the feature extraction structures differ, with distinct weights or network layers. Consequently, this architecture has nearly twice as many training parameters as the traditional Siamese network, providing greater flexibility.\n In this paper, we employ a Vision Transformer as the backbone for our dual-branch network, ensuring no parameter sharing between the two branches. After extracting the global and local features from the input image, we compute similarity"}, {"title": "III. METHODOLOGY", "content": "In this section, we first provide definition on the few-shot image classification task. Next, we outline the framework of our proposed method. Following that, we provide a detailed description of our Siamese Transformer Network module and explain the classification method based on two metrics of global features and local features."}, {"title": "A. Problem Definition", "content": "Few-shot image classification is primarily concerned with the N-way K-shot problem, where N denotes the number of categories and K represents the number of instances within each category. Typically, K is relatively small like 1 or 5.\n Datasets designed for few-shot learning typically comprise three distinct parts: training set $\\mathcal{D}_{train}$, validation set $\\mathcal{D}_{val}$, and test set $\\mathcal{D}_{test}$. Notably, they feature non-overlapping categories, indicating that images in the test set are entirely unseen during training and validation. This lack of overlap poses a significant challenge for few-shot learning. Typically, all the three datasets contain numerous categories and examples. To emulate the conditions of few-shot learning, researchers adopt an episode training mechanism [8]. In this mechanism, episodes are randomly sampled from the datasets, each consisting of a support set $\\mathcal{S} = \\{(x_i, y_i)\\}_{i=1}^{N_K}$ and a query set $\\mathcal{Q} = \\{(x_i, y_i)\\}_{i=1}^{N_T}$, where T is the number of query examples contained in each class. Both the $\\mathcal{S}$ and $\\mathcal{Q}$ have identical labels, but their samples do not overlap ($\\mathcal{S} \\cap \\mathcal{Q} = \\emptyset$). The support set contains a sparse selection of labeled samples, acting as the few-shot training data, while the query set is used for evaluation.\n During the training phase, a large number of episodes sampled from the training set are utilized to update the model parameters until reaching convergence. For validation and testing, episodes from the validation and test sets are employed to ensure that the test conditions mirror real-world scenarios."}, {"title": "B. The Proposed Framework", "content": "The framework of our STN model, as illustrated in Figure 2, leverages the Vision Transformer (ViT) as the feature extractor. Each input image is first partitioned into non-overlapping patches and then encoded using the pre-trained ViT [35], resulting in both class embeddings and patch embeddings. The class embeddings capture global image information, while the patch embeddings focus on local details specific to each patch.\n To conduct a comprehensive analysis of the input images (including both query and support images), we design a dual-branch network architecture. The first branch is dedicated to extracting global features, while the second branch focuses on extracting local features. To measure similarity between images based on these features, two distinct strategies are employed. Specifically, Euclidean distance serves as the similarity metric for global features, while KL divergence assesses the differences between images based on local features. This"}, {"title": null, "content": "approach leverages the strengths of each feature type, providing a more thorough evaluation of similarity by combining measurements from both branches. To integrate the similarity information from global and local features, we first normalize the measurement results using the L2 normalization. The final similarity score is then obtained through a weighted summation of the normalized values. In this process, the weighted coefficients act as the sole hyperparameter, eliminating the need for complex feature adaptation modules.\n In contrast to traditional methods, the proposed approach introduces several key innovations. First, it utilizes both global and local features to assess image similarity and effectively combines the outcomes of these complementary measures. Second, it eliminates the need for complex feature adaptation modules by employing a feature extraction model that generates diverse feature perspectives, thereby enhancing the model's generalization capability. Prior studies addressing image similarity measurement have either focused on adjusting global features to become more task-specific or used them to enhance the semantic information of local features. However, these methods typically rely on a single type of feature for comparison, failing to fully exploit the advantages of combining global and local characteristics. Additionally, these approaches often depend on supplementary network modules for parameter optimization and learning, which can increase model complexity and negatively impact the generalization ability.\n The proposed approach eliminates the need for additional network modules by employing a concise strategy for integrating global and local features. Specifically, similarity measures based on global and local features are calculated independently and then synthesized using a single weighted parameter. This streamlined method reduces model complexity and enhances generalization ability by minimizing the introduction of excessive learnable parameters outside the feature extractor. Through this weighted fusion mechanism, the approach effectively combines global contextual information with detailed localized semantics, resulting in improved performance on the image similarity measurement task."}, {"title": "C. Siamese Transformer Networks", "content": "Our Siamese Transformer Network consists of two branches, each utilizing a Vision Transformer as the backbone. In a typical few-shot task, the support set usually consists of N classes, commonly set to five, following the standard few-shot learning setup. Each support category contains K shots, and the average of the embedding values is used as the prototype embeddings [3]. After the ViT encoding process, we obtain five sets of combinations of class embeddings and patch embeddings:\n$\\mathcal{S}_i = [\\text{class}_i, \\text{patch}_{i,1}, \\text{patch}_{i,2},..., \\text{patch}_{i,M}],$ (1)\n$i \\in [1,5]$,\nwhere M is the number of patches.\n For a query image, we have a combination of class embedding and patch embeddings:\n$\\mathcal{Q} = [\\text{class}', \\text{patch}'_1, \\text{patch}'_2, ..., \\text{patch}'_M],$ (2)"}, {"title": null, "content": "where M is the number of patches. Here, $\\text{class}$ represents the class embedding that captures the global information of the image, while $\\text{patch}$ denotes the patch embeddings that capture features from local regions of the image.\n Global features are valued for their invariance but can be easily affected by image noise, particularly in complex backgrounds. In contrast, local features excel at capturing detailed image information and are robust to noise and small variations, although they often struggle with alignment issues. To address these challenges, the proposed approach employs different similarity measures based on these two types of features and subsequently fuses the measurement results. This strategy leverages the complementary advantages of both feature types, thereby enhancing the overall performance of the model.\n For the first branch network, the class embedding is obtained as the global feature:\n$\\mathcal{S}_g = \\frac{1}{K} \\sum_{i=1}^K \\text{class}^i$ (3)\n$\\mathcal{Q}_g = \\text{class}^q$ (4)"}, {"title": null, "content": "For the second branch network, the patch embeddings are obtained as the local features:\n$\\mathcal{S}_l = [\\text{patch}_{1,1}, \\text{patch}_{1,2},\\cdots, \\text{patch}_{1,M}],$ (5)\n$\\mathcal{Q}_l = [\\text{patch}^q_1, \\text{patch}^q_2, ..., \\text{patch}^q_M],$ (6)\nwhere M is the number of patches.\n Based on the global features, we utilize squared Euclidean distance as the measurement. This metric is widely used as a similarity measure across various fields [3], [12] and has demonstrated effectiveness in practice.\n$\\mathcal{D}_{Ed}(\\mathcal{Q}, \\mathcal{S}^n) = ||\\mathcal{Q}_g - \\mathcal{S}_g^n||^2,$ (7)\n$n\\in [1,5]$,\nwhere $\\mathcal{S}_g^n$ represents the global feature of the n-th support class.\n Based on the local features, we employ an asymmetric Kullback-Leibler (KL) divergence measure [24] to align the distribution of a query with that of a support class, capturing"}, {"title": null, "content": "global distribution-level asymmetric relations. The KL divergence assumes that the distribution of local features extracted from an image or a support class follows a multivariate Gaussian distribution. Specifically, the distribution of a query image can be represented as $\\mathcal{Q} = \\mathcal{N}(\\mu_q, \\Sigma_q)$, while the distribution for a support class can be expressed as $\\mathcal{S} = \\mathcal{N}(\\mu_s, \\Sigma_s)$, where $\\mu \\in \\mathbb{R}^{e}$ and $\\Sigma \\in \\mathbb{R}^{e \\times e}$ denote the mean vector and covariance matrix of the respective distributions. Thus, the KL divergence [36] between $\\mathcal{Q}$ and $\\mathcal{S}$ can be defined as:\n$\\mathcal{D}_{KL}(\\mathcal{Q} || \\mathcal{S}) = \\frac{1}{2}\\Big(\\text{trace}(\\Sigma_s^{-1}\\Sigma_q) + \\ln \\frac{\\text{det}(\\Sigma_s)}{\\text{det}(\\Sigma_q)} + (\\mu_s - \\mu_q)^T \\Sigma_s^{-1} (\\mu_s - \\mu_q) - c \\Big),$ (8)\nwhere trace(\u00b7) represents the trace operation of a matrix, ln(\u00b7) denotes the logarithm with base e, and det indicates the determinant of a square matrix. Equation (8) considers both the mean and covariance to calculate the distance between two distributions. A significant advantage of using this equation is its ability to naturally capture the asymmetric relationship between a query image and a support class through the local feature set. This encourages the query images to be closer to the corresponding true class during network training [24].\n Finally, we combine the distribution-based KL divergence measure with the Euclidean distance measure to simultaneously capture both global and local relations."}, {"title": "D. Classification With an Additive Fusion Strategy", "content": "As two distinct types of metrics have been calculated local-level relations quantified by the Kullback-Leibler divergence measure, and global-level relations produced by the Euclidean distance measure a fusion strategy must be designed to integrate these two components. To address this, the proposed approach adopts a hyperparametric weight \u03b1 to implement the fusion. It is worth noting that since the KL divergence indicates dissimilarity rather than similarity, the negative of this divergence is used to obtain a similarity measure, akin to the Euclidean distance. Specifically, the final fused similarity between a query $\\mathcal{Q}$ and a class $\\mathcal{S}$ can be defined as follows:\n$D(\\mathcal{Q}, \\mathcal{S}) = -\\alpha \\cdot D_{KL}(\\mathcal{Q} || \\mathcal{S}) - (1 - \\alpha) \\cdot D_{Ed}(\\mathcal{Q}, \\mathcal{S})$ (9)\n For a 5-way 1-shot task and a specific query $\\mathcal{Q}$, the output from each branch produces a 5-dimensional similarity vector. Subsequently, L2 normalization is applied to balance the scales of the two similarity components. Following this, we use an additive fusion strategy, as described in Equation (9), to combine the two metrics. This process results in an integrated 5-dimensional similarity vector derived from two distinct feature-based metrics. Finally, a non-parametric nearest neighbor classifier is employed to obtain the final classification results."}, {"title": null, "content": "independently using the cross-entropy loss function. For the first branch network, the Euclidean distance measurement is adopted based on global features. The probability of the i-th query image belonging to the n-th support class can then be calculated as follows:\n$p_{ni} = \\frac{\\exp(-\\mathcal{D}_{Ed}(Q_i, S_n))}{\\sum_{n'=1}^{N} \\exp(-\\mathcal{D}_{Ed}(Q_i, S_{n'}))},$ (10)\nfor the second branch network, KL divergence is adopted based on local features, and the probability is as follows:\n$p'_{ni} = \\frac{\\exp(-\\mathcal{D}_{KL}(Q_i || S_n))}{\\sum_{n'=1}^{N} \\exp(-\\mathcal{D}_{KL}(Q_i || S_{n'}))}.$ (11)\nFor a given episode, the loss function of the first branch network can be formulated as follows:\n$Loss_g = -\\frac{1}{NQ} \\sum_{i=1}^{NQ} \\sum_{n=1}^{N} I(y_i^{(Q)} = n)\\log p_{ni},$ (12)\nand the loss function of the second branch network can be formulated as follows:\n$Loss_l = -\\frac{1}{NQ} \\sum_{i=1}^{NQ} \\sum_{n=1}^{N} I(y_i^{(Q)} = n)\\log p'_{ni},$ (13)\nwhere $y_i^{(Q)}$ denotes the label of the i-th query image and $I(.)$ is an indicator function that equals one if its arguments are true and zero otherwise. In the first branch network, all the learnable weights are fine-tuned by minimizing the loss function defined in Equation (12). Correspondingly, in the second branch network, all the learnable weights are adjusted using the minimization of the loss function in Equation (13), where a randomly selected sample of training episodes is utilized.\n Inference. Given an episode sampled from the unseen test classes, the probability of a query image belonging to each class can be calculated according to Equation (9). Then, we assign the label of the class with the maximum probability to the corresponding query image. It is worth noting that, once fine-tuned on the training classes, the method does not require any adjustments when generalizing to unseen test classes. This is in contrast to FewTURE [29], which needs all images of an episode's support set together with their labels to learn the importance of each individual patch token via online optimization at inference time. As a result, the proposed approach is much simpler than FewTURE in terms of inference."}, {"title": "E. Training and Inference", "content": "In the training process, we adopt a strategy of separation and parallelism. Each branch network computes its own evaluation results based on different feature measures and optimizes"}, {"title": "IV. EXPERIMENTS", "content": "This section first details the experimental settings. Subsequently, a comparison with competing methods on benchmark datasets is presented. Finally, an ablation study of the proposed framework is conducted."}, {"title": "A. Datasets", "content": "In the standard few-shot classification task, four popular benchmark datasets are commonly used: miniImageNet [37], tieredImageNet [38], CIFAR-FS [39], and FC100 [40]."}, {"title": null, "content": "The miniImageNet dataset comprises 100 categories sampled from ILSVRC-2012 [41], with each category containing 600 images, totaling 60,000 images. In the standard setting, the dataset is randomly partitioned into training, validation, and testing sets, consisting of 64, 16, and 20 categories, respectively.\n The tieredImageNet dataset, also sourced from ILSVRC-2012, features a larger scale of data. It consists of 34 super-categories, divided into training, validation, and testing sets, containing 20, 6, and 8 super-categories, respectively. In total, the dataset includes 608 categories, with 351, 97, and 160 categories in each partitioned set.\n Both CIFAR-FS and FC100 are derived from CIFAR-100, which consists of 100 classes with 600 images per class. These datasets feature small-resolution images, each measuring 32 x 32 pixels. Specifically, CIFAR-FS is randomly divided into 64 training classes, 16 validation classes, and 20 testing classes. In contrast, FC100 includes 100 classes sourced from 36 super-classes in CIFAR-100, which are organized into 12 training super-classes (60 classes), 4 validation super-classes (20 classes), and 4 testing super-classes (20 classes)."}, {"title": "B. Implementation Details", "content": "Motivated by the scalability and effectiveness of pre-training techniques, we employ a Masked Image Modeling (MIM)-pretrained Vision Transformer as the backbone of our model. Specifically, we utilize the ViT-Small architecture with a patch size of 16. During the pre-training phase, we adopt the same strategy as outlined in [29] to pretrain our ViT-Small backbones, adhering closely to the hyperparameter settings reported in their work. In the meta-training phase, we employ the AdamW optimizer with default settings. The initial learning rate is set to 1 \u00d7 10\u22125 and decays to 1 \u00d7 10\u22126 following a cosine learning rate schedule. For the miniImageNet and tieredImageNet datasets, we resize the images to 224 \u00d7 224 pixels and train for 100 epochs, with each epoch consisting"}, {"title": null, "content": "of 200 episodes. Similarly, for the CIFAR-FS and FC100 datasets, we resize the images to 224 x 224 pixels and train for 90 epochs, with 200 episodes per epoch. During fine-tuning, we implement an episodic training mechanism. To ensure consistency with prior studies, we utilize the validation set to select the best-performing models. Additionally, we apply standard data augmentation techniques, including random resizing and horizontal flipping.\n During the meta-testing phase, we randomly sample 1000 tasks, each containing 15 query images per class. We report the mean accuracy along with the corresponding 95% confidence interval."}, {"title": "C. Performance Comparison", "content": "In accordance with established conventions in few-shot learning, we conduct experiments on four popular few-shot classification benchmarks, with the results presented in Tables I and II. The results indicate that our proposed method, STN, consistently achieves competitive performance compared to state-of-the-art (SOTA) approaches on both 5-way 1-shot and 5-way 5-shot tasks."}, {"title": "D. Ablation Study", "content": "In this subsection, we conduct ablation experiments to analyze the impact of each component on the performance of our method. Specifically, we focus on the 5-way setting using the miniImagenet dataset and the pre-trained Vision Transformer backbone. By examining the performance variations resulting from these ablations, we gain insights into the contribution of each component to the overall effectiveness of our method.\n In particular, we investigate the influence of the following variations:\n Utilizing both global and local features: To evaluate the effectiveness of our Siamese Transformer Network, we design two comparative models. In the first approach, we exclude local features, and both branches use global features to compute the similarity score. In the second approach, we"}, {"title": null, "content": "Results on miniImagenet and tieredImagenet datasets.\nTable I presents a comparison of the 1-shot and 5-shot performance of our method against baseline models on the miniImagenet and tieredImagenet datasets. We achieve significant improvements over existing SOTA results by leveraging both global and local features. For instance, on the miniImagenet dataset, our method, STN, surpasses FewTURE by 2.02% in the 1-shot setting and 3.49% in the 5-shot setting. Similarly, on the tieredImagenet dataset, STN outperforms FewTURE by 5.04% and 4.28% in the 1-shot and 5-shot settings, respectively. Compared to CPEA, which employs global features to enhance local features, our method achieves competitive results in the 1-shot setting and approximately 1% improvement in the 5-shot setting.\n Compared to FewTURE and CPEA, our method demonstrates superior accuracy while effectively utilizing both global and local features. The significant performance gap between our approach and the comparative baselines further validates the contributions of our method, which captures more comprehensive information from the data and maintains strong generalization capability.\n Results on small-resolution datasets. To assess the adaptability of our model, we conduct further experiments on two small-resolution datasets and compare our results with those of other methods. This approach allows us to evaluate the performance of our method across various data scenarios, ensuring a fair and comprehensive analysis.\n Table II presents the 1-shot and 5-shot classification performance on the small-resolution datasets CIFAR-FS and FC100. For CIFAR-FS, STN outperforms CPEA by 1.99% in the 1-shot setting and 1.83% in the 5-shot setting. Similarly, for FC100, our method surpasses CPEA by 0.55% in the 1-shot setting and 1.67% in the 5-shot setting.\n These results demonstrate the effectiveness of our method across all four datasets, as it consistently achieves superior performance in all settings. The substantial improvements observed further affirm the strengths of our approach, which effectively utilizes both global and local features to enhance image representation. The efficacy of this method is evidenced by the significant performance gains achieved across diverse few-shot learning tasks."}, {"title": null, "content": "exclude global features, and both branches rely solely on local features to measure the similarity score.\n As shown in Table III, the results clearly demonstrate the significant improvement achieved by our method, STN, compared to the baseline models. This outcome confirms the effectiveness of STN in enhancing performance for few-shot learning tasks.\n Distance functions employed in STN: To identify the most suitable metric for global feature-based measurement, we evaluate four methods: dot product (dot), Manhattan dis-"}, {"title": null, "content": "Parameters between the two branch"}]}