{"title": "Siamese Transformer Networks for Few-shot Image Classification", "authors": ["Weihao Jiang", "Shuoxi Zhang", "Kun He"], "abstract": "Humans exhibit remarkable proficiency in visual classification tasks, accurately recognizing and classifying new images with minimal examples. This ability is attributed to their capacity to focus on details and identify common features between previously seen and new images. In contrast, existing few-shot image classification methods often emphasize either global features or local features, with few studies considering the integration of both. To address this limitation, we propose a novel approach based on the Siamese Transformer Network (STN). Our method employs two parallel branch networks utilizing the pre-trained Vision Transformer (ViT) architecture to extract global and local features, respectively. Specifically, we implement the ViT-Small network architecture and initialize the branch networks with pre-trained model parameters obtained through self-supervised learning. We apply the Euclidean distance measure to the global features and the Kullback-Leibler (KL) divergence measure to the local features. To integrate the two metrics, we first employ L2 normalization and then weight the normalized results to obtain the final similarity score. This strategy leverages the advantages of both global and local features while ensuring their complementary benefits. During the training phase, we adopt a meta-learning approach to fine-tune the entire network. Our strategy effectively harnesses the potential of global and local features in few-shot image classification, circumventing the need for complex feature adaptation modules and enhancing the model's generalization ability. Extensive experiments demonstrate that our framework is simple yet effective, achieving superior performance compared to state-of-the-art baselines on four popular few-shot classification benchmarks in both 5-shot and 1-shot scenarios.", "sections": [{"title": "I. INTRODUCTION", "content": "Learning from just a few examples and applying that knowledge to diverse situations exemplifies human visual intelligence. In recent years, leveraging deep learning techniques and large-scale labeled datasets has led to notable advancements in image recognition. Nevertheless, machine learning models have yet to match the adaptability of human visual cognition. Humans effortlessly learn to identify new object classes with minimal examples, a cognitive feat enabling flexible adaptation of existing knowledge to new tasks. Inspired by this human capability, few-shot learning [1]\u2013[5] addresses this challenge through knowledge transfer, employing a metric-based meta-learning approach [3], [6], [7] known for its simplicity and effectiveness. This approach significantly streamlines the application of deep learning in multimedia systems, garnering increasing attention in current literature.\nThe few-shot learning method based on metric learning primarily comprises two essential components: feature extraction from all query and support images, and the calculation of distances between query images and each support image, prototype, or class center using metrics. Feature extraction is a crucial step for achieving efficient measurement, laying the foundation for subsequent processing. Current feature extraction strategies can be divided into two main approaches, namely global feature-based [3], [8]\u2013[21] and local feature-based [6], [10], [22]\u2013[30]. The global feature-based approach compresses the entire image into a unified feature vector, simplifying subsequent measurement process. To enhance the model's representational capacity, researchers focus on two optimization solutions: designing more effective network architectures [11], [12], [19]\u2013[21] to extract more precise prototype"}, {"title": "II. RELATED WORKS", "content": "Here, we briefly review few-shot learning methods and categorize existing methods into two groups based on the characteristics of adopted features. We then introduce Siamese neural networks and analyze how our approach differs."}, {"title": "A. Global Feature-Based Methods", "content": "Global feature-based methods utilize global features, representing an image through a singular global feature. Metric-based few-shot learning (FSL) methods embed both support and query images into a shared feature space, classifying query images by measuring their distance or similarity to support samples. These approaches primarily focus on enhancing similarity measures and improving feature extraction techniques. Several models have been developed employing various metric methods, including Matching Networks [8], Prototypical Networks [3], MSML [9], and DeepEMD [10]. To obtain more distinctive features, researchers have begun to adopt a holistic approach to the entire task, developing external modules to enhance feature representation. For instance, CTM [11] introduces a Category Traversal Module that traverses the entire support set simultaneously, identifying task-relevant features based on intra-class commonality and inter-class uniqueness"}, {"title": "B. Local Feature-Based Methods", "content": "In contrast, local feature-based methods focus on local features by decomposing an image into a series of features that represent different local regions. Each local region often contains distinct content and carries unique semantic meaning, leading researchers to explore these methods in two main directions. On one hand, there is ongoing work to design more fine-grained local feature measures that capture subtle differences between features. On the other hand, researchers are seeking strategies to achieve explicit semantic alignment between images.\nIn the first aspect, several methods have been proposed. For example, CovaMNet [22] introduces a deep covariance metric to measure the consistency of distributions between query samples and support samples. DN4 [23] calculates instance-to-class similarity between a query image and support images using k-nearest neighbors. RelationNet [6] employs a learnable deep metric network to assess the relationships between images. Additionally, ADM [24] proposes a novel asymmetric distribution measure network specifically for few-shot learning. In the second direction, researchers achieve semantic alignment by designing learnable modules that guide the model to focus on target areas within the image. For instance, SAML [32] utilizes an attention mechanism to diminish the influence of semantically unrelated areas. DeepEMD [10] promotes semantic correspondence between images by minimizing the Earth Mover's Distance. CAN [25] generates cross-attention maps for each pair of support feature maps and query sample feature maps to emphasize target object regions,"}, {"title": "C. Siamese Neural Networks", "content": "Siamese neural networks were first proposed by Bromley and LeCun in the early 1990s to address signature verification as an image matching problem [1]. These networks consist of two branches, each receiving an input, mapping it to a high-dimensional feature space, and outputting the corresponding representation. By calculating the distance between the two representations commonly using metrics like Euclidean distance the model can assess the similarity between the inputs. The branch networks can be composed of convolutional neural networks (CNNs) or recurrent neural networks (RNNs), with weights optimized using energy functions or classification losses [33].\nIn a narrow sense, Siamese neural networks are defined as having two identical neural networks with shared weights [1]. A generalized version, known as a \u201cPseudo-siamese network\", can consist of any two neural networks [34]. The primary distinction is that in Pseudo-Siamese networks, the weights of the two branches are not shared, resulting in a true dual-branch architecture. Each branch represents a different mapping function, meaning the feature extraction structures differ, with distinct weights or network layers. Consequently, this architecture has nearly twice as many training parameters as the traditional Siamese network, providing greater flexibility. In this paper, we employ a Vision Transformer as the backbone for our dual-branch network, ensuring no parameter sharing between the two branches. After extracting the global and local features from the input image, we compute similarity"}, {"title": "III. METHODOLOGY", "content": "In this section, we first provide definition on the few-shot image classification task. Next, we outline the framework of our proposed method. Following that, we provide a detailed description of our Siamese Transformer Network module and explain the classification method based on two metrics of global features and local features."}, {"title": "A. Problem Definition", "content": "Few-shot image classification is primarily concerned with the N-way K-shot problem, where N denotes the number of categories and K represents the number of instances within each category. Typically, K is relatively small like 1 or 5. Datasets designed for few-shot learning typically comprise three distinct parts: training set $D_{train}$, validation set $D_{val}$, and test set $D_{test}$. Notably, they feature non-overlapping categories, indicating that images in the test set are entirely unseen during training and validation. This lack of overlap poses a significant challenge for few-shot learning. Typically, all the three datasets contain numerous categories and examples. To emulate the conditions of few-shot learning, researchers adopt an episode training mechanism [8]. In this mechanism, episodes are randomly sampled from the datasets, each consisting of a support set $S = \\{(x_i, y_i)\\}_{i=1}^{N_K}$ and a query set $Q = \\{(x_i, y_i)\\}_{i=1}^{N_T}$, where $N_T$ is the number of query examples contained in each class. Both the S and Q have identical labels, but their samples do not overlap ($S \\cap Q = \\emptyset$). The support set contains a sparse selection of labeled samples, acting as the few-shot training data, while the query set is used for evaluation.\nDuring the training phase, a large number of episodes sampled from the training set are utilized to update the model parameters until reaching convergence. For validation and testing, episodes from the validation and test sets are employed to ensure that the test conditions mirror real-world scenarios."}, {"title": "B. The Proposed Framework", "content": "The framework of our STN model, as illustrated in Figure 2, leverages the Vision Transformer (ViT) as the feature extractor. Each input image is first partitioned into non-overlapping patches and then encoded using the pre-trained ViT [35], resulting in both class embeddings and patch embeddings. The class embeddings capture global image information, while the patch embeddings focus on local details specific to each patch. To conduct a comprehensive analysis of the input images (including both query and support images), we design a dual-branch network architecture. The first branch is dedicated to extracting global features, while the second branch focuses on extracting local features. To measure similarity between images based on these features, two distinct strategies are employed. Specifically, Euclidean distance serves as the similarity metric for global features, while KL divergence assesses the differences between images based on local features. This approach leverages the strengths of each feature type, providing a more thorough evaluation of similarity by combining measurements from both branches. To integrate the similarity information from global and local features, we first normalize the measurement results using the L2 normalization. The final similarity score is then obtained through a weighted summation of the normalized values. In this process, the weighted coefficients act as the sole hyperparameter, eliminating the need for complex feature adaptation modules.\nIn contrast to traditional methods, the proposed approach introduces several key innovations. First, it utilizes both global and local features to assess image similarity and effectively combines the outcomes of these complementary measures. Second, it eliminates the need for complex feature adaptation modules by employing a feature extraction model that generates diverse feature perspectives, thereby enhancing the model's generalization capability. Prior studies addressing image similarity measurement have either focused on adjusting global features to become more task-specific or used them to enhance the semantic information of local features. However, these methods typically rely on a single type of feature for comparison, failing to fully exploit the advantages of combining global and local characteristics. Additionally, these approaches often depend on supplementary network modules for parameter optimization and learning, which can increase model complexity and negatively impact the generalization ability.\nThe proposed approach eliminates the need for additional network modules by employing a concise strategy for integrating global and local features. Specifically, similarity measures based on global and local features are calculated independently and then synthesized using a single weighted parameter. This streamlined method reduces model complexity and enhances generalization ability by minimizing the introduction of excessive learnable parameters outside the feature extractor. Through this weighted fusion mechanism, the approach effectively combines global contextual information with detailed localized semantics, resulting in improved performance on the image similarity measurement task."}, {"title": "C. Siamese Transformer Networks", "content": "Our Siamese Transformer Network consists of two branches, each utilizing a Vision Transformer as the backbone. In a typical few-shot task, the support set usually consists of N classes, commonly set to five, following the standard few-shot learning setup. Each support category contains K shots, and the average of the embedding values is used as the prototype embeddings [3]. After the ViT encoding process, we obtain five sets of combinations of class embeddings and patch embeddings:\n$S_i = [class_i, patch_{i,1}, patch_{i,2},..., patch_{i,M}]$, (1)\n$i \\in [1,5]$,\nwhere M is the number of patches.\nFor a query image, we have a combination of class embedding and patch embeddings:\n$Q = [class', patch'_1, patch'_2, ..., patch'_{M}]$, (2)\nwhere M is the number of patches. Here, class represents the class embedding that captures the global information of the image, while patch denotes the patch embeddings that capture features from local regions of the image.\nGlobal features are valued for their invariance but can be easily affected by image noise, particularly in complex backgrounds. In contrast, local features excel at capturing detailed image information and are robust to noise and small variations, although they often struggle with alignment issues. To address these challenges, the proposed approach employs different similarity measures based on these two types of features and subsequently fuses the measurement results. This strategy leverages the complementary advantages of both feature types, thereby enhancing the overall performance of the model.\nFor the first branch network, the class embedding is obtained as the global feature:\n$S_g= \\frac{1}{K} \\sum_{i=1}^{K}class^i$ (3)\n$Q_g = class^q$ (4)\nFor the second branch network, the patch embeddings are obtained as the local features:\n$S_l = [patch_{l,1}^s, patch_{l,2}^s,..., patch_{l,M}^s]$, (5)\n$Q_l = [patch_1^q, patch_2^q, ..., patch_{M}^q]$, (6)\nwhere M is the number of patches.\nBased on the global features, we utilize squared Euclidean distance as the measurement. This metric is widely used as a similarity measure across various fields [3], [12] and has demonstrated effectiveness in practice.\n$D_{Ed}(Q, S^n) = ||Q_g - S_g^n||^2$, (7)\n$n \\in [1,5]$,\nwhere $S_g^n$ represents the global feature of the n-th support class.\nBased on the local features, we employ an asymmetric Kullback-Leibler (KL) divergence measure [24] to align the distribution of a query with that of a support class, capturing"}, {"title": "D. Classification With an Additive Fusion Strategy", "content": "As two distinct types of metrics have been calculated local-level relations quantified by the Kullback-Leibler divergence measure, and global-level relations produced by the Euclidean distance measure a fusion strategy must be designed to integrate these two components. To address this, the proposed approach adopts a hyperparametric weight \u03b1 to implement the fusion. It is worth noting that since the KL divergence indicates dissimilarity rather than similarity, the negative of this divergence is used to obtain a similarity measure, akin to the Euclidean distance. Specifically, the final fused similarity between a query Q and a class S can be defined as follows:\n$D(Q, S) = -\\alpha \u00b7 D_{KL}(Q||S) - (1 - \\alpha) \u00b7 D_{Ed}(Q, S)$ (9)\nFor a 5-way 1-shot task and a specific query Q, the output from each branch produces a 5-dimensional similarity vector. Subsequently, L2 normalization is applied to balance the scales of the two similarity components. Following this, we use an additive fusion strategy, as described in Equation (9), to combine the two metrics. This process results in an integrated 5-dimensional similarity vector derived from two distinct feature-based metrics. Finally, a non-parametric nearest neighbor classifier is employed to obtain the final classification results."}, {"title": "E. Training and Inference", "content": "In the training process, we adopt a strategy of separation and parallelism. Each branch network computes its own evaluation results based on different feature measures and optimizes independently using the cross-entropy loss function. For the first branch network, the Euclidean distance measurement is adopted based on global features. The probability of the i-th query image belonging to the n-th support class can then be calculated as follows:\n$p_{ni} = \\frac{exp(-D_{Ed}(Q_i, S_n)}{\\sum_{n'=1}^{N}exp(-D_{Ed}(Q_i, S_{n'}))}$, (10)\nfor the second branch network, KL divergence is adopted based on local features, and the probability is as follows:\n$p'_{ni} = \\frac{exp(-D_{KL}(Q||S_n)}{\\sum_{n'=1}^{N}exp(-D_{KL}(Q||S_{n'}))}$ (11)\nFor a given episode, the loss function of the first branch network can be formulated as follows:\n$Loss_g = -\\frac{1}{NQ} \\sum_{i=1}^{NQ} \\sum_{n=1}^{N} I(y^{(Q)}_i = n)logp_{ni}$, (12)\nand the loss function of the second branch network can be formulated as follows:\n$Loss_l = -\\frac{1}{NQ} \\sum_{i=1}^{NQ} \\sum_{n=1}^{N} I(y^{(Q)}_i = n)logp'_{ni}$, (13)\nwhere $y^{(Q)}$ denotes the label of the i-th query image and I(.) is an indicator function that equals one if its arguments are true and zero otherwise. In the first branch network, all the learnable weights are fine-tuned by minimizing the loss function defined in Equation (12). Correspondingly, in the second branch network, all the learnable weights are adjusted using the minimization of the loss function in Equation (13), where a randomly selected sample of training episodes is utilized.\nInference. Given an episode sampled from the unseen test classes, the probability of a query image belonging to each class can be calculated according to Equation (9). Then, we assign the label of the class with the maximum probability to the corresponding query image. It is worth noting that, once fine-tuned on the training classes, the method does not require any adjustments when generalizing to unseen test classes. This is in contrast to FewTURE [29], which needs all images of an episode's support set together with their labels to learn the importance of each individual patch token via online optimization at inference time. As a result, the proposed approach is much simpler than FewTURE in terms of inference."}, {"title": "IV. EXPERIMENTS", "content": "This section first details the experimental settings. Subsequently, a comparison with competing methods on benchmark datasets is presented. Finally, an ablation study of the proposed framework is conducted."}, {"title": "A. Datasets", "content": "In the standard few-shot classification task, four popular benchmark datasets are commonly used: miniImageNet [37], tieredImageNet [38], CIFAR-FS [39], and FC100 [40]."}, {"title": "B. Implementation Details", "content": "Motivated by the scalability and effectiveness of pre-training techniques, we employ a Masked Image Modeling (MIM)-pretrained Vision Transformer as the backbone of our model. Specifically, we utilize the ViT-Small architecture with a patch size of 16. During the pre-training phase, we adopt the same strategy as outlined in [29] to pretrain our ViT-Small backbones, adhering closely to the hyperparameter settings reported in their work. In the meta-training phase, we employ the AdamW optimizer with default settings. The initial learning rate is set to 1 \u00d7 10\u22125 and decays to 1 \u00d7 10\u22126 following a cosine learning rate schedule. For the miniImageNet and tieredImageNet datasets, we resize the images to 224 \u00d7 224 pixels and train for 100 epochs, with each epoch consisting of 200 episodes. Similarly, for the CIFAR-FS and FC100 datasets, we resize the images to 224 x 224 pixels and train for 90 epochs, with 200 episodes per epoch. During fine-tuning, we implement an episodic training mechanism. To ensure consistency with prior studies, we utilize the validation set to select the best-performing models. Additionally, we apply standard data augmentation techniques, including random resizing and horizontal flipping.\nDuring the meta-testing phase, we randomly sample 1000 tasks, each containing 15 query images per class. We report the mean accuracy along with the corresponding 95% confidence interval."}, {"title": "C. Performance Comparison", "content": "In accordance with established conventions in few-shot learning, we conduct experiments on four popular few-shot classification benchmarks, with the results presented in Tables I and II. The results indicate that our proposed method, STN, consistently achieves competitive performance compared to state-of-the-art (SOTA) approaches on both 5-way 1-shot and 5-way 5-shot tasks."}, {"title": "D. Ablation Study", "content": "In this subsection, we conduct ablation experiments to analyze the impact of each component on the performance of our method. Specifically, we focus on the 5-way setting using the miniImageNet dataset and the pre-trained Vision Transformer backbone. By examining the performance variations resulting from these ablations, we gain insights into the contribution of each component to the overall effectiveness of our method. In particular, we investigate the influence of the following variations:\nUtilizing both global and local features: To evaluate the effectiveness of our Siamese Transformer Network, we design two comparative models. In the first approach, we exclude local features, and both branches use global features to compute the similarity score. In the second approach, we"}, {"title": "E. Visualization Analysis", "content": "To evaluate the proposed approach, we randomly select a task from the miniImageNet dataset to demonstrate the changes in the model's attention map. As shown in Figure 4, after applying the feature fusion process, the model exhibits an improved ability to focus on the key areas of the images. These results further validate the effectiveness of our approach, indicating that by fusing global and local features from a single image, the resulting query and support features become more distinguishable."}, {"title": "V. CONCLUSION", "content": "In few-shot learning based on metric learning, feature adaptation modules are typically designed to enhance global feature representation or align semantically related local regions. Existing approaches often learn the similarity between support and query pairs using either global features or local features, neglecting the use of their combination. To address this limitation, we propose a Siamese Transformer Network (STN) framework that employs a dual-branch architecture to extract global and local features separately. To effectively leverage these features, we utilize Euclidean distance for global features and Kullback-Leibler divergence for local features. Based on these measurements, we develop a simple"}]}