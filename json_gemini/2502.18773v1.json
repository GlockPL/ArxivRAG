{"title": "Research on Edge Computing and Cloud Collaborative Resource Scheduling Optimization Based on Deep Reinforcement Learning", "authors": ["Yuqing Wang", "Xiao Yang"], "abstract": "This study addresses the challenge of resource scheduling optimization in edge-cloud collaborative computing using deep reinforcement learning (DRL). The proposed DRL-based approach improves task processing efficiency, reduces overall processing time, enhances resource utilization, and effectively controls task migrations. Experimental results demonstrate the superiority of DRL over traditional scheduling algorithms, particularly in managing complex task allocation, dynamic workloads, and multiple resource constraints. Despite its advantages, further improvements are needed to enhance learning efficiency, reduce training time, and address convergence issues. Future research should focus on increasing the algorithm's fault tolerance to handle more complex and uncertain scheduling scenarios, thereby advancing the intelligence and efficiency of edge-cloud computing systems.", "sections": [{"title": "1. Introduction", "content": "The rapid development of emerging technologies such as the Internet of Things (IoT) and 5G has led to a growing demand for computing power and low-latency processing. In this context, edge computing and cloud computing have emerged as key players in providing scalable and efficient computing resources. Edge computing helps alleviate issues such as high latency and excessive bandwidth usage, both of which are common in traditional cloud-based models, by moving processing closer to the user [1]. The ability to process data close to the source of generation offers significant advantages, particularly in real-time applications where speed is critical. However, while edge computing is closer to the user, resources at the edge are often limited, especially compared to the vast computing power of cloud platforms.\nGiven these limitations, the need for a coordinated approach between edge and cloud computing has become apparent. By strategically offloading specific tasks between the edge and the cloud, we can optimize resource usage and achieve better overall system performance [2]. This interaction between edge and cloud resources poses challenges for task scheduling, as the system must be able to dynamically assign tasks to the most appropriate resources based on real-time conditions. Efficient resource management between these two computing paradigms requires intelligent scheduling strategies to ensure that tasks are assigned in a way that minimizes latency, optimizes resource utilization, and balances computational load.\nThis study addresses the complex problem of joint resource scheduling between edge and cloud computing platforms and proposes a solution through advanced techniques such as deep"}, {"title": "2. Related Work", "content": "In the field of edge computing and cloud collaborative resource scheduling, numerous studies have explored various strategies to enhance system efficiency and performance. Most research has focused on task scheduling algorithms, resource management, and load balancing. Traditional optimization approaches, including heuristic algorithms, priority-based scheduling, and load balancing techniques, have been widely adopted [4]. However, as computing demands increase and system environments become more complex, these conventional methods face significant challenges, particularly in handling large-scale, dynamically changing tasks due to their limited adaptability and flexibility.\nHeuristic algorithms are commonly employed in resource scheduling for edge and cloud computing, allocating resources based on predefined rules or heuristic criteria. While these methods can improve system performance to some extent, they often struggle with real-time adjustments in dynamic environments, leading to suboptimal resource utilization and performance constraints dictated by initial conditions [5]. Load-balancing strategies aim to distribute tasks evenly across nodes to prevent overloading. However, their effectiveness is often restricted when managing real-time tasks and fluctuating workloads, as they do not fully address the uneven distribution of computing resources.\nAnother research direction focuses on priority-based scheduling strategies, where tasks are allocated to computing nodes based on urgency and importance. While such methods can enhance scheduling efficiency in certain scenarios, they lack flexibility in environments with continuously evolving task and resource requirements. To address these limitations, researchers have increasingly explored machine learning-based approaches to improve the intelligence and adaptability of scheduling systems. By leveraging historical data and real-time system status, machine learning algorithms enable dynamic resource allocation, enhancing resource utilization and reducing latency [6].\nRecently, deep reinforcement learning (DRL) has emerged as a promising optimization tool for collaborative resource scheduling in edge-cloud environments. DRL combines the advantages of reinforcement learning and deep learning, allowing systems to autonomously optimize scheduling strategies through continuous interaction with the environment. Unlike traditional scheduling algorithms, DRL exhibits strong adaptability to changing task demands and constraints, making it"}, {"title": "3. System model and optimization problem", "content": "In modern computing architectures, the collaboration between edge and cloud computing has become a critical research direction in resource scheduling optimization. With the rapid advancement of emerging technologies such as the Internet of Things (IoT) and 5G, edge computing effectively offloads computational tasks from the cloud to edge nodes closer to users, thereby reducing data transmission latency and alleviating bandwidth pressure. However, due to the limited resources available at the edge, efficient collaboration with cloud computing introduces new challenges in task allocation and resource scheduling [8]. Consequently, designing an effective resource scheduling mechanism that leverages the synergy between edge and cloud computing to enhance overall system performance is an important research problem."}, {"title": "3.1 Edge computing and cloud collaboration architecture", "content": "The system architecture consists of three key components: edge computing nodes, cloud computing platforms, and the network layer. Edge computing nodes, typically located near data sources such as routers, base stations, or small-scale data centers, process data locally and respond to user requests with low latency. The cloud computing platform provides extensive computational power and storage capacity to handle large-scale data processing tasks. The network layer facilitates data transmission, enabling communication between user devices and edge nodes, as well as data exchange between edge nodes and cloud computing platforms.\nIn a collaborative computing environment, tasks and resources are shared between edge nodes and cloud platforms. When an edge node experiences high computational load, some tasks can be offloaded to the cloud for processing, and vice versa. Edge computing primarily handles tasks with stringent real-time requirements, while cloud computing manages tasks involving large data volumes or intensive computations [9]. Given the dynamic nature and real-time constraints of task scheduling, an efficient scheduling algorithm is essential to ensure that tasks are assigned to the most suitable computing resources."}, {"title": "3.2 Definition and Challenges of Resource Scheduling Optimization Problem", "content": "Within this system architecture, the resource scheduling optimization problem can be formulated as determining the optimal allocation of computing tasks across edge and cloud resources, given the current system state and task requirements, to minimize overall latency and maximize resource utilization.\nConsider a system with N tasks, M edge computing nodes, and K cloud computing nodes. The resource scheduling problem can be modeled as an integer linear programming (ILP) problem. Let Ci"}, {"title": "4. Design of Deep Reinforcement Learning Algorithm", "content": "As the complexity of collaborative resource scheduling between edge and cloud computing increases, traditional scheduling algorithms struggle to meet the demands of efficient, real-time, and dynamic task allocation. Deep reinforcement learning (DRL) has emerged as a powerful optimization tool capable of autonomously learning optimal scheduling strategies in complex environments. By continuously interacting with the system and adjusting task allocation strategies, DRL minimizes total processing time, improves resource utilization, and reduces system latency."}, {"title": "4.1 Overview of Deep Reinforcement Learning Algorithm", "content": "Deep reinforcement learning combines the strengths of reinforcement learning and deep learning. In traditional reinforcement learning (RL), an agent interacts with the environment, receives rewards, and updates its strategy accordingly. Deep learning leverages neural networks to process high-dimensional input data, making it well-suited for problems with large state and action spaces. In the context of edge-cloud collaborative resource scheduling, DRL enables the system to autonomously learn and make optimal decisions in dynamic environments. The fundamental components of a DRL-based scheduling framework include:\n\u2022 State (s): Represents the system's current status, including available resources, task demands, and network conditions.\n\u2022 Action (a): Defines possible scheduling decisions, such as allocating a task to an edge node or offloading it to the cloud.\n\u2022 Policy (\u03c0(a|s)): A mapping that determines the probability of selecting an action a given state s."}, {"title": "4.2 Algorithm design and problem modeling", "content": "To develop a deep reinforcement learning (DRL)-based resource scheduling algorithm, it is essential to first define the state space, action space, and reward function that guide the agent's learning process."}, {"title": "4.2.1 State space", "content": "In collaborative resource scheduling between edge and cloud computing, the state space St encodes key system information, including:\n\u2022 The workload of edge computing nodes,\n\u2022 Available computing resources of the cloud platform,\n\u2022 Computational requirements of tasks, and\n\u2022 Current network conditions.\nFormally, the state space can be represented as:\n$S_t = (R_1, R_2, ..., R_M, S_1, S_2, ..., S_K, T_1, T_2, ..., T_N)$"}, {"title": "4.2.2 Action Space", "content": "The action space at defines the set of possible actions the agent can take at each time step. In resource scheduling, an action corresponds to assigning tasks to either edge nodes or cloud computing nodes. Assuming that at time t, there are N tasks, and the system has M edge computing nodes and K cloud computing nodes, the size of the action space is M \u00d7 N + K \u00d7 N, meaning the action space consists of all possible task-node mappings. For example, with three tasks and two nodes, the action space is expressed as:\n$A_t= {a_1, a_2, a_3}$"}, {"title": "4.2.3 Reward function", "content": "The reward function quantifies the effectiveness of an agent's scheduling decisions at each time step. It should reflect the total processing time, resource utilization, and task migration cost to encourage optimal scheduling behavior. Let:\n\u2022 Ttotal be the total processing time of the system,\n\u2022 Utotal be the resource utilization of the system,\n\u2022 Dtotal be the cost of task migration.\nThe the reward function is then formulated as:\n$r_t = -(\u03b1T_{total} + \u03b2\u00b7 (1 - U_{total}) + \u03b3 \u00b7 D_{total})$\nThrough this reward function, the DRL agent learns to minimize processing time, improve resource utilization, and reduce migration overhead, leading to more efficient task scheduling."}, {"title": "4.3 Reinforcement learning algorithm selection", "content": "In this study, we adopt Deep Q-Network (DQN) as the primary reinforcement learning algorithm for scheduling optimization. DQN integrates deep learning with Q-learning, leveraging neural networks to approximate the Q-value function while employing experience replay and target networks to enhance learning stability.\nThe core idea behind DQN is to estimate the Q-value for each state-action pair by training a deep neural network, enabling the agent to select optimal scheduling strategies dynamically.\nThe Q-value update rule in DQN is:\n$Q(s_t, a_t) \u2190 Q(s_t, a_t) + \u03b1 [r_t + \u03b3maxQ(s_{t+1}, a') - Q(s_t, a_t)]$\nwhere:\n\u2022 a is the learning rate,\n\u2022 y is the discount factor,\n\u2022 $max_a Q(s_{t+1}, a')$ represents the maximum Q value of all possible actions in the next state $s_{t+1}$-\nThrough iterative learning, DQN enables the agent to develop an adaptive and efficient scheduling policy, dynamically balancing task allocation between edge and cloud resources."}, {"title": "5. Experiment and Results Analysis", "content": "In this study on edge computing and cloud collaborative resource scheduling optimization using deep reinforcement learning (DRL), the experiment and results analysis section aims to validate the effectiveness and superiority of the proposed algorithm. To achieve this, we designed a series of experiments to compare the performance of DRL-based scheduling against traditional scheduling algorithms.\nThe experimental setup includes both edge and cloud computing platforms, featuring diverse resource configurations and task scheduling scenarios. By conducting these experiments, we analyze performance metrics from multiple perspectives, allowing for a comprehensive evaluation of the scheduling effectiveness of different algorithms."}, {"title": "5.1 Experimental Environment and Results Display", "content": "To evaluate the proposed algorithm's performance, we conducted experiments under a well-defined computational environment. The setup consists of 10 edge computing nodes and 3 cloud computing platform nodes, each with varying computational capacities. The number of tasks ranges from 100 to 800, with dynamically changing computational requirements following a uniform distribution.\nThe primary evaluation criteria include:\n\u2022 Total processing time\n\u2022 Resource utilization efficiency\n\u2022 Number of task migrations\nThese metrics provide a holistic view of the scheduling strategies' efficiency."}, {"title": "5.2 Performance comparison and analysis", "content": "The experimental results demonstrate that the deep reinforcement learning (DRL) scheduling algorithm outperforms traditional scheduling approaches in terms of total processing time, resource utilization, and the number of task migrations. Specifically, the Deep Q-Network (DQN) algorithm effectively minimizes total processing time and task migrations while maintaining high resource utilization through intelligent learning and decision-making. Figures 1 and 2 illustrate the comparative performance of different scheduling algorithms under a task load of 500. The following analysis provides a detailed discussion of these results.\nFigure 1 presents the total processing time for various scheduling algorithms as the number of tasks increases. The results indicate that traditional scheduling methods, such as Priority Scheduling (PS) and Load Balancing Scheduling (LBS), exhibit a linear increase in total processing time. In contrast, the DQN-based approach mitigates this trend, maintaining a lower overall processing time. This improvement highlights the ability of DRL-based scheduling to dynamically allocate computing resources and optimize task execution, thereby reducing processing delays. Traditional algorithms lack real-time adaptability, leading to significant performance degradation as task loads increase."}, {"title": "6. Discussion", "content": "This study addresses the problem of resource scheduling optimization in edge computing and cloud collaboration using deep reinforcement learning (DRL). Experimental results demonstrate that DRL algorithms significantly enhance resource utilization, reduce task processing time, and control task migrations. However, despite their strong performance in various experimental scenarios, several challenges remain for practical deployment and further optimization.\nOne key challenge is the computational cost and convergence time of DRL algorithms, particularly in complex multi-task and high-dimensional scheduling environments. While DRL is well-suited for dynamic adaptation and rapid decision-making, training these models often requires substantial computing resources and extended training time. This issue becomes more pronounced in large-scale systems where efficient learning, reduced convergence time, and avoidance of local optima remain critical research directions. Although this study successfully employs the Deep Q-Network (DQN) algorithm, optimizing its learning efficiency for larger-scale systems remains an open challenge."}, {"title": "7. Conclusion", "content": "This study addresses the challenge of resource scheduling optimization in edge-cloud collaboration using deep reinforcement learning (DRL) and validates the effectiveness of the proposed approach through experimental analysis. By leveraging DRL, the study enhances task processing efficiency, reduces overall processing time, improves resource utilization, and effectively controls task migrations. Compared to traditional scheduling algorithms, DRL demonstrates notable advantages in handling complex task allocation, dynamic workloads, and multiple resource constraints. Through adaptive scheduling strategies, DRL enables real-time task allocation, offering a promising solution for resource coordination between edge and cloud computing platforms.\nDespite these advantages, several challenges remain. As system scale increases, improving learning efficiency, reducing training time, and addressing convergence issues continue to be critical research directions. Furthermore, while the proposed DRL model effectively adapts to dynamic environments, its robustness under extreme network conditions or system failures requires further enhancement. Future research should focus on improving the fault tolerance of DRL-based scheduling algorithms to ensure stable system operation under complex and uncertain conditions.\nOverall, this study demonstrates that DRL provides a viable and effective optimization method for collaborative resource scheduling in edge-cloud computing. However, with the rapid development of the Internet of Things (IoT), 5G, and other emerging technologies, scheduling challenges will become increasingly complex. Future research must continue to enhance the efficiency, stability, and adaptability of DRL-based algorithms to address evolving application scenarios and drive the development of more intelligent and efficient edge-cloud computing systems."}]}