{"title": "Enhancing Stability for Large Models Training in Constrained Bandwidth Networks", "authors": ["Yun Dai", "Tejas Dharamsi", "Byron Hsu", "Tao Song", "Hamed Firooz"], "abstract": "Training extremely large language models with billions of parameters is a computationally intensive task that pushes the limits of current data-parallel training systems. While techniques like ZeRO++ (Wang et al., 2024) have enabled efficient distributed training of such giant models on inexpensive low-bandwidth clusters, they can suffer from convergence issues due to potential race conditions in the hierarchical partitioning (hpZ) scheme employed to reduce cross-machine communication. In this work, we first show how these race conditions cause instability when training models with billions of parameters. We then propose a modification to the partitioning algorithm that addresses these convergence challenges while maintaining competitive training efficiency. Empirical evaluation on training the multi-billion parameters Falcon Models and LLama-2 models demonstrates the updated algorithm's ability to achieve reliable convergence on these massive models, where stock ZeRO++ hpZ fails to converge. The updated algorithm enables robust training of larger models with 98% throughput and model training speed improvement without sacrificing the quality of convergence.", "sections": [{"title": "1. Introduction", "content": "The ambition to create more expansive and capable AI models has catalyzed a continual push to achieve greater scale in machine learning model training frameworks. Transformer-based model architectures like GPT (Achiam et al., 2023), Falcon (Almazrouei et al., 2023), Mistral (Jiang et al., 2023) LLAMA (Touvron et al., 2023) and many other have surpassed billions of parameters, enabling superior performance on a wide range of language understanding and generation tasks. The remarkable capabilities unlocked by these massive models have spurred surging interest from both industry and academia to explore the limits of scale for large Al models. However, the immense computational requirements for training these giant neural networks stretch the limits of modern hardware and distributed training frameworks. Crucially, the pursuit of ever-larger models risks exacerbating disparities, as the specialized acceleration hardware required remains cost-prohibitive and inaccessible for many. Democratizing access to train massive models efficiently on modest, commodity hardware is vital to ensure equitable AI development globally.\nData parallelism, where a model's parameters are partitioned across multiple accelerator devices (e.g. CPUs, GPUs, TPUs) during training, has been a key enabling technique for training large deep neural network models. Classic data-parallel implementations like BytePS (Jiang et al., 2020), Horovod (Sergeev & Balso, 2018) and PyTorch DDP (Li et al., 2020) rely on replicating the full model across all devices, leading to substantial memory overheads that impose a hard limit on maximum model size.\nMegatron-LM (Shoeybi et al., 2020) from NVIDIA attempted to scale by leveraging model parallelism in addition to data parallelism. However, this approach required specialized high-bandwidth interconnects like NVLink and InfiniBand that are not widely accessible to most developers and researchers.\nIn contrast, methods like ZeRO (Rajbhandari et al., 2020) took a different approach - eliminating redundant parameter copies across data-parallel devices through intelligent parameter partitioning schemes. ZeRO-Offload (Ren et al., 2021) further optimized memory usage by offloading activations and optimizer states to CPU memory during the respective forward and backward passes. While more memory-efficient, these ZeRO techniques still relied heavily on frequent inter-node communication for collective operations.\nA key bottleneck faced by both Megatron-LM and ZeRO was the relatively low inter-node communication bandwidth compared to the intra-node bandwidth within a single multi-"}, {"title": "2. Background", "content": "As we push to hundreds of billions parameters models, the memory becomes a burden for training. The ZeRO3 algorithm shards model parameters, gradients, and optimizer states to significantly reduce the memory footprint. However, this reduction comes at the cost of increased communication overhead. Specifically, the algorithm requires AllGather operations on the weights during both the forward pass to compute activations and the backward pass to compute gradients, followed by reduce-scatter operations to distribute the gradients across accelerators.\nThis results in a high dependence on the communication bandwidth within the cluster. Recent research has shown that the latency for inter-node GPU communication is typically higher compared to intra-node GPU communication. This can be attributed to the differences in interconnect technologies such as NVLink NVSwitch (NVIDIA, 2024b) for intra-node and Infiniband (NVIDIA, 2024a) for inter-node communication. As a result, the training process often encounters bottlenecks due to slower inter-node communication, making the inter-node network more likely to become the system bottleneck. Improving inter-node network speed can lead to significant performance gains for multi-GPU applications.\nTo address these bottlenecks, several approaches have been proposed to maximize intra-node communication while minimizing inter-node communication. For example, PyTorch's hybrid shard FSDP (Zhao et al., 2023) performs ZeRO3 only within a node and uses AllReduce to synchronize gradients across nodes, similar to the Distributed Data Parallel approach. Another approach, MiCS (Zhang et al., 2022), creates subgroups of GPUs with high intra-group bandwidth, limiting AllGather operations to within these subgroups and using AllReduce for inter-group communication to minimize the inter-subgroup communication overhead.\nZeRO++ (Wang et al., 2024) extends these ideas with the introduction of hierarchical partitioning hpZ, quantized gradient qgZ, and quantized weight qwZ schemes. The hpZ scheme ensures a full copy of the model within a node and uses AllReduce to synchronize gradients across nodes, effectively reducing inter-node communication. The qgZ scheme quantizes gradients before ReduceScatter operations, and the qwZ scheme quantizes weights before AllGather operations, both of which further optimize communication efficiency."}, {"title": "3. Algorithm", "content": "ZeRO++ hpZ reduces communication overhead by eliminating cross-node AllGather communication during the backward pass, with an extra cost of memory. After the forward pass of a layer is done, instead of re-spreading its full weights across all GPUs, ZeRO++ hpZ partitions the weights into a secondary copy which is replicated on each node. AllGather in the backward pass thus operates on the secondary copy and will only involve intra-node com-"}, {"title": "5. Conclusion", "content": "In this work we identified and addressed a key convergence issue that affects the training of large language models using the ZeRO++ algorithm on commodity hardware with limited network bandwidth. We analyzed the root cause of the instability, which stems from a race condition between the asynchronous parameter partitioning and collective communication operations in ZeRO++'s hierarchical partitioning scheme. We resolve this bottleneck by introducing explicit CUDA synchronization. This ensures parameter partitioning completes correctly before any collective communication over the partitioned data occurs. Our empirical evaluation demonstrated that the updated algorithm restores reliable convergence when training giant transformer models like the 40B parameter Falcon and 70B parameter LLama-2 on the challenging MMLU dataset."}]}