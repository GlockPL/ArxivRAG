{"title": "Enhancing Stability for Large Models Training in Constrained Bandwidth Networks", "authors": ["Yun Dai", "Tejas Dharamsi", "Byron Hsu", "Tao Song", "Hamed Firooz"], "abstract": "Training extremely large language models with billions of parameters is a computationally intensive task that pushes the limits of current data-parallel training systems. While techniques like ZeRO++ (Wang et al., 2024) have enabled efficient distributed training of such giant models on inexpensive low-bandwidth clusters, they can suffer from convergence issues due to potential race conditions in the hierarchical partitioning (hpZ) scheme employed to reduce cross-machine communication. In this work, we first show how these race conditions cause instability when training models with billions of parameters. We then propose a modification to the partitioning algorithm that addresses these convergence challenges while maintaining competitive training efficiency. Empirical evaluation on training the multi-billion parameters Falcon Models and LLama-2 models demonstrates the updated algorithm's ability to achieve reliable convergence on these massive models, where stock ZeRO++ hpZ fails to converge. The updated algorithm enables robust training of larger models with 98% throughput and model training speed improvement without sacrificing the quality of convergence.", "sections": [{"title": "1. Introduction", "content": "The ambition to create more expansive and capable AI models has catalyzed a continual push to achieve greater scale in machine learning model training frameworks. Transformer-based model architectures like GPT (Achiam et al., 2023), Falcon (Almazrouei et al., 2023), Mistral (Jiang et al., 2023) LLAMA (Touvron et al., 2023) and many other have surpassed billions of parameters, enabling superior performance on a wide range of language understanding and generation tasks. The remarkable capabilities unlocked by these massive models have spurred surging interest from both industry and academia to explore the limits of scale for large Al models. However, the immense computational requirements for training these giant neural networks stretch the limits of modern hardware and distributed training frameworks. Crucially, the pursuit of ever-larger models risks exacerbating disparities, as the specialized acceleration hardware required remains cost-prohibitive and inaccessible for many. Democratizing access to train massive models efficiently on modest, commodity hardware is vital to ensure equitable AI development globally.\nData parallelism, where a model's parameters are partitioned across multiple accelerator devices (e.g. CPUs, GPUs, TPUs) during training, has been a key enabling technique for training large deep neural network models. Classic data-parallel implementations like BytePS (Jiang et al., 2020), Horovod (Sergeev & Balso, 2018) and PyTorch DDP (Li et al., 2020) rely on replicating the full model across all devices, leading to substantial memory overheads that impose a hard limit on maximum model size.\nMegatron-LM (Shoeybi et al., 2020) from NVIDIA attempted to scale by leveraging model parallelism in addition to data parallelism. However, this approach required specialized high-bandwidth interconnects like NVLink and InfiniBand that are not widely accessible to most developers and researchers.\nIn contrast, methods like ZeRO (Rajbhandari et al., 2020) took a different approach - eliminating redundant parameter copies across data-parallel devices through intelligent parameter partitioning schemes. ZeRO-Offload (Ren et al., 2021) further optimized memory usage by offloading activations and optimizer states to CPU memory during the respective forward and backward passes. While more memory-efficient, these ZeRO techniques still relied heavily on frequent inter-node communication for collective operations.\nA key bottleneck faced by both Megatron-LM and ZeRO was the relatively low inter-node communication bandwidth compared to the intra-node bandwidth within a single multi-"}, {"title": "2. Background", "content": "As we push to hundreds of billions parameters models, the memory becomes a burden for training. The ZeRO3 algorithm shards model parameters, gradients, and optimizer states to significantly reduce the memory footprint. However, this reduction comes at the cost of increased communication overhead. Specifically, the algorithm requires AllGather operations on the weights during both the forward pass to compute activations and the backward pass to compute gradients, followed by reduce-scatter operations to distribute the gradients across accelerators.\nThis results in a high dependence on the communication bandwidth within the cluster. Recent research (Liang et al., 2024) (Li et al., 2019) (Ren et al., 2019) has shown that the latency for inter-node GPU communication is typically higher compared to intra-node GPU communication. This can be attributed to the differences in interconnect technologies such as NVLink NVSwitch (NVIDIA, 2024b) for intra-node and Infiniband (NVIDIA, 2024a) for inter-node communication. As a result, the training process often encounters bottlenecks due to slower inter-node communication, making the inter-node network more likely to become the system bottleneck. Improving inter-node network speed can lead to significant performance gains for multi-GPU applications.\nTo address these bottlenecks, several approaches have been proposed to maximize intra-node communication while minimizing inter-node communication. For example, PyTorch's hybrid shard FSDP (Zhao et al., 2023) performs ZeRO3 only within a node and uses AllReduce to synchronize gradients across nodes, similar to the Distributed Data Parallel approach. Another approach, MiCS (Zhang et al., 2022), creates subgroups of GPUs with high intra-group bandwidth, limiting AllGather operations to within these subgroups and using AllReduce for inter-group communication to minimize the inter-subgroup communication overhead.\nZeRO++ (Wang et al., 2024) extends these ideas with the introduction of hierarchical partitioning hpZ, quantized gradient qgZ, and quantized weight qwZ schemes. The hpZ scheme ensures a full copy of the model within a node and uses AllReduce to synchronize gradients across nodes, effectively reducing inter-node communication. The qgZ scheme quantizes gradients before ReduceScatter operations, and the qwZ scheme quantizes weights before AllGather operations, both of which further optimize communication efficiency."}, {"title": "3. Algorithm", "content": "ZeRO++ hpZ reduces communication overhead by eliminating cross-node AllGather communication during the backward pass, with an extra cost of memory. After the forward pass of a layer is done, instead of re-spreading its full weights across all GPUs, ZeRO++ hpZ partitions the weights into a secondary copy which is replicated on each node. AllGather in the backward pass thus operates on the secondary copy and will only involve intra-node communication, which has multiple factors higher bandwidth than inter-node.\nDuring partition, the secondary copy tensor gets allocated as torch.empty of the following shape\n$\\begin{equation*} || L_{i,second}|| = \\frac{N}{secondaryWorldSize} \\tag{1} \\end{equation*}$ where N is the total number of elements in the weights and secondaryWorldSize is typically equal to the number of GPUs per node. The weights corresponding to the local rank then get copied from the full parameter tensor to the secondary copy.\nHowever, since the Memcpy is from and to tensors both allocated on GPU, hence an asynchronous D2D (device-to-device) copy, there is no guarantee that the secondary copy is settled when the following AllGather kernel on it is launched.\nWith prefetch in ZeRO, which enqueues AllGather kernels for following modules beforehand instead of as late as when the backward pass actually happens, a race condition may happen: at the time when a module is still being partitioned into the secondary copy, the AllGather kernel for the backward pass on it can be immediately enqueued and launched. This results in AllGather aggregating on arbitrarily initialized tensor values, leading to model instability during training and often results in Not-a-Number (NaN) in aggregated parameter values and hence the observed loss.\nThe race condition is mainly the result of full asynchronization between two operators, Memcpy and AllGather. To avoid such a race condition, we add a CUDA synchronization operator between D2D Memcpy and AllGather."}, {"title": "4. Experimentation", "content": "All experiments are conducted on NVIDIA-A100 GPUs. Eight GPUs compose one GPU node, where GPUs within a node are connected using NVIDIA NVLINK with 600 GB/s bandwidth. To simulate commodity low bandwidth connections, we use 1\u00d7 12.5Gbps Ethernet NIC per node.\nWe leverage two families of off-the-shelf publicly available large language models for our experiments with different sizes: 1) Llama-2 (Touvron et al., 2023), 2) Falcon (Almazrouei et al., 2023). We perform full parameter fine-tuning using the MMLU dataset (Hendrycks et al., 2020) on these models.\nHere we define training as unstable if the loss during training diverges to NaN or the training loss does not decrease with the same hyperparameters. We measure the throughput of the training by determining how many tokens are processed in one second given a full GPU node, a.k.a tokens/s/node."}, {"title": "4.2. Divergence Analysis", "content": "To demonstrate model training instability, we train various off-the-shelf LLMs with and without stock ZeRO++ hpZ and compare the stability with modified hpZ outlined in Algorithm 1. As demonstrated in Table 1, for many publicly available large models, the training is unstable with the original ZeRO++ hierarchical partitioning.\nAs shown in Table 2, the async operation between AllGather and memory copy in ZeRO++ sometimes results in lower training throughput compared to stock hpZ, but as explained in Table 1, it comes with instability and divergence costs. The modified hierarchical partitioning algorithms have up to 98% higher throughput compared to when hpZ is not enabled while keeping the training stability unchanged.\nDespite having better training throughput, the modified hpZ has no impact on model optimization performance and the training is stable. Figure 2 shows the validation loss during training for MMLU without hpZ and with modified hpZ for Llama-2-7B. As one can see, there is almost no difference in validation loss convergence per optimization step while from Table 2 the modified hpZ converges faster in wall-clock time."}, {"title": "5. Conclusion", "content": "In this work we identified and addressed a key convergence issue that affects the training of large language models using the ZeRO++ algorithm on commodity hardware with limited network bandwidth. We analyzed the root cause of the instability, which stems from a race condition between the asynchronous parameter partitioning and collective communication operations in ZeRO++'s hierarchical partitioning scheme. We resolve this bottleneck by introducing explicit CUDA synchronization. This ensures parameter partitioning completes correctly before any collective communication over the partitioned data occurs. Our empirical evaluation demonstrated that the updated algorithm restores reliable convergence when training giant transformer models like the 40B parameter Falcon and 70B parameter LLama-2 on the challenging MMLU dataset."}]}