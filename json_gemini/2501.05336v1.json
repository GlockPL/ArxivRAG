{"title": "Stream Aligner: Efficient Sentence-Level Alignment via Distribution Induction", "authors": ["Hantao Lou", "Jiaming Ji", "Kaile Wang", "Yaodong Yang"], "abstract": "The rapid advancement of large language models (LLMs) has led to significant improvements in their capabilities, but also to increased concerns about their alignment with human values and intentions. Current alignment strategies, including adaptive training and inference-time methods, have demonstrated potential in this area. However, these approaches still struggle to balance deployment complexity and capability across various tasks and difficulties. In this work, we introduce the Streaming Distribution Induce Aligner (Stream Aligner), a novel alignment paradigm that combines efficiency with enhanced performance in various tasks throughout the generation process. Stream Aligner achieves dynamic sentence-level correction by using a small model to learn the preferences of the suffix sentence, iteratively correcting the suffix sentence output by the upstream model, and then using the corrected sentence to replace the suffix sentence in subsequent generations. Compared to Aligner, our experiments demonstrate that Stream Aligner reduces reliance on the capabilities of additional models, enhances the reasoning abilities of LLMs, and decreases latency during user interaction. Specifically, Stream Aligner-2B model has achieved a maximum improvement of 41.2% in helpfulness, 36.0% in harmlessness on the tested Llama2-70B-chat model, and Stream Aligner-8B has achieved an improvement of 3.5% on the math ability of the tested Llama3-70B-Instruct model.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) can perform various downstream tasks (Touvron et al. 2023; Achiam et al. 2023), but they may exhibit unintended behaviors (Ji et al. 2024d; Hubinger et al. 2024). The alignment of LLMs aims to ensure the behaviours of LLMs are consistent with human intention and value (Ji et al. 2023, 2024e). As LLMs continue to scale up in size and capability, the need for lightweight, model-agnostic, yet efficient alignment methods becomes increasingly critical.\nCurrently, training methods such as supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF) (Ouyang et al. 2022; Taori et al. 2023; Ji et al. 2024e) are the most widely recognized approaches to alignment (Bai et al. 2022a; Rafailov et al. 2024; Bai et al. 2022b; Dai et al. 2023). However, as the scale of LLMs increases, these training methods also face issues of rising data requirements, computational power consumption (Ding et al. 2023), and extremely sensitive to parameters and training data, especially in reasoning-related tasks (Casper et al. 2023).\nInference-time methods, including speculative sampling (Chen et al. 2023), and safe-decoding (Xu et al. 2024), refine the inference algorithms to alter the text generation mechanisms of LLMs. These adjustments seek to align the responses and better elicit the latent knowledge of LLMs in a manner that avoids the expense and time commitment of additional training (Paul, Ajeya, and Xu 2024). Inference-time methods share the advantage of lightweight and deploying convenience, but they generally struggle to precisely distill human value and intent into the LLMs outputs in long context generation since a single token output cannot carry a complete unit of semantics (Ji et al. 2024a).\nMeanwhile, the approach of incorporating additional models to base models has been drawing much attention recently. These approaches aim to distill human preferences within a smaller additional model, which is later incorporated into the predefined generation pipeline along with the targeted upstream model, as demonstrated by works like Aligner (Ji et al. 2024a; Yang et al. 2024). These methods can achieve outstanding performances in areas such as value alignment and multi-objective alignment. However, these methods also have certain drawbacks: they are not able to fully elicit the latent knowledge of the upstream model, leading to a high dependency on the capabilities of the additional models in tasks related to model capability, thus making it difficult to achieve good performance; at the same time, these methods have relatively high latency, which affects user experience. This brings about a further need: How can we elicit the capabilities of upstream models according to human preferences in the inference process?\nIn this work, we combine the advantage of inference-time strategies and additional models to propose Streaming Distribution Induce Aligner (Stream Aligner), a sentence-level correction mechanism that stimulates the potential of the base model while conserving the distillation of human preferences. This is achieved by narrowing down the correction scope of Aligner to sentence level, feeding the corrected output back to the base model, and repeating this process. Specifically, Stream Aligner is fine-tuned on a preference dataset to learn the residuals of preferred and non-preferred last sentences under a fixed prompt and answer prefix. It is then integrated into the generation cycle depicted in Figure 1, continuously correcting sentences generated by the upstream model and incorporating them into the prefix to achieve sentence-level alignment. Compared to Aligner's single-round generation, Stream Aligner's paradigm has the following advantages:\n\u2022 Reduced dependency on additional model capabilities Stream Aligner achieves distribution induction through sentence-level correction, thereby leveraging more of the performance of the upstream model and reducing dependence on the size and scale of the additional model. Specifically, we apply the Stream Aligner 2B model to correct the Llama2-70B-chat model, resulting in an increase in the helpfulness of responses by 41.2% and an increase in the harmlessness of responses by 36.0%, which is significantly higher than the results of Aligner-7B.\n\u2022 Enhanced reasoning abilities through step-by-step correction In tasks related to reasoning, Stream Aligner's distribution induction is observed to correct the incorrect part of the reasoning process in the upstream model and to add inductions to the correct answer for subsequent steps, thereby enhancing the model's reasoning abilities. For a detailed comparison between Stream Aligner and the classic generation method, please refer to Figure 2. The experiments show that the longer the average intervention by Stream Aligner on the test set, the higher the accuracy after the intervention, as shown in Figure 3."}, {"title": "2 Formulation of Stream Paradigm", "content": "Preliminary: SFT and the Aligner Paradigm\nSupervised Fine-tuning(SFT) SFT aims to fine-tune pretrained LLM through supervised learning to generate target answers. For a high-quality dataset DSFT = {x(i),y(i)}1, the SFT objective is to obtain a model \u03c0\u00a7FT to minimize the negative log-likelihood loss:\nL (0; DSFT) = -E(x,y)~DSFT [log \u03c0\u04e9(Y|x)].\n(1)\nThe Aligner Paradigm The Aligner (Ji et al. 2024a) finetunes the model based on a preference dataset M to learn the correction residuals between preferred and non-preferred responses. For a dataset M = {x(i), yo), y)}1, where x represents the user's query, yo is the original answer, and ye is the corrected answer according to established principles, Aligner is a conditional seq2seq model parameterized by \u03c6, denoted as \u00b5\u03c6(Yc|Yo, x). The model reassigns the preliminary answer yo to the aligned answer ye. The training objective of Aligner is to minimize the following loss:\nLAligner(, M) = \u2212Em [log \u00b5\u03c6(yc|Yo, x)] . (2)\nAlgorithm 1: Stream Aligner Module\nRequire: Sentence-level preference dataset D where it contains {qi, Pi, Y, Y}=1; pre-train model M; upstream model MB; prompt dataset Dq:{qi}=1\nEnsure: Generated dataset Dfull:{qi, ai}=1\nInitialize model A with weights from M\nStage1: Stream Aligner Training\nfor each epoch do\nfor each (qi, Pi, y, yz) \u2208 D do\n\u0177i A.generate(qi + Pi + y)\n\u04e9\u0434 \u2190 \u03b8a \u2013 n\u2207\u04e9L(\u0177i, Y)\nend for\nend for\nStage2: Stream Aligner Inference\nfor each qi \u2208 Dq do\nInitialize pi\u53e3\nwhile True do\ny\u2190 Mr.generate(qi + Pi)\n2\ny A.generate(qi + pi + y)\npi\u2190 concatenate(pi, y)\nif y = \u00d8 or pi \u2265 max_length| then\nbreak;\nend if\nend while\nAcquire final answer a\u2081 = pi\nDfull.append(qi, ai)\nend for\nTraining and Inference Pipeline of Stream Aligner\nCompared to Aligner, Stream Aligner refines the original correction process by correcting each sentence step by step, thereby improving the accuracy of the correction paradigm. The following describes the specific training and inference process of Stream Aligner. The entire pipeline of the Stream Aligner algorithm is shown in Algorithm 1.\nStream Aligner Model Training Stream Aligner learns the residuals between preferred and non-preferred responses through a sentence-level preference dataset D. For a sentence-level preference dataset D = {qi, Pi, Y, Y}=1, where q represents the user's query, p is the common response prefix of y\u00b9 and y\u00b2, and y\u00b9 is the original answer while y\u00b2 is the corrected answer according to established principles, the Stream Aligner model, parameterized by @ and denoted as A, reduces the residual between y\u00b9 and y\u00b2 conditioned on the question q and the prefix p. The training objective of Stream Aligner is to minimize the following loss:\nLStream Aligner(0, D) = \u2212Ep [log A(y2|y\u00b9,q+p)]. (3)\nStream Aligner Model Inference During the inference process of Stream Aligner, the Stream Aligner takes the question and the prefix q + p as input, where p is initialized as \u00d8, and the upstream model MB generates the original answer y\u00b9 step by step. The trained Stream Aligner model then corrects this answer with y\u00b2. Each generated correction is incorporated into the prefix p until the generation stops or the prefix exceeds the maximum length. The final answer to the question q is the resulting prefix p."}, {"title": "3 Experiments", "content": "In this section, we assess the effectiveness of Stream Aligner in three evaluation metrics: helpful and harmless QA, math questions, and summary tasks. We further analyze the evaluation results and do an ablation study on these situations.\nExperiment Setup\nDataset We utilize different datasets for each task: HH-RLHF (Bai et al. 2022a) for helpful and harmless QA, and MATH (Hendrycks et al. 2021) for math and reasoning tasks 1. Considering that no current dataset constructs a fine-grained reward in these two datasets, we create two additional fine-grained preference datasets based on the prompts\nWe choose these two distinct tasks to prove the effectiveness of Stream Aligner on both QA tasks and reasoning tasks. given in these two datasets. Following the Stream Aligner generation pipeline, we use Alpaca-7B (Taori et al. 2023), Llama2-(7B, 70B)-chat (Touvron et al. 2023), Llama3-(8B, 70B)-Instruct (Meta 2024) as upstream models to generate original answer sentences, and we take GPT-4 (Achiam et al. 2023), Llama3-70B-Instruct, Qwen1.5-110B-Chat (Team 2024) as annotators to refine the suffix sentences after generation. These refinements were conducted under a well-written prompt demonstrating the constraint and principles of our correction paradigm, which we expect the Stream Aligner to learn from: Rewrite the bad, improve the neutral, and keep the good. For more details, please refer to the Appendix.\nModels We train Stream Aligner-(2B, 8B) models based on Gemma1.1-2B (Team et al. 2024) and Llama3-8B foundation models using the dataset above. We then incorporate them into the deploying pipeline described in Section 2, with Llama3-(8B,70B)-Instruct as upstream models.\nEvaluation Metrics Our evaluation metrics vary on different tasks, but the core idea remains the same: Compete the win rate with the direct answer of the upstream model. We sample a test set from BeaverTails (Ji et al. 2024c,b) and MATH for evaluation. We directly generate answers from upstream models and use the Stream Aligner pipeline to generate a corrected answer. We then utilize strong models (in our case, GPT-4 for helpful & harmless QA and math) to evaluate the two answers and took the win rate of Stream Aligner against the upstream model as the evaluation result. For more details about dataset acquisition and evaluation implementation, please refer to the Appendix.\nExperiment Results\nThe performance of Stream Aligner pipeline on math and helpful & harmless QA tasks is shown in Figure 3. Under these tasks, we can observe an outstanding improvement in performance, with a maximum win rate of over 76.1% in helpfulness, 36.0% in harmlessness, and 19.0% in math tasks. It's also worth noting that Stream Aligner-(2B,8B) model can be utilized to correct the answer of up to 70B models, demonstrating the scalability and efficiency of our method.\nPerformance on Helpful & Harmless QA Figure 4 shows the distribution shift of helpful and harmless scores after different rounds of sentence-level correction. In terms of helpfulness and harmlessness, due to the instability of the evaluation methods, the performance of Stream Aligner shows significant fluctuations, yet there is an overall upward trend in win rates during the early rounds. Beyond a certain number of rounds, the helpfulness of Stream Aligner begins to decline, while harmlessness continues to rise. This is because as the responses become excessively verbose over time, it impacts the level of helpfulness; however, since Stream Aligner extensively learns from human preferences, its output remains consistently safe, thus showing a generally upward trend in safety. Notably, over many rounds, the win rates for helpfulness and harmlessness tend to converge to a stable value.\nPerformance on Math Task In mathematical tasks, the performance of Stream Aligner monotonically increases with the number of rounds, indicating that Stream Aligner performs well in reasoning-based tasks such as mathematics. In these tasks, we utilized two combinations of Stream Aligner models and upstream models: Llama3-8B-Instruct + Stream Aligner-2B and Llama3-70B-Instruct + Stream Aligner-8B. By training on the suffix preference dataset, both sets of models are able to enhance the mathematical capabilities of the upstream model after certain rounds of correction. Additionally, as is shown in Figure 3, the number of rounds required for Llama3-8B-Instruct + Stream Aligner-2B to exceed the baseline is significantly fewer than that for Llama3-70B-Instruct + Stream Aligner-8B, which also aligns with the respective difficulties in achieving their baselines.\nAblation Study\nAblation on Stream Aligner Pipeline To verify the correction capabilities of the Stream Aligner paradigm with different supervision quantities and different generation pipelines, we conducted ablation studies on the generation methods and the number of correction sentences within the Stream Aligner pipeline across all tasks.\n\u2022 Generation-Correction Frequency As seen in Figure 5 and Table 1, the performance of the Stream Aligner pipeline increases significantly with the number of generation-correction cycles and continues to rise after surpassing the original model. This demonstrates that Stream Aligner can enhance the performance of the upstream model with limited supervision and achieve even higher capabilities under conditions of ample supervision.\n\u2022 Generation Methods Given the heavy reliance of the generation-correction pipeline on the Stream Aligner, we performed an ablation study between two methods within our generation-correction pipeline. The main method, as previously described, iterates through cycles of generation and correction, whereas another method use Stream Aligner to continue generation until the end of the answer, followed by the final correction cycle. The performance of these two pipelines on math tasks and their performance on QA tasks are shown in Figure 5 and Table 1. On math and harmless QA, this new pipeline demonstrated excellent performance. It reached 80% of its maximum win rate on helpfulness at 1 round and achieved a win rate above zero on math at 1 round, which is significantly lower than the 4 and 6 rounds required by the classic pipeline. We hypothesize that this improvement in reasoning tasks may be attributed to the performance relying heavily on the completeness and correctness of certain key steps rather than a uniform distribution across all steps. However, since the continue generation pipeline is more compute-consuming and converges to a similar result compared to the direct generation pipeline, we still use the direct generation pipeline as the main method.\nThese findings highlight the adaptability of the Stream Aligner pipeline in varying supervisory conditions and its potential to significantly improve the efficiency and effectiveness of model performance across diverse tasks.\nAblation on the size of Stream Aligner To validate that Stream Aligner can fully elicit knowledge of the upstream model, we conducted the ablation study about model sizes. Specifically, we performed the experiment on math task but with different size of upstream model:\n\u2022 We trained a Stream Aligner based on Llama3-70B-Instruct on math tasks to correct the Llama3-70B-Instruct. The results indicate that the Stream Aligner-70B, after 15 rounds of sentence-level correction, enhanced the original model's accuracy by approximately 3.6%, nearly identical to that of the Stream Aligner-8B. In the subsequent five rounds, the accuracy improvement by Stream Aligner-70B could reach 4.1%.\n\u2022 We also performed the same experiment on the Llama3-8B-Instruct model, where we compared the inference time and the performance of Stream Aligner-8B, Stream Aligner-2B, and Stream Aligner-0.5B (the result is displayed in Figure 6). The result shows that Stream Aligner-8B can achieve an accuracy improvement of 11.3% on Llama3-8B-Instruct, but Stream Aligner-2B also reached 6.1% improvement using nearly half of the inference time. Moreover, the Stream Aligner-0.5B can also improve the performance of Llama3-8B-Instruct, given the huge gap of additional model size.\nConsidering the huge gap in performance between the models of difference size (Hoffmann et al. 2022) and the narrow gap of accuracy improvement, Stream Aligner paradigm can elicit the knowledge of the upstream Model to a large extent (Christiano, Xu, and Cotra 2021).\nComparison to other alignment methods To demonstrate the performance enhancements of Stream Aligner compared to other alignment methods, we constructed answer preference datasets and sentence-level preference datasets using the same prompt dataset across QA and reasoning tasks. Using these datasets, we conducted an ablation study comparing Stream Aligner with other alignment methods, such as Supervised Finetuning (SFT) and Direct Preference Optimization (DPO). On Llama3-8B-Instruct, the accuracy improvements for SFT and DPO were -0.5% and 0.3%, respectively, whereas Stream Aligner achieved a significant accuracy improvement of 5.8%. These results highlight the superiority of Stream Aligner over conventional alignment methods.\nWe also conducted ablation studies on Stream Aligner, Aligner, and proxy-tuning (Liu et al. 2024). We reproduced Aligner and proxy-tuning on the math task, using the Llama3-70B-Instruct as the upstream model and Llama3.2-2B-Instruct, Llama3-8B-Instruct, Llama3-70B-Instruct as the additional model. The accuracy improvements of Stream Aligner and other inference-time methods are presented in Figure 7. Our results demonstrate that Stream Aligner consistently outperforms other inference-time methods across all model sizes. Notably, Stream Aligner achieves the performance of Aligner-70B using only 2B parameters, showcasing both superior performance and efficiency in model size. Additionally, we evaluated the inference time per token and first-token latency to illustrate Stream Aligner's suitability for deployment scenarios. Our experiments reveal that under a standard pipeline generation, the per-token inference time of Stream Aligner is only 0.80 times that of Aligner when using the same upstream model and two correction models of equivalent size. This improvement stems from the Stream Aligner paradigm, where more tokens are generated directly by the Stream Aligner model, which is typically smaller and faster than the upstream model. Moreover, the first-token latency of Aligner is 10 times higher than that of Stream Aligner. This is because, in Stream Aligner, the first token can be output immediately after generating the first suffix, whereas in Aligner, the first token is unavailable until the entire answer is generated."}, {"title": "4 Interpretability of the Stream Aligner", "content": "Similar to Aligner, we observed a distinct correction mechanism in Stream Aligner models: the correction behavior is not binary between correct and copy, but rather a conditional paradigm, and the degree of reference to the original suffix and the extent of extra correction mostly depends on the quality of the original suffix. To demonstrate that Stream Aligner has learned this correction mechanism as a representation, we conduct the experiment based on representation engineering (Zou et al. 2023) and activation steering (Turner et al. 2023; Li et al. 2024). Specifically, we perform representation extraction and Linear Artificial Tomography (LAT) scan to the Stream Aligner model trained on Llama2-7B for math tasks. We then utilized the extracted representation to control the Stream Aligner's generation.\nAs shown in Figure 9, the ratio of adding (or subtracting, if the ratio is below zero) the representation vector in the Stream Aligner activation will affect the quantity of correction performed to the original suffix, ranging from directly copying the original response to substantially increasing the extent of normal correction. This provides strong evidence that Stream Aligner has internalized the correction paradigm as a representation, just as the Aligner. After confirming the effectiveness of the representations, we can conduct a deeper analysis of the relationship between the representations and the number of layers as mentioned in Figure 8, from which we can harvest two insights:\n\u2022 The similarity of correction mechanism across tasks. The correction mechanism of Stream Aligner is quite similar to Aligner in terms of representations, where both first decide on the extent of additional correction to introduce and then implement this decision in the remaining layers. This aligns with our initial intent when designing the correction module: building an implicit discriminator and generator within the model.\n\u2022 The difference between helpful & harmless QA and math correction tasks. On the other hand, the number of decision layers involved in introducing additional correction in Stream Aligner seems to be significantly more than in Aligner, and similarly, Stream Aligner slightly exceeds Aligner in the decision-making for copying. This not only confirms the complexity of mathematical tasks compared to helpful & harmless QA but also aligns with the intuitive approach to mathematical tasks: in mathematics, identifying the exact location of an error is usually the main task in correcting it."}, {"title": "5 Related Work", "content": "Inference strategies refinement These works aim to refine the pipeline inference strategies of the Transformer to acquire better performance without additional training of original models (Chen et al. 2023; Xu et al. 2024; Lu et al. 2023), providing lightweight yet quite effective alignment methods after training. For example:\n\u2022 IPA (Lu et al. 2023) incorporates a lightweight policy to replace the calculation of the next-token probability and thereby optimize the performance of the generation, but it directly needs the logit distribution of upstream models.\n\u2022 Speculative sampling (Chen et al. 2023) is another inference strategies refinement approach, where a draft model generates multiple token predictions in parallel, which are then selected by a reject sampling policy. Speculative sampling focuses more on accelerating the generation, rather than performing alignment at inference time.\nIn our work, the Stream Aligner is incorporated into the generation pipeline of upstream models, but it does not directly require the logits and the weights of upstream models, instead, it induces the policy of upstream models and can be applied to various upstream models with only once training.\nIncorporating additional model to the inference pipeline These works aim to distill the alignment strategies into an additional model that is incorporated into the inference pipeline without accessing the internal parameters of the upstream models (Ji et al. 2024a; Welleck et al. 2022; Yang and Klein 2021; Dathathri et al. 2019). For example:\n\u2022 Self-correction (Welleck et al. 2022) trains a self-corrector using an online sampling process, and uses this corrector to correct the output of upstream models directly.\n\u2022 RAIN (Li et al. 2023) applies a self-evaluation and correction mechanism to refine the output of the upstream model, thereby achieving self-alignment.\n\u2022 Proxy-tuning (Liu et al. 2024) is a lightweight decoding-time algorithm that uses a small tuned model (expert) and its untuned version (anti-expert) to guide the predictions of a large pretrained model, by applying a logit offset based on the differences between the expert and anti-expert outputs.\nCompared to previous works, Stream Aligner has the following strengths:\n\u2022 Stream Aligner focuses more on eliciting the latent knowledge of the upstream model. In fact, Stream Aligner utilizes sentence-level distribution induction rather than directly performing correction on the upstream model output. This successfully elicited the knowledge of upstream models, making Stream Aligner less reliant on the capabilities of additional models.\n\u2022 Stream Aligner helps to make the alignment methods more lightweight, enabling the use of smaller additional models and balancing between training stage efficiency and inference stage effectiveness.\nCombined with the advantages of the Stream Aligner method, this results in a better user experience."}, {"title": "6 Conclusion", "content": "We introduce the Streaming Distribution Induce Aligner (Stream Aligner), a novel alignment paradigm that better elicits the latent knowledge of the upstream model and combines efficiency with enhanced performance in various tasks throughout the generation process. Stream Aligner has achieved good results in both model size and performance. In helpful & harmless QA, the Stream Aligner-2B has managed to improve the helpfulness of the Llama2-70B-chat model by 41.2%, and harmlessness by 36.0%. Furthermore, the Stream Aligner-8B has achieved an improvement of 3.5% in the math ability of the tested Llama3-70B-Instruct model.\nLimitations While Stream Aligner introduces significant improvements in aligning LLMs through sentence-level dynamic correction, several limitations are left unhandled in this paper, and need further exploration: (1). Although Stream Aligner employs smaller models compared to its predecessor, Aligner, it still introduces additional computational overhead during the inference phase. (2). Stream Aligner's performance needs relatively high-quality training data, and since Stream Aligner uses smaller models, it naturally has trouble dealing with extremely difficult out-of-distribution inputs. (3). Limited to the computation resources, Stream Aligner only focuses on two tasks: helpful & harmless QA and math, representing value alignment and knowledge alignment tasks."}, {"title": "A Appendix: Details about Experiments Set-Up", "content": "Training and Evaluation Datasets\nHelpful & Harmless QA We randomly selected 20,000, 10,000, and 10,000 entries respectively from the helpful, harmless, and red-team subsets of HH-RLHF (Bai et al. 2022a), using a weighted sampling method where the weights were determined by taking the square root of the length of each prompt. After merging these samples and removing duplicate prompts, we obtained a training dataset consisting of 22,495 entries. Using the same method, we then extracted an additional 700 entries from PKU-SafeRLHF for the test dataset, ensuring there were no duplicates with the training dataset.\nMath We selected questions from every type and the first three difficulty levels of MATH (Hendrycks et al. 2021). We took all the 3,504 questions from the train split as the prompt of the train set and randomly selected 700 questions from the test split as the test set.\nIn Table 2 we will display some examples of our two training datasets.\nEvaluation Calculation Methods\nFor helpful and harmless QA tasks, we utilize GPT-4 to annotate preferences for the original and corrected answers. Subsequently, we compute the helpfulness and harmlessness win rates using the following formula:\n= 3\n= \\frac{\u039d\u03c9 - \u039d\u03b9}{Nw + Ni + Ne} 100%\n(4)\nwhere w represents the success rate, while Nw, Ne, and Ni denote the counts of wins, draws, and losses for the correctional answers.\nAs for math tasks, we still use GPT-4 to determine whether the given answer is aligned with the ground truth answer included in the dataset and GPT-4 query, and we calculate the accuracy of each model on the test set. We then compute the win rate by directly subtracting the accuracy of the original answer from the accuracy of the corrected answer.\nGPT-4 Evaluation\nWe use GPT-4 to evaluate the results. For helpful and harmless QA, the prompt used by GPT-4 is shown in Table 4 and Table 5. For math tasks, the prompt used by GPT-4 is shown in Table 3\nHyperparameter of Stream Aligner Training\nThe hyperparameters of Stream Aligner training and inference are shown in Table 6 and Table 7.\nDetails of Interpretability experiments\nIn Section 4, we interpret the correction paradigm of the Stream Aligner using representation engineering methods. To acquire the representation vector, we primarily used the representation reading methods given by (Zou et al. 2023). Specifically, given a decoder Stream Aligner model M, a template t(qi, ai, ci) which maps a tuple of question, answer, and correction(give it a miss when correction is empty) to the model input, a set of question-answer pair Sqa, we first generate the corresponding correction of each question-answer pair by our Stream Aligner to form full stimuli set Sqac:\nSqac = {qi, ai, Ci | Ci = M[t(qi, Ai)],\n(qi, ai) \u2208 Sqa}\nNext, we compute and collect two sets of neural activity based on copy and correction set using a function R(M, t(, )) that returns the representation of given model and prompt:\nAcorrection = {R(M, t(qi, ai, ai,0..k)) |\n(qi, ai, Ci) \u2208 Sqac,\nfor 0 < k < max(|ai|, |ci|)}\nAcopy = {R(M, t(qi, ai, Ci,0..k)) |\n(qi, ai, Ci) \u2208 Sqac,\nfor 0 < k < max(|ai|, |ci|)}\nGiven these two activation sets, we can acquire the hidden state of each set: Hcorrection, Hcopy and perform dimension reduction(in this case, we simply used PCA) to the normalized diff of hidden state to get the representation vector:\nV = PCA{normalized(Hcorrection Hcopy)\n| for 0 < i < |Hcorrection|}\nWe further utilized this representation vector to evaluate the correction activation scale r on layer l and generated token k:\nr(l,k) = R(M,t(qi, ai, Ci,0..k))[I]T \u2022 V\nTo evaluate the effectiveness of this representation vector, we used it to control the behavior of Stream Aligner and assessed the degree to which the corrections were influenced by measuring the Levenshtein Ratio between the controlled corrections and the original responses. For a linear control scale a and original model M, we can acquire the controlled model M' by directly adding the vector to the residual stream:\n\u039c'\u03bf = \u039c\u03bf + a. V\nFor answer a and correction c, the Levenshtein Ratio of the correction c is defined by\nDL(a, c) = \\frac{L(T(a), T(c))}{|T(a)|}\nThus, the Average Levenshtein Ratio for given dataset Sqa and controlled model M' is\nSqa\nDL,avg = \\frac{1}{Sqa} DL(ai, Ci),\ni=0\nwhere ci = M'[t(qi, ai)], and (qi, ai) \u2208 Sqa\nSqa"}, {"title": "B Ethical Statement", "content": "With its comprehensive composition of preference ranking annotations concerning helpfulness and harmlessness, the Stream Aligner model and dataset hold immense potential as a resource for developing beneficial AI assistants aligned with optimal helpfulness and harmlessness along with enhancing the reasoning of LLMs. However, we acknowledge an inherent risk: the same dataset could theoretically be used to train Al assistants in a harmful or malicious manner. As the creators of the Stream Aligner model and dataset, we are committed to fostering the development of helpful, safe AI technologies and have no desire to witness any regression of human progress due to the misuse of these technologies. We emphatically condemn any malicious usage of the Stream Aligner dataset and advocate for its responsible and ethical use."}]}