{"title": "Multi-Scale Incremental Modeling for Enhanced\nHuman Motion Prediction in Human-Robot\nCollaboration", "authors": ["Juncheng Zou"], "abstract": "Accurate human motion prediction is crucial for safe human-robot collaboration but remains\nchallenging due to the complexity of modeling intricate and variable human movements. This\npaper presents Parallel Multi-scale Incremental Prediction (PMS), a novel framework that\nexplicitly models incremental motion across multiple spatio-temporal scales to capture subtle joint\nevolutions and global trajectory shifts. PMS encodes these multi-scale increments using parallel\nsequence branches, enabling iterative refinement of predictions. A multi-stage training procedure\nwith a full-timeline loss integrates temporal context. Extensive experiments on four datasets\ndemonstrate substantial improvements in continuity, biomechanical consistency, and long-term\nforecast stability by modeling inter-frame increments. PMS achieves state-of-the-art performance,\nincreasing prediction accuracy by 16.3%-64.2% over previous methods. The proposed multi-scale\nincremental approach provides a powerful technique for advancing human motion prediction\ncapabilities critical for seamless human-robot interaction.", "sections": [{"title": "1 Introduction", "content": "Human-robot collaboration presents a transformative prospect applicable to diverse scenarios\nsuch as manufacturing, surgery, disaster relief, and home care. However, effective human motion\nforecasting over long time horizons remains an open challenge. Illustratively, envision an\nautomated production set-up wherein robots operate alongside humans. In such instances, the\nnecessity for robots to precisely anticipate human motions and intentions for the dual purpose of\nefficient operation and safety are paramount. A flawed estimation of a human's planned action\ncould culminate in accidents, productivity interruptions, or efficiency drop-offs. This necessitates\nan accurate prognostication of human movements, which forms the bedrock of our research.\nDespite the evident advantages of such interactions, they frequently confront intractable hurdles\ndue to the inherent complexity, variability, and unpredictability associated with human movement."}, {"title": "2 Related Work", "content": "Human motion prediction has attracted substantial research attention, with various modeling\nparadigms explored. In this section, we provide a comprehensive review of the most relevant\nexisting techniques, organized into three main categories: deep learning-based methods,\ninter-frame variation modeling, and motion feature-based approaches."}, {"title": "2.1 Deep Learning-based Methods", "content": "Deep learning techniques have extensively contributed to progress in human motion\nprediction with their adaptive capacity of capturing spatial-temporal dependencies in data.\nHowever, the recognition of intricate dependencies remains a challenge to be addressed.\nConvolutional Neural Networks (CNNs) [36, 37], for instance, show promising results in\nextracting local motion features and encoding spatial relationships [5-7]. They may find it\nchallenging, however, to handle the complex multi-scale feature fusion and modeling long-term\ndependencies since they primarily operate on fixed-sized local receptive fields, thus capturing\nglobal contextual information might be challenging [40, 53].\nOn the other hand, Graph Convolutional Networks (GCNs) have demonstrated considerable\nadeptness at identifying intricate spatial relationships between body joints and effectively\nintegrating multi-scale features [8-15]. Nonetheless, it's worth considering that these networks\nmay encounter hurdles when faced with spatial-temporal data irregularities such as inconsistency\nin motion sequence lengths or missing data points. These challenges arise from GCN's inherent\nassumptions about consistently structured graphs and fixed input sizes [41, 54]. This necessitates\nintense hyperparameter tuning, including selecting the right graph convolution operators,\nneighborhood definitions, and regularization strategies, to derive the best possible performance\nfrom GCNs [55].\nContinuing this trend, transformer-based models [16-19] have recently attracted a lot of\nattention due to their proficiency in capturing long-range dependencies in time series data. It is,"}, {"title": "2.2 Inter-frame Variation Modeling", "content": "Explicitly modeling the incremental transformations between frames has emerged as key for\nensuring temporally coherent predictions over long horizons. Methods have aimed to strike a\nbalance between precision and computational complexity. Most existing approaches focus on\npredicting absolute human joint positions, neglecting the importance of modeling inter-frame\nmotion transformations. However, incremental transformation prediction has been shown to be\nless susceptible to accuracy degradation due to incorrect actions and is vital for achieving\ncontinuous long-term prediction [45, 46].\nSeveral methods have been proposed to explicitly model inter-frame variations, such as\nmutation networks [23], geodesic loss functions [24], spatio-temporal branch networks [1],\nresidual blocks [2], and differential generation combined with attention mechanisms [3]. These\napproaches aim to capture local and global motion information, improve temporal connectivity,\nand enhance long-term prediction accuracy.\nHowever, even these promising methods come with some limitations. For instance, geodesic\nloss-based methods can be computationally intensive due to the calculation of geodesic distances\non non-Euclidean manifolds [24]. Furthermore, some commonly used techniques like attention\nmechanisms might incur high computational costs when dealing with long sequences or\nhigh-dimensional data [3], and the performance of residual blocks heavily depends on careful\nhyperparameter tuning [2]."}, {"title": "2.3 Motion Feature-based Approaches", "content": "Incorporating explicit motion domain knowledge and biomechanical constraints have shown\npromise in improving prediction plausibility. However, balancing model interpretability, accuracy\nand efficiency remains an open challenge.\nSeveral techniques for incorporating motion features and prior knowledge into prediction\nmodels have been explored. Some of these features can be derived from physical laws [27-30],\nmotion phases and subsequent elements [31-33], or specific techniques designed to reduce\nprediction errors [34, 35]. Notably, physics-based methods apply principles of human dynamics\nand biomechanics to add physical constraints and plausibility in motion predictions [27-30]. On\nthe downside, these methods might be computationally demanding due to the simulation of\ncomplex physical interactions and might fail to capture both physical and cognitive elements of\nhuman motion.\nDifferent methods have been proposed including the decomposition of motion into phases or"}, {"title": "3 Method", "content": null}, {"title": "3. 1 Methodology Overview", "content": "Problem Definition: The research problem tackled in this paper is human motion prediction,\nspecifically, forecasting a sequence of L future pose frames based on K observed frames. Here,\nthe total sequence length is T = K + L. The input comprises K historic observations X1:K =\n{X1, ..., XK} of dimension (K, N, 3), with each Xt \u2208 RN\u00d7D_representing the human pose at time t\nas N joints in D spatial dimensions.\nSpecifically, the i-th pose frame Pt contains N joint coordinate tuples Pt = {Ji,1,Ji,2, \u2026, Ji,n},\nwhere each tuple Ji,k = {Xi,k, Yi,k, Zi,k} gives the 3D coordinates of joint k.\nThe objective is to predict future poses XK+1:K+L = {XK+1, ..., XK+L} that minimize the error\nbetween the predictions and ground truth poses XK+1:K+L = {XK+1, ..., XK+L}. The predictions are\nof dimension (L, N, 3)."}, {"title": "3.2 PMS network", "content": "Existing methods for human motion prediction have limitations in modeling long-term\ndependencies. Prior works have explored pose-based prediction [20], multi-stage frameworks with\ngraph convolutions [40], and motion attention mechanisms [47]. However, they lack sufficient\ncapacity to capture joint dynamics across long time horizons.\nTo address these challenges, we propose a parallel prediction approach with a multi-scale\nincremental modeling for improved long-term forecasting. The proposed model (see Figure 1)\ncomprises five sets of normalized joint position coordinates from a human skeleton model as input.\nSpecifically, our contributions include:\n1) A multiscale computation module calculating the velocity and acceleration differences\nfrom the input to capture multi-scale motion dynamics, shown in blue and orange in Figure 1,\nrespectively. This module advances frame-level [47] or subsequence-level modeling by\nrepresenting joint dynamics across multiple time intervals. By modeling these incremental\nchanges in joint velocity and acceleration across different timescales, our approach can effectively\nmine informative prior patterns from historical motion data at various granularities. The rationale\nbehind this is that human motion comprises both short-term and long-term dynamics, and\ncapturing these incremental changes at multiple timescales allows the model to better represent\nand learn these intricate patterns.\n2) A fusion module, which is the first column in Figure 1, then synthesizes the velocity and\nacceleration differences into multi-scale features. A dedicated differential learning module (shown\nin green in Figure 1) predicts velocity increments and acceleration changes using these fused\nfeatures as input. The velocity prediction is consequently corrected by the acceleration prediction\nbefore the final adjustment of the motion prediction (shown in yellow in Figure 1). The fusion\nmodule is updated by an iterative adjustment mechanism and can represent global dynamics in\nparallel across time scales, in contrast to sequential prediction [40]. This parallel and iterative\napproach aims to capture the complex interplay between velocity and acceleration dynamics,\nenabling more accurate and robust long-term motion modeling.\n3) An optimized full-time loss function incorporating long-range context across horizons,\nadvancing limited context in prior losses [20, 40, 47]. By considering past, present, and future\ntime steps in the loss function, the model can better learn and account for both short-term and\nlong-term dependencies, leading to improved predictions across the entire motion sequence."}, {"title": "1. Normalization", "content": "To enhance the capability of our network to learn motion patterns, each\nmotion sequence X1:K = {X1, ..., XK} for a specific action X, is subjected to a normalization\nprocess. The normalization process involves several steps:\ni) We first determine the maximum values {Xmax, Ymax, Zmax} and minimum values\n{Xmin, Ymin, Zmin} along the x, y, and z dimensions respectively, of action X.\nii) We then centered the interval by subtracting the mean of the maximum and minimum\nvalues from the original coordinates.\n$X_r = X - (X_{max} + X_{min})/2$\n$Y_r = Y - (Y_{max} + Y_{min})/2$\n$Z_r = Z - (Z_{max} + Z_{min})/2$\n(1)\nWhere xr, yr, and zy are the original coordinates.\niii) Each centered element X, is then scaled to the range [-1,1] by division with half of the\nrespective range yielding normalized coordinates:\n$\nX\" = \\frac{X_r}{|X_{max} - X_{min}|/2}$\n$\nY\" = \\frac{Y_r}{|Y_{max} - Y_{min}|/2}$\n$\nZ\" = \\frac{Z_r}{|Z_{max} - Z_{min}|/2}$\n(2)\n{x\", y\", z\"} denotes the normalized coordinates of X."}, {"title": "2. Multi-scale incremental fusion parallel prediction module", "content": "This module aims to capture both short-term and long-term motion patterns by modeling\nincremental changes in joint velocity and acceleration across multiple timescales.\nModule 1: Incremental computation. The velocity and acceleration differences are\ncomputed over three different time intervals: 6 = 10, 5, and 2 frames (Equation 3). This allows\nthe model to represent variations in short-term and long-term dependencies of motion patterns.\nCapturing finer-grained incremental motions helps retain precise pose details over time rather than\nmean pose.\n{Xr:r+10, Xr+10:r+20, Xr+20:r+30, Xr+30:r+40, Xr+40:r+50}\n{Xr:r+5, Xr+5:r+10, Xr+10:r+15, Xr+15:r+20, Xr+20:r+25}\n{Xr:r+2, Xr+2:r+4, Xr+4:r+6, Xr+6:r+8, Xr+8:r+10}\n(3)\nThe speed difference is calculated at three time intervals:\n\u0394Xt = Xr+k*\u03b4:r+(k+1)*\u03b4 - Xr+(k-1)*\u03b4:r+k*\u03b4\n(4)\nHere, AX denotes the k-th velocity difference, r denotes the starting position of the\ninterval calculation, k denotes the number of intervals, and 8 denotes the time interval (10, 5,\n2).\nThe acceleration difference is calculated from the velocity difference:\n\u0394\u0394Xt = \u0394Xt+1 \u2013 \u0394Xt\n(5)"}, {"title": "Module 2: Incremental Learning Module", "content": "Let AAX denote the k-th acceleration difference.\nThe velocity and acceleration differences are then combined using multi-scale weighted\ncombinations (Equations (6) and (7)) to synthesize comprehensive velocity and acceleration\nfeatures. This weighted combination allows the model to capture motion patterns at different\ntimescales, with the weights determining the relative importance of each timescale. By integrating\nvelocity and acceleration differences over multiple time scales, the model can capture both\nshort-term and long-term motion patterns to improve modeling and prediction. Multi-scale\nweighted combination to synthesize velocity features:\nX^{vt} = \\sum_{i=1}^{N1} a_i \\Delta X_i^{vt}\n(6)\nAmong them, Xt represents the speed difference with an interval of 8, a is the coefficient\nof the speed difference, \u03a3\u03af=1 \u03b1\u03af = 1, the setting of the coefficient will significantly affect the\nlearning and prediction of the network. N1 is the number of speed differences.\nMulti-scale weighted combination comprehensive acceleration feature:\nX^{at} = \\sum_{i=1}^{N2} \\beta_i \\Delta \\Delta X_i^{at}\n(7)\nThe X said for delta acceleration difference between integrated, \u1e9ei the delta for\nacceleration differential coefficient, \u03a3\u2081 \u03b2 = 1. N2 is the number of acceleration differences.\nBy integrating velocity and acceleration differences over multiple time scales, the model can\ncapture both short-term and long-term motion patterns to improve modeling and prediction.\nModule 2: Incremental Learning Module. This module consists of two submodules:\nVelocity Difference Incremental Learning and Acceleration Incremental Difference Learning.\nTheir primary function is to predict future motion increments.\nIn the velocity difference learning process (Equations (8) to (11)), the synthesized velocity\nfeatures are first processed by a fully connected layer, followed by an Long Short-Term Memory\n(LSTM) layer to capture temporal dependencies. The output of the LSTM is then passed through\ntwo additional fully connected layers with batch normalization and dropout for regularization.\nThis submodule learns to predict the velocity increments based on the input velocity features.\nSpeed difference learning:"}, {"title": null, "content": "$\\overline{^1X_t} = X_t W + b$\n(8)"}, {"title": null, "content": "Here, Wi and bi represent the network layer weights and offsets for X velocity synthesis,\nrespectively. The LSTM layer is used to capture temporal dependencies.\n$\\overline{^2X_t} = LSTM(\\overline{^1X_t})$\n(9)\nFully connected layers, regularized using batch Normalization (BN) and dropout (DP):\n$\\overline{^3X_t} = \\sigma (DP(BN(\\sum_{m=1}^{K} \\overline{^2X_t} W + b)))$\n(10)\nHere, BN represents batch normalization, DP represents dropout, and \u03c3represents the\nactivation function. W and be denote the network layer weights and offsets of 2X velocity\nsynthesis, respectively.\n$\\overline{^4X_t} = \\sum_{m=1}^{K} \\overline{^3X_t}W + b$\n(11)\nSimilarly, in the acceleration difference learning process (Equations (12) to (15)), the\nsynthesized acceleration features are processed through the same network architecture to predict\nthe acceleration increments. Acceleration difference learning:\n$\\underline{^1X_t} = \\sum_{m=1}^{K} XW + b$\n(12)\n$\\underline{^2X_t} = LSTM( \\underline{^1X_t})$\n(13)\n$\\underline{^3X_t} = \\sigma (DP(BN(\\sum_{m=1}^{K} \\underline{^2X_t}W2 + b2)))$\n(14)\n$\\underline{^4X_t} = \\sum_{m=1}^{K} \\underline{^3X_t}W+b$\n(15)\nThe predicted acceleration increment is then used to correct the predicted velocity increment\n(Equation (16)). This step accounts for the interplay between velocity and acceleration dynamics,\naiming to improve the overall motion prediction accuracy. Correction of acceleration to velocity:\n$\\overline{^4X_t} = \\overline{^4X_t} + \\underline{^4X_t}$\n(16)\nFinally, the corrected velocity increment is used to predict the future pose by applying an\nattenuation regulation mechanism (Equation (17)). The attenuation coefficients yn allow the\nmodel to adjust the predicted pose based on the velocity dynamics, further enhancing the\nprediction robustness. Prediction of attenuation regulation based on velocity:\nXr+k*\u03b4:r+(k+1)*\u03b4 = Xr+k*d:r+(k+1)*\u03b4 - \u03a3nYnX\n(17)"}, {"title": "3. Comprehensive Time Loss Function", "content": "During the motion optimization step, to enable the\nmodel to learn both short-term and long-term motion dependencies, we propose the\ncomprehensive time loss function, La. This function optimizes the motion by jointly considering a"}, {"title": null, "content": "single loss Lc, a past loss Lp, and a future loss Lf. This intricate configuration allows us to cater\nto both local and global dependencies more effectively. By directly comparing the predicted poses\nto the ground truth at each precise timeframe, our full temporal loss function helps retain precise\npose details over the past, present, and future prediction horizons. This prevents averaging and\nblurring of poses over time.\nLa = Lp + Lc + Lf\n(18)\nLp is the sum of the L1 loss between the predicted 3D joint positions and ground truth X\nover 4 past frames, summed over 4 = {2, 5, 10} frames:\n$\\ Lp = \\sum_{t-\u0394:t} \\sum_{\u0394\u2208{2,5,10}} \\sum_{t=T-\u0394}|X_{t-\u0394|t} - X_{t}$|\n(19)\nHere, T denotes the length of the unit sequence.\nLe is the L1 loss between predicted and ground truth pose at the current time t:\n$\\ L_c = \\sum_{t=0}^{T}|x_t - x_t|$\n(20)\nLf is the sum of L1 loss between predicted and ground truth pose X, over 4 future frames,\nsummed over 4 = {20, 30} frames:\n$\\ L_f = \\sum_{t:t+\u0394} \\sum_{\u0394\u2208{20, 30}} |x_{t+\u0394} - X_{t+\u0394}| $\n(21)"}, {"title": "4 Experiment", "content": "In this study, we aim to address the following research questions:\nRQ1: How does the proposed Parallel Multi-scale Incremental Prediction (PMS) method\ncompare with existing state-of-the-art human motion prediction methods, and what is its overall\nperformance?\nRQ2: What insights can be gleaned from the experimental results regarding the contribution\nof the different components of PMS to its performance, and what are its potential limitations?\nTo answer these research questions, we conducted a comprehensive set of experiments on\nmultiple benchmark datasets. The experimental setup and implementation details are presented in\nSections 4.1 and 4.2, respectively. Section 4.3 addresses RQ1 by comparing the performance of\nPMS with various baseline methods, while Section 4.4 provides insights into RQ2 by analyzing\nthe contribution of different components and discussing the limitations of the proposed approach.\nStatistical analyses, including measures of central tendency and variability, are provided to support\nthe claims and comparisons made."}, {"title": "4.1 Data setup", "content": "1) Human3.6M[48]: This dataset comprises 3.6 million motion sequences spanning 15 action"}, {"title": "4.2 Implementation details", "content": "Implementation Details: All experiments were executed on an NVIDIA RTX 3090 24G using the\nPaddlePaddle framework [54]. All models were trained using the Adam optimizer [55] with an\ninitial learning rate of 5e-3. For the Human3.6M dataset, we will first train each class of actions 10\ntimes, then train 10 times by combining the single loss and the total loss of the accumulated 5\nlosses, and finally reduce the learning rate to le-3 for another 10 times. The training process also\nincluded ten iterations combining the long-term prediction loss. The memory consumption during"}, {"title": "4.3 Comparison with existing methods (RQ1)", "content": "1) Baseline\nTo comprehensively evaluate the performance of the proposed PMS method, we compared it\nwith 14 state-of-the-art baseline models spanning various architectures, including Graph\nConvolutional Networks (GCNs) [8, 36, 38, 42, 45, 46], fully connected layers [37, 43],\nConvolutional Neural Networks (CNNs) [6], Recurrent Neural Networks (RNNs) [39], Generative\nAdversarial Networks (GANs) [44], attention-based models [47], and stage-based models [40, 41].\nThe selection of these baseline models was based on their relevance to the problem domain\nand their reported state-of-the-art performance in human motion prediction tasks. By comparing\nagainst a diverse set of architectures and approaches, we aimed to provide a comprehensive\nevaluation of the proposed PMS method's performance relative to existing techniques.\nThe results of the comparison with existing methods are presented and analyzed for each\ndataset in the following subsections.\n2) Results\nH3.6M: The Mean Per Joint Position Error (MPJPE) comparison of our model's results on the\nHuman3.6M dataset for short-term and long-term predictions is presented in Tables 1 and 2.\nFigures 3 and 4 illustrate the comparison of the mean MPJPE values for each action class.\nUpon analyzing Table 1, it is evident that in the short-term prediction of the Human3.6M\ndataset, our PMS model falls short of the Motionmixer and Rele-GCN models in two time\nintervals of the \"Walking\" action. This is primarily due to insufficient prediction at 320ms and\n400ms. Figure 3 demonstrates that the Motionmixer, Rele-GCN, and our PMS models outperform\nother methods in the short-term prediction of various types of actions. Except for the \"Walking\"\naction, the PMS model significantly surpasses other methods.\nThe PMS model, an MLP-based approach, outperforms other methods for all other actions,\nparticularly for \"Sitting down\" and \"Taking Photo\" actions. For the \"Sitting down\" action, it is\n64.2%, 57.4%, 49.0%, and 44.4% lower than the second-best method, Motionmixer, across four\nprediction periods. For the \"Taking Photo\" action, it is 63.6%, 54.8%, 43.9%, and 38.7% lower"}, {"title": null, "content": "than Motionmixer across four prediction cycles. From the \"Average\" analysis, it is 70.0%, 47.0%,\n29.4%, and 22.0% lower than Motionmixer across four time intervals.\nThese results indicate that the proposed PMS method significantly improves short-term\nprediction, and the speed incremental learning mode of PMS enhances performance. Figure 3 also\nshows that the MPJPE of the prediction method fluctuates depending on the actions, and\nmitigating such fluctuations is crucial for improving the generalization ability of the prediction\nmethod.\nThe mean MPJPE of each action can be calculated according to the MPJPE of each method\nin all actions, and the action variance of each method can be calculated according to the mean.\nFinally, the influence degree of the action on the prediction of the method can be judged according\nto the method. Upon computation, it was found that the action variance of the PMS method was\n3.8. This value surpasses the second-best action variance of 6.0 from the Motionmixer method and\nthe third-best action variance of 6.6 from the Rele-GCN method. These findings indicate that the\npredictive performance of the PMS method is less susceptible to variations in actions."}, {"title": "5 Discussions", "content": "While the proposed PMS method demonstrates promising results and outperforms several\nstate-of-the-art approaches, there are some limitations that warrant further investigation and\nimprovement:\n1. Prediction of actions with large joint position changes: As observed in the qualitative\nanalysis, the PMS method struggles to accurately predict actions involving substantial changes in\njoint positions and movements with significant trend alterations. This limitation may arise from\nthe inherent challenges in modeling and capturing the complex dynamics of such motions within\nthe current framework.\n2. Generalization across diverse action types: Although the PMS method exhibits improved\ngeneralization compared to some existing methods, there is still room for enhancement in terms of\nconsistent performance across a wider range of action types. Certain actions, such as \"Basketball\"\nand \"Basketball signal\" in the CMU dataset, were less effectively learned by the PMS method,\nsuggesting the need for further improvements in the model's ability to generalize to diverse motion\npatterns.\n3. Long-term prediction accuracy: While the PMS method demonstrates improved long-term\nprediction performance compared to baselines, the prediction errors tend to accumulate and\nbecome more significant as the prediction horizon increases, as evidenced by the larger MPJPE\nvalues at longer time intervals (e.g., 1000ms).\nFuture research directions to address these limitations and further enhance the PMS method\ncould involve:\n1. Exploring more advanced architectures and techniques capable of better capturing and\nmodeling the complex dynamics of human motion, particularly for actions with large joint\nposition changes and abrupt trend alterations.\n2. Incorporating additional data augmentation techniques or transfer learning strategies to\nimprove the model's generalization capabilities across diverse action types and motion patterns.\n3. Investigating techniques for mitigating error accumulation in long-term predictions, such\nas incorporating additional constraints, leveraging hierarchical or multi-scale representations, or\nemploying techniques from sequence-to-sequence learning.\n4. Exploring the integration of additional modalities or contextual information (e.g., scene\ngeometry, object interactions) to enhance the model's understanding and prediction of human\nmotion in real-world scenarios.\n5. Evaluating the PMS method on larger and more diverse datasets to further validate its\nperformance and identify potential areas for improvement.\nBy addressing these limitations and exploring new research directions, the PMS method can\nbe further refined and extended, ultimately contributing to more accurate and robust human\nmotion prediction systems with practical applications in various domains."}, {"title": "6 Conclusion", "content": "This study proposed a novel Parallel Multi-scale Incremental Prediction (PMS) framework\nfor accurate human motion forecasting. The primary contributions are threefold:1) A multi-scale\ncomputation module capturing motion dynamics across multiple time scales by modeling velocity\nand acceleration differences in parallel branches. 2) A fusion module synthesizing multi-scale\ndynamics, enabling iterative differential learning and correction of motion predictions. 3) An\noptimized full-time loss function incorporating long-range context to better capture short-term and\nlong-term dependencies. Extensive experiments validated PMS's efficacy, achieving\nstate-of-the-art performance with 16.3%-64.2% accuracy improvements over previous methods by\nenhancing biomechanical consistency, continuity, and long-term stability.\nThe significance and potential impact of this work are broader than the technical\ncontributions alone. By achieving more accurate and stable long-term motion prediction, PMS\npaves the way for safer, seamless human-robot collaboration across many realms including\nmanufacturing, healthcare services, home assistance care, and beyond. The abilities to forecast\ncomplex human behaviors and preemptively adapt could revolutionize how machines interact with\nand aid people in diverse environments. We encourage further exploration into alternative\nrepresentations, attentions mechanisms, and contextual fusion techniques to handle even more\nerratic, rapidly changing motions over long horizons. There remain open challenges, but the\nadvancements made here impart momentum and inspiration to enhance prediction\ncapabilities-not just marginal gains in accuracy metrics, but toward unraveling the intricacies of\ndynamic motor coordination through modeling, prediction algorithmic innovations. The future\npromises great progress at this exciting intersection bridging data sciences, human movement\nbiomechanics, and interactive robotics."}]}