{"title": "EasyRec: Simple yet Effective Language Models for Recommendation", "authors": ["Xubin Ren", "Chao Huang"], "abstract": "Deep neural networks have become a powerful technique for learning representations from user-item interaction data in collaborative filtering (CF) for recommender systems. However, many existing methods heavily rely on unique user and item IDs, which limits their ability to perform well in practical zero-shot learning scenarios where sufficient training data may be unavailable. Inspired by the success of language models (LMs) and their strong generalization capabilities, a crucial question arises: How can we harness the potential of language models to empower recommender systems and elevate its generalization capabilities to new heights? In this study, we propose EasyRec - an effective and easy-to-use approach that seamlessly integrates text-based semantic understanding with collaborative signals. EasyRec employs a text-behavior alignment framework, which combines contrastive learning with collaborative language model tuning, to ensure a strong alignment between the text-enhanced semantic space and the collaborative behavior information. Extensive empirical evaluations across diverse real-world datasets demonstrate the superior performance of EasyRec compared to state-of-the-art alternative models, particularly in the challenging text-based zero-shot recommendation scenarios. Furthermore, the study highlights the potential of seamlessly integrating EasyRec as a plug-and-play component into text-enhanced collaborative filtering frameworks, thereby empowering existing recommender systems to elevate their recommendation performance and adapt to the evolving user preferences in dynamic environments.", "sections": [{"title": "1 INTRODUCTION", "content": "Deep learning has established itself as a highly promising and powerful solution for capturing user preferences in the context of online recommender systems [42, 50]. This approach harnesses the power of deep neural networks to learn rich and meaningful user and item representations by analyzing the complex patterns of user-item interactions. This, in turn, enables recommendation algorithms to accurately infer user preferences and provide highly relevant and personalized recommendations [30, 41].\nIn recent years, there has been a notable surge of advancements in enhancing recommender systems through the incorporation of neural network-powered collaborative filtering frameworks, particularly in the domain of graph neural networks (GNNs) [10, 32, 37]. By effectively leveraging the inherent graph structure present in the data, GNNs have demonstrated exceptional capabilities in capturing high-order relationships and complex dependencies among users and items. Notable examples of such GNN-based approaches include NGCF [32] and LightGCN [10]. These methods have showcased impressive performance in recommendation tasks by virtue of their ability to model the collaborative signals present in the data through recursive message passing mechanisms.\nThe issue of data scarcity in recommender systems poses a significant challenge for existing deep collaborative filtering models, hindering their ability to learn precise user/item representations, particularly when dealing with sparse interaction data [9, 18, 26, 33]. This challenge primarily arises from the limited availability of training labels, which in turn impedes the models' capacity to capture the intricate relationships and dependencies that exist between users and items. To alleviate data scarcity effects, recent studies explored the potential of self-supervised learning to provide effective data augmentation. For example, in terms of contrastive augmentation, methods like SGL [34] and NCL [19] leverage graph contrastive learning to supplement the supervised recommendation task with the cross-view mutual information maximization. For generative augmentation, approaches such as AutoCF [36] are built upon the masked autoencoding mechanism, enabling the models to reconstruct the interaction structures for self-supervision.\nWhile the recent advancements in self-supervised learning techniques have offered promising avenues for mitigating the impact of data scarcity in current collaborative filtering models, they also come with a significant limitation [45]. This limitation stems from the inherent design of the models, which heavily relies on using unique identities (IDs) to represent users and items throughout the entire process of learning representations. In practical recommenders, however, we often come across new recommendation data collected from different domains or time periods, involving diverse sets of users and items. This creates a challenge as existing ID-based recommendation models struggle to effectively incorporate and adapt to such new data, especially when there is a change in the set of identity tokens of users and items for zero-shot recommendation. The rigid dependence on user and item IDs in these models hinders their ability to generalize and perform well in scenarios where the user and item spaces are not static or fully overlapping. For instance, the constant generation of new items like videos and social media content necessitates accurate recommendations in real-life recommender systems, even when there are limited interaction observations. While cross-domain recommendation methods [1, 39] draw inspiration from leveraging information and knowledge across multiple domains to enhance recommendation performance, they often assume that users from different domains belong to the same set [3, 39]. Unfortunately, this assumption significantly restricts the flexibility and generalization capabilities of recommenders. As a result, existing methods may encounter difficulties in adapting and delivering accurate recommendations when confronted with diverse user populations across different domains.\nLanguage Models as Zero-Shot Recommenders. Considering the challenges and motivations discussed earlier, the objective of this study is to introduce a recommender system that functions as a zero-shot learner, possessing robust generalization capabilities and the flexibility to adapt to new recommendation data seamlessly. To accomplish this objective, we propose integrating language models with collaborative relation modeling, forming an effective text embedder-EasyRec that is both lightweight and effective. This integration seamlessly combines text-based semantic encoding with high-order collaborative signals, resulting in a recommender system that offers a strong model generalization ability.\nRecent research has explored leveraging large language models (LLMs) to enhance recommender systems. Existing approaches broadly fall into two categories. The first uses LLMs for data augmentation (e.g., RLMRec [25], AlterRec [15]), encoding textual information to complement collaborative filtering. While this combines LLM and collaborative strengths, these methods remain ID-based and struggle to generalize. The second approach utilizes LLMs to directly generate user-item interaction predictions based on language (e.g., LLaRA [17], COLLM [49]). However, such LLM-based recommenders face significant real-world limitations, suffering from poor efficiency (e.g., ~1 second per prediction), rendering them impractical for large-scale recommendation tasks. These challenges highlight the need for more efficient, scalable solutions that seamlessly integrate the semantic understanding of textual information with the collaborative strengths of zero-shot recommenders.\nOur proposed model has demonstrated superior performance in comparison to state-of-the-art language models, as illustrated in Figure 1. This performance advantage is achieved within a cost-efficient parameter space, ranging from 100 to 400 million parameters, with the computational cost of ~0.01 second per prediction. Notably, our model exhibits the scaling law phenomenon, where its performance continually improves as the parameter size is increased. Furthermore, in contrast to existing approaches that suffer from poor efficiency, our model is designed to be highly scalable and practical for large-scale recommendation tasks.\nIn summary, this work makes the following key contributions:\n\u2022 Motivation. The primary objective of this study is to introduce a novel recommender system that is built upon language models to function as a zero-shot learner. This innovative approach aims to demonstrate exceptional adaptability to diverse recommendation data, while also exhibiting robust generalization capabilities.\n\u2022 Methodology. To align text-based semantic encoding with collaborative signals from user behavior, we propose a novel contrastive learning-powered collaborative language modeling approach, which allows the system to capture both the semantic representations of users/items, as well as the underlying behavioral patterns and interactions within the recommendation data.\n\u2022 Zero-Shot Recommendation Capacity. The EasyRec is extensively evaluated through rigorous experiments as a text-based zero-shot recommender. Performance comparisons reveal that it consistently achieves significant advantages over baseline methods in terms of recommendation accuracy and generalization capabilities. Furthermore, the study demonstrates the model's remarkable potential in generating dynamic user profiles that are highly adaptive to time-evolving user preferences.\n\u2022 Existing CF Model Enhancement. Our proposed EasyRec has been seamlessly integrated as a lightweight, plug-and-play component with state-of-the-art collaborative filtering (CF) models. The lightweight and modular design of our language model is a key strength, as it facilitates the adoption of our novel recommendation paradigm across a wide range of use cases."}, {"title": "2 PRELIMINARIES", "content": "In recommender systems, we have a set of users U and a set of items I, along with the interactions between them (e.g., clicks, purchases). For each user \\(u \\in U\\), we define \\(N_u\\) as the set of items that user u has interacted with. Likewise, for each item \\(i \\in I\\), we define \\(N_i\\) as the set of users who have interacted with that item. To represent these user-item interactions, we can use an interaction matrix \\(A_{|U|\\times|I|}\\), where the entry \\(A_{u,i}\\) is 1 if user u has interacted with item i, and 0 otherwise. The primary goal of a recommender model is to estimate the probability \\(p_{u,i}\\) of a future interaction between a user u and an item i. This predicted probability can then be used to generate personalized item recommendations for each user, tailored to their individual preferences and past behavior.\nText-based Zero-Shot Recommendation is essential in recommender systems, as it can address the common cold-start problem. In real-world scenarios, new users and new items often lack sufficient interaction data, making it challenging to provide effective personalized recommendations. By leveraging the textual descriptions of users and items, such as product titles, details, and user profiles, the language models can construct semantic representations to enable text-based recommendations for these cold-start situations. This overcomes the limitations of traditional collaborative filtering methods, by offering a distinct advantage over the traditional ID-based paradigm. By leveraging robust language models, this approach exhibits remarkable potential in \"zero-shot\u201d scenarios, where the testing data have not been previously encountered.\nFormally, we define \\(P_u\\) and \\(P_i\\) as the generated text-based profiles of user u and item i, respectively, which are encoded into"}, {"title": "3 METHODOLOGY", "content": "In this section, we first discuss how we gather textual profiles for users and items in recommender systems, which are essential for pre-training and evaluating our model. Next, we'll dive into the specifics of EasyRec and its training approach. Lastly, we'll introduce our method for diversifying user profiles, which improves the model's ability to adapt to various situations."}, {"title": "3.1 Collaborative User and Item Profiling", "content": "In real-world recommenders, the only available information may be raw text data, such as product titles and categories, associated with the items. Privacy concerns often make it difficult to collect comprehensive user-side information. Furthermore, directly leveraging such textual information may overlook the crucial collaborative relationships needed for accurate user behavior modeling and preference understanding. To address these limitations, we propose to generate textual profiles by leveraging large language models (e.g., GPT, LLAMA series) [25] to inject collaborative information into the textual profiles. This allows us to capture both the semantic and collaborative aspects of the items in a unified textual profile."}, {"title": "3.1.1 Item Profiling.", "content": "Given the raw item information, such as title \\(h_i\\), categories \\(c_i\\), and description \\(d_i\\) (e.g., book summary), we aim to generate a comprehensive item profile \\(P_i\\) that captures both the semantic and collaborative aspects. To reflect user-item interactions, we incorporate the textual information (e.g., posted reviews \\(r_{u,i}\\)) provided by the item's corresponding users. Formally, the item profile generation process is:\n\\[P_i = \\text{LLM}(M_i, h_i, c_i, \\{r_{u,i}\\}) \\text{ or } \\text{LLM}(M_i, h_i, d_i),\\]\nHere, \\(M_i\\) represents the generation instruction, while \\(h_i\\) and \\(d_i\\) serve as input. By leveraging large language models (LLMs), we can generate a concise yet informative item profile \\(P_i\\)."}, {"title": "3.1.2 User Profiling.", "content": "In practical scenarios, privacy concerns often limit the feasibility of generating user profiles based on demographic information. Instead, we can profile users by considering their collaborative relationships, using the generated profile information from their interacted items. This approach allows the user profiles to effectively capture the collaborative signals that reflect their preferences. Formally, the user profile generation process is:\n\\[P_u = \\text{LLM}(M_u, \\{h_i, P_i, r_{u,i}|i \\in N_u\\}).\\]\nHere, \\(M_u\\) represents the instruction for generating the user profile using a large language model (LLM). We sample a set of interacted items \\(N_u\\) from the user's purchase history. We then combine the user's feedback \\(r_{u,i}\\) with the pre-generated item profiles \\(P_i\\) to create the user's text description \\(P_u\\), which captures their preferences."}, {"title": "3.1.3 Advantages of Collaborative Profiling.", "content": "Our collaborative user and item profiling framework offers two key advantages for real-world recommendation scenarios which are elaborated as:\n\u2022 Collaborative Information Preserved. Our collaborative profiling approach goes beyond just the original textual content, also capturing the semantics of user/item characteristics and their interaction patterns. By encoding these rich profiles into a shared feature space using a recommendation-oriented language model, the resulting embeddings of interacted users and items are brought closer together. This enables the recommenders to better identify relevant matches, even for \"zero-shot\" users and items (those without prior interactions) which are ubiquitous in real-world scenarios. The system can leverage the collaborative signals encoded within the text-based profiles to make better recommendations, bridging the gap for these cold-start cases."}, {"title": "3.2 Profile Embedder with Collaborative LM", "content": "So far, we have generated rich textual profiles for users and items, moving beyond conventional ID-based embeddings. However, directly encoding these textual profiles into latent embeddings for making recommendations may have two key limitations:\n\u2022 Capturing Recommendation-Specific Semantics. The text-based embeddings, while expressive, may not be optimized for the specific semantics and relationships most relevant to the recommendation task. For example, consider two item profiles: (1) \"This user is passionate about advanced AI techniques, focusing on deep learning and AI research works\". (2) \"With a passion for advanced Al development, this user delves into science fiction and AI-themed novels\". Though the profiles share textual similarity about AI, their target audiences differ - the first caters to Al scientists, the second to sci-fi readers. Directly encoding these profiles may overlook nuanced, recommendation-specific semantics. Refinement is needed to better align embeddings with the specific context and requirements of the recommendation system, beyond just textual similarity.\n\u2022 Overlooking High-Order Collaborative Signals. While textual profiles offer rich semantic information, relying solely on them may cause us to overlook valuable high-order collaborative patterns that emerge from complex user-item interactions [32, 36]. These higher-order signals, such as transitive associations and community-level preferences, can provide additional insights that complement the textual data for user preference learning in recommender systems.\nTo address these limitations, we propose a collaborative language modeling paradigm that seamlessly integrates the strengths of the semantic richness of the profiles and the valuable collaborative signals encoded from complex user-item interaction behaviors."}, {"title": "3.2.1 Bidirectional Transformer Encoder as Embedder.", "content": "We leverage a multi-layer bidirectional Transformer encoder as the embedder backbone, considering two key benefits: 1) Efficient Encoding: The encoder-only architecture focuses solely on generating effective text representations, enabling faster inference in recommendation systems. 2) Flexible Adaptation: By building on pre-trained Transformer models, we can leverage transfer learning to optimize the embedder for specific recommendation tasks.\nLet's consider a user's profile as a passage of n words: \\(P = w_1,..., w_n\\). We start by adding a special token [CLS] at the beginning of the word sequence. The tokenization layer then encodes the input sequence into initial embeddings, which serve as the input for the Transformer layers:\n\\[\\{x_{[CLS]}^{(0)},..., x_n^{(0)}\\} = \\text{Tokenization}(\\{w_{[CLS]},..., w_n\\}).\\]\nHere, \\(x^{(0)}\\in \\mathbb{R}^{d}\\) is the embedding retrieved from the embedding table corresponding to the tokens, and the (0) superscript indicates that it is the input to the (0)-th layer of the language model. The tokenization process also adds positional embeddings to the initial embeddings. The language model then encodes a sequence of final embeddings (one for each token):\n\\[\\{e_{[CLS]},..., e_n\\} = \\text{Encoder}(\\{x_{[CLS]}^{(0)},..., x_n^{(0)}\\}),\\]\nwhere Encoder(.) refers to the Transformer-based encoder-only language model. The key operation in the encoding process is the self-attention mechanism:\n\\[\\text{Attention}(Q, K, V) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d}})V\\]\nw.r.t. \\(Q = XW^Q\\), \\(K = XW^K\\), \\(V = XW^V\\).\nHere, \\(X \\in \\mathbb{R}^{n \\times d}\\) represents the stack of token embeddings, and \\(W^{Q/K/V}\\) are the parameter matrices that map these embeddings into queries, keys, and values. This self-attention mechanism allows each token to aggregate information from all other tokens, ensuring that each token is informed about the entire sequence. Finally, we select the first embedding \\(e_{[CLS]}\\), which corresponds to the [CLS] token, as the representative embedding for the entire profile. This embedding is then passed through a multi-layer perceptron to obtain the final encoded representation e, as mentioned in Eq.(1):\n\\[e = \\text{MLP}(e_{[CLS]}) = \\text{LM}(P).\\]\nWith these encoded text embeddings e for each user and item, we can predict the likelihood of interaction using cosine similarity and make recommendations as described in Eq.(2)."}, {"title": "3.2.2 Collaborative LM with Contrastive Learning.", "content": "The motivation behind fine-tuning the collaborative language model using contrastive learning is to effectively capture and incorporate high-order collaborative signals into the recommendation model. Traditional recommenders using Bayesian Personalized Ranking (BPR) [27] optimize encoded embeddings with only one negative item per training sample. This approach limits the model's ability to capture complex global user-item relationships."}, {"title": "3.3 Augmentation with Profile Diversification", "content": "The goal of our profile diversification approach is to enhance the model's ability to generalize to unseen users and items. Representing each user or item with a single profile inherently limits the diversity of the representations, which can negatively impact the model's performance and generalization. To address this, we propose augmenting the existing user/item profiles to allow for multiple profiles per entity. These augmented profiles capture the same semantic meaning, such as the personalized interaction preferences of users or the varied characteristics of items. Our two specific augmentation methods introduce controlled variations in the profiles while preserving the core semantic meaning.\nInspired by self-instruction mechanisms [40, 40], large language models (LLMs) can be leveraged to rephrase user or item profiles while preserving their underlying meaning. This allows generating multiple semantically similar yet distinctly worded profiles from a single input. Applying this iterative rephrasing process can create a diverse set of augmented profiles, substantially expanding the available training data. This data augmentation technique is particularly valuable when the original dataset is limited, as the LLM-generated profiles can improve model generalization and robustness.\nBy leveraging large language models for profile diversification, we can create a set of diverse profiles for each user u and item i.\n\\[\\{P\\}_u = \\{P_u; P_1, P_2, ..., P_t\\},\\]\n\\[\\{P\\}_i = \\{P_i; P_1, P_2, ..., P_t\\}.\\]"}, {"title": "4 EVALUATION", "content": "This section evaluates the performance of the proposed EasyRec framework in addressing the following research questions (RQs):\n\u2022 RQ1: How effectively does the proposed EasyRec perform in matching unseen users and items (zero-shot) within text-based recommendation scenarios?\n\u2022 RQ2: How effectively does EasyRec integrate with and enhance various recommenders within text-based collaborative filtering scenarios, leveraging its capabilities as a language embedder?\n\u2022 RQ3: How effective is our proposed profile diversification mechanism for data augmentation in improving the performance of the recommendation language model?\n\u2022 RQ4: How well can our proposed text-based EasyRec paradigm adapt to accommodate changes in users' dynamic preferences?"}, {"title": "4.1 Experimental Settings", "content": "4.1.1 Datasets. To assess our proposed model's capability in encoding user/item textual profiles into embeddings for recommendation, we curated diverse datasets across various domains and platforms. A portion was used for training, while the remainder served as test sets for zero-shot evaluation.\n4.1.2 Evaluation Protocols. We employ two commonly used ranking-based evaluation metrics, Recall@N and NDCG@N, to"}, {"title": "4.2 Performance Comparision for Text-based Recommendation (RQ1)", "content": "We evaluate the performance of various language models (LMs) for zero-shot text-based recommendation on the unseen Sports, Steam, and Yelp datasets. This approach directly leverages the encoded embeddings derived from user/item profiles to make recommendations, without any additional training on the target datasets.\n4.2.1 Baseline Methods and Settings. For our comparative evaluation, we included a diverse set of language models as text embedders: (i) General Language Models: BERT [4], ROBERTa [21], and BART [14]; (ii) Language Models for Dense Retrieval: SimCSE [7], GTR [23], and BGE [38]; (iii) Pre-trained Language Models for Recommendation: BLaIR [12]. Additionally, we also compared against the state-of-the-art text embedding models provided by OpenAI.\n4.2.2 Result Analysis. The overall comparison of different models is presented in Table 3. This evaluation reveals several noteworthy observations, which are outlined below:\n\u2022 Superiority across Diverse Datasets. Our evaluation consistently shows that the EasyRec outperforms all other models across the three datasets spanning different platforms. This provides strong evidence for the effectiveness of the EasyRec. We attribute these improvements to two key factors: i) By injecting collaborative signals into the language models, we effectively optimized our EasyRec using supervised contrastive learning within the recommendation context. This approach allows the model to inherently encode user and item text embeddings that are well-suited for recommendation tasks. ii) By integrating a diverse array of datasets across multiple categories and utilizing data augmentation techniques to enrich the text descriptions for training, our EasyRec exhibits impressive generalization capabilities, enabling it to effectively handle unseen data.\n\u2022 Scaling Law Investigation of EasyRec Model. Our experiments revealed that as the size of the EasyRec model increases (from small to large), its performance consistently improves across all three datasets. This observation reflects a scaling law,"}, {"title": "4.2.3 Impact of Training Objectives.", "content": "Furthermore, we evaluate the impact of different training objectives on the language model's learning process. To this end, we implemented EasyRec-Large training using BPR loss (i.e., one negative item per training sample) for comparison with the contrastive learning results. This approach allows us to directly assess how the choice of training objective influences model performance. As shown in Table 4, the performance of the model trained with contrastive learning generally outperforms that of the model trained with BPR loss. This outcome highlights the effectiveness of employing contrastive learning to better incorporate collaborative information into the language models, thereby enhancing their overall performance in recommendation tasks."}, {"title": "4.3 Performance of Text-enhanced CF (RQ2)", "content": "In addition to our investigation of zero-shot recommendation scenarios, we explore the potential of EasyRec as an enhancement when integrated with CF models. To assess the effectiveness of various LMs in CF, we employ two widely used ID-based methods as backbone models: GCCF [2] and LightGCN [10], which were chosen for their proven effectiveness and efficiency. Furthermore, we utilize the advanced model-agnostic text-enhanced framework RLMRec [25] with contrastive alignment to conduct our investigation."}, {"title": "4.4 Effectiveness of Profile Diversification (RQ3)", "content": "In this section, we examine the impact of diversifying user and item profiles with large language models (LLMs) on model performance. As mentioned in Section 3.3, we perform LLM-based diversification three times on the original generated profiles. This process continuously increases the number of profiles in the training set. To investigate whether data augmentation positively affects model performance, we conduct experiments with three variants of the EasyRec under different numbers of diversified profiles.\n\u2022 Effectiveness of Profile Diversification. The increase in the number of diversified profiles (from 0 to 3) enhances model performance, particularly for larger models. This finding underscores the effectiveness of our augmentation approach using LLMs for profile diversification, and emphasizes the significance of increasing training data for improved outcomes.\n\u2022 Scaling Relationship: The scaling experiments on both model size and data size reveal a crucial relationship that influences model performance. This demonstrates that our approach of training the language model with collaborative signals follows a scaling law, indicating that model performance benefits from both increased capacity and data volume. Such scaling laws are vital as they provide insights into how model capacity and data availability interact, guiding future research and development."}, {"title": "4.5 Model Fast Adaptation Case Study (RQ4)", "content": "As mentioned in Section 3.1.3, a key advantage of EasyRec is its ability to empower recommender systems to efficiently adapt to shifts"}, {"title": "5 RELATED WORK", "content": "LMs-Powered Recommender Systems. Recent advancements in recommender systems increasingly incorporate textual modalities [45], thus enhancing traditional approaches. The semantic representations encoded by pre-trained language models are essential features for improving recommender models, particularly in click-through rate prediction [8, 35] and transferable sequence recommendations [13]. These embeddings capture informative content relevant to recommendation tasks [28]. Some works also leverage text-based agents to enhance performance [47, 48]. A notable recent contribution is RLMRec [25], a text-enhanced framework that improves ID-based recommenders using principles from information theory. However, many prior studies have relied on general text embeddings, such as BERT-based models [5, 45] or proprietary OpenAI embeddings [25], rather than those specifically tailored for recommendation purposes. A recent work BLAIR [12] leverages the item metadata and interaction-level user feedback on this item for LMs training, yielding promising results in query-based item retrieval. In contrast, our approach assigns each user and item a collaboratively generated profile that reflects their preferences. EasyRec optimizes the learned correlations between user and item entities using CF signals, which not only demonstrates impressive zero-shot performance but also enhances text-augmented results.\nCross-Domain Recommendation. The fundamental concept behind cross-domain recommendation is to enhance recommendations in one domain by leveraging data from another domain, which is typically more abundant, to address data sparsity and improve personalization [46]. For instance, the graph collaborative filtering network [20] has been introduced to aggregate both common and domain-specific user features using Graph Neural Networks (GNNs) that capture high-order user-item connections. Recent studies have further enriched cross-domain recommender systems through the integration of self-supervised learning techniques. For example, C2DSR [1] utilizes contrastive learning with both single-domain and cross-domain representations. CCDR [39] proposes intra-domain and inter-domain contrastive learning to enhance recommendation performance. Moreover, SITN [29] employs self-attention modules as sequence encoders to represent two user-specific sequences from the source and target domains, followed by contrastive learning between them. However, a significant limitation in current cross-domain research is its reliance on correlations (e.g., overlapping users) between source and target data, which constrains its applicability and generalizability. In contrast, text-based zero-shot learning does not has such constraint, allowing for effective transfer even across different datasets. Our proposed EasyRec takes a fundamental step forward in this line of research.\nGraph Collaborative Filtering for Recommendation. Graph Neural Networks have recently emerged as a promising approach for recommendation systems, enabling the modeling of complex user-item interactions and capturing high-order dependencies in recommendation [6]. These models are capable of learning representations of users and items by aggregating their interactions in a graph structure, such as PinSage [43], NGCF [32], and Light-GCN [10]. To enhance the representation capacity of GNNs against data sparsity in recommender systems, further studies realize the marriage between self-supervised learning and collaborative filtering with data augmentation. Examples include SGL [34], SimGCL [44], and HCCF [37]. These developments showcase the potential of self-supervised graph learning in powering recommenders through graph augmentation based on node self-discrimination."}, {"title": "6 CONCLUSION", "content": "The EasyRec framework effectively integrates LMs to enhance recommendation tasks. Our new paradigm, which is both straightforward and effective, has consistently proven to excel across various scenarios, including text-based zero-shot recommendation and text-enhanced collaborative filtering. At the heart of EasyRec's success lies an innovative methodology that combines collaborative language model tuning with the transformative capabilities of contrastive learning. This unique approach has empowered EasyRec to capture nuanced semantics and high-order collaborative signals-critical elements that have been instrumental in driving remarkable improvements in recommendation performance. Our extensive experiments, which span a diverse array of datasets, have consistently validated the superiority of EasyRec over existing language models. This robust and generalized performance underscores the framework's remarkable capacity to adapt to dynamic user preferences, making it well-suited for real-world industry scenarios. Furthermore, the consistent improvements observed across different settings indicate that EasyRec is not only effective but also versatile in its application. Looking ahead, the potential for EasyRec to be seamlessly integrated with multi-modal information presents an enticing frontier for our future investigations."}]}