{"title": "SPARSE AUTOENCODERS REVEAL UNIVERSAL FEATURE SPACES ACROSS LARGE LANGUAGE MODELS", "authors": ["Michael Lan", "Philip Torr", "Austin Meek", "Ashkan Khakzar", "David Krueger", "Fazl Barez"], "abstract": "We investigate feature universality in large language models (LLMs), a research field that aims to understand how different models similarly represent concepts in the latent spaces of their intermediate layers. Demonstrating feature universality allows discoveries about latent representations to generalize across several models. However, comparing features across LLMs is challenging due to polysemanticity, in which individual neurons often correspond to multiple features rather than distinct ones. This makes it difficult to disentangle and match features across different models. To address this issue, we employ a method known as dictionary learning by using sparse autoencoders (SAEs) to transform LLM activations into more interpretable spaces spanned by neurons corresponding to individual features. After matching feature neurons across models via activation correlation, we apply representational space similarity metrics like Singular Value Canonical Correlation Analysis to analyze these SAE features across different LLMs. Our experiments reveal significant similarities in SAE feature spaces across various LLMs, providing new evidence for feature universality.", "sections": [{"title": "INTRODUCTION", "content": "Large language models (LLMs) have demonstrated remarkable capabilities across a wide range of natural language tasks (Bubeck et al., 2023; Naveed et al., 2024; Liu et al., 2023). However, understanding how these models represent and process information internally remains a significant challenge (Bereska & Gavves, 2024). A key question in this domain is whether different LLMs learn similar internal representations, or if each model develops its own unique way of encoding linguistic knowledge. This question of feature universality is crucial for several reasons: it impacts the generalizability of interpretability findings across models, it could accelerate the development of more efficient training techniques, and it may provide insights into safer and more controllable AI systems (Chughtai et al., 2023; Gurnee et al., 2024; Sharkey et al., 2024; Bricken et al., 2023).\nComparing features across LLMs is inherently difficult due to the phenomenon of polysemanticity, where individual neurons often correspond to multiple features rather than distinct ones Elhage et al. (2022). This superposition of features makes it challenging to disentangle and match representations across different models. To address these challenges, we propose a novel approach leveraging sparse autoencoders (SAEs) to transform LLM activations into more interpretable spaces. SAEs allow us to decompose the complex, superposed representations within LLMs into distinct features that are easier to analyze and compare across models (Makhzani & Frey, 2013; Cunningham et al., 2023; Bricken et al., 2023). We then apply specific representational space similarity metrics to these SAE features, enabling a rigorous quantitative analysis of feature universality across different LLMs (Klabunde et al., 2023). Our methodology involves several key steps: obtaining SAEs trained on the activations of multiple LLMs to extract interpretable features, developing and applying representational similarity"}, {"title": "BACKGROUND", "content": "Sparse Autoencoders. Sparse autoencoders (SAEs) are a type of autoencoder designed to learn efficient, sparse representations of input data by imposing a sparsity constraint on the hidden layer (Makhzani & Frey, 2013). The network takes in an input $x \\in \\mathbb{R}^n$ and reconstructs it as an output $\\hat{x}$ using the equation $\\hat{x} = W'\\sigma(Wx + b)$, where $W \\in \\mathbb{R}^{h \\times n}$ is the encoder weight matrix, $b$ is a bias term, $\\sigma$ is a nonlinear activation function, and $W'$ is the decoder matrix, which often uses the transpose of the encoder weights. SAE training aims to both encourage sparsity in the activations $h = \\sigma(Wx + b)$ and to minimize the reconstruction loss $||x - \\hat{x}||^2$.\nThe sparsity constraint encourages a large number of the neuron activations to remain inactive (close to zero) for any given input, while a few neurons, called feature neurons, are highly active. Recent work has replaced the regularization term by using the TopK activation function as the nonlinear activation function, which selects only the top K feature neurons with the highest activations (Gao"}, {"title": "Representational Space Similarity", "content": "Representational Space Similarity: Representational similarity measures typically compare neural network activations by assessing the similarity between activations from a consistent set of inputs (Klabunde et al., 2023). In this paper, we take a different approach by comparing the representations using the decoder weight matrices $W'$ of SAEs, where the columns correspond to the feature neurons. We calculate a similarity score $m(W, W')$ for pairs of representations $W$ in SAE A and $W'$ from SAE B. We obtain two scores via the following two representational similarity metrics:\nSingular Value Canonical Correlation Analysis (SVCCA). Canonical Correlation Analysis (CCA) seeks to identify pairs of canonical variables, $u_i$ and $v_i$, from two sets of variables, $X \\in \\mathbb{R}^{n \\times d_1}$ and $Y \\in \\mathbb{R}^{n \\times d_2}$, that are maximally correlated with each other (Hotelling, 1936). Singular Value Canonical Correlation Analysis (SVCCA) (Raghu et al., 2017) enhances CCA by first applying Singular Value Decomposition (SVD) to both $X$ and $Y$, which can be expressed as:\n$X^T X = U_xS_xV_x^T, Y = U_yS_yV_y^T$\nwhere $U_x$ and $U_y$ are the matrices containing the left singular vectors (informative directions), and $S_x$ and $S_y$ are diagonal matrices containing the singular values. By projecting the original data onto these informative directions, noise is reduced. SVCCA then applies CCA on the transformed datasets, $U_x$ and $U_y$, resulting in correlation scores between the most informative components. These scores are averaged to obtain a similarity score.\nRepresentational Similarity Analysis (RSA). Representational Similarity Analysis (RSA) (Kriegeskorte et al., 2008) works by first computing, for each space, a Representational Dissimilarity Matrix (RDM) $D \\in \\mathbb{R}^{n \\times n}$, where each element of the matrix represents the dissimilarity (or similarity) between every pair of data points within a space. The RDM essentially summarizes the pairwise distances between all possible data pairs in the feature space. A common distance metric used is the Euclidean distance. After an RDM is computed for each space, a correlation measurement like Spearman's rank correlation coefficient is applied to the pair of RDMs to obtain a similarity score. RSA has been applied to measure relational similarity of stimuli across brain regions and of activations across neural network layers (Klabunde et al., 2023).\nSVCCA vs RSA. SVCCA focuses on the comparison of entire vector spaces by aligning the principal components (singular vectors) of two representation spaces and then computing the canonical correlations between them. In other words, it identifies directions in the space that carry the most variance and checks how well these align between two models or layers. On the other hand, RSA is primarily concerned with the structure of relationships, or relational structure, between different data points (like stimuli) within a representation space. It allows us to measure how well relations such as king-queen transfer across models. Thus, SVCCA focuses on comparing the aligned subspaces of two representation spaces, while RSA is more suitable for analyzing the structural similarity between representations at the level of pairwise relationships."}, {"title": "METHODOLOGY", "content": "Representational similarity is important for capturing the across-model generalizations of both: 1) Feature Spaces, which are spaces spanned by groups of features, and 2) Feature Relations. We compare SAEs trained on layer $A_i$ from LLM A and layer $B_i$ from LLM B. This is done for every layer pair. To compare latent spaces using our selected metrics, we have to solve both permutation and"}, {"title": "EXPERIMENTS", "content": "EXPERIMENTAL SETUP\nLLM Models. We compare LLMs that use the Transformer model architecture (Vaswani et al., 2017). We compare models that use the same tokenizer because the mean activation correlation pairing relies on comparing two activations using the same tokens. We compare the residual streams of Pythia-70m, which has 6 layers and 512 dimensions, vs Pythia-160m, which has 12 layers and 768 dimensions (Biderman et al., 2023). We compare the residual streams of Gemma-1-2B, which has 18 layers and 2048 dimensions, to Gemma-2-2B, which has 26 layers and 2304 dimensions Team et al. (2024a;b).\nSAE Models. For Pythia, we use pretrained SAEs with 32768 feature neurons trained on residual stream layers from Eleuther (EleutherAI, 2023). For Gemma, we use pretrained SAEs with 16384 feature neurons trained on residual stream layers (Lieberum et al., 2024) We access pretrained Gemma-1-2B SAEs for layers 0, 6, 10, 12, and 17, and Gemma-2-2B SAEs for layers 0 to 26, through the SAELens library (Bloom & Chanin, 2024).\nDatasets. We obtain SAE activations using OpenWebText, a dataset that scraps internet content (Gokaslan & Cohen, 2019). We use 100 samples with a max sequence length of 300 for Pythia for a total of 30k tokens, and we use 150 samples with a max sequence length of 150 for Gemma for a total of 22.5k tokens. Top activating dataset tokens were also obtained using OpenWebText samples.\nComputing Resources. We run experiments on an A100 GPU."}, {"title": "SAE SPACE SIMILARITY", "content": "In summary, for almost all layer pairs after layer 0, we find the p-value of SAE experiments for SVCCA, which measures rotationally-invariant alignment, to be low; all of them are between 0.00 to 0.01 using 100 samples. While RSA also passes many p-value tests, their values are much lower,"}, {"title": "SIMILARITY OF SEMANTICALLY MATCHED SAE FEATURE SPACES", "content": "We find that for all layers and for all concept categories, Test 2 described in \u00a73 is passed. Thus, we only report specific results for Test 1 in Tables 1 and 2. Overall, in both Pythia and Gemma models and for many concept categories, we find that semantic subspaces are more similar to one another than non-semantic subspaces.\nPythia-70m vs Pythia-160m. We compare every layer of Pythia-70m to every layer of Pythia-160m for several concept categories. While many layer pairs have similar semantic subspaces. Middle layers appear to have the semantic subspaces with the highest similarity. Table 1 demonstrates one example of this by comparing the SVCCA score for layer 3 to layer 5 of Pythia-160m, which shows high similarity for semantic subspaces across models, as the concept categories all pass Test 1, having p-values below 0.05. We show the scores for other layer pairs and categories in Figures 15 and 17 in Appendix E, which include RSA scores in Table 9.\nGemma-1-2B vs Gemma-2-2B. In Table 2, we compare L12 of Gemma-1-2B vs L14 of Gemma-2- 2B. As shown in Figure 7, this layer pair has a very high similarity for most concept spaces; as such, they likely have high similar semantically-meaningful feature subspaces. Notably, not all concept"}, {"title": "RELATED WORK", "content": "Superposition and Sparse Autoencoders. Superposition is a phenomenon in which a model, in response to the issue of having to represent more features than it has parameters, learns feature representations distributed across many parameters (Elhage et al., 2022). This causes its parameters, or neurons, to be polysemantic, which means that each neuron is involved in representing more than one feature. This leads to issues for interpretability, which aims to disentangle and cleanly identify features in models in order to manipulate them, such as by steering a model away from deception (Templeton et al., 2024). To address the issue of polysemanticity, Sparse Autoencoders (SAEs) have been applied to disentangle an LLM's polysemantic neuron activations into monosemantic \"feature neurons\", which are encouraged to represent an isolated concept (Makhzani & Frey, 2013; Cunningham et al., 2023; Gao et al., 2024; Rajamanoharan et al., 2024a;b). Features interactions in terms of circuits have also been studied (Marks et al., 2024).\nFeature Universality. To the best of our knowledge, only Bricken et al. (2023) has done a quantitative study on individual SAE feature similarity for two 1-layer toy models, finding that individual SAE feature correlations are stronger than individual LLM neuron correlations; however, this study did not analyze the global properties of feature spaces. Gurnee et al. (2024) found evidence of universal neurons across language models by applying pairwise correlation metrics, and taxonimized families of neurons based on patterns of downstream, functional effects. O'Neill et al. (2024) discover \"feature families\", which represent related hierarchical concepts, in SAEs across two different datasets. Recent work has shown that as vision and language models are trained with more parameters and with better methods, their representational spaces converge to more similar representations (Huh et al., 2024).\nRepresentational Space Similarity. Previous work has studied neuron activation spaces by utilizing metrics to compare the geometric similarities of representational spaces (Raghu et al., 2017; Wang et al., 2018; Kornblith et al., 2019; Klabunde et al., 2024; 2023; Kriegeskorte et al., 2008). It was found that even models with different architectures may share similar representation spaces, hinting at feature universality. However, these techniques have yet to be applied to the feature spaces of sparse autoencoders trained on LLMs. Moreover, these techniques compare the similarity of paired input activations. Our work differs as it compares the similarity of paired feature weights. Essentially, previous works do not show that one can match LLMs by features that both LLMs find in common.\nFeature Manifolds. In real world data, features may lie on a manifold, where nearby features respond to similar data. (Olah & Batson, 2023; Bricken et al., 2023). Previous work has discovered the existence of features corresponding to months and days of the week arranged in a circular structure in SAE feature spaces across models (Engels et al., 2024). Studying how feature arrangements generalize across models may shed light on how features lie on a manifold.\nMechanistic Interpretability. Recent work has made notable progress in neuron interpretation (Foote et al., 2023; Garde et al., 2023) and interpreting the interactions between Attention Heads and MLPs (Neo et al., 2024). Other work in mechanistic interpretability has traditionally focused on 'circuits'-style analysis (Elhage et al., 2021), as well as the use of steering vectors to direct"}, {"title": "CONCLUSION", "content": "In this study, we address the underexplored domain of feature universality in sparse autoencoders (SAEs) trained on LLMs with multiple layers and across multiple model pairs. To achieve these insights, we developed a novel methodology to pair features with high mean activation correlations, and then assess their global similarity of the resulting subspaces using SVCCA and RSA. Our findings reveal a high degree of similarity in SAE feature spaces across various models. Furthermore, our research reveals that subspaces of features associated with specific semantic concepts, such as calendar or people tokens, demonstrate remarkably high similarity across different models. This suggests that certain semantic feature subspaces are universally encoded across varied LLM architectures. Our work enhances understanding of how LLMs represent information at a fundamental level, opening new avenues for further research into model interpretability and the transferability of learned features."}, {"title": "LIMITATIONS", "content": "We perform analysis on available pre-trained SAEs for similar LLMs that use the same tokenizer, and preferrably on SAEs with the same number of features to match them. However, there are a limited number of pre-trained SAEs on large models. Our study is also limited by the inherent challenges in interpreting and comparing high-dimensional feature spaces. The methods we used to measure similarity, while robust, may not capture all nuances of feature interactions and representations within the SAEs. Additionally, our analysis is constrained by the computational resources available, which may have limited the scale and depth of our comparisons. The generalizability of our findings to SAEs trained on more diverse datasets or specialized domain-specific LLMs remains an open question. Furthermore, the static nature of our analysis does not account for potential temporal dynamics in feature representation that might occur during the training process of both LLMs and SAEs."}, {"title": "FUTURE WORK", "content": "We performed similar analysis on the LLMs that the SAEs in this paper were trained on, and found that they have some similarity, but much lower than that of the SAEs, with no patterns showing that the middle layers were the most similar. Additionally, the LLMs did not have corresponding weights for the \"residual stream\", which is an addition of activations from the MLP and attention head layers, so we compared the MLPs of LLMs. Unlike the SAE dimensions, the MLP dimensions of the LLMs had different dimensions. Future work could study the comparison of LLM similarity by feature weights to SAEs. Additional experiments could include using ground-truth feature experiments for LLMs that partially share ground-truth features to see if SAEs are modeling the same features (Sharkey et al., 2022), and performing functional similarity experiments (Bricken et al., 2023)."}, {"title": "AUTHOR CONTRIBUTIONS", "content": "Michael conceived the study and designed the experiments with help from Fazl. Michael performed the experiments with help from Austin and led the writing of the manuscript. Philp, Ashkan and David provided advice throughout the project."}, {"title": "ILLUSTRATED STEPS OF COMPARISON METHOD", "content": "Figure 4 demonstrates steps 1 to 3 to carry out our similarity comparison experiments."}, {"title": "NOISE OF MANY-TO-1 MAPPINGS", "content": "Overall, we do not aim for mappings that uniquely pair every feature of both spaces as we hypothesize that it is unlikely that all the features are the same in each SAE; rather, we are looking for large subspaces where the features match by activations and we can map one subspace to another.\nWe hypothesize that these many-to-1 features may contribute a lot of noise to feature space alignment similarity, as information from a larger space (eg. a ball) is collapsed into a dimension with much less information (eg. a point). Thus, when we only include 1-1 features in our similarity scores, we receive scores with much less noise.\nAs shown in the comparisons of Figure 5 to Figure 6, we find the 1-1 feature mappings give slightly higher scores for many layer pairs, like for the SVCCA scores at L2 vs L3 for Pythia-70m vs Pythia- 160m, though they give slightly lower scores for a few layer pairs, like for the SVCCA scores at L5 vs L4 for Pythia-70m vs Pythia-160m. Given that most layer pair scores are slightly higher, we use 1-1 feature mappings in our main text results.\nAdditionally, we notice that for semantic subspace experiments, 1-1 gave vastly better scores than many-to-1. We hypothesize that this is because since these feature subspaces are smaller, the noise from the many-to-1 mappings have a greater impact on the score."}, {"title": "NOISE OF NON-CONCEPT FEATURES", "content": "We define \"non-concept features\" as features which are not modeling specific concepts that can be mapped well across models. Their highest activations are on new lines, spaces, punctuation, and EOS tokens. As such, they may introduce noise when computing similarity scores, as their removal appears to greatly improve similarity scores. We hypothesize that one reason they introduce noise"}, {"title": "CONCEPT CATEGORIES LIST AND FURTHER ANALYSIS", "content": "The keywords used for each concept group for the semantic subspace experiments are given in Table 4. We generate these keywords using GPT-4 Bubeck et al. (2023) using the prompt \"give a list of N single token keywords as a python list that are related to the concept of C\", such that N is a number (we use N=50 or 100) and C is a concept like Emotions. We then manually filter these lists to avoid using keywords which have meanings outside of the concept group (eg. \"even\" does not largely mean \"divisible by two\" because it is often used to mean \"even if...\"). We also aim to select keywords which are single-tokens with no spaces, and are case-insensitive.\nWhen searching for keyword matches from each feature's list of top 5 highest activating tokens, we use keyword matches that avoid features with dataset samples which use the keywords in compound words. For instance, when searching for features that activate on the token \"king\" from the People/Roles concept, we avoid using features that activate on \"king\" when \"king\" is a part of the word \"seeking\", as that is not related to the People/Roles concept.\nWe perform interpretability experiments to check which keywords of the features kept are activated on. We find that for many feature pairs, they activate on the same keyword and are monosemantic. Most keywords in each concept group are not kept. Additionally, for the layer pairs we checked on, after filtering there were only a few keywords that multiplied features fired on. This shows that the high similarity is not because the same keyword is over-represented in a space.\nFor instance, for Layer 3 in Pythia-70m vs Layer 5 in Pythia-160m, and for the concept category of Emotions, we find which keywords from the category appear in the top 5 activating tokens of each of the Pythia-70m features from the 24 feature mappings described in Table 1. We only count a feature if it appears once in the top 5 of a feature's top activating tokens; if three out of five of a feature's top activating tokens are \"smile\", then smile is only counted once. The results for Models A and B are given in Table 3. Not only are their counts similar, indicating similar features, but there is not a great over-representation of features. There are a total of 24 features, and a total of 25 keywords (as some features can activate on multiple keywords in their top 5). We note that even if a feature fires for a keyword does not mean that feature's purpose is only to \"recognize that keyword\", as dataset examples as just one indicator of what a feature's purpose is (which still can have multiple, hard-to-interpret purposes).\nHowever, as there are many feature pairs, we did not perform a thorough analysis of this yet, and thus did not include this analysis in this paper. We also check this in LLMs, and find that the majority of LLM neurons are polysemantic. We do not just check the top 5 dataset samples for LLM neurons, but the top 90, as the SAEs have an expansion factor that is around 16x or 32x bigger than the LLM dimensions they take in as input.\nCalendar is a subset of Time, removing keywords like \"after\u201d and \u201csoon\", and keeping only \"day\" and \"month\" type of keywords pertaining to dates."}]}