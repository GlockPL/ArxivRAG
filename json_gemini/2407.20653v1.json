{"title": "FACL-Attack: Frequency-Aware Contrastive Learning for Transferable Adversarial Attacks", "authors": ["Hunmin Yang", "Jongoh Jeong", "Kuk-Jin Yoon"], "abstract": "Deep neural networks are known to be vulnerable to security risks due to the inherent transferable nature of adversarial examples. Despite the success of recent generative model-based attacks demonstrating strong transferability, it still remains a challenge to design an efficient attack strategy in a real-world strict black-box setting, where both the target domain and model architectures are unknown. In this paper, we seek to explore a feature contrastive approach in the frequency domain to generate adversarial examples that are robust in both cross-domain and cross-model settings. With that goal in mind, we propose two modules that are only employed during the training phase: a Frequency-Aware Domain Randomization (FADR) module to randomize domain-variant low- and high-range frequency components and a Frequency-Augmented Contrastive Learning (FACL) module to effectively separate domain-invariant mid-frequency features of clean and perturbed image. We demonstrate strong transferability of our generated adversarial perturbations through extensive cross-domain and cross-model experiments, while keeping the inference time complexity.", "sections": [{"title": "Introduction", "content": "Deep neural networks have brought forth tremendous improvements in visual recognition tasks. However, the inherent transferable nature of adversarial examples still exposes the security vulnerability to malicious attackers targeting such susceptible classifiers, causing serious threats and undesirable outcomes in real-world applications. The majority of current attack methods can be primarily classified into two main categories: iterative or optimization-based approaches, and generative model-based approaches. Over the past years, iterative attack methods (Goodfellow, Shlens, and Szegedy 2015; Madry et al. 2017; Croce and Hein 2020; Lorenz et al. 2021; Dong et al. 2018; Xie et al. 2019; Lu et al. 2020; Naseer et al. 2020) have been the standard attack protocol for its simplicity and effectiveness. However, this iterative approach is frequently constrained by inefficient time complexity and the potential risk of over-fitting to the training data and models. Moreover, it has shown limited applicability in practical situations due to the low transferability to unknown models and domains.\nRegarding the transferability of adversarial attacks, threat model is typically carried out in three different settings (i.e., white-box, black-box, and strict black-box) depending on the prior knowledge of the model architecture and data distributions by the adversary. In each respective setting, the adversary has either complete knowledge of the target model profile (i.e., architecture and weights) and data distributions reflecting the target domain, query access to the limited black-box only, or no information at all. In this work, we specifically consider the strict black-box case in which the victim attributes are completely unknown to the attacker since such a scenario is commonly encountered in practical real-world settings. We believe that crafting adversarial examples in this strict black box setting has practical values towards stronger transferabilty, as well as safe and reliable deployment of deep learning models.\nIn this light, generative attacks (Poursaeed et al. 2018; Naseer et al. 2019; Nakka and Salzmann 2021; Naseer et al. 2021; Zhang et al. 2022) have recently gained attention, demonstrating the high transferability across unknown models and domains. Moreover, generator-based attacks yield lower time complexity than iterative or optimization-based methods in the inference stage, which is also a crucial part"}, {"title": "Related Work", "content": "for real-world attacks. While the current chain of generative attack methods (Poursaeed et al. 2018; Naseer et al. 2019, 2021; Nakka and Salzmann 2021; Zhang et al. 2022; Wu et al. 2020) are time-efficient and effective against various black-box settings, we remark that their methods do not actively leverage domain-related characteristics to facilitate more transferable attacks.\nIn that sense, our work is inspired by frequency domain manipulations (Yin et al. 2019; Wang et al. 2020a,b) in domain adaptation (DA) (Yang and Soatto 2020) and generalization (DG) (Huang et al. 2021; Xu et al. 2021), demonstrating the superior generalization capabilities of the trained model. As we target transferable attack on unknown target domains and victim models to boost the transferability in a similar setting, we seek to exploit domain-related characteristics from simple yet effective frequency manipulations.\nSeveral recent studies have focused on frequency-based adversarial attacks to manipulate adversarial examples, aimed at deeper understanding of their dataset dependency (Maiya et al. 2021), adversarial robustness (Duan et al. 2021), and the security vulnerability (Dziugaite, Ghahramani, and Roy 2016). With a slightly different motive, SSAH (Luo et al. 2022) aims to improve the perceptual quality, whereas (Guo, Frank, and Weinberger 2019) designs low-frequency perturbations to enhance the efficiency of black-box queries. Although low-frequency perturbations are efficient, they are known to provide less effective transfer between models (Sharma, Ding, and Brubaker 2019). As such, we delve deeper into frequency-driven approaches that effectively enhance the transferability of adversarial examples, especially crafted in a generative framework."}, {"title": "Generator-based Adversarial Attack", "content": "Generative attack (Poursaeed et al. 2018) employs the concept of adversarial training (Goodfellow et al. 2020) to create perturbations across entire data distributions. This is achieved by regarding a pre-trained surrogate model as a discriminator, and it is advantageous due to the ability of generating diverse forms of perturbations across multiple images simultaneously. Existing methods aim to enhance the generator training by leveraging both the cross-entropy (CE) loss (Poursaeed et al. 2018) and the relativistic CE loss (Naseer et al. 2019), improving the transferability across domains and models. Recent studies (Nakka and Salzmann 2021; Zhang et al. 2022) utilize features extracted from the mid-level layers of the surrogate model, which encompass a higher degree of shared information among different model architectures. We follow the traces of the recent works and explore a method to further enhance the transferability by introducing a novel perspective from the frequency domain."}, {"title": "Frequency-based Approach for Generalization", "content": "Convolutional neural networks are known to exhibit intriguing attributes within the frequency domain (Yin et al. 2019; Tsuzuku and Sato 2019; Yin et al. 2019; Wang et al. 2020a,b), demonstrating proficient generalization capability by effectively harnessing the band-specific information derived from Fourier filtering (Dziugaite, Ghahramani, and Roy 2016; Guo et al. 2017; Long et al. 2022). Spectral manipulations for enhancing the generalization capability can be achieved through simple yet powerful transformations like the Fast Fourier Transform (FFT), which dissects an image into amplitude components that vary across domains and phase components that remain consistent across different domains (Xu et al. 2021). The Discrete Cosine Transform (DCT) also serves as an efficient technique to decompose spectral elements into domain-agnostic mid-frequency components (mid-FCs) and domain-specific low- and high-FCs, which contributed to the effective spectral domain randomization in FSDR (Huang et al. 2021). In our work, we also employ the DCT to decompose images into domain-agnostic and domain-specific frequency components, facilitating the effective domain randomization and feature-level contrastive learning for transferable attacks."}, {"title": "Feature Constrastive Learning", "content": "Manipulating image representations in the feature space has demonstrated significant performance improvement in real-world scenarios characterized by domain shifts. In the field of DA and DG, common approaches such as feature alignment (Yang et al. 2022) and intra-class feature distance minimization with inter-class maximization (Kang et al. 2019; Luo et al. 2022; Jeong and Kim 2022) are successful in mitigating the domain discrepancies. Specifically, several studies (Wang et al. 2022; Kim et al. 2021) have directly addressed the domain gap issue by manipulating pairs of domain-invariant representations from various domains that correspond to samples of the same class. Continuing in the realm of generative attacks, recent studies have employed CLIP-based (Aich et al. 2022) and object-centric (Aich et al. 2023) features for effective training of the perturbation generator. In our work, we leverage frequency-augmented feature contrastive learning on domain-agnostic mid-band feature pairs. Simultaneously, we reduce the significance of domain-specific features in the low- and high-bands to improve the adversarial transferability."}, {"title": "Proposed Attack Method: FACL-Attack", "content": "To this end, we propose a novel generative attack method, FACL-Attack, to facilitate transferable attacks across various domains and models from the frequency domain perspective. In our training, we introduce frequency-aware domain randomization and feature contrastive learning, explicitly leveraging band-specific characteristics of image attributes such as color, shape, and texture, as illustrated in Figure 1. We highlight our contributions as follows:\n\u2022 We propose two modules to boost the adversarial transferability, FADR and FACL, in which FADR randomizes domain-variant data components while FACL contrasts domain-invariant feature pairs in the frequency domain.\n\u2022 We achieve the state-of-the-art attack transferability across various domains and model architectures, demonstrating the effectiveness of our method.\n\u2022 Our plug-and-play modules can be easily integrated into existing generative attack frameworks, further boosting the transferability while keeping the time complexity."}, {"title": "Problem definition", "content": "Generating adversarial examples revolves around solving an optimization problem, whereas generating transferable adversarial examples addresses the challenge of generalization. Our goal is to train a generative model \\(G_{\\theta}(\\cdot)\\) to craft adversarial perturbations \\(\\delta\\) that are well transferable to arbitrary domains and victim models aimed to trigger mispredictions on the image classifier \\(f(\\cdot)\\). Specifically, the generator maps the clean image \\(x\\) to its corresponding adversarial example \\(x' = G_{\\theta}(x)\\) containing perturbations constrained by \\(||\\delta||_{\\infty} \\leq \\epsilon\\)."}, {"title": "Overview of FACL-Attack", "content": "Our method aims to train a robust perturbation generator that yields effective adversarial examples given arbitrary images from black-box domains to induce the unknown victim model to output misclassification. It consists of two key modular operations in the frequency domain, each applied to the input image data and features extracted from the surrogate model only during the training stage, as illustrated in Figure 2.\nAs inspired by the power of frequency domain augmentation in domain generalization (Huang et al. 2021; Xu et al. 2021), our first module, Frequency-Aware Domain Randomization (FADR), transforms a pixel-domain image to the frequency-domain components using DCT. It randomizes domain-variant low- and high-frequency components and preserves domain-invariant mid-frequency com-"}, {"title": "Frequency-Aware Domain Randomization", "content": "ponents in the input image. Then a perturbation generator is trained to craft bounded adversarial images \\(x'\\), i.e., perturbation \\(\\delta\\) added to the clean image \\(x_s\\) and constrained by perturbation projector \\(P(\\cdot)\\). We then spectrally decompose the randomized \\(x_s\\), and \\(x\\) into each low- and high-band, and mid-band frequency component, which are inversely transformed to the image domain by IDCT and passed through the pre-defined surrogate model for feature extraction. Following the recent line of works (Nakka and Salzmann 2021; Zhang et al. 2022) on transferable generative attacks, we leverage the mid-layer features \\(f_k(\\cdot)\\) for feature contrastive learning. Each band-specific clean and perturbed feature pair is contrasted in our Frequency-Augmented Contrastive Learning (FACL) module, whereby domain-agnostic mid-band FC pair is to repel and domain-specific low- and high-band FC pair is to attract each other. This straightforward but effective data- and feature-level guidance in the frequency domain significantly contributes to boost the adversarial transferability as demonstrated in the following sections.\nThis subsection describes our FADR module designed to boost the robustness of perturbation generator \\(G_{\\theta}(\\cdot)\\) against arbitrary domain shifts in practical real-world scenarios. Inspired by recent works that convert the training image"}, {"title": "Frequency-Augmented Contrastive Learning", "content": "from pixel space into frequency space for boosting the domain generalization capabilities (Huang et al. 2021; Xu et al. 2021), we decompose the input training images into multiple-range FCs by leveraging DCT, and apply random masked filtering operation on domain-specific image attributes that lie in the low- and high-frequency bands. While FSDR (Huang et al. 2021) and FACT (Xu et al. 2021) each employs histogram matching and Fourier-based amplitude mix-up, our proposed FADR module explicitly manipulates the DCT coefficients to diversify input images, aligning with a recent work (Long et al. 2022) that narrows the gap between the surrogate model and possible victim models via spectrum transformation. We remark that our approach applies domain randomization exclusively to domain-specific FCs that are subject to change from various domains, whereas the existing work (Long et al. 2022) applies spectral transformation over the whole frequency bands containing not only domain-specific information, but also domain-agnostic semantic details.\nIn converting the input images into the frequency domain, we apply DCT to each channel separately. We then apply random masked filtering to diversify the input images for boosting the cross-domain transferability. Our spectral transformation operation \\(T_{FADR}(\\cdot)\\) for source images \\(x_s\\) can be mathematically expressed as follows:\n\\[T_{FADR}(x_s) = \\phi^{-1} (\\phi(x_s + \\xi) \\odot M),\\]\nwith the mask \\(M\\) defined as follows:\n\\[M = \\begin{cases}\nU(1 - \\rho, 1 + \\rho), & \\text{if } f < f_l, \\\\\n1, & \\text{if } f_l \\leq f < f_h, \\\\\nU(1 - \\rho, 1 + \\rho), & \\text{if } f \\geq f_h,\n\\end{cases}\\]\nwhere, \\(\\phi\\), \\(\\phi^{-1}\\) denote Hadamard product, DCT, and inverse DCT (IDCT) operation, respectively. The random noise \\(\\xi \\sim N(0, \\sigma^2 I)\\) is sampled from a Gaussian distribution, and the mask values are randomly sampled from Uniform distribution, denoted as \\(U\\). For the random mask matrix"}, {"title": "Experiments", "content": "Recent works on multi-object scene attacks have highlighted the importance of feature-level contrast for transferable generative attacks. In a similar approach to their ideas of exploiting local patch differences (Aich et al. 2023) or CLIP features (Aich et al. 2022), our FACL module seeks to apply feature contrast specifically in the domain-agnostic mid-frequency range for improving the generalization capability of the trained perturbation generator \\(G_{\\theta}(\\cdot)\\).\nAccording to the training pipeline of our FACL-Attack in Figure 2, the generated adversarial image \\(x'\\) undergoes spectral decomposition before feature extraction from the surrogate model. This process is carried out by using a band-pass filter \\(M_{bp}\\) and a band-reject filter \\(M_{br}\\), to decompose the surrogate model inputs into mid- and low/high-band FCs, respectively. The spectral decomposition operator is defined as follows:\n\\[M_{bp} = \\begin{cases}\n1, & \\text{if } f_l \\leq f < f_h, \\\\\n0, & \\text{otherwise},\n\\end{cases}\\]\nwhere \\(M_{br}\\) is the opposite of \\(M_{bp}\\), holding its values in reverse. Then the spectrally decomposed features from the surrogate model \\(f\\) are defined as:\n\\[Z_{band} = f_k (\\phi^{-1} (\\phi(input) \\odot M_{band})),\\]\nwhere \\(M_{band}\\) is set to either \\(M_{bp}\\) or \\(M_{br}\\), and \\(f_k(\\cdot)\\) denotes the \\(k\\)-th layer of \\(f\\). Given \\(x_s\\), and \\(x'\\) as input, we finally obtain two pairs of band-specific frequency-augmented features to contrast, i.e., \\((z_m, z'_m)\\) for repelling, and \\((z_{lh}, z'_{lh})\\) for attracting each other.\nThe baseline loss \\(\\mathcal{L}_{orig}\\) for attacking the surrogate model via contrasting clean and adversarial feature pairs is defined as follows:\n\\[\\mathcal{L}_{orig} = sim(f_k (x_s), f_k (x'_s)),\\]\nwhere \\(sim\\) refers to the standard cosine similarity metric. To boost the attack transferability, our FACL module effectively exploits the spectrally decomposed feature pairs in our proposed FACL loss function defined as follows:\n\\[\\mathcal{L}_{FACL} = sim(z_m, z'_m) - sim(z_{lh}, z'_{lh}),\\]\nwhere the goal of \\(\\mathcal{L}_{FACL}\\) is to reinforce the effectiveness of domain-agnostic mid-band feature contrast \\((z_m, z'_m)\\), while minimizing the importance of domain-specific low-"}, {"title": "Experimental Setup", "content": "To this end, we propose a novel generative attack method, FACL-Attack, to facilitate transferable attacks across various domains and models from the frequency domain perspective. In our training, we introduce frequency-aware domain randomization and feature contrastive learning, explicitly leveraging band-specific characteristics of image attributes such as color, shape, and texture, as illustrated in Figure 1.\nWe highlight our contributions as follows:\n\u2022 We propose two modules to boost the adversarial transferability, FADR and FACL, in which FADR randomizes domain-variant data components while FACL contrasts domain-invariant feature pairs in the frequency domain.\n\u2022 We achieve the state-of-the-art attack transferability across various domains and model architectures, demonstrating the effectiveness of our method.\n\u2022 Our plug-and-play modules can be easily integrated into existing generative attack frameworks, further boosting the transferability while keeping the time complexity."}, {"title": "Datasets and attack settings", "content": "We evaluate our method over challenging strict black-box settings (i.e., cross-domain and cross-model) in the image classification task. We set the target domain and victim model to be different from the source domain and surrogate model. The perturbation generator is trained on ImageNet-1K (Russakovsky et al. 2015) and evaluated on CUB-201-2011 (Wah et al. 2011), Stanford Cars (Krause et al. 2013), and FGVC Aircraft (Maji et al. 2013). As BIA (Zhang et al. 2022) highlights the importance of using a large-scale dataset for training, we train on ImageNet-1K accordingly. For the cross-model setting, we evaluate our method over black-box models but white-box domain (i.e., ImageNet-1K) setting. The details for the datasets are described in Table 1."}, {"title": "Main Results", "content": "and high-band feature difference \\((z_{lh}, z'_{lh})\\). In this approach, our \\(\\mathcal{L}_{FACL}\\) facilitates the push-pull action among the band-specific feature pairs, further guiding the perturbation generation towards more robust regime, as shown in Figure 4.\nWe train our perturbation generator by minimizing the total loss function as follows:\n\\[\\min_{\\theta} (\\lambda_{orig} \\cdot \\mathcal{L}_{orig} + \\lambda_{FACL} \\cdot \\mathcal{L}_{FACL}),\\]\nwhere \\(\\lambda_{orig}\\) and \\(\\lambda_{FACL}\\) are loss coefficients. The objective guides our generator \\(G_{\\theta}(\\cdot)\\) to generate more robust perturbations to domain shifts as well as model variances."}, {"title": "Surrogate and victim models", "content": "Our perturbation generator is trained against ImageNet-1K pre-trained surrogate models (e.g., VGG-16 (Simonyan and Zisserman 2015)). For the cross-model evaluation, we investigate other architectures including VGG-19 (Simonyan and Zisserman 2015), ResNet50 (Res-50), ResNet152 (Res-152) (He et al. 2016), DenseNet121 (Dense-121), DenseNet169 (Dense-169) (Huang et al. 2017) and Inception-v3 (Inc-v3) (Szegedy"}, {"title": "Implementation details", "content": "Cross-domain evaluation results. We compare our FACL-Attack with the state-of-the-art generative-model based attacks on various black-box domains with black-box models. During the training stage, we leverage the ImageNet-1K as the source domain to train a perturbation generator against a pre-trained surrogate model. In the inference stage, the trained perturbation generator is evaluated on various black-box domains (i.e., CUB-200-2011, Stanford Cars, and FGVC Aircraft) with black-box victim models. The victim models include pre-trained models which were trained via DCL framework with three different backbones (i.e., Res-50, SENet154, and SE-Res101).\nAs shown in Table 2, our FACL-Attack outperforms on most cross-domain benchmarks, among which are also cross-model, by significant margins. This demonstrates the strong and robust transferable capability of the generator trained by our novel approach with data- and feature-level"}, {"title": "Evaluation metric and competitors", "content": "We closely follow the implementation of recent works on generative attacks (Naseer et al. 2019; Nakka and Salzmann 2021; Zhang et al. 2022; Aich et al. 2022) for fair comparison. Our perturbation generator consists of down-sampling, residual, and up-sampling blocks that translate clean images into adversarial examples. The surrogate model layer from which we extract features is Maxpool.3 for VGG-16. We train with an Adam optimizer (\\(\\beta_1 = 0.5, \\beta_2 = 0.999\\)) (Kingma and Ba 2015) with the learning rate of 2 \\times 10^{-4}, and the batch size of 16 for 1 epoch. The perturbation budget for crafting the adversarial image is \\(l_\\infty \\leq 10\\). For the FADR hyper-parameters, we follow a prior work (Huang et al. 2021) to set the low and high frequency threshold to \\(f_l = 7\\) and \\(f_h = 112\\), respectively. We use \\(\\rho = 0.01\\) and \\(\\sigma = 8\\) for spectral transformation and describe more details in Supplementary.\nWe choose the top-1 classification accuracy after attacks as our main evaluation metric, unless otherwise stated. The reported results are the average values obtained from three random seed runs. The competitors include the state-of-the-art generative attacks such as GAP (Poursaeed et al. 2018), CDA (Naseer et al. 2019), LTP (Nakka and Salzmann 2021), and BIA (Zhang et al. 2022). We set BIA as our baseline."}, {"title": "Discussion", "content": "guidance in the frequency domain. We posit that the remarkable generalization ability of FACL-Attack owes to the synergy between our two proposed modules that effectively guide feature-level separation in the domain-agnostic mid-frequency band (i.e., FACL), complemented by data-level randomization only applied to the domain-specific frequency components (i.e., FADR). In other words, our spectral approach does help improve the generalization capability of the perturbation generator to other black-box domains as well as unknown network architectures. Moreover, our proposed training modules are complementary with existing generative attack frameworks and can further improve the attack transferability, as shown in Supplementary."}, {"title": "Cross-model evaluation results", "content": "Although we demonstrated the effectiveness of FACL-attack on boosting the transferability in strict black-box settings (i.e., cross-domain as well as cross-model) as shown in Table 2, we further investigated on the black-box model scenario in a controlled white-box domain (i.e., ImageNet-1K). In other words, the generator is trained against a surrogate model (i.e., VGG-16) and evaluated on various victim models which include VGG-16 (white-box), VGG-19, Res-50, Res-152, Dense-121, Dense-169, and Inc-v3.\nAs shown in Table 3, ours also outperforms on most generative attacks where they seem to partially overfit to the white-box model (i.e., VGG-16). Our outperforming results validate the strong transferability in cross-model attacks, in addition to cross-domain. We posit that the frequency-augmented feature learning could help the perturbation generator craft more robust perturbations, which exhibit better generalization capability to unknown feature space. This"}, {"title": "Ablation study on our proposed modules", "content": "aligns with a recent finding (Long et al. 2022) that spectral data randomization contributes to enhance the transferability via simulating diverse victim models."}, {"title": "Sensitivity on frequency thresholds", "content": "We investigated the sensitivity of the chosen frequency thresholds to verify the"}, {"title": "More Analyses", "content": "We examined different attack designs to find out how our proposed modules contribute to the attack transferability. As shown in Table 4, we trained the perturbation generator by employing each method and evaluated under realistic black-box settings. Cross-Domain is defined as ImageNet-1K \u2192 {CUB-200-2011, Stanford Cars, FGVC Aircraft} and Cross-Model indicates VGG-16 \u2192 {VGG-16, VGG-19, Res-50, Res-152, Dense-121, Dense-169, Inc-v3}. Baseline is trained with \\(\\mathcal{L}_{orig}\\) without any data randomization or band-specific feature contrast. FADR is trained with \\(\\mathcal{L}_{orig}\\) and frequency-aware domain randomization using \\(T_{FADR}\\). FACL is trained with \\(\\mathcal{L}_{orig}\\) and band-specific feature contrast using \\(\\mathcal{L}_{FACL}\\).\nAs shown in Table 4, Baseline trained with naive mid-"}, {"title": "More Analyses", "content": "Image quality. Although our work is focused on generating more powerful adversarial perturbations, the image quality of the crafted adversarial examples should also be carefully examined. As shown in Figure 6, FACL-Attack can craft effective and high-quality adversarial images with imperceptible perturbations. We also conducted a quantitative evaluation of image dissimilarity metrics between clean and adversarial image pairs, including SSIM, PSNR, and LPIPS. As shown in Table 6, we found that ours with a lower perturbation of \\(l_\\infty < 9\\) demonstrates superior image quality than the baseline with \\(l_\\infty \\leq 10\\) while achieving better attack performance. In other words, it can yield better attack transferability with lower perturbation power and better image quality, which are very remarkable assets for real-world black-box attacks. We refer to Supplementary for more qualitative and quantitative evaluation results."}, {"title": "Conclusion", "content": "In this paper, we have introduced a novel generator-based transferable attack method, leveraging spectral transformation and feature contrast in the frequency domain. Our work drew inspiration from domain generalization approaches that utilize frequency domain techniques, adapting and enhancing them for the attack framework. In our method, we target spectral data randomization on domain-specific image components, and domain-agnostic feature contrast for training a more robust perturbation generator. Extensive evaluation results validate the effectiveness in practical black-box scenarios with domain shifts and model variances. It can also be integrated into existing attack frameworks, further boosting the transferability while keeping the inference time."}]}