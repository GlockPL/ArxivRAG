{"title": "FACL-Attack: Frequency-Aware Contrastive Learning\nfor Transferable Adversarial Attacks", "authors": ["Hunmin Yang", "Jongoh Jeong", "Kuk-Jin Yoon"], "abstract": "Deep neural networks are known to be vulnerable to security\nrisks due to the inherent transferable nature of adversarial ex-\namples. Despite the success of recent generative model-based\nattacks demonstrating strong transferability, it still remains a\nchallenge to design an efficient attack strategy in a real-world\nstrict black-box setting, where both the target domain and\nmodel architectures are unknown. In this paper, we seek to ex-\nplore a feature contrastive approach in the frequency domain\nto generate adversarial examples that are robust in both cross-\ndomain and cross-model settings. With that goal in mind,\nwe propose two modules that are only employed during the\ntraining phase: a Frequency-Aware Domain Randomization\n(FADR) module to randomize domain-variant low- and high-\nrange frequency components and a Frequency-Augmented\nContrastive Learning (FACL) module to effectively separate\ndomain-invariant mid-frequency features of clean and per-\nturbed image. We demonstrate strong transferability of our\ngenerated adversarial perturbations through extensive cross-\ndomain and cross-model experiments, while keeping the in-\nference time complexity.", "sections": [{"title": "Introduction", "content": "Deep neural networks have brought forth tremendous im-\nprovements in visual recognition tasks. However, the in-\nherent transferable nature of adversarial examples still ex-\nposes the security vulnerability to malicious attackers tar-\ngeting such susceptible classifiers, causing serious threats\nand undesirable outcomes in real-world applications. The\nmajority of current attack methods can be primarily classi-\nfied into two main categories: iterative or optimization-based\napproaches, and generative model-based approaches. Over\nthe past years, iterative attack methods (Goodfellow, Shlens,\nand Szegedy 2015; Madry et al. 2017; Croce and Hein 2020;\nLorenz et al. 2021; Dong et al. 2018; Xie et al. 2019; Lu et al.\n2020; Naseer et al. 2020) have been the standard attack pro-\ntocol for its simplicity and effectiveness. However, this iter-\native approach is frequently constrained by inefficient time\ncomplexity and the potential risk of over-fitting to the train-\ning data and models. Moreover, it has shown limited appli-\ncability in practical situations due to the low transferability\nto unknown models and domains."}, {"title": "Related Work", "content": "Generator-based Adversarial Attack\nGenerative attack (Poursaeed et al. 2018) employs the con-\ncept of adversarial training (Goodfellow et al. 2020) to\ncreate perturbations across entire data distributions. This\nis achieved by regarding a pre-trained surrogate model as\na discriminator, and it is advantageous due to the ability\nof generating diverse forms of perturbations across mul-\ntiple images simultaneously. Existing methods aim to en-\nhance the generator training by leveraging both the cross-\nentropy (CE) loss (Poursaeed et al. 2018) and the relativis-\ntic CE loss (Naseer et al. 2019), improving the transferabil-\nity across domains and models. Recent studies (Nakka and\nSalzmann 2021; Zhang et al. 2022) utilize features extracted\nfrom the mid-level layers of the surrogate model, which en-\ncompass a higher degree of shared information among dif-\nferent model architectures. We follow the traces of the recent\nworks and explore a method to further enhance the trans-\nferability by introducing a novel perspective from the fre-\nquency domain.\nFrequency-based Approach for Generalization\nConvolutional neural networks are known to exhibit in-\ntriguing attributes within the frequency domain (Yin et al.\n2019; Tsuzuku and Sato 2019; Yin et al. 2019; Wang et al.\n2020a,b), demonstrating proficient generalization capabil-\nity by effectively harnessing the band-specific information\nderived from Fourier filtering (Dziugaite, Ghahramani, and\nRoy 2016; Guo et al. 2017; Long et al. 2022). Spectral ma-\nnipulations for enhancing the generalization capability can\nbe achieved through simple yet powerful transformations\nlike the Fast Fourier Transform (FFT), which dissects an\nimage into amplitude components that vary across domains\nand phase components that remain consistent across differ-\nent domains (Xu et al. 2021). The Discrete Cosine Trans-\nform (DCT) also serves as an efficient technique to decom-\npose spectral elements into domain-agnostic mid-frequency\ncomponents (mid-FCs) and domain-specific low- and high-\nFCs, which contributed to the effective spectral domain ran-\ndomization in FSDR (Huang et al. 2021). In our work, we\nalso employ the DCT to decompose images into domain-\nagnostic and domain-specific frequency components, facili-\ntating the effective domain randomization and feature-level\ncontrastive learning for transferable attacks.\nFeature Constrastive Learning\nManipulating image representations in the feature space has\ndemonstrated significant performance improvement in real-\nworld scenarios characterized by domain shifts. In the field\nof DA and DG, common approaches such as feature align-\nment (Yang et al. 2022) and intra-class feature distance min-\nimization with inter-class maximization (Kang et al. 2019;\nLuo et al. 2022; Jeong and Kim 2022) are successful in\nmitigating the domain discrepancies. Specifically, several\nstudies (Wang et al. 2022; Kim et al. 2021) have directly\naddressed the domain gap issue by manipulating pairs of\ndomain-invariant representations from various domains that\ncorrespond to samples of the same class. Continuing in the\nrealm of generative attacks, recent studies have employed\nCLIP-based (Aich et al. 2022) and object-centric (Aich et al.\n2023) features for effective training of the perturbation gen-\nerator. In our work, we leverage frequency-augmented fea-\nture contrastive learning on domain-agnostic mid-band fea-\nture pairs. Simultaneously, we reduce the significance of\ndomain-specific features in the low- and high-bands to im-\nprove the adversarial transferability."}, {"title": "Proposed Attack Method: FACL-Attack", "content": "Problem definition. Generating adversarial examples re-\nvolves around solving an optimization problem, whereas\ngenerating transferable adversarial examples addresses the\nchallenge of generalization. Our goal is to train a gener-\native model $G_\\theta(.)$ to craft adversarial perturbations $\\delta$ that\nare well transferable to arbitrary domains and victim mod-\nels aimed to trigger mispredictions on the image classifier\n$f(.)$. Specifically, the generator maps the clean image $x$ to\nits corresponding adversarial example $x' = G_\\theta(x)$ contain-\ning perturbations constrained by $|\\delta||_\\infty \\le \\epsilon$.\nOverview of FACL-Attack. Our method aims to train a\nrobust perturbation generator that yields effective adversar-\nial examples given arbitrary images from black-box domains\nto induce the unknown victim model to output misclassifi-\ncation. It consists of two key modular operations in the fre-\nquency domain, each applied to the input image data and\nfeatures extracted from the surrogate model only during the\ntraining stage, as illustrated in Figure 2.\nAs inspired by the power of frequency domain augmen-\ntation in domain generalization (Huang et al. 2021; Xu\net al. 2021), our first module, Frequency-Aware Domain\nRandomization (FADR), transforms a pixel-domain image\nto the frequency-domain components using DCT. It random-\nizes domain-variant low- and high-frequency band compo-\nnents and preserves domain-invariant mid-frequency com-"}, {"title": "Frequency-Aware Domain Randomization", "content": "This subsection describes our FADR module designed to\nboost the robustness of perturbation generator $G_\\theta(.)$ against\narbitrary domain shifts in practical real-world scenarios.\nInspired by recent works that convert the training image"}, {"title": "Frequency-Augmented Contrastive Learning", "content": "Recent works on multi-object scene attacks have highlighted\nthe importance of feature-level contrast for transferable gen-\nerative attacks. In a similar approach to their ideas of ex-\nploiting local patch differences (Aich et al. 2023) or CLIP\nfeatures (Aich et al. 2022), our FACL module seeks to ap-\nply feature contrast specifically in the domain-agnostic mid-\nfrequency range for improving the generalization capability\nof the trained perturbation generator $G_\\theta(.)$.\nSpectral decomposition. According to the training\npipeline of our FACL-Attack in Figure 2, the generated\nadversarial image $x'$ undergoes spectral decomposition\nbefore feature extraction from the surrogate model. This\nprocess is carried out by using a band-pass filter $M_{bp}$ and\na band-reject filter $M_{br}$, to decompose the surrogate model\ninputs into mid- and low/high-band FCs, respectively. The\nspectral decomposition operator is defined as follows:\n$M_{bp} = \\begin{cases}\n1, & \\text{if } f_l \\le f < f_h, \\\\\n0, & \\text{otherwise},\n\\end{cases}$\nwhere $M_{br}$ is the opposite of $M_{bp}$, holding its values in\nreverse. Then the spectrally decomposed features from the\nsurrogate model $f$ are defined as:\n$z_{band} = f_k (\\phi^{-1}(\\phi(input) \\otimes M_{band})),$ \nwhere $M_{band}$ is set to either $M_{bp}$ or $M_{br}$, and $f_k(.)$\ndenotes the $k$-th layer of $f$. Given $x_s$ and $x'_s$ as input,\nwe finally obtain two pairs of band-specific frequency-\naugmented features to contrast, i.e., $(z_m, z'_m)$ for repelling,\nand $(z_{lh}, z'_{lh})$ for attracting each other.\nLoss function. The baseline loss $L_{orig}$ for attacking the\nsurrogate model via contrasting clean and adversarial feature\npairs is defined as follows:\n$L_{orig} = sim(f_k(x_s), f_k(x'_s)),$\nwhere $sim$ refers to the standard cosine similarity metric. To\nboost the attack transferability, our FACL module effectively\nexploits the spectrally decomposed feature pairs in our pro-\nposed FACL loss function defined as follows:\n$L_{FACL} = sim(z_m, z'_m) - sim(z_{lh}, z'_{lh}),$\nwhere the goal of $L_{FACL}$ is to reinforce the effectiveness\nof domain-agnostic mid-band feature contrast $(z_m, z'_m)$,\nwhile minimizing the importance of domain-specific low-"}, {"title": "Experiments", "content": "Experimental Setup\nDatasets and attack settings. We evaluate our method\nover challenging strict black-box settings (i.e., cross-domain\nand cross-model) in the image classification task. We set\nthe target domain and victim model to be different from the\nsource domain and surrogate model. The perturbation gen-\nerator is trained on ImageNet-1K (Russakovsky et al. 2015)\nand evaluated on CUB-200-2011 (Wah et al. 2011), Stan-\nford Cars (Krause et al. 2013), and FGVC Aircraft (Maji\net al. 2013). As BIA (Zhang et al. 2022) highlights the im-\nportance of using a large-scale dataset for training, we train\non ImageNet-1K accordingly. For the cross-model setting,\nwe evaluate our method over black-box models but white-\nbox domain (i.e., ImageNet-1K) setting. The details for the\ndatasets are described in Table 1.\nSurrogate and victim models. Our perturbation gener-\nator is trained against ImageNet-1K pre-trained surrogate\nmodels (e.g., VGG-16 (Simonyan and Zisserman 2015)).\nFor the cross-model evaluation, we investigate other ar-\nchitectures including VGG-19 (Simonyan and Zisserman\n2015), ResNet50 (Res-50), ResNet152 (Res-152) (He et al.\n2016), DenseNet121 (Dense-121), DenseNet169 (Dense-\n169) (Huang et al. 2017) and Inception-v3 (Inc-v3) (Szegedy"}, {"title": "Conclusion", "content": "In this paper, we have introduced a novel generator-based\ntransferable attack method, leveraging spectral transforma-\ntion and feature contrast in the frequency domain. Our work\ndrew inspiration from domain generalization approaches\nthat utilize frequency domain techniques, adapting and en-\nhancing them for the attack framework. In our method, we\ntarget spectral data randomization on domain-specific image\ncomponents, and domain-agnostic feature contrast for train-\ning a more robust perturbation generator. Extensive evalua-\ntion results validate the effectiveness in practical black-box\nscenarios with domain shifts and model variances. It can also\nbe integrated into existing attack frameworks, further boost-\ning the transferability while keeping the inference time."}, {"title": "Supplementary Material", "content": "In this supplementary material, we provide contents that\nwere not included in our main paper due to space limita-\ntions. This includes additional experimental details, adapt-\ning FACL-Attack to other attacks, additional quantitative re-\nsults, and additional qualitative results.\nAdditional Experimental Details\nIn this section, we provide additional experimental specifics\non the algorithm details, implementation details, the FADR\nmodule, and the FACL module.\nAlgorithm Details\nWe outline the algorithm details of FACL-Attack in Alg. A1.\nThe learning objective is to train a robust perturbation gen-\nerator $G_\\theta(.)$ from which the crafted adversarial examples\ntransfer well to unknown target domain regardless of data\ndistributions or model architectures. The training is entirely\nconducted in ImageNet-1K (Russakovsky et al. 2015) source\ndomain with the data distribution of $X_s$.\nTo elaborate on our training strategy, we first randomly\ninitialize our perturbation generator $G_\\theta(.)$. Next, we ran-\ndomly sample a mini-batch $x$ with batch size $N$, derived\nfrom the source data distribution $X_s$. To prevent excessive\nspectral transformation in $TFADR(.)$ and ensure stable train-\ning, we exclusively transform $N/2$ samples within the mini-\nbatch in our FADR module. The augmented samples $x_s$\nare then forward-passed through $G_\\theta(.)$ and the unbounded\nadversarial examples $x$ are crafted. To ensure impercep-\ntibility, the adversaries are constrained by the perturbation\nprojection operator $P(.)$. Then, we forward-pass $x$ and $x_s$\nthrough the pre-trained surrogate model $f_k(.)$ after under-\ngoing spectral decomposition $D(.)$. Finally, we train the per-\nturbation generator with the total loss objective that includes\nthe baseline loss $L_{orig}$ and our contrastive loss $L_{FACL}$\nImplementation Details\nRegarding the implementation of our generative attack, we\nadhere to the training pipeline and the generator architec-\nture outlined in recent studies (Poursaeed et al. 2018; Naseer\net al. 2019; Nakka and Salzmann 2021; Zhang et al. 2022)\nfor fair comparison. Elaborating on the framework, a pertur-\nbation generator crafts an adversarial example from a clean\ninput image, and the resulting unbounded adversarial exam-\nple is constrained by a perturbation budget of $l_\\infty \\le 10$. Sub-\nsequently, the final pairs of adversarial and clean image are\nfed into the surrogate model for the attack.\nFor GAP (Poursaeed et al. 2018), we used their official\ncode for the training. As for CDA (Naseer et al. 2019), we\nalso used their pre-trained models for evaluation. We re-\nimplemented LTP (Nakka and Salzmann 2021), utilizing the\nsame generator architecture as BIA, but with their proposed\nL2 loss. We set BIA (Zhang et al. 2022) as our baseline and\nimplemented our proposed modules upon their code. We"}]}