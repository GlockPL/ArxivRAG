{"title": "MuQ: Self-Supervised Music Representation Learning with Mel Residual Vector Quantization", "authors": ["Haina Zhu", "Yizhi Zhou", "Hangting Chen", "Jianwei Yu", "Ziyang Ma", "Rongzhi Gu", "Wei Tan", "Xie Chen"], "abstract": "Recent years have witnessed the success of foundation models pre-trained with self-supervised learning (SSL) in various music informatics understanding tasks, including music tagging, instrument classification, key detection, and more. In this paper, we propose a self-supervised music representation learning model for music understanding. Distinguished from previous studies adopting random projection or existing neural codec, the proposed model, named MuQ, is trained to predict tokens generated by Mel Residual Vector Quantization (Mel-RVQ). Our Mel-RVQ utilizes residual linear projection structure for Mel spectrum quantization to enhance the stability and efficiency of target extraction and lead to better performance. Experiments in a large variety of downstream tasks demonstrate that MuQ outperforms previous self-supervised music representation models with only 0.9K hours of open-source pre-training data. Scaling up the data to over 160K hours and adopting iterative training consistently improve the model performance. To further validate the strength of our model, we present MuQ-MuLan, a joint music-text embedding model based on contrastive learning, which achieves state-of-the-art performance in the zero-shot music tagging task on the MagnaTagATune dataset. Code and checkpoints are open source in https://github.com/tencent-ailab/MuQ.", "sections": [{"title": "Introduction", "content": "Self-supervised learning (SSL) has been introduced into speech and audio signal processing as a technique for learning latent semantic relationships from unlabeled raw data. Recently, several works (Li et al. 2023; Won, Hung, and Le 2023) apply SSL to music informatics understanding, and a number of pre-trained foundation models (PFMs) with generalized representation capabilities are built. Learning from unsupervised music data, these foundation models can achieve stunning performance in music understanding tasks such as genre classification, emotion prediction, and key detection (Yuan et al. 2023), and further provide semantic representations for more downstream tasks like music generation (Agostinelli et al. 2023) and music captioning (Deng et al. 2024), as illustrated in Figure 1.\nA key challenge in music understanding and representation is that music is an extremely specific modality. Unlike speech or environmental sounds, music not only focuses on semantic information, but also emphasizes acoustic information, such as melody, chords, and tonality. As a result, previous semantics-oriented SSL methods have struggled to perform well on music tasks(Spijkervet and Burgoyne 2021; Li et al. 2022; Yuan et al. 2023), as they fail to simultaneously capture both semantic and acoustic information.\nRecently, several studies have sought to develop a universal music representation that integrates both semantic and acoustic aspects. Among these efforts, two remarkable models are MERT (Li et al. 2023) and MusicFM (Won, Hung, and Le 2023). MERT employs a BERT-style masked language modeling (MLM) proxy task to predict discrete tokens from the masked audio parts, with an Encodec (D\u00e9fossez et al. 2022) model as the tokenizer and uses an auxiliary Constant Q-Transform (CQT) target to enhance the modeling of acoustic information. MusicFM, on the other hand, directly utilizes a random projection quantizer derived from BEST-RQ (Chiu et al. 2022), and this tokenization approach provides a general target for learning music representation, without the need for additional CQT loss to capture acoustic modeling.\nAs discussed in (Chiu et al. 2022) and (Li et al. 2023), the target extractor (i.e., tokenizer) plays an important role in SSL, as models are trained to predict the tokenized pseudo-labels. BEST-RQ features a lightweight approach that allows for fast extraction of discrete targets. However, its performance is highly dependent on the initialization of the random projection layer, often requiring multiple attempts or a specific random seed to achieve optimal results. In contrast, the Encodec target (D\u00e9fossez et al. 2022) used in MERT produces a series of residual targets, with the multi-target strategy shown beneficial to musical SSL (Li et al. 2023). As a neural codec trained on audio data, Encodec produces more stable labels compared to its random counterparts. However, using Encodec as tokenization is computational heavy and consumes a lot of GPU memory when applying online extraction, which can reduce the training efficiency. Also, it needs to be coupled with additional CQT reconstruction loss to perform well in acoustic representation.\nTo address the initialization dependency of the random projection quantizer in BEST-RQ and the inefficiency stemming from the heavy computation cost of Encodec in MERT, we introduce a model called MuQ, which learns Music representations from Mel Quantization targets. MuQ leverages a Mel-RVQ as the tokenizer to generate targets. The proposed Mel-RVQ is pre-trained on music data and employs a linear RVQ to directly quantize audio Mel spectrograms. Compared to the random-projection quantizer in BEST-RQ, the pre-trained Mel-RVQ produces more stable targets for SSL training and eliminates the model's dependence on initialization. Additionally, compared to Encodec, the lightweight single-layer Mel-RVQ architecture offers greater extraction efficiency.\nTo further demonstrate the capabilities of the proposed MuQ model, we explore its application in another crucial area of music understanding: aligning music and text representations. For example, MuLan (Huang et al. 2022) employs contrastive learning to train both a music encoder and a text encoder, producing semantically consistent embeddings for both modalities and achieving coherent alignment between music and text. Recognizing the role that self-supervised learning models can play in providing effective initialization for downstream tasks, we leverage our MuQ to construct a joint music-text embedding model, named MuQ-MuLan.\nOur main contributions are listed as follows:\n\u2022 We introduce a novel music SSL model MuQ, which demonstrates state-of-the-art performance across a wide range of downstream music understanding tasks over previous MERT and MusicFM models.\n\u2022 We propose the Mel Residual Vector Quantization (Mel-RVQ), which directly quantizes the Mel spectrum using a single linear layer RVQ, improving both training stability and efficiency.\n\u2022 We further develop the MuQ-MuLan model, trained with contrastive learning on MuQ. MuQ-MuLan excels in aligning and jointly encoding music and text modalities, compared with the original MuLan model.\nOur experiments demonstrate that the Mel-RVQ significantly enhances SSL performance across a variety of music downstream tasks. Notably, MuQ outperforms previous state-of-the-art (SOTA) SSL models MERT and MusicFM, even when trained on just 0.9K hours of data, which is 100x less than what comparable models require (Li et al. 2023; Won, Hung, and Le 2023). Additionally, our results show that MuQ-MuLan achieves a ROC-AUC score of 79.3 on the MagnaTagATune zero-shot music tagging task, surpassing the previous SOTA result(Huang et al. 2022)."}, {"title": "Related Work", "content": "Self-supervised learning for speech and audio Self-supervised learning (SSL) (Mohamed et al. 2022) has been introduced to the field of speech and audio signal processing as a way to learn semantic representations from unlabeled audio data, and it is now widely used in tasks such as automatic speech recognition (Baevski et al. 2023) and audio event classification (Chen et al. 2024).\nSSL often relies on well-designed targets. HuBERT (Hsu et al. 2021), for instance, uses K-means clustering labels to guide self-supervised learning in Masked Language Model (MLM) style, with these labels being extracted offline. BEST-RQ (Chiu et al. 2022) learns directly from targets generated by a randomly initialized projection quantizer, which can be extracted online. MT4SSL (Ma et al. 2022) integrates both online and offline objectives, applying the concept of multi-target self-supervised learning, where a distinct linear prediction head is used for each type of target.\nAnother important topic in SSL is iterative refinement training. Most existing iterative training practices are based on clustering. For example, HuBERT applies K-means clustering on the features of an existing model to train the next iteration of models. Similarly, Seed-ASR (Bai et al. 2024) first trains an SSL model on BEST-RQ and then iteratively trains on its features using K-means clustering.\nIn this paper, we take an alternative approach by iteratively training directly through the quantizer itself, without the need to apply K-means clustering.\nMusic processing and understanding Music information retrieval (MIR) contains a large set of tasks to evaluate deep learning models' understanding of music from audio signals or symbolized notation. For example, automatic music tagging task require models to do multi-label classification for tags like genre, instrument, and mood, given a music track (Won et al. 2020). Other MIR tasks include key detection, pitch detection, emotion analysis, etc (Yuan et al. 2023).\nIn recent years, SSL has been introduced into music understanding for building universal representational models capable of handling various MIR tasks. Some of these efforts include CLMR (Spijkervet and Burgoyne 2021) based on contrastive learning; MERT (Li et al. 2023) based on MLM proxy task; and more recently, MusicFM (Won, Hung, and Le 2023). These SSL-based models yield impressive performance across multiple MIR tasks, showing a generalized understanding of music."}, {"title": "Method", "content": "MuQ employs a self-supervised learning approach based on masked language modeling (MLM) and tokenized targets. The overall framework of MuQ is presented in Figure 2."}, {"title": "Self-Supervised Framework of MuQ", "content": "MuQ directly takes the Mel spectrum of the music audio signal as input. The Mel spectrum is partially masked as random noise with a masking probability p, and then fed into multiple layers of Conformer (Gulati et al. 2020) for context learning. The Conformer output is passed through linear layers, which serve as prediction heads. Finally, we calculate the cross-entropy loss between the target and predicted labels as the optimization objective.\nThe target labels are tokens extracted from the Mel spectrum by the Mel-RVQ, a quantization tokenizer proposed in this paper. Since Mel-RVQ produces N tokens for each time step of the Mel spectrum, the MuQ model incorporates N distinct linear layers to predict the different target tokens."}, {"title": "Mel Residual Vector Quantization (Mel-RVQ)", "content": "As shown in Figure 3, we depict the proposed Mel Residual Vector Quantization (Mel-RVQ) compared with the random-projection quantizer in BEST-RQ and the Encodec target used by MERT.\nThe proposed Mel-RVQ directly takes the Mel spectrum as input and then quantizes the Mel spectrum using residual vector quantization (RVQ). The encoder of Mel-RVQ is designed as a simple single-layer linear projection, and the decoder is also a single linear layer.\nMel-RVQ needs to be trained on music data in advance before it can be used to generate quantization targets in SSL training.\nTraining of Mel-RVQ During the training of Mel-RVQ, the final loss function can be decomposed into three terms, as shown in Figure 3(c) :\n\u2022 Codebook loss trains the RVQ component to improve the fit of the codebook embedding $Q_i$ to the dimensionality-reduced features z. Formally,\n$losscode = \\sum_{x\\in B,} ||norm(Q_i) \u2013 norm(sg(z))||^2$ (1)\nwhere B refers to a mini-batch of data, x denotes Mel spectrum input and sg denotes stop-gradient. $M_p$ is the projection matrix and token is the closest to the projected vector z in the 12-normalized embedding space.\n\u2022 Commitment loss trains the projection (i.e. encoder) for more optimal dimension reduction and fitting to embeddings. Expressed as\n$losscomm = \\sum_{x\\in B,} ||norm(z) - norm(sg(Q_i))||^2$ (2)\nwhere z=Mpx\n\u2022 Reconstruction loss trains the linear decoder (denoted as $M_D$) and the codebook to restore the original feature x. The formula is\n$lossrecon = \\sum_{x\\in B} ||M_DQ_i \u2212 x||^2$ (3)\nThe final loss used to train the Mel-RVQ is the weighted sum of the above three losses:\n$loss = \\alpha losscode + \\beta losscomm + lossrecon$ (4)\nwhere \u03b1 and \u03b2 are weighting factors, set as \u03b1 = 1, \u03b2 = 0.25.\nIt is emphasized that both $M_D$ and $M_p$ used in Eq.(4) are simple linear layers, which distinguishes them from those in neural codecs like Encodec (D\u00e9fossez et al. 2022).\nResidual modeling of Mel-RVQ Applying the residual modeling method to Mel-RVQ means that it yields multiple tokens for each time step of the audio. Assume there are N"}, {"title": "Music-text joint embedding model", "content": "Bridging the gap between music and text modalities, music-text representation models learn joint embeddings from music-text pairwise data. Most of these models use a two-tower structure, consisting of both a music encoder and a text encoder, and are trained by contrastive learning loss.\nLAION-CLAP (Wu et al. 2023) is a powerful open-source music-text encoder, with various model structures and versions trained on extensive data. MuLan (Huang et al. 2022) is also an exceptional music-text embedding model, but it is not open source for either the model or the data.\nThe MuQ-MuLan model introduced in this paper follows the ideas of former work but replaces the music encoder with our proposed MuQ model."}, {"title": "Iterative refinement with RVQ", "content": "Iterative refinement is introduced in HuBERT (Hsu et al. 2021) as a method to improve the performance of SSL models, where clustering techniques like K-means are employed to produce new labels for the next iteration of training.\nWe suggest that residual vector quantization (RVQ) itself can serve as an alternative to the clustering in iterative enhancement. That is, we directly train a $Mel-RVQ_{iter}$ on the latent representations of an already trained MuQ model, to re-drive the training of MuQ at next iteration.\nSpecifically, the iterative training on MuQ has two stages:\n\u2022 In the initial stage, a Mel-RVQ is trained on the Mel spectrum feature, and then the first version of MuQ is trained on the tokens produced by Mel-RVQ.\n\u2022 In the iterative stage, the first version of MuQ is used to extract representation from the audio, and the l th layer latent is used to train $Mel-RVQ_{iter}$, which will be used in training the second version of MuQ, namely $MuQ_{iter}$."}, {"title": "Music-Text Contrastive Learning", "content": "To verify the effectiveness of MuQ in more sophisticated downstream tasks, we trained a music-text joint embedding model, MuQ-MuLan. Similar to MuLan (Huang et al. 2022), MuQ-MuLan employs a two-tower multimodal architecture and is trained on a large amount of (music, text) paired data.\nMuQ-MuLan consists of a music encoder and a text encoder. During training, the music and the corresponding text are fed to the encoders separately, where the text is a description of the music track, as shown in Figure 4.\nFor music modality inputs, a MuQ model with pre-trained parameters is first used to encode the audio into a latent representation. This representation is then average-pooled over the temporal dimension. Finally, it passes through a linear projection layer to produce a music embedding $e_m$ with dimension d.\nFor text modality inputs, they are encoded by a pre-trained RoBERTa (Liu et al. 2019) model. As described in a previous study (Vasilakis, Bittner, and Pauwels 2024), the text encoder is critical to the performance of the two-tower model, so we append a few additional layers of Transformer encoder after RoBERTa. Likewise, the textual latent representations are average-pooled and passed through a linear projection layer to get text embedding $e_t$ with the same dimension d.\nWe use decoupled contrastive learning (DCL) loss (Yeh et al. 2022). Formally, given the i-th (music, text) pair in a mini-batch, for the embeddings $({e_m}^{(i)}, e_t^{(i)})$, we have\n$L_{DCL} = -log \\frac{e^{sim(e_m^{(i)}, e_t^{(i)})/\\tau}}{\\sum_{j\\neq i} e^{sim(e_m^{(i)}, e_t^{(i)})/\\tau}}$ (6)\nwhere sim denotes dot-product similarity, and $\\sigma$ is temperature coefficient."}, {"title": "Experiments Setup", "content": "Model Settings\nMuQ We stack 12 layers of Conformer in MuQ, with 310M parameters in total. The number of codebooks for Mel-RVQ is set to N = 8 and the codebook size is K = 1024. The input audio is fixed at 30 seconds with a sampling rate of 24K Hz. Audio inputs are 128-dimensional Mel spectrum, which are pooled to 25Hz before feeding to Conformer. The code is implemented with the Fairseq toolkit (Ott et al. 2019).\nMuQ-MuLan We use the pre-trained MuQ as the initialization for the music encoder. A pre-trained multilingual RoBERTa model xlm-roberta-base is used for the text encoder followed by 8 layers of Transformer, with the embedding dimension d = 512. MuQ-MuLan has 630M parameters in total, and all parameters stay unfrozen during the contrastive training."}, {"title": "Pre-training", "content": "Dataset We initially conduct pre-training on a smaller dataset, Music4all (Santana et al. 2020), containing 0.9K hours of open access music data. Subsequently, we scale up to a large-scale in-house dataset with 160K hours of music.\nTraining Details Mel-RVQ is trained on the Music4all dataset using 1 GPU. Since the Mel-RVQ is very small (less than 0.3M), the training of Mel-RVQ is extremely fast and it takes less than 1 hour to complete.\nAll pre-training experiments are running with 32 A100-40G GPUs. Batch size is 192, and masking probability p = 0.6. For the pre-training on the Music4all dataset, we train for only 75K steps. For the experiments on the 160K hours of in-house data, we initially train MuQ for 200K steps. Subsequently, the l = 10th layer latent is used to train $Mel-RVQ_{iter}$, followed by 150K steps of iterative training from scratch to obtain $MuQ_{iter}$. The entire initial & iterative pre-training takes about 2 weeks."}, {"title": "Contrastive Training", "content": "Dataset The training data for contrast learning comes from 130K hours of music in-house, each with its corresponding text description. The text descriptions are limited to non-personal information such as genre, mood, instrument, tempo, keywords of the music.\nTraining Details MuQ-Mulan is trained on 32 V100-32G GPUs with a batch size of 768. Following the practice of MuLan (Huang et al. 2022), we switch to taking 10 seconds as the length of the input audio. This significantly reduces memory consumption and enlarges batch size, which is critical for contrastive learning."}, {"title": "Downstream Tasks", "content": "We evaluate MuQ on the MARBLE benchmark (Yuan et al. 2023) and conduct experiments on a total of 9 downstream tasks. Note that all datasets appearing in the downstream tasks are not included in the pre-training data of MuQ.\nDue to page limitations, we only provide a brief overview of the downstream tasks here. We encourage readers to refer to the Appendix I for details on these tasks.\n\u2022 Genre classification aims to determine the most suitable genre for a song. We use the GTZAN dataset (Tzanetakis and Cook 2002) and report the accuracy scores.\n\u2022 Key detection predicts the tonal scale and dominant pitch level. We use the Giantsteps dataset (Knees et al. 2015), and a refined accuracy metric (Raffel et al. 2014).\n\u2022 Emotional analysis is taken on the Emomusic dataset (Soleymani et al. 2013). The metrics are determination coefficients for valence ($R^2V$) and arousal ($R^2A$) score.\n\u2022 Singer identification on the VocalSet (Wilkins et al. 2018) dataset. The evaluation metric is accuracy.\n\u2022 Vocal technique detection on the VocalSet (Wilkins et al. 2018). The evaluation metric is accuracy.\n\u2022 Music tagging involves multi-label classification of 50 tags on the MagnaTagATune (Law et al. 2009b) dataset. The metrics are ROC-AUC and average precision (AP).\n\u2022 Music structure analysis predicts the functional label for music segments on the Harmonix dataset (Nieto et al. 2019). The evaluation metric is accuracy.\n\u2022 Instrument classification on the Nsynth dataset (Engel et al. 2017). The evaluation metric is accuracy.\n\u2022 Pitch classification of the 128-pitch on the Nsynth dataset (Engel et al. 2017). The metric is accuracy.\nIn evaluating the pre-trained MuQ, we extract features from each layer and use a linear probe to perform downstream tasks. For the MuQ-MuLan experiments, we use a zero-shot approach for music tagging. This involves directly calculating the similarity between music and text embeddings to determine the prediction probability for each tag."}, {"title": "Results and Analysis", "content": "Performance of Pre-trained MuQ\nWe compare MuQ with two popular models, MERT and MusicFM. For MERT, we use MERT-v1-330M version (Li et al. 2023), the most superior of its releases on Hugging-Face. For MusicFM, we use the checkpoint pre-trained on the MSD dataset (Bertin-Mahieux et al. 2011), which is publicly available by (Won, Hung, and Le 2023). All models are of around 330M parameters.\nIt is shown that for $MuQ_{m4a}$, pre-trained on the 0.9K-hour Music4All dataset, already outperforms MERT and MusicFM. Training MuQ on a larger dataset further amplifies this advantage. Iterative training slightly improves the effectiveness of MuQ, resulting in $MuQ_{iter}$ with an average score of 77.0, which is our best-performing model.\nMuQ outperforms previous models on many downstream tasks, especially in genre classification, singer identification, vocal technique detection, music structure analysis and instrument classification. We visualize the result of MuQ in Figure 5."}, {"title": "Ablation Study on Mel-RVQ", "content": "We carry out an ablation study for Mel-RVQ in Table 3 to demonstrate the effect of its key components. In the ablation experiments, all models are pre-trained 75K steps on the Music4all with 310M parameters, and tested on 5 tasks."}, {"title": "Ablation Study on Music SSL Target", "content": "We ablate the number of target tokens and tokenization SSL training, this involves some hyperparameters of Mel-RVQ, as well as a comparison with Encodec target.\nAs seen in Table 4, performance gradually improves as the number of codebooks increases. We believe this is because more codebooks allow the model to capture fine-grained audio features more effectively. This also implies the advantage of multi-target learning for SSL pre-training.\nWe also explore the depth of the Mel-RVQ encoder and decoder by scaling up the linear projection to a 3-layer fully connected network, referred to as Mel-RVQ (deeper) in Table 4. However, the results suggest that increasing the depth of the Mel-RVQ network actually degrades performance. This indicates that a single layer of linear projection is sufficient for Mel-RVQ to achieve optimal performance.\nFinally, we compare Mel-RVQ with the Encodec target. It can be seen in Table 4 that Mel-RVQ, despite being a very lightweight structure, outperforms the Encodec tokenization target. We attribute this to the fact that the token produced by Encodec is a high abstraction of the music audio, which leads to it being more difficult for the model to learn."}, {"title": "Performance of MuQ-MuLan", "content": "In Table 5, we compare MuQ-MuLan with LAION-CLAP (Wu et al. 2023), Microsoft-CLAP 2023(Elizalde, Deshmukh, and Wang 2023) and MuLan (Huang et al. 2022). We use the open-source larger_clap_music_and_speech version for LAION-CLAP, which is the best-performing checkpoint among its releases in our experiments. We also introduce two variants that employ MERT or MusicFM as the music encoder and refer to them as MERT-MuLan and MusicFM-MuLan.\nFollowing previous study (Vasilakis, Bittner, and Pauwels 2024), we choose the proven superior MusCALL prompt: \u201cA <label> track\u201d for all models to test in our experiments. MuQ-MuLan outperforms previous models in both ROC-AUC and PR-AUC and reaches the SOTA in the zero-shot setting. Impressively, MuQ-MuLan still has a slight advantage over MuLan despite the gap in data amounts.\nAlthough both use the same contrastive learning setup, MuQ-MuLan outperforms MERT-MuLan. Furthermore, even though MuQ and MusicFM have identical model structures, MuQ-MuLan still performs better. This suggests that MuQ provides a better initialization for contrastive training.\nWe visualize the performance of MuQ-MuLan in Figure 6, and plot the comparision with LAION-CLAP (Wu et al. 2023), Microsoft-CLAP 2023 (Elizalde, Deshmukh, and Wang 2023) and MuLan (Huang et al. 2022)."}, {"title": "Layer-wise Analysis", "content": "In Figure 7, we visualize the results of each layer's features in various downstream tasks during one run of the MuQ model. The analysis can be summarized as follows:\n\u2022 Semantic tasks, such as genre classification and structure analysis, generally achieve the best results at the higher layers. They focus more on semantic information such as the direction of the music and the overall style.\n\u2022 Acoustic tasks, such as singer identification, instrument classification, pitch classification, and key detection, perform best at the lower layers. This implies that they are more closely associated with low-level characteristics such as the original audio features.\n\u2022 Comprehensive tasks, such as music tagging, emotional analysis, and vocal tech classification, have a relatively even distribution across all layers. This suggests that they rely on various aspects."}, {"title": "Discussion", "content": "Additional explanation of Mel-RVQ Although in this paper, we only discuss the case of Mel-RVQ with up to 8 codebooks, this does not mean that more codebooks are meaningless. Exploring Mel-RVQ with more codebooks may help understand the reasons why Mel-RVQ works and the limits of this advantage. The selection of layer 10 in iterative training is based on empirical intuition rather than rigorous experimental comparison. Due to limited computational resources, we did not explore other layers. We believe that iterating over other layers could potentially lead to better results, and this is a worthwhile topic of future research.\nPotential role of virtual classes in SSL The concept of virtual classes, introduced in (Chen, Deng, and Shen 2018), suggests that adding dummy classes to classification tasks can improve performance. We note that in fact the random projection quantizer in BEST-RQ, while having low codebook utilization, those codes that are never hit may in fact provide a role similar to that of virtual classes. In addition, we also observe in our early experiments with Mel-RVQ that virtual classes have a potential effect on the SSL training. Although our approach does not involve virtual classes, we believe their role in SSL merits further investigation.\nLimitation and future work Mel-RVQ is required to be pre-trained on music data before being used for SSL targeting. While we emphasize that the training process for Mel-RVQ is very fast (less than 1 hour), this is still not a completely out-of-the-box approach like BEST-RQ. In future work, we will also explore different model sizes (e.g., 90M base size) for the requirements of different physical environments and computational conditions."}, {"title": "Conclusion", "content": "In this study, we introduce MuQ, a novel self-supervised learning model for music representation and understanding. Building on this, we also present MuQ-MuLan, a joint music-text embedding model. MuQ uses Mel-RVQ as a quantization target, applying a linear projection RVQ structure to the Mel spectrum, which provides a simple yet powerful SSL target. Our model shows significant improvements over previous models like MERT and MusicFM, as demonstrated by its performance on the MARBLE benchmark. Additionally, experiments on the zero-shot music tagging task reveal that MuQ-MuLan achieves state-of-the-art music-text modal alignment. Our model code and checkpoints will be made open-source to support future research."}, {"title": "Ethic Discussion", "content": "We recognize the potential ethical concerns associated with MuQ. Throughout the data handling process, we've made it a priority to follow ethical guidelines, ensuring that personal information is protected and that the model remains free from biases. We also guarantee that no audio will be distributed or shared. We believe our work can positively impact the community by advancing automated music analysis, and MuQ can serve as a valuable tool for musicians and creators."}, {"title": "Appendix I: Evaluation Details", "content": "Downstream Tasks\nWe present here a detailed description of 9 downstream tasks involved in the evaluation, with their respective datasets and evaluation metrics. Most of the downstream tasks are based on the awesome MARBLE-benchmark (Yuan et al. 2023) implementation.\nGenre classification Music Genre classification is an important tasks of Music Information Retrieval (MIR). It aims to determine the most suitable genre for each song in dataset. We use the GTZAN dataset (Tzanetakis and Cook 2002) for this task.\nThe GTZAN dataset has a collection of 10 genres with 100 audio files each, all having a length of 30 seconds. We set windows of 10-seconds as inputs to get the features respectively, then combine the results to get the features used for the downstream task. We utilize the standard \"fail-filtered\" split (Kereliuk, Sturm, and Larsen 2015) for training and evaluating, then we provide the accuracy scores on testing set to show the final result.\nKey detection Key detection estimates the tonal scale and dominant pitch level of each given song. Specifically, the labels of the task contains major and minor scales for all pitch classes, that is, 24 classes in total.\nWe use the Giantsteps dataset (Knees et al. 2015), which has a collection of annotations for 604 2-min audio previews of songs, as the test set, and a commonly-used subset of the Giantsteps-MTG-keys dataset (Korzeniowski and Widmer 2017), for training and validation. We used the split method that was described in (Castellon, Donahue, and Liang 2021). Finally, we used the refined accuracy with error tolerance score (Raffel et al. 2014) following Table 6 to evaluate the performance of models.\nEmotional analysis The Music Emotion Recognition aims to get the Emotion score of songs. We use the Emomusic dataset (Soleymani et al. 2013), which contains music clips of 45 seconds. The dataset has 1000 clips in total. After removing the duplicates, the size of the dataset is reduced to 744 songs. Each clip is labeled with two scores: valence score indicates positive and negative emotional responses, and arousal score indicates emotional intensity. The dataset split is same as (Castellon, Donahue, and Liang 2021).We divide the whole signal into 5s windows as the inputs of SSL models to get the features, then combine the features as the inputs of a simple regression models."}, {"title": "At testing stage, we can get multiple predictions. Each prediction of a given song is a pair of scores of valence score and arousal score, which is same as the ground truth. We calculate the determination coefficient (R2) between the model regression results and human annotations of arousal ($R^2A$) and valence ($R^2V$) (Soleymani et al. 2013) as metrics for the test results.", "content": "Singer identification Singer identification is to recognize the corresponding singer based on the given singing audio. We use the Vocalset dataset (Wilkins et al. 2018)", "12": 8, "singer": "no singer", "violin": ""}]}