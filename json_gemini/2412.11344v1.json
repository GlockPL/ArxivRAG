{"title": "Can AI Extract Antecedent Factors of Human Trust in AI?", "authors": ["Melanie McGrath", "Harrison Bailey", "Necva B\u00f6l\u00fcc\u00fc", "Xiang Dai", "Sarvnaz Karimi", "Cecile Paris"], "abstract": "Information extraction from the scientific literature is one of the main techniques to transform unstructured knowledge hidden in the text into structured data which can then be used for decision-making in down-stream tasks. One such area is Trust in AI, where factors contributing to human trust in artificial intelligence applications are studied. The relationships of these factors with human trust in such applications are complex. We hence explore this space from the lens of information extraction where, with the input of domain experts, we carefully design annotation guidelines, create the first annotated English dataset in this domain, investigate an LLM-guided annotation, and benchmark it with state-of-the-art methods using large language models in named entity and relation extraction. Our results indicate that this problem requires supervised learning which may not be currently feasible with prompt-based LLMs.", "sections": [{"title": "1 Introduction", "content": "The rapid rate at which Artificial Intelligence (AI) is developing and the accelerating rate at which it is becoming integrated into human life necessitates a thorough understanding of the dynamics of human trust in AI (Glikson and Woolley, 2020; Teaming, 2022). Addressing questions about the factors, or antecedents, influencing trust in specific AI systems and the thresholds for excessive or insufficient trust is crucial for using AI responsibly and preventing potential misuse (Parasuraman and Riley, 1997; Lockey et al., 2021).\nExisting literature in the behavioural and computer sciences offers extensive insight into this domain (e.g., Glikson and Woolley, 2020; Kaplan et al., 2021; Sa\u00dfmannshausen et al., 2023). However, this literature remains largely unstructured and is constantly expanding, making it increasingly difficult for researchers to review and extract relevant knowledge. To address this challenge, we create the Trust in AI dataset where the factors influencing trust are automatically captured in a structured dataset, making it more accessible and easier for domain experts to navigate. The specific information targeted for extraction, as directed by the domain experts, includes the type of AI application, the factor (trust antecedent), the type of factor (human, technological, contextual), and the relationship of the factor with trust. The resulting resource has practical applications in industry and commercial AI production. To build this resource, we utilise Information Extraction (IE) methods, including Named Entity Recognition (NER) and Relation Extraction (RE).\nOur contributions are as follows: (1) We formulate the challenging problem of information extraction for trust in AI, which is previously unexplored in the NLP domain (\u00a73); (2) Drawing inspiration from studies demonstrating the capabilities of large language models (LLMs) in simulating annotation (Bansal and Sharma, 2023; Goel et al., 2023; Zhang et al., 2023c), we provide LLM-guided annotation as a part of the annotation process; (3) We construct a dataset of factors empirically shown to influence trust development named Trust in AI (\u00a74); and, (4) We provide baseline results for the defined tasks (\u00a76)."}, {"title": "2 Related Work", "content": "Trust in AI Trust is critical to human willingness to adopt AI technology in a safe and productive way (Jacovi et al., 2021; Schaefer et al., 2021). Consequently, it is important to know what factors contribute to the development of an appropriate level of trust in an AI application. Over 450 distinct factors influencing trust development have been identified in the scientific literature (Sa\u00dfmannshausen et al., 2023). These antecedent factors can be classified as (1) properties of the trustor or human factors (e.g., experience); (2) properties of the trustee or technological factors (e.g., performance); or (3) properties of the task or interaction context (e.g., time pressure) (Hancock et al., 2011; Kaplan et al., 2021; Schaefer et al., 2016).\nWhich of these hundreds of antecedents influence trust in a particular AI application is highly variable. As a result, researchers interested in trust development are increasingly seeking approaches to specifying idiosyncratic models of trust in individual applications. The Trust in AI dataset will provide these domain experts with a resource to identify the most relevant factors for their application based on the existing literature. To our knowledge, this is the first such resource created for use by both NLP and Trust in AI researchers.\nScientific IE Dataset Annotation Annotating scientific IE datasets can be approached in two key ways: (a) annotating a small amount of data with the help of domain experts and carefully designed annotation guidelines (Friedrich et al., 2020; Karimi et al., 2015; Kim et al., 2003); and, (b) leveraging existing resources/tools to automatically annotate a large amount of data with no or little human intervention (Agrawal et al., 2019; Jain et al., 2020).\nEach group of studies have their pros and cons with the trade-off of cost, scale, and precision in annotations. Our study fits in the first category as the concepts of interest and their relationships are complex which entails an expert annotation for this first attempt to create such a resource in trust in AI domain.\nIE using LLMS IE using LLMs has gained prominence in the literature due to its potential advantages, particularly in scenarios with limited annotated data or in domains where traditional supervised approaches face challenges (Brown et al., 2020; Bubeck et al., 2023). LLMs show the capability of few-shot learning, useful when annotated data is scarce or expensive to obtain (Agrawal et al., 2022; Li et al., 2023; Zhang et al., 2023a). By employing prompt techniques, LLMs provide a consistent approach to various IE tasks through a single model (Wang et al., 2022). However, utilising LLMs with instructions for IE has been less successful (B\u00f6l\u00fcc\u00fc et al., 2023; Guti\u00e9rrez et al., 2022; Zhou et al., 2023; Zhang et al., 2023b). There is also a surge of generative IE methods that leverage LLMs to generate structural information rather than extracting structural information from text (Guo et al., 2023; Sainz et al., 2023; Qi et al., 2023)."}, {"title": "3 Problem Formulation", "content": "The Trust in AI dataset, can be conceptualised\nas $\\mathcal{D} = \\{S_i, P_i, L_i, R_i\\}_{i=1}^N$, where $N$ is the total\nnumber of sentences in the dataset. For each sentence $S_i$, $P_i$ represents its context, which is the\nparagraph where the sentence $S_i$ is located, $L_i$ is\nthe set of entity mentions, and $R_i$ is the set of identified relations within the sentence. Each element\n(entity) in $L_i$ is represented as a triplet, consisting\nof the start index of a span, the end index, and the\nentity category (i.e., human factor, technology factor, context factor, and application name). Each\nelement in $R_i$ captures the relationship between\none or a combination of two factors and the hidden\nconcept trust. Therefore, depending on the number\nof factors involved, elements in $r_i$ are represented\nby either a tuple $(L_{i,m}, r)$ or a triplet $(L_{i,l}, L_{i,m}, r)$,\nwhere $r$ is one of the pre-defined relation type: unspecified, null, positive, negative, interaction, or no\nrelation.\nThe dataset can be used for benchmarking two\ntasks: (1) Named Entity Recognition; for a given\nsentence $S_i$ and the related paragraph $P_i$, the objective is to recognise all elements in $L_i$; and, (2)\nRelation Extraction; for a given sentence $S_i$, the\nrelated paragraph $P_i$ and the recognised entities $L_i$,\nthe objective is to extract all elements in $R_i$."}, {"title": "4 Trust in AI Dataset", "content": "Our dataset comprises a diverse collection of English scientific publications focused on trust in automation and AI and trust in collaboration with Al. Appendix A.1 provides details on the dataset curation.\nTwo annotators (one researcher holding a PhD in social psychology and one student majoring in computer science and politics) take part in the annotation task. The annotation unfolded in two stages. In Stage I, the focus is on annotating factors and applications as spans, while Stage II focuses on annotating relationships between factors and the concept of trust. The Prodigy annotation tool (Montani and Honnibal, 2018) is employed for both stages, with details about the annotation interface available in Appendix A.2. The annotation of each stage is done in 5 phases:"}, {"title": "5 Experimental Setup", "content": "Dataset We split our dataset into training, development, and test sets. The test set contains samples annotated by two annotators (phase ii and iii),"}, {"title": "6 Results and Analysis", "content": "The results are shown in Table 2 for NER and 3 for RE. We observe that the supervised models outperform those using LLM, consistent with the study of (Guti\u00e9rrez et al., 2022; B\u00f6l\u00fcc\u00fc et al., 2023).\nThis finding highlights the necessity of a human annotated dataset for this complex domain.\nAt the annotation of factor types and application at Stage I, one word can refer to one or more factor types. For instance, the word adaptability in user and robot adaptability refers to both human and technology factors. Moreover, a mentioned factor may span several words, not all of which are included in the same factor. For instance, a publication might mention training of communication and trust calibration, where training of communication is a technology factor while training of trust calibration is human factor. This complexity makes the NER task challenging. Even though span-based models are applied to extract factors and applications, the results of SOTA models on the annotated dataset remain relatively low except for application. We note that human and contextual factors are the most confused factors. As a result, the annotation guideline is updated to provide clarity in distinguishing between the annotation of these factors. This shows that extracting these factors is even challenging for human annotators who have expertise in the domain. Finally, application is expected to be used to label entities that may contain the AI technology or the studied use case, potentially contributing to lower results for the technology factor.\nThe article selection, carried out by a domain expert, is crucial for the annotation of the relation between trust and factors at the RE task. Even in articles analysing trust in AI, factors may be discussed without any explicit relationship to trust being stated. This would be labelled as no relation in the dataset. The majority of factors in any given paper attract the no relation label, leading to an imbalanced distribution of relations in the dataset. The distribution of remaining relations is also imbalanced (see Table 4 in Appendix A.4 for details), with many labelled as unspecified, indicating that a relation is reported but its nature is not described. Hence the RE task is designed in two steps: first identifying the relation and then detecting the relation type. The supervised model outperforms LLM at both relation identification and relation classification. We also find LLM performs much better at relation classification than identification, as it predicts a relationship exists between most of the factors and trust, resulting in low precision in the lower."}, {"title": "7 Conclusion and Future Work", "content": "We explore information extraction for trust in AI and construct a dataset, comprising scientific articles. The dataset is constructed by annotating application and factors influencing trust, and the relation between trust and factors, formulated as named entity recognition and relation extraction tasks. We benchmark these tasks showing that LLM models using zero- and few-shot learning underperform supervised models, highlighting the need for the annotated dataset for this domain.\nIn the future, we plan to extend the dataset for entity resolution to identify and link entities that refer to the same entity, providing a more cohesive and accurate representation."}, {"title": "Ethics Statement", "content": "As we create a dataset, there are ethical considerations about using the data. The dataset used in our work is collected from scientific publications which are publicly available. However, some may require subscriptions to the journals for their users. We make links to the articles available so as not to redistribute those without their publishers' permission."}, {"title": "Limitations", "content": "Language. This dataset only uses English scientific literature, which may limit its usage for other languages.\nSubjectivity and Background Knowledge. The dataset annotation is done by two human annotators with different background knowledge, with one expert in the Trust in AI domain with a psychology background and another in computer science and politics.\nAccess. Due to IP considerations, the annotated dataset will be provided to the first author on requests."}, {"title": "A Details of Dataset", "content": "The publications are provided by domain experts, following a systematic review. To search the literature and gather scientific articles, 2 queries with terms related to artificial intelligence, robotics, automation, and trust-related concepts are used:\nTrust in automation and AI query:\n(artificial intelligence OR robot* OR automation OR machine intelligence OR autonomy)\nAND\n(trust* OR trust models OR trustworthiness OR trust calibration OR trust repair OR trust propensity OR trust development)\nTrust in collaboration with AI query:\n(human-robot collaboration OR hybrid intelligence OR collaborative intelligence OR robot* OR (collaboration AND artificial intelligence) OR human-Al collaboration OR human-robot team* OR human-autonomy team* OR augmented intelligence OR human-machine team*)\nAND\n(trust* OR trust models OR trustworthinessOR trust calibration OR trust repair OR trust propensity OR trust development)\nAfter conducting searches, a domain expert screens publications for relevance to the literature. The criteria for inclusion are addressing the topics; definitions of trust, experimental manipulations of trust, measurement of trust, antecedents of trust, or outcomes of trust. From the full set of papers retained for the review, a subset of publications coded as referencing antecedents of trust is used to construct the initial Trust in AI dataset.\nAs the purpose of the Trust in AI dataset is to capture trust antecedents that have been empirically tested, annotations are confined to those sections of a paper most likely to report its own findings, namely the abstract, results, and conclusion. Given that papers are collected from several disciplines, including robotics, psychology, human factors, ergonomics, and system sciences, the titles used to denote these sections of the paper vary. To address this, one annotator with knowledge of the domain reviews each paper in the dataset to identify the headings corresponding to the abstract, results, or conclusions sections."}, {"title": "A.2 Annotation Interface", "content": "The Prodigy annotation tool is utilised for annotation. We design a web page that integrates the Prodigy annotation tool, allowing annotators to input their names and select the publication and specific sections before initiating the annotation process. The interface for NER is illustrated in Figure 1. As depicted in the figure, context information, including the paragraph containing the sentence, as well as the title and section names of the publication, is presented for each sentence."}, {"title": "A.3 LLM Guidance Annotation", "content": "For LLM guidance, we called the gpt-3.5-turbo version of ChatGPT integrated into the spaCy library (Honnibal et al., 2020). We adopted a temperature of 0.3 and used ICL (random sampling- 3-shot) with spacy.SpanCat.v2 and spacy.TextCat.v2 components of spaCy and prompt defined by the library to pre-annotate the dataset for NER and RE tasks, respectively."}, {"title": "A.4 Further Analyses", "content": "The distribution of application and factor types (contextual, technology, human) annotated in NER (Stage I) is presented in Figure 2.\nAs stated, we examine at most two factors influencing the relation with trust (see Section 3 for details). The statistics of the factors are presented in Table 4 to provide insight into the task."}, {"title": "B Experimental Setups", "content": "All hyper-parameters used in supervised baseline models for NER and RE tasks are tuned on the development set. The details of experiments of SOTA models are given below for NER and RE tasks.\nNER For ROBERTa, the hyperparameters are the learning rate of 5e-4, max length of 128, context window of 200 tokens and batch size of 16, and models are trained for max epochs of 30. For Seq2seq-BERT and BiaffineNER, we use the default hyperparameters suggested by the authors except for the learning rate which is 5e-3.\nFor zero- and few-shot learning, we adopt the prompt template provided by EasyInstruct library"}, {"title": "C All results", "content": "NER We applied the ICL methods explained in the study of B\u00f6l\u00fcc\u00fc et al. (2023). The results of each ICL sample selection method are given in Table 5."}]}