{"title": "Exploring RL-based LLM Training for Formal Language Tasks with Programmed Rewards", "authors": ["Alexander G. Padula", "Dennis J.N.J. Soemers"], "abstract": "Proximal Policy Optimization (PPO) is commonly used in Reinforcement Learning from Human Feedback to align large language models (LLMs) with downstream tasks. This paper investigates the feasibility of using PPO for direct reinforcement learning (RL) from explicitly programmed reward signals, as opposed to indirect learning from human feedback via an intermediary reward model. We focus on tasks expressed through formal languages, such as mathematics and programming, where explicit reward functions can be programmed to automatically assess the quality of generated outputs. We apply this approach to a sentiment alignment task, a simple arithmetic task, and a more complex game synthesis task. The sentiment alignment task replicates prior research and serves to validate our experimental setup. Our results show that pure RL-based training for the two formal language tasks is challenging, with success being limited even for the simple arithmetic task. We propose a novel batch-entropy regularization term to aid exploration, although training is not yet entirely stable. Our findings suggest that direct RL training of LLMs may be more suitable for relatively minor changes, such as alignment, than for learning new tasks altogether, even if an informative reward signal can be expressed programmatically.", "sections": [{"title": "1 Introduction", "content": "Program synthesis models such as Copilot [9] have quickly become indispensable by improving productivity and making complex tasks more accessible. Significant advancements in this field have been achieved by training general-purpose Large Language Models (LLMs) to reproduce source code from widely used programming languages. Current state-of-the-art coding models [16,18,26] are trained using an auto-regressive next-token prediction objective, which maximizes the probability of predicting the next token in a sequence. Despite their success across a wide range of language processing tasks, next-token prediction suffers from the inherent limitation of being a surrogate objective, which can at times diverge from a task's true goals. When multiple valid solutions exist"}, {"title": "2 Background", "content": "Auto-regressive text generation tasks, such as program synthesis, can be modeled, following the standard RL problem formulation, as a finite-horizon Markov Decision Process (MDP). At any time step t, a state $s_t \\in S$ from a state space S is characterized by the sequence of non-masked tokens from the beginning of the sequence up to t. This representation captures the necessary context for subsequent token generation. The action $a_t \\in A$ from an action space A at time t corresponds to selecting the next token to add from a predefined vocabulary, extending the current state $s_t$ by one token. A policy $\\pi_{\\theta}$, parameterized by tunable parameters $\\theta$, outputs a probability distribution over A conditioned on the current state $s_t$. During training, the parameters are adjusted to maximize observed rewards. A reward function R provides a scalar signal R(7) for a trajectory $\\tau = (s_1, a_1, s_2, a_2,..., s_n, a_n)$. Given the auto-regressive nature of the task, where the final state $s_n$ encapsulates the entire sequence, the reward can also be expressed solely in terms of $s_n$ as R($s_n$). In auto-regressive text generation, a discount factor of 1 (i.e., no discounting) is typically used as the MDP is finite and there is no explicit preference for shorter solutions.\nThe field of RL [30] develops algorithms that update a policy's parameters based on experience, so as to maximize the rewards collected by the policy in future trajectories. Policy gradient methods define a differentiable objective function L(\\theta), which can be maximized using optimizers such as Adam or Stochastic Gradient Ascent to improve the policy's performance. Proximal Policy Optimization (PPO) [28] has become the de facto standard for RL-based training of LLMs following its use in RLHF. Schulman et al. [28] originally proposed two variants of PPO, both aiming to maximize the surrogate objective $L^{CPI}(\\theta)$:\n\n$r_t(\\theta) = \\frac{\\pi_{\\theta}(a_t | s_t)}{\\pi_{\\theta_{old}}(a_t | s_t)},$\n\n$L^{CPI}(\\theta) = E_t[r_t(\\theta) \\hat{A}_t],$\n\nwhere the expectation estimator $E_t[...]$ measures the empirical average over a finite batch of samples. The ratio $r_t(\\theta)$ moderates the extent of the policy updates based on how much more or less likely the action $a_t$ is under the new policy $\\pi_{\\theta}$ compared to the previous policy $\\pi_{\\theta_{old}}$. Compared to a traditional policy gradient objective [1], this surrogate objective gives more conservative updates and generally leads to a more stable learning process [28]. The advantage $\\hat{A}_t$ estimates the relative benefit of taking the action $a_t$ compared to all other actions available in $s_t$, weighted by their probability under $\\pi_{\\theta}$. It is used to isolate the effect of specific actions from the general quality of the states in which they are taken. Using the Bellman equation [5,30], we can express the advantage as $\\hat{A}_t = R_t + V(s_{t+1}) - V(s_t)$, where V(s) is the value (expected sum of future rewards) of a state s. A value function (e.g., neural network) trained to minimize the mean squared error between predicted and observed values estimates V.\nWhile both variants of PPO aim to maximize $L^{CPI}(\\theta)$, they differ in the constraints that they employ to avoid overly aggressive gradient updates. The Clipped Surrogate Objective variant of PPO disincentivizes $r_t(\\theta)$ from moving outside of the interval $[1 - \\epsilon, 1 + \\epsilon]$:\n\n$L^{CLIP}(\\theta) = E_t [min (r_t(\\theta) \\hat{A}_t, clip (r_t(\\theta), 1 \u2013 \\epsilon, 1 + \\epsilon) \\hat{A}_t)]$\n\nThe Adaptive KL Penalty Coefficient variant of PPO penalizes large changes to the policy $\\pi_{\\theta}(a_t | s_t)$ by measuring the Kullback-Leibler (KL) divergence [14] between $\\pi_{\\theta_{old}}(a_t | s_t)$ and $\\pi_{\\theta}(a_t | s_t)$:\n\n$L^{KLP}(\\theta) = E_t [r_t(\\theta) \\hat{A}_t \u2013 \\beta_{KL}KL[\\pi_{\\theta_{old}}(a_t | s_t), \\pi_{\\theta}(a_t | s_t)]]$"}, {"title": "3 Batch-Entropy Regularization", "content": "The standard entropy regularization term of Equation (1) encourages exploration by penalizing the policy for drifting away from the uniform policy on the basis of individual states: within a training batch, every state with a highly non-uniform policy receives a sizeable penalty. However, a potential failure mode of RL-based training may not just be a lack of exploration on a per-state basis, but rather a lack of exploration across the state space. A strong policy will often need to put most of its probability mass on a single (best) action per state\u2014a solution that conflicts with the standard entropy regularization-but would typically be expected to still pick different actions across different states.\nWe propose a novel batch-entropy regularization term, designed to encourage the policy to choose diverse actions for different states within the same batch, without penalizing it for having a highly non-uniform policy for individual states:\n\n$L^{BENT} (\\theta) = -\\beta_{BENT} \\sum_{s_t} E_{s_t \\sim B} [\\pi(a | s_t)] log E_{s_t \\sim B} [\\pi(a | s_t)], (2)$\n\nwhere $E_{s_t \\sim B}$ denotes the empirical mean over all states $s_t$ in a batch B. Such a batch-entropy term was previously used to analyze and evaluate the behavior of trained RL models [10], but our use as a regularization term is novel."}, {"title": "4 Language Model Fine-tuning Tasks", "content": "In this paper, we consider three different RL-based fine-tuning tasks for language models. Firstly, a sentiment alignment task (Subsection 4.1)\u2014for which successful results are known to be feasible from prior work [12]\u2014is used to verify the correctness and compatibility of our TRL-based implementation and experiment setup. Secondly, a synthetic arithmetic task (Subsection 4.2) is used as a formal language task which is simple enough to rapidly generate substantial training data. Thirdly, we consider the complex task of synthesising (novel) games in Ludii's formal game description language [7,24] (Subsection 4.3). This task is particularly compelling due to the scarcity of training samples which limits the effectiveness of supervised learning\u2014and the availability of established reward metrics to assess the quality of newly generated Ludii games [31]."}, {"title": "4.1 Sentiment Alignment Task", "content": "We initially seek to replicate results from prior research [12] in order to verify that the models we employ are compatible with TRL and confirm that the modifications we make to TRL adding entropy and batch-entropy losses, removing the KL penalty, and replacing the trained reward model with a programmed reward function-do not compromise the integrity of the experimental setup.\nThe task is to fine-tune GPT-2 [25] to generate positive movie reviews using RL. Initially, GPT-2 is pre-trained using a conventional masked language modeling (MLM) objective on the Stanford IMDB dataset [20]. Then, using PPO, the model is trained to complete reviews from the dataset while imbuing them with a positive sentiment. As part of the RLHF process, generated samples are evaluated by a reward model. For this purpose, the research we are reproducing employs a variant of DistilBERT [27] that was fine-tuned on user-labeled reviews in the IMDB dataset. As an alternative training method, we also replace the conventional reward model with an automated signal. For this, we use the VADER [13] implementation from NLTK [6], a rule-based sentiment analysis algorithm that returns a score between -1 and 1, which we use as a reward signal that quantifies how negative or positive the generated reviews are."}, {"title": "4.2 Synthetic Arithmetic Task", "content": "We define a simple arithmetic task designed to elucidate the potential advantages offered by RL-based training over a traditional MLM objective. In this task, n = 5 coefficients, $C_1, C_2,...,C_n$, are independently and uniformly drawn from the set of integers {0,1,...,9}. These coefficients are summed to form an initial expression, $Y_0 = C_1 + C_2 + ... + C_n$. The task simplifies $Y_0$ through a series of n steps, where at each step i, two randomly chosen, non-simplified terms from $Y_{i-1}$ are resolved (i.e., added together), resulting in $Y_i$. This process is repeated until the final expression, $Y_n$, is a single integer: the sum of all original coefficients.\nImportantly, due to the random order of summations, the sequence of intermediate expressions $Y_1, Y_2, ..., Y_n$ is non-deterministic. This randomness restricts the effectiveness of an imitation learning strategy in minimizing its loss, as it cannot leverage consistent sequential dependencies typically exploited in MLM tasks. For example, the sum 6+10+7+1+3 might first simplify to 6+17+1+3, then to 23 + 1 + 3, followed by 23 + 4, and finally to 27, with each step involving the addition of randomly selected terms from the previous expression. This task has properties that mirror program synthesis, where multiple equally valid solutions exist and their correctness can be quantified."}, {"title": "4.3 Ludii Game Synthesis Task", "content": "Ludii [24] is a general game playing system with a domain-specific language (DSL) for describing rules of games [7]. Any description of rules in this language can be compiled into a runnable game by the system. This DSL describes games as trees of ludemes, which are high-level keywords corresponding to common board game concepts such as board, is empty, is line, step, slide, and so on. For an example, see the game description for the connection game Hex:\nGenerating games in this DSL is ideally suited to exploring how a direct RL process can overcome limitations arising from limited data availability. Although board game representations in this DSL are succinct enough to fit within the context length of modern LLMs and can be directly compiled into fully playable and testable games, there are only in the order of 1000 existing board games implemented in the Ludii DSL. The scarcity of available data makes it challenging to train LLMs to learn Ludii using traditional supervised training methods.\nTo ease the model into learning the Ludii DSL, we define a fill-in-the-middle task. In this task, uniformly randomly sampled parentheticals are removed from game descriptions, and the model is trained to generate the missing sections. In this way, the dataset will range from simple prompts requiring the model to only fill in a small portion of a game, all the way to requiring the model to complete a whole game from scratch when the root parenthetical is sampled. The following example shows pre- and suffixes for the Hex game description, with the final part of the equipment section of the description having been removed:\nWhile the quality of a game description is more challenging to objectively quantify than the correctness of, e.g., a simple program or a solution for the arithmetic task, it is still possible to program a reasonable reward function. Inspired by fitness functions used by prior work on evolutionary game generation [8,31], we use a reward function based on the following five criteria:\n1. Compilability $C : S \\rightarrow {0,1}$: A binary signal indicating whether the game compiles, i.e., whether the game is syntactically valid and avoids semantic errors such as using a piece that was not defined in the equipment ludeme.\n2. Playability $P : S \\rightarrow {0,1}$: A binary signal indicating whether or not moves can be made without crashing.\n3. Balance $B: S\\rightarrow [0,1]$: A continuous signal defined as the largest difference in winrates between any pair of players. For example, it returns 1 if all players won the same number of games, and 0 if one player won them all.\n4. Completion Rate $F : S \\rightarrow [0,1]$: The fraction of games that terminated within 500 turns.\n5. Decisiveness $D: S\\rightarrow [0,1]$: The fraction of games that did not end in a draw. It returns 1 if all the games ended with a winner or loser.\n\nThe first criterion can be evaluated simply by having Ludii try to compile any given game description, whereas the other four require playing the game. We use 100 playthroughs (per generated game description) in which moves are selected uniformly at random to compute these criteria. Using games played between stronger agents could lead to more informative signals, but would have been prohibitive in terms of computation time. Ultimately, for any generated game description s, we use a reward of R(s) = 0 if s cannot be compiled (i.e., if C(s) = 0), R(s) = 0.1 if it is not playable (i.e., if P(s) = 0), or the geometric mean $(B(s) * F(s) * D(s))^{1/3}$ of the remaining three criteria otherwise."}, {"title": "5 Experiments", "content": "5.1 Sentiment Alignment Task\nIn this first experiment, we isolate each modification that we have introduced to TRL to ascertain their individual impacts on the performance of the system for the sentiment alignment task. In Fig. 1 (top), GPT-2's training run is consistent with Hugging Face's original results [12]. We also find that, while Pythia 410M converges as expected, LLama Code 13B fails to improve, despite the model being otherwise capable of generating sensible reviews during inference. We hypothesize that this is due to an incompatbility between (1) the version of TRL we use, (2) 8-bit quantization, and (3) Llama-architecture models. GPT-2 and Pythia 410M did not use quantization.\nReplacing the sentiment rewards obtained from the DistillBERT model with rewards calculated using the VADER algorithm, we find that the training runs in Fig. 1 (bottom) are consistent with those using DistillBERT, with GPT-2 and Pythia 410M steadily improving while LLama Code 13B shows no significant gains. We do, however, note an increased variance in rewards obtained during LLama Code 13B's training run with VADER rewards.\nFig. 2 shows that raising the entropy regularization coefficient $\\beta_{ENT}$ produces policies with higher entropy levels in their distributions over actions, as intended. However, in terms of rewards, this appears to lead to weaker policies. In contrast, when we raise the coefficient for our novel batch-entropy regularization variant (Fig. 3), we can produce policies with higher levels of batch-entropy (note that these numbers are not directly comparable to regular entropy number), with no substantial detriment to rewards that the models converge to. We cannot rule out that similar results might be possible with the standard entropy regularization, but this would require at least a more thorough hyperparameter sweep.\nRemoving the KL divergence penalty (see Fig. 4) improved both the convergence rate as well as the final performance. However, we cannot rule out the possibility that allowing the model to drift further from the pretrained model may have decreased the overall natural language quality of the outputs (e.g., in terms of style or grammar) whilst improving in terms of positive sentiment."}, {"title": "5.2 Arithmetic Task", "content": "In this experiment, we train a GPT-2-based model from scratch, tailored to handle arithmetic expressions. The model is configured with a context size of 64 tokens and utilizes a new word-piece tokenizer. The tokenizer's vocabulary consists of integers from 0 to 45, and the symbols '+' and '='.\nFig. 5 illustrates training under a conventional masked language modeling (MLM) objective. While the validation loss appears to converge, suggesting learning under the MLM objective, the reward deteriorates over time. The model learns to replicate approximately the correct structure, but fails to understand the mathematical semantics of the task.\nAs pre-training was largely ineffective, we start PPO training for the arithmetic task with an untrained model and no KL divergence penalty. Fig. 6 shows training using PPO to be more effective. The model quickly learns to output valid expressions and makes increasingly educated guesses toward the fully simplified expression, though it does not converge to a perfect solution. Figs. 7 and 8 show that without entropy or batch-entropy regularization, the entropy rapidly collapses, and the model converges on a naive policy which generates 23 regardless of the prompt it is given. This is a notable local optimum: it is the (rounded) mean of the population of problems we can generate in this task, as $E [X1+X2 + X3 + X4 + X5] = 22.5$ when $Xi \\sim {0,1,...,9}$. With $\\beta_{ENT} = 0.3$ or $\\beta_{BENT} = 0.3$, the entropy collapse can be delayed and the model's performance can exceed that of the naive policy. However, increasing $\\beta_{ENT}$ also appears to destabilize training."}, {"title": "5.3 Ludii Game Synthesis Task", "content": "The grammar of Ludii's DSL is complex enough that an untrained policy will face a flat reward landscape. This sets it apart from the arithmetic task, where it was feasible to start PPO training from an untrained model. In this task it is instead critical to first pre-train a minimally proficient model using a supervised MLM objective. We define a GPT-2 model with a custom tokenizer made up of all possible ludemes and primitives in the Ludii DSL. The GPT-2 variant was trained to convergence on the Ludii fill-in-the-middle dataset. However, despite efforts to simplify the tasks' representation using string masking (masking arbitrary strings such as names of games and pieces) and a custom tokenizer for the Ludii DSL, the model consistently failed to obtain a non-zero reward.\nFine-tuning Pythia 410M was more effective. Training this model to convergence on the training split of the Ludii dataset led to a mean reward above 0.9 out of 1 for both the training and validation splits. This is largely possible because the fill-in-the-middle dataset overrepresents smaller parentheticals. Filtering the dataset to only games where at least 20% of the game description has been masked, the model's validation reward averages around 0.3, offering ample space for improvement with reward-based training. While the Llama Code 13B model also achieved comparable pre-training performance, we were forced to exclude it from further reward-based training since Fig. 1 suggests that Llama Code 13B is incompatible with the version of TRL that we used.\nNone of the 11 PPO training runs that we conducted were able to improve the policy on the Ludii game synthesis task (see Fig. 9). Runs with larger entropy and batch-entropy regularization coefficients also appear to diverge more quickly. We tried reintroducing the KL divergence penalty, but noted no difference in behavior. The fact that neither form of entropy regularization improved the performance of PPO on this task suggests that the observed instability may not be (solely) attributed to a lack of exploration. One possibility is that this task is too complex and requires a substantially larger model than Pythia 410M. It is also possible that PPO, or reinforcement learning more generally, may not be sufficiently stable for LLM training tasks that go beyond RLHF-style alignment, where minor parameter adjustments are made to encourage or discourage previously acquired capabilities, possibly due to a loss of plasticity. [19,11]."}, {"title": "6 Related Work", "content": "Prior work on using direct RL (as opposed to RL with trained reward models) to fine-tune LLMs for formal language tasks tends to focus on settings for which ample training data is available, and pre-trained models are already highly capable, such as commonly used programming languages [15,29]. Outside of pure RL methods, researchers have also looked towards novel inference strategies to address similar shortcomings to those considered in this paper. Examples include iterative procedures for program synthesis that feed results or error reports from unit tests back into an LLM via additional prompts [17], adding lookahead search on top of RL to improve the math abilities of LLMs [32], and combining an LLM with evolutionary search for Ludii-based game synthesis [31]."}, {"title": "7 Conclusions & Future Work", "content": "This paper explores the feasibility of using direct Reinforcement Learning (RL) with programmed reward functions (as opposed to the more common trained reward models used in Reinforcement Learning from Human Feedback) to fine-tune LLMs for formal-language tasks which the model was not exposed to during pre-training.\nOur first experiments replicate prior work on sentiment analysis [12] and validate the correctness of our TRL-based implementation [33]. We then designed an arithmetic task that could not be effectively learned through supervised learning alone. RL-based training proved to be more effective; however, without entropy regularization, the model consistently converged to a naive local optimum. Both classical entropy regularization and our novel form of batch-entropy regularization improved upon this local optimum. While, theoretical reasoning and our empirical results suggest that batch-entropy regularization provides greater stability, a comprehensive hyperparameter sweep would be needed to confirm this observation. Our final experiments found that reward-based training of GPT-2 and Pythia 410M for the complex task of generating board games in Ludii's game description language was unstable.\nOur initial observation that PPO is effective at model alignment, such as encouraging a pre-trained model to write exclusively positive reviews, is consistent with the literature. However, we found that this performance does not generalize to unseen tasks, like learning to design board games, or even simply summing numbers. Since PPO is a state-of-the-art RL training algorithm, our findings highlight the need for fundamental improvements in RL training algorithms for reward-based training of LLMs. Potential avenues worth exploring include simpler methods like RLOO [2] and incorporating better domain-specific inductive biases, such as task-specific positional encodings. For complex tasks, such as game synthesis, it is also plausible that substantially larger models or more computationally expensive search algorithms [32,31] are required. Nevertheless, exploring the limits of improving RL training before resorting to such resource-intensive methods remains a compelling area of investigation."}]}