{"title": "Takin-VC: Zero-shot Voice Conversion via Jointly Hybrid Content and Memory-Augmented Context-Aware Timbre Modeling", "authors": ["Yuguang Yang", "Yu Pan", "Jixun Yao", "Xiang Zhang", "Jianhao Ye", "Hongbin Zhou", "Lei Xie", "Lei Ma", "Jianjun Zhao"], "abstract": "Zero-shot voice conversion (VC) aims to transform the source speaker timbre into an arbitrary unseen one without altering the original speech content. While recent advancements in zero-shot VC methods have shown remarkable progress, there still remains considerable potential for improvement in terms of improving speaker similarity and speech naturalness. In this paper, we propose Takin-VC, a novel zero-shot VC framework based on jointly hybrid content and memory-augmented context-aware timbre modeling to tackle this challenge. Specifically, an effective hybrid content encoder, guided by neural codec training, that leverages quantized features from pre-trained WavLM and HybridFormer is first presented to extract the linguistic content of the source speech. Subsequently, we introduce an advanced cross-attention-based context-aware timbre modeling approach that learns the fine-grained, semantically associated target timbre features. To further enhance both speaker similarity and real-time performance, we utilize a conditional flow matching model to reconstruct the Mel-spectrogram of the source speech. Additionally, we advocate an efficient memory-augmented module designed to generate high-quality conditional target inputs for the flow matching process, thereby improving the overall performance of the proposed system. Experimental results demonstrate that the proposed Takin-VC method surpasses state-of-the-art zero-shot VC systems, delivering superior performance in terms of both speech naturalness and speaker similarity.", "sections": [{"title": "Introduction", "content": "Zero-shot voice conversion (VC) refers to the task of modifying the timbre of a source speech to match that of a previously unseen speaker, while preserving the original phonetic or linguistic content. This technology has found broad applications in various practical domains [1, 2, 3].\nIn recent times, zero-shot VC has witnessed great progressions, with numerous state-of-the-art (SOTA) approaches [4, 5, 6, 7, 8, 9] exhibiting impressive results in converting natural and realistic utterances. The key idea behind these methods is to factorize speech into distinct elements, such as linguistic content and timbre elements, and then leverage the source speech content alongside the target speaker timbre to synthesize the desired target speech. In this paradigm, the quality of content and timbre representations, as well as the quality of their disentanglement, significantly impact their final performance. Consequently, numerous studies have sought to improve VC performance by designing more advanced modules [10, 11, 12, 13, 14, 15], information disentanglement approaches [16, 12, 17, 18] and so forth. However, achieving high-quality decoupling of utterances into distinct components remains a challenging task [19, 20, 21, 22], and existing zero-shot VC systems still exhibit subpar performance when handling unseen speakers primarily due to the underlying issues. First, current methods cannot fully mitigate the influence of source speaker timbre during the extraction of linguistic content features, a problem commonly referred to as \"timbre leakage.\" Second, they normally employ pre-trained speaker-verification (SV) models to capture target timbre features and cast them as globally time-invariant representations. However, as highlighted in [23], the timbre representations may vary with the linguistic content, rendering the performance of these approaches less optimal. Recently, the advances in large-scale speech language models [24, 25] have tried to tackle this issue by leveraging robust in-context learning capabilities for predicting target speech from concise utterances as prompts. Nevertheless, these methods may suffer from stability issues and error accumulation due to their auto-regressive nature, which can gradually degrade conversion quality.\nTo address the aforementioned limitations, we introduce Takin-VC, an effective VC framework with advanced modeling of content, timbre and audio quality in a zero-shot fashion. Specifically, we propose a hybrid content encoder guided by neural codec training that integrates the phonetic posterior-grams (PPGs) features and quantized self-supervised learning (SSL) representations from two pre-trained models, i.e., HybridFormer [26] and WavLM [27], so as to precisely capture the linguistic content. For speaker timbre modeling, we first propose a content-aware timbre modeling method that employs cross-attention (CA) to leverage the target voiceprint (VP) features extracted from a pre-trained speaker verification (SV) model [28], with the captured source content. This integration enables our proposed approach to learn target timbre representations associated with source content. Additionally, to further enhance speaker similarity, we advocate a memory-augmented module capable of generating high-quality conditional target inputs for a conditional flow matching (CFM) model [29], ultimately culminating in the synthesis of the target speech using a pre-trained vocoder [30].\nTo evaluate the performance of the Takin-VC system, we conduct extensive experiments on the both large-scale 500k-hour multilingual (Mandarin and English) and publicly available LibriTTS [31] datasets. Experimental results demonstrate that Takin-VC consistently outperforms state-of-the-art (SOTA) zero-shot VC methods in terms of both speaker similarity and speech naturalness. Notably, Takin-VC achieves significant improvements in both subjective and objective metrics compared with all baseline systems, further validating its effectiveness. For more detailed speech samples, please visit our demo page 3.\nIn summary, the main contributions of this work are outlined as follows:"}, {"title": "Background", "content": "Zero-shot Voice Conversion.\nIn contrast to previous few-shot [32, 33] and one-shot [12, 34] VC approaches, zero-shot VC presents a more challenging task, as it requires the model to generalize to unseen speakers without any additional training or fine-tuning. In recent years, advancements in deep learning techniques, such as SSL speech models and diffusion models, have led to significant progress in zero-shot VC. SEF-VC [8] learns speaker timbre from reference speech using a CA mechanism and reconstructs waveforms from HuBERT [35] tokens. [6] introduced Diff-HierVC, a diffusion-based hierarchical VC method that uses XLS-R [36] for content extraction and employs two diffusion models to generate high-fidelity converted pitch and Mel-spectrograms. The utilization of robust SSL features, which are rich in phonetic and paralinguistic nuances, has led to improved performance in these methods compared to prior works [37, 38]. Despite these impressive results, SSL-based zero-shot VC approaches [17, 5, 4] may still encounter the timbre leakage problem, as SSL features do not explicitly disentangle timbre information, while diffusion-based VC methods [39, 40] often struggle with poor real-time performance. Another cutting-edge zero-shot VC paradigm [41, 24, 42] involves decoupling speech into semantic and acoustic tokens using neural codecs [43, 44, 45] and SSL speech models [27, 46], subsequently leveraging language models to generate the converted speech. However, these methods still possess great potential for improvement regarding speaker similarity and naturalness/intelligibility.\nFlow Matching-based Generative Models.\nRecently, flow matching-based generative models [47, 48, 49] have garnered considerable attention in the realm of generative tasks, particularly in the image generation task [50, 51, 52]. These methods focus on approximating the transport probability path from random noise to the target distribution by estimating the associated vector field. By employing a neural ordinary differential equation (ODE), these models learn"}, {"title": "METHODS", "content": "3.1 Overivew\nAs shown in Fig. 1, our Takin-VC system comprises four components: the hybrid linguistic content encoder, memory-augmented & context-aware timbre modeling approach, and CFM model.\nTo elaborate, the hybrid content encoder is designed to precisely capture linguistic content $X_{scont}$ by leveraging the complementary strengths of PPG and SSL features with the guidance of neural codec-based training. For timbre modeling,\nwe extract Mel-spectrograms from randomly segmented reference waveform from the same speaker as the source speech, focusing on learning semantically correlated target timbre features and conditional target inputs for the CFM model, denoted as $X_{sett}$ and $X_{tcond}$. In our case, the duration of the reference wav is 4s. This process comprises two main components: context-aware timbre modeling and memory-augmented timbre modeling. The former begins by extracting the target VP features using a pre-trained speaker verification model\u00b9. These VP features are then concatenated with the reference Mel-spectrograms to form the key and value in the CA mechanism, while the attention query is derived from $X_{scont}$. The latter incorporates a stack of convolution, activation, and self-attention layers to generate high-quality conditional target inputs $X_{tcond}$ for the CFM model, which will be detailed below. Finally, we employ a CFM model to reconstruct the source Mel-spectrograms based on $X_{sett}$ and $X_{tcond}$, followed by employing a pre-trained Bigvgan vocoder to synthesize the desired target utterance."}, {"title": "Hybrid Linguistic Content Encoder", "content": "Current zero-shot VC methods typically rely on pre-trained automatic speech recognition (ASR) [59, 60, 61] methods or SSL speech models to extract linguistic content from the original waveform. Nonetheless, both approaches exhibit their respective limitations: the PPGs lacks certain essential paralinguistic nuances, while the latter does not explicitly disentangle timbre information. Therefore, to capture content representations with higher quality, our Takin-VC combines their merits through a neural codec training guided hybrid linguistic content encoder, as shown in Fig. 2.\nFormally, given an input source speech x, the proposed hybrid content encoder first extracts its corresponding PPG and SSL features, denoted as $X_{ppg}$ and $X_{ssl}$, using pre-trained HybridFormer and WavLM, respectively. For our scenario, the HybridFormer is trained on an in-house multilingual corpus of Mandarin and English, while the sixth-layer output features of WavLM are selected as $X_{ssl}$ for further processing.\nHowever, merely relying on the combination of SSL and PPG features is insufficient to achieve optimal VC performance. To further enhance overall performance and address potential timbre leakage, we incorporate a neural codec-based training approach [45] for end-to-end training of the proposed hybrid content encoder, as depicted in the left part of Fig. 2. Concretely, we regard WavLM as the encoder in our neural codec framework and employ a residual vector quantization-based quantizer like [43] to discretize the SSL features. To effectively leverage the PPGs alongside the quantized SSL features, we introduce a simple yet effective fusion module designed to adaptively combine these elements. This module comprises Conv1D layers and ReLU-based gating mechanisms to integrate the SSL and PPG features. The fusion process is formulated as follows:\n$X_{scont} = ReLU^{\\circ} (Conv1d (a_{ssl} \\cdot VQ(X_{ssl}) + a_{ppg}.X_{ppg})) (1)$\nwhere $a_{ssl}$ and $a_{ppg}$ are learnable hyperparameters, and VQ denotes the vector quantization operation, while Conv1d and $ReLU^{\\circ}(*)$ represent the convolution and ReLU operations, respectively.\nAs a consequence, with the guidance of neural codec training, the quality of the fused hybrid SSL and PPG features can be significantly enhanced, resulting in improved naturalness and intelligibility in Takin-VC."}, {"title": "Context-aware & Memory-augmented Timbre Modeling", "content": "3.3.1 Context-aware Timbre Modeling via Cross-attention\nCurrent mainstream VC methods typically regard speaker timbre as a global time-invariant representation [62, 8]. Nevertheless, recent work [23] has uncovered a close correlation between timbre modeling and content information.\nHence, drawing inspiration from this insight, we propose an innovative context-aware timbre modeling approach based on CA. First, we employ a pre-trained SV model to extract a target speaker's timbre embedding rather than using a global timbre encoder, and then concatenate it with the shuffled Mel-spectrograms of the target speech, denoted as $X_{ttimb}$, to minimize the influence of the target content. Subsequently, to learn semantically correlated timbre features that associate the source content with the timbre of the target speaker, we introduce an effective CA-based module. This module takes source content $X_{scont}$ as the query and $X_{ttimb}$ as both the key and value, consisting of a series of linear projection, multi-head CA, layer normalization, and position-wise feed-forward networks (FFN), as detailed in Fig. 3. Finally, we perform interpolation on the extracted features $X_{sett}$ to ensure that their dimensionality corresponds to that of the source Mel-spectrogram, thereby facilitating the subsequent training of the CFM model."}, {"title": "Memory-augmented Timbre Modeling", "content": "Since we use a CFM model to reconstruct the source Mel-spectrograms, obtaining high-quality conditional target inputs is quite essential, as they provide key guidance for training the CFM model. To this end, we design an efficient memory-augmented module that adaptively integrates the Mel-spectrogram and VP features of the reference speech, as outlined in Fig. 4. To be specific, our proposed memory-augmented module initially use a Conv1d layer to project the $x_{ref}$ to a latent feature space. Subsequently, we incorporate multiple SA blocks, each containing several group normalization, multi-head SA, and 1D Conv layers, followed by a Conv1d layer and a LeakyReLU activation layer. This design effectively leverages these features in a stable and learnable manner. At the end of memory module, we compute the average vector of the obtained representations across the time dimension to produce the final output $X_{tcond}$. Finally, $X_{tcond}$ is input into the Memory Fusion Layer (a combination of the Gated Activation Layer and FiLM Layer [63]) within the flow matching network"}, {"title": "Conditional Flow Matching-based Decoder", "content": "In Takin-VC, to facilitate more efficient training and faster inference, we leverage a CFM model with optimal-transport (OT-CFM) to approximate the distribution of source Mel-spectrograms and generate outputs conditioned on $X_{sett}$ and $X_{tcond}$, all in a simulation-free manner.\nAssume that the standard distribution and target distribution are denoted as $p_0(x)$ and $p_1(x)$, respectively. The OT flow $\\phi : [0,1] \\times R^d \\rightarrow R^d$ establishes the mapping between two density functions through the use of an ordinary differential equation (ODE):\n$\\frac{d}{dt} \\phi_t(x) = v(\\phi_t(x), t) (2)$\n$\\phi_0(x) \\sim p_0(x) = N(x; 0, I), \\phi_1(x) \\sim p_1(x)$\nwhere $v_t$ is a learnable time-dependent vector field, and $t \\in [0,1]$. Since multiple flows can generate this probability path, making it challenging to determine the optimal marginal flow, we adopt a simplified formulation, as proposed in [29]:\n$\\phi_t^{OT}(x) = \\mu_t(z) + \\sigma_t(z)x$\n$\\mu_t(z) = (1-(1-\\sigma_{min})t)z, \\sigma_t(z) = t (3)$\nwhere z represents the random variable, $\\sigma_{min}$ is a hyper-parameter set to 0.0001. As a consequence, the final training objective of the proposed Takin-VC can be formulated as:\n$L_{Takin} = E_{t, p(x_0), q(x_1)} [||(\\tilde x_1 - (1 - \\sigma) x_0) - u_t(\\phi_t^{OT}(x_0)|\\theta, h)||^2](4)$\nwhere $\\theta$ is the weights of the flow matching model, h is the conditional input $X_{tcond}"}, {"title": "Experimental Setup", "content": "4.1 Baseline System\nWe conduct a comparative experiment of the performance in zero-shot voice conversion between our proposed Takin-VC approach and baseline systems, encompassing the following system:\n\u2022 DiffVC [39]: A zero-shot VC system based on diffusion probabilistic modeling, which employs an averaged mel spectrogram aligned with phoneme to disentangle linguistic content and timbre information.\n\u2022 NS2VC5: A modified voice conversion edition of NaturalSpeech2 [64], which employ both diffusion and codec model to achieve zero-shot VC."}, {"title": "Evaluation Metrics", "content": "To evaluate the performance of our proposed Takin-VC and baseline systems, both subjective and objective metrics are introduced. For subjective metrics, we employ naturalness mean opinion score (NMOS) to evaluate the naturalness of the generated samples and similarity mean opinion scores (SMOS) to evaluate the speaker similarity. We invite 20 professional participants to listen to the generated samples and provide their subjective perception scores on a 5-point scale: '5' for excellent, '4' for good, '3' for fair, '2' for poor, and '1' for bad. For objective metrics, we employ word error rate (WER), UTMOS, and speaker embedding cosine similarity (SECS) to evaluate the intelligibility, quality, and speaker similarity. Specifically: 1) We use a pre-trained CTC-based ASR model to transcribe the generated speech and compare with ground-truth transcription; 2) We use a MOS prediction system that ranked first in the VoiceMOS Challenge 20227 to estimate the speech quality of the generated samples; 3) We use the WavLM-TDCNN speaker verification model to measure speaker similarity between generated speech and target speech."}, {"title": "Dataset", "content": "4.3.1 Small Scale Dataset\nWe employ the LibriTTS dataset to train our system and baseline systems, which contain 585 hours of recordings from 2,456 English speakers. We follow the official data split, using all training datasets for model training and \"dev-clean\" for model selection. The \"test-clean\" dataset is used to construct the evaluation set. All samples are processed at a 16kHz sampling rate.\n4.3.2 Large Scale Dataset\nTo train a robust Takin VC model, we collected a dataset of approximately 500k hours. During the data collection process, we used an internally constructed data pipeline specifically designed for audio large model tasks. This pipeline includes signal-to-noise ratio (SNR) filtering, audio spectrum filtering (filtering out 24k audio with insufficient high-frequency information and pseudo 24k audio), VAD (Voice Activity Detection), LiD+ASR (Language Identification + Automatic Speech Recognition), speaker separation and identification, punctuation prediction, and background noise filtering. Regarding the test set, to validate the effectiveness of the Takin-VC model, we collected speech data from the internet that includes 100 non-preset speakers for evaluation. These speakers represent a variety of attributes such as gender, age, language, and emotion to ensure a comprehensive evaluation of the model's performance."}, {"title": "Model Configuration", "content": "For the content encoder part, in the first stage, we used the 12-layer HybridFormer-base model trained on a large dataset of 500K hours. For the wavlm part, we used the output features of the 6th layer. In the VQ part, we adopted a single-layer 8200 codebook with a hidden dimension of 1024, trained for 1 million steps on 100K hours of data. The fusion layer, as described in Sec. 3.2, is a simple process of conv1d, activation layer, and weighted summation. The Decoder adopts the same structure and configuration as Hificodec [44].\nIn the part of timbre modeling and flow matching restoration, both the context-aware timbre module and the memory module use a transformer layer with 8 heads, 6 layers, and a hidden size of 1024, with only the form of attention being different. The main structure of flow matching uses a design of 10-layer Unet plus 3 layers of resblock, with a hidden size of 1280. A Memory Fusion Block is inserted into the 10-layer Unet to enhance the timbral similarity of the generated audio.\nFor the small-data experiments, we used four A800 GPUs, whereas the large-data experiments were conducted on eight A800 servers. The batch size on each card was set to 16, and the AdamW learning rate was set to 1e-4. In the inference section, experiments typically took 15 to 50 steps, with the final table uniformly adopting the results of 50 steps. The Classifier-Free Guidance (CFG) coefficient ranged from 0.1 to 1.0, with 0.7 used in the table. The specific experimental results will be detailed later."}, {"title": "Experimental Results", "content": "5.1 Experiments on small dataset\nWe first evaluate the performance of our proposed Takin-VC using subjective metrics. These metrics capture human perception of the enhanced speech's naturalness, intelligibility, and speaker similarity. As shown in Table 1, we can find that 1) our proposed system achieves the highest NMOS of 3.98, which is significantly higher than baseline systems; 2) the speaker similarity of our proposed system also outperforms all baseline systems. These results demonstrate that Takin-VC can achieve superior performance than the baseline system in the perceived aspect.\nFurthermore, we evaluate the performance using objective metrics. The WER of our proposed system is 2.35, only slightly higher than the ground truth samples, indicating that the samples generated by Takin-VC exhibit better intelligibility."}, {"title": "Ablation Study", "content": "We conduct ablation experiments to evaluate the effectiveness of each component in our proposed system for generating natural-sounding samples and accurate timbre modeling. As shown in Table 3, NMOS and WER results degrade when we replace the proposed hybrid content encoder with a conventional ASR encoder. This suggests that the conventional ASR encoder is less capable of disentangling linguistic content from the necessary paralinguistic information, underscoring the importance and effectiveness of our hybrid encoder in extracting linguistic content. Additionally, we observe a notable decline in speaker similarity when the voice print is removed from the attention module. We believe the voice print introduces a stronger timbre bias, which helps the attention module focus on capturing timbre information. Furthermore, when we remove the memory module, SMOS and SECS scores show significant degradation compared to the original Takin-VC, demonstrating the critical role of the memory module in improving timbre modeling. These ablation results demonstrate the effectiveness of each component proposed in our Takin-VC."}, {"title": "Discussion and Limitations", "content": "Takin is an effective and data-efficient zero-shot VC system that achieves comparable naturalness and speaker adaptation performance to its large-scale, autoregressive counterparts. The core of this approach lies in the neural codec training based hybrid linguistic content encoder, which captures high-quality speaker-agnostic content representations, and the introduction of both context-aware timbre modeling and memory-augmented modules to enhance speaker similarity performance. In many ways, our work provides a strong foundation for future studies, as we demonstrate that state-of-the-art performance in this task can be achieved without relying on complex training setups, representation quantization steps, or costly autoregressive models.\nThis work primarily focuses on zero-shot capabilities for speech generation, while zero-shot capabilities for speech editing remain limited and are a subject for future exploration. Additionally, while high-quality zero-shot VC has great potential, it can also lead to negative social impacts, such as voice impersonation of public figures and non-consenting individuals. We highlight this as a potential misuse of the technology to raise awareness of its ethical implications."}, {"title": "CONCLUSIONS", "content": "In this study, we propose a novel framework called Takin-VC, designed to achieve high quality and similarity in zero-shot VC. We introduce an effective neural codec training guided hybrid content encoder that leverages quantized features from both pre-trained HybridFormer and WavLM to extract the linguistic content of the source speech. This hybrid content encoder improves the naturalness and intelligibility of the converted speech. Additionally, we present an advanced cross-attention-based, context-aware timbre modeling approach that captures fine-grained, semantically associated target timbre features. Furthermore, we employ a conditional flow-matching model to efficiently reconstruct the Mel-spectrogram of the source speech and propose an efficient memory-augmented module for the flow-matching process, enhancing the overall performance of the generated samples. Experimental results demonstrate that Takin-VC outperforms all baseline systems in naturalness and speaker similarity on benchmark datasets. Ablation studies also confirm the effectiveness of each component in our approach."}]}