{"title": "Eliminating the Language Bias for Visual Question Answering with fine-grained Causal Intervention", "authors": ["Ying Liu", "Ge Bai", "Chenji Lu", "Shilong Li", "Zhang Zhang", "Ruifang Liu", "Wenbin Guo"], "abstract": "Despite the remarkable advancements in Visual Question Answering (VQA), the challenge of mitigating the lan-guage bias introduced by textual information remains unresolved. Previous approaches capture language bias from a coarse-grained perspective. However, the finer-grained information within a sentence, such as context and keywords, can result in different biases. Due to the ignorance of fine-grained information, most existing methods fail to sufficiently capture language bias. In this paper, we propose a novel causal intervention training scheme named CIBi to eliminate language bias from a finer-grained perspective. Specifically, we divide the language bias into context bias and keyword bias. We employ causal intervention and contrastive learning to eliminate context bias and improve the multi-modal representation. Additionally, we design a new question-only branch based on counterfactual generation to distill and eliminate keyword bias. Experimental results illustrate that CIBi is applicable to various VQA models, yielding competitive performance.", "sections": [{"title": "I. INTRODUCTION", "content": "Visual Question Answering (VQA) [1], [2] is a challenging multi-modal task. Given a question and an image, the model's extractor [3], [4] extracts the textual and visual features inde-pendently, which are then used for joint reasoning. However, recent studies [5], [6] have shown that most existing VQA models over-rely on superficial correlations between certain types of questions and answers, rather than truly compre-hending both textual and visual cues. One straightforward solution [7], [8] to mitigate language bias is to enhance the training data by using extra annotations or data augmentation, which requires high costs. Some existing methods [6], [9]\u2013[11] attempt to mitigate language bias by designing complex model branches, which are utilized during the training phase to capture biases introduced by the textual modality. Besides, counterfactual generation based methods [12]-[15] result in significant performance improvements compared to other de-biasing methods by balancing the training data. These methods generate counterfactual inputs by fully masking the question or partially masking keywords from a coarse-grained perspective. However, due to the existence of fine-grained language bias in syntactic structure, keyword and context, most existing methods either fail to sufficiently capture language bias or overly eliminate the textual information, leading to the loss of some useful textual features. For instance, in VQA v1.0 [16], approximately 80% of questions containing \u201cbanana\" have the answer \"yellow\", which we refer to as keyword bias. And about 90% of questions starting with \"Do you see a\" have the answer \"yes\" [12], which we refer to as syntactic structure bias. Additionally, almost 80% specific pattern of question-type words and keyword such as \u201cWhat color\" + \"plate\" have the answer \"white\", which we refer to as context bias.\nInspired by this, we propose a fine-grained training scheme named CIBi to eliminate language bias from a causal per-spective [17], [18]. Different from the previous coarse-grained debiasing methods, we define the language bias as the con-founder bias and further divide it into syntactic structure, keyword and context bias. Then we eliminate fine-grained biases that exist in syntactic structure, keyword and context with causal intervention at token-level. For context bias, we generate two corresponding counterfactual samples for each original VQA training sample based on two synthesizing mechanisms. Specifically, we synthesize counterfactual ques-tions by replacing keywords and syntactic structure with se-mantically similar synonyms, respectively. The original image and counterfactual question compose a new VQ pair. Then we utilize contrastive learning to induce the model to learn a unbiased multi-modal representation using counterfactual samples. For syntactic structure and keyword bias, we generate counterfactual questions by masking certain keywords and syntactic structure. By training the keywords and syntactic structure debiasing branch, we distill the effect of keywords and syntactic structure bias at token-level and subtract it from the total causal effect. Overall, we have eliminated language bias from a fine-grained perspective. It is worth noting that each branch can be considered as a plug-and-play auxiliary module directly integrated into existing VQA models to enhance their unbiased reasoning capabilities.\nWe summarize our main contributions as follows:\n\u2022\n\u2022\nWe explain the causes of bias at a fine-grained level and formulate the VQA debiasing process from a causal perspective.\nWe propose a generic training scheme called CIBi to eliminate syntactic structure, keyword and context bias respectively with causal intervention."}, {"title": "II. RELATED WORK", "content": "A. Language Bias in VQA\nThe Visual Question Answering (VQA) task takes images and textual questions as input, aiming to make prediction based on both textual and visual features. Current research [5], [6], [10] shows that VQA models often tend to overly rely on language biases present in the dataset for answer reasoning, resulting in lower generalization performance. When the most of the answers to the question \"What color are the bananas\" are \"yellow\", VQA models learn from the statistical regularities between the most occurring answer yellow and certain patterns \u201cwhat color\u201d in the question, neglecting visual modality from the images [6]. To better address language bias problem in VQA, Agrawa propose the VQA-CP [5] dataset in which training and test sets have different distributions. The perfor-mance of many conventional VQA models drop significantly on VQA-CP due to distributional differences.\nB. Debiasing Strategies in VQA\nMost of recent solutions to reduce the language bias in VQA can be grouped into two categories. One straightforward solution is to create more balanced training data by im-plicit/explicit data argumentation. For example, Zhang et.al. [19] collected complementary abstract scenes with opposite answers for all binary questions. And Goyal et.al. [20] ex-tended this idea into real images and all types of questions. Das [21] and Park [22] exploited human visual and textual explanations respectively to strengthen the visual grounding in VQA. Another solution is to design a separated QA branch to capture and eliminate the language prior which can further be grouped into two types: adversary-based and ensemble-based. Adversary-based methods [23] train the question-only branch to weakened unwanted correlations between questions and answers in an adversarial training scheme. Ensemble-based methods [6], [10] employ an ensemble strategy to combine the predictions of two models, deriving training gradients based on the fused answer distributions.\nC. Causal Inference\nRecently, causal inference [17] has been applied to many tasks of natural language processing and computer vision, and it shows promising results and provides strong interpretability and generalizability, including text generation, language un-derstanding, visual explanations, image recognition, zero-shot and few-shot learning, representation learning, semantic seg-mentation, and visual Question Answering tasks. In computer vision tasks, previous efforts have tackled the problem of im-balanced data distribution through counterfactual intervention, causal effect disentanglement, the back-door and front-door adjustments. Different from the previous methods, our method utilizes fine-grained causal inference to generate counterfactual input."}, {"title": "III. METHODOLOGY", "content": "Fig.2 gives an overview of CIBi, which is composed of four parts: base VQA model, context debiasing branch, syntactic structure and keyword debiasing branch and classifier. In this section, we introduce the structural causal model for VQA (Section III-B) and debiasing training scheme CIBi (Section 3.3, 3.4, 3.5, 3.6)\nA. Task Definition\nIn accordance with the common formulation [6], we define the VQA task as a multi-class classification problem. Given a multi-modal dataset D = {Vi, Qi, Ai}N where each sample is a triplet, including a picture Vi \u2208 V, a question Qi \u2208 Q, and an answer Ai \u2208 A, the task of VQA model is to learn a mapping function Fuqa : V \u00d7 Q \u2192 R|A|. For each image Vi, visual features vi are extracted by an image encoder Ev : V \u2192 Rnu\u00d7dv to generate a set of nn vectors of dimension dr. For each question Qi, textual features qi are extracted by a question encoder Eq : Q \u2192 Rnq\u00d7dq_to generate a set of ng vectors of dimension dq. The final answer distribution is predicted by a mapping function Fuqa(Vi, qi). The classical learning strategy of VQA models is to minimize the standard cross-entropy criterion over a dataset of size N:\n$L_{VQA} = \\frac{1}{N-1} \\sum_{i=0} log(softmax(F_{UQA} (V_i, q_i)))[A_i]$ (1)\nB. Structural Causal Model for VQA\nStructural Causal Models (SCM) are designed for causal analysis. As shown in Figure 1, Q, V, R, and A represent textual modality information, visual modality information, multimodal representation and answer, respectively. Context bias exists in syntactic structures and keywords, influencing the learning of the multi-modal representation R and subse-quently affecting the generation of the answer A. We consider high-frequency context patterns as confounder Uc and context bias as confounder bias. The existence of confounder Uc enables a backdoor path [17] between Q and R, making them spuriously correlated even if there is no direct causality between them. As shown in Figure 1a, where the answer A is influenced by three paths: V \u2192 R \u2192 A, Q \u2192 R \u2192 A, and Uc\u2192R\u2192 A. To mitigate context bias, we block the"}, {"title": "C. Selection Of Fine-grained Bias", "content": "backdoor path by applying the backdoor adjustment theorem [17]:\n$P(A\\do(Q = q), V = v) = \\sum_u P(Q = q, V = v, u)P(u)$ (2)\nwhere do() is the do-operator (do-calculus) [17] and do(Q = q) can be understood as cutting all the original incoming arrows to Q, making Q and Uc independent.\nIn Figure 1b, the syntactic structure and keywords Qs,k influence A by two paths: Qs,k \u2192 R \u2192 A and Qs,k \u2192 A. We formulate the spurious correlation between keyword bias and the answer as the direct causal effect of the keyword on the answer, which is the path Qs,k \u2192 A. We mitigate the keyword bias by distilling the direct causal effect of the keyword and subtracting it from the total causal effect.\nIn this section, we have provided a detailed explanation of the process for determining the specific concept for each component leading to fine-grained bias, including syntactic structure, keywords, and context. For example, given a sample with the question \"What color is the banana?\", we extract question-type words such as \"What color\u201d for each question as the syntactic structure, and select \"banana\u201d as the keyword. We define the co-occurrence pattern of \"what color\" and \"banana\" as the context.\nSyntactic Structure (S): We consider the 65 question prefixes in the annotation of the original VQA dataset as the concept of syntactic structure.\nKeyword (K): Following Si [24] which utilize the mutual information to measure the mutual dependence, for a QA pair (q, a), we calculate the mutual information of each token e \u2208 q in the remaining sentence (except the question-type words) to the ground-truth answer a as:\n$MI(w, a) = log \\frac{\u0397(\u03c9, \u03b1)}{\u0397(w) * H(\u03b1)/\u039a}$ (3)\nwhere H(w), H(a) and H(w, a) respectively represent the to-tal numbers of samples in which w, a and their co-occurrence occur. K is the total number of samples in the dataset. Richer mutual information means stronger correlation between the word and the answer. we select the word with highest score from the remaining sentence (except the question-type words) as keyword.\nContext (C): We defined the sepcific pattern of question-type words and keyword from the remaining sentence as the concept of context for a given sample."}, {"title": "D. Context Debiasing", "content": "As described in Section III-B, the confounding variable undermines the confidence in the correlation between Q and R, further affecting the answer A. Based on the theory of causal intervention, we design a context-debiasing branch to control confounder bias and disrupt the backdoor path Uc \u2192 Q by counterfactual generation and contrastive learning. Specifically, for counterfactual generation, we first maintain a fixed syntactic structure fragment and replace the keywords in the question with synonyms. And then we keep the keywords fixed and replace the syntactic structure in the question with semantically equivalent context. As illustrated in Figure 2, the shaded portion represents the unchanged part of the question.\nAfter obtaining the causally intervened question represen-tation, we perform contrastive learning to enhance the in-teraction between modalities and magnify the differentiation of fusion representation among samples. We create positive pairs (V, Q+) by matching V with its causally intervened question Q+ and negative pairs by matching V with its unpaired question Q\u00af, which are the images within the mini-batch except the matched Q. For this purpose, we utilize the commonly used normalized temperature-scaled cross-entropy loss (InfoNCE) as the objective loss function for contrastive learning, formulated as follows:"}, {"title": "E. Syntactic Structure and Keyword Debiasing", "content": "In this section, we specifically introduce how to reduce keyword bias from the newly designed question-only branch called SK-debiasing branch. Technically, the question-only branch is added alongside the base VQA model. We generate counterfactual samples by randomly masking keywords and syntactic structure, and then input them into the question encoder Eq to extract question embedding q. Then we distill and calculate keyword bias by a mapping Fqa (q), which is a neural network consisting of three linear layers and a sigmoid function. We utilize fusion function H to fuse Fqa(q) and Fuqa (Vi, qi):\n$F_{oga}(V_i, q_i) = H(F_{vqa}(V_i, q_i), F_{qa}(q))$ (6)\n$H(F_{vqa}(V_i, q_i), F_{qa} (q)) = log \u03c3(F_{vqa}(V_i, q_i) \u2013 F_{qa}(q))$ (7)\nThe training strategy follows RUBi [6]. The base VQA model and question-only branch are trained with the objective of minimizing the standard cross entropy losses, which are formalized as follows:\n$L_{QA} = - \\frac{1}{N-1} \\sum_{i=0} log(softmax (F_{qa} (q_i))) [A_i]$ (8)\n$L_{VQA} = - \\frac{1}{N-1} \\sum_{i=0} log(softmax (F_{vqa} (V_i, q_i))) [A_i]$ (9)"}, {"title": "F. Model Training and Testing Strategy", "content": "In general, we construct our total loss Ltotal by summing the LCL, LVQA and LQA together in the following equation:\n$L_{total} = \u03bbL_{CL} + L_{VQA} + L_{QA}$ (10)\nwhere LVQA, LQA are over Fuqa (Vi, qi), Fqa(q), LCL represents the objective loss function for contrastive learning, \u03bb is a weighting hyperparameter. We backpropagate Ltotal to optimize all the related parameters, including the parameters of the base VQA model, the context-debiasing branch, and the"}, {"title": "IV. EXPERIMENT", "content": "question-only branch. Additionally, we prevent the gradients computed from LQA from being propagated back to the question encoder to avoid direct capture of keyword bias. The two branches are employed only during training to learn an unbiased model from biased data. When testing, we remove both of them to maintain the base VQA model unchanged.\nA. Datasets and experiment settings\nWe evaluate the effectiveness of our method on VQA-CP v2 [5]. VQA-CP v2 was built by reorganizing VQA v2 [16], which is designed to test the robustness of the VQA models. Additionally, we present results on the balanced VQA v2 dataset to assess whether our method over-corrects language bias.\nThe models are evaluated via standard VQA evaluation metric [16]: the accuracy of all answers and different question types. We conduct experiments based on the following baseline models: RUBi [6], LPF [10], CF-VQA [12], CSS [13]. The weight adjustment parameter A in the loss Ltotal is set to 0.4 with sensitivity analysis provided(Figure 4). Each training batch consists of 256 samples, and there are 30 training epochs. All other experimental settings remain consistent with the baseline model.\nB. Results and analysis\nQuantitative analysis. For the sake of fairness, we select several representative VQA models as the strong baselines for comparison, all of which mainly focus on the textual modality. RUBi [6], LPF [10], and CF-VQA [12] mitigate language bias by a separate question-only branch. CSS [13] balances training data so as to change the training distribution for unbiased learning. Based on the baseline RUBi [6], our approach reaches an average overall accuracy of 49.62% with an obvious improvement (+2.51%). In addition, when built upon LPF [10] and CF-VQA [12], CIBi achieves an accuracy gain of +1.37% and +2.19%, respectively. By utilizing CSS [13] as the baseline, we achieve a relatively slight improve-ment (+0.63%). Nevertheless, the results still indicate that our method contributes to discerning fine-grained biases, thereby enhancing the accuracy of \"Yes/No\u201d questions and \"other\u201d questions. Note that CIBi surpasses the baseline models RUBi [6], LPF [10], CSS [13] by an obvious margin (+3.91%, +0.71% and +2.57%) on \u201cYes/No\u201d questions and surpasses the baseline models RUBi [6], LPF [10], CF-VQA [12] (+11.33%, +7.20% and +20.13%) on \u201cNumber\" questions. Because these two types of question are susceptible to fine-grained context bias and keyword bias, which is ignored by previous methods. Additionally, the results show a certain correlation with the baseline. This is because our method to some extent relies on the model's inherent ability to capture language bias. The competitive performance on VQA-CP v2 shows that our approach mitigates the fine-grained language bias on answers effectively and integrates well with other VQA models. Additionally, the results on VQA v1 also demonstrate the competitive performance and robustness of our method, indicating that we have not overly mitigated bias.\nQualitative analysis. Figure 4 illustrates answer distribu-tions for VQA-CP v2 train and test sets, the RUBi and the CIBi of two question examples: \u201cWhat color are the bananas?\u201d and \"is this...?\". For \u201cis this\" questions, RUBi tends to overfit the answer distribution of the training set and predicts the frequently-used answer \u201cyes\u201d. Compared to RUBi, our method can better recover the answer distribution in the test set on both question examples, which validates the effectiveness of CIBi in mitigating finer-grained language bias.\nC. Ablation Studies\nTo validate the effectiveness of the key components in CI\u0412\u0456, we re-train different versions of our model by ablating certain components. The results on VQA-CP v2 are listed in Table II. Compared to Baseline, w/o context-debiasing branch and w/o keyword-debiasing branch respectively achieved an overall accuracy increase of +1.26% and +1.48%, but still performs worse than the complete model."}, {"title": "V. CONCLUSION", "content": "In this work, we propose CIBi, a casual intervention training scheme designed to be model agnostic, to reduce fine-grained language bias learned by the VQA model. It is based on two debiasing branches that capture and remove syntactic structure, keyword and context bias from the textual modality, respectively. Experiments show that CIBi is effective with different kinds of common VQA models. In future work, we will explore the applicability of our learning scheme in addressing uni-modal biases across various multi-modal tasks."}]}