{"title": "Learning Traffic Anomalies from\nGenerative Models on Real-Time Observations", "authors": ["Fotis I. Giasemis", "Alexandros Sopasakis"], "abstract": "Abstract-Accurate detection of traffic anomalies is crucial for\neffective urban traffic management and congestion mitigation.\nWe use the Spatiotemporal Generative Adversarial Network\n(STGAN) framework combining Graph Neural Networks and\nLong Short-Term Memory networks to capture complex spatial\nand temporal dependencies in traffic data. We apply STGAN to\nreal-time, minute-by-minute observations from 42 traffic cameras\nacross Gothenburg, Sweden, collected over several months in\n2020. The images are processed to compute a flow metric\nrepresenting vehicle density, which serves as input for the model.\nTraining is conducted on data from April to November 2020, and\nvalidation is performed on a separate dataset from November 14\nto 23, 2020. Our results demonstrate that the model effectively\ndetects traffic anomalies with high precision and low false\npositive rates. The detected anomalies include camera signal\ninterruptions, visual artifacts, and extreme weather conditions\naffecting traffic flow.", "sections": [{"title": "I. INTRODUCTION", "content": "Urban traffic management systems rely heavily on the\neffective detection of traffic anomalies to prevent congestion\nand reduce accidents [1]. Rapid urbanization and the increas-\ning complexity of traffic networks have rendered traditional\nstatistical methods [2]\u2013[4] insufficient for accurate traffic\nforecasting. The high-dimensional and non-linear nature of\ntraffic data necessitates advanced modeling techniques capable\nof capturing both spatial and temporal dependencies [5]\u2013[7].\nRecent advancements in artificial intelligence, particularly\ndeep learning, have shown promise in improving traffic fore-\ncasting accuracy by leveraging large-scale data and computa-\ntional resources [8]. These methods include graph-based deep\nlearning models that efficiently handle spatial-temporal corre-\nlations and diffusion models that transform traffic forecasting\ninto a conditional image generation task [9]. Furthermore,\ninnovative approaches that incorporate feature engineering\nwithout relying on real-time data have been developed to\naddress challenges in urban traffic flow prediction [10].\nGraph Neural Networks (GNNs) have emerged as powerful\ntools for modeling graph-structured data, effectively capturing\nspatial dependencies in traffic networks [8], [11]. Meanwhile,\nRecurrent Neural Networks (RNNs), such as Long Short-Term\nMemory (LSTM) networks, are renowned for their ability\nto model temporal patterns [12], [13]. However, standalone\nLSTMs often fall short in exploiting the inherent spatial\ndependencies present in traffic data [14].\nIn this research, we use the Spatiotemporal Generative Ad-\nversarial Network (STGAN) framework to improve anomaly\ndetection in urban traffic systems by incorporating external\nfactors and applying it to real-time data from Gothenburg's\ntraffic network.\nFurthermore, we construct a dynamic, real-world digital\ntwin simulation of Gothenburg's urban traffic network by\nrepresenting real-time traffic cameras as nodes and roads as\nedges [8]. This detailed modeling approach enables a more\naccurate representation of spatial dependencies and traffic flow\nvariations across the city. By applying the STGAN framework\nto this comprehensive model, we are able to capture both\nshort-term and long-term traffic patterns more effectively than\nexisting methods."}, {"title": "II. RELATED WORK", "content": "Detecting traffic anomalies is a critical challenge in modern\ntraffic management and urban planning, where understanding\nthe spatial and temporal evolution of traffic is essential for\neffective decision-making [5]. Machine learning approaches,\nparticularly neural networks and generative models, have been\nwidely applied to this problem [7], [15]\u2013[17]. However, ex-\nisting methods often fail to fully capture the complexity of\nurban traffic systems, which are influenced by both temporal\ndynamics and spatial topology [8].\nLong Short-Term Memory (LSTM) networks [12] have been\nextensively used for traffic forecasting due to their ability to\nmodel long-term temporal dependencies. Prior research on the\nGothenburg dataset primarily employed LSTM networks to\nmodel temporal dynamics from individual cameras [13]. While\neffective in capturing temporal patterns, these methods over-\nlook the crucial spatial dependencies between traffic nodes,\nwhich are essential for accurately predicting traffic flow [14],\n[18].\nGraph Neural Networks (GNNs) provide a natural ex-\ntension to traffic forecasting by explicitly modeling spatial\ndependencies between traffic nodes. GNNs have shown great"}, {"title": "III. METHODS", "content": "In this section, we present the framework of STGAN,\nbuilding upon [10]. We aim to enhance anomaly detection in\ntraffic data by effectively capturing both spatial and temporal\ndependencies.\nA. Traffic Network Representation\nA traffic network is represented as a weighted graph\nG = (V, E, W), where V is a finite set of N nodes (traffic\ncameras), E is the set of edges representing connections\nbetween nodes, and W \u2208 RN\u00d7N is the weighted adjacency\nmatrix representing spatial correlations.\nThe adjacency between nodes is determined based on their\nspatial proximity. Two nodes vi and vj are considered adjacent\nif the distance between them is within a predefined threshold\nor if they are connected by a road segment. The edge weight\nbetween nodes vi and vj is given by:\n$W_{ij} = \\begin{cases}\n    \\exp\\left(-\\frac{\\text{dist}(v_i, v_j)}{\\sigma^2}\\right), & \\text{if } e_{ij} = 1, \\\\\n    0, & \\text{otherwise,}\n\\end{cases}$\nwhere dist(vi, vj) denotes the Euclidean distance between\nnodes vi and vj, o is the standard deviation of all pairwise\ndistances between nodes, and eij indicates adjacency."}, {"title": "B. Data Representation and Problem Definition", "content": "Each data point is represented as the triplet s = (v,t, xv,t),\nwhere v \u2208 V is the node index, t is the time index, and\nxv,t \u2208 RF is the feature vector at node v and time t.\nThe historical data over T time steps is denoted as SE\n][RT\u00d7N\u00d7F, representing the traffic dynamics of all nodes. Given\nthe traffic network G and historical data S, our goal is to\nidentify anomalies at the next time step T +1 by predicting\nexpected traffic measurements and detecting deviations indica-\ntive of anomalies.\nThe STGAN framework comprises two primary compo-\nnents:\n\u2022 Spatiotemporal Generator (Go): Generates predicted\nsequences of traffic data.\n\u2022 Spatiotemporal Discriminator (D): Distinguishes be-\ntween real and generated sequences.\n1) Spatiotemporal Generator Components: The spatiotem-\nporal generator Ge consists of three modules designed to\ncapture different aspects of the traffic data:\n1) Recent Module: Captures short-term spatiotemporal\ndependencies using a Graph Convolutional Gated Re-\ncurrent Unit (GCGRU).\n2) Trend Module: Learns long-term temporal patterns\nusing an LSTM network.\n3) External Module: Incorporates external factors (e.g.,\ntime of day, day of the week) using a fully connected\nlayer.\nThe outputs of these modules are fused using a Graph Convo-\nlutional Network (GCN) layer to produce the final prediction\nXvt for each node v at time t.\n2) Graph Convolutional Gated Recurrent Unit (GCGRU):\nThe GCGRU extends the traditional GRU by integrating graph\nconvolution operations to capture spatial dependencies among"}, {"title": "C. Loss Functions", "content": "The generator aims to produce sequences that are both\nrealistic and close to the true data. Its loss function combines\nadversarial loss and reconstruction loss:\n$L_{G}(\\theta) = \\sum_{S} \\left[-\\log(D_{\\phi}(\\hat{S}_{v,t})) + \\lambda_{G}||G_{\\theta}(v,t) - X_{v,t}||^{2}\\right]$,\nwhere $D(\\hat{S}_{v,t})$ denotes the discriminator's probability that\nthe generated sequence is real, and $A_{G}$ balances the adversarial\nand reconstruction losses. The discriminator seeks to correctly\nclassify real and generated sequences. Its loss function is:\n$L_{D}(\\phi) = \\sum_{S}[-\\log(D_{\\phi}(S_{v,t})) - \\log(1 - D_{\\phi}(\\hat{S}_{v,t}))]$."}, {"title": "F. Implementation Details", "content": "1) Architecture and Training Parameters: The model archi-\ntecture and training parameters are crucial for its performance.\nFor the recent module, we use two layers of GCGRU with\na hidden dimension of 64, allowing the model to capture\ncomplex spatiotemporal dependencies. The trend module con-\nsists of two LSTM layers with a hidden dimension of 64\nto model long-term temporal patterns. The external module\nprocesses the 31-dimensional encoded time features through a\nfully connected layer with an output dimension of 64.\nDuring training, we set the learning rate for both the\ngenerator and discriminator to 0.001 and use a batch size of 64.\nThe model is trained for 6 epochs, which was determined to\nbe sufficient for convergence based on validation performance.\nThe hyperparameters Ag in Eq. (5) and X in the anomaly score\ncalculation were both set to 1 in our experiments.\nOptimization is performed using the Adam optimizer [22],\nwhich adapts the learning rate during training for faster\nconvergence. All experiments were conducted using PyTorch\nand the code is available at https://gitlab.cern.ch/gdl4hep/\ntraffic-anomaly-detection.\n2) Dataset and Preprocessing: The dataset is derived from\ncameras distributed across the city of Gothenburg in Sweden,\nprovided by Trafikverket. The video data comprises images\ncaptured at one-minute intervals. Each camera is identified by\na unique ID and accompanied by GPS coordinates indicating\nits location and orientation.\nThe images are processed to compute the flow metric flowv,t\nfor camera v at time t, representing the coverage of the roads\ntargeted by the camera. The flow metric is calculated using:\n$flow_{v,t}=\\frac{\\text{Number of detected vehicles}}{\\text{Maximum vehicle capacity}}$\nwhere the number of detected vehicles is obtained using a\nvehicle detection algorithm (e.g., YOLOv5 [23]), and the\nmaximum vehicle capacity is a predefined constant based on\nroad characteristics.\nBefore feeding the data into the algorithm, another process-\ning is performed in order to smooth the data, patch the missing\nminutes, and truncate the times before 4:53 am and after 9:00\npm. Data from April 1st, 2020, until November 13th, 2020,\nare used for training. The results are verified on 10 days from\nNovember 14th, 2020, to November 23rd, 2020.\nFirst, the data from all the cameras used, 42 in our case,\nare concatenated along the time direction. The missing minute\nholes are filled by propagating the last valid observation to\nnext valid, using the forward fill method. The 1-minute data\npoints are then reduced to 5-minute intervals by averaging, in\norder to smooth the variations. The node distances, the node\nsubgraphs Gu, and the time feature are calculated. The time\nfeature, for the external module, in our case is represented as,\n$E=[O_{\\text{weekday}}; O_{\\text{hour}}]$,\nwhere Oweekday is a one-hot vector of length 7 representing\nthe day of the week, and Ohour is a one-hot vector of length"}, {"title": "D. Anomaly Score Calculation", "content": "Anomalies are detected by comparing the generated data to\nthe real data and assessing the discriminator's confidence. The\nanomaly score for a data point s = (v, t, xv,t) is computed as:\n$s_{G}(v,t) = ||G_{\\theta}(v,t) - X_{v,t}||^{2}$,\n$s_{D}(v,t) = D_{\\phi}(S_{v,t}) - D_{\\phi}(\\hat{S}_{v,t})$,\n$\\text{score}(v, t) = s_{G}(v,t) + \\lambda s_{D}(v,t)$,\nwhere A balances the contributions of sq and SD.\nE. Model Training Procedure\nThe STGAN is trained using an adversarial learning process,\nwhere the generator and discriminator are updated alternately\n[21]. The training procedure is summarized in Algorithm 1."}, {"title": "IV. RESULTS", "content": "Training metrics such as discriminator and generator binary\nloss as well as discriminator accuracy and generator MSE, are\npresented in Figure 2. The last 10 values of these metrics are\nsummarized in Table I. The low accuracies of the discriminator\nsuggests that the generator has become good at generating fake\nsequences that are hard to distinguish from the real data.\nFollowing [10], we calculate the anomaly scores of all the\ndata in the test set. We label the data points with the top K%\nanomaly scores as anomalies.\nEvaluating anomaly detection in real-world scenarios re-\nmains an open challenge, as obtaining a complete set of ground\ntruth data is difficult. Since we were unable to do this, we take\nthe output of the algorithm and manually verify each flagged\nanomaly. The precision is calculated by:"}, {"title": "V. DISCUSSION", "content": "The results presented provide compelling evidence of the\neffectiveness of the STGAN framework for traffic anomaly\ndetection. Table I illustrates the performance metrics of both\nthe generator and discriminator during training. The relatively\nstable loss values for both components indicate that the model"}, {"title": "VI. CONCLUSIONS", "content": "Our findings indicate that the STGAN framework effectively\ncaptures complex spatiotemporal patterns in traffic data, lead-\ning to improved anomaly detection performance compared to\ntraditional methods.\nWhen compared to existing methodologies, utilizing LSTM\nnetworks alone [13], the STGAN framework demonstrates\nsuperior accuracy in capturing both spatial dependencies and\ntemporal dynamics. For example, while LSTMs can effectively\nmodel long-term temporal dependencies, they often overlook\ncritical spatial relationships between traffic nodes [14]. In"}]}