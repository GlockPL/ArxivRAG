{"title": "TP-UNet: Temporal Prompt Guided UNet for Medical Image Segmentation", "authors": ["Ranmin Wang", "Limin Zhuang", "Hongkun Chen", "Boyan Xu", "Ruichu Cai"], "abstract": "The advancement of medical image segmentation techniques has been propelled by the adoption of deep learning techniques, particularly UNet-based approaches, which exploit semantic information to improve the accuracy of segmentations. However, the order of organs in scanned images has been disregarded by current medical image segmentation approaches based on UNet. Furthermore, the inherent network structure of UNet does not provide direct capabilities for integrating temporal information. To efficiently integrate temporal information, we propose TP-UNet that utilizes temporal prompts, encompassing organ-construction relationships, to guide the segmentation UNet model. Specifically, our framework is featured with cross-attention and semantic alignment based on unsupervised contrastive learning to combine temporal prompts and image features effectively. Extensive evaluations on two medical image segmentation datasets demonstrate the state-of-the-art performance of TP-UNet. Our implementation will be open-sourced after acceptance.", "sections": [{"title": "I. INTRODUCTION", "content": "Medical image segmentation holds a pivotal position within the realm of modern medicine, playing a fundamental role in disease diagnosis, surgical planning, and treatment monitor-ing [1]. The primary objective of this task is to accurately separate and label distinct structures or tissues depicted in medical images, enabling healthcare professionals to conduct meticulous analysis and achieve precise diagnoses. Notably, with the advancements in deep learning techniques, certain networks built upon UNet and its variants have exhibited commendable segmentation accuracy by leveraging semantic information extracted from medical images [2].\nAlthough promising results were reported, existing UNet-based approaches lack consideration for the temporal infor-mation present in scanned medical images [3]. To better understand the temporal information, we have visualized it in Fig. 1. Incorporating temporal information, which repre-sents a sequence of medical images, has the potential to enhance the accuracy of medical segmentation. For instance, in a series of N slices from a patient, denoted as $N_{1:N}$. Notably, certain organs such as the stomach, large intestine, and small intestine exhibit specific temporal patterns within a given interval ($N_i/N, N_j/N$), often following a normal distribution. Specifically, their normal distributions can be expressed as $N_{stomach}(\\mu_{stomach}, \\sigma_{stomach}), N_{large} (\\mu_{large}, \\sigma_{large})$, and $N_{small} (\\mu_{small}, \\sigma_{small})$, respectively. In imaging modalities like MRI or CT scans, where the imaging process typically progresses from top to bottom, the stomach predominantly appears in the early to mid-time intervals, while the small intestine is more prevalent in the mid-time interval, and the large intestine is primarily observed during the late-time interval. Consequently, we have $\\mu_{stomach} \\leq \\mu_{small} \\leq \\mu_{large}$. Thus, within a specific temporal interval, it is essential to prioritize stomach segmentation during the early stages and focus on large intestine segmentation in the later stages to improve the performance of medical segmentation models. Despite the evident importance of temporal information in enhancing segmentation ac-curacy, its consideration is often overlooked in current research.\nTo exploit the temporal information inherent in medical images, we propose TP-UNet, a framework that leverages temporal prompts to guide the learning process of the UNet model. The temporal prompt offers textual signals for guiding the segmentation model in learning semantic and sequential information from medical images. These textual signals first undergo high-dimensional embedding via a well-trained encoder and then interact with the feature map from the image encoder. Due to the different encoding processes of text and image, simply using a linear mapping between image embeddings and textual embeddings for interaction may lead to suboptimal fusion results or even degrade model performance [4]. To address this, we perform a semantic alignment operation before the interaction between text and image modalities, utilizing unsupervised contrastive learning for text representation and image representation to narrow the semantic gap. Finally, for modality fusion, we employ a cross-attention mechanism to aggregate the aforementioned updated text representation and image representation. This process yields a unified representation that serves as input to the decoder of the UNet model. Our main contributions can be summarized as follows:\n\u2022 We propose TP-UNet, a simple and effective framework for medical image segmentation, which can guide the segmentation model to learn the temporal information in medical images through textual prompts.\n\u2022 We propose a two-stage process of semantic alignment and modal fusion to narrow the semantic gap between temporal prompts and image features and effectively aggregate them into a unified representation."}, {"title": "II. RELATED WORK", "content": "A. Prompt Learning\nPrompt learning is a crucial research area in the field of natural language processing (NLP). It focuses on designing effective prompts or questions to guide models in generating accurate and relevant outputs for specific tasks. By tailoring prompts to the task at hand, prompt learning helps the model concentrate on essential information and reduces the search space, thus improving model performance. This technique has proven successful in various NLP tasks, including text classification, named entity recognition, and machine translation.\nRecently, this approach has been applied to medical segmentation, aiming to enhance the segmentation capabilities of models through text prompts. Jie Liu et al. constructed prompts based on the names of organs that need to be segmented [5]. Junde Wu et al., on the other hand, built prompts based on textual descriptions of organs, including their functions, shapes, and appearances, achieving significant improvements in segmentation performance [6]. However, it is evident that neither of these approaches utilized temporal information from medical images. In this study, we designed prompts based on the temporal information of medical images, aiming to guide the segmentation model using temporal information for better performance.\nB. Multimodal Contrastive Learning\nMultimodal contrastive learning is a powerful technique in the field of multimodal learning, which considers multiple modalities such as text, images, and audio. The goal of this method is to learn meaningful representations by maximizing the similarity be-tween samples from the same category and minimizing the simi-larity between samples from different categories. In the context of multimodal learning, this involves aligning the representations of different modalities in a shared embedding space to capture their relationships and interactions. Multimodal contrastive learning lever-ages complementary information from multiple modalities, enhancing the model's understanding of the overall data and its representation learning capabilities. This approach has been widely applied in the medical domain. Yuhao Zhang et al. proposed ConVIRT [7], which uses a bidirectional contrastive objective function to maximize the consistency between true matches and random pairs of images and texts, achieving unsupervised training. This method leverages paired text data across domains without requiring additional expert input. In image classification tasks, ConVIRT achieves high data efficiency, as it can achieve comparable or even better performance than models initialized with ImageNet using only 10% labeled training data. Shih-Cheng Huang et al. proposed the GLORIA [8] framework, which max-imizes the correlation between medical images and texts using global and local contrastive loss functions, leading to improved performance in downstream tasks. In summary, multimodal contrastive learning can enhance data efficiency, align semantic information between different modalities, and improve performance in downstream tasks. The contrastive learning used in this paper addresses the issue of similarity between different modalities, resulting in better general-ization and segmentation performance for downstream medical image segmentation tasks."}, {"title": "III. METHODOLOGY", "content": "In this section, we introduce the TP-UNet model (as shown in Fig 2), which addresses the issue of temporal information forgetting in medical image analysis by designing the Temporal Prompt module. Additionally, we utilize the Semantic Align module to bridge the semantic gap between temporal prompt and image modalities. The combined effect of these two key components significantly enhances the performance of TP-UNet in medical image segmentation, en-abling more precise and consistent segmentation of dynamic images.\nA. Temporal Prompt\nTemporal information plays a crucial role in improving model segmentation performance. we devised a set of prompts to guide the model in understanding the temporal information of medical images. In this study, temporal information is represented as $N_i/N$, indicating that this information is mapped to the interval [0, 1]. The occurrence probability of organs follows a normal distribution within this interval, allowing the model to comprehend the varying probabilities of organ appearance at different timestamps, thereby adjusting its focus on different organs accordingly. The temporal prompt template defined in this study is as follows: \"This is {an MRI / a CT} of the {organ} with a segmentation period of {$N_i/N$}\". Here, the type of medical image and the organ can be selected, while N is determined by the size of a set of slices. In this study, the temporal prompts are automatically generated. Before being input into TP-UNet, a set of prompts is automatically created using the numpy and pandas libraries based on the type of image selected by the physician. In specific situations, radiologists can also choose the range of timestamps for segmentation by dragging to select the desired range. This allows TP-UNet to save a significant amount of time during inference by generating prompts only for the specified range of slices. The generation of a temporal prompt for a single set of slices takes less than 1ms, which is highly significant for clinical applications in radiology.\nB. Multimodal Encoder\nWe first define the input medical image as I and the generated temporal prompt as $P_t$. For the text-based temporal prompt and the medical image requiring segmentation, we designed a multimodal en-coder. For the input text modality $P_t$, we adopted two encoding meth-ods. The first method utilizes the popular multimodal text encoder CLIP [9]. While CLIP performs well with general natural language, directly applying it to medical text may introduce a domain gap. Therefore, we employed parameter-efficient fine-tuning (PEFT) [10] using the LoRA [11] method to adapt CLIP more effectively to our task. The second text encoder we used is Electra [12], another popular text encoder. We compared the performance of these two encoders in the experiments section. We performed supervised fine-tuning (SFT) [13] on the pre-trained Electra model. Both fine-tuned text encoders demonstrated good performance.\nFor the medical image modality, we used the conventional UNet method for segmentation. We integrated the low-level semantics extracted by UNet with the temporal prompt to guide the model for more effective segmentation based on temporal information. The details of the fusion method will be presented in Section III-D.\nC. Semantic Align\nIn this context, we define the encoded image feature $F_m \\in R^{B\\times C \\times H \\times W}$ and the textual temporal prompt encoded feature $F_t \\in R^{B\\times L \\times D}$. Before modality fusion, I undergoes an UNet encoder block, while $P_t$ via the text encoder. Due to the disparate network architectures of the two models, they originate from different seman-tic spaces, potentially leading to a performance decrease after fusion. Therefore, it becomes essential to align the semantics of $F_m$ and $F_t$ before modality fusion. To achieve this, we introduce a semantic align module, aiming to bring semantically similar pairs of $F_{mi}$ and $F_{ti}$ closer together in a batch, while pushing semantically dissimilar $F_{mi}$ and non-corresponding $F_{tj}$ further apart. Consequently, the first contrastive loss function is an image-to-text contrastive loss for the i-th pair:\n$((F_{m\\rightarrow F_t})_i = -log\\frac{exp((F_{mi},F_{ti})/\\tau)}{\\sum_{k=1}^{N} exp((F_{mi},F_{tk})/\\tau)} \\qquad (1)$\n$(F_{mi}, F_{ti}) = \\frac{F_{mi}F_{ti}}{||F_{mi}||||F_{ti}||} \\qquad (2)$\nwhere $\\tau \\in R^+$ represents a temperature parameter.\nThe second loss function is a text-to-image contrastive loss for the i-th pair:\n$l(F_{t\\rightarrow F_m}) = -log\\frac{exp((F_{ti},F_{mi})/\\tau)}{\\sum_{k=1}^{N} exp((F_{ti},F_{mk})/\\tau)} \\qquad (3)$\nIn the end, the loss we need to optimize is:\n$L_{contrastive} = \\lambda l(F_{m\\rightarrow F_t}) + (1-\\lambda)l(F_{t\\rightarrow F_m}) \\qquad (4)$\nwhere $\\lambda \\in [0, 1]$ is a scalar weight. Through the semantic alignment module, the semantic representations of different modalities are aligned. This lays a solid foundation for the subsequent modality fusion.\nD. Modality Fusion\nTemporal prompt is crucial to improve the segmentation per-formance of the model. Consequently, there should be increased emphasis on the design of modality fusion between the temporal prompt modality and the visual modality. Therefore, we designed the cross-attention mechanism, which can be represented as follows:\n$F = softmax(\\frac{([F_m; F_t]W^Q([F_m; F_t]W^K)^T)}{\\sqrt{d_k}}) [F_m; F_t]W^V \\qquad (5)$\nWhere [;] represents the concatenate operation, F is the pixel-wise attention map, and $d_k$ is a scaling factor. $F_m$ and $F_t$ are the projections of $F_m$ and $F_t$. $W^Q$, $W^K$ and $W^V$ are the corresponding weight matrices. Finally, the feature map F is concatenated with the first-level skip-connection feature map of the UNet. It undergoes a convolutional layer and a ReLU activation function before passing through a 1\u00d71 convolutional layer to generate the final segmentation image."}, {"title": "IV. EXPERIMENTS", "content": "A. Experimental Settings\n1) Dataset\nUW-Madison Dataset Collection: \u00b9 The dataset originates from MRI scan images of multiple patients at the Carbone Cancer Center of the University of Wisconsin-Madison. It primarily consists of MRI images of the colon and stomach regions. We call it the UW-Madison dataset [24] and divide it according to a 7: 1: 2 ratio into training, validation, and testing set. Among them, the training set contains 26746 images, the validation set contains 3820 images, and the testing set contains 7642 images.\nLITS Dataset Collection: \u00b2 LITS [25] is an acronym for Liver Tumor Segmentation Benchmark. The data and segmentations are provided by various clinical sites around the world. The dataset contains CT scans of 130 patients. But these scans are 3D nii files, what we need are 2D slices. We divide the LITS dataset into 58638 2D slices, but a large number of 2D slices are also redundant. We finally selected 10967 2D slices with sequence information, and divided these slices according to a 7: 1: 2 ratio into training, validation, and testing set.\n2) Implementation details\nWe choose the Adaptive Momentum Estimation with a weight decay of 0.000001 as the training optimizer. Meanwhile, the initial learning rate is 0.00003, and the weights change with the cosine annealing learning rate; the initial temperature is 25, and the maximum temperature is 96.875. We use the PyTorch training framework and some data augmentation methods, such as CoarseDropout, Hori-zontalFlip, and ShiftScaleRotate. The loss function uniformly adopts the average value of Binary Cross-Entropy (LBCE) and Tversky loss (LTversky), $L_{Tversky}$ is the loss function to solve imbalanced classification problems.\n3) Evaluation metrics\nWe use the Jaccard coefficient and the Dice coefficient to evaluate the performance of the model, which can measure the performance of the model by calculating the similarity between the ground-truth annotation and the predicted annotation. Their calculation can be expressed as follows:\n$Jaccard = \\frac{A \\cap B}{A \\cup B} \\qquad (6)$\n$Dice = \\frac{2(A \\cap B)}{A+B}$\nwhere A and B are binary matrices representing the ground-truth annotation and the predicted annotation, respectively.\nB. Comparison with the Baselines\nTo demonstrate the effectiveness of TP-UNet, we conducted exten-sive experiments on two different datasets. Several commonly used medical image segmentation models are selected for experimental comparison. We use the Jaccard coefficient and the Dice coefficient to evaluate the performance.\nAs shown in Table I, TP-UNet achieved the best performance in all three organ categories (large intestine, small intestine, and stomach) as well as the overall average performance on the UW-Madison dataset. Compared to UNet, the Dice score improved by an average of 4.44%, with the most significant improvement of 5.32% in the Small Intestine category. In addition to UNet, we also compared our TP-UNet with several other methods listed in Table I. TP-UNet outperformed the current state-of-the-art (Swin UNet [22]) by 1.3% in the Dice score, with the most significant improvement of 1.7% in the Small Intestine category.\nWe also conducted experiments on the LITS 2017 dataset. As shown in Table II, our method achieved the highest Dice and Jaccard scores for liver segmentation. Compared to UNet, the Dice score for the liver increased by 6.08%, and the Jaccard score increased by 6.33%. On the LITS 2017 dataset, TP-UNet outperformed the current state-of-the-art method by 9.21% in Dice score, with the most significant improvement of 9.47% in the Small Intestine category.\nC. Case Study\nThis case study provides a comparative analysis of TP-UNet and other baseline methods. TP-UNet, our proposed model, demonstrated superior performance over the traditional U-Net and the transformer-based TransUNet in the segmentation of CT scans.\nWe visualized three segmentation results on LITS2017 dataset, including those from our method. From a visual standpoint, our results closely resemble the ground truth. Our algorithm demonstrates superior performance in areas where many commonly used segmen-tation techniques fail, especially when dealing with details that are imperceptible to the naked eye.\nD. Ablation Study\nWe conducted ablation experiments on the UW-Madison dataset and the LITS2017 dataset to demonstrate the effectiveness of the TP-UNet methods.\nThe experimental results are shown in Table III, where a lower score indicates a greater contribution of the module to the TP-UNet model. First, we verified the effectiveness of the temporal information by removing the timestamp from the temporal prompt, fixing the prompt template to \"This is an MRI / a CT of the organ\". Keeping other settings unchanged, we observed a 2.1% decrease in the mDice score on the UW-Madison dataset. This further demonstrates the effectiveness of incorporating temporal information, which significantly helps guide the model in enhancing segmentation performance.\nNext, we removed the entire temporal prompt and did not use a text encoder. Consequently, the modality fusion changed to a self-attention mechanism. From the results, we observed a significant decrease of 5.36% in the mDice score on the LITS dataset. This demonstrates that the temporal prompt not only provides valuable temporal information but also that the selected organ and image type are beneficial for segmentation.\nWe also explored the effectiveness of the semantic alignment module. We performed modality fusion directly without semantic alignment beforehand. The results showed that the mDice score on the UW-Madison dataset decreased by 1.01%. This demonstrates that semantic alignment is essential for multimodal fusion, as it helps reduce the domain gap between different modality encoders. This improvement enhances the efficiency of multimodal fusion and the overall performance of the model.\nWe also investigated the performance of the modality fusion module. We replaced the modality fusion with a direct concatenation of the inputs before the final segmentation decoder. The results indicated that the modality fusion module is indispensable for the TP-UNet framework. The proposed modality fusion method in this paper demonstrates both efficiency and superior performance.\nThrough the above four sets of experiments, we can identify the effectiveness of the Temporal Prompt, the necessity of the semantic alignment module, and the efficiency of the modality fusion module. These three components are indispensable for TP-UNet and are crucial for its superior performance."}, {"title": "V. CONCLUSION", "content": "In this paper, we propose a temporal prompt guiding framework for medical image segmentation, which guides the learning of a segmentation model to the inherent temporal information of scanned images through straightforward temporal prompts. In addition, we further propose a two-stage process including semantic alignment and modality fusion to aggregate temporal prompts textual representation and image representations via multimodal contrast learning and cross-attention mechanisms. Our proposed framework also validates the necessity of temporal information in medical image segmentation tasks through promising results. In future work, we will extend the proposed framework to more complex scenarios."}]}