{"title": "Development and Validation of Heparin Dosing Policies Using an Offline Reinforcement Learning Algorithm", "authors": ["Yooseok Lim", "Inbeom Park", "Sujee Lee"], "abstract": "Appropriate medication dosages in the intensive care unit (ICU) are critical for patient survival. Heparin, used to treat thrombosis and inhibit blood clotting in the ICU, requires careful administration due to its complexity and sensitivity to various factors, including patient clinical characteristics, underlying medical conditions, and potential drug interactions. Incorrect dosing can lead to severe complications such as strokes or excessive bleeding. To address these challenges, this study proposes a reinforcement learning (RL)-based personalized optimal heparin dosing policy that guides dosing decisions reliably within the therapeutic range based on individual patient conditions.\n\nA batch-constrained policy was implemented to minimize out-of-distribution errors in an offline RL environment and effectively integrate RL with existing clinician policies. The policy's effectiveness was evaluated using weighted importance sampling, an off-policy evaluation method, and the rela-tionship between state representations and Q-values was explored using t-SNE. Both quantitative and qualitative analyses were conducted using the Medical Information Mart for Intensive Care III (MIMIC-III) database, demonstrating the efficacy of the proposed RL-based medication policy. Leveraging advanced machine learning techniques and extensive clinical data, this research enhances heparin administration practices and establishes a precedent for the development of sophisticated decision-support tools in medicine.", "sections": [{"title": "1 Introduction", "content": "Thrombosis, characterized by blood clot accu-mulation in a vessel, disrupts circulation due to blockages. The symptoms of this condition can vary depending on the location and size of the clot, including swelling, pain, and shortness of breath. Untreated thrombosis can lead to blockage of the pulmonary artery, which can cause severe symptoms such as chest pain and difficulty in breathing, potentially resulting in death. Treating such clots typically involves administering anti-coagulants such as heparin to prevent further clotting or thrombolytic agents to dissolve exist-ing clots. However, anticoagulants are susceptible drugs whose effects are not immediately measur-able; therefore, continuous monitoring is essential for guidance of an appropriate therapeutic range. Inappropriate dosing can have significant conse-quences, including strokes and excessive bleeding, with about 7,000 related deaths annually in the United States alone [27].\n\nTraditionally, clinicians determine heparin doses using basic protocols, personal experience, and ongoing patient monitoring, methods that may not adequately account for individual patient variability [21]. This complexity necessitates reli-able therapeutic tools to aid physicians in mak-ing precise dosing decisions. To this end, this study introduces a reinforcement learning (RL)-based, personalized heparin dosing policy aimed at optimally guiding therapy within the therapeutic range according to individual patient conditions. Heparin administration is framed as a Markov decision process (MDP), where clinicians assess the patient's condition and blood clotting status, administer medication, and adjust based on the activated partial thromboplastin time (aPTT) test results. This test, which simulates blood clotting, is crucial for monitoring and adjusting treatment until the desired clotting efficacy is achieved.\n\nPrevious approaches to heparin dosing include the use of partially observable MDPs (POMDPs), leveraging models like the hidden Markov model (HMM) and Q-learning to develop personalized protocols [2, 25]. However, typical off-policy deep reinforcement learning (DRL) algorithms strug-gle with extrapolation errors when training data deviates from actual distributions, often overesti-mating the value of new state-action pairs [8]. This overestimation can lead to significant learning challenges in offline settings where direct inter-action with the environment is not feasible. To address these limitations, we employed a batch-constrained deep Q-learning (BCQ) [7] algorithm designed to minimize extrapolation errors and improve the accuracy of Q-value estimations.\n\nPrevious studies have typically evaluated RL policies using trajectory comparisons and statisti-cal tests [2, 8, 21, 25], which may not accurately determine if the RL has optimized rewards effec-tively, particularly in dynamic decision-making scenarios. In offline RL, where direct environmen-tal interaction is absent, traditional evaluation methods struggle to assess the true efficacy of a policy, as they cannot simulate real-time feed-back or explore the effects of policy adjustments. To address these challenges, this study employs weighted importance sampling (WIS) [22] for pol-icy evaluation, an off-policy method crucial for assessing the agent's learning effectiveness in the absence of direct environmental interaction.\n\nFurthermore, to enhance our understanding of the learned RL policies, this study utilizes t-distributed Stochastic Neighbor Embedding (t-SNE), a dimensionality reduction technique that excels in visualizing high-dimensional data. By applying t-SNE, we can visually inspect the clus-tering of state representations, providing insights into the agent's decision-making processes. This visualization helps demonstrate how different patient states are managed under the RL policy, offering a clear view of the policy's effectiveness and its alignment with clinical outcomes.\n\nIn summary, this study leverages a batch-constrained approach to reduce extrapolation errors in off-policy algorithms, employs the WIS technique for quantitative performance evaluation of the agent, and utilizes t-SNE to verify if the RL"}, {"title": "2 Related Work", "content": "Healthcare studies aiming to support clinician decision-making by introducing artificial intelli-gence systems have been conducted in various fields such as disease detection [4, 30], health man-agement [12, 14], and drug administration [28, 34]. In particular, studies using supervised learning and RL have been proposed for optimizing hep-arin drug administration, aiming to control the degree of blood clotting in critically ill patients to maintain a normal state. Ghassemi et al. [9] pro-posed an optimal initial-dosing prediction model using logistic regression. The model demonstrated that AI tools and electronic medical record (EMR) data can be utilized to quickly bring blood coag-ulation levels in critically ill patients within the therapeutic range. However, the decision support tools in these approaches only consider the static state of the moment to suggest the initial dose and do not consider the continuous nature of the patient care process, which requires ongoing care. Most medical practices, not only medication administration, are continuous processes in which patients interact with physicians to achieve opti-mal health. Consequently, these approaches have fundamental limitations.\n\nRL solves continuous decision-making prob-lems and is ideally suitable for heparin-dosing problems. Nemati et al. [25] proposed an RL algorithm to determine the optimal dose of unfrac-tionated heparin (UH) for a patient and to reduce the risk of health deterioration owing to medica-tion errors. It defines the heparin administration process as a problem with a continuous state and discrete actions considering a partially observable Markov decision process (POMDP), and combines a discriminative hidden Markov model (DHMM) and a Q-learning model to solve drug adminis-tration problems. Baucum et al. [2] proposed a method for optimizing the heparin dosing problem from a model-based RL perspective, which com-bines a supervised learning-based model with an RL algorithm to guide an agent to learn reliably. The aforementioned study utilized a transition variational autoencoder (tVAE) as an environ-mental model that inputs the patient state and clinician action to generate the subsequent state. As an RL algorithm, an asynchronous advantage actor-critic (A3C) was used, which has both a policy and value network, to optimize the hep-arin dosing policy. This system maximizes the effectiveness of RL by building and utilizing an environmental model rather than directly using the given data for learning."}, {"title": "2.2 Offline RL", "content": "Deep RL has recently demonstrated promising results in specific fields, such as games [1, 24, 33] and robot control [16]. However, unlike the super-vised learning methods that have been utilized in various fields based on large amounts of data, there are several conditions stemming from the MDP and Bellman Equation for applying RL. First, the problem must be defined as a sequential process that satisfies the MDP over time. Sec-ond, an active interaction with the environment is essential to evaluate and improve the performance of the policy during the learning process. The second constraint is a major factor that compli-cates the application of RL to real-world decision-making problems. Interacting with the real world (driving a car or treating a patient) is costly and risky. Offline RL [7, 8, 19, 20] is a methodology designed to overcome these issues and extend the generality of RL; it learns an optimal policy using only previously collected data without interact-ing with the environment. This approach is widely used in robotics [6, 15, 23], autonomous driving"}, {"title": "2.3 Off Policy Evaluation", "content": "In RL, the cumulative reward of a policy is typically estimated to assess its performance. In environments where interaction is possible, the learned policy can directly measure returns through such interactions. However, in offline RL, where no interaction with the environment is pos-sible, alternative evaluation methods are required. Importance sampling serves as a robust tech-nique for evaluating a policy using trajectories collected from a different policy. Na\u00efve impor-tance sampling allows for the computation of unbiased estimators from batch trajectories but suffers from high variance as trajectory length increases due to the product of importance-sampling weights. Weighted importance sampling"}, {"title": "3 Materials and Methods", "content": "presents a high-level diagram of the pro-posed approach. In clinical settings, heparin dos-ing protocols are defined based on the body weight, and the effect of treatment is measured using the aPTT laboratory test [9], which assesses the blood clotting ability. Since the effects of treatment are not immediate, the aPTT test is typically performed 4 to 6 hours after administer-ing heparin. Based on the test results and other clinical indicators, clinicians adjust the heparin dose until clotting levels normalize.\n\nThe proposed approach uses RL to formu-late an optimal dosing strategy that maintains patient blood coagulation levels within a ther-apeutic range. For training the RL model, we utilize a trajectory \\(T\\) comprising data from \\(N\\) patients. Each patient's data includes laboratory test results \\(s\\), clinician-administered actions \\(a\\), and rewards \\(r\\) defined by the aPTT levels.\n\n\\(T= [\\{(s^{(1)},a^{(1)},r^{(1)}),..., (s^{(1)}_{t_1}, a^{(1)}_{t_1}, r^{(1)}_{t_1})\\}, ..., \\{(s^{(N)}_{1}, a^{(N)}_{1}, r^{(N)}_{1}),..., (s^{(N)}_{i,t_N}, a^{(N)}_{i,t_N}, r^{(N)}_{i,t_N})\\}]\\)\n(1)\n\nEach trajectory varies in length, correspond-ing to the specific duration of treatment for each patient. The state \\(s^{(n)}_{i,t_n}\\) represents i-dimensional laboratory values for the nth patient at time tn, while \\(a^{(n)}_{i,t_n}\\) and \\(r^{(n)}_{i,t_n}\\) include 1-dimensional actions and rewards, respectively. Missing values in \\(r^{(\u03b7)}\\) are possible due to the timing of aPTT measure-ments post-heparin administration. The goal is to learn an optimal RL policy \\(\\pi^*\\) that maximizes the cumulative reward based on the trajectories \\(\\tau\\)."}, {"title": "3.1 Dataset", "content": "MIMIC-III data [13] was utilized, which is a de-identified health-related public database of more than 40,000 patients admitted to the Beth Israel Deaconess Medical Center ICU between 2001 and 2012. It contains various types of information regarding patient care, such as the demograph-ics, laboratory test results, procedures, vital sign measurements, and caregiver notes.\n\nTo obtain the data of patients receiving intra-venous heparin in the ICU, we extracted approxi-mately 21 laboratory test results, vital signs, and heparin dosing results related to the health and anticoagulation of patients aged > 18 years. Since the MIMIC-III database compiled this informa-tion from two distinct sources, we standardized the measurement units to 'units' for heparin doses, aligning disparate records that originally included 'units', 'ml', and 'units/ml'. We then collected data from each patient ranging from a minimum of 7 to a maximum of 72 hours following the administration of the initial heparin dose.\n\nGiven that RL typically requires data with a sequential flow over fixed time intervals, we resam-pled the data hourly. During this resampling, we averaged multiple occurrences within the same hour. However, as laboratory tests and vital sign checks in hospitals are performed based on clinical need rather than at fixed intervals, this resam-pling increased the incidence of missing values. We addressed this by calculating the missing rates for each feature and excluding those with high miss-ing rates. Remaining missing values, post-outlier removal, were imputed using the Sample-and-Hold and k-nearest neighbor (KNN) methods [25].\n\nFor the RL model, we utilized the following variables: age, gender, Glasgow coma score (GCS), diastolic and systolic blood pressure (DBP and SBP), respiratory rate (RR), hemoglobin (HGB), temperature, white blood cell count (WBC), platelet count, activated partial thromboplastin time (aPTT), prothrombin time (PT), arterial carbon dioxide (ACD), creatinine, bilirubin, inter-national normalized ratio of prothrombin (INR), and weight. We applied z-score normalization to ensure stable training. The dataset comprises 17 features, corresponding to 1,911 hospitalization records and 1,838 unique patients. Detailed statis-tics of the features are presented in Table 1 and the complete flow of data extraction, preprocess-ing, and normalization processes described above is illustrated in Fig. 2."}, {"title": "3.2 State Space and Action Space", "content": "The state space is defined by the characteristics that change in response to actions and rewards."}, {"title": "3.3 Reward Formula", "content": "A fundamental component of RL is the reward function, which motivates the agent to achieve the problem's objective. This function either rewards or penalizes the agent's behaviors, depending on their outcomes. An agent acts in state St to reach the next state St+1 and receives a reward r. In our study, the primary goal is to regulate the patient's blood clotting levels within a therapeutic range. The aPTT is a critical indicator of blood clot-ting ability and thus serves as the basis for our reward metric. Utilizing aPTT, Nemati et al. [25] proposed a reward function, which is defined in (2), that has been widely adopted in subsequent studies [2, 3]. This function allocates a reward of approximately 1 for aPTT values within the desired therapeutic range of 60-100 seconds, and a reward of approximately 1 for values outside this range, as illustrated in Fig. 4.\n\n\\(R_t = \\frac{2}{1+e^{-(aPTT-60)}}-\\frac{2}{1+ e^{-(aPTT-100)}}-1\\)\n(2)"}, {"title": "3.4 Batch-Constrained Policy", "content": "To enhance learning efficiency in an offline envi-ronment and prevent Q-function overestimation while incorporating clinician drug administration data, we employed the BCQ algorithm, which combines Double DQN and \\(G_w\\). The objective of the Double DQN is to maximize the expected return, and the state and action value functions are given by (3) and (4) where e denotes the tar-get network, 0 refers to the current network, r is the reward, and \\(\\gamma\\) is the decay rate. The value of the next state has a recursive relationship with that of the current state.\n\n\\(V(s) = E[r + \\gamma V(s')]\\)\n(3)\n\n\\(Q(s, a) = E[r + \\gamma Q_{\\theta'}(s', argmax_{a'} Q_{\\theta}(s', a'))]\\)\n(4)\n\nWe used \\(G_w\\) to leverage the clinician dosing knowledge for RL. The value network is trained to estimate the optimal value, which depends on the reward defined using the aPTT. However, in real-world medical environments, clinician decisions do not always positively correlate with aPTT results due to various circumstances. To ensure safety beyond merely maximizing rewards, we incorpo-rated clinician dosing policies into the learning process to develop safe patient treatment strate-gies. \\(G_w\\) helps the RL agent learn a policy that maximizes the reward within a range of actions that satisfy the clinician dosing knowledge. The loss is expressed by (5).\n\n\\(L(w) = \\sum_{i=1}^m y_i log(\\hat{y_i}), \\hat{y_i} \\sim G_w(s), y_i \\sim \\pi_{\\theta}\\)\n(5)\n\nMethods that reflect the clinician dosing knowledge in the policy also have advantages in terms of value-learning. In an offline RL environ-ment, a typical off-policy algorithm may overes-timate a previously unseen (s, a, r, s') tuple while learning the Q-value. Consequently, overestima-tion hinders the estimation of accurate Q-values and causes the model to fail to learn. In BCQ, \\(G_w\\) limits the computation of Q-functions for state-action pairs that the RL agent did not observe during learning, thereby reducing the likelihood of overestimation and enabling stable learning."}, {"title": "3.5 Off-Policy Evaluation with WIS", "content": "WIS was employed to quantitatively assess the policy. The evaluation uses importance sampling (IS) with data derived from behavioral policies to estimate the expected value of the target policy. It computes the importance ratio for each state-action pair in the dataset, which is then used to estimate the expected return of the target policy. The importance ratio is defined as the probabil-ity of a sample occurring in the target distribution divided by its probability in the behavioral distri-bution. The IS calculation is detailed in (8) and involves adjusting for the variance inherent in this method.\n\n\\(J(\\pi_0) = E_{\\tau \\sim \\pi_0(\\tau)} [\\frac{\\pi^*{\\tau}}{\\pi_0({\\tau})} \\sum_{t=0}^H r(s,a)]\\)\n\n\\(= \\frac{E_{\\tau \\sim \\pi_0(\\tau)} [\\prod_{t=0}^H \\frac{\\pi^*({a_t}|s_t)}{\\pi_0({a_t}|s_t)} \\sum_{t=0}^H r(s,a)]}{\\sum_t \\pi_0(r)} \\)\n\n\\(\\approx \\frac{1}{n} \\sum_{i=1}^n \\psi\\frac{\\sum_{t=0}^H r(s,a)}{\\sum_{t=0}^H \\frac{\\pi^*{\\tau}}{\\pi_0({\\tau})}} \\)\n(8)\n\nwhere H is the time horizon of the sample, \\(w = \\prod_{t'=0}^H \\frac{\\pi^*({a_t}|s_t)}{\\pi_0({a_t}|s_t)}\\), and \\(\\{s_0, a_0, r_0, s_1,...\\}_{i=1}^n\\) is n trajectory samples of \\(\\pi_0(\\tau)\\).\n\nWhile IS provides a straightforward way to estimate policy values, its high variance can make accurate estimations challenging. To address this, WIS was utilized as described in (9), providing a more stable estimate by self-normalizing the importance ratios.\n\n\\(J(\\pi_0) \\approx \\frac{1}{\\sum_{i=1}^n \\psi} \\sum_{i=1}^n \\psi\\frac{\\sum_{t=0}^H r(s,a)}{\\sum_{t=0}^H \\frac{\\pi^*{\\tau}}{\\pi_0({\\tau})}} \\)\n(9)"}, {"title": "4 Experimental Results", "content": "Unless specified otherwise, all networks share the same architecture and hyper-parameters. The architectures of both the Q-network and the clin-ician network are illustrated in Fig. 1.\n\nThe state inputs are processed through a three-layer linear network. The input layer has a dimen-sion of 16, each linear layer contains 256 nodes, and the output dimension is 6. The Q-network outputs the Q-values for each action, with ReLU activation functions applied after each layer. In the BCQ methods, the linear layer is common to both the Q-network and the clinician network, but the clinician network also includes an additional two linear layers, each with 128 nodes. The final output of the clinician network passes through a softmax activation function to determine the prob-abilities for each action. The hyper-parameters used across all algorithms are uniform and detailed in Table 2."}, {"title": "4.2 Policy Evaluation", "content": "To assess the effectiveness of RL-based treat-ment policies, we used the MIMIC-III dataset. We conducted policy learning using a state space com-prising 16 features and a discrete action space with six categories. The dataset was split into an 80% training set and a 20% test set. Performance evalu-ation was based on the expected return calculated using Weighted Importance Sampling (WIS).\n\nThrough analysis of actual ICU patient care data, we successfully trained the heparin dosing policy. The policy's development over time, indi-cated by WIS and average predicted Q-values, is depicted in Fig. 5. The training involved 100,000 episodes utilizing an experience replay buffer con-taining the entire set of patient trajectories. The performance was assessed every 200 episodes, with each dot on the graph representing one evaluation. The graphs display the average off-policy evalu-ation performance per episode and the average predicted Q-values; the left side shows results from the training set, and the right side shows results from the test set. Over the course of training, both WIS and action values progressively increased, reflecting a positive reinforcement of the policy according to the defined reward function.\n\nFurthermore, to assess our approach's effec-tiveness, we compared the BCQ with several value-based DRL algorithms known for their effi-cacy across various RL domains. Specifically, we evaluated deep Q-learning (DQN), dueling deep Q-learning (Dueling DQN), and double deep Q-learning (Double DQN). We also examined the performance of these RL policies against existing clinical policies to determine if they offer improve-ments over standard clinician-driven approaches.\n\nFig. 6 presents the results, showing the esti-mated cumulative rewards for each policy. The experimental setup mirrors that of Fig. 5, with mean values and standard deviations visualized based on five random seeds. The left graph dis-plays results from the training set, while the right graph shows outcomes from the test set. Notably, the expected return of BCQ demonstrates sta-ble convergence within a specific range. Among the various algorithms, the policy trained using BCQ consistently outperforms others, exhibit-ing less variance and indicating that the Batch-Constrained approach contributes positively to consistent learning and enhanced performance in offline RL settings. Additionally, as the number of episodes increases, the RL-derived policy con-sistently surpasses the expected rewards of the clinician's policy, underscoring the effectiveness of the RL approach in improving heparin treatment."}, {"title": "4.3 State Representation With Maximum Expected Reward", "content": "We utilized t-SNE to visualize the state represen-tations under the RL medication policy. Fig. 7 shows the results of this analysis, where the BCQ agent's activity against a replay buffer was evalu-ated, and t-SNE was applied to the state features. Each point in the figure is colored based on the maximum expected reward predicted by BCQ for that particular patient state, with colors ranging from red (highest value) to blue (lowest value). Larger points indicate data from the test set, while smaller points represent the training set.\n\nTo qualitatively analyze the states based on their value, we examined the percentage of thera-peutic aPTT range for states classified into high, medium, and low-value regions. In the pie chart, 'Therapeutic' states indicate the proportion of points where the aPTT values are within the opti-mal range of 60 to 100, and 'Untherapeutic' states represent the proportions outside this range.\n\nThe analysis revealed that areas assigned high values (top right of the figure) contain a sig-nificant number of patient states corresponding to the 'Therapeutic' state, suggesting that these states are either achieving immediate rewards or have already reached the optimal state. Con-versely, regions assigned low values (middle of the figure) show a lower proportion of 'Therapeutic' states, indicating that these patients are far from the desired stable state, thus far from achieving rewards. Medium value regions (top left) show intermediate proportions, aligning with expec-tations. The t-SNE algorithm effectively groups similar BCQ state representations close together, demonstrating that the policy learned by BCQ aligns well with the objectives defined by the reward function."}, {"title": "4.4 Action Distribution", "content": "To gain a deeper understanding of the RL policy, we analyzed and compared the actions taken by the clinician policy and the RL policy. Fig. 8 dis-plays the aggregate results of all actions selected by each policy at every timestep within the test dataset. The number of actions indicates how fre-quently a specific heparin dose is administered. From the analysis, it is apparent that, in com-parison to the clinician's policy, the AI's policy tends to administer higher doses of heparin more frequently. While the appropriate dosage is contin-gent upon the patient's condition, the AI policies developed in this study demonstrate a propen-sity for significant variability, particularly from the perspective of aPTT management."}, {"title": "5 Conclusion", "content": "We adopted a batch-constrained approach to effec-tively learn RL policies using EMR data. As evidenced by Table 3 and Fig. 6, the performance of traditional deep reinforcement learning (DRL) algorithms in offline settings is often limited. By integrating an expert behavior network, we miti-gated the overestimation of Q-values and achieved a balance between the behavioral policy and opti-mal policy estimation using the Bellman equation. The BCQ algorithm demonstrated superior per-formance over existing DRL approaches.\n\nAdditionally, we employed t-SNE to analyze the relationship between state representations and expected values, confirming that the policy accu-rately learns the objectives set by the reward function. High-value states were found to be closer to achieving the desired reward, whereas low-value states were distant. This fixed dataset training system without environmental interaction is particularly apt for the medical field, where simulations are subject to stringent constraints.\n\nDecision support systems powered by artificial intelligence show promise in thrombosis treat-ment. Nonetheless, understanding the directional intent of DRL policies is crucial for their prac-tical application. As shown in Fig. 8, the RL policy tends to favor a more aggressive strategy to maximize rewards. While there is no univer-sally correct approach to heparin dosing, it is vital for clinicians to comprehend the tendencies of DRL policies when applied in real-world scenar-ios. If needed, the policy's aggressiveness can be moderated through strategic constraints.\n\nThis study has limitations, including the use of discrete actions and reliance solely on acti-vated Partial Thromboplastin Time (aPTT) as the reward function. Given that drug dosing is typically a continuous variable, future improve-ments should include transforming the action space to accommodate more precise dose pre-dictions. Moreover, we aim to refine the reward function to better reflect induction into the ther-apeutic range, changes in the patient's organ function, and their clinical implications. Future research will focus on designing an enhanced RL framework to incorporate these aspects."}]}