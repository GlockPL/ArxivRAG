{"title": "FloNa: Floor Plan Guided Embodied Visual Navigation", "authors": ["Jiaxin Li", "Weiqi Huang", "Zan Wang", "Wei Liang", "Huijun Di", "Feng Liu"], "abstract": "Humans naturally rely on floor plans to navigate in unfamiliar environments, as they are readily available, reliable, and provide rich geometrical guidance. However, existing visual navigation settings overlook this valuable prior knowledge, leading to limited efficiency and accuracy. To eliminate this gap, we introduce a novel navigation task: Floor Plan Visual Navigation (FloNa), the first attempt to incorporate floor plan into embodied visual navigation. While the floor plan offers significant advantages, two key challenges emerge: (1) handling the spatial inconsistency between the floor plan and the actual scene layout for collision-free navigation, and (2) aligning observed images with the floor plan sketch despite their distinct modalities. To address these challenges, we propose FloDiff, a novel diffusion policy framework incorporating a localization module to facilitate alignment between the current observation and the floor plan. We further collect 20k navigation episodes across 117 scenes in the iGibson simulator to support the training and evaluation. Extensive experiments demonstrate the effectiveness and efficiency of our framework in unfamiliar scenes using floor plan knowledge. Project website: https://gauleejx.github.io/flona/.", "sections": [{"title": "Introduction", "content": "An essential task in embodied AI is enabling agents to navigate in diverse environments toward a goal, represented as points (Savva et al. 2019), images (Zhu et al. 2017; Shah et al. 2023), objects (Chaplot et al. 2020a), or language instructions (Anderson et al. 2018b; Wang et al. 2023b). Recently, researchers have increasingly leveraged easily accessible prior knowledge (Shah et al. 2021) to improve efficiency and accuracy. Floor plans, in particular, serve as a valuable and widely available source of such knowledge, offering high-level semantic and geometric information that can assist agents in localizing and navigating in unfamiliar spaces. Moreover, incorporating the floor plan benefits various applications such as emergency response, search and rescue, and pathfinding in dynamic public environments.\nPrevious studies have explored integrating floor plans to facilitate localization and navigation, often relying on multi-sensor fusion (Li et al. 2021b) or imposing constraints on the floor plan structure (Boniardi et al. 2019b), which limits their practical applicability. Inspired by the human ability to navigate efficiently in unfamiliar environments using minimal abstract information and visual cues, we aim to reduce both sensor dependence and structural constraints on the floor plan in navigation.\nIn this work, we introduce a novel navigation task, Floor Plan Visual Navigation (FloNa), where agents navigate within an environment using an abstract floor plan and a series of RGB observations, as depicted in Fig. 1. Despite the valuable prior knowledge provided by the floor plan, this task remains challenging for two primary reasons. First, spatial inconsistency exists due to the significant difference between the floor plan and the actual observed layout, primarily caused by furniture placement in the scenes. This inconsistency can lead to collisions during navigation. Second, observation misalignment arises because the floor plan offers abstract topological information, whereas the RGB observations capture the appearance of natural scenes from a specific viewpoint. Such misalignment results in incorrect localization of the current observation within the floor plan, thereby hindering the effectiveness of planning.\nTo address these challenges, we develop FloDiff, a novel diffusion policy framework that exploits strong action distribution modeling capability to learn to handle the spatial inconsistency from extensive demonstrations implicitly. FloDiff also integrates an explicit localization module to align the observation and floor plan, resulting in two variants based on how current agent pose is derived: (1) Naive-FloDiff, which learns to predict the pose during training, and (2) Loc-FloDiff, which directly uses either ground truth poses or predictions from a pre-trained model. Both variants employ a Transformer backbone to fuse the floor plan and current observation, followed by passing the fusion into a policy network that learns to denoise the action sequence.\nFor benchmarking, we collect a dataset comprising approximately 20k navigation episodes across 117 distinct scenes using the iGibson simulator (Li et al. 2021a). The dataset includes around 3.3M images captured with a 45-degree field of view. We split the scenes into 67 for training and 50 for testing to assess the model's generalization capability to unseen environments. Each scene comprises a floor plan, a traversability map, and sufficient navigation episodes. Each episode contains an A*-generated trajectory paired with corresponding RGB observations.\nExtensive experiments demonstrate the effectiveness and efficiency of our method in navigating within unseen environments using a floor plan. Compared to baseline methods, our method achieves a higher Success Rate (SR) and greater efficiency, as measured by Success Weighted by Path Length (SPL). Additionally, we comprehensively analyze FloDiff's different capabilities, including localization, collision avoidance, planning with diverse goals, and robustness. Real-world deployment on an Automated Guided Vehicle (AGV) without finetuning further highlights its robustness and generalization, proving its potential to handle unseen scenarios effectively in practical settings.\nOur contributions are summarized as follows:\n\u2022 We introduce FloNa, a novel task of navigating toward a goal using RGB observations and a floor plan, enriching the application scenarios in embodied visual navigation.\n\u2022 We propose a novel end-to-end diffusion policy-based framework, i.e., FloDiff, incorporating explicit agent localization to solve FloNa efficiently and effectively.\n\u2022 We conduct extensive experiments on our curated dataset and thoroughly analyze FloDiff's capabilities across various dimensions, demonstrating its superiority over baseline methods."}, {"title": "Related Work", "content": "Visual Navigation\nVisual navigation aims at enabling robots or autonomous agents to navigate through environments using visual information. The definition of navigation tasks has evolved alongside the advancements in the field. Anderson et al. (2018a) are the first to explicitly categorize three navigation tasks based on goal types: PointGoal, ObjectGoal, and AreaGoal. The PointGoal task requires the agent to navigate to a specific location, emphasizing the self-localization capabilities (Wijmans et al. 2019; Ramakrishnan et al. 2021; Zhao et al. 2021; Partsey et al. 2022; Tang et al. 2022). In contrast, the ObjectGoal and AreaGoal tasks involve commonsense knowledge of the environment, such as recognizing objects and understanding where they are typically located (Yadav et al. 2023; Ramakrishnan et al. 2022; Gadre et al. 2023). With advancements in image and natural language understanding, the field has shifted focus toward navigation tasks guided by high-level semantics. Tasks like image navigation (Zhu et al. 2017; Krantz et al. 2023; Yadav et al. 2023; Chaplot et al. 2020b) and language navigation (Anderson et al. 2018b; Song et al. 2023; Min et al. 2021; Wang et al. 2023a,b) have emerged, filling gaps in various real-world and natural application scenarios. Beyond vision and language, navigation using other modalities, such as sound, has also been explored (Chen et al. 2020; Gan et al. 2020; Chen et al. 2021). Despite the promising progress in prior studies, efficient and effective exploration remains a critical challenge in tasks involving unknown environments. In this work, we explore the potential of leveraging readily available prior information, i.e., floor plans, to mitigate the dependence on time-consuming exploration, further enhancing the effectiveness and efficiency of navigation.\nFloor Plan for Localization and Navigation\nFloor plans, as stable structural information, have been widely explored in localization and navigation tasks. One line of research approaches localization within a traditional optimization framework, leveraging floor plans in combination with various sensor signals such as LiDAR (Boniardi et al. 2019a, 2017; Li, Ang, and Rus 2020; Mendez et al."}, {"title": "Task Setting", "content": "Task Definition The goal of FloNa requires embodied agents to navigate from a starting position to a specific goal position in a 3D environment. During the navigation, the agent can only receive the ego-centric RGB image, the environment's floor plan image, and a goal position, represented as a red dot in Fig. 3. Specifically, given the floor plan of, the agent receives the observation image $o_t$ at each timestep t and produces action $a_t$ to reach the goal g. A resulting episode {$x_0, a_1, x_1, \\dots, a_T, x_T$}, where $x_i$ comprises the agent's location $p_i$ and orientation $r_i$, is considered successful if the last position $p_T$ is within a specified distance threshold $T_d$ from the goal and the number of collisions with the scene does not exceed a given threshold $T_C$. $x_0 = \\{p_0, r_0\\}$ is the starting pose and T is the total step.\nSimulator Setup Following Li et al. (2021a), we build the simulation environment upon the iGibson simulator and employ a Locobot as the embodied agent. The agent has a height of 0.85 meters and a base radius of 0.18 meters, and is equipped with an RGB camera with a resolution of 512\u00d7512 pixels. We define the action as $a_t := p_t - p_{t-1}$, $a_t \\in R^2$, representing continuous movement in the 2D plane."}, {"title": "Dataset Design", "content": "To facilitate the benchmarking, we collect a large-scale dataset comprising 20, 214 navigation episodes across 117 static indoor scenes from Gibson (Xia et al. 2018), resulting in a total of 3, 312, 480 images captured with a 45-degree field of view. The Gibson scenes are reconstructed from the homes and offices with a Matterport device, which preserves the textures observed by the sensor, thus minimizing the sim-to-real gap. We split the dataset into training and testing sets, which comprise 67 scenes and 50 scenes, respectively.\nFor each scene, we provide a floor plan and a navigable map. We directly adopt the manually-annotated floor plans from Chen et al. (2024). For navigable maps, we employ a coarse-to-fine approach to obtain them by executing the following two steps. First, we generate a rough one from the scene's mesh. Then, we manually refine it to address scanning artifacts; for example, the poorly reconstructed chair legs may lead to a navigable area. Based on scene size, we"}, {"title": "Preliminary", "content": "Diffusion Model\nThe diffusion model (Ho, Jain, and Abbeel 2020; Sohl-Dickstein et al. 2015) is a probabilistic framework used in generative tasks. It learns to gradually denoise the sampled Gaussian noise to generate desired data through an iterative denoising process. Specifically, beginning with sampled noise $x_k$, the model performs K iterations of denoising to produce a series of intermediate poses $x_{k-1}, \\dots, x_0$. The process can be denoted as:\n$x_{k-1} = \\alpha(x_k - \\gamma \\epsilon_\\theta(x_k, k) + \\sigma z),$ (1)\nwhere $\\alpha, \\gamma, \\sigma$ are noise schedule functions, $\\epsilon_\\theta$ is the noise prediction network with parameters $\\theta$ and $z \\sim N(0, I)$.\nDiffusion Policy\nDiffusion policy (Chi et al. 2023) extends the concept of diffusion models, which are typically used for content generation, to policy learning, allowing an agent to make decisions based on learned probabilistic distributions. The policy usually incorporates an encoder to transform the observation into a lower-dimensional feature, serving as the condition for the denoising process. Eq. (1) can be adapted to:\n$A_{k-1} = \\alpha(A_k - \\gamma \\epsilon_\\theta(A_k, O_t, k) + \\sigma z),$ (2)\nwhere $A_t$ and $O_t$ denote the action and the observation at each time step t, respectively."}, {"title": "Method", "content": "Building upon diffusion policy, we propose a novel end-to-end framework, i.e., FloDiff, for solving FloNa. The overall architecture is illustrated in Fig. 3. Next, we will detail the FloDiff framework and the training process.\nDiffusion Policy for FloNa\nFramework Overview FloDiff consists of a transformer-based backbone that fuses visual observations with the floor plan and a policy network designed to navigate within the scene. As discussed, an ideal agent must be capable of (1) navigating to the goal without colliding with the scene and (2) aligning visual observations with the floor plan. To address the first challenge, we employ the powerful diffusion policy to learn navigation in crowded scenes from extensive demonstrations. For the second challenge, we explicitly predict the agent pose from observations or utilize a pre-trained model for pose prediction.\nObservation Context In our work, the visual observation $O_t$ contains the observed images $\\{o_{t-l}, o_{t-l+1}, \\dots, o_t\\}$, where $l$ denotes the context length, and the floor plan image $o_f$. Inspired by a wealth of prior work on training high-capacity policies (Sridhar et al. 2023; Shah et al. 2023), we employ a transformer-based backbone to process the observation context. Specifically, for each observed image $o_i$, $i = t - l, \\dots, t$, we employ a shared EfficientNet-B0 (Tan and Le 2019) as the encoder, denoted as $\\phi$, to produce the visual latent features independently. Following Visual Navigation Transformer (ViNT) (Shah et al. 2023) that encodes the relative difference between the current observation and the goal image, we employ another EfficientNet-B0 as encoder $\\psi$ to process the floor plan image. We train both $\\phi$ and $\\psi$ from scratch. Subsequently, we apply multi-headed attention (Vaswani et al. 2017) layers $f(\\cdot)$ to fuse the two branch features. The final output feature serves as the observation context vector $C_t$.\nGoal Position We compute goal position $p_g$ in the world coordinate according to the marked goal point g. Precisely, we first determine the pixel coordinates of the goal on the floor plan image, denoted as $u_g$. We then convert the coordinates into the world coordinates using the floor plan resolution $u$ defined as the distance per pixel and the offset $\\delta$ provided by Chen et al. (2024) through $p_g = \\frac{g}{u} + \\delta$.\nPolicy Based on different ways of obtaining the current agent pose, we propose two variants of FloDiff.\nFirst, as described above, $f(\\cdot)$ fuses the observed images with the floor plan, indicating that $C_t$ encodes valuable information for predicting the agent poses. To achieve this, we introduce a fully connected network $f_p$ to predict the agent pose $x_t = (p_t, r_t)$ from the observation context vector $C_t$. Following Shah et al. (2023), we also use an additional fully connected network $f_a$ to predict the shortest path distance $d(t, g)$ between the current position and the goal, as illustrated in Fig. 3. This design helps mitigate the policy's tendency to move directly toward the goal, which could otherwise lead to dead ends. Finally, we concatenate the observation context vector $C_t$, goal position $p_g$, and predicted agent pose $x_t$ as a conditional vector, from which the diffusion policy $\\theta$ learns to denoise the sampled noise and generate the action sequence. We denote this variant as Naive-FloDiff.\nSecond, we can directly use the ground truth agent pose or predicted pose from a pre-trained model, i.e., F3Loc (Chen et al. 2024). Unlike Naive-FloDiff, this design eliminates the need for $f_p$. We denote such FloDiff variants as Loc-FloDiff, including Loc-FloDiff (GT) and Loc-FloDiff (F\u00b3).\nFor all FloDiff variants, we model the predicted action sequence $A_t$ at time step t as the future action sequence with a horizon of $H_p$, where the agent executes the first $H_a$ steps. During inference, we sample the predicted action sequence as described in Eq. (2) and compute the next position by"}, {"title": "Training", "content": "We train FloDiff on the training set, which consists of 67 indoor scenes, encompassing 11, 575 episodes and approximately 26 hours of trajectory data. During training, we randomly sample trajectory segments with a fixed horizon from the episodes as training samples. To increase the diversity of target goals, we also randomly select a target position behind the sampled trajectory rather than using the episode endpoint as the target. We train Naive-FloDiff in an end-to-end manner using the following loss functions:\n$\\mathcal{L}(\\phi, \\psi, f, \\theta, f_a, f_p) = MSE(\\epsilon_k, \\epsilon_\\theta(C_t, A_t + \\epsilon_k, k))$\n$+\\lambda_1 MSE(d(t, g), f_a(C_t, x_t, p_g))$ (3)\n$+\\lambda_2 MSE(x_t, f_p(C_t)).$\nSimilarly, we train Loc-FloDiff using:\n$\\mathcal{L}(\\phi, \\psi, f, \\theta, f_a) = MSE(\\epsilon_k, \\epsilon_\\theta(C_t, A_t + \\epsilon_k, k))$\n$+\\lambda_3 MSE(d(t, g), f_a(C_t, x_t, p_g)).$ (4)\n$\\lambda_1$, $\\lambda_2$, and $\\lambda_3$ are hyperparameters for weighting different loss terms. $MSE(\\cdot)$ computes mean square error.\nIn the implementation, FloDiff is trained for 5 epochs using AdamW (Loshchilov, Hutter et al. 2017) optimizer with a fixed learning rate of 0.0001. We empirically set $\\lambda_1 = 3 = 0.001$ and $\\lambda_2 = 0.005$. The attention layers are built using the native PyTorch implementation. The number of multi-head attention layers and heads are both 4. We set the dimension of the observation context vector $C_t$ to 256. The diffusion policy is trained using the Square Cosine Noise Scheduler (Nichol and Dhariwal 2021) with K = 10 denoising steps. The noise prediction network $\\epsilon_\\theta$ adopts a conditional U-Net architecture following (Janner et al. 2022) with 15 convolutional layers. We set the diffusion horizon as $H_p = 32$ and employ the first $H_a = 16$ steps to execute in each iteration. We train FloDiff using one NVIDIA RTX3090 GPU and assign a batch size of 256."}, {"title": "Experiments", "content": "This section begins with an introduction to the experimental setup, including the baseline methods and the evaluation metrics. We then present the results and the performance analysis regarding localization, collision avoidance, planning, and robustness, followed by a real-world deployment.\nExperimental Setting\nWe evaluate our proposed method on 50 test indoor scenes. 10 pairs of start and end points are randomly selected in each scene, totaling 500 test pairs. Tab. 1 presents statistics on the straight-line distance and travel distance between the start and end points of the 500 test episodes. Collisions may occur because the floor plan does not account for furniture obstacles. To mitigate this, we adopt a strategy in which the agent will rotate 45 degrees clockwise and re-predict future actions when a collision happens."}, {"title": "Baselines", "content": "In our evaluation, in addition to Naive-FloDiff and Loc-FloDiff, we further design a modular baseline including a localization module and an explicit path-planning module, denoted as Loc-A*. The localization modules in both Loc-FloDiff and Loc-A* are implemented in two distinct ways, i.e., using GT or F\u00b3Loc (Chen et al. 2024), comparing their effectiveness across different settings. The five methods are described as follows:\n\u2022 Loc-A* (F\u00b3): In this baseline, the agent localizes itself using pre-trained F3Loc (Chen et al. 2024). Subsequently, the A* algorithm is employed to plan a trajectory from the predicted position to the goal on the floor plan and execute the mapped actions in the environment.\n\u2022 Loc-A* (GT): Unlike Loc-A* (F3), this baseline replaces the predicted pose with the ground truth pose.\n\u2022 Naive-FloDiff: This method directly learns to predict the agent pose during policy training.\n\u2022 Loc-FloDiff (F\u00b3): This method is based on our Loc-FloDiff model and utilizes the same localization module as Loc-A* (F3).\n\u2022 Loc-FloDiff (GT): Unlike Loc-FloDiff (F3), the predicted poses are replaced by the ground truth poses.\nMetrics The performance of the above navigation methods is evaluated using two primary metrics, i.e.,"}, {"title": "Results and Discussions", "content": "Main Results We present the main comparison results in Tab. 2, computing the SR and SPL under varying $T_C$ and $T_d$. We observe that Loc-FloDiff (GT) consistently outperforms all other methods across varying conditions in both SR and SPL, indicating that the proposed diffusion policy-based method can effectively solve the FloNa. Below, we analyze the methods' performance across various dimensions, including localization, collision avoidance, planning, and robustness. We provide additional experimental results and qualitative visualization in supplementary material."}, {"title": "Localization Analysis", "content": "The results in Tab. 2 show that the success rate of Naive-FloDiff is lower than both Loc-FloDiff (GT) and Loc-FloDiff (F3). This suggests that the modular method of decoupling the localization module is more effective in tackling FloNa compared to the end-to-end method. It appears that the encoders face challenges in simultaneously encoding information for both localization and planning. Notably, Loc-A* (F3) and Loc-FloDiff (F3) perform worse than their ground truth counterparts. Upon reviewing the planned trajectories, we observe that agents using F\u00b3Loc often become stuck due to collisions with obstacles in confined areas. This issue is primarily caused by inaccurate localization, which results in unreasonable path planning. We suppose this unsatisfactory localization stems from using F\u00b3Loc, which is pre-trained on data collected from an agent with a height of 1.70m, significantly different from our agent with 0.85m height. We also find that, even under similar unsatisfactory localization conditions, the success rate of LocFloDiff (F\u00b3) does not degrade as severely as that of Loc-A*. Furthermore, Loc-FloDiff (F3) even performs better than Loc-A* (GT) when $T_C = \\infty$ and $T_d = 0.25m$ or $T_d = 0.3m$. This result suggests that while Loc-FloDiff is affected by localization accuracy, it demonstrates a certain robustness to noisy input, allowing it to perform reasonably even under less accurate localization."}, {"title": "Collision Avoidance", "content": "To quantitatively evaluate the collision avoidance performance, we present the mean collision counts of successful cases with $\\tau_d = 0.3m$ under varying $T_c$ in Tab. 3. The results demonstrate that Loc-FloDiff (GT) outperforms Loc-A* (GT), and Loc-FloDiff (F3) also results in fewer collisions than Loc-A* (F3). These findings suggest"}, {"title": "Planning", "content": "The SR and SPL results in Tab. 2 demonstrate that Loc-FloDiff (GT) achieves superior performance, highlighting its effectiveness and efficiency in path planning."}, {"title": "Robustness", "content": "As previously mentioned, Fig. 4 qualitatively demonstrates that our method can robustly predict collision-free actions most of the time, even with noisy position inputs. To further evaluate the robustness of our method quantitatively, we conduct three experiments in which the Loc-FloDiff (GT) agent navigates to the goal with noisy pose inputs. In each experiment, the agent receives poses with Gaussian noise at a specific variance, i.e., var = 0.1m, var = 0.3m, and var = 0.5m. All three experiments are evaluated with 50 cases, with one case per scene from the test set. The results, including SR and SPL metrics under the conditions $T_e = 50$ and $\\tau_d = 0.3m$, are presented in Tab. 4. Although adding noise to the input pose leads to a decline in both SR and SPL, the performance does not degrade continuously as the noise variance increases. This stability highlights the model's capability of handling uncertainties in localization while maintaining navigation performance."}, {"title": "Real-world Deployment", "content": "To validate the effectiveness of the model, we deploy our policy on an Automated Guided Vehicle (AGV) called \u201cUP\u201d, developed by Yunji Technology. We observe that the agent surprisingly completes the navigation tasks in an unseen real-world environment even without finetuning, demonstrating its robustness and generalization capabilities. We recommend referring to our project website for the demonstration video of the planning results.\nWe conduct the experiment in an unseen apartment with an area of about 108m\u00b2. Fig. 6a and Fig. 6b show the scene layout and corresponding floor plan. Fig. 6c shows the appearance of robot \u201cUP\u201d. The agent stands 0.36 meters tall, with a rectangular base measuring 0.46 meters in length and 0.42 meters in width. We additionally mounted an RGB camera on it and adjusted the camera height to 0.8m. The apartment is pre-scanned, and we evaluate our Loc-FloDiff (GT) in the scanned scene. The model generates actions based on RGB observations in simulation, which are then applied to \"UP\" and converted into linear and angular velocities using PD control in the real world. Our model achieves an inference rate of approximately 1.88Hz when running on an NVIDIA Jetson AGX Orin. To provide the current pose for the agent, we use a single-line LiDAR-based odometry algorithm. Before the testing, we manually annotate the agent's starting pose, represented as $T_0$, in the scene coordinate system. During navigation, the LiDAR odometry continuously calculates the agent's pose relative to $T_0$ and converts it to the scene coordinate system. At time t, the agent pose $T_t$ is calculated as $T_t = T_0 \\circ T_{t-0}$, where $T_{t-0}$ represents the relative pose from t to 0 and represents the coordinate transformation operation."}, {"title": "Conclusion", "content": "In summary, this work introduces FloNa, the first to incorporate floor plans into embodied visual navigation. To tackle the FloNa, we develop FloDiff, an efficient and effective diffusion policy framework integrating an explicit localization module, and curate a dataset for benchmarking. The superior performance of FloDiff over comparison baselines highlights the promising potential of our approach. By presenting this practical setting and solution, we aim to inspire further research in the visual navigation community and contribute to advancements in the field."}, {"title": "FloNa: Floor Plan Guided Embodied Visual Navigation (Supplementary Material)", "content": "Distinctions from PointNav\nThe distinctions between FloNa and PointNav (Anderson et al. 2018a) lie mainly in two aspects. First, PointNav typically involves time-consuming exploration of the environment, either explicitly or implicitly, due to the lack of global information. In contrast, FloNa leverages the geometric information embedded in floor plans to enable more efficient navigation. Secondly, PointNav defines the goal in relative coordinates, whereas FloNa specifies the goal in the global coordinate system without including information about the relative position between the agent and the goal, requiring the model to incorporate the localization capability.\nDataset Details\nWe collect the dataset in 117 static indoor scenes, which are split into 67 for training and 50 for testing. Each scene is accompanied by a floor plan, a navigable map, and multiple episodes of data. Depending on the scene size, we collected 150 episodes in small scenes (with areas less than 20 m\u00b2), 180 in medium scenes (areas between 20 and 80 m\u00b2), and 200 in large scenes (areas greater than 80 m\u00b2). Each episode includes a trajectory and the observations captured throughout. In total, the dataset comprises 20,214 navigation episodes, resulting in 3,312,480 RGB images. Fig. Al shows three examples of our scenes as well as corresponding floor plans and navigable maps.\nNavigable Maps Scene and corresponding floor plans are provided by Gibson (Xia et al. 2018) and Chen et al. (2024), respectively. For the navigable maps, we employ a two-stage generation process. In the first stage, we generate a rough navigable map by selecting meshes with heights between 0.15 m and 1.70 m. These meshes are treated as obstacles and marked in black, while the remaining non-obstacle areas are marked in white. However, due to imperfect reconstruction (e.g., incomplete chair leg reconstruction or incorrect floor reconstruction), some unnavigable areas may be mistakenly marked as navigable, and vice versa. Therefore, in the second stage, we manually refine the rough map to get a fine navigable map.\nEpisodes Sampling For each episode, we first dilate the unnavigable areas to approximate the radius of the robots, ensuring collision-free travel. Using this dilated map, we randomly select two points at least 3 meters apart within the navigable area as the starting and target points. We then use the A* algorithm to generate the shortest path, represented as a sequence of pixel coordinates. These pixel coordinates are then converted into coordinates in the scene coordinate system (in meters) to form the trajectory data. To ensure the smoothness of the trajectory, the orientation at each position is determined by the location of the sixth future point, i.e., $r_i = p_{i+6} - p_i$, where $r_i$ and $p_i$ denote the orientation and"}]}