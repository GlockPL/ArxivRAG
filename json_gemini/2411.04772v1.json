{"title": "Attention Masks Help Adversarial Attacks to Bypass Safety Detectors", "authors": ["Yunfan Shi"], "abstract": "Despite recent research advancements in adversarial attack methods, current approaches against XAI monitors are still discoverable and slower. In this paper, we present an adaptive framework for attention mask generation to enable stealthy, explainable and efficient PGD image classification adversarial attack under XAI monitors. Specifically, we utilize mutation XAI mixture and multitask self-supervised X-UNet for attention mask generation to guide PGD attack. Experiments on MNIST (MLP), CIFAR-10 (AlexNet) have shown that our system can outperform benchmark PGD, Sparsefool and SOTA SINIFGSM in balancing among stealth, efficiency and explainability which is crucial for effectively fooling SOTA defense protected classifiers.", "sections": [{"title": "1. INTRODUCTION", "content": "With the advancement of deep learning models for various tasks such as classification [1] and segmentation [2], they are increasingly deployed to numerous use cases [3] for better performance and autonomy. This brings to the issue of adversarial robustness [4] of all these autonomous systems with models learnt from large datasets that may be biased where some potential behaviours are difficult to discover and yet fatal to safety and reliability. Once malicious inputs are provided as input, the functionality can be problematic.\nRecent advances in XAI [5] significantly help adversarial defense and monitoring which makes it necessary for novel adversarial attack that can fool XAI algorithms with similar outputs and related safety monitors which essentially compare both outputs via different mechanisms.\nIn practice, additional adversarial safety monitors [6] especially XAI based [15][17] is added to address the above issue without making large costly modifications to the already deployed model. For real time systems, attack efficiency is also crucial.\nOur contributions are as follows:\n1. Efficient explainable attention mask guided PGD: 17%faster and 0.01% less effective with 14% more stealth\n2. Better XAI adversarial explanation without discriminator training using deconvolution layers under selfsupervision."}, {"title": "1.1. Motivation", "content": "Current attack methods including PGD is still not stealthy enough to pass XAI monitors, not explainable enough and not efficient enough to fool target classifiers. We propose both XAI mixture mutation pgd attention mask generation algorithm without auxiliary model training and deep learning based X-UNet for attention mask generation to guide PGD for the discovery of query-efficient adversarial examples stealthy enough to pass XAI monitors and pixelwise saliency of such attacks under whitebox setting. We focus on attention mask generation via our X-U-Net with novel multi- task self-supervised loss function, activation function and weight initialization."}, {"title": "2. RELATED WORK", "content": "Methods can be generally divided into reactive and proactive. reactive methods include adversarial detecting [7] and abstraction based [8] while proactive methods updates the model via retraining [9] or distillation [10]. These are reported to be empirical based defense, there are also Certified denfense with provable guarantee such as [11]. Monitoring can be applied on DNN inputs, intermediate values and outputs. Here, we focus on proactive XAI based monitor on DNN inputs given that it is reported by [12] [7] to be more effective when detecting current state of the art adversarial attacks."}, {"title": "2. Stealthy attack:", "content": "Traditional efforts focus on reducing attack space usage, reducing epsilon, reducing step size which could not achieve a good balance between stealth and fooling rate. This task in itself is data dependent and is difficult to extract general pattern without injecting prior knowledge on certain datasets to refine the task [13]. To address this, current research utilizes generative deep learning such as GAN to produce sparse attack noise [14] and a hybrid approach which involves an algorithm which does not need an auxiliary model and training and thus can be more economical [15], together with GAN the system achieves better balance and flexibility in practice. Weights and masks are often applied for guidance using Hadamard Product [16]. In our setting, we fix space usage, epsilon and step size and combining ideas of sparsity, auxiliary model and hadamard product projection into an attention mask guided PGD attack."}, {"title": "3. Explainable attack:", "content": "We focus on pixelwise saliency for attack explainability [17] as they can be of general use for attack guidance, explanation, defense and many other tasks. [18] mix Integrated Gradient with LRP via weighted relevance mask for better explainablity of transformers to outperform attention visualization. XAI mixture masks are used by [19] to improve the stealth of the attack as well as explainability. [20] propose attack towards explainable and efficient attack."}, {"title": "3. DESIGN", "content": ""}, {"title": "3.1. Metrics", "content": "Balance ratio for overall task effectiveness:"}, {"title": "3.2. Framework Design", "content": ""}, {"title": "3.2.1. Workflow", "content": "First we use Integrated Gradient (IG) and LRP to get explanations of the target model on given dataset, and then we use mixture algorithm to generate partial attention masks which can be used to guide PGD attack on target model or as partial label to training X-UNet to further improve attack stealth and explainability."}, {"title": "3.2.2. XAI safety monitor", "content": "The most common pattern is to first calculate the output value of adversarial examples via certain XAI methods [12] and then pass through a discriminator where different outputs result in a detection of adversary. Hence, the core task is to reduce the difference between clean image and adversarial examples across different metrics. Here, we simplify the task as reducing the Cosine Similarity [21] difference between adversarial examples and clean images."}, {"title": "3.2.4 X-UNet", "content": "We tried to modify every fundamental building blocks of the standard UNet as shown in Fig 1 & 2 and try to build a simple and elegant solution for our task. UNet configuration and novel building blocks inspired by [22] with state of the art components are dedicated for higher model discerning ability with efficiency in mind in the process of attention mask generation which ultimately makes our model output mask guided attack more stealthy. Loss:\n$\\lambda1*L1loss(advm,data)+\\lambda2*L1loss(mask,mix)+\\lambda3*\\delta_{acc}$"}, {"title": "3.2.5. Activation Function SLU(x,\na=0.5):", "content": "$\\max(0,x)+ a * \\sin(x)$"}, {"title": "3.2.6. Convolution weight initialization", "content": "Normal distribution with variance:\n$Var = \\frac{1}{2*inC+outC +outC * KernelSize}$"}, {"title": "4. RESULTS & EVALUATION", "content": ""}, {"title": "4.1. SOTA Attack benchmark", "content": "The experiments here are designed to show our attention mask can improve vanilla PGD in terms of stealth, explainability and efficiency as well as our guided PGD can outperform SOTA attack methods.\n\u2022 Attack efficiency benchmark: (average of 3 runs) 12%increase\n[clean accuracy baseline: 55%]\nOur method outperforms SOTA in terms of speed defined in metrics, despite SparseFool achieving best accuracy.\n\u2022 Attack stealth benchmark: (average of 3 runs) 10% increase\nOur method outperforms SOTA in terms of stealth, efficiency and explainability balance as in Table 4.1 though SINIFGSM provides 2.3% more stealth.\n\u2022 Attack explanation benchmark:\nSeveral key parts of digit 5 is assigned with higher weight and there are some background attack patterns"}, {"title": "4.2. Limitation/future improvements", "content": "\u2022 Efficiency: The performance is still slow for first time|G/LRP generation given data and model in whitebox setting. In this case, the time is not acceptable since the Integrated Gradient and LRP computation time is slower than attack itself by an order of 10. We can only assume an inference setting where data and model are fixed for a number of queries before recalculating XAI.\n\u2022 Explanation: Our framework provides mostly quali- tative explanation no enough mathematical guarantee compared to Integrated Gradient. A more robust metric should be employed."}, {"title": "5. CONCLUSION", "content": "In this paper, we presented our novel explainable efficient stealthy attack. Results have shown that our system can outperform benchmark vanilla PGD, Sparsefool and SINIFGSM in balance among stealth, efficiency and explainability. Results and pixelwise saliency provided by our novel framework can in turn benefit research for relevant tasks such as adversarial defense."}]}