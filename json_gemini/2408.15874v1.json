{"title": "Robust Statistical Scaling of Outlier Scores: Improving the Quality of Outlier Probabilities for Outliers (Extended Version)", "authors": ["Philipp R\u00f6chner", "Henrique O. Marques", "Ricardo J. G. B. Campello", "Arthur Zimek", "Franz Rothlauf"], "abstract": "Outlier detection algorithms typically assign an outlier score to each observation in a dataset, indicating the degree to which an observation is an outlier. However, these scores are often not comparable across algorithms and can be difficult for humans to interpret. Statistical scaling addresses this problem by transforming outlier scores into outlier probabilities without using ground-truth labels, thereby improving interpretability and comparability across algorithms. However, the quality of this transformation can be different for outliers and inliers. Missing outliers in scenarios where they are of particular interest such as health-care, finance, or engineering can be costly or dangerous. Thus, ensuring good probabilities for outliers is essential. This paper argues that statistical scaling, as commonly used in the literature, does not produce equally good probabilities for outliers as for inliers. Therefore, we propose robust statistical scaling, which uses robust estimators to improve the probabilities for outliers. We evaluate several variants of our method against other outlier score transformations for real-world datasets and outlier detection algorithms, where it can improve the probabilities for outliers.", "sections": [{"title": "1 Introduction", "content": "Outlier detection algorithms compute real-valued outlier scores to identify outliers, which are observations that are significantly different from the other observations in the dataset, the so-called inliers [11]. The rare outliers may be produced by a different mechanism than the inliers, in which case they are often referred to as anomalies [11,1]. Alternatively, outliers and inliers may result from the same underlying mechanism. In both cases, the outliers or anomalies are less likely to occur than the inliers [10].\n\nThe real-valued outlier scores are often difficult for humans to interpret and are not comparable across algorithms. Therefore, several transformations have been proposed to convert outlier scores into outlier probabilities [8,15,4,2,26], which quantifies the probability that an observation is an outlier [31].\n\nOutlier probabilities are commonly used. By transforming outlier scores into outlier probabilities, one can better separate outliers from inliers [8], normalize the outputs of multiple outlier detection algorithms for combining them into outlier ensembles [8,15,2], generate biased samples for constructing sequential outlier ensembles [30], compute weights for the internal evaluation of unsupervised outlier detection results [20,21], and quantify the sample-wise confidence of outlier detectors [26].\n\nGood outlier probabilities are concentrated around zero and one, called sharpness, separate outliers from inliers, called refinement, and reflect the frequency of outliers for observations with a similar outlier probability, called calibration [31]. It is usually important to discuss the quality of the probabilities for outliers and inliers separately: Users are often particularly interested in outliers, where it is necessary to determine good probabilities for outliers; otherwise, the probabilities of the outliers can be misleading to users and subsequent methods. If, for example, outlier score transformations underestimate the probabilities of outliers, important implausible information from cancer patients could be missed [32].\n\nTo our knowledge, there is a lack of research on the differences between the quality of probabilities of outliers and inliers. For supervised classification on imbalanced datasets, the quality of probabilities for observations from different classes can be significantly different [37]. In general, we expect the quality of probabilities to be different for outliers and inliers because outliers are rare compared to inliers and significantly different from inliers.\n\nWe study outlier score transformations that use only the outlier scores and no external information, such as which observation is an inlier or an outlier. We argue and empirically show that statistical scaling [15], a commonly used outlier score transformation [30,20], computes inferior probabilities for outliers than for inliers. Therefore, we propose robust statistical scaling, which uses robust estimators to compute outlier probabilities. We evaluate several variants of our method against other outlier score transformations for real-world datasets and outlier detection algorithms, where it can improve the probabilities for outliers.\n\nThe following section discusses the calibration of class probabilities in supervised learning and outlier score transformations. In Section 3, we define the problem of this study; in Section 4, we discuss the limitations of statistical scaling. Section 5 proposes robust statistical scaling using robust estimators. In Section 6, we describe our experiments and discuss our results in Section 7, demonstrating empirically that robust statistical scaling can improve the probabilities of outliers. Section 8 concludes our study."}, {"title": "2 Related Work", "content": "For supervised classification, improving probability estimates has been intensively studied [25]. Commonly used methods are Platt scaling [28], isotonic regression [39,40], and beta calibration [16]. On imbalanced datasets, standard supervised calibration approaches do not necessarily compute the class probabilities of the minority class well [37]. Therefore, it has been proposed to create multiple balanced datasets by down-sampling the majority class, training sigmoid functions on the down-sampled datasets, and averaging the calibrated probabilities for each observation. This improves the class probabilities of the minority class without drastically worsening the class probabilities of the majority class [37].\n\nUnlike the calibration of class probabilities in supervised classification, we explore the unsupervised transformation of outlier scores into outlier probabilities without using ground-truth labels for outliers and inliers. Outlier scores can be linearly scaled to the interval [0, 1], which is necessary but insufficient for outlier probabilities to be good, that is, sharp, refined, and calibrated [31]. Therefore, outlier score transformations, such as statistical scaling, have been proposed [8,15,26]."}, {"title": "3 Problem Statement", "content": "We study an n-dimensional real-valued dataset  X = {xi}i=1N with N observations, where xi \u2208 Rn, and an outlier detection algorithm Dx : Rn \u2192 R with outlier scores S := {si}i=1N, where si := Dx(xi).\n\nWe seek a transformation Ts : R \u2192 [0,1] of outlier scores S into outlier probabilities p = {pi}i=1N, where\n\npi := Ts(si) = Ts(Dx (xi)) \u2208 [0, 1],\n\nso that the probabilities p are sharp, refined, and calibrated for outliers and inliers: Sharp outlier probabilities are concentrated around zero and one, refined probabilities have pure ground-truth labels for observations with similar outlier probabilities, and calibrated outlier probabilities match the fraction of outliers for observations with similar outlier probabilities [31].\n\nThe outlier score transformation Ts is unsupervised; that is, it depends only on the outlier scores S and has no ground truth information about whether an observation is an outlier or an inlier."}, {"title": "4 Background: Non-robust Statistical Scaling of Outlier Scores", "content": "Statistical scaling first fits a parametric distribution to the frequency distribution of outlier scores computed by a given outlier detection algorithm on a dataset [15]. Therefore, the parameters of the parametric distributions are determined from the data using, for example, the method of moments (MoM) or maximum likelihood estimation (MLE). Outlier probabilities are then calculated using a modified cumulative distribution function of the approximated frequency distribution [15].\n\nIn the following, we refer to statistical scaling as non-robust statistical scaling [15]. We also limit the following discussion to non-robust Gaussian scaling, a variant of non-robust statistical scaling that uses Gaussian distributions, but similar arguments apply to other distributions.\n\nDefinition 1 (Non-robust Gaussian Scaling). For an outlier score distribution SCR and an outlier score s \u2208 S, its outlier probability using non-robust Gaussian scaling GSs is\n\nGSS(s)\n:= max (0, erf( (s \u2212 \u00b5s) / (\u03c3s\u221a2) )),\n\nwith the mean \u00b5s and the standard deviation (SD) \u03c3s of the Gaussian distribution fitted to the outlier scores S; erf is the error function.\n\nThe error function is a scaled and translated variant of the cumulative distribution function of the Gaussian distribution: it is sigmoid-shaped, monotonically increasing, and maps the real numbers to the interval ]-1, 1[. For x > 0 and a normally distributed random variable X with zero mean and a SD of 5, erf(x) is equal to the probability that X is in the interval [-x,x] [22].\n\nAccording to Equation (1) and because the Gaussian error function is negative for negative arguments, all observations with outlier scores less than or equal to the mean us have an outlier probability of zero. The error function maps the outlier scores greater than the mean \u00b5s to the interval [0, 1[.\n\nFor a Gaussian distribution, the parameters derived by the MoM are the same as those derived by MLE: the mean \u00b5s and the SD os of the Gaussian distribution are the sample mean and sample SD of the outlier scores S. The mean \u00b5s of the Gaussian distribution is also called the center or location; the SD os is also called the scale, spread, or dispersion.\n\nIn the following, we assume that a sufficiently well-functioning outlier detection algorithm has calculated the outlier scores such that high outlier scores correspond to outliers and low outlier scores correspond to inliers. We also assume that we have determined the parameters of the Gaussian distribution in Definition 1 using an approach sensitive to long-tailed distributions, such as the MoM or, equivalently, MLE.\n\nSince there are typically fewer outliers than inliers, the outlier score distribution of a proper outlier detection algorithm will have a long upper tail. This asymmetry of the outlier score distribution shifts the mean to the right because the mean is sensitive to extreme values. As a result, Gaussian scaling maps many outlier scores to an outlier probability of zero (see Equation (1)), which would be correct for inliers but incorrect for outliers. Because the SD is also sensitive to extreme values, the scale of the Gaussian distribution that approximates the"}, {"title": "5 Robust Statistical Scaling of Outlier Scores", "content": "We argued that non-robust statistical scaling, which fits a parametric distribution to the outlier scores in a way that is sensitive to the high scores for outliers, underestimates the probabilities of outliers. To improve the probabilities of outliers, we propose robust statistical scaling, which uses robust estimators to fit a distribution to the outlier scores. We discuss robust Gaussian scaling as an example of robust statistical scaling, but similar arguments apply to robust statistical scaling using other distributions.\n\nDefinition 2 (Robust Gaussian Scaling). For an outlier score distribution S C R and an outlier score s \u2208 S, its outlier probability using robust Gaussian scaling rGSs is\n\nrGSs(s) := max (0, erf( (s \u2212 \u00b5robust) / (\u03c3robust\u221a2) )),\n\nwith a center \u00b5robust and a scale orobust of the Gaussian distribution robustly fitted to the outlier scores S; erf is the error function.\n\nBecause a robustly fitted Gaussian distribution is less sensitive to extreme values, its center and scale are smaller than the sample mean and sample SD. As a result, robust Gaussian scaling maps fewer outlier scores to outlier probabilities of 0. Moreover, for outlier scores greater than the center of the outlier score distribution, the outlier probabilities increase faster. As a result, probabilities of outliers computed with robust Gaussian scaling are larger than those computed with non-robust Gaussian scaling.\n\nThere are several robust estimators for outlier score distributions. For example, we can replace the sample mean \u00b5s and SD os in Equation (1) with robust estimates for the center and scale of the outlier score distribution.\n\nExamples of robust estimators of a distribution's center are the median or the asymmetric trimmed mean [19]. For the asymmetric trimmed mean, we remove a certain percentage of the largest scores from the outlier score distribution, leaving the lower tail unchanged, and calculate the mean of the remaining outlier scores.\n\nThe normalized median absolute deviation from the median (nMAD), normalized interquartile range (nIQR), and trimmed standard deviation are examples of robust estimators of a distribution's scale [19]. The trimmed standard deviation removes large quadratic deviations between outlier scores and the distribution's center before calculating the average deviation.\n\nIn general, it is unclear how to choose how many outlier scores to trim because we have no ground-truth labels for evaluation. We can view the amount of trimming as a hyperparameter of the outlier score transformation and could use unsupervised model selection approaches to select it [20,42].\n\nInstead of computing a robust center and a robust scale separately, M-estimators can robustly estimate the center and scale of distributions simultaneously [19]: They iteratively estimate moments of distributions starting from initial values, such as the median for the center and the nMAD for the scale. The center and scale estimates are then updated based on the residuals between the current center and scale estimates and the observed data, with larger residuals weighted less than smaller residuals [19], which can be thought of as soft trimming [36]."}, {"title": "6 Experiments", "content": "We first compute outlier scores for real-world datasets using outlier detection algorithms (Section 6.1), then transform the outlier scores into outlier probabilities using outlier score transformations (Section 6.2), and finally evaluate their outlier probabilities (Section 6.3).\n\nWe compute outlier scores on 21 real-world datasets [6], excluding the KDD and ALOI datasets for computational reasons, using 11 outlier detection algorithms: Principal Component Analysis [33], Kernel Principal Component Analysis [12], Gaussian Mixture Model [7], k-Nearest Neighbors Detector [29], Local Outlier Factor [5], Isolation Forest [18], Histogram-Based Outlier Detection [9], Lightweight On-line Detector of Anomalies [27], Connectivity-Based Outlier Factor [35], Outlier Detection Based on Sampling [34], and Unsupervised Outlier Detection Using Empirical Cumulative Distribution Functions [17]. Combining the real-world datasets and outlier detection algorithms results in 231 sets of outlier scores. For all outlier detection algorithms, we used the Python library PyOD [41] and set the hyperparameters to their default values.\n\nWe investigate linear scaling [15], non-robust Gaussian scaling [15], and 10 variants of robust Gaussian scaling (Definition 2) to convert the 231 outlier score distributions mentioned above into outlier probabilities.\n\nFor non-robust Gaussian scaling in Equation (1), \u00b5s is the sample mean and as the sample SD of the outlier scores S.\n\nFor robust Gaussian scaling (Definition 2), we determine the center of the outlier score distributions by the median and asymmetric trimmed mean, removing 10% of the largest outlier scores; the scale of the distribution is computed by the nMAD and nIQR. For both parameters, we examine the combination between all robust estimators and the non-robust sample mean and SD. We chose 10% for trimming because it is typically larger than the percentage of outliers in a dataset and could remove the long upper tail of the outlier score distribution caused by the outliers. For robust Gaussian scaling with M-estimators, we use Huber's proposal 2 with Huber's T and Tukey's biweight, also called bisquare, as weighting functions to iteratively update the distribution's center and scale estimates depending on the residuals [14,19]."}, {"title": "6.3 Evaluation of Outlier Score Transformations", "content": "We evaluate outlier probabilities using the Brier score, sharpness, refinement, and calibration errors and their stratified variants [31]. We briefly explain sharpness, refinement, and calibration in Section 3. The Brier score measures a mixture of sharpness, refinement, and calibration. Since inliers can dominate the above measures, the stratified measures evaluate the quality of outliers and inliers separately [31]. All of the above measures have values between zero and one, with lower values corresponding to better outlier probabilities [31].\n\nIn our experiments, we measure sharpness using the average entropy, refinement using the Gini index computed for bins, and calibration using the binned L\u00b9 calibration error [31]. We compute the refinement and calibration errors using 5 to 20 equiareal bins and report the average [31].\n\nTo compare an outlier probability distribution p with reference outlier probabilities pref for an outlier probability measure M, we introduce a skill score [23]; this also makes the quality of outlier probabilities computed from different outlier score distributions better comparable.\n\nDefinition 3 (Skill Score). For an outlier probability measure M using ground-truth labels y, the skill score MSS of outlier probabilities p with respect to reference outlier probabilities Pref CRis\n\nMSS(p, pref) := MSS(p, pref, , Pref, y) := - log2( M(p, y) / M(pref, y) )\n\nThe negative logarithm in Equation (3) maps the values in the intervals [0, 1] and [1,\u221e[ of the ratio (p.y) / M(pre) to the visually better comparable intervals [0, 8[ and] - \u221e,0].\n\nThe Brier score, sharpness, refinement, and calibration errors are candidates for the outlier probability measure M in Definition 3 (skill score). For these measures, a positive skill score indicates better, a skill score of zero indicates equal, and a negative skill score indicates inferior outlier probabilities p compared to the reference outlier probabilities pref.\n\nTo simultaneously evaluate the quality of an outlier probability distribution for all three measures, we aggregate the sharpness, refinement, and calibration errors into a single number. Although these measures theoretically have values between zero and one, they are not necessarily comparable, and we cannot aggregate them directly. Therefore, we examine the improvement of an outlier probability distribution over a randomly selected outlier probability distribution, where we approximate the expected value of the performance of the randomly selected outlier probability distribution by the average performance of all the outlier probability distributions examined. In other words, we first normalize the measures by dividing them by the average performance of all the outlier probability distributions examined. Similar to the F\u2081 score for binary classification, we then compute the harmonic improvement score as the harmonic mean of these normalized values across different measures."}, {"title": "7 Results", "content": "Are the outlier probabilities computed by non-robust Gaussian scaling similarly good for outliers and inliers?\n\nFirst, we evaluate whether non-robust Gaussian scaling has equally good probabilities for outliers and inliers. Figure 2 shows the skill score MSS (Definition 3) of non-robust Gaussian scaling for the stratified Brier score, calibration, refinement, and separation errors for outliers compared to the corresponding stratified measure for inliers.\n\nFor all four outlier probability measures examined, the distributions of the skill scores for non-robust Gaussian scaling are predominantly negative; that is, for non-robust Gaussian scaling, the stratified measures for outliers are inferior to those for inliers, consistent with our discussion in Section 4."}, {"title": "7.2 Does robust Gaussian scaling improve the probabilities of outliers compared to non-robust Gaussian scaling?", "content": "We examine whether robust Gaussian scaling improves the probabilities of outliers compared to non-robust Gaussian scaling. Figure 3 shows skill scores MSS (Definition 3) comparing the stratified Brier score, sharpness, refinement, and calibration errors for outliers of linear scaling and variants of robust Gaussian scaling Toutlier with non-robust Gaussian scaling GSoutlier.\n\nCompared to non-robust Gaussian scaling, all variants of robust Gaussian scaling shift the distributions of the stratified Brier skill scores for outliers above zero; this means that overall, the probabilities for outliers computed by robust Gaussian scaling have better Brier scores than those computed by non-robust Gaussian scaling. Robust Gaussian scaling with trimmed mean as center and nMAD as scale improves the Brier score for outliers the most.\n\nMost variants of robust Gaussian scaling improve the sharpness and refinement of probabilities for outliers compared to non-robust Gaussian scaling. The stratified sharpness error for outliers is best for the mean as the center and nMAD as the scale for robust Gaussian scaling.\n\nLinear scaling improves the stratified refinement and calibration errors for outliers the most compared to non-robust Gaussian scaling, followed by robust Gaussian scaling using the median and nMAD for the refinement and Huber's T for the calibration error.\n\nAs expected, robust Gaussian scaling can improve the Brier score, sharpness, and refinement of outliers compared to non-robust Gaussian scaling; only its calibration is inferior."}, {"title": "7.3 Which Gaussian scaling variant best balances the overall quality of the probabilities for outliers and inliers?", "content": "To examine the effect of robust Gaussian scaling on the probabilities of outliers and inliers, we evaluate the probabilities when outliers and inliers, as well as sharpness, refinement, and calibration, are of equal importance. Figure 4 shows the average rank of linear scaling, non-robust, and robust Gaussian scaling variants over the studied outlier score distributions for the harmonic improvement"}, {"title": "8 Conclusions", "content": "We argue and empirically demonstrate that non-robust statistical scaling, a commonly used outlier score transformation, computes inferior probabilities for outliers than for inliers. Therefore, we propose robust statistical scaling using robust estimators. We empirically evaluate several variants of our method and show that it can improve the probabilities of outliers. When outliers and inliers, as well as sharpness, refinement, and calibration, are equally important, statistical scaling with a non-robust center and a robust scale performed best.\n\nThis study investigated unsupervised outlier score transformations that do not use ground-truth labels. We used, however, external knowledge about which observations are inliers and which are outliers to evaluate the outlier score transformations. In future work, we plan to evaluate outlier probabilities using unsupervised internal measures without using ground-truth labels for outliers and inliers."}]}